{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport the class for the given class name.", "response": "def import_class(class_path):\n    '''\n        Imports the class for the given class name.\n    '''\n    module_name, class_name = class_path.rsplit(\".\", 1)\n    module = import_module(module_name)\n    claz = getattr(module, class_name)\n    return claz"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _executor(self):\n        '''\n            Creating an ExecutorPool is a costly operation. Executor needs to be instantiated only once.\n        '''\n        if self.EXECUTE_PARALLEL is False:\n            executor_path = \"batch_requests.concurrent.executor.SequentialExecutor\"\n            executor_class = import_class(executor_path)\n            return executor_class()\n        else:\n            executor_path = self.CONCURRENT_EXECUTOR\n            executor_class = import_class(executor_path)\n            return executor_class(self.NUM_WORKERS)", "response": "Creating an ExecutorPool is a costly operation. Executor needs to be instantiated only once."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a new label for a key in the bucket.", "response": "def make_label(self, path):\n        \"\"\"\n        this borrows too much from the internals of ofs\n        maybe expose different parts of the api?\n        \"\"\"\n        from datetime import datetime\n        from StringIO import StringIO\n        path = path.lstrip(\"/\")\n        bucket, label = path.split(\"/\", 1)\n\n        bucket = self.ofs._require_bucket(bucket)\n        key = self.ofs._get_key(bucket, label)\n        if key is None:\n            key = bucket.new_key(label)\n            self.ofs._update_key_metadata(key, { '_creation_time': str(datetime.utcnow()) })\n            key.set_contents_from_file(StringIO(''))\n        key.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the host and headers for the remote", "response": "def get_proxy_config(self, headers, path):\n        \"\"\"\n        stub. this really needs to be a call to the remote\n        restful interface to get the appropriate host and\n        headers to use for this upload\n        \"\"\"\n        self.ofs.conn.add_aws_auth_header(headers, 'PUT', path)\n        from pprint import pprint\n        pprint(headers)\n        host = self.ofs.conn.server_name()\n        return host, headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proxy_upload(self, path, filename, content_type=None, content_encoding=None,\n                     cb=None, num_cb=None):\n        \"\"\"\n        This is the main function that uploads. We assume the bucket\n        and key (== path) exists. What we do here is simple. Calculate\n        the headers we will need, (e.g. md5, content-type, etc). Then\n        we ask the self.get_proxy_config method to fill in the authentication\n        information and tell us which remote host we should talk to\n        for the upload. From there, the rest is ripped from\n        boto.key.Key.send_file\n        \"\"\"\n        from boto.connection import AWSAuthConnection\n        import mimetypes\n        from hashlib import md5\n        import base64\n\n        BufferSize = 65536 ## set to something very small to make sure\n                           ## chunking is working properly\n        fp = open(filename)\n\n        headers = { 'Content-Type': content_type }\n\n        if content_type is None:\n            content_type = mimetypes.guess_type(filename)[0] or \"text/plain\"\n        headers['Content-Type'] = content_type\n        if content_encoding is not None:\n            headers['Content-Encoding'] = content_encoding\n\n        m = md5()\n        fp.seek(0)\n        s = fp.read(BufferSize)\n        while s:\n            m.update(s)\n            s = fp.read(BufferSize)\n        self.size = fp.tell()\n        fp.seek(0)\n\n        self.md5 = m.hexdigest()\n        headers['Content-MD5'] = base64.encodestring(m.digest()).rstrip('\\n')\n        headers['Content-Length'] = str(self.size)\n\n        headers['Expect'] = '100-Continue'\n\n        host, headers = self.get_proxy_config(headers, path)\n\n        ### how to do this same thing with curl instead...\n        print(\"curl -i --trace-ascii foo.log -T %s -H %s https://%s%s\" % (\n            filename,\n            \" -H \".join(\"'%s: %s'\" % (k,v) for k,v in headers.items()),\n            host, path\n            ))\n\n        def sender(http_conn, method, path, data, headers):\n            http_conn.putrequest(method, path)\n            for key in headers:\n                http_conn.putheader(key, headers[key])\n            http_conn.endheaders()\n            fp.seek(0)\n            http_conn.set_debuglevel(0) ### XXX set to e.g. 4 to see what going on\n            if cb:\n                if num_cb > 2:\n                    cb_count = self.size / BufferSize / (num_cb-2)\n                elif num_cb < 0:\n                    cb_count = -1\n                else:\n                    cb_count = 0\n                i = total_bytes = 0\n                cb(total_bytes, self.size)\n            l = fp.read(BufferSize)\n            while len(l) > 0:\n                http_conn.send(l)\n                if cb:\n                    total_bytes += len(l)\n                    i += 1\n                    if i == cb_count or cb_count == -1:\n                        cb(total_bytes, self.size)\n                        i = 0\n                l = fp.read(BufferSize)\n            if cb:\n                cb(total_bytes, self.size)\n            response = http_conn.getresponse()\n            body = response.read()\n            fp.seek(0)\n            if response.status == 500 or response.status == 503 or \\\n                    response.getheader('location'):\n                # we'll try again\n                return response\n            elif response.status >= 200 and response.status <= 299:\n                self.etag = response.getheader('etag')\n                if self.etag != '\"%s\"'  % self.md5:\n                    raise Exception('ETag from S3 did not match computed MD5')\n                return response\n            else:\n                #raise provider.storage_response_error(\n                #    response.status, response.reason, body)\n                raise Exception(response.status, response.reason, body)\n\n        awsc = AWSAuthConnection(host,\n                                 aws_access_key_id=\"key_id\",\n                                 aws_secret_access_key=\"secret\")\n        awsc._mexe('PUT', path, None, headers, sender=sender)", "response": "This function is the main function that uploads a file to a bucket and returns a dictionary of the key - value pairs that we will use to store the file in the bucket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_all_mood_stations(self, terr=KKBOXTerritory.TAIWAN):\n        '''\n        Fetches all mood stations.\n\n        :param terr: the current territory.\n        :return: API response.\n        :rtype: dict\n\n        See `https://docs-en.kkbox.codes/v1.1/reference#moodstations`.\n        '''\n        url = 'https://api.kkbox.com/v1.1/mood-stations'\n        url += '?' + url_parse.urlencode({'territory': terr})\n        return self.http._post_data(url, None, self.http._headers_with_access_token())", "response": "Fetches all mood stations."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a mood station by given ID.", "response": "def fetch_mood_station(self, station_id, terr=KKBOXTerritory.TAIWAN):\n        '''\n        Fetches a mood station by given ID.\n\n        :param station_id: the station ID\n        :param terr: the current territory.\n        :return: API response.\n        :rtype: dict\n\n        See `https://docs-en.kkbox.codes/v1.1/reference#moodstations-station_id`.\n        '''\n        url = 'https://api.kkbox.com/v1.1/mood-stations/%s' % station_id\n        url += '?' + url_parse.urlencode({'territory': terr})\n        return self.http._post_data(url, None, self.http._headers_with_access_token())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch next page based on previously fetched data.", "response": "def fetch_next_page(self, data):\n        '''\n        Fetches next page based on previously fetched data.\n        Will get the next page url from data['paging']['next'].\n\n        :param data: previously fetched API response.\n        :type data: dict        \n        :return: API response.\n        :rtype: dict\n        '''\n        next_url = data['paging']['next']\n        if next_url != None:\n            next_data = self.http._post_data(next_url, None, self.http._headers_with_access_token())\n            return next_data\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch data from specific url.", "response": "def fetch_data(self, url):\n        ''' \n        Fetches data from specific url.\n\n        :return: The response.\n        :rtype: dict\n        '''\n        return self.http._post_data(url, None, self.http._headers_with_access_token())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a shared playlist by given ID.", "response": "def fetch_shared_playlist(self, playlist_id, terr=KKBOXTerritory.TAIWAN):\n        '''\n        Fetches a shared playlist by given ID.\n\n        :param playlist_id: the playlist ID.\n        :type playlist_id: str\n        :param terr: the current territory.\n        :return: API response.\n        :rtype: dictcd\n\n        See `https://docs-en.kkbox.codes/v1.1/reference#sharedplaylists-playlist_id`.\n        '''\n        url = 'https://api.kkbox.com/v1.1/shared-playlists/%s' % playlist_id\n        url += '?' + url_parse.urlencode({'territory': terr})\n        return self.http._post_data(url, None, self.http._headers_with_access_token())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a FirewallRule object based on a server uuid and rule position.", "response": "def get_firewall_rule(self, server_uuid, firewall_rule_position, server_instance=None):\n        \"\"\"\n        Return a FirewallRule object based on server uuid and rule position.\n        \"\"\"\n        url = '/server/{0}/firewall_rule/{1}'.format(server_uuid, firewall_rule_position)\n        res = self.get_request(url)\n        return FirewallRule(**res['firewall_rule'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all firewall rules based on a server instance or uuid.", "response": "def get_firewall_rules(self, server):\n        \"\"\"\n        Return all FirewallRule objects based on a server instance or uuid.\n        \"\"\"\n        server_uuid, server_instance = uuid_and_instance(server)\n\n        url = '/server/{0}/firewall_rule'.format(server_uuid)\n        res = self.get_request(url)\n\n        return [\n            FirewallRule(server=server_instance, **firewall_rule)\n            for firewall_rule in res['firewall_rules']['firewall_rule']\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_firewall_rule(self, server, firewall_rule_body):\n        server_uuid, server_instance = uuid_and_instance(server)\n\n        url = '/server/{0}/firewall_rule'.format(server_uuid)\n        body = {'firewall_rule': firewall_rule_body}\n        res = self.post_request(url, body)\n\n        return FirewallRule(server=server_instance, **res['firewall_rule'])", "response": "Create a new firewall rule for a given server uuid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a firewall rule based on a server uuid and rule position.", "response": "def delete_firewall_rule(self, server_uuid, firewall_rule_position):\n        \"\"\"\n        Delete a firewall rule based on a server uuid and rule position.\n        \"\"\"\n        url = '/server/{0}/firewall_rule/{1}'.format(server_uuid, firewall_rule_position)\n        return self.request('DELETE', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(self, data):\n        uri = '{}/sinkhole'.format(self.client.remote)\n        self.logger.debug(uri)\n\n        if PYVERSION == 2:\n            try:\n                data = data.decode('utf-8')\n            except Exception:\n                data = data.decode('latin-1')\n\n        data = {\n            'message': data\n        }\n\n        body = self.client.post(uri, data)\n        return body", "response": "POST a raw SMTP message to the Sinkhole API"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the lowered method. Capitalize headers, prepend HTTP_ and change - to _.", "response": "def pre_process_method_headers(method, headers):\n    '''\n        Returns the lowered method.\n        Capitalize headers, prepend HTTP_ and change - to _.\n    '''\n    method = method.lower()\n\n    # Standard WSGI supported headers\n    _wsgi_headers = [\"content_length\", \"content_type\", \"query_string\",\n                     \"remote_addr\", \"remote_host\", \"remote_user\",\n                     \"request_method\", \"server_name\", \"server_port\"]\n\n    _transformed_headers = {}\n\n    # For every header, replace - to _, prepend http_ if necessary and convert\n    # to upper case.\n    for header, value in headers.items():\n\n        header = header.replace(\"-\", \"_\")\n        header = \"http_{header}\".format(\n            header=header) if header.lower() not in _wsgi_headers else header\n        _transformed_headers.update({header.upper(): value})\n\n    return method, _transformed_headers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef headers_to_include_from_request(curr_request):\n    '''\n        Define headers that needs to be included from the current request.\n    '''\n    return {\n        h: v for h, v in curr_request.META.items() if h in _settings.HEADERS_TO_INCLUDE}", "response": "Define headers that need to be included from the current request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_wsgi_request_object(curr_request, method, url, headers, body):\n    '''\n        Based on the given request parameters, constructs and returns the WSGI request object.\n    '''\n    x_headers = headers_to_include_from_request(curr_request)\n    method, t_headers = pre_process_method_headers(method, headers)\n\n    # Add default content type.\n    if \"CONTENT_TYPE\" not in t_headers:\n        t_headers.update({\"CONTENT_TYPE\": _settings.DEFAULT_CONTENT_TYPE})\n\n    # Override existing batch requests headers with the new headers passed for this request.\n    x_headers.update(t_headers)\n\n    content_type = x_headers.get(\"CONTENT_TYPE\", _settings.DEFAULT_CONTENT_TYPE)\n\n    # Get hold of request factory to construct the request.\n    _request_factory = BatchRequestFactory()\n    _request_provider = getattr(_request_factory, method)\n\n    secure = _settings.USE_HTTPS\n\n    request = _request_provider(url, data=body, secure=secure,\n                                content_type=content_type, **x_headers)\n\n    return request", "response": "Constructs and returns the WSGI request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _base_environ(self, **request):\n        '''\n            Override the default values for the wsgi environment variables.\n        '''\n        # This is a minimal valid WSGI environ dictionary, plus:\n        # - HTTP_COOKIE: for cookie support,\n        # - REMOTE_ADDR: often useful, see #8551.\n        # See http://www.python.org/dev/peps/pep-3333/#environ-variables\n\n        environ = {\n            'HTTP_COOKIE': self.cookies.output(header='', sep='; '),\n            'PATH_INFO': str('/'),\n            'REMOTE_ADDR': str('127.0.0.1'),\n            'REQUEST_METHOD': str('GET'),\n            'SCRIPT_NAME': str(''),\n            'SERVER_NAME': str('localhost'),\n            'SERVER_PORT': str('8000'),\n            'SERVER_PROTOCOL': str('HTTP/1.1'),\n            'wsgi.version': (1, 0),\n            'wsgi.url_scheme': str('http'),\n            'wsgi.input': FakePayload(b''),\n            'wsgi.errors': self.errors,\n            'wsgi.multiprocess': True,\n            'wsgi.multithread': True,\n            'wsgi.run_once': False,\n        }\n        environ.update(self.defaults)\n        environ.update(request)\n        return environ", "response": "Returns a dictionary of the wsgi environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a request to a given endpoint in UpCloud s API.", "response": "def request(self, method, endpoint, body=None, timeout=-1):\n        \"\"\"\n        Perform a request with a given body to a given endpoint in UpCloud's API.\n\n        Handles errors with __error_middleware.\n        \"\"\"\n        if method not in set(['GET', 'POST', 'PUT', 'DELETE']):\n            raise Exception('Invalid/Forbidden HTTP method')\n\n        url = '/' + self.api_v + endpoint\n        headers = {\n            'Authorization': self.token,\n            'Content-Type': 'application/json'\n        }\n\n        if body:\n            json_body_or_None = json.dumps(body)\n        else:\n            json_body_or_None = None\n\n        call_timeout = timeout if timeout != -1 else self.timeout\n\n        APIcall = getattr(requests, method.lower())\n        res = APIcall('https://api.upcloud.com' + url,\n                      data=json_body_or_None,\n                      headers=headers,\n                      timeout=call_timeout)\n\n        if res.text:\n            res_json = res.json()\n        else:\n            res_json = {}\n\n        return self.__error_middleware(res, res_json)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_request(self, endpoint, body=None, timeout=-1):\n        return self.request('POST', endpoint, body, timeout)", "response": "Perform a POST request to a given endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_stream(self, bucket, label, stream_object, params={}):\n        ''' Create a new file to swift object storage. '''\n        self.claim_bucket(bucket) \n        self.connection.put_object(bucket, label, stream_object,\n                                   headers=self._convert_to_meta(params))", "response": "Create a new file to swift object storage."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef h(gbm, array_or_frame, indices_or_columns = 'all'):\n\n    if indices_or_columns == 'all':\n        if gbm.max_depth < array_or_frame.shape[1]:\n            raise \\\n                Exception(\n                    \"gbm.max_depth == {} < array_or_frame.shape[1] == {}, so indices_or_columns must not be 'all'.\"\n                    .format(gbm.max_depth, array_or_frame.shape[1])\n                )\n    else:\n        if gbm.max_depth < len(indices_or_columns):\n            raise \\\n                Exception(\n                    \"gbm.max_depth == {}, so indices_or_columns must contain at most {} {}.\"\n                    .format(gbm.max_depth, gbm.max_depth, \"element\" if gbm.max_depth == 1 else \"elements\")\n                )\n    check_args_contd(array_or_frame, indices_or_columns)\n\n    arr, model_inds = get_arr_and_model_inds(array_or_frame, indices_or_columns)\n\n    width = arr.shape[1]\n    f_vals = {}\n    for n in range(width, 0, -1):\n        for inds in itertools.combinations(range(width), n):\n            f_vals[inds] = compute_f_vals(gbm, model_inds, arr, inds)\n\n    return compute_h_val(f_vals, arr, tuple(range(width)))", "response": "This function computes Friedman and Popescu s H statistic in order to look for an interaction among the variables represented by the elements of the array or frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npurposing Compute Friedman and Popescu's two-variable H statistic, in order to look for an interaction in the passed gradient- boosting model between each pair of variables represented by the elements of the passed array or frame and specified by the passed indices or columns. See Jerome H. Friedman and Bogdan E. Popescu, 2008, \"Predictive learning via rule ensembles\", Ann. Appl. Stat. 2:916-954, http://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908046, s. 8.1. ARGUMENTS gbm should be a scikit-learn gradient-boosting model (instance of sklearn.ensemble.GradientBoostingClassifier or sklearn.ensemble.GradientBoostingRegressor) that has been fitted to array_or_frame (and a target, not used here). array_or_frame should be a two-dimensional NumPy array or a pandas data frame (instance of numpy.ndarray or pandas .DataFrame). indices_or_columns is optional, with default value 'all'. It should be 'all' or a list of indices of columns of array_or_frame if array_or_frame is a NumPy array or a list of columns of array_or_frame if array_or_frame is a pandas data frame. If it is 'all', then all columns of array_or_frame are used. RETURNS A dict whose keys are pairs (2-tuples) of indices or columns and whose values are the H statistic of the pairs of variables or NaN if a computation is spoiled by weak main effects and rounding errors. H varies from 0 to 1. The larger H, the stronger the evidence for an interaction between a pair of variables. EXAMPLE Friedman and Popescu's (2008) formula (44) for every j and k corresponds to h_all_pairs(F, x) NOTES 1. Per Friedman and Popescu, only variables with strong main effects should be examined for interactions. Strengths of main effects are available as gbm.feature_importances_ once gbm has been fitted. 2. Per Friedman and Popescu, collinearity among variables can lead to interactions in gbm that are not present in the target function. To forestall such spurious interactions, check for strong correlations among variables before fitting gbm.", "response": "def h_all_pairs(gbm, array_or_frame, indices_or_columns = 'all'):\n    \"\"\"\n    PURPOSE\n\n    Compute Friedman and Popescu's two-variable H statistic, in order to look for an interaction in the passed gradient-\n    boosting model between each pair of variables represented by the elements of the passed array or frame and specified\n    by the passed indices or columns.\n\n    See Jerome H. Friedman and Bogdan E. Popescu, 2008, \"Predictive learning via rule ensembles\", Ann. Appl. Stat.\n    2:916-954, http://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908046, s. 8.1.\n\n\n    ARGUMENTS\n\n    gbm should be a scikit-learn gradient-boosting model (instance of sklearn.ensemble.GradientBoostingClassifier or\n    sklearn.ensemble.GradientBoostingRegressor) that has been fitted to array_or_frame (and a target, not used here).\n\n    array_or_frame should be a two-dimensional NumPy array or a pandas data frame (instance of numpy.ndarray or pandas\n    .DataFrame).\n\n    indices_or_columns is optional, with default value 'all'. It should be 'all' or a list of indices of columns of\n    array_or_frame if array_or_frame is a NumPy array or a list of columns of array_or_frame if array_or_frame is a\n    pandas data frame. If it is 'all', then all columns of array_or_frame are used.\n\n\n    RETURNS\n\n    A dict whose keys are pairs (2-tuples) of indices or columns and whose values are the H statistic of the pairs of\n    variables or NaN if a computation is spoiled by weak main effects and rounding errors.\n\n    H varies from 0 to 1. The larger H, the stronger the evidence for an interaction between a pair of variables.\n\n\n    EXAMPLE\n\n    Friedman and Popescu's (2008) formula (44) for every j and k corresponds to\n\n        h_all_pairs(F, x)\n\n\n    NOTES\n\n    1. Per Friedman and Popescu, only variables with strong main effects should be examined for interactions. Strengths \n    of main effects are available as gbm.feature_importances_ once gbm has been fitted.\n\n    2. Per Friedman and Popescu, collinearity among variables can lead to interactions in gbm that are not present in\n    the target function. To forestall such spurious interactions, check for strong correlations among variables before\n    fitting gbm.\n    \"\"\"\n\n    if gbm.max_depth < 2:\n        raise Exception(\"gbm.max_depth must be at least 2.\")\n    check_args_contd(array_or_frame, indices_or_columns)\n\n    arr, model_inds = get_arr_and_model_inds(array_or_frame, indices_or_columns)\n\n    width = arr.shape[1]\n    f_vals = {}\n    for n in [2, 1]:\n        for inds in itertools.combinations(range(width), n):\n            f_vals[inds] = compute_f_vals(gbm, model_inds, arr, inds)\n\n    h_vals = {}\n    for inds in itertools.combinations(range(width), 2):\n        h_vals[inds] = compute_h_val(f_vals, arr, inds)\n    if indices_or_columns != 'all':\n        h_vals = {tuple(model_inds[(inds,)]): h_vals[inds] for inds in h_vals.keys()}\n    if not isinstance(array_or_frame, np.ndarray):\n        all_cols = array_or_frame.columns.values\n        h_vals = {tuple(all_cols[(inds,)]): h_vals[inds] for inds in h_vals.keys()}\n\n    return h_vals"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, q, limit=None):\n        uri = '{}/predict?q={}'.format(self.client.remote, q)\n        self.logger.debug(uri)\n\n        body = self.client.get(uri)\n        return body['score']", "response": "Performs a search against the predict endpoint and returns the score of the item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exists(self, bucket, label):\n        '''Whether a given bucket:label object already exists.'''\n        fn = self._zf(bucket, label)\n        try:\n            self.z.getinfo(fn)\n            return True\n        except KeyError:\n            return False", "response": "Whether a given bucket label object already exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_labels(self, bucket):\n        '''List labels for the given bucket. Due to zipfiles inherent arbitrary ordering,\n        this is an expensive operation, as it walks the entire archive searching for individual\n        'buckets'\n\n        :param bucket: bucket to list labels for.\n        :return: iterator for the labels in the specified bucket.\n        '''\n        for name in self.z.namelist():\n            container, label = self._nf(name.encode(\"utf-8\"))\n            if container == bucket and label != MD_FILE:\n                yield label", "response": "List the labels for the given bucket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all buckets managed by this OFS instance.", "response": "def list_buckets(self):\n        '''List all buckets managed by this OFS instance. Like list_labels, this also\n        walks the entire archive, yielding the bucketnames. A local set is retained so that\n        duplicates aren't returned so this will temporarily pull the entire list into memory\n        even though this is a generator and will slow as more buckets are added to the set.\n\n        :return: iterator for the buckets.\n        '''\n        buckets = set()\n        for name in self.z.namelist():\n            bucket, _ = self._nf(name)\n            if bucket not in buckets:\n                buckets.add(bucket)\n                yield bucket"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a bitstream for the given bucket and label combination.", "response": "def get_stream(self, bucket, label, as_stream=True):\n        '''Get a bitstream for the given bucket:label combination.\n\n        :param bucket: the bucket to use.\n        :return: bitstream as a file-like object\n        '''\n        if self.mode == \"w\":\n            raise OFSException(\"Cannot read from archive in 'w' mode\")\n        elif self.exists(bucket, label):\n            fn = self._zf(bucket, label)\n            if as_stream:\n                return self.z.open(fn)\n            else:\n                return self.z.read(fn)\n        else:\n            raise OFSFileNotFound"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_url(self, bucket, label):\n        '''Get a URL that should point at the bucket:labelled resource. Aimed to aid web apps by allowing them to redirect to an open resource, rather than proxy the bitstream.\n\n        :param bucket: the bucket to use.\n        :param label: the label of the resource to get\n        :return: a string URI - eg 'zip:file:///home/.../foo.zip!/bucket/label'\n        '''\n        if self.exists(bucket, label):\n            root = \"zip:file//%s\" % os.path.abspath(self.zipfile)\n            fn = self._zf(bucket, label)\n            return \"!/\".join(root, fn)\n        else:\n            raise OFSFileNotFound", "response": "Get a URL that should point at the bucket labelled resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put_stream(self, bucket, label, stream_object, params=None, replace=True, add_md=True):\n        '''Put a bitstream (stream_object) for the specified bucket:label identifier.\n\n        :param bucket: as standard\n        :param label: as standard\n        :param stream_object: file-like object to read from or bytestring.\n        :param params: update metadata with these params (see `update_metadata`)\n        '''\n        if self.mode == \"r\":\n            raise OFSException(\"Cannot write into archive in 'r' mode\")\n        else:\n            params = params or {}\n            fn = self._zf(bucket, label)\n            params['_creation_date'] = datetime.now().isoformat().split(\".\")[0]  ## '2010-07-08T19:56:47'\n            params['_label'] = label\n            if self.exists(bucket, label) and replace==True:\n                # Add then Replace? Let's see if that works...\n                #z = ZipFile(self.zipfile, self.mode, self.compression, self.allowZip64)\n                zinfo = self.z.getinfo(fn)\n                size, chksum = self._write(self.z, bucket, label, stream_object)\n                self._del_stream(zinfo)\n                #z.close()\n                params['_content_length'] = size\n                if chksum:\n                    params['_checksum'] = chksum\n            else:\n                #z = ZipFile(self.zipfile, self.mode, self.compression, self.allowZip64)\n                size, chksum = self._write(self.z, bucket, label, stream_object)\n                #z.close()\n                params['_content_length'] = size\n                if chksum:\n                    params['_checksum'] = chksum\n            if add_md:\n                params = self.update_metadata(bucket, label, params)\n            return params", "response": "Put a bitstream ( stream_object for the specified bucket label identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef del_stream(self, bucket, label):\n        '''Delete a bitstream. This needs more testing - file deletion in a zipfile\n        is problematic. Alternate method is to create second zipfile without the files\n        in question, which is not a nice method for large zip archives.\n        '''\n        if self.exists(bucket, label):\n            name = self._zf(bucket, label)\n            #z = ZipFile(self.zipfile, self.mode, self.compression, self.allowZip64)\n            self._del_stream(name)", "response": "Delete a bitstream. This needs more testing - file deletion in a zipfile\n        is problematic."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the metadata for this bucket and label identifier.", "response": "def get_metadata(self, bucket, label):\n        '''Get the metadata for this bucket:label identifier.\n        '''\n        if self.mode !=\"w\":\n            try:\n                jsn = self._get_bucket_md(bucket)\n            except OFSFileNotFound:\n                # No MD found...\n                return {}\n            except OFSException as e:\n                raise OFSException(e)\n            if label in jsn:\n                return jsn[label]\n            else:\n                return {}\n        else:\n            raise OFSException(\"Cannot read md from archive in 'w' mode\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_metadata(self, bucket, label, params):\n        '''Update the metadata with the provided dictionary of params.\n\n        :param parmams: dictionary of key values (json serializable).\n        '''\n        if self.mode !=\"r\":\n            try:\n                payload = self._get_bucket_md(bucket)\n            except OFSFileNotFound:\n                # No MD found... create it\n                payload = {}\n                for l in self.list_labels(bucket):\n                    payload[l] = {}\n                    payload[l]['_label'] = l\n                if not self.quiet:\n                    print(\"Had to create md file for %s\" % bucket)\n            except OFSException as e:\n                raise OFSException(e)\n            if not label in payload:\n                payload[label] = {}\n            payload[label].update(params)\n            self.put_stream(bucket, MD_FILE, json.dumps(payload).encode('utf-8'), params={}, replace=True, add_md=False)\n            return payload[label]\n        else:\n            raise OFSException(\"Cannot update MD in archive in 'r' mode\")", "response": "Update the metadata with the provided dictionary of params."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef del_metadata_keys(self, bucket, label, keys):\n        '''Delete the metadata corresponding to the specified keys.\n        '''\n        if self.mode !=\"r\":\n            try:\n                payload = self._get_bucket_md(bucket)\n            except OFSFileNotFound:\n                # No MD found...\n                raise OFSFileNotFound(\"Couldn't find a md file for %s bucket\" % bucket)\n            except OFSException as e:\n                raise OFSException(e)\n            if payload.has_key(label):\n                for key in [x for x in keys if payload[label].has_key(x)]:\n                    del payload[label][key]\n            self.put_stream(bucket, MD_FILE, json.dumps(payload), params={}, replace=True, add_md=False)\n        else:\n            raise OFSException(\"Cannot update MD in archive in 'r' mode\")", "response": "Delete the metadata corresponding to the specified keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_response(wsgi_request):\n    '''\n        Given a WSGI request, makes a call to a corresponding view\n        function and returns the response.\n    '''\n    service_start_time = datetime.now()\n    # Get the view / handler for this request\n    view, args, kwargs = resolve(wsgi_request.path_info)\n\n    kwargs.update({\"request\": wsgi_request})\n\n    # Let the view do his task.\n    try:\n        resp = view(*args, **kwargs)\n    except Exception as exc:\n        resp = HttpResponseServerError(content=exc.message)\n\n    headers = dict(resp._headers.values())\n    # Convert HTTP response into simple dict type.\n    d_resp = {\"status_code\": resp.status_code, \"reason_phrase\": resp.reason_phrase,\n              \"headers\": headers}\n    try:\n        d_resp.update({\"body\": resp.content})\n    except ContentNotRenderedError:\n        resp.render()\n        d_resp.update({\"body\": resp.content})\n\n    # Check if we need to send across the duration header.\n    if _settings.ADD_DURATION_HEADER:\n        d_resp['headers'].update({_settings.DURATION_HEADER_NAME: (datetime.now() - service_start_time).seconds})\n\n    return d_resp", "response": "Given a WSGI request returns a response dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the individual requests and create a new WSGIRequest object for each.", "response": "def get_wsgi_requests(request):\n    '''\n        For the given batch request, extract the individual requests and create\n        WSGIRequest object for each.\n    '''\n    valid_http_methods = [\"get\", \"post\", \"put\", \"patch\", \"delete\", \"head\", \"options\", \"connect\", \"trace\"]\n    requests = json.loads(request.body)\n\n    if type(requests) not in (list, tuple):\n        raise BadBatchRequest(\"The body of batch request should always be list!\")\n\n    # Max limit check.\n    no_requests = len(requests)\n\n    if no_requests > _settings.MAX_LIMIT:\n        raise BadBatchRequest(\"You can batch maximum of %d requests.\" % (_settings.MAX_LIMIT))\n\n    # We could mutate the current request with the respective parameters, but mutation is ghost in the dark,\n    # so lets avoid. Construct the new WSGI request object for each request.\n\n    def construct_wsgi_from_data(data):\n        '''\n            Given the data in the format of url, method, body and headers, construct a new\n            WSGIRequest object.\n        '''\n        url = data.get(\"url\", None)\n        method = data.get(\"method\", None)\n\n        if url is None or method is None:\n            raise BadBatchRequest(\"Request definition should have url, method defined.\")\n\n        if method.lower() not in valid_http_methods:\n            raise BadBatchRequest(\"Invalid request method.\")\n\n        body = data.get(\"body\", \"\")\n        headers = data.get(\"headers\", {})\n        return get_wsgi_request_object(request, method, url, headers, body)\n\n    return [construct_wsgi_from_data(data) for data in requests]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_batch_requests(request, *args, **kwargs):\n    '''\n        A view function to handle the overall processing of batch requests.\n    '''\n    batch_start_time = datetime.now()\n    try:\n        # Get the Individual WSGI requests.\n        wsgi_requests = get_wsgi_requests(request)\n    except BadBatchRequest as brx:\n        return HttpResponseBadRequest(content=brx.message)\n\n    # Fire these WSGI requests, and collect the response for the same.\n    response = execute_requests(wsgi_requests)\n\n    # Evrything's done, return the response.\n    resp = HttpResponse(\n        content=json.dumps(response), content_type=\"application/json\")\n\n    if _settings.ADD_DURATION_HEADER:\n        resp.__setitem__(_settings.DURATION_HEADER_NAME, str((datetime.now() - batch_start_time).seconds))\n    return resp", "response": "A view function to handle the overall processing of batch requests."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, keyword, types=[], terr=KKBOXTerritory.TAIWAN):\n        '''\n        Searches within KKBOX's database.\n\n        :param keyword: the keyword.\n        :type keyword: str\n        :param types: the search types.\n        :return: list\n        :param terr: the current territory.\n        :return: API response.\n        :rtype: dict\n\n        See `https://docs-en.kkbox.codes/v1.1/reference#search_1`.\n        '''\n        url = 'https://api.kkbox.com/v1.1/search'\n        url += '?' + url_parse.urlencode({'q': keyword, 'territory': terr})\n        if len(types) > 0:\n            url += '&type=' + ','.join(types)\n        return self.http._post_data(url, None, self.http._headers_with_access_token())", "response": "Search within KKBOX s database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the current state of the current IP address.", "response": "def save(self):\n        \"\"\"\n        IPAddress can only change its PTR record. Saves the current state, PUT /ip_address/uuid.\n        \"\"\"\n        body = {'ip_address': {'ptr_record': self.ptr_record}}\n        data = self.cloud_manager.request('PUT', '/ip_address/' + self.address, body)\n        self._reset(**data['ip_address'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_ip_address_objs(ip_addresses, cloud_manager):\n        # ip-addresses might be provided as a flat array or as a following dict:\n        # {'ip_addresses': {'ip_address': [...]}} || {'ip_address': [...]}\n\n        if 'ip_addresses' in ip_addresses:\n            ip_addresses = ip_addresses['ip_addresses']\n\n        if 'ip_address' in ip_addresses:\n            ip_addresses = ip_addresses['ip_address']\n\n        return [\n            IPAddress(cloud_manager=cloud_manager, **ip_addr)\n            for ip_addr in ip_addresses\n        ]", "response": "Create IPAddress objects from API response data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the objects attributes.", "response": "def _reset(self, **kwargs):\n        \"\"\"\n        Reset the objects attributes.\n\n        Accepts servers as either unflattened or flattened UUID strings or Server objects.\n        \"\"\"\n        super(Tag, self)._reset(**kwargs)\n\n        # backup name for changing it (look: Tag.save)\n        self._api_name = self.name\n\n        # flatten { servers: { server: [] } }\n        if 'server' in self.servers:\n            self.servers = kwargs['servers']['server']\n\n        # convert UUIDs into server objects\n        if self.servers and isinstance(self.servers[0], six.string_types):\n            self.servers = [Server(uuid=server, populated=False) for server in self.servers]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_servers(self, populate=False, tags_has_one=None, tags_has_all=None):\n        if tags_has_all and tags_has_one:\n            raise Exception('only one of (tags_has_all, tags_has_one) is allowed.')\n\n        request = '/server'\n        if tags_has_all:\n            tags_has_all = [str(tag) for tag in tags_has_all]\n            taglist = ':'.join(tags_has_all)\n            request = '/server/tag/{0}'.format(taglist)\n\n        if tags_has_one:\n            tags_has_one = [str(tag) for tag in tags_has_one]\n            taglist = ','.join(tags_has_one)\n            request = '/server/tag/{0}'.format(taglist)\n\n        servers = self.get_request(request)['servers']['server']\n\n        server_list = list()\n        for server in servers:\n            server_list.append(Server(server, cloud_manager=self))\n\n        if populate:\n            for server_instance in server_list:\n                server_instance.populate()\n\n        return server_list", "response": "Returns a list of Server instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_server(self, UUID):\n        server, IPAddresses, storages = self.get_server_data(UUID)\n\n        return Server(\n            server,\n            ip_addresses=IPAddresses,\n            storage_devices=storages,\n            populated=True,\n            cloud_manager=self\n        )", "response": "Returns a populated Server instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a Server instance by its IP.", "response": "def get_server_by_ip(self, ip_address):\n        \"\"\"\n        Return a (populated) Server instance by its IP.\n\n        Uses GET '/ip_address/x.x.x.x' to retrieve machine UUID using IP-address.\n        \"\"\"\n        data = self.get_request('/ip_address/{0}'.format(ip_address))\n        UUID = data['ip_address']['server']\n        return self.get_server(UUID)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_server(self, server):\n        if isinstance(server, Server):\n            body = server.prepare_post_body()\n        else:\n            server = Server._create_server_obj(server, cloud_manager=self)\n            body = server.prepare_post_body()\n\n        res = self.post_request('/server', body)\n\n        server_to_return = server\n        server_to_return._reset(\n            res['server'],\n            cloud_manager=self,\n            populated=True\n        )\n        return server_to_return", "response": "Create a server and its storages based on a given server object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying the server with the given UUID.", "response": "def modify_server(self, UUID, **kwargs):\n        \"\"\"\n        modify_server allows updating the server's updateable_fields.\n\n        Note: Server's IP-addresses and Storages are managed by their own add/remove methods.\n        \"\"\"\n        body = dict()\n        body['server'] = {}\n        for arg in kwargs:\n            if arg not in Server.updateable_fields:\n                Exception('{0} is not an updateable field'.format(arg))\n            body['server'][arg] = kwargs[arg]\n\n        res = self.request('PUT', '/server/{0}'.format(UUID), body)\n        server = res['server']\n\n        # Populate subobjects\n        IPAddresses = IPAddress._create_ip_address_objs(server.pop('ip_addresses'),\n                                                          cloud_manager=self)\n\n        storages = Storage._create_storage_objs(server.pop('storage_devices'),\n                                                cloud_manager=self)\n\n        return Server(\n            server,\n            ip_addresses=IPAddresses,\n            storage_devices=storages,\n            populated=True,\n            cloud_manager=self\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_server_data(self, UUID):\n        data = self.get_request('/server/{0}'.format(UUID))\n        server = data['server']\n\n        # Populate subobjects\n        IPAddresses = IPAddress._create_ip_address_objs(server.pop('ip_addresses'),\n                                                          cloud_manager=self)\n\n        storages = Storage._create_storage_objs(server.pop('storage_devices'),\n                                                cloud_manager=self)\n\n        return server, IPAddresses, storages", "response": "Returns the server data in Python dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef feed(f, limit=25):\n    if '/' not in f:\n        raise ValueError('feed name must be formatted like: '\n                         'csirtgadgets/scanners')\n\n    user, f = f.split('/')\n\n    return Feed().show(user, f, limit=limit)", "response": "Pull a feed by name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an indicator in a feed", "response": "def indicator_create(f, i):\n    \"\"\"\n    Create an indicator in a feed\n    :param f: feed name (eg: wes/test)\n    :param i: indicator dict (eg: {'indicator': 'example.com', 'tags': ['ssh'],\n    'description': 'this is a test'})\n    :return: dict of indicator\n    \"\"\"\n    if '/' not in f:\n        raise ValueError('feed name must be formatted like: '\n                         'csirtgadgets/scanners')\n\n    if not i:\n        raise ValueError('missing indicator dict')\n\n    u, f = f.split('/')\n\n    i['user'] = u\n    i['feed'] = f\n\n    ret = Indicator(i).submit()\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_from_file(file):\n    if isinstance(file, six_string_types):\n        with open(file, 'rb') as f:\n            return _internal_convert(f)\n    else:\n        return _internal_convert(file)", "response": "Reads the content of file in IDX format converts it into numpy. ndarray and\nBridge returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _internal_convert(inp):\n    '''\n    Converts file in IDX format provided by file-like input into numpy.ndarray\n    and returns it.\n    '''\n\n    # Read the \"magic number\" - 4 bytes.\n    try:\n        mn = struct.unpack('>BBBB', inp.read(4))\n    except struct.error:\n        raise FormatError(struct.error)\n\n    # First two bytes are always zero, check it.\n    if mn[0] != 0 or mn[1] != 0:\n        msg = (\"Incorrect first two bytes of the magic number: \" +\n               \"0x{0:02X} 0x{1:02X}\".format(mn[0], mn[1]))\n        raise FormatError(msg)\n\n    # 3rd byte is the data type code.\n    dtype_code = mn[2]\n    if dtype_code not in _DATA_TYPES_IDX:\n        msg = \"Incorrect data type code: 0x{0:02X}\".format(dtype_code)\n        raise FormatError(msg)\n\n    # 4th byte is the number of dimensions.\n    dims = int(mn[3])\n\n    # See possible data types description.\n    dtype, dtype_s, el_size = _DATA_TYPES_IDX[dtype_code]\n\n    # 4-byte integer for length of each dimension.\n    try:\n        dims_sizes = struct.unpack('>' + 'I' * dims, inp.read(4 * dims))\n    except struct.error as e:\n        raise FormatError('Dims sizes: {0}'.format(e))\n\n    # Full length of data.\n    full_length = reduce(operator.mul, dims_sizes, 1)\n\n    # Create a numpy array from the data\n    try:\n        result_array = numpy.frombuffer(\n            inp.read(full_length * el_size),\n            dtype=numpy.dtype(dtype)\n        ).reshape(dims_sizes)\n    except ValueError as e:\n        raise FormatError('Error creating numpy array: {0}'.format(e))\n\n    # Check for superfluous data.\n    if len(inp.read(1)) > 0:\n        raise FormatError('Superfluous data detected.')\n\n    return result_array", "response": "Convert an IDX file into a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the contents of the numpy. ndarray ndarr to file in IDX format.", "response": "def convert_to_file(file, ndarr):\n    \"\"\"\n    Writes the contents of the numpy.ndarray ndarr to file in IDX format.\n    file is a file-like object (with write() method) or a file name.\n    \"\"\"\n    if isinstance(file, six_string_types):\n        with open(file, 'wb') as fp:\n            _internal_write(fp, ndarr)\n    else:\n        _internal_write(file, ndarr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_to_string(ndarr):\n    with contextlib.closing(BytesIO()) as bytesio:\n        _internal_write(bytesio, ndarr)\n        return bytesio.getvalue()", "response": "Converts a numpy. ndarray ndarr to bytes in IDX format and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite numpy. ndarray arr to a file - like object in IDX format.", "response": "def _internal_write(out_stream, arr):\n    \"\"\"\n    Writes numpy.ndarray arr to a file-like object (with write() method) in\n    IDX format.\n    \"\"\"\n\n    if arr.size == 0:\n        raise FormatError('Cannot encode empty array.')\n\n    try:\n        type_byte, struct_lib_type = _DATA_TYPES_NUMPY[str(arr.dtype)]\n    except KeyError:\n        raise FormatError('numpy ndarray type not supported by IDX format.')\n\n    if arr.ndim > _MAX_IDX_DIMENSIONS:\n        raise FormatError(\n            'IDX format cannot encode array with dimensions > 255')\n\n    if max(arr.shape) > _MAX_AXIS_LENGTH:\n        raise FormatError('IDX format cannot encode array with more than ' +\n                          str(_MAX_AXIS_LENGTH) + ' elements along any axis')\n\n    # Write magic number\n    out_stream.write(struct.pack('BBBB', 0, 0, type_byte, arr.ndim))\n\n    # Write array dimensions\n    out_stream.write(struct.pack('>' + 'I' * arr.ndim, *arr.shape))\n\n    # Horrible hack to deal with horrible bug when using struct.pack to encode\n    # unsigned ints in 2.7 and lower, see http://bugs.python.org/issue2263\n    if sys.version_info < (2, 7) and str(arr.dtype) == 'uint8':\n        arr_as_list = [int(i) for i in arr.reshape(-1)]\n        out_stream.write(struct.pack('>' + struct_lib_type * arr.size,\n                                     *arr_as_list))\n    else:\n        # Write array contents - note that the limit to number of arguments\n        # doesn't apply to unrolled arguments\n        out_stream.write(struct.pack('>' + struct_lib_type * arr.size,\n                                     *arr.reshape(-1)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching an access token by client credentials.", "response": "def fetch_access_token_by_client_credentials(self):\n        '''\n        There are three ways to let you start using KKBOX's Open/Partner\n        API. The first way among them is to generate a client\n        credential to fetch an access token to let KKBOX identify\n        you. It allows you to access public data from KKBOX such as\n        public albums, playlists and so on.\n\n        However, you cannot use client credentials to access private\n        data of a user. You have to let users to log-in into KKBOX and\n        grant permissions for you to do so. You cannot use client\n        credentials to do media playback either, since it requires a\n        Premium Membership.\n\n        :return: an access token\n        :rtype: :class:`kkbox_sdk.KKBOXAccessToken`\n\n        See `https://docs-en.kkbox.codes/docs/appendix-a`.\n        '''\n        client_credential_base = '%s:%s' % (self.client_id, self.client_secret)\n        try:\n            client_credentials = base64.b64encode(\n                bytes(client_credential_base, 'utf-8'))\n        except:\n            client_credentials = base64.b64encode(client_credential_base)\n        client_credentials = client_credentials.decode('utf-8')\n        headers = {'Authorization': 'Basic ' + client_credentials,\n                   'Content-type': 'application/x-www-form-urlencoded'}\n        post_parameters = {'grant_type': 'client_credentials',\n                           'scope': 'user_profile user_territory'}\n        json_object = self.http._post_data(KKBOXOAuth.OAUTH_TOKEN_URL, post_parameters,\n                                      headers)\n        self.access_token = KKBOXAccessToken(**json_object)\n        return self.access_token"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_OS_UUID(cls, os):\n        if os in cls.templates:\n            return cls.templates[os]\n\n        uuid_regexp = '^[0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12}$'\n        if re.search(uuid_regexp, os):\n            return os\n\n        raise Exception((\n            \"Invalid OS -- valid options are: 'CentOS 6.5', 'CentOS 7.0', \"\n            \"'Debian 7.8', 'Debian 8.0' ,'Ubuntu 12.04', 'Ubuntu 14.04', 'Ubuntu 16.04', \"\n            \"'Windows 2008', 'Windows 2012'\"\n        ))", "response": "Validate Storage OS and its UUID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the requests in parallel in an asynchronous way.", "response": "def execute(self, requests, resp_generator, *args, **kwargs):\n        '''\n            Calls the resp_generator for all the requests in parallel in an asynchronous way.\n        '''\n        result_futures = [self.executor_pool.submit(resp_generator, req, *args, **kwargs) for req in requests]\n        resp = [res_future.result() for res_future in result_futures]\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self, requests, resp_generator, *args, **kwargs):\n        '''\n            Calls the resp_generator for all the requests in sequential order.\n        '''\n        return [resp_generator(request) for request in requests]", "response": "Calls the resp_generator for all the requests in sequential order."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up basic logging for the current user.", "response": "def setup_logging(args):\n    \"\"\"\n    Sets up basic logging\n\n    :param args: ArgParse arguments\n    :return: nothing. sets logger up globally\n    \"\"\"\n    loglevel = logging.WARNING\n    if args.verbose:\n        loglevel = logging.INFO\n    if args.debug:\n        loglevel = logging.DEBUG\n\n    console = logging.StreamHandler()\n    logging.getLogger('').setLevel(loglevel)\n    console.setFormatter(logging.Formatter(LOG_FORMAT))\n    logging.getLogger('').addHandler(console)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ip(self, address):\n        res = self.get_request('/ip_address/' + address)\n        return IPAddress(cloud_manager=self, **res['ip_address'])", "response": "Get an IPAddress object with the IP address string from the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all IP objects from the API.", "response": "def get_ips(self):\n        \"\"\"\n        Get all IPAddress objects from the API.\n        \"\"\"\n        res = self.get_request('/ip_address')\n        IPs = IPAddress._create_ip_address_objs(res['ip_addresses'], cloud_manager=self)\n        return IPs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattach a new ( random ) IPAddress to the given server.", "response": "def attach_ip(self, server, family='IPv4'):\n        \"\"\"\n        Attach a new (random) IPAddress to the given server (object or UUID).\n        \"\"\"\n        body = {\n            'ip_address': {\n                'server': str(server),\n                'family': family\n            }\n        }\n\n        res = self.request('POST', '/ip_address', body)\n        return IPAddress(cloud_manager=self, **res['ip_address'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef modify_ip(self, ip_addr, ptr_record):\n        body = {\n            'ip_address': {\n                'ptr_record': ptr_record\n            }\n        }\n\n        res = self.request('PUT', '/ip_address/' + str(ip_addr), body)\n        return IPAddress(cloud_manager=self, **res['ip_address'])", "response": "Modify an IP address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self, user, name, description=None):\n        uri = self.client.remote + '/users/{0}/feeds'.format(user)\n\n        data = {\n            'feed': {\n                'name': name,\n                'description': description\n            }\n        }\n\n        resp = self.client.post(uri, data)\n        return resp", "response": "Creates a new Feed object with the given user and name and description."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a feed from the cache.", "response": "def delete(self, user, name):\n        \"\"\"\n        Removes a feed\n\n        :param user: feed username\n        :param name: feed name\n        :return: true/false\n        \"\"\"\n\n        uri = self.client.remote + '/users/{}/feeds/{}'.format(user, name)\n\n        resp = self.client.session.delete(uri)\n        return resp.status_code"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of Feeds from the API", "response": "def index(self, user):\n        \"\"\"\n        Returns a list of Feeds from the API\n\n        :param user: feed username\n        :return: list\n\n        Example:\n            ret = feed.index('csirtgadgets')\n        \"\"\"\n        uri = self.client.remote + '/users/{0}/feeds'.format(user)\n        return self.client.get(uri)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show(self, user, name, limit=None, lasttime=None):\n        uri = self.client.remote + '/users/{0}/feeds/{1}'.format(user, name)\n        return self.client.get(uri, params={'limit': limit, 'lasttime': lasttime})", "response": "Returns a specific Feed from the API\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Codenerix context", "response": "def codenerix(request):\n    '''\n    Codenerix CONTEXT\n    '''\n    # Get values\n    DEBUG = getattr(settings, 'DEBUG', False)\n    VERSION = getattr(settings, 'VERSION', _('WARNING: No version set to this code, add VERSION contant to your configuration'))\n    \n    # Set environment\n    return {\n        'DEBUG': DEBUG,\n        'VERSION': VERSION,\n        'CODENERIX_VERSION': __version__,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride django - bakery to skip profiles that raise 404", "response": "def build_object(self, obj):\n        \"\"\"Override django-bakery to skip profiles that raise 404\"\"\"\n        try:\n            build_path = self.get_build_path(obj)\n            self.request = self.create_request(build_path)\n            self.request.user = AnonymousUser()\n            self.set_kwargs(obj)\n            self.build_file(build_path, self.get_content())\n        except Http404:\n            # cleanup directory\n            self.unbuild_object(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_schedule_row(schedule_day, slot, seen_items):\n    row = ScheduleRow(schedule_day, slot)\n    skip = {}\n    expanding = {}\n    all_items = list(slot.scheduleitem_set\n                     .select_related('talk', 'page', 'venue')\n                     .all())\n\n    for item in all_items:\n        if item in seen_items:\n            # Inc rowspan\n            seen_items[item]['rowspan'] += 1\n            # Note that we need to skip this during colspan checks\n            skip[item.venue] = seen_items[item]\n            continue\n        scheditem = {'item': item, 'rowspan': 1, 'colspan': 1}\n        row.items[item.venue] = scheditem\n        seen_items[item] = scheditem\n        if item.expand:\n            expanding[item.venue] = []\n\n    empty = []\n    expanding_right = None\n    skipping = 0\n    skip_item = None\n    for venue in schedule_day.venues:\n        if venue in skip:\n            # We need to skip all the venues this item spans over\n            skipping = 1\n            skip_item = skip[venue]\n            continue\n        if venue in expanding:\n            item = row.items[venue]\n            for empty_venue in empty:\n                row.items.pop(empty_venue)\n                item['colspan'] += 1\n            empty = []\n            expanding_right = item\n        elif venue in row.items:\n            empty = []\n            expanding_right = None\n        elif expanding_right:\n            expanding_right['colspan'] += 1\n        elif skipping > 0 and skipping < skip_item['colspan']:\n            skipping += 1\n        else:\n            skipping = 0\n            empty.append(venue)\n            row.items[venue] = {'item': None, 'rowspan': 1, 'colspan': 1}\n\n    return row", "response": "Create a row for the schedule table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nallowing adding a render_description parameter", "response": "def get_context_data(self, **kwargs):\n        \"\"\"Allow adding a 'render_description' parameter\"\"\"\n        context = super(ScheduleXmlView, self).get_context_data(**kwargs)\n        if self.request.GET.get('render_description', None) == '1':\n            context['render_description'] = True\n        else:\n            context['render_description'] = False\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a iCal file from the schedule", "response": "def get(self, request):\n        \"\"\"Create a iCal file from the schedule\"\"\"\n        # Heavily inspired by https://djangosnippets.org/snippets/2223/ and\n        # the icalendar documentation\n        calendar = Calendar()\n        site = get_current_site(request)\n        calendar.add('prodid', '-//%s Schedule//%s//' % (site.name, site.domain))\n        calendar.add('version', '2.0')\n\n        # Since we don't need to format anything here, we can just use a list\n        # of schedule items\n        for item in ScheduleItem.objects.all():\n            sched_event = Event()\n            sched_event.add('dtstamp', item.last_updated)\n            sched_event.add('summary', item.get_title())\n            sched_event.add('location', item.venue.name)\n            sched_event.add('dtstart', item.get_start_datetime())\n            sched_event.add('duration', datetime.timedelta(minutes=item.get_duration_minutes()))\n            sched_event.add('class', 'PUBLIC')\n            sched_event.add('uid', '%s@%s' % (item.pk, site.domain))\n            calendar.add_component(sched_event)\n        response = HttpResponse(calendar.to_ical(), content_type=\"text/calendar\")\n        response['Content-Disposition'] = 'attachment; filename=schedule.ics'\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlook up a page by url which is a tree of slugs", "response": "def slug(request, url):\n    \"\"\"Look up a page by url (which is a tree of slugs)\"\"\"\n    page = None\n\n    if url:\n        for slug in url.split('/'):\n            if not slug:\n                continue\n            try:\n                page = Page.objects.get(slug=slug, parent=page)\n            except Page.DoesNotExist:\n                raise Http404\n    else:\n        try:\n            page = Page.objects.get(slug='index', parent=None)\n        except Page.DoesNotExist:\n            return TemplateView.as_view(\n                template_name='wafer/index.html')(request)\n\n    if 'edit' in request.GET:\n        if not request.user.has_perm('pages.change_page'):\n            raise PermissionDenied\n        return EditPage.as_view()(request, pk=page.id)\n\n    if 'compare' in request.GET:\n        if not request.user.has_perm('pages.change_page'):\n            raise PermissionDenied\n        return ComparePage.as_view()(request, pk=page.id)\n\n    return ShowPage.as_view()(request, pk=page.id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_object(self, obj):\n        if not obj.exclude_from_static:\n            super(ShowPage, self).build_object(obj)", "response": "Override django - bakery to skip pages marked exclude_from_static"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_object(self, obj):\n        try:\n            super(TalkView, self).build_object(obj)\n        except PermissionDenied:\n            # We cleanup the directory created\n            self.unbuild_object(obj)", "response": "Override django - bakery to skip talks that raise 403"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_object(self, *args, **kwargs):\n        '''Only talk owners can see talks, unless they've been accepted'''\n        object_ = super(TalkView, self).get_object(*args, **kwargs)\n        if not object_.can_view(self.request.user):\n            raise PermissionDenied\n        return object_", "response": "Only talk owners can see talks unless they've been accepted"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, request, *args, **kwargs):\n        talk = self.get_object()\n        talk.status = WITHDRAWN\n        talk.save()\n        revisions.set_user(self.request.user)\n        revisions.set_comment(\"Talk Withdrawn\")\n        return HttpResponseRedirect(self.success_url)", "response": "Override delete to only withdraw"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_queryset(self):\n        paths = [(os.path.join(self.build_prefix, 'index.html'), {})]\n        self.request = None\n        queryset = self.get_queryset()\n        paginator = self.get_paginator(queryset, self.get_paginate_by(queryset))\n        for page in paginator.page_range:\n            paths.append(\n                (os.path.join(self.build_prefix, 'page',\n                              '%d' % page, 'index.html'), {'page': page}))\n        for build_path, kwargs in paths:\n            self.request = self.create_request(build_path)\n            # Add a user with no permissions\n            self.request.user = AnonymousUser()\n            # Fake context so views work as expected\n            self.kwargs = kwargs\n            self.prep_directory(build_path)\n            target_path = os.path.join(settings.BUILD_DIR, build_path)\n            self.build_file(target_path, self.get_content())", "response": "Override django - bakery s build logic to fake pagination."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of groups that are available in the current language.", "response": "def get_groups(self, gs=None, processed=[], initial=True):\n        '''\n        <--------------------------------------- 12 columns ------------------------------------>\n                    <--- 6 columns --->                           <--- 6 columns --->\n         ------------------------------------------   ------------------------------------------\n        | Info                                     | | Personal                                 |\n        |==========================================| |==========================================|\n        |  -----------------   ------------------  | |                                          |\n        | | Passport        | | Name             | | | Phone                          Zipcode   |\n        | |=================| | [.....]  [.....] | | | [...........................]  [.......] |\n        | | CID     Country | | <- 6 ->  <- 6 -> | | |       <--- 8 columns --->      <-4 col-> |\n        | | [.....] [.....] | |                  | | |                                          |\n        | | <- 6 -> <- 6 -> |  -----------------   | | Address                                  |\n        |  -----------------                       | | [.....................................]  |\n         ------------------------------------------  |           <--- 12 columns --->           |\n                                                     | [..] number                              |\n                                                     |           <--- 12 columns --->           |\n                                                     |                                          |\n                                                      ------------------------------------------\n        group = [\n                (_('Info'),(6,'#8a6d3b','#fcf8e3','center'),\n                    (_('Identification'),6,\n                        [\"cid\",6],\n                        [\"country\",6],\n                    ),\n                    (None,6,\n                        [\"name\",None,6],\n                        [\"surname\",None,6,False],\n                    ),\n                ),\n                (_('Personal'),6,\n                    [\"phone\",None,8],\n                    [\"zipcode\",None,4],\n                    [\"address\",None,12],\n                    [\"number\",None,12, True],\n                ),\n            ]\n\n        Group: it is defined as tuple with 3 or more elements:\n            Grammar: (<Name>, <Attributes>, <Element1>, <Element2>, ..., <ElementN>)\n            If <Name> is None: no name will be given to the group and no panel decoration will be shown\n            If <Size in columns> is None: default of 6 will be used\n\n            <Attributes>:\n                it can be an integer that represent the size in columns\n                it can be a tuple with several attributes where each element represents:\n                    (<Size in columns>,'#<Font color>','#<Background color>','<Alignment>')\n\n            <Element>:\n                it can be a Group\n                it can be a Field\n\n            Examples:\n            ('Info', 6, [\"name\",6], [\"surname\",6]) -> Info panel using 6 columns with 2 boxes 6 columns for each with name and surname inputs\n            ('Info', (6,None,'#fcf8e3','center'), [\"name\",6], [\"surname\",6]) -> Info panel using 6 columns with a yellow brackground in centered title, 2 boxes, 6 columns for each with name and surname inputs\n            ('Info', 12, ('Name', 6, [\"name\",12]), ('Surname',6, [\"surname\",12])) -> Info panel using 12 columns with 2 panels inside\n              of 6 columns each named \"Name\" and \"Surname\" and inside each of them an input \"name\" and \"surname\" where it belongs.\n\n        Field: must be a list with at least 1 element in it:\n            Grammar: [<Name of field>, <Size in columns>, <Label>]\n\n            <Name of field>:\n                This must be filled always\n                It is the input's name inside the form\n                Must exists as a form element or as a grouped form element\n\n            <Size in columns>:\n                Size of the input in columns\n                If it is not defined or if it is defined as None: default of 6 will be used\n\n            <Label>:\n                It it is defined as False: the label for this field will not be shown\n                If it is not defined or if it is defined as None: default of True will be used (default input's label will be shown)\n                If it is a string: this string will be shown as a label\n\n            Examples:\n            ['age']                             Input 'age' will be shown with 6 columns and its default label\n            ['age',8]                           Input 'age' will be shown with 8 columns and its default label\n            ['age', None, False]                Input 'age' will be shown with 6 columns and NO LABEL\n            ['age',8,False]                     Input 'age' will be shown with 8 columns and NO LABEL\n            ['age',8,_(\"Age in days\")]          Input 'age' will be shown with 8 columns and translated label text \"Age in days\" to user's language\n            ['age',8,_(\"Age in days\"), True]    Input 'age' will be shown with 8 columns and translated label text \"Age in days\" to user's language, and input inline with label\n            ['age',6, None, None, None, None, None, [\"ng-click=functionjs('param1')\", \"ng-change=functionjs2()\"]]    Input 'age' with extras functions\n            ['age',None,None,None,None, 'filter']    Input 'age' with extras filter ONLY DETAILS\n            ['age',6, {'color': 'red'}          Input 'age' will be shown with red title\n        '''\n\n        # Check if language is set\n        if not self.__language:\n            raise IOError(\"ERROR: No language suplied!\")\n\n        # Initialize the list\n        if initial:\n            processed = []\n        # Where to look for fields\n        if 'list_fields' in dir(self):\n            list_fields = self.list_fields\n            check_system = \"html_name\"\n        else:\n            list_fields = self\n            check_system = \"name\"\n\n        # Default attributes for fields\n        attributes = [\n            ('columns', 6),\n            ('color', None),\n            ('bgcolor', None),\n            ('textalign', None),\n            ('inline', False),  # input in line with label\n            ('label', True),\n            ('extra', None),\n            ('extra_div', None),\n            ('foreign_info', {}),\n        ]\n        labels = [x[0] for x in attributes]\n\n        # Get groups if none was given\n        if gs is None:\n            gs = self.__groups__()\n\n        # Prepare the answer\n        groups = []\n\n        # Prepare focus control\n        focus_first = None\n        focus_must = None\n\n        # html helper for groups and fields\n        html_helper = self.html_helper()\n\n        # Start processing\n        for g in gs:\n            token = {}\n            token['name'] = g[0]\n\n            if token['name'] in html_helper:\n                if 'pre' in html_helper[token['name']]:\n                    token[\"html_helper_pre\"] = html_helper[token['name']]['pre']\n                if 'post' in html_helper[token['name']]:\n                    token[\"html_helper_post\"] = html_helper[token['name']]['post']\n\n            styles = g[1]\n            if type(styles) is tuple:\n                if len(styles) >= 1:\n                    token['columns'] = g[1][0]\n                if len(styles) >= 2:\n                    token['color'] = g[1][1]\n                if len(styles) >= 3:\n                    token['bgcolor'] = g[1][2]\n                if len(styles) >= 4:\n                    token['textalign'] = g[1][3]\n                if len(styles) >= 5:\n                    token['inline'] = g[1][4]\n                if len(styles) >= 7:\n                    token['extra'] = g[1][5]\n                if len(styles) >= 8:\n                    token['extra_div'] = g[1][6]\n            else:\n                token['columns'] = g[1]\n            fs = g[2:]\n            fields = []\n            for f in fs:\n                # Field\n                atr = {}\n                # Decide weather this is a Group or not\n                if type(f) == tuple:\n                    # Recursive\n                    fields += self.get_groups([list(f)], processed, False)\n                else:\n                    try:\n                        list_type = [str, unicode, ]\n                    except NameError:\n                        list_type = [str, ]\n                    # Check if it is a list\n                    if type(f) == list:\n                        # This is a field with attributes, get the name\n                        field = f[0]\n\n                        if html_helper and token['name'] in html_helper and 'items' in html_helper[token['name']] and field in html_helper[token['name']]['items']:\n                            if 'pre' in html_helper[token['name']]['items'][field]:\n                                atr[\"html_helper_pre\"] = html_helper[token['name']]['items'][field]['pre']\n                            if 'post' in html_helper[token['name']]['items'][field]:\n                                atr[\"html_helper_post\"] = html_helper[token['name']]['items'][field]['post']\n\n                        # Process each attribute (if any)\n                        dictionary = False\n                        for idx, element in enumerate(f[1:]):\n                            if type(element) == dict:\n                                dictionary = True\n                                for key in element.keys():\n                                    if key in labels:\n                                        atr[key] = element[key]\n                                    else:\n                                        raise IOError(\"Unknown attribute '{0}' as field '{1}' in list of fields\".format(key, field))\n                            else:\n                                if not dictionary:\n                                    if element is not None:\n                                        atr[attributes[idx][0]] = element\n                                else:\n                                    raise IOError(\"We already processed a dicionary element in this list of fields, you can not add anoother type of elements to it, you must keep going with dictionaries\")\n                    elif type(f) in list_type:\n                        field = f\n                    else:\n                        raise IOError(\"Uknown element type '{0}' inside group '{1}'\".format(type(f), token['name']))\n\n                    # Get the Django Field object\n                    found = None\n                    for infield in list_fields:\n                        if infield.__dict__[check_system] == field:\n                            found = infield\n                            break\n\n                    if found:\n                        # Get attributes (required and original attributes)\n                        wrequired = found.field.widget.is_required\n                        wattrs = found.field.widget.attrs\n                        # Fill base attributes\n                        atr['name'] = found.html_name\n                        atr['input'] = found\n                        atr['focus'] = False\n                        # Set focus\n                        if focus_must is None:\n                            if focus_first is None:\n                                focus_first = atr\n                            if wrequired:\n                                focus_must = atr\n                        # Autocomplete\n                        if 'autofill' in dir(self.Meta):\n                            autofill = self.Meta.autofill.get(found.html_name, None)\n                            atr['autofill'] = autofill\n                            if autofill:\n                                # Check format of the request\n                                autokind = autofill[0]\n                                if type(autokind) == str:\n                                    # Using new format\n                                    if autokind == 'select':\n                                        # If autofill is True for this field set the DynamicSelect widget\n                                        found.field.widget = DynamicSelect(wattrs)\n                                    elif autokind == 'multiselect':\n                                        # If autofill is True for this field set the DynamicSelect widget\n                                        found.field.widget = MultiDynamicSelect(wattrs)\n                                    elif autokind == 'input':\n                                        # If autofill is True for this field set the DynamicSelect widget\n                                        found.field.widget = DynamicInput(wattrs)\n                                    else:\n                                        raise IOError(\"Autofill filled using new format but autokind is '{}' and I only know 'input' or 'select'\".format(autokind))\n\n                                    # Configure the field\n                                    found.field.widget.is_required = wrequired\n                                    found.field.widget.form_name = self.form_name\n                                    found.field.widget.field_name = infield.html_name\n                                    found.field.widget.autofill_deepness = autofill[1]\n                                    found.field.widget.autofill_url = autofill[2]\n                                    found.field.widget.autofill = autofill[3:]\n                                else:\n                                    # Get old information [COMPATIBILITY WITH OLD VERSION]\n                                    # If autofill is True for this field set the DynamicSelect widget\n                                    found.field.widget = DynamicSelect(wattrs)\n                                    found.field.widget.is_required = wrequired\n                                    found.field.widget.form_name = self.form_name\n                                    found.field.widget.field_name = infield.html_name\n                                    found.field.widget.autofill_deepness = autofill[0]\n                                    found.field.widget.autofill_url = autofill[1]\n                                    found.field.widget.autofill = autofill[2:]\n                        else:\n\n                            # Set we don't have autofill for this field\n                            atr['autofill'] = None\n\n                        # Check if we have to replace the widget with a newer one\n                        if isinstance(found.field.widget, Select) and not isinstance(found.field.widget, DynamicSelect):\n                            if not isinstance(found.field.widget, MultiStaticSelect):\n                                found.field.widget = StaticSelect(wattrs)\n                            found.field.widget.choices = found.field.choices\n                            found.field.widget.is_required = wrequired\n                            found.field.widget.form_name = self.form_name\n                            found.field.widget.field_name = infield.html_name\n\n                        # Fill all attributes\n                        for (attribute, default) in attributes:\n                            if attribute not in atr.keys():\n                                atr[attribute] = default\n                        # Fill label\n                        if atr['label'] is True:\n                            atr['label'] = found.label\n                        # Set language\n                        flang = getattr(found.field, \"set_language\", None)\n                        if flang:\n                            flang(self.__language)\n                        flang = getattr(found.field.widget, \"set_language\", None)\n                        if flang:\n                            flang(self.__language)\n                        # Attach the element\n                        fields.append(atr)\n                        # Remember we have processed it\n                        processed.append(found.__dict__[check_system])\n                    else:\n                        raise IOError(\"Unknown field '{0}' specified in group '{1}'\".format(f, token['name']))\n\n            token['fields'] = fields\n            groups.append(token)\n\n        # Add the rest of attributes we didn't use yet\n        if initial:\n            fields = []\n            for infield in list_fields:\n                if infield.__dict__[check_system] not in processed:\n                    # Get attributes (required and original attributes)\n                    wattrs = infield.field.widget.attrs\n                    wrequired = infield.field.widget.is_required\n                    # Prepare attr\n                    atr = {}\n                    # Fill base attributes\n                    atr['name'] = infield.html_name\n                    atr['input'] = infield\n                    atr['focus'] = False\n                    # Set focus\n                    if focus_must is None:\n                        if focus_first is None:\n                            focus_first = atr\n                        if wrequired:\n                            focus_must = atr\n                    # Autocomplete\n                    if 'autofill' in dir(self.Meta):\n                        autofill = self.Meta.autofill.get(infield.html_name, None)\n                        atr['autofill'] = autofill\n                        if autofill:\n                            # Check format of the request\n                            autokind = autofill[0]\n                            if type(autokind) == str:\n                                # Get old information\n\n                                # Using new format\n                                if autokind == 'select':\n                                    # If autofill is True for this field set the DynamicSelect widget\n                                    infield.field.widget = DynamicSelect(wattrs)\n                                elif autokind == 'multiselect':\n                                    # If autofill is True for this field set the DynamicSelect widget\n                                    infield.field.widget = MultiDynamicSelect(wattrs)\n                                elif autokind == 'input':\n                                    # If autofill is True for this field set the DynamicSelect widget\n                                    infield.field.widget = DynamicInput(wattrs)\n                                else:\n                                    raise IOError(\"Autofill filled using new format but autokind is '{}' and I only know 'input' or 'select'\".format(autokind))\n\n                                # Configure the field\n                                infield.field.widget.is_required = wrequired\n                                infield.field.widget.form_name = self.form_name\n                                infield.field.widget.field_name = infield.html_name\n                                infield.field.widget.autofill_deepness = autofill[1]\n                                infield.field.widget.autofill_url = autofill[2]\n                                infield.field.widget.autofill = autofill[3:]\n                            else:\n                                # Get old information [COMPATIBILITY WITH OLD VERSION]\n                                # If autofill is True for this field set the DynamicSelect widget\n                                infield.field.widget = DynamicSelect(wattrs)\n                                infield.field.widget.is_required = wrequired\n                                infield.field.widget.form_name = self.form_name\n                                infield.field.widget.field_name = infield.html_name\n                                infield.field.widget.autofill_deepness = autofill[0]\n                                infield.field.widget.autofill_url = autofill[1]\n                                infield.field.widget.autofill = autofill[2:]\n                    else:\n\n                        # Set we don't have autofill for this field\n                        atr['autofill'] = None\n\n                    # Check if we have to replace the widget with a newer one\n                    if isinstance(infield.field.widget, Select) and not isinstance(infield.field.widget, DynamicSelect):\n                        if isinstance(infield.field, NullBooleanField):\n                            infield.field.widget = CheckboxInput(wattrs)\n                        elif not isinstance(infield.field.widget, MultiStaticSelect):\n                            infield.field.widget = StaticSelect(wattrs)\n                        if hasattr(infield.field.widget, 'choices') and hasattr(infield.field, 'choices'):\n                            infield.field.widget.choices = infield.field.choices\n                        infield.field.widget.is_required = wrequired\n                        infield.field.widget.form_name = self.form_name\n                        infield.field.widget.field_name = infield.html_name\n\n                    # Fill all attributes\n                    for (attribute, default) in attributes:\n                        if attribute not in atr.keys():\n                            atr[attribute] = default\n                    # Fill label\n                    if atr['label'] is True:\n                        atr['label'] = infield.label\n                    # Set language\n                    flang = getattr(infield.field, \"set_language\", None)\n                    if flang:\n                        flang(self.__language)\n                    flang = getattr(infield.field.widget, \"set_language\", None)\n                    if flang:\n                        flang(self.__language)\n                    # Attach the attribute\n                    fields.append(atr)\n\n            # Save the new elements\n            if fields:\n                groups.append({'name': None, 'columns': 12, 'fields': fields})\n\n        # Set focus\n        if focus_must:\n            focus_must['focus'] = True\n        elif focus_first is not None:\n            focus_first['focus'] = True\n\n        # Return the resulting groups\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef site_info(request):\n    '''Expose the site's info to templates'''\n    site = get_current_site(request)\n    context = {\n        'WAFER_CONFERENCE_NAME': site.name,\n        'WAFER_CONFERENCE_DOMAIN': site.domain,\n    }\n    return context", "response": "Expose the site s info to templates"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpose whether to display the navigation header and footer", "response": "def navigation_info(request):\n    '''Expose whether to display the navigation header and footer'''\n    if request.GET.get('wafer_hide_navigation') == \"1\":\n        nav_class = \"wafer-invisible\"\n    else:\n        nav_class = \"wafer-visible\"\n    context = {\n        'WAFER_NAVIGATION_VISIBILITY': nav_class,\n    }\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexposes selected settings to templates", "response": "def registration_settings(request):\n    '''Expose selected settings to templates'''\n    context = {}\n    for setting in (\n            'WAFER_SSO',\n            'WAFER_HIDE_LOGIN',\n            'WAFER_REGISTRATION_OPEN',\n            'WAFER_REGISTRATION_MODE',\n            'WAFER_TALKS_OPEN',\n            'WAFER_VIDEO_LICENSE',\n    ):\n        context[setting] = getattr(settings, setting, None)\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the rolls this people is related with", "response": "def profiles(self):\n        '''\n        return the rolls this people is related with\n        '''\n        limit = []\n\n        if self.is_admin():\n            limit.append(_(\"Administrator\"))\n        limit.sort()\n\n        return limit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matern_function(Xi, Xj, *args):\n    num_dim = len(args) - 2\n    nu = args[1]\n    \n    if isinstance(Xi, scipy.ndarray):\n        if isinstance(Xi, scipy.matrix):\n            Xi = scipy.asarray(Xi, dtype=float)\n            Xj = scipy.asarray(Xj, dtype=float)\n        \n        tau = scipy.asarray(Xi - Xj, dtype=float)\n        l_mat = scipy.tile(args[-num_dim:], (tau.shape[0], 1))\n        r2l2 = scipy.sum((tau / l_mat)**2, axis=1)\n        y = scipy.sqrt(2.0 * nu * r2l2)\n        k = 2.0**(1 - nu) / scipy.special.gamma(nu) * y**nu * scipy.special.kv(nu, y)\n        k[r2l2 == 0] = 1\n    else:\n        try:\n            tau = [xi - xj for xi, xj in zip(Xi, Xj)]\n        except TypeError:\n            tau = Xi - Xj\n        try:\n            r2l2 = sum([(t / l)**2 for t, l in zip(tau, args[2:])])\n        except TypeError:\n            r2l2 = (tau / args[2])**2\n        y = mpmath.sqrt(2.0 * nu * r2l2)\n        k = 2.0**(1 - nu) / mpmath.gamma(nu) * y**nu * mpmath.besselk(nu, y)\n    k *= args[0]**2.0\n    return k", "response": "r Matern covariance function of arbitrary dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the kernel directly at the given values of tau.", "response": "def _compute_k(self, tau):\n        r\"\"\"Evaluate the kernel directly at the given values of `tau`.\n        \n        Parameters\n        ----------\n        tau : :py:class:`Matrix`, (`M`, `D`)\n            `M` inputs with dimension `D`.\n        \n        Returns\n        -------\n        k : :py:class:`Array`, (`M`,)\n            :math:`k(\\tau)` (less the :math:`\\sigma^2` prefactor).\n        \"\"\"\n        y, r2l2 = self._compute_y(tau, return_r2l2=True)\n        k = 2.0**(1.0 - self.nu) / scipy.special.gamma(self.nu) * y**(self.nu / 2.0) * scipy.special.kv(self.nu, scipy.sqrt(y))\n        k[r2l2 == 0] = 1.0\n        return k"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_y(self, tau, return_r2l2=False):\n        r2l2 = self._compute_r2l2(tau)\n        y = 2.0 * self.nu * r2l2\n        if return_r2l2:\n            return (y, r2l2)\n        else:\n            return y", "response": "Covert tau to y = 2 \\ nu \\ sum_i(\\ tau_i^2 / l_i^2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for _compute_y that handles the conversion of tau to y", "response": "def _compute_y_wrapper(self, *args):\n        r\"\"\"Convert tau to :math:`y=\\sqrt{2\\nu\\sum_i(\\tau_i^2/l_i^2)}`.\n        \n        Takes `tau` as an argument list for compatibility with :py:func:`mpmath.diff`.\n        \n        Parameters\n        ----------\n        tau[0] : scalar float\n            First element of `tau`.\n        tau[1] : And so on...\n        \n        Returns\n        -------\n        y : scalar float\n            Inner part of Matern kernel at the given `tau`.\n        \"\"\"\n        return self._compute_y(scipy.atleast_2d(scipy.asarray(args, dtype=float)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the derivative of the outer form of the Matern kernel.", "response": "def _compute_dk_dy(self, y, n):\n        r\"\"\"Evaluate the derivative of the outer form of the Matern kernel.\n        \n        Uses the general Leibniz rule to compute the n-th derivative of:\n        \n        .. math::\n        \n            f(y) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} y^{\\nu/2} K_\\nu(y^{1/2})\n        \n        Parameters\n        ----------\n        y : :py:class:`Array`, (`M`,)\n            `M` inputs to evaluate at.\n        n : non-negative scalar int.\n            Order of derivative to compute.\n        \n        Returns\n        -------\n        dk_dy : :py:class:`Array`, (`M`,)\n            Specified derivative at specified locations.\n        \"\"\"\n        return 2.0**(1 - self.nu) / (scipy.special.gamma(self.nu)) * yn2Kn2Der(self.nu, y, n=n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_dy_dtau(self, tau, b, r2l2):\n        if len(b) == 0:\n            return self._compute_y(tau)\n        elif len(b) == 1:\n            return 4.0 * self.nu * tau[:, b[0]] / (self.params[2 + b[0]])**2.0\n        elif (len(b) == 2) and (b[0] == b[1]):\n            return 4.0 * self.nu / (self.params[2 + b[0]])**2.0\n        else:\n            return scipy.zeros_like(r2l2)", "response": "Compute the derivative of the inner argument of the Matern kernel."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the term inside the sum of Faa di Bruno s formula for the given partition.", "response": "def _compute_dk_dtau_on_partition(self, tau, p):\n        \"\"\"Evaluate the term inside the sum of Faa di Bruno's formula for the given partition.\n        \n        Overrides the version from :py:class:`gptools.kernel.core.ChainRuleKernel`\n        in order to get the correct behavior at the origin.\n        \n        Parameters\n        ----------\n        tau : :py:class:`Matrix`, (`M`, `D`)\n            `M` inputs with dimension `D`.\n        p : list of :py:class:`Array`\n            Each element is a block of the partition representing the\n            derivative orders to use.\n        \n        Returns\n        -------\n        dk_dtau : :py:class:`Array`, (`M`,)\n            The specified derivatives over the given partition at the specified\n            locations.\n        \"\"\"\n        # Find the derivative order:\n        n = len(p)\n        y, r2l2 = self._compute_y(tau, return_r2l2=True)\n        # Keep track of how many times a given variable has a block of length 1:\n        n1 = 0\n        # Build the dy/dtau factor up iteratively:\n        dy_dtau_factor = scipy.ones_like(y)\n        for b in p:\n            # If the partial derivative is exactly zero there is no sense in\n            # continuing the computation:\n            if (len(b) > 2) or ((len(b) == 2) and (b[0] != b[1])):\n                return scipy.zeros_like(y)\n            dy_dtau_factor *= self._compute_dy_dtau(tau, b, r2l2)\n            # Count the number of blocks of length 1:\n            if len(b) == 1:\n                n1 += 1.0\n        # Compute d^(|pi|)f/dy^(|pi|) term:\n        dk_dy = self._compute_dk_dy(y, n)\n        if n1 > 0:\n            mask = (y == 0.0)\n            tau_pow = 2 * (self.nu - n) + n1\n            if tau_pow == 0:\n                # In this case the limit does not exist, so it is set to NaN:\n                dk_dy[mask] = scipy.nan\n            elif tau_pow > 0:\n                dk_dy[mask] = 0.0\n                \n        return dk_dy * dy_dtau_factor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_logging_parser(main_parser):\n    \"Build an argparse argument parser to parse the command line.\"\n\n    main_parser.set_defaults(setup_logging=set_logging_level)\n\n    verbosity_group = main_parser.add_mutually_exclusive_group(required=False)\n    verbosity_group.add_argument(\n        '--verbose',\n        '-v',\n        action='count',\n        help='Output more verbose logging. Can be specified multiple times.')\n    verbosity_group.add_argument(\n        '--quiet',\n        '-q',\n        action='count',\n        help='Output less information to the console during operation. Can be \\\n            specified multiple times.')\n\n    main_parser.add_argument(\n        '--silence-urllib3',\n        action='store_true',\n        help='Silence urllib3 warnings. See '\n        'https://urllib3.readthedocs.org/en/latest/security.html for details.')\n\n    return verbosity_group", "response": "Build an argparse argument parser to parse the command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute and sets the logging level from the parsed arguments.", "response": "def set_logging_level(args):\n    \"Computes and sets the logging level from the parsed arguments.\"\n    root_logger = logging.getLogger()\n    level = logging.INFO\n    logging.getLogger('requests.packages.urllib3').setLevel(logging.WARNING)\n    if \"verbose\" in args and args.verbose is not None:\n        logging.getLogger('requests.packages.urllib3').setLevel(0)  # Unset\n        if args.verbose > 1:\n            level = 5  # \"Trace\" level\n        elif args.verbose > 0:\n            level = logging.DEBUG\n        else:\n            logging.critical(\"verbose is an unexpected value. (%s) exiting.\",\n                             args.verbose)\n            sys.exit(2)\n    elif \"quiet\" in args and args.quiet is not None:\n        if args.quiet > 1:\n            level = logging.ERROR\n        elif args.quiet > 0:\n            level = logging.WARNING\n        else:\n            logging.critical(\"quiet is an unexpected value. (%s) exiting.\",\n                             args.quiet)\n    if level is not None:\n        root_logger.setLevel(level)\n\n    if args.silence_urllib3:\n        # See: https://urllib3.readthedocs.org/en/latest/security.html\n        requests.packages.urllib3.disable_warnings()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_auth(user):\n    '''\n    Check if the user should or shouldn't be inside the system:\n    - If the user is staff or superuser: LOGIN GRANTED\n    - If the user has a Person and it is not \"disabled\": LOGIN GRANTED\n    - Elsewhere: LOGIN DENIED\n    '''\n\n    # Initialize authentication\n    auth = None\n    person = None\n\n    # Check if there is an user\n    if user:\n\n        # It means that Django accepted the user and it is active\n        if user.is_staff or user.is_superuser:\n            # This is an administrator, let it in\n            auth = user\n        else:\n            # It is a normal user, check if there is a person behind\n            person = getattr(user, \"person\", None)\n            if not person:\n                # Check if there is related one\n                person_related = getattr(user, \"people\", None)\n                if person_related:\n                    # Must be only one\n                    if person_related.count() == 1:\n                        person = person_related.get()\n\n            if person and ((person.disabled is None) or (person.disabled > timezone.now())):\n                # There is a person, no disabled found or the found one is fine to log in\n                auth = user\n\n    # Return back the final decision\n    return auth", "response": "Checks if the user is in the system and returns the final decision\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the debugging to a file containing the given message.", "response": "def debug(self, msg):\n        '''\n        Handle the debugging to a file\n        '''\n        # If debug is not disabled\n        if self.__debug is not False:\n\n            # If never was set, try to set it up\n            if self.__debug is None:\n\n                # Check what do we have inside settings\n                debug_filename = getattr(settings, \"AD_DEBUG_FILE\", None)\n                if debug_filename:\n                    # Open the debug file pointer\n                    self.__debug = open(settings.AD_DEBUG_FILE, 'a')\n                else:\n                    # Disable debuging forever\n                    self.__debug = False\n\n            if self.__debug:\n                # Debug the given message\n                self.__debug.write(\"{}\\n\".format(msg))\n                self.__debug.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self, *args, **kwargs):\n        '''\n        Authenticate the user agains LDAP\n        '''\n\n        # Get config\n        username = kwargs.get(\"username\", None)\n        password = kwargs.get(\"password\", None)\n\n        # Check user in Active Directory (authorization == None if can not connect to Active Directory Server)\n        authorization = self.ldap_link(username, password, mode='LOGIN')\n\n        if authorization:\n            # The user was validated in Active Directory\n            user = self.get_or_create_user(username, password)\n            # Get or get_create_user will revalidate the new user\n            if user:\n                # If the user has been properly validated\n                user.is_active = True\n                user.save()\n        else:\n            # Locate user in our system\n            user = User.objects.filter(username=username).first()\n            if user and not user.is_staff:\n                # If access was denied\n                if authorization is False or getattr(settings, \"AD_LOCK_UNAUTHORIZED\", False):\n                    # Deactivate the user\n                    user.is_active = False\n                    user.save()\n\n            # No access and no user here\n            user = None\n\n        # Return the final decision\n        return user", "response": "Authenticate the user agains LDAP"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting or create a user from the database.", "response": "def get_or_create_user(self, username, password):\n        '''\n        Get or create the given user\n        '''\n\n        # Get the groups for this user\n        info = self.get_ad_info(username, password)\n        self.debug(\"INFO found: {}\".format(info))\n\n        # Find the user\n        try:\n            user = User.objects.get(username=username)\n        except User.DoesNotExist:\n            user = User(username=username)\n\n        # Update user\n        user.first_name = info.get('first_name', '')\n        user.last_name = info.get('last_name', '')\n        user.email = info.get('email', '')\n\n        # Check if the user is in the Administrators groups\n        is_admin = False\n        for domain in info['groups']:\n            if 'Domain Admins' in info['groups'][domain]:\n                is_admin = True\n                break\n\n        # Set the user permissions\n        user.is_staff = is_admin\n        user.is_superuser = is_admin\n\n        # Refresh the password\n        user.set_password(password)\n\n        # Validate the selected user and gotten information\n        user = self.validate(user, info)\n        if user:\n            self.debug(\"User got validated!\")\n\n            # Autosave the user until this point\n            user.save()\n\n            # Synchronize user\n            self.synchronize(user, info)\n        else:\n            self.debug(\"User didn't pass validation!\")\n\n        # Finally return user\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the free hyperparameters in self. params to the new parameter values in new_params.", "response": "def set_hyperparams(self, new_params):\n        \"\"\"Sets the free hyperparameters to the new parameter values in new_params.\n        \n        Parameters\n        ----------\n        new_params : :py:class:`Array` or other Array-like, (len(:py:attr:`self.free_params`),)\n            New parameter values, ordered as dictated by the docstring for the\n            class.\n        \"\"\"\n        new_params = scipy.asarray(new_params, dtype=float)\n        \n        if len(new_params) == len(self.free_params):\n            if self.enforce_bounds:\n                for idx, new_param, bound in zip(range(0, len(new_params)), new_params, self.free_param_bounds):\n                    if bound[0] is not None and new_param < bound[0]:\n                        new_params[idx] = bound[0]\n                    elif bound[1] is not None and new_param > bound[1]:\n                        new_params[idx] = bound[1]\n            self.params[~self.fixed_params] = new_params\n        else:\n            raise ValueError(\"Length of new_params must be %s!\" % (len(self.free_params),))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_r2l2(self, tau, return_l=False):\n        l_mat = scipy.tile(self.params[-self.num_dim:], (tau.shape[0], 1))\n        tau_over_l = tau / l_mat\n        tau_over_l[(tau == 0) & (l_mat == 0)] = 0.0\n        r2l2 = scipy.sum((tau_over_l)**2, axis=1)\n        if return_l:\n            return (r2l2, l_mat)\n        else:\n            return r2l2", "response": "r Compute the anisotropic r^2 and l^2 term for the given tau."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets enforce_bounds for both kernels to a new value.", "response": "def enforce_bounds(self, v):\n        \"\"\"Set `enforce_bounds` for both of the kernels to a new value.\n        \"\"\"\n        self._enforce_bounds = v\n        self.k1.enforce_bounds = v\n        self.k2.enforce_bounds = v"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef free_param_bounds(self):\n        return scipy.concatenate((self.k1.free_param_bounds, self.k2.free_param_bounds))", "response": "Returns the bounds of the free hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef free_param_names(self):\n        return scipy.concatenate((self.k1.free_param_names, self.k2.free_param_names))", "response": "Returns the names of the free hyperparameters in order."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the ( free ) hyperparameters.", "response": "def set_hyperparams(self, new_params):\n        \"\"\"Set the (free) hyperparameters.\n        \n        Parameters\n        ----------\n        new_params : :py:class:`Array` or other Array-like\n            New values of the free parameters.\n        \n        Raises\n        ------\n        ValueError\n            If the length of `new_params` is not consistent with :py:attr:`self.params`.\n        \"\"\"\n        new_params = scipy.asarray(new_params, dtype=float)\n        \n        if len(new_params) == len(self.free_params):\n            num_free_k1 = sum(~self.k1.fixed_params)\n            self.k1.set_hyperparams(new_params[:num_free_k1])\n            self.k2.set_hyperparams(new_params[num_free_k1:])\n        else:\n            raise ValueError(\"Length of new_params must be %s!\" % (len(self.free_params),))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_dk_dtau(self, tau, n):\n        # Construct the derivative pattern:\n        # For each dimension, this will contain the index of the dimension\n        # repeated a number of times equal to the order of derivative with\n        # respect to that dimension.\n        # Example: For d^3 k(x, y, z) / dx^2 dy, n would be [2, 1, 0] and\n        # deriv_pattern should be [0, 0, 1]. For k(x, y, z) deriv_pattern is [].\n        deriv_pattern = []\n        for idx in xrange(0, len(n)):\n            deriv_pattern.extend(n[idx] * [idx])\n        deriv_pattern = scipy.asarray(deriv_pattern, dtype=int)\n        # Handle non-derivative case separately for efficiency:\n        if len(deriv_pattern) == 0:\n            return self._compute_k(tau)\n        else:\n            # Compute all partitions of the deriv_pattern:\n            deriv_partitions = generate_set_partitions(deriv_pattern)\n            # Compute the requested derivative using the multivariate Faa di Bruno's equation:\n            dk_dtau = scipy.zeros(tau.shape[0])\n            # Loop over the partitions:\n            for partition in deriv_partitions:\n                dk_dtau += self._compute_dk_dtau_on_partition(tau, partition)\n            return dk_dtau", "response": "Compute the derivative of the k - component at the specified locations with the specified derivatives."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the term inside the sum of Faa di Bruno s formula for the given partition.", "response": "def _compute_dk_dtau_on_partition(self, tau, p):\n        \"\"\"Evaluate the term inside the sum of Faa di Bruno's formula for the given partition.\n        \n        Parameters\n        ----------\n        tau : :py:class:`Matrix`, (`M`, `D`)\n            `M` inputs with dimension `D`.\n        p : list of :py:class:`Array`\n            Each element is a block of the partition representing the\n            derivative orders to use.\n        \n        Returns\n        -------\n        dk_dtau : :py:class:`Array`, (`M`,)\n            The specified derivatives over the given partition at the specified\n            locations.\n        \"\"\"\n        y, r2l2 = self._compute_y(tau, return_r2l2=True)\n        # Compute the d^(|pi|)f/dy term:\n        dk_dtau = self._compute_dk_dy(y, len(p))\n        # Multiply in each of the block terms:\n        for b in p:\n            dk_dtau *= self._compute_dy_dtau(tau, b, r2l2)\n        return dk_dtau"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mask_cov_func(self, *args):\n        # Have to do it in two cases to get the 1d unwrapped properly:\n        if self.num_dim == 1:\n            return self.cov_func(args[0], args[1], *self.params)\n        else:\n            return self.cov_func(args[:self.num_dim], args[self.num_dim:], *self.params)", "response": "Masks the covariance function into a form usable by mpmath. diff."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef constant(X, n, mu, hyper_deriv=None):\n    if (n == 0).all():\n        if hyper_deriv is not None:\n            return scipy.ones(X.shape[0])\n        else:\n            return mu * scipy.ones(X.shape[0])\n    else:\n        return scipy.zeros(X.shape[0])", "response": "Function implementing a constant mean suitable for use with MeanFunction."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies hyperbolic tangent function mtanh.", "response": "def mtanh(alpha, z):\n    \"\"\"Modified hyperbolic tangent function mtanh(z; alpha).\n    \n    Parameters\n    ----------\n    alpha : float\n        The core slope of the mtanh.\n    z : float or array\n        The coordinate of the mtanh.\n    \"\"\"\n    z = scipy.asarray(z)\n    ez = scipy.exp(z)\n    enz = 1.0 / ez\n    return ((1 + alpha * z) * ez - enz) / (ez + enz)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mtanh_profile(X, n, x0, delta, alpha, h, b, hyper_deriv=None):\n    X = X[:, 0]\n    z = (x0 - X) / delta\n    if n[0] == 0:\n        if hyper_deriv is not None:\n            if hyper_deriv == 0:\n                return (h - b) / (2.0 * delta * (scipy.cosh(z))**2) * (\n                    1.0 + alpha / 4.0 * (1.0 + 2.0 * z + scipy.exp(2.0 * z))\n                )\n            elif hyper_deriv == 1:\n                return -(h - b) * z / (2.0 * delta * (scipy.cosh(z))**2) * (\n                    1.0 + alpha / 4.0 * (1.0 + 2.0 * z + scipy.exp(2.0 * z))\n                )\n            elif hyper_deriv == 2:\n                ez = scipy.exp(z)\n                enz = 1.0 / ez\n                return (h - b) / 2.0 * z * ez / (ez + enz)\n            elif hyper_deriv == 3:\n                ez = scipy.exp(z)\n                enz = 1.0 / ez\n                return  0.5 * (1.0 + ((1.0 + alpha * z) * ez - enz) / (ez + enz))\n            elif hyper_deriv == 4:\n                ez = scipy.exp(z)\n                enz = 1.0 / ez\n                return  0.5 * (1.0 - ((1.0 + alpha * z) * ez - enz) / (ez + enz))\n            else:\n                raise ValueError(\"Invalid value for hyper_deriv, \" + str(hyper_deriv))\n        else:\n            return (h + b) / 2.0 + (h - b) * mtanh(alpha, z) / 2.0\n    elif n[0] == 1:\n        if hyper_deriv is not None:\n            if hyper_deriv == 0:\n                return -(h - b) / (2.0 * delta**2.0 * (scipy.cosh(z))**2.0) * (\n                    alpha - (alpha * z + 2) * scipy.tanh(z)\n                )\n            elif hyper_deriv == 1:\n                return (h - b) / (2.0 * delta**2.0 * (scipy.cosh(z))**2.0) * (\n                    1.0 + alpha / 4.0 * (1.0 + 2.0 * z + scipy.exp(2.0 * z)) +\n                    z * (alpha - (alpha * z + 2) * scipy.tanh(z))\n                )\n            elif hyper_deriv == 2:\n                return -(h - b) / (8.0 * delta * (scipy.cosh(z))**2.0) * (\n                    1.0 + 2.0 * z + scipy.exp(2.0 * z)\n                )\n            elif hyper_deriv == 3:\n                return -1.0 / (2.0 * delta * (scipy.cosh(z))**2.0) * (\n                    1.0 + alpha / 4.0 * (1.0 + 2.0 * z + scipy.exp(2.0 * z))\n                )\n            elif hyper_deriv == 4:\n                return 1.0 / (2.0 * delta * (scipy.cosh(z))**2.0) * (\n                    1.0 + alpha / 4.0 * (1.0 + 2.0 * z + scipy.exp(2.0 * z))\n                )\n            else:\n                raise ValueError(\"Invalid value for hyper_deriv, \" + str(hyper_deriv))\n        else:\n            return -(h - b) / (2.0 * delta * (scipy.cosh(z))**2) * (\n                1 + alpha / 4.0 * (1 + 2 * z + scipy.exp(2 * z))\n            )\n    else:\n        raise NotImplementedError(\"Derivatives of order greater than 1 are not supported!\")", "response": "This function computes the profile used with the mtanh function to fit the data at X."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linear(X, n, *args, **kwargs):\n    hyper_deriv = kwargs.pop('hyper_deriv', None)\n    m = scipy.asarray(args[:-1])\n    b = args[-1]\n    if sum(n) > 1:\n        return scipy.zeros(X.shape[0])\n    elif sum(n) == 0:\n        if hyper_deriv is not None:\n            if hyper_deriv < len(m):\n                return X[:, hyper_deriv]\n            elif hyper_deriv == len(m):\n                return scipy.ones(X.shape[0])\n            else:\n                raise ValueError(\"Invalid value for hyper_deriv, \" + str(hyper_deriv))\n        else:\n            return (m * X).sum(axis=1) + b\n    else:\n        # sum(n) == 1:\n        if hyper_deriv is not None:\n            if n[hyper_deriv] == 1:\n                return scipy.ones(X.shape[0])\n            else:\n                return scipy.zeros(X.shape[0])\n        return m[n == 1] * scipy.ones(X.shape[0])", "response": "Linear mean function of arbitrary dimension suitable for use with MeanFunction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_schedule_items(*args, **kw):\n    slot = kw.pop('instance', None)\n    if not slot:\n        return\n    for item in slot.scheduleitem_set.all():\n        item.save(update_fields=['last_updated'])\n    # We also need to update the next slot, in case we changed it's\n    # times as well\n    next_slot = slot.slot_set.all()\n    if next_slot.count():\n        # From the way we structure the slot tree, we know that\n        # there's only 1 next slot that could have changed.\n        for item in next_slot[0].scheduleitem_set.all():\n            item.save(update_fields=['last_updated'])", "response": "We save all the schedule items associated with this slot and so\n       the last_update time is updated to reflect any changes to the\n       timing of the slots."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the difference between the current revision and a previous version", "response": "def make_diff(current, revision):\n    \"\"\"Create the difference between the current revision and a previous version\"\"\"\n    the_diff = []\n    dmp = diff_match_patch()\n\n    for field in (set(current.field_dict.keys()) | set(revision.field_dict.keys())):\n        # These exclusions really should be configurable\n        if field == 'id' or field.endswith('_rendered'):\n            continue\n        # KeyError's may happen if the database structure changes\n        # between the creation of revisions. This isn't ideal,\n        # but should not be a fatal error.\n        # Log this?\n        missing_field = False\n        try:\n            cur_val = current.field_dict[field] or \"\"\n        except KeyError:\n            cur_val = \"No such field in latest version\\n\"\n            missing_field = True\n        try:\n            old_val = revision.field_dict[field] or \"\"\n        except KeyError:\n            old_val = \"No such field in old version\\n\"\n            missing_field = True\n        if missing_field:\n            # Ensure that the complete texts are marked as changed\n            # so new entries containing any of the marker words\n            # don't show up as differences\n            diffs = [(dmp.DIFF_DELETE, old_val), (dmp.DIFF_INSERT, cur_val)]\n            patch =  dmp.diff_prettyHtml(diffs)\n        elif isinstance(cur_val, Markup):\n            # we roll our own diff here, so we can compare of the raw\n            # markdown, rather than the rendered result.\n            if cur_val.raw == old_val.raw:\n                continue\n            diffs = dmp.diff_main(old_val.raw, cur_val.raw)\n            patch =  dmp.diff_prettyHtml(diffs)\n        elif cur_val == old_val:\n            continue\n        else:\n            # Compare the actual field values\n            diffs = dmp.diff_main(force_text(old_val), force_text(cur_val))\n            patch = dmp.diff_prettyHtml(diffs)\n        the_diff.append((field, patch))\n\n    the_diff.sort()\n    return the_diff"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare_view(self, request, object_id, version_id, extra_context=None):\n        opts = self.model._meta\n        object_id = unquote(object_id)\n        # get_for_object's ordering means this is always the latest revision.\n        # The reversion we want to compare to\n        current = Version.objects.get_for_object_reference(self.model, object_id)[0]\n        revision = Version.objects.get_for_object_reference(self.model, object_id).filter(id=version_id)[0]\n\n        the_diff = make_diff(current, revision)\n\n        context = {\n            \"title\": _(\"Comparing current %(model)s with revision created %(date)s\") % {\n                'model': current,\n                'date' : get_date(revision),\n                },\n            \"opts\": opts,\n            \"compare_list_url\": reverse(\"%s:%s_%s_comparelist\" % (self.admin_site.name, opts.app_label, opts.model_name),\n                                                                  args=(quote(object_id),)),\n            \"diff_list\": the_diff,\n        }\n\n        extra_context = extra_context or {}\n        context.update(extra_context)\n        return render(request, self.compare_template or self._get_template_list(\"compare.html\"),\n                      context)", "response": "Actually compare two versions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef comparelist_view(self, request, object_id, extra_context=None):\n        opts = self.model._meta\n        object_id = unquote(object_id)\n        current = get_object_or_404(self.model, pk=object_id)\n        # As done by reversion's history_view\n        action_list = [\n            {\n                \"revision\": version.revision,\n                \"url\": reverse(\"%s:%s_%s_compare\" % (self.admin_site.name, opts.app_label, opts.model_name), args=(quote(version.object_id), version.id)),\n            } for version in self._reversion_order_version_queryset(Version.objects.get_for_object_reference(\n                self.model,\n                object_id).select_related(\"revision__user\"))]\n        context = {\"action_list\": action_list,\n                   \"opts\": opts,\n                   \"object_id\": quote(object_id),\n                   \"original\": current,\n                  }\n        extra_context = extra_context or {}\n        context.update(extra_context)\n        return render(request, self.compare_list_template or self._get_template_list(\"compare_list.html\"),\n                      context)", "response": "Allow selecting versions to compare."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup(self, request):\n        '''\n        Entry point for this class, here we decide basic stuff\n        '''\n\n        # Get details from self\n        info = model_inspect(self)\n        self._appname = getattr(self, 'appname', info['appname'])\n        self._modelname = getattr(self, 'modelname', info['modelname'])\n\n        # Get user information\n        if not hasattr(self, 'user'):\n            self.user = self.request.user\n        # Get profile\n        self.profile = get_profile(self.user)\n\n        # Get language\n        self.language = get_language()\n\n        # Default value for no foreign key attribute\n        if 'no_render_as_foreign' not in self.extra_context:\n            self.extra_context['no_render_as_foreign'] = []", "response": "Setup the object attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the list of templates related to this user", "response": "def get_template_names(self):\n        '''\n        Build the list of templates related to this user\n        '''\n\n        # Get user template\n        template_model = getattr(self, 'template_model', \"{0}/{1}_{2}\".format(self._appname.lower(), self._modelname.lower(), self.get_template_names_key))\n        template_model_ext = getattr(self, 'template_model_ext', 'html')\n        templates = get_template(template_model, self.user, self.language, template_model_ext, raise_error=False)\n        if type(templates) == list:\n            templates.append(\"codenerix/{0}.html\".format(self.get_template_names_key))\n\n        # Return thet of templates\n        return templates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context_data(self, **kwargs):\n        '''\n        Set a base context\n        '''\n\n        # Call the base implementation first to get a context\n        context = super(GenBase, self).get_context_data(**kwargs)\n\n        # Update general context with the stuff we already calculated\n        if hasattr(self, 'html_head'):\n            context['html_head'] = self.html_head(self.object)\n\n        # Add translation system\n        if hasattr(self, 'gentrans'):\n            context['gentranslate'] = self.gentrans.copy()\n            context['gentranslate'].update(self.gentranslate)\n        else:\n            context['gentranslate'] = self.gentranslate\n\n        # Return context\n        return context", "response": "Get a context dictionary with the data we already calculated."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a queryset of all GenModel instances.", "response": "def get_queryset(self, raw_query=False):\n        # Call the base implementation\n        if not self.haystack:\n            queryset = super(GenList, self).get_queryset()\n        else:\n            queryset = SearchQuerySet().models(self.model)\n\n        # Optional tweak methods\n        Mfields = None\n        MlimitQ = None\n        MsearchF = None\n        MsearchQ = None\n        if hasattr(self, '__fields__'):\n            Mfields = self.__fields__\n        if hasattr(self, '__limitQ__'):\n            MlimitQ = self.__limitQ__\n        if hasattr(self, '__searchF__'):\n            MsearchF = self.__searchF__\n        if hasattr(self, '__searchQ__'):\n            MsearchQ = self.__searchQ__\n\n        self._viewname = self.__module__\n\n        # Link to our context and kwargs\n        context = self.__context\n\n        # Update kwargs if json key is present\n        jsonquerytxt = self.request.GET.get('json', self.request.POST.get('json', None))\n        if jsonquerytxt is not None:\n            # Decode json\n            try:\n                jsonquery = json.loads(jsonquerytxt)\n            except json.JSONDecodeError as e:\n                raise IOError(\"json argument in your GET/POST parameters is not a valid JSON string\")\n\n            # Set json context\n            jsondata = self.set_context_json(jsonquery)\n\n            # Get listid\n            listid = jsondata.pop('listid')\n            # Get elementid\n            elementid = jsondata.pop('elementid')\n        else:\n            listid = None\n            elementid = None\n            jsondata = {}\n            jsonquery = {}\n\n        # Build info for GenModel methods\n        MODELINF = MODELINFO(self.model, self._appname, self._modelname, self._viewname, self.request, self.user, self.profile, jsonquery, Mfields, MlimitQ, MsearchF, MsearchQ, listid, elementid, self.__kwargs)\n\n        # Process the filter\n        context['filters'] = []\n        context['filters_obj'] = {}\n\n        # Get field list\n        fields = getattr(self, 'fields', MODELINF.fields())\n\n        # Save GET values\n        context['get'] = []\n        context['getval'] = {}\n        for name in jsondata:\n            struct = {}\n            struct['name'] = name\n            if name == 'rowsperpage':\n                struct['value'] = self.default_rows_per_page\n            elif name == 'page':\n                struct['value'] = 1\n            elif name == 'pages_to_bring':\n                struct['value'] = 1\n            else:\n                struct['value'] = jsondata[name]\n            context['get'].append(struct)\n            context['getval'][name] = struct['value']\n\n        # Filter on limits\n        limits = MODELINF.limitQ()\n        qobjects = None\n        distinct = False\n        for name in limits:\n            if name == 'i_distinct' or name == 'e_distinct':\n                distinct = True\n            else:\n                if qobjects:\n                    qobjects &= limits[name]\n                else:\n                    qobjects = limits[name]\n\n        if qobjects:\n            queryset = queryset.filter(qobjects)\n\n        if hasattr(self, 'annotations'):\n            if not self.haystack:\n                # Prepare annotations\n                if callable(self.annotations):\n                    anot = self.annotations(MODELINF)\n                else:\n                    anot = self.annotations\n\n                # Set annotations\n                queryset = queryset.annotate(**anot)\n            else:\n                raise IOError(\"Haystack doesn't support annotate\")\n\n        if distinct:\n            queryset = queryset.distinct()\n\n        # Filters on fields requested by the user request\n        try:\n            filters_get = jsondata.get('filters', '{}')\n            if type(filters_get) == dict:\n                filters_by_struct = filters_get\n            else:\n                filters_by_struct = json.loads(str(filters_get))\n        except Exception:\n            filters_by_struct = []\n\n        listfilters = {}\n        # Autofilter system\n        if self.autofiltering:\n            listfilters.update(self.autoSearchF(MODELINF))\n        # List of filters from the MODELINF\n        listfilters.update(MODELINF.searchF())\n\n        # Process the search\n        filters_struct = {}\n        for key in filters_by_struct:\n            # Get the value of the original filter\n            value = filters_by_struct[key]\n            # If there is something to filter, filter is not being changed and filter is known by the class\n            try:\n                value = int(value)\n            except ValueError:\n                pass\n            except TypeError:\n                pass\n\n            # ORIG if (key in listfilters) and ((value>0) or (type(value) == list)):\n            # V1 if (value and type(value) == int and key in listfilters) and ((value > 0) or (type(value) == list)):\n            # V2 if (value and type(value) == int and key in listfilters) or ((value > 0) or (type(value) == list)):\n            if value and key in listfilters:\n                # Add the filter to the queryset\n                rule = listfilters[key]\n                # Get type\n                typekind = rule[2]\n                if type(typekind) == list:\n                    # Compatibility: set typekind and fv in the old fassion\n                    if type(value) == int:\n                        fv = typekind[value - 1][0]\n                        queryset = queryset.filter(rule[1](fv))\n                        typekind = 'select'\n                elif typekind == 'select':\n                    # Get selected value from rule\n                    if type(value) == int:\n                        fv = rule[3][value - 1][0]\n                        queryset = queryset.filter(rule[1](fv))\n                elif typekind in ['multiselect', 'multidynamicselect']:\n                    # Get selected values from rule\n                    if type(value) in (list, tuple) and len(value):\n                        qobjects = Q(rule[1](value[0]))\n                        for fvt in value[1:]:\n                            qobjects |= Q(rule[1](fvt))\n                        queryset = queryset.filter(qobjects)\n                elif typekind in ['daterange', 'input']:\n                    # No arguments\n                    fv = value\n                    queryset = queryset.filter(rule[1](fv))\n                elif typekind in ['checkbox', ]:\n                    fv = value\n                    queryset = queryset.filter(rule[1](fv))\n                else:\n                    raise IOError(\"Wrong typekind '{0}' for filter '{1}'\".format(typekind, key))\n                # Save it in the struct as a valid filter\n                filters_struct[key] = value\n\n        # Rewrite filters_json updated\n        filters_json = json.dumps(filters_struct)\n\n        # Build the clean get for filters\n        get = context['get']\n        filters_get = []\n        for element in get:\n            if element['name'] not in ['filters']:\n                struct = {}\n                struct['name'] = element['name']\n                struct['value'] = element['value']\n                filters_get.append(struct)\n\n        # Add filter_json\n        struct = {}\n        struct['name'] = 'filters'\n        struct['value'] = filters_json\n        filters_get.append(struct)\n        context['filters_get'] = filters_get\n\n        # Get the list of filters allowed by this class\n        filters = []\n        for key in listfilters:\n            typekind = listfilters[key][2]\n            if type(typekind) == list:\n                # Compatibility: set typekind and fv in the old fassion\n                choice = [_('All')]\n                for value in typekind:\n                    choice.append(value[1])\n\n                # Decide the choosen field\n                if key in filters_struct.keys():\n                    value = int(filters_struct[key])\n                else:\n                    value = 0\n                typekind = 'select'\n                argument = choice\n            elif typekind == 'select':\n                typevalue = listfilters[key][3]\n                choice = [_('All')]\n                for value in typevalue:\n                    choice.append(value[1])\n\n                # Decide the choosen field\n                if key in filters_struct.keys():\n                    value = int(filters_struct[key])\n                else:\n                    value = 0\n                # Set choice as the command's argument\n                argument = choice\n            elif typekind in ['multiselect', 'multidynamicselect']:\n                if typekind == 'multiselect':\n                    typevalue = listfilters[key][3]\n                    choice = []\n                    for value in typevalue:\n                        choice.append({'id': value[0], 'label': value[1]})\n                else:\n                    choice = list(listfilters[key][3:])\n                    choice[1] = reverse_lazy(choice[1], kwargs={'search': 'a'})[:-1]\n\n                # Decide the choosen field\n                if key in filters_struct.keys():\n                    value = filters_struct[key]\n                else:\n                    value = []\n\n                # Set choice as the command's argument\n                argument = choice\n            elif typekind in ['daterange', 'input']:\n                # Commands withouth arguments\n                argument = None\n                # Get the selected value\n                if key in filters_struct.keys():\n                    value = filters_struct[key]\n                else:\n                    value = None\n            elif typekind in ['checkbox']:\n                # Commands withouth arguments\n                argument = None\n                # Get the selected value\n                if key in filters_struct.keys():\n                    value = filters_struct[key]\n                else:\n                    value = None\n            else:\n                raise IOError(\"Wrong typekind '{0}' for filter '{1}'\".format(typekind, key))\n\n            # Build filtertuple\n            filtertuple = (key, listfilters[key][0], typekind, argument, value)\n            # Save this filter in the corresponding list\n            filters.append(filtertuple)\n\n        # Save all filters\n        context['filters'] = filters\n\n        # Search filter button\n        search_filter_button = jsondata.get('search_filter_button', None)\n        if search_filter_button is not None:\n            self.search_filter_button = search_filter_button\n\n        # Search text in all fields\n        search = jsondata.get('search', '').lower()\n        # Remove extra spaces\n        newlen = len(search)\n        oldlen = 0\n        while newlen != oldlen:\n            oldlen = newlen\n            search = search.replace(\"  \", \" \")\n            newlen = len(search)\n        if len(search) > 0 and search[0] == ' ':\n            search = search[1:]\n        if len(search) > 0 and search[-1] == ' ':\n            search = search[:-1]\n\n        # Save in context\n        context['search'] = search\n        datetimeQ = None\n        if len(search) > 0:\n\n            # Get ID\n            tid = None\n            if 'id:' in search:\n                tid = search.split(\":\")[1].split(\" \")[0]\n                # Decide if it is what we expect\n                try:\n                    tid = int(tid)\n                except Exception:\n                    tid = None\n                # Remove the token\n                if tid:\n                    search = search.replace(\"id:%s\" % (tid), '')\n                    search = search.replace(\"  \", \" \")\n\n            # Get PK\n            tpk = None\n            if 'pk:' in search:\n                tpk = search.split(\":\")[1].split(\" \")[0]\n                # Decide if it is what we expect\n                try:\n                    tpk = int(tpk)\n                except Exception:\n                    tpk = None\n                # Remove the token\n                if tpk:\n                    search = search.replace(\"pk:%s\" % (tpk), '')\n                    search = search.replace(\"  \", \" \")\n\n            # Spaces on front and behind\n            if len(search) > 0 and search[0] == ' ':\n                search = search[1:]\n            if len(search) > 0 and search[-1] == ' ':\n                search = search[:-1]\n\n            searchs = {}\n            # Autofilter system\n            if self.autofiltering:\n                searchs.update(self.autoSearchQ(MODELINF, search))\n\n            # Fields to search in from the MODELINF\n            tmp_search = MODELINF.searchQ(search)\n            if type(tmp_search) == dict:\n                searchs.update(tmp_search)\n            else:\n                searchs['autoSearchQ'] &= tmp_search\n            qobjects = {}\n            qobjectsCustom = {}\n            for name in searchs:\n                # Extract the token\n                qtoken = searchs[name]\n                if qtoken == 'datetime':\n                    # If it is a datetime\n                    datetimeQ = name\n                    continue\n                elif (type(qtoken) == str) or (type(qtoken) == list):\n                    # Prepare query\n                    if type(qtoken) == tuple:\n                        (query, func) = qtoken\n                    else:\n\n                        def lambdax(x):\n                            return x\n\n                        func = lambdax\n                        query = qtoken\n\n                    # If it is a string\n                    if search:\n                        for word in search.split(\" \"):\n                            # If there is a word to process\n                            if len(word) > 0:\n                                # Build the key for the arguments and set the word as a value for the Q search\n                                if word[0] == '-':\n                                    # If negated request\n                                    # key=\"-{}\".format(hashlib.md5(word[1:].encode()).hexdigest())\n                                    qdict = {'{}'.format(query): func(word[1:])}\n                                    qtokens_element = ~Q(**qdict)\n                                else:\n                                    # If positive request\n                                    # key=\"-{}\".format(hashlib.md5(word[1:].encode()).hexdigest())\n                                    qdict = {'{}'.format(query): func(word)}\n                                    qtokens_element = Q(**qdict)\n\n                                # Safe the token\n                                if word in qobjects:\n                                    qobjects[word].append(qtokens_element)\n                                else:\n                                    qobjects[word] = [qtokens_element]\n                else:\n                    if qobjectsCustom:\n                        qobjectsCustom |= searchs[name]\n                    else:\n                        qobjectsCustom = searchs[name]\n\n            # Build positive/negative\n            qdata = None\n            if search and qobjects:\n                for word in search.split(\" \"):\n                    if word.split(\":\")[0] not in ['id', 'pk']:\n                        if word[0] == '-':\n                            negative = True\n                        else:\n                            negative = False\n                        qword = None\n                        for token in qobjects[word]:\n                            if qword:\n                                if negative:\n                                    qword &= token\n                                else:\n                                    qword |= token\n                            else:\n                                qword = token\n                        if qword:\n                            if qdata:\n                                qdata &= qword\n                            else:\n                                qdata = qword\n\n            # Process ID/PK specific searches\n            if tid:\n                queryset = queryset.filter(id=tid)\n            if tpk:\n                queryset = queryset.filter(pk=tpk)\n            # Add custom Q-objects\n            if qobjectsCustom:\n                queryset = queryset.filter(qobjectsCustom)\n            # Add word by word search Q-objects\n            if qdata:\n                queryset = queryset.filter(qdata)\n        else:\n            # Look for datetimeQ field\n            searchs = MODELINF.searchQ(search)\n            for name in searchs:\n                if (searchs[name] == 'datetime'):\n                    datetimeQ = name\n                    continue\n\n        # Datetime Q\n        context['datetimeQ'] = datetimeQ\n        if datetimeQ:\n            # Inicialization\n            f = {}\n            f['year'] = (1900, 2100, False)\n            f['month'] = (1, 12, False)\n            f['day'] = (1, 31, False)\n            f['hour'] = (0, 23, False)\n            f['minute'] = (0, 59, False)\n            f['second'] = (0, 59, False)\n            date_elements = [None, 'year', 'month', 'day', 'hour', 'minute', 'second']\n            # Get configuration of dates and set limits to the queryset\n            for element in date_elements[1:]:\n                value = jsondata.get(element, None)\n                if value:\n                    f[element] = (int(value), int(value), True)\n            if f['year'][2] and f['month'][2] and not f['day'][2]:\n                (g, lastday) = calendar.monthrange(f['year'][1], f['month'][1])\n                f['day'] = (f['day'][0], lastday, f['day'][2])\n            # Limits\n            date_min = datetime.datetime(f['year'][0], f['month'][0], f['day'][0], f['hour'][0], f['minute'][0], f['second'][0])\n            date_max = datetime.datetime(f['year'][1], f['month'][1], f['day'][1], f['hour'][1], f['minute'][1], f['second'][1])\n            qarg1 = {\"{}__gte\".format(datetimeQ): date_min}\n            qarg2 = {\"{}__lte\".format(datetimeQ): date_max}\n            qarg3 = {datetimeQ: None}\n            queryset = queryset.filter((Q(**qarg1) & Q(**qarg2)) | Q(**qarg3))\n\n            # Find actual deepness\n            deepness_index = 0\n            for element in date_elements[1:]:\n                if f[element][2]:\n                    deepness_index += 1\n                else:\n                    break\n\n            # Get results from dates to set the new order\n            exclusion = {}\n            exclusion[datetimeQ] = None\n            date_results = queryset.exclude(**exclusion).values_list(datetimeQ, flat=True)\n            # Remove empty results (usefull when the date is allowed to be empty)\n            if f['day'][0] != f['day'][1]:\n                if f['month'][0] == f['month'][1]:\n                    date_results = date_results.datetimes(datetimeQ, 'day')\n                elif f['year'][0] == f['year'][1]:\n                    date_results = date_results.datetimes(datetimeQ, 'month')\n                else:\n                    date_results = date_results.datetimes(datetimeQ, 'year')\n\n            get = context['get']\n            context['datefilter'] = {}\n            # Save the deepness\n            if deepness_index + 1 == len(date_elements):\n                context['datefilter']['deepness'] = None\n            else:\n                context['datefilter']['deepness'] = date_elements[deepness_index + 1]\n            context['datefilter']['deepnessback'] = []\n            context['datefilter']['deepnessinit'] = []\n            for element in get:\n                if (not element['name'] in date_elements):\n                    struct = {}\n                    struct['name'] = element['name']\n                    struct['value'] = element['value']\n                    context['datefilter']['deepnessinit'].append(struct)\n                    context['datefilter']['deepnessback'].append(struct)\n                elif (element['name'] != date_elements[deepness_index] and f[element['name']][2]):\n                    struct = {}\n                    struct['name'] = element['name']\n                    struct['value'] = element['value']\n                    context['datefilter']['deepnessback'].append(struct)\n            # Build the list of elements\n            context['datefilter']['data'] = []\n            for element in date_results:\n                # Save the data\n                context['datefilter']['data'].append(element.timetuple()[deepness_index])\n            context['datefilter']['data'] = list(set(context['datefilter']['data']))\n            context['datefilter']['data'].sort()\n\n            # Prepare the rightnow result\n            if self.json_worker:\n                rightnow = {}\n                for key in ['year', 'month', 'day', 'hour', 'minute', 'second']:\n                    rightnow[key] = (f[key][2] and f[key][0]) or None\n            else:\n                if f['month'][2]:\n                    month = monthname(f['month'][0])\n                else:\n                    month = '__'\n                if f['hour'][2]:\n                    rightnow = string_concat(grv(f, 'day'), \"/\", month, \"/\", grv(f, 'year'), \" \", grv(f, 'hour'), \":\", grv(f, 'minute'), \":\", grv(f, 'second'))\n                else:\n                    rightnow = string_concat(grv(f, 'day'), \"/\", month, \"/\", grv(f, 'year'))\n            context['datefilter']['rightnow'] = rightnow\n        else:\n            context['datefilter'] = None\n\n        # Distinct\n        # queryset=queryset.distinct()\n\n        # Ordering field autofill\n        try:\n            order_get = jsondata.get('ordering', [])\n            if type(order_get) == list:\n                order_by_struct = order_get\n            else:\n                order_by_struct = json.loads(str(order_get))\n        except Exception:\n            order_by_struct = []\n        order_by = []\n        position = {}\n        counter = 1\n\n        # Build the columns structure and the fields list\n        context['columns'] = []\n        self.__fields = []\n        for value in fields:\n            self.__fields.append(value[0])\n\n        # Auto build rules\n        self.__autorules = self.autorules()\n\n        for order in order_by_struct:\n            name = list(order.keys())[0]\n            lbl = None\n            # use __autofields for ordering by alias\n            for field in self.__autorules:\n                if \"{}:\".format(name) in field:\n                    name = field.split(\":\")[0]\n                    lbl = field.split(\":\")[1]\n                    break\n            direction = order[name]\n\n            if lbl and not lbl.startswith('get_') and not lbl.endswith('_display'):\n                name = lbl\n\n            if direction == 'asc':\n                order_by.append(\"%s\" % (remove_getdisplay(name)))\n            elif direction == 'desc':\n                order_by.append(\"-%s\" % (remove_getdisplay(name)))\n            position[name] = counter\n            counter += 1\n\n        if order_by:\n            queryset = queryset.order_by(*order_by)\n        else:\n            if hasattr(self, 'default_ordering'):\n                if type(self.default_ordering) == list:\n                    queryset = queryset.order_by(*self.default_ordering)\n                else:\n                    queryset = queryset.order_by(self.default_ordering)\n            else:\n                queryset = queryset.order_by(\"pk\")\n\n        # Ordering field autofill\n        sort = {}\n        for value in fields:\n            # Get values\n            if value[0]:\n                name = value[0].split(\":\")[0]\n                order_key = name\n                type_field = self.get_type_field(value[0].split(\":\")[-1])\n            else:\n                name = value[0]\n                # not usable fields, example: fields.append((None, _('Selector'))) in airportslist\n                hash_key = hashlib.md5(value[1].encode()).hexdigest()\n                order_key = \"#{}\".format(hash_key)\n                type_field = None\n\n            publicname = value[1]\n            if len(value) > 2:\n                size = value[2]\n            else:\n                size = None\n            if len(value) > 3:\n                align = value[3]\n            else:\n                align = None\n            # filter column\n            if len(value) > 4:\n                filter_column = value[4]\n            else:\n                filter_column = None\n\n            # Process ordering\n            ordering = []\n            found = False\n            for order in order_by_struct:\n                subname = list(order.keys())[0]\n                direction = order[subname]\n                if order_key == subname:\n                    if direction == 'desc':\n                        direction = ''\n                        sort_class = 'headerSortUp'\n                    elif direction == 'asc':\n                        direction = 'desc'\n                        sort_class = 'headerSortDown'\n                    else:\n                        sort_class = ''\n                        direction = 'asc'\n                    found = True\n                if direction == 'asc' or direction == 'desc':\n                    ordering.append({subname: direction})\n\n            if not found:\n                ordering.append({order_key: 'asc'})\n                sort_class = ''\n            # Save the ordering method\n            sort[order_key] = {}\n            sort[order_key]['id'] = name\n            sort[order_key]['name'] = publicname\n            sort[order_key]['align'] = align\n            sort[order_key]['type'] = type_field\n\n            if filter_column:\n                sort[order_key]['filter'] = filter_column\n\n            if jsonquery is None:\n                sort[order_key]['size'] = size\n                sort[order_key]['class'] = sort_class\n                if order_key and order_key[0] != '*':\n                        sort[order_key]['ordering'] = json.dumps(ordering).replace('\"', '\\\\\"')\n                if order_key in position:\n                    sort[order_key]['position'] = position[order_key]\n\n        # Save ordering in the context\n        if jsonquery is not None:\n            context['ordering'] = order_by_struct\n\n        # Build the columns structure and the fields list\n        context['columns'] = []\n        for value in fields:\n            field = value[0]\n            if field:\n                context['columns'].append(sort[field.split(\":\")[0]])\n            else:\n                hash_key = hashlib.md5(value[1].encode()).hexdigest()\n                field = \"#{}\".format(hash_key)\n                # selector\n                context['columns'].append(sort[field])\n\n        # Auto build rules\n        # self.__autorules = self.autorules()\n\n        # Columns\n        self.__columns = ['pk']\n        # self.__columns = ['id']\n        self.__foreignkeys = []\n        for column in self.model._meta.fields:\n            self.__columns.append(column.name)\n            if column.is_relation:\n                self.__foreignkeys.append(column.name)\n\n        # Localfields\n        self.__related_objects = []\n        for f in self.model._meta.related_objects:\n            self.__related_objects.append(f.name)\n\n        # Model properties\n        model_properties = self.__columns + self.__related_objects\n\n        # === Queryset optimization ===\n        # Get autorules ordered\n        autorules_keys = sorted(self.__autorules.keys())\n        #\n        query_renamed = {}\n        query_optimizer = []\n        query_verifier = []\n        query_select_related = []\n        fields_related_model = []\n\n        for rule in autorules_keys:\n            found = False\n            # name rule origin\n            rule_org = rule\n            # If rule is an alias\n            rulesp = rule.split(\":\")\n            if len(rulesp) == 2:\n                (alias, rule) = rulesp\n            else:\n                alias = rule\n\n            # If rule has a foreign key path (check first level attributes only, nfrule = no foreign rule)\n            nfrule = rule.split(\"__\")\n            do_select_related = False\n            model = self.model\n            if len(nfrule) > 1:\n                ruletmp = []\n                field_related_model = []\n                for n in nfrule:\n                    if model:\n                        for fi in model._meta.fields:\n                            if fi.name == n:\n                                found = True\n                                ruletmp.append(n)\n                                if fi.is_relation:\n                                    model = fi.related_model\n                                    field_related_model.append(fi.name)\n                                else:\n                                    do_select_related = True\n                                    model = None\n                                break\n                    if not found or model is None:\n                        break\n                if field_related_model:\n                    fields_related_model.append(\"__\".join(field_related_model))\n\n                if ruletmp != nfrule:\n                    do_select_related = False\n            elif nfrule[0] in [x.name for x in self.model._meta.fields] or nfrule[0] == 'pk':\n                found = True\n                for fi in model._meta.fields:\n                    if fi.name == nfrule[0] and fi.is_relation:\n                        fields_related_model.append(nfrule[0])\n\n            if not self.haystack and (do_select_related or rule in self.__foreignkeys):\n                # Compatibility with Django 1.10\n                if \"__\" in rule:\n                    query_select_related.append(\"__\".join(rule.split('__')[0:-1]))\n                else:\n                    query_select_related.append(rule)\n\n            nfrule = nfrule[0]\n\n            if nfrule in self.__columns:\n\n                ############################\n                # dejo comentada la restriccion, si se deja y hay una FK \"nunca\" usaria .extra ni .value\n                # no la elimino del todo por si hubiera algun fallo mas adelante,\n                # y se tuviera que parametrizarse de algun otro modo\n                ############################\n\n                # if nfrule not in self.__foreignkeys:\n                if rule not in fields_related_model:\n                    # Save verifier name\n                    query_verifier.append(rule_org)\n\n                # Save renamed field\n                if alias != rule:\n                    query_renamed[alias] = F(rule)\n                    query_optimizer.append(alias)\n                else:\n                    # Save final name\n                    query_optimizer.append(rule)\n\n            if hasattr(self, 'annotations'):\n                # Prepare annotations\n                if callable(self.annotations):\n                    anot = self.annotations(MODELINF)\n                else:\n                    anot = self.annotations\n\n                # Process annotations\n                for xnfrule in anot.keys():\n                    found = True\n                    if xnfrule not in query_verifier:\n                        query_verifier.append(xnfrule)\n                        query_optimizer.append(xnfrule)\n\n            if not found:\n                query_renamed = {}\n                query_optimizer = []\n                query_verifier = []\n                query_select_related = []\n                break\n\n        for rename in query_renamed.keys():\n            if rename in model_properties:\n                if rename in self.__foreignkeys:\n                    msg = \"Invalid alias. The alias '{}' is a foreign key from model '{}' inside app '{}'\"\n                elif rename in self.__columns:\n                    msg = \"Invalid alias. The alias '{}' is a columns from model '{}' inside app '{}'\"\n                elif rename in self.__related_objects:\n                    msg = \"Invalid alias. The alias '{}' is a related object from model '{}' inside app '{}'\"\n                raise Exception(msg.format(rename, self._modelname, self._appname))\n\n        if found and query_select_related:\n            queryset = queryset.select_related(*query_select_related)\n\n        # If we got the query_optimizer to optimize everything, use it\n        # use_extra = False\n        query_verifier.sort()\n        autorules_keys.sort()\n        if found and query_verifier == autorules_keys:\n            # use_extra = True\n            if query_renamed:\n                # queryset=queryset.extra(select=query_renamed).values(*query_optimizer)\n                queryset = queryset.annotate(**query_renamed).values(*query_optimizer)\n            else:\n                queryset = queryset.values(*query_optimizer)\n\n        # Custom queryset\n        if hasattr(self, 'custom_queryset'):\n            queryset = self.custom_queryset(queryset, MODELINF)\n\n        # Internal Codenerix DEBUG for Querysets\n        \"\"\"\n        raise Exception(\"FOUND: {} -- __foreignkeys: {} -- __columns: {} -- autorules_keys: {} -- \\\n            query_select_related: {} -- query_renamed: {} -- query_optimizer: {} | use_extra: {}| -- \\\n            query: {} -- meta.fields: {} -- fields_related_model: {} -- query_verifier: {}\\\n            -- ??? {} == {}\".format(\n                found,\n                self.__foreignkeys, self.__columns, autorules_keys,\n            query_select_related, query_renamed, query_optimizer,use_extra,\n            queryset.query,\n            [x.name for x in self.model._meta.fields],\n            fields_related_model, query_verifier,\n            query_verifier.sort(),autorules_keys.sort()\n            ))\n        #\"\"\"\n\n        # Check if the user requested to return a raw queryset\n        if raw_query:\n            return queryset\n        else:\n            # Check the total count of registers + rows per page\n            total_rows_per_page = jsondata.get('rowsperpage', self.default_rows_per_page)\n            pages_to_bring = jsondata.get('pages_to_bring', 1)\n            if total_rows_per_page == 'All' or self.export:\n                total_rows_per_page = queryset.count()\n            paginator = Paginator(queryset, total_rows_per_page)\n            total_registers = paginator.count\n\n            # Rows per page\n            if total_rows_per_page:\n                try:\n                    total_rows_per_page = int(total_rows_per_page)\n                except Exception:\n                    total_rows_per_page = 'All'\n            else:\n                total_rows_per_page = self.default_rows_per_page\n            if total_rows_per_page == 'All':\n                page_number = 1\n                total_rows_per_page = total_registers\n                total_rows_per_page_out = _('All')\n                total_pages = 1\n            else:\n                total_rows_per_page = int(total_rows_per_page)  # By default 10 rows per page\n                total_rows_per_page_out = total_rows_per_page\n                total_pages = int(total_registers / total_rows_per_page)\n                if total_registers % total_rows_per_page:\n                    total_pages += 1\n                page_number = jsondata.get('page', 1)  # If no page specified use first page\n                if page_number == 'last':\n                    page_number = total_pages\n                else:\n                    try:\n                        page_number = int(page_number)\n                    except Exception:\n                        page_number = 1\n                    if page_number < 1:\n                        page_number = 1\n                    if page_number > total_pages:\n                        page_number = total_pages\n\n            # Build the list of page counters allowed\n            choice = {}\n            c = self.default_rows_per_page\n            chk = 1\n            while total_registers >= c:\n                choice[c] = c\n                if chk == 1:\n                    # From 5 to 10\n                    c = c * 2\n                    # Next level\n                    chk = 2\n                elif chk == 2:\n                    # From 10 to 25 (10*2+10/2)\n                    c = c * 2 + int(c / 2)\n                    # Next level\n                    chk = 3\n                elif chk == 3:\n                    # From 25 to 50\n                    c *= 2\n                    chk = 1\n                # Don't give a too long choice\n                if c > 2000:\n                    break\n\n            # Add all choice in any case\n            if settings.ALL_PAGESALLOWED:\n                choice['All'] = _('All')\n\n            # Save the pagination in the structure\n            context['rowsperpageallowed'] = choice\n            context['rowsperpage'] = total_rows_per_page_out\n            context['pages_to_bring'] = pages_to_bring\n            context['pagenumber'] = page_number\n\n            # Get the full number of registers and save it to context\n            context['total_registers'] = total_registers\n            if total_rows_per_page == 'All':\n                # Remove total_rows_per_page if is all\n                total_rows_per_page = None\n                context['page_before'] = None\n                context['page_after'] = None\n                context['start_register'] = 1\n                context['showing_registers'] = total_registers\n            else:\n                # Page before\n                if page_number <= 1:\n                    context['page_before'] = None\n                else:\n                    context['page_before'] = page_number-1\n                # Page after\n                if page_number >= total_pages:\n                    context['page_after'] = None\n                else:\n                    context['page_after'] = page_number+1\n                # Starting on register number\n                context['start_register'] = (page_number-1)*total_rows_per_page+1\n                context['showing_registers'] = total_rows_per_page\n\n            # Calculate end\n            context['end_register'] = min(context['start_register']+context['showing_registers']-1, total_registers)\n\n            # Add pagination\n            regs = []\n            if paginator.count:\n                desired_page_number = page_number\n                try:\n                    range_pages_to_bring = xrange(pages_to_bring)\n                except NameError:\n                    range_pages_to_bring = range(pages_to_bring)\n                for p in range_pages_to_bring:\n                    try:\n                        regs += paginator.page(desired_page_number)\n                        desired_page_number += 1\n                    except PageNotAnInteger:\n                        # If page is not an integer, deliver first page.\n                        regs += paginator.page(1)\n                        desired_page_number = 2\n                    except EmptyPage:\n                        # If page is out of range (e.g. 9999), deliver last page of results.\n                        if pages_to_bring == 1:\n                            regs += paginator.page(paginator.num_pages)\n                        # Leave bucle\n                        break\n\n            # Fill pages\n            if total_registers:\n                context['pages'] = pages(paginator, page_number)\n                try:\n                    range_fill = xrange(pages_to_bring-1)\n                except NameError:\n                    range_fill = range(pages_to_bring-1)\n                for p in range_fill:\n                    page_number += 1\n                    context['pages'] += pages(paginator, page_number)\n            else:\n                context['pages'] = []\n\n            # Return queryset\n            return regs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_context_data(self, **kwargs):\n        '''\n        Generic list view with validation included and object transfering support\n        '''\n\n        # Call the base implementation first to get a context\n        context = super(GenList, self).get_context_data(**kwargs)\n        # Update general context with the stuff we already calculated\n        context.update(self.__context)\n\n        # Initialize with our timestamp\n        context['now'] = epochdate(time.time())\n        context['profile'] = self.profile\n\n        # Check vtable\n        context['vtable'] = getattr(self, 'vtable', False)\n\n        # Export to excel\n        context['export_excel'] = getattr(self, 'export_excel', True)\n        context['export_name'] = getattr(self, 'export_name', 'list')\n\n        # Check ngincludes\n        context['ngincludes'] = getattr(self, 'ngincludes', {})\n        if 'table' not in context['ngincludes'].keys():\n            context['ngincludes']['table'] = \"{}codenerix/partials/table.html\".format(settings.STATIC_URL)\n\n        # Check linkadd\n        context['linkadd'] = getattr(self, 'linkadd', self.auth_permission('add') or getattr(self, 'public', False))\n\n        # Check linkedit\n        context['linkedit'] = getattr(self, 'linkedit', self.auth_permission('change') or getattr(self, 'public', False))\n\n        # Check showdetails\n        context['show_details'] = getattr(self, 'show_details', False)\n\n        # Check showmodal\n        context['show_modal'] = getattr(self, 'show_modal', False)\n\n        # Set search filter button\n        context['search_filter_button'] = getattr(self, 'search_filter_button', False)\n\n        # Get base template\n        if not self.json_worker:\n            template_base = getattr(self, 'template_base', 'base/base')\n            template_base_ext = getattr(self, 'template_base_ext', 'html')\n            context['template_base'] = get_template(template_base, self.user, self.language, extension=template_base_ext)\n\n        # Try to convert object_id to a numeric id\n        object_id = kwargs.get('object_id', None)\n        try:\n            object_id = int(object_id)\n        except Exception:\n            pass\n\n        # Python 2 VS Python 3 compatibility\n        try:\n            unicode('codenerix')\n            unicodetest = unicode\n        except NameError:\n            unicodetest = str\n\n        if isinstance(object_id, str) or isinstance(object_id, unicodetest):\n            # If object_id is a string, we have a name not an object\n            context['object_name'] = object_id\n            object_obj = None\n        else:\n            # If is not an string\n            if object_id:\n                # If we got one, load the object\n                obj = context['obj']\n                object_obj = get_object_or_404(obj, pk=object_id)\n            else:\n                # There is no object\n                object_obj = None\n            context['object_obj'] = object_obj\n\n        # Attach extra_context\n        context.update(self.extra_context)\n        # Return new context\n        return context", "response": "Generic list view with validation included and object transfering support support\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a base answer for a json answer", "response": "def get_context_json(self, context):\n        '''\n        Return a base answer for a json answer\n        '''\n        # Initialize answer\n        answer = {}\n\n        # Metadata builder\n        answer['meta'] = self.__jcontext_metadata(context)\n\n        # Filter builder\n        answer['filter'] = self.__jcontext_filter(context)\n\n        # Head builder\n        answer['table'] = {}\n        answer['table']['head'] = self.__jcontext_tablehead(context)\n        answer['table']['body'] = None\n        answer['table']['header'] = None\n        answer['table']['summary'] = None\n\n        # Return answer\n        return answer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_context_json(self, jsonquery):\n        '''\n        Get a json parameter and rebuild the context back to a dictionary (probably kwargs)\n        '''\n\n        # Make sure we are getting dicts\n        if type(jsonquery) != dict:\n            raise IOError(\"set_json_context() method can be called only with dictionaries, you gave me a '{}'\".format(type(jsonquery)))\n\n        # Set we will answer json to this request\n        self.json = True\n\n        # Transfer keys\n        newget = {}\n        for key in ['search', 'search_filter_button', 'page', 'pages_to_bring', 'rowsperpage', 'filters', 'year', 'month', 'day', 'hour', 'minute', 'second']:\n            if key in jsonquery:\n                newget[key] = jsonquery[key]\n\n        # Add transformed ordering\n        json_ordering = jsonquery.get('ordering', None)\n        if json_ordering:\n            # Convert to list\n            ordering = []\n            for key in json_ordering:\n                ordering.append({key: jsonquery['ordering'][key]})\n\n            # Order the result from ordering\n            # ordering = sorted(ordering, key=lambda x: abs(x.values()[0]))\n            ordering = sorted(ordering, key=lambda x: abs(list(x.values())[0]))\n            # Save ordering\n            newget['ordering'] = []\n            for orderer in ordering:\n                key = list(orderer.keys())[0]\n                value = orderer[key]\n                if value > 0:\n                    value = 'asc'\n                elif value < 0:\n                    value = 'desc'\n                else:\n                    value = None\n                if value:\n                    newget['ordering'].append({key: value})\n\n        # Get listid\n        newget['listid'] = jsonquery.get(\"listid\", None)\n\n        # Get elementid\n        newget['elementid'] = jsonquery.get(\"elementid\", None)\n\n        # Return new get\n        return newget", "response": "Set the context to a dictionary and rebuild the context back to a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_form(self, form_class=None):\n        '''\n        Set form groups to the groups specified in the view if defined\n        '''\n        formobj = super(GenModify, self).get_form(form_class)\n\n        # Set requested group to this form\n        selfgroups = getattr(self, \"form_groups\", None)\n        if selfgroups:\n            if type(selfgroups) == list:\n                formobj.__groups__ = lambda: selfgroups\n            else:\n                formobj.__groups__ = selfgroups\n        else:\n            selfgroups = getattr(self, \"__groups__\", None)\n            if selfgroups:\n                formobj.__groups__ = selfgroups\n\n        # Return the new updated form\n        return formobj", "response": "Returns a new form object with the current set of groups"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_filled_structure(self, subgroup=None):\n        '''\n        method in charged of filling an structure containing the object fields\n        values taking into account the 'group' attribute from the corresponding\n        form object, which is necesary to fill the details form as it is configured\n        in the 'group' attribute\n        '''\n        # initilize the result structure\n        result = []\n\n        # the object corresponding model content is taken into a dictionary\n        object_content = model_to_dict(self.object)\n\n        # generallically some common or specific fields are not interesting\n        if 'exclude_fields' not in dir(self):\n            self.exclude_fields = []\n\n        self.exclude_fields.append(\"id\")\n\n        for field in (self.exclude_fields):\n            if field in object_content.keys():\n                object_content.pop(field)\n        # following is going to be created an structure with the appropieate caption\n        # for every existing field in the current model\n        verbose_names = {}\n        for field in object_content.keys():\n            verbose_names[field] = self.model._meta.get_field(field).verbose_name\n        # the found fields in the groups structure are going to be taked into account\n        gr_object_content = []\n\n        if subgroup:\n            group_array = subgroup\n        else:\n            group_array = self.groups\n\n        for group in group_array:\n\n            # raise Exception(group)\n            item = {}\n\n            item[\"name\"] = smart_text(group[0])\n            item[\"col\"] = group[1]\n            item_elements = group[2:]\n\n            sublist = []\n\n            idx = 0\n            for item_element in item_elements:\n                # the element can contains another groups\n                if (idx > 1) and (type(item_element) == tuple):\n                    # Recursive\n                    sublist.append(self.get_filled_structure([subgroup]))\n                else:\n                    filter_field = None\n                    # Check if it is a list\n                    if type(item_element) == list:\n                        # if it is a list, that means that can be found the\n                        # corresponding values for colums and any other\n                        field = item_element[0]\n                        # take into account that field caption can be passed as\n                        # third list element\n                        if len(item_element) >= 3 and item_element[2]:\n                            verbose_names[field] = _(item_element[2])\n                        if len(item_element) >= 9:\n                            filter_field = item_element[8]\n                    else:\n\n                        field = item_element\n\n                    if field not in verbose_names:\n                        if field.startswith('get_') and field.endswith('_display'):\n                            label_field = remove_getdisplay(field)\n                            if self.model:\n                                try:\n                                    verbose_names[field] = self.model._meta.get_field(label_field).verbose_name\n                                except FieldDoesNotExist:\n                                    verbose_names[field] = _(label_field)\n                            else:\n                                verbose_names[field] = _(label_field)\n                        else:\n                            label_field = field\n                            verbose_names[field] = _(label_field)\n\n                    args = {}\n\n                    value = None\n                    for field_split in field.split('__'):\n                        if value is None:\n                            try:\n                                verbose_names[field] = self.object._meta.get_field(field_split).verbose_name\n                            except AttributeError:\n                                pass\n                            except FieldDoesNotExist:\n                                pass\n\n                            value = getattr(self.object, field_split, None)\n                        else:\n                            try:\n                                verbose_names[field] = value._meta.get_field(field_split).verbose_name\n                            except AttributeError:\n                                pass\n                            except FieldDoesNotExist:\n                                pass\n                            value = getattr(value, field_split, None)\n\n                    if callable(value):\n                        # if 'request' in value.func_code.co_varnames:\n                        related = (getattr(value, 'all', None) is not None)\n                        if related:\n                            value = \", \".join([str(x) for x in value.all()])\n                        else:\n                            if 'request' in value.__code__.co_varnames:\n                                args['request'] = self.request\n                                # Call the method\n                            value = value(**args)\n\n                    sublist.append({\n                        \"name\": _(verbose_names[field]),\n                        \"value\": value,\n                        \"filter\": filter_field,\n                    })\n                    gr_object_content.append(field)\n\n                # Increment index\n                idx += 1\n\n            item[\"value\"] = sublist\n            result.append(item)\n\n        for field in object_content.keys():\n            item = {}\n            if field not in gr_object_content:\n                item[\"name\"] = _(verbose_names[field])\n                item[\"value\"] = getattr(self.object, field)\n                result.append(item)\n\n        return result", "response": "method to fill the structure with the object fields and the details form object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a dictionary of attributes to a single string.", "response": "def flatatt(attrs):\n    \"\"\"\n    Pilfered from `django.forms.utils`:\n    Convert a dictionary of attributes to a single string.\n    The returned string will contain a leading space followed by key=\"value\",\n    XML-style pairs. In the case of a boolean value, the key will appear\n    without a value. Otherwise, the value is formatted through its own dict of `attrs`,\n    which can be useful to parametrize Angular directives.\n    It is assumed that the keys do not need to be\n    XML-escaped. If the passed dictionary is empty, then return an empty\n    string.\n\n    The result is passed through 'mark_safe' (by way of 'format_html_join').\n    \"\"\"\n    key_value_attrs = []\n    boolean_attrs = []\n    for attr, value in attrs.items():\n        if isinstance(value, bool):\n            if value:\n                boolean_attrs.append((attr,))\n        else:\n            try:\n                value = value.format(**attrs)\n            except KeyError:\n                pass\n            key_value_attrs.append((attr, value))\n\n    return (\n        format_html_join('', ' {}=\"{}\"', sorted(key_value_attrs)) +\n        format_html_join('', ' {}', sorted(boolean_attrs))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the kernel directly at the given values of tau.", "response": "def _compute_k(self, tau):\n        r\"\"\"Evaluate the kernel directly at the given values of `tau`.\n        \n        Parameters\n        ----------\n        tau : :py:class:`Matrix`, (`M`, `D`)\n            `M` inputs with dimension `D`.\n        \n        Returns\n        -------\n            k : :py:class:`Array`, (`M`,)\n                :math:`k(\\tau)` (less the :math:`\\sigma^2` prefactor).\n        \"\"\"\n        y = self._compute_y(tau)\n        return y**(-self.params[1])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the derivative of the outer form of the RQ kernel.", "response": "def _compute_dk_dy(self, y, n):\n        \"\"\"Evaluate the derivative of the outer form of the RQ kernel.\n        \n        Parameters\n        ----------\n        y : :py:class:`Array`, (`M`,)\n            `M` inputs to evaluate at.\n        n : non-negative scalar int\n            Order of derivative to compute.\n        \n        Returns\n        -------\n        dk_dy : :py:class:`Array`, (`M`,)\n            Specified derivative at specified locations.\n        \"\"\"\n        p = fixed_poch(1.0 - self.params[1] - n, n)\n        return p * y**(-self.params[1] - n)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a directory name return the Page representing it in the menu heirarchy.", "response": "def get_parent(self, directory):\n        \"\"\"\n        Given a directory name, return the Page representing it in the menu\n        heirarchy.\n        \"\"\"\n        assert settings.PAGE_DIR.startswith('/')\n        assert settings.PAGE_DIR.endswith('/')\n\n        parents = directory[len(settings.PAGE_DIR):]\n\n        page = None\n        if parents:\n            for slug in parents.split('/'):\n                page = Page.objects.get(parent=page, slug=slug)\n        return page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the correct URL to SSO with the given method.", "response": "def wafer_sso_url(context, sso_method):\n    '''\n    Return the correct URL to SSO with the given method.\n    '''\n    request = context.request\n    url = reverse(getattr(views, '%s_login' % sso_method))\n    if 'next' in request.GET:\n        url += '?' + urlencode({'next': request.GET['next']})\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authorize(args):\n    oauth2_instance = oauth2.build_oauth2(args.app, args)\n    oauth2_instance.build_authorizer()\n    logging.info('Application \"%s\" authorized!', args.app)", "response": "Authorizes Coursera s OAuth2 client for using coursera. org API servers for a specific application"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck courseraoauth2client s connectivity to the coursera. org API servers for a specific application", "response": "def check_auth(args):\n    \"\"\"\n    Checks courseraoauth2client's connectivity to the coursera.org API servers\n    for a specific application\n    \"\"\"\n    oauth2_instance = oauth2.build_oauth2(args.app, args)\n    auth = oauth2_instance.build_authorizer()\n    my_profile_url = (\n        'https://api.coursera.org/api/externalBasicProfiles.v1?'\n        'q=me&fields=name'\n    )\n    r = requests.get(my_profile_url, auth=auth)\n    if r.status_code != 200:\n        logging.error('Received response code %s from the basic profile API.',\n                      r.status_code)\n        logging.debug('Response body:\\n%s', r.text)\n        sys.exit(1)\n    try:\n        external_id = r.json()['elements'][0]['id']\n    except:\n        logging.error(\n            'Could not parse the external id out of the response body %s',\n            r.text)\n        external_id = None\n\n    try:\n        name = r.json()['elements'][0]['name']\n    except:\n        logging.error(\n            'Could not parse the name out of the response body %s',\n            r.text)\n        name = None\n\n    if not args.quiet > 0:\n        print 'Name: %s' % name\n        print 'External ID: %s' % external_id\n\n    if name is None or external_id is None:\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying the state of the authentication cache.", "response": "def display_auth_cache(args):\n    '''\n    Writes to the screen the state of the authentication cache. (For debugging\n    authentication issues.) BEWARE: DO NOT email the output of this command!!!\n    You must keep the tokens secure. Treat them as passwords.\n    '''\n    oauth2_instance = oauth2.build_oauth2(args.app, args)\n    if not args.quiet > 0:\n        token = oauth2_instance.token_cache['token']\n        if not args.no_truncate and token is not None:\n            token = token[:10] + '...'\n        print \"Auth token: %s\" % token\n\n        expires_time = oauth2_instance.token_cache['expires']\n        expires_in = int((expires_time - time.time()) * 10) / 10.0\n        print \"Auth token expires in: %s seconds.\" % expires_in\n\n        if 'refresh' in oauth2_instance.token_cache:\n            refresh = oauth2_instance.token_cache['refresh']\n            if not args.no_truncate and refresh is not None:\n                refresh = refresh[:10] + '...'\n            print \"Refresh token: %s\" % refresh\n        else:\n            print \"No refresh token found.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tanh_warp_arb(X, l1, l2, lw, x0):\n    if isinstance(X, scipy.ndarray):\n        if isinstance(X, scipy.matrix):\n            X = scipy.asarray(X, dtype=float)\n        return 0.5 * ((l1 + l2) - (l1 - l2) * scipy.tanh((X - x0) / lw))\n    else:\n        return 0.5 * ((l1 + l2) - (l1 - l2) * mpmath.tanh((X - x0) / lw))", "response": "r Wars the X coordinate with the tanh model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gauss_warp_arb(X, l1, l2, lw, x0):\n    if isinstance(X, scipy.ndarray):\n        if isinstance(X, scipy.matrix):\n            X = scipy.asarray(X, dtype=float)\n        return l1 - (l1 - l2) * scipy.exp(-4.0 * scipy.log(2.0) * (X - x0)**2.0 / (lw**2.0))\n    else:\n        return l1 - (l1 - l2) * mpmath.exp(-4.0 * mpmath.log(2.0) * (X - x0)**2.0 / (lw**2.0))", "response": "r Wars the X coordinate with a Gaussian - shaped divot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tanh_warp(x, n, l1, l2, lw, x0):\n    if n == 0:\n        return (l1 + l2) / 2.0 - (l1 - l2) / 2.0 * scipy.tanh((x - x0) / lw)\n    elif n == 1:\n        return -(l1 - l2) / (2.0 * lw) * (scipy.cosh((x - x0) / lw))**(-2.0)\n    else:\n        raise NotImplementedError(\"Only derivatives up to order 1 are supported!\")", "response": "r Implements a tanh warping function and its derivative."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef double_tanh_warp(x, n, lcore, lmid, ledge, la, lb, xa, xb):\n    a, b, c = scipy.dot([[-0.5, 0, 0.5], [0, 0.5, -0.5], [0.5, 0.5, 0]],\n                        [[lcore], [ledge], [lmid]])\n    a = a[0]\n    b = b[0]\n    c = c[0]\n    if n == 0:\n        return a * scipy.tanh((x - xa) / la) + b * scipy.tanh((x - xb) / lb) + c\n    elif n == 1:\n        return (a / la * (scipy.cosh((x - xa) / la))**(-2.0) +\n                b / lb * (scipy.cosh((x - xb) / lb))**(-2.0))\n    else:\n        raise NotImplementedError(\"Only derivatives up to order 1 are supported!\")", "response": "r Implements a sum - of - tanh warping function and its derivative."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cubic_bucket_warp(x, n, l1, l2, l3, x0, w1, w2, w3):\n    x1 = x0 - w2 / 2.0 - w1 / 2.0\n    x2 = x0 + w2 / 2.0 + w3 / 2.0\n    x_shift_1 = (x - x1 + w1 / 2.0) / w1\n    x_shift_2 = (x - x2 + w3 / 2.0) / w3\n    if n == 0:\n        return (\n            l1 * (x <= (x1 - w1 / 2.0)) + (\n                -2.0 * (l2 - l1) * (x_shift_1**3 - 3.0 / 2.0 * x_shift_1**2) + l1\n            ) * ((x > (x1 - w1 / 2.0)) & (x < (x1 + w1 / 2.0))) +\n            l2 * ((x >= (x1 + w1 / 2.0)) & (x <= x2 - w3 / 2.0)) + (\n                -2.0 * (l3 - l2) * (x_shift_2**3 - 3.0 / 2.0 * x_shift_2**2) + l2\n            ) * ((x > (x2 - w3 / 2.0)) & (x < (x2 + w3 / 2.0))) +\n            l3 * (x >= (x2 + w3 / 2.0))\n        )\n    elif n == 1:\n        return (\n            (\n                -2.0 * (l2 - l1) * (3 * x_shift_1**2 - 3.0 * x_shift_1) / w1\n            ) * ((x > (x1 - w1 / 2.0)) & (x < (x1 + w1 / 2.0))) +\n            (\n                -2.0 * (l3 - l2) * (3 * x_shift_2**2 - 3.0 * x_shift_2) / w3\n            ) * ((x > (x2 - w3 / 2.0)) & (x < (x2 + w3 / 2.0)))\n        )\n    else:\n        raise NotImplementedError(\"Only up to first derivatives are supported!\")", "response": "This function warps the length scale with a piecewise cubic bucket shape."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quintic_bucket_warp(x, n, l1, l2, l3, x0, w1, w2, w3):\n    x1 = x0 - w2 / 2.0 - w1 / 2.0\n    x2 = x0 + w2 / 2.0 + w3 / 2.0\n    x_shift_1 = 2.0 * (x - x1) / w1\n    x_shift_3 = 2.0 * (x - x2) / w3\n    if n == 0:\n        return (\n            l1 * (x <= (x1 - w1 / 2.0)) + (\n                0.5 * (l2 - l1) * (\n                    3.0 / 8.0 * x_shift_1**5 -\n                    5.0 / 4.0 * x_shift_1**3 +\n                    15.0 / 8.0 * x_shift_1\n                ) + (l1 + l2) / 2.0\n            ) * ((x > (x1 - w1 / 2.0)) & (x < (x1 + w1 / 2.0))) +\n            l2 * ((x >= (x1 + w1 / 2.0)) & (x <= x2 - w3 / 2.0)) + (\n                0.5 * (l3 - l2) * (\n                    3.0 / 8.0 * x_shift_3**5 -\n                    5.0 / 4.0 * x_shift_3**3 +\n                    15.0 / 8.0 * x_shift_3\n                ) + (l2 + l3) / 2.0\n            ) * ((x > (x2 - w3 / 2.0)) & (x < (x2 + w3 / 2.0))) +\n            l3 * (x >= (x2 + w3 / 2.0))\n        )\n    elif n == 1:\n        return (\n            (\n                0.5 * (l2 - l1) * (\n                    5.0 * 3.0 / 8.0 * x_shift_1**4 -\n                    3.0 * 5.0 / 4.0 * x_shift_1**2 +\n                    15.0 / 8.0\n                ) / w1\n            ) * ((x > (x1 - w1 / 2.0)) & (x < (x1 + w1 / 2.0))) + (\n                0.5 * (l3 - l2) * (\n                    5.0 * 3.0 / 8.0 * x_shift_3**4 -\n                    3.0 * 5.0 / 4.0 * x_shift_3**2 +\n                    15.0 / 8.0\n                ) / w3\n            ) * ((x > (x2 - w3 / 2.0)) & (x < (x2 + w3 / 2.0)))\n        )\n    else:\n        raise NotImplementedError(\"Only up to first derivatives are supported!\")", "response": "Returns the length scale with a piecewise quintic bucket shape."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_multiple_choices_required(self):\n        errors = []\n        if self.required:\n            for key, msg in self.error_messages.items():\n                if key == 'required':\n                    errors.append(('$error.required', msg))\n        return errors", "response": "Returns a list of errors that can be used to check if a MultipleChoiceField is required."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sso(user, desired_username, name, email, profile_fields=None):\n    if not user:\n        if not settings.REGISTRATION_OPEN:\n            raise SSOError('Account registration is closed')\n        user = _create_desired_user(desired_username)\n        _configure_user(user, name, email, profile_fields)\n\n    if not user.is_active:\n        raise SSOError('Account disabled')\n\n    # login() expects the logging in backend to be set on the user.\n    # We are bypassing login, so fake it.\n    user.backend = settings.AUTHENTICATION_BACKENDS[0]\n    return user", "response": "Create a user from the parameters and log it in."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parallel_compute_ll_matrix(gp, bounds, num_pts, num_proc=None):\n    if num_proc is None:\n        num_proc = multiprocessing.cpu_count()\n    \n    present_free_params = gp.free_params\n    \n    bounds = scipy.atleast_2d(scipy.asarray(bounds, dtype=float))\n    if bounds.shape[1] != 2:\n        raise ValueError(\"Argument bounds must have shape (n, 2)!\")\n    # If bounds is a single tuple, repeat it for each free parameter:\n    if bounds.shape[0] == 1:\n        bounds = scipy.tile(bounds, (len(present_free_params), 1))\n    # If num_pts is a single value, use it for all of the parameters:\n    try:\n        iter(num_pts)\n    except TypeError:\n        num_pts = num_pts * scipy.ones(bounds.shape[0], dtype=int)\n    else:\n        num_pts = scipy.asarray(num_pts, dtype=int)\n        if len(num_pts) != len(present_free_params):\n            raise ValueError(\"Length of num_pts must match the number of free parameters of kernel!\")\n    \n    # Form arrays to evaluate parameters over:\n    param_vals = []\n    for k in xrange(0, len(present_free_params)):\n        param_vals.append(scipy.linspace(bounds[k, 0], bounds[k, 1], num_pts[k]))\n    \n    pv_cases = list()\n    gp_cases = list()\n    num_pts_cases = list()\n    for k in xrange(0, len(param_vals[0])):\n        specific_param_vals = list(param_vals)\n        specific_param_vals[0] = param_vals[0][k]\n        pv_cases.append(specific_param_vals)\n        \n        gp_cases += [copy.deepcopy(gp)]\n        \n        num_pts_cases.append(num_pts)\n    \n    pool =  multiprocessing.Pool(processes=num_proc)    \n    try:\n        vals = scipy.asarray(\n            pool.map(\n                _compute_ll_matrix_wrapper,\n                zip(gp_cases, pv_cases, num_pts_cases)\n            )\n        )\n    finally:\n        pool.close()\n    \n    return (vals, param_vals)", "response": "Compute the matrix of the log likelihood over the parameter space in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slice_plot(*args, **kwargs):\n    names = kwargs.get('names', None)\n    n = kwargs.get('n', 100)\n    num_axes = len(args) - 1\n    if num_axes < 3:\n        raise GPArgumentError(\"Must pass at least four arguments to slice_plot!\")\n    if num_axes != args[0].ndim:\n        raise GPArgumentError(\"Number of dimensions of the first argument \"\n                              \"must match the number of additional arguments \"\n                              \"provided!\")\n    if names is None:\n        names = [str(k) for k in range(2, num_axes)]\n    f = plt.figure()\n    height_ratios = [8]\n    height_ratios += (num_axes - 2) * [1]\n    gs = mplgs.GridSpec(num_axes - 2 + 1, 2, height_ratios=height_ratios, width_ratios=[8, 1])\n    \n    a_main = f.add_subplot(gs[0, 0])\n    a_cbar = f.add_subplot(gs[0, 1])\n    a_sliders = []\n    for idx in xrange(0, num_axes - 2):\n        a_sliders.append(f.add_subplot(gs[idx+1, :]))\n    \n    title = f.suptitle(\"\")\n    \n    def update(val):\n        \"\"\"Update the slice shown.\n        \"\"\"\n        a_main.clear()\n        a_cbar.clear()\n        idxs = [int(slider.val) for slider in sliders]\n        vals = [args[k + 3][idxs[k]] for k in range(0, num_axes - 2)]\n        descriptions = tuple(itertools.chain.from_iterable(itertools.izip(names[2:], vals)))\n        fmt = \"Slice\" + (num_axes - 2) * \", %s: %f\"\n        title.set_text(fmt % descriptions)\n        \n        a_main.set_xlabel(names[1])\n        a_main.set_ylabel(names[0])\n        cs = a_main.contour(\n            args[2],\n            args[1],\n            args[0][scipy.s_[:, :] + tuple(idxs)].squeeze(),\n            n,\n            vmin=args[0].min(),\n            vmax=args[1].max()\n        )\n        cbar = f.colorbar(cs, cax=a_cbar)\n        cbar.set_label(\"LL\")\n        \n        f.canvas.draw()\n    \n    idxs_0 = (num_axes - 2) * [0]\n    sliders = []\n    for idx in xrange(0, num_axes - 2):\n        sliders.append(\n            mplw.Slider(\n                a_sliders[idx],\n                '%s index' % names[idx + 2],\n                0,\n                len(args[idx + 3]) - 1,\n                valinit=idxs_0[idx],\n                valfmt='%d'\n            )\n        )\n        sliders[-1].on_changed(update)\n    \n    update(idxs_0)\n    \n    f.canvas.mpl_connect('key_press_event', lambda evt: arrow_respond(sliders[0], evt))\n    \n    return f", "response": "Constructs a Matplotlib figure that lets you look at slices through a multidimensional array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles arrow key events in plot windows.", "response": "def arrow_respond(slider, event):\n    \"\"\"Event handler for arrow key events in plot windows.\n    \n    Pass the slider object to update as a masked argument using a lambda function::\n        \n        lambda evt: arrow_respond(my_slider, evt)\n    \n    Parameters\n    ----------\n    slider : Slider instance associated with this handler.\n    event : Event to be handled.\n    \"\"\"\n    if event.key == 'right':\n        slider.set_val(min(slider.val + 1, slider.valmax))\n    elif event.key == 'left':\n        slider.set_val(max(slider.val - 1, slider.valmin))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef debit(self, amount, credit_account, description, debit_memo=\"\", credit_memo=\"\", datetime=None):\n\n        assert amount >= 0\n        return self.post(amount, credit_account, description, self_memo=debit_memo, other_memo=credit_memo, datetime=datetime)", "response": "Post a debit of amount against this account and credit_account respectively."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npost a credit of amount against this account and credit_account respectively.", "response": "def credit(self, amount, debit_account, description, debit_memo=\"\", credit_memo=\"\", datetime=None):\n        \"\"\" Post a credit of 'amount' and a debit of -amount against this account and credit_account respectively.\n\n        note amount must be non-negative.\n        \"\"\"\n        assert amount >= 0\n        return self.post(-amount, debit_account, description, self_memo=credit_memo, other_memo=debit_memo, datetime=datetime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post(self, amount, other_account, description, self_memo=\"\", other_memo=\"\", datetime=None):\n\n        #Note: debits are always positive, credits are always negative.  They should be negated before displaying\n        #(expense and liability?) accounts\n        tx = self._new_transaction()\n\n        if datetime:\n            tx.t_stamp = datetime\n        #else now()\n\n        tx.description = description\n        tx.save()\n\n        a1 = self._make_ae(self._DEBIT_IN_DB() * amount, self_memo, tx)\n        a1.save()\n        a2 = other_account._make_ae(-self._DEBIT_IN_DB() * amount, other_memo, tx)\n        a2.save()\n\n        return (a1, a2)", "response": "Post a transaction of amount against this account and the negative amount against other_account."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the account balance as of the given date", "response": "def balance(self, date=None):\n        \"\"\" returns the account balance as of 'date' (datetime stamp) or now().  \"\"\"\n\n        qs = self._entries()\n        if date:\n            qs = qs.filter(transaction__t_stamp__lt=date)\n        r = qs.aggregate(b=Sum('amount'))\n        b = r['b']\n\n        flip = self._DEBIT_IN_DB()\n        if self._positive_credit():\n            flip *= -1\n\n        if b is None:\n            b = Decimal(\"0.00\")\n        b *= flip\n\n        #print \"returning balance %s for %s\" % (b, self)\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Totals object containing the sum of all debits credits and net change over the period of time from start to end.", "response": "def totals(self, start=None, end=None):\n        \"\"\"Returns a Totals object containing the sum of all debits, credits\n        and net change over the period of time from start to end.\n\n        'start' is inclusive, 'end' is exclusive\n        \"\"\"\n\n        qs = self._entries_range(start=start, end=end)\n        qs_positive = qs.filter(amount__gt=Decimal(\"0.00\")).all().aggregate(Sum('amount'))\n        qs_negative = qs.filter(amount__lt=Decimal(\"0.00\")).all().aggregate(Sum('amount'))\n\n        #Is there a cleaner way of saying this?  Should the sum of 0 things be None?\n        positives = qs_positive['amount__sum'] if qs_positive['amount__sum'] is not None else 0\n        negatives = -qs_negative['amount__sum'] if qs_negative['amount__sum'] is not None else 0\n\n        if self._DEBIT_IN_DB() > 0:\n            debits = positives\n            credits = negatives\n        else:\n            debits = negatives\n            credits = positives\n\n        net = debits-credits\n        if self._positive_credit():\n            net = -net\n\n        return self.Totals(credits, debits, net)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ledger(self, start=None, end=None):\n\n        DEBIT_IN_DB = self._DEBIT_IN_DB()\n\n        flip = 1\n        if self._positive_credit():\n            flip *= -1\n\n        qs = self._entries_range(start=start, end=end)\n        qs = qs.order_by(\"transaction__t_stamp\", \"transaction__tid\")\n\n        balance = Decimal(\"0.00\")\n        if start:\n            balance = self.balance(start)\n\n        if not qs:\n            return []\n\n        #helper is a hack so the caller can test for no entries.\n        def helper(balance_in):\n            balance = balance_in\n            for e in qs.all():\n                amount = e.amount * DEBIT_IN_DB\n                o_balance = balance\n                balance += flip * amount\n\n                yield LedgerEntry(amount, e, o_balance, balance)\n\n        return helper(balance)", "response": "Returns a list of entries for this account in chronological order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the account for the given third - party. Raise <something > if the third - party doesn t belong to this bookset.", "response": "def get_third_party(self, third_party):\n        \"\"\"Return the account for the given third-party.  Raise <something> if the third party doesn't belong to this bookset.\"\"\"\n        actual_account = third_party.get_account()\n        assert actual_account.get_bookset() == self\n        return ThirdPartySubAccount(actual_account, third_party=third_party)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_third_party(self, third_party):\n        actual_account = third_party.get_account()\n\n        assert actual_account.get_bookset() == self.get_bookset()\n        return ProjectAccount(actual_account, project=self, third_party=third_party)", "response": "Return the account for the given third - party. Raise <something > if the third - party doesn t belong to this bookset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding any slots that overlap with the current one", "response": "def find_overlapping_slots(all_slots):\n    \"\"\"Find any slots that overlap\"\"\"\n    overlaps = set([])\n    for slot in all_slots:\n        # Because slots are ordered, we can be more efficient than this\n        # N^2 loop, but this is simple and, since the number of slots\n        # should be low, this should be \"fast enough\"\n        start = slot.get_start_time()\n        end = slot.end_time\n        for other_slot in all_slots:\n            if other_slot.pk == slot.pk:\n                continue\n            if other_slot.get_day() != slot.get_day():\n                # different days, can't overlap\n                continue\n            # Overlap if the start_time or end_time is bounded by our times\n            # start_time <= other.start_time < end_time\n            # or\n            # start_time < other.end_time <= end_time\n            other_start = other_slot.get_start_time()\n            other_end = other_slot.end_time\n            if start <= other_start and other_start < end:\n                overlaps.add(slot)\n                overlaps.add(other_slot)\n            elif start < other_end and other_end <= end:\n                overlaps.add(slot)\n                overlaps.add(other_slot)\n    return overlaps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_non_contiguous(all_items):\n    non_contiguous = []\n    for item in all_items:\n        if item.slots.count() < 2:\n            # No point in checking\n            continue\n        last_slot = None\n        for slot in item.slots.all().order_by('end_time'):\n            if last_slot:\n                if last_slot.end_time != slot.get_start_time():\n                    non_contiguous.append(item)\n                    break\n            last_slot = slot\n    return non_contiguous", "response": "Find any items that have slots that aren t contiguous"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_items(all_items):\n    validation = []\n    for item in all_items:\n        if item.talk is not None and item.page is not None:\n            validation.append(item)\n        elif item.talk is None and item.page is None:\n            validation.append(item)\n        elif item.talk and item.talk.status not in [ACCEPTED, CANCELLED]:\n            validation.append(item)\n    return validation", "response": "Find errors in the schedule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_duplicate_schedule_items(all_items):\n    duplicates = []\n    seen_talks = {}\n    for item in all_items:\n        if item.talk and item.talk in seen_talks:\n            duplicates.append(item)\n            if seen_talks[item.talk] not in duplicates:\n                duplicates.append(seen_talks[item.talk])\n        else:\n            seen_talks[item.talk] = item\n        # We currently allow duplicate pages for cases were we need disjoint\n        # schedule items, like multiple open space sessions on different\n        # days and similar cases. This may be revisited later\n    return duplicates", "response": "Find talks / pages assigned to mulitple schedule items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_clashes(all_items):\n    clashes = {}\n    seen_venue_slots = {}\n    for item in all_items:\n        for slot in item.slots.all():\n            pos = (item.venue, slot)\n            if pos in seen_venue_slots:\n                if seen_venue_slots[pos] not in clashes:\n                    clashes[pos] = [seen_venue_slots[pos]]\n                clashes[pos].append(item)\n            else:\n                seen_venue_slots[pos] = item\n    # We return a list, to match other validators\n    return clashes.items()", "response": "Find schedule items which clash ( common slot and venue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind venues assigned slots that aren t on the allowed list of days.", "response": "def find_invalid_venues(all_items):\n    \"\"\"Find venues assigned slots that aren't on the allowed list\n       of days.\"\"\"\n    venues = {}\n    for item in all_items:\n        valid = False\n        item_days = list(item.venue.days.all())\n        for slot in item.slots.all():\n            for day in item_days:\n                if day == slot.get_day():\n                    valid = True\n                    break\n        if not valid:\n            venues.setdefault(item.venue, [])\n            venues[item.venue].append(item)\n    return venues.items()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_schedule():\n    all_items = prefetch_schedule_items()\n    for validator, _type, _msg in SCHEDULE_ITEM_VALIDATORS:\n        if validator(all_items):\n            return False\n\n    all_slots = prefetch_slots()\n    for validator, _type, _msg in SLOT_VALIDATORS:\n        if validator(all_slots):\n            return False\n    return True", "response": "Helper routine to easily test if the schedule is valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_form(self, request, obj=None, **kwargs):\n        if obj is None:\n            # Adding a new Slot\n            kwargs['form'] = SlotAdminAddForm\n        return super(SlotAdmin, self).get_form(request, obj, **kwargs)", "response": "Change the form depending on whether we re adding or editing the slot."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cached_menus():\n    items = cache.get(CACHE_KEY)\n    if items is None:\n        menu = generate_menu()\n        cache.set(CACHE_KEY, menu.items)\n    else:\n        menu = Menu(items)\n    return menu", "response": "Return the menus from the cache or generate them if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_obj(str_or_obj):\n    if not isinstance(str_or_obj, six.string_types):\n        return str_or_obj\n    parts = str_or_obj.split(\".\")\n    mod, modname = None, None\n    for p in parts:\n        modname = p if modname is None else \"%s.%s\" % (modname, p)\n        try:\n            mod = __import__(modname)\n        except ImportError:\n            if mod is None:\n                raise\n            break\n    obj = mod\n    for p in parts[1:]:\n        obj = getattr(obj, p)\n    return obj", "response": "Returns the object that corresponds to the passed string or None if it is a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a new list of menus.", "response": "def generate_menu():\n    \"\"\"Generate a new list of menus.\"\"\"\n    root_menu = Menu(list(copy.deepcopy(settings.WAFER_MENUS)))\n    for dynamic_menu_func in settings.WAFER_DYNAMIC_MENUS:\n        dynamic_menu_func = maybe_obj(dynamic_menu_func)\n        dynamic_menu_func(root_menu)\n    return root_menu"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lock(self):\n        '''\n        Try to get locked the file\n        - the function will wait until the file is unlocked if 'wait' was defined as locktype\n        - the funciton will raise AlreadyLocked exception if 'lock' was defined as locktype\n        '''\n\n        # Open file\n        self.__fd = open(self.__lockfile, \"w\")\n\n        # Get it locked\n        if self.__locktype == \"wait\":\n            # Try to get it locked until ready\n            fcntl.flock(self.__fd.fileno(), fcntl.LOCK_EX)\n        elif self.__locktype == \"lock\":\n            # Try to get the locker if can not raise an exception\n            try:\n                fcntl.flock(self.__fd.fileno(), fcntl.LOCK_EX|fcntl.LOCK_NB)\n            except IOError:\n                raise AlreadyLocked(\"File is already locked\")", "response": "Try to get locked the file and lock it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_handler(state_token, done_function):\n    '''\n    Makes a a handler class to use inside the basic python HTTP server.\n\n    state_token is the expected state token.\n    done_function is a function that is called, with the code passed to it.\n    '''\n\n    class LocalServerHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n\n        def error_response(self, msg):\n            logging.warn(\n                'Error response: %(msg)s. %(path)s',\n                msg=msg,\n                path=self.path)\n            self.send_response(400)\n            self.send_header('Content-type', 'text/plain')\n            self.end_headers()\n            self.wfile.write(msg)\n\n        def do_GET(self):\n            parsed = urlparse.urlparse(self.path)\n            if len(parsed.query) == 0 or parsed.path != '/callback':\n                self.error_response(\n                    'We encountered a problem with your request.')\n                return\n\n            params = urlparse.parse_qs(parsed.query)\n            if params['state'] != [state_token]:\n                self.error_response(\n                    'Attack detected: state tokens did not match!')\n                return\n\n            if len(params['code']) != 1:\n                self.error_response('Wrong number of \"code\" query parameters.')\n                return\n\n            self.send_response(200)\n            self.send_header('Content-type', 'text/plain')\n            self.end_headers()\n            self.wfile.write(\n                \"courseraoauth2client: we have captured Coursera's response \"\n                \"code. Feel free to close this browser window now and return \"\n                \"to your terminal. Thanks!\")\n            done_function(params['code'][0])\n\n    return LocalServerHandler", "response": "Makes a handler class to use inside the basic python HTTP server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configuration():\n    'Loads configuration from the file system.'\n    defaults = '''\n[oauth2]\nhostname = localhost\nport = 9876\napi_endpoint = https://api.coursera.org\nauth_endpoint = https://accounts.coursera.org/oauth2/v1/auth\ntoken_endpoint = https://accounts.coursera.org/oauth2/v1/token\nverify_tls = True\ntoken_cache_base = ~/.coursera\n\n[manage_graders]\nclient_id = NS8qaSX18X_Eu0pyNbLsnA\nclient_secret = bUqKqGywnGXEJPFrcd4Jpw\nscopes = view_profile manage_graders\n\n[manage_research_exports]\nclient_id = sDHC8Nfp-b1XMbzZx8Wa4w\nclient_secret = pgD4adDd7lm-ksfG7UazUA\nscopes = view_profile manage_research_exports\n'''\n    cfg = ConfigParser.SafeConfigParser()\n    cfg.readfp(io.BytesIO(defaults))\n    cfg.read([\n        '/etc/coursera/courseraoauth2client.cfg',\n        os.path.expanduser('~/.coursera/courseraoauth2client.cfg'),\n        'courseraoauth2client.cfg',\n    ])\n    return cfg", "response": "Loads configuration from the file system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the local fs cache for pre - authorized access tokens", "response": "def _load_token_cache(self):\n        'Reads the local fs cache for pre-authorized access tokens'\n        try:\n            logging.debug('About to read from local file cache file %s',\n                          self.token_cache_file)\n            with open(self.token_cache_file, 'rb') as f:\n                fs_cached = cPickle.load(f)\n                if self._check_token_cache_type(fs_cached):\n                    logging.debug('Loaded from file system: %s', fs_cached)\n                    return fs_cached\n                else:\n                    logging.warn('Found unexpected value in cache. %s',\n                                 fs_cached)\n                    return None\n        except IOError:\n            logging.debug(\n                'Did not find file: %s on the file system.',\n                self.token_cache_file)\n            return None\n        except:\n            logging.info(\n                'Encountered exception loading from the file system.',\n                exc_info=True)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _save_token_cache(self, new_cache):\n        'Write out to the filesystem a cache of the OAuth2 information.'\n        logging.debug('Looking to write to local authentication cache...')\n        if not self._check_token_cache_type(new_cache):\n            logging.error('Attempt to save a bad value: %s', new_cache)\n            return\n        try:\n            logging.debug('About to write to fs cache file: %s',\n                          self.token_cache_file)\n            with open(self.token_cache_file, 'wb') as f:\n                cPickle.dump(new_cache, f, protocol=cPickle.HIGHEST_PROTOCOL)\n                logging.debug('Finished dumping cache_value to fs cache file.')\n        except:\n            logging.exception(\n                'Could not successfully cache OAuth2 secrets on the file '\n                'system.')", "response": "Write out to the filesystem a cache of the OAuth2 information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_token_cache_type(self, cache_value):\n        '''\n        Checks the cache_value for appropriate type correctness.\n\n        Pass strict=True for strict validation to ensure the latest types are\n        being written.\n\n        Returns true is correct type, False otherwise.\n        '''\n        def check_string_value(name):\n            return (\n                isinstance(cache_value[name], str) or\n                isinstance(cache_value[name], unicode)\n            )\n\n        def check_refresh_token():\n            if 'refresh' in cache_value:\n                return check_string_value('refresh')\n            else:\n                return True\n\n        return (\n            isinstance(cache_value, dict) and\n            'token' in cache_value and\n            'expires' in cache_value and\n            check_string_value('token') and\n            isinstance(cache_value['expires'], float) and\n            check_refresh_token()\n        )", "response": "Checks the cache_value for correct type correctness."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _authorize_new_tokens(self):\n        '''\n        Stands up a new localhost http server and retrieves new OAuth2 access\n        tokens from the Coursera OAuth2 server.\n        '''\n        logging.info('About to request new OAuth2 tokens from Coursera.')\n        # Attempt to request new tokens from Coursera via the browser.\n        state_token = uuid.uuid4().hex\n        authorization_url = self._build_authorizaton_url(state_token)\n\n        sys.stdout.write(\n            'Please visit the following URL to authorize this app:\\n')\n        sys.stdout.write('\\t%s\\n\\n' % authorization_url)\n        if _platform == 'darwin':\n            # OS X -- leverage the 'open' command present on all modern macs\n            sys.stdout.write(\n                'Mac OS X detected; attempting to auto-open the url '\n                'in your default browser...\\n')\n            try:\n                subprocess.check_call(['open', authorization_url])\n            except:\n                logging.exception('Could not call `open %(url)s`.',\n                                  url=authorization_url)\n\n        if self.local_webserver_port is not None:\n            # Boot up a local webserver to retrieve the response.\n            server_address = ('', self.local_webserver_port)\n            code_holder = CodeHolder()\n\n            local_server = BaseHTTPServer.HTTPServer(\n                server_address,\n                _make_handler(state_token, code_holder))\n\n            while not code_holder.has_code():\n                local_server.handle_request()\n            coursera_code = code_holder.code\n        else:\n            coursera_code = raw_input('Please enter the code from Coursera: ')\n\n        form_data = {\n            'code': coursera_code,\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'redirect_uri': self._redirect_uri,\n            'grant_type': 'authorization_code',\n        }\n        return self._request_tokens_from_token_endpoint(form_data)", "response": "Request new OAuth2 tokens from Coursera."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexchanges a refresh token for an access token", "response": "def _exchange_refresh_tokens(self):\n        'Exchanges a refresh token for an access token'\n        if self.token_cache is not None and 'refresh' in self.token_cache:\n            # Attempt to use the refresh token to get a new access token.\n            refresh_form = {\n                'grant_type': 'refresh_token',\n                'refresh_token': self.token_cache['refresh'],\n                'client_id': self.client_id,\n                'client_secret': self.client_secret,\n            }\n            try:\n                tokens = self._request_tokens_from_token_endpoint(refresh_form)\n                tokens['refresh'] = self.token_cache['refresh']\n                return tokens\n            except OAuth2Exception:\n                logging.exception(\n                    'Encountered an exception during refresh token flow.')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef foreignkey(element, exceptions):\n    '''\n    function to determine if each select field needs a create button or not\n    '''\n    label = element.field.__dict__['label']\n    try:\n        label = unicode(label)\n    except NameError:\n        pass\n    if (not label) or (label in exceptions):\n        return False\n    else:\n        return \"_queryset\" in element.field.__dict__", "response": "function to determine if each select field needs a create button or not"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hyperprior(self):\n        hp = self.k.hyperprior * self.noise_k.hyperprior\n        if self.mu is not None:\n            hp *= self.mu.hyperprior\n        return hp", "response": "Combined hyperprior for the kernel noise kernel and mean function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the set of fixed hyperparameter flags for the kernel noise kernel and mean function.", "response": "def fixed_params(self):\n        \"\"\"Combined fixed hyperparameter flags for the kernel, noise kernel and (if present) mean function.\n        \"\"\"\n        fp = CombinedBounds(self.k.fixed_params, self.noise_k.fixed_params)\n        if self.mu is not None:\n            fp = CombinedBounds(fp, self.mu.fixed_params)\n        return fp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef params(self):\n        p = CombinedBounds(self.k.params, self.noise_k.params)\n        if self.mu is not None:\n            p = CombinedBounds(p, self.mu.params)\n        return p", "response": "Returns the hyperparameters for the kernel noise kernel and mean function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef param_names(self):\n        pn = CombinedBounds(self.k.param_names, self.noise_k.param_names)\n        if self.mu is not None:\n            pn = CombinedBounds(pn, self.mu.param_names)\n        return pn", "response": "Combined names for the hyperparameters for the kernel noise kernel and mean function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef free_params(self):\n        p = CombinedBounds(self.k.free_params, self.noise_k.free_params)\n        if self.mu is not None:\n            p = CombinedBounds(p, self.mu.free_params)\n        return p", "response": "Combined free hyperparameters for the kernel noise kernel and mean function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the free parameters.", "response": "def free_params(self, value):\n        \"\"\"Set the free parameters. Note that this bypasses enforce_bounds.\n        \"\"\"\n        value = scipy.asarray(value, dtype=float)\n        self.K_up_to_date = False\n        self.k.free_params = value[:self.k.num_free_params]\n        self.noise_k.free_params = value[self.k.num_free_params:self.k.num_free_params + self.noise_k.num_free_params]\n        if self.mu is not None:\n            self.mu.free_params = value[self.k.num_free_params + self.noise_k.num_free_params:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef free_param_bounds(self):\n        fpb = CombinedBounds(self.k.free_param_bounds, self.noise_k.free_param_bounds)\n        if self.mu is not None:\n            fpb = CombinedBounds(fpb, self.mu.free_param_bounds)\n        return fpb", "response": "Combined free hyperparameter bounds for the kernel noise kernel and mean function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncombine free hyperparameter names for the kernel noise kernel and mean function.", "response": "def free_param_names(self):\n        \"\"\"Combined free hyperparameter names for the kernel, noise kernel and (if present) mean function.\n        \"\"\"\n        p = CombinedBounds(self.k.free_param_names, self.noise_k.free_param_names)\n        if self.mu is not None:\n            p = CombinedBounds(p, self.mu.free_param_names)\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_data(self, X, y, err_y=0, n=0, T=None):   \n        # Verify y has only one non-trivial dimension:\n        y = scipy.atleast_1d(scipy.asarray(y, dtype=float))\n        if len(y.shape) != 1:\n            raise ValueError(\n                \"Training targets y must have only one dimension with length \"\n                \"greater than one! Shape of y given is %s\" % (y.shape,)\n            )\n        \n        # Handle scalar error or verify shape of array error matches shape of y:\n        try:\n            iter(err_y)\n        except TypeError:\n            err_y = err_y * scipy.ones_like(y, dtype=float)\n        else:\n            err_y = scipy.asarray(err_y, dtype=float)\n            if err_y.shape != y.shape:\n                raise ValueError(\n                    \"When using array-like err_y, shape must match shape of y! \"\n                    \"Shape of err_y given is %s, shape of y given is %s.\" % (err_y.shape, y.shape)\n                )\n        if (err_y < 0).any():\n            raise ValueError(\"All elements of err_y must be non-negative!\")\n        \n        # Handle scalar training input or convert array input into 2d.\n        X = scipy.atleast_2d(scipy.asarray(X, dtype=float))\n        # Correct single-dimension inputs:\n        if self.num_dim == 1 and X.shape[0] == 1:\n            X = X.T\n        if T is None and X.shape != (len(y), self.num_dim):\n            raise ValueError(\n                \"Shape of training inputs must be (len(y), k.num_dim)! X given \"\n                \"has shape %s, shape of y is %s and num_dim=%d.\" % (X.shape, y.shape, self.num_dim)\n            )\n        \n        # Handle scalar derivative orders or verify shape of array derivative\n        # orders matches shape of y:\n        try:\n            iter(n)\n        except TypeError:\n            n = n * scipy.ones_like(X, dtype=int)\n        else:\n            n = scipy.atleast_2d(scipy.asarray(n, dtype=int))\n            # Correct single-dimension inputs:\n            if self.num_dim == 1 and n.shape[1] != 1:\n                n = n.T\n            if n.shape != X.shape:\n                raise ValueError(\n                    \"When using array-like n, shape must be (len(y), k.num_dim)! \"\n                    \"Shape of n given is %s, shape of y given is %s and num_dim=%d.\"\n                    % (n.shape, y.shape, self.num_dim)\n                )\n        if (n < 0).any():\n            raise ValueError(\"All elements of n must be non-negative integers!\")\n        \n        # Handle transform:\n        if T is None and self.T is not None:\n            T = scipy.eye(len(y))\n        if T is not None:\n            T = scipy.atleast_2d(scipy.asarray(T, dtype=float))\n            if T.ndim != 2:\n                raise ValueError(\"T must have exactly 2 dimensions!\")\n            if T.shape[0] != len(y):\n                raise ValueError(\n                    \"T must have as many rows are there are elements in y!\"\n                )\n            if T.shape[1] != X.shape[0]:\n                raise ValueError(\n                    \"There must be as many columns in T as there are rows in X!\"\n                )\n            if self.T is None and self.X is not None:\n                self.T = scipy.eye(len(self.y))\n            \n            if self.T is None:\n                self.T = T\n            else:\n                self.T = scipy.linalg.block_diag(self.T, T)\n        \n        if self.X is None:\n            self.X = X\n        else:\n            self.X = scipy.vstack((self.X, X))\n        self.y = scipy.append(self.y, y)\n        self.err_y = scipy.append(self.err_y, err_y)\n        if self.n is None:\n            self.n = n\n        else:\n            self.n = scipy.vstack((self.n, n))\n        self.K_up_to_date = False", "response": "Add data to the training data set of the GaussianProcess instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncondenses duplicate points using a transformation matrix.", "response": "def condense_duplicates(self):\n        \"\"\"Condense duplicate points using a transformation matrix.\n        \n        This is useful if you have multiple non-transformed points at the same\n        location or multiple transformed points that use the same quadrature\n        points.\n        \n        Won't change the GP if all of the rows of [X, n] are unique. Will create\n        a transformation matrix T if necessary. Note that the order of the\n        points in [X, n] will be arbitrary after this operation.\n        \n        If there are any transformed quantities (i.e., `self.T` is not None), it\n        will also remove any quadrature points for which all of the weights are\n        zero (even if all of the rows of [X, n] are unique).\n        \"\"\"\n        unique, inv = unique_rows(\n            scipy.hstack((self.X, self.n)),\n            return_inverse=True\n        )\n        # Only proceed if there is anything to be gained:\n        if len(unique) != len(self.X):\n            if self.T is None:\n                self.T = scipy.eye(len(self.y))\n            new_T = scipy.zeros((len(self.y), unique.shape[0]))\n            for j in xrange(0, len(inv)):\n                new_T[:, inv[j]] += self.T[:, j]\n            self.T = new_T\n            self.n = unique[:, self.X.shape[1]:]\n            self.X = unique[:, :self.X.shape[1]]\n        # Also remove any points which don't enter into the calculation:\n        if self.T is not None:\n            # Find the columns of T which actually enter in:\n            # Recall that T is (n, n_Q), X is (n_Q, n_dim).\n            good_cols = (self.T != 0.0).any(axis=0)\n            self.T = self.T[:, good_cols]\n            self.X = self.X[good_cols, :]\n            self.n = self.n[good_cols, :]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_outliers(self, thresh=3, **predict_kwargs):\n        mean = self.predict(\n            self.X, n=self.n, noise=False, return_std=False,\n            output_transform=self.T, **predict_kwargs\n        )\n        deltas = scipy.absolute(mean - self.y) / self.err_y\n        deltas[self.err_y == 0] = 0\n        bad_idxs = (deltas >= thresh)\n        good_idxs = ~bad_idxs\n        \n        # Pull out the old values so they can be returned:\n        y_bad = self.y[bad_idxs]\n        err_y_bad = self.err_y[bad_idxs]\n        if self.T is not None:\n            T_bad = self.T[bad_idxs, :]\n            non_zero_cols = (T_bad != 0).all(axis=0)\n            T_bad = T_bad[:, non_zero_cols]\n            X_bad = self.X[non_zero_cols, :]\n            n_bad = self.n[non_zero_cols, :]\n        else:\n            X_bad = self.X[bad_idxs, :]\n            n_bad = self.n[bad_idxs, :]\n        \n        # Delete the offending points:\n        if self.T is None:\n            self.X = self.X[good_idxs, :]\n            self.n = self.n[good_idxs, :]\n        else:\n            self.T = self.T[good_idxs, :]\n            non_zero_cols = (self.T != 0).all(axis=0)\n            self.T = self.T[:, non_zero_cols]\n            self.X = self.X[non_zero_cols, :]\n            self.n = self.n[non_zero_cols, :]\n        self.y = self.y[good_idxs]\n        self.err_y = self.err_y[good_idxs]\n        self.K_up_to_date = False\n        \n        if self.T is None:\n            return (X_bad, y_bad, err_y_bad, n_bad, bad_idxs)\n        else:\n            return (X_bad, y_bad, err_y_bad, n_bad, bad_idxs, T_bad)", "response": "This method removes outliers from the GP with very simplistic outlier detection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npredicting the mean and covariance at the inputs Xstar.", "response": "def predict(self, Xstar, n=0, noise=False, return_std=True, return_cov=False,\n                full_output=False, return_samples=False, num_samples=1,\n                samp_kwargs={}, return_mean_func=False, use_MCMC=False,\n                full_MC=False, rejection_func=None, ddof=1, output_transform=None,\n                **kwargs):\n        \"\"\"Predict the mean and covariance at the inputs `Xstar`.\n        \n        The order of the derivative is given by `n`. The keyword `noise` sets\n        whether or not noise is included in the prediction.\n        \n        Parameters\n        ----------\n        Xstar : array, (`M`, `D`)\n            `M` test input values of dimension `D`.\n        n : array, (`M`, `D`) or scalar, non-negative int, optional\n            Order of derivative to predict (0 is the base quantity). If `n` is\n            scalar, the value is used for all points in `Xstar`. If non-integer\n            values are passed, they will be silently rounded. Default is 0\n            (return base quantity).\n        noise : bool, optional\n            Whether or not noise should be included in the covariance. Default\n            is False (no noise in covariance).\n        return_std : bool, optional\n            Set to True to compute and return the standard deviation for the\n            predictions, False to skip this step. Default is True (return tuple\n            of (`mean`, `std`)).\n        return_cov : bool, optional\n            Set to True to compute and return the full covariance matrix for the\n            predictions. This overrides the `return_std` keyword. If you want\n            both the standard deviation and covariance matrix pre-computed, use\n            the `full_output` keyword.\n        full_output : bool, optional\n            Set to True to return the full outputs in a dictionary with keys:\n            \n                ================= ===========================================================================\n                mean              mean of GP at requested points\n                std               standard deviation of GP at requested points\n                cov               covariance matrix for values of GP at requested points\n                samp              random samples of GP at requested points (only if `return_samples` is True)\n                mean_func         mean function of GP (only if `return_mean_func` is True)\n                cov_func          covariance of mean function of GP (zero if not using MCMC)\n                std_func          standard deviation of mean function of GP (zero if not using MCMC)\n                mean_without_func mean of GP minus mean function of GP\n                cov_without_func  covariance matrix of just the GP portion of the fit\n                std_without_func  standard deviation of just the GP portion of the fit\n                ================= ===========================================================================\n        \n        return_samples : bool, optional\n            Set to True to compute and return samples of the GP in addition to\n            computing the mean. Only done if `full_output` is True. Default is\n            False.\n        num_samples : int, optional\n            Number of samples to compute. If using MCMC this is the number of\n            samples per MCMC sample, if using present values of hyperparameters\n            this is the number of samples actually returned. Default is 1.\n        samp_kwargs : dict, optional\n            Additional keywords to pass to :py:meth:`draw_sample` if\n            `return_samples` is True. Default is {}.\n        return_mean_func : bool, optional\n            Set to True to return the evaluation of the mean function in\n            addition to computing the mean of the process itself. Only done if\n            `full_output` is True and `self.mu` is not None. Default is False.\n        use_MCMC : bool, optional\n            Set to True to use :py:meth:`predict_MCMC` to evaluate the\n            prediction marginalized over the hyperparameters.\n        full_MC : bool, optional\n            Set to True to compute the mean and covariance matrix using Monte\n            Carlo sampling of the posterior. The samples will also be returned\n            if full_output is True. The sample mean and covariance will be\n            evaluated after filtering through `rejection_func`, so conditional\n            means and covariances can be computed. Default is False (do not use\n            full sampling).\n        rejection_func : callable, optional\n            Any samples where this function evaluates False will be rejected,\n            where it evaluates True they will be kept. Default is None (no\n            rejection). Only has an effect if `full_MC` is True.\n        ddof : int, optional\n            The degree of freedom correction to use when computing the covariance\n            matrix when `full_MC` is True. Default is 1 (unbiased estimator).\n        output_transform: array, (`L`, `M`), optional\n            Matrix to use to transform the output vector of length `M` to one of\n            length `L`. This can, for instance, be used to compute integrals.\n        **kwargs : optional kwargs\n            All additional kwargs are passed to :py:meth:`predict_MCMC` if\n            `use_MCMC` is True.\n        \n        Returns\n        -------\n        mean : array, (`M`,)\n            Predicted GP mean. Only returned if `full_output` is False.\n        std : array, (`M`,)\n            Predicted standard deviation, only returned if `return_std` is True, `return_cov` is False and `full_output` is False.\n        cov : array, (`M`, `M`)\n            Predicted covariance matrix, only returned if `return_cov` is True and `full_output` is False.\n        full_output : dict\n            Dictionary with fields for mean, std, cov and possibly random samples and the mean function. Only returned if `full_output` is True.\n        \n        Raises\n        ------\n        ValueError\n            If `n` is not consistent with the shape of `Xstar` or is not entirely\n            composed of non-negative integers.\n        \"\"\"\n        if use_MCMC:\n            res = self.predict_MCMC(\n                Xstar,\n                n=n,\n                noise=noise,\n                return_std=return_std or full_output,\n                return_cov=return_cov or full_output,\n                return_samples=full_output and (return_samples or rejection_func),\n                return_mean_func=full_output and return_mean_func,\n                num_samples=num_samples,\n                samp_kwargs=samp_kwargs,\n                full_MC=full_MC,\n                rejection_func=rejection_func,\n                ddof=ddof,\n                output_transform=output_transform,\n                **kwargs\n            )\n            if full_output:\n                return res\n            elif return_cov:\n                return (res['mean'], res['cov'])\n            elif return_std:\n                return (res['mean'], res['std'])\n            else:\n                return res['mean']\n        else:\n            # Process Xstar:\n            Xstar = scipy.atleast_2d(scipy.asarray(Xstar, dtype=float))\n            # Handle 1d x case where array is passed in:\n            if self.num_dim == 1 and Xstar.shape[0] == 1:\n                Xstar = Xstar.T\n            if Xstar.shape[1] != self.num_dim:\n                raise ValueError(\n                    \"Second dimension of Xstar must be equal to self.num_dim! \"\n                    \"Shape of Xstar given is %s, num_dim is %d.\"\n                    % (Xstar.shape, self.num_dim)\n                )\n            \n            # Process T:\n            if output_transform is not None:\n                output_transform = scipy.atleast_2d(scipy.asarray(output_transform, dtype=float))\n                if output_transform.ndim != 2:\n                    raise ValueError(\n                        \"output_transform must have exactly 2 dimensions! Shape \"\n                        \"of output_transform given is %s.\"\n                        % (output_transform.shape,)\n                    )\n                if output_transform.shape[1] != Xstar.shape[0]:\n                    raise ValueError(\n                        \"output_transform must have the same number of columns \"\n                        \"the number of rows in Xstar! Shape of output_transform \"\n                        \"given is %s, shape of Xstar is %s.\"\n                        % (output_transform.shape, Xstar.shape,)\n                    )\n            \n            # Process n:\n            try:\n                iter(n)\n            except TypeError:\n                n = n * scipy.ones(Xstar.shape, dtype=int)\n            else:\n                n = scipy.atleast_2d(scipy.asarray(n, dtype=int))\n                if self.num_dim == 1 and n.shape[0] == 1:\n                    n = n.T\n                if n.shape != Xstar.shape:\n                    raise ValueError(\n                        \"When using array-like n, shape must match shape of Xstar! \"\n                        \"Shape of n given is %s, shape of Xstar given is %s.\"\n                        % (n.shape, Xstar.shape)\n                    )\n            if (n < 0).any():\n                raise ValueError(\"All elements of n must be non-negative integers!\")\n            \n            self.compute_K_L_alpha_ll()\n            Kstar = self.compute_Kij(self.X, Xstar, self.n, n)\n            if noise:\n                Kstar = Kstar + self.compute_Kij(self.X, Xstar, self.n, n, noise=True)\n            if self.T is not None:\n                Kstar = self.T.dot(Kstar)\n            mean = Kstar.T.dot(self.alpha)\n            if self.mu is not None:\n                mean_func = scipy.atleast_2d(self.mu(Xstar, n)).T\n                mean += mean_func\n            if output_transform is not None:\n                mean = output_transform.dot(mean)\n                if return_mean_func and self.mu is not None:\n                    mean_func = output_transform.dot(mean_func)\n            mean = mean.ravel()\n            if return_mean_func and self.mu is not None:\n                mean_func = mean_func.ravel()\n            if return_std or return_cov or full_output or full_MC:\n                v = scipy.linalg.solve_triangular(self.L, Kstar, lower=True)\n                Kstarstar = self.compute_Kij(Xstar, None, n, None)\n                if noise:\n                    Kstarstar = Kstarstar + self.compute_Kij(Xstar, None, n, None, noise=True)\n                covariance = Kstarstar - v.T.dot(v)\n                if output_transform is not None:\n                    covariance = output_transform.dot(covariance.dot(output_transform.T))\n                if return_samples or full_MC:\n                    samps = self.draw_sample(\n                        Xstar, n=n, num_samp=num_samples, mean=mean,\n                        cov=covariance, **samp_kwargs\n                    )\n                    if rejection_func:\n                        good_samps = []\n                        for samp in samps.T:\n                            if rejection_func(samp):\n                                good_samps.append(samp)\n                        if len(good_samps) == 0:\n                            raise ValueError(\"Did not get any good samples!\")\n                        samps = scipy.asarray(good_samps, dtype=float).T\n                    if full_MC:\n                        mean = scipy.mean(samps, axis=1)\n                        covariance = scipy.cov(samps, rowvar=1, ddof=ddof)\n                std = scipy.sqrt(scipy.diagonal(covariance))\n                if full_output:\n                    out = {\n                        'mean': mean,\n                        'std': std,\n                        'cov': covariance\n                    }\n                    if return_samples or full_MC:\n                        out['samp'] = samps\n                    if return_mean_func and self.mu is not None:\n                        out['mean_func'] = mean_func\n                        out['cov_func'] = scipy.zeros(\n                            (len(mean_func), len(mean_func)),\n                            dtype=float\n                        )\n                        out['std_func'] = scipy.zeros_like(mean_func)\n                        out['mean_without_func'] = mean - mean_func\n                        out['cov_without_func'] = covariance\n                        out['std_without_func'] = std\n                    return out\n                else:\n                    if return_cov:\n                        return (mean, covariance)\n                    elif return_std:\n                        return (mean, std)\n                    else:\n                        return mean\n            else:\n                return mean"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the Gaussian process at the current hyperparameters.", "response": "def plot(self, X=None, n=0, ax=None, envelopes=[1, 3], base_alpha=0.375,\n             return_prediction=False, return_std=True, full_output=False,\n             plot_kwargs={}, **kwargs):\n        \"\"\"Plots the Gaussian process using the current hyperparameters. Only for num_dim <= 2.\n        \n        Parameters\n        ----------\n        X : array-like (`M`,) or (`M`, `num_dim`), optional\n            The values to evaluate the Gaussian process at. If None, then 100\n            points between the minimum and maximum of the data's X are used for\n            a univariate Gaussian process and a 50x50 grid is used for a\n            bivariate Gaussian process. Default is None (use 100 points between\n            min and max).\n        n : int or list, optional\n            The order of derivative to compute. For num_dim=1, this must be an\n            int. For num_dim=2, this must be a list of ints of length 2.\n            Default is 0 (don't take derivative).\n        ax : axis instance, optional\n            Axis to plot the result on. If no axis is passed, one is created.\n            If the string 'gca' is passed, the current axis (from plt.gca())\n            is used. If X_dim = 2, the axis must be 3d.\n        envelopes: list of float, optional\n            +/-n*sigma envelopes to plot. Default is [1, 3].\n        base_alpha : float, optional\n            Alpha value to use for +/-1*sigma envelope. All other envelopes `env`\n            are drawn with `base_alpha`/`env`. Default is 0.375.\n        return_prediction : bool, optional\n            If True, the predicted values are also returned. Default is False.\n        return_std : bool, optional\n            If True, the standard deviation is computed and returned along with\n            the mean when `return_prediction` is True. Default is True.\n        full_output : bool, optional\n            Set to True to return the full outputs in a dictionary with keys:\n            \n                ==== ==========================================================================\n                mean mean of GP at requested points\n                std  standard deviation of GP at requested points\n                cov  covariance matrix for values of GP at requested points\n                samp random samples of GP at requested points (only if `return_sample` is True)\n                ==== ==========================================================================\n            \n        plot_kwargs : dict, optional\n            The entries in this dictionary are passed as kwargs to the plotting\n            command used to plot the mean. Use this to, for instance, change the\n            color, line width and line style.\n        **kwargs : extra arguments for predict, optional\n            Extra arguments that are passed to :py:meth:`predict`.\n        \n        Returns\n        -------\n        ax : axis instance\n            The axis instance used.\n        mean : :py:class:`Array`, (`M`,)\n            Predicted GP mean. Only returned if `return_prediction` is True and `full_output` is False.\n        std : :py:class:`Array`, (`M`,)\n            Predicted standard deviation, only returned if `return_prediction` and `return_std` are True and `full_output` is False.\n        full_output : dict\n            Dictionary with fields for mean, std, cov and possibly random samples. Only returned if `return_prediction` and `full_output` are True.\n        \"\"\"\n        if self.num_dim > 2:\n            raise ValueError(\"Plotting is not supported for num_dim > 2!\")\n        \n        if self.num_dim == 1:\n            if X is None:\n                X = scipy.linspace(self.X.min(), self.X.max(), 100)\n        elif self.num_dim == 2:\n            if X is None:\n                x1 = scipy.linspace(self.X[:, 0].min(), self.X[:, 0].max(), 50)\n                x2 = scipy.linspace(self.X[:, 1].min(), self.X[:, 1].max(), 50)\n                X1, X2 = scipy.meshgrid(x1, x2)\n                X1 = X1.flatten()\n                X2 = X2.flatten()\n                X = scipy.hstack((scipy.atleast_2d(X1).T, scipy.atleast_2d(X2).T))\n            else:\n                X1 = scipy.asarray(X[:, 0]).flatten()\n                X2 = scipy.asarray(X[:, 1]).flatten()\n        \n        if envelopes or (return_prediction and (return_std or full_output)):\n            out = self.predict(X, n=n, full_output=True, **kwargs)\n            mean = out['mean']\n            std = out['std']\n        else:\n            mean = self.predict(X, n=n, return_std=False, **kwargs)\n            std = None\n        \n        if self.num_dim == 1:\n            univariate_envelope_plot(\n                X,\n                mean,\n                std,\n                ax=ax,\n                base_alpha=base_alpha,\n                envelopes=envelopes,\n                **plot_kwargs\n            )\n        elif self.num_dim == 2:\n            if ax is None:\n                f = plt.figure()\n                ax = f.add_subplot(111, projection='3d')\n            elif ax == 'gca':\n                ax = plt.gca()\n            if 'linewidths' not in kwargs:\n                kwargs['linewidths'] = 0\n            s = ax.plot_trisurf(X1, X2, mean, **plot_kwargs)\n            for i in envelopes:\n                kwargs.pop('alpha', base_alpha)\n                ax.plot_trisurf(X1, X2, mean - std, alpha=base_alpha / i, **kwargs)\n                ax.plot_trisurf(X1, X2, mean + std, alpha=base_alpha / i, **kwargs)\n        \n        if return_prediction:\n            if full_output:\n                return (ax, out)\n            elif return_std:\n                return (ax, out['mean'], out['std'])\n            else:\n                return (ax, out['mean'])\n        else:\n            return ax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw a sample from the GP given the current set of hyperparameters Xstar and n.", "response": "def draw_sample(self, Xstar, n=0, num_samp=1, rand_vars=None,\n                    rand_type='standard normal', diag_factor=1e3,\n                    method='cholesky', num_eig=None, mean=None, cov=None,\n                    modify_sign=None, **kwargs):\n        \"\"\"Draw a sample evaluated at the given points `Xstar`.\n        \n        Note that this function draws samples from the GP given the current\n        values for the hyperparameters (which may be in a nonsense state if you\n        just created the instance or called a method that performs MCMC sampling).\n        If you want to draw random samples from MCMC output, use the\n        `return_samples` and `full_output` keywords to :py:meth:`predict`.\n        \n        Parameters\n        ----------\n        Xstar : array, (`M`, `D`)\n            `M` test input values of dimension `D`.\n        n : array, (`M`, `D`) or scalar, non-negative int, optional\n            Derivative order to evaluate at. Default is 0 (evaluate value).\n        noise : bool, optional\n            Whether or not to include the noise components of the kernel in the\n            sample. Default is False (no noise in samples).\n        num_samp : Positive int, optional\n            Number of samples to draw. Default is 1. Cannot be used in\n            conjunction with `rand_vars`: If you pass both `num_samp` and\n            `rand_vars`, `num_samp` will be silently ignored.\n        rand_vars : array, (`M`, `P`), optional\n            Vector of random variables :math:`u` to use in constructing the\n            sample :math:`y_* = f_* + Lu`, where :math:`K=LL^T`. If None,\n            values will be produced using\n            :py:func:`numpy.random.multivariate_normal`. This allows you to use\n            pseudo/quasi random numbers generated by an external routine. Note\n            that, when `method` is 'eig', the eigenvalues are in *ascending*\n            order.\n            Default is None (use :py:func:`multivariate_normal` directly).\n        rand_type : {'standard normal', 'uniform'}, optional\n            Type of distribution the inputs are given with.\n            \n                * 'standard normal': Standard (`mu` = 0, `sigma` = 1) normal\n                  distribution (this is the default)\n                * 'uniform': Uniform distribution on [0, 1). In this case\n                  the required Gaussian variables are produced with inversion.\n                  \n        diag_factor : float, optional\n            Number (times machine epsilon) added to the diagonal of the\n            covariance matrix prior to computing its Cholesky decomposition.\n            This is necessary as sometimes the decomposition will fail because,\n            to machine precision, the matrix appears to not be positive definite.\n            If you are getting errors from :py:func:`scipy.linalg.cholesky`, try\n            increasing this an order of magnitude at a time. This parameter only\n            has an effect when using rand_vars. Default value is 1e3. \n        method : {'cholesky', 'eig'}, optional\n            Method to use for constructing the matrix square root. Default is\n            'cholesky' (use lower-triangular Cholesky decomposition).\n            \n                * 'cholesky': Perform Cholesky decomposition on the covariance\n                  matrix: :math:`K=LL^T`, use :math:`L` as the matrix square\n                  root.\n                * 'eig': Perform an eigenvalue decomposition on the covariance\n                  matrix: :math:`K=Q \\\\Lambda Q^{-1}`, use :math:`Q\\\\Lambda^{1/2}`\n                  as the matrix square root.\n        \n        num_eig : int or None, optional\n            Number of eigenvalues to compute. Can range from 1 to `M` (the\n            number of test points). If it is None, then all eigenvalues are\n            computed. Default is None (compute all eigenvalues). This keyword\n            only has an effect if `method` is 'eig'.\n        mean : array, (`M`,), optional\n            If you have pre-computed the mean and covariance matrix, then you\n            can simply pass them in with the `mean` and `cov` keywords to save\n            on having to call :py:meth:`predict`.\n        cov : array, (`M`, `M`), optional\n            If you have pre-computed the mean and covariance matrix, then you\n            can simply pass them in with the `mean` and `cov` keywords to save\n            on having to call :py:meth:`predict`.\n        modify_sign : {None, 'left value', 'right value', 'left slope', 'right slope', 'left concavity', 'right concavity'}, optional\n            If None (the default), the eigenvectors as returned by\n            :py:func:`scipy.linalg.eigh` are used without modification. To\n            modify the sign of the eigenvectors (necessary for some advanced use\n            cases), set this kwarg to one of the following:\n            \n                * 'left value': forces the first value of each eigenvector to be\n                  positive.\n                * 'right value': forces the last value of each eigenvector to be\n                  positive.\n                * 'left slope': forces the slope to be positive at the start of\n                  each eigenvector.\n                * 'right slope': forces the slope to be positive at the end of\n                  each eigenvector.\n                * 'left concavity': forces the second derivative to be positive\n                  at the start of each eigenvector.\n                * 'right concavity': forces the second derivative to be positive\n                  at the end of each eigenvector.\n        \n        **kwargs : optional kwargs\n            All extra keyword arguments are passed to :py:meth:`predict` when\n            evaluating the mean and covariance matrix of the GP.\n        \n        Returns\n        -------\n        samples : :py:class:`Array` (`M`, `P`) or (`M`, `num_samp`)\n            Samples evaluated at the `M` points.\n        \n        Raises\n        ------\n        ValueError\n            If rand_type or method is invalid.\n        \"\"\"\n        # All of the input processing for Xstar and n will be done in here:\n        if mean is None or cov is None:\n            out = self.predict(Xstar, n=n, full_output=True, **kwargs)\n            mean = out['mean']\n            cov = out['cov']\n        if rand_vars is None and method != 'eig':\n            try:\n                return numpy.random.multivariate_normal(mean, cov, num_samp).T\n            except numpy.linalg.LinAlgError as e:\n                if self.verbose:\n                    warnings.warn(\n                        \"Failure when drawing from MVN! Falling back on eig. \"\n                        \"Exception was:\\n%s\"\n                        % (e,),\n                        RuntimeWarning\n                    )\n                method = 'eig'\n        \n        if num_eig is None or num_eig > len(mean):\n            num_eig = len(mean)\n        elif num_eig < 1:\n            num_eig = 1\n        if rand_vars is None:\n            rand_vars = numpy.random.standard_normal((num_eig, num_samp))\n        valid_types = ('standard normal', 'uniform')\n        if rand_type not in valid_types:\n            raise ValueError(\n                \"rand_type %s not recognized! Valid options are: %s.\"\n                % (rand_type, valid_types,)\n            )\n        if rand_type == 'uniform':\n            rand_vars = scipy.stats.norm.ppf(rand_vars)\n        \n        if method == 'cholesky':\n            L = scipy.linalg.cholesky(\n                cov + diag_factor * sys.float_info.epsilon * scipy.eye(cov.shape[0]),\n                lower=True,\n                check_finite=False\n            )\n        elif method == 'eig':\n            # TODO: Add support for specifying cutoff eigenvalue!\n            # Not technically lower triangular, but we'll keep the name L:\n            eig, Q = scipy.linalg.eigh(\n                cov + diag_factor * sys.float_info.epsilon * scipy.eye(cov.shape[0]),\n                eigvals=(len(mean) - 1 - (num_eig - 1), len(mean) - 1)\n            )\n            if modify_sign is not None:\n                if modify_sign == 'left value':\n                    modify_mask = (Q[0, :] < 0.0)\n                elif modify_sign == 'right value':\n                    modify_mask = (Q[-1, :] < 0.0)\n                elif modify_sign == 'left slope':\n                    modify_mask = ((Q[1, :] - Q[0, :]) < 0.0)\n                elif modify_sign == 'right slope':\n                    modify_mask = ((Q[-1, :] - Q[-2, :]) < 0.0)\n                elif modify_sign == 'left concavity':\n                    modify_mask = ((Q[2, :] - 2 * Q[1, :] + Q[0, :]) < 0.0)\n                elif modify_sign == 'right concavity':\n                    modify_mask = ((Q[-1, :] - 2 * Q[-2, :] + Q[-3, :]) < 0.0)\n                else:\n                    raise ValueError(\n                        \"modify_sign %s not recognized!\" % (modify_sign,)\n                    )\n                Q[:, modify_mask] *= -1.0\n            Lam_1_2 = scipy.diag(scipy.sqrt(eig))\n            L = Q.dot(Lam_1_2)\n        else:\n            raise ValueError(\"method %s not recognized!\" % (method,))\n        return scipy.atleast_2d(mean).T + L.dot(rand_vars[:num_eig, :])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_K_L_alpha_ll(self):\n        if not self.K_up_to_date:\n            y = self.y\n            err_y = self.err_y\n            self.K = self.compute_Kij(self.X, None, self.n, None, noise=False)\n            # If the noise kernel is meant to be strictly diagonal, it should\n            # yield a diagonal noise_K:\n            if isinstance(self.noise_k, ZeroKernel):\n                self.noise_K = scipy.zeros((self.X.shape[0], self.X.shape[0]))\n            elif isinstance(self.noise_k, DiagonalNoiseKernel):\n                self.noise_K = self.noise_k.params[0]**2.0 * scipy.eye(self.X.shape[0])\n            else:\n                self.noise_K = self.compute_Kij(self.X, None, self.n, None, noise=True)\n            \n            K = self.K\n            noise_K = self.noise_K\n            if self.T is not None:\n                KnK = self.T.dot(K + noise_K).dot(self.T.T)\n            else:\n                KnK = K + noise_K\n            K_tot = (\n                KnK +\n                scipy.diag(err_y**2.0) +\n                self.diag_factor * sys.float_info.epsilon * scipy.eye(len(y))\n            )\n            self.L = scipy.linalg.cholesky(K_tot, lower=True)\n            # Need to make the mean-subtracted y that appears in the expression\n            # for alpha:\n            if self.mu is not None:\n                mu_alph = self.mu(self.X, self.n)\n                if self.T is not None:\n                    mu_alph = self.T.dot(mu_alph)\n                y_alph = self.y - mu_alph\n            else:\n                y_alph = self.y\n            self.alpha = scipy.linalg.cho_solve((self.L, True), scipy.atleast_2d(y_alph).T)\n            self.ll = (\n                -0.5 * scipy.atleast_2d(y_alph).dot(self.alpha) -\n                scipy.log(scipy.diag(self.L)).sum() - \n                0.5 * len(y) * scipy.log(2.0 * scipy.pi)\n            )[0, 0]\n            # Apply hyperpriors:\n            self.ll += self.hyperprior(self.params)\n            \n            if self.use_hyper_deriv:\n                warnings.warn(\"Use of hyperparameter derivatives is experimental!\")\n                \n                # Only compute for the free parameters, since that is what we\n                # want to optimize:\n                self.ll_deriv = scipy.zeros(len(self.free_params))\n                # Combine the kernel and noise kernel so we only need one loop:\n                if isinstance(self.noise_k, ZeroKernel):\n                    knk = self.k\n                elif isinstance(self.noise_k, DiagonalNoiseKernel):\n                    knk = self.k\n                    # Handle DiagonalNoiseKernel specially:\n                    if not self.noise_k.fixed_params[0]:\n                        dK_dtheta_i = 2.0 * self.noise_k.params[0] * scipy.eye(len(y))\n                        self.ll_deriv[len(self.k.free_params)] = 0.5 * (\n                            self.alpha.T.dot(dK_dtheta_i.dot(self.alpha)) -\n                            scipy.trace(scipy.linalg.cho_solve((self.L, True), dK_dtheta_i))\n                        )\n                else:\n                    knk = self.k + self.noise_k\n                \n                # Get the indices of the free params in knk.params:\n                free_param_idxs = scipy.arange(0, len(knk.params), dtype=int)[~knk.fixed_params]\n                # Handle the kernel and noise kernel:\n                for i, pi in enumerate(free_param_idxs):\n                    dK_dtheta_i = self.compute_Kij(\n                        self.X, None, self.n, None, k=knk, hyper_deriv=pi\n                    )\n                    if self.T is not None:\n                        dK_dtheta_i = self.T.dot(dK_dtheta_i).dot(self.T.T)\n                    self.ll_deriv[i] = 0.5 * (\n                        self.alpha.T.dot(dK_dtheta_i.dot(self.alpha)) -\n                        scipy.trace(scipy.linalg.cho_solve((self.L, True), dK_dtheta_i))\n                    )\n                \n                # Handle the mean function:\n                if self.mu is not None:\n                    # Get the indices of the free params in self.mu.params:\n                    free_param_idxs = scipy.arange(0, len(self.mu.params), dtype=int)[~self.mu.fixed_params]\n                    for i, pi in enumerate(free_param_idxs):\n                        dmu_dtheta_i = scipy.atleast_2d(self.mu(self.X, self.n, hyper_deriv=pi)).T\n                        if self.T is not None:\n                            dmu_dtheta_i = self.T.dot(dmu_dtheta_i)\n                        self.ll_deriv[i + len(knk.free_params)] = dmu_dtheta_i.T.dot(self.alpha)\n                \n                # Handle the hyperprior:\n                # Get the indices of the free params in self.params:\n                free_param_idxs = scipy.arange(0, len(self.params), dtype=int)[~self.fixed_params]\n                for i, pi in enumerate(free_param_idxs):\n                    self.ll_deriv[i] += self.hyperprior(self.params, hyper_deriv=pi)\n            \n            self.K_up_to_date = True", "response": "r Compute K L alpha and log - likelihood according to the first part of Algorithm 2. 1 in R&W."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_ll_matrix(self, bounds, num_pts):\n        present_free_params = self.free_params[:]\n        bounds = scipy.atleast_2d(scipy.asarray(bounds, dtype=float))\n        if bounds.shape[1] != 2:\n            raise ValueError(\"Argument bounds must have shape (n, 2)!\")\n        # If bounds is a single tuple, repeat it for each free parameter:\n        if bounds.shape[0] == 1:\n            bounds = scipy.tile(bounds, (len(present_free_params), 1))\n        # If num_pts is a single value, use it for all of the parameters:\n        try:\n            iter(num_pts)\n        except TypeError:\n            num_pts = num_pts * scipy.ones(bounds.shape[0], dtype=int)\n        else:\n            num_pts = scipy.asarray(num_pts, dtype=int)\n            if len(num_pts) != len(present_free_params):\n                raise ValueError(\n                    \"Length of num_pts must match the number of free parameters!\"\n                )\n        \n        # Form arrays to evaluate parameters over:\n        param_vals = []\n        for k in xrange(0, len(present_free_params)):\n            param_vals.append(scipy.linspace(bounds[k, 0], bounds[k, 1], num_pts[k]))\n        ll_vals = self._compute_ll_matrix(0, param_vals, num_pts)\n        \n        # Reset the parameters to what they were before:\n        self.update_hyperparameters(scipy.asarray(present_free_params, dtype=float))\n        \n        return (ll_vals, param_vals)", "response": "Compute the log likelihood matrix over the free parameter space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_hyperparameter_posterior(self, nwalkers=200, nsamp=500, burn=0,\n                                        thin=1, num_proc=None, sampler=None,\n                                        plot_posterior=False,\n                                        plot_chains=False, sampler_type='ensemble',\n                                        ntemps=20, sampler_a=2.0, **plot_kwargs):\n        \"\"\"Produce samples from the posterior for the hyperparameters using MCMC.\n        \n        Returns the sampler created, because storing it stops the GP from being\n        pickleable. To add more samples to a previous sampler, pass the sampler\n        instance in the `sampler` keyword.\n        \n        Parameters\n        ----------\n        nwalkers : int, optional\n            The number of walkers to use in the sampler. Should be on the order\n            of several hundred. Default is 200.\n        nsamp : int, optional\n            Number of samples (per walker) to take. Default is 500.\n        burn : int, optional\n            This keyword only has an effect on the corner plot produced when\n            `plot_posterior` is True and the flattened chain plot produced\n            when `plot_chains` is True. To perform computations with burn-in,\n            see :py:meth:`compute_from_MCMC`. The number of samples to discard\n            at the beginning of the chain. Default is 0.\n        thin : int, optional\n            This keyword only has an effect on the corner plot produced when\n            `plot_posterior` is True and the flattened chain plot produced\n            when `plot_chains` is True. To perform computations with thinning,\n            see :py:meth:`compute_from_MCMC`. Every `thin`-th sample is kept.\n            Default is 1.\n        num_proc : int or None, optional\n            Number of processors to use. If None, all available processors are\n            used. Default is None (use all available processors).\n        sampler : :py:class:`Sampler` instance\n            The sampler to use. If the sampler already has samples, the most\n            recent sample will be used as the starting point. Otherwise a\n            random sample from the hyperprior will be used.\n        plot_posterior : bool, optional\n            If True, a corner plot of the posterior for the hyperparameters\n            will be generated. Default is False.\n        plot_chains : bool, optional\n            If True, a plot showing the history and autocorrelation of the\n            chains will be produced.\n        sampler_type : str, optional\n            The type of sampler to use. Valid options are \"ensemble\" (affine-\n            invariant ensemble sampler) and \"pt\" (parallel-tempered ensemble\n            sampler).\n        ntemps : int, optional\n            Number of temperatures to use with the parallel-tempered ensemble\n            sampler.\n        sampler_a : float, optional\n            Scale of the proposal distribution.\n        plot_kwargs : additional keywords, optional\n            Extra arguments to pass to :py:func:`~gptools.utils.plot_sampler`.\n        \"\"\"\n        if num_proc is None:\n            num_proc = multiprocessing.cpu_count()\n        # Needed for emcee to do it right:\n        if num_proc == 0:\n            num_proc = 1\n        ndim = len(self.free_params)\n        if sampler is None:\n            if sampler_type == 'ensemble':\n                sampler = emcee.EnsembleSampler(\n                    nwalkers,\n                    ndim,\n                    _ComputeLnProbEval(self),\n                    threads=num_proc,\n                    a=sampler_a\n                )\n            elif sampler_type == 'pt':\n                # TODO: Finish this!\n                raise NotImplementedError(\"PTSampler not done yet!\")\n                sampler = emcee.PTSampler(\n                    ntemps,\n                    nwalkers,\n                    ndim,\n                    logl,\n                    logp\n                )\n            else:\n                raise NotImplementedError(\n                    \"Sampler type %s not supported!\" % (sampler_type,)\n                )\n        else:\n            sampler.a = sampler_a\n        if sampler.chain.size == 0:\n            theta0 = self.hyperprior.random_draw(size=nwalkers).T\n            theta0 = theta0[:, ~self.fixed_params]\n        else:\n            # Start from the stopping point of the previous chain:\n            theta0 = sampler.chain[:, -1, :]\n        \n        sampler.run_mcmc(theta0, nsamp)\n        if plot_posterior or plot_chains:\n            flat_trace = sampler.chain[:, burn::thin, :]\n            flat_trace = flat_trace.reshape((-1, flat_trace.shape[2]))\n        \n        if plot_posterior and plot_chains:\n            plot_sampler(\n                sampler,\n                labels=['$%s$' % (l,) for l in self.free_param_names],\n                burn=burn,\n                **plot_kwargs\n            )\n        else:\n            if plot_posterior:\n                triangle.corner(\n                    flat_trace,\n                    plot_datapoints=False,\n                    labels=['$%s$' % (l,) for l in self.free_param_names]\n                )\n            if plot_chains:\n                f = plt.figure()\n                for k in xrange(0, ndim):\n                    # a = f.add_subplot(3, ndim, k + 1)\n                    # a.acorr(\n                    #     sampler.flatchain[:, k],\n                    #     maxlags=100,\n                    #     detrend=plt.mlab.detrend_mean\n                    # )\n                    # a.set_xlabel('lag')\n                    # a.set_title('$%s$ autocorrelation' % (self.free_param_names[k],))\n                    a = f.add_subplot(ndim, 1, 0 * ndim + k + 1)\n                    for chain in sampler.chain[:, :, k]:\n                        a.plot(chain)\n                    a.set_xlabel('sample')\n                    a.set_ylabel('$%s$' % (self.free_param_names[k],))\n                    a.set_title('$%s$ all chains' % (self.free_param_names[k],))\n                    a.axvline(burn, color='r', linewidth=3, ls='--')\n                    # a = f.add_subplot(2, ndim, 1 * ndim + k + 1)\n                    # a.plot(flat_trace[:, k])\n                    # a.set_xlabel('sample')\n                    # a.set_ylabel('$%s$' % (self.free_param_names[k],))\n                    # a.set_title('$%s$ flattened, burned and thinned chain' % (self.free_param_names[k],))\n        \n        # Print a summary of the sampler:\n        print(\"MCMC parameter summary:\")\n        print(\"param\\tmean\\t95% posterior interval\")\n        mean, ci_l, ci_u = summarize_sampler(sampler, burn=burn)\n        names = self.free_param_names[:]\n        for n, m, l, u in zip(names, mean, ci_l, ci_u):\n            print(\"%s\\t%4.4g\\t[%4.4g, %4.4g]\" % (n, m, l, u))\n        \n        return sampler", "response": "Produce samples from the posterior for the hyperparameter model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_from_MCMC(self, X, n=0, return_mean=True, return_std=True,\n                          return_cov=False, return_samples=False,\n                          return_mean_func=False, num_samples=1, noise=False,\n                          samp_kwargs={}, sampler=None, flat_trace=None, burn=0,\n                          thin=1, **kwargs):\n        \"\"\"Compute desired quantities from MCMC samples of the hyperparameter posterior.\n        \n        The return will be a list with a number of rows equal to the number of\n        hyperparameter samples. The columns depend on the state of the boolean\n        flags, but will be some subset of (mean, stddev, cov, samples), in that\n        order. Samples will be the raw output of :py:meth:`draw_sample`, so you\n        will need to remember to convert to an array and flatten if you want to\n        work with a single sample.\n        \n        Parameters\n        ----------\n        X : array-like (`M`,) or (`M`, `num_dim`)\n            The values to evaluate the Gaussian process at.\n        n : non-negative int or list, optional\n            The order of derivative to compute. For num_dim=1, this must be an\n            int. For num_dim=2, this must be a list of ints of length 2.\n            Default is 0 (don't take derivative).\n        return_mean : bool, optional\n            If True, the mean will be computed at each hyperparameter sample.\n            Default is True (compute mean).\n        return_std : bool, optional\n            If True, the standard deviation will be computed at each\n            hyperparameter sample. Default is True (compute stddev).\n        return_cov : bool, optional\n            If True, the covariance matrix will be computed at each\n            hyperparameter sample. Default is True (compute stddev).\n        return_samples : bool, optional\n            If True, random sample(s) will be computed at each hyperparameter\n            sample. Default is False (do not compute samples).\n        num_samples : int, optional\n            Compute this many samples if `return_sample` is True. Default is 1.\n        noise : bool, optional\n            If True, noise is included in the predictions and samples. Default\n            is False (do not include noise).\n        samp_kwargs : dict, optional\n            If `return_sample` is True, the contents of this dictionary will be\n            passed as kwargs to :py:meth:`draw_sample`.\n        sampler : :py:class:`Sampler` instance or None, optional\n            :py:class:`Sampler` instance that has already been run to the extent\n            desired on the hyperparameter posterior. If None, a new sampler will\n            be created with :py:meth:`sample_hyperparameter_posterior`. In this\n            case, all extra kwargs will be passed on, allowing you to set the\n            number of samples, etc. Default is None (create sampler).\n        flat_trace : array-like (`nsamp`, `ndim`) or None, optional\n            Flattened trace with samples of the free hyperparameters. If present,\n            overrides `sampler`. This allows you to use a sampler other than the\n            ones from :py:mod:`emcee`, or to specify arbitrary values you wish\n            to evaluate the curve at. Note that this WILL be thinned and burned\n            according to the following two kwargs. \"Flat\" refers to the fact\n            that you must have combined all chains into a single one. Default is\n            None (use `sampler`).\n        burn : int, optional\n            The number of samples to discard at the beginning of the chain.\n            Default is 0.\n        thin : int, optional\n            Every `thin`-th sample is kept. Default is 1.\n        num_proc : int, optional\n            The number of processors to use for evaluation. This is used both\n            when calling the sampler and when evaluating the Gaussian process.\n            If None, the number of available processors will be used. If zero,\n            evaluation will proceed in parallel. Default is to use all available\n            processors.\n        **kwargs : extra optional kwargs\n            All additional kwargs are passed to\n            :py:meth:`sample_hyperparameter_posterior`.\n        \n        Returns\n        -------\n        out : dict\n            A dictionary having some or all of the fields 'mean', 'std', 'cov'\n            and 'samp'. Each entry is a list of array-like. The length of this\n            list is equal to the number of hyperparameter samples used, and the\n            entries have the following shapes:\n            \n                ==== ====================\n                mean (`M`,)\n                std  (`M`,)\n                cov  (`M`, `M`)\n                samp (`M`, `num_samples`)\n                ==== ====================\n        \"\"\"\n        output_transform = kwargs.pop('output_transform', None)\n        if flat_trace is None:\n            if sampler is None:\n                sampler = self.sample_hyperparameter_posterior(burn=burn, **kwargs)\n                # If we create the sampler, we need to make sure we clean up its pool:\n                try:\n                    sampler.pool.close()\n                except AttributeError:\n                    # This will occur if only one thread is used.\n                    pass\n                \n            flat_trace = sampler.chain[:, burn::thin, :]\n            flat_trace = flat_trace.reshape((-1, flat_trace.shape[2]))\n        else:\n            flat_trace = flat_trace[burn::thin, :]\n        \n        num_proc = kwargs.get('num_proc', multiprocessing.cpu_count())\n        \n        if num_proc > 1:\n            pool = InterruptiblePool(processes=num_proc)\n            map_fun = pool.map\n        else:\n            map_fun = map\n        try:\n            res = map_fun(\n                _ComputeGPWrapper(\n                    self,\n                    X,\n                    n,\n                    return_mean,\n                    return_std,\n                    return_cov,\n                    return_samples,\n                    return_mean_func,\n                    num_samples,\n                    noise,\n                    samp_kwargs,\n                    output_transform\n                ),\n                flat_trace\n            )\n        finally:\n            if num_proc > 1:\n                pool.close()\n        out = dict()\n        if return_mean:\n            out['mean'] = [r['mean'] for r in res if r is not None]\n        if return_std:\n            out['std'] = [r['std'] for r in res if r is not None]\n        if return_cov:\n            out['cov'] = [r['cov'] for r in res if r is not None]\n        if return_samples:\n            out['samp'] = [r['samp'] for r in res if r is not None]\n        if return_mean_func and self.mu is not None:\n            out['mean_func'] = [r['mean_func'] for r in res if r is not None]\n            out['cov_func'] = [r['cov_func'] for r in res if r is not None]\n            out['std_func'] = [r['std_func'] for r in res if r is not None]\n            \n            out['mean_without_func'] = [r['mean_without_func'] for r in res if r is not None]\n            out['cov_without_func'] = [r['cov_without_func'] for r in res if r is not None]\n            out['std_without_func'] = [r['std_without_func'] for r in res if r is not None]\n        return out", "response": "Compute the desired quantities of the hyperparameter posterior at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the desired quantities from the MCMC samples of the hyperparameter posterior.", "response": "def compute_l_from_MCMC(self, X, n=0, sampler=None, flat_trace=None, burn=0, thin=1, **kwargs):\n        \"\"\"Compute desired quantities from MCMC samples of the hyperparameter posterior.\n        \n        The return will be a list with a number of rows equal to the number of\n        hyperparameter samples. The columns will contain the covariance length\n        scale function.\n        \n        Parameters\n        ----------\n        X : array-like (`M`,) or (`M`, `num_dim`)\n            The values to evaluate the Gaussian process at.\n        n : non-negative int or list, optional\n            The order of derivative to compute. For num_dim=1, this must be an\n            int. For num_dim=2, this must be a list of ints of length 2.\n            Default is 0 (don't take derivative).\n        sampler : :py:class:`Sampler` instance or None, optional\n            :py:class:`Sampler` instance that has already been run to the extent\n            desired on the hyperparameter posterior. If None, a new sampler will\n            be created with :py:meth:`sample_hyperparameter_posterior`. In this\n            case, all extra kwargs will be passed on, allowing you to set the\n            number of samples, etc. Default is None (create sampler).\n        flat_trace : array-like (`nsamp`, `ndim`) or None, optional\n            Flattened trace with samples of the free hyperparameters. If present,\n            overrides `sampler`. This allows you to use a sampler other than the\n            ones from :py:mod:`emcee`, or to specify arbitrary values you wish\n            to evaluate the curve at. Note that this WILL be thinned and burned\n            according to the following two kwargs. \"Flat\" refers to the fact\n            that you must have combined all chains into a single one. Default is\n            None (use `sampler`).\n        burn : int, optional\n            The number of samples to discard at the beginning of the chain.\n            Default is 0.\n        thin : int, optional\n            Every `thin`-th sample is kept. Default is 1.\n        num_proc : int, optional\n            The number of processors to use for evaluation. This is used both\n            when calling the sampler and when evaluating the Gaussian process.\n            If None, the number of available processors will be used. If zero,\n            evaluation will proceed in parallel. Default is to use all available\n            processors.\n        **kwargs : extra optional kwargs\n            All additional kwargs are passed to\n            :py:meth:`sample_hyperparameter_posterior`.\n        \n        Returns\n        -------\n        out : array of float\n            Length scale function at the indicated points.\n        \"\"\"\n        if flat_trace is None:\n            if sampler is None:\n                sampler = self.sample_hyperparameter_posterior(burn=burn, **kwargs)\n                # If we create the sampler, we need to make sure we clean up\n                # its pool:\n                try:\n                    sampler.pool.close()\n                except AttributeError:\n                    # This will occur if only one thread is used.\n                    pass\n                \n            flat_trace = sampler.chain[:, burn::thin, :]\n            flat_trace = flat_trace.reshape((-1, flat_trace.shape[2]))\n        else:\n            flat_trace = flat_trace[burn::thin, :]\n        \n        num_proc = kwargs.get('num_proc', multiprocessing.cpu_count())\n        \n        if num_proc > 1:\n            pool = InterruptiblePool(processes=num_proc)\n            try:\n                res = pool.map(_ComputeLWrapper(self, X, n), flat_trace)\n            finally:\n                pool.close()\n        else:\n            res = map(_ComputeLWrapper(self, X, n), flat_trace)\n        \n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_MCMC(self, X, ddof=1, full_MC=False, rejection_func=None, **kwargs):\n        return_std = kwargs.get('return_std', True)\n        return_cov = kwargs.get('return_cov', False)\n        if full_MC:\n            kwargs['return_mean'] = False\n            kwargs['return_std'] = False\n            kwargs['return_cov'] = False\n            kwargs['return_samples'] = True\n        else:\n            kwargs['return_mean'] = True\n        return_samples = kwargs.get('return_samples', True)\n        res = self.compute_from_MCMC(X, **kwargs)\n        \n        out = {}\n        \n        if return_samples:\n            samps = scipy.asarray(scipy.hstack(res['samp']))\n        \n        if full_MC:\n            if rejection_func:\n                good_samps = []\n                for samp in samps.T:\n                    if rejection_func(samp):\n                        good_samps.append(samp)\n                if len(good_samps) == 0:\n                    raise ValueError(\"Did not get any good samples!\")\n                samps = scipy.asarray(good_samps, dtype=float).T\n            mean = scipy.mean(samps, axis=1)\n            cov = scipy.cov(samps, rowvar=1, ddof=ddof)\n            std = scipy.sqrt(scipy.diagonal(cov))\n        else:\n            means = scipy.asarray(res['mean'])\n            mean = scipy.mean(means, axis=0)\n            \n            # TODO: Allow use of robust estimators!\n            if 'cov' in res:\n                covs = scipy.asarray(res['cov'])\n                cov = scipy.mean(covs, axis=0) + scipy.cov(means, rowvar=0, ddof=ddof)\n                std = scipy.sqrt(scipy.diagonal(cov))\n            elif 'std' in res:\n                vars_ = scipy.asarray(scipy.asarray(res['std']))**2\n                std = scipy.sqrt(scipy.mean(vars_, axis=0) +\n                                 scipy.var(means, axis=0, ddof=ddof))\n            if 'mean_func' in res:\n                mean_funcs = scipy.asarray(res['mean_func'])\n                cov_funcs = scipy.asarray(res['cov_func'])\n                mean_func = scipy.mean(mean_funcs, axis=0)\n                cov_func = scipy.mean(cov_funcs, axis=0) + scipy.cov(mean_funcs, rowvar=0, ddof=ddof)\n                std_func = scipy.sqrt(scipy.diagonal(cov_func))\n                \n                mean_without_funcs = scipy.asarray(res['mean_without_func'])\n                cov_without_funcs = scipy.asarray(res['cov_without_func'])\n                mean_without_func = scipy.mean(mean_without_funcs, axis=0)\n                cov_without_func = (\n                    scipy.mean(cov_without_funcs, axis=0) +\n                    scipy.cov(mean_without_funcs, rowvar=0, ddof=ddof)\n                )\n                std_without_func = scipy.sqrt(scipy.diagonal(cov_without_func))\n                \n                out['mean_func'] = mean_func\n                out['cov_func'] = cov_func\n                out['std_func'] = std_func\n                out['mean_without_func'] = mean_without_func\n                out['cov_without_func'] = cov_without_func\n                out['std_without_func'] = std_without_func\n        \n        out['mean'] = mean\n        if return_samples:\n            out['samp'] = samps\n        if return_std or return_cov:\n            out['std'] = std\n        if return_cov:\n            out['cov'] = cov\n        \n        return out", "response": "Make a prediction using MCMC samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild an argparse argument parser to parse the command line.", "response": "def build_parser():\n    \"Build an argparse argument parser to parse the command line.\"\n\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Coursera OAuth2 client CLI. This tool\n        helps users of the Coursera App Platform to programmatically access\n        Coursera APIs.\"\"\",\n        epilog=\"\"\"Please file bugs on github at:\n        https://github.com/coursera/courseraoauth2client/issues. If you\n        would like to contribute to this tool's development, check us out at:\n        https://github.com/coursera/courseraoauth2client\"\"\")\n    parser.add_argument('-c', '--config', help='the configuration file to use')\n    utils.add_logging_parser(parser)\n\n    # We support multiple subcommands. These subcommands have their own\n    # subparsers. Each subcommand should set a default value for the 'func'\n    # option. We then call the parsed 'func' function, and execution carries on\n    # from there.\n    subparsers = parser.add_subparsers()\n\n    commands.config.parser(subparsers)\n    commands.version.parser(subparsers)\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbooting up the command line tool", "response": "def main():\n    \"Boots up the command line tool\"\n    logging.captureWarnings(True)\n    args = build_parser().parse_args()\n    # Configure logging\n    args.setup_logging(args)\n    # Dispatch into the appropriate subcommand function.\n    try:\n        return args.func(args)\n    except SystemExit:\n        raise\n    except:\n        logging.exception('Problem when running command. Sorry!')\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding sponsors and packages links to the given root_menu.", "response": "def sponsor_menu(\n        root_menu, menu=\"sponsors\", label=_(\"Sponsors\"),\n        sponsors_item=_(\"Our sponsors\"),\n        packages_item=_(\"Sponsorship packages\")):\n    \"\"\"Add sponsor menu links.\"\"\"\n    root_menu.add_menu(menu, label, items=[])\n    for sponsor in (\n            Sponsor.objects.all()\n            .order_by('packages', 'order', 'id')\n            .prefetch_related('packages')):\n        symbols = sponsor.symbols()\n        if symbols:\n            item_name = u\"\u00bb %s %s\" % (sponsor.name, symbols)\n        else:\n            item_name = u\"\u00bb %s\" % (sponsor.name,)\n        with menu_logger(logger, \"sponsor %r\" % (sponsor.name,)):\n            root_menu.add_item(\n                item_name, sponsor.get_absolute_url(), menu=menu)\n\n    if sponsors_item:\n        with menu_logger(logger, \"sponsors page link\"):\n            root_menu.add_item(\n                sponsors_item, reverse(\"wafer_sponsors\"), menu)\n    if packages_item:\n        with menu_logger(logger, \"sponsorship package page link\"):\n            root_menu.add_item(\n                packages_item, reverse(\"wafer_sponsorship_packages\"), menu)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef objectatrib(instance, atrib):\n    '''\n    this filter is going to be useful to execute an object method or get an\n    object attribute dynamically. this method is going to take into account\n    the atrib param can contains underscores\n    '''\n    atrib = atrib.replace(\"__\", \".\")\n    atribs = []\n    atribs = atrib.split(\".\")\n\n    obj = instance\n    for atrib in atribs:\n        if type(obj) == dict:\n            result = obj[atrib]\n        else:\n            try:\n                result = getattr(obj, atrib)()\n            except Exception:\n                result = getattr(obj, atrib)\n\n        obj = result\n    return result", "response": "get an object attribute from an object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender the field as a widget.", "response": "def as_widget(self, widget=None, attrs=None, only_initial=False):\n        \"\"\"\n        Renders the field.\n        \"\"\"\n        attrs = attrs or {}\n        attrs.update(self.form.get_widget_attrs(self))\n        if hasattr(self.field, 'widget_css_classes'):\n            css_classes = self.field.widget_css_classes\n        else:\n            css_classes = getattr(self.form, 'widget_css_classes', None)\n        if css_classes:\n            attrs.update({'class': css_classes})\n        widget_classes = self.form.fields[self.name].widget.attrs.get('class', None)\n        if widget_classes:\n            if attrs.get('class', None):\n                attrs['class'] += ' ' + widget_classes\n            else:\n                attrs.update({'class': widget_classes})\n        return super(NgBoundField, self).as_widget(widget, attrs, only_initial)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts widgets to counterpart suitable to .", "response": "def convert_widgets(self):\n        \"\"\"\n        During form initialization, some widgets have to be replaced by a counterpart suitable to\n        be rendered the AngularJS way.\n        \"\"\"\n        for field in self.base_fields.values():\n            try:\n                new_widget = field.get_converted_widget()\n            except AttributeError:\n                pass\n            else:\n                if new_widget:\n                    field.widget = new_widget"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef epochdate(timestamp):\n    '''\n    Convet an epoch date to a tuple in format (\"yyyy-mm-dd\",\"hh:mm:ss\")\n    Example: \"1023456427\" -> (\"2002-06-07\",\"15:27:07\")\n\n    Parameters:\n    - `timestamp`: date in epoch format\n    '''\n\n    dt = datetime.fromtimestamp(float(timestamp)).timetuple()\n    fecha = \"{0:d}-{1:02d}-{2:02d}\".format(dt.tm_year, dt.tm_mon, dt.tm_mday)\n    hora = \"{0:02d}:{1:02d}:{2:02d}\".format(dt.tm_hour, dt.tm_min, dt.tm_sec)\n    return (fecha, hora)", "response": "Converts an epoch date to a tuple in format ( yyyy - mm - dd hh : mm : ss"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef model_inspect(obj):\n        '''\n        Analize itself looking for special information, right now it returns:\n        - Application name\n        - Model name\n        '''\n        # Prepare the information object\n        info = {}\n        if hasattr(obj, '_meta'):\n            info['verbose_name'] = getattr(obj._meta, 'verbose_name', None)\n        else:\n            info['verbose_name'] = None\n\n        # Get info from the object\n        if hasattr(obj, 'model') and obj.model:\n            model = obj.model\n        else:\n            model = obj.__class__\n\n        namesp = str(model)\n        namesp = namesp.replace(\"<class \", \"\").replace(\">\", \"\").replace(\"'\", \"\").split(\".\")\n\n        # Remember information\n        info['appname'] = namesp[-3]\n        info['modelname'] = namesp[-1]\n        info['model'] = model\n\n        # Return the info\n        return info", "response": "Analyizes the object and returns the special information about the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves get_FIELD_NAME_display from the field_name", "response": "def remove_getdisplay(field_name):\n    '''\n    for string 'get_FIELD_NAME_display' return 'FIELD_NAME'\n    '''\n    str_ini = 'get_'\n    str_end = '_display'\n    if str_ini == field_name[0:len(str_ini)] and str_end == field_name[(-1) * len(str_end):]:\n        field_name = field_name[len(str_ini):(-1) * len(str_end)]\n    return field_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates context with context processors from settings", "response": "def context_processors_update(context, request):\n    '''\n    Update context with context_processors from settings\n    Usage:\n        from codenerix.helpers import context_processors_update\n        context_processors_update(context, self.request)\n    '''\n    for template in settings.TEMPLATES:\n        for context_processor in template['OPTIONS']['context_processors']:\n            path = context_processor.split('.')\n            name = path.pop(-1)\n            processor = getattr(importlib.import_module('.'.join(path)), name, None)\n            if processor:\n                context.update(processor(request))\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, filename_in_zip, file_contents):\n        '''\n        Appends a file with name filename_in_zip and contents of\n        file_contents to the in-memory zip.\n        '''\n        # Set the file pointer to the end of the file\n        self.in_memory_zip.seek(-1, io.SEEK_END)\n\n        # Get a handle to the in-memory zip in append mode\n        zf = zipfile.ZipFile(self.in_memory_zip, \"a\", zipfile.ZIP_DEFLATED, False)\n\n        # Write the file to the in-memory zip\n        zf.writestr(filename_in_zip, file_contents)\n\n        # Mark the files as having been created on Windows so that\n        # Unix permissions are not inferred as 0000\n        for zfile in zf.filelist:\n            zfile.create_system = 0\n\n        # Close the ZipFile\n        zf.close()\n\n        # Rewind the file\n        self.in_memory_zip.seek(0)\n\n        return self", "response": "Append a file with name filename_in_zip and contents of file_contents to the in - memory zip."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the in - memory zip to a file.", "response": "def writetofile(self, filename):\n        '''Writes the in-memory zip to a file.'''\n        f = open(filename, \"w\")\n        f.write(self.read())\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sponsor_image_url(sponsor, name):\n    if sponsor.files.filter(name=name).exists():\n        # We avoid worrying about multiple matches by always\n        # returning the first one.\n        return sponsor.files.filter(name=name).first().item.url\n    return ''", "response": "Returns the corresponding url from the sponsors images"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sponsor_tagged_image(sponsor, tag):\n    if sponsor.files.filter(tag_name=tag).exists():\n        return sponsor.files.filter(tag_name=tag).first().tagged_file.item.url\n    return ''", "response": "returns the corresponding url from the tagged image list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks to see if the currently logged in user belongs to a specific specific group.", "response": "def ifusergroup(parser, token):\n    \"\"\" Check to see if the currently logged in user belongs to a specific\n    group. Requires the Django authentication contrib app and middleware.\n\n    Usage: {% ifusergroup Admins %} ... {% endifusergroup %}, or\n           {% ifusergroup Admins Clients Sellers %} ... {% else %} ... {% endifusergroup %}\n\n    \"\"\"\n    try:\n        tokensp = token.split_contents()\n        groups = []\n        groups+=tokensp[1:]\n    except ValueError:\n        raise template.TemplateSyntaxError(\"Tag 'ifusergroup' requires at least 1 argument.\")\n\n    nodelist_true = parser.parse(('else', 'endifusergroup'))\n    token = parser.next_token()\n\n    if token.contents == 'else':\n        nodelist_false = parser.parse(tuple(['endifusergroup',]))\n        parser.delete_first_token()\n    else:\n        nodelist_false = NodeList()\n\n    return GroupCheckNode(groups, nodelist_true, nodelist_false)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef OpenHandle(self):\n        '''Gets a handle for use with other vSphere Guest API functions. The guest library\n           handle provides a context for accessing information about the virtual machine.\n\n           Virtual machine statistics and state data are associated with a particular guest library\n           handle, so using one handle does not affect the data associated with another handle.'''\n        if hasattr(self, 'handle'):\n            return self.handle\n        else:\n            handle = c_void_p()\n            ret = vmGuestLib.VMGuestLib_OpenHandle(byref(handle))\n            if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n            return handle", "response": "Gets a handle for use with other vSphere Guest API functions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CloseHandle(self):\n        '''Releases a handle acquired with VMGuestLib_OpenHandle'''\n        if hasattr(self, 'handle'):\n            ret = vmGuestLib.VMGuestLib_CloseHandle(self.handle.value)\n            if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n            del(self.handle)", "response": "Releases a handle acquired with VMGuestLib_OpenHandle'''Releases a handle acquired with VMGuestLib_OpenHandle'''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate information about the virtual machine.", "response": "def UpdateInfo(self):\n        '''Updates information about the virtual machine. This information is associated with\n           the VMGuestLibHandle.\n\n           VMGuestLib_UpdateInfo requires similar CPU resources to a system call and\n           therefore can affect performance. If you are concerned about performance, minimize\n           the number of calls to VMGuestLib_UpdateInfo.\n\n           If your program uses multiple threads, each thread must use a different handle.\n           Otherwise, you must implement a locking scheme around update calls. The vSphere\n           Guest API does not implement internal locking around access with a handle.'''\n        ret = vmGuestLib.VMGuestLib_UpdateInfo(self.handle.value)\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the VMSessionID for the current session. Call this function after calling .", "response": "def GetSessionId(self):\n        '''Retrieves the VMSessionID for the current session. Call this function after calling\n           VMGuestLib_UpdateInfo. If VMGuestLib_UpdateInfo has never been called,\n           VMGuestLib_GetSessionId returns VMGUESTLIB_ERROR_NO_INFO.'''\n        sid = c_void_p()\n        ret = vmGuestLib.VMGuestLib_GetSessionId(self.handle.value, byref(sid))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return sid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the upperlimit of processor use in MHz available to the virtual machine.", "response": "def GetCpuLimitMHz(self):\n        '''Retrieves the upperlimit of processor use in MHz available to the virtual\n           machine. For information about setting the CPU limit, see \"Limits and\n           Reservations\" on page 14.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetCpuLimitMHz(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetCpuReservationMHz(self):\n        '''Retrieves the minimum processing power in MHz reserved for the virtual\n           machine. For information about setting a CPU reservation, see \"Limits and\n           Reservations\" on page 14.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetCpuReservationMHz(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the minimum processing power in MHz reserved for the virtual\n           machine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the number of milliseconds that the virtual machine was in a ready state.", "response": "def GetCpuStolenMs(self):\n        '''Retrieves the number of milliseconds that the virtual machine was in a\n           ready state (able to transition to a run state), but was not scheduled to run.'''\n        counter = c_uint64()\n        ret = vmGuestLib.VMGuestLib_GetCpuStolenMs(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the number of milliseconds during which the virtual machine has used the CPU. This value is a subset of elapsedMs.", "response": "def GetCpuUsedMs(self):\n        '''Retrieves the number of milliseconds during which the virtual machine\n           has used the CPU. This value includes the time used by the guest\n           operating system and the time used by virtualization code for tasks for this\n           virtual machine. You can combine this value with the elapsed time\n           (VMGuestLib_GetElapsedMs) to estimate the effective virtual machine\n           CPU speed. This value is a subset of elapsedMs.'''\n        counter = c_uint64()\n        ret = vmGuestLib.VMGuestLib_GetCpuUsedMs(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetElapsedMs(self):\n        '''Retrieves the number of milliseconds that have passed in the virtual\n           machine since it last started running on the server. The count of elapsed\n           time restarts each time the virtual machine is powered on, resumed, or\n           migrated using VMotion. This value counts milliseconds, regardless of\n           whether the virtual machine is using processing power during that time.\n\n           You can combine this value with the CPU time used by the virtual machine\n           (VMGuestLib_GetCpuUsedMs) to estimate the effective virtual machine\n           CPU speed. cpuUsedMs is a subset of this value.'''\n        counter = c_uint64()\n        ret = vmGuestLib.VMGuestLib_GetElapsedMs(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the number of milliseconds that have passed in the virtual machine since it last started running on the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetHostCpuUsedMs(self):\n        '''Undocumented.'''\n        counter = c_uint64()\n        ret = vmGuestLib.VMGuestLib_GetHostCpuUsedMs(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of CPU usage of the host process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the size of memory that is reserved for the host memory.", "response": "def GetHostMemKernOvhdMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemKernOvhdMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetHostMemMappedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemMappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is mapped to the host memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the number of physical memory allocated to the host memory.", "response": "def GetHostMemPhysFreeMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemPhysFreeMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetHostMemPhysMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemPhysMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the physical memory size of the host memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetHostMemSharedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemSharedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is shared by the host."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the amount of memory that has been swapped for the host memory.", "response": "def GetHostMemSwappedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemSwappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetHostMemUnmappedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemUnmappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is not mapped to the host memory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetHostMemUsedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostMemUsedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is used by the host process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the number of CPU cores that the host is using.", "response": "def GetHostNumCpuCores(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostNumCpuCores(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetHostProcessorSpeed(self):\n        '''Retrieves the speed of the ESX system's physical CPU in MHz.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetHostProcessorSpeed(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the speed of the ESX system s physical CPU in MHz."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the amount of memory the virtual machine is actively using its estimated working set size.", "response": "def GetMemActiveMB(self):\n        '''Retrieves the amount of memory the virtual machine is actively using its\n           estimated working set size.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemActiveMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetMemBalloonedMB(self):\n        '''Retrieves the amount of memory that has been reclaimed from this virtual\n           machine by the vSphere memory balloon driver (also referred to as the\n           \"vmmemctl\" driver).'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemBalloonedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that has been reclaimed from this virtual\n           machine by the vSphere memory balloon driver."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetMemBalloonMaxMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemBalloonMaxMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the maximum amount of memory balloon available for the virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetMemBalloonTargetMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemBalloonTargetMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is required to balloon the virtual machine can use."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the upper limit of memory that is available to the virtual machine. For information about setting a memory limit see Limits and Reservations on page 14.", "response": "def GetMemLimitMB(self):\n        '''Retrieves the upper limit of memory that is available to the virtual\n           machine. For information about setting a memory limit, see \"Limits and\n           Reservations\" on page 14.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemLimitMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetMemLLSwappedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemLLSwappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is longer than the current memory space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetMemMappedMB(self):\n        '''Retrieves the amount of memory that is allocated to the virtual machine.\n           Memory that is ballooned, swapped, or has never been accessed is\n           excluded.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemMappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that is allocated to the virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetMemOverheadMB(self):\n        '''Retrieves the amount of \"overhead\" memory associated with this virtual\n           machine that is currently consumed on the host system. Overhead\n           memory is additional memory that is reserved for data structures required\n           by the virtualization layer.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemOverheadMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of overhead memory associated with this virtual machine that is currently consumed on the host system. Overhead Memory is reserved for data structures required by the virtualization layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the minimum amount of memory that is reserved for the virtual machine.", "response": "def GetMemReservationMB(self):\n        '''Retrieves the minimum amount of memory that is reserved for the virtual\n           machine. For information about setting a memory reservation, see \"Limits\n           and Reservations\" on page 14.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemReservationMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the amount of physical memory that is shared with this virtual machine that is copy - on - write on the host.", "response": "def GetMemSharedMB(self):\n        '''Retrieves the amount of physical memory associated with this virtual\n           machine that is copy-on-write (COW) shared on the host.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemSharedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetMemSharedSavedMB(self):\n        '''Retrieves the estimated amount of physical memory on the host saved\n           from copy-on-write (COW) shared guest physical memory.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemSharedSavedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the estimated amount of physical memory on the host saved\n           from copy - on - write ( COW ) shared guest physical memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the number of memory shares allocated to the virtual machine.", "response": "def GetMemShares(self):\n        '''Retrieves the number of memory shares allocated to the virtual machine.\n           For information about how an ESX server uses memory shares to manage\n           virtual machine priority, see the vSphere Resource Management Guide.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemShares(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetMemSwappedMB(self):\n        '''Retrieves the amount of memory that has been reclaimed from this virtual\n           machine by transparently swapping guest memory to disk.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemSwappedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that has been reclaimed from this virtual\n           machine by transparently swapping guest memory to disk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the amount of memory that the VM can swap into.", "response": "def GetMemSwapTargetMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemSwapTargetMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetMemTargetSizeMB(self):\n        '''Retrieves the size of the target memory allocation for this virtual machine.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemTargetSizeMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the size of the target memory allocation for this virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the estimated amount of physical host memory currently consumed for this virtual machine s physical memory.", "response": "def GetMemUsedMB(self):\n        '''Retrieves the estimated amount of physical host memory currently\n           consumed for this virtual machine's physical memory.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemUsedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the amount of memory that is compressed for the virtual machine.", "response": "def GetMemZippedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemZippedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetMemZipSavedMB(self):\n        '''Undocumented.'''\n        counter = c_uint()\n        ret = vmGuestLib.VMGuestLib_GetMemZipSavedMB(self.handle.value, byref(counter))\n        if ret != VMGUESTLIB_ERROR_SUCCESS: raise VMGuestLibException(ret)\n        return counter.value", "response": "Retrieves the amount of memory that has been saved to the memory zip."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates a B - or M - or I - spline with the specified internal knots order and coefficients.", "response": "def spev(t_int, C, deg, x, cov_C=None, M_spline=False, I_spline=False, n=0):\n    \"\"\"Evaluate a B-, M- or I-spline with the specified internal knots, order and coefficients.\n    \n    `deg` boundary knots are appended at both sides of the domain.\n    \n    The zeroth order basis functions are modified to ensure continuity at the\n    right-hand boundary.\n    \n    Note that the I-splines include the :math:`i=0` case in order to have a \"DC\n    offset\". This way your functions do not have to start at zero. If you want\n    to not include this, simply set the first coefficient in `C` to zero.\n    \n    Parameters\n    ----------\n    t_int : array of float, (`M`,)\n        The internal knot locations. Must be monotonic (this is NOT checked).\n    C : array of float, (`M + deg - 1`,)\n        The coefficients applied to the basis functions.\n    deg : nonnegative int\n        The polynomial degree to use.\n    x : array of float, (`N`,)\n        The locations to evaluate the spline at.\n    cov_C : array of float, (`M + deg - 1`,) or (`M + deg - 1`, `M + deg - 1`), optional\n        The covariance matrix of the coefficients. If a 1d array is passed, this\n        is treated as the variance. If None, then the uncertainty is not\n        computed.\n    M_spline : bool, optional\n        If True, compute the M-spline instead of the B-spline. M-splines are\n        normalized to integrate to unity, as opposed to B-splines which sum to\n        unity at all points. Default is False (compute B-spline).\n    I_spline : bool, optional\n        If True, compute the I-spline instead of the B-spline. Note that this\n        will override `M_spline`. I-splines are the integrals of the M-splines,\n        and hence ensure curves are monotonic if all coefficients are of the\n        same sign. Note that the I-splines returned will be of polynomial degree\n        `deg` (i.e., the integral of what is returned from calling the function\n        with `deg=deg-1` and `M_spline=True`. Default is False (compute B-spline\n        or M-spline).\n    n : int, optional\n        The derivative order to compute. Default is 0. If `n>d`, all zeros are\n        returned (i.e., the discontinuities are not included).\n    \n    Returns\n    -------\n    `y` or (`y`, `cov_y`): The values (and possibly uncertainties) of the spline\n    at the specified locations.\n    \"\"\"\n    C = scipy.asarray(C, dtype=float)\n    t_int = scipy.asarray(t_int, dtype=float)\n    if (t_int != scipy.sort(t_int)).any():\n        raise ValueError(\"Knots must be in increasing order!\")\n    # if len(scipy.unique(t_int)) != len(t_int):\n    #     raise ValueError(\"Knots must be unique!\")\n    if n > deg:\n        return scipy.zeros_like(x, dtype=float)\n    if I_spline:\n        # I_{i,k} = int_L^x M_{i,k}(u)du, so just take the derivative of the\n        # underlying M-spline. Discarding the first coefficient dumps the \"DC\n        # offset\" term.\n        if cov_C is not None:\n            cov_C = scipy.asarray(cov_C)\n            if cov_C.ndim == 1:\n                cov_C = cov_C[1:]\n            elif cov_C.ndim == 2:\n                cov_C = cov_C[1:, 1:]\n        if n > 0:\n            return spev(\n                t_int, C[1:], deg - 1, x,\n                cov_C=cov_C, M_spline=True, I_spline=False, n=n - 1\n            )\n        M_spline = True\n    if n > 0:\n        if M_spline:\n            t = scipy.concatenate(([t_int[0]] * deg, t_int, [t_int[-1]] * deg))\n            C = (deg + 1.0) * (\n                C[1:] / (t[deg + 2:len(t_int) + 2 * deg] - t[1:len(t_int) + deg - 1]) -\n                C[:-1] / (t[deg + 1:len(t_int) + 2 * deg - 1] - t[:len(t_int) + deg - 2])\n            )\n        else:\n            C = C[1:] - C[:-1]\n        return spev(\n            t_int, C, deg - 1, x,\n            cov_C=cov_C, M_spline=True, I_spline=False, n=n - 1\n        )\n    \n    if len(C) != len(t_int) + deg - 1:\n        raise ValueError(\"Length of C must be equal to M + deg - 1!\")\n    \n    # Append the external knots directly at the boundary:\n    t = scipy.concatenate(([t_int[0]] * deg, t_int, [t_int[-1]] * deg))\n    \n    # Compute the different orders:\n    B = scipy.zeros((deg + 1, len(t) - 1, len(x)))\n    \n    # NOTE: The first dimension is indexed by deg, and is zero-indexed.\n    \n    # Zeroth order: constant function\n    d = 0\n    for i in xrange(deg, deg + len(t_int) - 2 + 1):\n        # The second condition contains a hack to make the basis functions\n        # continuous at the right-hand edge.\n        mask = (t[i] <= x) & (\n            (x < t[i + 1]) | ((i == deg + len(t_int) - 2) & (x == t[-1]))\n        )\n        B[d, i, mask] = 1.0 / (t[i + 1] - t[i]) if M_spline else 1.0\n    \n    # Loop over other orders:\n    for d in xrange(1, deg + 1):\n        for i in xrange(deg - d, deg + len(t_int) - 2 + 1):\n            if t[i + d] != t[i]:\n                v = (x - t[i]) * B[d - 1, i, :]\n                if not M_spline:\n                    v /= t[i + d] - t[i]\n                B[d, i, :] += v\n            if t[i + d + 1] != t[i + 1]:\n                v = (t[i + d + 1] - x) * B[d - 1, i + 1, :]\n                if not M_spline:\n                    v /= t[i + d + 1] - t[i + 1]\n                B[d, i, :] += v\n            if M_spline and ((t[i + d] != t[i]) or (t[i + d + 1] != t[i + 1])):\n                B[d, i, :] *= (d + 1) / (d * (t[i + d + 1] - t[i]))\n    \n    B = B[deg, 0:len(C), :].T\n    \n    # Now compute the I-splines, if needed:\n    if I_spline:\n        I = scipy.zeros_like(B)\n        for i in xrange(0, len(C)):\n            for m in xrange(i, len(C)):\n                I[:, i] += (t[m + deg + 1] - t[m]) * B[:, m] / (deg + 1.0)\n        B = I\n    \n    y = B.dot(C)\n    if cov_C is not None:\n        cov_C = scipy.asarray(cov_C)\n        # If there are no covariances, promote cov_C to a diagonal matrix\n        if cov_C.ndim == 1:\n            cov_C = scipy.diag(cov_C)\n        cov_y = B.dot(cov_C).dot(B.T)\n        return (y, cov_y)\n    else:\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild an argparse argument parser to parse the command line.", "response": "def parser(subparsers):\n    \"Build an argparse argument parser to parse the command line.\"\n\n    # create the parser for the version subcommand.\n    parser_version = subparsers.add_parser(\n        'version',\n        help=\"Output the version of %(prog)s to the console.\")\n    parser_version.set_defaults(func=command_version)\n\n    return parser_version"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the specified Crispy FormHelper and instantiate it.", "response": "def wafer_form_helper(context, helper_name):\n    '''\n    Find the specified Crispy FormHelper and instantiate it.\n    Handy when you are crispyifying other apps' forms.\n    '''\n    request = context.request\n    module, class_name = helper_name.rsplit('.', 1)\n    if module not in sys.modules:\n        __import__(module)\n    mod = sys.modules[module]\n    class_ = getattr(mod, class_name)\n    return class_(request=request)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nredirecting to the profile URL", "response": "def redirect_profile(request):\n    '''\n    The default destination from logging in, redirect to the actual profile URL\n    '''\n    if request.user.is_authenticated:\n        return HttpResponseRedirect(reverse('wafer_user_profile',\n                                            args=(request.user.username,)))\n    else:\n        return redirect_to_login(next=reverse(redirect_profile))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reviewed_badge(user, talk):\n    context = {\n        'reviewed': False,\n    }\n\n    review = None\n    if user and not user.is_anonymous():\n        review = talk.reviews.filter(reviewer=user).first()\n\n    if review:\n        context['reviewed'] = True\n        context['review_is_current'] = review.is_current()\n\n    return context", "response": "Returns a badge for the user s reviews of the talk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef linear_warp(X, d, n, *args):\n    X = scipy.asarray(X, dtype=float)\n    \n    a = args[2 * d]\n    b = args[2 * d + 1]\n    \n    if n == 0:\n        return (X - a) / (b - a)\n    elif n == 1:\n        return 1.0 / (b - a) * scipy.ones_like(X)\n    else:\n        return scipy.zeros_like(X)", "response": "r Linear warping of the inputs X to the unit hypercube."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the possibly recursive warping function and its derivatives.", "response": "def w_func(self, X, d, n):\n        \"\"\"Evaluate the (possibly recursive) warping function and its derivatives.\n        \n        Parameters\n        ----------\n        X : array, (`M`,)\n            The points (from dimension `d`) to evaluate the warping function at.\n        d : int\n            The dimension to warp.\n        n : int\n            The derivative order to compute. So far only 0 and 1 are supported.\n        \"\"\"\n        if n == 0:\n            wX = self.w(X, d, 0)\n            if isinstance(self.k, WarpedKernel):\n                wX = self.k.w_func(wX, d, 0)\n            return wX\n        elif n == 1:\n            wXn = self.w(X, d, n)\n            if isinstance(self.k, WarpedKernel):\n                wX = self.w_func(X, d, 0)\n                wXn *= self.k.w_func(wX, d, n)\n            return wXn\n        else:\n            raise ValueError(\"Derivative orders greater than one are not supported!\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enforce_bounds(self, v):\n        self._enforce_bounds = v\n        self.k.enforce_bounds = v\n        self.w.enforce_bounds = v", "response": "Set enforce_bounds for both kernels and kernels to a new value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the free parameters.", "response": "def free_params(self, value):\n        \"\"\"Set the free parameters. Note that this bypasses enforce_bounds.\n        \"\"\"\n        value = scipy.asarray(value, dtype=float)\n        self.K_up_to_date = False\n        self.k.free_params = value[:self.k.num_free_params]\n        self.w.free_params = value[self.k.num_free_params:self.k.num_free_params + self.w.num_free_params]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the ( free ) hyperparameters.", "response": "def set_hyperparams(self, new_params):\n        \"\"\"Set the (free) hyperparameters.\n        \n        Parameters\n        ----------\n        new_params : :py:class:`Array` or other Array-like\n            New values of the free parameters.\n        \n        Raises\n        ------\n        ValueError\n            If the length of `new_params` is not consistent with :py:attr:`self.params`.\n        \"\"\"\n        new_params = scipy.asarray(new_params, dtype=float)\n        \n        if len(new_params) == len(self.free_params):\n            num_free_k = sum(~self.k.fixed_params)\n            self.k.set_hyperparams(new_params[:num_free_k])\n            self.w.set_hyperparams(new_params[num_free_k:])\n        else:\n            raise ValueError(\"Length of new_params must be %s!\" % (len(self.free_params),))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, request, *args, **kwargs):\n\n        # Prepare base\n        if 'pk' in kwargs:\n            self.object = self.get_object()\n        else:\n            self.object = None\n        form_class = self.get_form_class()\n\n        # Get prefix\n        if 'field_prefix' in form_class.Meta.__dict__:\n            # Get name from the form\n            field_prefix = form_class.Meta.field_prefix\n        else:\n            # Get name from the class\n            field_prefix = str(form_class).split(\"'\")[1].split(\".\")[-1]\n        self.field_prefix = field_prefix\n\n        # Build form\n        form = self.get_form(form_class)\n\n        # Find groups\n        if 'groups' in dir(self):\n            # Save groups\n            groups = self.groups\n            # Redefine groups inside the form\n            form.__groups__ = lambda: groups\n            # Initialize list of fields\n            fields = []\n        else:\n            groups = None\n\n        # Add special prefix support to properly support form independency\n        form.add_prefix = lambda fields_name, field_prefix=field_prefix: \"%s_%s\" % (field_prefix, fields_name)\n        if 'autofill' not in dir(form.Meta):\n            form.Meta.autofill = {}\n\n        # For every extra form\n        forms = []\n        position_form_default = 0\n        for (formelement, linkerfield, modelfilter) in self.forms:\n            if formelement is None:\n                formobj = form\n                position_form_default = len(forms)\n            else:\n                # Locate linked element\n                if self.object:\n                    related_name = formelement._meta.model._meta.get_field(linkerfield).related_query_name()\n                    queryset = getattr(self.object, related_name)\n                    if modelfilter:\n                        queryset = queryset.filter(eval(\"Q(%s)\" % (modelfilter)))\n                    get_method = getattr(queryset, 'get', None)\n                    if get_method:\n                        instance = queryset.get()\n                    else:\n                        instance = queryset\n                else:\n                    instance = None\n\n                if 'autofill' in dir(formelement.Meta):\n                    formname = str(formelement).split('.')[-1].split(\"'\")[0]\n                    for key in formelement.Meta.autofill:\n                        form.Meta.autofill['{}_{}'.format(formname, key)] = formelement.Meta.autofill[key]\n\n                # Get prefix\n                if 'field_prefix' in formelement.Meta.__dict__:\n                    # Get name from the form\n                    field_prefix = formelement.Meta.field_prefix\n                else:\n                    # Get name from the class\n                    field_prefix = str(formelement).split(\"'\")[1].split(\".\")[-1]\n                self.field_prefix = field_prefix\n\n                # Prepare form\n                formobj = formelement(instance=instance)\n                formobj.form_name = form.form_name\n\n                # Excluded fields\n                if 'exclude' not in formobj.Meta.__dict__:\n                    formobj.Meta.exclude = [linkerfield]\n                elif linkerfield not in formobj.Meta.exclude:\n                    formobj.Meta.exclude.append(linkerfield)\n                if linkerfield in formobj.fields:\n                    del(formobj.fields[linkerfield])\n\n                # Add special prefix support to properly support form independency\n                formobj.add_prefix = lambda fields_name, field_prefix=field_prefix: \"%s_%s\" % (field_prefix, fields_name)\n                formobj.scope_prefix = field_prefix\n\n            # Save fields to the list\n            if groups:\n                for field in formobj:\n                    fields.append(field)\n            else:\n                # Add the form to the list of forms\n                forms.append(formobj)\n\n        if position_form_default == 0:\n            open_tabs = 1\n        else:\n            open_tabs = 0\n        # Remember list of fields\n        if groups:\n            form.list_fields = fields\n\n        # Add context and return new context\n        return self.render_to_response(self.get_context_data(form=form, forms=forms, open_tabs=open_tabs, position_form_default=position_form_default))", "response": "Handles GET requests and instantiates blank versions of the form and its inline formsets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(self, request, *args, **kwargs):\n\n        # Prepare base\n        if 'pk' in kwargs:\n            self.object = self.get_object()\n        else:\n            self.object = None\n        form_class = self.get_form_class()\n\n        # Get prefix\n        if 'field_prefix' in form_class.Meta.__dict__:\n            # Get name from the form\n            field_prefix = form_class.Meta.field_prefix\n            # Initialize list of fields\n        else:\n            # Get name from the class\n            field_prefix = str(form_class).split(\"'\")[1].split(\".\")[-1]\n        self.field_prefix = field_prefix\n\n        # Build the form\n        form = self.get_form(form_class)\n\n        # Find groups\n        if 'groups' in dir(self):\n            # Save groups\n            groups = self.groups\n            # Redefine groups inside the form\n            form.__groups__ = lambda: groups\n            # Initialize list of fields\n            fields = []\n        else:\n            groups = None\n\n        # Add special prefix support to properly support form independency\n        form.add_prefix = lambda fields_name, field_prefix=field_prefix: \"%s_%s\" % (field_prefix, fields_name)\n\n        # Check validation\n        valid = form.is_valid()\n        if (not valid) and ('non_field_errors' in dir(self)):\n            errors = [element[5] for element in list(self.non_field_errors())[:-1]]\n        elif form.errors.as_data():\n            errors = []\n            for element in form.errors.as_data():\n                for err in form.errors.as_data()[element][0]:\n                    errors.append(err)\n        else:\n            errors = []\n\n        # For every extra form\n        temp_forms = []\n        position_form_default = 0\n        for (formelement, linkerfield, modelfilter) in self.forms:\n            if formelement is None:\n                formobj = form\n                position_form_default = len(temp_forms)\n            else:\n                # Locate linked element\n                if self.object:\n                    related_name = formelement._meta.model._meta.get_field(linkerfield).related_query_name()\n                    queryset = getattr(self.object, related_name)\n                    if modelfilter:\n                        queryset = queryset.filter(eval(\"Q(%s)\" % (modelfilter)))\n                    get_method = getattr(queryset, 'get', None)\n                    if get_method:\n                        instance = queryset.get()\n                    else:\n                        instance = queryset\n                else:\n                    instance = None\n\n                # Get prefix\n                if 'field_prefix' in formelement.Meta.__dict__:\n                    # Get name from the form\n                    field_prefix = formelement.Meta.field_prefix\n                else:\n                    # Get name from the class\n                    field_prefix = str(formelement).split(\"'\")[1].split(\".\")[-1]\n                self.field_prefix = field_prefix\n\n                # Prepare form\n                formobj = formelement(instance=instance, data=self.request.POST)\n                formobj.form_name = form.form_name\n\n                # Excluded fields\n                if 'exclude' not in formobj.Meta.__dict__:\n                    formobj.Meta.exclude = [linkerfield]\n                elif linkerfield not in formobj.Meta.exclude:\n                    formobj.Meta.exclude.append(linkerfield)\n                if linkerfield in formobj.fields:\n                    del(formobj.fields[linkerfield])\n\n                # Link it to the main model\n                formobj.add_prefix = lambda fields_name, field_prefix=field_prefix: \"%s_%s\" % (field_prefix, fields_name)\n\n                # Validate\n                valid *= formobj.is_valid()\n\n                # append error\n                if not formobj.is_valid() and ('non_field_errors' in dir(formobj)):\n                    errors += [element[5] for element in list(formobj.non_field_errors())[:-1]]\n\n            # Save fields to the list\n            if groups:\n                for field in formobj:\n                    # raise Exception (field.__dict__)\n                    if 'unblock_t2ime' in field.html_name:\n                        raise Exception(field.field.__dict__)\n                    fields.append(field)\n\n            # Add a new form\n            temp_forms.append((formobj, linkerfield))\n\n        # execute validation specified\n        validate_forms = None\n        if valid and (\"validate\" in dir(self)):\n            validate_forms = [tform[0] for tform in temp_forms]\n            errors = self.validate(*validate_forms)\n            # valid = len(errors) == 0\n            valid = False\n            if errors is None or len(errors) == 0:\n                valid = True\n\n        # Remember list of fields\n        if groups:\n            form.list_fields = fields\n            forms = []\n        else:\n            if validate_forms:\n                forms = validate_forms\n            else:\n                forms = [tform[0] for tform in temp_forms]\n\n        if position_form_default == 0:\n            open_tabs = 1\n        else:\n            open_tabs = 0\n\n        # Check validation result\n        if valid:\n            # Everything is OK, call valid\n            return self.form_valid(form, temp_forms)\n        else:\n            # Something went wrong, attach error and call invalid\n            form.list_errors = errors\n            return self.form_invalid(form, forms, open_tabs, position_form_default)", "response": "This method is called by the base class when a POST request is made."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef form_valid(self, form, forms):\n        if self.object:\n            form.save()\n            for (formobj, linkerfield) in forms:\n                if form != formobj:\n                    formobj.save()\n        else:\n            self.object = form.save()\n            for (formobj, linkerfield) in forms:\n                if form != formobj:\n                    setattr(formobj.instance, linkerfield, self.object)\n                    formobj.save()\n        return HttpResponseRedirect(self.get_success_url())", "response": "Called if all forms are valid. Creates a Recipe instance along with associated Ingredients and Instructions and redirects to a success page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef form_invalid(self, form, forms, open_tabs, position_form_default):\n        # return self.render_to_response( self.get_context_data( form = form, forms = forms ) )\n        return self.render_to_response(self.get_context_data(form=form, forms=forms, open_tabs=open_tabs, position_form_default=position_form_default))", "response": "Called if a form is invalid. Re - renders the context data with the data filled forms and errors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_fmin_slsqp(fun, guess, opt_kwargs={}):\n    opt_kwargs = dict(opt_kwargs)\n\n    opt_kwargs.pop('method', None)\n\n    eqcons = []\n    ieqcons = []\n    if 'constraints' in opt_kwargs:\n        if isinstance(opt_kwargs['constraints'], dict):\n            opt_kwargs['constraints'] = [opt_kwargs['constraints'],]\n        for con in opt_kwargs.pop('constraints'):\n            if con['type'] == 'eq':\n                eqcons += [con['fun'],]\n            elif con['type'] == 'ineq':\n                ieqcons += [con['fun'],]\n            else:\n                raise ValueError(\"Invalid constraint type %s!\" % (con['type'],))\n\n    if 'jac' in opt_kwargs:\n        warnings.warn(\"Jacobian not supported for default solver SLSQP!\",\n                      RuntimeWarning)\n        opt_kwargs.pop('jac')\n\n    if 'tol' in opt_kwargs:\n        opt_kwargs['acc'] = opt_kwargs.pop('tol')\n\n    if 'options' in opt_kwargs:\n        opts = opt_kwargs.pop('options')\n        opt_kwargs = dict(opt_kwargs.items() + opts.items())\n\n    # Other keywords with less likelihood for causing failures are silently ignored:\n    opt_kwargs.pop('hess', None)\n    opt_kwargs.pop('hessp', None)\n    opt_kwargs.pop('callback', None)\n\n    out, fx, its, imode, smode = scipy.optimize.fmin_slsqp(\n        fun,\n        guess,\n        full_output=True,\n        eqcons=eqcons,\n        ieqcons=ieqcons,\n        **opt_kwargs\n    )\n\n    Result = collections.namedtuple('Result',\n                                    ['status', 'success', 'fun', 'x', 'message', 'nit'])\n\n    return Result(status=imode,\n                  success=(imode == 0),\n                  fun=fx,\n                  x=out,\n                  message=smode,\n                  nit=its)", "response": "A wrapper for fmin_slsqp that allows the user to call a function with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef yn2Kn2Der(nu, y, n=0, tol=5e-4, nterms=1, nu_step=0.001):\n    n = int(n)\n    y = scipy.asarray(y, dtype=float)\n    \n    if n == 0:\n        K = y**(nu / 2.0) * scipy.special.kv(nu, scipy.sqrt(y))\n        K[y == 0.0] = scipy.special.gamma(nu) / 2.0**(1.0 - nu)\n    else:\n        K = scipy.zeros_like(y)\n        for k in scipy.arange(0.0, n + 1.0, dtype=float):\n            K += (\n                scipy.special.binom(n, k) * fixed_poch(1.0 + nu / 2.0 - k, k) *\n                y**(nu / 2.0 - k) * Kn2Der(nu, y, n=n-k)\n            )\n        # Do the extra work to handle y == 0 only if we need to:\n        mask = (y == 0.0)\n        if (mask).any():\n            if int(nu) == nu:\n                K[mask] = 0.5 * (\n                    yn2Kn2Der(nu - nu_step, y[mask], n=n, tol=tol, nterms=nterms, nu_step=nu_step) +\n                    yn2Kn2Der(nu + nu_step, y[mask], n=n, tol=tol, nterms=nterms, nu_step=nu_step)\n                )\n            else:\n                if n > nu:\n                    K[mask] = scipy.special.gamma(-nu) * fixed_poch(1 + nu - n, n) * scipy.inf\n                else:\n                    K[mask] = scipy.special.gamma(nu) * scipy.special.gamma(n + 1.0) / (\n                        2.0**(1.0 - nu + 2.0 * n) * fixed_poch(1.0 - nu, n) *\n                        scipy.special.factorial(n)\n                    )\n    if tol > 0.0:\n        # Replace points within tol (absolute distance) of zero with the power\n        # series approximation:\n        mask = (y <= tol) & (y > 0.0)\n        K[mask] = 0.0\n        if int(nu) == nu:\n            K[mask] = 0.5 * (\n                yn2Kn2Der(nu - nu_step, y[mask], n=n, tol=tol, nterms=nterms, nu_step=nu_step) +\n                yn2Kn2Der(nu + nu_step, y[mask], n=n, tol=tol, nterms=nterms, nu_step=nu_step)\n            )\n        else:\n            for k in scipy.arange(n, n + nterms, dtype=float):\n                K[mask] += (\n                    scipy.special.gamma(nu) * fixed_poch(1.0 + k - n, n) * y[mask]**(k - n) / (\n                        2.0**(1.0 - nu + 2 * k) * fixed_poch(1.0 - nu, k) * scipy.special.factorial(k))\n                    )\n            for k in scipy.arange(0, nterms, dtype=float):\n                K[mask] += (\n                    scipy.special.gamma(-nu) * fixed_poch(1.0 + nu + k - n, n) *\n                    y[mask]**(nu + k - n) / (\n                        2.0**(1.0 + nu + 2.0 * k) * fixed_poch(1.0 + nu, k) *\n                        scipy.special.factorial(k)\n                    )\n                )\n    \n    return K", "response": "r Returns the modified Bessel function and its derivatives."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incomplete_bell_poly(n, k, x):\n    if n == 0 and k == 0:\n        return scipy.ones(x.shape[0], dtype=float)\n    elif k == 0 and n >= 1:\n        return scipy.zeros(x.shape[0], dtype=float)\n    elif n == 0 and k >= 1:\n        return scipy.zeros(x.shape[0], dtype=float)\n    else:\n        result = scipy.zeros(x.shape[0], dtype=float)\n        for m in xrange(0, n - k + 1):\n            result += x[:, m] * scipy.special.binom(n - 1, m) * incomplete_bell_poly(n - (m + 1), k - 1, x)\n        return result", "response": "rRecursive evaluation of the incomplete Bell polynomial."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the restricted growth strings for all of the partitions of a set of n - member sets.", "response": "def generate_set_partition_strings(n):\n    \"\"\"Generate the restricted growth strings for all of the partitions of an `n`-member set.\n    \n    Uses Algorithm H from page 416 of volume 4A of Knuth's `The Art of Computer\n    Programming`. Returns the partitions in lexicographical order.\n    \n    Parameters\n    ----------\n    n : scalar int, non-negative\n        Number of (unique) elements in the set to be partitioned.\n    \n    Returns\n    -------\n    partitions : list of :py:class:`Array`\n        List has a number of elements equal to the `n`-th Bell number (i.e.,\n        the number of partitions for a set of size `n`). Each element has\n        length `n`, the elements of which are the restricted growth strings\n        describing the partitions of the set. The strings are returned in\n        lexicographic order.\n    \"\"\"\n    # Handle edge cases:\n    if n == 0:\n        return []\n    elif n == 1:\n        return [scipy.array([0])]\n    \n    partitions = []\n    \n    # Step 1: Initialize\n    a = scipy.zeros(n, dtype=int)\n    b = scipy.ones(n, dtype=int)\n    \n    while True:\n        # Step 2: Visit\n        partitions.append(a.copy())\n        if a[-1] == b[-1]:\n            # Step 4: Find j. j is the index of the first element from the end\n            # for which a != b, with the exception of the last element.\n            j = (a[:-1] != b[:-1]).nonzero()[0][-1]\n            # Step 5: Increase a_j (or terminate):\n            if j == 0:\n                break\n            else:\n                a[j] += 1\n                # Step 6: Zero out a_{j+1} to a_n:\n                b[-1] = b[j] + (a[j] == b[j])\n                a[j + 1:] = 0\n                b[j + 1 :-1] = b[-1]\n        else:\n            # Step 3: Increase a_n:\n            a[-1] += 1\n    \n    return partitions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_set_partitions(set_):\n    set_ = scipy.asarray(set_)\n    strings = generate_set_partition_strings(len(set_))\n    partitions = []\n    for string in strings:\n        blocks = []\n        for block_num in scipy.unique(string):\n            blocks.append(set_[string == block_num])\n        partitions.append(blocks)\n    \n    return partitions", "response": "Generates all of the partitions of a set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds unique rows in numpy. array.", "response": "def unique_rows(arr, return_index=False, return_inverse=False):\n    \"\"\"Returns a copy of arr with duplicate rows removed.\n    \n    From Stackoverflow \"Find unique rows in numpy.array.\"\n    \n    Parameters\n    ----------\n    arr : :py:class:`Array`, (`m`, `n`)\n        The array to find the unique rows of.\n    return_index : bool, optional\n        If True, the indices of the unique rows in the array will also be\n        returned. I.e., unique = arr[idx]. Default is False (don't return\n        indices).\n    return_inverse: bool, optional\n        If True, the indices in the unique array to reconstruct the original\n        array will also be returned. I.e., arr = unique[inv]. Default is False\n        (don't return inverse).\n    \n    Returns\n    -------\n    unique : :py:class:`Array`, (`p`, `n`) where `p` <= `m`\n        The array `arr` with duplicate rows removed.\n    \"\"\"\n    b = scipy.ascontiguousarray(arr).view(\n        scipy.dtype((scipy.void, arr.dtype.itemsize * arr.shape[1]))\n    )\n    try:\n        out = scipy.unique(b, return_index=True, return_inverse=return_inverse)\n        dum = out[0]\n        idx = out[1]\n        if return_inverse:\n            inv = out[2]\n    except TypeError:\n        if return_inverse:\n            raise RuntimeError(\n                \"Error in scipy.unique on older versions of numpy prevents \"\n                \"return_inverse from working!\"\n            )\n        # Handle bug in numpy 1.6.2:\n        rows = [_Row(row) for row in b]\n        srt_idx = sorted(range(len(rows)), key=rows.__getitem__)\n        rows = scipy.asarray(rows)[srt_idx]\n        row_cmp = [-1]\n        for k in xrange(1, len(srt_idx)):\n            row_cmp.append(rows[k-1].__cmp__(rows[k]))\n        row_cmp = scipy.asarray(row_cmp)\n        transition_idxs = scipy.where(row_cmp != 0)[0]\n        idx = scipy.asarray(srt_idx)[transition_idxs]\n    out = arr[idx]\n    if return_index:\n        out = (out, idx)\n    elif return_inverse:\n        out = (out, inv)\n    elif return_index and return_inverse:\n        out = (out, idx, inv)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_stats(vals, check_nan=False, robust=False, axis=1, plot_QQ=False, bins=15, name=''):\n    if axis != 1 and robust:\n        raise NotImplementedError(\"Values of axis other than 1 are not supported \"\n                                  \"with the robust keyword at this time!\")\n    if robust:\n        # TODO: This stuff should really be vectorized if there is something that allows it!\n        if check_nan:\n            mean = scipy.stats.nanmedian(vals, axis=axis)\n            # TODO: HANDLE AXIS PROPERLY!\n            std = scipy.zeros(vals.shape[0], dtype=float)\n            for k in xrange(0, len(vals)):\n                ch = vals[k]\n                ok_idxs = ~scipy.isnan(ch)\n                if ok_idxs.any():\n                    std[k] = (scipy.stats.scoreatpercentile(ch[ok_idxs], 75) -\n                              scipy.stats.scoreatpercentile(ch[ok_idxs], 25))\n                else:\n                    # Leave a nan where there are no non-nan values:\n                    std[k] = scipy.nan\n            std /= IQR_TO_STD\n        else:\n            mean = scipy.median(vals, axis=axis)\n            # TODO: HANDLE AXIS PROPERLY!\n            std = scipy.asarray([scipy.stats.scoreatpercentile(ch, 75.0) -\n                                 scipy.stats.scoreatpercentile(ch, 25.0)\n                                 for ch in vals]) / IQR_TO_STD\n    else:\n        if check_nan:\n            mean = scipy.stats.nanmean(vals, axis=axis)\n            std = scipy.stats.nanstd(vals, axis=axis)\n        else:\n            mean = scipy.mean(vals, axis=axis)\n            std = scipy.std(vals, axis=axis)\n    if plot_QQ:\n        f = plt.figure()\n        gs = mplgs.GridSpec(2, 2, height_ratios=[8, 1])\n        a_QQ = f.add_subplot(gs[0, 0])\n        a_hist = f.add_subplot(gs[0, 1])\n        a_slider = f.add_subplot(gs[1, :])\n        \n        title = f.suptitle(\"\")\n        \n        def update(val):\n            \"\"\"Update the index from the results to be displayed.\n            \"\"\"\n            a_QQ.clear()\n            a_hist.clear()\n            idx = slider.val\n            title.set_text(\"%s, n=%d\" % (name, idx))\n            \n            nan_idxs = scipy.isnan(vals[idx, :])\n            if not nan_idxs.all():\n                osm, osr = scipy.stats.probplot(vals[idx, ~nan_idxs], dist='norm', plot=None, fit=False)\n                a_QQ.plot(osm, osr, 'bo', markersize=10)\n                a_QQ.set_title('QQ plot')\n                a_QQ.set_xlabel('quantiles of $\\mathcal{N}(0,1)$')\n                a_QQ.set_ylabel('quantiles of data')\n                \n                a_hist.hist(vals[idx, ~nan_idxs], bins=bins, normed=True)\n                locs = scipy.linspace(vals[idx, ~nan_idxs].min(), vals[idx, ~nan_idxs].max())\n                a_hist.plot(locs, scipy.stats.norm.pdf(locs, loc=mean[idx], scale=std[idx]))\n                a_hist.set_title('Normalized histogram and reported PDF')\n                a_hist.set_xlabel('value')\n                a_hist.set_ylabel('density')\n            \n            f.canvas.draw()\n        \n        def arrow_respond(slider, event):\n            \"\"\"Event handler for arrow key events in plot windows.\n\n            Pass the slider object to update as a masked argument using a lambda function::\n\n                lambda evt: arrow_respond(my_slider, evt)\n\n            Parameters\n            ----------\n            slider : Slider instance associated with this handler.\n            event : Event to be handled.\n            \"\"\"\n            if event.key == 'right':\n                slider.set_val(min(slider.val + 1, slider.valmax))\n            elif event.key == 'left':\n                slider.set_val(max(slider.val - 1, slider.valmin))\n\n        slider = mplw.Slider(a_slider,\n                             'index',\n                             0,\n                             len(vals) - 1,\n                             valinit=0,\n                             valfmt='%d')\n        slider.on_changed(update)\n        update(0)\n        f.canvas.mpl_connect('key_press_event', lambda evt: arrow_respond(slider, evt))\n    \n    return (mean, std)", "response": "Compute the average statistics for the given values along the specified axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a plot of a mean curve with uncertainty envelopes.", "response": "def univariate_envelope_plot(x, mean, std, ax=None, base_alpha=0.375, envelopes=[1, 3], lb=None, ub=None, expansion=10, **kwargs):\n    \"\"\"Make a plot of a mean curve with uncertainty envelopes.\n    \"\"\"\n    if ax is None:\n        f = plt.figure()\n        ax = f.add_subplot(1, 1, 1)\n    elif ax == 'gca':\n        ax = plt.gca()\n    \n    mean = scipy.asarray(mean, dtype=float).copy()\n    std = scipy.asarray(std, dtype=float).copy()\n    \n    # Truncate the data so matplotlib doesn't die:\n    if lb is not None and ub is not None and expansion != 1.0:\n        expansion *= ub - lb\n        ub = ub + expansion\n        lb = lb - expansion\n    if ub is not None:\n        mean[mean > ub] = ub\n    if lb is not None:\n        mean[mean < lb] = lb\n    \n    l = ax.plot(x, mean, **kwargs)\n    color = plt.getp(l[0], 'color')\n    e = []\n    for i in envelopes:\n        lower = mean - i * std\n        upper = mean + i * std\n        if ub is not None:\n            lower[lower > ub] = ub\n            upper[upper > ub] = ub\n        if lb is not None:\n            lower[lower < lb] = lb\n            upper[upper < lb] = lb\n        e.append(ax.fill_between(x, lower, upper, facecolor=color, alpha=base_alpha / i))\n    return (l, e)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef summarize_sampler(sampler, weights=None, burn=0, ci=0.95, chain_mask=None):\n    try:\n        k = sampler.flatchain.shape[-1]\n    except AttributeError:\n        # Assumes array input is only case where there is no \"flatchain\" attribute.\n        k = sampler.shape[-1]\n    \n    if isinstance(sampler, emcee.EnsembleSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.chain.shape[0], dtype=bool)\n        flat_trace = sampler.chain[chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, emcee.PTSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.nwalkers, dtype=bool)\n        flat_trace = sampler.chain[temp_idx, chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, scipy.ndarray):\n        if sampler.ndim == 4:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[1], dtype=bool)\n            flat_trace = sampler[temp_idx, chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[temp_idx, chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 3:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[0], dtype=bool)\n            flat_trace = sampler[chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 2:\n            flat_trace = sampler[burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[burn:]\n                weights = weights.ravel()\n    else:\n        raise ValueError(\"Unknown sampler class: %s\" % (type(sampler),))\n    \n    cibdry = 100.0 * (1.0 - ci) / 2.0\n    if weights is None:\n        mean = scipy.mean(flat_trace, axis=0)\n        ci_l, ci_u = scipy.percentile(flat_trace, [cibdry, 100.0 - cibdry], axis=0)\n    else:\n        mean = weights.dot(flat_trace) / weights.sum()\n        ci_l = scipy.zeros(k)\n        ci_u = scipy.zeros(k)\n        p = scipy.asarray([cibdry, 100.0 - cibdry])\n        for i in range(0, k):\n            srt = flat_trace[:, i].argsort()\n            x = flat_trace[srt, i]\n            w = weights[srt]\n            Sn = w.cumsum()\n            pn = 100.0 / Sn[-1] * (Sn - w / 2.0)\n            j = scipy.digitize(p, pn) - 1\n            ci_l[i], ci_u[i] = x[j] + (p - pn[j]) / (pn[j + 1] - pn[j]) * (x[j + 1] - x[j])\n    \n    return (mean, ci_l, ci_u)", "response": "r Create summary statistics of the sampler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_sampler(\n        sampler, suptitle=None, labels=None, bins=50,\n        plot_samples=False, plot_hist=True, plot_chains=True,\n        burn=0, chain_mask=None, temp_idx=0, weights=None, cutoff_weight=None,\n        cmap='gray_r', hist_color='k', chain_alpha=0.1,\n        points=None, covs=None, colors=None, ci=[0.95],\n        max_hist_ticks=None, max_chain_ticks=6,\n        label_chain_y=False, hide_chain_yticklabels=False, chain_ytick_pad=2.0,\n        label_fontsize=None, ticklabel_fontsize=None, chain_label_fontsize=None,\n        chain_ticklabel_fontsize=None, xticklabel_angle=90.0,\n        bottom_sep=0.075, suptitle_space=0.1, fixed_height=None,\n        fixed_width=None, l=0.1, r=0.9, t1=None, b1=None, t2=0.2, b2=0.1,\n        ax_space=0.1\n    ):\n    \"\"\"Plot the results of MCMC sampler (posterior and chains).\n    \n    Loosely based on triangle.py. Provides extensive options to format the plot.\n    \n    Parameters\n    ----------\n    sampler : :py:class:`emcee.Sampler` instance or array, (`n_temps`, `n_chains`, `n_samp`, `n_dim`), (`n_chains`, `n_samp`, `n_dim`) or (`n_samp`, `n_dim`)\n        The sampler to plot the chains/marginals of. Can also be an array of\n        samples which matches the shape of the `chain` attribute that would be\n        present in a :py:class:`emcee.Sampler` instance.\n    suptitle : str, optional\n        The figure title to place at the top. Default is no title.\n    labels : list of str, optional\n        The labels to use for each of the free parameters. Default is to leave\n        the axes unlabeled.\n    bins : int, optional\n        Number of bins to use for the histograms. Default is 50.\n    plot_samples : bool, optional\n        If True, the samples are plotted as individual points. Default is False.\n    plot_hist : bool, optional\n        If True, histograms are plotted. Default is True.\n    plot_chains : bool, optional\n        If True, plot the sampler chains at the bottom. Default is True.\n    burn : int, optional\n        The number of samples to burn before making the marginal histograms.\n        Default is zero (use all samples).\n    chain_mask : (index) array, optional\n        Mask identifying the chains to keep before plotting, in case there are\n        bad chains. Default is to use all chains.\n    temp_idx : int, optional\n        Index of the temperature to plot when plotting a\n        :py:class:`emcee.PTSampler`. Default is 0 (samples from the posterior).\n    weights : array, (`n_temps`, `n_chains`, `n_samp`), (`n_chains`, `n_samp`) or (`n_samp`,), optional\n        The weight for each sample. This is useful for post-processing the\n        output from MultiNest sampling, for instance. Default is to not weight\n        the samples.\n    cutoff_weight : float, optional\n        If `weights` and `cutoff_weight` are present, points with\n        `weights < cutoff_weight * weights.max()` will be excluded. Default is\n        to plot all points.\n    cmap : str, optional\n        The colormap to use for the histograms. Default is 'gray_r'.\n    hist_color : str, optional\n        The color to use for the univariate histograms. Default is 'k'.\n    chain_alpha : float, optional\n        The transparency to use for the plots of the individual chains. Setting\n        this to something low lets you better visualize what is going on.\n        Default is 0.1.\n    points : array, (`D`,) or (`N`, `D`), optional\n        Array of point(s) to plot onto each marginal and chain. Default is None.\n    covs : array, (`D`, `D`) or (`N`, `D`, `D`), optional\n        Covariance matrix or array of covariance matrices to plot onto each\n        marginal. If you do not want to plot a covariance matrix for a specific\n        point, set its corresponding entry to `None`. Default is to not plot\n        confidence ellipses for any points.\n    colors : array of str, (`N`,), optional\n        The colors to use for the points in `points`. Default is to use the\n        standard matplotlib RGBCMYK cycle.\n    ci : array, (`num_ci`,), optional\n        List of confidence intervals to plot for each non-`None` entry in `covs`.\n        Default is 0.95 (just plot the 95 percent confidence interval).\n    max_hist_ticks : int, optional\n        The maximum number of ticks for the histogram plots. Default is None\n        (no limit).\n    max_chain_ticks : int, optional\n        The maximum number of y-axis ticks for the chain plots. Default is 6.\n    label_chain_y : bool, optional\n        If True, the chain plots will have y axis labels. Default is False.\n    hide_chain_yticklabels : bool, optional\n        If True, hide the y axis tick labels for the chain plots. Default is\n        False (show y tick labels).\n    chain_ytick_pad : float, optional\n        The padding (in points) between the y-axis tick labels and the axis for\n        the chain plots. Default is 2.0.\n    label_fontsize : float, optional\n        The font size (in points) to use for the axis labels. Default is\n        `axes.labelsize`.\n    ticklabel_fontsize : float, optional\n        The font size (in points) to use for the axis tick labels. Default is\n        `xtick.labelsize`.\n    chain_label_fontsize : float, optional\n        The font size (in points) to use for the labels of the chain axes.\n        Default is `axes.labelsize`.\n    chain_ticklabel_fontsize : float, optional\n        The font size (in points) to use for the chain axis tick labels. Default\n        is `xtick.labelsize`.\n    xticklabel_angle : float, optional\n        The angle to rotate the x tick labels, in degrees. Default is 90.\n    bottom_sep : float, optional\n        The separation (in relative figure units) between the chains and the\n        marginals. Default is 0.075.\n    suptitle_space : float, optional\n        The amount of space (in relative figure units) to leave for a figure\n        title. Default is 0.1.\n    fixed_height : float, optional\n        The desired figure height (in inches). Default is to automatically\n        adjust based on `fixed_width` to make the subplots square.\n    fixed_width : float, optional\n        The desired figure width (in inches). Default is `figure.figsize[0]`.\n    l : float, optional\n        The location (in relative figure units) of the left margin. Default is\n        0.1.\n    r : float, optional\n        The location (in relative figure units) of the right margin. Default is\n        0.9.\n    t1 : float, optional\n        The location (in relative figure units) of the top of the grid of\n        histograms. Overrides `suptitle_space` if present.\n    b1 : float, optional\n        The location (in relative figure units) of the bottom of the grid of\n        histograms. Overrides `bottom_sep` if present. Defaults to 0.1 if\n        `plot_chains` is False.\n    t2 : float, optional\n        The location (in relative figure units) of the top of the grid of chain\n        plots. Default is 0.2.\n    b2 : float, optional\n        The location (in relative figure units) of the bottom of the grid of\n        chain plots. Default is 0.1.\n    ax_space : float, optional\n        The `w_space` and `h_space` to use (in relative figure units). Default\n        is 0.1.\n    \"\"\"\n    masked_weights = None\n    if points is not None:\n        points = scipy.atleast_2d(points)\n        if covs is not None and len(covs) != len(points):\n            raise ValueError(\n                \"If covariance matrices are provided, len(covs) must equal len(points)!\"\n            )\n        elif covs is None:\n            covs = [None,] * len(points)\n        if colors is None:\n            c_cycle = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n            colors = [c_cycle.next() for p in points]\n    # Create axes:\n    try:\n        k = sampler.flatchain.shape[-1]\n    except AttributeError:\n        # Assumes array input is only case where there is no \"flatchain\" attribute.\n        k = sampler.shape[-1]\n    \n    if labels is None:\n        labels = [''] * k\n    \n    # Set up geometry:\n    # plot_chains =\n    #  True:         False:\n    # +-----------+ +-----------+\n    # | +-------+ | | +-------+ |\n    # | |       | | | |       | |\n    # | |       | | | |       | |\n    # | |       | | | |       | |\n    # | +-------+ | | +-------+ |\n    # | +-------+ | +-----------+\n    # | |       | |\n    # | +-------+ |\n    # +-----------+\n    \n    # We retain support for the original suptitle_space keyword, but can\n    # override with t1 as needed:\n    if t1 is None:\n        t1 = 1 - suptitle_space\n    \n    # We retain support for the original bottom_sep keyword, but can override\n    # with b1 as needed:\n    if b1 is None:\n        if plot_chains:\n            b1 = t2 + bottom_sep\n        else:\n            b1 = 0.1\n    \n    if fixed_height is None and fixed_width is None:\n        # Default: use matplotlib's default width, handle remaining parameters\n        # with the fixed width case below:\n        fixed_width = matplotlib.rcParams['figure.figsize'][0]\n    \n    if fixed_height is None and fixed_width is not None:\n        # Only width specified, compute height to yield square histograms:\n        fixed_height = fixed_width * (r - l) / (t1 - b1)\n    elif fixed_height is not None and fixed_width is None:\n        # Only height specified, compute width to yield square histograms\n        fixed_width = fixed_height * (t1 - b1) / (r - l)\n    # Otherwise width and height are fixed, and we may not have square\n    # histograms, at the user's discretion.\n    \n    wspace = ax_space\n    hspace = ax_space\n    \n    # gs1 is the histograms, gs2 is the chains:\n    f = plt.figure(figsize=(fixed_width, fixed_height))\n    gs1 = mplgs.GridSpec(k, k)\n    gs1.update(bottom=b1, top=t1, left=l, right=r, wspace=wspace, hspace=hspace)\n    if plot_chains:\n        gs2 = mplgs.GridSpec(1, k)\n        gs2.update(bottom=b2, top=t2, left=l, right=r, wspace=wspace, hspace=hspace)\n    axes = []\n    # j is the row, i is the column.\n    for j in xrange(0, k + int(plot_chains)):\n        row = []\n        for i in xrange(0, k):\n            if i > j:\n                row.append(None)\n            else:\n                sharey = row[-1] if i > 0 and i < j and j < k else None\n                sharex = axes[-1][i] if j > i and j < k else \\\n                    (row[-1] if i > 0 and j == k else None)\n                gs = gs1[j, i] if j < k else gs2[:, i]\n                row.append(f.add_subplot(gs, sharey=sharey, sharex=sharex))\n                if j < k and ticklabel_fontsize is not None:\n                    row[-1].tick_params(labelsize=ticklabel_fontsize)\n                elif j >= k and chain_ticklabel_fontsize is not None:\n                    row[-1].tick_params(labelsize=chain_ticklabel_fontsize)\n        axes.append(row)\n    axes = scipy.asarray(axes)\n    \n    # Update axes with the data:\n    if isinstance(sampler, emcee.EnsembleSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.chain.shape[0], dtype=bool)\n        flat_trace = sampler.chain[chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, emcee.PTSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.nwalkers, dtype=bool)\n        flat_trace = sampler.chain[temp_idx, chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, scipy.ndarray):\n        if sampler.ndim == 4:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[1], dtype=bool)\n            flat_trace = sampler[temp_idx, chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[temp_idx, chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 3:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[0], dtype=bool)\n            flat_trace = sampler[chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 2:\n            flat_trace = sampler[burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[burn:]\n                weights = weights.ravel()\n        if cutoff_weight is not None and weights is not None:\n            mask = weights >= cutoff_weight * weights.max()\n            flat_trace = flat_trace[mask, :]\n            masked_weights = weights[mask]\n        else:\n            masked_weights = weights\n    else:\n        raise ValueError(\"Unknown sampler class: %s\" % (type(sampler),))\n    \n    # j is the row, i is the column.\n    for i in xrange(0, k):\n        axes[i, i].clear()\n        if plot_hist:\n            axes[i, i].hist(flat_trace[:, i], bins=bins, color=hist_color, weights=masked_weights, normed=True, histtype='stepfilled')\n        if plot_samples:\n            axes[i, i].plot(flat_trace[:, i], scipy.zeros_like(flat_trace[:, i]), ',', alpha=0.1)\n        if points is not None:\n            # axvline can only take a scalar x, so we have to loop:\n            for p, c, cov in zip(points, colors, covs):\n                axes[i, i].axvline(x=p[i], linewidth=3, color=c)\n                if cov is not None:\n                    xlim = axes[i, i].get_xlim()\n                    i_grid = scipy.linspace(xlim[0], xlim[1], 100)\n                    axes[i, i].plot(\n                        i_grid,\n                        scipy.stats.norm.pdf(\n                            i_grid,\n                            loc=p[i],\n                            scale=scipy.sqrt(cov[i, i])\n                        ),\n                        c,\n                        linewidth=3.0\n                    )\n                    axes[i, i].set_xlim(xlim)\n        if i == k - 1:\n            axes[i, i].set_xlabel(labels[i], fontsize=label_fontsize)\n            plt.setp(axes[i, i].xaxis.get_majorticklabels(), rotation=xticklabel_angle)\n        if i < k - 1:\n            plt.setp(axes[i, i].get_xticklabels(), visible=False)\n        plt.setp(axes[i, i].get_yticklabels(), visible=False)\n        for j in xrange(i + 1, k):\n            axes[j, i].clear()\n            if plot_hist:\n                ct, x, y, im = axes[j, i].hist2d(\n                    flat_trace[:, i],\n                    flat_trace[:, j],\n                    bins=bins,\n                    cmap=cmap,\n                    weights=masked_weights\n                )\n            if plot_samples:\n                axes[j, i].plot(flat_trace[:, i], flat_trace[:, j], ',', alpha=0.1)\n            if points is not None:\n                for p, c, cov in zip(points, colors, covs):\n                    axes[j, i].plot(p[i], p[j], 'o', color=c)\n                    if cov is not None:\n                        Sigma = scipy.asarray([[cov[i, i], cov[i, j]], [cov[j, i], cov[j, j]]], dtype=float)\n                        lam, v = scipy.linalg.eigh(Sigma)\n                        chi2 = [-scipy.log(1.0 - cival) * 2.0 for cival in ci]\n                        a = [2.0 * scipy.sqrt(chi2val * lam[-1]) for chi2val in chi2]\n                        b = [2.0 * scipy.sqrt(chi2val * lam[-2]) for chi2val in chi2]\n                        ang = scipy.arctan2(v[1, -1], v[0, -1])\n                        for aval, bval in zip(a, b):\n                            ell = mplp.Ellipse(\n                                [p[i], p[j]],\n                                aval,\n                                bval,\n                                angle=scipy.degrees(ang),\n                                facecolor='none',\n                                edgecolor=c,\n                                linewidth=3\n                            )\n                            axes[j, i].add_artist(ell)\n                # axes[j, i].plot(points[i], points[j], 'o')\n            # xmid = 0.5 * (x[1:] + x[:-1])\n            # ymid = 0.5 * (y[1:] + y[:-1])\n            # axes[j, i].contour(xmid, ymid, ct.T, colors='k')\n            if j < k - 1:\n                plt.setp(axes[j, i].get_xticklabels(), visible=False)\n            if i != 0:\n                plt.setp(axes[j, i].get_yticklabels(), visible=False)\n            if i == 0:\n                axes[j, i].set_ylabel(labels[j], fontsize=label_fontsize)\n            if j == k - 1:\n                axes[j, i].set_xlabel(labels[i], fontsize=label_fontsize)\n                plt.setp(axes[j, i].xaxis.get_majorticklabels(), rotation=xticklabel_angle)\n        if plot_chains:\n            axes[-1, i].clear()\n            if isinstance(sampler, emcee.EnsembleSampler):\n                axes[-1, i].plot(sampler.chain[:, :, i].T, alpha=chain_alpha)\n            elif isinstance(sampler, emcee.PTSampler):\n                axes[-1, i].plot(sampler.chain[temp_idx, :, :, i].T, alpha=chain_alpha)\n            else:\n                if sampler.ndim == 4:\n                    axes[-1, i].plot(sampler[temp_idx, :, :, i].T, alpha=chain_alpha)\n                elif sampler.ndim == 3:\n                    axes[-1, i].plot(sampler[:, :, i].T, alpha=chain_alpha)\n                elif sampler.ndim == 2:\n                    axes[-1, i].plot(sampler[:, i].T, alpha=chain_alpha)\n            # Plot the weights on top of the chains:\n            if weights is not None:\n                a_wt = axes[-1, i].twinx()\n                a_wt.plot(weights, alpha=chain_alpha, linestyle='--', color='r')\n                plt.setp(a_wt.yaxis.get_majorticklabels(), visible=False)\n                a_wt.yaxis.set_ticks_position('none')\n                # Plot the cutoff weight as a horizontal line and the first sample\n                # which is included as a vertical bar. Note that this won't be quite\n                # the right behavior if the weights are not roughly monotonic.\n                if cutoff_weight is not None:\n                    a_wt.axhline(cutoff_weight * weights.max(), linestyle='-', color='r')\n                    wi, = scipy.where(weights >= cutoff_weight * weights.max())\n                    a_wt.axvline(wi[0], linestyle='-', color='r')\n            if burn > 0:\n                axes[-1, i].axvline(burn, color='r', linewidth=3)\n            if points is not None:\n                for p, c in zip(points, colors):\n                    axes[-1, i].axhline(y=p[i], linewidth=3, color=c)\n                # Reset the xlim since it seems to get messed up:\n                axes[-1, i].set_xlim(left=0)\n                # try:\n                #     [axes[-1, i].axhline(y=pt, linewidth=3) for pt in points[i]]\n                # except TypeError:\n                #     axes[-1, i].axhline(y=points[i], linewidth=3)\n            if label_chain_y:\n                axes[-1, i].set_ylabel(labels[i], fontsize=chain_label_fontsize)\n            axes[-1, i].set_xlabel('step', fontsize=chain_label_fontsize)\n            plt.setp(axes[-1, i].xaxis.get_majorticklabels(), rotation=xticklabel_angle)\n            for tick in axes[-1, i].get_yaxis().get_major_ticks():\n                tick.set_pad(chain_ytick_pad)\n                tick.label1 = tick._get_text1()\n    \n    for i in xrange(0, k):\n        if max_hist_ticks is not None:\n            axes[k - 1, i].xaxis.set_major_locator(plt.MaxNLocator(nbins=max_hist_ticks - 1))\n            axes[i, 0].yaxis.set_major_locator(plt.MaxNLocator(nbins=max_hist_ticks - 1))\n        if plot_chains and max_chain_ticks is not None:\n            axes[k, i].yaxis.set_major_locator(plt.MaxNLocator(nbins=max_chain_ticks - 1))\n            axes[k, i].xaxis.set_major_locator(plt.MaxNLocator(nbins=max_chain_ticks - 1))\n        if plot_chains and hide_chain_yticklabels:\n            plt.setp(axes[k, i].get_yticklabels(), visible=False)\n    \n    if suptitle is not None:\n        f.suptitle(suptitle)\n    f.canvas.draw()\n    return f", "response": "Plots the results of the sampler."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a plot of the sampler's \"fingerprint\": univariate marginal histograms for all hyperparameters. The hyperparameters are mapped to [0, 1] using :py:meth:`hyperprior.elementwise_cdf`, so this can only be used with prior distributions which implement this function. Returns the figure and axis created. Parameters ---------- sampler : :py:class:`emcee.Sampler` instance or array, (`n_temps`, `n_chains`, `n_samp`, `n_dim`), (`n_chains`, `n_samp`, `n_dim`) or (`n_samp`, `n_dim`) The sampler to plot the chains/marginals of. Can also be an array of samples which matches the shape of the `chain` attribute that would be present in a :py:class:`emcee.Sampler` instance. hyperprior : :py:class:`~gptools.utils.JointPrior` instance The joint prior distribution for the hyperparameters. Used to map the values to [0, 1] so that the hyperparameters can all be shown on the same axis. weights : array, (`n_temps`, `n_chains`, `n_samp`), (`n_chains`, `n_samp`) or (`n_samp`,), optional The weight for each sample. This is useful for post-processing the output from MultiNest sampling, for instance. cutoff_weight : float, optional If `weights` and `cutoff_weight` are present, points with `weights < cutoff_weight * weights.max()` will be excluded. Default is to plot all points. nbins : int or array of int, (`D`,), optional The number of bins dividing [0, 1] to use for each histogram. If a single int is given, this is used for all of the hyperparameters. If an array of ints is given, these are the numbers of bins for each of the hyperparameters. The default is to determine the number of bins using the Freedman-Diaconis rule. labels : array of str, (`D`,), optional The labels for each hyperparameter. Default is to use empty strings. burn : int, optional The number of samples to burn before making the marginal histograms. Default is zero (use all samples). chain_mask : (index) array, optional Mask identifying the chains to keep before plotting, in case there are bad chains. Default is to use all chains. temp_idx : int, optional Index of the temperature to plot when plotting a :py:class:`emcee.PTSampler`. Default is 0 (samples from the posterior). points : array, (`D`,) or (`N`, `D`), optional Array of point(s) to plot as horizontal lines. Default is None. plot_samples : bool, optional If True, the samples are plotted as horizontal lines. Default is False. sample_color : str, optional The color to plot the samples in. Default is 'k', meaning black. point_color : str or list of str, optional The color to plot the individual points in. Default is to loop through matplotlib's default color sequence. If a list is provided, it will be cycled through. point_lw : float, optional Line width to use when plotting the individual points. title : str, optional Title to use for the plot. rot_x_labels : bool, optional If True, the labels for the x-axis are rotated 90 degrees. Default is False (do not rotate labels). figsize : 2-tuple, optional The figure size to use. Default is to use the matplotlib default.", "response": "def plot_sampler_fingerprint(\n        sampler, hyperprior, weights=None, cutoff_weight=None, nbins=None,\n        labels=None, burn=0, chain_mask=None, temp_idx=0, points=None,\n        plot_samples=False, sample_color='k', point_color=None, point_lw=3,\n        title='', rot_x_labels=False, figsize=None\n    ):\n    \"\"\"Make a plot of the sampler's \"fingerprint\": univariate marginal histograms for all hyperparameters.\n    \n    The hyperparameters are mapped to [0, 1] using\n    :py:meth:`hyperprior.elementwise_cdf`, so this can only be used with prior\n    distributions which implement this function.\n    \n    Returns the figure and axis created.\n    \n    Parameters\n    ----------\n    sampler : :py:class:`emcee.Sampler` instance or array, (`n_temps`, `n_chains`, `n_samp`, `n_dim`), (`n_chains`, `n_samp`, `n_dim`) or (`n_samp`, `n_dim`)\n        The sampler to plot the chains/marginals of. Can also be an array of\n        samples which matches the shape of the `chain` attribute that would be\n        present in a :py:class:`emcee.Sampler` instance.\n    hyperprior : :py:class:`~gptools.utils.JointPrior` instance\n        The joint prior distribution for the hyperparameters. Used to map the\n        values to [0, 1] so that the hyperparameters can all be shown on the\n        same axis.\n    weights : array, (`n_temps`, `n_chains`, `n_samp`), (`n_chains`, `n_samp`) or (`n_samp`,), optional\n        The weight for each sample. This is useful for post-processing the\n        output from MultiNest sampling, for instance.\n    cutoff_weight : float, optional\n        If `weights` and `cutoff_weight` are present, points with\n        `weights < cutoff_weight * weights.max()` will be excluded. Default is\n        to plot all points.\n    nbins : int or array of int, (`D`,), optional\n        The number of bins dividing [0, 1] to use for each histogram. If a\n        single int is given, this is used for all of the hyperparameters. If an\n        array of ints is given, these are the numbers of bins for each of the\n        hyperparameters. The default is to determine the number of bins using\n        the Freedman-Diaconis rule.\n    labels : array of str, (`D`,), optional\n        The labels for each hyperparameter. Default is to use empty strings.\n    burn : int, optional\n        The number of samples to burn before making the marginal histograms.\n        Default is zero (use all samples).\n    chain_mask : (index) array, optional\n        Mask identifying the chains to keep before plotting, in case there are\n        bad chains. Default is to use all chains.\n    temp_idx : int, optional\n        Index of the temperature to plot when plotting a\n        :py:class:`emcee.PTSampler`. Default is 0 (samples from the posterior).\n    points : array, (`D`,) or (`N`, `D`), optional\n        Array of point(s) to plot as horizontal lines. Default is None.\n    plot_samples : bool, optional\n        If True, the samples are plotted as horizontal lines. Default is False.\n    sample_color : str, optional\n        The color to plot the samples in. Default is 'k', meaning black.\n    point_color : str or list of str, optional\n        The color to plot the individual points in. Default is to loop through\n        matplotlib's default color sequence. If a list is provided, it will be\n        cycled through.\n    point_lw : float, optional\n        Line width to use when plotting the individual points.\n    title : str, optional\n        Title to use for the plot.\n    rot_x_labels : bool, optional\n        If True, the labels for the x-axis are rotated 90 degrees. Default is\n        False (do not rotate labels).\n    figsize : 2-tuple, optional\n        The figure size to use. Default is to use the matplotlib default.\n    \"\"\"\n    try:\n        k = sampler.flatchain.shape[-1]\n    except AttributeError:\n        # Assumes array input is only case where there is no \"flatchain\" attribute.\n        k = sampler.shape[-1]\n    # Process the samples:\n    if isinstance(sampler, emcee.EnsembleSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.chain.shape[0], dtype=bool)\n        flat_trace = sampler.chain[chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, emcee.PTSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.nwalkers, dtype=bool)\n        flat_trace = sampler.chain[temp_idx, chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, scipy.ndarray):\n        if sampler.ndim == 4:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[1], dtype=bool)\n            flat_trace = sampler[temp_idx, chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[temp_idx, chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 3:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[0], dtype=bool)\n            flat_trace = sampler[chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 2:\n            flat_trace = sampler[burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[burn:]\n                weights = weights.ravel()\n        if cutoff_weight is not None and weights is not None:\n            mask = weights >= cutoff_weight * weights.max()\n            flat_trace = flat_trace[mask, :]\n            weights = weights[mask]\n    else:\n        raise ValueError(\"Unknown sampler class: %s\" % (type(sampler),))\n    \n    if labels is None:\n        labels = [''] * k\n    \n    u = scipy.asarray([hyperprior.elementwise_cdf(p) for p in flat_trace], dtype=float).T\n    if nbins is None:\n        lq, uq = scipy.stats.scoreatpercentile(u, [25, 75], axis=1)\n        h = 2.0 * (uq - lq) / u.shape[0]**(1.0 / 3.0)\n        n = scipy.asarray(scipy.ceil(1.0 / h), dtype=int)\n    else:\n        try:\n            iter(nbins)\n            n = nbins\n        except TypeError:\n            n = nbins * scipy.ones(u.shape[0])\n    \n    hist = [scipy.stats.histogram(uv, numbins=nv, defaultlimits=[0, 1], weights=weights) for uv, nv in zip(u, n)]\n    max_ct = max([max(h.count) for h in hist])\n    min_ct = min([min(h.count) for h in hist])\n    \n    f = plt.figure(figsize=figsize)\n    a = f.add_subplot(1, 1, 1)\n    for i, (h, pn) in enumerate(zip(hist, labels)):\n        a.imshow(\n            scipy.atleast_2d(scipy.asarray(h.count[::-1], dtype=float)).T,\n            cmap='gray_r',\n            interpolation='nearest',\n            vmin=min_ct,\n            vmax=max_ct,\n            extent=(i, i + 1, 0, 1),\n            aspect='auto'\n        )\n    \n    if plot_samples:\n        for p in u:\n            for i, uv in enumerate(p):\n                a.plot([i, i + 1], [uv, uv], sample_color, alpha=0.1)\n    \n    if points is not None:\n        points = scipy.atleast_2d(scipy.asarray(points, dtype=float))\n        u_points = [hyperprior.elementwise_cdf(p) for p in points]\n        if point_color is None:\n            c_cycle = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n        else:\n            c_cycle = itertools.cycle(scipy.atleast_1d(point_color))\n        for p in u_points:\n            c = c_cycle.next()\n            for i, uv in enumerate(p):\n                a.plot([i, i + 1], [uv, uv], color=c, lw=point_lw)\n    \n    a.set_xlim(0, len(hist))\n    a.set_ylim(0, 1)\n    a.set_xticks(0.5 + scipy.arange(0, len(hist), dtype=float))\n    a.set_xticklabels(labels)\n    if rot_x_labels:\n        plt.setp(a.xaxis.get_majorticklabels(), rotation=90)\n    a.set_xlabel(\"parameter\")\n    a.set_ylabel(\"$u=F_P(p)$\")\n    a.set_title(title)\n    \n    return f, a"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a plot of the sampler's correlation or covariance matrix. Returns the figure and axis created. Parameters ---------- sampler : :py:class:`emcee.Sampler` instance or array, (`n_temps`, `n_chains`, `n_samp`, `n_dim`), (`n_chains`, `n_samp`, `n_dim`) or (`n_samp`, `n_dim`) The sampler to plot the chains/marginals of. Can also be an array of samples which matches the shape of the `chain` attribute that would be present in a :py:class:`emcee.Sampler` instance. method : {'corr', 'cov'} Whether to plot the correlation matrix ('corr') or the covariance matrix ('cov'). The covariance matrix is often not useful because different parameters have wildly different scales. Default is to plot the correlation matrix. labels : array of str, (`D`,), optional The labels for each hyperparameter. Default is to use empty strings. burn : int, optional The number of samples to burn before making the marginal histograms. Default is zero (use all samples). chain_mask : (index) array, optional Mask identifying the chains to keep before plotting, in case there are bad chains. Default is to use all chains. temp_idx : int, optional Index of the temperature to plot when plotting a :py:class:`emcee.PTSampler`. Default is 0 (samples from the posterior). cbar_label : str, optional The label to use for the colorbar. The default is chosen based on the value of the `method` keyword. title : str, optional Title to use for the plot. rot_x_labels : bool, optional If True, the labels for the x-axis are rotated 90 degrees. Default is False (do not rotate labels). figsize : 2-tuple, optional The figure size to use. Default is to use the matplotlib default. xlabel_on_top : bool, optional If True, the x-axis labels are put on top (the way mathematicians present matrices). Default is True.", "response": "def plot_sampler_cov(\n        sampler, method='corr', weights=None, cutoff_weight=None, labels=None,\n        burn=0, chain_mask=None, temp_idx=0, cbar_label=None, title='',\n        rot_x_labels=False, figsize=None, xlabel_on_top=True\n    ):\n    \"\"\"Make a plot of the sampler's correlation or covariance matrix.\n    \n    Returns the figure and axis created.\n    \n    Parameters\n    ----------\n    sampler : :py:class:`emcee.Sampler` instance or array, (`n_temps`, `n_chains`, `n_samp`, `n_dim`), (`n_chains`, `n_samp`, `n_dim`) or (`n_samp`, `n_dim`)\n        The sampler to plot the chains/marginals of. Can also be an array of\n        samples which matches the shape of the `chain` attribute that would be\n        present in a :py:class:`emcee.Sampler` instance.\n    method : {'corr', 'cov'}\n        Whether to plot the correlation matrix ('corr') or the covariance matrix\n        ('cov'). The covariance matrix is often not useful because different\n        parameters have wildly different scales. Default is to plot the\n        correlation matrix.\n    labels : array of str, (`D`,), optional\n        The labels for each hyperparameter. Default is to use empty strings.\n    burn : int, optional\n        The number of samples to burn before making the marginal histograms.\n        Default is zero (use all samples).\n    chain_mask : (index) array, optional\n        Mask identifying the chains to keep before plotting, in case there are\n        bad chains. Default is to use all chains.\n    temp_idx : int, optional\n        Index of the temperature to plot when plotting a\n        :py:class:`emcee.PTSampler`. Default is 0 (samples from the posterior).\n    cbar_label : str, optional\n        The label to use for the colorbar. The default is chosen based on the\n        value of the `method` keyword.\n    title : str, optional\n        Title to use for the plot.\n    rot_x_labels : bool, optional\n        If True, the labels for the x-axis are rotated 90 degrees. Default is\n        False (do not rotate labels).\n    figsize : 2-tuple, optional\n        The figure size to use. Default is to use the matplotlib default.\n    xlabel_on_top : bool, optional\n        If True, the x-axis labels are put on top (the way mathematicians\n        present matrices). Default is True.\n    \"\"\"\n    try:\n        k = sampler.flatchain.shape[-1]\n    except AttributeError:\n        # Assumes array input is only case where there is no \"flatchain\" attribute.\n        k = sampler.shape[-1]\n    # Process the samples:\n    if isinstance(sampler, emcee.EnsembleSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.chain.shape[0], dtype=bool)\n        flat_trace = sampler.chain[chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, emcee.PTSampler):\n        if chain_mask is None:\n            chain_mask = scipy.ones(sampler.nwalkers, dtype=bool)\n        flat_trace = sampler.chain[temp_idx, chain_mask, burn:, :]\n        flat_trace = flat_trace.reshape((-1, k))\n    elif isinstance(sampler, scipy.ndarray):\n        if sampler.ndim == 4:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[1], dtype=bool)\n            flat_trace = sampler[temp_idx, chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[temp_idx, chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 3:\n            if chain_mask is None:\n                chain_mask = scipy.ones(sampler.shape[0], dtype=bool)\n            flat_trace = sampler[chain_mask, burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[chain_mask, burn:]\n                weights = weights.ravel()\n        elif sampler.ndim == 2:\n            flat_trace = sampler[burn:, :]\n            flat_trace = flat_trace.reshape((-1, k))\n            if weights is not None:\n                weights = weights[burn:]\n                weights = weights.ravel()\n        if cutoff_weight is not None and weights is not None:\n            mask = weights >= cutoff_weight * weights.max()\n            flat_trace = flat_trace[mask, :]\n            weights = weights[mask]\n    else:\n        raise ValueError(\"Unknown sampler class: %s\" % (type(sampler),))\n    \n    if labels is None:\n        labels = [''] * k\n    \n    if cbar_label is None:\n        cbar_label = r'$\\mathrm{cov}(p_1, p_2)$' if method == 'cov' else r'$\\mathrm{corr}(p_1, p_2)$'\n    \n    if weights is None:\n        if method == 'corr':\n            cov = scipy.corrcoef(flat_trace, rowvar=0, ddof=1)\n        else:\n            cov = scipy.cov(flat_trace, rowvar=0, ddof=1)\n    else:\n        cov = scipy.cov(flat_trace, rowvar=0, aweights=weights)\n        if method == 'corr':\n            stds = scipy.sqrt(scipy.diag(cov))\n            STD_1, STD_2 = scipy.meshgrid(stds, stds)\n            cov = cov / (STD_1 * STD_2)\n    \n    f_cov = plt.figure(figsize=figsize)\n    a_cov = f_cov.add_subplot(1, 1, 1)\n    a_cov.set_title(title)\n    if method == 'cov':\n        vmax = scipy.absolute(cov).max()\n    else:\n        vmax = 1.0\n    cax = a_cov.pcolor(cov, cmap='seismic', vmin=-1 * vmax, vmax=vmax)\n    divider = make_axes_locatable(a_cov)\n    a_cb = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n    cbar = f_cov.colorbar(cax, cax=a_cb, label=cbar_label)\n    a_cov.set_xlabel('parameter')\n    a_cov.set_ylabel('parameter')\n    a_cov.axis('square')\n    a_cov.invert_yaxis()\n    if xlabel_on_top:\n        a_cov.xaxis.tick_top()\n        a_cov.xaxis.set_label_position('top')\n    a_cov.set_xticks(0.5 + scipy.arange(0, flat_trace.shape[1], dtype=float))\n    a_cov.set_yticks(0.5 + scipy.arange(0, flat_trace.shape[1], dtype=float))\n    a_cov.set_xticklabels(labels)\n    if rot_x_labels:\n        plt.setp(a_cov.xaxis.get_majorticklabels(), rotation=90)\n    a_cov.set_yticklabels(labels)\n    a_cov.set_xlim(0, flat_trace.shape[1])\n    a_cov.set_ylim(flat_trace.shape[1], 0)\n    \n    return f_cov, a_cov"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a sample to random variates uniform on : math : [ 0 1 )", "response": "def elementwise_cdf(self, p):\n        r\"\"\"Convert a sample to random variates uniform on :math:`[0, 1]`.\n        \n        For a univariate distribution, this is simply evaluating the CDF. To\n        facilitate efficient sampling, this function returns a *vector* of CDF\n        values, one value for each variable. Basically, the idea is that, given\n        a vector :math:`q` of `num_params` values each of which is distributed\n        according to the prior, this function will return variables uniform on\n        :math:`[0, 1]` corresponding to each variable. This is the inverse\n        operation to :py:meth:`sample_u`.\n        \n        Parameters\n        ----------\n        p : array-like, (`num_params`,)\n            Values to evaluate CDF at.\n        \"\"\"\n        p1_num_params = len(self.p1.bounds)\n        return scipy.concatenate(\n            (\n                self.p1.elementwise_cdf(p[:p1_num_params]),\n                self.p2.elementwise_cdf(p[p1_num_params:])\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw random samples of the hyperparameters.", "response": "def random_draw(self, size=None):\n        \"\"\"Draw random samples of the hyperparameters.\n        \n        The outputs of the two priors are stacked vertically.\n        \n        Parameters\n        ----------\n        size : None, int or array-like, optional\n            The number/shape of samples to draw. If None, only one sample is\n            returned. Default is None.\n        \"\"\"\n        draw_1 = self.p1.random_draw(size=size)\n        draw_2 = self.p2.random_draw(size=size)\n        \n        if draw_1.ndim == 1:\n            return scipy.hstack((draw_1, draw_2))\n        else:\n            return scipy.vstack((draw_1, draw_2))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a sample to random variates uniform on the CDF uniform on the CDF.", "response": "def elementwise_cdf(self, p):\n        r\"\"\"Convert a sample to random variates uniform on :math:`[0, 1]`.\n        \n        For a univariate distribution, this is simply evaluating the CDF. To\n        facilitate efficient sampling, this function returns a *vector* of CDF\n        values, one value for each variable. Basically, the idea is that, given\n        a vector :math:`q` of `num_params` values each of which is distributed\n        according to the prior, this function will return variables uniform on\n        :math:`[0, 1]` corresponding to each variable. This is the inverse\n        operation to :py:meth:`sample_u`.\n        \n        Parameters\n        ----------\n        p : array-like, (`num_params`,)\n            Values to evaluate CDF at.\n        \"\"\"\n        p = scipy.atleast_1d(p)\n        if len(p) != len(self.bounds):\n            raise ValueError(\"length of p must equal the number of parameters!\")\n        if p.ndim != 1:\n            raise ValueError(\"p must be one-dimensional!\")\n        c = scipy.zeros(len(self.bounds))\n        for k in xrange(0, len(self.bounds)):\n            if p[k] <= self.bounds[k][0]:\n                c[k] = 0.0\n            elif p[k] >= self.bounds[k][1]:\n                c[k] = 1.0\n            else:\n                c[k] = (p[k] - self.bounds[k][0]) / (self.bounds[k][1] - self.bounds[k][0])\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw random samples of the hyperparameters.", "response": "def random_draw(self, size=None):\n        \"\"\"Draw random samples of the hyperparameters.\n        \n        Parameters\n        ----------\n        size : None, int or array-like, optional\n            The number/shape of samples to draw. If None, only one sample is\n            returned. Default is None.\n        \"\"\"\n        return scipy.asarray([numpy.random.uniform(low=b[0], high=b[1], size=size) for b in self.bounds])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_draw(self, size=None):\n        if size is None:\n            size = 1\n            single_val = True\n        else:\n            single_val = False\n        \n        out_shape = [len(self.bounds)]\n        try:\n            out_shape.extend(size)\n        except TypeError:\n            out_shape.append(size)\n        \n        out = scipy.zeros(out_shape)\n        for j in xrange(0, len(self.bounds)):\n            if j != 2:\n                out[j, :] = numpy.random.uniform(low=self.bounds[j][0],\n                                                 high=self.bounds[j][1],\n                                                 size=size)\n            else:\n                out[j, :] = numpy.random.uniform(low=self.bounds[j][0],\n                                                 high=out[j - 1, :],\n                                                 size=size)\n        if not single_val:\n            return out\n        else:\n            return out.ravel()", "response": "Draw random samples of the hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample_u(self, q):\n        q = scipy.atleast_1d(q)\n        if len(q) != len(self.univariate_priors):\n            raise ValueError(\"length of q must equal the number of parameters!\")\n        if q.ndim != 1:\n            raise ValueError(\"q must be one-dimensional!\")\n        if (q < 0).any() or (q > 1).any():\n            raise ValueError(\"q must be within [0, 1]!\")\n        return scipy.asarray([p.ppf(v) for v, p in zip(q, self.univariate_priors)])", "response": "r Returns a sample from random variates uniformly on the CDF at the given vector q."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef random_draw(self, size=None):\n        return scipy.asarray([p.rvs(size=size) for p in self.univariate_priors])", "response": "Draw random samples of the hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a sample to random variates uniform on the CDF at a specific key.", "response": "def elementwise_cdf(self, p):\n        r\"\"\"Convert a sample to random variates uniform on :math:`[0, 1]`.\n        \n        For a univariate distribution, this is simply evaluating the CDF. To\n        facilitate efficient sampling, this function returns a *vector* of CDF\n        values, one value for each variable. Basically, the idea is that, given\n        a vector :math:`q` of `num_params` values each of which is distributed\n        according to the prior, this function will return variables uniform on\n        :math:`[0, 1]` corresponding to each variable. This is the inverse\n        operation to :py:meth:`sample_u`.\n        \n        Parameters\n        ----------\n        p : array-like, (`num_params`,)\n            Values to evaluate CDF at.\n        \"\"\"\n        p = scipy.atleast_1d(p)\n        if len(p) != len(self.sigma):\n            raise ValueError(\"length of p must equal the number of parameters!\")\n        if p.ndim != 1:\n            raise ValueError(\"p must be one-dimensional!\")\n        return scipy.asarray([scipy.stats.norm.cdf(v, loc=m, scale=s) for v, s, m in zip(p, self.sigma, self.mu)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws random samples of the hyperparameters.", "response": "def random_draw(self, size=None):\n        \"\"\"Draw random samples of the hyperparameters.\n        \n        Parameters\n        ----------\n        size : None, int or array-like, optional\n            The number/shape of samples to draw. If None, only one sample is\n            returned. Default is None.\n        \"\"\"\n        return scipy.asarray([scipy.stats.norm.rvs(loc=m, scale=s, size=size) for s, m in zip(self.sigma, self.mu)])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_u(self, q):\n        q = scipy.atleast_1d(q)\n        if len(q) != len(self.sigma):\n            raise ValueError(\"length of q must equal the number of parameters!\")\n        if q.ndim != 1:\n            raise ValueError(\"q must be one-dimensional!\")\n        if (q < 0).any() or (q > 1).any():\n            raise ValueError(\"q must be within [0, 1]!\")\n        return scipy.asarray([scipy.stats.lognorm.ppf(v, s, loc=0, scale=em) for v, s, em in zip(q, self.sigma, self.emu)])", "response": "r Returns a sample from random variates uniformly on the CDF at the given vector q."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef elementwise_cdf(self, p):\n        p = scipy.atleast_1d(p)\n        if len(p) != len(self.sigma):\n            raise ValueError(\"length of p must equal the number of parameters!\")\n        if p.ndim != 1:\n            raise ValueError(\"p must be one-dimensional!\")\n        return scipy.asarray([scipy.stats.lognorm.cdf(v, s, loc=0, scale=em) for v, s, em in zip(p, self.sigma, self.emu)])", "response": "Convert a sample to random variates uniform on 0. 0 1. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_draw(self, size=None):\n        return scipy.asarray([scipy.stats.lognorm.rvs(s, loc=0, scale=em, size=size) for s, em in zip(self.sigma, self.emu)])", "response": "Draw random samples of the hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bounds(self):\n        return [scipy.stats.gamma.interval(self.i, a, loc=0, scale=1.0 / b) for a, b in zip(self.a, self.b)]", "response": "The bounds of the random variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_draw(self, size=None):\n        return scipy.asarray([scipy.stats.gamma.rvs(a, loc=0, scale=1.0 / b, size=size) for a, b in zip(self.a, self.b)])", "response": "Draw random samples of the hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_draw(self, size=None):\n        if size is None:\n            size = 1\n            single_val = True\n        else:\n            single_val = False\n        \n        out_shape = [self.num_var]\n        try:\n            out_shape.extend(size)\n        except TypeError:\n            out_shape.append(size)\n        \n        out = scipy.sort(\n            numpy.random.uniform(\n                low=self.lb,\n                high=self.ub,\n                size=out_shape\n            ),\n            axis=0\n        )\n        if not single_val:\n            return out\n        else:\n            return out.ravel()", "response": "Draw random samples of the hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zapier_cancel_hook(request):\n    '''\n    Zapier can post something like this when tickets are cancelled\n    {\n        \"ticket_type\": \"Individual (Regular)\",\n        \"barcode\": \"12345678\",\n        \"email\": \"demo@example.com\"\n    }\n    '''\n    if request.META.get('HTTP_X_ZAPIER_SECRET', None) != settings.WAFER_TICKETS_SECRET:\n        raise PermissionDenied('Incorrect secret')\n\n    # This is required for python 3, and in theory fine on python 2\n    payload = json.loads(request.body.decode('utf8'))\n    ticket = Ticket.objects.filter(barcode=payload['barcode'])\n    if ticket.exists():\n        # delete the ticket\n        ticket.delete()\n    return HttpResponse(\"Cancelled\\n\", content_type='text/plain')", "response": "Cancels the current ticket in Zapier"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_token(self):\n        grant_type = 'client_credentials'\n\n        channel = yield self._tvm.ticket_full(\n            self._client_id, self._client_secret, grant_type, {})\n        ticket = yield channel.rx.get()\n\n        raise gen.Return(self._make_token(ticket))", "response": "Gains token from secure backend service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_secure_adaptor(service, mod, client_id, client_secret, tok_update_sec=None):\n        if mod == 'TVM':\n            return SecureServiceAdaptor(service, TVM(client_id, client_secret), tok_update_sec)\n\n        return SecureServiceAdaptor(service, Promiscuous(), tok_update_sec)", "response": "Creates a SecureServiceAdaptor for the given service and mod."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts information from an albacore summary file. Only reads which have a >0 length are returned. The fields below may or may not exist, depending on the type of sequencing performed. Fields 1-14 are for 1D sequencing. Fields 1-23 for 2D sequencing. Fields 24-27, 2-5, 22-23 for 1D^2 (1D2) sequencing Fields 28-38 for barcoded workflows 1 filename 2 read_id 3 run_id 4 channel 5 start_time 6 duration 7 num_events 8 template_start 9 num_events_template 10 template_duration 11 num_called_template 12 sequence_length_template 13 mean_qscore_template 14 strand_score_template 15 complement_start 16 num_events_complement 17 complement_duration 18 num_called_complement 19 sequence_length_complement 20 mean_qscore_complement 21 strand_score_complement 22 sequence_length_2d 23 mean_qscore_2d 24 filename1 25 filename2 26 read_id1 27 read_id2 28 barcode_arrangement 29 barcode_score 30 barcode_full_arrangement 31 front_score 32 rear_score 33 front_begin_index 34 front_foundseq_length 35 rear_end_index 36 rear_foundseq_length 37 kit 38 variant", "response": "def process_summary(summaryfile, **kwargs):\n    \"\"\"Extracting information from an albacore summary file.\n\n    Only reads which have a >0 length are returned.\n\n    The fields below may or may not exist, depending on the type of sequencing performed.\n    Fields 1-14 are for 1D sequencing.\n    Fields 1-23 for 2D sequencing.\n    Fields 24-27, 2-5, 22-23 for 1D^2 (1D2) sequencing\n    Fields 28-38 for barcoded workflows\n     1  filename\n     2  read_id\n     3  run_id\n     4  channel\n     5  start_time\n     6  duration\n     7  num_events\n     8  template_start\n     9  num_events_template\n    10  template_duration\n    11  num_called_template\n    12  sequence_length_template\n    13  mean_qscore_template\n    14  strand_score_template\n    15  complement_start\n    16    num_events_complement\n    17    complement_duration\n    18    num_called_complement\n    19    sequence_length_complement\n    20    mean_qscore_complement\n    21    strand_score_complement\n    22    sequence_length_2d\n    23    mean_qscore_2d\n    24    filename1\n    25    filename2\n    26    read_id1\n    27    read_id2\n    28    barcode_arrangement\n    29    barcode_score\n    30    barcode_full_arrangement\n    31    front_score\n    32    rear_score\n    33    front_begin_index\n    34    front_foundseq_length\n    35    rear_end_index\n    36    rear_foundseq_length\n    37    kit\n    38    variant\n    \"\"\"\n    logging.info(\"Nanoget: Collecting metrics from summary file {} for {} sequencing\".format(\n        summaryfile, kwargs[\"readtype\"]))\n    ut.check_existance(summaryfile)\n    if kwargs[\"readtype\"] == \"1D\":\n        cols = [\"read_id\", \"run_id\", \"channel\", \"start_time\", \"duration\",\n                \"sequence_length_template\", \"mean_qscore_template\"]\n    elif kwargs[\"readtype\"] in [\"2D\", \"1D2\"]:\n        cols = [\"read_id\", \"run_id\", \"channel\", \"start_time\", \"duration\",\n                \"sequence_length_2d\", \"mean_qscore_2d\"]\n    if kwargs[\"barcoded\"]:\n        cols.append(\"barcode_arrangement\")\n        logging.info(\"Nanoget: Extracting metrics per barcode.\")\n    try:\n        datadf = pd.read_csv(\n            filepath_or_buffer=summaryfile,\n            sep=\"\\t\",\n            usecols=cols,\n        )\n    except ValueError:\n        logging.error(\"Nanoget: did not find expected columns in summary file {}:\\n {}\".format(\n            summaryfile, ', '.join(cols)))\n        sys.exit(\"ERROR: expected columns in summary file {} not found:\\n {}\".format(\n            summaryfile, ', '.join(cols)))\n    if kwargs[\"barcoded\"]:\n        datadf.columns = [\"readIDs\", \"runIDs\", \"channelIDs\", \"time\", \"duration\",\n                          \"lengths\", \"quals\", \"barcode\"]\n    else:\n        datadf.columns = [\"readIDs\", \"runIDs\", \"channelIDs\", \"time\", \"duration\", \"lengths\", \"quals\"]\n    logging.info(\"Nanoget: Finished collecting statistics from summary file {}\".format(summaryfile))\n    return ut.reduce_memory_usage(datadf.loc[datadf[\"lengths\"] != 0].copy())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_bam(bam, samtype=\"bam\"):\n    ut.check_existance(bam)\n    samfile = pysam.AlignmentFile(bam, \"rb\")\n    if not samfile.has_index():\n        pysam.index(bam)\n        samfile = pysam.AlignmentFile(bam, \"rb\")  # Need to reload the samfile after creating index\n        logging.info(\"Nanoget: No index for bam file could be found, created index.\")\n    if not samfile.header['HD']['SO'] == 'coordinate':\n        logging.error(\"Nanoget: Bam file {} not sorted by coordinate!.\".format(bam))\n        sys.exit(\"Please use a bam file sorted by coordinate.\")\n    if samtype == \"bam\":\n        logging.info(\"Nanoget: Bam file {} contains {} mapped and {} unmapped reads.\".format(\n            bam, samfile.mapped, samfile.unmapped))\n        if samfile.mapped == 0:\n            logging.error(\"Nanoget: Bam file {} does not contain aligned reads.\".format(bam))\n            sys.exit(\"FATAL: not a single read was mapped in bam file {}\".format(bam))\n    return samfile", "response": "Check if bam file is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_ubam(bam, **kwargs):\n    logging.info(\"Nanoget: Starting to collect statistics from ubam file {}.\".format(bam))\n    samfile = pysam.AlignmentFile(bam, \"rb\", check_sq=False)\n    if not samfile.has_index():\n        pysam.index(bam)\n        # Need to reload the samfile after creating index\n        samfile = pysam.AlignmentFile(bam, \"rb\")\n        logging.info(\"Nanoget: No index for bam file could be found, created index.\")\n    datadf = pd.DataFrame(\n        data=[(read.query_name, nanomath.ave_qual(read.query_qualities), read.query_length)\n              for read in samfile.fetch(until_eof=True)],\n        columns=[\"readIDs\", \"quals\", \"lengths\"]) \\\n        .dropna(axis='columns', how='all') \\\n        .dropna(axis='index', how='any')\n    logging.info(\"Nanoget: ubam {} contains {} reads.\".format(\n        bam, datadf[\"lengths\"].size))\n    return ut.reduce_memory_usage(datadf)", "response": "Extracting metrics from unaligned bam format\n    Extracting lengths\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncombining metrics from bam after extraction.", "response": "def process_bam(bam, **kwargs):\n    \"\"\"Combines metrics from bam after extraction.\n\n    Processing function: calls pool of worker functions\n    to extract from a bam file the following metrics:\n    -lengths\n    -aligned lengths\n    -qualities\n    -aligned qualities\n    -mapping qualities\n    -edit distances to the reference genome scaled by read length\n    Returned in a pandas DataFrame\n    \"\"\"\n    logging.info(\"Nanoget: Starting to collect statistics from bam file {}.\".format(bam))\n    samfile = check_bam(bam)\n    chromosomes = samfile.references\n    params = zip([bam] * len(chromosomes), chromosomes)\n    with cfutures.ProcessPoolExecutor() as executor:\n        datadf = pd.DataFrame(\n            data=[res for sublist in executor.map(extract_from_bam, params) for res in sublist],\n            columns=[\"readIDs\", \"quals\", \"aligned_quals\", \"lengths\",\n                     \"aligned_lengths\", \"mapQ\", \"percentIdentity\"]) \\\n            .dropna(axis='columns', how='all') \\\n            .dropna(axis='index', how='any')\n    logging.info(\"Nanoget: bam {} contains {} primary alignments.\".format(\n        bam, datadf[\"lengths\"].size))\n    return ut.reduce_memory_usage(datadf)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract metrics from bam. Worker function per chromosome loop over a bam file and create list with tuples containing metrics: -qualities -aligned qualities -lengths -aligned lengths -mapping qualities -edit distances to the reference genome scaled by read length", "response": "def extract_from_bam(params):\n    \"\"\"Extracts metrics from bam.\n\n    Worker function per chromosome\n    loop over a bam file and create list with tuples containing metrics:\n    -qualities\n    -aligned qualities\n    -lengths\n    -aligned lengths\n    -mapping qualities\n    -edit distances to the reference genome scaled by read length\n    \"\"\"\n    bam, chromosome = params\n    samfile = pysam.AlignmentFile(bam, \"rb\")\n    return [\n        (read.query_name,\n         nanomath.ave_qual(read.query_qualities),\n         nanomath.ave_qual(read.query_alignment_qualities),\n         read.query_length,\n         read.query_alignment_length,\n         read.mapping_quality,\n         get_pID(read))\n        for read in samfile.fetch(reference=chromosome, multiple_iterators=True)\n        if not read.is_secondary]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pID(read):\n    try:\n        return 100 * (1 - read.get_tag(\"NM\") / read.query_alignment_length)\n    except KeyError:\n        try:\n            return 100 * (1 - (parse_MD(read.get_tag(\"MD\")) + parse_CIGAR(read.cigartuples))\n                          / read.query_alignment_length)\n        except KeyError:\n            return None\n    except ZeroDivisionError:\n        return None", "response": "Return the percent identity of a read."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning handles from compressed files according to file extension.", "response": "def handle_compressed_input(inputfq, file_type=\"fastq\"):\n    \"\"\"Return handles from compressed files according to extension.\n\n    Check for which fastq input is presented and open a handle accordingly\n    Can read from compressed files (gz, bz2, bgz) or uncompressed\n    Relies on file extensions to recognize compression\n    \"\"\"\n    ut.check_existance(inputfq)\n    if inputfq.endswith(('.gz', 'bgz')):\n        import gzip\n        logging.info(\"Nanoget: Decompressing gzipped {} {}\".format(file_type, inputfq))\n        return gzip.open(inputfq, 'rt')\n    elif inputfq.endswith('.bz2'):\n        import bz2\n        logging.info(\"Nanoget: Decompressing bz2 compressed {} {}\".format(file_type, inputfq))\n        return bz2.open(inputfq, 'rt')\n    elif inputfq.endswith(('.fastq', '.fq', 'fasta', '.fa', '.fas')):\n        return open(inputfq, 'r')\n    else:\n        logging.error(\"INPUT ERROR: Unrecognized file extension {}\".format(inputfq))\n        sys.exit('INPUT ERROR:\\nUnrecognized file extension in {}\\n'\n                 'Supported are gz, bz2, bgz, fastq, fq, fasta, fa and fas'.format(inputfq))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_fasta(fasta, **kwargs):\n    logging.info(\"Nanoget: Starting to collect statistics from a fasta file.\")\n    inputfasta = handle_compressed_input(fasta, file_type=\"fasta\")\n    return ut.reduce_memory_usage(pd.DataFrame(\n        data=[len(rec) for rec in SeqIO.parse(inputfasta, \"fasta\")],\n        columns=[\"lengths\"]\n    ).dropna())", "response": "Combine metrics extracted from a fasta file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_fastq_plain(fastq, **kwargs):\n    logging.info(\"Nanoget: Starting to collect statistics from plain fastq file.\")\n    inputfastq = handle_compressed_input(fastq)\n    return ut.reduce_memory_usage(pd.DataFrame(\n        data=[res for res in extract_from_fastq(inputfastq) if res],\n        columns=[\"quals\", \"lengths\"]\n    ).dropna())", "response": "Combine metrics extracted from a plain fastq file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_from_fastq(fq):\n    for rec in SeqIO.parse(fq, \"fastq\"):\n        yield nanomath.ave_qual(rec.letter_annotations[\"phred_quality\"]), len(rec)", "response": "Extract metrics from a fastq file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stream_fastq_full(fastq, threads):\n    logging.info(\"Nanoget: Starting to collect full metrics from plain fastq file.\")\n    inputfastq = handle_compressed_input(fastq)\n    with cfutures.ProcessPoolExecutor(max_workers=threads) as executor:\n        for results in executor.map(extract_all_from_fastq, SeqIO.parse(inputfastq, \"fastq\")):\n            yield results\n    logging.info(\"Nanoget: Finished collecting statistics from plain fastq file.\")", "response": "Generator for returning metrics extracted from a fastq file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_all_from_fastq(rec):\n    return (rec.id,\n            len(rec),\n            nanomath.ave_qual(rec.letter_annotations[\"phred_quality\"]),\n            nanomath.median_qual(rec.letter_annotations[\"phred_quality\"]))", "response": "Extract metrics from a fastq file.\n            is a dict with the keys id read length average quality and median quality."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_fastq_rich(fastq, **kwargs):\n    logging.info(\"Nanoget: Starting to collect statistics from rich fastq file.\")\n    inputfastq = handle_compressed_input(fastq)\n    res = []\n    for record in SeqIO.parse(inputfastq, \"fastq\"):\n        try:\n            read_info = info_to_dict(record.description)\n            res.append(\n                (nanomath.ave_qual(record.letter_annotations[\"phred_quality\"]),\n                 len(record),\n                 read_info[\"ch\"],\n                 read_info[\"start_time\"],\n                 read_info[\"runid\"]))\n        except KeyError:\n            logging.error(\"Nanoget: keyerror when processing record {}\".format(record.description))\n            sys.exit(\"Unexpected fastq identifier:\\n{}\\n\\n \\\n            missing one or more of expected fields 'ch', 'start_time' or 'runid'\".format(\n                record.description))\n    df = pd.DataFrame(\n        data=res,\n        columns=[\"quals\", \"lengths\", \"channelIDs\", \"timestamp\", \"runIDs\"]).dropna()\n    df[\"channelIDs\"] = df[\"channelIDs\"].astype(\"int64\")\n    return ut.reduce_memory_usage(df)", "response": "Extract metrics from a richer fastq file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fq_minimal(fq):\n    try:\n        while True:\n            time = next(fq)[1:].split(\" \")[4][11:-1]\n            length = len(next(fq))\n            next(fq)\n            next(fq)\n            yield time, length\n    except StopIteration:\n        yield None", "response": "Minimal fastq metrics extractor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_fastq_minimal(fastq, **kwargs):\n    infastq = handle_compressed_input(fastq)\n    try:\n        df = pd.DataFrame(\n            data=[rec for rec in fq_minimal(infastq) if rec],\n            columns=[\"timestamp\", \"lengths\"]\n        )\n    except IndexError:\n        logging.error(\"Fatal: Incorrect file structure for fastq_minimal\")\n        sys.exit(\"Error: file does not match expected structure for fastq_minimal\")\n    return ut.reduce_memory_usage(df)", "response": "Swiftly extract minimal features from a rich fastq file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the appropriate object for the given piece.", "response": "def _get_piece(string, index):\n    \"\"\"\n    Returns Piece subclass given index of piece.\n\n    :type: index: int\n    :type: loc Location\n\n    :raise: KeyError\n    \"\"\"\n    piece = string[index].strip()\n    piece = piece.upper()\n    piece_dict = {'R': Rook,\n                  'P': Pawn,\n                  'B': Bishop,\n                  'N': Knight,\n                  'Q': Queen,\n                  'K': King}\n    try:\n        return piece_dict[piece]\n    except KeyError:\n        raise ValueError(\"Piece {} is invalid\".format(piece))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incomplete_alg(alg_str, input_color, position):\n    edge_rank = 0 \\\n        if input_color == color.white \\\n        else 7\n\n    if alg_str is None or len(alg_str) <= 1:\n        raise ValueError(\"algebraic string {} is invalid\".format(alg_str))\n\n    # King-side castle\n    if alg_str in [\"00\", \"oo\", \"OO\", \"0-0\", \"o-o\", \"O-O\"]:\n        return Move(end_loc=Location(edge_rank, 6),\n                    piece=King(input_color, Location(edge_rank, 4)),\n                    status=notation_const.KING_SIDE_CASTLE,\n                    start_loc=Location(edge_rank, 4))\n\n    # Queen-side castle\n    if alg_str in [\"000\", \"ooo\", \"OOO\", \"0-0-0\", \"o-o-o\", \"O-O-O\"]:\n        return Move(end_loc=Location(edge_rank, 2),\n                    piece=King(input_color, Location(edge_rank, 4)),\n                    status=notation_const.QUEEN_SIDE_CASTLE,\n                    start_loc=Location(edge_rank, 4))\n    try:\n        end_location = Location.from_string(alg_str[-2:])\n    except ValueError:\n        end_location = Location.from_string(alg_str[-4:-2])\n\n    # Pawn movement\n    if len(alg_str) == 2:\n        possible_pawn = position.piece_at_square(end_location.shift_back(input_color))\n        if type(possible_pawn) is Pawn and \\\n                possible_pawn.color == input_color:\n            start_location = end_location.shift_back(input_color)\n        else:\n            start_location = end_location.shift_back(input_color, times=2)\n        return Move(end_loc=end_location,\n                    piece=position.piece_at_square(start_location),\n                    status=notation_const.MOVEMENT,\n                    start_loc=start_location)\n\n    # Non-pawn Piece movement\n    if len(alg_str) == 3:\n        possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                   input_color,\n                                                                   _get_piece(alg_str, 0),\n                                                                   position)\n        return Move(end_loc=end_location,\n                    piece=possible_piece,\n                    status=notation_const.MOVEMENT,\n                    start_loc=start_location)\n\n    # Multiple options (Capture or Piece movement with file specified)\n    if len(alg_str) == 4:\n\n        # Capture\n        if alg_str[1].upper() == \"X\":\n\n            # Pawn capture\n            if not alg_str[0].isupper():\n                pawn_location = Location(end_location.rank, ord(alg_str[0]) - 97).shift_back(input_color)\n                possible_pawn = position.piece_at_square(pawn_location)\n                if type(possible_pawn) is Pawn and \\\n                        possible_pawn.color == input_color:\n                    en_passant_pawn = position.piece_at_square(end_location.shift_back(input_color))\n                    if type(en_passant_pawn) is Pawn and \\\n                            en_passant_pawn.color != input_color and \\\n                            position.is_square_empty(end_location):\n                        return Move(end_loc=end_location,\n                                    piece=position.piece_at_square(pawn_location),\n                                    status=notation_const.EN_PASSANT,\n                                    start_loc=pawn_location)\n                    else:\n                        return Move(end_loc=end_location,\n                                    piece=position.piece_at_square(pawn_location),\n                                    status=notation_const.CAPTURE,\n                                    start_loc=pawn_location)\n\n            # Piece capture\n            elif alg_str[0].isupper():\n                possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                           input_color,\n                                                                           _get_piece(alg_str, 0),\n                                                                           position)\n                return Move(end_loc=end_location,\n                            piece=possible_piece,\n                            status=notation_const.CAPTURE,\n                            start_loc=start_location)\n\n        # Pawn Promotion\n        elif alg_str[2] == \"=\":\n            promote_end_loc = Location.from_string(alg_str[:2])\n            if promote_end_loc.rank != 0 and promote_end_loc.rank != 7:\n                raise ValueError(\"Promotion {} must be on the last rank\".format(alg_str))\n            return Move(end_loc=promote_end_loc,\n                        piece=Pawn(input_color, promote_end_loc),\n                        status=notation_const.PROMOTE,\n                        promoted_to_piece=_get_piece(alg_str, 3),\n                        start_loc=promote_end_loc.shift_back(input_color))\n\n        # Non-pawn Piece movement with file specified (aRb7)\n        elif alg_str[1].isupper() and not alg_str[0].isdigit():\n            possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                       input_color,\n                                                                       _get_piece(alg_str, 1),\n                                                                       position,\n                                                                       start_file=alg_str[0])\n            return Move(end_loc=end_location,\n                        piece=possible_piece,\n                        status=notation_const.MOVEMENT,\n                        start_loc=start_location)\n\n        # (alt) Non-pawn Piece movement with file specified (Rab7)\n        elif alg_str[0].isupper() and not alg_str[1].isdigit():\n            possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                       input_color,\n                                                                       _get_piece(alg_str, 0),\n                                                                       position,\n                                                                       start_file=alg_str[1])\n            return Move(end_loc=end_location,\n                        piece=possible_piece,\n                        status=notation_const.MOVEMENT,\n                        start_loc=start_location)\n\n        # Non-pawn Piece movement with rank specified (R1b7)\n        elif alg_str[0].isupper() and alg_str[1].isdigit():\n            possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                       input_color,\n                                                                       _get_piece(alg_str, 0),\n                                                                       position,\n                                                                       start_rank=alg_str[1])\n            return Move(end_loc=end_location,\n                        piece=possible_piece,\n                        status=notation_const.MOVEMENT,\n                        start_loc=start_location)\n\n    # Multiple options\n    if len(alg_str) == 5:\n\n        # Non-pawn Piece movement with rank and file specified (a2Ra1\n        if not alg_str[0].isdigit() and \\\n                alg_str[1].isdigit() and \\\n                alg_str[2].isupper() and \\\n                not alg_str[3].isdigit() and \\\n                alg_str[4].isdigit:\n            start_loc = Location.from_string(alg_str[:2])\n            return Move(end_loc=end_location,\n                        piece=_get_piece(alg_str, 2)(input_color, end_location),\n                        status=notation_const.MOVEMENT,\n                        start_loc=start_loc)\n\n        # Multiple Piece capture options\n        if alg_str[2].upper() == \"X\":\n\n            # Piece capture with rank specified (R1xa1)\n            if alg_str[1].isdigit():\n                possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                           input_color,\n                                                                           _get_piece(alg_str, 0),\n                                                                           position,\n                                                                           start_rank=alg_str[1])\n                return Move(end_loc=end_location,\n                            piece=possible_piece,\n                            status=notation_const.CAPTURE,\n                            start_loc=start_location)\n\n            # Piece capture with file specified (Rdxd7)\n            else:\n                possible_piece, start_location = _get_piece_start_location(end_location,\n                                                                           input_color,\n                                                                           _get_piece(alg_str, 0),\n                                                                           position,\n                                                                           start_file=alg_str[1])\n                return Move(end_loc=end_location,\n                            piece=possible_piece,\n                            status=notation_const.CAPTURE,\n                            start_loc=start_location)\n\n    # Pawn promotion with capture\n    if len(alg_str) == 6 and alg_str[4] == \"=\":\n        start_file = ord(alg_str[0]) - 97\n        promote_capture_end_loc = Location.from_string(alg_str[2:4])\n        return Move(end_loc=promote_capture_end_loc,\n                    piece=Pawn(input_color, promote_capture_end_loc),\n                    status=notation_const.CAPTURE_AND_PROMOTE,\n                    promoted_to_piece=_get_piece(alg_str, 5),\n                    start_loc=Location(end_location.shift_back(input_color).rank, start_file))\n\n    raise ValueError(\"algebraic string {} is invalid in \\n{}\".format(alg_str, position))", "response": "Converts a string written in short algebraic form into an incomplete move."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an incomplete move into a complete move with the most likely starting point specified.", "response": "def make_legal(move, position):\n    \"\"\"\n    Converts an incomplete move (initial ``Location`` not specified)\n    and the corresponding position into the a complete move\n    with the most likely starting point specified. If no moves match, ``None``\n    is returned.\n\n    :type: move: Move\n    :type: position: Board\n    :rtype: Move\n    \"\"\"\n    assert isinstance(move, Move)\n    for legal_move in position.all_possible_moves(move.color):\n\n        if move.status == notation_const.LONG_ALG:\n            if move.end_loc == legal_move.end_loc and \\\n                    move.start_loc == legal_move.start_loc:\n                return legal_move\n\n        elif move == legal_move:\n            return legal_move\n\n    raise ValueError(\"Move {} not legal in \\n{}\".format(repr(move), position))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef short_alg(algebraic_string, input_color, position):\n    return make_legal(incomplete_alg(algebraic_string, input_color, position), position)", "response": "This function converts a string written in short algebraic form into a valid move in the alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a string written in long algebraic form and the corresponding position into a complete move .", "response": "def long_alg(alg_str, position):\n    \"\"\"\n    Converts a string written in long algebraic form\n    and the corresponding position into a complete move\n    (initial location specified). Used primarily for\n    UCI, but can be used for other purposes.\n\n    :type: alg_str: str\n    :type: position: Board\n    :rtype: Move\n    \"\"\"\n    if alg_str is None or len(alg_str) < 4 or len(alg_str) > 6:\n        raise ValueError(\"Invalid string input {}\".format(alg_str))\n\n    end = Location.from_string(alg_str[2:])\n    start = Location.from_string(alg_str[:2])\n    piece = position.piece_at_square(start)\n\n    if len(alg_str) == 4:\n        return make_legal(Move(end_loc=end,\n                               piece=piece,\n                               status=notation_const.LONG_ALG,\n                               start_loc=start), position)\n\n    promoted_to = _get_piece(alg_str, 4)\n    if promoted_to is None or \\\n            promoted_to is King or \\\n            promoted_to is Pawn:\n        raise Exception(\"Invalid move input\")\n\n    return make_legal(Move(end_loc=end,\n                           piece=piece,\n                           status=notation_const.LONG_ALG,\n                           start_loc=start,\n                           promoted_to_piece=promoted_to), position)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting hyb and neighbors marks to atoms.", "response": "def reset_query_marks(self):\n        \"\"\"\n        set or reset hyb and neighbors marks to atoms.\n        \"\"\"\n        for i, atom in self.atoms():\n            neighbors = 0\n            hybridization = 1\n            # hybridization 1- sp3; 2- sp2; 3- sp1; 4- aromatic\n            for j, bond in self._adj[i].items():\n                if self._node[j].element != 'H':\n                    neighbors += 1\n\n                if hybridization in (3, 4):\n                    continue\n                order = bond.order\n                if order == 4:\n                    hybridization = 4\n                elif order == 3:\n                    hybridization = 3\n                elif order == 2:\n                    if hybridization == 2:\n                        hybridization = 3\n                    else:\n                        hybridization = 2\n\n            atom._neighbors = neighbors\n            atom._hybridization = hybridization\n        self.flush_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving explicit hydrogen if possible :return: number of removed hydrogens", "response": "def implicify_hydrogens(self):\n        \"\"\"\n        remove explicit hydrogen if possible\n\n        :return: number of removed hydrogens\n        \"\"\"\n        explicit = defaultdict(list)\n        c = 0\n        for n, atom in self.atoms():\n            if atom.element == 'H':\n                for m in self.neighbors(n):\n                    if self._node[m].element != 'H':\n                        explicit[m].append(n)\n\n        for n, h in explicit.items():\n            atom = self._node[n]\n            len_h = len(h)\n            for i in range(len_h, 0, -1):\n                hi = h[:i]\n                if atom.get_implicit_h([y.order for x, y in self._adj[n].items() if x not in hi]) == i:\n                    for x in hi:\n                        self.remove_node(x)\n                        c += 1\n                    break\n\n        self.flush_cache()\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef explicify_hydrogens(self):\n        tmp = []\n        for n, atom in self.atoms():\n            if atom.element != 'H':\n                for _ in range(atom.get_implicit_h([x.order for x in self._adj[n].values()])):\n                    tmp.append(n)\n        for n in tmp:\n            self.add_bond(n, self.add_atom(H), Bond())\n\n        self.flush_cache()\n        return len(tmp)", "response": "add explicit hydrogens to atoms\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef substructure(self, atoms, meta=False, as_view=True):\n        s = super().substructure(atoms, meta, as_view)\n        if as_view:\n            s.check_valence = s.explicify_hydrogens = s.implicify_hydrogens = s.reset_query_marks = frozen\n            s.standardize = s.aromatize = frozen\n        return s", "response": "create substructure containing atoms from nbunch list\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks valences of all atoms in the environment", "response": "def check_valence(self):\n        \"\"\"\n        check valences of all atoms\n\n        :return: list of invalid atoms\n        \"\"\"\n        return [x for x, atom in self.atoms() if not atom.check_valence(self.environment(x))]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning VF2 if self is a cgr - cgr container and other is a cgr - cgr container", "response": "def _matcher(self, other):\n        \"\"\"\n        return VF2 GraphMatcher\n\n        MoleculeContainer < MoleculeContainer\n        MoleculeContainer < CGRContainer\n        \"\"\"\n        if isinstance(other, (self._get_subclass('CGRContainer'), MoleculeContainer)):\n            return GraphMatcher(other, self, lambda x, y: x == y, lambda x, y: x == y)\n        raise TypeError('only cgr-cgr possible')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_datetime(date):\n    return datetime.datetime.combine(date, datetime.datetime.min.time())", "response": "Turn a date into a datetime at midnight."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_size_changes(self, issue):\n\n        # Find the first size change, if any\n        try:\n            size_changes = list(filter(lambda h: h.field == 'Story Points',\n                                       itertools.chain.from_iterable([c.items for c in issue.changelog.histories])))\n        except AttributeError:\n            return\n\n        # If we have no size changes and the issue has a current size then a size must have ben specified at issue creation time.\n        # Return the size at creation time\n\n        try:\n            current_size = issue.fields.__dict__[self.fields['StoryPoints']]\n        except:\n            current_size = None\n\n        size = (size_changes[0].fromString) if len(size_changes)  else current_size\n\n        # Issue was created\n        yield IssueSizeSnapshot(\n            change=None,\n            key=issue.key,\n            date=dateutil.parser.parse(issue.fields.created),\n            size=size\n        )\n\n        for change in issue.changelog.histories:\n            change_date = dateutil.parser.parse(change.created)\n\n            #sizes = list(filter(lambda i: i.field == 'Story Points', change.items))\n            #is_resolved = (sizes[-1].to is not None) if len(sizes) > 0 else is_resolved\n\n            for item in change.items:\n                if item.field == 'Story Points':\n                    # StoryPoints value was changed\n                    size = item.toString\n                    yield IssueSizeSnapshot(\n                        change=item.field,\n                        key=issue.key,\n                        date=change_date,\n                        size=size\n                    )", "response": "Yields an IssueSizeSnapshot for each time the issue size changed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding an IssueSnapshot for each time the issue changed status or resolution", "response": "def iter_changes(self, issue, include_resolution_changes=True):\n        \"\"\"Yield an IssueSnapshot for each time the issue changed status or\n        resolution\n        \"\"\"\n\n        is_resolved = False\n\n        # Find the first status change, if any\n        try:\n            status_changes = list(filter(\n                lambda h: h.field == 'status',\n                itertools.chain.from_iterable([c.items for c in issue.changelog.histories])))\n        except AttributeError:\n            return\n        last_status = status_changes[0].fromString if len(status_changes) > 0 else issue.fields.status.name\n        last_resolution = None\n\n        # Issue was created\n        yield IssueSnapshot(\n            change=None,\n            key=issue.key,\n            date=dateutil.parser.parse(issue.fields.created),\n            status=last_status,\n            resolution=None,\n            is_resolved=is_resolved\n        )\n\n        for change in issue.changelog.histories:\n            change_date = dateutil.parser.parse(change.created)\n\n            resolutions = list(filter(lambda i: i.field == 'resolution', change.items))\n            is_resolved = (resolutions[-1].to is not None) if len(resolutions) > 0 else is_resolved\n\n            for item in change.items:\n                if item.field == 'status':\n                    # Status was changed\n                    last_status = item.toString\n                    yield IssueSnapshot(\n                        change=item.field,\n                        key=issue.key,\n                        date=change_date,\n                        status=last_status,\n                        resolution=last_resolution,\n                        is_resolved=is_resolved\n                    )\n                elif item.field == 'resolution':\n                    last_resolution = item.toString\n                    if include_resolution_changes:\n                        yield IssueSnapshot(\n                            change=item.field,\n                            key=issue.key,\n                            date=change_date,\n                            status=last_status,\n                            resolution=last_resolution,\n                            is_resolved=is_resolved\n                        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_issues(self, criteria={}, jql=None, order='KEY ASC', verbose=False, changelog=True):\n\n        query = []\n\n        if criteria.get('project', False):\n            query.append('project IN (%s)' % ', '.join(['\"%s\"' % p for p in criteria['project']]))\n\n        if criteria.get('issue_types', False):\n            query.append('issueType IN (%s)' % ', '.join(['\"%s\"' % t for t in criteria['issue_types']]))\n\n        if criteria.get('valid_resolutions', False):\n            query.append('(resolution IS EMPTY OR resolution IN (%s))' % ', '.join(['\"%s\"' % r for r in criteria['valid_resolutions']]))\n\n        if criteria.get('jql_filter') is not None:\n            query.append('(%s)' % criteria['jql_filter'])\n\n        if jql is not None:\n            query.append('(%s)' % jql)\n\n        queryString = \"%s ORDER BY %s\" % (' AND '.join(query), order,)\n\n        if verbose:\n            print(\"Fetching issues with query:\", queryString)\n\n        fromRow=0\n        issues = []\n        while True:\n            try:\n                if changelog:\n                    pageofissues = self.jira.search_issues(queryString, expand='changelog', maxResults=self.settings['max_results'],startAt=fromRow)\n                else:\n                    pageofissues = self.jira.search_issues(queryString, maxResults=self.settings['max_results'],startAt=fromRow)\n\n                fromRow = fromRow + int(self.settings['max_results'])\n                issues += pageofissues\n                if verbose:\n                    print(\"Got %s lines per jira query from result starting at line number %s \" % (self.settings['max_results'],  fromRow))\n                if len(pageofissues)==0:\n                    break\n            except JIRAError as e:\n                print(\"Jira query error with: {}\\n{}\".format(queryString, e))\n                return []\n\n\n        if verbose:\n            print(\"Fetched\", len(issues), \"issues\")\n\n        return issues", "response": "Return a list of issues with changelog metadata."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_catalogs(self):\n        _form = CatalogSelectForm(current=self.current)\n        _form.set_choices_of('catalog', [(i, i) for i in fixture_bucket.get_keys()])\n        self.form_out(_form)", "response": "Lists existing catalogs respect to ui view template format\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting existing catalog and fill the form with the model data.", "response": "def get_catalog(self):\n        \"\"\"\n        Get existing catalog and fill the form with the model data.\n        If given key not found as catalog, it generates an empty catalog data form.\n        \"\"\"\n\n        catalog_data = fixture_bucket.get(self.input['form']['catalog'])\n\n        # define add or edit based on catalog data exists\n        add_or_edit = \"Edit\" if catalog_data.exists else \"Add\"\n\n        # generate form\n        catalog_edit_form = CatalogEditForm(\n            current=self.current,\n            title='%s: %s' % (add_or_edit, self.input['form']['catalog']))\n\n        # add model data to form\n        if catalog_data.exists:\n            if type(catalog_data.data) == list:\n                # if catalog data is an array it means no other language of value defined, therefor the value is turkish\n                for key, data in enumerate(catalog_data.data):\n                    catalog_edit_form.CatalogDatas(catalog_key=key or \"0\", en='', tr=data)\n            if type(catalog_data.data) == dict:\n                for key, data in catalog_data.data.items():\n                    catalog_edit_form.CatalogDatas(catalog_key=key, en=data['en'], tr=data['tr'])\n\n        else:\n            catalog_edit_form.CatalogDatas(catalog_key=\"0\", en='', tr='')\n\n        self.form_out(catalog_edit_form)\n\n        # schema key for get back what key will be saved, used in save_catalog form\n        self.output[\"object_key\"] = self.input['form']['catalog']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the catalog data to given key Cancels if the cmd is cancel", "response": "def save_catalog(self):\n        \"\"\"\n        Saves the catalog data to given key\n        Cancels if the cmd is cancel\n        Notifies user with the process.\n        \"\"\"\n        if self.input[\"cmd\"] == 'save_catalog':\n            try:\n                edited_object = dict()\n                for i in self.input[\"form\"][\"CatalogDatas\"]:\n                    edited_object[i[\"catalog_key\"]] = {\"en\": i[\"en\"], \"tr\": i[\"tr\"]}\n\n                newobj = fixture_bucket.get(self.input[\"object_key\"])\n                newobj.data = edited_object\n                newobj.store()\n\n                # notify user by passing notify in output object\n                self.output[\"notify\"] = \"catalog: %s successfully updated.\" % self.input[\n                    \"object_key\"]\n            except:\n                raise HTTPError(500, \"Form object could not be saved\")\n        if self.input[\"cmd\"] == 'cancel':\n            self.output[\"notify\"] = \"catalog: %s canceled.\" % self.input[\"object_key\"]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef date_to_solr(d):\n    return \"{y}-{m}-{day}T00:00:00Z\".format(day=d[:2], m=d[3:5], y=d[6:]) if d else d", "response": "converts a date to Solr date"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solr_to_date(d):\n    return \"{day}:{m}:{y}\".format(y=d[:4], m=d[5:7], day=d[8:10]) if d else d", "response": "converts YYYY - MM - DDT00 : 00Z to DD - MM - YYYY"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_safe_str(s):\n    # TODO: This is insufficient as it doesn't do anything for other non-ascii chars\n    return re.sub(r'[^0-9a-zA-Z]+', '_', s.strip().replace(u'\u011f', 'g').replace(u'\u00f6', 'o').replace(\n        u'\u00e7', 'c').replace(u'\u00c7','c').replace(u'\u00d6', u'O').replace(u'\u015e', 's').replace(\n        u'\u00dc', 'u').replace(u'\u0131', 'i').replace(u'\u0130','i').replace(u'\u011e', 'g').replace(\n        u'\u00f6', 'o').replace(u'\u015f', 's').replace(u'\u00fc', 'u').lower(), re.UNICODE)", "response": "Convert some non - ascii chars to ascii counterparts and return the result as lowercase\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_truthy(*dicts):\n    merged = {}\n    for d in dicts:\n        for k, v in d.items():\n            merged[k] = v or merged.get(k, v)\n    return merged", "response": "Merge multiple dictionaries keeping the truthy values in case of key collisions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming the version upgrade on the database.", "response": "def perform(self):\n        \"\"\"Perform the version upgrade on the database.\n        \"\"\"\n        db_versions = self.table.versions()\n\n        version = self.version\n        if (version.is_processed(db_versions) and\n                not self.config.force_version == self.version.number):\n            self.log(\n                u'version {} is already installed'.format(version.number)\n            )\n            return\n\n        self.start()\n        try:\n            self._perform_version(version)\n        except Exception:\n            if sys.version_info < (3, 4):\n                msg = traceback.format_exc().decode('utf8', errors='ignore')\n            else:\n                msg = traceback.format_exc()\n            error = u'\\n'.join(self.logs + [u'\\n', msg])\n            self.table.record_log(version.number, error)\n            raise\n        self.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs out view. Simply deletes the session object. For showing logout message: 'show_logout_message' field should be True in current.task_data, Message should be sent in current.task_data with 'logout_message' field. Message title should be sent in current.task_data with 'logout_title' field. current.task_data['show_logout_message'] = True current.task_data['logout_title'] = 'Message Title' current.task_data['logout_message'] = 'Message' Args: current: :attr:`~zengine.engine.WFCurrent` object.", "response": "def logout(current):\n    \"\"\"\n    Log out view.\n    Simply deletes the session object.\n    For showing logout message:\n        'show_logout_message' field should be True in current.task_data,\n        Message should be sent in current.task_data with 'logout_message' field.\n        Message title should be sent in current.task_data with 'logout_title' field.\n\n        current.task_data['show_logout_message'] = True\n        current.task_data['logout_title'] = 'Message Title'\n        current.task_data['logout_message'] = 'Message'\n\n    Args:\n        current: :attr:`~zengine.engine.WFCurrent` object.\n    \"\"\"\n    current.user.is_online(False)\n    current.session.delete()\n    current.output['cmd'] = 'logout'\n    if current.task_data.get('show_logout_message', False):\n        current.output['title'] = current.task_data.get('logout_title', None)\n        current.output['msg'] = current.task_data.get('logout_message', None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _do_upgrade(self):\n        self.current.output['cmd'] = 'upgrade'\n        self.current.output['user_id'] = self.current.user_id\n        self.terminate_existing_login()\n        self.current.user.bind_private_channel(self.current.session.sess_id)\n        user_sess = UserSessionID(self.current.user_id)\n        user_sess.set(self.current.session.sess_id)\n        self.current.user.is_online(True)\n        # Clean up the locale from session to allow it to be re-read from the user preferences after login\n        for k in translation.DEFAULT_PREFS.keys():\n            self.current.session[k] = ''", "response": "Open websocket connection and upgrade the user preferences"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_view(self):\n        self.current.output['login_process'] = True\n        self.current.task_data['login_successful'] = False\n        if self.current.is_auth:\n            self._do_upgrade()\n        else:\n            try:\n                auth_result = self.current.auth.authenticate(\n                    self.current.input['username'],\n                    self.current.input['password'])\n                self.current.task_data['login_successful'] = auth_result\n                if auth_result:\n                    self._do_upgrade()\n            except ObjectDoesNotExist:\n                self.current.log.exception(\"Wrong username or another error occurred\")\n                pass\n            except:\n                raise\n            if self.current.output.get('cmd') != 'upgrade':\n                self.current.output['status_code'] = 403\n            else:\n                KeepAlive(self.current.user_id).reset()", "response": "Authenticate user with given credentials and connect user s queue and exchange."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the view of the login form.", "response": "def show_view(self):\n        \"\"\"\n        Show :attr:`LoginForm` form.\n        \"\"\"\n        self.current.output['login_process'] = True\n        if self.current.is_auth:\n            self._do_upgrade()\n        else:\n            self.current.output['forms'] = LoginForm(current=self.current).serialize()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nskip all the non - duplicate entries in the sequence", "response": "def skip(mapping):\n    \"\"\"\n    :param mapping: generator\n    :return: filtered generator\n    \"\"\"\n    found = set()\n    for m in mapping:\n        matched_atoms = set(m.values())\n        if found.intersection(matched_atoms):\n            continue\n        found.update(matched_atoms)\n        yield m"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_mapping(self, structures):\n        for c in permutations(structures, len(self.__patterns)):\n            for m in product(*(x.get_substructure_mapping(y, limit=0) for x, y in zip(self.__patterns, c))):\n                mapping = {}\n                for i in m:\n                    mapping.update(i)\n                if mapping:\n                    yield mapping", "response": "Returns a generator of all possible mappings for each pattern to each molecule."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, default=None):\n        d = cache.get(self.key)\n        return ((json.loads(d.decode('utf-8')) if self.serialize else d)\n                if d is not None\n                else default)", "response": "get the cached value or default if it can t be found"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, val, lifetime=None):\n        cache.set(self.key,\n                  (json.dumps(val) if self.serialize else val),\n                  lifetime or settings.DEFAULT_CACHE_EXPIRE_TIME)\n        return val", "response": "set cache value\n\n        :param val: any picklable object\n        :param lifetime: exprition time in sec\n        :return: val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, val):\n        return cache.lpush(self.key, json.dumps(val) if self.serialize else val)", "response": "Adds given value to the item list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all(self):\n        result = cache.lrange(self.key, 0, -1)\n        return (json.loads(item.decode('utf-8')) for item in result if\n                item) if self.serialize else result", "response": "Get all list items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving given item from the list.", "response": "def remove_item(self, val):\n        \"\"\"\n        Removes given item from the list.\n\n        Args:\n            val: Item\n\n        Returns:\n            Cache backend response.\n        \"\"\"\n        return cache.lrem(self.key, json.dumps(val))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(cls, *args):\n        return _remove_keys([], [(cls._make_key(args) if args else cls.PREFIX) + '*'])", "response": "Removes all keys of this namespace\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_or_expire_session(self):\n        if not hasattr(self, 'key'):\n            return\n        now = time.time()\n        timestamp = float(self.get() or 0) or now\n        sess_id = self.sess_id or UserSessionID(self.user_id).get()\n        if sess_id and now - timestamp > self.SESSION_EXPIRE_TIME:\n            Session(sess_id).delete()\n            return False\n        else:\n            self.set(now)\n            return True", "response": "Updates the keepalive request if the keepalive request expired otherwise updates the keepalive timestamp value if the keepalive request expired otherwise updates the keepalive timestamp value if the keepalive request expired otherwise returns False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_message_for_lane_change(sender, **kwargs):\n    current = kwargs['current']\n    owners = kwargs['possible_owners']\n    if 'lane_change_invite' in current.task_data:\n        msg_context = current.task_data.pop('lane_change_invite')\n    else:\n        msg_context = DEFAULT_LANE_CHANGE_INVITE_MSG\n\n    wfi = WFCache(current).get_instance()\n\n    # Deletion of used passive task invitation which belongs to previous lane.\n    TaskInvitation.objects.filter(instance=wfi, role=current.role, wf_name=wfi.wf.name).delete()\n\n    today = datetime.today()\n    for recipient in owners:\n        inv = TaskInvitation(\n            instance=wfi,\n            role=recipient,\n            wf_name=wfi.wf.name,\n            progress=30,\n            start_date=today,\n            finish_date=today + timedelta(15)\n        )\n        inv.title = current.task_data.get('INVITATION_TITLE') or wfi.wf.title\n        inv.save()\n\n        # try to send notification, if it fails go on\n        try:\n\n            recipient.send_notification(title=msg_context['title'],\n                                        message=\"%s %s\" % (wfi.wf.title, msg_context['body']),\n                                        typ=1,  # info\n                                        url='',\n                                        sender=sender\n                                        )\n        except: # todo: specify which exception\n            pass", "response": "Sends a message to the possible owners of the current workflows\n     next lane."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencrypts password of the user.", "response": "def set_password(sender, **kwargs):\n    \"\"\"\n    Encrypts password of the user.\n    \"\"\"\n    if sender.model_class.__name__ == 'User':\n        usr = kwargs['object']\n        if not usr.password.startswith('$pbkdf2'):\n            usr.set_password(usr.password)\n            usr.save()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef channel_choice_control(self):\n        self.current.task_data['control'], self.current.task_data['msg'] \\\n            = self.selection_error_control(self.input['form'])\n        if self.current.task_data['control']:\n            self.current.task_data['option'] = self.input['cmd']\n            self.current.task_data['split_operation'] = False\n            keys, names = self.return_selected_form_items(self.input['form']['ChannelList'])\n            self.current.task_data['chosen_channels'] = keys\n            self.current.task_data['chosen_channels_names'] = names", "response": "It controls errors. It returns channel list screen with error message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new channel", "response": "def create_new_channel(self):\n        \"\"\"\n        Features of new channel are specified like channel's name, owner etc.\n        \"\"\"\n\n        self.current.task_data['new_channel'] = True\n        _form = NewChannelForm(Channel(), current=self.current)\n        _form.title = _(u\"Specify Features of New Channel to Create\")\n        _form.forward = fields.Button(_(u\"Create\"), flow=\"find_target_channel\")\n        self.form_out(_form)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef choose_existing_channel(self):\n\n        if self.current.task_data.get('msg', False):\n            self.show_warning_messages()\n\n        _form = ChannelListForm()\n        _form.title = _(u\"Choose a Channel Which Will Be Merged With Chosen Channels\")\n\n        for channel in Channel.objects.filter(typ=15).exclude(\n                key__in=self.current.task_data['chosen_channels']):\n            owner_name = channel.owner.username\n            _form.ChannelList(choice=False, name=channel.name, owner=owner_name,\n                              key=channel.key)\n\n        _form.choose = fields.Button(_(u\"Choose\"))\n        self.form_out(_form)", "response": "This method is used to choose existing channel. It is a channel choice list and chosen channels\n        at previous step should not be on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_channel(self):\n\n        if self.current.task_data.get('msg', False):\n            self.show_warning_messages()\n\n        self.current.task_data['split_operation'] = True\n        channel = Channel.objects.get(self.current.task_data['chosen_channels'][0])\n\n        _form = SubscriberListForm(title=_(u'Choose Subscribers to Migrate'))\n\n        for subscriber in Subscriber.objects.filter(channel=channel):\n            subscriber_name = subscriber.user.username\n            _form.SubscriberList(choice=False, name=subscriber_name, key=subscriber.key)\n\n        _form.new_channel = fields.Button(_(u\"Move to a New Channel\"), cmd=\"create_new_channel\")\n        _form.existing_channel = fields.Button(_(u\"Move to an Existing Channel\"),\n                                               cmd=\"choose_existing_channel\")\n        self.form_out(_form)", "response": "A channel can be splitted to new or other existing channel. It creates subscribers list as selectable to move."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove all the chosen channels and their subscribers to the new channel or existing channel.", "response": "def move_complete_channel(self):\n        \"\"\"\n        Channels and theirs subscribers are moved\n        completely to new channel or existing channel.\n        \"\"\"\n\n        to_channel = Channel.objects.get(self.current.task_data['target_channel_key'])\n        chosen_channels = self.current.task_data['chosen_channels']\n        chosen_channels_names = self.current.task_data['chosen_channels_names']\n\n        with BlockSave(Subscriber, query_dict={'channel_id': to_channel.key}):\n            for s in Subscriber.objects.filter(channel_id__in=chosen_channels, typ=15):\n                s.channel = to_channel\n                s.save()\n\n        with BlockDelete(Message):\n            Message.objects.filter(channel_id__in=chosen_channels, typ=15).delete()\n\n        with BlockDelete(Channel):\n            Channel.objects.filter(key__in=chosen_channels).delete()\n\n        self.current.task_data[\n            'msg'] = _(u\"Chosen channels(%s) have been merged to '%s' channel successfully.\") % \\\n                     (', '.join(chosen_channels_names), to_channel.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving chosen subscribers and messages of them to new channel.", "response": "def move_chosen_subscribers(self):\n        \"\"\"\n        After splitting operation, only chosen subscribers\n        are moved to new channel or existing channel.\n        \"\"\"\n        from_channel = Channel.objects.get(self.current.task_data['chosen_channels'][0])\n        to_channel = Channel.objects.get(self.current.task_data['target_channel_key'])\n\n        with BlockSave(Subscriber, query_dict={'channel_id': to_channel.key}):\n            for subscriber in Subscriber.objects.filter(\n                    key__in=self.current.task_data['chosen_subscribers']):\n                subscriber.channel = to_channel\n                subscriber.save()\n\n        if self.current.task_data['new_channel']:\n            self.copy_and_move_messages(from_channel, to_channel)\n\n        self.current.task_data[\n            'msg'] = _(u\"Chosen subscribers and messages of them migrated from '%s' channel to \"\n                       u\"'%s' channel successfully.\") % (from_channel.name, to_channel.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_and_move_messages(from_channel, to_channel):\n        with BlockSave(Message, query_dict={'channel_id': to_channel.key}):\n            for message in Message.objects.filter(channel=from_channel, typ=15):\n                message.key = ''\n                message.channel = to_channel\n                message.save()", "response": "Copy messages from one channel to another."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show_warning_messages(self, title=_(u\"Incorrect Operation\"), box_type='warning'):\n        msg = self.current.task_data['msg']\n        self.current.output['msgbox'] = {'type': box_type, \"title\": title, \"msg\": msg}\n        del self.current.task_data['msg']", "response": "It shows incorrect operations or successful operation messages."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eratosthenes():\n    d = {}  # map each composite integer to its first-found prime factor\n    for q in count(2):  # q gets 2, 3, 4, 5, ... ad infinitum\n        p = d.pop(q, None)\n        if p is None:\n            # q not a key in D, so q is prime, therefore, yield it\n            yield q\n            # mark q squared as not-prime (with q as first-found prime factor)\n            d[q * q] = q\n        else:\n            # let x <- smallest (N*p)+q which wasn't yet known to be composite\n            # we just learned x is composite, with p first-found prime factor,\n            # since p is the first-found prime factor of q -- find and mark it\n            x = p + q\n            while x in d:\n                x += p\n            d[x] = p", "response": "Yields the sequence of prime numbers via the Sieve of Eratosthenes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef atoms_order(self):\n        if not len(self):  # for empty containers\n            return {}\n        elif len(self) == 1:  # optimize single atom containers\n            return dict.fromkeys(self, 2)\n\n        params = {n: (int(node), tuple(sorted(int(edge) for edge in self._adj[n].values())))\n                  for n, node in self.atoms()}\n        newlevels = {}\n        countprime = iter(primes)\n        weights = {x: newlevels.get(y) or newlevels.setdefault(y, next(countprime))\n                   for x, y in sorted(params.items(), key=itemgetter(1))}\n\n        tries = len(self) * 4\n\n        numb = len(set(weights.values()))\n        stab = 0\n\n        while tries:\n            oldnumb = numb\n            neweights = {}\n            countprime = iter(primes)\n\n            # weights[n] ** 2 NEED for differentiation of molecules like A-B or any other complete graphs.\n            tmp = {n: reduce(mul, (weights[x] for x in m), weights[n] ** 2) for n, m in self._adj.items()}\n\n            weights = {x: (neweights.get(y) or neweights.setdefault(y, next(countprime)))\n                       for x, y in sorted(tmp.items(), key=itemgetter(1))}\n\n            numb = len(set(weights.values()))\n            if numb == len(self):  # each atom now unique\n                break\n            elif numb == oldnumb:\n                x = Counter(weights.values())\n                if x[min(x)] > 1:\n                    if stab == 3:\n                        break\n                elif stab >= 2:\n                    break\n\n                stab += 1\n            elif stab:\n                stab = 0\n\n            tries -= 1\n            if not tries and numb < oldnumb:\n                warning('morgan. number of attempts exceeded. uniqueness has decreased. next attempt will be made')\n                tries = 1\n        else:\n            warning('morgan. number of attempts exceeded')\n\n        return weights", "response": "Morgan like algorithm for graph nodes ordering\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_manual(cls, pawn_value, knight_value, bishop_value, rook_value, queen_value, king_value):\n        piece_values = cls()\n        piece_values.PAWN_VALUE = pawn_value\n        piece_values.KNIGHT_VALUE = knight_value\n        piece_values.BISHOP_VALUE = bishop_value\n        piece_values.ROOK_VALUE = rook_value\n        piece_values.QUEEN_VALUE = queen_value\n        piece_values.KING_VALUE = king_value\n        return piece_values", "response": "This method is used to initialize the manual piece values for external pieces."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef val(self, piece, ref_color):\n        if piece is None:\n            return 0\n\n        if ref_color == piece.color:\n            const = 1\n        else:\n            const = -1\n\n        if isinstance(piece, Pawn):\n            return self.PAWN_VALUE * const\n        elif isinstance(piece, Queen):\n            return self.QUEEN_VALUE * const\n        elif isinstance(piece, Bishop):\n            return self.BISHOP_VALUE * const\n        elif isinstance(piece, Rook):\n            return self.ROOK_VALUE * const\n        elif isinstance(piece, Knight):\n            return self.KNIGHT_VALUE * const\n        elif isinstance(piece, King):\n            return self.KING_VALUE * const\n        return 0", "response": "Returns the value of the key in the record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef play(self):\n        colors = [lambda: self.white_move(), lambda: self.black_move()]\n        colors = itertools.cycle(colors)\n\n        while True:\n            color_fn = next(colors)\n            if game_state.no_moves(self.position):\n                if self.position.get_king(color.white).in_check(self.position):\n                    return 1\n\n                elif self.position.get_king(color.black).in_check(self.position):\n                    return 0\n\n                else:\n                    return 0.5\n\n            color_fn()", "response": "Starts game and returns one of 3 results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef white_move(self):\n        move = self.player_white.generate_move(self.position)\n        move = make_legal(move, self.position)\n        self.position.update(move)", "response": "Calls the white player s generate_move method and updates the board with the move returned."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling the black player s generate_move method and updates the board with the move returned.", "response": "def black_move(self):\n        \"\"\"\n        Calls the black player's ``generate_move()``\n        method and updates the board with the move returned.\n        \"\"\"\n        move = self.player_black.generate_move(self.position)\n        move = make_legal(move, self.position)\n        self.position.update(move)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_field_cache(self, cache_type='es'):\n        if cache_type == 'kibana':\n            try:\n                search_results = urlopen(self.get_url).read().decode('utf-8')\n            except HTTPError:  # as e:\n                # self.pr_err(\"get_field_cache(kibana), HTTPError: %s\" % e)\n                return []\n            index_pattern = json.loads(search_results)\n            # Results look like: {\"_index\":\".kibana\",\"_type\":\"index-pattern\",\"_id\":\"aaa*\",\"_version\":6,\"found\":true,\"_source\":{\"title\":\"aaa*\",\"fields\":\"<what we want>\"}}  # noqa\n            fields_str = index_pattern['_source']['fields']\n            return json.loads(fields_str)\n        elif cache_type == 'es' or cache_type.startswith('elastic'):\n            search_results = urlopen(self.es_get_url).read().decode('utf-8')\n            es_mappings = json.loads(search_results)\n            # Results look like: {\"<index_name>\":{\"mappings\":{\"<doc_type>\":{\"<field_name>\":{\"full_name\":\"<field_name>\",\"mapping\":{\"<sub-field_name>\":{\"type\":\"date\",\"index_name\":\"<sub-field_name>\",\"boost\":1.0,\"index\":\"not_analyzed\",\"store\":false,\"doc_values\":false,\"term_vector\":\"no\",\"norms\":{\"enabled\":false},\"index_options\":\"docs\",\"index_analyzer\":\"_date/16\",\"search_analyzer\":\"_date/max\",\"postings_format\":\"default\",\"doc_values_format\":\"default\",\"similarity\":\"default\",\"fielddata\":{},\"ignore_malformed\":false,\"coerce\":true,\"precision_step\":16,\"format\":\"dateOptionalTime\",\"null_value\":null,\"include_in_all\":false,\"numeric_resolution\":\"milliseconds\",\"locale\":\"\"}}},  # noqa\n            # now convert the mappings into the .kibana format\n            field_cache = []\n            for (index_name, val) in iteritems(es_mappings):\n                if index_name != self.index:  # only get non-'.kibana' indices\n                    # self.pr_dbg(\"index: %s\" % index_name)\n                    m_dict = es_mappings[index_name]['mappings']\n                    # self.pr_dbg('m_dict %s' % m_dict)\n                    mappings = self.get_index_mappings(m_dict)\n                    # self.pr_dbg('mappings %s' % mappings)\n                    field_cache.extend(mappings)\n            field_cache = self.dedup_field_cache(field_cache)\n            return field_cache\n        self.pr_err(\"Unknown cache type: %s\" % cache_type)\n        return None", "response": "Return a list of fields mappings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_field_cache(self, field_cache):\n        index_pattern = self.field_cache_to_index_pattern(field_cache)\n        # self.pr_dbg(\"request/post: %s\" % index_pattern)\n        resp = requests.post(self.post_url, data=index_pattern).text\n        # resp = {\"_index\":\".kibana\",\"_type\":\"index-pattern\",\"_id\":\"aaa*\",\"_version\":1,\"created\":true}  # noqa\n        resp = json.loads(resp)\n        return 0", "response": "Post a list of fields to the index pattern."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a. kibana index - pattern doc_type", "response": "def field_cache_to_index_pattern(self, field_cache):\n        \"\"\"Return a .kibana index-pattern doc_type\"\"\"\n        mapping_dict = {}\n        mapping_dict['customFormats'] = \"{}\"\n        mapping_dict['title'] = self.index_pattern\n        # now post the data into .kibana\n        mapping_dict['fields'] = json.dumps(field_cache, separators=(',', ':'))\n        # in order to post, we need to create the post string\n        mapping_str = json.dumps(mapping_dict, separators=(',', ':'))\n        return mapping_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts minimum set of fields in cache does not validate contents", "response": "def check_mapping(self, m):\n        \"\"\"Assert minimum set of fields in cache, does not validate contents\"\"\"\n        if 'name' not in m:\n            self.pr_dbg(\"Missing %s\" % \"name\")\n            return False\n        # self.pr_dbg(\"Checking %s\" % m['name'])\n        for x in ['analyzed', 'indexed', 'type', 'scripted', 'count']:\n            if x not in m or m[x] == \"\":\n                self.pr_dbg(\"Missing %s\" % x)\n                self.pr_dbg(\"Full %s\" % m)\n                return False\n        if 'doc_values' not in m or m['doc_values'] == \"\":\n            if not m['name'].startswith('_'):\n                self.pr_dbg(\"Missing %s\" % \"doc_values\")\n                return False\n            m['doc_values'] = False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_index_mappings(self, index):\n        fields_arr = []\n        for (key, val) in iteritems(index):\n            # self.pr_dbg(\"\\tdoc_type: %s\" % key)\n            doc_mapping = self.get_doc_type_mappings(index[key])\n            # self.pr_dbg(\"\\tdoc_mapping: %s\" % doc_mapping)\n            if doc_mapping is None:\n                return None\n            # keep adding to the fields array\n            fields_arr.extend(doc_mapping)\n        return fields_arr", "response": "Converts all index s doc_types to. kibana"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_doc_type_mappings(self, doc_type):\n        doc_fields_arr = []\n        found_score = False\n        for (key, val) in iteritems(doc_type):\n            # self.pr_dbg(\"\\t\\tfield: %s\" % key)\n            # self.pr_dbg(\"\\tval: %s\" % val)\n            add_it = False\n            retdict = {}\n            # _ are system\n            if not key.startswith('_'):\n                if 'mapping' not in doc_type[key]:\n                    self.pr_err(\"No mapping in doc_type[%s]\" % key)\n                    return None\n                if key in doc_type[key]['mapping']:\n                    subkey_name = key\n                else:\n                    subkey_name = re.sub('.*\\.', '', key)\n                if subkey_name not in doc_type[key]['mapping']:\n                    self.pr_err(\n                        \"Couldn't find subkey \" +\n                        \"doc_type[%s]['mapping'][%s]\" % (key, subkey_name))\n                    return None\n                # self.pr_dbg(\"\\t\\tsubkey_name: %s\" % subkey_name)\n                retdict = self.get_field_mappings(\n                    doc_type[key]['mapping'][subkey_name])\n                add_it = True\n            # system mappings don't list a type,\n            # but kibana makes them all strings\n            if key in self.sys_mappings:\n                retdict['analyzed'] = False\n                retdict['indexed'] = False\n                if key == '_source':\n                    retdict = self.get_field_mappings(\n                        doc_type[key]['mapping'][key])\n                    retdict['type'] = \"_source\"\n                elif key == '_score':\n                    retdict['type'] = \"number\"\n                elif 'type' not in retdict:\n                    retdict['type'] = \"string\"\n                add_it = True\n            if add_it:\n                retdict['name'] = key\n                retdict['count'] = 0  # always init to 0\n                retdict['scripted'] = False  # I haven't observed a True yet\n                if not self.check_mapping(retdict):\n                    self.pr_err(\"Error, invalid mapping\")\n                    return None\n                # the fields element is an escaped array of json\n                # make the array here, after all collected, then escape it\n                doc_fields_arr.append(retdict)\n        if not found_score:\n            doc_fields_arr.append(\n                {\"name\": \"_score\",\n                 \"type\": \"number\",\n                 \"count\": 0,\n                 \"scripted\": False,\n                 \"indexed\": False,\n                 \"analyzed\": False,\n                 \"doc_values\": False})\n        return doc_fields_arr", "response": "Converts all doc_types fields to. kibana"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_field_mappings(self, field):\n        retdict = {}\n        retdict['indexed'] = False\n        retdict['analyzed'] = False\n        for (key, val) in iteritems(field):\n            if key in self.mappings:\n                if (key == 'type' and\n                    (val == \"long\" or\n                     val == \"integer\" or\n                     val == \"double\" or\n                     val == \"float\")):\n                    val = \"number\"\n                # self.pr_dbg(\"\\t\\t\\tkey: %s\" % key)\n                # self.pr_dbg(\"\\t\\t\\t\\tval: %s\" % val)\n                retdict[key] = val\n            if key == 'index' and val != \"no\":\n                retdict['indexed'] = True\n                # self.pr_dbg(\"\\t\\t\\tkey: %s\" % key)\n                # self.pr_dbg(\"\\t\\t\\t\\tval: %s\" % val)\n                if val == \"analyzed\":\n                    retdict['analyzed'] = True\n        return retdict", "response": "Converts ES field mappings to. kibana field mappings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest if k_cache is incomplete", "response": "def is_kibana_cache_incomplete(self, es_cache, k_cache):\n        \"\"\"Test if k_cache is incomplete\n\n        Assume k_cache is always correct, but could be missing new\n        fields that es_cache has\n        \"\"\"\n        # convert list into dict, with each item's ['name'] as key\n        k_dict = {}\n        for field in k_cache:\n            # self.pr_dbg(\"field: %s\" % field)\n            k_dict[field['name']] = field\n            for ign_f in self.mappings_ignore:\n                k_dict[field['name']][ign_f] = 0\n        es_dict = {}\n        for field in es_cache:\n            es_dict[field['name']] = field\n            for ign_f in self.mappings_ignore:\n                es_dict[field['name']][ign_f] = 0\n        es_set = set(es_dict.keys())\n        k_set = set(k_dict.keys())\n        # reasons why kibana cache could be incomplete:\n        #     k_dict is missing keys that are within es_dict\n        #     We don't care if k has keys that es doesn't\n        # es {1,2} k {1,2,3}; intersection {1,2}; len(es-{}) 0\n        # es {1,2} k {1,2};   intersection {1,2}; len(es-{}) 0\n        # es {1,2} k {};      intersection {};    len(es-{}) 2\n        # es {1,2} k {1};     intersection {1};   len(es-{}) 1\n        # es {2,3} k {1};     intersection {};    len(es-{}) 2\n        # es {2,3} k {1,2};   intersection {2};   len(es-{}) 1\n        return len(es_set - k_set.intersection(es_set)) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a list into a dictionary that can be used to query easier", "response": "def list_to_compare_dict(self, list_form):\n        \"\"\"Convert list into a data structure we can query easier\"\"\"\n        compare_dict = {}\n        for field in list_form:\n            if field['name'] in compare_dict:\n                self.pr_dbg(\"List has duplicate field %s:\\n%s\" %\n                            (field['name'], compare_dict[field['name']]))\n                if compare_dict[field['name']] != field:\n                    self.pr_dbg(\"And values are different:\\n%s\" % field)\n                return None\n            compare_dict[field['name']] = field\n            for ign_f in self.mappings_ignore:\n                compare_dict[field['name']][ign_f] = 0\n        return compare_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare the fields in original with replica.", "response": "def compare_field_caches(self, replica, original):\n        \"\"\"Verify original is subset of replica\"\"\"\n        if original is None:\n            original = []\n        if replica is None:\n            replica = []\n        self.pr_dbg(\"Comparing orig with %s fields to replica with %s fields\" %\n                    (len(original), len(replica)))\n        # convert list into dict, with each item's ['name'] as key\n        orig = self.list_to_compare_dict(original)\n        if orig is None:\n            self.pr_dbg(\"Original has duplicate fields\")\n            return 1\n        repl = self.list_to_compare_dict(replica)\n        if repl is None:\n            self.pr_dbg(\"Replica has duplicate fields\")\n            return 1\n        # search orig for each item in repl\n        # if any items in repl not within orig or vice versa, then complain\n        # make sure contents of each item match\n        orig_found = {}\n        for (key, field) in iteritems(repl):\n            field_name = field['name']\n            if field_name not in orig:\n                self.pr_dbg(\"Replica has field not found in orig %s: %s\" %\n                            (field_name, field))\n                return 1\n            orig_found[field_name] = True\n            if orig[field_name] != field:\n                self.pr_dbg(\"Field in replica doesn't match orig:\")\n                self.pr_dbg(\"orig:%s\\nrepl:%s\" % (orig[field_name], field))\n                return 1\n        unfound = set(orig_found.keys()) - set(repl.keys())\n        if len(unfound) > 0:\n            self.pr_dbg(\"Orig contains fields that were not in replica\")\n            self.pr_dbg('%s' % unfound)\n            return 1\n        # We don't care about case when replica has more fields than orig\n        # unfound = set(repl.keys()) - set(orig_found.keys())\n        # if len(unfound) > 0:\n        #     self.pr_dbg(\"Replica contains fields that were not in orig\")\n        #     self.pr_dbg('%s' % unfound)\n        #     return 1\n        self.pr_dbg(\"Original matches replica\")\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a deamon thread for a given target function and arguments.", "response": "def start_daemon_thread(target, args=()):\n    \"\"\"starts a deamon thread for a given target function and arguments.\"\"\"\n    th = Thread(target=target, args=args)\n    th.daemon = True\n    th.start()\n    return th"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize_dict_keys(d, prefix=\"\"):\n    keys = []\n    for k, v in d.iteritems():\n        fqk = '%s%s' % (prefix, k)\n        keys.append(fqk)\n        if isinstance(v, dict):\n            keys.extend(serialize_dict_keys(v, prefix=\"%s.\" % fqk))\n\n    return keys", "response": "returns all the keys in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting user data to session.", "response": "def set_user(self, user):\n        \"\"\"\n        Writes user data to session.\n\n        Args:\n            user: User object\n\n        \"\"\"\n        self.session['user_id'] = user.key\n        self.session['user_data'] = user.clean_value()\n        role = self.get_role()\n        # TODO: this should be remembered from previous login\n        # self.session['role_data'] = default_role.clean_value()\n        self.session['role_id'] = role.key\n        self.current.role_id = role.key\n        self.current.user_id = user.key\n        # self.perm_cache = PermissionCache(role.key)\n        self.session['permissions'] = role.get_permissions()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef contains_opposite_color_piece(self, square, position):\n        return not position.is_square_empty(square) and \\\n            position.piece_at_square(square).color != self.color", "response": "Finds if a piece is occupied by a piece with the same color."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a translated message.", "response": "def gettext(message, domain=DEFAULT_DOMAIN):\n    \"\"\"Mark a message as translateable, and translate it.\n\n    All messages in the application that are translateable should be wrapped with this function.\n    When importing this function, it should be renamed to '_'. For example:\n\n    .. code-block:: python\n\n        from zengine.lib.translation import gettext as _\n        print(_('Hello, world!'))\n        'Merhaba, d\u00fcnya!'\n\n    For the messages that will be formatted later on, instead of using the position-based\n    formatting, key-based formatting should be used. This gives the translator an idea what\n    the variables in the format are going to be, and makes it possible for the translator\n    to reorder the variables. For example:\n\n    .. code-block:: python\n\n        name, number = 'Elizabeth', 'II'\n        _('Queen %(name)s %(number)s') % {'name': name, 'number': number}\n        'Krali\u00e7e II. Elizabeth'\n\n    The message returned by this function depends on the language of the current user.\n    If this function is called before a language is installed (which is normally done\n    by ZEngine when the user connects), this function will simply return the message\n    without modification.\n\n    If there are messages containing unicode characters, in Python 2 these messages must\n    be marked as unicode. Otherwise, python will not be able to correctly match these\n    messages with translations. For example:\n\n    .. code-block:: python\n\n        print(_('Caf\u00e9'))\n        'Caf\u00e9'\n        print(_(u'Caf\u00e9'))\n        'Kahve'\n\n    Args:\n        message (basestring, unicode): The input message.\n        domain (basestring): The domain of the message. Defaults to 'messages', which\n            is the domain where all application messages should be located.\n\n    Returns:\n        unicode: The translated message.\n    \"\"\"\n\n    if six.PY2:\n        return InstalledLocale._active_catalogs[domain].ugettext(message)\n    else:\n        return InstalledLocale._active_catalogs[domain].gettext(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gettext_lazy(message, domain=DEFAULT_DOMAIN):\n    return LazyProxy(gettext, message, domain=domain, enable_cache=False)", "response": "Returns a LazyProxy that can be used to translate a message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmark a message as translateable and translate it considering plural forms.", "response": "def ngettext(singular, plural, n, domain=DEFAULT_DOMAIN):\n    \"\"\"Mark a message as translateable, and translate it considering plural forms.\n\n    Some messages may need to change based on a number. For example, consider a message\n    like the following:\n\n    .. code-block:: python\n\n        def alert_msg(msg_count): print(\n        'You have %d %s' % (msg_count, 'message' if msg_count == 1 else 'messages'))\n\n        alert_msg(1)\n        'You have 1 message'\n        alert_msg(5)\n        'You have 5 messages'\n\n    To translate this message, you can use ngettext to consider the plural forms:\n\n    .. code-block:: python\n\n        from zengine.lib.translation import ngettext\n        def alert_msg(msg_count): print(ngettext('You have %(count)d message',\n                                                 'You have %(count)d messages',\n                                                 msg_count) % {'count': msg_count})\n        alert_msg(1)\n        '1 mesaj\u0131n\u0131z var'\n\n        alert_msg(5)\n        '5 mesajlar\u0131n\u0131z var'\n\n    When doing formatting, both singular and plural forms of the message should\n    have the exactly same variables.\n\n    Args:\n        singular (unicode): The singular form of the message.\n        plural (unicode): The plural form of the message.\n        n (int): The number that is used to decide which form should be used.\n        domain (basestring): The domain of the message. Defaults to 'messages', which\n            is the domain where all application messages should be located.\n    Returns:\n        unicode: The correct pluralization, translated.\n\n    \"\"\"\n\n    if six.PY2:\n        return InstalledLocale._active_catalogs[domain].ungettext(singular, plural, n)\n    else:\n        return InstalledLocale._active_catalogs[domain].ngettext(singular, plural, n)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark a message with plural forms translateable and delay the translation until the message is used.", "response": "def ngettext_lazy(singular, plural, n, domain=DEFAULT_DOMAIN):\n    \"\"\"Mark a message with plural forms translateable, and delay the translation\n    until the message is used.\n\n    Works the same was a `ngettext`, with a delaying functionality similiar to `gettext_lazy`.\n\n    Args:\n        singular (unicode): The singular form of the message.\n        plural (unicode): The plural form of the message.\n        n (int): The number that is used to decide which form should be used.\n        domain (basestring): The domain of the message. Defaults to 'messages', which\n                             is the domain where all application messages should be located.\n    Returns:\n        unicode: The correct pluralization, with the translation being\n                 delayed until the message is used.\n    \"\"\"\n    return LazyProxy(ngettext, singular, plural, n, domain=domain, enable_cache=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrap_locale_formatter(fn, locale_type):\n\n    def wrapped_locale_formatter(*args, **kwargs):\n        \"\"\"A Babel formatting function, wrapped to automatically use the\n        currently installed language.\n\n        The wrapped function will not throw any exceptions for unknown locales,\n        if Babel doesn't recognise the locale, we will simply fall back to\n        the default language.\n\n        The locale used by the wrapped function can be overriden by passing it a `locale` keyword.\n        To learn more about this function, check the documentation of Babel for the function of\n        the same name.\n        \"\"\"\n        # Get the current locale from the class\n        kwargs_ = {'locale': getattr(InstalledLocale, locale_type)}\n        # By creating a dict then updating it, we allow locale to be overridden\n        kwargs_.update(kwargs)\n        try:\n            formatted = fn(*args, **kwargs_)\n        except UnknownLocaleError:\n            log.warning(\n                \"\"\"Can\\'t do formatting for language code {locale},\n                           falling back to default {default}\"\"\".format(\n                    locale=kwargs_['locale'],\n                    default=settings.DEFAULT_LANG)\n            )\n            kwargs_['locale'] = settings.DEFAULT_LANG\n            formatted = fn(*args, **kwargs_)\n        return formatted\n\n    return wrapped_locale_formatter", "response": "Wrap a Babel data formatting function to automatically format the currently installed language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef install_language(cls, language_code):\n        # Skip if the language is already installed\n        if language_code == cls.language:\n            return\n        try:\n            cls._active_catalogs = cls._translation_catalogs[language_code]\n            cls.language = language_code\n            log.debug('Installed language %s', language_code)\n        except KeyError:\n            default = settings.DEFAULT_LANG\n            log.warning('Unknown language %s, falling back to %s', language_code, default)\n            cls._active_catalogs = cls._translation_catalogs[default]\n            cls.language = default", "response": "Install the translations for the specified language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef install_locale(cls, locale_code, locale_type):\n\n        # Skip if the locale is already installed\n        if locale_code == getattr(cls, locale_type):\n            return\n        try:\n            # We create a Locale instance to see if the locale code is supported\n            locale = Locale(locale_code)\n            log.debug('Installed locale %s', locale_code)\n        except UnknownLocaleError:\n            default = settings.DEFAULT_LOCALIZATION_FORMAT\n            log.warning('Unknown locale %s, falling back to %s', locale_code, default)\n            locale = Locale(default)\n        setattr(cls, locale_type, locale.language)", "response": "Install the locale specified by language_code for localizations of type locale_type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrotate x y vector over x2 y2 x1 y1 angle", "response": "def _rotate_vector(x, y, x2, y2, x1, y1):\n        \"\"\"\n        rotate x,y vector over x2-x1, y2-y1 angle\n        \"\"\"\n        angle = atan2(y2 - y1, x2 - x1)\n        cos_rad = cos(angle)\n        sin_rad = sin(angle)\n        return cos_rad * x + sin_rad * y, -sin_rad * x + cos_rad * y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_static_table_mapping():\n    static_table_mapping = {}\n    for index, (name, value) in enumerate(CocaineHeaders.STATIC_TABLE, 1):\n        header_name_search_result = static_table_mapping.setdefault(name, (index, {}))\n        header_name_search_result[1][value] = index\n    return static_table_mapping", "response": "Build static table mapping from header name to tuple with next structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the entry specified by index.", "response": "def get_by_index(self, index):\n        \"\"\"\n        Returns the entry specified by index\n\n        Note that the table is 1-based ie an index of 0 is\n        invalid.  This is due to the fact that a zero value\n        index signals that a completely unindexed header\n        follows.\n\n        The entry will either be from the static table or\n        the dynamic table depending on the value of index.\n        \"\"\"\n        index -= 1\n        if 0 <= index < len(CocaineHeaders.STATIC_TABLE):\n            return CocaineHeaders.STATIC_TABLE[index]\n        index -= len(CocaineHeaders.STATIC_TABLE)\n        if 0 <= index < len(self.dynamic_entries):\n            return self.dynamic_entries[index]\n        raise InvalidTableIndex(\"Invalid table index %d\" % index)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, name, value):\n        # We just clear the table if the entry is too big\n        size = table_entry_size(name, value)\n        if size > self._maxsize:\n            self.dynamic_entries.clear()\n            self._current_size = 0\n\n        # Add new entry if the table actually has a size\n        elif self._maxsize > 0:\n            self.dynamic_entries.appendleft((name, value))\n            self._current_size += size\n            self._shrink()", "response": "Adds a new entry to the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches the table for the entry specified by name and value.", "response": "def search(self, name, value):\n        \"\"\"\n        Searches the table for the entry specified by name\n        and value\n\n        Returns one of the following:\n            - ``None``, no match at all\n            - ``(index, name, None)`` for partial matches on name only.\n            - ``(index, name, value)`` for perfect matches.\n        \"\"\"\n        partial = None\n\n        header_name_search_result = CocaineHeaders.STATIC_TABLE_MAPPING.get(name)\n        if header_name_search_result:\n            index = header_name_search_result[1].get(value)\n            if index is not None:\n                return index, name, value\n            partial = (header_name_search_result[0], name, None)\n\n        offset = len(CocaineHeaders.STATIC_TABLE)\n        for (i, (n, v)) in enumerate(self.dynamic_entries):\n            if n == name:\n                if v == value:\n                    return i + offset + 1, n, v\n                elif partial is None:\n                    partial = (i + offset + 1, n, None)\n        return partial"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshrinks the dynamic table to be at or below maxsize", "response": "def _shrink(self):\n        \"\"\"\n        Shrinks the dynamic table to be at or below maxsize\n        \"\"\"\n        cursize = self._current_size\n        while cursize > self._maxsize:\n            name, value = self.dynamic_entries.pop()\n            cursize -= table_entry_size(name, value)\n        self._current_size = cursize"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new value for the given key.", "response": "def add(self, name, value):\n        # type: (str, str) -> None\n        \"\"\"Adds a new value for the given key.\"\"\"\n        self._last_key = name\n        if name in self:\n            self._dict[name] = value\n            self._as_list[name].append(value)\n        else:\n            self[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterable of all ( name value ) pairs.", "response": "def get_all(self):\n        # type: () -> typing.Iterable[typing.Tuple[str, str]]\n        \"\"\"Returns an iterable of all (name, value) pairs.\n\n        If a header has multiple values, multiple pairs will be\n        returned with the same name.\n        \"\"\"\n        for name, values in six.iteritems(self._as_list):\n            for value in values:\n                yield (name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the view used to edit permissions of the object.", "response": "def edit_permissions(self):\n        \"\"\"Creates the view used to edit permissions.\n\n        To create the view, data in the following format is passed to the UI\n        in the objects field:\n\n        .. code-block:: python\n            {\n                \"type\": \"tree-toggle\",\n                \"action\": \"set_permission\",\n                \"tree\": [\n                    {\n                        \"checked\": true,\n                        \"name\": \"Workflow 1 Name\",\n                        \"id\": \"workflow1\",\n                        \"children\":\n                            [\n                                {\n                                    \"checked\": true,\n                                    \"name\": \"Task 1 Name\",\n                                    \"id\": \"workflow1..task1\",\n                                    \"children\": []\n                                },\n                                {\n                                    \"checked\": false,\n                                    \"id\": \"workflow1..task2\",\n                                    \"name\": \"Task 2 Name\",\n                                    \"children\": []\n                                }\n                            ]\n                    },\n                    {\n                        \"checked\": true,\n                        \"name\": \"Workflow 2 Name\",\n                        \"id\": \"workflow2\",\n                        \"children\": [\n                            {\n                                \"checked\": true,\n                                \"name\": \"Workflow 2 Lane 1 Name\",\n                                \"id\": \"workflow2.lane1\",\n                                \"children\": [\n                                    {\n                                        \"checked\": true,\n                                        \"name\": \"Workflow 2 Task 1 Name\",\n                                        \"id\": \"workflow2.lane1.task1\",\n                                        \"children\": []\n                                    },\n                                    {\n                                        \"checked\": false,\n                                        \"name\": \"Workflow 2 Task 2 Name\",\n                                        \"id\": \"workflow2.lane1.task2\",\n                                        \"children\": []\n                                    }\n                                ]\n                            }\n                        ]\n                    }\n                ]\n            }\n\n        \"type\" field denotes that the object is a tree view which has elements that can be toggled.\n        \"action\" field is the\n\n        \"name\" field is the human readable name.\n        \"id\" field is used to make requests to the backend.\n        \"checked\" field shows whether the role has the permission or not.\n        \"children\" field is the sub-permissions of the permission.\n        \"\"\"\n        # Get the role that was selected in the CRUD view\n        key = self.current.input['object_id']\n        self.current.task_data['role_id'] = key\n        role = RoleModel.objects.get(key=key)\n        # Get the cached permission tree, or build a new one if there is none cached\n        # TODO: Add an extra view in case there was no cache, as in 'please wait calculating permissions'\n        permission_tree = self._permission_trees(PermissionModel.objects)\n        # Apply the selected role to the permission tree, setting the 'checked' field\n        # of the permission the role has\n        role_tree = self._apply_role_tree(permission_tree, role)\n        # Apply final formatting, and output the tree to the UI\n        self.output['objects'] = [\n            {\n                'type': 'tree-toggle',\n                'action': 'apply_change',\n                'trees': self._format_tree_output(role_tree),\n            },\n        ]\n        self.form_out(PermissionForm())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _permission_trees(permissions):\n        treecache = PermissionTreeCache()\n        cached = treecache.get()\n        if not cached:\n            tree = PermissionTreeBuilder()\n            for permission in permissions:\n                tree.insert(permission)\n            result = tree.serialize()\n            treecache.set(result)\n            return result\n        return cached", "response": "Get the cached permission tree or build a new one if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraversing the permission tree returning the permission at given permission path.", "response": "def _traverse_tree(tree, path):\n        \"\"\"Traverses the permission tree, returning the permission at given permission path.\"\"\"\n        path_steps = (step for step in path.split('.') if step != '')\n        # Special handling for first step, because the first step isn't under 'objects'\n        first_step = path_steps.next()\n        subtree = tree[first_step]\n\n        for step in path_steps:\n            subtree = subtree['children'][step]\n        return subtree"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_change(self):\n        changes = self.input['change']\n        key = self.current.task_data['role_id']\n        role = RoleModel.objects.get(key=key)\n        for change in changes:\n            permission = PermissionModel.objects.get(code=change['id'])\n            if change['checked'] is True:\n                role.add_permission(permission)\n            else:\n                role.remove_permission(permission)\n        role.save()", "response": "Applies changes to the permissions of the role."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef seek(self, offset):\n        if self._shifts:\n            if 0 <= offset < len(self._shifts):\n                current_pos = self._file.tell()\n                new_pos = self._shifts[offset]\n                if current_pos != new_pos:\n                    if current_pos == self._shifts[-1]:  # reached the end of the file\n                        self._data = self.__reader()\n                        self.__file = iter(self._file.readline, '')\n                    self._file.seek(new_pos)\n            else:\n                raise IndexError('invalid offset')\n        else:\n            raise self._implement_error", "response": "Seeks to a given number of record in the original file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntell the file where the record was processed", "response": "def tell(self):\n        \"\"\"\n        :return: number of records processed from the original file\n        \"\"\"\n        if self._shifts:\n            t = self._file.tell()\n            return bisect_left(self._shifts, t)\n        raise self._implement_error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, data):\n        m = self._convert_structure(data)\n        self._file.write(self._format_mol(*m))\n        self._file.write('M  END\\n')\n\n        for k, v in data.meta.items():\n            self._file.write(f'>  <{k}>\\n{v}\\n')\n        self._file.write('$$$$\\n')", "response": "write single molecule into file\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_workflow_to_cache(self, serialized_wf_instance):\n        # self.current.task_data['flow'] = None\n        task_data = self.current.task_data.copy()\n        for k, v in list(task_data.items()):\n            if k.startswith('_'):\n                del task_data[k]\n        if 'cmd' in task_data:\n            del task_data['cmd']\n\n        self.wf_state.update({'step': serialized_wf_instance,\n                              'data': task_data,\n                              'name': self.current.workflow_name,\n                              'wf_id': self.workflow_spec.wf_id\n                              })\n\n        if self.current.lane_id:\n            self.current.pool[self.current.lane_id] = self.current.role.key\n        self.wf_state['pool'] = self.current.pool\n        self.current.log.debug(\"POOL Content before WF Save: %s\" % self.current.pool)\n        self.current.wf_cache.save(self.wf_state)", "response": "Save the workflow instance to the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pool_context(self):\n        # TODO: Add in-process caching\n        \"\"\"\n        Builds context for the WF pool.\n\n        Returns:\n            Context dict.\n        \"\"\"\n        context = {self.current.lane_id: self.current.role, 'self': self.current.role}\n        for lane_id, role_id in self.current.pool.items():\n            if role_id:\n                context[lane_id] = lazy_object_proxy.Proxy(\n                    lambda: self.role_model(super_context).objects.get(role_id))\n        return context", "response": "Builds a context dictionary for the WF pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the serialized workflow state and data from cache and updates the self. current. task_data with the data from the cache.", "response": "def load_workflow_from_cache(self):\n        \"\"\"\n        loads the serialized wf state and data from cache\n        updates the self.current.task_data\n        \"\"\"\n        if not self.current.new_token:\n            self.wf_state = self.current.wf_cache.get(self.wf_state)\n            self.current.task_data = self.wf_state['data']\n            self.current.set_client_cmds()\n            self.current.pool = self.wf_state['pool']\n            return self.wf_state['step']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize the current WF state data.", "response": "def serialize_workflow(self):\n        \"\"\"\n        Serializes the current WF.\n\n        Returns:\n            WF state data.\n        \"\"\"\n        self.workflow.refresh_waiting_tasks()\n        return CompactWorkflowSerializer().serialize_workflow(self.workflow,\n                                                              include_spec=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to find the path of the BPMN diagram file . bpmn", "response": "def find_workflow_path(self):\n        \"\"\"\n        Tries to find the path of the workflow diagram file\n        in `WORKFLOW_PACKAGES_PATHS`.\n\n        Returns:\n            Path of the workflow spec file (BPMN diagram)\n        \"\"\"\n        for pth in settings.WORKFLOW_PACKAGES_PATHS:\n            path = \"%s/%s.bpmn\" % (pth, self.current.workflow_name)\n            if os.path.exists(path):\n                return path\n        err_msg = \"BPMN file cannot found: %s\" % self.current.workflow_name\n        log.error(err_msg)\n        raise RuntimeError(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_worfklow_spec(self):\n        # TODO: convert from in-process to redis based caching\n        if self.current.workflow_name not in self.workflow_spec_cache:\n            # path = self.find_workflow_path()\n            # spec_package = InMemoryPackager.package_in_memory(self.current.workflow_name, path)\n            # spec = BpmnSerializer().deserialize_workflow_spec(spec_package)\n\n            try:\n                self.current.wf_object = BPMNWorkflow.objects.get(name=self.current.workflow_name)\n            except ObjectDoesNotExist:\n                self.current.wf_object = BPMNWorkflow.objects.get(name='not_found')\n                self.current.task_data['non-existent-wf'] = self.current.workflow_name\n                self.current.workflow_name = 'not_found'\n            xml_content = self.current.wf_object.xml.body\n            spec = ZopsSerializer().deserialize_workflow_spec(xml_content, self.current.workflow_name)\n\n            spec.wf_id = self.current.wf_object.key\n            self.workflow_spec_cache[self.current.workflow_name] = spec\n        return self.workflow_spec_cache[self.current.workflow_name]", "response": "Generates and caches the workflow spec package from disk and returns the SpiffWorkflow Spec object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the workflow with given request, response objects and diagram name. Args: session: input: workflow_name (str): Name of workflow diagram without \".bpmn\" suffix. File must be placed under one of configured :py:attr:`~zengine.settings.WORKFLOW_PACKAGES_PATHS`", "response": "def start_engine(self, **kwargs):\n        \"\"\"\n        Initializes the workflow with given request, response objects and diagram name.\n\n        Args:\n            session:\n            input:\n            workflow_name (str): Name of workflow diagram without \".bpmn\" suffix.\n             File must be placed under one of configured :py:attr:`~zengine.settings.WORKFLOW_PACKAGES_PATHS`\n\n        \"\"\"\n        self.current = WFCurrent(**kwargs)\n        self.wf_state = {'in_external': False, 'finished': False}\n        if not self.current.new_token:\n            self.wf_state = self.current.wf_cache.get(self.wf_state)\n            self.current.workflow_name = self.wf_state['name']\n            # if we have a pre-selected object to work with,\n            # inserting it as current.input['id'] and task_data['object_id']\n            if 'subject' in self.wf_state:\n                self.current.input['id'] = self.wf_state['subject']\n                self.current.task_data['object_id'] = self.wf_state['subject']\n        self.check_for_authentication()\n        self.check_for_permission()\n        self.workflow = self.load_or_create_workflow()\n\n        # if form data exists in input (user submitted)\n        # put form data in wf task_data\n        if 'form' in self.current.input:\n            form = self.current.input['form']\n            if 'form_name' in form:\n                self.current.task_data[form['form_name']] = form\n\n        # in wf diagram, if property is stated as init = True\n        # demanded initial values are assigned and put to cache\n        start_init_values = self.workflow_spec.wf_properties.get('init', 'False') == 'True'\n        if start_init_values:\n            WFInit = get_object_from_path(settings.WF_INITIAL_VALUES)()\n            WFInit.assign_wf_initial_values(self.current)\n\n        log_msg = (\"\\n\\n::::::::::: ENGINE STARTED :::::::::::\\n\"\n                   \"\\tWF: %s (Possible) TASK:%s\\n\"\n                   \"\\tCMD:%s\\n\"\n                   \"\\tSUBCMD:%s\" % (\n                       self.workflow.name,\n                       self.workflow.get_tasks(Task.READY),\n                       self.current.input.get('cmd'), self.current.input.get('subcmd')))\n        log.debug(log_msg)\n        sys._zops_wf_state_log = log_msg\n        self.current.workflow = self.workflow"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_wf_state_log(self):\n        output = '\\n- - - - - -\\n'\n        output += \"WORKFLOW: %s ( %s )\" % (self.current.workflow_name.upper(),\n                                           self.current.workflow.name)\n\n        output += \"\\nTASK: %s ( %s )\\n\" % (self.current.task_name, self.current.task_type)\n        output += \"DATA:\"\n        for k, v in self.current.task_data.items():\n            if v:\n                output += \"\\n\\t%s: %s\" % (k, v)\n        output += \"\\nCURRENT:\"\n        output += \"\\n\\tACTIVITY: %s\" % self.current.activity\n        output += \"\\n\\tPOOL: %s\" % self.current.pool\n        output += \"\\n\\tIN EXTERNAL: %s\" % self.wf_state['in_external']\n        output += \"\\n\\tLANE: %s\" % self.current.lane_name\n        output += \"\\n\\tTOKEN: %s\" % self.current.token\n        sys._zops_wf_state_log = output\n        return output", "response": "Generates the log of the state of the workflow and content of the current task_data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef switch_from_external_to_main_wf(self):\n\n        \"\"\"\n        Main workflow switcher.\n\n        This method recreates main workflow from `main wf` dict which\n        was set by external workflow swicther previously.\n\n        \"\"\"\n\n        # in external assigned as True in switch_to_external_wf.\n        # external_wf should finish EndEvent and it's name should be\n        # also EndEvent for switching again to main wf.\n        if self.wf_state['in_external'] and self.current.task_type == 'EndEvent' and \\\n                self.current.task_name == 'EndEvent':\n\n            # main_wf information was copied in switch_to_external_wf and it takes this information.\n            main_wf = self.wf_state['main_wf']\n\n            # main_wf_name is assigned to current workflow name again.\n            self.current.workflow_name = main_wf['name']\n\n            # For external WF, check permission and authentication. But after cleaning current task.\n            self._clear_current_task()\n\n            # check for auth and perm. current task cleared, do against new workflow_name\n            self.check_for_authentication()\n            self.check_for_permission()\n\n            # WF knowledge is taken for main wf.\n            self.workflow_spec = self.get_worfklow_spec()\n\n            # WF instance is started again where leave off.\n            self.workflow = self.deserialize_workflow(main_wf['step'])\n\n            # Current WF is this WF instance.\n            self.current.workflow = self.workflow\n\n            # in_external is assigned as False\n            self.wf_state['in_external'] = False\n\n            # finished is assigned as False, because still in progress.\n            self.wf_state['finished'] = False\n\n            # pool info of main_wf is assigned.\n            self.wf_state['pool'] = main_wf['pool']\n            self.current.pool = self.wf_state['pool']\n\n            # With main_wf is executed.\n            self.run()", "response": "This method recreates main workflow from external workflow dict which was set by switch_to_external_wf."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching to external WF.", "response": "def switch_to_external_wf(self):\n        \"\"\"\n        External workflow switcher.\n\n        This method copies main workflow information into\n        a temporary dict `main_wf` and makes external workflow\n        acting as main workflow.\n\n        \"\"\"\n\n        # External WF name should be stated at main wf diagram and type should be service task.\n        if (self.current.task_type == 'ServiceTask' and\n                self.current.task.task_spec.type == 'external'):\n\n            log.debug(\"Entering to EXTERNAL WF\")\n\n            # Main wf information is copied to main_wf.\n            main_wf = self.wf_state.copy()\n\n            # workflow name from main wf diagram is assigned to current workflow name.\n            # workflow name must be either in task_data with key 'external_wf' or in main diagram's\n            # topic.\n            self.current.workflow_name = self.current.task_data.pop('external_wf', False) or self.\\\n                current.task.task_spec.topic\n\n            # For external WF, check permission and authentication. But after cleaning current task.\n            self._clear_current_task()\n\n            # check for auth and perm. current task cleared, do against new workflow_name\n            self.check_for_authentication()\n            self.check_for_permission()\n\n            # wf knowledge is taken for external wf.\n            self.workflow_spec = self.get_worfklow_spec()\n            # New WF instance is created for external wf.\n            self.workflow = self.create_workflow()\n            # Current WF is this WF instance.\n            self.current.workflow = self.workflow\n            # main_wf: main wf information.\n            # in_external: it states external wf in progress.\n            # finished: it shows that main wf didn't finish still progress in external wf.\n            self.wf_state = {'main_wf': main_wf, 'in_external': True, 'finished': False}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the current task related attributes", "response": "def _clear_current_task(self):\n\n        \"\"\"\n        Clear tasks related attributes, checks permissions\n        While switching WF to WF, authentication and permissions are checked for new WF.\n        \"\"\"\n        self.current.task_name = None\n        self.current.task_type = None\n        self.current.task = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_for_rerun_user_task(self):\n        data = self.current.input\n        if 'wf_meta' in data:\n            return\n\n        current_task = self.workflow.get_tasks(Task.READY)[0]\n        current_task_type = current_task.task_spec.__class__.__name__\n        pre_task = current_task.parent\n        pre_task_type = pre_task.task_spec.__class__.__name__\n\n        if pre_task_type != 'UserTask':\n            return\n\n        if current_task_type == 'EndEvent':\n            return\n\n        pre_lane = pre_task.task_spec.lane\n        current_lane = current_task.task_spec.lane\n        if pre_lane == current_lane:\n            pre_task._set_state(Task.READY)\n            current_task._set_state(Task.MAYBE)", "response": "Checks that the user task needs to re - run."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nswitch the language of the current user.", "response": "def switch_lang(self):\n        \"\"\"Switch to the language of the current user.\n\n        If the current language is already the specified one, nothing will be done.\n        \"\"\"\n        locale = self.current.locale\n        translation.InstalledLocale.install_language(locale['locale_language'])\n        translation.InstalledLocale.install_locale(locale['locale_datetime'], 'datetime')\n        translation.InstalledLocale.install_locale(locale['locale_number'], 'number')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntriggering a lane_user_change signal if we switched to a new lane and we don t have a user_id", "response": "def catch_lane_change(self):\n        \"\"\"\n        trigger a lane_user_change signal if we switched to a new lane\n        and new lane's user is different from current one\n        \"\"\"\n        if self.current.lane_name:\n            if self.current.old_lane and self.current.lane_name != self.current.old_lane:\n                # if lane_name not found in pool or it's user different from the current(old) user\n                if (self.current.lane_id not in self.current.pool or\n                            self.current.pool[self.current.lane_id] != self.current.user_id):\n                    self.current.log.info(\"LANE CHANGE : %s >> %s\" % (self.current.old_lane,\n                                                                      self.current.lane_name))\n                    if self.current.lane_auto_sendoff:\n                        self.current.sendoff_current_user()\n                    self.current.flow_enabled = False\n                    if self.current.lane_auto_invite:\n                        self.current.invite_other_parties(self._get_possible_lane_owners())\n                    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the client message that defined in the current workflow task s inputOutput extension and adds the appropriate message to the current workflow message.", "response": "def parse_workflow_messages(self):\n        \"\"\"\n        Transmits client message that defined in\n        a workflow task's inputOutput extension\n\n       .. code-block:: xml\n\n            <bpmn2:extensionElements>\n            <camunda:inputOutput>\n            <camunda:inputParameter name=\"client_message\">\n            <camunda:map>\n              <camunda:entry key=\"title\">Te\u015fekk\u00fcrler</camunda:entry>\n              <camunda:entry key=\"body\">\u0130\u015flem Ba\u015far\u0131l\u0131</camunda:entry>\n              <camunda:entry key=\"type\">info</camunda:entry>\n            </camunda:map>\n            </camunda:inputParameter>\n            </camunda:inputOutput>\n            </bpmn2:extensionElements>\n\n        \"\"\"\n        if 'client_message' in self.current.spec.data:\n            m = self.current.spec.data['client_message']\n            self.current.msg_box(title=m.get('title'),\n                                 msg=m.get('body'),\n                                 typ=m.get('type', 'info'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the method that referenced from current task", "response": "def run_activity(self):\n        \"\"\"\n        runs the method that referenced from current task\n        \"\"\"\n        activity = self.current.activity\n        if activity:\n            if activity not in self.wf_activities:\n                self._load_activity(activity)\n            self.current.log.debug(\n                \"Calling Activity %s from %s\" % (activity, self.wf_activities[activity]))\n            self.wf_activities[self.current.activity](self.current)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _import_object(self, path, look_for_cls_method):\n        last_nth = 2 if look_for_cls_method else 1\n        path = path.split('.')\n        module_path = '.'.join(path[:-last_nth])\n        class_name = path[-last_nth]\n        module = importlib.import_module(module_path)\n        if look_for_cls_method and path[-last_nth:][0] == path[-last_nth]:\n            class_method = path[-last_nth:][1]\n        else:\n            class_method = None\n        return getattr(module, class_name), class_name, class_method", "response": "Imports the object from the module that contains the referenced method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_activity(self, activity):\n        fpths = []\n        full_path = ''\n        errors = []\n        paths = settings.ACTIVITY_MODULES_IMPORT_PATHS\n        number_of_paths = len(paths)\n        for index_no in range(number_of_paths):\n            full_path = \"%s.%s\" % (paths[index_no], activity)\n            for look4kls in (0, 1):\n                try:\n                    self.current.log.info(\"try to load from %s[%s]\" % (full_path, look4kls))\n                    kls, cls_name, cls_method = self._import_object(full_path, look4kls)\n                    if cls_method:\n                        self.current.log.info(\"WILLCall %s(current).%s()\" % (kls, cls_method))\n                        self.wf_activities[activity] = lambda crnt: getattr(kls(crnt), cls_method)()\n                    else:\n                        self.wf_activities[activity] = kls\n                    return\n                except (ImportError, AttributeError):\n                    fpths.append(full_path)\n                    errmsg = \"{activity} not found under these paths:\\n\\n >>> {paths} \\n\\n\" \\\n                             \"Error Messages:\\n {errors}\"\n                    errors.append(\"\\n========================================================>\\n\"\n                                  \"| PATH | %s\"\n                                  \"\\n========================================================>\\n\\n\"\n                                  \"%s\" % (full_path, traceback.format_exc()))\n                    assert index_no != number_of_paths - 1, errmsg.format(activity=activity,\n                                                                          paths='\\n >>> '.join(\n                                                                              set(fpths)),\n                                                                          errors='\\n\\n'.join(errors)\n                                                                          )\n                except:\n                    self.current.log.exception(\"Cannot found the %s\" % activity)", "response": "Loads the given activity from the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_for_authentication(self):\n        auth_required = self.current.workflow_name not in settings.ANONYMOUS_WORKFLOWS\n        if auth_required and not self.current.is_auth:\n            self.current.log.debug(\"LOGIN REQUIRED:::: %s\" % self.current.workflow_name)\n            raise HTTPError(401, \"Login required for %s\" % self.current.workflow_name)", "response": "Checks current workflow against the list of anonymous workflows."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the user has permission to access a specific lane.", "response": "def check_for_lane_permission(self):\n        \"\"\"\n        One or more permissions can be associated with a lane\n        of a workflow. In a similar way, a lane can be\n        restricted with relation to other lanes of the workflow.\n\n        This method called on lane changes and checks user has\n        required permissions and relations.\n\n        Raises:\n             HTTPForbidden: if the current user hasn't got the\n              required permissions and proper relations\n\n        \"\"\"\n        # TODO: Cache lane_data in app memory\n        if self.current.lane_permission:\n            log.debug(\"HAS LANE PERM: %s\" % self.current.lane_permission)\n            perm = self.current.lane_permission\n            if not self.current.has_permission(perm):\n                raise HTTPError(403, \"You don't have required lane permission: %s\" % perm)\n\n        if self.current.lane_relations:\n            context = self.get_pool_context()\n            log.debug(\"HAS LANE RELS: %s\" % self.current.lane_relations)\n            try:\n                cond_result = eval(self.current.lane_relations, context)\n            except:\n                log.exception(\"CONDITION EVAL ERROR : %s || %s\" % (\n                    self.current.lane_relations, context))\n                raise\n            if not cond_result:\n                log.debug(\"LANE RELATION ERR: %s %s\" % (self.current.lane_relations, context))\n                raise HTTPError(403, \"You aren't qualified for this lane: %s\" %\n                                self.current.lane_relations)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if current user has the required permission for current workflow step.", "response": "def check_for_permission(self):\n        # TODO: Works but not beautiful, needs review!\n        \"\"\"\n        Checks if current user (or role) has the required permission\n        for current workflow step.\n\n        Raises:\n            HTTPError: if user doesn't have required permissions.\n        \"\"\"\n        if self.current.task:\n            lane = self.current.lane_id\n            permission = \"%s.%s.%s\" % (self.current.workflow_name, lane, self.current.task_name)\n        else:\n            permission = self.current.workflow_name\n        log.debug(\"CHECK PERM: %s\" % permission)\n\n        if (self.current.task_type not in PERM_REQ_TASK_TYPES or\n                permission.startswith(tuple(settings.ANONYMOUS_WORKFLOWS)) or\n                (self.current.is_auth and permission.startswith(tuple(settings.COMMON_WORKFLOWS)))):\n            return\n        # FIXME:needs hardening\n\n        log.debug(\"REQUIRE PERM: %s\" % permission)\n        if not self.current.has_permission(permission):\n            raise HTTPError(403, \"You don't have required permission: %s\" % permission)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_wf_finalization(self):\n        if ((not self.current.flow_enabled or (\n            self.current.task_type.startswith('End') and not self.are_we_in_subprocess())) and\n                    'token' in self.current.output):\n            del self.current.output['token']", "response": "Removes the token key from the current. output if WF is over."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_rdkit_molecule(data):\n    m = MoleculeContainer()\n    atoms, mapping = [], []\n    for a in data.GetAtoms():\n        atom = {'element': a.GetSymbol(), 'charge': a.GetFormalCharge()}\n        atoms.append(atom)\n        mapping.append(a.GetAtomMapNum())\n\n        isotope = a.GetIsotope()\n        if isotope:\n            atom['isotope'] = isotope\n        radical = a.GetNumRadicalElectrons()\n        if radical:\n            atom['multiplicity'] = radical + 1\n\n    conformers = data.GetConformers()\n    if conformers:\n        for atom, (x, y, z) in zip(atoms, conformers[0].GetPositions()):\n            atom['x'] = x\n            atom['y'] = y\n            atom['z'] = z\n\n    for atom, mapping in zip(atoms, mapping):\n        a = m.add_atom(atom)\n        if mapping:\n            m.atom(a)._parsed_mapping = mapping\n\n    for bond in data.GetBonds():\n        m.add_bond(bond.GetBeginAtomIdx() + 1, bond.GetEndAtomIdx() + 1, _rdkit_bond_map[bond.GetBondType()])\n\n    return m", "response": "Convert a RDKit molecule object to a MoleculeContainer object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_rdkit_molecule(data):\n    mol = RWMol()\n    conf = Conformer()\n    mapping = {}\n    is_3d = False\n    for n, a in data.atoms():\n        ra = Atom(a.number)\n        ra.SetAtomMapNum(n)\n        if a.charge:\n            ra.SetFormalCharge(a.charge)\n        if a.isotope != a.common_isotope:\n            ra.SetIsotope(a.isotope)\n        if a.radical:\n            ra.SetNumRadicalElectrons(a.radical)\n        mapping[n] = m = mol.AddAtom(ra)\n        conf.SetAtomPosition(m, (a.x, a.y, a.z))\n        if a.z:\n            is_3d = True\n    if not is_3d:\n        conf.Set3D(False)\n\n    for n, m, b in data.bonds():\n        mol.AddBond(mapping[n], mapping[m], _bond_map[b.order])\n\n    mol.AddConformer(conf)\n    SanitizeMol(mol)\n    return mol", "response": "Convert a MoleculeContainer to RDKit molecule object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_args_parser():\n    parser = argparse.ArgumentParser(\n        description='Marabunta: Migrating ants for Odoo')\n    parser.add_argument('--migration-file', '-f',\n                        action=EnvDefault,\n                        envvar='MARABUNTA_MIGRATION_FILE',\n                        required=True,\n                        help='The yaml file containing the migration steps')\n    parser.add_argument('--database', '-d',\n                        action=EnvDefault,\n                        envvar='MARABUNTA_DATABASE',\n                        required=True,\n                        help=\"Odoo's database\")\n    parser.add_argument('--db-user', '-u',\n                        action=EnvDefault,\n                        envvar='MARABUNTA_DB_USER',\n                        required=True,\n                        help=\"Odoo's database user\")\n    parser.add_argument('--db-password', '-w',\n                        action=EnvDefault,\n                        envvar='MARABUNTA_DB_PASSWORD',\n                        required=True,\n                        help=\"Odoo's database password\")\n    parser.add_argument('--db-port', '-p',\n                        default=os.environ.get('MARABUNTA_DB_PORT', 5432),\n                        help=\"Odoo's database port\")\n    parser.add_argument('--db-host', '-H',\n                        default=os.environ.get('MARABUNTA_DB_HOST',\n                                               'localhost'),\n                        help=\"Odoo's database host\")\n    parser.add_argument('--mode',\n                        action=EnvDefault,\n                        envvar='MARABUNTA_MODE',\n                        required=False,\n                        help=\"Specify the mode in which we run the migration,\"\n                             \"such as 'demo' or 'prod'. Additional operations \"\n                             \"of this mode will be executed after the main \"\n                             \"operations and the addons list of this mode \"\n                             \"will be merged with the main addons list.\")\n    parser.add_argument('--allow-serie',\n                        action=BoolEnvDefault,\n                        required=False,\n                        envvar='MARABUNTA_ALLOW_SERIE',\n                        help='Allow to run more than 1 version upgrade at a '\n                             'time.')\n    parser.add_argument('--force-version',\n                        required=False,\n                        default=os.environ.get('MARABUNTA_FORCE_VERSION'),\n                        help='Force upgrade of a version, even if it has '\n                             'already been applied.')\n\n    group = parser.add_argument_group(\n        title='Web',\n        description='Configuration related to the internal web server, '\n                    'used to publish a maintenance page during the migration.',\n    )\n    group.add_argument('--web-host',\n                       required=False,\n                       default=os.environ.get('MARABUNTA_WEB_HOST', '0.0.0.0'),\n                       help='Host for the web server')\n    group.add_argument('--web-port',\n                       required=False,\n                       default=os.environ.get('MARABUNTA_WEB_PORT', 8069),\n                       help='Port for the web server')\n    group.add_argument('--web-custom-html',\n                       required=False,\n                       default=os.environ.get(\n                           'MARABUNTA_WEB_CUSTOM_HTML'\n                       ),\n                       help='Path to a custom html file to publish')\n    return parser", "response": "Returns a parser for command line options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_current(self, current):\n        self.current = current\n        self.input = current.input\n        # self.req = current.request\n        # self.resp = current.response\n        self.output = current.output\n        self.cmd = current.task_data['cmd']\n\n        if self.cmd and NEXT_CMD_SPLITTER in self.cmd:\n            self.cmd, self.next_cmd = self.cmd.split(NEXT_CMD_SPLITTER)\n        else:\n            self.next_cmd = None", "response": "Sets the current attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef form_out(self, _form=None):\n        _form = _form or self.object_form\n        self.output['forms'] = _form.serialize()\n        self._add_meta_props(_form)\n        self.output['forms']['grouping'] = _form.Meta.grouping\n        self.output['forms']['constraints'] = _form.Meta.constraints\n        self._patch_form(self.output['forms'])\n        self.set_client_cmd('form')", "response": "Renders a single object form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding given cmd to the internal list of client commands.", "response": "def set_client_cmd(self, *args):\n        \"\"\"\n        Adds given cmd(s) to ``self.output['client_cmd']``\n\n        Args:\n            *args: Client commands.\n        \"\"\"\n        self.client_cmd.update(args)\n        self.output['client_cmd'] = list(self.client_cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        from zengine.models import User\n        user = User(username=self.manager.args.username, superuser=self.manager.args.super)\n        user.set_password(self.manager.args.password)\n        user.save()\n        print(\"New user created with ID: %s\" % user.key)", "response": "Creates user encrypts password."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        print(\"Development server started on http://%s:%s. \\n\\nPress Ctrl+C to stop\\n\" % (\n            self.manager.args.addr,\n            self.manager.args.port)\n              )\n        if self.manager.args.server_type == 'falcon':\n            self.run_with_falcon()\n        elif self.manager.args.server_type == 'tornado':\n            self.run_with_tornado()", "response": "Starts a development server for the zengine application."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the tornado test server with tornado.", "response": "def run_with_tornado(self):\n        \"\"\"\n        runs the tornado/websockets based test server\n        \"\"\"\n        from zengine.tornado_server.server import runserver\n        runserver(self.manager.args.addr, int(self.manager.args.port))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the falcon based test server", "response": "def run_with_falcon(self):\n        \"\"\"\n        runs the falcon/http based test server\n        \"\"\"\n        from wsgiref import simple_server\n        from zengine.server import app\n        httpd = simple_server.make_server(self.manager.args.addr, int(self.manager.args.port), app)\n        httpd.serve_forever()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a development server for the zengine application.", "response": "def run(self):\n        \"\"\"\n        Starts a development server for the zengine application\n        \"\"\"\n        from zengine.wf_daemon import run_workers, Worker\n\n        worker_count = int(self.manager.args.workers or 1)\n        if not self.manager.args.daemonize:\n            print(\"Starting worker(s)\")\n\n        if worker_count > 1 or self.manager.args.autoreload:\n            run_workers(worker_count,\n                        self.manager.args.paths.split(' '),\n                        self.manager.args.daemonize)\n        else:\n            worker = Worker()\n            worker.run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prepare_domain(mapping):\n        # Parse the domain-directory mapping\n        try:\n            domain, dir = mapping.split(':')\n        except ValueError:\n            print(\"Please provide the sources in the form of '<domain>:<directory>'\")\n            sys.exit(1)\n\n        try:\n            default_language = settings.TRANSLATION_DOMAINS[domain]\n        except KeyError:\n            print(\"Unknown domain {domain}, check the settings file to make sure\"\n                  \" this domain is set in TRANSLATION_DOMAINS\".format(domain=domain))\n            sys.exit(1)\n        # Create a temporary file to hold the `.pot` file for this domain\n        handle, path = tempfile.mkstemp(prefix='zengine_i18n_', suffix='.pot')\n        return (domain, {\n            'default': default_language,\n            'pot': path,\n            'source': dir,\n        })", "response": "Prepare a helper dictionary for the domain to temporarily hold some information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that all domains specified in the settings were provided in the options.", "response": "def _validate_domains(domains):\n        \"\"\"Check that all domains specified in the settings was provided in the options.\"\"\"\n        missing = set(settings.TRANSLATION_DOMAINS.keys()) - set(domains.keys())\n        if missing:\n            print('The following domains have been set in the configuration, '\n                  'but their sources were not provided, use the `--source` '\n                  'option to specify their sources: {domains}'.format(domains=', '.join(missing)))\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the translations into. pot files for the specified domains.", "response": "def _extract_translations(self, domains):\n        \"\"\"Extract the translations into `.pot` files\"\"\"\n        for domain, options in domains.items():\n            # Create the extractor\n            extractor = babel_frontend.extract_messages()\n            extractor.initialize_options()\n            # The temporary location to write the `.pot` file\n            extractor.output_file = options['pot']\n            # Add the comments marked with 'tn:' to the translation file for translators to read. Strip the marker.\n            extractor.add_comments = ['tn:']\n            extractor.strip_comments = True\n            # The directory where the sources for this domain are located\n            extractor.input_paths = [options['source']]\n            # Pass the metadata to the translator\n            extractor.msgid_bugs_address = self.manager.args.contact\n            extractor.copyright_holder = self.manager.args.copyright\n            extractor.version = self.manager.args.version\n            extractor.project = self.manager.args.project\n            extractor.finalize_options()\n            # Add keywords for lazy translation functions, based on their non-lazy variants\n            extractor.keywords.update({\n                'gettext_lazy': extractor.keywords['gettext'],\n                'ngettext_lazy': extractor.keywords['ngettext'],\n                '__': extractor.keywords['gettext'],  # double underscore for lazy\n            })\n            # Do the extraction\n            _run_babel_command(extractor)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_update_po_files(self, domains):\n        for language in settings.TRANSLATIONS:\n            for domain, options in domains.items():\n                if language == options['default']: continue  # Default language of the domain doesn't need translations\n                if os.path.isfile(_po_path(language, domain)):\n                    # If the translation already exists, update it, keeping the parts already translated\n                    self._update_po_file(language, domain, options['pot'])\n                else:\n                    # The translation doesn't exist, create a new translation file\n                    self._init_po_file(language, domain, options['pot'])", "response": "Update or initialize the. po files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cleanup(self, domains):\n        for option in domains.values():\n            try:\n                os.remove(option['pot'])\n            except (IOError, OSError):\n                # It is not a problem if we can't actually remove the temporary file\n                pass", "response": "Remove the temporary. pot files that were created for the domains."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        from zengine.lib.cache import WFSpecNames\n\n        if self.manager.args.clear:\n            self._clear_models()\n            return\n\n        if self.manager.args.wf_path:\n            paths = self.get_wf_from_path(self.manager.args.wf_path)\n        else:\n            paths = self.get_workflows()\n\n        self.count = 0\n\n        self.do_with_submit(self.load_diagram, paths, threads=self.manager.args.threads)\n\n        WFSpecNames().refresh()\n\n        print(\"%s BPMN file loaded\" % self.count)", "response": "reads workflows checks if it s updated tries to update if there aren t any running instances of that wf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_wf_from_path(self, path):\n        with open(path) as fp:\n            content = fp.read()\n        return [(os.path.basename(os.path.splitext(path)[0]), content), ]", "response": "Load xml from given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields XML content of all workflows found under WORKFLOW_PACKAGES_PATHS", "response": "def get_workflows(self):\n        \"\"\"\n        Scans and loads all wf found under WORKFLOW_PACKAGES_PATHS\n\n        Yields: XML content of diagram file\n\n        \"\"\"\n        for pth in settings.WORKFLOW_PACKAGES_PATHS:\n            for f in glob.glob(\"%s/*.bpmn\" % pth):\n                with open(f) as fp:\n                    yield os.path.basename(os.path.splitext(f)[0]), fp.read()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_migration_and_solr(self):\n        from pyoko.db.schema_update import SchemaUpdater\n        from socket import error as socket_error\n        from pyoko.conf import settings\n        from importlib import import_module\n\n        import_module(settings.MODELS_MODULE)\n        registry = import_module('pyoko.model').model_registry\n        models = [model for model in registry.get_base_models()]\n        try:\n            print(__(u\"Checking migration and solr ...\"))\n            updater = SchemaUpdater(models, 1, False)\n            updater.run(check_only=True)\n\n        except socket_error as e:\n            print(__(u\"{0}Error not connected, open redis and rabbitmq{1}\").format(CheckList.FAIL,\n                                                                                   CheckList.ENDC))", "response": "Check migration and solr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the connection to the Redis server", "response": "def check_redis():\n        \"\"\"\n            Redis checks the connection\n            It displays on the screen whether or not you have a connection.\n        \"\"\"\n        from pyoko.db.connection import cache\n        from redis.exceptions import ConnectionError\n\n        try:\n            cache.ping()\n            print(CheckList.OKGREEN + \"{0}Redis is working{1}\" + CheckList.ENDC)\n        except ConnectionError as e:\n            print(__(u\"{0}Redis is not working{1} \").format(CheckList.FAIL,\n                                                            CheckList.ENDC), e.message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the RabbitMQ connection and displays the connection details.", "response": "def check_mq_connection(self):\n        \"\"\"\n        RabbitMQ checks the connection\n        It displays on the screen whether or not you have a connection.\n        \"\"\"\n        import pika\n        from zengine.client_queue import BLOCKING_MQ_PARAMS\n        from pika.exceptions import ProbableAuthenticationError, ConnectionClosed\n\n        try:\n            connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)\n            channel = connection.channel()\n            if channel.is_open:\n                print(__(u\"{0}RabbitMQ is working{1}\").format(CheckList.OKGREEN, CheckList.ENDC))\n            elif self.channel.is_closed or self.channel.is_closing:\n                print(__(u\"{0}RabbitMQ is not working!{1}\").format(CheckList.FAIL, CheckList.ENDC))\n        except ConnectionClosed as e:\n            print(__(u\"{0}RabbitMQ is not working!{1}\").format(CheckList.FAIL, CheckList.ENDC), e)\n        except ProbableAuthenticationError as e:\n            print(__(u\"{0}RabbitMQ username and password wrong{1}\").format(CheckList.FAIL,\n                                                                           CheckList.ENDC))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the environment variables are correct and if they are using the correct variables.", "response": "def check_encoding_and_env():\n        \"\"\"\n        It brings the environment variables to the screen.\n        The user checks to see if they are using the correct variables.\n        \"\"\"\n        import sys\n        import os\n        if sys.getfilesystemencoding() in ['utf-8', 'UTF-8']:\n            print(__(u\"{0}File system encoding correct{1}\").format(CheckList.OKGREEN,\n                                                                   CheckList.ENDC))\n        else:\n            print(__(u\"{0}File system encoding wrong!!{1}\").format(CheckList.FAIL,\n                                                                   CheckList.ENDC))\n        check_env_list = ['RIAK_PROTOCOL', 'RIAK_SERVER', 'RIAK_PORT', 'REDIS_SERVER',\n                          'DEFAULT_BUCKET_TYPE', 'PYOKO_SETTINGS',\n                          'MQ_HOST', 'MQ_PORT', 'MQ_USER', 'MQ_VHOST',\n                          ]\n        env = os.environ\n        for k, v in env.items():\n            if k in check_env_list:\n                print(__(u\"{0}{1} : {2}{3}\").format(CheckList.BOLD, k, v, CheckList.ENDC))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind if the game is over.", "response": "def no_moves(position):\n    \"\"\"\n    Finds if the game is over.\n\n    :type: position: Board\n    :rtype: bool\n    \"\"\"\n    return position.no_moves(color.white) \\\n        or position.no_moves(color.black)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_checkmate(position, input_color):\n    return position.no_moves(input_color) and \\\n        position.get_king(input_color).in_check(position)", "response": "Returns True if particular King is checkmated."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a QuerySet object and a pagination data dict as a tuple containing the current page number and the number of objects per page.", "response": "def _paginate(self, current_page, query_set, per_page=10):\n    \"\"\"\n    Handles pagination of object listings.\n\n    Args:\n        current_page int:\n            Current page number\n        query_set (:class:`QuerySet<pyoko:pyoko.db.queryset.QuerySet>`):\n            Object listing queryset.\n        per_page int:\n            Objects per page.\n\n    Returns:\n        QuerySet object, pagination data dict as a tuple\n    \"\"\"\n    total_objects = query_set.count()\n    total_pages = int(total_objects / per_page or 1)\n    # add orphans to last page\n    current_per_page = per_page + (\n        total_objects % per_page if current_page == total_pages else 0)\n    pagination_data = dict(page=current_page,\n                           total_pages=total_pages,\n                           total_objects=total_objects,\n                           per_page=current_per_page)\n    query_set = query_set.set_params(rows=current_per_page, start=(current_page - 1) * per_page)\n    return query_set, pagination_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new message for the given channel.", "response": "def create_message(current):\n    \"\"\"\n    Creates a message for the given channel.\n\n    .. code-block:: python\n\n        # request:\n        {\n            'view':'_zops_create_message',\n            'message': {\n                'channel': key,     # of channel\n                'body': string,     # message text.,\n                'type': int,        # zengine.messaging.model.MSG_TYPES,\n                'attachments': [{\n                    'description': string,  # can be blank,\n                    'name': string,         # file name with extension,\n                    'content': string,      # base64 encoded file content\n                    }]}\n        # response:\n            {\n            'status': 'Created',\n            'code': 201,\n            'msg_key': key,     # key of the message object,\n            }\n\n    \"\"\"\n    msg = current.input['message']\n    msg_obj = Channel.add_message(msg['channel'], body=msg['body'], typ=msg['type'],\n                                  sender=current.user,\n                                  title=msg['title'], receiver=msg['receiver'] or None)\n    current.output = {\n        'msg_key': msg_obj.key,\n        'status': 'Created',\n        'code': 201\n    }\n    if 'attachment' in msg:\n        for atch in msg['attachments']:\n            typ = current._dedect_file_type(atch['name'], atch['content'])\n            Attachment(channel_id=msg['channel'], msg=msg_obj, name=atch['name'],\n                       file=atch['content'], description=atch['description'], typ=typ).save()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the channel content.", "response": "def show_channel(current, waited=False):\n    \"\"\"\n    Initial display of channel content.\n    Returns channel description, members, no of members, last 20 messages etc.\n\n\n    .. code-block:: python\n\n        #  request:\n            {\n                'view':'_zops_show_channel',\n                'key': key,\n            }\n\n        #  response:\n            {\n            'channel_key': key,\n            'description': string,\n            'no_of_members': int,\n            'member_list': [\n                {'name': string,\n                 'is_online': bool,\n                 'avatar_url': string,\n                }],\n            'name': string,\n            'last_messages': [MSG_DICT]\n            'status': 'OK',\n            'code': 200\n            }\n    \"\"\"\n    ch = Channel(current).objects.get(current.input['key'])\n    sbs = ch.get_subscription_for_user(current.user_id)\n    current.output = {'key': current.input['key'],\n                      'description': ch.description,\n                      'name': sbs.name,\n                      'actions': sbs.get_actions(),\n                      'avatar_url': ch.get_avatar(current.user),\n                      'no_of_members': len(ch.subscriber_set),\n                      'member_list': [{'name': sb.user.full_name,\n                                       'is_online': sb.user.is_online(),\n                                       'avatar_url': sb.user.get_avatar_url()\n                                       } for sb in ch.subscriber_set.objects.all()],\n                      'last_messages': [],\n                      'status': 'OK',\n                      'code': 200\n                      }\n    for msg in ch.get_last_messages():\n        current.output['last_messages'].insert(0, msg.serialize(current.user))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef channel_history(current):\n    current.output = {\n        'status': 'OK',\n        'code': 201,\n        'messages': []\n    }\n\n    for msg in list(Message.objects.filter(channel_id=current.input['channel_key'],\n                                      updated_at__lte=current.input['timestamp'])[:20]):\n        current.output['messages'].insert(0, msg.serialize(current.user))\n    # FIXME: looks like  pyoko's __lt is broken\n    # TODO: convert lte to lt and remove this block, when __lt filter fixed\n    if current.output['messages']:\n        current.output['messages'].pop(-1)", "response": "Get old messages for a channel. 20 messages per request\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes timestamp of latest message of an ACTIVE channel. This view should be called with timestamp of latest message; - When user opens (clicks on) a channel. - Periodically (eg: setInterval for 15secs) while user staying in a channel. .. code-block:: python # request: { 'view':'_zops_last_seen_msg', 'channel_key': key, 'key': key, 'timestamp': datetime, } # response: { 'status': 'OK', 'code': 200, }", "response": "def report_last_seen_message(current):\n    \"\"\"\n    Push timestamp of latest message of an ACTIVE channel.\n\n    This view should be called with timestamp of latest message;\n    - When user opens (clicks on) a channel.\n    - Periodically (eg: setInterval for 15secs) while user staying in a channel.\n\n\n    .. code-block:: python\n\n        #  request:\n            {\n            'view':'_zops_last_seen_msg',\n            'channel_key': key,\n            'key': key,\n            'timestamp': datetime,\n            }\n\n        #  response:\n            {\n            'status': 'OK',\n            'code': 200,\n            }\n    \"\"\"\n    sbs = Subscriber(current).objects.filter(channel_id=current.input['channel_key'],\n                                             user_id=current.user_id)[0]\n    sbs.last_seen_msg_time = current.input['timestamp']\n    sbs.save()\n    current.output = {\n        'status': 'OK',\n        'code': 200}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_channels(current):\n    current.output = {\n        'status': 'OK',\n        'code': 200,\n        'channels': []}\n    for sbs in current.user.subscriptions.objects.filter(is_visible=True):\n        try:\n            current.output['channels'].append(sbs.get_channel_listing())\n        except ObjectDoesNotExist:\n            # FIXME: This should not happen,\n            log.exception(\"UNPAIRED DIRECT EXCHANGES!!!!\")\n            sbs.delete()", "response": "List the channel memberships of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unread_count(current):\n    unread_ntf = 0\n    unread_msg = 0\n    for sbs in current.user.subscriptions.objects.filter(is_visible=True):\n        try:\n            if sbs.channel.key == current.user.prv_exchange:\n                unread_ntf += sbs.unread_count()\n            else:\n                unread_msg += sbs.unread_count()\n        except ObjectDoesNotExist:\n            # FIXME: This should not happen,\n            log.exception(\"MULTIPLE PRV EXCHANGES!!!!\")\n            sbs.delete()\n    current.output = {\n        'status': 'OK',\n        'code': 200,\n        'notifications': unread_ntf,\n        'messages': unread_msg\n    }", "response": "Unread count of all unread messages for current user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_notifications(current):\n    current.output = {\n        'status': 'OK',\n        'code': 200,\n        'notifications': [],\n    }\n    amount = current.input.get('amount', 8)\n    try:\n        notif_sbs = current.user.subscriptions.objects.get(channel_id=current.user.prv_exchange)\n    except MultipleObjectsReturned:\n        # FIXME: This should not happen,\n        log.exception(\"MULTIPLE PRV EXCHANGES!!!!\")\n        sbs = current.user.subscriptions.objects.filter(channel_id=current.user.prv_exchange)\n        sbs[0].delete()\n        notif_sbs = sbs[1]\n    for msg in notif_sbs.channel.message_set.objects.all()[:amount]:\n        current.output['notifications'].insert(0, {\n            'title': msg.msg_title,\n            'body': msg.body,\n            'type': msg.typ,\n            'url': msg.url,\n            'channel_key': msg.channel.key,\n            'message_key': msg.key,\n            'timestamp': msg.updated_at})", "response": "Returns last N notifications for current user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_channel(current):\n    channel = Channel(name=current.input['name'],\n                      description=current.input['description'],\n                      owner=current.user,\n                      typ=15).save()\n    with BlockSave(Subscriber):\n        Subscriber.objects.get_or_create(user=channel.owner,\n                                         channel=channel,\n                                         can_manage=True,\n                                         can_leave=False)\n    current.input['key'] = channel.key\n    show_channel(current)\n    current.output.update({\n        'status': 'Created',\n        'code': 201\n    })", "response": "Create a public channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes member to a channel and returns a new list of members.", "response": "def add_members(current):\n    \"\"\"\n        Subscribe member(s) to a channel\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_add_members',\n                'channel_key': key,\n                'read_only': boolean, # true if this is a Broadcast channel,\n                                      # false if it's a normal chat room\n                'members': [key, key],\n                }\n\n            #  response:\n                {\n                'existing': [key,], # existing members\n                'newly_added': [key,], # newly added members\n                'status': 'Created',\n                'code': 201\n                }\n    \"\"\"\n    newly_added, existing = [], []\n    read_only = current.input['read_only']\n    for member_key in current.input['members']:\n        sb, new = Subscriber(current).objects.get_or_create(user_id=member_key,\n                                                            read_only=read_only,\n                                                            channel_id=current.input['channel_key'])\n        if new:\n            newly_added.append(member_key)\n        else:\n            existing.append(member_key)\n\n    current.output = {\n        'existing': existing,\n        'newly_added': newly_added,\n        'status': 'OK',\n        'code': 201\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_unit_to_channel(current):\n    read_only = current.input['read_only']\n    newly_added, existing = [], []\n    for member_key in UnitModel.get_user_keys(current, current.input['unit_key']):\n        sb, new = Subscriber(current).objects.get_or_create(user_id=member_key,\n                                                            read_only=read_only,\n                                                            channel_id=current.input['channel_key'])\n        if new:\n            newly_added.append(member_key)\n        else:\n            existing.append(member_key)\n\n    current.output = {\n        'existing': existing,\n        'newly_added': newly_added,\n        'status': 'OK',\n        'code': 201\n    }", "response": "Subscribes users of a given unit to a given channel"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_user(current):\n    current.output = {\n        'results': [],\n        'status': 'OK',\n        'code': 201\n    }\n    qs = UserModel(current).objects.exclude(key=current.user_id).search_on(\n        *settings.MESSAGING_USER_SEARCH_FIELDS,\n        contains=current.input['query'])\n    # FIXME: somehow exclude(key=current.user_id) not working with search_on()\n\n    for user in qs:\n        if user.key != current.user_id:\n            current.output['results'].append((user.full_name, user.key, user.get_avatar_url()))", "response": "Search users for adding to a public room\n            or creating one to one direct messaging\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching on units for subscribing it s users to a channel", "response": "def search_unit(current):\n    \"\"\"\n        Search on units for subscribing it's users to a channel\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_search_unit',\n                'query': string,\n                }\n\n            #  response:\n                {\n                'results': [('name', 'key'), ],\n                'status': 'OK',\n                'code': 200\n                }\n    \"\"\"\n    current.output = {\n        'results': [],\n        'status': 'OK',\n        'code': 201\n    }\n    for user in UnitModel(current).objects.search_on(*settings.MESSAGING_UNIT_SEARCH_FIELDS,\n                                                     contains=current.input['query']):\n        current.output['results'].append((user.name, user.key))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a One - To - One channel between current and selected user.", "response": "def create_direct_channel(current):\n    \"\"\"\n    Create a One-To-One channel between current and selected user.\n\n\n    .. code-block:: python\n\n        #  request:\n            {\n            'view':'_zops_create_direct_channel',\n            'user_key': key,\n            }\n\n        #  response:\n            {\n            'description': string,\n            'no_of_members': int,\n            'member_list': [\n                {'name': string,\n                 'is_online': bool,\n                 'avatar_url': string,\n                }],\n            'last_messages': [MSG_DICT]\n            'status': 'Created',\n            'code': 201,\n            'channel_key': key, # of just created channel\n            'name': string, # name of subscribed channel\n            }\n    \"\"\"\n    channel, sub_name = Channel.get_or_create_direct_channel(current.user_id,\n                                                             current.input['user_key'])\n    current.input['key'] = channel.key\n    show_channel(current)\n    current.output.update({\n        'status': 'Created',\n        'code': 201\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding a message in the database.", "response": "def find_message(current):\n    \"\"\"\n        Search in messages. If \"channel_key\" given, search will be limited to that channel,\n        otherwise search will be performed on all of user's subscribed channels.\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_search_unit,\n                'channel_key': key,\n                'query': string,\n                'page': int,\n                }\n\n            #  response:\n                {\n                'results': [MSG_DICT, ],\n                'pagination': {\n                    'page': int, # current page\n                    'total_pages': int,\n                    'total_objects': int,\n                    'per_page': int, # object per page\n                    },\n                'status': 'OK',\n                'code': 200\n                }\n    \"\"\"\n    current.output = {\n        'results': [],\n        'status': 'OK',\n        'code': 201\n    }\n    query_set = Message(current).objects.search_on(['msg_title', 'body', 'url'],\n                                                   contains=current.input['query'])\n    if current.input['channel_key']:\n        query_set = query_set.filter(channel_id=current.input['channel_key'])\n    else:\n        subscribed_channels = Subscriber.objects.filter(user_id=current.user_id).values_list(\n            \"channel_id\", flatten=True)\n        query_set = query_set.filter(channel_id__in=subscribed_channels)\n\n    query_set, pagination_data = _paginate(current_page=current.input['page'], query_set=query_set)\n    current.output['pagination'] = pagination_data\n    for msg in query_set:\n        current.output['results'].append(msg.serialize(current.user))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_channel(current):\n    ch_key = current.input['channel_key']\n\n    ch = Channel(current).objects.get(owner_id=current.user_id, key=ch_key)\n    ch.delete()\n    Subscriber.objects.filter(channel_id=ch_key).delete()\n    Message.objects.filter(channel_id=ch_key).delete()\n    current.output = {'status': 'Deleted', 'code': 200}", "response": "Delete a channel in the\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nediting channel name or description.", "response": "def edit_channel(current):\n    \"\"\"\n        Update channel name or description\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_edit_channel,\n                'channel_key': key,\n                'name': string,\n                'description': string,\n                }\n\n            #  response:\n                {\n                'status': 'OK',\n                'code': 200\n                }\n    \"\"\"\n    ch = Channel(current).objects.get(owner_id=current.user_id,\n                                      key=current.input['channel_key'])\n    ch.name = current.input['name']\n    ch.description = current.input['description']\n    ch.save()\n    for sbs in ch.subscriber_set.objects.all():\n        sbs.name = ch.name\n        sbs.save()\n    current.output = {'status': 'OK', 'code': 200}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npin a channel to top of the channel list", "response": "def pin_channel(current):\n    \"\"\"\n        Pin a channel to top of channel list\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_pin_channel,\n                'channel_key': key,\n                }\n\n            #  response:\n                {\n                'status': 'OK',\n                'code': 200\n                }\n    \"\"\"\n    try:\n        Subscriber(current).objects.filter(user_id=current.user_id,\n                                           channel_id=current.input['channel_key']).update(\n            pinned=True)\n        current.output = {'status': 'OK', 'code': 200}\n    except ObjectDoesNotExist:\n        raise HTTPError(404, \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a message from the resource.", "response": "def delete_message(current):\n    \"\"\"\n        Delete a message\n\n        .. code-block:: python\n\n            #  request:\n                {\n                'view':'_zops_delete_message,\n                'message_key': key,\n                }\n\n            #  response:\n                {\n                'key': key,\n                'status': 'OK',\n                'code': 200\n                }\n    \"\"\"\n    try:\n        Message(current).objects.get(sender_id=current.user_id,\n                                     key=current.input['key']).delete()\n        current.output = {'status': 'Deleted', 'code': 200, 'key': current.input['key']}\n    except ObjectDoesNotExist:\n        raise HTTPError(404, \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit a message a user own.", "response": "def edit_message(current):\n    \"\"\"\n    Edit a message a user own.\n\n    .. code-block:: python\n\n        # request:\n        {\n            'view':'_zops_edit_message',\n            'message': {\n                'body': string,     # message text\n                'key': key\n                }\n        }\n        # response:\n            {\n            'status': string,   # 'OK' for success\n            'code': int,        # 200 for success\n            }\n\n    \"\"\"\n    current.output = {'status': 'OK', 'code': 200}\n    in_msg = current.input['message']\n    try:\n        msg = Message(current).objects.get(sender_id=current.user_id, key=in_msg['key'])\n        msg.body = in_msg['body']\n        msg.save()\n    except ObjectDoesNotExist:\n        raise HTTPError(404, \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flag_message(current):\n    current.output = {'status': 'Created', 'code': 201}\n    FlaggedMessage.objects.get_or_create(user_id=current.user_id,\n                                         message_id=current.input['key'])", "response": "Flag inappropriate messages\n\n    .. code-block:: python\n\n        # request:\n        {\n            'view':'_zops_flag_message',\n            'message_key': key,\n        }\n        # response:\n            {\n            '\n            'status': 'Created',\n            'code': 201,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unflag_message(current):\n    current.output = {'status': 'OK', 'code': 200}\n\n    FlaggedMessage(current).objects.filter(user_id=current.user_id,\n                                           message_id=current.input['key']).delete()", "response": "Unflag a message in a node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_message_actions(current):\n    current.output = {'status': 'OK',\n                      'code': 200,\n                      'actions': Message.objects.get(\n                          current.input['key']).get_actions_for(current.user)}", "response": "Returns applicable actions for current user for given message key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_to_favorites(current):\n    msg = Message.objects.get(current.input['key'])\n    current.output = {'status': 'Created', 'code': 201}\n    fav, new = Favorite.objects.get_or_create(user_id=current.user_id, message=msg)\n    current.output['favorite_key'] = fav.key", "response": "Add a new message to the list of favorites."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_from_favorites(current):\n    try:\n        current.output = {'status': 'OK', 'code': 200}\n        Favorite(current).objects.get(user_id=current.user_id,\n                                      key=current.input['key']).delete()\n    except ObjectDoesNotExist:\n        raise HTTPError(404, \"\")", "response": "Remove a message from the list of user s favorites"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_favorites(current):\n    current.output = {'status': 'OK', 'code': 200, 'favorites': []}\n    query_set = Favorite(current).objects.filter(user_id=current.user_id)\n    if current.input['channel_key']:\n        query_set = query_set.filter(channel_id=current.input['channel_key'])\n    current.output['favorites'] = [{\n                                       'key': fav.key,\n                                       'channel_key': fav.channel.key,\n                                       'message_key': fav.message.key,\n                                       'message_summary': fav.summary,\n                                       'channel_name': fav.channel_name\n                                   } for fav in query_set]", "response": "List user s favorites. If channel_key is given will return favorites belong to that channel. If user_id is given will return favorites belong to that channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a direct messaging channel between two user objects.", "response": "def get_or_create_direct_channel(cls, initiator_key, receiver_key):\n        \"\"\"\n        Creates a  direct messaging channel between two user\n\n        Args:\n            initiator: User, who want's to make first contact\n            receiver: User, other party\n\n        Returns:\n            (Channel, receiver_name)\n        \"\"\"\n        existing = cls.objects.OR().filter(\n            code_name='%s_%s' % (initiator_key, receiver_key)).filter(\n            code_name='%s_%s' % (receiver_key, initiator_key))\n        receiver_name = UserModel.objects.get(receiver_key).full_name\n        if existing:\n            channel = existing[0]\n        else:\n            channel_name = '%s_%s' % (initiator_key, receiver_key)\n            channel = cls(is_direct=True, code_name=channel_name, typ=10).blocking_save()\n        with BlockSave(Subscriber):\n            Subscriber.objects.get_or_create(channel=channel,\n                                             user_id=initiator_key,\n                                             name=receiver_name)\n            Subscriber.objects.get_or_create(channel=channel,\n                                             user_id=receiver_key,\n                                             name=UserModel.objects.get(initiator_key).full_name)\n        return channel, receiver_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a MQ exchange for this channel.", "response": "def create_exchange(self):\n        \"\"\"\n        Creates MQ exchange for this channel\n        Needs to be defined only once.\n        \"\"\"\n        mq_channel = self._connect_mq()\n        mq_channel.exchange_declare(exchange=self.code_name,\n                                    exchange_type='fanout',\n                                    durable=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_exchange(self):\n        mq_channel = self._connect_mq()\n        mq_channel.exchange_delete(exchange=self.code_name)", "response": "Deletes the MQ exchange for this channel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_channel_listing(self):\n        return {'name': self.name,\n                'key': self.channel.key,\n                'type': self.channel.typ,\n                'read_only': self.read_only,\n                'is_online': self.is_online(),\n                'actions': self.get_actions(),\n                'unread': self.unread_count()}", "response": "Returns a dict of the channel listing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_exchange(self):\n        channel = self._connect_mq()\n        channel.exchange_declare(exchange=self.user.prv_exchange,\n                                 exchange_type='fanout',\n                                 durable=True)", "response": "Creates a private exchange for the current user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbind users private exchange to channel", "response": "def bind_to_channel(self):\n        \"\"\"\n        Binds (subscribes) users private exchange to channel exchange\n        Automatically called at creation of subscription record.\n        \"\"\"\n        if self.channel.code_name != self.user.prv_exchange:\n            channel = self._connect_mq()\n            channel.exchange_bind(source=self.channel.code_name, destination=self.user.prv_exchange)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize the object for given user.", "response": "def serialize(self, user=None):\n        \"\"\"\n        Serializes message for given user.\n\n        Note:\n            Should be called before first save(). Otherwise \"is_update\" will get wrong value.\n\n        Args:\n            user: User object\n\n        Returns:\n            Dict. JSON serialization ready dictionary object\n        \"\"\"\n        return {\n            'content': self.body,\n            'type': self.typ,\n            'updated_at': self.updated_at,\n            'timestamp': self.updated_at,\n            'is_update': not hasattr(self, 'unsaved'),\n            'attachments': [attachment.serialize() for attachment in self.attachment_set],\n            'title': self.msg_title,\n            'url': self.url,\n            'sender_name': self.sender.full_name,\n            'sender_key': self.sender.key,\n            'channel_key': self.channel.key,\n            'cmd': 'message',\n            'avatar_url': self.sender.avatar,\n            'key': self.key,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef defaultCrawlId():\n\n    timestamp = datetime.now().isoformat().replace(':', '_')\n    user = getuser()\n    return '_'.join(('crawl', user, timestamp))", "response": "Provide a reasonable default crawl name using the user name and date"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(argv=None):\n    global Verbose, Mock\n    if argv is None:\n        argv = sys.argv\n\n    if len(argv) < 5: die('Bad args')\n    try:\n        opts, argv = getopt.getopt(argv[1:], 'hs:p:mv',\n          ['help', 'server=', 'port=', 'mock', 'verbose'])\n    except getopt.GetoptError as err:\n        # print help information and exit:\n        print(err) # will print something like \"option -a not recognized\"\n        die()\n\n    serverEndpoint = DefaultServerEndpoint\n    # TODO: Fix this\n    for opt, val in opts:\n        if opt   in ('-h', '--help'):    echo2(USAGE); sys.exit()\n        elif opt in ('-s', '--server'):  serverEndpoint = val\n        elif opt in ('-p', '--port'):    serverEndpoint = 'http://localhost:%s' % val\n        elif opt in ('-m', '--mock'):    Mock = 1\n        elif opt in ('-v', '--verbose'): Verbose = 1\n        else: die(USAGE)\n\n    cmd = argv[0]\n    crawlId = argv[1]\n    confId = argv[2]\n    urlDir = argv[3]\n    args = {}\n    if len(argv) > 4: args = eval(argv[4])\n\n    nt = Nutch(crawlId, confId, serverEndpoint, urlDir)\n    nt.Jobs().create(cmd, **args)", "response": "Run Nutch command using REST API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the Nutch Server and return the response.", "response": "def call(self, verb, servicePath, data=None, headers=None, forceText=False, sendJson=True):\n        \"\"\"Call the Nutch Server, do some error checking, and return the response.\n\n        :param verb: One of nutch.RequestVerbs\n        :param servicePath: path component of URL to append to endpoint, e.g. '/config'\n        :param data: Data to attach to this request\n        :param headers: headers to attach to this request, default are JsonAcceptHeader\n        :param forceText: don't trust the response headers and just get the text\n        :param sendJson: Whether to treat attached data as JSON or not\n        \"\"\"\n\n        default_data = {} if sendJson else \"\"\n        data = data if data else default_data\n\n        headers = headers if headers else JsonAcceptHeader.copy()\n\n        if not sendJson:\n            headers.update(TextSendHeader)\n\n        if verb not in RequestVerbs:\n            die('Server call verb must be one of %s' % str(RequestVerbs.keys()))\n        if Verbose:\n            echo2(\"%s Endpoint:\" % verb.upper(), servicePath)\n            echo2(\"%s Request data:\" % verb.upper(), data)\n            echo2(\"%s Request headers:\" % verb.upper(), headers)\n        verbFn = RequestVerbs[verb]\n\n        if sendJson:\n            resp = verbFn(self.serverEndpoint + servicePath, json=data, headers=headers)\n        else:\n            resp = verbFn(self.serverEndpoint + servicePath, data=data, headers=headers)\n\n        if Verbose:\n            echo2(\"Response headers:\", resp.headers)\n            echo2(\"Response status:\", resp.status_code)\n        if resp.status_code != 200:\n            if self.raiseErrors:\n                error = NutchException(\"Unexpected server response: %d\" % resp.status_code)\n                error.status_code = resp.status_code\n                raise error\n            else:\n                warn('Nutch server returned status:', resp.status_code)\n        if forceText or 'content-type' not in resp.headers or resp.headers['content-type'] == 'text/plain':\n            if Verbose:\n                echo2(\"Response text:\", resp.text)\n            return resp.text\n\n        content_type = resp.headers['content-type']\n        if content_type == 'application/json' and not forceText:\n            if Verbose:\n                echo2(\"Response JSON:\", resp.json())\n            return resp.json()\n        else:\n            die('Did not understand server response: %s' % resp.headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new named ( cid ) configuration from a parameter dictionary.", "response": "def create(self, cid, configData):\n        \"\"\"\n        Create a new named (cid) configuration from a parameter dictionary (config_data).\n        \"\"\"\n        configArgs = {'configId': cid, 'params': configData, 'force': True}\n        cid = self.server.call('post', \"/config/create\", configArgs, forceText=True, headers=TextAcceptHeader)\n        new_config = Config(cid, self.server)\n        return new_config"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all the jobs owned by this Client.", "response": "def list(self, allJobs=False):\n        \"\"\"\n        Return list of jobs at this endpoint.\n\n        Call get(allJobs=True) to see all jobs, not just the ones managed by this Client\n        \"\"\"\n\n        jobs = self.server.call('get', '/job')\n\n        return [Job(job['id'], self.server) for job in jobs if allJobs or self._job_owned(job)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, command, **args):\n\n        command = command.upper()\n        if command not in LegalJobs:\n            warn('Nutch command must be one of: %s' % ', '.join(LegalJobs))\n        else:\n            echo2('Starting %s job with args %s' % (command, str(args)))\n        parameters = self.parameters.copy()\n        parameters['type'] = command\n        parameters['crawlId'] = self.crawlId\n        parameters['confId'] = self.confId\n        parameters['args'].update(args)\n\n        job_info = self.server.call('post', \"/job/create\", parameters, JsonAcceptHeader)\n\n        job = Job(job_info['id'], self.server)\n        return job", "response": "Create a job given a command and args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninjects a new job into a seed list.", "response": "def inject(self, seed=None, urlDir=None, **args):\n        \"\"\"\n        :param seed: A Seed object (this or urlDir must be specified)\n        :param urlDir: The directory on the server containing the seed list (this or urlDir must be specified)\n        :param args: Extra arguments for the job\n        :return: a created Job object\n        \"\"\"\n\n        if seed:\n            if urlDir and urlDir != seed.seedPath:\n                raise NutchException(\"Can't specify both seed and urlDir\")\n            urlDir = seed.seedPath\n        elif urlDir:\n            pass\n        else:\n            raise NutchException(\"Must specify seed or urlDir\")\n        args['url_dir'] = urlDir\n        return self.create('INJECT', **args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new named ( sid ) Seed from a list of seed URLs.", "response": "def create(self, sid, seedList):\n        \"\"\"\n        Create a new named (sid) Seed from a list of seed URLs\n\n        :param sid: the name to assign to the new seed list\n        :param seedList: the list of seeds to use\n        :return: the created Seed object\n        \"\"\"\n\n        seedUrl = lambda uid, url: {\"id\": uid, \"url\": url}\n\n        if not isinstance(seedList,tuple):\n            seedList = (seedList,)\n\n        seedListData = {\n            \"id\": \"12345\",\n            \"name\": sid,\n            \"seedUrls\": [seedUrl(uid, url) for uid, url in enumerate(seedList)]\n        }\n\n        # As per resolution of https://issues.apache.org/jira/browse/NUTCH-2123\n        seedPath = self.server.call('post', \"/seed/create\", seedListData, TextAcceptHeader)\n        new_seed = Seed(sid, seedPath, self.server)\n        return new_seed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new named ( sid ) Seed from a file containing URLs", "response": "def createFromFile(self, sid, filename):\n        \"\"\"\n        Create a new named (sid) Seed from a file containing URLs\n        It's assumed URLs are whitespace seperated.\n\n        :param sid: the name to assign to the new seed list\n        :param filename: the name of the file that contains URLs\n        :return: the created Seed object\n        \"\"\"\n\n        urls = []\n        with open(filename) as f:\n            for line in f:\n                for url in line.split():\n                    urls.append(url)\n\n        return self.create(sid, tuple(urls))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the next job in a given round.", "response": "def _nextJob(self, job, nextRound=True):\n        \"\"\"\n        Given a completed job, start the next job in the round, or return None\n\n        :param nextRound: whether to start jobs from the next round if the current round is completed.\n        :return: the newly started Job, or None if no job was started\n        \"\"\"\n\n        jobInfo = job.info()\n        assert jobInfo['state'] == 'FINISHED'\n\n        roundEnd = False\n        if jobInfo['type'] == 'INJECT':\n            nextCommand = 'GENERATE'\n        elif jobInfo['type'] == 'GENERATE':\n            nextCommand = 'FETCH'\n        elif jobInfo['type'] == 'FETCH':\n            nextCommand = 'PARSE'\n        elif jobInfo['type'] == 'PARSE':\n            nextCommand = 'UPDATEDB'\n        elif jobInfo['type'] == 'UPDATEDB':\n            nextCommand = 'INVERTLINKS'\n        elif jobInfo['type'] == 'INVERTLINKS':\n            nextCommand = 'DEDUP'\n        elif jobInfo['type'] == 'DEDUP':\n            if self.enable_index:\n                nextCommand = 'INDEX'\n            else:\n                roundEnd = True\n        elif jobInfo['type'] == 'INDEX':\n            roundEnd = True\n        else:\n            raise NutchException(\"Unrecognized job type {}\".format(jobInfo['type']))\n\n        if roundEnd:\n            if nextRound and self.currentRound < self.totalRounds:\n                nextCommand = 'GENERATE'\n                self.currentRound += 1\n            else:\n                return None\n\n        return self.jobClient.create(nextCommand)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef progress(self, nextRound=True):\n\n        currentJob = self.currentJob\n        if currentJob is None:\n            return currentJob\n\n        jobInfo = currentJob.info()\n\n        if jobInfo['state'] == 'RUNNING':\n            return currentJob\n        elif jobInfo['state'] == 'FINISHED':\n            nextJob = self._nextJob(currentJob, nextRound)\n            self.currentJob = nextJob\n            return nextJob\n        else:\n            error = NutchCrawlException(\"Unexpected job state: {}\".format(jobInfo['state']))\n            error.current_job = currentJob\n            raise NutchCrawlException", "response": "Check the status of the current job and activate the next job if it s finished and return the active job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute all jobs in the current round and return when they have finished.", "response": "def nextRound(self):\n        \"\"\"\n        Execute all jobs in the current round and return when they have finished.\n\n        If a job fails, a NutchCrawlException will be raised, with all completed jobs from this round attached\n        to the exception.\n\n        :return: a list of all completed Jobs\n        \"\"\"\n\n        finishedJobs = []\n        if self.currentJob is None:\n            self.currentJob = self.jobClient.create('GENERATE')\n\n        activeJob = self.progress(nextRound=False)\n        while activeJob:\n            oldJob = activeJob\n            activeJob = self.progress(nextRound=False)  # updates self.currentJob\n            if oldJob and oldJob != activeJob:\n                finishedJobs.append(oldJob)\n            sleep(self.sleepTime)\n        self.currentRound += 1\n        return finishedJobs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes all queued rounds and return when they have finished.", "response": "def waitAll(self):\n        \"\"\"\n        Execute all queued rounds and return when they have finished.\n\n        If a job fails, a NutchCrawlException will be raised, with all completed jobs attached\n        to the exception\n\n        :return: a list of jobs completed for each round, organized by round (list-of-lists)\n        \"\"\"\n\n        finishedRounds = [self.nextRound()]\n\n        while self.currentRound < self.totalRounds:\n            finishedRounds.append(self.nextRound())\n\n        return finishedRounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a JobClient for listing and creating jobs.", "response": "def Jobs(self, crawlId=None):\n        \"\"\"\n        Create a JobClient for listing and creating jobs.\n        The JobClient inherits the confId from the Nutch client.\n\n        :param crawlId: crawlIds to use for this client.  If not provided, will be generated\n         by nutch.defaultCrawlId()\n        :return: a JobClient\n        \"\"\"\n        crawlId = crawlId if crawlId else defaultCrawlId()\n        return JobClient(self.server, crawlId, self.confId)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Crawl(self, seed, seedClient=None, jobClient=None, rounds=1, index=True):\n        if seedClient is None:\n            seedClient = self.Seeds()\n        if jobClient is None:\n            jobClient = self.Jobs()\n\n        if type(seed) != Seed:\n            seed = seedClient.create(jobClient.crawlId + '_seeds', seed)\n        return CrawlClient(self.server, seed, jobClient, rounds, index)", "response": "Launch a crawl using the given seed and jobClient."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nginx_access(line):\n    '''\n    >>> import pprint\n    >>> input_line1 = '{ \\\n                    \"remote_addr\": \"127.0.0.1\",\"remote_user\": \"-\",\"timestamp\": \"1515144699.201\", \\\n                    \"request\": \"GET / HTTP/1.1\",\"status\": \"200\",\"request_time\": \"0.000\", \\\n                    \"body_bytes_sent\": \"396\",\"http_referer\": \"-\",\"http_user_agent\": \"python-requests/2.18.4\", \\\n                    \"http_x_forwarded_for\": \"-\",\"upstream_response_time\": \"-\" \\\n                        }'\n    >>> output_line1 = nginx_access(input_line1)\n    >>> pprint.pprint(output_line1)\n    {'data': {u'body_bytes_sent': 396.0,\n              u'http_referer': u'-',\n              u'http_user_agent': u'python-requests/2.18.4',\n              u'http_x_forwarded_for': u'-',\n              u'remote_addr': u'127.0.0.1',\n              u'remote_user': u'-',\n              u'request': u'GET / HTTP/1.1',\n              u'request_time': 0.0,\n              u'status': u'200',\n              u'timestamp': '2018-01-05T09:31:39.201000',\n              u'upstream_response_time': 0.0},\n     'event': 'nginx_event',\n     'timestamp': '2018-01-05T09:31:39.201000',\n     'type': 'metric'}\n\n    >>> input_line2 = '{ \\\n                    \"remote_addr\": \"192.158.0.51\",\"remote_user\": \"-\",\"timestamp\": \"1515143686.415\", \\\n                    \"request\": \"POST /mpub?topic=heartbeat HTTP/1.1\",\"status\": \"404\",\"request_time\": \"0.000\", \\\n                    \"body_bytes_sent\": \"152\",\"http_referer\": \"-\",\"http_user_agent\": \"python-requests/2.18.4\", \\\n                    \"http_x_forwarded_for\": \"-\",\"upstream_response_time\": \"-\" \\\n                       }'\n    >>> output_line2 = nginx_access(input_line2)\n    >>> pprint.pprint(output_line2)\n    {'data': {u'body_bytes_sent': 152.0,\n              u'http_referer': u'-',\n              u'http_user_agent': u'python-requests/2.18.4',\n              u'http_x_forwarded_for': u'-',\n              u'remote_addr': u'192.158.0.51',\n              u'remote_user': u'-',\n              u'request': u'POST /mpub?topic=heartbeat HTTP/1.1',\n              u'request_time': 0.0,\n              u'status': u'404',\n              u'timestamp': '2018-01-05T09:14:46.415000',\n              u'upstream_response_time': 0.0},\n     'event': 'nginx_event',\n     'timestamp': '2018-01-05T09:14:46.415000',\n     'type': 'metric'}\n    '''\n#TODO Handle nginx error logs\n    log = json.loads(line)\n    timestamp_iso = datetime.datetime.utcfromtimestamp(float(log['timestamp'])).isoformat()\n    log.update({'timestamp':timestamp_iso})\n    if '-' in log.get('upstream_response_time'):\n        log['upstream_response_time'] = 0.0\n    log['body_bytes_sent'] = float(log['body_bytes_sent'])\n    log['request_time'] = float(log['request_time'])\n    log['upstream_response_time'] = float(log['upstream_response_time'])\n\n    return dict(\n        timestamp=log.get('timestamp',' '),\n        data=log,\n        type='metric',\n        event='nginx_event',\n    )", "response": "This function is used to parse the line of a nginx access request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef django(line):\n    '''\n    >>> import pprint\n    >>> input_line1 = '[23/Aug/2017 11:35:25] INFO [app.middleware_log_req:50]View func called:{\"exception\": null,\"processing_time\": 0.00011801719665527344, \"url\": \"<url>\",host\": \"localhost\", \"user\": \"testing\", \"post_contents\": \"\", \"method\": \"POST\" }'\n    >>> output_line1 = django(input_line1)\n    >>> pprint.pprint(output_line1)\n    {'data': {'loglevel': 'INFO',\n              'logname': '[app.middleware_log_req:50]',\n              'message': 'View func called:{\"exception\": null,\"processing_time\": 0.00011801719665527344, \"url\": \"<url>\",host\": \"localhost\", \"user\": \"testing\", \"post_contents\": \"\", \"method\": \"POST\" }',\n              'timestamp': '2017-08-23T11:35:25'},\n     'level': 'INFO',\n     'timestamp': '2017-08-23T11:35:25'}\n\n    >>> input_line2 = '[22/Sep/2017 06:32:15] INFO [app.function:6022] {\"UUID\": \"c47f3530-9f5f-11e7-a559-917d011459f7\", \"timestamp\":1506061932546, \"misc\": {\"status\": 200, \"ready_state\": 4, \"end_time_ms\": 1506061932546, \"url\": \"/api/function?\", \"start_time_ms\": 1506061932113, \"response_length\": 31, \"status_message\": \"OK\", \"request_time_ms\": 433}, \"user\": \"root\", \"host_url\": \"localhost:8888\", \"message\": \"ajax success\"}'\n    >>> output_line2 = django(input_line2)\n    >>> pprint.pprint(output_line2)\n    {'data': {'loglevel': 'INFO',\n              'logname': '[app.function:6022]',\n              'message': {u'UUID': u'c47f3530-9f5f-11e7-a559-917d011459f7',\n                          u'host_url': u'localhost:8888',\n                          u'message': u'ajax success',\n                          u'misc': {u'end_time_ms': 1506061932546L,\n                                    u'ready_state': 4,\n                                    u'request_time_ms': 433,\n                                    u'response_length': 31,\n                                    u'start_time_ms': 1506061932113L,\n                                    u'status': 200,\n                                    u'status_message': u'OK',\n                                    u'url': u'/api/function?'},\n                          u'timestamp': 1506061932546L,\n                          u'user': u'root'},\n              'timestamp': '2017-09-22T06:32:15'},\n     'level': 'INFO',\n     'timestamp': '2017-09-22T06:32:15'}\n\n        Case2:\n    [18/Sep/2017 05:40:36] ERROR [app.apps:78] failed to get the record, collection = Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, serverselectiontimeoutms=3000), u'collection_cache'), u'function_dummy_version')\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python2.7/dist-packages/mongo_cache/mongocache.py\", line 70, in __getitem__\n    result = self.collection.find_one({\"_id\": key})\n    OperationFailure: not authorized on collection_cache to execute command { find: \"function\", filter: { _id: \"zydelig-cosine-20\" }, limit: 1, singleBatch: true }\n    '''\n#TODO we need to handle case2 logs\n    data = {}\n    log = re.findall(r'^(\\[\\d+/\\w+/\\d+ \\d+:\\d+:\\d+\\].*)', line)\n    if len(log) == 1:\n        data['timestamp'] = datetime.datetime.strptime(re.findall(r'(\\d+/\\w+/\\d+ \\d+:\\d+:\\d+)',\\\n                log[0])[0],\"%d/%b/%Y %H:%M:%S\").isoformat()\n        data['loglevel'] = re.findall('[A-Z]+', log[0])[1]\n        data['logname'] = re.findall('\\[\\D+.\\w+:\\d+\\]', log[0])[0]\n        message = re.findall('\\{.+\\}', log[0])\n        try:\n            if len(message) > 0:\n                message = json.loads(message[0])\n            else:\n                message = re.split(']', log[0])\n                message = ''.join(message[2:])\n        except ValueError:\n            message = re.split(']', log[0])\n            message = ''.join(message[2:])\n\n        data['message'] = message\n\n        return dict(\n                timestamp=data['timestamp'],\n                level=data['loglevel'],\n                data=data,\n            )\n    else:\n        return dict(\n            timestamp=datetime.datetime.isoformat(datetime.datetime.utcnow()),\n            data={raw:line}\n        )", "response": "This function is used to run django on a single line of the application."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new base64 - encoded version of a single line.", "response": "def basescript(line):\n    '''\n    >>> import pprint\n    >>> input_line = '{\"level\": \"warning\", \"timestamp\": \"2018-02-07T06:37:00.297610Z\", \"event\": \"exited via keyboard interrupt\", \"type\": \"log\", \"id\": \"20180207T063700_4d03fe800bd111e89ecb96000007bc65\", \"_\": {\"ln\": 58, \"file\": \"/usr/local/lib/python2.7/dist-packages/basescript/basescript.py\", \"name\": \"basescript.basescript\", \"fn\": \"start\"}}'\n    >>> output_line1 = basescript(input_line)\n    >>> pprint.pprint(output_line1)\n    {'data': {u'_': {u'file': u'/usr/local/lib/python2.7/dist-packages/basescript/basescript.py',\n                     u'fn': u'start',\n                     u'ln': 58,\n                     u'name': u'basescript.basescript'},\n              u'event': u'exited via keyboard interrupt',\n              u'id': u'20180207T063700_4d03fe800bd111e89ecb96000007bc65',\n              u'level': u'warning',\n              u'timestamp': u'2018-02-07T06:37:00.297610Z',\n              u'type': u'log'},\n     'event': u'exited via keyboard interrupt',\n     'id': u'20180207T063700_4d03fe800bd111e89ecb96000007bc65',\n     'level': u'warning',\n     'timestamp': u'2018-02-07T06:37:00.297610Z',\n     'type': u'log'}\n    '''\n\n    log = json.loads(line)\n\n    return dict(\n        timestamp=log['timestamp'],\n        data=log,\n        id=log['id'],\n        type=log['type'],\n        level=log['level'],\n        event=log['event']\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef elasticsearch(line):\n    '''\n    >>> import pprint\n    >>> input_line = '[2017-08-30T06:27:19,158] [WARN ][o.e.m.j.JvmGcMonitorService] [Glsuj_2] [gc][296816] overhead, spent [1.2s] collecting in the last [1.3s]'\n    >>> output_line = elasticsearch(input_line)\n    >>> pprint.pprint(output_line)\n    {'data': {'garbage_collector': 'gc',\n              'gc_count': 296816.0,\n              'level': 'WARN',\n              'message': 'o.e.m.j.JvmGcMonitorService',\n              'plugin': 'Glsuj_2',\n              'query_time_ms': 1200.0,\n              'resp_time_ms': 1300.0,\n              'timestamp': '2017-08-30T06:27:19,158'},\n     'event': 'o.e.m.j.JvmGcMonitorService',\n     'level': 'WARN ',\n     'timestamp': '2017-08-30T06:27:19,158',\n     'type': 'metric'}\n\n    Case 2:\n    [2017-09-13T23:15:00,415][WARN ][o.e.i.e.Engine           ] [Glsuj_2] [filebeat-2017.09.09][3] failed engine [index]\n    java.nio.file.FileSystemException: /home/user/elasticsearch/data/nodes/0/indices/jsVSO6f3Rl-wwBpQyNRCbQ/3/index/_0.fdx: Too many open files\n            at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91) ~[?:?]\n    '''\n\n    # TODO we need to handle case2 logs\n    elasticsearch_log = line\n    actuallog = re.findall(r'(\\[\\d+\\-+\\d+\\d+\\-+\\d+\\w+\\d+:\\d+:\\d+,+\\d\\d\\d+\\].*)', elasticsearch_log)\n    if len(actuallog) == 1:\n        keys = ['timestamp','level','message','plugin','garbage_collector','gc_count','query_time_ms', 'resp_time_ms']\n        values = re.findall(r'\\[(.*?)\\]', actuallog[0])\n        for index, i in enumerate(values):\n            if not isinstance(i, str):\n                continue\n            if len(re.findall(r'.*ms$', i)) > 0 and 'ms' in re.findall(r'.*ms$', i)[0]:\n                num = re.split('ms', i)[0]\n                values[index]  = float(num)\n                continue\n            if len(re.findall(r'.*s$', i)) > 0 and 's' in re.findall(r'.*s$', i)[0]:\n                num = re.split('s', i)[0]\n                values[index] = float(num) * 1000\n                continue\n\n        data = dict(zip(keys,values))\n        if 'level' in data and data['level'][-1] == ' ':\n            data['level'] = data['level'][:-1]\n        if 'gc_count' in data:\n            data['gc_count'] = float(data['gc_count'])\n        event = data['message']\n        level=values[1]\n        timestamp=values[0]\n\n        return dict(\n                timestamp=timestamp,\n                level=level,\n                type='metric',\n                data=data,\n                event=event\n        )\n\n    else:\n        return dict(\n                timestamp=datetime.datetime.isoformat(datetime.datetime.now()),\n                data={'raw': line}\n        )", "response": "This function will parse the line and return the elasticsearch log"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the log line is a partial log line.", "response": "def elasticsearch_ispartial_log(line):\n    '''\n    >>> line1 = '  [2018-04-03T00:22:38,048][DEBUG][o.e.c.u.c.QueueResizingEsThreadPoolExecutor] [search17/search]: there were [2000] tasks in [809ms], avg task time [28.4micros], EWMA task execution [790nanos], [35165.36 tasks/s], optimal queue is [35165], current capacity [1000]'\n    >>> line2 = '  org.elasticsearch.ResourceAlreadyExistsException: index [media_corpus_refresh/6_3sRAMsRr2r63J6gbOjQw] already exists'\n    >>> line3 = '   at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:151) ~[elasticsearch-6.2.0.jar:6.2.0]'\n    >>> elasticsearch_ispartial_log(line1)\n    False\n    >>> elasticsearch_ispartial_log(line2)\n    True\n    >>> elasticsearch_ispartial_log(line3)\n    True\n    '''\n    match_result = []\n\n    for p in LOG_BEGIN_PATTERN:\n        if re.match(p, line) != None:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an attribute defined by this session", "response": "def get(self, attr, default=None):\n        \"\"\"Get an attribute defined by this session\"\"\"\n\n        attrs = self.body.get('attributes') or {}\n        return attrs.get(attr, default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all the neccesary data for a given catalog", "response": "def get_all(self, cat):\n        \"\"\"\n        if data can't found in cache then it will be fetched from db,\n         parsed and stored to cache for each lang_code.\n\n        :param cat: cat of catalog data\n        :return:\n        \"\"\"\n        return self._get_from_local_cache(cat) or self._get_from_cache(cat) or self._get_from_db(cat)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fill_get_item_cache(self, catalog, key):\n        lang = self._get_lang()\n        keylist = self.get_all(catalog)\n        self.ITEM_CACHE[lang][catalog] = dict([(i['value'],  i['name']) for i in keylist])\n        return self.ITEM_CACHE[lang][catalog].get(key)", "response": "fill the item cache with the key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, host, port, debug=True, validate_requests=True):\n\n        if debug:\n            # Turn on all alexandra log output\n            logging.basicConfig(level=logging.DEBUG)\n\n        app = self.create_wsgi_app(validate_requests)\n        run_simple(host, port, app, use_reloader=debug, use_debugger=debug)", "response": "Utility method to quickly get a server up and running."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a parsed JSON request object call the correct Intent Launch or SessionEnded function.", "response": "def dispatch_request(self, body):\n        \"\"\"Given a parsed JSON request object, call the correct Intent, Launch,\n        or SessionEnded function.\n\n        This function is called after request parsing and validaion and will\n        raise a `ValueError` if an unknown request type comes in.\n\n        :param body: JSON object loaded from incoming request's POST data.\n        \"\"\"\n\n        req_type = body.get('request', {}).get('type')\n        session_obj = body.get('session')\n\n        session = Session(session_obj) if session_obj else None\n\n        if req_type == 'LaunchRequest':\n            return self.launch_fn(session)\n\n        elif req_type == 'IntentRequest':\n            intent = body['request']['intent']['name']\n            intent_fn = self.intent_map.get(intent, self.unknown_intent_fn)\n\n            slots = {\n                slot['name']: slot.get('value')\n                for _, slot in\n                body['request']['intent'].get('slots', {}).items()\n            }\n\n            arity = intent_fn.__code__.co_argcount\n\n            if arity == 2:\n                return intent_fn(slots, session)\n\n            return intent_fn()\n\n        elif req_type == 'SessionEndedRequest':\n            return self.session_end_fn()\n\n        log.error('invalid request type: %s', req_type)\n        raise ValueError('bad request: %s', body)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef intent(self, intent_name):\n\n        # nested decorator so we can have params.\n        def _decorator(func):\n            arity = func.__code__.co_argcount\n\n            if arity not in [0, 2]:\n                raise ValueError(\"expected 0 or 2 argument function\")\n\n            self.intent_map[intent_name] = func\n            return func\n\n        return _decorator", "response": "Decorator to register a handler for the given intent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the password of the object.", "response": "def set_password(self, raw_password):\n        \"\"\"\n        Kullan\u0131c\u0131 \u015fifresini encrypt ederek set eder.\n\n        Args:\n            raw_password (str)\n        \"\"\"\n        self.password = pbkdf2_sha512.encrypt(raw_password, rounds=10000,\n                                              salt_size=10)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencrypting password if not already encrypted", "response": "def encrypt_password(self):\n        \"\"\" encrypt password if not already encrypted \"\"\"\n        if self.password and not self.password.startswith('$pbkdf2'):\n            self.set_password(self.password)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message to users private mq exchange", "response": "def send_notification(self, title, message, typ=1, url=None, sender=None):\n        \"\"\"\n        sends message to users private mq exchange\n        Args:\n            title:\n            message:\n            sender:\n            url:\n            typ:\n        \"\"\"\n        self.created_channels.channel.add_message(\n            channel_key=self.prv_exchange,\n            body=message,\n            title=title,\n            typ=typ,\n            url=url,\n            sender=sender,\n            receiver=self\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_client_cmd(self, data, cmd=None, via_queue=None):\n        mq_channel = self._connect_mq()\n        if cmd:\n            data['cmd'] = cmd\n        if via_queue:\n            mq_channel.basic_publish(exchange='',\n                                     routing_key=via_queue,\n                                     body=json.dumps(data))\n        else:\n            mq_channel.basic_publish(exchange=self.prv_exchange,\n                                     routing_key='',\n                                     body=json.dumps(data))", "response": "Send arbitrary cmd and data to client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef seek(self, offset):\n        if self._shifts:\n            if 0 <= offset < len(self._shifts):\n                current_pos = self._file.tell()\n                new_pos = self._shifts[offset]\n                if current_pos != new_pos:\n                    if current_pos == self._shifts[-1]:  # reached the end of the file\n                        self._data = self.__reader()\n                        self.__file = iter(self._file.readline, '')\n                        self._file.seek(0)\n                        next(self._data)\n                        if offset:  # move not to the beginning of the file\n                            self._file.seek(new_pos)\n                    else:\n                        if not self.__already_seeked:\n                            if self._shifts[0] < current_pos:  # in the middle of the file\n                                self._data.send(True)\n                            self.__already_seeked = True\n                        self._file.seek(new_pos)\n            else:\n                raise IndexError('invalid offset')\n        else:\n            raise self._implement_error", "response": "seeks to a given number of record in the original file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tell(self):\n        if self._shifts:\n            t = self._file.tell()\n            if t == self._shifts[0]:\n                return 0\n            elif t == self._shifts[-1]:\n                return len(self._shifts) - 1\n            elif t in self._shifts:\n                return bisect_left(self._shifts, t)\n            else:\n                return bisect_left(self._shifts, t) - 1\n        raise self._implement_error", "response": "tells the file where the record was processed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assign_yourself(self):\n        task_invitation = TaskInvitation.objects.get(self.task_invitation_key)\n        wfi = task_invitation.instance\n\n        if not wfi.current_actor.exist:\n            wfi.current_actor = self.current.role\n            wfi.save()\n            [inv.delete() for inv in TaskInvitation.objects.filter(instance=wfi) if\n             not inv == task_invitation]\n            title = _(u\"Successful\")\n            msg = _(u\"You have successfully assigned the job to yourself.\")\n        else:\n            title = _(u\"Unsuccessful\")\n            msg = _(u\"Unfortunately, this job is already taken by someone else.\")\n\n        self.current.msg_box(title=title, msg=msg)", "response": "Assign the workflow to itself."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_role(self):\n\n        roles = [(m.key, m.__unicode__()) for m in RoleModel.objects.filter(\n            abstract_role=self.current.role.abstract_role,\n            unit=self.current.role.unit) if m != self.current.role]\n\n        if roles:\n            _form = forms.JsonForm(title=_(u'Assign to workflow'))\n            _form.select_role = fields.Integer(_(u\"Chose Role\"), choices=roles)\n            _form.explain_text = fields.String(_(u\"Explain Text\"), required=False)\n            _form.send_button = fields.Button(_(u\"Send\"))\n            self.form_out(_form)\n        else:\n            title = _(u\"Unsuccessful\")\n            msg = _(u\"Assign role not found\")\n            self.current.msg_box(title=title, msg=msg)", "response": "The workflow method to be assigned to the person with the same role and unit as the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_workflow(self):\n        task_invitation = TaskInvitation.objects.get(self.task_invitation_key)\n        wfi = task_invitation.instance\n        select_role = self.input['form']['select_role']\n        if wfi.current_actor == self.current.role:\n            task_invitation.role = RoleModel.objects.get(select_role)\n            wfi.current_actor = RoleModel.objects.get(select_role)\n            wfi.save()\n            task_invitation.save()\n            [inv.delete() for inv in TaskInvitation.objects.filter(instance=wfi) if\n             not inv == task_invitation]\n            title = _(u\"Successful\")\n            msg = _(u\"The workflow was assigned to someone else with success.\")\n        else:\n            title = _(u\"Unsuccessful\")\n            msg = _(u\"This workflow does not belong to you, you cannot assign it to someone else.\")\n\n        self.current.msg_box(title=title, msg=msg)", "response": "Send the workflow to the user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_postponed_date(self):\n\n        _form = forms.JsonForm(title=\"Postponed Workflow\")\n        _form.start_date = fields.DateTime(\"Start Date\")\n        _form.finish_date = fields.DateTime(\"Finish Date\")\n        _form.save_button = fields.Button(\"Save\")\n        self.form_out(_form)", "response": "Select the time intervals at which the workflow is to be extended are determined."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_date(self):\n        task_invitation = TaskInvitation.objects.get(self.task_invitation_key)\n        wfi = task_invitation.instance\n        if wfi.current_actor.exist and wfi.current_actor == self.current.role:\n\n            dt_start = datetime.strptime(self.input['form']['start_date'], \"%d.%m.%Y\")\n            dt_finish = datetime.strptime(self.input['form']['finish_date'], \"%d.%m.%Y\")\n\n            task_invitation.start_date = dt_start\n            task_invitation.finish_date = dt_finish\n            task_invitation.save()\n\n            wfi.start_date = dt_start\n            wfi.finish_date = dt_finish\n            wfi.save()\n\n            title = _(u\"Successful\")\n            msg = _(u\"You've extended the workflow time.\")\n        else:\n            title = _(u\"Unsuccessful\")\n            msg = _(u\"This workflow does not belong to you.\")\n\n        self.current.msg_box(title=title, msg=msg)", "response": "Save the date of the current task."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef suspend(self):\n        task_invitation = TaskInvitation.objects.get(self.task_invitation_key)\n        wfi = task_invitation.instance\n\n        if wfi.current_actor.exist and wfi.current_actor == self.current.role:\n            for m in RoleModel.objects.filter(abstract_role=self.current.role.abstract_role,\n                                              unit=self.current.role.unit):\n                if m != self.current.role:\n                    task_invitation.key = ''\n                    task_invitation.role = m\n                    task_invitation.save()\n\n            wfi.current_actor = RoleModel()\n            wfi.save()\n            title = _(u\"Successful\")\n            msg = _(u\"You left the workflow.\")\n        else:\n            title = _(u\"Unsuccessful\")\n            msg = _(u\"Unfortunately, this workflow does not belong to you or is already idle.\")\n\n        self.current.msg_box(title=title, msg=msg)", "response": "Suspends the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_home_row(self, location=None):\n        location = location or self.location\n        return (self.color == color.white and location.rank == 1) or \\\n               (self.color == color.black and location.rank == 6)", "response": "Returns true if the piece is on the home row."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding if move from current get_location would result in promotion", "response": "def would_move_be_promotion(self, location=None):\n        \"\"\"\n        Finds if move from current get_location would result in promotion\n\n        :type: location: Location\n        :rtype: bool\n        \"\"\"\n        location = location or self.location\n        return (location.rank == 1 and self.color == color.black) or \\\n                (location.rank == 6 and self.color == color.white)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef square_in_front(self, location=None):\n        location = location or self.location\n        return location.shift_up() if self.color == color.white else location.shift_down()", "response": "Finds square directly in front of Pawn\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward_moves(self, position):\n        if position.is_square_empty(self.square_in_front(self.location)):\n            \"\"\"\n            If square in front is empty add the move\n            \"\"\"\n            if self.would_move_be_promotion():\n                for move in self.create_promotion_moves(notation_const.PROMOTE):\n                    yield move\n            else:\n                yield self.create_move(end_loc=self.square_in_front(self.location),\n                                       status=notation_const.MOVEMENT)\n\n            if self.on_home_row() and \\\n                    position.is_square_empty(self.two_squares_in_front(self.location)):\n                \"\"\"\n                If pawn is on home row and two squares in front of the pawn is empty\n                add the move\n                \"\"\"\n                yield self.create_move(\n                    end_loc=self.square_in_front(self.square_in_front(self.location)),\n                    status=notation_const.MOVEMENT\n                )", "response": "Returns a generator that yields the possible moves for the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a single diagonal as a capture move if it is one.", "response": "def _one_diagonal_capture_square(self, capture_square, position):\n        \"\"\"\n        Adds specified diagonal as a capture move if it is one\n        \"\"\"\n        if self.contains_opposite_color_piece(capture_square, position):\n\n            if self.would_move_be_promotion():\n                for move in self.create_promotion_moves(status=notation_const.CAPTURE_AND_PROMOTE,\n                                                        location=capture_square):\n                    yield move\n\n            else:\n                yield self.create_move(end_loc=capture_square,\n                                       status=notation_const.CAPTURE)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef capture_moves(self, position):\n        try:\n            right_diagonal = self.square_in_front(self.location.shift_right())\n            for move in self._one_diagonal_capture_square(right_diagonal, position):\n                yield move\n        except IndexError:\n            pass\n\n        try:\n            left_diagonal = self.square_in_front(self.location.shift_left())\n            for move in self._one_diagonal_capture_square(left_diagonal, position):\n                yield move\n        except IndexError:\n            pass", "response": "Finds out all possible capture moves in the given position."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind out if pawn is on enemy center rank.", "response": "def on_en_passant_valid_location(self):\n        \"\"\"\n        Finds out if pawn is on enemy center rank.\n\n        :rtype: bool\n        \"\"\"\n        return (self.color == color.white and self.location.rank == 4) or \\\n               (self.color == color.black and self.location.rank == 3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_en_passant_valid(self, opponent_pawn_location, position):\n        try:\n            pawn = position.piece_at_square(opponent_pawn_location)\n            return pawn is not None and \\\n                isinstance(pawn, Pawn) and \\\n                pawn.color != self.color and \\\n                position.piece_at_square(opponent_pawn_location).just_moved_two_steps\n        except IndexError:\n            return False", "response": "Checks if the en passant is valid for the given position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_one_en_passant_move(self, direction, position):\n        try:\n            if self._is_en_passant_valid(direction(self.location), position):\n                yield self.create_move(\n                    end_loc=self.square_in_front(direction(self.location)),\n                    status=notation_const.EN_PASSANT\n                )\n        except IndexError:\n            pass", "response": "Yields en_passant moves in given direction if it is legal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef en_passant_moves(self, position):\n\n        # if pawn is not on a valid en passant get_location then return None\n        if self.on_en_passant_valid_location():\n            for move in itertools.chain(self.add_one_en_passant_move(lambda x: x.shift_right(), position),\n                                        self.add_one_en_passant_move(lambda x: x.shift_left(), position)):\n                yield move", "response": "Finds possible en passant moves."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef possible_moves(self, position):\n        for move in itertools.chain(self.forward_moves(position),\n                                    self.capture_moves(position),\n                                    self.en_passant_moves(position)):\n            yield move", "response": "Returns a list of possible moves."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    print(\"Creating a new game...\")\n\n    new_game = Game(Human(color.white), Human(color.black))\n    result = new_game.play()\n\n    print(\"Result is \", result)", "response": "Main method for the\n    game."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a dict containing a valid response to an Alexa request.", "response": "def respond(text=None, ssml=None, attributes=None, reprompt_text=None,\n            reprompt_ssml=None, end_session=True):\n    \"\"\" Build a dict containing a valid response to an Alexa request.\n\n    If speech output is desired, either of `text` or `ssml` should\n    be specified.\n\n    :param text: Plain text speech output to be said by Alexa device.\n    :param ssml: Speech output in SSML form.\n    :param attributes: Dictionary of attributes to store in the session.\n    :param end_session: Should the session be terminated after this response?\n    :param reprompt_text, reprompt_ssml: Works the same as\n        `text`/`ssml`, but instead sets the reprompting speech output.\n    \"\"\"\n\n    obj = {\n        'version': '1.0',\n        'response': {\n            'outputSpeech': {'type': 'PlainText', 'text': ''},\n            'shouldEndSession': end_session\n        },\n        'sessionAttributes': attributes or {}\n    }\n\n    if text:\n        obj['response']['outputSpeech'] = {'type': 'PlainText', 'text': text}\n    elif ssml:\n        obj['response']['outputSpeech'] = {'type': 'SSML', 'ssml': ssml}\n\n    reprompt_output = None\n    if reprompt_text:\n        reprompt_output = {'type': 'PlainText', 'text': reprompt_text}\n    elif reprompt_ssml:\n        reprompt_output = {'type': 'SSML', 'ssml': reprompt_ssml}\n\n    if reprompt_output:\n        obj['response']['reprompt'] = {'outputSpeech': reprompt_output}\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure the request s timestamp doesn t fall outside of the app s specified tolerance.", "response": "def validate_request_timestamp(req_body, max_diff=150):\n    \"\"\"Ensure the request's timestamp doesn't fall outside of the\n    app's specified tolerance.\n\n    Returns True if this request is valid, False otherwise.\n\n    :param req_body: JSON object parsed out of the raw POST data of a request.\n    :param max_diff: Maximum allowable difference in seconds between request\n        timestamp and system clock. Amazon requires <= 150 seconds for\n        published skills.\n    \"\"\"\n\n    time_str = req_body.get('request', {}).get('timestamp')\n\n    if not time_str:\n        log.error('timestamp not present %s', req_body)\n        return False\n\n    req_ts = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%SZ\")\n    diff = (datetime.utcnow() - req_ts).total_seconds()\n\n    if abs(diff) > max_diff:\n        log.error('timestamp difference too high: %d sec', diff)\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the certificate and signature specified in the request headers are truely from Amazon and correctly verify the request data.", "response": "def validate_request_certificate(headers, data):\n    \"\"\"Ensure that the certificate and signature specified in the\n    request headers are truely from Amazon and correctly verify.\n\n    Returns True if certificate verification succeeds, False otherwise.\n\n    :param headers: Dictionary (or sufficiently dictionary-like) map of request\n        headers.\n    :param data: Raw POST data attached to this request.\n    \"\"\"\n\n    # Make sure we have the appropriate headers.\n    if 'SignatureCertChainUrl' not in headers or \\\n       'Signature' not in headers:\n        log.error('invalid request headers')\n        return False\n\n    cert_url = headers['SignatureCertChainUrl']\n    sig = base64.b64decode(headers['Signature'])\n\n    cert = _get_certificate(cert_url)\n\n    if not cert:\n        return False\n\n    try:\n        # ... wtf kind of API decision is this\n        crypto.verify(cert, sig, data, 'sha1')\n        return True\n    except:\n        log.error('invalid request signature')\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_certificate(cert_url):\n    global _cache\n\n    if cert_url in _cache:\n        cert = _cache[cert_url]\n        if cert.has_expired():\n            _cache = {}\n        else:\n            return cert\n\n    url = urlparse(cert_url)\n    host = url.netloc.lower()\n    path = posixpath.normpath(url.path)\n\n    # Sanity check location so we don't get some random person's cert.\n    if url.scheme != 'https' or \\\n       host not in ['s3.amazonaws.com', 's3.amazonaws.com:443'] or \\\n       not path.startswith('/echo.api/'):\n        log.error('invalid cert location %s', cert_url)\n        return\n\n    resp = urlopen(cert_url)\n    if resp.getcode() != 200:\n        log.error('failed to download certificate')\n        return\n\n    cert = crypto.load_certificate(crypto.FILETYPE_PEM, resp.read())\n\n    if cert.has_expired() or cert.get_subject().CN != 'echo-api.amazon.com':\n        log.error('certificate expired or invalid')\n        return\n\n    _cache[cert_url] = cert\n    return cert", "response": "Download and validate a specified Amazon PEM file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if version is already applied in the database.", "response": "def is_processed(self, db_versions):\n        \"\"\"Check if version is already applied in the database.\n\n        :param db_versions:\n        \"\"\"\n        return self.number in (v.number for v in db_versions if v.date_done)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if version is a no operation version.", "response": "def is_noop(self):\n        \"\"\"Check if version is a no operation version.\n        \"\"\"\n        has_operations = [mode.pre_operations or mode.post_operations\n                          for mode in self._version_modes.values()]\n        has_upgrade_addons = [mode.upgrade_addons or mode.remove_addons\n                              for mode in self._version_modes.values()]\n        noop = not any((has_upgrade_addons, has_operations))\n        return noop"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a VersionMode for a mode name.", "response": "def _get_version_mode(self, mode=None):\n        \"\"\"Return a VersionMode for a mode name.\n\n        When the mode is None, we are working with the 'base' mode.\n        \"\"\"\n        version_mode = self._version_modes.get(mode)\n        if not version_mode:\n            version_mode = self._version_modes[mode] = VersionMode(name=mode)\n        return version_mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_operation(self, operation_type, operation, mode=None):\n        version_mode = self._get_version_mode(mode=mode)\n        if operation_type == 'pre':\n            version_mode.add_pre(operation)\n        elif operation_type == 'post':\n            version_mode.add_post(operation)\n        else:\n            raise ConfigurationError(\n                u\"Type of operation must be 'pre' or 'post', got %s\" %\n                (operation_type,)\n            )", "response": "Add an operation to the version of the current version of the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a backup operation to the version.", "response": "def add_backup_operation(self, backup, mode=None):\n        \"\"\"Add a backup operation to the version.\n\n        :param backup: To either add or skip the backup\n        :type backup: Boolean\n        :param mode: Name of the mode in which the operation is executed\n                     For now, backups are mode-independent\n        :type mode: String\n        \"\"\"\n        try:\n            if self.options.backup:\n                self.options.backup.ignore_if_operation().execute()\n        except OperationError:\n            self.backup = backup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pre_operations(self, mode=None):\n        version_mode = self._get_version_mode(mode=mode)\n        return version_mode.pre_operations", "response": "Return the pre - operations for the mode asked"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_operations(self, mode=None):\n        version_mode = self._get_version_mode(mode=mode)\n        return version_mode.post_operations", "response": "Return the post - operations for the mode asked"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upgrade_addons_operation(self, addons_state, mode=None):\n        installed = set(a.name for a in addons_state\n                        if a.state in ('installed', 'to upgrade'))\n\n        base_mode = self._get_version_mode()\n        addons_list = base_mode.upgrade_addons.copy()\n        if mode:\n            add_mode = self._get_version_mode(mode=mode)\n            addons_list |= add_mode.upgrade_addons\n\n        to_install = addons_list - installed\n        to_upgrade = installed & addons_list\n\n        return UpgradeAddonsOperation(self.options, to_install, to_upgrade)", "response": "Return a new upgrade addons operation that will be applied to the current version of the current mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget copy of object", "response": "def copy(self):\n        \"\"\"\n        get copy of object\n\n        :return: ReactionContainer\n        \"\"\"\n        return type(self)(reagents=[x.copy() for x in self.__reagents], meta=self.__meta.copy(),\n                          products=[x.copy() for x in self.__products],\n                          reactants=[x.copy() for x in self.__reactants])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove explicit hydrogens from the reagents reactants and products", "response": "def implicify_hydrogens(self):\n        \"\"\"\n        remove explicit hydrogens if possible\n\n        :return: number of removed hydrogens\n        \"\"\"\n        total = 0\n        for ml in (self.__reagents, self.__reactants, self.__products):\n            for m in ml:\n                if hasattr(m, 'implicify_hydrogens'):\n                    total += m.implicify_hydrogens()\n        if total:\n            self.flush_cache()\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset hyb and neighbors marks to atoms.", "response": "def reset_query_marks(self):\n        \"\"\"\n        set or reset hyb and neighbors marks to atoms.\n        \"\"\"\n        for ml in (self.__reagents, self.__reactants, self.__products):\n            for m in ml:\n                if hasattr(m, 'reset_query_marks'):\n                    m.reset_query_marks()\n        self.flush_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfix coordinates of molecules in reaction", "response": "def fix_positions(self):\n        \"\"\"\n        fix coordinates of molecules in reaction\n        \"\"\"\n        shift_x = 0\n        for m in self.__reactants:\n            max_x = self.__fix_positions(m, shift_x, 0)\n            shift_x = max_x + 1\n        arrow_min = shift_x\n\n        if self.__reagents:\n            for m in self.__reagents:\n                max_x = self.__fix_positions(m, shift_x, 1.5)\n                shift_x = max_x + 1\n        else:\n            shift_x += 3\n        arrow_max = shift_x - 1\n\n        for m in self.__products:\n            max_x = self.__fix_positions(m, shift_x, 0)\n            shift_x = max_x + 1\n        self._arrow = (arrow_min, arrow_max)\n        self.flush_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_role_keys(cls, unit_key):\n        stack = Role.objects.filter(unit_id=unit_key).values_list('key', flatten=True)\n        for unit_key in cls.objects.filter(parent_id=unit_key).values_list('key', flatten=True):\n            stack.extend(cls.get_role_keys(unit_key))\n        return stack", "response": "Get the role keys of subunits in a given unit."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of Permission objects that are associated with the user.", "response": "def get_permissions(self):\n        \"\"\"\n        Permissions of the user.\n\n        Returns:\n            List of Permission objects.\n        \"\"\"\n        user_role = self.last_login_role() if self.last_login_role_key else self.role_set[0].role\n        return user_role.get_permissions()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_permissions(self):\n        return [p.permission.code for p in self.Permissions if p.permission.code]", "response": "Return a list of all permissions in a set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_permission(self, perm):\n        self.Permissions(permission=perm)\n        PermissionCache.flush()\n        self.save()", "response": "Add a permission to the set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a permission with given name.", "response": "def add_permission_by_name(self, code, save=False):\n        \"\"\"\n        Adds a permission with given name.\n\n        Args:\n            code (str): Code name of the permission.\n            save (bool): If False, does nothing.\n        \"\"\"\n        if not save:\n            return [\"%s | %s\" % (p.name, p.code) for p in\n                    Permission.objects.filter(code__contains=code)]\n        for p in Permission.objects.filter(code__contains=code):\n            if p not in self.Permissions:\n                self.Permissions(permission=p)\n        if p:\n            self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_notification(self, title, message, typ=1, url=None, sender=None):\n        self.user.send_notification(title=title, message=message, typ=typ, url=url,\n                                    sender=sender)", "response": "Sends a message to the user of this role s private mq exchange"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding if move from current location would be promotion", "response": "def would_move_be_promotion(self):\n        \"\"\"\n        Finds if move from current location would be a promotion\n        \"\"\"\n        return (self._end_loc.rank == 0 and not self.color) or \\\n            (self._end_loc.rank == 7 and self.color)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, receiver, sender=None, weak=True, dispatch_uid=None):\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        if weak:\n            ref = weakref.ref\n            receiver_object = receiver\n            # Check for bound methods\n            if hasattr(receiver, '__self__') and hasattr(receiver, '__func__'):\n                ref = WeakMethod\n                receiver_object = receiver.__self__\n            if six.PY3:\n                receiver = ref(receiver)\n                weakref.finalize(receiver_object, self._remove_receiver)\n            else:\n                receiver = ref(receiver, self._remove_receiver)\n\n        with self.lock:\n            self._clear_dead_receivers()\n            for r_key, _ in self.receivers:\n                if r_key == lookup_key:\n                    break\n            else:\n                self.receivers.append((lookup_key, receiver))\n            self.sender_receivers_cache.clear()", "response": "Connects receiver to sender for signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisconnects receiver from sender for signal.", "response": "def disconnect(self, receiver=None, sender=None, dispatch_uid=None):\n        \"\"\"\n        Disconnect receiver from sender for signal.\n\n        If weak references are used, disconnect need not be called. The receiver\n        will be remove from dispatch automatically.\n\n        Arguments:\n\n            receiver\n                The registered receiver to disconnect. May be none if\n                dispatch_uid is specified.\n\n            sender\n                The registered sender to disconnect\n\n            dispatch_uid\n                the unique identifier of the receiver to disconnect\n        \"\"\"\n\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        disconnected = False\n        with self.lock:\n            self._clear_dead_receivers()\n            for index in range(len(self.receivers)):\n                (r_key, _) = self.receivers[index]\n                if r_key == lookup_key:\n                    disconnected = True\n                    del self.receivers[index]\n                    break\n            self.sender_receivers_cache.clear()\n        return disconnected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef migrate(config):\n    webapp = WebApp(config.web_host, config.web_port,\n                    custom_maintenance_file=config.web_custom_html)\n\n    webserver = WebServer(webapp)\n    webserver.daemon = True\n    webserver.start()\n\n    migration_parser = YamlParser.parse_from_file(config.migration_file)\n    migration = migration_parser.parse()\n\n    database = Database(config)\n\n    with database.connect() as lock_connection:\n        application_lock = ApplicationLock(lock_connection)\n        application_lock.start()\n\n        while not application_lock.acquired:\n            time.sleep(0.5)\n        else:\n            if application_lock.replica:\n                # when a replica could finally acquire a lock, it\n                # means that the concurrent process has finished the\n                # migration or that it failed to run it.\n                # In both cases after the lock is released, this process will\n                # verify if it has still to do something (if the other process\n                # failed mainly).\n                application_lock.stop = True\n                application_lock.join()\n            # we are not in the replica or the lock is released: go on for the\n            # migration\n\n        try:\n            table = MigrationTable(database)\n            runner = Runner(config, migration, database, table)\n            runner.perform()\n        finally:\n            application_lock.stop = True\n            application_lock.join()", "response": "Perform a migration according to config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the command line and run the migrate function.", "response": "def main():\n    \"\"\"Parse the command line and run :func:`migrate`.\"\"\"\n    parser = get_args_parser()\n    args = parser.parse_args()\n    config = Config.from_parse_args(args)\n    migrate(config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate permissions for all CrudView based class methods.", "response": "def get_permissions(cls):\n        \"\"\"\n        Generates permissions for all CrudView based class methods.\n\n        Returns:\n            List of Permission objects.\n        \"\"\"\n        perms = []\n        for kls_name, kls in cls.registry.items():\n            for method_name in cls.__dict__.keys():\n                if method_name.endswith('_view'):\n                    perms.append(\"%s.%s\" % (kls_name, method_name))\n        return perms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_object_menu_models():\n    from pyoko.conf import settings\n    enabled_models = []\n    for entry in settings.OBJECT_MENU.values():\n        for mdl in entry:\n            if 'wf' not in mdl:\n                enabled_models.append(mdl['name'])\n    return enabled_models", "response": "Get list of models that can be used in CRUD"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(cls, code_name, name='', description=''):\n        if code_name not in cls.registry:\n            cls.registry[code_name] = (code_name, name or code_name, description)\n        return code_name", "response": "add a new permission to the registry"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets mapping from self to other", "response": "def get_mapping(self, other):\n        \"\"\"\n        get self to other mapping\n        \"\"\"\n        m = next(self._matcher(other).isomorphisms_iter(), None)\n        if m:\n            return {v: k for k, v in m.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_substructure_mapping(self, other, limit=1):\n        i = self._matcher(other).subgraph_isomorphisms_iter()\n        if limit == 1:\n            m = next(i, None)\n            if m:\n                return {v: k for k, v in m.items()}\n            return\n        elif limit == 0:\n            return ({v: k for k, v in m.items()} for m in i)\n        return [{v: k for k, v in m.items()} for m in islice(i, limit)]", "response": "get the mapping from self to other substructure"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_string(cls, alg_str):\n        try:\n            return cls(int(alg_str[1]) - 1, ord(alg_str[0]) - 97)\n        except ValueError as e:\n            raise ValueError(\"Location.from_string {} invalid: {}\".format(alg_str, e))", "response": "Creates a Location object from a two character string consisting of \n        the file then rank written in algebraic notation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshifts the calendar in the specified direction.", "response": "def shift(self, direction):\n        \"\"\"\n        Shifts in direction provided by ``Direction`` enum.\n\n        :type: direction: Direction\n        :rtype: Location\n        \"\"\"\n        try:\n            if direction == Direction.UP:\n                return self.shift_up()\n            elif direction == Direction.DOWN:\n                return self.shift_down()\n            elif direction == Direction.RIGHT:\n                return self.shift_right()\n            elif direction == Direction.LEFT:\n                return self.shift_left()\n            else:\n                raise IndexError(\"Invalid direction {}\".format(direction))\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind Location shifted up by 1", "response": "def shift_up(self, times=1):\n        \"\"\"\n        Finds Location shifted up by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank + times, self._file)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds Location shifted down by 1", "response": "def shift_down(self, times=1):\n        \"\"\"\n        Finds Location shifted down by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank - times, self._file)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shift_right(self, times=1):\n        try:\n            return Location(self._rank, self._file + times)\n        except IndexError as e:\n            raise IndexError(e)", "response": "Finds Location shifted right by 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind Location shifted left by 1", "response": "def shift_left(self, times=1):\n        \"\"\"\n        Finds Location shifted left by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank, self._file - times)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shift_up_right(self, times=1):\n        try:\n            return Location(self._rank + times, self._file + times)\n        except IndexError as e:\n            raise IndexError(e)", "response": "Finds Location shifted up right by 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind Location shifted up left by 1", "response": "def shift_up_left(self, times=1):\n        \"\"\"\n        Finds Location shifted up left by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank + times, self._file - times)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds Location shifted down right by 1", "response": "def shift_down_right(self, times=1):\n        \"\"\"\n        Finds Location shifted down right by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank - times, self._file + times)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds Location shifted down left by 1", "response": "def shift_down_left(self, times=1):\n        \"\"\"\n        Finds Location shifted down left by 1\n\n        :rtype: Location\n        \"\"\"\n        try:\n            return Location(self._rank - times, self._file - times)\n        except IndexError as e:\n            raise IndexError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstandardize functional groups :return: number of found groups", "response": "def standardize(self):\n        \"\"\"\n        standardize functional groups\n\n        :return: number of found groups\n        \"\"\"\n        self.reset_query_marks()\n        seen = set()\n        total = 0\n        for n, atom in self.atoms():\n            if n in seen:\n                continue\n            for k, center in central.items():\n                if center != atom:\n                    continue\n                shell = tuple((bond, self._node[m]) for m, bond in self._adj[n].items())\n                for shell_query, shell_patch, atom_patch in query_patch[k]:\n                    if shell_query != shell:\n                        continue\n                    total += 1\n                    for attr_name, attr_value in atom_patch.items():\n                        setattr(atom, attr_name, attr_value)\n                    for (bond_patch, atom_patch), (bond, atom) in zip(shell_patch, shell):\n                        bond.update(bond_patch)\n                        for attr_name, attr_value in atom_patch.items():\n                            setattr(atom, attr_name, attr_value)\n                    seen.add(n)\n                    seen.update(self._adj[n])\n                    break\n                else:\n                    continue\n                break\n        if total:\n            self.flush_cache()\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all files staged for the current commit.", "response": "def get_staged_files():\n    \"\"\"Get all files staged for the current commit.\n    \"\"\"\n    proc = subprocess.Popen(('git', 'status', '--porcelain'),\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE)\n    out, _ = proc.communicate()\n    staged_files = modified_re.findall(out)\n    return staged_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self):\n        sess_id = self._get_sess_id()\n        if sess_id:\n            self.application.pc.websockets[self._get_sess_id()] = self\n            self.write_message(json.dumps({\"cmd\": \"status\", \"status\": \"open\"}))\n        else:\n            self.write_message(json.dumps({\"cmd\": \"error\", \"error\": \"Please login\", \"code\": 401}))", "response": "Called on new websocket connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when a new message is received from the websocket.", "response": "def on_message(self, message):\n        \"\"\"\n        called on new websocket message,\n        \"\"\"\n        log.debug(\"WS MSG for %s: %s\" % (self._get_sess_id(), message))\n        self.application.pc.redirect_incoming_message(self._get_sess_id(), message, self.request)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving duplicate data from log dict and brings it out.", "response": "def _remove_redundancy(self, log):\n        \"\"\"Removes duplicate data from 'data' inside log dict and brings it\n        out.\n\n        >>> lc = LogCollector('file=/path/to/log_file.log:formatter=logagg.formatters.basescript', 30)\n\n        >>> log = {'id' : 46846876, 'type' : 'log',\n        ...         'data' : {'a' : 1, 'b' : 2, 'type' : 'metric'}}\n        >>> lc._remove_redundancy(log)\n        {'data': {'a': 1, 'b': 2}, 'type': 'metric', 'id': 46846876}\n        \"\"\"\n        for key in log:\n            if key in log and key in log['data']:\n                log[key] = log['data'].pop(key)\n        return log"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the log format of the log", "response": "def validate_log_format(self, log):\n        '''\n        >>> lc = LogCollector('file=/path/to/file.log:formatter=logagg.formatters.basescript', 30)\n\n        >>> incomplete_log = {'data' : {'x' : 1, 'y' : 2},\n        ...                     'raw' : 'Not all keys present'}\n        >>> lc.validate_log_format(incomplete_log)\n        'failed'\n\n        >>> redundant_log = {'one_invalid_key' : 'Extra information',\n        ...  'data': {'x' : 1, 'y' : 2},\n        ...  'error': False,\n        ...  'error_tb': '',\n        ...  'event': 'event',\n        ...  'file': '/path/to/file.log',\n        ...  'formatter': 'logagg.formatters.mongodb',\n        ...  'host': 'deepcompute-ThinkPad-E470',\n        ...  'id': '0112358',\n        ...  'level': 'debug',\n        ...  'raw': 'some log line here',\n        ...  'timestamp': '2018-04-07T14:06:17.404818',\n        ...  'type': 'log'}\n        >>> lc.validate_log_format(redundant_log)\n        'failed'\n\n        >>> correct_log = {'data': {'x' : 1, 'y' : 2},\n        ...  'error': False,\n        ...  'error_tb': '',\n        ...  'event': 'event',\n        ...  'file': '/path/to/file.log',\n        ...  'formatter': 'logagg.formatters.mongodb',\n        ...  'host': 'deepcompute-ThinkPad-E470',\n        ...  'id': '0112358',\n        ...  'level': 'debug',\n        ...  'raw': 'some log line here',\n        ...  'timestamp': '2018-04-07T14:06:17.404818',\n        ...  'type': 'log'}\n        >>> lc.validate_log_format(correct_log)\n        'passed'\n        '''\n\n        keys_in_log = set(log)\n        keys_in_log_structure = set(self.LOG_STRUCTURE)\n        try:\n            assert (keys_in_log == keys_in_log_structure)\n        except AssertionError as e:\n            self.log.warning('formatted_log_structure_rejected' ,\n                                key_not_found = list(keys_in_log_structure-keys_in_log),\n                                extra_keys_found = list(keys_in_log-keys_in_log_structure),\n                                num_logs=1,\n                                type='metric')\n            return 'failed'\n\n        for key in log:\n            try:\n                assert isinstance(log[key], self.LOG_STRUCTURE[key])\n            except AssertionError as e:\n                self.log.warning('formatted_log_structure_rejected' ,\n                                    key_datatype_not_matched = key,\n                                    datatype_expected = type(self.LOG_STRUCTURE[key]),\n                                    datatype_got = type(log[key]),\n                                    num_logs=1,\n                                    type='metric')\n                return 'failed'\n\n        return 'passed'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign default log values to a new log file.", "response": "def assign_default_log_values(self, fpath, line, formatter):\n        '''\n        >>> lc = LogCollector('file=/path/to/log_file.log:formatter=logagg.formatters.basescript', 30)\n        >>> from pprint import pprint\n\n        >>> formatter = 'logagg.formatters.mongodb'\n        >>> fpath = '/var/log/mongodb/mongodb.log'\n        >>> line = 'some log line here'\n\n        >>> default_log = lc.assign_default_log_values(fpath, line, formatter)\n        >>> pprint(default_log) #doctest: +ELLIPSIS\n        {'data': {},\n         'error': False,\n         'error_tb': '',\n         'event': 'event',\n         'file': '/var/log/mongodb/mongodb.log',\n         'formatter': 'logagg.formatters.mongodb',\n         'host': '...',\n         'id': None,\n         'level': 'debug',\n         'raw': 'some log line here',\n         'timestamp': '...',\n         'type': 'log'}\n        '''\n        return dict(\n            id=None,\n            file=fpath,\n            host=self.HOST,\n            formatter=formatter,\n            event='event',\n            data={},\n            raw=line,\n            timestamp=datetime.datetime.utcnow().isoformat(),\n            type='log',\n            level='debug',\n            error= False,\n            error_tb='',\n          )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _scan_fpatterns(self, state):\n        '''\n        For a list of given fpatterns, this starts a thread\n        collecting log lines from file\n\n        >>> os.path.isfile = lambda path: path == '/path/to/log_file.log'\n        >>> lc = LogCollector('file=/path/to/log_file.log:formatter=logagg.formatters.basescript', 30)\n\n        >>> print(lc.fpaths)\n        file=/path/to/log_file.log:formatter=logagg.formatters.basescript\n\n        >>> print('formatters loaded:', lc.formatters)\n        {}\n        >>> print('log file reader threads started:', lc.log_reader_threads)\n        {}\n        >>> state = AttrDict(files_tracked=list())\n        >>> print('files bieng tracked:', state.files_tracked)\n        []\n\n\n        >>> if not state.files_tracked:\n        >>>     lc._scan_fpatterns(state)\n        >>>     print('formatters loaded:', lc.formatters)\n        >>>     print('log file reader threads started:', lc.log_reader_threads)\n        >>>     print('files bieng tracked:', state.files_tracked)\n\n\n        '''\n        for f in self.fpaths:\n            fpattern, formatter =(a.split('=')[1] for a in f.split(':', 1))\n            self.log.debug('scan_fpatterns', fpattern=fpattern, formatter=formatter)\n            # TODO code for scanning fpatterns for the files not yet present goes here\n            fpaths = glob.glob(fpattern)\n            # Load formatter_fn if not in list\n            fpaths = list(set(fpaths) - set(state.files_tracked))\n            for fpath in fpaths:\n                try:\n                    formatter_fn = self.formatters.get(formatter,\n                                  load_formatter_fn(formatter))\n                    self.log.info('found_formatter_fn', fn=formatter)\n                    self.formatters[formatter] = formatter_fn\n                except (SystemExit, KeyboardInterrupt): raise\n                except (ImportError, AttributeError):\n                    self.log.exception('formatter_fn_not_found', fn=formatter)\n                    sys.exit(-1)\n                # Start a thread for every file\n                self.log.info('found_log_file', log_file=fpath)\n                log_f = dict(fpath=fpath, fpattern=fpattern,\n                                formatter=formatter, formatter_fn=formatter_fn)\n                log_key = (fpath, fpattern, formatter)\n                if log_key not in self.log_reader_threads:\n                    self.log.info('starting_collect_log_lines_thread', log_key=log_key)\n                    # There is no existing thread tracking this log file. Start one\n                    log_reader_thread = util.start_daemon_thread(self.collect_log_lines, (log_f,))\n                    self.log_reader_threads[log_key] = log_reader_thread\n                state.files_tracked.append(fpath)\n        time.sleep(self.SCAN_FPATTERNS_INTERVAL)", "response": "This function scans the log files for a list of given fpatterns and creates a thread holding log lines for each of them."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_links(self, **kw):\n\n        links = [a for a in dir(self) if isinstance(getattr(self, a), Model)\n                 and not a.startswith('_model')]\n\n        return [\n            {\n                'field': l,\n                'mdl': getattr(self, l).__class__,\n            } for l in links\n        ]", "response": "Prepare links of form by mimicing pyoko s get_links method s result\n\n clf."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling form with data.", "response": "def set_data(self, data):\n        \"\"\"\n        Fills form with data\n\n        Args:\n            data (dict): Data to assign form fields.\n\n        Returns:\n            Self. Form object.\n\n        \"\"\"\n        for name in self._fields:\n            setattr(self, name, data.get(name))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serialize(self):\n        result = {\n            \"schema\": {\n                \"title\": self.title,\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": []\n            },\n            \"form\": [\n                {\n                    \"type\": \"help\",\n                    \"helpvalue\": self.help_text\n                }\n            ],\n            \"model\": {}\n        }\n        for itm in self.META_TO_FORM_ROOT:\n            if itm in self.Meta.__dict__:\n                result[itm] = self.Meta.__dict__[itm]\n\n        if self._model.is_in_db():\n            result[\"model\"]['object_key'] = self._model.key\n            result[\"model\"]['model_type'] = self._model.__class__.__name__\n            result[\"model\"]['unicode'] = six.text_type(self._model)\n\n        # if form intentionally marked as fillable from task data by assigning False to always_blank\n        # field in Meta class, form_data is retrieved from task_data if exist in else None\n        form_data = None\n        if not self.Meta.always_blank:\n            form_data = self.context.task_data.get(self.__class__.__name__, None)\n\n        for itm in self._serialize():\n            item_props = {'type': itm['type'], 'title': itm['title']}\n\n            if not itm.get('value') and 'kwargs' in itm and 'value' in itm['kwargs']:\n                itm['value'] = itm['kwargs'].pop('value')\n\n            if 'kwargs' in itm and 'widget' in itm['kwargs']:\n                item_props['widget'] = itm['kwargs'].pop('widget')\n\n            if form_data:\n                if form_data[itm['name']] and (itm['type'] == 'date' or itm['type'] == 'datetime'):\n                    value_to_serialize = datetime.strptime(\n                        form_data[itm['name']], itm['format'])\n                else:\n                    value_to_serialize = form_data[itm['name']]\n                value = self._serialize_value(value_to_serialize)\n                if itm['type'] == 'button':\n                    value = None\n            # if form_data is empty, value will be None, so it is needed to fill the form from model\n            # or leave empty\n            else:\n                # if itm['value'] is not None returns itm['value']\n                # else itm['default']\n                if itm['value'] is not None:\n                    value = itm['value']\n                else:\n                    value = itm['default']\n\n            result[\"model\"][itm['name']] = value\n\n            if itm['type'] == 'model':\n                item_props['model_name'] = itm['model_name']\n\n            if itm['type'] not in ['ListNode', 'model', 'Node']:\n                if 'hidden' in itm['kwargs']:\n                    # we're simulating HTML's hidden form fields\n                    # by just setting it in \"model\" dict and bypassing other parts\n                    continue\n                else:\n                    item_props.update(itm['kwargs'])\n            if itm.get('choices'):\n                self._handle_choices(itm, item_props, result)\n            else:\n                result[\"form\"].append(itm['name'])\n\n            if 'help_text' in itm:\n                item_props['help_text'] = itm['help_text']\n\n            if 'schema' in itm:\n                item_props['schema'] = itm['schema']\n\n            # this adds default directives for building\n            # add and list views of linked models\n            if item_props['type'] == 'model':\n                # this control for passing test.\n                # object gets context but do not use it. why is it for?\n                if self.context:\n                    if self.context.has_permission(\"%s.select_list\" % item_props['model_name']):\n                        item_props.update({\n                            'list_cmd': 'select_list',\n                            'wf': 'crud',\n                        })\n                    if self.context.has_permission(\"%s.add_edit_form\" % item_props['model_name']):\n                        item_props.update({\n                            'add_cmd': 'add_edit_form',\n                            'wf': 'crud',\n                        })\n                else:\n                    item_props.update({\n                        'list_cmd': 'select_list',\n                        'add_cmd': 'add_edit_form',\n                        'wf': 'crud'\n                    })\n            result[\"schema\"][\"properties\"][itm['name']] = item_props\n\n\n            if itm['required']:\n                result[\"schema\"][\"required\"].append(itm['name'])\n        self._cache_form_details(result)\n        return result", "response": "Returns a JSON ready dict of the form and model that can be used to create a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncaches some form details to lates process and validate incoming form data", "response": "def _cache_form_details(self, form):\n        \"\"\"\n        Caches some form details to lates process and validate incoming (response) form data\n\n        Args:\n            form: form dict\n        \"\"\"\n        cache = FormCache()\n        form['model']['form_key'] = cache.form_id\n        form['model']['form_name'] = self.__class__.__name__\n        cache.set(\n            {\n                'model': list(form['model'].keys()),  # In Python 3, dictionary keys are not serializable\n                'non_data_fields': self.non_data_fields\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the log message for MongoDB and return a dict of the log records.", "response": "def _parse_msg_for_mongodb(self, msgs):\n        '''\n        >>> mdbf = MongoDBForwarder('no_host', '27017', 'deadpool',\n        ...                             'chimichanga', 'logs', 'collection')\n        >>> log = [{u'data': {u'_': {u'file': u'log.py',\n        ...                    u'fn': u'start',\n        ...                    u'ln': 8,\n        ...                    u'name': u'__main__'},\n        ...             u'a': 1,\n        ...             u'b': 2,\n        ...             u'msg': u'this is a dummy log'},\n        ...   u'error': False,\n        ...   u'error_tb': u'',\n        ...   u'event': u'some_log',\n        ...   u'file': u'/var/log/sample.log',\n        ...   u'formatter': u'logagg.formatters.basescript',\n        ...   u'host': u'deepcompute',\n        ...   u'id': u'20180409T095924_aec36d313bdc11e89da654e1ad04f45e',\n        ...   u'level': u'info',\n        ...   u'raw': u'{...}',\n        ...   u'timestamp': u'2018-04-09T09:59:24.733945Z',\n        ...   u'type': u'metric'}]\n\n        >>> records = mdbf._parse_msg_for_mongodb(log)\n        >>> from pprint import pprint\n        >>> pprint(records)\n        [{'_id': u'20180409T095924_aec36d313bdc11e89da654e1ad04f45e',\n          u'data': {u'_': {u'file': u'log.py',\n                           u'fn': u'start',\n                           u'ln': 8,\n                           u'name': u'__main__'},\n                    u'a': 1,\n                    u'b': 2,\n                    u'msg': u'this is a dummy log'},\n          u'error': False,\n          u'error_tb': u'',\n          u'event': u'some_log',\n          u'file': u'/var/log/sample.log',\n          u'formatter': u'logagg.formatters.basescript',\n          u'host': u'deepcompute',\n          u'level': u'info',\n          u'raw': u'{...}',\n          u'timestamp': u'2018-04-09T09:59:24.733945Z',\n          u'type': u'metric'}]\n        '''\n        msgs_list = []\n        for msg in msgs:\n            try:\n                msg['_id'] = msg.pop('id')\n            except KeyError:\n                self.log.exception('collector_failure_id_not_found', log=msg)\n            msgs_list.append(msg)\n        return msgs_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a tag and field for a log record.", "response": "def _tag_and_field_maker(self, event):\n        '''\n        >>> idbf = InfluxDBForwarder('no_host', '8086', 'deadpool',\n        ...                             'chimichanga', 'logs', 'collection')\n        >>> log = {u'data': {u'_': {u'file': u'log.py',\n        ...                         u'fn': u'start',\n        ...                         u'ln': 8,\n        ...                         u'name': u'__main__'},\n        ...             u'a': 1,\n        ...             u'b': 2,\n        ...             u'__ignore_this': 'some_string',\n        ...             u'msg': u'this is a dummy log'},\n        ...   u'error': False,\n        ...   u'error_tb': u'',\n        ...   u'event': u'some_log',\n        ...   u'file': u'/var/log/sample.log',\n        ...   u'formatter': u'logagg.formatters.basescript',\n        ...   u'host': u'deepcompute',\n        ...   u'id': u'20180409T095924_aec36d313bdc11e89da654e1ad04f45e',\n        ...   u'level': u'info',\n        ...   u'raw': u'{...}',\n        ...   u'timestamp': u'2018-04-09T09:59:24.733945Z',\n        ...   u'type': u'metric'}\n\n        >>> tags, fields = idbf._tag_and_field_maker(log)\n        >>> from pprint import pprint\n        >>> pprint(tags)\n        {u'data.msg': u'this is a dummy log',\n         u'error_tb': u'',\n         u'file': u'/var/log/sample.log',\n         u'formatter': u'logagg.formatters.basescript',\n         u'host': u'deepcompute',\n         u'level': u'info'}\n        >>> pprint(fields)\n        {u'data._': \"{u'ln': 8, u'fn': u'start', u'file': u'log.py', u'name': u'__main__'}\",\n         u'data.a': 1,\n         u'data.b': 2}\n\n        '''\n        data = event.pop('data')\n        data = flatten_dict({'data': data})\n\n        t = dict((k, event[k]) for k in event if k not in self.EXCLUDE_TAGS)\n        f = dict()\n\n        for k in data:\n            v = data[k]\n\n            if is_number(v) or isinstance(v, MarkValue):\n                f[k] = v\n            else:\n                #if v.startswith('_'): f[k] = eval(v.split('_', 1)[1])\n                t[k] = v\n\n        return t, f"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the log message for InfluxDB.", "response": "def _parse_msg_for_influxdb(self, msgs):\n        '''\n        >>> from logagg.forwarders import InfluxDBForwarder\n        >>> idbf = InfluxDBForwarder('no_host', '8086', 'deadpool',\n        ...                             'chimichanga', 'logs', 'collection')\n\n        >>> valid_log = [{u'data': {u'_force_this_as_field': 'CXNS CNS nbkbsd',\n        ...             u'a': 1,\n        ...             u'b': 2,\n        ...             u'msg': u'this is a dummy log'},\n        ...   u'error': False,\n        ...   u'error_tb': u'',\n        ...   u'event': u'some_log',\n        ...   u'file': u'/var/log/sample.log',\n        ...   u'formatter': u'logagg.formatters.basescript',\n        ...   u'host': u'deepcompute',\n        ...   u'id': u'20180409T095924_aec36d313bdc11e89da654e1ad04f45e',\n        ...   u'level': u'info',\n        ...   u'raw': u'{...}',\n        ...   u'timestamp': u'2018-04-09T09:59:24.733945Z',\n        ...   u'type': u'metric'}]\n\n        >>> pointvalues = idbf._parse_msg_for_influxdb(valid_log)\n        >>> from pprint import pprint\n        >>> pprint(pointvalues)\n        [{'fields': {u'data._force_this_as_field': \"'CXNS CNS nbkbsd'\",\n                     u'data.a': 1,\n                     u'data.b': 2},\n          'measurement': u'some_log',\n          'tags': {u'data.msg': u'this is a dummy log',\n                   u'error_tb': u'',\n                   u'file': u'/var/log/sample.log',\n                   u'formatter': u'logagg.formatters.basescript',\n                   u'host': u'deepcompute',\n                   u'level': u'info'},\n          'time': u'2018-04-09T09:59:24.733945Z'}]\n\n        >>> invalid_log = valid_log\n        >>> invalid_log[0]['error'] = True\n        >>> pointvalues = idbf._parse_msg_for_influxdb(invalid_log)\n        >>> pprint(pointvalues)\n        []\n\n        >>> invalid_log = valid_log\n        >>> invalid_log[0]['type'] = 'log'\n        >>> pointvalues = idbf._parse_msg_for_influxdb(invalid_log)\n        >>> pprint(pointvalues)\n        []\n        '''\n\n        series = []\n\n        for msg in msgs:\n            if msg.get('error'):\n                continue\n\n            if msg.get('type').lower() == 'metric':\n                time = msg.get('timestamp')\n                measurement = msg.get('event')\n                tags, fields = self._tag_and_field_maker(msg)\n                pointvalues = {\n                    \"time\": time,\n                    \"measurement\": measurement,\n                    \"fields\": fields,\n                    \"tags\": tags}\n                series.append(pointvalues)\n\n        return series"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget input and process according to the input data type.", "response": "def get_input(source, files, threads=4, readtype=\"1D\",\n              combine=\"simple\", names=None, barcoded=False):\n    \"\"\"Get input and process accordingly.\n\n    Data can be:\n    - a uncompressed, bgzip, bzip2 or gzip compressed fastq file\n    - a uncompressed, bgzip, bzip2 or gzip compressed fasta file\n    - a rich fastq containing additional key=value information in the description,\n      as produced by MinKNOW and albacore with the same compression options as above\n    - a sorted bam file\n    - a sorted cram file\n    - a (compressed) sequencing_summary.txt file generated by albacore\n\n    Handle is passed to the proper functions to get DataFrame with metrics\n    Multiple files of the same type can be used to extract info from, which is done in parallel\n    Arguments:\n    - source: defines the input data type and the function that needs to be called\n    - files: is a list of one or more files to operate on, from the type of <source>\n    - threads: is the amount of workers which can be used\n    - readtype: (only relevant for summary input) and specifies which columns have to be extracted\n    - combine: is either 'simple' or 'track', with the difference that  with 'track' an additional\n      field is created with the name of the dataset\n    - names: if combine=\"track\", the names to be used for the datasets. Needs to have same length as\n      files, or None\n    \"\"\"\n    proc_functions = {\n        'fastq': ex.process_fastq_plain,\n        'fasta': ex.process_fasta,\n        'bam': ex.process_bam,\n        'summary': ex.process_summary,\n        'fastq_rich': ex.process_fastq_rich,\n        'fastq_minimal': ex.process_fastq_minimal,\n        'cram': ex.process_cram,\n        'ubam': ex.process_ubam, }\n    filethreads = min(len(files), threads)\n    threadsleft = threads - filethreads\n    with cfutures.ProcessPoolExecutor(max_workers=filethreads) as executor:\n        extration_function = partial(proc_functions[source],\n                                     threads=threadsleft,\n                                     readtype=readtype,\n                                     barcoded=barcoded)\n        datadf = combine_dfs(\n            dfs=[out for out in executor.map(extration_function, files)],\n            names=names or files,\n            method=combine)\n    if \"readIDs\" in datadf and pd.isna(datadf[\"readIDs\"]).any():\n        datadf.drop(\"readIDs\", axis='columns', inplace=True)\n    datadf = calculate_start_time(datadf)\n    logging.info(\"Nanoget: Gathered all metrics of {} reads\".format(len(datadf)))\n    if len(datadf) == 0:\n        logging.critical(\"Nanoget: no reads retrieved.\".format(len(datadf)))\n        sys.exit(\"Fatal: No reads found in input.\")\n    else:\n        return datadf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_dfs(dfs, names, method):\n    if method == \"track\":\n        res = list()\n        for df, identifier in zip(dfs, names):\n            df[\"dataset\"] = identifier\n            res.append(df)\n        return pd.concat(res, ignore_index=True)\n    elif method == \"simple\":\n        return pd.concat(dfs, ignore_index=True)", "response": "Combine dataframes.\n\n    Combination is either done simple by just concatenating the DataFrames\n    or performs tracking by adding the name of the dataset as a column."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_start_time(df):\n    if \"time\" in df:\n        df[\"time_arr\"] = pd.Series(df[\"time\"], dtype='datetime64[s]')\n    elif \"timestamp\" in df:\n        df[\"time_arr\"] = pd.Series(df[\"timestamp\"], dtype=\"datetime64[ns]\")\n    else:\n        return df\n    if \"dataset\" in df:\n        for dset in df[\"dataset\"].unique():\n            time_zero = df.loc[df[\"dataset\"] == dset, \"time_arr\"].min()\n            df.loc[df[\"dataset\"] == dset, \"start_time\"] = \\\n                df.loc[df[\"dataset\"] == dset, \"time_arr\"] - time_zero\n    else:\n        df[\"start_time\"] = df[\"time_arr\"] - df[\"time_arr\"].min()\n    return df.drop([\"time\", \"timestamp\", \"time_arr\"], axis=1, errors=\"ignore\")", "response": "Calculate the start time per read."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parser_from_buffer(cls, fp):\n        yaml = YAML(typ=\"safe\")\n        return cls(yaml.load(fp))", "response": "Construct YamlParser from a file pointer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_dict_expected_keys(self, expected_keys, current, dict_name):\n        if not isinstance(current, dict):\n            raise ParseError(u\"'{}' key must be a dict\".format(dict_name),\n                             YAML_EXAMPLE)\n        expected_keys = set(expected_keys)\n        current_keys = {key for key in current}\n        extra_keys = current_keys - expected_keys\n        if extra_keys:\n            message = u\"{}: the keys {} are unexpected. (allowed keys: {})\"\n            raise ParseError(\n                message.format(\n                    dict_name,\n                    list(extra_keys),\n                    list(expected_keys),\n                ),\n                YAML_EXAMPLE,\n            )", "response": "Check that we don t have unknown keys in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self):\n        if not self.parsed.get('migration'):\n            raise ParseError(u\"'migration' key is missing\", YAML_EXAMPLE)\n        self.check_dict_expected_keys(\n            {'options', 'versions'}, self.parsed['migration'], 'migration',\n        )\n        return self._parse_migrations()", "response": "Check input and return a : class : Migration instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a : class : Migration instance.", "response": "def _parse_migrations(self):\n        \"\"\"Build a :class:`Migration` instance.\"\"\"\n        migration = self.parsed['migration']\n        options = self._parse_options(migration)\n        versions = self._parse_versions(migration, options)\n        return Migration(versions, options)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_options(self, migration):\n        options = migration.get('options', {})\n        install_command = options.get('install_command')\n        backup = options.get('backup')\n        if backup:\n            self.check_dict_expected_keys(\n                {'command', 'ignore_if', 'stop_on_failure'},\n                options['backup'], 'backup',\n            )\n            backup = MigrationBackupOption(\n                command=backup.get('command'),\n                ignore_if=backup.get('ignore_if'),\n                stop_on_failure=backup.get('stop_on_failure', True),\n            )\n        return MigrationOption(\n            install_command=install_command,\n            backup=backup,\n        )", "response": "Parse the migration options and return a MigrationOption and MigrationBackupOption instances."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_message(self, title, msg, typ, url=None):\n        return self.user.send_notification(title=title,\n                                           message=msg,\n                                           typ=typ,\n                                           url=url)", "response": "Sets user notification message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if current user has the given permission.", "response": "def has_permission(self, perm):\n        \"\"\"\n        Checks if current user (or role) has the given permission.\n\n        Args:\n            perm: Permmission code or object.\n             Depends on the :attr:`~zengine.auth.auth_backend.AuthBackend` implementation.\n\n        Returns:\n            Boolean.\n        \"\"\"\n        return self.user.superuser or self.auth.has_permission(perm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef msg_box(self, msg, title=None, typ='info'):\n        self.output['msgbox'] = {'type': typ, \"title\": title or msg[:20], \"msg\": msg}", "response": "Create a message box"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending off current user that s/he finished it s job for now.", "response": "def sendoff_current_user(self):\n        \"\"\"\n        Tell current user that s/he finished it's job for now.\n        We'll notify if workflow arrives again to his/her WF Lane.\n        \"\"\"\n        msgs = self.task_data.get('LANE_CHANGE_MSG', DEFAULT_LANE_CHANGE_MSG)\n        self.msg_box(title=msgs['title'], msg=msgs['body'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invite_other_parties(self, possible_owners):\n        signals.lane_user_change.send(sender=self.user,\n                                      current=self,\n                                      old_lane=self.old_lane,\n                                      possible_owners=possible_owners\n                                      )", "response": "Invites the next lane s owner to participate\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the task object with the current task s data and the lane data.", "response": "def _update_task(self, task):\n        \"\"\"\n        Assigns current task step to self.task\n        then updates the task's data with self.task_data\n\n        Args:\n            task: Task object.\n        \"\"\"\n        self.task = task\n        self.task.data.update(self.task_data)\n        self.task_type = task.task_spec.__class__.__name__\n        self.spec = task.task_spec\n        self.task_name = task.get_name()\n        self.activity = getattr(self.spec, 'service_class', '')\n        self._set_lane_data()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_client_cmds(self):\n        self.task_data['cmd'] = self.input.get('cmd')\n\n        self.task_data['flow'] = self.input.get('flow')\n\n        filters = self.input.get('filters', {})\n\n        try:\n            if isinstance(filters, dict):\n                # this is the new form, others will be removed when ui be ready\n                self.task_data['object_id'] = filters.get('object_id')['values'][0]\n            elif filters[0]['field'] == 'object_id':\n                self.task_data['object_id'] = filters[0]['values'][0]\n        except:\n            if 'object_id' in self.input:\n                self.task_data['object_id'] = self.input.get('object_id')", "response": "This method is called by the command and flow functions in the workflows. It will set the appropriate client variables for the current command and flow for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_move(self, position):\n        while True:\n            print(position)\n            raw = input(str(self.color) + \"\\'s move \\n\")\n            move = converter.short_alg(raw, self.color, position)\n\n            if move is None:\n                continue\n\n            return move", "response": "Generates a valid and legal move given position\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in_check_as_result(self, pos, move):\n        test = cp(pos)\n        test.update(move)\n        test_king = test.get_king(move.color)\n\n        return self.loc_adjacent_to_opponent_king(test_king.location, test)", "response": "Returns True if the move would make both kings meet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loc_adjacent_to_opponent_king(self, location, position):\n        for fn in self.cardinal_directions:\n            try:\n                if isinstance(position.piece_at_square(fn(location)), King) and \\\n                        position.piece_at_square(fn(location)).color != self.color:\n                    return True\n\n            except IndexError:\n                pass\n\n        return False", "response": "Returns True if 2 kings are touching given the position of the opponent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, func, position):\n        try:\n            if self.loc_adjacent_to_opponent_king(func(self.location), position):\n                return\n        except IndexError:\n            return\n\n        if position.is_square_empty(func(self.location)):\n            yield self.create_move(func(self.location), notation_const.MOVEMENT)\n\n        elif position.piece_at_square(func(self.location)).color != self.color:\n            yield self.create_move(func(self.location), notation_const.CAPTURE)", "response": "Add all 8 cardinal directions as moves for the King if legal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rook_legal_for_castle(self, rook):\n        return rook is not None and \\\n            type(rook) is Rook and \\\n            rook.color == self.color and \\\n            not rook.has_moved", "response": "Returns True if given rook exists and is eligible to castle."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if set of squares in between King and Rook are empty and safe for the king to castle.", "response": "def _empty_not_in_check(self, position, direction):\n        \"\"\"\n        Checks if set of squares in between ``King`` and ``Rook`` are empty and safe\n        for the king to castle.\n\n        :type: position: Position\n        :type: direction: function\n        :type: times: int\n        :rtype: bool\n        \"\"\"\n        def valid_square(square):\n            return position.is_square_empty(square) and \\\n                   not self.in_check(position, square)\n\n        return valid_square(direction(self.location, 1)) and \\\n            valid_square(direction(self.location, 2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd kingside and queenside castling moves if legal and empty not in check.", "response": "def add_castle(self, position):\n        \"\"\"\n        Adds kingside and queenside castling moves if legal\n\n        :type: position: Board\n        \"\"\"\n        if self.has_moved or self.in_check(position):\n            return\n\n        if self.color == color.white:\n            rook_rank = 0\n        else:\n            rook_rank = 7\n\n        castle_type = {\n            notation_const.KING_SIDE_CASTLE: {\n                \"rook_file\": 7,\n                \"direction\": lambda king_square, times: king_square.shift_right(times)\n            },\n            notation_const.QUEEN_SIDE_CASTLE: {\n                \"rook_file\": 0,\n                \"direction\": lambda king_square, times: king_square.shift_left(times)\n            }\n        }\n        for castle_key in castle_type:\n            castle_dict = castle_type[castle_key]\n            castle_rook = position.piece_at_square(Location(rook_rank, castle_dict[\"rook_file\"]))\n            if self._rook_legal_for_castle(castle_rook) and \\\n                    self._empty_not_in_check(position, castle_dict[\"direction\"]):\n                yield self.create_move(castle_dict[\"direction\"](self.location, 2), castle_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef possible_moves(self, position):\n        # Chain used to combine multiple generators\n        for move in itertools.chain(*[self.add(fn, position) for fn in self.cardinal_directions]):\n            yield move\n\n        for move in self.add_castle(position):\n            yield move", "response": "Generates a list of possible moves for the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the king is in check or False otherwise.", "response": "def in_check(self, position, location=None):\n        \"\"\"\n        Finds if the king is in check or if both kings are touching.\n\n        :type: position: Board\n        :return: bool\n        \"\"\"\n        location = location or self.location\n        for piece in position:\n\n            if piece is not None and piece.color != self.color:\n                if not isinstance(piece, King):\n                    for move in piece.possible_moves(position):\n\n                        if move.end_loc == location:\n                            return True\n                else:\n                    if self.loc_adjacent_to_opponent_king(piece.location, position):\n                        return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the keep - alive setting for the peer socket.", "response": "def set_keep_alive(sock, idle=10, interval=5, fails=5):\n    \"\"\"Sets the keep-alive setting for the peer socket.\n\n    :param sock: Socket to be configured.\n    :param idle: Interval in seconds after which for an idle connection a keep-alive probes\n      is start being sent.\n    :param interval: Interval in seconds between probes.\n    :param fails: Maximum number of failed probes.\n    \"\"\"\n    import sys\n\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n    if sys.platform in ('linux', 'linux2'):\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, idle)\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, interval)\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, fails)\n    elif sys.platform == 'darwin':\n        sock.setsockopt(socket.IPPROTO_TCP, 0x10, interval)\n    else:\n        # Do nothing precise for unsupported platforms.\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Board with the standard chess starting position.", "response": "def init_default(cls):\n        \"\"\"\n        Creates a ``Board`` with the standard chess starting position.\n\n        :rtype: Board\n        \"\"\"\n        return cls([\n\n            # First rank\n            [Rook(white, Location(0, 0)), Knight(white, Location(0, 1)), Bishop(white, Location(0, 2)),\n             Queen(white, Location(0, 3)), King(white, Location(0, 4)), Bishop(white, Location(0, 5)),\n             Knight(white, Location(0, 6)), Rook(white, Location(0, 7))],\n\n            # Second rank\n            [Pawn(white, Location(1, file)) for file in range(8)],\n\n            # Third rank\n            [None for _ in range(8)],\n\n            # Fourth rank\n            [None for _ in range(8)],\n\n            # Fifth rank\n            [None for _ in range(8)],\n\n            # Sixth rank\n            [None for _ in range(8)],\n\n            # Seventh rank\n            [Pawn(black, Location(6, file)) for file in range(8)],\n\n            # Eighth rank\n            [Rook(black, Location(7, 0)), Knight(black, Location(7, 1)), Bishop(black, Location(7, 2)),\n             Queen(black, Location(7, 3)), King(black, Location(7, 4)), Bishop(black, Location(7, 5)),\n             Knight(black, Location(7, 6)), Rook(black, Location(7, 7))]\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef material_advantage(self, input_color, val_scheme):\n\n        if self.get_king(input_color).in_check(self) and self.no_moves(input_color):\n            return -100\n\n        if self.get_king(-input_color).in_check(self) and self.no_moves(-input_color):\n            return 100\n\n        return sum([val_scheme.val(piece, input_color) for piece in self])", "response": "Returns the advantage of a particular side possesses given a value scheme."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef advantage_as_result(self, move, val_scheme):\n        test_board = cp(self)\n        test_board.update(move)\n        return test_board.material_advantage(move.color, val_scheme)", "response": "Calculates the advantage after a piece is played and returns it as a double."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if all the possible moves have already been calculated and stores it in self. possible_moves.", "response": "def all_possible_moves(self, input_color):\n        \"\"\"\n        Checks if all the possible moves has already been calculated\n        and is stored in `possible_moves` dictionary. If not, it is calculated\n        with `_calc_all_possible_moves`.\n        \n        :type: input_color: Color\n        :rtype: list\n        \"\"\"\n        position_tuple = self.position_tuple\n        if position_tuple not in self.possible_moves:\n            self.possible_moves[position_tuple] = tuple(self._calc_all_possible_moves(input_color))\n\n        return self.possible_moves[position_tuple]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calc_all_possible_moves(self, input_color):\n        for piece in self:\n\n            # Tests if square on the board is not empty\n            if piece is not None and piece.color == input_color:\n\n                for move in piece.possible_moves(self):\n\n                    test = cp(self)\n                    test_move = Move(end_loc=move.end_loc,\n                                     piece=test.piece_at_square(move.start_loc),\n                                     status=move.status,\n                                     start_loc=move.start_loc,\n                                     promoted_to_piece=move.promoted_to_piece)\n                    test.update(test_move)\n\n                    if self.king_loc_dict is None:\n                        yield move\n                        continue\n\n                    my_king = test.piece_at_square(self.king_loc_dict[input_color])\n\n                    if my_king is None or \\\n                            not isinstance(my_king, King) or \\\n                            my_king.color != input_color:\n                        self.king_loc_dict[input_color] = test.find_king(input_color)\n                        my_king = test.piece_at_square(self.king_loc_dict[input_color])\n\n                    if not my_king.in_check(test):\n                        yield move", "response": "Calculates all possible moves for the given color."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning multiple processes in parallel.", "response": "def runInParallel(*fns):\n        \"\"\"\n        Runs multiple processes in parallel.\n\n        :type: fns: def\n        \"\"\"\n        proc = []\n        for fn in fns:\n            p = Process(target=fn)\n            p.start()\n            proc.append(p)\n        for p in proc:\n            p.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the first Location of the first piece that matches piece.", "response": "def find_piece(self, piece):\n        \"\"\"\n        Finds Location of the first piece that matches piece.\n        If none is found, Exception is raised.\n\n        :type: piece: Piece\n        :rtype: Location\n        \"\"\"\n        for i, _ in enumerate(self.position):\n            for j, _ in enumerate(self.position):\n                loc = Location(i, j)\n\n                if not self.is_square_empty(loc) and \\\n                        self.piece_at_square(loc) == piece:\n                    return loc\n\n        raise ValueError(\"{} \\nPiece not found: {}\".format(self, piece))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_piece(self, piece_type, input_color):\n        for loc in self:\n            piece = self.piece_at_square(loc)\n\n            if not self.is_square_empty(loc) and \\\n                    isinstance(piece, piece_type) and \\\n                    piece.color == input_color:\n                return loc\n\n        raise Exception(\"{} \\nPiece not found: {}\".format(self, piece_type))", "response": "Gets the location of a piece on the board given the type and color."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplaces a piece at given location.", "response": "def place_piece_at_square(self, piece, location):\n        \"\"\"\n        Places piece at given get_location\n\n        :type: piece: Piece\n        :type: location: Location\n        \"\"\"\n        self.position[location.rank][location.file] = piece\n        piece.location = location"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef move_piece(self, initial, final):\n        self.place_piece_at_square(self.piece_at_square(initial), final)\n        self.remove_piece_at_square(initial)", "response": "Moves a piece from one location to another."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _balance(self, ):\n        meta = h.meta\n        for n in (skin_reagent.keys() | skin_product.keys()):\n            lost = skin_reagent[n]\n            cycle_lost = cycle(lost)\n            new = skin_product[n]\n            cycle_new = cycle(new)\n            atom = h._node[n]\n            dr = atom.p_radical - atom.radical\n            # radical balancing\n            if dr > 0:  # radical added or increased.\n                for _, m in zip(range(dr), cycle_lost):  # homolysis\n                    s_atom = h._node[m]\n                    s_atom.p_multiplicity = radical_unmap[s_atom.p_radical + 1]\n                    meta.setdefault('rule #14. atom lost. common atom radical added or increased. '\n                                    'lost atom radical added', []).append((m, n))\n                for m in lost[dr:]:\n                    meta.setdefault('rule #15. atom lost. common atom radical added or increased. '\n                                    'lost atom radical unchanged', []).append((m, n))\n            elif dr < 0:  # radical removed or decreased.\n                if n in skin_product:\n                    for m in lost:\n                        meta.setdefault('rule #20. atom lost. common atom radical removed or decreased. '\n                                        'lost atom radical unchanged', []).append((m, n))\n                else:\n                    for _, m in zip(range(-dr), cycle_lost):  # radical elimination\n                        s_atom = h._node[m]\n                        s_atom.p_multiplicity = radical_unmap[s_atom.p_radical + 1]\n                        meta.setdefault('rule #21. atom lost. common atom radical removed or decreased. '\n                                        'lost atom radical added', []).append((m, n))\n                    for m in lost[-dr:]:\n                        meta.setdefault('rule #20. atom lost. common atom radical removed or decreased. '\n                                        'lost atom radical unchanged', []).append((m, n))\n            else:\n                env = h.environment(n)\n                sv = atom.get_valence([(b.reagent, a.reagent) for b, a in env if b.order])\n                pv = atom.p_get_valence([(b.product, a.product) for b, a in env if b.p_order])\n                sh, ph = h.atom_total_h(n)\n\n                dv = pv - sv\n                dh = ph - sh\n                dc = atom.p_charge - atom.charge\n\n                if not (dv or dh or dc):  # common atom unchanged. Substitution, Elimination\n                    for m in skins:\n                        meta.setdefault('rule #1. atom lost. common atom unchanged. '\n                                        'substitution, elimination, addition', []).append((m, n))\n                elif dv == dh == dc < 0:  # explicit hydrogen removing\n                    for m in skins:\n                        h._node[m].p_charge = 1\n                        meta.setdefault('rule #4. atom lost. common atom deprotonation', []).append((m, n))\n                else:\n                    for m in skins:\n                        meta.setdefault('rule #5. atom lost. common atom changed. '\n                                        'convert to reduction or oxidation', []).append((m, n))\n\n                    pth = ph + sum(h.atom_total_h(x)[1] for x in skins)\n                    if n in skin_product:\n                        sth = sh + sum(h.atom_total_h(x)[0] for x in skin_product[n])\n                    else:\n                        sth = sh\n                    dth = pth - sth\n\n        for n, skins in skin_product.items():\n            cycle_skins = cycle(skins)\n            atom = h._node[n]\n            dr = atom.p_radical - atom.radical\n            # radical balancing\n            if dr > 0:  # radical added or increased.\n                if n in skin_reagent:\n                    for m in skins:\n                        meta.setdefault('rule #16. atom new. common atom radical added or increased. '\n                                        'new atom radical unchanged', []).append((m, n))\n                else:\n                    for _, m in zip(range(dr), cycle_skins):  # radical addition\n                        s_atom = h._node[m]\n                        s_atom.multiplicity = radical_unmap[s_atom.radical + 1]\n                        meta.setdefault('rule #17. atom new. common atom radical added or increased. '\n                                        'new atom radical added', []).append((m, n))\n                    for m in skins[dr:]:\n                        meta.setdefault('rule #16. atom new. common atom radical added or increased. '\n                                        'new atom radical unchanged', []).append((m, n))\n            elif dr < 0:  # radical removed or decreased.\n                for _, m in zip(range(-dr), cycle_skins):  # recombination\n                    s_atom = h._node[m]\n                    s_atom.multiplicity = radical_unmap[s_atom.radical + 1]\n                    meta.setdefault('rule #18. atom new. common atom radical removed or decreased. '\n                                    'new atom radical added', []).append((m, n))\n                for m in skins[-dr:]:\n                    meta.setdefault('rule #19. atom new. common atom radical removed or decreased. '\n                                    'new atom radical unchanged', []).append((m, n))\n            else:\n                env = h.environment(n)\n                sv = atom.get_valence([(b.reagent, a.reagent) for b, a in env if b.order])\n                pv = atom.p_get_valence([(b.product, a.product) for b, a in env if b.p_order])\n                sh, ph = h.atom_total_h(n)\n\n                dv = pv - sv\n                dh = ph - sh\n                dc = atom.p_charge - atom.charge\n\n                if not (dv or dh or dc):  # common atom unchanged. Substitution, Addition\n                    for m in skins:\n                        meta.setdefault('rule #2. atom new. common atom unchanged. '\n                                        'substitution, elimination, addition', []).append((m, n))\n                elif dv == dh == dc > 0:  # explicit hydrogen addition\n                    for m in skins:\n                        h._node[m].charge = 1\n                        h.meta.setdefault('rule #3. atom new. common atom protonation', []).append((m, n))\n                else:\n                    for m in skins:\n                        meta.setdefault('rule #6. atom new. common atom changed. '\n                                        'convert to reduction or oxidation', []).append((m, n))\n\n                    sth = sh + sum(h.atom_total_h(x)[0] for x in skins)\n                    if n in skin_reagent:\n                        pth = ph + sum(h.atom_total_h(x)[1] for x in skin_reagent[n])\n                    else:\n                        pth = ph\n                    dth = pth - sth\n\n        for n, sp in reverse_ext.items():\n\n            # charge neutralization\n            if dc > 0:\n                for _ in range(dc):\n                    h.meta.setdefault('rule #7. charge neutralization. hydroxide radical added',\n                                      []).append(h.add_atom(O(multiplicity=2), O(charge=-1)))\n            elif dc < 0:\n                for _ in range(-dc):\n                    h.meta.setdefault('rule #8. charge neutralization. hydrogen radical added',\n                                      []).append(h.add_atom(H(multiplicity=2), H(charge=1)))\n\n            # hydrogen balancing\n            if dth > 0:\n                red_e = 0\n                for m in sp['products']:\n                    if h.nodes[m]['element'] == 'H':  # set reduction H if explicit H count increased\n                        h.nodes[m]['s_radical'] = 2\n                        red_e += 1\n                        h.meta.setdefault('rule #11. protonation. new explicit hydrogen radical added',\n                                          []).append(m)\n\n                red = []\n                for _ in range(dth - red_e):  # add reduction agents\n                    m = h.add_atom(H(multiplicity=2), H())\n                    red.append(m)\n                    h.meta.setdefault('rule #10. protonation. hydrogen radical added', []).append(m)\n                red = iter(red)\n\n                dih = sub(*h.atom_implicit_h(n))\n                if dih < 0:  # attach reduction H to central atom if implicit H atoms count increased\n                    for _ in range(-dih):\n                        m = next(red)\n                        h.add_bond(m, n, None)\n                        h.meta.setdefault('rule #12. protonation. new implicit hydrogen radical added',\n                                          []).append(m)\n\n                for m in sp['reagents']:  # attach reduction H if detached group implicit H count increased\n                    dih = sub(*h.atom_implicit_h(m))\n                    if dih < 0:\n                        for _ in range(-dih):\n                            o = next(red)\n                            h.add_bond(o, m, None)\n            elif dth < 0:\n                oxo = []\n                for _ in range(-dth):\n                    m = h.add_atom(O(multiplicity=2), O())\n                    oxo.append(m)\n                    h.meta.setdefault('rule #9. deprotonation. hydroxide radical added', []).append(m)\n                oxo = iter(oxo)\n\n                for m in sp['reagents']:\n                    if h.nodes[m]['element'] == 'H':\n                        o = next(oxo)\n                        h.add_bond(o, m, None)\n                        h.meta.setdefault('rule #13. hydrogen accepting by hydroxide radical added',\n                                          []).append(m)\n\n        return h", "response": "calculate unbalanced charges and radicals for skin atoms\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone_subgraphs(self, g):\n        if not isinstance(g, CGRContainer):\n            raise InvalidData('only CGRContainer acceptable')\n\n        r_group = []\n        x_group = {}\n        r_group_clones = []\n        newcomponents = []\n\n        ''' search bond breaks and creations\n        '''\n        components, lost_bonds, term_atoms = self.__split_graph(g)\n        lost_map = {x: y for x, y in lost_bonds}\n        ''' extract subgraphs and sort by group type (R or X)\n        '''\n        x_terminals = set(lost_map.values())\n        r_terminals = set(lost_map)\n\n        for i in components:\n            x_terminal_atom = x_terminals.intersection(i)\n            if x_terminal_atom:\n                x_group[x_terminal_atom.pop()] = i\n                continue\n\n            r_terminal_atom = r_terminals.intersection(i)\n            if r_terminal_atom:\n                r_group.append([r_terminal_atom, i])\n                continue\n\n            newcomponents.append(i)\n        ''' search similar R groups and patch.\n        '''\n        tmp = g\n        for i in newcomponents:\n            for k, j in r_group:\n                gm = GraphMatcher(j, i, node_match=self.__node_match_products,\n                                  edge_match=self.__edge_match_products)\n                ''' search for similar R-groups started from bond breaks.\n                '''\n                mapping = next((x for x in gm.subgraph_isomorphisms_iter() if k.issubset(x) and\n                                all(x[y] in term_atoms for y in k)), None)\n                if mapping:\n                    r_group_clones.append([k, mapping])\n                    tmp = compose(tmp, self.__remap_group(j, tmp, mapping)[0])\n                    break\n\n        ''' add lose X groups to R groups\n        '''\n        for i, j in r_group_clones:\n            for k in i:\n                remappedgroup, mapping = self.__remap_group(x_group[lost_map[k]], tmp, {})\n                tmp = CGRcore.union(tmp, remappedgroup)\n                tmp.add_edge(j[k], mapping[lost_map[k]], s_bond=1, sp_bond=(1, None))\n\n        if r_group_clones:\n            tmp.meta.update(g.meta)\n            return tmp\n\n        return tmp.copy()", "response": "clone subgraphs from another CGRContainer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_substitution_paths(g):\n        for n, nbrdict in g.adjacency():\n            for m, l in combinations(nbrdict, 2):\n                nms = nbrdict[m]['sp_bond']\n                nls = nbrdict[l]['sp_bond']\n                if nms == (1, None) and nls == (None, 1):\n                    yield m, n, l\n                elif nms == (None, 1) and nls == (1, None):\n                    yield l, n, m", "response": "get atoms paths from detached atom to attached atom"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef versions(self):\n        if self._versions is None:\n            with self.database.cursor_autocommit() as cursor:\n                query = \"\"\"\n                SELECT number,\n                       date_start,\n                       date_done,\n                       log,\n                       addons\n                FROM {}\n                \"\"\".format(self.table_name)\n                cursor.execute(query)\n                rows = cursor.fetchall()\n                versions = []\n                for row in rows:\n                    row = list(row)\n                    # convert 'addons' to json\n                    row[4] = json.loads(row[4]) if row[4] else []\n                    versions.append(\n                        self.VersionRecord(*row)\n                    )\n                self._versions = versions\n        return self._versions", "response": "Read versions from the table\nCOOKIE table and store them in self. _versions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of menu entries for auto - generated models.", "response": "def simple_crud():\n        \"\"\"\n        Prepares menu entries for auto-generated model CRUD views.\n        This is simple version of :attr:`get_crud_menus()` without\n        Category support and permission control.\n        Just for development purposes.\n\n        Returns:\n            Dict of list of dicts (``{'':[{}],}``). Menu entries.\n\n        \"\"\"\n        results = defaultdict(list)\n        for mdl in model_registry.get_base_models():\n            results['other'].append({\"text\": mdl.Meta.verbose_name_plural,\n                                     \"wf\": 'crud',\n                                     \"model\": mdl.__name__,\n                                     \"kategori\": settings.DEFAULT_OBJECT_CATEGORY_NAME})\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of dicts with keys of object type and values of objects of type CRUD.", "response": "def get_crud_menus(self):\n        \"\"\"\n        Generates menu entries according to\n        :attr:`zengine.settings.OBJECT_MENU` and permissions\n        of current user.\n\n        Returns:\n            Dict of list of dicts (``{'':[{}],}``). Menu entries.\n        \"\"\"\n        results = defaultdict(list)\n        for object_type in settings.OBJECT_MENU:\n            for model_data in settings.OBJECT_MENU[object_type]:\n                if self.current.has_permission(model_data.get('wf', model_data['name'])):\n                    self._add_crud(model_data, object_type, results)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_crud(self, model_data, object_type, results):\n        model = model_registry.get_model(model_data['name'])\n        field_name = model_data.get('field')\n        verbose_name = model_data.get('verbose_name', model.Meta.verbose_name_plural)\n        category = model_data.get('category', settings.DEFAULT_OBJECT_CATEGORY_NAME)\n        wf_dict = {\"text\": verbose_name,\n                   \"wf\": model_data.get('wf', \"crud\"),\n                   \"model\": model_data['name'],\n                   \"kategori\": category}\n        if field_name:\n            wf_dict['param'] = field_name\n        results[object_type].append(wf_dict)\n        self._add_to_quick_menu(wf_dict['model'], wf_dict)", "response": "Adds a new entry for a given object type to the results dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_to_quick_menu(self, key, wf):\n        if key in settings.QUICK_MENU:\n            self.output['quick_menu'].append(wf)", "response": "Adds entry to the quick menu according to key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_workflow_menus(self):\n        results = defaultdict(list)\n        from zengine.lib.cache import WFSpecNames\n        for name, title, category in WFSpecNames().get_or_set():\n            if self.current.has_permission(name) and category != 'hidden':\n                wf_dict = {\n                    \"text\": title,\n                    \"wf\": name,\n                    \"kategori\": category,\n                    \"param\": \"id\"\n                }\n                results['other'].append(wf_dict)\n                self._add_to_quick_menu(name, wf_dict)\n        return results", "response": "Creates menu entries for custom workflows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_request(self, req, resp):\n        # req.stream corresponds to the WSGI wsgi.input environ variable,\n        # and allows you to read bytes from the request body.\n        #\n        # See also: PEP 3333\n        if req.content_length in (None, 0):\n            # Nothing to do\n            req.context['data'] = req.params.copy()\n            req.context['result'] = {}\n            return\n        else:\n            req.context['result'] = {}\n\n        body = req.stream.read()\n        if not body:\n            raise falcon.HTTPBadRequest('Empty request body',\n                                        'A valid JSON document is required.')\n\n        try:\n            json_data = body.decode('utf-8')\n            req.context['data'] = json.loads(json_data)\n            try:\n                log.info(\"REQUEST DATA: %s\" % json_data)\n            except:\n                log.exception(\"ERR: REQUEST DATA CANT BE LOGGED \")\n        except (ValueError, UnicodeDecodeError):\n            raise falcon.HTTPError(falcon.HTTP_753,\n                                   'Malformed JSON',\n                                   'Could not decode the request body. The '\n                                   'JSON was incorrect or not encoded as '\n                                   'UTF-8.')", "response": "Process the request body and return the object that represents the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_response(self, req, resp, resource):\n        if 'result' not in req.context:\n            return\n        req.context['result']['is_login'] = 'user_id' in req.env['session']\n        if settings.DEBUG:\n            req.context['result']['_debug_queries'] = sys._debug_db_queries\n            sys._debug_db_queries = []\n        if resp.body is None and req.context['result']:\n            resp.body = json.dumps(req.context['result'])\n\n        try:\n            log.debug(\"RESPONSE: %s\" % resp.body)\n        except:\n            log.exception(\"ERR: RESPONSE CANT BE LOGGED \")", "response": "Processes the response from Zengine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self):\n        if self.connecting:\n            log.info('PikaClient: Already connecting to RabbitMQ')\n            return\n\n        log.info('PikaClient: Connecting to RabbitMQ')\n        self.connecting = True\n\n        self.connection = TornadoConnection(NON_BLOCKING_MQ_PARAMS,\n                                            stop_ioloop_on_close=False,\n                                            custom_ioloop=self.io_loop,\n                                            on_open_callback=self.on_connected)", "response": "Creates connection to RabbitMQ server"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_connected(self, connection):\n        log.info('PikaClient: connected to RabbitMQ')\n        self.connected = True\n        self.in_channel = self.connection.channel(self.on_channel_open)", "response": "Called when RabbitMQ connection is established."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninput channel creation callback Queue declaration done here Args: channel: input channel", "response": "def on_channel_open(self, channel):\n        \"\"\"\n        Input channel creation callback\n        Queue declaration done here\n\n        Args:\n            channel: input channel\n        \"\"\"\n        self.in_channel.exchange_declare(exchange='input_exc', type='topic', durable=True)\n        channel.queue_declare(callback=self.on_input_queue_declare, queue=self.INPUT_QUEUE_NAME)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninput queue declaration callback.", "response": "def on_input_queue_declare(self, queue):\n        \"\"\"\n        Input queue declaration callback.\n        Input Queue/Exchange binding done here\n\n        Args:\n            queue: input queue\n        \"\"\"\n        self.in_channel.queue_bind(callback=None,\n                                   exchange='input_exc',\n                                   queue=self.INPUT_QUEUE_NAME,\n                                   routing_key=\"#\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wsgi_app(self, request):\n\n        try:\n            if request.method != 'POST':\n                abort(400)\n\n            try:\n                # Python 2.7 compatibility\n                data = request.data\n                if isinstance(data, str):\n                    body = json.loads(data)\n                else:\n                    body = json.loads(data.decode('utf-8'))\n            except ValueError:\n                abort(400)\n\n            if self.validate:\n                valid_cert = util.validate_request_certificate(\n                    request.headers, request.data)\n\n                valid_ts = util.validate_request_timestamp(body)\n\n                if not valid_cert or not valid_ts:\n                    log.error('failed to validate request')\n                    abort(403)\n\n            resp_obj = self.alexa.dispatch_request(body)\n            return Response(response=json.dumps(resp_obj, indent=4),\n                            status=200,\n                            mimetype='application/json')\n\n        except HTTPException as exc:\n            log.exception('Failed to handle request')\n            return exc", "response": "This function is called by the server when a request is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self, force=False):\n        if self.__write:\n            self.write = self.__write_adhoc\n            self.__write = False\n\n        if not self._is_buffer or force:\n            self._file.close()", "response": "close the file and all of its children"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_reaction(self, reaction):\n        if not (reaction['reactants'] or reaction['products'] or reaction['reagents']):\n            raise ValueError('empty reaction')\n        maps = {'reactants': [], 'products': [], 'reagents': []}\n        for i, tmp in maps.items():\n            for molecule in reaction[i]:\n                used = set()\n                for atom in molecule['atoms']:\n                    m = atom['mapping']\n                    if m:\n                        if m in used:\n                            if not self._ignore:\n                                raise MappingError('mapping in molecules should be unique')\n                            warning(f'non-unique mapping in molecule: {m}')\n                        else:\n                            used.add(m)\n                    tmp.append(m)\n\n        length = count(max(max(maps['products'], default=0), max(maps['reactants'], default=0),\n                           max(maps['reagents'], default=0)) + 1)\n\n        ''' map unmapped atoms.\n        '''\n        for i, tmp in maps.items():\n            used = set()\n            maps[i] = remap = []\n            for m in tmp:\n                if not m:\n                    remap.append(next(length))\n                elif m in used:\n                    if not self._ignore:\n                        raise MappingError('mapping in reagents or products or reactants should be unique')\n                    # force remap non unique atoms in molecules.\n                    remap.append(next(length))\n                    warning(f'mapping changed: {m} to {remap[-1]}')\n                else:\n                    remap.append(m)\n                    used.add(m)\n\n        if maps['reagents']:\n            tmp = (set(maps['reactants']) | set(maps['products'])) & set(maps['reagents'])\n            if tmp:\n                e = f'reagents has map intersection with reactants or products: {tmp}'\n                if not self._ignore:\n                    raise MappingError(e)\n                warning(e)\n                maps['reagents'] = [x if x not in tmp else next(length) for x in maps['reagents']]\n\n        ''' find breaks in map. e.g. 1,2,5,6. 3,4 - skipped\n        '''\n        if self.__remap:\n            lose = sorted(set(range(1, next(length))) - set(maps['reactants']) - set(maps['products']) -\n                          set(maps['reagents']), reverse=True)\n            if lose:\n                for i, tmp in maps.items():\n                    if not tmp:\n                        continue\n                    for j in lose:\n                        maps[i] = tmp = [x if x < j else x - 1 for x in tmp]\n        ''' end\n        '''\n        rc = ReactionContainer(meta=reaction['meta'])\n        for i, tmp in maps.items():\n            shift = 0\n            for j in reaction[i]:\n                atom_len = len(j['atoms'])\n                remapped = {x: y for x, y in enumerate(tmp[shift: atom_len + shift])}\n                shift += atom_len\n                g = self.__convert_structure(j, remapped)\n                rc[i].append(g)\n        return rc", "response": "Convert reaction to a new reaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aromatize(self):\n        rings = [x for x in self.sssr if 4 < len(x) < 7]\n        if not rings:\n            return 0\n        total = 0\n        while True:\n            c = self._quinonize(rings, 'order')\n            if c:\n                total += c\n            elif total:\n                break\n\n            c = self._aromatize(rings, 'order')\n            if not c:\n                break\n            total += c\n\n        if total:\n            self.flush_cache()\n        return total", "response": "convert structure to aromatic form\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the crawl job for n rounds", "response": "def crawl_cmd(self, seed_list, n):\n        '''\n        Runs the crawl job for n rounds\n        :param seed_list: lines of seed URLs\n        :param n: number of rounds\n        :return: number of successful rounds\n        '''\n\n        print(\"Num Rounds \"+str(n))\n\n        cc = self.proxy.Crawl(seed=seed_list, rounds=n)\n        rounds = cc.waitAll()\n        print(\"Completed %d rounds\" % len(rounds))\n        return len(rounds)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new config from xml file.", "response": "def load_xml_conf(self, xml_file, id):\n        '''\n        Creates a new config from xml file.\n        :param xml_file: path to xml file. Format : nutch-site.xml or nutch-default.xml\n        :param id:\n        :return: config object\n        '''\n\n        # converting nutch-site.xml to key:value pairs\n        import xml.etree.ElementTree as ET\n        tree = ET.parse(xml_file)\n        params = {}\n        for prop in tree.getroot().findall(\".//property\"):\n            params[prop.find('./name').text.strip()] = prop.find('./value').text.strip()\n        return self.proxy.Configs().create(id, configData=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsubbing - command AttributeNames Create", "response": "def create_cmd(self, args):\n        '''\n        'create' sub-command\n        :param args: cli arguments\n        :return:\n        '''\n        cmd = args.get('cmd_create')\n        if cmd == 'conf':\n            conf_file = args['conf_file']\n            conf_id = args['id']\n            return self.load_xml_conf(conf_file, conf_id)\n        else:\n            print(\"Error: Create %s is invalid or not implemented\" % cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the MRV file and write the close tag of MRV file and close opened file", "response": "def close(self, *args, **kwargs):\n        \"\"\"\n        write close tag of MRV file and close opened file\n\n        :param force: force closing of externally opened file or buffer\n        \"\"\"\n        if not self.__finalized:\n            self._file.write('</cml>')\n            self.__finalized = True\n        super().close(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a single molecule or reaction into the file", "response": "def write(self, data):\n        \"\"\"\n        write single molecule or reaction into file\n        \"\"\"\n        self._file.write('<cml>')\n        self.__write(data)\n        self.write = self.__write"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the cache file and return the list of byte offsets from the file", "response": "def _load_cache(self):\n        \"\"\"\n        the method is implemented for the purpose of optimization, byte positions will not be re-read from a file\n        that has already been used, if the content of the file has changed, and the name has been left the same,\n        the old version of byte offsets will be loaded\n        :return: list of byte offsets from existing file\n        \"\"\"\n        try:\n            with open(self.__cache_path, 'rb') as f:\n                return load(f)\n        except FileNotFoundError:\n            return\n        except IsADirectoryError as e:\n            raise IsADirectoryError(f'Please delete {self.__cache_path} directory') from e\n        except (UnpicklingError, EOFError) as e:\n            raise UnpicklingError(f'Invalid cache file {self.__cache_path}. Please delete it') from e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping the cache of the given _shifts into the cache directory.", "response": "def _dump_cache(self, _shifts):\n        \"\"\"\n        _shifts dumps in /tmp directory after reboot it will drop\n        \"\"\"\n        with open(self.__cache_path, 'wb') as f:\n            dump(_shifts, f)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting task types for current user", "response": "def get_task_types(current):\n    \"\"\"\n           List task types for current user\n\n\n           .. code-block:: python\n\n               #  request:\n                   {\n                   'view': '_zops_get_task_types',\n                   }\n\n               #  response:\n                   {\n                   'task_types': [\n                       {'name': string, # wf name\n                        'title': string,  # title of workflow\n                        },]\n                        }\n    \"\"\"\n    current.output['task_types'] = [{'name': bpmn_wf.name,\n                                     'title': bpmn_wf.title}\n                                    for bpmn_wf in BPMNWorkflow.objects.all()\n                                    if current.has_permission(bpmn_wf.name)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_task_detail(current):\n    task_inv = TaskInvitation.objects.get(current.input['key'])\n    obj = task_inv.instance.get_object()\n    current.output['task_title'] = task_inv.instance.task.name\n    current.output['task_detail'] = \"\"\"Explain: %s\n    State: %s\"\"\" % (obj.__unicode__() if obj else '', task_inv.progress)", "response": "Show task details\n\n           .. code-block:: python\n\n               #  request:\n                   {\n                   'view': '_zops_get_task_detail',\n                   'key': key,\n                   }\n\n               #  response:\n                   {\n                   'task_title': string,\n                   'task_detail': string, # markdown formatted text\n                    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of actions for the current user", "response": "def get_task_actions(current):\n    \"\"\"\n           List task types for current user\n\n\n           .. code-block:: python\n\n               #  request:\n                   {\n                   'view': '_zops_get_task_actions',\n                   'key': key,\n                   }\n\n               #  response:\n                   {\n                   'key': key,\n                   'actions': [{\"title\":':'Action Title', \"wf\": \"workflow_name\"},]\n                    }\n    \"\"\"\n    task_inv = TaskInvitation.objects.get(current.input['key'])\n    actions = [{\"title\": __(u\"Assign Someone Else\"), \"wf\": \"assign_same_abstract_role\"},\n               {\"title\": __(u\"Suspend\"), \"wf\": \"suspend_workflow\"},\n               {\"title\": __(u\"Postpone\"), \"wf\": \"postpone_workflow\"}]\n    if task_inv.instance.current_actor != current.role:\n        actions.append({\"title\": __(u\"Assign Yourself\"), \"wf\": \"task_assign_yourself\"})\n\n    current.output['key'] = task_inv.key\n    current.output['actions'] = actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tasks(current):\n    # TODO: Also return invitations for user's other roles\n    # TODO: Handle automatic role switching\n\n    STATE_DICT = {\n        'active': [20, 30],\n        'future': 10,\n        'finished': 40,\n        'expired': 90\n    }\n    state = STATE_DICT[current.input['state']]\n    if isinstance(state, list):\n        queryset = TaskInvitation.objects.filter(progress__in=state)\n    else:\n        queryset = TaskInvitation.objects.filter(progress=state)\n\n    if 'inverted' in current.input:\n        # show other user's tasks\n        allowed_workflows = [bpmn_wf.name for bpmn_wf in BPMNWorkflow.objects.all()\n                             if current.has_permission(bpmn_wf.name)]\n        queryset = queryset.exclude(role_id=current.role_id).filter(wf_name__in=allowed_workflows)\n    else:\n        # show current user's tasks\n        queryset = queryset.filter(role_id=current.role_id)\n\n    if 'query' in current.input:\n        queryset = queryset.filter(search_data__contains=current.input['query'].lower())\n    if 'wf_type' in current.input:\n        queryset = queryset.filter(wf_name=current.input['wf_type'])\n    if 'start_date' in current.input:\n        queryset = queryset.filter(start_date__gte=datetime.strptime(current.input['start_date'], \"%d.%m.%Y\"))\n    if 'finish_date' in current.input:\n        queryset = queryset.filter(finish_date__lte=datetime.strptime(current.input['finish_date'], \"%d.%m.%Y\"))\n    current.output['task_list'] = [\n        {\n            'token': inv.instance.key,\n            'key': inv.key,\n            'title': inv.title,\n            'wf_type': inv.wf_name,\n            'state': inv.progress,\n            'start_date': format_date(inv.start_date),\n            'finish_date': format_date(inv.finish_date),\n            'description': inv.instance.wf.description,\n            'status': inv.ownership}\n        for inv in queryset\n        ]\n    task_inv_list = TaskInvitation.objects.filter(role_id=current.role_id)\n    current.output['task_count']= {\n        'active': task_inv_list.filter(progress__in=STATE_DICT['active']).count(),\n        'future' : task_inv_list.filter(progress=STATE_DICT['future']).count(),\n        'finished' : task_inv_list.filter(progress=STATE_DICT['finished']).count(),\n        'expired' : task_inv_list.filter(progress=STATE_DICT['expired']).count()\n    }", "response": "Return the list of tasks of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reduce_memory_usage(df):\n    usage_pre = df.memory_usage(deep=True).sum()\n    if \"runIDs\" in df:\n        df.loc[:, \"runIDs\"] = df.loc[:, \"runIDs\"].astype(\"category\")\n    df_int = df.select_dtypes(include=['int'])\n    df_float = df.select_dtypes(include=['float'])\n    df.loc[:, df_int.columns] = df_int.apply(pd.to_numeric, downcast='unsigned')\n    df.loc[:, df_float.columns] = df_float.apply(pd.to_numeric, downcast='float')\n    usage_post = df.memory_usage(deep=True).sum()\n    logging.info(\"Reduced DataFrame memory usage from {}Mb to {}Mb\".format(\n        usage_pre / 1024**2, usage_post / 1024**2))\n    if usage_post > 4e9 and \"readIDs\" in df:\n        logging.info(\"DataFrame of features is too big, dropping read identifiers.\")\n        return df.drop([\"readIDs\"], axis=1, errors=\"ignore\")\n    else:\n        return df", "response": "reduce memory usage of the dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_existance(f):\n    if not opath.isfile(f):\n        logging.error(\"Nanoget: File provided doesn't exist or the path is incorrect: {}\".format(f))\n        sys.exit(\"File provided doesn't exist or the path is incorrect: {}\".format(f))", "response": "Check if the file supplied as input exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_user_roles(self):\n        _form = JsonForm(current=self.current, title=_(u\"Switch Role\"))\n        _form.help_text = \"Your current role: %s %s\" % (self.current.role.unit.name,\n                                                        self.current.role.abstract_role.name)\n        switch_roles = self.get_user_switchable_roles()\n        _form.role_options = fields.Integer(_(u\"Please, choose the role you want to switch:\")\n                                            , choices=switch_roles, default=switch_roles[0][0],\n                                            required=True)\n        _form.switch = fields.Button(_(u\"Switch\"))\n        self.form_out(_form)", "response": "Lists user roles as selectable except user s current role."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_user_role(self):\n\n        # Get chosen role_key from user form.\n        role_key = self.input['form']['role_options']\n        # Assign chosen switch role key to user's last_login_role_key field\n        self.current.user.last_login_role_key = role_key\n        self.current.user.save()\n        auth = AuthBackend(self.current)\n        # According to user's new role, user's session set again.\n        auth.set_user(self.current.user)\n        # Dashboard is reloaded according to user's new role.\n        self.current.output['cmd'] = 'reload'", "response": "Changes user s role from current role to chosen role."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of tuples user s role list except current role as a tuple", "response": "def get_user_switchable_roles(self):\n        \"\"\"\n        Returns user's role list except current role as a tuple\n        (role.key, role.name)\n\n        Returns:\n            (list): list of tuples, user's role list except current role\n\n        \"\"\"\n        roles = []\n        for rs in self.current.user.role_set:\n            # rs.role != self.current.role is not True after python version 2.7.12\n            if rs.role.key != self.current.role.key:\n                roles.append((rs.role.key, '%s %s' % (rs.role.unit.name,\n                                                      rs.role.abstract_role.name)))\n        return roles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_object(self, obj):\n        # TODO consider putting into a ES class\n        self.pr_dbg('put_obj: %s' % self.json_dumps(obj))\n        \"\"\"\n        Wrapper for es.index, determines metadata needed to index from obj.\n        If you have a raw object json string you can hard code these:\n        index is .kibana (as of kibana4);\n        id can be A-Za-z0-9\\- and must be unique;\n        doc_type is either visualization, dashboard, search\n            or for settings docs: config, or index-pattern.\n        \"\"\"\n        if obj['_index'] is None or obj['_index'] == \"\":\n            raise Exception(\"Invalid Object, no index\")\n        if obj['_id'] is None or obj['_id'] == \"\":\n            raise Exception(\"Invalid Object, no _id\")\n        if obj['_type'] is None or obj['_type'] == \"\":\n            raise Exception(\"Invalid Object, no _type\")\n        if obj['_source'] is None or obj['_source'] == \"\":\n            raise Exception(\"Invalid Object, no _source\")\n        self.connect_es()\n        self.es.indices.create(index=obj['_index'], ignore=400, timeout=\"2m\")\n        try:\n            resp = self.es.index(index=obj['_index'],\n                                 id=obj['_id'],\n                                 doc_type=obj['_type'],\n                                 body=obj['_source'], timeout=\"2m\")\n        except RequestError as e:\n            self.pr_err('RequestError: %s, info: %s' % (e.error, e.info))\n            raise\n        return resp", "response": "Put an object into the index and returns the index entry"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef del_object(self, obj):\n        if obj['_index'] is None or obj['_index'] == \"\":\n            raise Exception(\"Invalid Object\")\n        if obj['_id'] is None or obj['_id'] == \"\":\n            raise Exception(\"Invalid Object\")\n        if obj['_type'] is None or obj['_type'] == \"\":\n            raise Exception(\"Invalid Object\")\n        self.connect_es()\n        self.es.delete(index=obj['_index'],\n                       id=obj['_id'],\n                       doc_type=obj['_type'])", "response": "Debug deletes an object of type obj [ _type with id of obj [ _id ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert obj to json string and write to file", "response": "def write_object_to_file(self, obj, path='.', filename=None):\n        \"\"\"Convert obj (dict) to json string and write to file\"\"\"\n        output = self.json_dumps(obj) + '\\n'\n        if filename is None:\n            filename = self.safe_filename(obj['_type'], obj['_id'])\n        filename = os.path.join(path, filename)\n        self.pr_inf(\"Writing to file: \" + filename)\n        with open(filename, 'w') as f:\n            f.write(output)\n        # self.pr_dbg(\"Contents: \" + output)\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a list of related objs to file", "response": "def write_pkg_to_file(self, name, objects, path='.', filename=None):\n        \"\"\"Write a list of related objs to file\"\"\"\n        # Kibana uses an array of docs, do the same\n        # as opposed to a dict of docs\n        pkg_objs = []\n        for _, obj in iteritems(objects):\n            pkg_objs.append(obj)\n        sorted_pkg = sorted(pkg_objs, key=lambda k: k['_id'])\n        output = self.json_dumps(sorted_pkg) + '\\n'\n        if filename is None:\n            filename = self.safe_filename('Pkg', name)\n        filename = os.path.join(path, filename)\n        self.pr_inf(\"Writing to file: \" + filename)\n        with open(filename, 'w') as f:\n            f.write(output)\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_objects(self, search_field, search_val):\n        query = (\"{ size: \" + str(self.max_hits) + \", \" +\n                 \"query: { filtered: { filter: { \" +\n                 search_field + \": { value: \\\"\" + search_val + \"\\\"\" +\n                 \" } } } } } }\")\n        self.connect_es()\n        res = self.es.search(index=self.index, body=query)\n        # self.pr_dbg(\"%d Hits:\" % res['hits']['total'])\n        objects = {}\n        for doc in res['hits']['hits']:\n            objects[doc['_id']] = {}\n            # To make uploading easier in the future:\n            # Record all those bits into the backup.\n            # Mimics how ES returns the result.\n            # Prevents having to store this in some external, contrived, format\n            objects[doc['_id']]['_index'] = self.index  # also in doc['_index']\n            objects[doc['_id']]['_type'] = doc['_type']\n            objects[doc['_id']]['_id'] = doc['_id']\n            objects[doc['_id']]['_source'] = doc['_source']  # the actual result\n        return objects", "response": "Return all objects of type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting DB and all objs needed to duplicate it", "response": "def get_dashboard_full(self, db_name):\n        \"\"\"Get DB and all objs needed to duplicate it\"\"\"\n        objects = {}\n        dashboards = self.get_objects(\"type\", \"dashboard\")\n        vizs = self.get_objects(\"type\", \"visualization\")\n        searches = self.get_objects(\"type\", \"search\")\n        if db_name not in dashboards:\n            return None\n        self.pr_inf(\"Found dashboard: \" + db_name)\n        objects[db_name] = dashboards[db_name]\n        panels = json.loads(dashboards[db_name]['_source']['panelsJSON'])\n        for panel in panels:\n            if 'id' not in panel:\n                continue\n            pid = panel['id']\n            if pid in searches:\n                self.pr_inf(\"Found search:    \" + pid)\n                objects[pid] = searches[pid]\n            elif pid in vizs:\n                self.pr_inf(\"Found vis:       \" + pid)\n                objects[pid] = vizs[pid]\n                emb = vizs[pid].get('_source', {}).get('savedSearchId', None)\n                if emb is not None and emb not in objects:\n                    if emb not in searches:\n                        self.pr_err('Missing search %s' % emb)\n                        return objects\n                    objects[emb] = searches[emb]\n        return objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_node(self, node):\n        spec = super(CamundaProcessParser, self).parse_node(node)\n        spec.data = self._parse_input_data(node)\n        spec.data['lane_data'] = self._get_lane_properties(node)\n        spec.defines = spec.data\n        service_class = node.get(full_attr('assignee'))\n        if service_class:\n            self.parsed_nodes[node.get('id')].service_class = node.get(full_attr('assignee'))\n        return spec", "response": "Overrides ProcessParser. parse_node\n        Parses and attaches the inputOutput tags that created by Camunda Modeller\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_description(self):\n        ns = {'ns': '{%s}' % BPMN_MODEL_NS}\n        desc = (\n            self.doc_xpath('.//{ns}collaboration/{ns}documentation'.format(**ns)) or\n            self.doc_xpath('.//{ns}process/{ns}documentation'.format(**ns)) or\n            self.doc_xpath('.//{ns}collaboration/{ns}participant/{ns}documentation'.format(**ns))\n        )\n        if desc:\n            return desc[0].findtext('.')", "response": "Tries to get WF description from collabration process or pariticipant"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_name(self):\n        ns = {'ns': '{%s}' % BPMN_MODEL_NS}\n        for path in ('.//{ns}process',\n                     './/{ns}collaboration',\n                     './/{ns}collaboration/{ns}participant/'):\n            tag = self.doc_xpath(path.format(**ns))\n            if tag:\n                name = tag[0].get('name')\n                if name:\n                    return name\n        return self.get_id()", "response": "Tries to get WF name from process collobration or pariticipant"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse inputOutput part camunda modeller extensions.", "response": "def _parse_input_data(self, node):\n        \"\"\"\n        Parses inputOutput part camunda modeller extensions.\n        Args:\n            node: SpiffWorkflow Node object.\n\n        Returns:\n            Data dict.\n        \"\"\"\n        data = DotDict()\n        try:\n            for nod in self._get_input_nodes(node):\n                data.update(self._parse_input_node(nod))\n        except Exception as e:\n            log.exception(\"Error while processing node: %s\" % node)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the given XML node and returns a dictionary of properties for the given lane.", "response": "def _get_lane_properties(self, node):\n        \"\"\"\n        Parses the given XML node\n\n        Args:\n            node (xml): XML node.\n\n        .. code-block:: xml\n\n             <bpmn2:lane id=\"Lane_8\" name=\"Lane 8\">\n                <bpmn2:extensionElements>\n                    <camunda:properties>\n                        <camunda:property value=\"foo,bar\" name=\"perms\"/>\n                    </camunda:properties>\n                </bpmn2:extensionElements>\n            </bpmn2:lane>\n\n        Returns:\n            {'perms': 'foo,bar'}\n        \"\"\"\n        lane_name = self.get_lane(node.get('id'))\n        lane_data = {'name': lane_name}\n        for a in self.xpath(\".//bpmn:lane[@name='%s']/*/*/\" % lane_name):\n            lane_data[a.attrib['name']] = a.attrib['value'].strip()\n        return lane_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the XML node to get the key - value pair.", "response": "def _parse_input_node(cls, node):\n        \"\"\"\n        :param node: xml node\n        :return: dict\n        \"\"\"\n        data = {}\n        child = node.getchildren()\n        if not child and node.get('name'):\n            val = node.text\n        elif child:  # if tag = \"{http://activiti.org/bpmn}script\" then data_typ = 'script'\n            data_typ = child[0].tag.split('}')[1]\n            val = getattr(cls, '_parse_%s' % data_typ)(child[0])\n        data[node.get('name')] = val\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate wf packages from workflow diagrams.", "response": "def package_in_memory(cls, workflow_name, workflow_files):\n        \"\"\"\n        Generates wf packages from workflow diagrams.\n\n        Args:\n            workflow_name: Name of wf\n            workflow_files:  Diagram  file.\n\n        Returns:\n            Workflow package (file like) object\n        \"\"\"\n        s = StringIO()\n        p = cls(s, workflow_name, meta_data=[])\n        p.add_bpmn_files_by_glob(workflow_files)\n        p.create_package()\n        return s.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompose a reaction container from another reaction container", "response": "def compose(self, data):\n        \"\"\"\n        condense reaction container to CGR. see init for details about cgr_type\n\n        :param data: ReactionContainer\n        :return: CGRContainer\n        \"\"\"\n        g = self.__separate(data) if self.__cgr_type in (1, 2, 3, 4, 5, 6) else self.__condense(data)\n        g.meta.update(data.meta)\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds atom to the internal dictionary", "response": "def add_atom(self, atom, _map=None):\n        \"\"\"\n        new atom addition\n        \"\"\"\n        if _map is None:\n            _map = max(self, default=0) + 1\n        elif _map in self._node:\n            raise KeyError('atom with same number exists')\n\n        attr_dict = self.node_attr_dict_factory()\n        if isinstance(atom, str):\n            attr_dict.element = atom\n        elif isinstance(atom, int):\n            attr_dict.element = elements_list[atom - 1]\n        else:\n            attr_dict.update(atom)\n\n        self._adj[_map] = self.adjlist_inner_dict_factory()\n        self._node[_map] = attr_dict\n        self.flush_cache()\n        return _map"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tuple of pairs of ( bond atom ) connected to atom", "response": "def environment(self, atom):\n        \"\"\"\n        pairs of (bond, atom) connected to atom\n\n        :param atom: number\n        :return: list\n        \"\"\"\n        return tuple((bond, self._node[n]) for n, bond in self._adj[atom].items())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a substructure containing atoms from nbunch list", "response": "def substructure(self, atoms, meta=False, as_view=True):\n        \"\"\"\n        create substructure containing atoms from nbunch list\n\n        :param atoms: list of atoms numbers of substructure\n        :param meta: if True metadata will be copied to substructure\n        :param as_view: If True, the returned graph-view provides a read-only view\n            of the original structure scaffold without actually copying any data.\n        \"\"\"\n        s = self.subgraph(atoms)\n        if as_view:\n            s.add_atom = s.add_bond = s.delete_atom = s.delete_bond = frozen  # more informative exception\n            return s\n        s = s.copy()\n        if not meta:\n            s.graph.clear()\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a substructure of the graph with the given atoms and their neighbors", "response": "def augmented_substructure(self, atoms, dante=False, deep=1, meta=False, as_view=True):\n        \"\"\"\n        create substructure containing atoms and their neighbors\n\n        :param atoms: list of core atoms in graph\n        :param dante: if True return list of graphs containing atoms, atoms + first circle, atoms + 1st + 2nd,\n            etc up to deep or while new nodes available\n        :param deep: number of bonds between atoms and neighbors\n        :param meta: copy metadata to each substructure\n        :param as_view: If True, the returned graph-view provides a read-only view\n            of the original graph without actually copying any data\n        \"\"\"\n        nodes = [set(atoms)]\n        for i in range(deep):\n            n = {y for x in nodes[-1] for y in self._adj[x]} | nodes[-1]\n            if n in nodes:\n                break\n            nodes.append(n)\n        if dante:\n            return [self.substructure(a, meta, as_view) for a in nodes]\n        else:\n            return self.substructure(nodes[-1], meta, as_view)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits disconnected structure to connected substructures", "response": "def split(self, meta=False):\n        \"\"\"\n        split disconnected structure to connected substructures\n\n        :param meta: copy metadata to each substructure\n        :return: list of substructures\n        \"\"\"\n        return [self.substructure(c, meta, False) for c in connected_components(self)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bonds(self):\n        seen = set()\n        for n, m_bond in self._adj.items():\n            seen.add(n)\n            for m, bond in m_bond.items():\n                if m not in seen:\n                    yield n, m, bond", "response": "iterate other all bonds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_subclass(name):\n        return next(x for x in BaseContainer.__subclasses__() if x.__name__ == name)", "response": "get the subclass of the given name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a urllib3. PoolManager object that has SSL verification enabled and uses the certifi certificates.", "response": "def _default_make_pool(http, proxy_info):\n    \"\"\"Creates a urllib3.PoolManager object that has SSL verification enabled\n    and uses the certifi certificates.\"\"\"\n\n    if not http.ca_certs:\n        http.ca_certs = _certifi_where_for_ssl_version()\n\n    ssl_disabled = http.disable_ssl_certificate_validation\n\n    cert_reqs = 'CERT_REQUIRED' if http.ca_certs and not ssl_disabled else None\n\n    if isinstance(proxy_info, collections.Callable):\n        proxy_info = proxy_info()\n    if proxy_info:\n        if proxy_info.proxy_user and proxy_info.proxy_pass:\n            proxy_url = 'http://{}:{}@{}:{}/'.format(\n                proxy_info.proxy_user, proxy_info.proxy_pass,\n                proxy_info.proxy_host, proxy_info.proxy_port,\n            )\n            proxy_headers = urllib3.util.request.make_headers(\n                proxy_basic_auth='{}:{}'.format(\n                    proxy_info.proxy_user, proxy_info.proxy_pass,\n                )\n            )\n        else:\n            proxy_url = 'http://{}:{}/'.format(\n                proxy_info.proxy_host, proxy_info.proxy_port,\n            )\n            proxy_headers = {}\n\n        return urllib3.ProxyManager(\n            proxy_url=proxy_url,\n            proxy_headers=proxy_headers,\n            ca_certs=http.ca_certs,\n            cert_reqs=cert_reqs,\n        )\n    return urllib3.PoolManager(\n        ca_certs=http.ca_certs,\n        cert_reqs=cert_reqs,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch(make_pool=_default_make_pool):\n    setattr(httplib2, '_HttpOriginal', httplib2.Http)\n    httplib2.Http = Http\n    Http._make_pool = make_pool", "response": "Monkey - patches httplib2. Http to be httplib2shim. Http."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a given address is an IPv6 address.", "response": "def _is_ipv6(addr):\n    \"\"\"Checks if a given address is an IPv6 address.\"\"\"\n    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n        return True\n    except socket.error:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the right location for certifi certifications for the current SSLVersion.", "response": "def _certifi_where_for_ssl_version():\n    \"\"\"Gets the right location for certifi certifications for the current SSL\n    version.\n\n    Older versions of SSL don't support the stronger set of root certificates.\n    \"\"\"\n    if not ssl:\n        return\n\n    if ssl.OPENSSL_VERSION_INFO < (1, 0, 2):\n        warnings.warn(\n            'You are using an outdated version of OpenSSL that '\n            'can\\'t use stronger root certificates.')\n        return certifi.old_where()\n\n    return certifi.where()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap a urllib3 response to a httplib2 Response.", "response": "def _map_response(response, decode=False):\n    \"\"\"Maps a urllib3 response to a httplib/httplib2 Response.\"\"\"\n    # This causes weird deepcopy errors, so it's commented out for now.\n    # item._urllib3_response = response\n    item = httplib2.Response(response.getheaders())\n    item.status = response.status\n    item['status'] = str(item.status)\n    item.reason = response.reason\n    item.version = response.version\n\n    # httplib2 expects the content-encoding header to be stripped and the\n    # content length to be the length of the uncompressed content.\n    # This does not occur for 'HEAD' requests.\n    if decode and item.get('content-encoding') in ['gzip', 'deflate']:\n        item['content-length'] = str(len(response.data))\n        item['-content-encoding'] = item.pop('content-encoding')\n\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap an exception from urlib3 to httplib2.", "response": "def _map_exception(e):\n    \"\"\"Maps an exception from urlib3 to httplib2.\"\"\"\n    if isinstance(e, urllib3.exceptions.MaxRetryError):\n        if not e.reason:\n            return e\n        e = e.reason\n    message = e.args[0] if e.args else ''\n    if isinstance(e, urllib3.exceptions.ResponseError):\n        if 'too many redirects' in message:\n            return httplib2.RedirectLimit(message)\n    if isinstance(e, urllib3.exceptions.NewConnectionError):\n        if ('Name or service not known' in message or\n                'nodename nor servname provided, or not known' in message):\n            return httplib2.ServerNotFoundError(\n                'Unable to find hostname.')\n        if 'Connection refused' in message:\n            return socket.error((errno.ECONNREFUSED, 'Connection refused'))\n    if isinstance(e, urllib3.exceptions.DecodeError):\n        return httplib2.FailedToDecompressContent(\n            'Content purported as compressed but not uncompressable.',\n            httplib2.Response({'status': 500}), '')\n    if isinstance(e, urllib3.exceptions.TimeoutError):\n        return socket.timeout('timed out')\n    if isinstance(e, urllib3.exceptions.SSLError):\n        return ssl.SSLError(*e.args)\n\n    return e"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts workers with no_subprocess and watch_paths", "response": "def run_workers(no_subprocess, watch_paths=None, is_background=False):\n    \"\"\"\n    subprocess handler\n    \"\"\"\n    import atexit, os, subprocess, signal\n    if watch_paths:\n        from watchdog.observers import Observer\n        # from watchdog.observers.fsevents import FSEventsObserver as Observer\n        # from watchdog.observers.polling import PollingObserver as Observer\n        from watchdog.events import FileSystemEventHandler\n\n    def on_modified(event):\n        if not is_background:\n            print(\"Restarting worker due to change in %s\" % event.src_path)\n        log.info(\"modified %s\" % event.src_path)\n        try:\n            kill_children()\n            run_children()\n        except:\n            log.exception(\"Error while restarting worker\")\n\n    handler = FileSystemEventHandler()\n    handler.on_modified = on_modified\n\n    # global child_pids\n    child_pids = []\n    log.info(\"starting %s workers\" % no_subprocess)\n\n    def run_children():\n        global child_pids\n        child_pids = []\n        for i in range(int(no_subprocess)):\n            proc = subprocess.Popen([sys.executable, __file__],\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            child_pids.append(proc.pid)\n            log.info(\"Started worker with pid %s\" % proc.pid)\n\n    def kill_children():\n        \"\"\"\n        kill subprocess on exit of manager (this) process\n        \"\"\"\n        log.info(\"Stopping worker(s)\")\n        for pid in child_pids:\n            if pid is not None:\n                os.kill(pid, signal.SIGTERM)\n\n    run_children()\n    atexit.register(kill_children)\n    signal.signal(signal.SIGTERM, kill_children)\n    if watch_paths:\n        observer = Observer()\n        for path in watch_paths:\n            if not is_background:\n                print(\"Watching for changes under %s\" % path)\n            observer.schedule(handler, path=path, recursive=True)\n        observer.start()\n    while 1:\n        try:\n            sleep(1)\n        except KeyboardInterrupt:\n            log.info(\"Keyboard interrupt, exiting\")\n            if watch_paths:\n                observer.stop()\n                observer.join()\n            sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exit(self, signal=None, frame=None):\n        self.input_channel.close()\n        self.client_queue.close()\n        self.connection.close()\n        log.info(\"Worker exiting\")\n        sys.exit(0)", "response": "Properly close the AMQP connections"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking amqp connection and create channels and queue binding", "response": "def connect(self):\n        \"\"\"\n        make amqp connection and create channels and queue binding\n        \"\"\"\n        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)\n        self.client_queue = ClientQueue()\n        self.input_channel = self.connection.channel()\n\n        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,\n                                            type='topic',\n                                            durable=True)\n        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)\n        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)\n        log.info(\"Bind to queue named '%s' queue with exchange '%s'\" % (self.INPUT_QUEUE_NAME,\n                                                                        self.INPUT_EXCHANGE))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing outs all messages from INPUT_QUEUE_NAME", "response": "def clear_queue(self):\n        \"\"\"\n        clear outs all messages from INPUT_QUEUE_NAME\n        \"\"\"\n        def remove_message(ch, method, properties, body):\n            print(\"Removed message: %s\" % body)\n        self.input_channel.basic_consume(remove_message, queue=self.INPUT_QUEUE_NAME, no_ack=True)\n        try:\n            self.input_channel.start_consuming()\n        except (KeyboardInterrupt, SystemExit):\n            log.info(\" Exiting\")\n            self.exit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        self.input_channel.basic_consume(self.handle_message,\n                                         queue=self.INPUT_QUEUE_NAME,\n                                         no_ack=True\n                                         )\n        try:\n            self.input_channel.start_consuming()\n        except (KeyboardInterrupt, SystemExit):\n            log.info(\" Exiting\")\n            self.exit()", "response": "This method is run when the main thread is started."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_progress(start, finish):\n    now = datetime.now()\n    dif_time_start = start - now\n    dif_time_finish = finish - now\n\n    if dif_time_start.days < 0 and dif_time_finish.days < 0:\n        return PROGRESS_STATES[3][0]\n    elif dif_time_start.days < 0 and dif_time_finish.days >= 1:\n        return PROGRESS_STATES[2][0]\n    elif dif_time_start.days >= 1 and dif_time_finish.days >= 1:\n        return PROGRESS_STATES[0][0]\n    else:\n        return PROGRESS_STATES[2][0]", "response": "Returns the state of the current time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync_wf_cache(current):\n    wf_cache = WFCache(current)\n    wf_state = wf_cache.get()  # unicode serialized json to dict, all values are unicode\n    if 'role_id' in wf_state:\n        # role_id inserted by engine, so it's a sign that we get it from cache not db\n        try:\n            wfi = WFInstance.objects.get(key=current.input['token'])\n        except ObjectDoesNotExist:\n            # wf's that not started from a task invitation\n            wfi = WFInstance(key=current.input['token'])\n            wfi.wf = BPMNWorkflow.objects.get(name=wf_state['name'])\n        if not wfi.current_actor.exist:\n            # we just started the wf\n            try:\n                inv = TaskInvitation.objects.get(instance=wfi, role_id=wf_state['role_id'])\n                inv.delete_other_invitations()\n                inv.progress = 20\n                inv.save()\n            except ObjectDoesNotExist:\n                current.log.exception(\"Invitation not found: %s\" % wf_state)\n            except MultipleObjectsReturned:\n                current.log.exception(\"Multiple invitations found: %s\" % wf_state)\n        wfi.step = wf_state['step']\n        wfi.name = wf_state['name']\n        wfi.pool = wf_state['pool']\n        wfi.current_actor_id = str(wf_state['role_id'])  # keys must be str not unicode\n        wfi.data = wf_state['data']\n        if wf_state['finished']:\n            wfi.finished = True\n            wfi.finish_date = wf_state['finish_date']\n            wf_cache.delete()\n        wfi.save()\n\n    else:\n        # if cache already cleared, we have nothing to sync\n        pass", "response": "Synchronizes the state of a specific wf with the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a diagram XML instance and whether or not it s already exists.", "response": "def get_or_create_by_content(cls, name, content):\n        \"\"\"\n        if xml content updated, create a new entry for given wf name\n        Args:\n            name: name of wf\n            content: xml content\n\n        Returns (DiagramXML(), bool): A tuple with two members.\n        (DiagramXML instance and True if it's new or False it's already exists\n        \"\"\"\n        new = False\n        diagrams = cls.objects.filter(name=name)\n        if diagrams:\n            diagram = diagrams[0]\n            if diagram.body != content:\n                new = True\n        else:\n            new = True\n        if new:\n            diagram = cls(name=name, body=content).save()\n        return diagram, new"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to get WF description from collabration process or pariticipant", "response": "def get_description(self):\n        \"\"\"\n        Tries to get WF description from 'collabration' or 'process' or 'pariticipant'\n\n        Returns str: WF description\n\n        \"\"\"\n        paths = ['bpmn:collaboration/bpmn:participant/bpmn:documentation',\n                 'bpmn:collaboration/bpmn:documentation',\n                 'bpmn:process/bpmn:documentation']\n        for path in paths:\n            elm = self.root.find(path, NS)\n            if elm is not None and elm.text:\n                return elm.text"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to get WF name from process collobration or pariticipant", "response": "def get_name(self):\n        \"\"\"\n        Tries to get WF name from 'process' or 'collobration' or 'pariticipant'\n\n        Returns:\n            str. WF name.\n        \"\"\"\n        paths = ['bpmn:process',\n                 'bpmn:collaboration/bpmn:participant/',\n                 'bpmn:collaboration',\n                 ]\n        for path in paths:\n            tag = self.root.find(path, NS)\n            if tag is not None and len(tag):\n                name = tag.get('name')\n                if name:\n                    return name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_xml(self, diagram, force=False):\n        no_of_running = WFInstance.objects.filter(wf=self, finished=False, started=True).count()\n        if no_of_running and not force:\n            raise RunningInstancesExist(\n                \"Can't update WF diagram! Running %s WF instances exists for %s\" % (\n                    no_of_running, self.name\n                ))\n        else:\n            self.xml = diagram\n            parser = BPMNParser(diagram.body)\n            self.description = parser.get_description()\n            self.title = parser.get_name() or self.name.replace('_', ' ').title()\n            extensions = dict(parser.get_wf_extensions())\n            self.programmable = extensions.get('programmable', False)\n            self.task_type = extensions.get('task_type', None)\n            self.menu_category = extensions.get('menu_category', settings.DEFAULT_WF_CATEGORY_NAME)\n            self.save()", "response": "Updates the xml link of the current object with the given diagram."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating wf instances. Args: roles (list): role list Returns: (list): wf instances", "response": "def create_wf_instances(self, roles=None):\n        \"\"\"\n        Creates wf instances.\n        Args:\n            roles (list): role list\n\n        Returns:\n            (list): wf instances\n        \"\"\"\n\n        # if roles specified then create an instance for each role\n        # else create only one instance\n\n        if roles:\n            wf_instances = [\n                WFInstance(\n                    wf=self.wf,\n                    current_actor=role,\n                    task=self,\n                    name=self.wf.name\n                ) for role in roles\n                ]\n        else:\n            wf_instances = [\n                WFInstance(\n                    wf=self.wf,\n                    task=self,\n                    name=self.wf.name\n                )\n            ]\n\n        # if task type is not related with objects save instances immediately.\n        if self.task_type in [\"C\", \"D\"]:\n            return [wfi.save() for wfi in wf_instances]\n\n        # if task type is related with its objects, save populate instances per object\n        else:\n            wf_obj_instances = []\n            for wfi in wf_instances:\n                role = wfi.current_actor if self.task_type == \"A\" else None\n                keys = self.get_object_keys(role)\n                wf_obj_instances.extend(\n                    [WFInstance(\n                        wf=self.wf,\n                        current_actor=role,\n                        task=self,\n                        name=self.wf.name,\n                        wf_object=key,\n                        wf_object_type=self.object_type\n                    ).save() for key in keys]\n                )\n\n            return wf_obj_instances"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a list of WFInstance per object and TaskInvitation per role.", "response": "def create_tasks(self):\n        \"\"\"\n        will create a WFInstance per object\n        and per TaskInvitation for each role and WFInstance\n        \"\"\"\n        roles = self.get_roles()\n\n        if self.task_type in [\"A\", \"D\"]:\n            instances = self.create_wf_instances(roles=roles)\n            self.create_task_invitation(instances)\n\n        elif self.task_type in [\"C\", \"B\"]:\n            instances = self.create_wf_instances()\n            self.create_task_invitation(instances, roles)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn objects keys according to self. object_query_code alid", "response": "def get_object_query_dict(self):\n        \"\"\"returns objects keys according to self.object_query_code\n        which can be json encoded queryset filter dict or key=value set\n         in the following format: ```\"key=val, key2 = val2 , key3= value with spaces\"```\n\n        Returns:\n             (dict): Queryset filtering dicqt\n        \"\"\"\n        if isinstance(self.object_query_code, dict):\n            # _DATE_ _DATETIME_\n            return self.object_query_code\n        else:\n            # comma separated, key=value pairs. wrapping spaces will be ignored\n            # eg: \"key=val, key2 = val2 , key3= value with spaces\"\n            return dict(pair.split('=') for pair in self.object_query_code.split(','))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns object keys according to task definition which can be explicitly selected one object", "response": "def get_object_keys(self, wfi_role=None):\n        \"\"\"returns object keys according to task definition\n        which can be explicitly selected one object (self.object_key) or\n        result of a queryset filter.\n\n        Returns:\n            list of object keys\n        \"\"\"\n        if self.object_key:\n            return [self.object_key]\n        if self.object_query_code:\n            model = model_registry.get_model(self.object_type)\n            return [m.key for m in\n                    self.get_model_objects(model, wfi_role, **self.get_object_query_dict())]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_model_objects(model, wfi_role=None, **kwargs):\n        query_dict = {}\n        for k, v in kwargs.items():\n            if isinstance(v, list):\n                query_dict[k] = [str(x) for x in v]\n            else:\n                parse = str(v).split('.')\n                if parse[0] == 'role' and wfi_role:\n                    query_dict[k] = wfi_role\n                    for i in range(1, len(parse)):\n                        query_dict[k] = query_dict[k].__getattribute__(parse[i])\n                else:\n                    query_dict[k] = parse[0]\n\n        return model.objects.all(**query_dict)", "response": "Fetches all model objects by filtering with kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all the roles that are defined in the current task definition.", "response": "def get_roles(self):\n        \"\"\"\n        Returns:\n            Role instances according to task definition.\n        \"\"\"\n        if self.role.exist:\n            # return explicitly selected role\n            return [self.role]\n        else:\n            roles = []\n            if self.role_query_code:\n                #  use given \"role_query_code\"\n                roles = RoleModel.objects.filter(**self.role_query_code)\n            elif self.unit.exist:\n                # get roles from selected unit or sub-units of it\n                if self.recursive_units:\n                    # this returns a list, we're converting it to a Role generator!\n                    roles = (RoleModel.objects.get(k) for k in\n                             UnitModel.get_role_keys(self.unit.key))\n                else:\n                    roles = RoleModel.objects.filter(unit=self.unit)\n            elif self.get_roles_from:\n                # get roles from selected predefined \"get_roles_from\" method\n                return ROLE_GETTER_METHODS[self.get_roles_from](RoleModel)\n\n        if self.abstract_role.exist and roles:\n            # apply abstract_role filtering on roles we got\n            if isinstance(roles, (list, types.GeneratorType)):\n                roles = [a for a in roles if a.abstract_role.key == self.abstract_role.key]\n            else:\n                roles = roles.filter(abstract_role=self.abstract_role)\n        else:\n            roles = RoleModel.objects.filter(abstract_role=self.abstract_role)\n\n        return roles"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_other_invitations(self):\n        # TODO: Signal logged-in users to remove the task from their task list\n        self.objects.filter(instance=self.instance).exclude(key=self.key).delete()", "response": "When one person use an invitation we should delete other invitations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the wf state to the cache", "response": "def save(self, wf_state):\n        \"\"\"\n        write wf state to DB through MQ >> Worker >> _zops_sync_wf_cache\n\n        Args:\n            wf_state dict: wf state\n        \"\"\"\n        self.wf_state = wf_state\n        self.wf_state['role_id'] = self.current.role_id\n        self.set(self.wf_state)\n        if self.wf_state['name'] not in settings.EPHEMERAL_WORKFLOWS:\n            self.publish(job='_zops_sync_wf_cache',\n                         token=self.db_key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_to_default_exchange(self, sess_id, message=None):\n        msg = json.dumps(message, cls=ZEngineJSONEncoder)\n        log.debug(\"Sending following message to %s queue through default exchange:\\n%s\" % (\n            sess_id, msg))\n        self.get_channel().publish(exchange='', routing_key=sess_id, body=msg)", "response": "Send a message through RabbitMQ s default exchange."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend messages through logged in users private exchange.", "response": "def send_to_prv_exchange(self, user_id, message=None):\n        \"\"\"\n        Send messages through logged in users private exchange.\n\n        Args:\n            user_id string: User key\n            message dict: Message object\n\n        \"\"\"\n        exchange = 'prv_%s' % user_id.lower()\n        msg = json.dumps(message, cls=ZEngineJSONEncoder)\n        log.debug(\"Sending following users \\\"%s\\\" exchange:\\n%s \" % (exchange, msg))\n        self.get_channel().publish(exchange=exchange, routing_key='', body=msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompose two graphs to a CGRContainer", "response": "def compose(self, other):\n        \"\"\"\n        compose 2 graphs to CGR\n\n        :param other: Molecule or CGR Container\n        :return: CGRContainer\n        \"\"\"\n        if not isinstance(other, Compose):\n            raise TypeError('CGRContainer or MoleculeContainer [sub]class expected')\n\n        cgr = self._get_subclass('CGRContainer')\n        common = self._node.keys() & other\n\n        if not common:\n            if not (isinstance(self, cgr) or isinstance(other, cgr)):\n                return cgr() | self | other\n            return self | other\n\n        unique_reactant = self._node.keys() - common\n        unique_product = other._node.keys() - common\n\n        h = cgr()\n        atoms = h._node\n        bonds = []\n        common_adj = {n: {} for n in common}\n        common_bonds = []\n\n        r_atoms = {}\n        r_skin = defaultdict(list)\n        if isinstance(self, cgr):\n            for n in unique_reactant:\n                h.add_atom(self._node[n], n)\n                for m, bond in self._adj[n].items():\n                    if m not in atoms:\n                        if m in common:  # bond to common atoms is broken bond\n                            r_bond = bond._reactant\n                            if r_bond is None:  # skip None>None\n                                continue\n                            r_skin[n].append(m)\n                            bond = DynBond.__new__(DynBond)\n                            bond.__init_copy__(r_bond, None)\n                        bonds.append((n, m, bond))\n            for n in common:\n                r_atoms[n] = self._node[n]._reactant\n                for m, bond in self._adj[n].items():\n                    if m not in r_atoms and m in common:\n                        tmp = [bond._reactant, None]\n                        common_adj[n][m] = common_adj[m][n] = tmp\n                        common_bonds.append((n, m, tmp))\n        else:\n            for n in unique_reactant:\n                atom = DynAtom.__new__(DynAtom)  # add unique atom into CGR\n                atom.__init_copy__(self._node[n], self._node[n])\n                h.add_atom(atom, n)\n                for m, r_bond in self._adj[n].items():  # unique atom neighbors\n                    if m not in atoms:  # bond not analyzed yet\n                        bond = DynBond.__new__(DynBond)\n                        if m in common:  # bond to common atoms\n                            r_skin[n].append(m)\n                            bond.__init_copy__(r_bond, None)\n                        else:  # bond static\n                            bond.__init_copy__(r_bond, r_bond)\n                        bonds.append((n, m, bond))\n            for n in common:\n                r_atoms[n] = self._node[n]\n                for m, bond in self._adj[n].items():\n                    if m not in r_atoms and m in common:  # analyze only common atoms bonds\n                        tmp = [bond, None]  # reactant state only\n                        common_adj[n][m] = common_adj[m][n] = tmp\n                        common_bonds.append((n, m, tmp))\n\n        p_atoms = {}\n        p_skin = defaultdict(list)\n        if isinstance(other, cgr):\n            for n in unique_product:\n                h.add_atom(other._node[n], n)\n                for m, bond in other._adj[n].items():\n                    if m not in atoms:\n                        if m in common:  # bond to common atoms is new bond\n                            p_bond = bond._product\n                            if p_bond is None:  # skip None>None\n                                continue\n                            p_skin[n].append(m)\n                            bond = DynBond.__new__(DynBond)\n                            bond.__init_copy__(None, p_bond)\n                        bonds.append((n, m, bond))\n            for n in common:\n                p_atoms[n] = other._node[n]._product\n                n_bonds = common_adj[n]\n                for m, bond in other._adj[n].items():\n                    if m in n_bonds:\n                        n_bonds[m][1] = bond._product\n                    elif m not in p_atoms and m in common:  # new bond of reaction\n                        p_bond = bond._product\n                        if p_bond is None:  # skip None>None\n                            continue\n                        bond = DynBond.__new__(DynBond)\n                        bond.__init_copy__(None, p_bond)\n                        bonds.append((n, m, bond))\n        else:\n            for n in unique_product:\n                atom = DynAtom.__new__(DynAtom)\n                atom.__init_copy__(other._node[n], other._node[n])\n                h.add_atom(atom, n)\n                for m, p_bond in other._adj[n].items():\n                    if m not in atoms:\n                        bond = DynBond.__new__(DynBond)\n                        if m in common:\n                            p_skin[n].append(m)\n                            bond.__init_copy__(None, p_bond)\n                        else:\n                            bond.__init_copy__(p_bond, p_bond)\n                        bonds.append((n, m, bond))\n            for n in common:\n                p_atoms[n] = other._node[n]\n                n_bonds = common_adj[n]\n                for m, p_bond in other._adj[n].items():\n                    if m in n_bonds:  # set product state of changed bond\n                        n_bonds[m][1] = p_bond\n                    elif m not in p_atoms and m in common:  # new bond of reaction\n                        bond = DynBond.__new__(DynBond)\n                        bond.__init_copy__(None, p_bond)\n                        bonds.append((n, m, bond))\n\n        for n, r_atom in r_atoms.items():  # prepare common DynAtom's\n            p_atom = p_atoms[n]\n            if r_atom.element != p_atom.element or r_atom.isotope != p_atom.isotope:\n                raise ValueError('atom-to-atom mapping invalid')\n            atom = DynAtom.__new__(DynAtom)\n            atom.__init_copy__(r_atom, p_atom)\n            h.add_atom(atom, n)\n\n        for n, m, (r_bond, p_bond) in common_bonds:\n            if r_bond is p_bond is None:  # skip None>None\n                continue\n            bond = DynBond.__new__(DynBond)\n            bond.__init_copy__(r_bond, p_bond)\n            h.add_bond(n, m, bond)\n\n        for n, m, bond in bonds:\n            h.add_bond(n, m, bond)\n\n        return h"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecompose the CGR to pair of Molecules which represents reactants and products state of reaction .", "response": "def decompose(self):\n        \"\"\"\n        decompose CGR to pair of Molecules, which represents reactants and products state of reaction\n\n        :return: tuple of two molecules\n        \"\"\"\n        mc = self._get_subclass('MoleculeContainer')\n        reactants = mc()\n        products = mc()\n\n        for n, atom in self.atoms():\n            reactants.add_atom(atom._reactant, n)\n            products.add_atom(atom._product, n)\n\n        for n, m, bond in self.bonds():\n            if bond._reactant is not None:\n                reactants.add_bond(n, m, bond._reactant)\n            if bond._product is not None:\n                products.add_bond(n, m, bond._product)\n        return reactants, products"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cycle_data(self, verbose=False, result_cycle=None, result_size=None, result_edges=None,changelog=True):\n\n        cycle_names = [s['name'] for s in self.settings['cycle']]\n        accepted_steps = set(s['name'] for s in self.settings['cycle'] if s['type'] == StatusTypes.accepted)\n        completed_steps = set(s['name'] for s in self.settings['cycle'] if s['type'] == StatusTypes.complete)\n\n        series = {\n            'key': {'data': [], 'dtype': str},\n            'url': {'data': [], 'dtype': str},\n            'issue_type': {'data': [], 'dtype': str},\n            'summary': {'data': [], 'dtype': str},\n            'status': {'data': [], 'dtype': str},\n            'resolution': {'data': [], 'dtype': str},\n            'cycle_time': {'data': [], 'dtype': 'timedelta64[ns]'},\n            'completed_timestamp': {'data': [], 'dtype': 'datetime64[ns]'},\n            'created_timestamp': {'data': [], 'dtype': 'datetime64[ns]'}\n        }\n\n        if sys.platform.startswith('win'):\n            buffer = open(\"cycledata.tmp\", \"w+\",1)\n            # Opens a file for writing only in binary format. Overwrites the file if the file exists.\n            # buffering value is 1\n            # Windows users seem to have a problem with spooled file\n        else:\n            buffer = tempfile.SpooledTemporaryFile(max_size=50000, mode='w+t')\n\n        #issuelinks = open(\"issuelinks.csv\", \"w+\", 1)\n        #df_edges = pd.DataFrame()\n        #df_edges = pd.DataFrame(columns=['Source', 'OutwardLink', 'Target', 'Inwardlink','LinkType'])\n        #df_edges.to_csv(issuelinks, columns=['Source', 'OutwardLink', 'Target', 'Inwardlink','LinkType'], header=True, index=None, sep='\\t',encoding='utf-8')\n\n        df_size_history = pd.DataFrame( columns=['key','fromDate','toDate','size'])\n        df_size_history.to_csv(buffer, columns=['key', 'fromDate', 'toDate', 'size'], header=True, index=None, sep='\\t',encoding='utf-8')\n\n        for cycle_name in cycle_names:\n            series[cycle_name] = {'data': [], 'dtype': 'datetime64[ns]'}\n\n        for name in self.fields.keys():\n            series[name] = {'data': [], 'dtype': 'object'}\n\n        if self.settings['query_attribute']:\n            series[self.settings['query_attribute']] = {'data': [], 'dtype': str}\n\n        for criteria in self.settings['queries']:\n            for issue in self.find_issues(criteria, order='updatedDate DESC', verbose=verbose, changelog=changelog):\n\n                # Deal with the differences in strings between Python 2 & 3\n                if (sys.version_info > (3, 0)):\n                    # Python 3 code in this block\n                    item = {\n                        'key': issue.key,\n                        'url': \"%s/browse/%s\" % (self.jira._options['server'], issue.key,),\n                        'issue_type': issue.fields.issuetype.name,\n                        'summary': issue.fields.summary,  # .encode('utf-8'),\n                        'status': issue.fields.status.name,\n                        'resolution': issue.fields.resolution.name if issue.fields.resolution else None,\n                        'cycle_time': None,\n                        'completed_timestamp': None,\n                        'created_timestamp':  issue.fields.created[:19]\n                    }\n                else:\n                    # Python 2 code in this block\n                    item = {\n                        'key': issue.key,\n                        'url': \"%s/browse/%s\" % (self.jira._options['server'], issue.key,),\n                        'issue_type': issue.fields.issuetype.name,\n                        'summary': issue.fields.summary.encode('utf-8'),\n                        'status': issue.fields.status.name,\n                        'resolution': issue.fields.resolution.name if issue.fields.resolution else None,\n                        'cycle_time': None,\n                        'completed_timestamp': None,\n                        'created_timestamp': issue.fields.created[:19]\n                    }\n\n                for name, field_name in self.fields.items():\n                    item[name] = self.resolve_field_value(issue, name, field_name)\n\n                if self.settings['query_attribute']:\n                    item[self.settings['query_attribute']] = criteria.get('value', None)\n\n                for cycle_name in cycle_names:\n                    item[cycle_name] = None\n\n                # Get the relationships for this issue\n                edges = []  # Source, Target, Inward Link, Outward Link, Type\n                issuelinks = issue.fields.issuelinks\n\n                # It is seems that having an Epic Parent does not record an Epic Link, just the name \"Epic Name\"\n                # Creating Epic relationship requires more work. Also each Jira instance will have different customfields for Epic data\n                # Remove this code.\n                #issueEpic = issue.fields.customfield_10008 if issue.fields.customfield_10008 else None  # Epic Link\n                #if issueEpic is not None:\n                #    data = {'Source':issueEpic, 'Target':issue.key, 'InwardLink':'Belongs to Epic', 'OutwardLink':'Issue in Epic', 'LinkType':'EpicIssue'}\n                #    edges.append(data)\n\n                for link in issuelinks:\n                    inwardissue = None\n                    outwardissue = None\n                    try:\n                        inwardissue = link.inwardIssue.key\n                    except:\n                        outwardissue = link.outwardIssue.key\n\n                    if inwardissue is not None:\n                        data = {'LinkID':link.id,'Source':inwardissue, 'Target':issue.key, 'InwardLink':link.type.inward, 'OutwardLink': link.type.outward, 'LinkType':link.type.name}\n                    else:\n                        data = {'LinkID':link.id,'Source':issue.key, 'Target': outwardissue, 'InwardLink':link.type.inward, 'OutwardLink':link.type.outward, 'LinkType':link.type.name}\n                    edges.append(data)\n\n                if len(edges)>0:\n                    try:\n                        df_edges\n                    except NameError:\n                        #print('Not found')\n                        df_edges = pd.DataFrame(edges)\n                    else:\n                        df_links = pd.DataFrame(edges)\n                        df_edges=df_edges.append(df_links)  # = pd.DataFrame(edges)\n                # Got all the relationships for this issue\n\n                rows = []\n                try:\n                    for snapshot in self.iter_size_changes(issue):\n                        data= {'key':snapshot.key,'fromDate':snapshot.date,'size':snapshot.size}\n                        rows.append(data)\n\n                    df = pd.DataFrame(rows)\n                    # Create the toDate column\n                    df_toDate=df['fromDate'].shift(-1)\n                    df_toDate.loc[len(df_toDate)-1] = datetime.datetime.now(pytz.utc)\n                    df['toDate'] = df_toDate\n                except:\n                    df = pd.DataFrame(columns = ['key', 'fromDate', 'toDate', 'size'])\n\n                # Round Down datetimes to full dates\n                df['fromDate'] = df['fromDate'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day))\n                df['toDate'] = df['toDate'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day))\n                # If we only have one row of size changes and current issue has a size then it must have been created with a size value at creation.\n                # This size will not be recorded in the size_change record.\n                # Hence update the single row we have with the current issue size.\n                # Get Story Points size changes history\n                #If condition is met update the size cell\n                if getattr(item, 'StoryPoints', None) is not None and (df.shape[0]==1):\n                #if (item['StoryPoints'] is not None ) and (len(df)==1):\n                    df.loc[df.index[0], 'size'] = item['StoryPoints']\n\n                # Append to csv file\n                df.to_csv(buffer, columns=['key', 'fromDate', 'toDate', 'size'], header=None,\n                           mode='a', sep='\\t', date_format='%Y-%m-%d',encoding='utf-8')\n                #print(rows)\n\n                # If the first column in item lifecycle was scipted put the created data in it.\n                if item[cycle_names[0]] is None:\n                    item[cycle_names[0]] = dateutil.parser.parse(item['created_timestamp']) #item['created_timestamp']\n                    # Figure out why the first Column does not have created date\n                    #print(dateutil.parser.parse(item['created_timestamp']))\n\n                # Record date of status changes\n                for snapshot in self.iter_changes(issue, True):\n                    snapshot_cycle_step = self.settings['cycle_lookup'].get(snapshot.status.lower(), None)\n                    if snapshot_cycle_step is None:\n                        if verbose:\n                            print(issue.key, \"transitioned to unknown JIRA status\", snapshot.status)\n                        continue\n\n                    snapshot_cycle_step_name = snapshot_cycle_step['name']\n\n                    # Keep the first time we entered a step\n                    if item[snapshot_cycle_step_name] is None:\n                        item[snapshot_cycle_step_name] = snapshot.date\n\n                    # Wipe any subsequent dates, in case this was a move backwards\n                    found_cycle_name = False\n                    for cycle_name in cycle_names:\n                        if not found_cycle_name and cycle_name == snapshot_cycle_step_name:\n                            found_cycle_name = True\n                            continue\n                        elif found_cycle_name and item[cycle_name] is not None:\n                            if verbose:\n                                print(issue.key, \"moved backwards to\", snapshot_cycle_step_name, \"wiping date for subsequent step\", cycle_name)\n                            item[cycle_name] = None\n\n                # Wipe timestamps if items have moved backwards; calculate cycle time\n\n                previous_timestamp = None\n                accepted_timestamp = None\n                completed_timestamp = None\n\n                for cycle_name in cycle_names:\n                    if item[cycle_name] is not None:\n                        previous_timestamp = item[cycle_name]\n\n                        if accepted_timestamp is None and previous_timestamp is not None and cycle_name in accepted_steps:\n                            accepted_timestamp = previous_timestamp\n                        if completed_timestamp is None and previous_timestamp is not None and cycle_name in completed_steps:\n                            completed_timestamp = previous_timestamp\n\n                if accepted_timestamp is not None and completed_timestamp is not None:\n                    item['cycle_time'] = completed_timestamp - accepted_timestamp\n                    item['completed_timestamp'] = completed_timestamp\n\n                for k, v in item.items():\n                    series[k]['data'].append(v)\n\n        data = {}\n        for k, v in series.items():\n            data[k] = pd.Series(v['data'], dtype=v['dtype'])\n\n        result_cycle = pd.DataFrame(data,\n                    columns=['key', 'url', 'issue_type', 'summary', 'status', 'resolution'] +\n                    sorted(self.fields.keys()) +\n                    ([self.settings['query_attribute']] if self.settings['query_attribute'] else []) +\n                    ['cycle_time', 'completed_timestamp'] +\n                    cycle_names\n        )\n\n        result_size = pd.DataFrame()\n        buffer.seek(0)\n        result_size = result_size.from_csv(buffer, sep='\\t')\n        buffer.close()\n\n        try:\n            df_edges\n        except NameError:\n            # print('Not found')\n            df_edges = pd.DataFrame()\n        try:\n            df_edges = df_edges[['Source', 'OutwardLink', 'Target', 'InwardLink','LinkType','LinkID']] # Specify dataframe sort order\n            #df_edges.to_csv(\"myedges.csv\", sep='\\t', index=False,encoding='utf-8')\n        except KeyError:\n            print('Info: No issue edges found.')\n\n        result_edges=df_edges\n\n        # There maybe no result_size data is we might not have any change history\n        try:\n            result_size.set_index('key')\n        except KeyError:\n            result_size = pd.DataFrame(index= ['key'],columns = ['fromDate', 'toDate', 'size'])\n\n        result_size['toDate'] = pd.to_datetime(result_size['toDate'], format=('%Y-%m-%d'))\n        result_size['fromDate'] = pd.to_datetime(result_size['fromDate'], format=('%Y-%m-%d'))\n\n        return result_cycle, result_size, result_edges", "response": "Get data from JIRA for cycle times and story points size change."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size_history(self,size_data):\n\n        def my_merge(df1, df2):\n            # http://stackoverflow.com/questions/34411495/pandas-merge-several-dataframes\n            res = pd.merge(df1, df2, how='outer', left_index=True, right_index=True)\n            cols = sorted(res.columns)\n            pairs = []\n            for col1, col2 in zip(cols[:-1], cols[1:]):\n                if col1.endswith('_x') and col2.endswith('_y'):\n                    pairs.append((col1, col2))\n            for col1, col2 in pairs:\n                res[col1[:-2]] = res[col1].combine_first(res[col2])\n                res = res.drop([col1, col2], axis=1)\n            return res\n\n        dfs_key = []\n        # Group the dataframe by regiment, and for each regiment,\n        for name, group in size_data.groupby('key'):\n            dfs = []\n            for row in group.itertuples():\n                # print(row.Index, row.fromDate,row.toDate, row.size)\n                dates = pd.date_range(start=row.fromDate, end=row.toDate)\n                sizes = [row.size] * len(dates)\n                data = {'date': dates, 'size': sizes}\n                df2 = pd.DataFrame(data, columns=['date', 'size'])\n                pd.to_datetime(df2['date'], format=('%Y-%m-%d'))\n                df2.set_index(['date'], inplace=True)\n                dfs.append(df2)\n            # df_final = reduce(lambda left,right: pd.merge(left,right), dfs)\n            df_key = (reduce(my_merge, dfs))\n            df_key.columns = [name if x == 'size' else x for x in df_key.columns]\n            dfs_key.append(df_key)\n        df_all = (reduce(my_merge, dfs_key))\n\n        # Sort the columns based on Jira Project code and issue number\n        mykeys = df_all.columns.values.tolist()\n        mykeys.sort(key=lambda x: x.split('-')[0] + '-' + str(int(x.split('-')[1])).zfill(6))\n        df_all = df_all[mykeys]\n\n        # Reindex to make sure we have all dates\n        start, end = df_all.index.min(), df_all.index.max()\n        df_all = df_all.reindex(pd.date_range(start, end, freq='D'), method='ffill')\n\n        return df_all", "response": "Return a DataFrame with columns containing story size for each issue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cfd(self, cycle_data,size_history= None, pointscolumn= None, stacked = True ):\n\n        # Define helper function\n        def cumulativeColumnStates(df,stacked):\n            \"\"\"\n            Calculate the column sums, were the incoming matrix columns represents items in workflow states\n            States progress from left to right.\n            We what to zero out items, other than right most value to avoid counting items in prior states.\n            :param df:\n            :return: pandas dataframe row with sum of column items\n            \"\"\"\n\n            # Helper functions to return the right most cells in 2D array\n            def last_number(lst):\n                if all(map(lambda x: x == 0, lst)):\n                    return 0\n                elif lst[-1] != 0:\n                    return len(lst) - 1\n                else:\n                    return last_number(lst[:-1])\n\n            def fill_others(lst):\n                new_lst = [0] * len(lst)\n                new_lst[last_number(lst)] = lst[last_number(lst)]\n                return new_lst\n\n            df_zeroed = df.fillna(value=0)  # ,inplace = True   Get rid of non numeric items. Make a ?deep? copy\n            if stacked:\n                df_result = df_zeroed.apply(lambda x: fill_others(x.values.tolist()), axis=1)\n            else:\n                df_result = df_zeroed\n\n            sum_row = df_result[df.columns].sum()  # Sum Columns\n            return pd.DataFrame(data=sum_row).T  # Transpose into row dataframe and return\n\n        # Helper function to return the right most cells in 2D array\n        def keeprightmoststate(df):\n            \"\"\"\n            Incoming matrix columns represents items in workflow states\n            States progress from left to right.\n            We what to zero out items, other than right most value.\n            :param df:\n            :return: pandas dataframe row with sum of column items\n            \"\"\"\n\n            def last_number(lst):\n                if all(map(lambda x: x == 0, lst)):\n                    return 0\n                elif lst[-1] != 0:\n                    return len(lst) - 1\n                else:\n                    return last_number(lst[:-1])\n\n            def fill_others(lst):\n                new_lst = [0] * len(lst)\n                new_lst[last_number(lst)] = lst[last_number(lst)]\n                return new_lst\n\n            df_zeroed = df.fillna(value=0)  # ,inplace = True   Get rid of non numeric items. Make a ?deep? copy\n            df_result = df_zeroed.apply(lambda x: fill_others(x.values.tolist()), axis=1)\n            return df_result\n\n        # Define helper function\n        def hide_greater_than_date(cell, adate):\n            \"\"\" Helper function to compare date values in cells\n            \"\"\"\n            result = False\n            try:\n                celldatetime = datetime.date(cell.year, cell.month, cell.day)\n            except:\n                return True\n            if celldatetime > adate:\n                return True\n            return False  # We have a date value in cell and it is less than or equal to input date\n\n        # Helper function\n        def appendDFToCSV(df, csvFilePath, sep=\"\\t\",date_format='%Y-%m-%d', encoding='utf-8'):\n            import os\n            if not os.path.isfile(csvFilePath):\n                df.to_csv(csvFilePath, mode='a', index=False, sep=sep, date_format=date_format, encoding=encoding)\n            elif len(df.columns) != len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns):\n                raise Exception(\n                    \"Columns do not match!! Dataframe has \" + str(len(df.columns)) + \" columns. CSV file has \" + str(\n                        len(pd.read_csv(csvFilePath, nrows=1, sep=sep).columns)) + \" columns.\")\n            elif not (df.columns == pd.read_csv(csvFilePath, nrows=1, sep=sep).columns).all():\n                raise Exception(\"Columns and column order of dataframe and csv file do not match!!\")\n            else:\n                df.to_csv(csvFilePath, mode='a', index=False, sep=sep, header=False, date_format=date_format, encoding=encoding)\n\n\n        #print(pointscolumn)\n\n        # List of all state change columns that may have date value in them\n        cycle_names = [s['name'] for s in self.settings['cycle']]\n\n        # Create list of columns that we want to return in our results dataFrame\n        slice_columns = list(self.settings['none_sized_statuses']) # Make a COPY of the list so that we dont modify the reference.\n        if pointscolumn:\n            for size_state in self.settings['sized_statuses']:  # states_to_size:\n                sizedStateName = size_state + 'Sized'\n                slice_columns.append(sizedStateName)\n            # Check that it works if we use all columns as sized.\n            slice_columns = []\n            for size_state in cycle_names:\n                sizedStateName = size_state + 'Sized'\n                slice_columns.append(sizedStateName)\n        else:\n            slice_columns = cycle_names\n\n\n        # Build a dataframe of just the \"date\" columns\n        df = cycle_data[cycle_names].copy()\n\n        # Strip out times from all dates\n        df = pd.DataFrame(\n            np.array(df.values, dtype='<M8[ns]').astype('<M8[D]').astype('<M8[ns]'),\n            columns=df.columns,\n            index=df.index\n        )\n\n        # No history provided this thus we return dataframe with just column headers.\n        if size_history is None:\n            return df\n\n        # Get a list of dates that a issue changed state\n        state_changes_on_dates_set = set()\n        for state in cycle_names:\n            state_changes_on_dates_set = state_changes_on_dates_set.union(set(df[state]))\n            # How many unique days did a issue stage state\n        # Remove non timestamp vlaues and sort the list\n        state_changes_on_dates = filter(lambda x: type(x.date()) == datetime.date,\n                                        sorted(list(state_changes_on_dates_set)))\n\n\n\n        # Replace missing NaT values (happens if a status is skipped) with the subsequent timestamp\n        df = df.fillna(method='bfill', axis=1)\n\n\n        if pointscolumn:\n            storypoints = cycle_data[pointscolumn] # As at today\n            ids = cycle_data['key']\n\n\n        # create blank results dataframe\n        df_results = pd.DataFrame()\n        # For each date on which we had a issue state change we want to count and sum the totals for each of the given states\n        # 'Open','Analysis','Backlog','In Process','Done','Withdrawn'\n        timenowstr = datetime.datetime.now().strftime('-run-%Y-%m-%d_%H-%M-%S')\n        for date_index,statechangedate in enumerate(state_changes_on_dates):\n            if date_index%10 == 0: # Print out Progress every tenth\n                pass #print(\"CFD state change {} of {} \".format(date_index,len(state_changes_on_dates)))\n            if type(statechangedate.date()) == datetime.date:\n                # filterdate.year,filterdate.month,filterdate.day\n                filterdate = datetime.date(statechangedate.year, statechangedate.month,\n                                           statechangedate.day)  # statechangedate.datetime()\n\n                # Apply function to each cell and only make it visible if issue was in state on or after the filter date\n                df_filtered = df.applymap(lambda x: 0 if hide_greater_than_date(x, filterdate) else 1)\n\n                if stacked:\n                    df_filtered=keeprightmoststate(df_filtered)\n\n                if pointscolumn and (size_history is not None):\n\n                    # For debug\n                    #if filterdate.isoformat() == '2016-11-22':\n                    #    size_history.loc[filterdate.isoformat()].to_csv(\"debug-size-history.csv\")\n                    storypoints_series_on = size_history.loc[filterdate.isoformat()].T\n                    df_size_on_day = pd.Series.to_frame(storypoints_series_on)\n                    df_size_on_day.columns = [pointscolumn]\n\n                    # Make sure get size data in the same sequence as ids.\n                    left = pd.Series.to_frame(ids)\n                    right = df_size_on_day\n                    result = left.join(right, on=['key'])  # http://pandas.pydata.org/pandas-docs/stable/merging.html\\\n                    df_countable = pd.concat([result, df_filtered], axis=1)\n                    # for debuging and analytics append the days state to file\n                    df_countable['date'] = filterdate.isoformat()\n                    if stacked:\n                        file_name = \"daily-cfd-stacked-run-at\"+ timenowstr + \".csv\"\n                    else:\n                        file_name = \"daily-cfd-run-at\" + timenowstr + \".csv\"\n                    appendDFToCSV(df_countable, file_name )\n                else:\n                    df_countable = df_filtered\n\n                # Because we size issues with Story Points we need to add some additional columns\n                # for each state based on size not just count\n                if pointscolumn:\n                    for size_state in self.settings['sized_statuses']: #states_to_size:\n                        sizedStateName = size_state + 'Sized'\n                        df_countable[sizedStateName] = df_countable.apply( lambda row: (row[pointscolumn] * row[size_state] ), axis=1)\n\n                # For debugging write dataframe to sheet for current day.\n                #file_name=\"countable-cfd-for-day-\"+ filterdate.isoformat()+timenowstr+\".csv\"\n                #df_countable.to_csv(file_name, sep='\\t', encoding='utf-8', quoting=csv.QUOTE_ALL)\n\n                df_slice = df_countable.loc[:,slice_columns].copy()\n                df_sub_sum = cumulativeColumnStates(df_slice,stacked)\n                final_table = df_sub_sum.rename(index={0: filterdate})\n\n                # append to results\n                df_results = df_results.append(final_table)\n        df_results.sort_index(inplace=True)\n\n        df= df_results\n        # Count number of times each date occurs, preserving column order\n        #df = pd.concat({col: df[col].value_counts() for col in df}, axis=1)[cycle_names]\n\n        # Fill missing dates with 0 and run a cumulative sum\n        #df = df.fillna(0).cumsum(axis=0)\n\n        # Reindex to make sure we have all dates\n        start, end = df.index.min(), df.index.max()\n        try: # If we have no change history we will not have any data in the df and will get a ValueError on reindex\n            df = df.reindex(pd.date_range(start, end, freq='D'), method='ffill')\n        except ValueError:\n            pass\n\n        return df", "response": "Return the data to build a cumulative flow diagram for each item in the given cycle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef histogram(self, cycle_data, bins=10):\n        values, edges = np.histogram(cycle_data['cycle_time'].astype('timedelta64[D]').dropna(), bins=bins)\n\n        index = []\n        for i, v in enumerate(edges):\n            if i == 0:\n                continue\n            index.append(\"%.01f to %.01f\" % (edges[i - 1], edges[i],))\n\n        return pd.Series(values, name=\"Items\", index=index)", "response": "Return histogram data for the cycle times in cycle_data. Returns a Pandas Series with the values and edges of numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef throughput_data(self, cycle_data, frequency='1D',pointscolumn= None):\n        if len(cycle_data)<1:\n           return None # Note completed items yet, return None\n\n        if pointscolumn:\n            return cycle_data[['completed_timestamp', pointscolumn]] \\\n                .rename(columns={pointscolumn: 'sum'}) \\\n                .groupby('completed_timestamp').sum() \\\n                .resample(frequency).sum() \\\n                .fillna(0)\n        else:\n            return cycle_data[['completed_timestamp', 'key']] \\\n                .rename(columns={'key': 'count'}) \\\n                .groupby('completed_timestamp').count() \\\n                .resample(frequency).sum() \\\n                .fillna(0)", "response": "Return a data frame with columns completed_timestamp count sum of value specified by pointscolumn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn scatterplot data for the cycle times in cycle_data. Returns a data frame containing only those items in cycle_data where values are set for completed_timestamp and cycle_time are normalised to whole days and with those two columns as the first two.", "response": "def scatterplot(self, cycle_data):\n        \"\"\"Return scatterplot data for the cycle times in `cycle_data`. Returns\n        a data frame containing only those items in `cycle_data` where values\n        are set for `completed_timestamp` and `cycle_time`, and with those two\n        columns as the first two, both normalised to whole days, and with\n        `completed_timestamp` renamed to `completed_date`.\n        \"\"\"\n\n        columns = list(cycle_data.columns)\n        columns.remove('cycle_time')\n        columns.remove('completed_timestamp')\n        columns = ['completed_timestamp', 'cycle_time'] + columns\n\n        data = (\n            cycle_data[columns]\n            .dropna(subset=['cycle_time', 'completed_timestamp'])\n            .rename(columns={'completed_timestamp': 'completed_date'})\n        )\n\n        data['cycle_time'] = data['cycle_time'].astype('timedelta64[D]')\n        data['completed_date'] = data['completed_date'].map(pd.Timestamp.date)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_ready(self, topic_name):\n        '''\n        Is NSQ running and have space to receive messages?\n        '''\n        url = 'http://%s/stats?format=json&topic=%s' % (self.nsqd_http_address, topic_name)\n        #Cheacking for ephmeral channels\n        if '#' in topic_name:\n            topic_name, tag =topic_name.split(\"#\", 1)\n\n        try:\n            data = self.session.get(url).json()\n            '''\n            data = {u'start_time': 1516164866, u'version': u'1.0.0-compat', \\\n                    u'health': u'OK', u'topics': [{u'message_count': 19019, \\\n                    u'paused': False, u'topic_name': u'test_topic', u'channels': [], \\\n                    u'depth': 19019, u'backend_depth': 9019, u'e2e_processing_latency': {u'count': 0, \\\n                    u'percentiles': None}}]}\n            '''\n            topics = data.get('topics', [])\n            topics = [t for t in topics if t['topic_name'] == topic_name]\n\n            if not topics:\n                raise Exception('topic_missing_at_nsq')\n\n            topic = topics[0]\n            depth = topic['depth']\n            depth += sum(c.get('depth', 0) for c in topic['channels'])\n            self.log.debug('nsq_depth_check', topic=topic_name,\n                            depth=depth, max_depth=self.nsq_max_depth)\n\n            if depth < self.nsq_max_depth:\n                return\n            else:\n                raise Exception('nsq_is_full_waiting_to_clear')\n        except:\n            raise", "response": "Check if NSQ is ready to receive messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _matcher(self, other):\n        if isinstance(other, MoleculeContainer):\n            return GraphMatcher(other, self, lambda x, y: y == x, lambda x, y: y == x)\n        elif isinstance(other, (QueryContainer, QueryCGRContainer)):\n            return GraphMatcher(other, self, lambda x, y: x == y, lambda x, y: x == y)\n        raise TypeError('only query-molecule, query-query or query-cgr_query possible')", "response": "Returns a matcher that matches the given object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a matcher that matches the given instance of the class.", "response": "def _matcher(self, other):\n        \"\"\"\n        QueryCGRContainer < CGRContainer\n        QueryContainer < QueryCGRContainer[more general]\n        \"\"\"\n        if isinstance(other, CGRContainer):\n            return GraphMatcher(other, self, lambda x, y: y == x, lambda x, y: y == x)\n        elif isinstance(other, QueryCGRContainer):\n            return GraphMatcher(other, self, lambda x, y: x == y, lambda x, y: x == y)\n        raise TypeError('only cgr_query-cgr or cgr_query-cgr_query possible')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate2d(self, force=False, scale=1):\n        dist = {}\n        # length forces\n        for n, m_bond in self._adj.items():\n            dist[n] = {}\n            for m in m_bond:\n                dist[n][m] = .825\n\n        # angle forces\n        for n, m_bond in self._adj.items():\n            if len(m_bond) == 2:  # single-single or single-double bonds has angle = 120, other 180\n                (m1, b1), (m2, b2) = m_bond.items()\n                dist[m1][m2] = dist[m2][m1] = 1.43 if b1.order + b2.order in (2, 3) else 1.7  # +.05\n            elif len(m_bond) == 3:\n                m1, m2, m3 = m_bond\n                dist[m1][m2] = dist[m1][m3] = dist[m2][m3] = dist[m3][m2] = dist[m2][m1] = dist[m3][m1] = 1.43\n            elif len(m_bond) == 4:\n                #    1\n                #\n                # 2  X  4\n                #\n                #    3\n                m1, m2, m3, m4 = m_bond\n                dist[m1][m2] = dist[m1][m4] = dist[m2][m1] = dist[m2][m3] = 1.17\n                dist[m3][m2] = dist[m3][m4] = dist[m4][m1] = dist[m4][m3] = 1.17\n                dist[m1][m3] = dist[m3][m1] = dist[m2][m4] = dist[m4][m2] = 1.7  # +.05\n\n        # cycle forces\n        for r in self.sssr:\n            if len(r) == 6:\n                #    6\n                #\n                # 1     5\n                #\n                # 2     4\n                #\n                #    3\n                m1, m2, m3, m4, m5, m6 = r\n                dist[m1][m4] = dist[m4][m1] = dist[m2][m5] = dist[m5][m2] = dist[m3][m6] = dist[m6][m3] = 1.7  # +.05\n\n        if force:\n            pos = None\n        else:\n            pos = {n: (atom.x or uniform(0, .01), atom.y or uniform(0, .01)) for n, atom in self.atoms()}\n\n        for n, xy in kamada_kawai_layout(self, dist=dict(dist), pos=pos, scale=scale).items():\n            atom = self._node[n]\n            atom.x, atom.y = xy\n\n        self.flush_cache()", "response": "recalculate 2d coordinates. currently rings can be calculated badly."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef possible_moves(self, position):\n        for direction in [0, 1, 2, 3]:\n            angles = self._rotate_direction_ninety_degrees(direction)\n            for angle in angles:\n                try:\n                    end_loc = self.location.shift(angle).shift(direction).shift(direction)\n                    if position.is_square_empty(end_loc):\n                        status = notation_const.MOVEMENT\n                    elif not position.piece_at_square(end_loc).color == self.color:\n                        status = notation_const.CAPTURE\n                    else:\n                        continue\n\n                    yield Move(end_loc=end_loc,\n                               piece=self,\n                               status=status,\n                               start_loc=self.location)\n\n                except IndexError:\n                    pass", "response": "Finds all possible knight moves in the board."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef centers_list(self):\n        center = set()\n        adj = defaultdict(set)\n        for n, atom in self.atoms():\n            if atom._reactant != atom._product:\n                center.add(n)\n\n        for n, m, bond in self.bonds():\n            if bond._reactant != bond._product:\n                adj[n].add(m)\n                adj[m].add(n)\n                center.add(n)\n                center.add(m)\n\n        out = []\n        while center:\n            n = center.pop()\n            if n in adj:\n                c = set(self.__plain_bfs(adj, n))\n                out.append(list(c))\n                center.difference_update(c)\n            else:\n                out.append([n])\n\n        return out", "response": "get a list of lists of atoms of reaction centers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget list of atoms of reaction center", "response": "def center_atoms(self):\n        \"\"\" get list of atoms of reaction center (atoms with dynamic: bonds, charges, radicals).\n        \"\"\"\n        nodes = set()\n        for n, atom in self.atoms():\n            if atom._reactant != atom._product:\n                nodes.add(n)\n\n        for n, m, bond in self.bonds():\n            if bond._reactant != bond._product:\n                nodes.add(n)\n                nodes.add(m)\n\n        return list(nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets list of bonds of reaction center", "response": "def center_bonds(self):\n        \"\"\" get list of bonds of reaction center (bonds with dynamic orders).\n        \"\"\"\n        return [(n, m) for n, m, bond in self.bonds() if bond._reactant != bond._product]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting hyb and neighbors marks to atoms.", "response": "def reset_query_marks(self):\n        \"\"\"\n        set or reset hyb and neighbors marks to atoms.\n        \"\"\"\n        for i, atom in self.atoms():\n            neighbors = 0\n            hybridization = 1\n            p_neighbors = 0\n            p_hybridization = 1\n            # hyb 1- sp3; 2- sp2; 3- sp1; 4- aromatic\n            for j, bond in self._adj[i].items():\n                isnth = self._node[j].element != 'H'\n\n                order = bond.order\n                if order:\n                    if isnth:\n                        neighbors += 1\n                    if hybridization not in (3, 4):\n                        if order == 4:\n                            hybridization = 4\n                        elif order == 3:\n                            hybridization = 3\n                        elif order == 2:\n                            if hybridization == 2:\n                                hybridization = 3\n                            else:\n                                hybridization = 2\n                order = bond.p_order\n                if order:\n                    if isnth:\n                        p_neighbors += 1\n                    if p_hybridization not in (3, 4):\n                        if order == 4:\n                            p_hybridization = 4\n                        elif order == 3:\n                            p_hybridization = 3\n                        elif order == 2:\n                            if p_hybridization == 2:\n                                p_hybridization = 3\n                            else:\n                                p_hybridization = 2\n\n            atom._reactant._neighbors = neighbors\n            atom._reactant._hybridization = hybridization\n            atom._product._neighbors = p_neighbors\n            atom._product._hybridization = p_hybridization\n            atom.__dict__.clear()  # flush cache\n        self.flush_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates substructure containing atoms from nbunch list", "response": "def substructure(self, atoms, meta=False, as_view=True):\n        \"\"\"\n        create substructure containing atoms from nbunch list\n\n        :param atoms: list of atoms numbers of substructure\n        :param meta: if True metadata will be copied to substructure\n        :param as_view: If True, the returned graph-view provides a read-only view\n            of the original structure scaffold without actually copying any data\n        \"\"\"\n        s = super().substructure(atoms, meta, as_view)\n        if as_view:\n            s.reset_query_marks = frozen\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a GraphMatcher that matches the given instance of the class with the given instance of the class.", "response": "def _matcher(self, other):\n        \"\"\"\n        CGRContainer < CGRContainer\n        \"\"\"\n        if isinstance(other, CGRContainer):\n            return GraphMatcher(other, self, lambda x, y: x == y, lambda x, y: x == y)\n        raise TypeError('only cgr-cgr possible')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmodifying NX fast BFS node generator", "response": "def __plain_bfs(adj, source):\n        \"\"\"modified NX fast BFS node generator\"\"\"\n        seen = set()\n        nextlevel = {source}\n        while nextlevel:\n            thislevel = nextlevel\n            nextlevel = set()\n            for v in thislevel:\n                if v not in seen:\n                    yield v\n                    seen.add(v)\n                    nextlevel.update(adj[v])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the authorization token provided by Cocaine.", "response": "def token(self):\n        \"\"\"\n        Returns authorization token provided by Cocaine.\n\n        The real meaning of the token is determined by its type. For example OAUTH2 token will\n        have \"bearer\" type.\n\n        :return: A tuple of token type and body.\n        \"\"\"\n        if self._token is None:\n            token_type = os.getenv(TOKEN_TYPE_KEY, '')\n            token_body = os.getenv(TOKEN_BODY_KEY, '')\n            self._token = _Token(token_type, token_body)\n        return self._token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to the broker.", "response": "def _send(self):\n        \"\"\" Send a message lazy formatted with args.\n        External log attributes can be passed via named attribute `extra`,\n        like in logging from the standart library.\n\n        Note:\n            * Attrs must be dict, otherwise the whole message would be skipped.\n            * The key field in an attr is converted to string.\n            * The value is sent as is if isinstance of (str, unicode, int, float, long, bool),\n              otherwise we convert the value to string.\n        \"\"\"\n        buff = BytesIO()\n        while True:\n            msgs = list()\n            try:\n                msg = yield self.queue.get()\n\n                # we need to connect first, as we issue verbosity request just after connection\n                # and channels should strictly go in ascending order\n                if not self._connected:\n                    yield self.connect()\n\n                try:\n                    while True:\n                        msgs.append(msg)\n                        counter = next(self.counter)\n                        msgpack_pack([counter, EMIT, msg], buff)\n                        msg = self.queue.get_nowait()\n                except queues.QueueEmpty:\n                    pass\n\n                try:\n                    yield self.pipe.write(buff.getvalue())\n                except Exception:\n                    pass\n                # clean the buffer or we will end up without memory\n                buff.truncate(0)\n            except Exception:\n                for message in msgs:\n                    self._log_to_fallback(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef moves_in_direction(self, direction, position):\n        current_square = self.location\n\n        while True:\n            try:\n                current_square = direction(current_square)\n            except IndexError:\n                return\n\n            if self.contains_opposite_color_piece(current_square, position):\n                yield self.create_move(current_square, notation_const.CAPTURE)\n\n            if not position.is_square_empty(current_square):\n                return\n\n            yield self.create_move(current_square, notation_const.MOVEMENT)", "response": "Returns a generator that yields all the moves in a given direction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all possible rook moves in the given position.", "response": "def possible_moves(self, position):\n        \"\"\"\n        Returns all possible rook moves.\n\n        :type: position: Board\n        :rtype: list\n        \"\"\"\n        for move in itertools.chain(*[self.moves_in_direction(fn, position) for fn in self.cross_fn]):\n            yield move"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking overlap between two arrays.", "response": "def overlap_status(a, b):\n    \"\"\"Check overlap between two arrays.\n\n    Parameters\n    ----------\n    a, b : array-like\n        Arrays to check. Assumed to be in the same unit.\n\n    Returns\n    -------\n    result : {'full', 'partial', 'none'}\n        * 'full' - ``a`` is within or same as ``b``\n        * 'partial' - ``a`` partially overlaps with ``b``\n        * 'none' - ``a`` does not overlap ``b``\n\n    \"\"\"\n    # Get the endpoints\n    a1, a2 = a.min(), a.max()\n    b1, b2 = b.min(), b.max()\n\n    # Do the comparison\n    if a1 >= b1 and a2 <= b2:\n        result = 'full'\n    elif a2 < b1 or b2 < a1:\n        result = 'none'\n    else:\n        result = 'partial'\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking integrated flux for invalid values.", "response": "def validate_totalflux(totalflux):\n    \"\"\"Check integrated flux for invalid values.\n\n    Parameters\n    ----------\n    totalflux : float\n        Integrated flux.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Input is zero, negative, or not a number.\n\n    \"\"\"\n    if totalflux <= 0.0:\n        raise exceptions.SynphotError('Integrated flux is <= 0')\n    elif np.isnan(totalflux):\n        raise exceptions.SynphotError('Integrated flux is NaN')\n    elif np.isinf(totalflux):\n        raise exceptions.SynphotError('Integrated flux is infinite')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the wavelengths in the array are valid and that they are monotonic and that they are not duplicate.", "response": "def validate_wavelengths(wavelengths):\n    \"\"\"Check wavelengths for ``synphot`` compatibility.\n\n    Wavelengths must satisfy these conditions:\n\n        * valid unit type, if given\n        * no zeroes\n        * monotonic ascending or descending\n        * no duplicate values\n\n    Parameters\n    ----------\n    wavelengths : array-like or `~astropy.units.quantity.Quantity`\n        Wavelength values.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Wavelengths unit type is invalid.\n\n    synphot.exceptions.DuplicateWavelength\n        Wavelength array contains duplicate entries.\n\n    synphot.exceptions.UnsortedWavelength\n        Wavelength array is not monotonic.\n\n    synphot.exceptions.ZeroWavelength\n        Negative or zero wavelength occurs in wavelength array.\n\n    \"\"\"\n    if isinstance(wavelengths, u.Quantity):\n        units.validate_wave_unit(wavelengths.unit)\n        wave = wavelengths.value\n    else:\n        wave = wavelengths\n\n    if np.isscalar(wave):\n        wave = [wave]\n\n    wave = np.asarray(wave)\n\n    # Check for zeroes\n    if np.any(wave <= 0):\n        raise exceptions.ZeroWavelength(\n            'Negative or zero wavelength occurs in wavelength array',\n            rows=np.where(wave <= 0)[0])\n\n    # Check for monotonicity\n    sorted_wave = np.sort(wave)\n    if not np.alltrue(sorted_wave == wave):\n        if np.alltrue(sorted_wave[::-1] == wave):\n            pass  # Monotonic descending is allowed\n        else:\n            raise exceptions.UnsortedWavelength(\n                'Wavelength array is not monotonic',\n                rows=np.where(sorted_wave != wave)[0])\n\n    # Check for duplicate values\n    if wave.size > 1:\n        dw = sorted_wave[1:] - sorted_wave[:-1]\n        if np.any(dw == 0):\n            raise exceptions.DuplicateWavelength(\n                'Wavelength array contains duplicate entries',\n                rows=np.where(dw == 0)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the wavelengths for a single set of wavelengths.", "response": "def generate_wavelengths(minwave=500, maxwave=26000, num=10000, delta=None,\n                         log=True, wave_unit=u.AA):\n    \"\"\"Generate wavelength array to be used for spectrum sampling.\n\n    .. math::\n\n        minwave \\\\le \\\\lambda < maxwave\n\n    Parameters\n    ----------\n    minwave, maxwave : float\n        Lower and upper limits of the wavelengths.\n        These must be values in linear space regardless of ``log``.\n\n    num : int\n        The number of wavelength values.\n        This is only used when ``delta=None``.\n\n    delta : float or `None`\n        Delta between wavelength values.\n        When ``log=True``, this is the spacing in log space.\n\n    log : bool\n        If `True`, the wavelength values are evenly spaced in log scale.\n        Otherwise, spacing is linear.\n\n    wave_unit : str or `~astropy.units.core.Unit`\n        Wavelength unit. Default is Angstrom.\n\n    Returns\n    -------\n    waveset : `~astropy.units.quantity.Quantity`\n        Generated wavelength set.\n\n    waveset_str : str\n        Info string associated with the result.\n\n    \"\"\"\n    wave_unit = units.validate_unit(wave_unit)\n\n    if delta is not None:\n        num = None\n\n    waveset_str = 'Min: {0}, Max: {1}, Num: {2}, Delta: {3}, Log: {4}'.format(\n        minwave, maxwave, num, delta, log)\n\n    # Log space\n    if log:\n        logmin = np.log10(minwave)\n        logmax = np.log10(maxwave)\n\n        if delta is None:\n            waveset = np.logspace(logmin, logmax, num, endpoint=False)\n        else:\n            waveset = 10 ** np.arange(logmin, logmax, delta)\n\n    # Linear space\n    else:\n        if delta is None:\n            waveset = np.linspace(minwave, maxwave, num, endpoint=False)\n        else:\n            waveset = np.arange(minwave, maxwave, delta)\n\n    return waveset.astype(np.float64) * wave_unit, waveset_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_wavelengths(waveset1, waveset2, threshold=1e-12):\n    if waveset1 is None and waveset2 is None:\n        out_wavelengths = None\n    elif waveset1 is not None and waveset2 is None:\n        out_wavelengths = waveset1\n    elif waveset1 is None and waveset2 is not None:\n        out_wavelengths = waveset2\n    else:\n        out_wavelengths = np.union1d(waveset1, waveset2)\n        delta = out_wavelengths[1:] - out_wavelengths[:-1]\n        i_good = np.where(delta > threshold)\n\n        # Remove \"too close together\" duplicates\n        if len(i_good[0]) < delta.size:\n            out_wavelengths = np.append(\n                out_wavelengths[i_good], out_wavelengths[-1])\n\n    return out_wavelengths", "response": "Return the union of two wavelengths."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload all data files from CDBS to given root directory.", "response": "def download_data(cdbs_root, verbose=True, dry_run=False):\n    \"\"\"Download CDBS data files to given root directory.\n    Download is skipped if a data file already exists.\n\n    Parameters\n    ----------\n    cdbs_root : str\n        Root directory for CDBS data files.\n\n    verbose : bool\n        Print extra information to screen.\n\n    dry_run : bool\n        Go through the logic but skip the actual download.\n        This would return a list of files that *would have been*\n        downloaded without network calls.\n        Use this option for debugging or testing.\n\n    Raises\n    ------\n    OSError\n        Problem with directory.\n\n    Returns\n    -------\n    file_list : list of str\n        A list of downloaded files.\n\n    \"\"\"\n    from .config import conf  # Avoid potential circular import\n\n    if not os.path.exists(cdbs_root):\n        os.makedirs(cdbs_root, exist_ok=True)\n        if verbose:  # pragma: no cover\n            print('Created {}'.format(cdbs_root))\n    elif not os.path.isdir(cdbs_root):\n        raise OSError('{} must be a directory'.format(cdbs_root))\n\n    host = 'http://ssb.stsci.edu/cdbs/'\n    file_list = []\n\n    if not cdbs_root.endswith(os.sep):\n        cdbs_root += os.sep\n\n    # See https://github.com/astropy/astropy/issues/8524\n    for cfgitem in conf.__class__.__dict__.values():\n        if (not isinstance(cfgitem, ConfigItem) or\n                not cfgitem.name.endswith('file')):\n            continue\n\n        url = cfgitem()\n\n        if not url.startswith(host):\n            if verbose:  # pragma: no cover\n                print('{} is not from {}, skipping download'.format(\n                    url, host))\n            continue\n\n        dst = url.replace(host, cdbs_root).replace('/', os.sep)\n\n        if os.path.exists(dst):\n            if verbose:  # pragma: no cover\n                print('{} already exists, skipping download'.format(dst))\n            continue\n\n        # Create sub-directories, if needed.\n        subdirs = os.path.dirname(dst)\n        os.makedirs(subdirs, exist_ok=True)\n\n        if not dry_run:  # pragma: no cover\n            try:\n                src = download_file(url)\n                copyfile(src, dst)\n            except Exception as exc:\n                print('Download failed - {}'.format(str(exc)))\n                continue\n\n        file_list.append(dst)\n        if verbose:  # pragma: no cover\n            print('{} downloaded to {}'.format(url, dst))\n\n    return file_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndemonstrate functionality of PyVLX.", "response": "async def main(loop):\n    \"\"\"Demonstrate functionality of PyVLX.\"\"\"\n    pyvlx = PyVLX('pyvlx.yaml', loop=loop)\n    # Alternative:\n    # pyvlx = PyVLX(host=\"192.168.2.127\", password=\"velux123\", loop=loop)\n\n    # Runing scenes:\n    await pyvlx.load_scenes()\n    await pyvlx.scenes[\"All Windows Closed\"].run()\n\n    # Changing position of windows:\n    await pyvlx.load_nodes()\n    await pyvlx.nodes['Bath'].open()\n    await pyvlx.nodes['Bath'].close()\n    await pyvlx.nodes['Bath'].set_position(Position(position_percent=45))\n\n    # Changing of on-off switches:\n    # await pyvlx.nodes['CoffeeMaker'].set_on()\n    # await pyvlx.nodes['CoffeeMaker'].set_off()\n\n    # You can easily rename nodes:\n    # await pyvlx.nodes[\"Window 10\"].rename(\"Window 11\")\n\n    await pyvlx.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a node to the list of nodes.", "response": "def add(self, node):\n        \"\"\"Add Node, replace existing node if node with node_id is present.\"\"\"\n        if not isinstance(node, Node):\n            raise TypeError()\n        for i, j in enumerate(self.__nodes):\n            if j.node_id == node.node_id:\n                self.__nodes[i] = node\n                return\n        self.__nodes.append(node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def load(self, node_id=None):\n        if node_id is not None:\n            await self._load_node(node_id=node_id)\n        else:\n            await self._load_all_nodes()", "response": "Load nodes from KLF 200."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads single node via API.", "response": "async def _load_node(self, node_id):\n        \"\"\"Load single node via API.\"\"\"\n        get_node_information = GetNodeInformation(pyvlx=self.pyvlx, node_id=node_id)\n        await get_node_information.do_api_call()\n        if not get_node_information.success:\n            raise PyVLXException(\"Unable to retrieve node information\")\n        notification_frame = get_node_information.notification_frame\n        node = convert_frame_to_node(self.pyvlx, notification_frame)\n        if node is not None:\n            self.add(node)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _load_all_nodes(self):\n        get_all_nodes_information = GetAllNodesInformation(pyvlx=self.pyvlx)\n        await get_all_nodes_information.do_api_call()\n        if not get_all_nodes_information.success:\n            raise PyVLXException(\"Unable to retrieve node information\")\n        self.clear()\n        for notification_frame in get_all_nodes_information.notification_frames:\n            node = convert_frame_to_node(self.pyvlx, notification_frame)\n            if node is not None:\n                self.add(node)", "response": "Load all nodes from API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def handle_frame(self, frame):\n        if isinstance(frame, FrameGetAllNodesInformationConfirmation):\n            self.number_of_nodes = frame.number_of_nodes\n            # We are still waiting for FrameGetAllNodesInformationNotification\n            return False\n        if isinstance(frame, FrameGetAllNodesInformationNotification):\n            self.notification_frames.append(frame)\n        if isinstance(frame, FrameGetAllNodesInformationFinishedNotification):\n            if self.number_of_nodes != len(self.notification_frames):\n                PYVLXLOG.warning(\"Number of received scenes does not match expected number\")\n            self.success = True\n            return True\n        return False", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling incoming API frame return True if this was the expected frame.", "response": "async def handle_frame(self, frame):\n        \"\"\"Handle incoming API frame, return True if this was the expected frame.\"\"\"\n        if isinstance(frame, FrameGetNodeInformationConfirmation) and frame.node_id == self.node_id:\n            # We are still waiting for GetNodeInformationNotification\n            return False\n        if isinstance(frame, FrameGetNodeInformationNotification) and frame.node_id == self.node_id:\n            self.notification_frame = frame\n            self.success = True\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npulsing every timeout seconds until stopped.", "response": "async def loop(self):\n        \"\"\"Pulse every timeout seconds until stopped.\"\"\"\n        while not self.stopped:\n            self.timeout_handle = self.pyvlx.connection.loop.call_later(\n                self.timeout_in_seconds, self.loop_timeout)\n            await self.loop_event.wait()\n            if not self.stopped:\n                self.loop_event.clear()\n                await self.pulse()\n        self.cancel_loop_timeout()\n        self.stopped_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends get state request to API to keep the connection alive.", "response": "async def pulse(self):\n        \"\"\"Send get state request to API to keep the connection alive.\"\"\"\n        get_state = GetState(pyvlx=self.pyvlx)\n        await get_state.do_api_call()\n        if not get_state.success:\n            raise PyVLXException(\"Unable to send get state.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts string to bytes add padding.", "response": "def string_to_bytes(string, size):\n    \"\"\"Convert string to bytes add padding.\"\"\"\n    if len(string) > size:\n        raise PyVLXException(\"string_to_bytes::string_to_large\")\n    encoded = bytes(string, encoding='utf-8')\n    return encoded + bytes(size-len(encoded))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bytes_to_string(raw):\n    ret = bytes()\n    for byte in raw:\n        if byte == 0x00:\n            return ret.decode(\"utf-8\")\n        ret += bytes([byte])\n    return ret.decode(\"utf-8\")", "response": "Convert bytes to string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_payload(self):\n        payload = bytes([self.node_id])\n        payload += bytes([self.state])\n        payload += bytes(self.current_position.raw)\n        payload += bytes(self.target.raw)\n        payload += bytes(self.current_position_fp1.raw)\n        payload += bytes(self.current_position_fp2.raw)\n        payload += bytes(self.current_position_fp3.raw)\n        payload += bytes(self.current_position_fp4.raw)\n        payload += bytes([self.remaining_time >> 8 & 255, self.remaining_time & 255])\n        payload += struct.pack(\">I\", self.timestamp)\n        return payload", "response": "Return the payload of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenable house status monitor.", "response": "async def house_status_monitor_enable(pyvlx):\n    \"\"\"Enable house status monitor.\"\"\"\n    status_monitor_enable = HouseStatusMonitorEnable(pyvlx=pyvlx)\n    await status_monitor_enable.do_api_call()\n    if not status_monitor_enable.success:\n        raise PyVLXException(\"Unable enable house status monitor.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling house status monitor.", "response": "async def house_status_monitor_disable(pyvlx):\n    \"\"\"Disable house status monitor.\"\"\"\n    status_monitor_disable = HouseStatusMonitorDisable(pyvlx=pyvlx)\n    await status_monitor_disable.do_api_call()\n    if not status_monitor_disable.success:\n        raise PyVLXException(\"Unable disable house status monitor.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def handle_frame(self, frame):\n        if not isinstance(frame, FrameHouseStatusMonitorEnableConfirmation):\n            return False\n        self.success = True\n        return True", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def handle_frame(self, frame):\n        if not isinstance(frame, FrameHouseStatusMonitorDisableConfirmation):\n            return False\n        self.success = True\n        return True", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, pbar):\n\n        if pbar.seconds_elapsed < 2e-6 or pbar.currval < 2e-6:\n            scaled = bitmath.Byte()\n        else:\n            speed = pbar.currval / pbar.seconds_elapsed\n            scaled = bitmath.Byte(speed).best_prefix(system=self.system)\n\n        return scaled.format(self.format)", "response": "Updates the widget with the current NIST / SI speed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def handle_frame(self, frame):\n        if isinstance(frame, FrameCommandSendConfirmation) and frame.session_id == self.session_id:\n            if frame.status == CommandSendConfirmationStatus.ACCEPTED:\n                self.success = True\n            return not self.wait_for_completion\n        if isinstance(frame, FrameCommandRemainingTimeNotification) and frame.session_id == self.session_id:\n            # Ignoring FrameCommandRemainingTimeNotification\n            return False\n        if isinstance(frame, FrameCommandRunStatusNotification) and frame.session_id == self.session_id:\n            # At the moment I don't reall understand what the FrameCommandRunStatusNotification is good for.\n            # Ignoring these packets for now\n            return False\n        if isinstance(frame, FrameSessionFinishedNotification) and frame.session_id == self.session_id:\n            return True\n        return False", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autodetect():\n    logging.getLogger(__name__).debug('auto-detecting QT_API')\n    try:\n        logging.getLogger(__name__).debug('trying PyQt5')\n        import PyQt5\n        os.environ[QT_API] = PYQT5_API[0]\n        logging.getLogger(__name__).debug('imported PyQt5')\n    except ImportError:\n        try:\n            logging.getLogger(__name__).debug('trying PyQt4')\n            setup_apiv2()\n            import PyQt4\n            os.environ[QT_API] = PYQT4_API[0]\n            logging.getLogger(__name__).debug('imported PyQt4')\n        except ImportError:\n            try:\n                logging.getLogger(__name__).debug('trying PySide')\n                import PySide\n                os.environ[QT_API] = PYSIDE_API[0]\n                logging.getLogger(__name__).debug('imported PySide')\n            except ImportError:\n                raise PythonQtError('No Qt bindings could be found')", "response": "Auto - detects and uses the first available QT_API by importing them in the order of the order they were imported."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_config(cls, pyvlx, item):\n        name = item['name']\n        ident = item['id']\n        subtype = item['subtype']\n        typeid = item['typeId']\n        return cls(pyvlx, ident, name, subtype, typeid)", "response": "Read roller shutter from config."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def handle_frame(self, frame):\n        if isinstance(frame, FrameGetSceneListConfirmation):\n            self.count_scenes = frame.count_scenes\n            if self.count_scenes == 0:\n                self.success = True\n                return True\n            # We are still waiting for FrameGetSceneListNotification(s)\n            return False\n        if isinstance(frame, FrameGetSceneListNotification):\n            self.scenes.extend(frame.scenes)\n            if frame.remaining_scenes != 0:\n                # We are still waiting for FrameGetSceneListConfirmation(s)\n                return False\n            if self.count_scenes != len(self.scenes):\n                PYVLXLOG.warning(\"Warning: number of received scenes does not match expected number\")\n            self.success = True\n            return True\n        return False", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the payload of the current state of the object.", "response": "def get_payload(self):\n        \"\"\"Return Payload.\"\"\"\n        ret = bytes([len(self.scenes)])\n        for number, name in self.scenes:\n            ret += bytes([number])\n            ret += string_to_bytes(name, 64)\n        ret += bytes([self.remaining_scenes])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_payload(self, payload):\n        number_of_objects = payload[0]\n        self.remaining_scenes = payload[-1]\n        predicted_len = number_of_objects * 65 + 2\n        if len(payload) != predicted_len:\n            raise PyVLXException('scene_list_notification_wrong_length')\n        self.scenes = []\n        for i in range(number_of_objects):\n            scene = payload[(i*65+1):(i*65+66)]\n            number = scene[0]\n            name = bytes_to_string(scene[1:])\n            self.scenes.append((number, name))", "response": "Init frame from binary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads FITS or ASCII spectrum from a remote location.", "response": "def read_remote_spec(filename, encoding='binary', cache=True,\n                     show_progress=True, **kwargs):\n    \"\"\"Read FITS or ASCII spectrum from a remote location.\n\n    Parameters\n    ----------\n    filename : str\n        Spectrum filename.\n\n    encoding, cache, show_progress\n        See :func:`~astropy.utils.data.get_readable_fileobj`.\n\n    kwargs : dict\n        Keywords acceptable by :func:`read_fits_spec` (if FITS) or\n        :func:`read_ascii_spec` (if ASCII).\n\n    Returns\n    -------\n    header : dict\n        Metadata.\n\n    wavelengths, fluxes : `~astropy.units.quantity.Quantity`\n        Wavelength and flux of the spectrum.\n\n    \"\"\"\n    with get_readable_fileobj(filename, encoding=encoding, cache=cache,\n                              show_progress=show_progress) as fd:\n        header, wavelengths, fluxes = read_spec(fd, fname=filename, **kwargs)\n\n    return header, wavelengths, fluxes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread FITS or ASCII spectrum.", "response": "def read_spec(filename, fname='', **kwargs):\n    \"\"\"Read FITS or ASCII spectrum.\n\n    Parameters\n    ----------\n    filename : str or file pointer\n        Spectrum file name or pointer.\n\n    fname : str\n        Filename. This is *only* used if ``filename`` is a pointer.\n\n    kwargs : dict\n        Keywords acceptable by :func:`read_fits_spec` (if FITS) or\n        :func:`read_ascii_spec` (if ASCII).\n\n    Returns\n    -------\n    header : dict\n        Metadata.\n\n    wavelengths, fluxes : `~astropy.units.quantity.Quantity`\n        Wavelength and flux of the spectrum.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Read failed.\n\n    \"\"\"\n    if isinstance(filename, str):\n        fname = filename\n    elif not fname:  # pragma: no cover\n        raise exceptions.SynphotError('Cannot determine filename.')\n\n    if fname.endswith('fits') or fname.endswith('fit'):\n        read_func = read_fits_spec\n    else:\n        read_func = read_ascii_spec\n\n    return read_func(filename, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_ascii_spec(filename, wave_unit=u.AA, flux_unit=units.FLAM, **kwargs):\n    header = {}\n\n    dat = ascii.read(filename, **kwargs)\n\n    wave_unit = units.validate_unit(wave_unit)\n    flux_unit = units.validate_unit(flux_unit)\n\n    wavelengths = dat.columns[0].data.astype(np.float64) * wave_unit\n    fluxes = dat.columns[1].data.astype(np.float64) * flux_unit\n\n    return header, wavelengths, fluxes", "response": "Read an ASCII spectrum from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads FITS spectrum from file.", "response": "def read_fits_spec(filename, ext=1, wave_col='WAVELENGTH', flux_col='FLUX',\n                   wave_unit=u.AA, flux_unit=units.FLAM):\n    \"\"\"Read FITS spectrum.\n\n    Wavelength and flux units are extracted from ``TUNIT1`` and ``TUNIT2``\n    keywords, respectively, from data table (not primary) header.\n    If these keywords are not present, units are taken from\n    ``wave_unit`` and ``flux_unit`` instead.\n\n    Parameters\n    ----------\n    filename : str or file pointer\n        Spectrum file name or pointer.\n\n    ext: int\n        FITS extension with table data. Default is 1.\n\n    wave_col, flux_col : str\n        Wavelength and flux column names (case-insensitive).\n\n    wave_unit, flux_unit : str or `~astropy.units.core.Unit`\n        Wavelength and flux units, which default to Angstrom and FLAM,\n        respectively. These are *only* used if ``TUNIT1`` and ``TUNIT2``\n        keywords are not present in table (not primary) header.\n\n    Returns\n    -------\n    header : dict\n        Primary header only. Extension header is discarded.\n\n    wavelengths, fluxes : `~astropy.units.quantity.Quantity`\n        Wavelength and flux of the spectrum.\n\n    \"\"\"\n    fs = fits.open(filename)\n    header = dict(fs[str('PRIMARY')].header)\n    wave_dat = fs[ext].data.field(wave_col).copy()\n    flux_dat = fs[ext].data.field(flux_col).copy()\n    fits_wave_unit = fs[ext].header.get('TUNIT1')\n    fits_flux_unit = fs[ext].header.get('TUNIT2')\n\n    if fits_wave_unit is not None:\n        try:\n            wave_unit = units.validate_unit(fits_wave_unit)\n        except (exceptions.SynphotError, ValueError) as e:  # pragma: no cover\n            warnings.warn(\n                '{0} from FITS header is not valid wavelength unit, using '\n                '{1}: {2}'.format(fits_wave_unit, wave_unit, e),\n                AstropyUserWarning)\n\n    if fits_flux_unit is not None:\n        try:\n            flux_unit = units.validate_unit(fits_flux_unit)\n        except (exceptions.SynphotError, ValueError) as e:  # pragma: no cover\n            warnings.warn(\n                '{0} from FITS header is not valid flux unit, using '\n                '{1}: {2}'.format(fits_flux_unit, flux_unit, e),\n                AstropyUserWarning)\n\n    wave_unit = units.validate_unit(wave_unit)\n    flux_unit = units.validate_unit(flux_unit)\n\n    wavelengths = wave_dat * wave_unit\n    fluxes = flux_dat * flux_unit\n\n    if isinstance(filename, str):\n        fs.close()\n\n    return header, wavelengths, fluxes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite FITS spectrum to file.", "response": "def write_fits_spec(filename, wavelengths, fluxes, pri_header={},\n                    ext_header={}, overwrite=False, trim_zero=True,\n                    pad_zero_ends=True, precision=None, epsilon=0.00032,\n                    wave_col='WAVELENGTH', flux_col='FLUX',\n                    wave_unit=u.AA, flux_unit=units.FLAM):\n    \"\"\"Write FITS spectrum.\n\n    .. warning::\n\n        If data is being written out as single-precision but wavelengths\n        are in double-precision, some rows may be omitted.\n\n    Parameters\n    ----------\n    filename : str\n        Output spectrum filename.\n\n    wavelengths, fluxes : array-like or `~astropy.units.quantity.Quantity`\n        Wavelength and flux of the spectrum.\n\n    pri_header, ext_header : dict\n        Metadata to be added to primary and given extension FITS header,\n        respectively. Do *not* use this to define column names and units.\n\n    overwrite : bool\n        Overwrite existing file. Defaults to `False`.\n\n    trim_zero : bool\n        Remove rows with zero-flux. Default is `True`.\n\n    pad_zero_ends : bool\n        Pad each end of the spectrum with a row of zero flux\n        like :func:`synphot.spectrum.BaseSpectrum.taper`.\n        This is unnecessary if input is already tapered.\n\n    precision : {`None`, 'single', 'double'}\n        Precision of values in output file.\n        Use native flux precision by default.\n\n    epsilon : float\n        Single-precision :math:`\\\\epsilon` value, taken from IRAF SYNPHOT FAQ.\n        This is the minimum separation in wavelengths necessary for SYNPHOT\n        to read the entries as distinct single-precision numbers.\n        This is *only* used if ``precision='single'`` but data are in\n        double-precision. Default from the FAQ is 0.00032.\n\n    wave_col, flux_col : str\n        Wavelength and flux column names (case-insensitive).\n\n    wave_unit, flux_unit : str or `~astropy.units.core.Unit`\n        Wavelength and flux units, which default to Angstrom and FLAM,\n        respectively. These are *only* used if wavelengths and fluxes\n        are not in astropy quantities.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Wavelengths and fluxes have difference shapes or value precision\n        is not supported.\n\n    \"\"\"\n    if isinstance(wavelengths, u.Quantity):\n        wave_unit = wavelengths.unit\n        wave_value = wavelengths.value\n    else:\n        wave_value = wavelengths\n\n    if isinstance(fluxes, u.Quantity):\n        flux_unit = fluxes.unit\n        flux_value = fluxes.value\n    else:\n        flux_value = fluxes\n\n    wave_unit = units.validate_unit(wave_unit).to_string().upper()\n    flux_unit = units.validate_unit(flux_unit).to_string().upper()\n\n    if wave_value.shape != flux_value.shape:\n        raise exceptions.SynphotError(\n            'Wavelengths have shape {0} but fluxes have shape {1}'.format(\n                wave_value.shape, flux_value.shape))\n\n    # Remove rows with zero flux. Putting this before precision logic to avoid\n    # keeping duplicate wavelengths with zero flux.\n    if trim_zero:\n        idx = np.where(flux_value != 0)\n        wave_value = wave_value[idx]\n        flux_value = flux_value[idx]\n\n        n_thrown = wave_value.size - len(idx[0])\n        if n_thrown != 0:\n            log.info('{0} zero-flux rows are thrown out'.format(n_thrown))\n\n    # Only these Numpy types are supported\n    #    'f'   np.float32\n    #    'd'   np.float64\n    pcodes = {'d': 'D', 'f': 'E'}  # Numpy to FITS conversion\n\n    # Use native flux precision\n    if precision is None:\n        precision = flux_value.dtype.char\n        if precision not in pcodes:\n            raise exceptions.SynphotError('flux is not float32 or float64')\n\n    # Use user specified precision\n    else:\n        precision = precision.lower()\n        if precision == 'single':\n            precision = 'f'\n        elif precision == 'double':\n            precision = 'd'\n        else:\n            raise exceptions.SynphotError(\n                'precision must be single or double')\n\n    # Now check wavelength precision\n    wave_precision = wave_value.dtype.char\n    if wave_precision not in pcodes:\n        raise exceptions.SynphotError(\n            'wavelength is not float32 or float64')\n\n    # If wavelength is double-precision but data is written out as\n    # single-precision, wavelength values have to be recalculated\n    # so that they will still be sorted with no duplicates.\n    if wave_precision == 'd' and precision == 'f':\n        orig_size = wave_value.size\n        idx = np.where(np.abs(wave_value[1:] - wave_value[:-1]) > epsilon)\n        wave_value = np.append(wave_value[idx], wave_value[-1])\n        flux_value = np.append(flux_value[idx], flux_value[-1])\n        n_thrown = orig_size - wave_value.size\n        if n_thrown != 0:\n            warnings.warn(\n                '{0} rows are thrown out in converting wavelengths from '\n                'double- to single-precision'.format(n_thrown),\n                AstropyUserWarning)\n\n    # Keep one zero at each end\n    if pad_zero_ends:\n        w1 = wave_value[0] ** 2 / wave_value[1]\n        w2 = wave_value[-1] ** 2 / wave_value[-2]\n        wave_value = np.insert(wave_value, [0, wave_value.size], [w1, w2])\n        flux_value = np.insert(flux_value, [0, flux_value.size], [0.0, 0.0])\n\n    # Construct the columns\n    cw = fits.Column(name=wave_col, array=wave_value, unit=wave_unit,\n                     format=pcodes[precision])\n    cf = fits.Column(name=flux_col, array=flux_value, unit=flux_unit,\n                     format=pcodes[precision])\n\n    # These are written to the primary header:\n    #   1. Filename\n    #   2. Origin\n    #   3. User dictionary (can overwrite defaults)\n    hdr_hdu = fits.PrimaryHDU()\n    hdr_hdu.header['filename'] = (os.path.basename(filename), 'name of file')\n    hdr_hdu.header['origin'] = ('synphot', 'Version {0}'.format(__version__))\n    for key, val in pri_header.items():\n        hdr_hdu.header[key] = val\n\n    # Make the extension HDU and include user dictionary in extension header.\n    tab_hdu = fits.BinTableHDU.from_columns(fits.ColDefs([cw, cf]))\n    for key, val in ext_header.items():\n        tab_hdu.header[key] = val\n\n    # Write to file\n    hdulist = fits.HDUList([hdr_hdu])\n    hdulist.append(tab_hdu)\n    hdulist.writeto(filename, overwrite=overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spectral_density_vega(wav, vegaflux):\n    vega_photlam = vegaflux.to(\n        PHOTLAM, equivalencies=u.spectral_density(wav)).value\n\n    def converter(x):\n        \"\"\"Set nan/inf to -99 mag.\"\"\"\n        val = -2.5 * np.log10(x / vega_photlam)\n        result = np.zeros(val.shape, dtype=np.float64) - 99\n        mask = np.isfinite(val)\n        if result.ndim > 0:\n            result[mask] = val[mask]\n        elif mask:\n            result = np.asarray(val)\n        return result\n\n    def iconverter(x):\n        return vega_photlam * 10**(-0.4 * x)\n\n    return [(PHOTLAM, VEGAMAG, converter, iconverter)]", "response": "Returns a list of spectral density equivalencies between PHOTLAM and VEGAMAG."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spectral_density_count(wav, area):\n    from .binning import calculate_bin_widths, calculate_bin_edges\n\n    wav = wav.to(u.AA, equivalencies=u.spectral())\n    area = area.to(AREA)\n    bin_widths = calculate_bin_widths(calculate_bin_edges(wav))\n    factor = bin_widths.value * area.value\n\n    def converter_count(x):\n        return x * factor\n\n    def iconverter_count(x):\n        return x / factor\n\n    def converter_obmag(x):\n        return -2.5 * np.log10(x * factor)\n\n    def iconverter_obmag(x):\n        return 10**(-0.4 * x) / factor\n\n    return [(PHOTLAM, u.count, converter_count, iconverter_count),\n            (PHOTLAM, OBMAG, converter_obmag, iconverter_obmag)]", "response": "Flux equivalencies between PHOTLAM and count / OBMAG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting wavelengths and fluxes to the output flux units.", "response": "def convert_flux(wavelengths, fluxes, out_flux_unit, **kwargs):\n    \"\"\"Perform conversion for :ref:`supported flux units <synphot-flux-units>`.\n\n    Parameters\n    ----------\n    wavelengths : array-like or `~astropy.units.quantity.Quantity`\n        Wavelength values. If not a Quantity, assumed to be in\n        Angstrom.\n\n    fluxes : array-like or `~astropy.units.quantity.Quantity`\n        Flux values. If not a Quantity, assumed to be in PHOTLAM.\n\n    out_flux_unit : str or `~astropy.units.core.Unit`\n        Output flux unit.\n\n    area : float or `~astropy.units.quantity.Quantity`\n        Area that fluxes cover. If not a Quantity, assumed to be in\n        :math:`cm^{2}`. This value *must* be provided for conversions involving\n        OBMAG and count, otherwise it is not needed.\n\n    vegaspec : `~synphot.spectrum.SourceSpectrum`\n        Vega spectrum from :func:`~synphot.spectrum.SourceSpectrum.from_vega`.\n        This is *only* used for conversions involving VEGAMAG.\n\n    Returns\n    -------\n    out_flux : `~astropy.units.quantity.Quantity`\n        Converted flux values.\n\n    Raises\n    ------\n    astropy.units.core.UnitsError\n        Conversion failed.\n\n    synphot.exceptions.SynphotError\n        Area or Vega spectrum is not given when needed.\n\n    \"\"\"\n    if not isinstance(fluxes, u.Quantity):\n        fluxes = fluxes * PHOTLAM\n\n    out_flux_unit = validate_unit(out_flux_unit)\n    out_flux_unit_name = out_flux_unit.to_string()\n    in_flux_unit_name = fluxes.unit.to_string()\n\n    # No conversion necessary\n    if in_flux_unit_name == out_flux_unit_name:\n        return fluxes\n\n    in_flux_type = fluxes.unit.physical_type\n    out_flux_type = out_flux_unit.physical_type\n\n    # Wavelengths must Quantity\n    if not isinstance(wavelengths, u.Quantity):\n        wavelengths = wavelengths * u.AA\n\n    eqv = u.spectral_density(wavelengths)\n\n    # Use built-in astropy equivalencies\n    try:\n        out_flux = fluxes.to(out_flux_unit, eqv)\n\n    # Use PHOTLAM as in-between unit\n    except u.UnitConversionError:\n        # Convert input unit to PHOTLAM\n        if fluxes.unit == PHOTLAM:\n            flux_photlam = fluxes\n        elif in_flux_type != 'unknown':\n            flux_photlam = fluxes.to(PHOTLAM, eqv)\n        else:\n            flux_photlam = _convert_flux(\n                wavelengths, fluxes, PHOTLAM, **kwargs)\n\n        # Convert PHOTLAM to output unit\n        if out_flux_unit == PHOTLAM:\n            out_flux = flux_photlam\n        elif out_flux_type != 'unknown':\n            out_flux = flux_photlam.to(out_flux_unit, eqv)\n        else:\n            out_flux = _convert_flux(\n                wavelengths, flux_photlam, out_flux_unit, **kwargs)\n\n    return out_flux"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the fluxes to the specified units.", "response": "def _convert_flux(wavelengths, fluxes, out_flux_unit, area=None,\n                  vegaspec=None):\n    \"\"\"Flux conversion for PHOTLAM <-> X.\"\"\"\n    flux_unit_names = (fluxes.unit.to_string(), out_flux_unit.to_string())\n\n    if PHOTLAM.to_string() not in flux_unit_names:\n        raise exceptions.SynphotError(\n            'PHOTLAM must be one of the conversion units but get '\n            '{0}.'.format(flux_unit_names))\n\n    # VEGAMAG\n    if VEGAMAG.to_string() in flux_unit_names:\n        from .spectrum import SourceSpectrum\n\n        if not isinstance(vegaspec, SourceSpectrum):\n            raise exceptions.SynphotError('Vega spectrum is missing.')\n\n        flux_vega = vegaspec(wavelengths)\n\n        out_flux = fluxes.to(\n            out_flux_unit,\n            equivalencies=spectral_density_vega(wavelengths, flux_vega))\n\n    # OBMAG or count\n    elif (u.count in (fluxes.unit, out_flux_unit) or\n          OBMAG.to_string() in flux_unit_names):\n        if area is None:\n            raise exceptions.SynphotError(\n                'Area is compulsory for conversion involving count or OBMAG.')\n        elif not isinstance(area, u.Quantity):\n            area = area * AREA\n\n        out_flux = fluxes.to(\n            out_flux_unit,\n            equivalencies=spectral_density_count(wavelengths, area))\n\n    else:\n        raise u.UnitsError('{0} and {1} are not convertible'.format(\n            fluxes.unit, out_flux_unit))\n\n    return out_flux"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_unit(input_unit):\n    if isinstance(input_unit, str):\n        input_unit_lowcase = input_unit.lower()\n\n        # Backward-compatibility\n        if input_unit_lowcase == 'angstroms':\n            output_unit = u.AA\n        elif input_unit_lowcase == 'inversemicrons':\n            output_unit = u.micron ** -1\n        elif input_unit_lowcase in ('transmission', 'extinction',\n                                    'emissivity'):\n            output_unit = THROUGHPUT\n        elif input_unit_lowcase == 'jy':\n            output_unit = u.Jy\n\n        # Work around mag unit limitations\n        elif input_unit_lowcase in ('stmag', 'mag(st)'):\n            output_unit = u.STmag\n        elif input_unit_lowcase in ('abmag', 'mag(ab)'):\n            output_unit = u.ABmag\n\n        else:\n            try:  # astropy.units is case-sensitive\n                output_unit = u.Unit(input_unit)\n            except ValueError:  # synphot is case-insensitive\n                output_unit = u.Unit(input_unit_lowcase)\n\n    elif isinstance(input_unit, (u.UnitBase, u.LogUnit)):\n        output_unit = input_unit\n\n    else:\n        raise exceptions.SynphotError(\n            '{0} must be a recognized string or '\n            'astropy.units.core.Unit'.format(input_unit))\n\n    return output_unit", "response": "Validate a single unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_wave_unit(wave_unit):\n    output_unit = validate_unit(wave_unit)\n    unit_type = output_unit.physical_type\n\n    if unit_type not in ('length', 'wavenumber', 'frequency'):\n        raise exceptions.SynphotError(\n            'wavelength physical type is not length, wave number, or '\n            'frequency: {0}'.format(unit_type))\n\n    return output_unit", "response": "Like validate_unit but specific to wavelength."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a quantity in given unit.", "response": "def validate_quantity(input_value, output_unit, equivalencies=[]):\n    \"\"\"Validate quantity (value and unit).\n\n    .. note::\n\n        For flux conversion, use :func:`convert_flux` instead.\n\n    Parameters\n    ----------\n    input_value : number, array-like, or `~astropy.units.quantity.Quantity`\n        Quantity to validate. If not a Quantity, assumed to be\n        already in output unit.\n\n    output_unit : str or `~astropy.units.core.Unit`\n        Output quantity unit.\n\n    equivalencies : list of equivalence pairs, optional\n        See `astropy.units`.\n\n    Returns\n    -------\n    output_value : `~astropy.units.quantity.Quantity`\n        Validated quantity in given unit.\n\n    \"\"\"\n    output_unit = validate_unit(output_unit)\n\n    if isinstance(input_value, u.Quantity):\n        output_value = input_value.to(output_unit, equivalencies=equivalencies)\n    else:\n        output_value = input_value * output_unit\n\n    return output_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a device to the set.", "response": "def add(self, device):\n        \"\"\"Add device.\"\"\"\n        if not isinstance(device, Device):\n            raise TypeError()\n        self.__devices.append(device)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef data_import(self, json_response):\n        if 'data' not in json_response:\n            raise PyVLXException('no element data found: {0}'.format(\n                json.dumps(json_response)))\n        data = json_response['data']\n\n        for item in data:\n            if 'category' not in item:\n                raise PyVLXException('no element category: {0}'.format(\n                    json.dumps(item)))\n            category = item['category']\n            if category == 'Window opener':\n                self.load_window_opener(item)\n            elif category in ['Roller shutter', 'Dual Shutter']:\n                self.load_roller_shutter(item)\n            elif category in ['Blind']:\n                self.load_blind(item)\n            else:\n                self.pyvlx.logger.warning(\n                    'WARNING: Could not parse product: %s', category)", "response": "Import data from json response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_window_opener(self, item):\n        window = Window.from_config(self.pyvlx, item)\n        self.add(window)", "response": "Load window opener from JSON."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading roller shutter from JSON.", "response": "def load_roller_shutter(self, item):\n        \"\"\"Load roller shutter from JSON.\"\"\"\n        rollershutter = RollerShutter.from_config(self.pyvlx, item)\n        self.add(rollershutter)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a blind from a JSON object.", "response": "def load_blind(self, item):\n        \"\"\"Load blind from JSON.\"\"\"\n        blind = Blind.from_config(self.pyvlx, item)\n        self.add(blind)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the size of the terminal.", "response": "def get_terminal_size(fallback=(80, 24)):\n    \"\"\"\n    Return tuple containing columns and rows of controlling terminal, trying harder\n    than shutil.get_terminal_size to find a tty before returning fallback.\n\n    Theoretically, stdout, stderr, and stdin could all be different ttys that could\n    cause us to get the wrong measurements (instead of using the fallback) but the much more\n    common case is that IO is piped.\n    \"\"\"\n    for stream in [sys.__stdout__, sys.__stderr__, sys.__stdin__]:\n        try:\n            # Make WINSIZE call to terminal\n            data = fcntl.ioctl(stream.fileno(), TIOCGWINSZ, b\"\\x00\\x00\\00\\x00\")\n        except OSError:\n            pass\n        else:\n            # Unpack two shorts from ioctl call\n            lines, columns = struct.unpack(\"hh\", data)\n            break\n    else:\n        columns, lines = fallback\n\n    return columns, lines"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning checks on self. files and print the diff of the styled and unstyled output to stdout.", "response": "def run_diff(self):\n        \"\"\"\n        Run checks on self.files, printing diff of styled/unstyled output to stdout.\n        \"\"\"\n        files = tuple(self.files)\n        # Use same header as more.\n        header, footer = (termcolor.colored(\"{0}\\n{{}}\\n{0}\\n\".format(\n            \":\" * 14), \"cyan\"), \"\\n\") if len(files) > 1 else (\"\", \"\")\n\n        for file in files:\n            print(header.format(file), end=\"\")\n            try:\n                results = self._check(file)\n            except Error as e:\n                termcolor.cprint(e.msg, \"yellow\", file=sys.stderr)\n                continue\n\n            # Display results\n            if results.diffs:\n                print()\n                print(*self.diff(results.original, results.styled), sep=\"\\n\")\n                print()\n                conjunction = \"And\"\n            else:\n                termcolor.cprint(\"Looks good!\", \"green\")\n                conjunction = \"But\"\n\n            if results.diffs:\n                for type, c in sorted(self._warn_chars):\n                    color, verb = (\"on_green\", \"insert\") if type == \"+\" else (\"on_red\", \"delete\")\n                    termcolor.cprint(c, None, color, end=\"\")\n                    termcolor.cprint(\" means that you should {} a {}.\".format(\n                        verb, \"newline\" if c == \"\\\\n\" else \"tab\"), \"yellow\")\n\n            if results.comment_ratio < results.COMMENT_MIN:\n                termcolor.cprint(\"{} consider adding more comments!\".format(conjunction), \"yellow\")\n\n            if (results.comment_ratio < results.COMMENT_MIN or self._warn_chars) and results.diffs:\n                print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_json(self):\n        checks = {}\n        for file in self.files:\n            try:\n                results = self._check(file)\n            except Error as e:\n                checks[file] = {\n                    \"error\": e.msg\n                }\n            else:\n                checks[file] = {\n                    \"score\": results.score,\n                    \"comments\": results.comment_ratio >= results.COMMENT_MIN,\n                    \"diff\": \"<pre>{}</pre>\".format(\"\\n\".join(self.html_diff(results.original, results.styled))),\n                }\n\n        json.dump(checks, sys.stdout, indent=4)\n        print()", "response": "Run checks on self. files printing json object containing information relavent to the CS50 IDE plugin at the end."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_score(self):\n        diffs = 0\n        lines = 0\n        for file in self.files:\n\n            try:\n                results = self._check(file)\n            except Error as e:\n                termcolor.cprint(e.msg, \"yellow\", file=sys.stderr)\n                continue\n\n            diffs += results.diffs\n            lines += results.lines\n\n        try:\n            print(max(1 - diffs / lines, 0.0))\n        except ZeroDivisionError:\n            print(0.0)", "response": "Run checks on self. files and print the score of the checks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning apropriate check based on file s extension and return it if it is otherwise raise an Error", "response": "def _check(self, file):\n        \"\"\"\n        Run apropriate check based on `file`'s extension and return it,\n        otherwise raise an Error\n        \"\"\"\n\n        if not os.path.exists(file):\n            raise Error(\"file \\\"{}\\\" not found\".format(file))\n\n        _, extension = os.path.splitext(file)\n        try:\n            check = self.extension_map[extension[1:]]\n        except KeyError:\n            magic_type = magic.from_file(file)\n            for name, cls in self.magic_map.items():\n                if name in magic_type:\n                    check = cls\n                    break\n            else:\n                raise Error(\"unknown file type \\\"{}\\\", skipping...\".format(file))\n\n        try:\n            with open(file) as f:\n                code = \"\\n\".join(line.rstrip() for line in f)\n        except UnicodeDecodeError:\n            raise Error(\"file does not seem to contain text, skipping...\")\n\n        # Ensure we don't warn about adding trailing newline\n        try:\n            if code[-1] != '\\n':\n                code += '\\n'\n        except IndexError:\n            pass\n\n        return check(code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a generator yielding the side - by - side diff of old and new.", "response": "def split_diff(old, new):\n        \"\"\"\n        Returns a generator yielding the side-by-side diff of `old` and `new`).\n        \"\"\"\n        return map(lambda l: l.rstrip(),\n                   icdiff.ConsoleDiff(cols=COLUMNS).make_table(old.splitlines(), new.splitlines()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a generator yielding a unified diff between old and new.", "response": "def unified(old, new):\n        \"\"\"\n        Returns a generator yielding a unified diff between `old` and `new`.\n        \"\"\"\n        for diff in difflib.ndiff(old.splitlines(), new.splitlines()):\n            if diff[0] == \" \":\n                yield diff\n            elif diff[0] == \"?\":\n                continue\n            else:\n                yield termcolor.colored(diff, \"red\" if diff[0] == \"-\" else \"green\", attrs=[\"bold\"])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef html_diff(self, old, new):\n        def html_transition(old_type, new_type):\n            tags = []\n            for tag in [(\"/\", old_type), (\"\", new_type)]:\n                if tag[1] not in [\"+\", \"-\"]:\n                    continue\n                tags.append(\"<{}{}>\".format(tag[0], \"ins\" if tag[1] == \"+\" else \"del\"))\n            return \"\".join(tags)\n\n        return self._char_diff(old, new, html_transition, fmt=cgi.escape)", "response": "Return HTML formatted character - based diff between old and new."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns color - coded character - based diff between old and new.", "response": "def char_diff(self, old, new):\n        \"\"\"\n        Return color-coded character-based diff between `old` and `new`.\n        \"\"\"\n        def color_transition(old_type, new_type):\n            new_color = termcolor.colored(\"\", None, \"on_red\" if new_type ==\n                                          \"-\" else \"on_green\" if new_type == \"+\" else None)\n            return \"{}{}\".format(termcolor.RESET, new_color[:-len(termcolor.RESET)])\n\n        return self._char_diff(old, new, color_transition)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a char - based diff between old and new where each character is formatted by fmt and transitions between blocks are determined by transition.", "response": "def _char_diff(self, old, new, transition, fmt=lambda c: c):\n        \"\"\"\n        Returns a char-based diff between `old` and `new` where each character\n        is formatted by `fmt` and transitions between blocks are determined by `transition`.\n        \"\"\"\n\n        differ = difflib.ndiff(old, new)\n\n        # Type of difference.\n        dtype = None\n\n        # Buffer for current line.\n        line = []\n        while True:\n            # Get next diff or None if we're at the end.\n            d = next(differ, (None,))\n            if d[0] != dtype:\n                line += transition(dtype, d[0])\n                dtype = d[0]\n\n            if dtype is None:\n                break\n\n            if d[2] == \"\\n\":\n                if dtype != \" \":\n                    self._warn_chars.add((dtype, \"\\\\n\"))\n                    # Show added/removed newlines.\n                    line += [fmt(r\"\\n\"), transition(dtype, \" \")]\n\n                # Don't yield a line if we are removing a newline\n                if dtype != \"-\":\n                    yield \"\".join(line)\n                    line.clear()\n\n                line.append(transition(\" \", dtype))\n            elif dtype != \" \" and d[2] == \"\\t\":\n                # Show added/removed tabs.\n                line.append(fmt(\"\\\\t\"))\n                self._warn_chars.add((dtype, \"\\\\t\"))\n            else:\n                line.append(fmt(d[2]))\n\n        # Flush buffer before quitting.\n        last = \"\".join(line)\n        # Only print last line if it contains non-ANSI characters.\n        if re.sub(r\"\\x1b[^m]*m\", \"\", last):\n            yield last"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count_lines(self, code):\n        return sum(bool(line.strip()) for line in code.splitlines())", "response": "Count the number of lines of code."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun command passing it stdin from input raising a DependencyError if command is not found.", "response": "def run(command, input=None, exit=0, shell=False):\n        \"\"\"\n        Run `command` passing it stdin from `input`, throwing a DependencyError if comand is not found.\n        Throws Error if exit code of command is not `exit` (unless `exit` is None).\n        \"\"\"\n        if isinstance(input, str):\n            input = input.encode()\n\n        # Only pipe stdin if we have input to pipe.\n        stdin = {} if input is None else {\"stdin\": subprocess.PIPE}\n        try:\n            child = subprocess.Popen(command, stdout=subprocess.PIPE,\n                                     stderr=subprocess.PIPE, **stdin)\n        except FileNotFoundError as e:\n            # Extract name of command.\n            name = command.split(' ', 1)[0] if isinstance(command, str) else command[0]\n            raise DependencyError(name)\n\n        stdout, _ = child.communicate(input=input)\n        if exit is not None and child.returncode != exit:\n            raise Error(\"failed to stylecheck code\")\n        return stdout.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frame_from_raw(raw):\n    command, payload = extract_from_frame(raw)\n    frame = create_frame(command)\n    if frame is None:\n        PYVLXLOG.warning(\"Command %s not implemented, raw: %s\", command, \":\".join(\"{:02x}\".format(c) for c in raw))\n        return None\n    frame.validate_payload_len(payload)\n    frame.from_payload(payload)\n    return frame", "response": "Create and return a frame from raw bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_frame(command):\n    # pylint: disable=too-many-branches,too-many-return-statements\n    if command == Command.GW_ERROR_NTF:\n        return FrameErrorNotification()\n    if command == Command.GW_COMMAND_SEND_REQ:\n        return FrameCommandSendRequest()\n    if command == Command.GW_COMMAND_SEND_CFM:\n        return FrameCommandSendConfirmation()\n    if command == Command.GW_COMMAND_RUN_STATUS_NTF:\n        return FrameCommandRunStatusNotification()\n    if command == Command.GW_COMMAND_REMAINING_TIME_NTF:\n        return FrameCommandRemainingTimeNotification()\n    if command == Command.GW_SESSION_FINISHED_NTF:\n        return FrameSessionFinishedNotification()\n\n    if command == Command.GW_PASSWORD_ENTER_REQ:\n        return FramePasswordEnterRequest()\n    if command == Command.GW_PASSWORD_ENTER_CFM:\n        return FramePasswordEnterConfirmation()\n\n    if command == Command.GW_CS_DISCOVER_NODES_REQ:\n        return FrameDiscoverNodesRequest()\n    if command == Command.GW_CS_DISCOVER_NODES_CFM:\n        return FrameDiscoverNodesConfirmation()\n    if command == Command.GW_CS_DISCOVER_NODES_NTF:\n        return FrameDiscoverNodesNotification()\n\n    if command == Command.GW_GET_SCENE_LIST_REQ:\n        return FrameGetSceneListRequest()\n    if command == Command.GW_GET_SCENE_LIST_CFM:\n        return FrameGetSceneListConfirmation()\n    if command == Command.GW_GET_SCENE_LIST_NTF:\n        return FrameGetSceneListNotification()\n\n    if command == Command.GW_GET_NODE_INFORMATION_REQ:\n        return FrameGetNodeInformationRequest()\n    if command == Command.GW_GET_NODE_INFORMATION_CFM:\n        return FrameGetNodeInformationConfirmation()\n    if command == Command.GW_GET_NODE_INFORMATION_NTF:\n        return FrameGetNodeInformationNotification()\n\n    if command == Command.GW_GET_ALL_NODES_INFORMATION_REQ:\n        return FrameGetAllNodesInformationRequest()\n    if command == Command.GW_GET_ALL_NODES_INFORMATION_CFM:\n        return FrameGetAllNodesInformationConfirmation()\n    if command == Command.GW_GET_ALL_NODES_INFORMATION_NTF:\n        return FrameGetAllNodesInformationNotification()\n    if command == Command.GW_GET_ALL_NODES_INFORMATION_FINISHED_NTF:\n        return FrameGetAllNodesInformationFinishedNotification()\n\n    if command == Command.GW_ACTIVATE_SCENE_REQ:\n        return FrameActivateSceneRequest()\n    if command == Command.GW_ACTIVATE_SCENE_CFM:\n        return FrameActivateSceneConfirmation()\n\n    if command == Command.GW_GET_VERSION_REQ:\n        return FrameGetVersionRequest()\n    if command == Command.GW_GET_VERSION_CFM:\n        return FrameGetVersionConfirmation()\n    if command == Command.GW_GET_PROTOCOL_VERSION_REQ:\n        return FrameGetProtocolVersionRequest()\n    if command == Command.GW_GET_PROTOCOL_VERSION_CFM:\n        return FrameGetProtocolVersionConfirmation()\n\n    if command == Command.GW_SET_NODE_NAME_REQ:\n        return FrameSetNodeNameRequest()\n    if command == Command.GW_SET_NODE_NAME_CFM:\n        return FrameSetNodeNameConfirmation()\n\n    if command == Command.GW_NODE_INFORMATION_CHANGED_NTF:\n        return FrameNodeInformationChangedNotification()\n\n    if command == Command.GW_GET_STATE_REQ:\n        return FrameGetStateRequest()\n    if command == Command.GW_GET_STATE_CFM:\n        return FrameGetStateConfirmation()\n\n    if command == Command.GW_SET_UTC_REQ:\n        return FrameSetUTCRequest()\n    if command == Command.GW_SET_UTC_CFM:\n        return FrameSetUTCConfirmation()\n\n    if command == Command.GW_ACTIVATION_LOG_UPDATED_NTF:\n        return FrameActivationLogUpdatedNotification()\n\n    if command == Command.GW_HOUSE_STATUS_MONITOR_ENABLE_REQ:\n        return FrameHouseStatusMonitorEnableRequest()\n    if command == Command.GW_HOUSE_STATUS_MONITOR_ENABLE_CFM:\n        return FrameHouseStatusMonitorEnableConfirmation()\n    if command == Command.GW_HOUSE_STATUS_MONITOR_DISABLE_REQ:\n        return FrameHouseStatusMonitorDisableRequest()\n    if command == Command.GW_HOUSE_STATUS_MONITOR_DISABLE_CFM:\n        return FrameHouseStatusMonitorDisableConfirmation()\n\n    if command == Command.GW_NODE_STATE_POSITION_CHANGED_NTF:\n        return FrameNodeStatePositionChangedNotification()\n\n    return None", "response": "Create and return an empty Frame from a given command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def handle_frame(self, frame):\n        if not isinstance(frame, FramePasswordEnterConfirmation):\n            return False\n        if frame.status == PasswordEnterConfirmationStatus.FAILED:\n            PYVLXLOG.warning('Failed to authenticate with password \"%s****\"', self.password[:2])\n            self.success = False\n        if frame.status == PasswordEnterConfirmationStatus.SUCCESSFUL:\n            self.success = True\n        return True", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the payload of the current record.", "response": "def get_payload(self):\n        \"\"\"Return Payload.\"\"\"\n        return bytes(\n            [self.major_version >> 8 & 255, self.major_version & 255,\n             self.minor_version >> 8 & 255, self.minor_version & 255])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_payload(self, payload):\n        self.major_version = payload[0] * 256 + payload[1]\n        self.minor_version = payload[2] * 256 + payload[3]", "response": "Init frame from binary data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles data received from the broker.", "response": "def data_received(self, data):\n        \"\"\"Handle data received.\"\"\"\n        self.tokenizer.feed(data)\n        while self.tokenizer.has_tokens():\n            raw = self.tokenizer.get_next_token()\n            frame = frame_from_raw(raw)\n            if frame is not None:\n                self.frame_received_cb(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def connect(self):\n        tcp_client = TCPTransport(self.frame_received_cb, self.connection_closed_cb)\n        self.transport, _ = await self.loop.create_connection(\n            lambda: tcp_client,\n            host=self.config.host,\n            port=self.config.port,\n            ssl=self.create_ssl_context())\n        self.connected = True", "response": "Connect to gateway via SSL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a frame to the Bus.", "response": "def write(self, frame):\n        \"\"\"Write frame to Bus.\"\"\"\n        if not isinstance(frame, FrameBase):\n            raise PyVLXException(\"Frame not of type FrameBase\", frame_type=type(frame))\n        PYVLXLOG.debug(\"SEND: %s\", frame)\n        self.transport.write(slip_pack(bytes(frame)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and return an SSL Context.", "response": "def create_ssl_context():\n        \"\"\"Create and return SSL Context.\"\"\"\n        ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        ssl_context.check_hostname = False\n        ssl_context.verify_mode = ssl.CERT_NONE\n        return ssl_context"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef frame_received_cb(self, frame):\n        PYVLXLOG.debug(\"REC: %s\", frame)\n        for frame_received_cb in self.frame_received_cbs:\n            # pylint: disable=not-callable\n            self.loop.create_task(frame_received_cb(frame))", "response": "Called by the application when a message is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates nodes via frame.", "response": "async def process_frame(self, frame):\n        \"\"\"Update nodes via frame, usually received by house monitor.\"\"\"\n        if isinstance(frame, FrameNodeStatePositionChangedNotification):\n            if frame.node_id not in self.pyvlx.nodes:\n                return\n            node = self.pyvlx.nodes[frame.node_id]\n            if isinstance(node, OpeningDevice):\n                node.position = Position(frame.current_position)\n                await node.after_update()\n        elif isinstance(frame, FrameGetAllNodesInformationNotification):\n            if frame.node_id not in self.pyvlx.nodes:\n                return\n            node = self.pyvlx.nodes[frame.node_id]\n            if isinstance(node, OpeningDevice):\n                node.position = Position(frame.current_position)\n                await node.after_update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, scene):\n        if not isinstance(scene, Scene):\n            raise TypeError()\n        for i, j in enumerate(self.__scenes):\n            if j.scene_id == scene.scene_id:\n                self.__scenes[i] = scene\n                return\n        self.__scenes.append(scene)", "response": "Add a scene to the list of scenes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading scenes from KLF 200.", "response": "async def load(self):\n        \"\"\"Load scenes from KLF 200.\"\"\"\n        get_scene_list = GetSceneList(pyvlx=self.pyvlx)\n        await get_scene_list.do_api_call()\n        if not get_scene_list.success:\n            raise PyVLXException(\"Unable to retrieve scene information\")\n        for scene in get_scene_list.scenes:\n            self.add(Scene(pyvlx=self.pyvlx, scene_id=scene[0], name=scene[1]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef best_prefix(bytes, system=NIST):\n    if isinstance(bytes, Bitmath):\n        value = bytes.bytes\n    else:\n        value = bytes\n    return Byte(value).best_prefix(system=system)", "response": "Return a bitmath instance representing the best human - readable version of the number of bytes given by bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a bitmath. Byte instance for the capacity of a system block device.", "response": "def query_device_capacity(device_fd):\n    \"\"\"Create bitmath instances of the capacity of a system block device\n\nMake one or more ioctl request to query the capacity of a block\ndevice. Perform any processing required to compute the final capacity\nvalue. Return the device capacity in bytes as a :class:`bitmath.Byte`\ninstance.\n\nThanks to the following resources for help figuring this out Linux/Mac\nioctl's for querying block device sizes:\n\n* http://stackoverflow.com/a/12925285/263969\n* http://stackoverflow.com/a/9764508/263969\n\n   :param file device_fd: A ``file`` object of the device to query the\n   capacity of (as in ``get_device_capacity(open(\"/dev/sda\"))``).\n\n   :return: a bitmath :class:`bitmath.Byte` instance equivalent to the\n   capacity of the target device in bytes.\n\"\"\"\n    if os_name() != 'posix':\n        raise NotImplementedError(\"'bitmath.query_device_capacity' is not supported on this platform: %s\" % os_name())\n\n    s = os.stat(device_fd.name).st_mode\n    if not stat.S_ISBLK(s):\n        raise ValueError(\"The file descriptor provided is not of a device type\")\n\n    # The keys of the ``ioctl_map`` dictionary correlate to possible\n    # values from the ``platform.system`` function.\n    ioctl_map = {\n        # ioctls for the \"Linux\" platform\n        \"Linux\": {\n            \"request_params\": [\n                # A list of parameters to calculate the block size.\n                #\n                # ( PARAM_NAME , FORMAT_CHAR , REQUEST_CODE )\n                (\"BLKGETSIZE64\", \"L\", 0x80081272)\n                # Per <linux/fs.h>, the BLKGETSIZE64 request returns a\n                # 'u64' sized value. This is an unsigned 64 bit\n                # integer C type. This means to correctly \"buffer\" the\n                # result we need 64 bits, or 8 bytes, of memory.\n                #\n                # The struct module documentation include a reference\n                # chart relating formatting characters to native C\n                # Types. In this case, using the \"native size\", the\n                # table tells us:\n                #\n                # * Character 'L' - Unsigned Long C Type (u64) - Loads into a Python int type\n                #\n                # Confirm this character is right by running (on Linux):\n                #\n                #    >>> import struct\n                #    >>> print 8 == struct.calcsize('L')\n                #\n                # The result should be true as long as your kernel\n                # headers define BLKGETSIZE64 as a u64 type (please\n                # file a bug report at\n                # https://github.com/tbielawa/bitmath/issues/new if\n                # this does *not* work for you)\n            ],\n            # func is how the final result is decided. Because the\n            # Linux BLKGETSIZE64 call returns the block device\n            # capacity in bytes as an integer value, no extra\n            # calculations are required. Simply return the value of\n            # BLKGETSIZE64.\n            \"func\": lambda x: x[\"BLKGETSIZE64\"]\n        },\n        # ioctls for the \"Darwin\" (Mac OS X) platform\n        \"Darwin\": {\n            \"request_params\": [\n                # A list of parameters to calculate the block size.\n                #\n                # ( PARAM_NAME , FORMAT_CHAR , REQUEST_CODE )\n                (\"DKIOCGETBLOCKCOUNT\", \"L\", 0x40086419),\n                # Per <sys/disk.h>: get media's block count - uint64_t\n                #\n                # As in the BLKGETSIZE64 example, an unsigned 64 bit\n                # integer will use the 'L' formatting character\n                (\"DKIOCGETBLOCKSIZE\", \"I\", 0x40046418)\n                # Per <sys/disk.h>: get media's block size - uint32_t\n                #\n                # This request returns an unsigned 32 bit integer, or\n                # in other words: just a normal integer (or 'int' c\n                # type). That should require 4 bytes of space for\n                # buffering. According to the struct modules\n                # 'Formatting Characters' chart:\n                #\n                # * Character 'I' - Unsigned Int C Type (uint32_t) - Loads into a Python int type\n            ],\n            # OS X doesn't have a direct equivalent to the Linux\n            # BLKGETSIZE64 request. Instead, we must request how many\n            # blocks (or \"sectors\") are on the disk, and the size (in\n            # bytes) of each block. Finally, multiply the two together\n            # to obtain capacity:\n            #\n            #                      n Block * y Byte\n            # capacity (bytes)  =            -------\n            #                                1 Block\n            \"func\": lambda x: x[\"DKIOCGETBLOCKCOUNT\"] * x[\"DKIOCGETBLOCKSIZE\"]\n            # This expression simply accepts a dictionary ``x`` as a\n            # parameter, and then returns the result of multiplying\n            # the two named dictionary items together. In this case,\n            # that means multiplying ``DKIOCGETBLOCKCOUNT``, the total\n            # number of blocks, by ``DKIOCGETBLOCKSIZE``, the size of\n            # each block in bytes.\n        }\n    }\n\n    platform_params = ioctl_map[platform.system()]\n    results = {}\n\n    for req_name, fmt, request_code in platform_params['request_params']:\n        # Read the systems native size (in bytes) of this format type.\n        buffer_size = struct.calcsize(fmt)\n        # Construct a buffer to store the ioctl result in\n        buffer = ' ' * buffer_size\n\n        # This code has been ran on only a few test systems. If it's\n        # appropriate, maybe in the future we'll add try/except\n        # conditions for some possible errors. Really only for cases\n        # where it would add value to override the default exception\n        # message string.\n        buffer = fcntl.ioctl(device_fd.fileno(), request_code, buffer)\n\n        # Unpack the raw result from the ioctl call into a familiar\n        # python data type according to the ``fmt`` rules.\n        result = struct.unpack(fmt, buffer)[0]\n        # Add the new result to our collection\n        results[req_name] = result\n\n    return Byte(platform_params['func'](results))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getsize(path, bestprefix=True, system=NIST):\n    _path = os.path.realpath(path)\n    size_bytes = os.path.getsize(_path)\n    if bestprefix:\n        return Byte(size_bytes).best_prefix(system=system)\n    else:\n        return Byte(size_bytes)", "response": "Return a bitmath instance in the best human - readable representation\nof the file size at path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listdir(search_base, followlinks=False, filter='*',\n            relpath=False, bestprefix=False, system=NIST):\n    \"\"\"This is a generator which recurses the directory tree\n`search_base`, yielding 2-tuples of:\n\n* The absolute/relative path to a discovered file\n* A bitmath instance representing the \"apparent size\" of the file.\n\n    - `search_base` - The directory to begin walking down.\n    - `followlinks` - Whether or not to follow symbolic links to directories\n    - `filter` - A glob (see :py:mod:`fnmatch`) to filter results with\n      (default: ``*``, everything)\n    - `relpath` - ``True`` to return the relative path from `pwd` or\n      ``False`` (default) to return the fully qualified path\n    - ``bestprefix`` - set to ``False`` to get ``bitmath.Byte``\n      instances back instead.\n    - `system` - Provide a preferred unit system by setting `system`\n      to either ``bitmath.NIST`` (default) or ``bitmath.SI``.\n\n.. note:: This function does NOT return tuples for directory entities.\n\n.. note:: Symlinks to **files** are followed automatically\n\n    \"\"\"\n    for root, dirs, files in os.walk(search_base, followlinks=followlinks):\n        for name in fnmatch.filter(files, filter):\n            _path = os.path.join(root, name)\n            if relpath:\n                # RELATIVE path\n                _return_path = os.path.relpath(_path, '.')\n            else:\n                # REAL path\n                _return_path = os.path.realpath(_path)\n\n            if followlinks:\n                yield (_return_path, getsize(_path, bestprefix=bestprefix, system=system))\n            else:\n                if os.path.isdir(_path) or os.path.islink(_path):\n                    pass\n                else:\n                    yield (_return_path, getsize(_path, bestprefix=bestprefix, system=system))", "response": "This function returns a generator which yields 2 - tuples for each file in the directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_string(s):\n    # Strings only please\n    if not isinstance(s, (str, unicode)):\n        raise ValueError(\"parse_string only accepts string inputs but a %s was given\" %\n                         type(s))\n\n    # get the index of the first alphabetic character\n    try:\n        index = list([i.isalpha() for i in s]).index(True)\n    except ValueError:\n        # If there's no alphabetic characters we won't be able to .index(True)\n        raise ValueError(\"No unit detected, can not parse string '%s' into a bitmath object\" % s)\n\n    # split the string into the value and the unit\n    val, unit = s[:index], s[index:]\n\n    # see if the unit exists as a type in our namespace\n\n    if unit == \"b\":\n        unit_class = Bit\n    elif unit == \"B\":\n        unit_class = Byte\n    else:\n        if not (hasattr(sys.modules[__name__], unit) and isinstance(getattr(sys.modules[__name__], unit), type)):\n            raise ValueError(\"The unit %s is not a valid bitmath unit\" % unit)\n        unit_class = globals()[unit]\n\n    try:\n        val = float(val)\n    except ValueError:\n        raise\n    try:\n        return unit_class(val)\n    except:  # pragma: no cover\n        raise ValueError(\"Can't parse string %s into a bitmath object\" % s)", "response": "Parse a string with units and try to make a bitmath object out of it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_string_unsafe(s, system=SI):\n    if not isinstance(s, (str, unicode)) and \\\n       not isinstance(s, numbers.Number):\n        raise ValueError(\"parse_string_unsafe only accepts string/number inputs but a %s was given\" %\n                         type(s))\n\n    ######################################################################\n    # Is the input simple to parse? Just a number, or a number\n    # masquerading as a string perhaps?\n\n    # Test case: raw number input (easy!)\n    if isinstance(s, numbers.Number):\n        # It's just a number. Assume bytes\n        return Byte(s)\n\n    # Test case: a number pretending to be a string\n    if isinstance(s, (str, unicode)):\n        try:\n            # Can we turn it directly into a number?\n            return Byte(float(s))\n        except ValueError:\n            # Nope, this is not a plain number\n            pass\n\n    ######################################################################\n    # At this point:\n    # - the input is also not just a number wrapped in a string\n    # - nor is is just a plain number type\n    #\n    # We need to do some more digging around now to figure out exactly\n    # what we were given and possibly normalize the input into a\n    # format we can recognize.\n\n    # First we'll separate the number and the unit.\n    #\n    # Get the index of the first alphabetic character\n    try:\n        index = list([i.isalpha() for i in s]).index(True)\n    except ValueError:  # pragma: no cover\n        # If there's no alphabetic characters we won't be able to .index(True)\n        raise ValueError(\"No unit detected, can not parse string '%s' into a bitmath object\" % s)\n\n    # Split the string into the value and the unit\n    val, unit = s[:index], s[index:]\n\n    # Don't trust anything. We'll make sure the correct 'b' is in place.\n    unit = unit.rstrip('Bb')\n    unit += 'B'\n\n    # At this point we can expect `unit` to be either:\n    #\n    # - 2 Characters (for SI, ex: kB or GB)\n    # - 3 Caracters (so NIST, ex: KiB, or GiB)\n    #\n    # A unit with any other number of chars is not a valid unit\n\n    # SI\n    if len(unit) == 2:\n        # Has NIST parsing been requested?\n        if system == NIST:\n            # NIST units requested. Ensure the unit begins with a\n            # capital letter and is followed by an 'i' character.\n            unit = capitalize_first(unit)\n            # Insert an 'i' char after the first letter\n            _unit = list(unit)\n            _unit.insert(1, 'i')\n            # Collapse the list back into a 3 letter string\n            unit = ''.join(_unit)\n            unit_class = globals()[unit]\n        else:\n            # Default parsing (SI format)\n            #\n            # Edge-case checking: SI 'thousand' is a lower-case K\n            if unit.startswith('K'):\n                unit = unit.replace('K', 'k')\n            elif not unit.startswith('k'):\n                # Otherwise, ensure the first char is capitalized\n                unit = capitalize_first(unit)\n\n            # This is an SI-type unit\n            if unit[0] in SI_PREFIXES:\n                unit_class = globals()[unit]\n    # NIST\n    elif len(unit) == 3:\n        unit = capitalize_first(unit)\n\n        # This is a NIST-type unit\n        if unit[:2] in NIST_PREFIXES:\n            unit_class = globals()[unit]\n    else:\n        # This is not a unit we recognize\n        raise ValueError(\"The unit %s is not a valid bitmath unit\" % unit)\n\n    try:\n        unit_class\n    except UnboundLocalError:\n        raise ValueError(\"The unit %s is not a valid bitmath unit\" % unit)\n\n    return unit_class(float(val))", "response": "Attempt to parse a string with ambiguous units and try to make a base - 2 a\n bitmath object out of it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_script_main(cli_args):\n    choices = ALL_UNIT_TYPES\n\n    parser = argparse.ArgumentParser(\n        description='Converts from one type of size to another.')\n    parser.add_argument('--from-stdin', default=False, action='store_true',\n                        help='Reads number from stdin rather than the cli')\n    parser.add_argument(\n        '-f', '--from', choices=choices, nargs=1,\n        type=str, dest='fromunit', default=['Byte'],\n        help='Input type you are converting from. Defaultes to Byte.')\n    parser.add_argument(\n        '-t', '--to', choices=choices, required=False, nargs=1, type=str,\n        help=('Input type you are converting to. '\n              'Attempts to detect best result if omitted.'), dest='tounit')\n    parser.add_argument(\n        'size', nargs='*', type=float,\n        help='The number to convert.')\n\n    args = parser.parse_args(cli_args)\n\n    # Not sure how to cover this with tests, or if the functionality\n    # will remain in this form long enough for it to make writing a\n    # test worth the effort.\n    if args.from_stdin:  # pragma: no cover\n        args.size = [float(sys.stdin.readline()[:-1])]\n\n    results = []\n\n    for size in args.size:\n        instance = getattr(__import__(\n            'bitmath', fromlist=['True']), args.fromunit[0])(size)\n\n        # If we have a unit provided then use it\n        if args.tounit:\n            result = getattr(instance, args.tounit[0])\n        # Otherwise use the best_prefix call\n        else:\n            result = instance.best_prefix()\n\n        results.append(result)\n\n    return results", "response": "A command line interface to basic bitmath operations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_setup(self):\n        (self._base, self._power, self._name_singular, self._name_plural) = self._setup()\n        self._unit_value = self._base ** self._power", "response": "Setup basic parameters for this class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnormalize the input value into the fundamental unit for this prefix type.", "response": "def _norm(self, value):\n        \"\"\"Normalize the input value into the fundamental unit for this prefix\ntype.\n\n   :param number value: The input value to be normalized\n   :raises ValueError: if the input value is not a type of real number\n\"\"\"\n        if isinstance(value, self.valid_types):\n            self._byte_value = value * self._unit_value\n            self._bit_value = self._byte_value * 8.0\n        else:\n            raise ValueError(\"Initialization value '%s' is of an invalid type: %s. \"\n                             \"Must be one of %s\" % (\n                                 value,\n                                 type(value),\n                                 \", \".join(str(x) for x in self.valid_types)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef system(self):\n        if self._base == 2:\n            return \"NIST\"\n        elif self._base == 10:\n            return \"SI\"\n        else:\n            # I don't expect to ever encounter this logic branch, but\n            # hey, it's better to have extra test coverage than\n            # insufficient test coverage.\n            raise ValueError(\"Instances mathematical base is an unsupported value: %s\" % (\n                str(self._base)))", "response": "The system of units used to measure an instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unit(self):\n        global format_plural\n\n        if self.prefix_value == 1:\n            # If it's a '1', return it singular, no matter what\n            return self._name_singular\n        elif format_plural:\n            # Pluralization requested\n            return self._name_plural\n        else:\n            # Pluralization NOT requested, and the value is not 1\n            return self._name_singular", "response": "Returns the string that is the unit name of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_other(cls, item):\n        if isinstance(item, Bitmath):\n            return cls(bits=item.bits)\n        else:\n            raise ValueError(\"The provided items must be a valid bitmath class: %s\" %\n                             str(item.__class__))", "response": "Factory function to return instances of item converted into a new\ninstance of cls."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a representation of this instance formatted with user supplied syntax", "response": "def format(self, fmt):\n        \"\"\"Return a representation of this instance formatted with user\nsupplied syntax\"\"\"\n        _fmt_params = {\n            'base': self.base,\n            'bin': self.bin,\n            'binary': self.binary,\n            'bits': self.bits,\n            'bytes': self.bytes,\n            'power': self.power,\n            'system': self.system,\n            'unit': self.unit,\n            'unit_plural': self.unit_plural,\n            'unit_singular': self.unit_singular,\n            'value': self.value\n        }\n\n        return fmt.format(**_fmt_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the best prefix for the current instance.", "response": "def best_prefix(self, system=None):\n        \"\"\"Optional parameter, `system`, allows you to prefer NIST or SI in\nthe results. By default, the current system is used (Bit/Byte default\nto NIST).\n\nLogic discussion/notes:\n\nBase-case, does it need converting?\n\nIf the instance is less than one Byte, return the instance as a Bit\ninstance.\n\nElse, begin by recording the unit system the instance is defined\nby. This determines which steps (NIST_STEPS/SI_STEPS) we iterate over.\n\nIf the instance is not already a ``Byte`` instance, convert it to one.\n\nNIST units step up by powers of 1024, SI units step up by powers of\n1000.\n\nTake integer value of the log(base=STEP_POWER) of the instance's byte\nvalue. E.g.:\n\n    >>> int(math.log(Gb(100).bytes, 1000))\n    3\n\nThis will return a value >= 0. The following determines the 'best\nprefix unit' for representation:\n\n* result == 0, best represented as a Byte\n* result >= len(SYSTEM_STEPS), best represented as an Exbi/Exabyte\n* 0 < result < len(SYSTEM_STEPS), best represented as SYSTEM_PREFIXES[result-1]\n\n        \"\"\"\n\n        # Use absolute value so we don't return Bit's for *everything*\n        # less than Byte(1). From github issue #55\n        if abs(self) < Byte(1):\n            return Bit.from_other(self)\n        else:\n            if type(self) is Byte:  # pylint: disable=unidiomatic-typecheck\n                _inst = self\n            else:\n                _inst = Byte.from_other(self)\n\n        # Which table to consult? Was a preferred system provided?\n        if system is None:\n            # No preference. Use existing system\n            if self.system == 'NIST':\n                _STEPS = NIST_PREFIXES\n                _BASE = 1024\n            elif self.system == 'SI':\n                _STEPS = SI_PREFIXES\n                _BASE = 1000\n            # Anything else would have raised by now\n        else:\n            # Preferred system provided.\n            if system == NIST:\n                _STEPS = NIST_PREFIXES\n                _BASE = 1024\n            elif system == SI:\n                _STEPS = SI_PREFIXES\n                _BASE = 1000\n            else:\n                raise ValueError(\"Invalid value given for 'system' parameter.\"\n                                 \" Must be one of NIST or SI\")\n\n        # Index of the string of the best prefix in the STEPS list\n        _index = int(math.log(abs(_inst.bytes), _BASE))\n\n        # Recall that the log() function returns >= 0. This doesn't\n        # map to the STEPS list 1:1. That is to say, 0 is handled with\n        # special care. So if the _index is 1, we actually want item 0\n        # in the list.\n\n        if _index == 0:\n            # Already a Byte() type, so return it.\n            return _inst\n        elif _index >= len(_STEPS):\n            # This is a really big number. Use the biggest prefix we've got\n            _best_prefix = _STEPS[-1]\n        elif 0 < _index < len(_STEPS):\n            # There is an appropriate prefix unit to represent this\n            _best_prefix = _STEPS[_index - 1]\n\n        _conversion_method = getattr(\n            self,\n            'to_%sB' % _best_prefix)\n\n        return _conversion_method()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize the input value into the fundamental unit for this prefix type", "response": "def _norm(self, value):\n        \"\"\"Normalize the input value into the fundamental unit for this prefix\ntype\"\"\"\n        self._bit_value = value * self._unit_value\n        self._byte_value = self._bit_value / 8.0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_crc(raw):\n    crc = 0\n    for sym in raw:\n        crc = crc ^ int(sym)\n    return crc", "response": "Calculate cyclic redundancy check ( CRC."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts payload and command from frame.", "response": "def extract_from_frame(data):\n    \"\"\"Extract payload and command from frame.\"\"\"\n    if len(data) <= 4:\n        raise PyVLXException(\"could_not_extract_from_frame_too_short\", data=data)\n    length = data[0] * 256 + data[1] - 1\n    if len(data) != length + 3:\n        raise PyVLXException(\"could_not_extract_from_frame_invalid_length\", data=data, current_length=len(data), expected_length=length + 3)\n    if calc_crc(data[:-1]) != data[-1]:\n        raise PyVLXException(\"could_not_extract_from_frame_invalid_crc\", data=data, expected_crc=calc_crc(data[:-1]), current_crc=data[-1])\n    payload = data[4:-1]\n    try:\n        command = Command(data[2] * 256 + data[3])\n    except ValueError:\n        raise PyVLXException(\"could_not_extract_from_frame_command\", data=data)\n    return command, payload"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_payload(self):\n        payload = bytes([self.node_id])\n        payload += string_to_bytes(self.name, 64)\n        payload += bytes([self.order >> 8 & 255, self.order & 255])\n        payload += bytes([self.placement])\n        payload += bytes([self.node_variation.value])\n        return payload", "response": "Return the payload of the node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_payload(self, payload):\n        self.status = NodeInformationStatus(payload[0])\n        self.node_id = payload[1]", "response": "Init frame from binary data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_meta(obj):\n        if hasattr(obj, 'meta'):  # Spectrum or model\n            meta = deepcopy(obj.meta)\n        elif isinstance(obj, dict):  # Metadata\n            meta = deepcopy(obj)\n        else:  # Number\n            meta = {}\n        return meta", "response": "Extract metadata if any from given object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmerging metadata from left and right onto result.", "response": "def _merge_meta(left, right, result, clean=True):\n        \"\"\"Merge metadata from left and right onto results.\n\n        This is used during class initialization.\n        This should also be used by operators to merge metadata after\n        creating a new instance but before returning it.\n        Result's metadata is modified in-place.\n\n        Parameters\n        ----------\n        left, right : number, `BaseSpectrum`, or `~astropy.modeling.models`\n            Inputs of an operation.\n\n        result : `BaseSpectrum`\n            Output spectrum object.\n\n        clean : bool\n            Remove ``'header'`` and ``'expr'`` entries from inputs.\n\n        \"\"\"\n        # Copies are returned because they need some clean-up below.\n        left = BaseSpectrum._get_meta(left)\n        right = BaseSpectrum._get_meta(right)\n\n        # Remove these from going into result to avoid mess.\n        #   header = FITS header metadata\n        #   expr = ASTROLIB PYSYNPHOT expression\n        if clean:\n            for key in ('header', 'expr'):\n                for d in (left, right):\n                    if key in d:\n                        del d[key]\n\n        mid = metadata.merge(left, right, metadata_conflicts='silent')\n        result.meta = metadata.merge(result.meta, mid,\n                                     metadata_conflicts='silent')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_generic_param(pval, def_unit, equivalencies=[]):\n        if isinstance(pval, u.Quantity):\n            outval = pval.to(def_unit, equivalencies).value\n        else:  # Assume already in desired unit\n            outval = pval\n        return outval", "response": "Process generic model parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_wave_param(self, pval):\n        return self._process_generic_param(\n            pval, self._internal_wave_unit, equivalencies=u.spectral())", "response": "Process individual model parameter representing wavelength."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate wavelengths for sampling.", "response": "def _validate_wavelengths(self, wave):\n        \"\"\"Validate wavelengths for sampling.\"\"\"\n        if wave is None:\n            if self.waveset is None:\n                raise exceptions.SynphotError(\n                    'self.waveset is undefined; '\n                    'Provide wavelengths for sampling.')\n            wavelengths = self.waveset\n        else:\n            w = self._process_wave_param(wave)\n            utils.validate_wavelengths(w)\n            wavelengths = w * self._internal_wave_unit\n\n        return wavelengths"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconditions for other to satisfy before mul / div.", "response": "def _validate_other_mul_div(other):\n        \"\"\"Conditions for other to satisfy before mul/div.\"\"\"\n        if not isinstance(other, (u.Quantity, numbers.Number,\n                                  BaseUnitlessSpectrum, SourceSpectrum)):\n            raise exceptions.IncompatibleSources(\n                'Can only operate on scalar number/Quantity or spectrum')\n        elif (isinstance(other, u.Quantity) and\n              (other.unit.decompose() != u.dimensionless_unscaled or\n               not np.isscalar(other.value) or\n               not isinstance(other.value, numbers.Real))):\n            raise exceptions.IncompatibleSources(\n                'Can only operate on real scalar dimensionless Quantity')\n        elif (isinstance(other, numbers.Number) and\n              not (np.isscalar(other) and isinstance(other, numbers.Real))):\n            raise exceptions.IncompatibleSources(\n                'Can only operate on real scalar number')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nintegrate the entry in the current configuration item with the given wavelengths.", "response": "def integrate(self, wavelengths=None, **kwargs):\n        \"\"\"Perform integration.\n\n        This uses any analytical integral that the\n        underlying model has (i.e., ``self.model.integral``).\n        If unavailable, it uses the default fall-back integrator\n        set in the ``default_integrator`` configuration item.\n\n        If wavelengths are provided, flux or throughput is first resampled.\n        This is useful when user wants to integrate at specific end points\n        or use custom spacing; In that case, user can pass in desired\n        sampling array generated with :func:`numpy.linspace`,\n        :func:`numpy.logspace`, etc.\n        If not provided, then `waveset` is used.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for integration.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        kwargs : dict\n            Optional keywords to ``__call__`` for sampling.\n\n        Returns\n        -------\n        result : `~astropy.units.quantity.Quantity`\n            Integrated result.\n\n        Raises\n        ------\n        NotImplementedError\n            Invalid default integrator.\n\n        synphot.exceptions.SynphotError\n            `waveset` is needed but undefined or cannot integrate\n            natively in the given ``flux_unit``.\n\n        \"\"\"\n        # Cannot integrate per Hz units natively across wavelength\n        # without converting them to per Angstrom unit first, so\n        # less misleading to just disallow that option for now.\n        if 'flux_unit' in kwargs:\n            self._validate_flux_unit(kwargs['flux_unit'], wav_only=True)\n\n        x = self._validate_wavelengths(wavelengths)\n\n        # TODO: When astropy.modeling.models supports this, need to\n        #       make sure that this actually works, and gives correct unit.\n        # https://github.com/astropy/astropy/issues/5033\n        # https://github.com/astropy/astropy/pull/5108\n        try:\n            m = self.model.integral\n        except (AttributeError, NotImplementedError):\n            if conf.default_integrator == 'trapezoid':\n                y = self(x, **kwargs)\n                result = abs(np.trapz(y.value, x=x.value))\n                result_unit = y.unit\n            else:  # pragma: no cover\n                raise NotImplementedError(\n                    'Analytic integral not available and default integrator '\n                    '{0} is not supported'.format(conf.default_integrator))\n        else:  # pragma: no cover\n            start = x[0].value\n            stop = x[-1].value\n            result = (m(stop) - m(start))\n            result_unit = self._internal_flux_unit\n\n        # Ensure final unit takes account of integration across wavelength\n        if result_unit != units.THROUGHPUT:\n            if result_unit == units.PHOTLAM:\n                result_unit = u.photon / (u.cm**2 * u.s)\n            elif result_unit == units.FLAM:\n                result_unit = u.erg / (u.cm**2 * u.s)\n            else:  # pragma: no cover\n                raise NotImplementedError(\n                    'Integration of {0} is not supported'.format(result_unit))\n        else:\n            # Ideally flux can use this too but unfortunately this\n            # operation results in confusing output unit for flux.\n            result_unit *= self._internal_wave_unit\n\n        return result * result_unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef avgwave(self, wavelengths=None):\n        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n        num = np.trapz(y * x, x=x)\n        den = np.trapz(y, x=x)\n\n        if den == 0:  # pragma: no cover\n            avg_wave = 0.0\n        else:\n            avg_wave = abs(num / den)\n\n        return avg_wave * self._internal_wave_unit", "response": "Calculate the average wavelength of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef barlam(self, wavelengths=None):\n        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n        num = np.trapz(y * np.log(x) / x, x=x)\n        den = np.trapz(y / x, x=x)\n\n        if num == 0 or den == 0:  # pragma: no cover\n            bar_lam = 0.0\n        else:\n            bar_lam = np.exp(abs(num / den))\n\n        return bar_lam * self._internal_wave_unit", "response": "Calculate the mean log wavelength of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pivot(self, wavelengths=None):\n        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n        num = np.trapz(y * x, x=x)\n        den = np.trapz(y / x, x=x)\n\n        if den == 0:  # pragma: no cover\n            pivwv = 0.0\n        else:\n            pivwv = np.sqrt(abs(num / den))\n\n        return pivwv * self._internal_wave_unit", "response": "Calculate the pivot of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef force_extrapolation(self):\n        # We use _model here in case the spectrum is redshifted.\n        if isinstance(self._model, Empirical1D):\n            self._model.fill_value = np.nan\n            is_forced = True\n        else:\n            is_forced = False\n\n        return is_forced", "response": "Force the underlying model to extrapolate based on nearest point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntapers the empirical spectrum or bandpass.", "response": "def taper(self, wavelengths=None):\n        \"\"\"Taper the spectrum or bandpass.\n\n        The wavelengths to use for the first and last points are\n        calculated by using the same ratio as for the 2 interior points.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for tapering.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        Returns\n        -------\n        sp : `BaseSpectrum`\n            Tapered empirical spectrum or bandpass.\n            ``self`` is returned if already tapered (e.g., box model).\n\n        \"\"\"\n        x = self._validate_wavelengths(wavelengths)\n\n        # Calculate new end points for tapering\n        w1 = x[0] ** 2 / x[1]\n        w2 = x[-1] ** 2 / x[-2]\n\n        # Special handling for empirical data.\n        # This is to be compatible with ASTROLIB PYSYNPHOT behavior.\n        if isinstance(self._model, Empirical1D):\n            y1 = self._model.lookup_table[0]\n            y2 = self._model.lookup_table[-1]\n        # Other models can just evaluate at new end points\n        else:\n            y1 = self(w1)\n            y2 = self(w2)\n\n        # Nothing to do\n        if y1 == 0 and y2 == 0:\n            return self  # Do we need a deepcopy here?\n\n        y = self(x)\n\n        if y1 != 0:\n            x = np.insert(x, 0, w1)\n            y = np.insert(y, 0, 0.0)\n        if y2 != 0:\n            x = np.insert(x, x.size, w2)\n            y = np.insert(y, y.size, 0.0)\n\n        return self.__class__(Empirical1D, points=x, lookup_table=y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting sampled spectrum or bandpass in user units.", "response": "def _get_arrays(self, wavelengths, **kwargs):\n        \"\"\"Get sampled spectrum or bandpass in user units.\"\"\"\n        x = self._validate_wavelengths(wavelengths)\n        y = self(x, **kwargs)\n\n        if isinstance(wavelengths, u.Quantity):\n            w = x.to(wavelengths.unit, u.spectral())\n        else:\n            w = x\n\n        return w, y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _do_plot(x, y, title='', xlog=False, ylog=False,\n                 left=None, right=None, bottom=None, top=None,\n                 save_as=''):  # pragma: no cover\n        \"\"\"Plot worker.\n\n        Parameters\n        ----------\n        x, y : `~astropy.units.quantity.Quantity`\n            Wavelength and flux/throughput to plot.\n\n        kwargs\n            See :func:`plot`.\n\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:\n            log.error('No matplotlib installation found; plotting disabled '\n                      'as a result.')\n            return\n\n        fig, ax = plt.subplots()\n        ax.plot(x, y)\n\n        # Custom wavelength limits\n        if left is not None:\n            ax.set_xlim(left=left)\n        if right is not None:\n            ax.set_xlim(right=right)\n\n        # Custom flux/throughput limit\n        if bottom is not None:\n            ax.set_ylim(bottom=bottom)\n        if top is not None:\n            ax.set_ylim(top=top)\n\n        xu = x.unit\n        if xu.physical_type == 'frequency':\n            ax.set_xlabel('Frequency ({0})'.format(xu))\n        else:\n            ax.set_xlabel('Wavelength ({0})'.format(xu))\n\n        yu = y.unit\n        if yu is u.dimensionless_unscaled:\n            ax.set_ylabel('Unitless')\n        else:\n            ax.set_ylabel('Flux ({0})'.format(yu))\n\n        if title:\n            ax.set_title(title)\n\n        if xlog:\n            ax.set_xscale('log')\n        if ylog:\n            ax.set_yscale('log')\n\n        plt.draw()\n\n        if save_as:\n            plt.savefig(save_as)\n            log.info('Plot saved as {0}'.format(save_as))", "response": "Plots the wavelength and flux and throughput of a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(self, wavelengths=None, **kwargs):  # pragma: no cover\n        w, y = self._get_arrays(wavelengths)\n        self._do_plot(w, y, **kwargs)", "response": "Plot the spectrum.\n\n        .. note:: Uses ``matplotlib``.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        title : str\n            Plot title.\n\n        xlog, ylog : bool\n            Plot X and Y axes, respectively, in log scale.\n            Default is linear scale.\n\n        left, right : `None` or number\n            Minimum and maximum wavelengths to plot.\n            If `None`, uses the whole range. If a number is given,\n            must be in Angstrom.\n\n        bottom, top : `None` or number\n            Minimum and maximum flux/throughput to plot.\n            If `None`, uses the whole range. If a number is given,\n            must be in internal unit.\n\n        save_as : str\n            Save the plot to an image file. The file type is\n            automatically determined by given file extension.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the flux unit.", "response": "def _validate_flux_unit(new_unit, wav_only=False):\n        \"\"\"Make sure flux unit is valid.\"\"\"\n        new_unit = units.validate_unit(new_unit)\n        acceptable_types = ['spectral flux density wav',\n                            'photon flux density wav']\n        acceptable_names = ['PHOTLAM', 'FLAM']\n\n        if not wav_only:  # Include per Hz units\n            acceptable_types += ['spectral flux density',\n                                 'photon flux density']\n            acceptable_names += ['PHOTNU', 'FNU', 'Jy']\n\n        if new_unit.physical_type not in acceptable_types:\n            raise exceptions.SynphotError(\n                'Source spectrum cannot operate in {0}. Acceptable units: '\n                '{1}'.format(new_unit, ','.join(acceptable_names)))\n\n        return new_unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes the current object to the given Quantity and band.", "response": "def normalize(self, renorm_val, band=None, wavelengths=None, force=False,\n                  area=None, vegaspec=None):\n        \"\"\"Renormalize the spectrum to the given Quantity and band.\n\n        .. warning::\n\n            Redshift attribute (``z``) is reset to 0 in the normalized\n            spectrum even if ``self.z`` is non-zero.\n            This is because the normalization simply adds a scale\n            factor to the existing composite model.\n            This is confusing but should not affect the flux sampling.\n\n        Parameters\n        ----------\n        renorm_val : number or `~astropy.units.quantity.Quantity`\n            Value to renormalize the spectrum to. If not a Quantity,\n            assumed to be in internal unit.\n\n        band : `SpectralElement`\n           Bandpass to use in renormalization.\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for renormalization.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        force : bool\n            By default (`False`), renormalization is only done\n            when band wavelength limits are within ``self``\n            or at least 99% of the flux is within the overlap.\n            Set to `True` to force renormalization for partial overlap\n            (this changes the underlying model of ``self`` to always\n            extrapolate, if applicable).\n            Disjoint bandpass raises an exception regardless.\n\n        area, vegaspec\n            See :func:`~synphot.units.convert_flux`.\n\n        Returns\n        -------\n        sp : obj\n            Renormalized spectrum.\n\n        Raises\n        ------\n        synphot.exceptions.DisjointError\n            Renormalization band does not overlap with ``self``.\n\n        synphot.exceptions.PartialOverlap\n            Renormalization band only partially overlaps with ``self``\n            and significant amount of flux falls outside the overlap.\n\n        synphot.exceptions.SynphotError\n            Invalid inputs or calculation failed.\n\n        \"\"\"\n        warndict = {}\n\n        if band is None:\n            sp = self\n\n        else:\n            if not isinstance(band, SpectralElement):\n                raise exceptions.SynphotError('Invalid bandpass.')\n\n            stat = band.check_overlap(self, wavelengths=wavelengths)\n\n            if stat == 'none':\n                raise exceptions.DisjointError(\n                    'Spectrum and renormalization band are disjoint.')\n\n            elif 'partial' in stat:\n                if stat == 'partial_most':\n                    warn_str = 'At least'\n                elif stat == 'partial_notmost' and force:\n                    warn_str = 'Less than'\n                else:\n                    raise exceptions.PartialOverlap(\n                        'Spectrum and renormalization band do not fully '\n                        'overlap. You may use force=True to force the '\n                        'renormalization to proceed.')\n\n                warn_str = (\n                    'Spectrum is not defined everywhere in renormalization '\n                    'bandpass. {0} 99% of the band throughput has '\n                    'data. Spectrum will be').format(warn_str)\n\n                if self.force_extrapolation():\n                    warn_str = ('{0} extrapolated at constant '\n                                'value.').format(warn_str)\n                else:\n                    warn_str = ('{0} evaluated outside pre-defined '\n                                'waveset.').format(warn_str)\n\n                warnings.warn(warn_str, AstropyUserWarning)\n                warndict['PartialRenorm'] = warn_str\n\n            elif stat != 'full':  # pragma: no cover\n                raise exceptions.SynphotError(\n                    'Overlap result of {0} is unexpected.'.format(stat))\n\n            sp = self.__mul__(band)\n\n        if not isinstance(renorm_val, u.Quantity):\n            renorm_val = renorm_val * self._internal_flux_unit\n\n        renorm_unit_name = renorm_val.unit.to_string()\n        w = sp._validate_wavelengths(wavelengths)\n\n        if (renorm_val.unit == u.count or\n                renorm_unit_name == units.OBMAG.to_string()):\n            # Special handling for non-density units\n            flux_tmp = sp(w, flux_unit=u.count, area=area)\n            totalflux = flux_tmp.sum().value\n            stdflux = 1.0\n        else:\n            totalflux = sp.integrate(wavelengths=wavelengths).value\n\n            # VEGAMAG\n            if renorm_unit_name == units.VEGAMAG.to_string():\n                if not isinstance(vegaspec, SourceSpectrum):\n                    raise exceptions.SynphotError(\n                        'Vega spectrum is missing.')\n                stdspec = vegaspec\n\n            # Magnitude flux-density units\n            elif renorm_val.unit in (u.STmag, u.ABmag):\n                stdspec = SourceSpectrum(\n                    ConstFlux1D, amplitude=(0 * renorm_val.unit))\n\n            # Linear flux-density units\n            else:\n                stdspec = SourceSpectrum(\n                    ConstFlux1D, amplitude=(1 * renorm_val.unit))\n\n            if band is None:\n                # TODO: Cannot get this to agree with results\n                # from using a very large box bandpass.\n                # stdflux = stdspec.integrate(wavelengths=w).value\n                raise NotImplementedError('Must provide a bandpass')\n            else:\n                up = stdspec * band\n                stdflux = up.integrate(wavelengths=wavelengths).value\n\n        utils.validate_totalflux(totalflux)\n\n        # Renormalize in magnitudes\n        if (renorm_val.unit.decompose() == u.mag or\n                isinstance(renorm_val.unit, u.LogUnit)):\n            const = renorm_val.value + (2.5 *\n                                        np.log10(totalflux / stdflux))\n            newsp = self.__mul__(10**(-0.4 * const))\n        # Renormalize in linear flux units\n        else:\n            const = renorm_val.value * (stdflux / totalflux)\n            newsp = self.__mul__(const)\n\n        newsp.warnings = warndict\n        return newsp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing individual model parameter representing flux.", "response": "def _process_flux_param(self, pval, wave):\n        \"\"\"Process individual model parameter representing flux.\"\"\"\n        if isinstance(pval, u.Quantity):\n            self._validate_flux_unit(pval.unit)\n            outval = units.convert_flux(self._redshift_model(wave), pval,\n                                        self._internal_flux_unit).value\n        else:  # Assume already in internal unit\n            outval = pval\n        return outval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef model(self):\n        if self.z == 0:\n            m = self._model\n        else:\n            # wavelength\n            if self._internal_wave_unit.physical_type == 'length':\n                rs = self._redshift_model.inverse\n            # frequency or wavenumber\n            # NOTE: This will never execute as long as internal wavelength\n            #       unit remains Angstrom.\n            else:  # pragma: no cover\n                rs = self._redshift_model\n\n            if self.z_type == 'wavelength_only':\n                m = rs | self._model\n            else:  # conserve_flux\n                m = rs | self._model | self._redshift_flux_model\n\n        return m", "response": "Model of the spectrum with given redshift."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconditions for other to satisfy before add or sub.", "response": "def _validate_other_add_sub(self, other):\n        \"\"\"Conditions for other to satisfy before add/sub.\"\"\"\n        if not isinstance(other, self.__class__):\n            raise exceptions.IncompatibleSources(\n                'Can only operate on {0}.'.format(self.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the spectrum. .. note:: Uses :mod:`matplotlib`. Parameters ---------- wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None` Wavelength values for integration. If not a Quantity, assumed to be in Angstrom. If `None`, ``self.waveset`` is used. flux_unit : str or `~astropy.units.core.Unit` or `None` Flux is converted to this unit for plotting. If not given, internal unit is used. area, vegaspec See :func:`~synphot.units.convert_flux`. kwargs : dict See :func:`BaseSpectrum.plot`. Raises ------ synphot.exceptions.SynphotError Invalid inputs.", "response": "def plot(self, wavelengths=None, flux_unit=None, area=None, vegaspec=None,\n             **kwargs):  # pragma: no cover\n        \"\"\"Plot the spectrum.\n\n        .. note:: Uses :mod:`matplotlib`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for integration.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        flux_unit : str or `~astropy.units.core.Unit` or `None`\n            Flux is converted to this unit for plotting.\n            If not given, internal unit is used.\n\n        area, vegaspec\n            See :func:`~synphot.units.convert_flux`.\n\n        kwargs : dict\n            See :func:`BaseSpectrum.plot`.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid inputs.\n\n        \"\"\"\n        w, y = self._get_arrays(wavelengths, flux_unit=flux_unit, area=area,\n                                vegaspec=vegaspec)\n        self._do_plot(w, y, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the spectrum to a FITS file.", "response": "def to_fits(self, filename, wavelengths=None, flux_unit=None, area=None,\n                vegaspec=None, **kwargs):\n        \"\"\"Write the spectrum to a FITS file.\n\n        Parameters\n        ----------\n        filename : str\n            Output filename.\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        flux_unit : str or `~astropy.units.core.Unit` or `None`\n            Flux is converted to this unit before written out.\n            If not given, internal unit is used.\n\n        area, vegaspec\n            See :func:`~synphot.units.convert_flux`.\n\n        kwargs : dict\n            Keywords accepted by :func:`~synphot.specio.write_fits_spec`.\n\n        \"\"\"\n        w, y = self._get_arrays(wavelengths, flux_unit=flux_unit, area=area,\n                                vegaspec=vegaspec)\n\n        # There are some standard keywords that should be added\n        # to the extension header.\n        bkeys = {'tdisp1': 'G15.7', 'tdisp2': 'G15.7'}\n\n        if 'expr' in self.meta:\n            bkeys['expr'] = (self.meta['expr'], 'synphot expression')\n\n        if 'ext_header' in kwargs:\n            kwargs['ext_header'].update(bkeys)\n        else:\n            kwargs['ext_header'] = bkeys\n\n        specio.write_fits_spec(filename, w, y, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a source spectrum from a file.", "response": "def from_file(cls, filename, keep_neg=False, **kwargs):\n        \"\"\"Create a spectrum from file.\n\n        If filename has 'fits' or 'fit' suffix, it is read as FITS.\n        Otherwise, it is read as ASCII.\n\n        Parameters\n        ----------\n        filename : str\n            Spectrum filename.\n\n        keep_neg : bool\n            See `~synphot.models.Empirical1D`.\n\n        kwargs : dict\n            Keywords acceptable by\n            :func:`~synphot.specio.read_fits_spec` (if FITS) or\n            :func:`~synphot.specio.read_ascii_spec` (if ASCII).\n\n        Returns\n        -------\n        sp : `SourceSpectrum`\n            Empirical spectrum.\n\n        \"\"\"\n        header, wavelengths, fluxes = specio.read_spec(filename, **kwargs)\n        return cls(Empirical1D, points=wavelengths, lookup_table=fluxes,\n                   keep_neg=keep_neg, meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a new object from a vegaspec.", "response": "def from_vega(cls, **kwargs):\n        \"\"\"Load :ref:`Vega spectrum <synphot-vega-spec>`.\n\n        Parameters\n        ----------\n        kwargs : dict\n            Keywords acceptable by :func:`~synphot.specio.read_remote_spec`.\n\n        Returns\n        -------\n        vegaspec : `SourceSpectrum`\n            Empirical Vega spectrum.\n\n        \"\"\"\n        filename = conf.vega_file\n        header, wavelengths, fluxes = specio.read_remote_spec(\n            filename, **kwargs)\n        header['filename'] = filename\n        meta = {'header': header,\n                'expr': 'Vega from {0}'.format(os.path.basename(filename))}\n        return cls(Empirical1D, points=wavelengths, lookup_table=fluxes,\n                   meta=meta)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake sure the flux unit is dimensionless.", "response": "def _validate_flux_unit(new_unit):  # pragma: no cover\n        \"\"\"Make sure flux unit is valid.\"\"\"\n        new_unit = units.validate_unit(new_unit)\n\n        if new_unit.decompose() != u.dimensionless_unscaled:\n            raise exceptions.SynphotError(\n                'Unit {0} is not dimensionless'.format(new_unit))\n\n        return new_unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_overlap(self, other, wavelengths=None, threshold=0.01):\n        if not isinstance(other, BaseSpectrum):\n            raise exceptions.SynphotError(\n                'other must be spectrum or bandpass.')\n\n        # Special cases where no sampling wavelengths given and\n        # one of the inputs is continuous.\n        if wavelengths is None:\n            if other.waveset is None:\n                return 'full'\n            if self.waveset is None:\n                return 'partial_notmost'\n\n        x1 = self._validate_wavelengths(wavelengths)\n        y1 = self(x1)\n        a = x1[y1 > 0].value\n\n        b = other._validate_wavelengths(wavelengths).value\n\n        result = utils.overlap_status(a, b)\n\n        if result == 'partial':\n            # If there is no need to extrapolate or taper other\n            # (i.e., other is zero at self's wave limits),\n            # then we consider it as a full coverage.\n            # This logic assumes __call__ never returns mag or count!\n            if ((isinstance(other.model, Empirical1D) and\n                 other.model.is_tapered() or\n                 not isinstance(other.model,\n                                (Empirical1D, _CompoundModel))) and\n                    np.allclose(other(x1[::x1.size - 1]).value, 0)):\n                result = 'full'\n\n            # Check if the lack of overlap is significant.\n            else:\n                # Get all the flux\n                totalflux = self.integrate(wavelengths=wavelengths).value\n                utils.validate_totalflux(totalflux)\n\n                a_min, a_max = a.min(), a.max()\n                b_min, b_max = b.min(), b.max()\n\n                # Now get the other two pieces\n                excluded = 0.0\n                if a_min < b_min:\n                    excluded += self.integrate(\n                        wavelengths=np.array([a_min, b_min])).value\n                if a_max > b_max:\n                    excluded += self.integrate(\n                        wavelengths=np.array([b_max, a_max])).value\n\n                if excluded / totalflux < threshold:\n                    result = 'partial_most'\n                else:\n                    result = 'partial_notmost'\n\n        return result", "response": "Check for wavelength overlap between two spectra."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unit_response(self, area, wavelengths=None):\n        a = units.validate_quantity(area, units.AREA)\n\n        # Only correct if wavelengths are in Angstrom.\n        x = self._validate_wavelengths(wavelengths).value\n\n        y = self(x).value * x\n        int_val = abs(np.trapz(y, x=x))\n        uresp = units.HC / (a.cgs * int_val)\n\n        return uresp.value * units.FLAM", "response": "Calculate the unit response of a specific area and wavelength."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the bandpass RMS width of the given wavelengths.", "response": "def rmswidth(self, wavelengths=None, threshold=None):\n        \"\"\"Calculate the :ref:`bandpass RMS width <synphot-formula-rmswidth>`.\n        Not to be confused with :func:`photbw`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        threshold : float or `~astropy.units.quantity.Quantity`, optional\n            Data points with throughput below this value are not\n            included in the calculation. By default, all data points\n            are included.\n\n        Returns\n        -------\n        rms_width : `~astropy.units.quantity.Quantity`\n            RMS width of the bandpass.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Threshold is invalid.\n\n        \"\"\"\n        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n\n        if threshold is None:\n            wave = x\n            thru = y\n        else:\n            if (isinstance(threshold, numbers.Real) or\n                (isinstance(threshold, u.Quantity) and\n                 threshold.unit == self._internal_flux_unit)):\n                mask = y >= threshold\n            else:\n                raise exceptions.SynphotError(\n                    '{0} is not a valid threshold'.format(threshold))\n            wave = x[mask]\n            thru = y[mask]\n\n        a = self.avgwave(wavelengths=wavelengths).value\n        num = np.trapz((wave - a) ** 2 * thru, x=wave)\n        den = np.trapz(thru, x=wave)\n\n        if den == 0:  # pragma: no cover\n            rms_width = 0.0\n        else:\n            rms_width = np.sqrt(abs(num / den))\n\n        return rms_width * self._internal_wave_unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the FWHM of the equivalent gaussian.", "response": "def fwhm(self, **kwargs):\n        \"\"\"Calculate :ref:`synphot-formula-fwhm` of equivalent gaussian.\n\n        Parameters\n        ----------\n        kwargs : dict\n            See :func:`photbw`.\n\n        Returns\n        -------\n        fwhm_val : `~astropy.units.quantity.Quantity`\n            FWHM of equivalent gaussian.\n\n        \"\"\"\n        return np.sqrt(8 * np.log(2)) * self.photbw(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tpeak(self, wavelengths=None):\n        x = self._validate_wavelengths(wavelengths).value\n        return self(x).max()", "response": "Calculate the peak bandpass throughput of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wpeak(self, wavelengths=None):\n        x = self._validate_wavelengths(wavelengths)\n        return x[self(x) == self.tpeak(wavelengths=wavelengths)][0]", "response": "Calculate the frequency of the peak of the data point."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rectwidth(self, wavelengths=None):\n        equvw = self.equivwidth(wavelengths=wavelengths)\n        tpeak = self.tpeak(wavelengths=wavelengths)\n\n        if tpeak.value == 0:  # pragma: no cover\n            rectw = 0.0 * self._internal_wave_unit\n        else:\n            rectw = equvw / tpeak\n\n        return rectw", "response": "Calculate the bandpass rectangular width of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the dimensionless efficiency of the current object.", "response": "def efficiency(self, wavelengths=None):\n        \"\"\"Calculate :ref:`dimensionless efficiency <synphot-formula-qtlam>`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        Returns\n        -------\n        qtlam : `~astropy.units.quantity.Quantity`\n            Dimensionless efficiency.\n\n        \"\"\"\n        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n        qtlam = abs(np.trapz(y / x, x=x))\n        return qtlam * u.dimensionless_unscaled"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate equivalent monochromatic flux for a given area and wavelengths.", "response": "def emflx(self, area, wavelengths=None):\n        \"\"\"Calculate\n        :ref:`equivalent monochromatic flux <synphot-formula-emflx>`.\n\n        Parameters\n        ----------\n        area, wavelengths\n            See :func:`unit_response`.\n\n        Returns\n        -------\n        em_flux : `~astropy.units.quantity.Quantity`\n            Equivalent monochromatic flux.\n\n        \"\"\"\n        t_lambda = self.tlambda(wavelengths=wavelengths)\n\n        if t_lambda == 0:  # pragma: no cover\n            em_flux = 0.0 * units.FLAM\n        else:\n            uresp = self.unit_response(area, wavelengths=wavelengths)\n            equvw = self.equivwidth(wavelengths=wavelengths).value\n            em_flux = uresp * equvw / t_lambda\n\n        return em_flux"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Empirical bandpass from a file.", "response": "def from_file(cls, filename, **kwargs):\n        \"\"\"Creates a bandpass from file.\n\n        If filename has 'fits' or 'fit' suffix, it is read as FITS.\n        Otherwise, it is read as ASCII.\n\n        Parameters\n        ----------\n        filename : str\n            Bandpass filename.\n\n        kwargs : dict\n            Keywords acceptable by\n            :func:`~synphot.specio.read_fits_spec` (if FITS) or\n            :func:`~synphot.specio.read_ascii_spec` (if ASCII).\n\n        Returns\n        -------\n        bp : `SpectralElement`\n            Empirical bandpass.\n\n        \"\"\"\n        if 'flux_unit' not in kwargs:\n            kwargs['flux_unit'] = cls._internal_flux_unit\n\n        if ((filename.endswith('fits') or filename.endswith('fit')) and\n                'flux_col' not in kwargs):\n            kwargs['flux_col'] = 'THROUGHPUT'\n\n        header, wavelengths, throughput = specio.read_spec(filename, **kwargs)\n        return cls(Empirical1D, points=wavelengths, lookup_table=throughput,\n                   keep_neg=True, meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_filter(cls, filtername, **kwargs):\n        filtername = filtername.lower()\n\n        # Select filename based on filter name\n        if filtername == 'bessel_j':\n            cfgitem = Conf.bessel_j_file\n        elif filtername == 'bessel_h':\n            cfgitem = Conf.bessel_h_file\n        elif filtername == 'bessel_k':\n            cfgitem = Conf.bessel_k_file\n        elif filtername == 'cousins_r':\n            cfgitem = Conf.cousins_r_file\n        elif filtername == 'cousins_i':\n            cfgitem = Conf.cousins_i_file\n        elif filtername == 'johnson_u':\n            cfgitem = Conf.johnson_u_file\n        elif filtername == 'johnson_b':\n            cfgitem = Conf.johnson_b_file\n        elif filtername == 'johnson_v':\n            cfgitem = Conf.johnson_v_file\n        elif filtername == 'johnson_r':\n            cfgitem = Conf.johnson_r_file\n        elif filtername == 'johnson_i':\n            cfgitem = Conf.johnson_i_file\n        elif filtername == 'johnson_j':\n            cfgitem = Conf.johnson_j_file\n        elif filtername == 'johnson_k':\n            cfgitem = Conf.johnson_k_file\n        else:\n            raise exceptions.SynphotError(\n                'Filter name {0} is invalid.'.format(filtername))\n\n        filename = cfgitem()\n\n        if 'flux_unit' not in kwargs:\n            kwargs['flux_unit'] = cls._internal_flux_unit\n\n        if ((filename.endswith('fits') or filename.endswith('fit')) and\n                'flux_col' not in kwargs):\n            kwargs['flux_col'] = 'THROUGHPUT'\n\n        header, wavelengths, throughput = specio.read_remote_spec(\n            filename, **kwargs)\n        header['filename'] = filename\n        header['descrip'] = cfgitem.description\n        meta = {'header': header, 'expr': filtername}\n        return cls(Empirical1D, points=wavelengths, lookup_table=throughput,\n                   meta=meta)", "response": "Load an instance of the class object based on the filter name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def handle_frame(self, frame):\n        if isinstance(frame, FrameActivateSceneConfirmation) and frame.session_id == self.session_id:\n            if frame.status == ActivateSceneConfirmationStatus.ACCEPTED:\n                self.success = True\n            return not self.wait_for_completion\n        if isinstance(frame, FrameCommandRemainingTimeNotification) and frame.session_id == self.session_id:\n            # Ignoring FrameCommandRemainingTimeNotification\n            return False\n        if isinstance(frame, FrameCommandRunStatusNotification) and frame.session_id == self.session_id:\n            # At the moment I don't reall understand what the FrameCommandRunStatusNotification is good for.\n            # Ignoring these packets for now\n            return False\n        if isinstance(frame, FrameSessionFinishedNotification) and frame.session_id == self.session_id:\n            return True\n        return False", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_bins(self, binset):\n        if binset is None:\n            if self.bandpass.waveset is not None:\n                self._binset = self.bandpass.waveset\n            elif self.spectrum.waveset is not None:\n                self._binset = self.spectrum.waveset\n                log.info('Bandpass waveset is undefined; '\n                         'Using source spectrum waveset instead.')\n            else:\n                raise exceptions.UndefinedBinset(\n                    'Both source spectrum and bandpass have undefined '\n                    'waveset; Provide binset manually.')\n        else:\n            self._binset = self._validate_wavelengths(binset)\n\n        # binset must be in ascending order for calcbinflux()\n        # to work properly.\n        if self._binset[0] > self._binset[-1]:\n            self._binset = self._binset[::-1]\n\n        self._bin_edges = binning.calculate_bin_edges(self._binset)\n\n        # Merge bin edges and centers in with the natural waveset\n        spwave = utils.merge_wavelengths(\n            self._bin_edges.value, self._binset.value)\n        if self.waveset is not None:\n            spwave = utils.merge_wavelengths(spwave, self.waveset.value)\n\n        # Throw out invalid wavelengths after merging.\n        spwave = spwave[spwave > 0]\n\n        # Compute indices associated to each endpoint.\n        indices = np.searchsorted(spwave, self._bin_edges.value)\n        i_beg = indices[:-1]\n        i_end = indices[1:]\n\n        # Prepare integration variables.\n        flux = self(spwave)\n        avflux = (flux.value[1:] + flux.value[:-1]) * 0.5\n        deltaw = spwave[1:] - spwave[:-1]\n\n        # Sum over each bin.\n        binflux, intwave = binning.calcbinflux(\n            self._binset.size, i_beg, i_end, avflux, deltaw)\n\n        self._binflux = binflux * flux.unit", "response": "Initializes binned wavelength centers edges and flux for the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples unbinned data without interpolation.", "response": "def sample_binned(self, wavelengths=None, flux_unit=None, **kwargs):\n        \"\"\"Sample binned observation without interpolation.\n\n        To sample unbinned data, use ``__call__``.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `binset` is used.\n\n        flux_unit : str or `~astropy.units.core.Unit` or `None`\n            Flux is converted to this unit.\n            If not given, internal unit is used.\n\n        kwargs : dict\n            Keywords acceptable by :func:`~synphot.units.convert_flux`.\n\n        Returns\n        -------\n        flux : `~astropy.units.quantity.Quantity`\n            Binned flux in given unit.\n\n        Raises\n        ------\n        synphot.exceptions.InterpolationNotAllowed\n            Interpolation of binned data is not allowed.\n\n        \"\"\"\n        x = self._validate_binned_wavelengths(wavelengths)\n        i = np.searchsorted(self.binset, x)\n        if not np.allclose(self.binset[i].value, x.value):\n            raise exceptions.InterpolationNotAllowed(\n                'Some or all wavelength values are not in binset.')\n        y = self.binflux[i]\n\n        if flux_unit is None:\n            flux = y\n        else:\n            flux = units.convert_flux(x, y, flux_unit, **kwargs)\n\n        return flux"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_binned_arrays(self, wavelengths, flux_unit, area=None,\n                           vegaspec=None):\n        \"\"\"Get binned observation in user units.\"\"\"\n        x = self._validate_binned_wavelengths(wavelengths)\n        y = self.sample_binned(wavelengths=x, flux_unit=flux_unit, area=area,\n                               vegaspec=vegaspec)\n\n        if isinstance(wavelengths, u.Quantity):\n            w = x.to(wavelengths.unit, u.spectral())\n        else:\n            w = x\n\n        return w, y", "response": "Get binned observation in user units."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binned_waverange(self, cenwave, npix, **kwargs):\n        # Calculation is done in the unit of cenwave.\n        if not isinstance(cenwave, u.Quantity):\n            cenwave = cenwave * self._internal_wave_unit\n\n        bin_wave = units.validate_quantity(\n            self.binset, cenwave.unit, equivalencies=u.spectral())\n\n        return binning.wave_range(\n            bin_wave.value, cenwave.value, npix, **kwargs) * cenwave.unit", "response": "Calculate the wavelength range covered by the given number of pixels centered on the given central wavelengths of the binset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the number of pixels within the given wavelength range and binset.", "response": "def binned_pixelrange(self, waverange, **kwargs):\n        \"\"\"Calculate the number of pixels within the given wavelength\n        range and `binset`.\n\n        Parameters\n        ----------\n        waverange : tuple of float or `~astropy.units.quantity.Quantity`\n            Lower and upper limits of the desired wavelength range.\n            If not a Quantity, assumed to be in Angstrom.\n\n        kwargs : dict\n            Keywords accepted by :func:`synphot.binning.pixel_range`.\n\n        Returns\n        -------\n        npix : int\n            Number of pixels.\n\n        \"\"\"\n        x = units.validate_quantity(\n            waverange, self._internal_wave_unit, equivalencies=u.spectral())\n        return binning.pixel_range(self.binset.value, x.value, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the effective wavelength of a single object.", "response": "def effective_wavelength(self, binned=True, wavelengths=None,\n                             mode='efflerg'):\n        \"\"\"Calculate :ref:`effective wavelength <synphot-formula-effwave>`.\n\n        Parameters\n        ----------\n        binned : bool\n            Sample data in native wavelengths if `False`.\n            Else, sample binned data (default).\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` or `binset` is used, depending\n            on ``binned``.\n\n        mode : {'efflerg', 'efflphot'}\n            Flux is first converted to the unit below before calculation:\n\n                * 'efflerg' - FLAM\n                * 'efflphot' - PHOTLAM (deprecated)\n\n        Returns\n        -------\n        eff_lam : `~astropy.units.quantity.Quantity`\n            Observation effective wavelength.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid mode.\n\n        \"\"\"\n        mode = mode.lower()\n        if mode == 'efflerg':\n            flux_unit = units.FLAM\n        elif mode == 'efflphot':\n            warnings.warn(\n                'Usage of EFFLPHOT is deprecated.', AstropyDeprecationWarning)\n            flux_unit = units.PHOTLAM\n        else:\n            raise exceptions.SynphotError(\n                'mode must be \"efflerg\" or \"efflphot\"')\n\n        if binned:\n            x = self._validate_binned_wavelengths(wavelengths).value\n            y = self.sample_binned(wavelengths=x, flux_unit=flux_unit).value\n        else:\n            x = self._validate_wavelengths(wavelengths).value\n            y = units.convert_flux(x, self(x), flux_unit).value\n\n        num = np.trapz(y * x ** 2, x=x)\n        den = np.trapz(y * x, x=x)\n\n        if den == 0.0:  # pragma: no cover\n            eff_lam = 0.0\n        else:\n            eff_lam = abs(num / den)\n\n        return eff_lam * self._internal_wave_unit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the effective stimulus for a given flux unit.", "response": "def effstim(self, flux_unit=None, wavelengths=None, area=None,\n                vegaspec=None):\n        \"\"\"Calculate :ref:`effective stimulus <synphot-formula-effstim>`\n        for given flux unit.\n\n        Parameters\n        ----------\n        flux_unit : str or `~astropy.units.core.Unit` or `None`\n            The unit of effective stimulus.\n            COUNT gives result in count/s (see :meth:`countrate` for more\n            options).\n            If not given, internal unit is used.\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling. This must be given if\n            ``self.waveset`` is undefined for the underlying spectrum model(s).\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        area, vegaspec\n            See :func:`~synphot.units.convert_flux`.\n\n        Returns\n        -------\n        eff_stim : `~astropy.units.quantity.Quantity`\n            Observation effective stimulus based on given flux unit.\n\n        \"\"\"\n        if flux_unit is None:\n            flux_unit = self._internal_flux_unit\n\n        flux_unit = units.validate_unit(flux_unit)\n        flux_unit_name = flux_unit.to_string()\n\n        # Special handling of COUNT/OBMAG.\n        # This is special case of countrate calculations.\n        if flux_unit == u.count or flux_unit_name == units.OBMAG.to_string():\n            val = self.countrate(area, binned=False, wavelengths=wavelengths)\n\n            if flux_unit.decompose() == u.mag:\n                eff_stim = (-2.5 * np.log10(val.value)) * flux_unit\n            else:\n                eff_stim = val\n\n            return eff_stim\n\n        # Special handling of VEGAMAG.\n        # This is basically effstim(self)/effstim(Vega)\n        if flux_unit_name == units.VEGAMAG.to_string():\n            num = self.integrate(wavelengths=wavelengths)\n            den = (vegaspec * self.bandpass).integrate()\n            utils.validate_totalflux(num)\n            utils.validate_totalflux(den)\n            return (2.5 * (math.log10(den.value) -\n                           math.log10(num.value))) * units.VEGAMAG\n\n        # Sample the bandpass\n        x_band = self.bandpass._validate_wavelengths(wavelengths).value\n        y_band = self.bandpass(x_band).value\n\n        # Sample the observation in FLAM\n        inwave = self._validate_wavelengths(wavelengths).value\n        influx = units.convert_flux(inwave, self(inwave), units.FLAM).value\n\n        # Integrate\n        num = abs(np.trapz(inwave * influx, x=inwave))\n        den = abs(np.trapz(x_band * y_band, x=x_band))\n        utils.validate_totalflux(num)\n        utils.validate_totalflux(den)\n        val = (num / den) * units.FLAM\n\n        # Integration should always be done in FLAM and then\n        # converted to desired units as follows.\n        if flux_unit.physical_type == 'spectral flux density wav':\n            if flux_unit == u.STmag:\n                eff_stim = val.to(flux_unit)\n            else:  # FLAM\n                eff_stim = val\n        elif flux_unit.physical_type in (\n                'spectral flux density', 'photon flux density',\n                'photon flux density wav'):\n            w_pivot = self.bandpass.pivot()\n            eff_stim = units.convert_flux(w_pivot, val, flux_unit)\n        else:\n            raise exceptions.SynphotError(\n                'Flux unit {0} is invalid'.format(flux_unit))\n\n        return eff_stim"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the effective stimulus for a given area and binned wavelengths.", "response": "def countrate(self, area, binned=True, wavelengths=None, waverange=None,\n                  force=False):\n        \"\"\"Calculate :ref:`effective stimulus <synphot-formula-effstim>`\n        in count/s.\n\n        Parameters\n        ----------\n        area : float or `~astropy.units.quantity.Quantity`\n            Area that flux covers. If not a Quantity, assumed to be in\n            :math:`cm^{2}`.\n\n        binned : bool\n            Sample data in native wavelengths if `False`.\n            Else, sample binned data (default).\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling. This must be given if\n            ``self.waveset`` is undefined for the underlying spectrum model(s).\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` or `binset` is used, depending\n            on ``binned``.\n\n        waverange : tuple of float, Quantity, or `None`\n            Lower and upper limits of the desired wavelength range.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, the full range is used.\n\n        force : bool\n            If a wavelength range is given, partial overlap raises\n            an exception when this is `False` (default). Otherwise,\n            it returns calculation for the overlapping region.\n            Disjoint wavelength range raises an exception regardless.\n\n        Returns\n        -------\n        count_rate : `~astropy.units.quantity.Quantity`\n            Observation effective stimulus in count/s.\n\n        Raises\n        ------\n        synphot.exceptions.DisjointError\n            Wavelength range does not overlap with observation.\n\n        synphot.exceptions.PartialOverlap\n            Wavelength range only partially overlaps with observation.\n\n        synphot.exceptions.SynphotError\n            Calculation failed.\n\n        \"\"\"\n        # Sample the observation\n        if binned:\n            x = self._validate_binned_wavelengths(wavelengths).value\n            y = self.sample_binned(wavelengths=x, flux_unit=u.count,\n                                   area=area).value\n        else:\n            x = self._validate_wavelengths(wavelengths).value\n            y = units.convert_flux(x, self(x), u.count,\n                                   area=area).value\n\n        # Use entire wavelength range\n        if waverange is None:\n            influx = y\n\n        # Use given wavelength range\n        else:\n            w = units.validate_quantity(waverange, self._internal_wave_unit,\n                                        equivalencies=u.spectral()).value\n            stat = utils.overlap_status(w, x)\n            w1 = w.min()\n            w2 = w.max()\n\n            if stat == 'none':\n                raise exceptions.DisjointError(\n                    'Observation and wavelength range are disjoint.')\n            elif 'partial' in stat:\n                if force:\n                    warnings.warn(\n                        'Count rate calculated only for wavelengths in the '\n                        'overlap between observation and given range.',\n                        AstropyUserWarning)\n                    w1 = max(w1, x.min())\n                    w2 = min(w2, x.max())\n                else:\n                    raise exceptions.PartialOverlap(\n                        'Observation and wavelength range do not fully '\n                        'overlap. You may use force=True to force this '\n                        'calculation anyway.')\n            elif stat != 'full':  # pragma: no cover\n                raise exceptions.SynphotError(\n                    'Overlap result of {0} is unexpected'.format(stat))\n\n            if binned:\n                if wavelengths is None:\n                    bin_edges = self.bin_edges.value\n                else:\n                    bin_edges = binning.calculate_bin_edges(x).value\n                i1 = np.searchsorted(bin_edges, w1) - 1\n                i2 = np.searchsorted(bin_edges, w2)\n                influx = y[i1:i2]\n            else:\n                mask = ((x >= w1) & (x <= w2))\n                influx = y[mask]\n\n        val = math.fsum(influx)\n        utils.validate_totalflux(val)\n\n        return val * (u.count / u.s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot(self, binned=True, wavelengths=None, flux_unit=None, area=None,\n             vegaspec=None, **kwargs):  # pragma: no cover\n        \"\"\"Plot the observation.\n\n        .. note:: Uses ``matplotlib``.\n\n        Parameters\n        ----------\n        binned : bool\n            Plot data in native wavelengths if `False`.\n            Else, plot binned data (default).\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` or `binset` is used, depending\n            on ``binned``.\n\n        flux_unit : str or `~astropy.units.core.Unit` or `None`\n            Flux is converted to this unit for plotting.\n            If not given, internal unit is used.\n\n        area, vegaspec\n            See :func:`~synphot.units.convert_flux`.\n\n        kwargs : dict\n            See :func:`synphot.spectrum.BaseSpectrum.plot`.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid inputs.\n\n        \"\"\"\n        if binned:\n            w, y = self._get_binned_arrays(wavelengths, flux_unit, area=area,\n                                           vegaspec=vegaspec)\n        else:\n            w, y = self._get_arrays(wavelengths, flux_unit=flux_unit,\n                                    area=area, vegaspec=vegaspec)\n        self._do_plot(w, y, **kwargs)", "response": "Plot the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreduces the observation to an empirical source spectrum.", "response": "def as_spectrum(self, binned=True, wavelengths=None):\n        \"\"\"Reduce the observation to an empirical source spectrum.\n\n        An observation is a complex object with some restrictions\n        on its capabilities. At times, it would be useful to work\n        with the observation as a simple object that is easier to\n        manipulate and takes up less memory.\n\n        This is also useful for writing an observation as sampled\n        spectrum out to a FITS file.\n\n        Parameters\n        ----------\n        binned : bool\n            Write out data in native wavelengths if `False`.\n            Else, write binned data (default).\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` or `binset` is used, depending\n            on ``binned``.\n\n        Returns\n        -------\n        sp : `~synphot.spectrum.SourceSpectrum`\n            Empirical source spectrum.\n\n        \"\"\"\n        if binned:\n            w, y = self._get_binned_arrays(\n                wavelengths, self._internal_flux_unit)\n        else:\n            w, y = self._get_arrays(\n                wavelengths, flux_unit=self._internal_flux_unit)\n\n        header = {'observation': str(self), 'binned': binned}\n        return SourceSpectrum(Empirical1D, points=w, lookup_table=y,\n                              meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles incoming API frame return True if this was the expected frame.", "response": "async def handle_frame(self, frame):\n        \"\"\"Handle incoming API frame, return True if this was the expected frame.\"\"\"\n        if not isinstance(frame, FrameSetNodeNameConfirmation):\n            return False\n        self.success = frame.status == SetNodeNameConfirmationStatus.OK\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def do_api_call(self):\n        self.pyvlx.connection.register_frame_received_cb(\n            self.response_rec_callback)\n        await self.send_frame()\n        await self.start_timeout()\n        await self.response_received_or_timeout.wait()\n        await self.stop_timeout()\n        self.pyvlx.connection.unregister_frame_received_cb(self.response_rec_callback)", "response": "Start. Sending and waiting for answer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def main():\n    pyvlx = PyVLX('pyvlx.yaml')\n    # Alternative:\n    # pyvlx = PyVLX(host=\"192.168.2.127\", password=\"velux123\", timeout=60)\n\n    await pyvlx.load_devices()\n    print(pyvlx.devices[1])\n    print(pyvlx.devices['Fenster 4'])\n\n    await pyvlx.load_scenes()\n    print(pyvlx.scenes[0])\n    print(pyvlx.scenes['Bath Closed'])\n\n    # opening/ closing windows by running scenes, yay!\n    await pyvlx.scenes[1].run()\n\n    await pyvlx.disconnect()", "response": "Load devices and scenes run first scene."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset switch to desired state.", "response": "async def set_state(self, parameter):\n        \"\"\"Set switch to desired state.\"\"\"\n        command_send = CommandSend(pyvlx=self.pyvlx, node_id=self.node_id, parameter=parameter)\n        await command_send.do_api_call()\n        if not command_send.success:\n            raise PyVLXException(\"Unable to send command\")\n        self.parameter = parameter\n        await self.after_update()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an extinction curve for a specific Empirical entry.", "response": "def extinction_curve(self, ebv, wavelengths=None):\n        \"\"\"Generate extinction curve.\n\n        .. math::\n\n            A(V) = R(V) \\\\; \\\\times \\\\; E(B-V)\n\n            THRU = 10^{-0.4 \\\\; A(V)}\n\n        Parameters\n        ----------\n        ebv : float or `~astropy.units.quantity.Quantity`\n            :math:`E(B-V)` value in magnitude.\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        Returns\n        -------\n        extcurve : `ExtinctionCurve`\n            Empirical extinction curve.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid input.\n\n        \"\"\"\n        if isinstance(ebv, u.Quantity) and ebv.unit.decompose() == u.mag:\n            ebv = ebv.value\n        elif not isinstance(ebv, numbers.Real):\n            raise exceptions.SynphotError('E(B-V)={0} is invalid.'.format(ebv))\n\n        x = self._validate_wavelengths(wavelengths).value\n        y = 10 ** (-0.4 * self(x).value * ebv)\n        header = {\n            'E(B-V)': ebv,\n            'ReddeningLaw': self.meta.get('expr', 'unknown')}\n\n        return ExtinctionCurve(ExtinctionModel1D, points=x, lookup_table=y,\n                               meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the reddening law to a FITS file.", "response": "def to_fits(self, filename, wavelengths=None, **kwargs):\n        \"\"\"Write the reddening law to a FITS file.\n\n        :math:`R(V)` column is automatically named 'Av/E(B-V)'.\n\n        Parameters\n        ----------\n        filename : str\n            Output filename.\n\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        kwargs : dict\n            Keywords accepted by :func:`~synphot.specio.write_fits_spec`.\n\n        \"\"\"\n        w, y = self._get_arrays(wavelengths)\n\n        kwargs['flux_col'] = 'Av/E(B-V)'\n        kwargs['flux_unit'] = self._internal_flux_unit\n\n        # No need to trim/pad zeroes, unless user chooses to do so.\n        if 'pad_zero_ends' not in kwargs:\n            kwargs['pad_zero_ends'] = False\n        if 'trim_zero' not in kwargs:\n            kwargs['trim_zero'] = False\n\n        # There are some standard keywords that should be added\n        # to the extension header.\n        bkeys = {'tdisp1': 'G15.7', 'tdisp2': 'G15.7'}\n\n        if 'expr' in self.meta:\n            bkeys['expr'] = (self.meta['expr'], 'synphot expression')\n\n        if 'ext_header' in kwargs:\n            kwargs['ext_header'].update(bkeys)\n        else:\n            kwargs['ext_header'] = bkeys\n\n        specio.write_fits_spec(filename, w, y, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a reddening law from a file.", "response": "def from_file(cls, filename, **kwargs):\n        \"\"\"Create a reddening law from file.\n\n        If filename has 'fits' or 'fit' suffix, it is read as FITS.\n        Otherwise, it is read as ASCII.\n\n        Parameters\n        ----------\n        filename : str\n            Reddening law filename.\n\n        kwargs : dict\n            Keywords acceptable by\n            :func:`~synphot.specio.read_fits_spec` (if FITS) or\n            :func:`~synphot.specio.read_ascii_spec` (if ASCII).\n\n        Returns\n        -------\n        redlaw : `ReddeningLaw`\n            Empirical reddening law.\n\n        \"\"\"\n        if 'flux_unit' not in kwargs:\n            kwargs['flux_unit'] = cls._internal_flux_unit\n\n        if ((filename.endswith('fits') or filename.endswith('fit')) and\n                'flux_col' not in kwargs):\n            kwargs['flux_col'] = 'Av/E(B-V)'\n\n        header, wavelengths, rvs = specio.read_spec(filename, **kwargs)\n\n        return cls(Empirical1D, points=wavelengths, lookup_table=rvs,\n                   meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an empirical reddening object from a specific extinction model.", "response": "def from_extinction_model(cls, modelname, **kwargs):\n        \"\"\"Load :ref:`pre-defined extinction model <synphot_reddening>`.\n\n        Parameters\n        ----------\n        modelname : str\n            Extinction model name. Choose from 'lmc30dor', 'lmcavg', 'mwavg',\n            'mwdense', 'mwrv21', 'mwrv40', 'smcbar', or 'xgalsb'.\n\n        kwargs : dict\n            Keywords acceptable by :func:`~synphot.specio.read_remote_spec`.\n\n        Returns\n        -------\n        redlaw : `ReddeningLaw`\n            Empirical reddening law.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid extinction model name.\n\n        \"\"\"\n        modelname = modelname.lower()\n\n        # Select filename based on model name\n        if modelname == 'lmc30dor':\n            cfgitem = Conf.lmc30dor_file\n        elif modelname == 'lmcavg':\n            cfgitem = Conf.lmcavg_file\n        elif modelname == 'mwavg':\n            cfgitem = Conf.mwavg_file\n        elif modelname == 'mwdense':\n            cfgitem = Conf.mwdense_file\n        elif modelname == 'mwrv21':\n            cfgitem = Conf.mwrv21_file\n        elif modelname == 'mwrv40':\n            cfgitem = Conf.mwrv40_file\n        elif modelname == 'smcbar':\n            cfgitem = Conf.smcbar_file\n        elif modelname == 'xgalsb':\n            cfgitem = Conf.xgal_file\n        else:\n            raise exceptions.SynphotError(\n                'Extinction model {0} is invalid.'.format(modelname))\n\n        filename = cfgitem()\n\n        if 'flux_unit' not in kwargs:\n            kwargs['flux_unit'] = cls._internal_flux_unit\n\n        if ((filename.endswith('fits') or filename.endswith('fit')) and\n                'flux_col' not in kwargs):\n            kwargs['flux_col'] = 'Av/E(B-V)'\n\n        header, wavelengths, rvs = specio.read_remote_spec(filename, **kwargs)\n        header['filename'] = filename\n        header['descrip'] = cfgitem.description\n        meta = {'header': header, 'expr': modelname}\n\n        return cls(Empirical1D, points=wavelengths, lookup_table=rvs,\n                   meta=meta)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle incoming API frame return True if this was the expected frame.", "response": "async def handle_frame(self, frame):\n        \"\"\"Handle incoming API frame, return True if this was the expected frame.\"\"\"\n        if not isinstance(frame, FrameGetVersionConfirmation):\n            return False\n        self.version = frame.version\n        self.success = True\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds raw bytes from command and payload.", "response": "def build_frame(command, payload):\n        \"\"\"Build raw bytes from command and payload.\"\"\"\n        packet_length = 2 + len(payload) + 1\n        ret = struct.pack(\"BB\", 0, packet_length)\n        ret += struct.pack(\">H\", command.value)\n        ret += payload\n        ret += struct.pack(\"B\", calc_crc(ret))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def run(self, wait_for_completion=True):\n        activate_scene = ActivateScene(\n            pyvlx=self.pyvlx,\n            wait_for_completion=wait_for_completion,\n            scene_id=self.scene_id)\n        await activate_scene.do_api_call()\n        if not activate_scene.success:\n            raise PyVLXException(\"Unable to activate scene\")", "response": "Run scene.\n\n        Parameters:\n            * wait_for_completion: If set, function will return\n                after device has reached target position."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, scene):\n        if not isinstance(scene, Scene):\n            raise TypeError()\n        self.__scenes.append(scene)", "response": "Add a new scene."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload scenes from KLF 200.", "response": "async def load(self):\n        \"\"\"Load scenes from KLF 200.\"\"\"\n        json_response = await self.pyvlx.interface.api_call('scenes', 'get')\n        self.data_import(json_response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport scenes from JSON response.", "response": "def data_import(self, json_response):\n        \"\"\"Import scenes from JSON response.\"\"\"\n        if 'data' not in json_response:\n            raise PyVLXException('no element data found: {0}'.format(\n                json.dumps(json_response)))\n        data = json_response['data']\n        for item in data:\n            self.load_scene(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_scene(self, item):\n        scene = Scene.from_config(self.pyvlx, item)\n        self.add(scene)", "response": "Load a scene from json."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def handle_frame(self, frame):\n        if not isinstance(frame, FrameGetStateConfirmation):\n            return False\n        self.success = True\n        self.gateway_state = frame.gateway_state\n        self.gateway_sub_state = frame.gateway_sub_state\n        return True", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing alias array from raw bytes.", "response": "def parse_raw(self, raw):\n        \"\"\"Parse alias array from raw bytes.\"\"\"\n        if not isinstance(raw, bytes):\n            raise PyVLXException(\"AliasArray::invalid_type_if_raw\", type_raw=type(raw))\n        if len(raw) != 21:\n            raise PyVLXException(\"AliasArray::invalid_size\", size=len(raw))\n        nbr_of_alias = raw[0]\n        if nbr_of_alias > 5:\n            raise PyVLXException(\"AliasArray::invalid_nbr_of_alias\", nbr_of_alias=nbr_of_alias)\n        for i in range(0, nbr_of_alias):\n            self.alias_array_.append((raw[i*4+1:i*4+3], raw[i*4+3:i*4+5]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the next slip packet from raw data.", "response": "def get_next_slip(raw):\n    \"\"\"\n    Get the next slip packet from raw data.\n\n    Returns the extracted packet plus the raw data with the remaining data stream.\n    \"\"\"\n    if not is_slip(raw):\n        return None, raw\n    length = raw[1:].index(SLIP_END)\n    slip_packet = decode(raw[1:length+1])\n    new_raw = raw[length+2:]\n    return slip_packet, new_raw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def set_utc(pyvlx):\n    setutc = SetUTC(pyvlx=pyvlx)\n    await setutc.do_api_call()\n    if not setutc.success:\n        raise PyVLXException(\"Unable to set utc.\")", "response": "Enable house status monitor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def handle_frame(self, frame):\n        if not isinstance(frame, FrameSetUTCConfirmation):\n            return False\n        self.success = True\n        return True", "response": "Handle incoming API frame return True if this was the expected frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the edges of wavelength bins given the centers.", "response": "def calculate_bin_edges(centers):\n    \"\"\"Calculate the edges of wavelength bins given the centers.\n\n    The algorithm calculates bin edges as the midpoints between bin centers\n    and treats the first and last bins as symmetric about their centers.\n\n    Parameters\n    ----------\n    centers : array-like or `~astropy.units.quantity.Quantity`\n        Sequence of bin centers. Must be 1D and have at least two values.\n        If not a Quantity, assumed to be in Angstrom.\n\n    Returns\n    -------\n    edges : `~astropy.units.quantity.Quantity`\n        Array of bin edges. Will be 1D, have one more value\n        than ``centers``, and also the same unit.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Invalid input.\n\n    \"\"\"\n    if not isinstance(centers, u.Quantity):\n        centers = centers * u.AA\n\n    if centers.ndim != 1:\n        raise exceptions.SynphotError('Bin centers must be 1D array.')\n\n    if centers.size < 2:\n        raise exceptions.SynphotError(\n            'Bin centers must have at least two values.')\n\n    edges = np.empty(centers.size + 1, dtype=np.float64)\n    edges[1:-1] = (centers.value[1:] + centers.value[:-1]) * 0.5\n\n    # Compute the first and last by making them symmetric\n    edges[0] = 2.0 * centers.value[0] - edges[1]\n    edges[-1] = 2.0 * centers.value[-1] - edges[-2]\n\n    return edges * centers.unit"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_bin_widths(edges):\n    if not isinstance(edges, u.Quantity):\n        edges = edges * u.AA\n\n    if edges.ndim != 1:\n        raise exceptions.SynphotError('Bin edges must be 1D array.')\n\n    if edges.size < 2:\n        raise exceptions.SynphotError(\n            'Bin edges must have at least two values.')\n\n    return np.abs(edges[1:] - edges[:-1])", "response": "Calculates the widths of wavelengths bins given their edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_bin_centers(edges):\n    if not isinstance(edges, u.Quantity):\n        edges = edges * u.AA\n\n    if edges.ndim != 1:\n        raise exceptions.SynphotError('Bin edges must be 1D array.')\n\n    if edges.size < 2:\n        raise exceptions.SynphotError(\n            'Bin edges must have at least two values.')\n\n    centers = np.empty(edges.size - 1, dtype=np.float64)\n    centers[0] = edges.value[:2].mean()\n\n    for i in range(1, centers.size):\n        centers[i] = 2.0 * edges.value[i] - centers[i - 1]\n\n    return centers * edges.unit", "response": "Calculate the centers of wavelengths bins given their edges."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wave_range(bins, cenwave, npix, mode='round'):\n    mode = mode.lower()\n\n    if mode not in ('round', 'min', 'max', 'none'):\n        raise exceptions.SynphotError(\n            'mode={0} is invalid, must be \"round\", \"min\", \"max\", '\n            'or \"none\".'.format(mode))\n\n    if not isinstance(npix, int):\n        raise exceptions.SynphotError('npix={0} is invalid.'.format(npix))\n\n    # Bin values must be in ascending order.\n    if bins[0] > bins[-1]:\n        bins = bins[::-1]\n\n    # Central wavelength must be within given bins.\n    if cenwave < bins[0] or cenwave > bins[-1]:\n        raise exceptions.OverlapError(\n            'cenwave={0} is not within binset (min={1}, max={2}).'.format(\n                cenwave, bins[0], bins[-1]))\n\n    # Find the index the central wavelength among bins\n    diff = cenwave - bins\n    ind = np.argmin(np.abs(diff))\n\n    # Calculate fractional index\n    frac_ind = float(ind)\n    if diff[ind] < 0:\n        frac_ind += diff[ind] / (bins[ind] - bins[ind - 1])\n    elif diff[ind] > 0:\n        frac_ind += diff[ind] / (bins[ind + 1] - bins[ind])\n\n    # Calculate fractional indices of the edges\n    half_npix = npix / 2.0\n    frac_ind1 = frac_ind - half_npix\n    frac_ind2 = frac_ind + half_npix\n\n    # Calculated edges must not exceed bin edges\n    if frac_ind1 < -0.5:\n        raise exceptions.OverlapError(\n            'Lower limit of wavelength range is out of bounds.')\n    if frac_ind2 > (bins.size - 0.5):\n        raise exceptions.OverlapError(\n            'Upper limit of wavelength range is out of bounds.')\n\n    frac1, int1 = np.modf(frac_ind1)\n    frac2, int2 = np.modf(frac_ind2)\n    int1 = int(int1)\n    int2 = int(int2)\n\n    if mode == 'round':\n        # Lower end of wavelength range\n        if frac1 >= 0:\n            # end is somewhere greater than binset[0] so we can just\n            # interpolate between two neighboring values going with upper edge\n            wave1 = bins[int1:int1 + 2].mean()\n        else:\n            # end is below the lowest binset value, but not by enough to\n            # trigger an exception\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n\n        # Upper end of wavelength range\n        if int2 < bins.shape[0] - 1:\n            # end is somewhere below binset[-1] so we can just interpolate\n            # between two neighboring values, going with the upper edge.\n            wave2 = bins[int2:int2 + 2].mean()\n        else:\n            # end is above highest binset value but not by enough to\n            # trigger an exception\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n\n    elif mode == 'min':\n        # Lower end of wavelength range\n        if frac1 <= 0.5 and int1 < bins.shape[0] - 1:\n            # not at the lowest possible edge and pixel i included\n            wave1 = bins[int1:int1 + 2].mean()\n        elif frac1 > 0.5 and int1 < bins.shape[0] - 2:\n            # not at the lowest possible edge and pixel i not included\n            wave1 = bins[int1 + 1:int1 + 3].mean()\n        elif frac1 == -0.5:\n            # at the lowest possible edge\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n        else:  # pragma: no cover\n            raise exceptions.SynphotError(\n                'mode={0} gets unexpected frac1={1}, int1={2}'.format(\n                    mode, frac1, int1))\n\n        # Upper end of wavelength range\n        if frac2 >= 0.5 and int2 < bins.shape[0] - 1:\n            # not out at the end and pixel i included\n            wave2 = bins[int2:int2 + 2].mean()\n        elif frac2 < 0.5 and int2 < bins.shape[0]:\n            # not out at end and pixel i not included\n            wave2 = bins[int2 - 1:int2 + 1].mean()\n        elif frac2 == 0.5 and int2 == bins.shape[0] - 1:\n            # at the very end\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n        else:  # pragma: no cover\n            raise exceptions.SynphotError(\n                'mode={0} gets unexpected frac2={1}, int2={2}'.format(\n                    mode, frac2, int2))\n\n    elif mode == 'max':\n        # Lower end of wavelength range\n        if frac1 < 0.5 and int1 < bins.shape[0]:\n            # not at the lowest possible edge and pixel i included\n            wave1 = bins[int1 - 1:int1 + 1].mean()\n        elif frac1 >= 0.5 and int1 < bins.shape[0] - 1:\n            # not at the lowest possible edge and pixel i not included\n            wave1 = bins[int1:int1 + 2].mean()\n        elif frac1 == -0.5:\n            # at the lowest possible edge\n            wave1 = bins[0] - (bins[0:2].mean() - bins[0])\n        else:  # pragma: no cover\n            raise exceptions.SynphotError(\n                'mode={0} gets unexpected frac1={1}, int1={2}'.format(\n                    mode, frac1, int1))\n\n        # Upper end of wavelength range\n        if frac2 > 0.5 and int2 < bins.shape[0] - 2:\n            # not out at the end and pixel i included\n            wave2 = bins[int2 + 1:int2 + 3].mean()\n        elif frac2 <= 0.5 and int2 < bins.shape[0] - 1:\n            # not out at end and pixel i not included\n            wave2 = bins[int2:int2 + 2].mean()\n        elif frac2 == 0.5 and int2 == bins.shape[0] - 1:\n            # at the very end\n            wave2 = bins[-1] + (bins[-1] - bins[-2:].mean())\n        else:  # pragma: no cover\n            raise exceptions.SynphotError(\n                'mode={0} gets unexpected frac2={1}, int2={2}'.format(\n                    mode, frac2, int2))\n\n    else:  # mode == 'none'\n        wave1 = bins[int1] + frac1 * (bins[int1 + 1] - bins[int1])\n        wave2 = bins[int2] + frac2 * (bins[int2 + 1] - bins[int2])\n\n    return wave1, wave2", "response": "Calculate the wavelength range of the given number of pixels in a given central wavelength of a given number of bins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pixel_range(bins, waverange, mode='round'):\n    mode = mode.lower()\n\n    if mode not in ('round', 'min', 'max', 'none'):\n        raise exceptions.SynphotError(\n            'mode={0} is invalid, must be \"round\", \"min\", \"max\", '\n            'or \"none\".'.format(mode))\n\n    if waverange[0] < waverange[-1]:\n        wave1 = waverange[0]\n        wave2 = waverange[-1]\n    else:\n        wave1 = waverange[-1]\n        wave2 = waverange[0]\n\n    # Bin values must be in ascending order.\n    if bins[0] > bins[-1]:\n        bins = bins[::-1]\n\n    # Wavelength range must be within bins\n    minwave = bins[0] - (bins[0:2].mean() - bins[0])\n    maxwave = bins[-1] + (bins[-1] - bins[-2:].mean())\n    if wave1 < minwave or wave2 > maxwave:\n        raise exceptions.OverlapError(\n            'Wavelength range ({0}, {1}) is out of bounds of bins '\n            '(min={2}, max={3}).'.format(wave1, wave2, minwave, maxwave))\n\n    if wave1 == wave2:\n        return 0\n\n    if mode == 'round':\n        ind1 = bins.searchsorted(wave1, side='right')\n        ind2 = bins.searchsorted(wave2, side='right')\n    else:\n        ind1 = bins.searchsorted(wave1, side='left')\n        ind2 = bins.searchsorted(wave2, side='left')\n\n    if mode == 'round':\n        npix = ind2 - ind1\n\n    elif mode == 'min':\n        # for ind1, figure out if pixel ind1 is wholly included or not.\n        # do this by figuring out where wave1 is between ind1 and ind1-1.\n        frac = (bins[ind1] - wave1) / (bins[ind1] - bins[ind1 - 1])\n        if frac < 0.5:\n            # ind1 is only partially included\n            ind1 += 1\n\n        # similar but reversed procedure for ind2\n        frac = (wave2 - bins[ind2 - 1]) / (bins[ind2] - bins[ind2 - 1])\n        if frac < 0.5:\n            # ind2 is only partially included\n            ind2 -= 1\n\n        npix = ind2 - ind1\n\n    elif mode == 'max':\n        # for ind1, figure out if pixel ind1-1 is partially included or not.\n        # do this by figuring out where wave1 is between ind1 and ind1-1.\n        frac = (wave1 - bins[ind1 - 1]) / (bins[ind1] - bins[ind1 - 1])\n        if frac < 0.5:\n            # ind1 is partially included\n            ind1 -= 1\n\n        # similar but reversed procedure for ind2\n        frac = (bins[ind2] - wave2) / (bins[ind2] - bins[ind2 - 1])\n        if frac < 0.5:\n            # ind2 is partially included\n            ind2 += 1\n\n        npix = ind2 - ind1\n\n    else:  # mode == 'none'\n        # calculate fractional indices\n        frac1 = ind1 - (bins[ind1] - wave1) / (bins[ind1] - bins[ind1 - 1])\n        frac2 = ind2 - (bins[ind2] - wave2) / (bins[ind2] - bins[ind2 - 1])\n        npix = frac2 - frac1\n\n    return npix", "response": "Calculate the number of pixels within a given wavelength range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to KLF 200.", "response": "async def connect(self):\n        \"\"\"Connect to KLF 200.\"\"\"\n        PYVLXLOG.warning(\"Connecting to KLF 200.\")\n        await self.connection.connect()\n        login = Login(pyvlx=self, password=self.config.password)\n        await login.do_api_call()\n        if not login.success:\n            raise PyVLXException(\"Login to KLF 200 failed, check credentials\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving version and protocol version from API.", "response": "async def update_version(self):\n        \"\"\"Retrieve version and protocol version from API.\"\"\"\n        get_version = GetVersion(pyvlx=self)\n        await get_version.do_api_call()\n        if not get_version.success:\n            raise PyVLXException(\"Unable to retrieve version\")\n        self.version = get_version.version\n        get_protocol_version = GetProtocolVersion(pyvlx=self)\n        await get_protocol_version.do_api_call()\n        if not get_protocol_version.success:\n            raise PyVLXException(\"Unable to retrieve protocol version\")\n        self.protocol_version = get_protocol_version.version\n        PYVLXLOG.warning(\n            \"Connected to: %s, protocol version: %s\",\n            self.version, self.protocol_version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a frame to the API via the connection.", "response": "async def send_frame(self, frame):\n        \"\"\"Send frame to API via connection.\"\"\"\n        if not self.connection.connected:\n            await self.connect()\n            await self.update_version()\n            await set_utc(pyvlx=self)\n            await house_status_monitor_enable(pyvlx=self)\n        self.connection.write(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread scene from configuration.", "response": "def from_config(cls, pyvlx, item):\n        \"\"\"Read scene from configuration.\"\"\"\n        name = item['name']\n        ident = item['id']\n        return cls(pyvlx, ident, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def refresh_token(self):\n        json_response = await self.api_call('auth', 'login', {'password': self.config.password}, add_authorization_token=False)\n        if 'token' not in json_response:\n            raise PyVLXException('no element token found in response: {0}'.format(json.dumps(json_response)))\n        self.token = json_response['token']", "response": "Refresh API token from KLF 200."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_body(action, params):\n        body = {}\n        body['action'] = action\n        if params is not None:\n            body['params'] = params\n        return body", "response": "Create http body for rest request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_frame_to_node(pyvlx, frame):\n    # pylint: disable=too-many-return-statements\n    if frame.node_type == NodeTypeWithSubtype.WINDOW_OPENER:\n        return Window(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name, rain_sensor=False)\n    if frame.node_type == NodeTypeWithSubtype.WINDOW_OPENER_WITH_RAIN_SENSOR:\n        return Window(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name, rain_sensor=True)\n    if frame.node_type == NodeTypeWithSubtype.ROLLER_SHUTTER or \\\n            frame.node_type == NodeTypeWithSubtype.DUAL_ROLLER_SHUTTER:\n        return RollerShutter(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name)\n    if frame.node_type == NodeTypeWithSubtype.INTERIOR_VENETIAN_BLIND or \\\n            frame.node_type == NodeTypeWithSubtype.VERTICAL_INTERIOR_BLINDS or \\\n            frame.node_type == NodeTypeWithSubtype.EXTERIOR_VENETIAN_BLIND or \\\n            frame.node_type == NodeTypeWithSubtype.LOUVER_BLIND:\n        return Blind(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name)\n    if frame.node_type == NodeTypeWithSubtype.VERTICAL_EXTERIOR_AWNING or \\\n            frame.node_type == NodeTypeWithSubtype.HORIZONTAL_AWNING:\n        return Awning(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name)\n    if frame.node_type == NodeTypeWithSubtype.ON_OFF_SWITCH:\n        return OnOffSwitch(pyvlx=pyvlx, node_id=frame.node_id, name=frame.name)\n\n    PYVLXLOG.warning(\"%s not implemented\", frame.node_type)\n    return None", "response": "Convert FrameGet [ All ] Node [ s ] InformationNotification into Node object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef temperature(self, what):\n        self._temperature = units.validate_quantity(what, u.K)", "response": "Set the temperature of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply emissivity to an existing beam to produce a thermal source spectrum.", "response": "def thermal_source(self):\n        \"\"\"Apply emissivity to an existing beam to produce a thermal\n        source spectrum (without optical counterpart).\n\n        Thermal source spectrum is calculated as follow:\n\n            #. Create a blackbody spectrum in PHOTLAM per square arcsec\n               with `temperature`.\n            #. Multiply the blackbody with `beam_fill_factor` and ``self``.\n\n        Returns\n        -------\n        sp : `~synphot.spectrum.SourceSpectrum`\n            Thermal source spectrum.\n\n        \"\"\"\n        sp = (SourceSpectrum(BlackBody1D, temperature=self.temperature) *\n              units.SR_PER_ARCSEC2 * self.beam_fill_factor * self)\n        sp.meta['temperature'] = self.temperature\n        sp.meta['beam_fill_factor'] = self.beam_fill_factor\n        return sp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Thermal Spectral Element from a file.", "response": "def from_file(cls, filename, temperature_key='DEFT',\n                  beamfill_key='BEAMFILL', **kwargs):\n        \"\"\"Creates a thermal spectral element from file.\n\n        .. note::\n\n            Only FITS format is supported.\n\n        Parameters\n        ----------\n        filename : str\n            Thermal spectral element filename.\n\n        temperature_key, beamfill_key : str\n            Keywords in FITS *table extension* that store temperature\n            (in Kelvin) and beam filling factor values.\n            Beam filling factor is set to 1 if its keyword is missing.\n\n        kwargs : dict\n            Keywords acceptable by :func:`~synphot.specio.read_fits_spec`.\n\n        Returns\n        -------\n        th : `ThermalSpectralElement`\n            Empirical thermal spectral element.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Invalid inputs.\n\n        \"\"\"\n        if not (filename.endswith('fits') or filename.endswith('fit')):\n            raise exceptions.SynphotError('Only FITS format is supported.')\n\n        # Extra info from table header\n        ext = kwargs.get('ext', 1)\n        tab_hdr = fits.getheader(filename, ext=ext)\n\n        temperature = tab_hdr.get(temperature_key)\n        if temperature is None:\n            raise exceptions.SynphotError(\n                'Missing {0} keyword.'.format(temperature_key))\n\n        beam_fill_factor = tab_hdr.get('BEAMFILL', 1)\n\n        if 'flux_unit' not in kwargs:\n            kwargs['flux_unit'] = cls._internal_flux_unit\n\n        if 'flux_col' not in kwargs:\n            kwargs['flux_col'] = 'EMISSIVITY'\n\n        header, wavelengths, em = specio.read_spec(filename, **kwargs)\n        return cls(\n            Empirical1D, temperature, beam_fill_factor=beam_fill_factor,\n            points=wavelengths, lookup_table=em, meta={'header': header})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_payload(self, payload):\n        self.status = AllNodesInformationStatus(payload[0])\n        self.number_of_nodes = payload[1]", "response": "Init frame from binary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_payload(self):\n        payload = bytes()\n        payload += bytes([self.node_id])\n        payload += bytes([self.order >> 8 & 255, self.order & 255])\n        payload += bytes([self.placement])\n        payload += bytes(string_to_bytes(self.name, 64))\n        payload += bytes([self.velocity.value])\n        payload += bytes([self.node_type.value >> 8 & 255, self.node_type.value & 255])\n        payload += bytes([self.product_group])\n        payload += bytes([self.product_type])\n        payload += bytes([self.node_variation.value])\n        payload += bytes([self.power_mode])\n        payload += bytes([self.build_number])\n        payload += bytes(self._serial_number)\n        payload += bytes([self.state])\n        payload += bytes(self.current_position.raw)\n        payload += bytes(self.target.raw)\n        payload += bytes(self.current_position_fp1.raw)\n        payload += bytes(self.current_position_fp2.raw)\n        payload += bytes(self.current_position_fp3.raw)\n        payload += bytes(self.current_position_fp4.raw)\n        payload += bytes([self.remaining_time >> 8 & 255, self.remaining_time & 255])\n        payload += struct.pack(\">I\", self.timestamp)\n        payload += bytes(self.alias_array)\n\n        return payload", "response": "Return the payload of the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting internal raw state from parameter.", "response": "def from_parameter(self, parameter):\n        \"\"\"Set internal raw state from parameter.\"\"\"\n        if not isinstance(parameter, Parameter):\n            raise Exception(\"parameter::from_parameter_wrong_object\")\n        self.raw = parameter.raw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_int(value):\n        if not isinstance(value, int):\n            raise PyVLXException(\"value_has_to_be_int\")\n        if not Parameter.is_valid_int(value):\n            raise PyVLXException(\"value_out_of_range\")\n        return bytes([value >> 8 & 255, value & 255])", "response": "Create raw out of position vlaue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntesting if value can be rendered out of int.", "response": "def is_valid_int(value):\n        \"\"\"Test if value can be rendered out of int.\"\"\"\n        if 0 <= value <= Parameter.MAX:  # This includes ON and OFF\n            return True\n        if value == Parameter.UNKNOWN_VALUE:\n            return True\n        if value == Parameter.CURRENT_POSITION:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_raw(raw):\n        if not isinstance(raw, bytes):\n            raise PyVLXException(\"Position::raw_must_be_bytes\")\n        if len(raw) != 2:\n            raise PyVLXException(\"Position::raw_must_be_two_bytes\")\n        if raw != Position.from_int(Position.CURRENT_POSITION) and \\\n                raw != Position.from_int(Position.UNKNOWN_VALUE) and \\\n                Position.to_int(raw) > Position.MAX:\n            raise PyVLXException(\"position::raw_exceed_limit\", raw=raw)\n        return raw", "response": "Test if raw packets are valid for initialization of Position."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate raw value out of percent position.", "response": "def from_percent(position_percent):\n        \"\"\"Create raw value out of percent position.\"\"\"\n        if not isinstance(position_percent, int):\n            raise PyVLXException(\"Position::position_percent_has_to_be_int\")\n        if position_percent < 0:\n            raise PyVLXException(\"Position::position_percent_has_to_be_positive\")\n        if position_percent > 100:\n            raise PyVLXException(\"Position::position_percent_out_of_range\")\n        return bytes([position_percent*2, 0])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef product(self):\n        if self.product_group == 14 and self.product_type == 3:\n            return \"KLF 200\"\n        return \"Unknown Product: {}:{}\".format(self.product_group, self.product_type)", "response": "Return product as human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the payload of the current software version.", "response": "def get_payload(self):\n        \"\"\"Return Payload.\"\"\"\n        ret = self._software_version\n        ret += bytes([self.hardware_version, self.product_group, self.product_type])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the payload of the current object.", "response": "def get_payload(self):\n        \"\"\"Return Payload.\"\"\"\n        ret = bytes([self.session_id >> 8 & 255, self.session_id & 255])\n        ret += bytes([self.originator.value])\n        ret += bytes([self.priority.value])\n        ret += bytes([self.scene_id])\n        ret += bytes([self.velocity.value])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_payload(self, payload):\n        self.session_id = payload[0]*256 + payload[1]\n        self.originator = Originator(payload[2])\n        self.priority = Priority(payload[3])\n        self.scene_id = payload[4]\n        self.velocity = Velocity(payload[5])", "response": "Init frame from binary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the payload of the message.", "response": "def get_payload(self):\n        \"\"\"Return Payload.\"\"\"\n        ret = bytes([self.status.value])\n        ret += bytes([self.session_id >> 8 & 255, self.session_id & 255])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_payload(self):\n        # Session id\n        ret = bytes([self.session_id >> 8 & 255, self.session_id & 255])\n        ret += bytes([self.originator.value])\n        ret += bytes([self.priority.value])\n        ret += bytes([0])  # ParameterActive pointing to main parameter (MP)\n        # FPI 1+2\n        ret += bytes([0])\n        ret += bytes([0])\n\n        # Main parameter + functional parameter\n        ret += bytes(self.parameter)\n        ret += bytes(32)\n\n        # Nodes array: Number of nodes + node array + padding\n        ret += bytes([len(self.node_ids)])  # index array count\n        ret += bytes(self.node_ids) + bytes(20-len(self.node_ids))\n\n        # Priority  Level Lock\n        ret += bytes([0])\n        # Priority Level information 1+2\n        ret += bytes([0, 0])\n        # Locktime\n        ret += bytes([0])\n        return ret", "response": "Return the payload of the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_payload(self):\n        ret = bytes([self.session_id >> 8 & 255, self.session_id & 255])\n        ret += bytes([self.status_id])\n        ret += bytes([self.index_id])\n        ret += bytes([self.node_parameter])\n        ret += bytes([self.parameter_value >> 8 & 255, self.parameter_value & 255])\n\n        # XXX: Missing implementation of run_status, status_reply and information_code\n        ret += bytes(6)\n        return ret", "response": "Return the payload of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_payload(self):\n        ret = bytes([self.session_id >> 8 & 255, self.session_id & 255])\n        ret += bytes([self.index_id])\n        ret += bytes([self.node_parameter])\n        ret += bytes([self.seconds >> 8 & 255, self.seconds & 255])\n        return ret", "response": "Return the payload of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def main(loop):\n    # Setting debug\n    PYVLXLOG.setLevel(logging.DEBUG)\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.DEBUG)\n    PYVLXLOG.addHandler(stream_handler)\n\n    # Connecting to KLF 200\n    pyvlx = PyVLX('pyvlx.yaml', loop=loop)\n    await pyvlx.load_scenes()\n    await pyvlx.load_nodes()\n\n    # and wait, increase this timeout if you want to\n    # log for a longer time.:)\n    await asyncio.sleep(90)\n\n    # Cleanup, KLF 200 is terrible in handling lost connections\n    await pyvlx.disconnect()", "response": "Log packets from Bus."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def rename(self, name):\n        set_node_name = SetNodeName(pyvlx=self.pyvlx, node_id=self.node_id, name=name)\n        await set_node_name.do_api_call()\n        if not set_node_name.success:\n            raise PyVLXException(\"Unable to rename node\")\n        self.name = name", "response": "Change name of node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset window to desired position.", "response": "async def set_position(self, position, wait_for_completion=True):\n        \"\"\"Set window to desired position.\n\n        Parameters:\n            * position: Position object containing the target position.\n            * wait_for_completion: If set, function will return\n                after device has reached target position.\n\n        \"\"\"\n        command_send = CommandSend(\n            pyvlx=self.pyvlx,\n            wait_for_completion=wait_for_completion,\n            node_id=self.node_id,\n            parameter=position)\n        await command_send.do_api_call()\n        if not command_send.success:\n            raise PyVLXException(\"Unable to send command\")\n        await self.after_update()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen window. Parameters: * wait_for_completion: If set, function will return after device has reached target position.", "response": "async def open(self, wait_for_completion=True):\n        \"\"\"Open window.\n\n        Parameters:\n            * wait_for_completion: If set, function will return\n                after device has reached target position.\n\n        \"\"\"\n        await self.set_position(\n            position=Position(position_percent=0),\n            wait_for_completion=wait_for_completion)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def close(self, wait_for_completion=True):\n        await self.set_position(\n            position=Position(position_percent=100),\n            wait_for_completion=wait_for_completion)", "response": "Close window.\n\n        Parameters:\n            * wait_for_completion: If set, function will return\n                after device has reached target position."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def stop(self, wait_for_completion=True):\n        await self.set_position(\n            position=CurrentPosition(),\n            wait_for_completion=wait_for_completion)", "response": "Stop window.\n\n        Parameters:\n            * wait_for_completion: If set, function will return\n                after device has reached target position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sampleset of a model or None if undefined.", "response": "def _get_sampleset(model):\n    \"\"\"Return sampleset of a model or `None` if undefined.\n    Model could be a real model or evaluated sampleset.\"\"\"\n    if isinstance(model, Model):\n        if hasattr(model, 'sampleset'):\n            w = model.sampleset()\n        else:\n            w = None\n    else:\n        w = model  # Already a sampleset\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the weighted version of the two models.", "response": "def _shift_wavelengths(model1, model2):\n    \"\"\"One of the models is either ``RedshiftScaleFactor`` or ``Scale``.\n\n    Possible combos::\n\n        RedshiftScaleFactor | Model\n        Scale | Model\n        Model | Scale\n\n    \"\"\"\n    if isinstance(model1, _models.RedshiftScaleFactor):\n        val = _get_sampleset(model2)\n        if val is None:\n            w = val\n        else:\n            w = model1.inverse(val)\n    elif isinstance(model1, _models.Scale):\n        w = _get_sampleset(model2)\n    else:\n        w = _get_sampleset(model1)\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets optimal wavelengths for sampling a given model.", "response": "def get_waveset(model):\n    \"\"\"Get optimal wavelengths for sampling a given model.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.Model`\n        Model.\n\n    Returns\n    -------\n    waveset : array-like or `None`\n        Optimal wavelengths. `None` if undefined.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Invalid model.\n\n    \"\"\"\n    if not isinstance(model, Model):\n        raise SynphotError('{0} is not a model.'.format(model))\n\n    if isinstance(model, _CompoundModel):\n        waveset = model._tree.evaluate(WAVESET_OPERATORS, getter=None)\n    else:\n        waveset = _get_sampleset(model)\n\n    return waveset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns metadata of a model.", "response": "def _get_meta(model):\n    \"\"\"Return metadata of a model.\n    Model could be a real model or evaluated metadata.\"\"\"\n    if isinstance(model, Model):\n        w = model.meta\n    else:\n        w = model  # Already metadata\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the metadata for a given model.", "response": "def get_metadata(model):\n    \"\"\"Get metadata for a given model.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.Model`\n        Model.\n\n    Returns\n    -------\n    metadata : dict\n        Metadata for the model.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Invalid model.\n\n    \"\"\"\n    if not isinstance(model, Model):\n        raise SynphotError('{0} is not a model.'.format(model))\n\n    if isinstance(model, _CompoundModel):\n        metadata = model._tree.evaluate(METADATA_OPERATORS, getter=None)\n    else:\n        metadata = deepcopy(model.meta)\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lambda_max(self):\n        return ((const.b_wien.value / self.temperature) * u.m).to(u.AA).value", "response": "Peak wavelength in Angstrom when the curve is expressed as\n        power density."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default bounding box for the current state of the logarithmic logarithm of the logarithmic logarithm", "response": "def bounding_box(self, factor=10.0):\n        \"\"\"Tuple defining the default ``bounding_box`` limits,\n        ``(x_low, x_high)``.\n\n        .. math::\n\n            x_{\\\\textnormal{low}} = 0\n\n            x_{\\\\textnormal{high}} = \\\\log(\\\\lambda_{\\\\textnormal{max}} \\\\;\\\n            (1 + \\\\textnormal{factor}))\n\n        Parameters\n        ----------\n        factor : float\n            Used to calculate ``x_high``.\n\n        \"\"\"\n        w0 = self.lambda_max\n        return (w0 * 0, np.log10(w0 + factor * w0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sampleset(self, factor_bbox=10.0, num=1000):\n        w1, w2 = self.bounding_box(factor=factor_bbox)\n\n        if self._n_models == 1:\n            w = np.logspace(w1, w2, num)\n        else:\n            w = list(map(partial(np.logspace, num=num), w1, w2))\n\n        return np.asarray(w)", "response": "Return x array that samples the feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the model. Parameters ---------- x : number or ndarray Wavelengths in Angstrom. temperature : number Temperature in Kelvin. Returns ------- y : number or ndarray Blackbody radiation in PHOTLAM per steradian.", "response": "def evaluate(x, temperature):\n        \"\"\"Evaluate the model.\n\n        Parameters\n        ----------\n        x : number or ndarray\n            Wavelengths in Angstrom.\n\n        temperature : number\n            Temperature in Kelvin.\n\n        Returns\n        -------\n        y : number or ndarray\n            Blackbody radiation in PHOTLAM per steradian.\n\n        \"\"\"\n        if ASTROPY_LT_2_0:\n            from astropy.analytic_functions.blackbody import blackbody_nu\n        else:\n            from astropy.modeling.blackbody import blackbody_nu\n\n        # Silence Numpy\n        old_np_err_cfg = np.seterr(all='ignore')\n\n        wave = np.ascontiguousarray(x) * u.AA\n        bbnu_flux = blackbody_nu(wave, temperature)\n        bbflux = (bbnu_flux * u.sr).to(\n            units.PHOTLAM, u.spectral_density(wave)) / u.sr  # PHOTLAM/sr\n\n        # Restore Numpy settings\n        np.seterr(**old_np_err_cfg)\n\n        return bbflux.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the model. Parameters ---------- x : number or ndarray Wavelengths in Angstrom. temperature : number Temperature in Kelvin. Returns ------- y : number or ndarray Blackbody radiation in PHOTLAM.", "response": "def evaluate(self, x, temperature):\n        \"\"\"Evaluate the model.\n\n        Parameters\n        ----------\n        x : number or ndarray\n            Wavelengths in Angstrom.\n\n        temperature : number\n            Temperature in Kelvin.\n\n        Returns\n        -------\n        y : number or ndarray\n            Blackbody radiation in PHOTLAM.\n\n        \"\"\"\n        bbflux = super(BlackBodyNorm1D, self).evaluate(x, temperature)\n        return bbflux * self._omega"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate sampleset for each model.", "response": "def _calc_sampleset(w1, w2, step, minimal):\n        \"\"\"Calculate sampleset for each model.\"\"\"\n        if minimal:\n            arr = [w1 - step, w1, w2, w2 + step]\n        else:\n            arr = np.arange(w1 - step, w2 + step + step, step)\n\n        return arr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a numpy array that samples the feature.", "response": "def sampleset(self, step=0.01, minimal=False):\n        \"\"\"Return ``x`` array that samples the feature.\n\n        Parameters\n        ----------\n        step : float\n            Distance of first and last points w.r.t. bounding box.\n\n        minimal : bool\n            Only return the minimal points needed to define the box;\n            i.e., box edges and a point outside on each side.\n\n        \"\"\"\n        w1, w2 = self.bounding_box\n\n        if self._n_models == 1:\n            w = self._calc_sampleset(w1, w2, step, minimal)\n        else:\n            w = list(map(partial(\n                self._calc_sampleset, step=step, minimal=minimal), w1, w2))\n\n        return np.asarray(w)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evaluate(self, x, *args):\n        a = (self.amplitude * np.ones_like(x)) * self._flux_unit\n        y = units.convert_flux(x, a, units.PHOTLAM)\n        return y.value", "response": "One dimensional constant flux model function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving negative flux from the lookup table.", "response": "def _process_neg_flux(self, x, y):\n        \"\"\"Remove negative flux.\"\"\"\n\n        if self._keep_neg:  # Nothing to do\n            return y\n\n        old_y = None\n\n        if np.isscalar(y):  # pragma: no cover\n            if y < 0:\n                n_neg = 1\n                old_x = x\n                old_y = y\n                y = 0\n        else:\n            x = np.asarray(x)  # In case input is just pure list\n            y = np.asarray(y)\n            i = np.where(y < 0)\n            n_neg = len(i[0])\n            if n_neg > 0:\n                old_x = x[i]\n                old_y = y[i]\n                y[i] = 0\n\n        if old_y is not None:\n            warn_str = ('{0} bin(s) contained negative flux or throughput'\n                        '; it/they will be set to zero.'.format(n_neg))\n            warn_str += '\\n  points: {0}\\n  lookup_table: {1}'.format(\n                old_x, old_y)  # Extra info\n            self.meta['warnings'].update({'NegativeFlux': warn_str})\n            warnings.warn(warn_str, AstropyUserWarning)\n\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the model. Parameters ---------- inputs : number or ndarray Wavelengths in same unit as ``points``. Returns ------- y : number or ndarray Flux or throughput in same unit as ``lookup_table``.", "response": "def evaluate(self, inputs):\n        \"\"\"Evaluate the model.\n\n        Parameters\n        ----------\n        inputs : number or ndarray\n            Wavelengths in same unit as ``points``.\n\n        Returns\n        -------\n        y : number or ndarray\n            Flux or throughput in same unit as ``lookup_table``.\n\n        \"\"\"\n        y = super(Empirical1D, self).evaluate(inputs)\n\n        # Assume NaN at both ends need to be extrapolated based on\n        # nearest end point.\n        if self.fill_value is np.nan:\n            # Cannot use sampleset() due to ExtinctionModel1D\n            x = np.squeeze(self.points)\n\n            if np.isscalar(y):  # pragma: no cover\n                if inputs < x[0]:\n                    y = self.lookup_table[0]\n                elif inputs > x[-1]:\n                    y = self.lookup_table[-1]\n            else:\n                y[inputs < x[0]] = self.lookup_table[0]\n                y[inputs > x[-1]] = self.lookup_table[-1]\n\n        return self._process_neg_flux(inputs, y)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the Gaussian absorption model function.", "response": "def evaluate(x, amplitude, mean, stddev):\n        \"\"\"\n        GaussianAbsorption1D model function.\n        \"\"\"\n        return 1.0 - Gaussian1D.evaluate(x, amplitude, mean, stddev)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning x array that samples the feature.", "response": "def sampleset(self, factor_step=0.05, **kwargs):\n        \"\"\"Return ``x`` array that samples the feature.\n\n        Parameters\n        ----------\n        factor_step : float\n            Factor for sample step calculation. The step is calculated\n            using ``factor_step * self.fwhm``.\n\n        kwargs : dict\n            Keyword(s) for ``bounding_box`` calculation.\n\n        \"\"\"\n        w1, w2 = self.bounding_box(**kwargs)\n        dw = factor_step * self.fwhm\n\n        if self._n_models == 1:\n            w = np.arange(w1, w2, dw)\n        else:\n            w = list(map(np.arange, w1, w2, dw))\n\n        return np.asarray(w)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the flux in PHOTLAM. Assume input wavelength is in Angstrom.", "response": "def evaluate(self, x, *args):\n        \"\"\"Return flux in PHOTLAM. Assume input wavelength is in Angstrom.\"\"\"\n        xx = x / self.x_0\n        y = (self.amplitude * xx ** (-self.alpha)) * self._flux_unit\n        flux = units.convert_flux(x, y, units.PHOTLAM)\n        return flux.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sampleset(self):\n        x1, x4 = self.bounding_box\n        dw = self.width * 0.5\n        x2 = self.x_0 - dw\n        x3 = self.x_0 + dw\n\n        if self._n_models == 1:\n            w = [x1, x2, x3, x4]\n        else:\n            w = list(zip(x1, x2, x3, x4))\n\n        return np.asarray(w)", "response": "Return x array that samples the feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the javascript expression that will be used to display the order model for the current order.", "response": "def get_payment_request(self, cart, request):\n        \"\"\"\n        From the given request, add a snippet to the page.\n        \"\"\"\n        try:\n            self.charge(cart, request)\n            thank_you_url = OrderModel.objects.get_latest_url()\n            js_expression = 'window.location.href=\"{}\";'.format(thank_you_url)\n            return js_expression\n        except (KeyError, stripe.error.StripeError) as err:\n            raise ValidationError(err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses the Stripe token from the request and charge immediately.", "response": "def charge(self, cart, request):\n        \"\"\"\n        Use the Stripe token from the request and charge immediately.\n        This view is invoked by the Javascript function `scope.charge()` delivered\n        by `get_payment_request`.\n        \"\"\"\n        token_id = cart.extra['payment_extra_data']['token_id']\n        if LooseVersion(SHOP_VERSION) < LooseVersion('0.11'):\n            charge = stripe.Charge.create(\n                amount=cart.total.as_integer(),\n                currency=cart.total.currency,\n                source=token_id,\n                description=settings.SHOP_STRIPE['PURCHASE_DESCRIPTION']\n            )\n            if charge['status'] == 'succeeded':\n                order = OrderModel.objects.create_from_cart(cart, request)\n                order.add_stripe_payment(charge)\n                order.save()\n        else:\n            order = OrderModel.objects.create_from_cart(cart, request)\n            charge = stripe.Charge.create(\n                amount=cart.total.as_integer(),\n                currency=cart.total.currency,\n                source=token_id,\n                transfer_group=order.get_number(),\n                description=settings.SHOP_STRIPE['PURCHASE_DESCRIPTION'],\n            )\n            if charge['status'] == 'succeeded':\n                order.populate_from_cart(cart, request)\n                order.add_stripe_payment(charge)\n                order.save()\n\n        if charge['status'] != 'succeeded':\n            msg = \"Stripe returned status '{status}' for id: {id}\"\n            raise stripe.error.InvalidRequestError(msg.format(**charge))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refund_payment(self):\n        Money = MoneyMaker(self.currency)\n        filter_kwargs = {\n            'transaction_id__startswith': 'ch_',\n            'payment_method': StripePayment.namespace,\n        }\n        for payment in self.orderpayment_set.filter(**filter_kwargs):\n            refund = stripe.Refund.create(charge=payment.transaction_id)\n            if refund['status'] == 'succeeded':\n                amount = Money(refund['amount']) / Money.subunits\n                OrderPayment.objects.create(order=self, amount=-amount, transaction_id=refund['id'],\n                                            payment_method=StripePayment.namespace)\n\n        del self.amount_paid  # to invalidate the cache\n        if self.amount_paid:\n            # proceed with other payment service providers\n            super(OrderWorkflowMixin, self).refund_payment()", "response": "Refund the order payment using Stripe s refunding API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self):\n        self.service.create()\n\n        # Set env vars for immediate use\n        zone_id = predix.config.get_env_key(self.use_class, 'zone_id')\n        zone = self.service.settings.data['zone']['http-header-value']\n        os.environ[zone_id] = zone\n\n        uri = predix.config.get_env_key(self.use_class, 'uri')\n        os.environ[uri] = self.service.settings.data['uri']", "response": "Create an instance of the US Weather Forecast Service with typical starting settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_to_manifest(self, manifest):\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n        zone_id = predix.config.get_env_key(self.use_class, 'zone_id')\n        manifest.add_env_var(zone_id,\n                self.service.settings.data['zone']['http-header-value'])\n\n        uri = predix.config.get_env_key(self.use_class, 'uri')\n        manifest.add_env_var(uri, self.service.settings.data['uri'])\n\n        manifest.write_manifest()", "response": "Add useful details to the manifest about this service."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the create is in progress.", "response": "def _create_in_progress(self):\n        \"\"\"\n        Creating this service is handled asynchronously so this method will\n        simply check if the create is in progress.  If it is not in progress,\n        we could probably infer it either failed or succeeded.\n        \"\"\"\n        instance = self.service.service.get_instance(self.service.name)\n        if (instance['last_operation']['state'] == 'in progress' and\n           instance['last_operation']['type'] == 'create'):\n               return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, max_wait=180, **kwargs):\n        # Will need to wait for the service to be provisioned before can add\n        # service keys and get env details.\n        self.service.create(async=True, create_keys=False)\n        while self._create_in_progress() and max_wait > 0:\n            time.sleep(1)\n            max_wait -= 1\n\n        # Now get the service env (via service keys)\n        cfg = self.service._get_service_config()\n        self.service.settings.save(cfg)\n\n        host = predix.config.get_env_key(self.use_class, 'host')\n        os.environ[host] = self.service.settings.data['host']\n\n        password = predix.config.get_env_key(self.use_class, 'password')\n        os.environ[password] = self.service.settings.data['password']\n\n        port = predix.config.get_env_key(self.use_class, 'port')\n        os.environ[port] = str(self.service.settings.data['port'])", "response": "Create an instance of the Predix Cache Service with their typical\n        starting settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding useful details to the manifest for this service so that it can be used in an application.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service so\n        that it can be used in an application.\n\n        :param manifest: A predix.admin.app.Manifest object instance\n            that manages reading/writing manifest config for a\n            cloud foundry app.\n        \"\"\"\n        manifest.add_service(self.service.name)\n\n        host = predix.config.get_env_key(self.use_class, 'host')\n        manifest.add_env_var(host, self.service.settings.data['host'])\n\n        password = predix.config.get_env_key(self.use_class, 'password')\n        manifest.add_env_var(password, self.service.settings.data['password'])\n\n        port = predix.config.get_env_key(self.use_class, 'port')\n        manifest.add_env_var(port, self.service.settings.data['port'])\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the uri for an existing instance.", "response": "def _get_uri(self):\n        \"\"\"\n        Will return the uri for an existing instance.\n        \"\"\"\n        if not self.service.exists():\n            logging.warning(\"Service does not yet exist.\")\n\n        return self.service.settings.data['uri']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the zone id for an existing instance.", "response": "def _get_zone_id(self):\n        \"\"\"\n        Will return the zone id for an existing instance.\n        \"\"\"\n        if not self.service.exists():\n            logging.warning(\"Service does not yet exist.\")\n\n        return self.service.settings.data['zone']['http-header-value']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of the Access Control Service with the typical starting settings.", "response": "def create(self):\n        \"\"\"\n        Create an instance of the Access Control Service with the typical\n        starting settings.\n        \"\"\"\n        self.service.create()\n\n        # Set environment variables for immediate use\n        predix.config.set_env_value(self.use_class, 'uri', self._get_uri())\n        predix.config.set_env_value(self.use_class, 'zone_id',\n                self._get_zone_id())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngrant the given client id all the scopes and authorities needed to work with the access control service.", "response": "def grant_client(self, client_id):\n        \"\"\"\n        Grant the given client id all the scopes and authorities\n        needed to work with the access control service.\n        \"\"\"\n        zone = self.service.settings.data['zone']['oauth-scope']\n\n        scopes = ['openid', zone,\n                  'acs.policies.read', 'acs.attributes.read',\n                  'acs.policies.write', 'acs.attributes.write']\n\n        authorities = ['uaa.resource', zone,\n                  'acs.policies.read', 'acs.policies.write',\n                  'acs.attributes.read', 'acs.attributes.write']\n\n        self.service.uaa.uaac.update_client_grants(client_id, scope=scopes,\n                authorities=authorities)\n\n        return self.service.uaa.uaac.get_client(client_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd useful details to the manifest about this service.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service\n        so that it can be used in an application.\n\n        :param manifest: An predix.admin.app.Manifest object\n            instance that manages reading/writing manifest config\n            for a cloud foundry app.\n        \"\"\"\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n        uri = predix.config.get_env_key(self.use_class, 'uri')\n        manifest.add_env_var(uri, self._get_uri())\n\n        zone_id = predix.config.get_env_key(self.use_class, 'zone_id')\n        manifest.add_env_var(zone_id, self._get_zone_id())\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, path, data=None, params=None):\n        uri = self.config.get_target() + path\n        headers = {\n            'Authorization': self.config.get_access_token()\n            }\n\n        logging.debug(\"URI=DELETE \" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n\n        response = self.session.delete(\n            uri, headers=headers, params=params, data=json.dumps(data))\n\n        if response.status_code == 204:\n            return response\n        else:\n            logging.debug(\"STATUS=\" + str(response.status_code))\n            logging.debug(\"CONTENT=\" + str(response.content))\n            response.raise_for_status()", "response": "Generic DELETE with headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a flat list of the names for the organizations user belongs to.", "response": "def get_orgs(self):\n        \"\"\"\n        Returns a flat list of the names for the organizations\n        user belongs.\n        \"\"\"\n        orgs = []\n        for resource in self._get_orgs()['resources']:\n            orgs.append(resource['entity']['name'])\n\n        return orgs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_apps(self):\n        apps = []\n        for resource in self._get_apps()['resources']:\n            apps.append(resource['entity']['name'])\n\n        return apps", "response": "Returns a flat list of the names for the apps in\n        the organization."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_user(self, user_name, role='user'):\n        role_uri = self._get_role_uri(role=role)\n        return self.api.put(path=role_uri, data={'username': user_name})", "response": "Adds a user to the org."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_user(self, user_name, role):\n        role_uri = self._get_role_uri(role=role)\n        return self.api.delete(path=role_uri, data={'username': user_name})", "response": "Calls CF s remove user with org\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a message to the rx_queue", "response": "def add_message(self, id, body, tags=False):\n        \"\"\"\n        add messages to the rx_queue\n        :param id: str message Id\n        :param body: str the message body\n        :param tags: dict[string->string] tags to be associated with the message\n        :return: self\n        \"\"\"\n        if not tags:\n            tags = {}\n        try:\n            self._tx_queue_lock.acquire()\n            self._tx_queue.append(\n                EventHub_pb2.Message(id=id, body=body, tags=tags, zone_id=self.eventhub_client.zone_id))\n        finally:\n            self._tx_queue_lock.release()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npublishing all messages that have been added to the queue for configured protocolVersion.", "response": "def publish_queue(self):\n        \"\"\"\n        Publish all messages that have been added to the queue for configured protocol\n        :return: None\n        \"\"\"\n        self.last_send_time = time.time()\n        try:\n            self._tx_queue_lock.acquire()\n            start_length = len(self._rx_queue)\n            publish_amount = len(self._tx_queue)\n            if self.config.protocol == PublisherConfig.Protocol.GRPC:\n                self._publish_queue_grpc()\n            else:\n                self._publish_queue_wss()\n            self._tx_queue = []\n        finally:\n            self._tx_queue_lock.release()\n\n        if self.config.publish_type == self.config.Type.SYNC:\n            start_time = time.time()\n            while time.time() - start_time < self.config.sync_timeout and \\\n                                    len(self._rx_queue) - start_length < publish_amount:\n                pass\n            return self._rx_queue"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ack_generator(self):\n        if self.config.is_sync():\n            logging.warning('cant use generator on a sync publisher')\n            return\n        while self._run_ack_generator:\n            while len(self._rx_queue) != 0:\n                logging.debug('yielding to client')\n                yield self._rx_queue.pop(0)\n        return", "response": "generator for acks to yield messages to the client"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _auto_send(self):\n        while True:\n            if time.time() - self.last_send_time > self.config.async_auto_send_interval_millis or \\\n                            len(self._tx_queue) >= self.config.async_auto_send_amount:\n                self.publish_queue()", "response": "auto send blocking function"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_publish_headers(self):\n        headers = {\n            'predix-zone-id': self.eventhub_client.zone_id\n        }\n        token = self.eventhub_client.service._get_bearer_token()\n        if self.config.is_grpc():\n            headers['authorization'] = token[(token.index(' ') + 1):]\n        else:\n            headers['authorization'] = token\n\n        if self.config.topic == '':\n            headers['topic'] = self.eventhub_client.zone_id + '_topic'\n        else:\n            headers['topic'] = self.config.topic\n\n        if self.config.publish_type == self.config.Type.SYNC:\n            headers['sync-acks'] = 'true'\n        else:\n            headers['sync-acks'] = 'false'\n            headers['send-acks-interval'] = str(self.config.async_cache_ack_interval_millis)\n            headers['acks'] = str(self.config.async_enable_acks).lower()\n            headers['nacks'] = str(self.config.async_enable_nacks_only).lower()\n            headers['cache-acks'] = str(self.config.async_cache_acks_and_nacks).lower()\n        return headers", "response": "Generate the headers to send to the event hub service based on the provided config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_grpc_publisher(self):\n        self._stub = EventHub_pb2_grpc.PublisherStub(channel=self._channel)\n        self.grpc_manager = Eventhub.GrpcManager(stub_call=self._stub.send,\n                                                 on_msg_callback=self._publisher_callback,\n                                                 metadata=self._generate_publish_headers().items())", "response": "Initializes the grpc publisher"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the messages in the tx queue to the GRPC manager.", "response": "def _publish_queue_grpc(self):\n        \"\"\"\n        send the messages in the tx queue to the GRPC manager\n        :return: None\n        \"\"\"\n        messages = EventHub_pb2.Messages(msg=self._tx_queue)\n        publish_request = EventHub_pb2.PublishRequest(messages=messages)\n        self.grpc_manager.send_message(publish_request)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the messages down the web socket connection as a json object", "response": "def _publish_queue_wss(self):\n        \"\"\"\n        send the messages down the web socket connection as a json object\n        :return: None\n        \"\"\"\n\n        msg = []\n        for m in self._tx_queue:\n            msg.append({'id': m.id, 'body': m.body, 'zone_id': m.zone_id})\n        self._ws.send(json.dumps(msg), opcode=websocket.ABNF.OPCODE_BINARY)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_publisher_ws(self):\n        logging.debug(\"Initializing new web socket connection.\")\n\n        url = ('wss://%s/v1/stream/messages/' % self.eventhub_client.host)\n\n        headers = self._generate_publish_headers()\n\n        logging.debug(\"URL=\" + str(url))\n        logging.debug(\"HEADERS=\" + str(headers))\n\n        websocket.enableTrace(False)\n        self._ws = websocket.WebSocketApp(url,\n                                          header=headers,\n                                          on_message=self._on_ws_message,\n                                          on_open=self._on_ws_open,\n                                          on_close=self._on_ws_close)\n        self._ws_thread = threading.Thread(target=self._ws.run_forever, kwargs={'ping_interval': 30})\n        self._ws_thread.daemon = True\n        self._ws_thread.start()\n        time.sleep(1)", "response": "Create a new web socket connection with proper headers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_ws_message(self, ws, message):\n        logging.debug(message)\n        json_list = json.loads(message)\n        for rx_ack in json_list:\n            ack = EventHub_pb2.Ack()\n            for key, value in rx_ack.items():\n                setattr(ack, key, value)\n            self._publisher_callback(ack)", "response": "This function is called when a message is received from the websocket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self):\n        self.service.create()\n        os.environ[self.__module__ + '.uri'] = self.service.settings.data['url']\n        os.environ[self.__module__ + '.zone_id'] = self.get_predix_zone_id()", "response": "Create an instance of the Parking Planning Service with the typical starting settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds useful details to the manifest about this service.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service\n        so that it can be used in an application.\n\n        :param manifest: An predix.admin.app.Manifest object\n            instance that manages reading/writing manifest config\n            for a cloud foundry app.\n        \"\"\"\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n        manifest.add_env_var(self.__module__ + '.uri',\n                self.service.settings.data['url'])\n        manifest.add_env_var(self.__module__ + '.zone_id',\n                self.get_predix_zone_id())\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_manifest(self, encrypted=None):\n        with open(self.manifest_path, 'r') as input_file:\n            self.manifest = yaml.safe_load(input_file)\n            if 'env' not in self.manifest:\n                self.manifest['env'] = {}\n            if 'services' not in self.manifest:\n                self.manifest['services'] = []\n\n            # If manifest is encrypted, use manifest key to\n            # decrypt each value before storing in memory.\n\n            if 'PREDIXPY_ENCRYPTED' in self.manifest['env']:\n                self.encrypted = True\n\n            if encrypted or self.encrypted:\n                key = predix.config.get_crypt_key(self.manifest_key)\n                f = Fernet(key)\n\n                for var in self.manifest['env'].keys():\n                    value = f.decrypt(bytes(self.manifest['env'][var], 'utf-8'))\n                    self.manifest['env'][var] = value.decode('utf-8')\n\n            self.app_name = self.manifest['applications'][0]['name']\n\n            input_file.close()", "response": "Read an existing manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_manifest(self):\n        self.manifest = {}\n        self.manifest['applications'] = [{'name': self.app_name}]\n        self.manifest['services'] = []\n        self.manifest['env'] = {\n                'PREDIXPY_VERSION': str(predix.version),\n                }\n\n        self.write_manifest()", "response": "Create a new manifest and write it to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn contents of the manifest where environment variables that are secret will be encrypted without modifying the existing state in memory which will remain unencrypted.", "response": "def _get_encrypted_manifest(self):\n        \"\"\"\n        Returns contents of the manifest where environment variables\n        that are secret will be encrypted without modifying the existing\n        state in memory which will remain unencrypted.\n        \"\"\"\n        key = predix.config.get_crypt_key(self.manifest_key)\n        f = Fernet(key)\n\n        manifest = copy.deepcopy(self.manifest)\n        for var in self.manifest['env'].keys():\n            value = str(self.manifest['env'][var])\n            manifest['env'][var] = f.encrypt(bytes(value, 'utf-8')).decode('utf-8')\n\n        return manifest"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_manifest(self, manifest_path=None, encrypted=None):\n        manifest_path = manifest_path or self.manifest_path\n        self.manifest['env']['PREDIXPY_VERSION'] = str(predix.version)\n\n        with open(manifest_path, 'w') as output_file:\n            if encrypted or self.encrypted:\n                self.manifest['env']['PREDIXPY_ENCRYPTED'] = self.manifest_key\n                content = self._get_encrypted_manifest()\n            else:\n                content = self.manifest   # shallow reference\n                if 'PREDIXPY_ENCRYPTED' in content['env']:\n                    del(content['env']['PREDIXPY_ENCRYPTED'])\n\n            yaml.safe_dump(content, output_file,\n                    default_flow_style=False, explicit_start=True)\n            output_file.close()", "response": "Write the manifest to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the given key / value as an environment variable.", "response": "def add_env_var(self, key, value):\n        \"\"\"\n        Add the given key / value as another environment\n        variable.\n        \"\"\"\n        self.manifest['env'][key] = value\n        os.environ[key] = str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the given service to the manifest.", "response": "def add_service(self, service_name):\n        \"\"\"\n        Add the given service to the manifest.\n        \"\"\"\n        if service_name not in self.manifest['services']:\n            self.manifest['services'].append(service_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting os. environ to the environment variables found in the the manifest file.", "response": "def set_os_environ(self):\n        \"\"\"\n        Will load any environment variables found in the\n        manifest file into the current process for use\n        by applications.\n\n        When apps run in cloud foundry this would happen\n        automatically.\n        \"\"\"\n        for key in self.manifest['env'].keys():\n            os.environ[key] = str(self.manifest['env'][key])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_client_id(self):\n        self._client_id = predix.config.get_env_value(predix.app.Manifest, 'client_id')\n        return self._client_id", "response": "Get the client id that should have all the services\n        needed scopes and authorities for the services\n        in this manifest."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the client secret that should correspond with the client id.", "response": "def get_client_secret(self):\n        \"\"\"\n        Return the client secret that should correspond with\n        the client id.\n        \"\"\"\n        self._client_secret = predix.config.get_env_value(predix.app.Manifest, 'client_secret')\n        return self._client_secret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_timeseries(self, *args, **kwargs):\n        import predix.data.timeseries\n        ts = predix.data.timeseries.TimeSeries(*args, **kwargs)\n        return ts", "response": "Returns an instance of the Time Series Service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_asset(self):\n        import predix.data.asset\n        asset = predix.data.asset.Asset()\n        return asset", "response": "Returns an instance of the Asset Service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_uaa(self):\n        import predix.security.uaa\n        uaa = predix.security.uaa.UserAccountAuthentication()\n        return uaa", "response": "Returns an insstance of the UAA Service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an instance of the Asset Control Service.", "response": "def get_acs(self):\n        \"\"\"\n        Returns an instance of the Asset Control Service.\n        \"\"\"\n        import predix.security.acs\n        acs = predix.security.acs.AccessControl()\n        return acs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_weather(self):\n        import predix.data.weather\n        weather = predix.data.weather.WeatherForecast()\n        return weather", "response": "Returns an instance of the Weather Service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the weather forecast for a given location.", "response": "def get_weather_forecast_days(self, latitude, longitude,\n            days=1, frequency=1, reading_type=None):\n        \"\"\"\n        Return the weather forecast for a given location.\n\n        ::\n\n            results = ws.get_weather_forecast_days(lat, long)\n            for w in results['hits']:\n                print w['start_datetime_local']\n                print w['reading_type'], w['reading_value']\n\n        For description of reading types:\n        https://graphical.weather.gov/xml/docs/elementInputNames.php\n        \"\"\"\n        params = {}\n\n        # Can get data from NWS1 or NWS3 representing 1-hr and 3-hr\n        # intervals.\n        if frequency not in [1, 3]:\n            raise ValueError(\"Reading frequency must be 1 or 3\")\n\n        params['days'] = days\n        params['source'] = 'NWS' + str(frequency)\n        params['latitude'] = latitude\n        params['longitude'] = longitude\n\n        if reading_type:\n            # url encoding will make spaces a + instead of %20, which service\n            # interprets as an \"and\" search which is undesirable\n            reading_type = reading_type.replace(' ', '%20')\n            params['reading_type'] = urllib.quote_plus(reading_type)\n\n        url = self.uri + '/v1/weather-forecast-days/'\n        return self.service._get(url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the weather forecast for a given location for specific location for specific start and end.", "response": "def get_weather_forecast(self, latitude, longitude, start, end,\n            frequency=1, reading_type=None):\n        \"\"\"\n        Return the weather forecast for a given location for specific\n        datetime specified in UTC format.\n\n        ::\n\n            results = ws.get_weather_forecast(lat, long, start, end)\n            for w in results['hits']:\n                print w['start_datetime_local']\n                print w['reading_type'], '=', w['reading_value']\n\n        For description of reading types:\n        https://graphical.weather.gov/xml/docs/elementInputNames.php\n        \"\"\"\n        params = {}\n\n        # Can get data from NWS1 or NWS3 representing 1-hr and 3-hr\n        # intervals.\n        if frequency not in [1, 3]:\n            raise ValueError(\"Reading frequency must be 1 or 3\")\n\n        params['source'] = 'NWS' + str(frequency)\n        params['latitude'] = latitude\n        params['longitude'] = longitude\n        params['start_datetime_utc'] = start\n        params['end_datetime_utc'] = end\n\n        if reading_type:\n            # Not using urllib.quote_plus() because its using a + which is\n            # being interpreted by service as an and instead of a space.\n            reading_type = reading_type.replace(' ', '%20')\n            params['reading_type'] = reading_type\n\n        url = self.uri + '/v1/weather-forecast-datetime/'\n        return self.service._get(url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a name based on the space service name and plan name", "response": "def _generate_name(self, space, service_name, plan_name):\n        \"\"\"\n        Can generate a name based on the space, service name and plan.\n        \"\"\"\n        return str.join('-', [space, service_name, plan_name]).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_config_path(self):\n        org = self.service.space.org.name\n        space = self.service.space.name\n        name = self.name\n\n        return \"~/.predix/%s/%s/%s.json\" % (org, space, name)", "response": "Return a sensible configuration path for caching config\n        settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a Cloud Foundry service that has custom parameters.", "response": "def _create_service(self, parameters={}, **kwargs):\n        \"\"\"\n        Create a Cloud Foundry service that has custom parameters.\n        \"\"\"\n        logging.debug(\"_create_service()\")\n        logging.debug(str.join(',', [self.service_name, self.plan_name,\n            self.name, str(parameters)]))\n\n        return self.service.create_service(self.service_name, self.plan_name,\n                self.name, parameters, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _delete_service(self, service_only=False):\n        logging.debug('_delete_service()')\n        return self.service.delete_service(self.service_name)", "response": "Delete a Cloud Foundry service and any associated data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a service key or create one if needed.", "response": "def _get_or_create_service_key(self):\n        \"\"\"\n        Get a service key or create one if needed.\n        \"\"\"\n        keys = self.service._get_service_keys(self.name)\n        for key in keys['resources']:\n            if key['entity']['name'] == self.service_name:\n                return self.service.get_service_key(self.name,\n                        self.service_name)\n\n        self.service.create_service_key(self.name, self.service_name)\n        return self.service.get_service_key(self.name, self.service_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the configuration for the service from a service key.", "response": "def _get_service_config(self):\n        \"\"\"\n        Will get configuration for the service from a service key.\n        \"\"\"\n        key = self._get_or_create_service_key()\n\n        config = {}\n        config['service_key'] = [{'name': self.name}]\n        config.update(key['entity']['credentials'])\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the service and store it in the local cache file.", "response": "def create(self, parameters={}, create_keys=True, **kwargs):\n        \"\"\"\n        Create the service.\n        \"\"\"\n        # Create the service\n        cs = self._create_service(parameters=parameters, **kwargs)\n\n        # Create the service key to get config details and\n        # store in local cache file.\n        if create_keys:\n            cfg = parameters\n            cfg.update(self._get_service_config())\n            self.settings.save(cfg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_or_create_uaa(self, uaa):\n        if isinstance(uaa, predix.admin.uaa.UserAccountAuthentication):\n            return uaa\n\n        logging.debug(\"Initializing a new UAA\")\n        return predix.admin.uaa.UserAccountAuthentication()", "response": "Returns a valid UAA instance for performing administrative functions\n        Returns a valid UAA instance for performing administrative functions\n        on services."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, parameters={}, **kwargs):\n        # Add parameter during create for UAA issuer\n        uri = self.uaa.service.settings.data['uri'] + '/oauth/token'\n        parameters[\"trustedIssuerIds\"] = [uri]\n        super(PredixService, self).create(parameters=parameters, **kwargs)", "response": "Create an instance of the US Weather Forecast Service with typical starting settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of the Time Series Service with the typical starting settings.", "response": "def create(self):\n        \"\"\"\n        Create an instance of the Time Series Service with the typical\n        starting settings.\n        \"\"\"\n        self.service.create()\n\n        os.environ[predix.config.get_env_key(self.use_class, 'host')] = self.get_eventhub_host()\n        os.environ[predix.config.get_env_key(self.use_class, 'port')] = self.get_eventhub_grpc_port()\n        os.environ[predix.config.get_env_key(self.use_class, 'wss_publish_uri')] = self.get_publish_wss_uri()\n        os.environ[predix.config.get_env_key(self.use_class, 'zone_id')] = self.get_zone_id()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grant_client(self, client_id, publish=False, subscribe=False, publish_protocol=None, publish_topics=None,\n                     subscribe_topics=None, scope_prefix='predix-event-hub', **kwargs):\n        \"\"\"\n        Grant the given client id all the scopes and authorities\n        needed to work with the eventhub service.\n        \"\"\"\n        scopes = ['openid']\n        authorities = ['uaa.resource']\n\n        zone_id = self.get_zone_id()\n        # always must be part of base user scope\n        scopes.append('%s.zones.%s.user' % (scope_prefix, zone_id))\n        authorities.append('%s.zones.%s.user' % (scope_prefix, zone_id))\n\n        if publish_topics is not None or subscribe_topics is not None:\n            raise Exception(\"multiple topics are not currently available in preidx-py\")\n\n        if publish_topics is None:\n            publish_topics = ['topic']\n\n        if subscribe_topics is None:\n            subscribe_topics = ['topic']\n\n        if publish:\n            # we are granting just the default topic\n            if publish_protocol is None:\n                scopes.append('%s.zones.%s.grpc.publish' % (scope_prefix, zone_id))\n                authorities.append('%s.zones.%s.grpc.publish' % (scope_prefix, zone_id))\n                scopes.append('%s.zones.%s.wss.publish' % (scope_prefix, zone_id))\n                authorities.append('%s.zones.%s.wss.publish' % (scope_prefix, zone_id))\n\n            else:\n                scopes.append('%s.zones.%s.%s.publish' % (scope_prefix, zone_id, publish_protocol))\n                authorities.append('%s.zones.%s.%s.publish' % (scope_prefix, zone_id, publish_protocol))\n\n            # we are requesting multiple topics\n            for topic in publish_topics:\n                if publish_protocol is None:\n                    scopes.append('%s.zones.%s.%s.grpc.publish' % (scope_prefix, zone_id, topic))\n                    scopes.append('%s.zones.%s.%s.wss.publish' % (scope_prefix, zone_id, topic))\n                    scopes.append('%s.zones.%s.%s.user' % (scope_prefix, zone_id, topic))\n                    authorities.append('%s.zones.%s.%s.grpc.publish' % (scope_prefix, zone_id, topic))\n                    authorities.append('%s.zones.%s.%s.wss.publish' % (scope_prefix, zone_id, topic))\n                    authorities.append('%s.zones.%s.%s.user' % (scope_prefix, zone_id, topic))\n                else:\n                    scopes.append('%s.zones.%s.%s.%s.publish' % (scope_prefix, zone_id, topic, publish_protocol))\n                    authorities.append('%s.zones.%s.%s.%s.publish' % (scope_prefix, zone_id, topic, publish_protocol))\n        if subscribe:\n            # we are granting just the default topic\n            scopes.append('%s.zones.%s.grpc.subscribe' % (scope_prefix, zone_id))\n            authorities.append('%s.zones.%s.grpc.subscribe' % (scope_prefix, zone_id))\n\n            # we are requesting multiple topics\n            for topic in subscribe_topics:\n                scopes.append('%s.zones.%s.%s.grpc.subscribe' % (scope_prefix, zone_id, topic))\n                authorities.append('%s.zones.%s.%s.grpc.subscribe' % (scope_prefix, zone_id, topic))\n\n        self.service.uaa.uaac.update_client_grants(client_id, scope=scopes,\n                                                   authorities=authorities)\n\n        return self.service.uaa.uaac.get_client(client_id)", "response": "Grant the given client id all the scopes and authorities needed to work with the eventhub service."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the publish grpc endpoint for ingestion.", "response": "def get_eventhub_host(self):\n        \"\"\"\n        returns the publish grpc endpoint for ingestion.\n        \"\"\"\n        for protocol in self.service.settings.data['publish']['protocol_details']:\n            if protocol['protocol'] == 'grpc':\n                return protocol['uri'][0:protocol['uri'].index(':')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd useful details to the manifest about this service.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service\n        so that it can be used in an application.\n\n        :param manifest: An predix.admin.app.Manifest object\n            instance that manages reading/writing manifest config\n            for a cloud foundry app.\n        \"\"\"\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n        manifest.add_env_var(predix.config.get_env_key(self.use_class, 'host'), self.get_eventhub_host())\n        manifest.add_env_var(predix.config.get_env_key(self.use_class, 'port'), self.get_eventhub_grpc_port())\n        manifest.add_env_var(predix.config.get_env_key(self.use_class, 'wss_publish_uri'), self.get_publish_wss_uri())\n        manifest.add_env_var(predix.config.get_env_key(self.use_class, 'zone_id'), self.get_zone_id())\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the host address for an instance of Blob Store service from environment inspection.", "response": "def _get_host(self):\n        \"\"\"\n        Returns the host address for an instance of Blob Store service from\n        environment inspection.\n        \"\"\"\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            host = services['predix-blobstore'][0]['credentials']['host']\n        else:\n            host = predix.config.get_env_value(self, 'host')\n\n        # Protocol may not always be included in host setting\n        if 'https://' not in host:\n            host = 'https://' + host\n\n        return host"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_access_key_id(self):\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            return services['predix-blobstore'][0]['credentials']['access_key_id']\n        else:\n            return predix.config.get_env_value(self, 'access_key_id')", "response": "Returns the access key for an instance of Blob Store service from environment inspection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_cloud_foundry_config(self):\n        config = os.path.expanduser(self.config_file)\n        if not os.path.exists(config):\n            raise CloudFoundryLoginError('You must run `cf login` to authenticate')\n\n        with open(config, \"r\") as data:\n            return json.load(data)", "response": "Reads the local cf CLI cache stored in the users\n        home directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the GUID for the organization currently targeted.", "response": "def get_organization_guid(self):\n        \"\"\"\n        Returns the GUID for the organization currently targeted.\n        \"\"\"\n        if 'PREDIX_ORGANIZATION_GUID' in os.environ:\n            return os.environ['PREDIX_ORGANIZATION_GUID']\n        else:\n            info = self._get_organization_info()\n            for key in ('Guid', 'GUID'):\n                if key in info.keys():\n                    return info[key]\n            raise ValueError('Unable to determine cf organization guid')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the GUID for the space currently targeted.", "response": "def get_space_guid(self):\n        \"\"\"\n        Returns the GUID for the space currently targeted.\n\n        Can be set by environment variable with PREDIX_SPACE_GUID.\n        Can be determined by ~/.cf/config.json.\n        \"\"\"\n        if 'PREDIX_SPACE_GUID' in os.environ:\n            return os.environ['PREDIX_SPACE_GUID']\n        else:\n            info = self._get_space_info()\n            for key in ('Guid', 'GUID'):\n                if key in info.keys():\n                    return info[key]\n            raise ValueError('Unable to determine cf space guid')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_crypt_key(key_path):\n    key_path = os.path.expanduser(key_path)\n    if os.path.exists(key_path):\n        with open(key_path, 'r') as data:\n            key = data.read()\n    else:\n        key = Fernet.generate_key()\n        with open(key_path, 'w') as output:\n            output.write(key)\n\n    return key", "response": "Get the user s PredixPy manifest key. Generate and store one if not yet generated."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_env_key(obj, key=None):\n    return str.join('_', [obj.__module__.replace('.','_').upper(),\n        key.upper()])", "response": "Returns environment variable key to use for lookups within a\n    namespace represented by the package name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the environment variable value for the given object and attribute.", "response": "def get_env_value(obj, attribute):\n    \"\"\"\n    Returns the environment variable value for the attribute of\n    the given object.\n\n    For example `get_env_value(predix.security.uaa, 'uri')` will\n    return value of environment variable PREDIX_SECURITY_UAA_URI.\n\n    \"\"\"\n    varname = get_env_key(obj, attribute)\n    var = os.environ.get(varname)\n    if not var:\n        raise ValueError(\"%s must be set in your environment.\" % varname)\n\n    return var"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the environment variable value for the given object attribute.", "response": "def set_env_value(obj, attribute, value):\n    \"\"\"\n    Set the environment variable value for the attribute of the\n    given object.\n\n    For example, `set_env_value(predix.security.uaa, 'uri', 'http://...')`\n    will set the environment variable PREDIX_SECURITY_UAA_URI to the given\n    uri.\n    \"\"\"\n    varname = get_env_key(obj, attribute)\n    os.environ[varname] = value\n    return varname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the GUID for the service instance with the given name.", "response": "def get_instance_guid(self, service_name):\n        \"\"\"\n        Returns the GUID for the service instance with\n        the given name.\n        \"\"\"\n        summary = self.space.get_space_summary()\n        for service in summary['services']:\n            if service['name'] == service_name:\n                return service['guid']\n\n        raise ValueError(\"No service with name '%s' found.\" % (service_name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_service_bindings(self, service_name):\n        instance = self.get_instance(service_name)\n        return self.api.get(instance['service_bindings_url'])", "response": "Get the service bindings for a service instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove service bindings to applications.", "response": "def delete_service_bindings(self, service_name):\n        \"\"\"\n        Remove service bindings to applications.\n        \"\"\"\n        instance = self.get_instance(service_name)\n        return self.api.delete(instance['service_bindings_url'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the service keys for the given service.", "response": "def _get_service_keys(self, service_name):\n        \"\"\"\n        Return the service keys for the given service.\n        \"\"\"\n        guid = self.get_instance_guid(service_name)\n        uri = \"/v2/service_instances/%s/service_keys\" % (guid)\n        return self.api.get(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a flat list of the names of the service keys for the given service.", "response": "def get_service_keys(self, service_name):\n        \"\"\"\n        Returns a flat list of the names of the service keys\n        for the given service.\n        \"\"\"\n        keys = []\n        for key in self._get_service_keys(service_name)['resources']:\n            keys.append(key['entity']['name'])\n\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the details of a specific service key.", "response": "def get_service_key(self, service_name, key_name):\n        \"\"\"\n        Returns the service key details.\n\n        Similar to `cf service-key`.\n        \"\"\"\n        for key in self._get_service_keys(service_name)['resources']:\n            if key_name == key['entity']['name']:\n                guid = key['metadata']['guid']\n\n                uri = \"/v2/service_keys/%s\" % (guid)\n                return self.api.get(uri)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a service key for the given service.", "response": "def create_service_key(self, service_name, key_name):\n        \"\"\"\n        Create a service key for the given service.\n        \"\"\"\n        if self.has_key(service_name, key_name):\n            logging.warning(\"Reusing existing service key %s\" % (key_name))\n            return self.get_service_key(service_name, key_name)\n\n        body = {\n            'service_instance_guid': self.get_instance_guid(service_name),\n            'name': key_name\n            }\n\n        return self.api.post('/v2/service_keys', body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a service key for the given service.", "response": "def delete_service_key(self, service_name, key_name):\n        \"\"\"\n        Delete a service key for the given service.\n        \"\"\"\n        key = self.get_service_key(service_name, key_name)\n        logging.info(\"Deleting service key %s for service %s\" % (key, service_name))\n        return self.api.delete(key['metadata']['url'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a service instance with the given name.", "response": "def get_instance(self, service_name):\n        \"\"\"\n        Retrieves a service instance with the given name.\n        \"\"\"\n        for resource in self.space._get_instances():\n            if resource['entity']['name'] == service_name:\n                return resource['entity']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the service plans available for a given service.", "response": "def get_service_plan_for_service(self, service_name):\n        \"\"\"\n        Return the service plans available for a given service.\n        \"\"\"\n        services = self.get_services()\n        for service in services['resources']:\n            if service['entity']['label'] == service_name:\n                response = self.api.get(service['entity']['service_plans_url'])\n                return response['resources']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_service_plan_guid(self, service_name, plan_name):\n        for plan in self.get_service_plan_for_service(service_name):\n            if plan['entity']['name'] == plan_name:\n                return plan['metadata']['guid']\n\n        return None", "response": "Get the service plan GUID for the given service and plan name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new service instance.", "response": "def create_service(self, service_type, plan_name, service_name, params,\n            async=False, **kwargs):\n        \"\"\"\n        Create a service instance.\n        \"\"\"\n        if self.space.has_service_with_name(service_name):\n            logging.warning(\"Service already exists with that name.\")\n            return self.get_instance(service_name)\n\n        if self.space.has_service_of_type(service_type):\n            logging.warning(\"Service type already exists.\")\n\n        guid = self.get_service_plan_guid(service_type, plan_name)\n        if not guid:\n            raise ValueError(\"No service plan named: %s\" % (plan_name))\n\n        body = {\n            'name': service_name,\n            'space_guid': self.space.guid,\n            'service_plan_guid': guid,\n            'parameters': params\n            }\n\n        uri = '/v2/service_instances?accepts_incomplete=true'\n\n        if async:\n            uri += '&async=true'\n\n        return self.api.post(uri, body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the given service.", "response": "def delete_service(self, service_name, params=None):\n        \"\"\"\n        Delete the service of the given name.  It may fail if there are\n        any service keys or app bindings.  Use purge() if you want\n        to delete it all.\n        \"\"\"\n        if not self.space.has_service_with_name(service_name):\n            logging.warning(\"Service not found so... succeeded?\")\n            return True\n\n        guid = self.get_instance_guid(service_name)\n        logging.info(\"Deleting service %s with guid %s\" % (service_name, guid))\n\n        # MAINT: this endpoint changes in newer version of api\n        return self.api.delete(\"/v2/service_instances/%s?accepts_incomplete=true\" %\n            (guid), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_query_uri(self):\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            predix_timeseries = services['predix-timeseries'][0]['credentials']\n            return predix_timeseries['query']['uri'].partition('/v1')[0]\n        else:\n            return predix.config.get_env_value(self, 'query_uri')", "response": "Returns the URI endpoint for performing queries of a specific Predix Time Series instance from environment inspection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ZoneId for performing queries of a Predix Time Series instance from environment inspection.", "response": "def _get_query_zone_id(self):\n        \"\"\"\n        Returns the ZoneId for performing queries of a Predix\n        Time Series instance from environment inspection.\n        \"\"\"\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            predix_timeseries = services['predix-timeseries'][0]['credentials']\n            return predix_timeseries['query']['zone-http-header-value']\n        else:\n            return predix.config.get_env_value(self, 'query_zone_id')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator that yields the values for the datapoints with the given args.", "response": "def get_values(self, *args, **kwargs):\n        \"\"\"\n        Convenience method that for simple single tag queries will\n        return just the values to be iterated on.\n        \"\"\"\n        if isinstance(args[0], list):\n            raise ValueError(\"Can only get_values() for a single tag.\")\n\n        response = self.get_datapoints(*args, **kwargs)\n        for value in response['tags'][0]['results'][0]['values']:\n            yield [datetime.datetime.utcfromtimestamp(value[0]/1000),\n                   value[1],\n                   value[2]]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_datapoints(self, tags, start=None, end=None, order=None,\n            limit=None, qualities=None, attributes=None, measurement=None,\n            aggregations=None, post=False):\n        \"\"\"\n        Returns all of the datapoints that match the given query.\n\n            - tags: list or string identifying the name/tag (ie. \"temp\")\n            - start: data after this, absolute or relative (ie. '1w-ago' or\n              1494015972386)\n            - end: data before this value\n            - order: ascending (asc) or descending (desc)\n            - limit: only return a few values (ie. 25)\n            - qualities: data quality value (ie. [ts.GOOD, ts.UNCERTAIN])\n            - attributes: dictionary of key-values (ie. {'unit': 'mph'})\n            - measurement: tuple of operation and value (ie. ('gt', 30))\n            - aggregations: summary statistics on data results (ie. 'avg')\n            - post: POST query instead of GET (caching implication)\n\n        A few additional observations:\n            - allow service to do most data validation\n            - order is applied before limit so resultset will differ\n\n        The returned results match what the service response is so you'll\n        need to unpack it as appropriate.  Oftentimes what you want for\n        a simple single tag query will be:\n\n            response['tags'][0]['results'][0]['values']\n\n        \"\"\"\n        params = {}\n\n        # Documentation says start is required for GET but not POST, but\n        # seems to be required all the time, so using sensible default.\n        if not start:\n            start = '1w-ago'\n            logging.warning(\"Defaulting query for data with start date %s\" % (start))\n\n        # Start date can be absolute or relative, only certain legal values\n        # but service will throw error if used improperly.  (ms, s, mi, h, d,\n        # w, mm, y).  Relative dates must end in -ago.\n        params['start'] = start\n\n        # Docs say when making POST with a start that end must also be\n        # specified, but this does not seem to be the case.\n        if end:\n            # MAINT: error when end < start which is handled by service\n            params['end'] = end\n\n        params['tags'] = []\n        if not isinstance(tags, list):\n            tags = [tags]\n\n        for tag in tags:\n            query = {}\n            query['name'] = tag\n\n            # Limit resultset with an integer value\n            if limit:\n                query['limit'] = int(limit)\n\n            # Order must be 'asc' or 'desc' but will get sensible error\n            # from service.\n            if order:\n                query['order'] = order\n\n            # Filters are complex and support filtering by\n            # quality, measurement, and attributes.\n            filters = {}\n\n            # Check for the quality of the datapoints\n            if qualities is not None:\n                if isinstance(qualities, int) or isinstance(qualities, str):\n                    qualities = [qualities]\n\n                # Timeseries expects quality to be a string, not integer,\n                # so coerce each into a string\n                for i, quality in enumerate(qualities):\n                    qualities[i] = str(quality)\n\n                filters['qualities'] = {\"values\": qualities}\n\n            # Check for attributes on the datapoints, expected to be\n            # a dictionary of key / value pairs that datapoints must match.\n            if attributes is not None:\n                if not isinstance(attributes, dict):\n                    raise ValueError(\"Attribute filters must be dictionary.\")\n\n                filters['attributes'] = attributes\n\n            # Check for measurements that meets a given comparison operation\n            # such as ge, gt, eq, ne, le, lt\n            if measurement is not None:\n                filters['measurements'] = {\n                        'condition': measurement[0],\n                        'values': measurement[1]\n                        }\n\n            # If we found any filters add them to the query\n            if filters:\n                query['filters'] = filters\n\n            # Handle any additional aggregations of dataset\n            if aggregations is not None:\n                if not isinstance(aggregations, list):\n                    aggregations = [aggregations]\n\n                query['aggregations'] = []\n                for aggregation in aggregations:\n                    query['aggregations'].append({\n                        'sampling': {'datapoints': 1},\n                        'type': aggregation })\n\n            params['tags'].append(query)\n\n        if post:\n            return self._post_datapoints(params)\n        else:\n            return self._get_datapoints({\"query\": json.dumps(params)})", "response": "Returns all of the datapoints that match the given query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_connection(self):\n        logging.debug(\"Initializing new websocket connection.\")\n        headers = {\n            'Authorization': self.service._get_bearer_token(),\n            'Predix-Zone-Id': self.ingest_zone_id,\n            'Content-Type': 'application/json',\n        }\n        url = self.ingest_uri\n\n        logging.debug(\"URL=\" + str(url))\n        logging.debug(\"HEADERS=\" + str(headers))\n\n        # Should consider connection pooling and longer timeouts\n        return websocket.create_connection(url, header=headers)", "response": "Create a new websocket connection with proper headers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a websocket connection.", "response": "def _get_websocket(self, reuse=True):\n        \"\"\"\n        Reuse existing connection or create a new connection.\n        \"\"\"\n        # Check if still connected\n        if self.ws and reuse:\n            if self.ws.connected:\n                return self.ws\n\n            logging.debug(\"Stale connection, reconnecting.\")\n\n        self.ws = self._create_connection()\n        return self.ws"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _send_to_timeseries(self, message):\n        logging.debug(\"MESSAGE=\" + str(message))\n\n        result = None\n        try:\n            ws = self._get_websocket()\n            ws.send(json.dumps(message))\n            result = ws.recv()\n        except (websocket.WebSocketConnectionClosedException, Exception) as e:\n            logging.debug(\"Connection failed, will try again.\")\n            logging.debug(e)\n\n            ws = self._get_websocket(reuse=False)\n            ws.send(json.dumps(message))\n            result = ws.recv()\n\n        logging.debug(\"RESULT=\" + str(result))\n        return result", "response": "Send a message to the timeseries service."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef queue(self, name, value, quality=None, timestamp=None,\n            attributes=None):\n        \"\"\"\n        To reduce network traffic, you can buffer datapoints and\n        then flush() anything in the queue.\n\n        :param name: the name / label / tag for sensor data\n\n        :param value: the sensor reading or value to record\n\n        :param quality: the quality value, use the constants BAD, GOOD, etc.\n            (optional and defaults to UNCERTAIN)\n\n        :param timestamp: the time the reading was recorded in epoch\n            milliseconds (optional and defaults to now)\n\n        :param attributes: dictionary for any key-value pairs to store with the\n            reading (optional)\n\n        \"\"\"\n        # Get timestamp first in case delay opening websocket connection\n        # and it must have millisecond accuracy\n        if not timestamp:\n            timestamp = int(round(time.time() * 1000))\n        else:\n            # Coerce datetime objects to epoch\n            if isinstance(timestamp, datetime.datetime):\n                timestamp = int(round(int(timestamp.strftime('%s')) * 1000))\n\n        # Only specific quality values supported\n        if quality not in [self.BAD, self.GOOD, self.NA, self.UNCERTAIN]:\n            quality = self.UNCERTAIN\n\n        # Check if adding to queue of an existing tag and add second datapoint\n        for point in self._queue:\n            if point['name'] == name:\n                point['datapoints'].append([timestamp, value, quality])\n                return\n\n        # If adding new tag, initialize and set any attributes\n        datapoint = {\n            \"name\": name,\n            \"datapoints\": [[timestamp, value, quality]]\n        }\n\n        # Attributes are extra details for a datapoint\n\n        if attributes is not None:\n            if not isinstance(attributes, dict):\n                raise ValueError(\"Attributes are expected to be a dictionary.\")\n\n            # Validate rules for attribute keys to provide guidance.\n            invalid_value = ':;= '\n            has_invalid_value = re.compile(r'[%s]' % (invalid_value)).search\n            has_valid_key = re.compile(r'^[\\w\\.\\/\\-]+$').search\n\n            for (key, val) in list(attributes.items()):\n                # Values cannot be empty\n                if (val == '') or (val is None):\n                    raise ValueError(\"Attribute (%s) must have a non-empty value.\" % (key))\n\n                # Values should be treated as a string for regex validation\n                val = str(val)\n\n                # Values cannot contain certain arbitrary characters\n                if bool(has_invalid_value(val)):\n                    raise ValueError(\"Attribute (%s) cannot contain (%s).\" %\n                            (key, invalid_value))\n\n                # Attributes have to be alphanumeric-ish\n                if not bool(has_valid_key):\n                    raise ValueError(\"Key (%s) not alphanumeric-ish.\" % (key))\n\n            datapoint['attributes'] = attributes\n\n        self._queue.append(datapoint)\n        logging.debug(\"QUEUE: \" + str(len(self._queue)))", "response": "Add a new datapoint to the queue of datapoints."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a message to the time series service.", "response": "def send(self, name=None, value=None, **kwargs):\n        \"\"\"\n        Can accept a name/tag and value to be queued and then send anything in\n        the queue to the time series service.  Optional parameters include\n        setting quality, timestamp, or attributes.\n\n        See spec for queue() for complete list of options.\n\n        Example of sending a batch of values:\n\n            queue('temp', 70.1)\n            queue('humidity', 20.4)\n            send()\n\n        Example of sending one and flushing queue immediately\n\n            send('temp', 70.3)\n            send('temp', 70.4, quality=ts.GOOD, attributes={'unit': 'F'})\n\n\n        \"\"\"\n        if name and value:\n            self.queue(name, value, **kwargs)\n\n        timestamp = int(round(time.time() * 1000))\n\n        # The label \"name\" or \"tag\" is sometimes used ambiguously\n        msg = {\n            \"messageId\": timestamp,\n            \"body\": self._queue\n        }\n\n        self._queue = []\n\n        return self._send_to_timeseries(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown(self):\n        self._run_health_checker = False\n        if self.publisher is not None:\n            self.publisher.shutdown()\n\n        if self.subscriber is not None:\n            self.subscriber.shutdown()", "response": "Shutdown the client shutdown the sub clients and stop the health checker"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a env variable as defined by the service admin", "response": "def get_service_env_value(self, key):\n        \"\"\"\n        Get a env variable as defined by the service admin\n        :param key: the base of the key to use\n        :return: the env if it exists\n        \"\"\"\n        service_key = predix.config.get_env_key(self, key)\n        value = os.environ[service_key]\n        if not value:\n            raise ValueError(\"%s env unset\" % key)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the grpc channel used for both publisher and subscriber.", "response": "def _init_channel(self):\n        \"\"\"\n        build the grpc channel used for both publisher and subscriber\n        :return: None\n        \"\"\"\n        host = self._get_host()\n        port = self._get_grpc_port()\n\n        if 'TLS_PEM_FILE' in os.environ:\n            with open(os.environ['TLS_PEM_FILE'], mode='rb') as f:  # b is important -> binary\n                file_content = f.read()\n            credentials = grpc.ssl_channel_credentials(root_certificates=file_content)\n        else:\n            credentials = grpc.ssl_channel_credentials()\n\n        self._channel = grpc.secure_channel(host + \":\" + port, credentials=credentials)\n        self._init_health_checker()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_health_checker(self):\n        stub = Health_pb2_grpc.HealthStub(channel=self._channel)\n        self._health_check = stub.Check\n        health_check_thread = threading.Thread(target=self._health_check_thread)\n        health_check_thread.daemon = True\n        health_check_thread.start()", "response": "start the health checker stub and start a thread to ping it every 30 seconds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _health_check_thread(self):\n        while self._run_health_checker:\n            response = self._health_check(Health_pb2.HealthCheckRequest(service='predix-event-hub.grpc.health'))\n            logging.debug('received health check: ' + str(response))\n            time.sleep(30)\n        return", "response": "Thread that checks the service every 30 seconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_temp_space():\n    # Truncating uuid to just take final 12 characters since space name\n    # is used to name services and there is a 50 character limit on instance\n    # names.  \n    # MAINT: hacky with possible collisions\n    unique_name = str(uuid.uuid4()).split('-')[-1]\n    admin = predix.admin.cf.spaces.Space()\n    res = admin.create_space(unique_name)\n\n    space = predix.admin.cf.spaces.Space(\n            guid=res['metadata']['guid'],\n            name=res['entity']['name'])\n    space.target()\n\n    return space", "response": "Create a new temporary cloud foundry space for\n    a project."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the marketplace services.", "response": "def _get_spaces(self):\n        \"\"\"\n        Get the marketplace services.\n        \"\"\"\n        guid = self.api.config.get_organization_guid()\n        uri = '/v2/organizations/%s/spaces' % (guid)\n        return self.api.get(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef target(self):\n        # MAINT: I don't like this, but will deal later\n        os.environ['PREDIX_SPACE_GUID'] = self.guid\n        os.environ['PREDIX_SPACE_NAME'] = self.name\n        os.environ['PREDIX_ORGANIZATION_GUID'] = self.org.guid\n        os.environ['PREDIX_ORGANIZATION_NAME'] = self.org.name", "response": "Target the current space for any forthcoming Cloud Foundry\n        operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_spaces(self):\n        self.spaces = []\n        for resource in self._get_spaces()['resources']:\n            self.spaces.append(resource['entity']['name'])\n\n        return self.spaces", "response": "Return a flat list of the names for spaces in the organization."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new space with the given name in the current target target.", "response": "def create_space(self, space_name, add_users=True):\n        \"\"\"\n        Create a new space with the given name in the current target\n        organization.\n        \"\"\"\n        body = {\n            'name': space_name,\n            'organization_guid': self.api.config.get_organization_guid()\n        }\n\n        # MAINT: may need to do this more generally later\n        if add_users:\n            space_users = []\n            org_users = self.org.get_users()\n            for org_user in org_users['resources']:\n                guid = org_user['metadata']['guid']\n                space_users.append(guid)\n\n            body['manager_guids'] = space_users\n            body['developer_guids'] = space_users\n\n        return self.api.post('/v2/spaces', body)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_space(self, name=None, guid=None):\n\n        if not guid:\n            if name:\n                spaces = self._get_spaces()\n                for space in spaces['resources']:\n                    if space['entity']['name'] == name:\n                        guid = space['metadata']['guid']\n                        break\n                if not guid:\n                    raise ValueError(\"Space with name %s not found.\" % (name))\n            else:\n                guid = self.guid\n\n        logging.warning(\"Deleting space (%s) and all services.\" % (guid))\n\n        return self.api.delete(\"/v2/spaces/%s\" % (guid), params={'recursive':\n        'true'})", "response": "Delete the current space or a space with the given name or guid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_services(self):\n        services = []\n        for resource in self._get_services()['resources']:\n            services.append(resource['entity']['label'])\n\n        return services", "response": "Returns a flat list of the service names available in this space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_instances(self, page_number=None):\n        instances = []\n        uri = '/v2/spaces/%s/service_instances' % self.guid\n        json_response = self.api.get(uri)\n        instances += json_response['resources']\n        while json_response['next_url'] is not None:\n            json_response = self.api.get(json_response['next_url'])\n            instances += json_response['resources']\n\n        return instances", "response": "Returns the service instances activated in this space."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_instances(self):\n        services = []\n        for resource in self._get_instances():\n            services.append(resource['entity']['name'])\n\n        return services", "response": "Returns a flat list of the names of services created\n        in this space."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests whether a service instance exists for the given service type.", "response": "def has_service_of_type(self, service_type):\n        \"\"\"\n        Tests whether a service instance exists for the given\n        service.\n        \"\"\"\n        summary = self.get_space_summary()\n        for instance in summary['services']:\n            if 'service_plan' in instance:\n                if service_type == instance['service_plan']['service']['label']:\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef purge(self):\n        logging.warning(\"Purging all services from space %s\" %\n                (self.name))\n\n        service = predix.admin.cf.services.Service()\n        for service_name in self.get_instances():\n            service.purge(service_name)\n\n        apps = predix.admin.cf.apps.App()\n        for app_name in self.get_apps():\n            apps.delete_app(app_name)", "response": "Remove all services and apps from the space."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of the PostgreSQL service with the typical starting settings.", "response": "def create(self, max_wait=300, allocated_storage=None,\n            encryption_at_rest=None, restore_to_time=None, **kwargs):\n        \"\"\"\n        Create an instance of the PostgreSQL service with the typical starting\n        settings.\n\n        :param max_wait: service is created asynchronously, so will only wait\n            this number of seconds before giving up.\n\n        :param allocated_storage: int for GBs to be allocated for storage\n\n        :param encryption_at_rest: boolean for encrypting data that is stored\n\n        :param restore_to_time: UTC date within recovery period for db\n            backup to be used when initiating\n\n        \"\"\"\n\n        # MAINT: Add these if there is demand for it and validated\n\n        if allocated_storage or encryption_at_rest or restore_to_time:\n            raise NotImplementedError()\n\n        # Will need to wait for the service to be provisioned before can add\n        # service keys and get env details.\n\n        self.service.create(async=True, create_keys=False)\n        while self._create_in_progress() and max_wait > 0:\n            if max_wait % 5 == 0:\n                logging.warning('Can take {}s for create to finish.'.format(max_wait))\n            time.sleep(1)\n            max_wait -= 1\n\n        # Now get the service env (via service keys)\n        cfg = self.service._get_service_config()\n        self.service.settings.save(cfg)\n\n        hostname = predix.config.get_env_key(self.use_class, 'hostname')\n        os.environ[hostname] = self.service.settings.data['hostname']\n\n        password = predix.config.get_env_key(self.use_class, 'password')\n        os.environ[password] = self.service.settings.data['password']\n\n        port = predix.config.get_env_key(self.use_class, 'port')\n        os.environ[port] = str(self.service.settings.data['port'])\n\n        username = predix.config.get_env_key(self.use_class, 'username')\n        os.environ[username] = self.service.settings.data['username']\n\n        uri = predix.config.get_env_key(self.use_class, 'uri')\n        os.environ[uri] = self.service.settings.data['uri']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding useful details to the manifest for this service so that it can be used in an application.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service so\n        that it can be used in an application.\n\n        :param manifest: A predix.admin.app.Manifest object instance\n            that manages reading/writing manifest config for a\n            cloud foundry app.\n        \"\"\"\n        manifest.add_service(self.service.name)\n\n        hostname = predix.config.get_env_key(self.use_class, 'hostname')\n        manifest.add_env_var(hostname, self.service.settings.data['hostname'])\n\n        password = predix.config.get_env_key(self.use_class, 'password')\n        manifest.add_env_var(password, self.service.settings.data['password'])\n\n        port = predix.config.get_env_key(self.use_class, 'port')\n        manifest.add_env_var(port, self.service.settings.data['port'])\n\n        username = predix.config.get_env_key(self.use_class, 'username')\n        manifest.add_env_var(username, self.service.settings.data['username'])\n\n        uri = predix.config.get_env_key(self.use_class, 'uri')\n        manifest.add_env_var(uri, self.service.settings.data['uri'])\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of the Analytics Framework Service with the typical starting settings.", "response": "def create(self, asset, timeseries, client_id, client_secret,\n            ui_client_id=None, ui_client_secret=None):\n        \"\"\"\n        Create an instance of the Analytics Framework Service with the\n        typical starting settings.\n\n        If not provided, will reuse the runtime client for the ui\n        as well.\n        \"\"\"\n        assert isinstance(asset, predix.admin.asset.Asset), \\\n            \"Require an existing predix.admin.asset.Asset instance\"\n        assert isinstance(timeseries, predix.admin.timeseries.TimeSeries), \\\n            \"Require an existing predix.admin.timeseries.TimeSeries instance\"\n\n        parameters = {\n            'predixAssetZoneId': asset.get_zone_id(),\n            'predixTimeseriesZoneId': timeseries.get_query_zone_id(),\n            'runtimeClientId': client_id,\n            'runtimeClientSecret': client_secret,\n            'uiClientId': ui_client_id or client_id,\n            'uiClientSecret': ui_client_secret or client_secret,\n            'uiDomainPrefix': self.service.name,\n        }\n\n        self.service.create(parameters=parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the service name for this resource.", "response": "def _find_service_name(self):\n        \"\"\"\n        For cloud operations there is support for multiple pools of resources\n        dedicated to logstash.  The service name as a result follows the\n        pattern logstash-{n} where n is some number.  We can find it from the\n        service marketplace.\n        \"\"\"\n        space = predix.admin.cf.spaces.Space()\n        services = space.get_services()\n        for service in services:\n            if service.startswith('logstash'):\n                return service\n\n        return 'logstash-3'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_to_manifest(self, manifest):\n        manifest.add_service(self.service.name)\n        manifest.write_manifest()", "response": "Add the service to the manifest"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_zone_id(self):\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            predix_asset = services['predix-asset'][0]['credentials']\n            return predix_asset['zone']['http-header-value']\n        else:\n            return predix.config.get_env_value(self, 'zone_id')", "response": "Returns the Predix Zone Id for the service that is a required\n        header in service calls."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_collections(self):\n        collections = []\n        for result in self._get_collections():\n            collections.append(result['collection'])\n\n        return collections", "response": "Returns a flat list of the names of collections in the asset\n            service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_collection(self, collection, filter=None, fields=None,\n            page_size=None):\n        \"\"\"\n        Returns a specific collection from the asset service with\n        the given collection endpoint.\n\n        Supports passing through parameters such as...\n        - filters such as \"name=Vesuvius\" following GEL spec\n        - fields such as \"uri,description\" comma delimited\n        - page_size such as \"100\" (the default)\n\n        \"\"\"\n        params = {}\n        if filter:\n            params['filter'] = filter\n        if fields:\n            params['fields'] = fields\n        if page_size:\n            params['pageSize'] = page_size\n\n        uri = self.uri + '/v1' + collection\n        return self.service._get(uri, params=params)", "response": "Returns a specific collection from the asset service with the given collection endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_guid(self, collection=None):\n        guid = str(uuid.uuid4())\n        if collection:\n            return str.join('/', [collection, guid])\n        else:\n            return guid", "response": "Returns a new guid for use in posting a new asset to a collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new collection.", "response": "def post_collection(self, collection, body):\n        \"\"\"\n        Creates a new collection.  This is mostly just transport layer\n        and passes collection and body along.  It presumes the body\n        already has generated.\n\n        The collection is *not* expected to have the id.\n        \"\"\"\n        assert isinstance(body, (list)), \"POST requires body to be a list\"\n        assert collection.startswith('/'), \"Collections must start with /\"\n        uri = self.uri + '/v1' + collection\n        return self.service._post(uri, body)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_collection(self, collection, body):\n\n        uri = self.uri + '/v1' + collection\n        return self.service._put(uri, body)", "response": "Updates an existing collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_collection(self, collection):\n        uri = str.join('/', [self.uri, collection])\n        return self.service._delete(uri)", "response": "Deletes an existing collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_collection(self, collection, changes):\n        uri = str.join('/', [self.uri, collection])\n        return self.service._patch(uri, changes)", "response": "This method will make specific updates to a record based on JSON Patch\n            documentation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, collection):\n        assert isinstance(collection, predix.data.asset.AssetCollection), \"Expected AssetCollection\"\n        collection.validate()\n        self.put_collection(collection.uri, collection.__dict__)", "response": "Save an asset collection to the service."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_manifest_from_space(self):\n        space = predix.admin.cf.spaces.Space()\n\n        summary = space.get_space_summary()\n        for instance in summary['services']:\n            service_type = instance['service_plan']['service']['label']\n            name = instance['name']\n            if service_type in self.supported:\n                service = self.supported[service_type](name=name)\n                service.add_to_manifest(self)\n            elif service_type == 'us-weather-forecast':\n                weather = predix.admin.weather.WeatherForecast(name=name)\n                weather.add_to_manifest(self)\n            else:\n                logging.warning(\"Unsupported service type: %s\" % service_type)", "response": "Populate a manifest file generated from details from the cloud foundry space environment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlocking the manifest to the current organization and space regardless of Cloud Foundry target.", "response": "def lock_to_org_space(self):\n        \"\"\"\n        Lock the manifest to the current organization and space regardless of\n        Cloud Foundry target.\n        \"\"\"\n        self.add_env_var('PREDIX_ORGANIZATION_GUID', self.space.org.guid)\n        self.add_env_var('PREDIX_ORGANIZATION_NAME', self.space.org.name)\n        self.add_env_var('PREDIX_SPACE_GUID', self.space.guid)\n        self.add_env_var('PREDIX_SPACE_NAME', self.space.name)\n        self.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_uaa(self, admin_secret, **kwargs):\n        uaa = predix.admin.uaa.UserAccountAuthentication(**kwargs)\n        if not uaa.exists():\n            uaa.create(admin_secret, **kwargs)\n\n        uaa.add_to_manifest(self)\n        return uaa", "response": "Creates an instance of UAA Service."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_client(self, client_id=None, client_secret=None, uaa=None):\n        if not uaa:\n            uaa = predix.admin.uaa.UserAccountAuthentication()\n\n        # Client id and secret can be generated if not provided as arguments\n\n        if not client_id:\n            client_id = uaa._create_id()\n\n        if not client_secret:\n            client_secret = uaa._create_secret()\n\n        uaa.create_client(client_id, client_secret)\n        uaa.add_client_to_manifest(client_id, client_secret, self)", "response": "Create a client and add it to the manifest."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_timeseries(self, **kwargs):\n        ts = predix.admin.timeseries.TimeSeries(**kwargs)\n        ts.create()\n\n        client_id = self.get_client_id()\n        if client_id:\n            ts.grant_client(client_id)\n\n        ts.add_to_manifest(self)\n        return ts", "response": "Creates an instance of the Time Series Service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of the Asset Service.", "response": "def create_asset(self, **kwargs):\n        \"\"\"\n        Creates an instance of the Asset Service.\n        \"\"\"\n        asset = predix.admin.asset.Asset(**kwargs)\n        asset.create()\n\n        client_id = self.get_client_id()\n        if client_id:\n            asset.grant_client(client_id)\n\n        asset.add_to_manifest(self)\n        return asset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an instance of the Asset Service.", "response": "def create_acs(self, **kwargs):\n        \"\"\"\n        Creates an instance of the Asset Service.\n        \"\"\"\n        acs = predix.admin.acs.AccessControl(**kwargs)\n        acs.create()\n\n        client_id = self.get_client_id()\n        if client_id:\n            acs.grant_client(client_id)\n\n        acs.grant_client(client_id)\n        acs.add_to_manifest(self)\n        return acs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_weather(self, **kwargs):\n        weather = predix.admin.weather.WeatherForecast(**kwargs)\n        weather.create()\n\n        client_id = self.get_client_id()\n        if client_id:\n            weather.grant_client(client_id)\n\n        weather.grant_client(client_id)\n        weather.add_to_manifest(self)\n        return weather", "response": "Creates an instance of the Asset Service."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_blobstore(self, **kwargs):\n        blobstore = predix.admin.blobstore.BlobStore(**kwargs)\n        blobstore.create()\n\n        blobstore.add_to_manifest(self)\n        return blobstore", "response": "Creates an instance of the BlobStore Service."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an instance of the Logging Service.", "response": "def create_logstash(self, **kwargs):\n        \"\"\"\n        Creates an instance of the Logging Service.\n        \"\"\"\n        logstash = predix.admin.logstash.Logging(**kwargs)\n        logstash.create()\n        logstash.add_to_manifest(self)\n\n        logging.info('Install Kibana-Me-Logs application by following GitHub instructions')\n        logging.info('git clone https://github.com/cloudfoundry-community/kibana-me-logs.git')\n\n        return logstash"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_cache(self, **kwargs):\n        cache = predix.admin.cache.Cache(**kwargs)\n        cache.create(**kwargs)\n        cache.add_to_manifest(self)\n        return cache", "response": "Creates an instance of the Cache Service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_eventhub(self, **kwargs):\n        eventhub = predix.admin.eventhub.EventHub(**kwargs)\n        eventhub.create()\n        eventhub.add_to_manifest(self)\n        eventhub.grant_client(client_id=self.get_client_id(), **kwargs)\n        eventhub.add_to_manifest(self)\n        return eventhub", "response": "Creates an instance of eventhub service with the given client_id and grant_client method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_service_marketplace(self, available=True, unavailable=False,\n            deprecated=False):\n        \"\"\"\n        Returns a list of service names.  Can return all services, just\n        those supported by PredixPy, or just those not yet supported by\n        PredixPy.\n\n        :param available: Return the services that are\n            available in PredixPy.  (Defaults to True)\n\n        :param unavailable: Return the services that are not yet\n            supported by PredixPy.  (Defaults to False)\n\n        :param deprecated: Return the services that are\n            supported by PredixPy but no longer available. (True)\n        \"\"\"\n\n        supported = set(self.supported.keys())\n        all_services = set(self.space.get_services())\n\n        results = set()\n        if available:\n            results.update(supported)\n        if unavailable:\n            results.update(all_services.difference(supported))\n        if deprecated:\n            results.update(supported.difference(all_services))\n\n        return list(results)", "response": "Returns a list of service names. Can return all services just\n            those not yet supported by PredixPy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _auto_authenticate(self):\n        client_id = predix.config.get_env_value(predix.app.Manifest, 'client_id')\n        client_secret = predix.config.get_env_value(predix.app.Manifest, 'client_secret')\n\n        if client_id and client_secret:\n            logging.info(\"Automatically authenticated as %s\" % (client_id))\n            self.uaa.authenticate(client_id, client_secret)", "response": "Auto - authenticate the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get(self, uri, params=None, headers=None):\n        if not headers:\n            headers = self._get_headers()\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n\n        response = self.session.get(uri, headers=headers, params=params)\n        logging.debug(\"STATUS=\" + str(response.status_code))\n        if response.status_code == 200:\n            return response.json()\n        else:\n            logging.error(b\"ERROR=\" + response.content)\n            response.raise_for_status()", "response": "Simple GET request for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _put(self, uri, data):\n        headers = self._get_headers()\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"BODY=\" + json.dumps(data))\n\n        response = self.session.put(uri, headers=headers,\n                data=json.dumps(data))\n        if response.status_code in [201, 204]:\n            return data\n        else:\n            logging.error(response.content)\n            response.raise_for_status()", "response": "Simple PUT operation for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _delete(self, uri):\n        headers = self._get_headers()\n\n        response = self.session.delete(uri, headers=headers)\n\n        # Will return a 204 on successful delete\n        if response.status_code == 204:\n            return response\n        else:\n            logging.error(response.content)\n            response.raise_for_status()", "response": "Simple DELETE operation for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _patch(self, uri, data):\n        headers = self._get_headers()\n        response = self.session.patch(uri, headers=headers,\n                data=json.dumps(data))\n\n        # Will return a 204 on successful patch\n        if response.status_code == 204:\n            return response\n        else:\n            logging.error(response.content)\n            response.raise_for_status()", "response": "Simple PATCH operation for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the full path that uniquely identifies the resource endpoint.", "response": "def _get_resource_uri(self, guid=None):\n        \"\"\"\n        Returns the full path that uniquely identifies\n        the resource endpoint.\n        \"\"\"\n        uri = self.uri + '/v1/resource'\n        if guid:\n            uri += '/' + urllib.quote_plus(guid)\n        return uri"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a specific resource by resource id.", "response": "def get_resource(self, resource_id):\n        \"\"\"\n        Returns a specific resource by resource id.\n        \"\"\"\n        # resource_id could be a path such as '/asset/123' so quote\n        uri = self._get_resource_uri(guid=resource_id)\n        return self.service._get(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npost a resource to the resource store.", "response": "def _post_resource(self, body):\n        \"\"\"\n        Create new resources and associated attributes.\n\n        Example:\n\n            acs.post_resource([\n                {\n                    \"resourceIdentifier\": \"masaya\",\n                    \"parents\": [],\n                    \"attributes\": [\n                        {\n                            \"issuer\": \"default\",\n                            \"name\": \"country\",\n                            \"value\": \"Nicaragua\"\n                            }\n                        ],\n                }\n            ])\n\n        The issuer is effectively a namespace, and in policy evaluations you\n        identify an attribute by a specific namespace.  Many examples provide\n        a URL but it could be any arbitrary string.\n\n        The body is a list, so many resources can be added at the same time.\n        \"\"\"\n        assert isinstance(body, (list)), \"POST for requires body to be a list\"\n        uri = self._get_resource_uri()\n        return self.service._post(uri, body)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_resource(self, resource_id):\n        # resource_id could be a path such as '/asset/123' so quote\n        uri = self._get_resource_uri(guid=resource_id)\n        return self.service._delete(uri)", "response": "Remove a specific resource by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _put_resource(self, resource_id, body):\n        assert isinstance(body, (dict)), \"PUT requires body to be a dict.\"\n        # resource_id could be a path such as '/asset/123' so quote\n        uri = self._get_resource_uri(guid=resource_id)\n        return self.service._put(uri, body)", "response": "Update a resource for the given resource id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a resource with a given identifier and attribute dictionary.", "response": "def add_resource(self, resource_id, attributes, parents=[],\n            issuer='default'):\n        \"\"\"\n        Will add the given resource with a given identifier and attribute\n        dictionary.\n\n            example/\n\n                add_resource('/asset/12', {'id': 12, 'manufacturer': 'GE'})\n        \"\"\"\n        # MAINT: consider test to avoid adding duplicate resource id\n        assert isinstance(attributes, (dict)), \"attributes expected to be dict\"\n\n        attrs = []\n        for key in attributes.keys():\n            attrs.append({\n                'issuer': issuer,\n                'name': key,\n                'value': attributes[key]\n                })\n\n        body = {\n            \"resourceIdentifier\": resource_id,\n            \"parents\": parents,\n            \"attributes\": attrs,\n        }\n\n        return self._put_resource(resource_id, body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_subject_uri(self, guid=None):\n        uri = self.uri + '/v1/subject'\n        if guid:\n            uri += '/' + urllib.quote_plus(guid)\n        return uri", "response": "Returns the full path that uniquely identifies\n            the subject endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a specific subject by subject id.", "response": "def get_subject(self, subject_id):\n        \"\"\"\n        Returns a specific subject by subject id.\n        \"\"\"\n        # subject_id could be a path such as '/user/j12y' so quote\n        uri = self._get_subject_uri(guid=subject_id)\n        return self.service._get(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nposting a subject to the ACS server.", "response": "def _post_subject(self, body):\n        \"\"\"\n        Create new subjects and associated attributes.\n\n        Example:\n\n            acs.post_subject([\n                {\n                    \"subjectIdentifier\": \"/role/evangelist\",\n                    \"parents\": [],\n                    \"attributes\": [\n                        {\n                            \"issuer\": \"default\",\n                            \"name\": \"role\",\n                            \"value\": \"developer evangelist\",\n                        }\n                    ]\n                }\n            ])\n\n        The issuer is effectively a namespace, and in policy evaluations\n        you identify an attribute by a specific namespace.  Many examples\n        provide a URL but it could be any arbitrary string.\n\n        The body is a list, so many subjects can be added at the same time.\n        \"\"\"\n        assert isinstance(body, (list)), \"POST requires body to be a list\"\n\n        uri = self._get_subject_uri()\n        return self.service._post(uri, body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_subject(self, subject_id):\n        # subject_id could be a path such as '/role/analyst' so quote\n        uri = self._get_subject_uri(guid=subject_id)\n        return self.service._delete(uri)", "response": "Remove a specific subject by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _put_subject(self, subject_id, body):\n        assert isinstance(body, (dict)), \"PUT requires body to be dict.\"\n\n        # subject_id could be a path such as '/asset/123' so quote\n        uri = self._get_subject_uri(guid=subject_id)\n        return self.service._put(uri, body)", "response": "Update a subject for the given subject id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a subject with a given identifier and attribute dictionary.", "response": "def add_subject(self, subject_id, attributes, parents=[],\n            issuer='default'):\n        \"\"\"\n        Will add the given subject with a given identifier and attribute\n        dictionary.\n\n            example/\n\n                add_subject('/user/j12y', {'username': 'j12y'})\n        \"\"\"\n        # MAINT: consider test to avoid adding duplicate subject id\n        assert isinstance(attributes, (dict)), \"attributes expected to be dict\"\n\n        attrs = []\n        for key in attributes.keys():\n            attrs.append({\n                'issuer': issuer,\n                'name': key,\n                'value': attributes[key]\n                })\n\n        body = {\n            \"subjectIdentifier\": subject_id,\n            \"parents\": parents,\n            \"attributes\": attrs,\n        }\n\n        return self._put_subject(subject_id, body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests whether or not the ACS service being monitored is alive.", "response": "def _get_monitoring_heartbeat(self):\n        \"\"\"\n        Tests whether or not the ACS service being monitored is alive.\n        \"\"\"\n        target = self.uri + '/monitoring/heartbeat'\n        response = self.session.get(target)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_alive(self):\n        response = self.get_monitoring_heartbeat()\n        if response.status_code == 200 and response.content == 'alive':\n            return True\n\n        return False", "response": "Checks whether the ACS service is up and alive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_policy_set_uri(self, guid=None):\n        uri = self.uri + '/v1/policy-set'\n        if guid:\n            uri += '/' + urllib.quote_plus(guid)\n        return uri", "response": "Returns the full path that uniquely identifies\n            the subject endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _put_policy_set(self, policy_set_id, body):\n        assert isinstance(body, (dict)), \"PUT requires body to be a dict.\"\n        uri = self._get_policy_set_uri(guid=policy_set_id)\n        return self.service._put(uri, body)", "response": "Create or update a policy set for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_policy_set(self, policy_set_id):\n        uri = self._get_policy_set_uri(guid=policy_set_id)\n        return self.service._get(uri)", "response": "Get a specific policy set by id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a specific policy set by id. Method is idempotent.", "response": "def delete_policy_set(self, policy_set_id):\n        \"\"\"\n        Delete a specific policy set by id.  Method is idempotent.\n        \"\"\"\n        uri = self._get_policy_set_uri(guid=policy_set_id)\n        return self.service._delete(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_policy(self, name, action, resource, subject, condition,\n            policy_set_id=None, effect='PERMIT'):\n        \"\"\"\n        Will create a new policy set to enforce the given policy details.\n\n        The name is just a helpful descriptor for the policy.\n\n        The action maps to a HTTP verb.\n\n        Policies are evaluated against resources and subjects.  They are\n        identified by matching a uriTemplate or attributes.\n\n        Examples::\n\n            resource = {\n                \"uriTemplate\": \"/asset/{id}\"\n                }\n\n            subject: {\n                \"attributes\": [{\n                    \"issuer\": \"default\",\n                    \"name\": \"role\"\n                    }]\n                }\n\n        The condition is expected to be a string that defines a groovy\n        operation that can be evaluated.\n\n        Examples::\n\n            condition = \"match.single(subject.attributes('default', 'role'),\n                'admin')\n\n        \"\"\"\n        # If not given a policy set id will generate one\n        if not policy_set_id:\n            policy_set_id = str(uuid.uuid4())\n\n        # Only a few operations / actions are supported in policy definitions\n        if action not in ['GET', 'PUT', 'POST', 'DELETE']:\n            raise ValueError(\"Invalid action\")\n\n        # Defines a single policy to be part of the policy set.\n        policy = {\n            \"name\": name,\n            \"target\": {\n                \"resource\": resource,\n                \"subject\": subject,\n                \"action\": action,\n                },\n            \"conditions\": [{\n                \"name\": \"\",\n                \"condition\": condition,\n                }],\n            \"effect\": effect,\n        }\n\n        # Body of the request is a list of policies\n        body = {\n            \"name\": policy_set_id,\n            \"policies\": [policy],\n        }\n\n        result = self._put_policy_set(policy_set_id, body)\n        return result", "response": "Adds a new policy to the given set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_allowed(self, subject_id, action, resource_id, policy_sets=[]):\n        body = {\n            \"action\": action,\n            \"subjectIdentifier\": subject_id,\n            \"resourceIdentifier\": resource_id,\n        }\n\n        if policy_sets:\n            body['policySetsEvaluationOrder'] = policy_sets\n\n        # Will return a 200 with decision\n        uri = self.uri + '/v1/policy-evaluation'\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"BODY=\" + str(body))\n\n        response = self.service._post(uri, body)\n\n        if 'effect' in response:\n            if response['effect'] in ['NOT_APPLICABLE', 'PERMIT']:\n                return True\n\n        return False", "response": "Evaluate a policy - set against a subject and resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fill_in_cainfo(self):\n        if self.cainfo:\n            cainfo = self.cainfo\n        else:\n            try:\n                cainfo = certifi.where()\n            except AttributeError:\n                cainfo = None\n        if cainfo:\n            self._pycurl.setopt(pycurl.CAINFO, cainfo)", "response": "Fill in the path of the PEM file containing the CA certificate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef curl(self):\n        c = self._pycurl\n        # Resume download\n        if os.path.exists(self.path) and self.resume:\n            mode = 'ab'\n            self.downloaded = os.path.getsize(self.path)\n            c.setopt(pycurl.RESUME_FROM, self.downloaded)\n        else:\n            mode = 'wb'\n        with open(self.path, mode) as f:\n            c.setopt(c.URL, utf8_encode(self.url))\n            if self.auth:\n                c.setopt(c.USERPWD, '%s:%s' % self.auth)\n            c.setopt(c.USERAGENT, self._user_agent)\n            c.setopt(c.WRITEDATA, f)\n            h = self._get_pycurl_headers()\n            if h is not None:\n                c.setopt(pycurl.HTTPHEADER, h)\n            c.setopt(c.NOPROGRESS, 0)\n            c.setopt(pycurl.FOLLOWLOCATION, 1)\n            c.setopt(c.PROGRESSFUNCTION, self.progress)\n            self._fill_in_cainfo()\n            if self._pass_through_opts:\n                for key, value in self._pass_through_opts.items():\n                    c.setopt(key, value)\n            c.perform()", "response": "Sends a single cURL request to download the current file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart downloading handling auto retry download resume and path", "response": "def start(self):\n        \"\"\"\n        Start downloading, handling auto retry, download resume and path\n        moving\n        \"\"\"\n        if not self.auto_retry:\n            self.curl()\n            return\n        while not self.is_finished:\n            try:\n                self.curl()\n            except pycurl.error as e:\n                # transfer closed with n bytes remaining to read\n                if e.args[0] == pycurl.E_PARTIAL_FILE:\n                    pass\n                # HTTP server doesn't seem to support byte ranges.\n                # Cannot resume.\n                elif e.args[0] == pycurl.E_HTTP_RANGE_ERROR:\n                    break\n                # Recv failure: Connection reset by peer\n                elif e.args[0] == pycurl.E_RECV_ERROR:\n                    if self._rst_retries < self.max_rst_retries:\n                        pass\n                    else:\n                        raise e\n                    self._rst_retries += 1\n                else:\n                    raise e\n        self._move_path()\n        self._done()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _move_path(self):\n        if is_temp_path(self._path) and self._pycurl is not None:\n            eurl = self._pycurl.getinfo(pycurl.EFFECTIVE_URL)\n            er = get_resource_name(eurl)\n            r = get_resource_name(self.url)\n            if er != r and os.path.exists(self.path):\n                new_path = self._get_path(self._path, eurl)\n                shutil.move(self.path, new_path)\n                self.path = new_path", "response": "Move the downloaded file to the authentic path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_to_manifest(self, manifest):\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variable to manifest\n        varname = predix.config.set_env_value(self.use_class, 'uri',\n                self._get_uri())\n        manifest.add_env_var(varname, self._get_uri())\n\n        manifest.write_manifest()", "response": "Add useful details to the manifest about this service and environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self):\n        # Make sure we've stored uri for use\n        predix.config.set_env_value(self.use_class, 'uri', self._get_uri())\n\n        self.uaac = predix.security.uaa.UserAccountAuthentication()\n        self.uaac.authenticate('admin', self._get_admin_secret(),\n                use_cache=False)\n        self.is_admin = True", "response": "Authenticate into the UAA instance as the admin user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_secret(self, length=12):\n        # Charset will have 64 +- characters\n        charset = string.digits + string.ascii_letters + '+-'\n        return \"\".join(random.SystemRandom().choice(charset) for _ in\n                range(length))", "response": "Create a secret for the cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_client(self, client_id, client_secret):\n        assert self.is_admin, \"Must authenticate() as admin to create client\"\n        return self.uaac.create_client(client_id, client_secret)", "response": "Create a new client for use by applications."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_client_to_manifest(self, client_id, client_secret, manifest):\n        assert self.is_admin, \"Must authenticate() as admin to create client\"\n        return self.uaac.add_client_to_manifest(client_id, client_secret,\n                manifest)", "response": "Add the client credentials to the specified manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the URI endpoint for an instance of a UAA service instance from environment inspection.", "response": "def _get_uaa_uri(self):\n        \"\"\"\n        Returns the URI endpoint for an instance of a UAA\n        service instance from environment inspection.\n        \"\"\"\n        if 'VCAP_SERVICES' in os.environ:\n            services = json.loads(os.getenv('VCAP_SERVICES'))\n            predix_uaa = services['predix-uaa'][0]['credentials']\n            return predix_uaa['uri']\n        else:\n            return predix.config.get_env_value(self, 'uri')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nauthenticating with the given client and secret.", "response": "def _authenticate_client(self, client, secret):\n        \"\"\"\n        Returns response of authenticating with the given client and\n        secret.\n        \"\"\"\n        client_s = str.join(':', [client, secret])\n        credentials = base64.b64encode(client_s.encode('utf-8')).decode('utf-8')\n\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n            'Cache-Control': 'no-cache',\n            'Authorization': 'Basic ' + credentials\n            }\n        params = {\n            'client_id': client,\n            'grant_type': 'client_credentials'\n            }\n        uri = self.uri + '/oauth/token'\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n        logging.debug(\"BODY=\" + str(params))\n\n        response = requests.post(uri, headers=headers, params=params)\n        if response.status_code == 200:\n            logging.debug(\"RESPONSE=\" + str(response.json()))\n            return response.json()\n        else:\n            logging.warning(\"Failed to authenticate as %s\" % (client))\n            response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _authenticate_user(self, user, password):\n        headers = self._get_headers()\n        params = {\n                'username': user,\n                'password': password,\n                'grant_type': 'password',\n                }\n        uri = self.uri + '/oauth/token'\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n        logging.debug(\"BODY=\" + str(params))\n\n        response = requests.post(uri, headers=headers, params=params)\n        if response.status_code == 200:\n            logging.debug(\"RESPONSE=\" + str(response.json()))\n            return response.json()\n        else:\n            logging.warning(\"Failed to authenticate %s\" % (user))\n            response.raise_for_status()", "response": "Authenticate with the given user and password."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the token has expired.", "response": "def is_expired_token(self, client):\n        \"\"\"\n        For a given client will test whether or not the token\n        has expired.\n\n        This is for testing a client object and does not look up\n        from client_id.  You can use _get_client_from_cache() to\n        lookup a client from client_id.\n        \"\"\"\n        if 'expires' not in client:\n            return True\n\n        expires = dateutil.parser.parse(client['expires'])\n        if expires < datetime.datetime.now():\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the uaa cache.", "response": "def _initialize_uaa_cache(self):\n        \"\"\"\n        If we don't yet have a uaa cache we need to\n        initialize it.  As there may be more than one\n        UAA instance we index by issuer and then store\n        any clients, users, etc.\n        \"\"\"\n        try:\n            os.makedirs(os.path.dirname(self._cache_path))\n        except OSError as exc:\n            if exc.errno != errno.EEXIST:\n                raise\n\n        data = {}\n        data[self.uri] = []\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_uaa_cache(self):\n        self._cache_path = os.path.expanduser('~/.predix/uaa.json')\n        if not os.path.exists(self._cache_path):\n            return self._initialize_uaa_cache()\n\n        with open(self._cache_path, 'r') as data:\n            return json.load(data)", "response": "Read cache of UAA client details."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the client from the cache.", "response": "def _get_client_from_cache(self, client_id):\n        \"\"\"\n        For the given client_id return what is\n        cached.\n        \"\"\"\n        data = self._read_uaa_cache()\n\n        # Only if we've cached any for this issuer\n        if self.uri not in data:\n            return\n\n        for client in data[self.uri]:\n            if client['id'] == client_id:\n                return client"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the new item to the uaa cache file.", "response": "def _write_to_uaa_cache(self, new_item):\n        \"\"\"\n        Cache the client details into a cached file on disk.\n        \"\"\"\n        data = self._read_uaa_cache()\n\n        # Initialize client list if first time\n        if self.uri not in data:\n            data[self.uri] = []\n\n        # Remove existing client record and any expired tokens\n        for client in data[self.uri]:\n            if new_item['id'] == client['id']:\n                data[self.uri].remove(client)\n                continue\n\n            # May have old tokens laying around to be cleaned up\n            if 'expires' in client:\n                expires = dateutil.parser.parse(client['expires'])\n                if expires < datetime.datetime.now():\n                    data[self.uri].remove(client)\n                    continue\n\n        data[self.uri].append(new_item)\n\n        with open(self._cache_path, 'w') as output:\n            output.write(json.dumps(data, sort_keys=True, indent=4))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self, client_id, client_secret, use_cache=True):\n        # We will reuse a token for as long as we have one cached\n        # and it hasn't expired.\n        if use_cache:\n            client = self._get_client_from_cache(client_id)\n            if (client) and (not self.is_expired_token(client)):\n                self.authenticated = True\n                self.client = client\n                return\n\n        # Let's authenticate the client\n        client = {\n            'id': client_id,\n            'secret': client_secret\n        }\n\n        res = self._authenticate_client(client_id, client_secret)\n        client.update(res)\n\n        expires = datetime.datetime.now() + \\\n                  datetime.timedelta(seconds=res['expires_in'])\n        client['expires'] = expires.isoformat()\n\n        # Cache it for repeated use until expired\n        self._write_to_uaa_cache(client)\n\n        self.client = client\n        self.authenticated = True", "response": "Authenticate the given client against UAA."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logout(self):\n        # Remove token from local cache\n        # MAINT: need to expire token on server\n        data = self._read_uaa_cache()\n        if self.uri in data:\n            for client in data[self.uri]:\n                if client['id'] == self.client['id']:\n                    data[self.uri].remove(client)\n\n        with open(self._cache_path, 'w') as output:\n            output.write(json.dumps(data, sort_keys=True, indent=4))", "response": "Log currently authenticated user out invalidating any existing tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the bare access token for the authorized client.", "response": "def get_token(self):\n        \"\"\"\n        Returns the bare access token for the authorized client.\n        \"\"\"\n        if not self.authenticated:\n            raise ValueError(\"Must authenticate() as a client first.\")\n\n        # If token has expired we'll need to refresh and get a new\n        # client credential\n        if self.is_expired_token(self.client):\n            logging.info(\"client token expired, will need to refresh token\")\n            self.authenticate(self.client['id'], self.client['secret'],\n                    use_cache=False)\n\n        return self.client['access_token']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the scopes for the authenticated client.", "response": "def get_scopes(self):\n        \"\"\"\n        Returns the scopes for the authenticated client.\n        \"\"\"\n        if not self.authenticated:\n            raise ValueError(\"Must authenticate() as a client first.\")\n\n        scope = self.client['scope']\n        return scope.split()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassert that the current user has permission to perform the operation.", "response": "def assert_has_permission(self, scope_required):\n        \"\"\"\n        Warn that the required scope is not found in the scopes\n        granted to the currently authenticated user.\n\n        ::\n\n            # The admin user should have client admin permissions\n            uaa.assert_has_permission('admin', 'clients.admin')\n\n        \"\"\"\n        if not self.authenticated:\n            raise ValueError(\"Must first authenticate()\")\n\n        if scope_required not in self.get_scopes():\n            logging.warning(\"Authenticated as %s\" % (self.client['id']))\n            logging.warning(\"Have scopes: %s\" % (str.join(',', self.get_scopes())))\n            logging.warning(\"Insufficient scope %s for operation\" % (scope_required))\n\n            raise ValueError(\"Client does not have permission.\")\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grant_client_permissions(self, client_id, admin=False, write=False,\n            read=False, secret=False):\n        \"\"\"\n        Grant the given client_id permissions for managing clients.\n\n        - clients.admin: super user scope to create, modify, delete\n        - clients.write: scope ot create and modify clients\n        - clients.read: scope to read info about clients\n        - clients.secret: scope to change password of a client\n\n        \"\"\"\n        self.assert_has_permission('clients.admin')\n\n        perms = []\n        if admin:\n            perms.append('clients.admin')\n\n        if write or admin:\n            perms.append('clients.write')\n\n        if read or admin:\n            perms.append('clients.read')\n\n        if secret or admin:\n            perms.append('clients.secret')\n\n        if perms:\n            self.update_client_grants(client_id, scope=perms,\n                    authorities=perms)", "response": "Grant the given client_id permissions for managing clients."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the clients stored in the instance of UAA.", "response": "def get_clients(self):\n        \"\"\"\n        Returns the clients stored in the instance of UAA.\n        \"\"\"\n        self.assert_has_permission('clients.read')\n\n        uri = self.uri + '/oauth/clients'\n        headers = self.get_authorization_headers()\n        response = requests.get(uri, headers=headers)\n        return response.json()['resources']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn details about a specific client by the client_id.", "response": "def get_client(self, client_id):\n        \"\"\"\n        Returns details about a specific client by the client_id.\n        \"\"\"\n        self.assert_has_permission('clients.read')\n\n        uri = self.uri + '/oauth/clients/' + client_id\n        headers = self.get_authorization_headers()\n        response = requests.get(uri, headers=headers)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            # Not found but don't raise\n            return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_client_grants(self, client_id, scope=[], authorities=[],\n            grant_types=[], redirect_uri=[], replace=False):\n        \"\"\"\n        Will extend the client with additional scopes or\n        authorities.  Any existing scopes and authorities will be left\n        as is unless asked to replace entirely.\n        \"\"\"\n        self.assert_has_permission('clients.write')\n\n        client = self.get_client(client_id)\n        if not client:\n            raise ValueError(\"Must first create client: '%s'\" % (client_id))\n\n        if replace:\n            changes = {\n                'client_id': client_id,\n                'scope': scope,\n                'authorities': authorities,\n                }\n        else:\n            changes = {'client_id': client_id}\n            if scope:\n                changes['scope'] = client['scope']\n                changes['scope'].extend(scope)\n\n            if authorities:\n                changes['authorities'] = client['authorities']\n                changes['authorities'].extend(authorities)\n\n            if grant_types:\n                if 'authorization_code' in grant_types and not redirect_uri:\n                    logging.warning(\"A redirect_uri is required for authorization_code.\")\n\n                changes['authorized_grant_types'] = client['authorized_grant_types']\n                changes['authorized_grant_types'].extend(grant_types)\n\n            if redirect_uri:\n                if 'redirect_uri' in client:\n                    changes['redirect_uri'] = client['redirect_uri']\n                    changes['redirect_uri'].extend(redirect_uri)\n                else:\n                    changes['redirect_uri'] = redirect_uri\n\n        uri = self.uri + '/oauth/clients/' + client_id\n        headers = {\n            \"pragma\": \"no-cache\",\n            \"Cache-Control\": \"no-cache\",\n            \"Content-Type\": \"application/json\",\n            \"Accepts\": \"application/json\",\n            \"Authorization\": \"Bearer \" + self.get_token()\n        }\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n        logging.debug(\"BODY=\" + json.dumps(changes))\n\n        response = requests.put(uri, headers=headers, data=json.dumps(changes))\n\n        logging.debug(\"STATUS=\" + str(response.status_code))\n        if response.status_code == 200:\n            return response\n        else:\n            logging.error(response.content)\n            response.raise_for_status()", "response": "Updates the grant types and redirect_uri of a client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grant_scim_permissions(self, client_id, read=False, write=False,\n            create=False, userids=False, zones=False, invite=False,\n            openid=False):\n        \"\"\"\n        Grant the given client_id permissions for managing users.  System\n        for Cross-domain Identity Management (SCIM) are required for accessing\n        /Users and /Groups endpoints of UAA.\n\n        - scim.read: scope for read access to all SCIM endpoints\n        - scim.write: scope for write access to all SCIM endpoints\n        - scim.create: scope to create/invite users and verify an account only\n        - scim.userids: scope for id and username+origin conversion\n        - scim.zones: scope for group management of users only\n        - scim.invite: scope to participate in invitations\n        - openid: scope to access /userinfo\n\n        \"\"\"\n        self.assert_has_permission('clients.admin')\n\n        perms = []\n        if read:\n            perms.append('scim.read')\n\n        if write:\n            perms.append('scim.write')\n\n        if create:\n            perms.append('scim.create')\n\n        if userids:\n            perms.append('scim.userids')\n\n        if zones:\n            perms.append('scim.zones')\n\n        if invite:\n            perms.append('scim.invite')\n\n        if openid:\n            perms.append('openid')\n\n        if perms:\n            self.update_client_grants(client_id, scope=perms, authorities=perms)", "response": "Grant the given client_id permissions for managing users."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new client for your application use.", "response": "def create_client(self, client_id, client_secret, manifest=None,\n            client_credentials=True, refresh_token=True,\n            authorization_code=False, redirect_uri=[]):\n        \"\"\"\n        Will create a new client for your application use.\n\n        - client_credentials: allows client to get access token\n        - refresh_token: can be used to get new access token when expired\n          without re-authenticating\n        - authorization_code: redirection-based flow for user authentication\n\n        More details about Grant types:\n        - https://github.com/cloudfoundry/uaa/blob/master/docs/UAA-Security.md\n        - https://tools.ietf.org/html/rfc6749\n\n        A redirect_uri is required when using authorization_code.  See:\n        https://www.predix.io/support/article/KB0013026\n\n        \"\"\"\n        self.assert_has_permission('clients.admin')\n\n        if authorization_code and not redirect_uri:\n            raise ValueError(\"Must provide a redirect_uri for clients used with authorization_code\")\n\n        # Check if client already exists\n        client = self.get_client(client_id)\n        if client:\n            return client\n\n        uri = self.uri + '/oauth/clients'\n        headers = {\n            \"pragma\": \"no-cache\",\n            \"Cache-Control\": \"no-cache\",\n            \"Content-Type\": \"application/json\",\n            \"Accepts\": \"application/json\",\n            \"Authorization\": \"Bearer \" + self.get_token()\n        }\n\n        grant_types = []\n\n        if client_credentials:\n            grant_types.append('client_credentials')\n        if refresh_token:\n            grant_types.append('refresh_token')\n        if authorization_code:\n            grant_types.append('authorization_code')\n\n        params = {\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"scope\": [\"uaa.none\"],\n            \"authorized_grant_types\": grant_types,\n            \"authorities\": [\"uaa.none\"],\n            \"autoapprove\": []\n        }\n\n        if redirect_uri:\n            params.append(redirect_uri)\n\n        response = requests.post(uri, headers=headers, data=json.dumps(params))\n        if response.status_code == 201:\n            if manifest:\n                self.add_client_to_manifest(client_id, client_secret, manifest)\n\n            client = {\n                'id': client_id,\n                'secret': client_secret\n                }\n            self._write_to_uaa_cache(client)\n            return response\n        else:\n            logging.error(response.content)\n            response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_client_to_manifest(self, client_id, client_secret, manifest):\n        client_id_key = 'PREDIX_APP_CLIENT_ID'\n        manifest.add_env_var(client_id_key, client_id)\n\n        client_secret_key = 'PREDIX_APP_CLIENT_SECRET'\n        manifest.add_env_var(client_secret_key, client_secret)\n\n        manifest.write_manifest()", "response": "Add the given client id and secret to the given manifest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new user account with the required details.", "response": "def create_user(self, username, password, family_name, given_name, primary_email,\n            details={}):\n        \"\"\"\n        Creates a new user account with the required details.\n\n        ::\n\n            create_user('j12y', 'my-secret', 'Delancey', 'Jayson', 'volcano@ge.com')\n\n        \"\"\"\n        self.assert_has_permission('scim.write')\n\n        data = {\n            'userName': username,\n            'password': password,\n            'name': {\n                'familyName': family_name,\n                'givenName': given_name,\n                },\n            'emails': [{\n                'value': primary_email,\n                'primary': True,\n                }]\n            }\n\n        if details:\n            data.update(details)\n\n        return self._post_user(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes user with given id.", "response": "def delete_user(self, id):\n        \"\"\"\n        Delete user with given id.\n        \"\"\"\n        self.assert_has_permission('scim.write')\n\n        uri = self.uri + '/Users/%s' % id\n        headers = self._get_headers()\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n\n        response = self.session.delete(uri, headers=headers)\n        logging.debug(\"STATUS=\" + str(response.status_code))\n        if response.status_code == 200:\n            return response\n        else:\n            logging.error(response.content)\n            response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of users stored in UAA.", "response": "def get_users(self, filter=None, sortBy=None, sortOrder=None,\n            startIndex=None, count=None):\n        \"\"\"\n        Returns users accounts stored in UAA.\n        See https://docs.cloudfoundry.org/api/uaa/#list63\n\n        For filtering help, see:\n        http://www.simplecloud.info/specs/draft-scim-api-01.html#query-resources\n        \"\"\"\n        self.assert_has_permission('scim.read')\n\n        params = {}\n        if filter:\n            params['filter'] = filter\n\n        if sortBy:\n            params['sortBy'] = sortBy\n\n        if sortOrder:\n            params['sortOrder'] = sortOrder\n\n        if startIndex:\n            params['startIndex'] = startIndex\n\n        if count:\n            params['count'] = count\n\n        return self._get(self.uri + '/Users', params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_by_username(self, username):\n        results = self.get_users(filter='username eq \"%s\"' % (username))\n        if results['totalResults'] == 0:\n            logging.warning(\"Found no matches for given username.\")\n            return\n        elif results['totalResults'] > 1:\n            logging.warning(\"Found %s matches for username %s\" %\n                (results['totalResults'], username))\n\n        return results['resources'][0]", "response": "Returns details for a user by username."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning details for a user with the given email address.", "response": "def get_user_by_email(self, email):\n        \"\"\"\n        Returns details for user with the given email address.\n\n        If there is more than one match will only return the first.  Use\n        get_users() for full result set.\n        \"\"\"\n        results = self.get_users(filter='email eq \"%s\"' % (email))\n        if results['totalResults'] == 0:\n            logging.warning(\"Found no matches for given email.\")\n            return\n        elif results['totalResults'] > 1:\n            logging.warning(\"Found %s matches for email %s\" %\n                (results['totalResults'], email))\n\n        return results['resources'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns details about the user with the given id.", "response": "def get_user(self, id):\n        \"\"\"\n        Returns details about the user for the given id.\n\n        Use get_user_by_email() or get_user_by_username() for help\n        identifiying the id.\n        \"\"\"\n        self.assert_has_permission('scim.read')\n        return self._get(self.uri + '/Users/%s' % (id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self):\n        self.service.create()\n\n        predix.config.set_env_value(self.use_class, 'ingest_uri',\n                self.get_ingest_uri())\n        predix.config.set_env_value(self.use_class, 'ingest_zone_id',\n                self.get_ingest_zone_id())\n\n        predix.config.set_env_value(self.use_class, 'query_uri',\n                self.get_query_uri())\n        predix.config.set_env_value(self.use_class, 'query_zone_id',\n                self.get_query_zone_id())", "response": "Create an instance of the Time Series Service with the typical\n        starting settings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grant_client(self, client_id, read=True, write=True):\n        scopes = ['openid']\n        authorities = ['uaa.resource']\n\n        if write:\n            for zone in self.service.settings.data['ingest']['zone-token-scopes']:\n                scopes.append(zone)\n                authorities.append(zone)\n\n        if read:\n            for zone in self.service.settings.data['query']['zone-token-scopes']:\n                scopes.append(zone)\n                authorities.append(zone)\n\n        self.service.uaa.uaac.update_client_grants(client_id, scope=scopes,\n                authorities=authorities)\n\n        return self.service.uaa.uaac.get_client(client_id)", "response": "Grant the given client id all the scopes and authorities needed to work with the timeseries service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_query_uri(self):\n        # Query URI has extra path we don't want so strip it off here\n        query_uri = self.service.settings.data['query']['uri']\n        query_uri = urlparse(query_uri)\n        return query_uri.scheme + '://' + query_uri.netloc", "response": "Return the uri used for queries on time series data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_to_manifest(self, manifest):\n        # Add this service to list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n        uri = predix.config.get_env_key(self.use_class, 'ingest_uri')\n        manifest.add_env_var(uri, self.get_ingest_uri())\n\n        zone_id = predix.config.get_env_key(self.use_class, 'ingest_zone_id')\n        manifest.add_env_var(zone_id, self.get_ingest_zone_id())\n\n        uri = predix.config.get_env_key(self.use_class, 'query_uri')\n        manifest.add_env_var(uri, self.get_query_uri())\n\n        zone_id = predix.config.get_env_key(self.use_class, 'query_zone_id')\n        manifest.add_env_var(zone_id, self.get_query_zone_id())\n\n        manifest.write_manifest()", "response": "Add useful details to the manifest about this service."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_app_guid(self, app_name):\n        summary = self.space.get_space_summary()\n        for app in summary['apps']:\n            if app['name'] == app_name:\n                return app['guid']", "response": "Returns the GUID for the app with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the given app.", "response": "def delete_app(self, app_name):\n        \"\"\"\n        Delete the given app.\n\n        Will fail intentionally if there are any service\n        bindings.  You must delete those first.\n        \"\"\"\n        if app_name not in self.space.get_apps():\n            logging.warning(\"App not found so... succeeded?\")\n            return True\n\n        guid = self.get_app_guid(app_name)\n        self.api.delete(\"/v2/apps/%s\" % (guid))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_service_config(self):\n        # Should work for windows, osx, and linux environments\n        if not os.path.exists(self.config_path):\n            try:\n                os.makedirs(os.path.dirname(self.config_path))\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    raise\n\n            return {}\n\n        with open(self.config_path, 'r') as data:\n            return json.load(data)", "response": "Reads in the config file of UAA credential information and generates one if not yet initialized."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the config to disk.", "response": "def _write_service_config(self):\n        \"\"\"\n        Will write the config out to disk.\n        \"\"\"\n        with open(self.config_path, 'w') as output:\n            output.write(json.dumps(self.data, sort_keys=True, indent=4))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an instance of the Blob Store Service with the typical starting settings.", "response": "def create(self, **kwargs):\n        \"\"\"\n        Create an instance of the Blob Store Service with the typical\n        starting settings.\n        \"\"\"\n        self.service.create(**kwargs)\n\n        predix.config.set_env_value(self.use_class, 'url',\n                self.service.settings.data['url'])\n        predix.config.set_env_value(self.use_class, 'access_key_id',\n                self.service.settings.data['access_key_id'])\n        predix.config.set_env_value(self.use_class, 'bucket_name',\n                self.service.settings.data['bucket_name'])\n        predix.config.set_env_value(self.use_class, 'host',\n                self.service.settings.data['host'])\n        predix.config.set_env_value(self.use_class, 'secret_access_key',\n                self.service.settings.data['secret_access_key'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds useful details to the manifest about this service.", "response": "def add_to_manifest(self, manifest):\n        \"\"\"\n        Add useful details to the manifest about this service\n        so that it can be used in an application.\n\n        :param manifest: An predix.admin.app.Manifest object\n            instance that manages reading/writing manifest config\n            for a cloud foundry app.\n        \"\"\"\n        # Add this service to the list of services\n        manifest.add_service(self.service.name)\n\n        # Add environment variables\n\n        url = predix.config.get_env_key(self.use_class, 'url')\n        manifest.add_env_var(url, self.service.settings.data['url'])\n\n        akid = predix.config.get_env_key(self.use_class, 'access_key_id')\n        manifest.add_env_var(akid, self.service.settings.data['access_key_id'])\n\n        bucket = predix.config.get_env_key(self.use_class, 'bucket_name')\n        manifest.add_env_var(bucket, self.service.settings.data['bucket_name'])\n\n        host = predix.config.get_env_key(self.use_class, 'host')\n        manifest.add_env_var(host, self.service.settings.data['host'])\n\n        secret_access_key = predix.config.get_env_key(self.use_class, 'secret_access_key')\n        manifest.add_env_var(secret_access_key, self.service.settings.data['secret_access_key'])\n\n        manifest.write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator for all subscribe messages", "response": "def subscribe(self):\n        \"\"\"\n        return a generator for all subscribe messages\n        :return: None\n        \"\"\"\n        while self.run_subscribe_generator:\n            if len(self._rx_messages) != 0:\n                yield self._rx_messages.pop(0)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_acks(self, message):\n        if isinstance(message, EventHub_pb2.Message):\n            ack = EventHub_pb2.Ack(partition=message.partition, offset=message.offset)\n            self.grpc_manager.send_message(EventHub_pb2.SubscriptionResponse(ack=ack))\n\n        elif isinstance(message, EventHub_pb2.SubscriptionMessage):\n            acks = []\n            for m in message.messages:\n                acks.append(EventHub_pb2.Ack(parition=m.partition, offset=m.offset))\n            self.grpc_manager.send_message(EventHub_pb2.SubscriptionAcks(ack=acks))", "response": "Send acks to the service\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the subscribe stub headers based on the supplied config", "response": "def _generate_subscribe_headers(self):\n        \"\"\"\n        generate the subscribe stub headers based on the supplied config\n        :return: i\n        \"\"\"\n        headers =[]\n        headers.append(('predix-zone-id', self.eventhub_client.zone_id))\n\n        token = self.eventhub_client.service._get_bearer_token()\n        headers.append(('subscribername', self._config.subscriber_name))\n        headers.append(('authorization', token[(token.index(' ') + 1):]))\n\n        if self._config.topics is []:\n            headers.append(('topic', self.eventhub_client.zone_id + '_topic'))\n        else:\n            for topic in self._config.topics:\n                headers.append(('topic', topic))\n\n        headers.append(('offset-newest', str(self._config.recency == self._config.Recency.NEWEST).lower()))\n\n        headers.append(('acks', str(self._config.acks_enabled).lower()))\n        if self._config.acks_enabled:\n            headers.append(('max-retries', str(self._config.ack_max_retries)))\n            headers.append(('retry-interval', str(self._config.ack_retry_interval_seconds) + 's'))\n            headers.append(('duration-before-retry', str(self._config.ack_duration_before_retry_seconds) + 's'))\n\n        if self._config.batching_enabled:\n            headers.append(('batch-size', str(self._config.batch_size)))\n            headers.append(('batch-interval', str(self._config.batch_interval_millis) + 'ms'))\n\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the raw results of an asset search for a given bounding box.", "response": "def _get_assets(self, bbox, size=None, page=None, asset_type=None,\n            device_type=None, event_type=None, media_type=None):\n        \"\"\"\n        Returns the raw results of an asset search for a given bounding\n        box.\n        \"\"\"\n        uri = self.uri + '/v1/assets/search'\n        headers = self._get_headers()\n\n        params = {\n                'bbox': bbox,\n                }\n\n        # Query parameters\n\n        params['q'] = []\n        if device_type:\n            if isinstance(device_type, str):\n                device_type = [device_type]\n\n            for device in device_type:\n                if device not in self.DEVICE_TYPES:\n                    logging.warning(\"Invalid device type: %s\" % device)\n\n                params['q'].append(\"device-type:%s\" % device)\n\n        if asset_type:\n            if isinstance(asset_type, str):\n                asset_type = [asset_type]\n\n            for asset in asset_type:\n                if asset not in self.ASSET_TYPES:\n                    logging.warning(\"Invalid asset type: %s\" % asset)\n                params['q'].append(\"assetType:%s\" % asset)\n\n        if media_type:\n            if isinstance(media_type, str):\n                media_type = [media_type]\n\n            for media in media_type:\n                if media not in self.MEDIA_TYPES:\n                    logging.warning(\"Invalid media type: %s\" % media)\n                params['q'].append(\"mediaType:%s\" % media)\n\n        if event_type:\n            if isinstance(event_type, str):\n                event_type = [event_type]\n\n            for event in event_type:\n                if event not in self.EVENT_TYPES:\n                    logging.warning(\"Invalid event type: %s\" % event)\n                params['q'].append(\"eventTypes:%s\" % event)\n\n        # Pagination parameters\n\n        if size:\n            params['size'] = size\n\n        if page:\n            params['page'] = page\n\n        return self.service._get(uri, params=params, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_assets(self, bbox, **kwargs):\n        response = self._get_assets(bbox, **kwargs)\n\n        # Remove broken HATEOAS _links but identify asset uid first\n        assets = []\n        for asset in response['_embedded']['assets']:\n            asset_url = asset['_links']['self']\n            uid = asset_url['href'].split('/')[-1]\n            asset['uid'] = uid\n\n            del(asset['_links'])\n            assets.append(asset)\n\n        return assets", "response": "Query the assets stored in the intelligent environment for a given bounding box and query."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn raw response for an given asset by its unique id.", "response": "def _get_asset(self, asset_uid):\n        \"\"\"\n        Returns raw response for an given asset by its unique id.\n        \"\"\"\n        uri = self.uri + '/v2/assets/' + asset_uid\n\n        headers = self._get_headers()\n\n        return self.service._get(uri, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlabel input grid with hysteresis method.", "response": "def label(self, input_grid):\n        \"\"\"\n        Label input grid with hysteresis method.\n\n        Args:\n            input_grid: 2D array of values.\n\n        Returns:\n            Labeled output grid.\n        \"\"\"\n        unset = 0\n        high_labels, num_labels = label(input_grid > self.high_thresh)\n        region_ranking = np.argsort(maximum(input_grid, high_labels, index=np.arange(1, num_labels + 1)))[::-1]\n        output_grid = np.zeros(input_grid.shape, dtype=int)\n        stack = []\n        for rank in region_ranking:\n            label_num = rank + 1\n            label_i, label_j = np.where(high_labels == label_num)\n            for i in range(label_i.size):\n                if output_grid[label_i[i], label_j[i]] == unset:\n                    stack.append((label_i[i], label_j[i]))\n            while len(stack) > 0:\n                index = stack.pop()\n                output_grid[index] = label_num\n                for i in range(index[0] - 1, index[0] + 2):\n                    for j in range(index[1] - 1, index[1] + 2):\n                        if 0 <= i < output_grid.shape[0] and 0 <= j < output_grid.shape[1]:\n                            if (input_grid[i, j] > self.low_thresh) and (output_grid[i, j] == unset):\n                                stack.append((i, j))\n        return output_grid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size_filter(labeled_grid, min_size):\n        out_grid = np.zeros(labeled_grid.shape, dtype=int)\n        slices = find_objects(labeled_grid)\n        j = 1\n        for i, s in enumerate(slices):\n            box = labeled_grid[s]\n            size = np.count_nonzero(box.ravel() == (i + 1))\n            if size >= min_size and box.shape[0] > 1 and box.shape[1] > 1:\n                out_grid[np.where(labeled_grid == i + 1)] = j\n                j += 1\n        return out_grid", "response": "Remove labeled objects that do not meet size threshold criteria."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef roc_curve(roc_objs, obj_labels, colors, markers, filename, figsize=(8, 8),\n              xlabel=\"Probability of False Detection\",\n              ylabel=\"Probability of Detection\",\n              title=\"ROC Curve\", ticks=np.arange(0, 1.1, 0.1), dpi=300,\n              legend_params=None, bootstrap_sets=None, ci=(2.5, 97.5),\n              label_fontsize=14, title_fontsize=16, tick_fontsize=12):\n    \"\"\"\n    Plots a set receiver/relative operating characteristic (ROC) curves from DistributedROC objects.\n\n    The ROC curve shows how well a forecast discriminates between two outcomes over a series of thresholds. It\n    features Probability of Detection (True Positive Rate) on the y-axis and Probability of False Detection\n    (False Alarm Rate) on the x-axis. This plotting function allows one to customize the colors and markers of the\n    ROC curves as well as the parameters of the legend and the title.\n\n    Args:\n        roc_objs (list): DistributedROC objects being plotted.\n        obj_labels (list): Label describing the forecast associated with a DistributedROC object.\n        colors (list): List of matplotlib-readable colors (names or hex-values) for each curve.\n        markers (list): Matplotlib marker (e.g. *, o, v, etc.) for each curve.\n        filename (str): Name of figure file being saved.\n        figsize (tuple): (Width, height) of the figure in inches.\n        xlabel (str): Label for the x-axis.\n        ylabel (str): Label for the y-axis.\n        title (str): The title of the figure.\n        ticks (numpy.ndarray): Values shown on the x and y axes.\n        dpi (int): Figure resolution in dots per inch.\n        legend_params (None, dict): Keyword arguments for the formatting of the figure legend.\n        bootstrap_sets (list): List of lists of DistributedROC objects that were bootstrap resampled for each model.\n        ci (tuple of 2 floats): Quantiles of the edges of the bootstrap confidence intervals ranging from 0 to 100.\n        label_fontsize (int): Font size of the x and y axis labels.\n        title_fontsize (int): Font size of the title.\n        tick_fontsize (int): Font size of the x and y tick labels.\n\n    Examples:\n\n        >>> from hagelslag.evaluation import DistributedROC\n        >>> import numpy as np\n        >>> forecasts = np.random.random(1000)\n        >>> obs = np.random.random_integers(0, 1, 1000)\n        >>> roc = DistributedROC()\n        >>> roc.update(forecasts, obs)\n        >>> roc_curve([roc], [\"Random\"], [\"orange\"], [\"o\"], \"random_roc.png\")\n    \"\"\"\n    if legend_params is None:\n        legend_params = dict(loc=4, fontsize=12, framealpha=1, frameon=True)\n    plt.figure(figsize=figsize, dpi=dpi)\n    plt.plot(ticks, ticks, \"k--\")\n    if bootstrap_sets is not None:\n        for b, b_set in enumerate(bootstrap_sets):\n            broc_curves = np.dstack([b_roc.roc_curve().values for b_roc in b_set])\n            pod_range = np.percentile(broc_curves[:,0], ci, axis=1)\n            pofd_range = np.percentile(broc_curves[:, 1], ci, axis=1)\n            pod_poly = np.concatenate((pod_range[1], pod_range[0, ::-1]))\n            pofd_poly = np.concatenate((pofd_range[0], pofd_range[1, ::-1]))\n            pod_poly[np.isnan(pod_poly)] = 0\n            pofd_poly[np.isnan(pofd_poly)] = 0\n            plt.fill(pofd_poly, pod_poly, alpha=0.5, color=colors[b])\n    for r, roc_obj in enumerate(roc_objs):\n        roc_data = roc_obj.roc_curve()\n        plt.plot(roc_data[\"POFD\"], roc_data[\"POD\"], marker=markers[r], color=colors[r], label=obj_labels[r])\n    plt.xlabel(xlabel, fontsize=label_fontsize)\n    plt.ylabel(ylabel, fontsize=label_fontsize)\n    plt.xticks(ticks, fontsize=tick_fontsize)\n    plt.yticks(ticks, fontsize=tick_fontsize)\n    plt.title(title, fontsize=title_fontsize)\n    plt.legend(**legend_params)\n    plt.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()", "response": "Plots the ROC curves for a set of DistributedROC objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef performance_diagram(roc_objs, obj_labels, colors, markers, filename, figsize=(8, 8),\n                        xlabel=\"Success Ratio (1-FAR)\",\n                        ylabel=\"Probability of Detection\", ticks=np.arange(0, 1.1, 0.1),\n                        dpi=300, csi_cmap=\"Blues\",\n                        csi_label=\"Critical Success Index\", title=\"Performance Diagram\",\n                        legend_params=None, bootstrap_sets=None, ci=(2.5, 97.5), label_fontsize=14,\n                        title_fontsize=16, tick_fontsize=12):\n    \"\"\"\n    Draws a performance diagram from a set of DistributedROC objects.\n\n    A performance diagram is a variation on the ROC curve in which the Probability of False Detection on the\n    x-axis has been replaced with the Success Ratio (1-False Alarm Ratio or Precision). The diagram also shows\n    the Critical Success Index (CSI or Threat Score) as a series of curved contours, and the frequency bias as\n    angled diagonal lines. Points along the 1:1 diagonal are unbiased, and better performing models should appear\n    in the upper right corner. The performance diagram is particularly useful for displaying verification for\n    severe weather warnings as it displays all three commonly used statistics (POD, FAR, and CSI) simultaneously\n    on the same chart.\n\n    Args:\n        roc_objs (list): DistributedROC objects being plotted.\n        obj_labels: list or array of labels describing each DistributedROC object.\n        obj_labels (list): Label describing the forecast associated with a DistributedROC object.\n        colors (list): List of matplotlib-readable colors (names or hex-values) for each curve.\n        markers (list): Matplotlib marker (e.g. *, o, v, etc.) for each curve.\n        filename (str): Name of figure file being saved.\n        figsize (tuple): (Width, height) of the figure in inches.\n        xlabel (str): Label for the x-axis.\n        ylabel (str): Label for the y-axis.\n        title (str): The title of the figure.\n        ticks (numpy.ndarray): Values shown on the x and y axes.\n        dpi (int): Figure resolution in dots per inch.\n        csi_cmap (str): Matplotlib colormap used to fill CSI contours.\n        csi_label (str): Label for CSI colormap.\n        legend_params (None or dict): Keyword arguments for the formatting of the figure legend.\n        bootstrap_sets (list): A list of arrays of bootstrapped DistributedROC objects. If not None,\n            confidence regions will be plotted.\n        ci (tuple): tuple of bootstrap confidence interval percentiles.\n        label_fontsize (int): Font size of the x and y axis labels.\n        title_fontsize (int): Font size of the title.\n        tick_fontsize (int): Font size of the x and y tick labels.\n\n    Examples:\n\n        >>> from hagelslag.evaluation import DistributedROC\n        >>> import numpy as np\n        >>> forecasts = np.random.random(1000)\n        >>> obs = np.random.random_integers(0, 1, 1000)\n        >>> roc = DistributedROC()\n        >>> roc.update(forecasts, obs)\n        >>> performance_diagram([roc], [\"Random\"], [\"orange\"], [\"o\"], \"random_performance.png\")\n    \"\"\"\n    if legend_params is None:\n        legend_params = dict(loc=4, fontsize=10, framealpha=1, frameon=True)\n    plt.figure(figsize=figsize)\n    grid_ticks = np.arange(0, 1.01, 0.01)\n    sr_g, pod_g = np.meshgrid(grid_ticks, grid_ticks)\n    bias = pod_g / sr_g\n    csi = 1.0 / (1.0 / sr_g + 1.0 / pod_g - 1.0)\n    csi_contour = plt.contourf(sr_g, pod_g, csi, np.arange(0.1, 1.1, 0.1), extend=\"max\", cmap=csi_cmap)\n    b_contour = plt.contour(sr_g, pod_g, bias, [0.5, 1, 1.5, 2, 4], colors=\"k\", linestyles=\"dashed\")\n    plt.clabel(b_contour, fmt=\"%1.1f\", manual=[(0.2, 0.9), (0.4, 0.9), (0.6, 0.9), (0.7, 0.7)])\n    if bootstrap_sets is not None:\n        for b, b_set in enumerate(bootstrap_sets):\n            perf_curves = np.dstack([b_roc.performance_curve().values for b_roc in b_set])\n            pod_range = np.nanpercentile(perf_curves[:, 0], ci, axis=1)\n            sr_range = np.nanpercentile(1 - perf_curves[:, 1], ci, axis=1)\n            pod_poly = np.concatenate((pod_range[1], pod_range[0, ::-1]))\n            sr_poly = np.concatenate((sr_range[1], sr_range[0, ::-1]))\n            pod_poly[np.isnan(pod_poly)] = 0\n            sr_poly[np.isnan(sr_poly)] = 1\n            plt.fill(sr_poly, pod_poly, alpha=0.5, color=colors[b])\n    for r, roc_obj in enumerate(roc_objs):\n        perf_data = roc_obj.performance_curve()\n        plt.plot(1 - perf_data[\"FAR\"], perf_data[\"POD\"], marker=markers[r], color=colors[r], label=obj_labels[r])\n    cbar = plt.colorbar(csi_contour)\n    cbar.set_label(csi_label)\n    plt.xlabel(xlabel, fontsize=label_fontsize)\n    plt.ylabel(ylabel, fontsize=label_fontsize)\n    plt.xticks(ticks, fontsize=tick_fontsize)\n    plt.yticks(ticks, fontsize=tick_fontsize)\n    plt.title(title, fontsize=title_fontsize)\n    plt.legend(**legend_params)\n    plt.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()", "response": "Draws a performance diagram from a set of DistributedROC objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the reliability curves against a 1 - diagonal figure.", "response": "def reliability_diagram(rel_objs, obj_labels, colors, markers, filename, figsize=(8, 8), xlabel=\"Forecast Probability\",\n                        ylabel=\"Observed Relative Frequency\", ticks=np.arange(0, 1.05, 0.05), dpi=300, inset_size=1.5,\n                        title=\"Reliability Diagram\", legend_params=None, bootstrap_sets=None, ci=(2.5, 97.5)):\n    \"\"\"\n    Plot reliability curves against a 1:1 diagonal to determine if probability forecasts are consistent with their\n    observed relative frequency.\n\n    Args:\n        rel_objs (list): List of DistributedReliability objects.\n        obj_labels (list): List of labels describing the forecast model associated with each curve.\n        colors (list): List of colors for each line\n        markers (list): List of line markers\n        filename (str): Where to save the figure.\n        figsize (tuple): (Width, height) of the figure in inches.\n        xlabel (str): X-axis label\n        ylabel (str): Y-axis label\n        ticks (array): Tick value labels for the x and y axes.\n        dpi (int): resolution of the saved figure in dots per inch.\n        inset_size (float): Size of inset\n        title (str): Title of figure\n        legend_params (dict): Keyword arguments for the plot legend.\n        bootstrap_sets (list): A list of arrays of bootstrapped DistributedROC objects. If not None,\n            confidence regions will be plotted.\n        ci (tuple): tuple of bootstrap confidence interval percentiles\n    \"\"\"\n    if legend_params is None:\n        legend_params = dict(loc=4, fontsize=10, framealpha=1, frameon=True)\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.plot(ticks, ticks, \"k--\")\n    inset_hist = inset_axes(ax, width=inset_size, height=inset_size, loc=2)\n    if bootstrap_sets is not None:\n        for b, b_set in enumerate(bootstrap_sets):\n            brel_curves = np.dstack([b_rel.reliability_curve().values for b_rel in b_set])\n            bin_range = np.percentile(brel_curves[:,0], ci, axis=1)\n            rel_range = np.percentile(brel_curves[:, 3], ci, axis=1)\n            bin_poly = np.concatenate((bin_range[1], bin_range[0, ::-1]))\n            rel_poly = np.concatenate((rel_range[1], rel_range[0, ::-1]))\n            bin_poly[np.isnan(bin_poly)] = 0\n            rel_poly[np.isnan(rel_poly)] = 0\n            plt.fill(bin_poly, rel_poly, alpha=0.5, color=colors[b])\n    for r, rel_obj in enumerate(rel_objs):\n        rel_curve = rel_obj.reliability_curve()\n        ax.plot(rel_curve[\"Bin_Start\"], rel_curve[\"Positive_Relative_Freq\"], color=colors[r], marker=markers[r],\n                 label=obj_labels[r])\n        inset_hist.semilogy(rel_curve[\"Bin_Start\"], rel_curve[\"Total_Relative_Freq\"], color=colors[r],\n                            marker=markers[r])\n        inset_hist.set_xlabel(\"Forecast Probability\")\n        inset_hist.set_ylabel(\"Forecast Relative Frequency\")\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(ticks)\n    ax.set_yticks(ticks)\n    ax.legend(**legend_params)\n    ax.set_title(title)\n    plt.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the attributes diagram of the DistributedReliability objects.", "response": "def attributes_diagram(rel_objs, obj_labels, colors, markers, filename, figsize=(8, 8), xlabel=\"Forecast Probability\",\n                       ylabel=\"Observed Relative Frequency\", ticks=np.arange(0, 1.05, 0.05), dpi=300, \n                       title=\"Attributes Diagram\", legend_params=None, inset_params=None,\n                       inset_position=(0.12, 0.72, 0.25, 0.25), bootstrap_sets=None, ci=(2.5, 97.5)):\n    \"\"\"\n    Plot reliability curves against a 1:1 diagonal to determine if probability forecasts are consistent with their\n    observed relative frequency. Also adds gray areas to show where the climatological probabilities lie and what\n    areas result in a positive Brier Skill Score.\n\n    Args:\n        rel_objs (list): List of DistributedReliability objects.\n        obj_labels (list): List of labels describing the forecast model associated with each curve.\n        colors (list): List of colors for each line\n        markers (list): List of line markers\n        filename (str): Where to save the figure.\n        figsize (tuple): (Width, height) of the figure in inches.\n        xlabel (str): X-axis label\n        ylabel (str): Y-axis label\n        ticks (array): Tick value labels for the x and y axes.\n        dpi (int): resolution of the saved figure in dots per inch.\n        title (str): Title of figure\n        legend_params (dict): Keyword arguments for the plot legend.\n        inset_params (dict): Keyword arguments for the inset axis.\n        inset_position (tuple): Position of the inset axis in normalized axes coordinates (left, bottom, width, height)\n        bootstrap_sets (list): A list of arrays of bootstrapped DistributedROC objects. If not None,\n            confidence regions will be plotted.\n        ci (tuple): tuple of bootstrap confidence interval percentiles\n    \"\"\"\n    if legend_params is None:\n        legend_params = dict(loc=4, fontsize=10, framealpha=1, frameon=True)\n    if inset_params is None:\n        inset_params = dict(width=\"25%\", height=\"25%\", loc=2, axes_kwargs=dict(axisbg='white'))\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.plot(ticks, ticks, \"k--\")\n    inset_hist = inset_axes(ax, **inset_params)\n    ip = InsetPosition(ax, inset_position)\n    inset_hist.set_axes_locator(ip)\n    climo = rel_objs[0].climatology()\n    no_skill = 0.5 * ticks + 0.5 * climo\n    skill_x = [climo, climo, 1, 1, climo, climo, 0, 0, climo]\n    skill_y = [climo, 1, 1, no_skill[-1], climo, 0, 0, no_skill[0], climo]\n    f = ax.fill(skill_x, skill_y, \"0.8\")\n    f[0].set_zorder(1)\n    ax.plot(ticks, np.ones(ticks.shape) * climo, \"k--\")\n    if bootstrap_sets is not None:\n        for b, b_set in enumerate(bootstrap_sets):\n            brel_curves = np.vstack([b_rel.reliability_curve()[\"Positive_Relative_Freq\"].values for b_rel in b_set])\n            rel_range = np.nanpercentile(brel_curves, ci, axis=0)\n            fb = ax.fill_between(b_rel.thresholds[:-1], rel_range[1], rel_range[0], alpha=0.5, color=colors[b])\n            fb.set_zorder(2)\n    for r, rel_obj in enumerate(rel_objs):\n        rel_curve = rel_obj.reliability_curve()\n        ax.plot(rel_curve[\"Bin_Start\"], rel_curve[\"Positive_Relative_Freq\"], color=colors[r],\n                marker=markers[r], label=obj_labels[r])\n        inset_hist.semilogy(rel_curve[\"Bin_Start\"] * 100, rel_obj.frequencies[\"Total_Freq\"][:-1], color=colors[r],\n                            marker=markers[r])\n    inset_hist.set_xlabel(\"Forecast Probability\")\n    inset_hist.set_ylabel(\"Frequency\")\n    ax.annotate(\"No Skill\", (0.6, no_skill[12]), rotation=22.5)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(ticks)\n    ax.set_xticklabels((ticks * 100).astype(int))\n    ax.set_yticks(ticks)\n    ax.set_yticklabels((ticks * 100).astype(int))\n    ax.legend(**legend_params)\n    ax.set_title(title)\n    plt.savefig(filename, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_string(self, prompt, default_str=None) -> str:\n        accept_event = threading.Event()\n        value_ref = [None]\n\n        def perform():\n            def accepted(text):\n                value_ref[0] = text\n                accept_event.set()\n\n            def rejected():\n                accept_event.set()\n\n            self.__message_column.remove_all()\n            pose_get_string_message_box(self.ui, self.__message_column, prompt, str(default_str), accepted, rejected)\n            #self.__message_column.add(self.__make_cancel_row())\n\n        with self.__lock:\n            self.__q.append(perform)\n            self.document_controller.add_task(\"ui_\" + str(id(self)), self.__handle_output_and_q)\n        accept_event.wait()\n        def update_message_column():\n            self.__message_column.remove_all()\n            self.__message_column.add(self.__make_cancel_row())\n        self.document_controller.add_task(\"ui_\" + str(id(self)), update_message_column)\n        if value_ref[0] is None:\n            raise Exception(\"Cancel\")\n        return value_ref[0]", "response": "Return a string value that the user enters. Raises exception for cancel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __accept_reject(self, prompt, accepted_text, rejected_text, display_rejected):\n        accept_event = threading.Event()\n        result_ref = [False]\n\n        def perform():\n            def accepted():\n                result_ref[0] = True\n                accept_event.set()\n\n            def rejected():\n                result_ref[0] = False\n                accept_event.set()\n\n            self.__message_column.remove_all()\n            pose_confirmation_message_box(self.ui, self.__message_column, prompt, accepted, rejected, accepted_text, rejected_text, display_rejected)\n            #self.__message_column.add(self.__make_cancel_row())\n\n        with self.__lock:\n            self.__q.append(perform)\n            self.document_controller.add_task(\"ui_\" + str(id(self)), self.__handle_output_and_q)\n        accept_event.wait()\n        def update_message_column():\n            self.__message_column.remove_all()\n            self.__message_column.add(self.__make_cancel_row())\n        self.document_controller.add_task(\"ui_\" + str(id(self)), update_message_column)\n        return result_ref[0]", "response": "Return a boolean value for accept or reject."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_weight(self, r, ytr=None, mask=None):\n        ytr = self._ytr if ytr is None else ytr\n        mask = self._mask if mask is None else mask\n        return compute_weight(r, ytr, mask)", "response": "Compute the weight of a record using OLS of r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isfinite(self):\n        \"Test whether the predicted values are finite\"\n        if self._multiple_outputs:\n            if self.hy_test is not None:\n                r = [(hy.isfinite() and (hyt is None or hyt.isfinite()))\n                     for hy, hyt in zip(self.hy, self.hy_test)]\n            else:\n                r = [hy.isfinite() for hy in self.hy]\n            return np.all(r)\n        return self.hy.isfinite() and (self.hy_test is None or\n                                       self.hy_test.isfinite())", "response": "Test whether the predicted values are finite"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an arbitrary python code to get the value of a named argument in a function call.", "response": "def value_of_named_argument_in_function(argument_name, function_name, search_str,\n                                        resolve_varname=False):\n    \"\"\" Parse an arbitrary block of python code to get the value of a named argument\n        from inside a function call\n    \"\"\"\n    try:\n        search_str = unicode(search_str)\n    except NameError:\n        pass\n    readline = StringIO(search_str).readline\n    try:\n        token_generator = tokenize.generate_tokens(readline)\n        tokens = [SimplifiedToken(toknum, tokval) for toknum, tokval, _, _, _ in token_generator]\n    except tokenize.TokenError as e:\n        raise ValueError('search_str is not parse-able python code: ' + str(e))\n    in_function = False\n    is_var = False\n    for i in range(len(tokens)):\n        if (\n            not in_function and\n            tokens[i].typenum == tokenize.NAME and tokens[i].value == function_name and\n            tokens[i+1].typenum == tokenize.OP and tokens[i+1].value == '('\n        ):\n            in_function = True\n            continue\n        elif (\n            in_function and\n            tokens[i].typenum == tokenize.NAME and tokens[i].value == argument_name and\n            tokens[i+1].typenum == tokenize.OP and tokens[i+1].value == '='\n        ):\n            # value is set to another variable which we are going to attempt to resolve\n            if resolve_varname and tokens[i+2].typenum == 1:\n                is_var = True\n                argument_name = tokens[i+2].value\n                break\n\n            # again, for a very specific usecase -- get the whole value and concatenate it\n            # this will match something like _version.__version__\n            j = 3\n            while True:\n                if tokens[i+j].value in (',', ')') or tokens[i+j].typenum == 58:\n                    break\n                j += 1\n\n            return ''.join([t.value for t in tokens[i+2:i+j]]).strip()\n\n    # this is very dumb logic, and only works if the function argument is set to a variable\n    # which is set to a string value\n    if is_var:\n        for i in range(len(tokens)):\n            if (\n                tokens[i].typenum == tokenize.NAME and tokens[i].value == argument_name and\n                tokens[i+1].typenum == tokenize.OP and tokens[i+1].value == '='\n            ):\n                return tokens[i+2].value.strip()\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef regex_in_file(regex, filepath, return_match=False):\n    file_content = get_file_content(filepath)\n    re_method = funcy.re_find if return_match else funcy.re_test\n    return re_method(regex, file_content)", "response": "Search for a regex in a file\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a regex in a file contained within a package directory.", "response": "def regex_in_package_file(regex, filename, package_name, return_match=False):\n    \"\"\" Search for a regex in a file contained within the package directory\n\n    If return_match is True, return the found object instead of a boolean\n    \"\"\"\n    filepath = package_file_path(filename, package_name)\n    return regex_in_file(regex, filepath, return_match=return_match)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntest to see if a string is a URL or not", "response": "def string_is_url(test_str):\n    \"\"\" Test to see if a string is a URL or not, defined in this case as a string for which\n    urlparse returns a scheme component\n\n    >>> string_is_url('somestring')\n    False\n    >>> string_is_url('https://some.domain.org/path')\n    True\n    \"\"\"\n    parsed = urlparse.urlparse(test_str)\n    return parsed.scheme is not None and parsed.scheme != ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbegin a transaction state for the given item.", "response": "def item_transaction(self, item) -> Transaction:\n        \"\"\"Begin transaction state for item.\n\n        A transaction state is exists to prevent writing out to disk, mainly for performance reasons.\n        All changes to the object are delayed until the transaction state exits.\n\n        This method is thread safe.\n        \"\"\"\n        items = self.__build_transaction_items(item)\n        transaction = Transaction(self, item, items)\n        self.__transactions.append(transaction)\n        return transaction"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts a new data item into the document model.", "response": "def insert_data_item(self, before_index, data_item, auto_display: bool = True) -> None:\n        \"\"\"Insert a new data item into document model.\n\n        This method is NOT threadsafe.\n        \"\"\"\n        assert data_item is not None\n        assert data_item not in self.data_items\n        assert before_index <= len(self.data_items) and before_index >= 0\n        assert data_item.uuid not in self.__uuid_to_data_item\n        # update the session\n        data_item.session_id = self.session_id\n        # insert in internal list\n        self.__insert_data_item(before_index, data_item, do_write=True)\n        # automatically add a display\n        if auto_display:\n            display_item = DisplayItem.DisplayItem(data_item=data_item)\n            self.append_display_item(display_item)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_data_item(self, data_item: DataItem.DataItem, *, safe: bool=False) -> typing.Optional[typing.Sequence]:\n        # remove data item from any computations\n        return self.__cascade_delete(data_item, safe=safe)", "response": "Remove a data item from the document model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __cascade_delete_inner(self, master_item, safe: bool=False) -> typing.Optional[typing.Sequence]:\n        # print(f\"cascade {master_item}\")\n        # this horrible little hack ensures that computation changed messages are delayed until the end of the cascade\n        # delete; otherwise there are cases where dependencies can be reestablished during the changed messages while\n        # this method is partially finished. ugh. see test_computation_deletes_when_source_cycle_deletes.\n        if self.__computation_changed_delay_list is None:\n            computation_changed_delay_list = list()\n            self.__computation_changed_delay_list = computation_changed_delay_list\n        else:\n            computation_changed_delay_list = None\n        undelete_log = list()\n        try:\n            items = list()\n            dependencies = list()\n            self.__build_cascade(master_item, items, dependencies)\n            cascaded = True\n            while cascaded:\n                cascaded = False\n                # adjust computation bookkeeping to remove deleted items, then delete unused computations\n                items_set = set(items)\n                for computation in copy.copy(self.computations):\n                    output_deleted = master_item in computation._outputs\n                    computation._inputs -= items_set\n                    computation._outputs -= items_set\n                    if computation not in items and computation != self.__current_computation:\n                        # computations are auto deleted if all inputs are deleted or any output is deleted\n                        if output_deleted or all(input in items for input in computation._inputs):\n                            self.__build_cascade(computation, items, dependencies)\n                            cascaded = True\n            # print(list(reversed(items)))\n            # print(list(reversed(dependencies)))\n            for source, target in reversed(dependencies):\n                self.__remove_dependency(source, target)\n            # now delete the actual items\n            for item in reversed(items):\n                for computation in self.computations:\n                    new_entries = computation.list_item_removed(item)\n                    undelete_log.extend(new_entries)\n                container = item.container\n                if isinstance(item, DataItem.DataItem):\n                    name = \"data_items\"\n                elif isinstance(item, DisplayItem.DisplayItem):\n                    name = \"display_items\"\n                elif isinstance(item, Graphics.Graphic):\n                    name = \"graphics\"\n                elif isinstance(item, DataStructure.DataStructure):\n                    name = \"data_structures\"\n                elif isinstance(item, Symbolic.Computation):\n                    name = \"computations\"\n                elif isinstance(item, Connection.Connection):\n                    name = \"connections\"\n                elif isinstance(item, DisplayItem.DisplayDataChannel):\n                    name = \"display_data_channels\"\n                else:\n                    name = None\n                    assert False, \"Unable to cascade delete type \" + str(type(item))\n                assert name\n                # print(container, name, item)\n                if container is self and name == \"data_items\":\n                    # call the version of __remove_data_item that doesn't cascade again\n                    index = getattr(container, name).index(item)\n                    item_dict = item.write_to_dict()\n                    # NOTE: __remove_data_item will notify_remove_item\n                    undelete_log.extend(self.__remove_data_item(item, safe=safe))\n                    undelete_log.append({\"type\": name, \"index\": index, \"properties\": item_dict})\n                elif container is self and name == \"display_items\":\n                    # call the version of __remove_data_item that doesn't cascade again\n                    index = getattr(container, name).index(item)\n                    item_dict = item.write_to_dict()\n                    # NOTE: __remove_display_item will notify_remove_item\n                    undelete_log.extend(self.__remove_display_item(item, safe=safe))\n                    undelete_log.append({\"type\": name, \"index\": index, \"properties\": item_dict})\n                elif container:\n                    container_ref = str(container.uuid)\n                    index = getattr(container, name).index(item)\n                    item_dict = item.write_to_dict()\n                    container_properties = container.save_properties() if hasattr(container, \"save_properties\") else dict()\n                    undelete_log.append({\"type\": name, \"container\": container_ref, \"index\": index, \"properties\": item_dict, \"container_properties\": container_properties})\n                    container.remove_item(name, item)\n                    # handle top level 'remove item' notifications for data structures, computations, and display items here\n                    # since they're not handled elsewhere.\n                    if container == self and name in (\"data_structures\", \"computations\"):\n                        self.notify_remove_item(name, item, index)\n        except Exception as e:\n            import sys, traceback\n            traceback.print_exc()\n            traceback.format_exception(*sys.exc_info())\n        finally:\n            # check whether this call of __cascade_delete is the top level one that will finish the computation\n            # changed messages.\n            if computation_changed_delay_list is not None:\n                self.__finish_computation_changed()\n        return undelete_log", "response": "Cascade delete an item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of data items that directly depends on data in this item.", "response": "def get_dependent_items(self, item) -> typing.List:\n        \"\"\"Return the list of data items containing data that directly depends on data in this item.\"\"\"\n        with self.__dependency_tree_lock:\n            return copy.copy(self.__dependency_tree_source_to_target_map.get(weakref.ref(item), list()))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_deep_dependent_item_set(self, item, item_set) -> None:\n        if not item in item_set:\n            item_set.add(item)\n            with self.__dependency_tree_lock:\n                for dependent in self.get_dependent_items(item):\n                    self.__get_deep_dependent_item_set(dependent, item_set)", "response": "Recursively returns the list of data items containing data that directly depends on this item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of data items containing data that directly depends on data in this item.", "response": "def get_dependent_data_items(self, data_item: DataItem.DataItem) -> typing.List[DataItem.DataItem]:\n        \"\"\"Return the list of data items containing data that directly depends on data in this item.\"\"\"\n        with self.__dependency_tree_lock:\n            return [data_item for data_item in self.__dependency_tree_source_to_target_map.get(weakref.ref(data_item), list()) if isinstance(data_item, DataItem.DataItem)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transaction_context(self):\n        class DocumentModelTransaction:\n            def __init__(self, document_model):\n                self.__document_model = document_model\n\n            def __enter__(self):\n                self.__document_model.persistent_object_context.enter_write_delay(self.__document_model)\n                return self\n\n            def __exit__(self, type, value, traceback):\n                self.__document_model.persistent_object_context.exit_write_delay(self.__document_model)\n                self.__document_model.persistent_object_context.rewrite_item(self.__document_model)\n\n        return DocumentModelTransaction(self)", "response": "Return a context object for a document - wide transaction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a context manager to put the data item in a live state.", "response": "def data_item_live(self, data_item):\n        \"\"\" Return a context manager to put the data item in a 'live state'. \"\"\"\n        class LiveContextManager:\n            def __init__(self, manager, object):\n                self.__manager = manager\n                self.__object = object\n            def __enter__(self):\n                self.__manager.begin_data_item_live(self.__object)\n                return self\n            def __exit__(self, type, value, traceback):\n                self.__manager.end_data_item_live(self.__object)\n        return LiveContextManager(self, data_item)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef begin_data_item_live(self, data_item):\n        with self.__live_data_items_lock:\n            old_live_count = self.__live_data_items.get(data_item.uuid, 0)\n            self.__live_data_items[data_item.uuid] = old_live_count + 1\n        if old_live_count == 0:\n            data_item._enter_live_state()\n            for dependent_data_item in self.get_dependent_data_items(data_item):\n                self.begin_data_item_live(dependent_data_item)", "response": "Begins a live state for the data item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef end_data_item_live(self, data_item):\n        with self.__live_data_items_lock:\n            live_count = self.__live_data_items.get(data_item.uuid, 0) - 1\n            assert live_count >= 0\n            self.__live_data_items[data_item.uuid] = live_count\n        if live_count == 0:\n            data_item._exit_live_state()\n            for dependent_data_item in self.get_dependent_data_items(data_item):\n                self.end_data_item_live(dependent_data_item)", "response": "Ends a live state for the data item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __construct_data_item_reference(self, hardware_source: HardwareSource.HardwareSource, data_channel: HardwareSource.DataChannel):\n        session_id = self.session_id\n        key = self.make_data_item_reference_key(hardware_source.hardware_source_id, data_channel.channel_id)\n        data_item_reference = self.get_data_item_reference(key)\n        with data_item_reference.mutex:\n            data_item = data_item_reference.data_item\n            # if we still don't have a data item, create it.\n            if data_item is None:\n                data_item = DataItem.DataItem()\n                data_item.ensure_data_source()\n                data_item.title = \"%s (%s)\" % (hardware_source.display_name, data_channel.name) if data_channel.name else hardware_source.display_name\n                data_item.category = \"temporary\"\n                data_item_reference.data_item = data_item\n\n                def append_data_item():\n                    self.append_data_item(data_item)\n                    self._update_data_item_reference(key, data_item)\n\n                self.__call_soon(append_data_item)\n\n            def update_session():\n                # update the session, but only if necessary (this is an optimization to prevent unnecessary display updates)\n                if data_item.session_id != session_id:\n                    data_item.session_id = session_id\n                session_metadata = ApplicationData.get_session_metadata_dict()\n                if data_item.session_metadata != session_metadata:\n                    data_item.session_metadata = session_metadata\n                if data_channel.processor:\n                    src_data_channel = hardware_source.data_channels[data_channel.src_channel_index]\n                    src_data_item_reference = self.get_data_item_reference(self.make_data_item_reference_key(hardware_source.hardware_source_id, src_data_channel.channel_id))\n                    data_channel.processor.connect_data_item_reference(src_data_item_reference)\n\n            self.__call_soon(update_session)\n\n            return data_item_reference", "response": "Construct a data item reference and assign a data item to it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new data item with computation specified by processing_id inputs and region_list_map.", "response": "def __make_computation(self, processing_id: str, inputs: typing.List[typing.Tuple[DisplayItem.DisplayItem, typing.Optional[Graphics.Graphic]]], region_list_map: typing.Mapping[str, typing.List[Graphics.Graphic]]=None, parameters: typing.Mapping[str, typing.Any]=None) -> DataItem.DataItem:\n        \"\"\"Create a new data item with computation specified by processing_id, inputs, and region_list_map.\n\n        The region_list_map associates a list of graphics corresponding to the required regions with a computation source (key).\n        \"\"\"\n        region_list_map = region_list_map or dict()\n\n        parameters = parameters or dict()\n\n        processing_descriptions = self._processing_descriptions\n        processing_description = processing_descriptions[processing_id]\n\n        # first process the sources in the description. match them to the inputs (which are data item/crop graphic tuples)\n        src_dicts = processing_description.get(\"sources\", list())\n        assert len(inputs) == len(src_dicts)\n        src_names = list()\n        src_texts = list()\n        src_labels = list()\n        regions = list()\n        region_map = dict()\n        for i, (src_dict, input) in enumerate(zip(src_dicts, inputs)):\n\n            display_item = input[0]\n            data_item = display_item.data_items[0] if display_item and len(display_item.data_items) > 0 else None\n\n            if not data_item:\n                return None\n\n            # each source can have a list of requirements, check through them\n            requirements = src_dict.get(\"requirements\", list())\n            for requirement in requirements:\n                requirement_type = requirement[\"type\"]\n                if requirement_type == \"dimensionality\":\n                    min_dimension = requirement.get(\"min\")\n                    max_dimension = requirement.get(\"max\")\n                    dimensionality = len(data_item.dimensional_shape)\n                    if min_dimension is not None and dimensionality < min_dimension:\n                        return None\n                    if max_dimension is not None and dimensionality > max_dimension:\n                        return None\n                if requirement_type == \"is_sequence\":\n                    if not data_item.is_sequence:\n                        return None\n\n            src_name = src_dict[\"name\"]\n            src_label = src_dict[\"label\"]\n            use_display_data = src_dict.get(\"use_display_data\", True)\n            xdata_property = \"display_xdata\" if use_display_data else \"xdata\"\n            if src_dict.get(\"croppable\"):\n                xdata_property = \"cropped_\" + xdata_property\n            elif src_dict.get(\"use_filtered_data\", False):\n                xdata_property = \"filtered_\" + xdata_property\n            src_text = \"{}.{}\".format(src_name, xdata_property)\n            src_names.append(src_name)\n            src_texts.append(src_text)\n            src_labels.append(src_label)\n\n            # each source can have a list of regions to be matched to arguments or created on the source\n            region_dict_list = src_dict.get(\"regions\", list())\n            src_region_list = region_list_map.get(src_name, list())\n            assert len(region_dict_list) == len(src_region_list)\n            for region_dict, region in zip(region_dict_list, src_region_list):\n                region_params = region_dict.get(\"params\", dict())\n                region_type = region_dict[\"type\"]\n                region_name = region_dict[\"name\"]\n                region_label = region_params.get(\"label\")\n                if region_type == \"point\":\n                    if region:\n                        assert isinstance(region, Graphics.PointGraphic)\n                        point_region = region\n                    else:\n                        point_region = Graphics.PointGraphic()\n                        for k, v in region_params.items():\n                            setattr(point_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(point_region)\n                    regions.append((region_name, point_region, region_label))\n                    region_map[region_name] = point_region\n                elif region_type == \"line\":\n                    if region:\n                        assert isinstance(region, Graphics.LineProfileGraphic)\n                        line_region = region\n                    else:\n                        line_region = Graphics.LineProfileGraphic()\n                        line_region.start = 0.25, 0.25\n                        line_region.end = 0.75, 0.75\n                        for k, v in region_params.items():\n                            setattr(line_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(line_region)\n                    regions.append((region_name, line_region, region_params.get(\"label\")))\n                    region_map[region_name] = line_region\n                elif region_type == \"rectangle\":\n                    if region:\n                        assert isinstance(region, Graphics.RectangleGraphic)\n                        rect_region = region\n                    else:\n                        rect_region = Graphics.RectangleGraphic()\n                        rect_region.center = 0.5, 0.5\n                        rect_region.size = 0.5, 0.5\n                        for k, v in region_params.items():\n                            setattr(rect_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(rect_region)\n                    regions.append((region_name, rect_region, region_params.get(\"label\")))\n                    region_map[region_name] = rect_region\n                elif region_type == \"ellipse\":\n                    if region:\n                        assert isinstance(region, Graphics.EllipseGraphic)\n                        ellipse_region = region\n                    else:\n                        ellipse_region = Graphics.RectangleGraphic()\n                        ellipse_region.center = 0.5, 0.5\n                        ellipse_region.size = 0.5, 0.5\n                        for k, v in region_params.items():\n                            setattr(ellipse_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(ellipse_region)\n                    regions.append((region_name, ellipse_region, region_params.get(\"label\")))\n                    region_map[region_name] = ellipse_region\n                elif region_type == \"spot\":\n                    if region:\n                        assert isinstance(region, Graphics.SpotGraphic)\n                        spot_region = region\n                    else:\n                        spot_region = Graphics.SpotGraphic()\n                        spot_region.center = 0.25, 0.75\n                        spot_region.size = 0.1, 0.1\n                        for k, v in region_params.items():\n                            setattr(spot_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(spot_region)\n                    regions.append((region_name, spot_region, region_params.get(\"label\")))\n                    region_map[region_name] = spot_region\n                elif region_type == \"interval\":\n                    if region:\n                        assert isinstance(region, Graphics.IntervalGraphic)\n                        interval_region = region\n                    else:\n                        interval_region = Graphics.IntervalGraphic()\n                        for k, v in region_params.items():\n                            setattr(interval_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(interval_region)\n                    regions.append((region_name, interval_region, region_params.get(\"label\")))\n                    region_map[region_name] = interval_region\n                elif region_type == \"channel\":\n                    if region:\n                        assert isinstance(region, Graphics.ChannelGraphic)\n                        channel_region = region\n                    else:\n                        channel_region = Graphics.ChannelGraphic()\n                        for k, v in region_params.items():\n                            setattr(channel_region, k, v)\n                        if display_item:\n                            display_item.add_graphic(channel_region)\n                    regions.append((region_name, channel_region, region_params.get(\"label\")))\n                    region_map[region_name] = channel_region\n\n        # now extract the script (full script) or expression (implied imports and return statement)\n        script = processing_description.get(\"script\")\n        if not script:\n            expression = processing_description.get(\"expression\")\n            if expression:\n                script = Symbolic.xdata_expression(expression)\n        assert script\n\n        # construct the computation\n        script = script.format(**dict(zip(src_names, src_texts)))\n        computation = self.create_computation(script)\n        computation.label = processing_description[\"title\"]\n        computation.processing_id = processing_id\n        # process the data item inputs\n        for src_dict, src_name, src_label, input in zip(src_dicts, src_names, src_labels, inputs):\n            in_display_item = input[0]\n            secondary_specifier = None\n            if src_dict.get(\"croppable\", False):\n                secondary_specifier = self.get_object_specifier(input[1])\n            display_data_channel = in_display_item.display_data_channel\n            computation.create_object(src_name, self.get_object_specifier(display_data_channel), label=src_label, secondary_specifier=secondary_specifier)\n        # process the regions\n        for region_name, region, region_label in regions:\n            computation.create_object(region_name, self.get_object_specifier(region), label=region_label)\n        # next process the parameters\n        for param_dict in processing_description.get(\"parameters\", list()):\n            parameter_value = parameters.get(param_dict[\"name\"], param_dict[\"value\"])\n            computation.create_variable(param_dict[\"name\"], param_dict[\"type\"], parameter_value, value_default=param_dict.get(\"value_default\"),\n                                        value_min=param_dict.get(\"value_min\"), value_max=param_dict.get(\"value_max\"),\n                                        control_type=param_dict.get(\"control_type\"), label=param_dict[\"label\"])\n\n        data_item0 = inputs[0][0].data_items[0]\n        new_data_item = DataItem.new_data_item()\n        prefix = \"{} of \".format(processing_description[\"title\"])\n        new_data_item.title = prefix + data_item0.title\n        new_data_item.category = data_item0.category\n\n        self.append_data_item(new_data_item)\n\n        new_display_item = self.get_display_item_for_data_item(new_data_item)\n\n        # next come the output regions that get created on the target itself\n        new_regions = dict()\n        for out_region_dict in processing_description.get(\"out_regions\", list()):\n            region_type = out_region_dict[\"type\"]\n            region_name = out_region_dict[\"name\"]\n            region_params = out_region_dict.get(\"params\", dict())\n            if region_type == \"interval\":\n                interval_region = Graphics.IntervalGraphic()\n                for k, v in region_params.items():\n                    setattr(interval_region, k, v)\n                new_display_item.add_graphic(interval_region)\n                new_regions[region_name] = interval_region\n\n        # now come the connections between the source and target\n        for connection_dict in processing_description.get(\"connections\", list()):\n            connection_type = connection_dict[\"type\"]\n            connection_src = connection_dict[\"src\"]\n            connection_src_prop = connection_dict.get(\"src_prop\")\n            connection_dst = connection_dict[\"dst\"]\n            connection_dst_prop = connection_dict.get(\"dst_prop\")\n            if connection_type == \"property\":\n                if connection_src == \"display_data_channel\":\n                    # TODO: how to refer to the data_items? hardcode to data_item0 for now.\n                    display_item0 = self.get_display_item_for_data_item(data_item0)\n                    display_data_channel0 = display_item0.display_data_channel if display_item0 else None\n                    connection = Connection.PropertyConnection(display_data_channel0, connection_src_prop, new_regions[connection_dst], connection_dst_prop, parent=new_data_item)\n                    self.append_connection(connection)\n            elif connection_type == \"interval_list\":\n                connection = Connection.IntervalListConnection(new_display_item, region_map[connection_dst], parent=new_data_item)\n                self.append_connection(connection)\n\n        # save setting the computation until last to work around threaded clone/merge operation bug.\n        # the bug is that setting the computation triggers the recompute to occur on a thread.\n        # the recompute clones the data item and runs the operation. meanwhile this thread\n        # updates the connection. now the recompute finishes and merges back the data item\n        # which was cloned before the connection was established, effectively reversing the\n        # update that matched the graphic interval to the slice interval on the display.\n        # the result is that the slice interval on the display would get set to the default\n        # value of the graphic interval. so don't actually update the computation until after\n        # everything is configured. permanent solution would be to improve the clone/merge to\n        # only update data that had been changed. alternative implementation would only track\n        # changes to the data item and then apply them again to the original during merge.\n        self.set_data_item_computation(new_data_item, computation)\n\n        return new_data_item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interpolate_colors(array: numpy.ndarray, x: int) -> numpy.ndarray:\n    out_array = []\n    for i in range(x):\n        if i % (x / (len(array) - 1)) == 0:\n            index = i / (x / (len(array) - 1))\n            out_array.append(array[int(index)])\n        else:\n            start_marker = array[math.floor(i / (x / (len(array) - 1)))]\n            stop_marker = array[math.ceil(i / (x / (len(array) - 1)))]\n            interp_amount = i % (x / (len(array) - 1)) / (x / (len(array) - 1))\n            interp_color = numpy.rint(start_marker + ((stop_marker - start_marker) * interp_amount))\n            out_array.append(interp_color)\n    out_array[-1] = array[-1]\n    return numpy.array(out_array).astype(numpy.uint8)", "response": "Interpolates the colors in a color map."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstars any gist by providing gistID or gistname or any unique Gistid", "response": "def star(self, **args):\n\t\t'''\n\t\tstar any gist by providing gistID or gistname(for authenticated user)\n\t\t'''\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Either provide authenticated user\\'s Unambigious Gistname or any unique Gistid to be starred')\n\n\t\tr = requests.put(\n\t\t\t'%s'%BASE_URL+'/gists/%s/star' % self.gist_id,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\t\tif (r.status_code == 204):\n\t\t\tresponse = {\n\t\t\t\t'id': self.gist_id\n\t\t\t}\n\t\t\treturn response\n\n\t\traise Exception('Gist can\\'t be starred')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fork(self, **args):\n\t\t'''\n\t\tfork any gist by providing gistID or gistname(for authenticated user)\n\t\t'''\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Either provide authenticated user\\'s Unambigious Gistname or any unique Gistid to be forked')\n\n\t\tr = requests.post(\n\t\t\t'%s'%BASE_URL+'/gists/%s/forks' % self.gist_id,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\t\tif (r.status_code == 201):\n\t\t\tresponse = {\n\t\t\t\t'id': self.gist_id,\n\t\t\t\t'description': r.json()['description'],\n\t\t\t\t'public': r.json()['public'],\n\t\t\t\t'comments': r.json()['comments']\n\t\t\t}\n\t\t\treturn response\n\n\t\traise Exception('Gist can\\'t be forked')", "response": "fork any gist by providing gistID or gistname(for authenticated user)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkifstar(self, **args):\n\t\t'''\n\t\tCheck a gist if starred by providing gistID or gistname(for authenticated user)\n\t\t'''\n\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Either provide authenticated user\\'s Unambigious Gistname or any unique Gistid to be checked for star')\n\n\t\tr = requests.get(\n\t\t\t'%s'%BASE_URL+'/gists/%s/star' % self.gist_id,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\t\tif (r.status_code == 204):\n\t\t\tresponse = {\n\t\t\t\t'starred': 'True',\n\t\t\t\t'id': self.gist_id\n\t\t\t\t}\n\t\telse:\n\t\t\tresponse = {\n\t\t\t\t'starred': 'False'\n\t\t\t}\n\n\t\treturn response", "response": "This method checks if a gist is starred by providing gistID or gistname or any unique Gistid to be checked for starred"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef salvar(self, destino=None, prefix='tmp', suffix='-sat.log'):\n        if destino:\n            if os.path.exists(destino):\n                raise IOError((errno.EEXIST, 'File exists', destino,))\n            destino = os.path.abspath(destino)\n            fd = os.open(destino, os.O_EXCL|os.O_CREAT|os.O_WRONLY)\n        else:\n            fd, destino = tempfile.mkstemp(prefix=prefix, suffix=suffix)\n\n        os.write(fd, self.conteudo())\n        os.fsync(fd)\n        os.close(fd)\n\n        return os.path.abspath(destino)", "response": "Salva o arquivo de log decodificado."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef analisar(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='ExtrairLogs',\n                classe_resposta=RespostaExtrairLogs,\n                campos=RespostaSAT.CAMPOS + (\n                        ('arquivoLog', unicode),\n                    ),\n                campos_alternativos=[\n                        # se a extra\u00e7\u00e3o dos logs falhar espera-se o padr\u00e3o de\n                        # campos no retorno...\n                        RespostaSAT.CAMPOS,\n                    ]\n            )\n        if resposta.EEEEE not in ('15000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Analisar uma : class : RespostaExtrairLogs a partir do retorno."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_data_old(self):\n        units = \"\"\n        if len(self.file_objects) == 1 and self.file_objects[0] is not None:\n            data = self.file_objects[0].variables[self.variable][self.forecast_hours]\n            if hasattr(self.file_objects[0].variables[self.variable], \"units\"):\n                units = self.file_objects[0].variables[self.variable].units\n        elif len(self.file_objects) > 1:\n            grid_shape = [len(self.file_objects), 1, 1]\n            for file_object in self.file_objects:\n                if file_object is not None:\n                    if self.variable in file_object.variables.keys():\n                        grid_shape = file_object.variables[self.variable].shape\n                    elif self.variable.ljust(6, \"_\") in file_object.variables.keys():\n                        grid_shape = file_object.variables[self.variable.ljust(6, \"_\")].shape\n\n                    else:\n                        print(\"{0} not found\".format(self.variable))\n                        raise KeyError\n                    break\n            data = np.zeros((len(self.file_objects), grid_shape[1], grid_shape[2]))\n            for f, file_object in enumerate(self.file_objects):\n                if file_object is not None:\n                    if self.variable in file_object.variables.keys():\n                        var_name = self.variable\n                    elif self.variable.ljust(6, \"_\") in file_object.variables.keys():\n                        var_name = self.variable.ljust(6, \"_\")\n                    else:\n                        print(\"{0} not found\".format(self.variable))\n                        raise KeyError\n                    data[f] = file_object.variables[var_name][0]\n                    if units == \"\" and hasattr(file_object.variables[var_name], \"units\"):\n                        units = file_object.variables[var_name].units\n        else:\n            data = None\n        return data, units", "response": "Loads time series of 2D data grids from each opened file. The code \n        handles loading a full time series from one file or individual time steps\n        from multiple files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload data from netCDF file objects or list of netCDF file objects. Handles special variable name formats.", "response": "def load_data(self):\n        \"\"\"\n        Load data from netCDF file objects or list of netCDF file objects. Handles special variable name formats.\n\n        Returns:\n            Array of data loaded from files in (time, y, x) dimensions, Units\n        \"\"\"\n        units = \"\"\n        if self.file_objects[0] is None:\n            raise IOError()\n        var_name, z_index = self.format_var_name(self.variable, list(self.file_objects[0].variables.keys()))\n        ntimes = 0\n        if 'time' in self.file_objects[0].variables[var_name].dimensions:\n            ntimes = len(self.file_objects[0].dimensions['time'])\n\n        if ntimes > 1:\n            if z_index is None:\n                data = self.file_objects[0].variables[var_name][self.forecast_hours].astype(np.float32)\n            else:\n                data = self.file_objects[0].variables[var_name][self.forecast_hours, z_index].astype(np.float32)\n        else:\n            y_dim, x_dim = self.file_objects[0].variables[var_name].shape[-2:]\n            data = np.zeros((len(self.valid_dates), y_dim, x_dim), dtype=np.float32)\n            for f, file_object in enumerate(self.file_objects):\n                if file_object is not None:\n                    if z_index is None:\n                        data[f] = file_object.variables[var_name][0]\n                    else:\n                        data[f] = file_object.variables[var_name][0, z_index]\n        if hasattr(self.file_objects[0].variables[var_name], \"units\"):\n            units = self.file_objects[0].variables[var_name].units\n        return data, units"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_var_name(variable, var_list):\n        z_index = None\n        if variable in var_list:\n            var_name = variable\n        elif variable.ljust(6, \"_\") in var_list:\n            var_name = variable.ljust(6, \"_\")\n        elif any([variable in v_sub.split(\"_\") for v_sub in var_list]):\n            var_name = var_list[[variable in v_sub.split(\"_\") for v_sub in var_list].index(True)]\n            z_index = var_name.split(\"_\").index(variable)\n        else:\n            raise KeyError(\"{0} not found in {1}\".format(variable, var_list))\n        return var_name, z_index", "response": "Returns the variable name and z - level of the variable in file containing relevant data and index of variable z - level if multiple variables in same array in same file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads data from flat data files containing total track information and information about each timestep.", "response": "def load_data(self, mode=\"train\", format=\"csv\"):\n        \"\"\"\n        Load data from flat data files containing total track information and information about each timestep.\n        The two sets are combined using merge operations on the Track IDs. Additional member information is gathered\n        from the appropriate member file.\n        Args:\n            mode: \"train\" or \"forecast\"\n            format:  file format being used. Default is \"csv\"\n        \"\"\"\n        if mode in self.data.keys():\n            run_dates = pd.DatetimeIndex(start=self.start_dates[mode],\n                                        end=self.end_dates[mode],freq=\"1D\")\n            run_date_str = [d.strftime(\"%Y%m%d-%H%M\") for d in run_dates.date]\n            print(run_date_str)\n            all_total_track_files = sorted(glob(getattr(self, mode + \"_data_path\") +\n                                                \"*total_\" + self.ensemble_name + \"*.\" + format))\n            all_step_track_files = sorted(glob(getattr(self, mode + \"_data_path\") +\n                                               \"*step_\" + self.ensemble_name + \"*.\" + format))\n            total_track_files = []\n            for track_file in all_total_track_files:\n                file_date = track_file.split(\"_\")[-1][:-4]\n                if file_date in run_date_str:\n                    total_track_files.append(track_file)\n            step_track_files = []\n            for step_file in all_step_track_files:\n                file_date = step_file.split(\"_\")[-1][:-4]\n                if file_date in run_date_str:\n                    step_track_files.append(step_file)            \n\t  \t   \n            self.data[mode][\"total\"] = pd.concat(map(pd.read_csv, total_track_files),\n                                                 ignore_index=True)\n            self.data[mode][\"total\"] = self.data[mode][\"total\"].fillna(value=0)\n            self.data[mode][\"total\"] = self.data[mode][\"total\"].replace([np.inf, -np.inf], 0)\n            self.data[mode][\"step\"] = pd.concat(map(pd.read_csv, step_track_files),\n                                                ignore_index=True)\n            self.data[mode][\"step\"] = self.data[mode][\"step\"].fillna(value=0)\n            self.data[mode][\"step\"] = self.data[mode][\"step\"].replace([np.inf, -np.inf], 0)\n            if mode == \"forecast\":\n                self.data[mode][\"step\"] = self.data[mode][\"step\"].drop_duplicates(\"Step_ID\")\n            self.data[mode][\"member\"] = pd.read_csv(self.member_files[mode])\n            self.data[mode][\"combo\"] = pd.merge(self.data[mode][\"step\"],\n                                                self.data[mode][\"total\"],\n                                                on=[\"Track_ID\", \"Ensemble_Name\", \"Ensemble_Member\", \"Run_Date\"])\n            self.data[mode][\"combo\"] = pd.merge(self.data[mode][\"combo\"],\n                                                self.data[mode][\"member\"],\n                                                on=\"Ensemble_Member\") \n            self.data[mode][\"total_group\"] = pd.merge(self.data[mode][\"total\"],\n                                                      self.data[mode][\"member\"],\n                                                      on=\"Ensemble_Member\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate a multivariate normal distribution from the training data for each group of ensemble members and saves it to a pickle file.", "response": "def calc_copulas(self,\n                     output_file,\n                     model_names=(\"start-time\", \"translation-x\", \"translation-y\"),\n                     label_columns=(\"Start_Time_Error\", \"Translation_Error_X\", \"Translation_Error_Y\")):\n        \"\"\"\n        Calculate a copula multivariate normal distribution from the training data for each group of ensemble members.\n        Distributions are written to a pickle file for later use.\n        Args:\n            output_file: Pickle file\n            model_names: Names of the tracking models\n            label_columns: Names of the data columns used for labeling\n        Returns:\n        \"\"\"\n        if len(self.data['train']) == 0:\n            self.load_data()\n        groups = self.data[\"train\"][\"member\"][self.group_col].unique()\n        copulas = {}\n        label_columns = list(label_columns)\n        for group in groups:\n            print(group)\n            group_data = self.data[\"train\"][\"total_group\"].loc[\n                self.data[\"train\"][\"total_group\"][self.group_col] == group]\n            group_data = group_data.dropna()\n            group_data.reset_index(drop=True, inplace=True)\n            copulas[group] = {}\n            copulas[group][\"mean\"] = group_data[label_columns].mean(axis=0).values\n            copulas[group][\"cov\"] = np.cov(group_data[label_columns].values.T)\n            copulas[group][\"model_names\"] = list(model_names)\n            del group_data\n        pickle.dump(copulas, open(output_file, \"w\"), pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit the condition models to predict whether or not hail will occur.", "response": "def fit_condition_models(self, model_names,\n                             model_objs,\n                             input_columns,\n                             output_column=\"Matched\",\n                             output_threshold=0.0):\n        \"\"\"\n        Fit machine learning models to predict whether or not hail will occur.\n        Args:\n            model_names: List of strings with the names for the particular machine learning models\n            model_objs: scikit-learn style machine learning model objects.\n            input_columns: list of the names of the columns used as input for the machine learning model\n            output_column: name of the column used for labeling whether or not the event occurs\n            output_threshold: splitting threshold to determine if event has occurred. Default 0.0\n        \"\"\"\n        print(\"Fitting condition models\")\n        groups = self.data[\"train\"][\"member\"][self.group_col].unique()\n        \n        weights = None\n\n        for group in groups:\n            print(group)\n            group_data = self.data[\"train\"][\"combo\"].loc[self.data[\"train\"][\"combo\"][self.group_col] == group] \n            if self.sector:\n        \n                lon_obj = data.loc[:,'Centroid_Lon']\n                lat_obj = data.loc[:,'Centroid_Lat']\n\n                left_lon,right_lon = self.grid_dict[\"sw_lon\"],self.grid_dict[\"ne_lon\"]\n                lower_lat,upper_lat = self.grid_dict[\"sw_lat\"],self.grid_dict[\"ne_lat\"]\n\n                weights = np.where((left_lon<=lon_obj)&(right_lon>=lon_obj) &\\\n                    (lower_lat<=lat_obj)&(upper_lat>=lat_obj),1,0.3)\n            \n            output_data = np.where(group_data[output_column] > output_threshold, 1, 0)\n            print(\"Ones: \", np.count_nonzero(output_data > 0), \"Zeros: \", np.count_nonzero(output_data == 0))\n            self.condition_models[group] = {}\n            for m, model_name in enumerate(model_names):\n                print(model_name)\n                self.condition_models[group][model_name] = deepcopy(model_objs[m])\n                try:\n                    self.condition_models[group][model_name].fit(group_data[input_columns], \n                                output_data,sample_weight=weights)\n                except:\n                    self.condition_models[group][model_name].fit(group_data[input_columns], output_data)\n\n                if hasattr(self.condition_models[group][model_name], \"best_estimator_\"):\n                    print(self.condition_models[group][model_name].best_estimator_)\n                    print(self.condition_models[group][model_name].best_score_)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit models to predict hail or no - hail and use cross - validation to determine the probability of a skill score.", "response": "def fit_condition_threshold_models(self, model_names, model_objs, input_columns, output_column=\"Matched\",\n                                       output_threshold=0.5, num_folds=5, threshold_score=\"ets\"):\n        \"\"\"\n        Fit models to predict hail/no-hail and use cross-validation to determine the probaility threshold that\n        maximizes a skill score.\n\n        Args:\n            model_names: List of machine learning model names\n            model_objs: List of Scikit-learn ML models\n            input_columns: List of input variables in the training data\n            output_column: Column used for prediction\n            output_threshold: Values exceeding this threshold are considered positive events; below are nulls\n            num_folds: Number of folds in the cross-validation procedure\n            threshold_score: Score available in ContingencyTable used for determining the best probability threshold\n        Returns:\n            None\n        \"\"\"\n        print(\"Fitting condition models\")\n        groups = self.data[\"train\"][\"member\"][self.group_col].unique()\n        \n        weights=None\n\n        for group in groups:\n            print(group)\n            group_data = self.data[\"train\"][\"combo\"].iloc[\n                np.where(self.data[\"train\"][\"combo\"][self.group_col] == group)[0]]\n            \n            if self.sector:\n                lon_obj = group_data.loc[:,'Centroid_Lon']\n                lat_obj = group_data.loc[:,'Centroid_Lat']\n                \n                conus_lat_lon_points = zip(lon_obj.values.ravel(),lat_obj.values.ravel())\n                center_lon, center_lat = self.proj_dict[\"lon_0\"],self.proj_dict[\"lat_0\"] \n            \n                distances = np.array([np.sqrt((x-center_lon)**2+\\\n                        (y-center_lat)**2) for (x, y) in conus_lat_lon_points])\n            \n                min_dist, max_minus_min = min(distances),max(distances)-min(distances)\n\n                distance_0_1 = [1.0-((d - min_dist)/(max_minus_min)) for d in distances]\n                weights = np.array(distance_0_1)\n        \n            output_data = np.where(group_data.loc[:, output_column] > output_threshold, 1, 0)\n            ones = np.count_nonzero(output_data > 0)\n            print(\"Ones: \", ones, \"Zeros: \", np.count_nonzero(output_data == 0))\n            self.condition_models[group] = {}\n            num_elements = group_data[input_columns].shape[0]\n            \n            for m, model_name in enumerate(model_names):\n                print(model_name)    \n                roc = DistributedROC(thresholds=np.arange(0, 1.1, 0.01))\n                self.condition_models[group][model_name] = deepcopy(model_objs[m])\n\n                try:\n                    kf = KFold(n_splits=num_folds)\n                    for train_index, test_index in kf.split(group_data[input_columns].values):\n                        if np.count_nonzero(output_data[train_index]) > 0:\n                            try:\n                                self.condition_models[group][model_name].fit(\n                                        group_data.iloc[train_index][input_columns],\n                                        output_data[train_index],sample_weight=weights[train_index])\n                            except:\n                                self.condition_models[group][model_name].fit(\n                                        group_data.iloc[train_index][input_columns],\n                                        output_data[train_index])\n                            \n                            cv_preds = self.condition_models[group][model_name].predict_proba(\n                                group_data.iloc[test_index][input_columns])[:,1]\n                            \n                            roc.update(cv_preds, output_data[test_index])\n                        \n                        else:\n                            continue\n\n                except TypeError:\n                    kf = KFold(num_elements,n_folds=num_folds)\n                    for train_index, test_index in kf:\n\n                        if np.count_nonzero(output_data[train_index]) > 0:\n                            try:\n                                self.condition_models[group][model_name].fit(\n                                        group_data.iloc[train_index][input_columns],\n                                        output_data[train_index],sample_weight=weights[train_index])\n                            except:\n                                self.condition_models[group][model_name].fit(\n                                        group_data.iloc[train_index][input_columns],\n                                        output_data[train_index])\n                            cv_preds = self.condition_models[group][model_name].predict_proba(\n                                group_data.iloc[test_index][input_columns])[:, 1]\n                            \n                            roc.update(cv_preds, output_data[test_index])\n                        \n                        else:\n                            continue\n\n                self.condition_models[group][\n                    model_name + \"_condition_threshold\"], _ = roc.max_threshold_score(threshold_score)\n                print(model_name + \" condition threshold: {0:0.3f}\".format(\n                self.condition_models[group][model_name + \"_condition_threshold\"]))\n                self.condition_models[group][model_name].fit(group_data[input_columns], output_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npredicts the probability of each condition model in the data.", "response": "def predict_condition_models(self, model_names,\n                                 input_columns,\n                                 metadata_cols,\n                                 data_mode=\"forecast\",\n                                 ):\n        \"\"\"\n        Apply condition modelsto forecast data.\n        Args:\n            model_names: List of names associated with each condition model used for prediction\n            input_columns: List of columns in data used as input into the model\n            metadata_cols: Columns from input data that should be included in the data frame with the predictions.\n            data_mode: Which data subset to pull from for the predictions, \"forecast\" by default\n        Returns:\n            A dictionary of data frames containing probabilities of the event and specified metadata\n        \"\"\"\n        groups = self.condition_models.keys()\n        predictions = pd.DataFrame(self.data[data_mode][\"combo\"][metadata_cols])\n        for group in groups:\n            print(group)\n            print(self.condition_models[group])\n            g_idxs = self.data[data_mode][\"combo\"][self.group_col] == group\n            group_count = np.count_nonzero(g_idxs)\n            if group_count > 0:\n                for m, model_name in enumerate(model_names):\n                    mn = model_name.replace(\" \", \"-\")\n                    predictions.loc[g_idxs, mn + \"_conditionprob\"] = self.condition_models[group][\n                                                                         model_name].predict_proba(\n                        self.data[data_mode][\"combo\"].loc[g_idxs, input_columns])[:, 1]\n                    predictions.loc[g_idxs,\n                                    mn + \"_conditionthresh\"] = np.where(predictions.loc[g_idxs, mn + \"_conditionprob\"]\n                                                                        >= self.condition_models[group][\n                                                                            model_name + \"_condition_threshold\"], 1, 0)\n\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting multitask machine learning models to predict the parameters of a size distribution of a specific class.", "response": "def fit_size_distribution_models(self, model_names, model_objs, input_columns,\n                                     output_columns=None, calibrate=False):\n        \"\"\"\n        Fits multitask machine learning models to predict the parameters of a size distribution\n        Args:\n            model_names: List of machine learning model names\n            model_objs: scikit-learn style machine learning model objects\n            input_columns: Training data columns used as input for ML model\n            output_columns: Training data columns used for prediction\n            calibrate: Whether or not to fit a log-linear regression to predictions from ML model\n        \"\"\"\n        if output_columns is None:\n            output_columns = [\"Shape\", \"Location\", \"Scale\"]\n        groups = np.unique(self.data[\"train\"][\"member\"][self.group_col])\n        \n        weights=None\n        \n        for group in groups:\n            group_data = self.data[\"train\"][\"combo\"].loc[self.data[\"train\"][\"combo\"][self.group_col] == group]\n            group_data = group_data.dropna()\n            group_data = group_data[group_data[output_columns[-1]] > 0]\n            if self.sector:\n        \n                lon_obj = group_data.loc[:,'Centroid_Lon']\n                lat_obj = group_data.loc[:,'Centroid_Lat']\n                \n                conus_lat_lon_points = zip(lon_obj.values.ravel(),lat_obj.values.ravel())\n                center_lon, center_lat = self.proj_dict[\"lon_0\"],self.proj_dict[\"lat_0\"] \n            \n                distances = np.array([np.sqrt((x-center_lon)**2+\\\n                        (y-center_lat)**2) for (x, y) in conus_lat_lon_points])\n            \n                min_dist, max_minus_min = min(distances),max(distances)-min(distances)\n\n                distance_0_1 = [1.0-((d - min_dist)/(max_minus_min)) for d in distances]\n                weights = np.array(distance_0_1)\n\n            self.size_distribution_models[group] = {\"multi\": {}, \"lognorm\": {}}\n            if calibrate:\n                self.size_distribution_models[group][\"calshape\"] = {}\n                self.size_distribution_models[group][\"calscale\"] = {}\n            log_labels = np.log(group_data[output_columns].values)\n            log_means = log_labels.mean(axis=0)\n            log_sds = log_labels.std(axis=0)\n            self.size_distribution_models[group]['lognorm']['mean'] = log_means\n            self.size_distribution_models[group]['lognorm']['sd'] = log_sds\n            for m, model_name in enumerate(model_names):\n                print(group, model_name)\n                self.size_distribution_models[group][\"multi\"][model_name] = deepcopy(model_objs[m])\n                try:\n                    self.size_distribution_models[group][\"multi\"][model_name].fit(group_data[input_columns],\n                                                                              (log_labels - log_means) / log_sds,\n                                                                        sample_weight=weights)\n                except:\n                    self.size_distribution_models[group][\"multi\"][model_name].fit(group_data[input_columns],\n                                                                              (log_labels - log_means) / log_sds)\n                if calibrate:\n                    training_predictions = self.size_distribution_models[\n                        group][\"multi\"][model_name].predict(group_data[input_columns])\n                    self.size_distribution_models[group][\"calshape\"][model_name] = LinearRegression()\n                    self.size_distribution_models[group][\"calshape\"][model_name].fit(training_predictions[:, 0:1],\n                                                                                     (log_labels[:, 0] - log_means[0]) /\n                                                                                     log_sds[\n                                                                                         0],\n                                                                                    sample_weight=weights)\n                    self.size_distribution_models[group][\"calscale\"][model_name] = LinearRegression()\n                    self.size_distribution_models[group][\"calscale\"][model_name].fit(training_predictions[:, 1:],\n                                                                                     (log_labels[:, 1] - log_means[1]) /\n                                                                                     log_sds[\n                                                                                         1],\n                                                                            sample_weight=weights)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit_size_distribution_component_models(self, model_names, model_objs, input_columns, output_columns):\n        groups = np.unique(self.data[\"train\"][\"member\"][self.group_col])\n        \n        weights=None\n\n        for group in groups:\n            print(group)\n            group_data = self.data[\"train\"][\"combo\"].loc[self.data[\"train\"][\"combo\"][self.group_col] == group]\n            group_data = group_data.dropna()\n            group_data = group_data.loc[group_data[output_columns[-1]] > 0]\n            \n            if self.sector:\n        \n                lon_obj = group_data.loc[:,'Centroid_Lon']\n                lat_obj = group_data.loc[:,'Centroid_Lat']\n                \n                conus_lat_lon_points = zip(lon_obj.values.ravel(),lat_obj.values.ravel())\n                center_lon, center_lat = self.proj_dict[\"lon_0\"],self.proj_dict[\"lat_0\"] \n            \n                distances = np.array([np.sqrt((x-center_lon)**2+\\\n                        (y-center_lat)**2) for (x, y) in conus_lat_lon_points])\n            \n                min_dist, max_minus_min = min(distances),max(distances)-min(distances)\n\n                distance_0_1 = [1.0-((d - min_dist)/(max_minus_min)) for d in distances]\n                weights = np.array(distance_0_1)\n\n            \n            self.size_distribution_models[group] = {\"lognorm\": {}}\n            self.size_distribution_models[group][\"lognorm\"][\"pca\"] = PCA(n_components=len(output_columns))\n            log_labels = np.log(group_data[output_columns].values)\n            log_labels[:, np.where(output_columns == \"Shape\")[0]] *= -1\n            log_means = log_labels.mean(axis=0)\n            log_sds = log_labels.std(axis=0)\n            log_norm_labels = (log_labels - log_means) / log_sds\n            out_pc_labels = self.size_distribution_models[group][\"lognorm\"][\"pca\"].fit_transform(log_norm_labels)\n            self.size_distribution_models[group]['lognorm']['mean'] = log_means\n            self.size_distribution_models[group]['lognorm']['sd'] = log_sds\n            for comp in range(len(output_columns)):\n                self.size_distribution_models[group][\"pc_{0:d}\".format(comp)] = dict()\n                for m, model_name in enumerate(model_names):\n                    print(model_name, comp)\n                    self.size_distribution_models[group][\n                        \"pc_{0:d}\".format(comp)][model_name] = deepcopy(model_objs[m])\n                    try:\n                        self.size_distribution_models[group][\n                            \"pc_{0:d}\".format(comp)][model_name].fit(group_data[input_columns],\n                                                                 out_pc_labels[:, comp],\n                                                            sample_weight=weights)\n                    except:\n                        self.size_distribution_models[group][\n                            \"pc_{0:d}\".format(comp)][model_name].fit(group_data[input_columns],\n                                                                 out_pc_labels[:, comp])\n        return", "response": "This function calculates the 2 principal components of the hail size distribution between the shape and scale parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes predictions using fitted size distribution models.", "response": "def predict_size_distribution_models(self, model_names, input_columns, metadata_cols,\n                                         data_mode=\"forecast\", location=6, calibrate=False):\n        \"\"\"\n        Make predictions using fitted size distribution models.\n        Args:\n            model_names: Name of the models for predictions\n            input_columns: Data columns used for input into ML models\n            metadata_cols: Columns from input data that should be included in the data frame with the predictions.\n            data_mode: Set of data used as input for prediction models\n            location: Value of fixed location parameter\n            calibrate: Whether or not to apply calibration model\n        Returns:\n            Predictions in dictionary of data frames grouped by group type\n        \"\"\"\n        groups = self.size_distribution_models.keys()\n        predictions = {}\n        for group in groups:\n            group_data = self.data[data_mode][\"combo\"].loc[self.data[data_mode][\"combo\"][self.group_col] == group]\n            predictions[group] = group_data[metadata_cols]\n            if group_data.shape[0] > 0:\n                log_mean = self.size_distribution_models[group][\"lognorm\"][\"mean\"]\n                log_sd = self.size_distribution_models[group][\"lognorm\"][\"sd\"]\n                for m, model_name in enumerate(model_names):\n                    multi_predictions = self.size_distribution_models[group][\"multi\"][model_name].predict(\n                        group_data[input_columns])\n                    if calibrate:\n                        multi_predictions[:, 0] = self.size_distribution_models[group][\"calshape\"][model_name].predict(\n                            multi_predictions[:, 0:1])\n                        multi_predictions[:, 1] = self.size_distribution_models[group][\"calscale\"][model_name].predict(\n                            multi_predictions[:, 1:])\n                    multi_predictions = np.exp(multi_predictions * log_sd + log_mean)\n                    if multi_predictions.shape[1] == 2:\n                        multi_predictions_temp = np.zeros((multi_predictions.shape[0], 3))\n                        multi_predictions_temp[:, 0] = multi_predictions[:, 0]\n                        multi_predictions_temp[:, 1] = location\n                        multi_predictions_temp[:, 2] = multi_predictions[:, 1]\n                        multi_predictions = multi_predictions_temp\n                    for p, pred_col in enumerate([\"shape\", \"location\", \"scale\"]):\n                        predictions[group][model_name].loc[:, model_name.replace(\" \", \"-\") + \"_\" + pred_col] = \\\n                            multi_predictions[:, p]\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking predictions using fitted size distribution models.", "response": "def predict_size_distribution_component_models(self, model_names, input_columns, output_columns, metadata_cols,\n                                                   data_mode=\"forecast\", location=6):\n        \"\"\"\n        Make predictions using fitted size distribution models.\n        Args:\n            model_names: Name of the models for predictions\n            input_columns: Data columns used for input into ML models\n            output_columns: Names of output columns\n            metadata_cols: Columns from input data that should be included in the data frame with the predictions.\n            data_mode: Set of data used as input for prediction models\n            location: Value of fixed location parameter\n        Returns:\n            Predictions in dictionary of data frames grouped by group type\n        \"\"\"\n        groups = self.size_distribution_models.keys()\n        predictions = pd.DataFrame(self.data[data_mode][\"combo\"][metadata_cols])\n        for group in groups:\n            group_idxs = self.data[data_mode][\"combo\"][self.group_col] == group\n            group_count = np.count_nonzero(group_idxs)\n            print(self.size_distribution_models[group])\n            if group_count > 0:\n                log_mean = self.size_distribution_models[group][\"lognorm\"][\"mean\"]\n                log_sd = self.size_distribution_models[group][\"lognorm\"][\"sd\"]\n                for m, model_name in enumerate(model_names):\n                    raw_preds = np.zeros((group_count, len(output_columns)))\n                    for c in range(len(output_columns)):\n                        raw_preds[:, c] = self.size_distribution_models[group][\n                            \"pc_{0:d}\".format(c)][model_name].predict(self.data[data_mode][\"combo\"].loc[group_idxs,\n                                                                                                        input_columns])\n                    log_norm_preds = self.size_distribution_models[group][\"lognorm\"][\"pca\"].inverse_transform(raw_preds)\n                    log_norm_preds[:, 0] *= -1\n                    multi_predictions = np.exp(log_norm_preds * log_sd + log_mean)\n                    if multi_predictions.shape[1] == 2:\n                        multi_predictions_temp = np.zeros((multi_predictions.shape[0], 3))\n                        multi_predictions_temp[:, 0] = multi_predictions[:, 0]\n                        multi_predictions_temp[:, 1] = location\n                        multi_predictions_temp[:, 2] = multi_predictions[:, 1]\n                        multi_predictions = multi_predictions_temp\n                    for p, pred_col in enumerate([\"shape\", \"location\", \"scale\"]):\n                        predictions.loc[group_idxs, model_name.replace(\" \", \"-\") + \"_\" + pred_col] = \\\n                            multi_predictions[:, p]\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit size models to produce discrete pdfs of forecast hail sizes.", "response": "def fit_size_models(self, model_names,\n                        model_objs,\n                        input_columns,\n                        output_column=\"Hail_Size\",\n                        output_start=5,\n                        output_step=5,\n                        output_stop=100):\n        \"\"\"\n        Fit size models to produce discrete pdfs of forecast hail sizes.\n        Args:\n            model_names: List of model names\n            model_objs: List of model objects\n            input_columns: List of input variables\n            output_column: Output variable name\n            output_start: Hail size bin start\n            output_step: hail size bin step\n            output_stop: hail size bin stop\n        \"\"\"\n        print(\"Fitting size models\")\n        groups = self.data[\"train\"][\"member\"][self.group_col].unique()\n        output_start = int(output_start)\n        output_step = int(output_step)\n        output_stop = int(output_stop)\n        for group in groups:\n            group_data = self.data[\"train\"][\"combo\"].loc[self.data[\"train\"][\"combo\"][self.group_col] == group]\n            group_data.dropna(inplace=True)\n            group_data = group_data[group_data[output_column] >= output_start]\n            output_data = group_data[output_column].values.astype(int)\n            output_data[output_data > output_stop] = output_stop\n            discrete_data = ((output_data - output_start) // output_step) * output_step + output_start\n            self.size_models[group] = {}\n            self.size_models[group][\"outputvalues\"] = np.arange(output_start, output_stop + output_step, output_step,\n                                                                dtype=int)\n            for m, model_name in enumerate(model_names):\n                print(\"{0} {1}\".format(group, model_name))\n                self.size_models[group][model_name] = deepcopy(model_objs[m])\n                self.size_models[group][model_name].fit(group_data[input_columns], discrete_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npredicts the size models for the given model_names.", "response": "def predict_size_models(self, model_names,\n                            input_columns,\n                            metadata_cols,\n                            data_mode=\"forecast\"):\n        \"\"\"\n        Apply size models to forecast data.\n        Args:\n            model_names:\n            input_columns:\n            metadata_cols:\n            data_mode:\n        \"\"\"\n        groups = self.size_models.keys()\n        predictions = {}\n        for group in groups:\n            group_data = self.data[data_mode][\"combo\"].loc[self.data[data_mode][\"combo\"][self.group_col] == group]\n            if group_data.shape[0] > 0:\n                predictions[group] = {}\n                output_values = self.size_models[group][\"outputvalues\"].astype(int)\n                for m, model_name in enumerate(model_names):\n                    print(\"{0} {1}\".format(group, model_name))\n                    pred_col_names = [model_name.replace(\" \", \"-\") + \"_{0:02d}\".format(p) for p in output_values]\n                    predictions[group][model_name] = group_data[metadata_cols]\n                    pred_vals = self.size_models[group][model_name].predict_proba(group_data[input_columns])\n                    pred_classes = self.size_models[group][model_name].classes_\n                    pred_pdf = np.zeros((pred_vals.shape[0], output_values.size))\n                    for pcv, pc in enumerate(pred_classes):\n                        idx = np.where(output_values == pc)[0][0]\n                        pred_pdf[:, idx] = pred_vals[:, pcv]\n                    for pcn, pred_col_name in enumerate(pred_col_names):\n                        predictions[group][model_name].loc[:, pred_col_name] = pred_pdf[:, pcn]\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit_track_models(self,\n                         model_names,\n                         model_objs,\n                         input_columns,\n                         output_columns,\n                         output_ranges,\n                         ):\n        \"\"\"\n        Fit machine learning models to predict track error offsets.\n            model_names:\n            model_objs:\n            input_columns:\n            output_columns:\n            output_ranges:\n        \"\"\"\n        print(\"Fitting track models\")\n        groups = self.data[\"train\"][\"member\"][self.group_col].unique()\n        for group in groups:\n            group_data = self.data[\"train\"][\"combo\"].loc[self.data[\"train\"][\"combo\"][self.group_col] == group]\n            group_data = group_data.dropna()\n            group_data = group_data.loc[group_data[\"Duration_Step\"] == 1]\n            for model_type, model_dict in self.track_models.items():\n                model_dict[group] = {}\n                output_data = group_data[output_columns[model_type]].values.astype(int)\n                output_data[output_data < output_ranges[model_type][0]] = output_ranges[model_type][0]\n                output_data[output_data > output_ranges[model_type][1]] = output_ranges[model_type][1]\n                discrete_data = (output_data - output_ranges[model_type][0]) // output_ranges[model_type][2] * \\\n                                output_ranges[model_type][2] + output_ranges[model_type][0]\n                model_dict[group][\"outputvalues\"] = np.arange(output_ranges[model_type][0],\n                                                              output_ranges[model_type][1] +\n                                                              output_ranges[model_type][2],\n                                                              output_ranges[model_type][2])\n                for m, model_name in enumerate(model_names):\n                    print(\"{0} {1} {2}\".format(group, model_type, model_name))\n                    model_dict[group][model_name] = deepcopy(model_objs[m])\n                    model_dict[group][model_name].fit(group_data[input_columns], discrete_data)", "response": "Fit track models to predict track error offsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the machine learning models to pickle files.", "response": "def save_models(self, model_path):\n        \"\"\"\n        Save machine learning models to pickle files.\n        \"\"\"\n        for group, condition_model_set in self.condition_models.items():\n            for model_name, model_obj in condition_model_set.items():\n                out_filename = model_path + \\\n                               \"{0}_{1}_condition.pkl\".format(group,\n                                                              model_name.replace(\" \", \"-\"))\n                with open(out_filename, \"wb\") as pickle_file:\n                    pickle.dump(model_obj,\n                                pickle_file,\n                                pickle.HIGHEST_PROTOCOL)\n        for group, size_model_set in self.size_models.items():\n            for model_name, model_obj in size_model_set.items():\n                out_filename = model_path + \\\n                               \"{0}_{1}_size.pkl\".format(group,\n                                                         model_name.replace(\" \", \"-\"))\n                with open(out_filename, \"wb\") as pickle_file:\n                    pickle.dump(model_obj,\n                                pickle_file,\n                                pickle.HIGHEST_PROTOCOL)\n        for group, dist_model_set in self.size_distribution_models.items():\n            for model_type, model_objs in dist_model_set.items():\n                for model_name, model_obj in model_objs.items():\n                    out_filename = model_path + \\\n                                   \"{0}_{1}_{2}_sizedist.pkl\".format(group,\n                                                                     model_name.replace(\" \", \"-\"),\n                                                                     model_type)\n                    with open(out_filename, \"wb\") as pickle_file:\n                        pickle.dump(model_obj,\n                                    pickle_file,\n                                    pickle.HIGHEST_PROTOCOL)\n        for model_type, track_type_models in self.track_models.items():\n            for group, track_model_set in track_type_models.items():\n                for model_name, model_obj in track_model_set.items():\n                    out_filename = model_path + \\\n                                   \"{0}_{1}_{2}_track.pkl\".format(group,\n                                                                  model_name.replace(\" \", \"-\"),\n                                                                  model_type)\n                    with open(out_filename, \"wb\") as pickle_file:\n                        pickle.dump(model_obj,\n                                    pickle_file,\n                                    pickle.HIGHEST_PROTOCOL)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_models(self, model_path):\n        condition_model_files = sorted(glob(model_path + \"*_condition.pkl\"))\n        if len(condition_model_files) > 0:\n            for condition_model_file in condition_model_files:\n                model_comps = condition_model_file.split(\"/\")[-1][:-4].split(\"_\")\n                if model_comps[0] not in self.condition_models.keys():\n                    self.condition_models[model_comps[0]] = {}\n                model_name = model_comps[1].replace(\"-\", \" \")\n                with open(condition_model_file, \"rb\") as cmf:\n                    if \"condition_threshold\" in condition_model_file:\n                        self.condition_models[model_comps[0]][model_name + \"_condition_threshold\"] = pickle.load(cmf)\n                    else:\n                        self.condition_models[model_comps[0]][model_name] = pickle.load(cmf)\n\n        size_model_files = sorted(glob(model_path + \"*_size.pkl\"))\n        if len(size_model_files) > 0:\n            for size_model_file in size_model_files:\n                model_comps = size_model_file.split(\"/\")[-1][:-4].split(\"_\")\n                if model_comps[0] not in self.size_models.keys():\n                    self.size_models[model_comps[0]] = {}\n                model_name = model_comps[1].replace(\"-\", \" \")\n                with open(size_model_file, \"rb\") as smf:\n                    self.size_models[model_comps[0]][model_name] = pickle.load(smf)\n\n        size_dist_model_files = sorted(glob(model_path + \"*_sizedist.pkl\"))\n        if len(size_dist_model_files) > 0:\n            for dist_model_file in size_dist_model_files:\n                model_comps = dist_model_file.split(\"/\")[-1][:-4].split(\"_\")\n                if model_comps[0] not in self.size_distribution_models.keys():\n                    self.size_distribution_models[model_comps[0]] = {}\n                if \"_\".join(model_comps[2:-1]) not in self.size_distribution_models[model_comps[0]].keys():\n                    self.size_distribution_models[model_comps[0]][\"_\".join(model_comps[2:-1])] = {}\n                model_name = model_comps[1].replace(\"-\", \" \")\n                with open(dist_model_file, \"rb\") as dmf:\n                    self.size_distribution_models[model_comps[0]][\"_\".join(model_comps[2:-1])][\n                        model_name] = pickle.load(dmf)\n\n        track_model_files = sorted(glob(model_path + \"*_track.pkl\"))\n        if len(track_model_files) > 0:\n            for track_model_file in track_model_files:\n                model_comps = track_model_file.split(\"/\")[-1][:-4].split(\"_\")\n                group = model_comps[0]\n                model_name = model_comps[1].replace(\"-\", \" \")\n                model_type = model_comps[2]\n                if model_type not in self.track_models.keys():\n                    self.track_models[model_type] = {}\n                if group not in self.track_models[model_type].keys():\n                    self.track_models[model_type][group] = {}\n                with open(track_model_file, \"rb\") as tmf:\n                    self.track_models[model_type][group][model_name] = pickle.load(tmf)", "response": "Load models from pickle files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output_forecasts_json(self, forecasts,\n                              condition_model_names,\n                              size_model_names,\n                              dist_model_names,\n                              track_model_names,\n                              json_data_path,\n                              out_path):\n        \"\"\"\n        Output forecast values to geoJSON file format.\n        :param forecasts:\n        :param condition_model_names:\n        :param size_model_names:\n        :param track_model_names:\n        :param json_data_path:\n        :param out_path:\n        :return:\n        \"\"\"\n        total_tracks = self.data[\"forecast\"][\"total\"]\n        for r in np.arange(total_tracks.shape[0]):\n            track_id = total_tracks.loc[r, \"Track_ID\"]\n            print(track_id)\n            track_num = track_id.split(\"_\")[-1]\n            ensemble_name = total_tracks.loc[r, \"Ensemble_Name\"]\n            member = total_tracks.loc[r, \"Ensemble_Member\"]\n            group = self.data[\"forecast\"][\"member\"].loc[self.data[\"forecast\"][\"member\"][\"Ensemble_Member\"] == member,\n                                                        self.group_col].values[0]\n            run_date = track_id.split(\"_\")[-4][:8]\n            step_forecasts = {}\n            for ml_model in condition_model_names:\n                step_forecasts[\"condition_\" + ml_model.replace(\" \", \"-\")] = forecasts[\"condition\"][group].loc[\n                    forecasts[\"condition\"][group][\"Track_ID\"] == track_id, ml_model]\n            for ml_model in size_model_names:\n                step_forecasts[\"size_\" + ml_model.replace(\" \", \"-\")] = forecasts[\"size\"][group][ml_model].loc[\n                    forecasts[\"size\"][group][ml_model][\"Track_ID\"] == track_id]\n            for ml_model in dist_model_names:\n                step_forecasts[\"dist_\" + ml_model.replace(\" \", \"-\")] = forecasts[\"dist\"][group][ml_model].loc[\n                    forecasts[\"dist\"][group][ml_model][\"Track_ID\"] == track_id]\n            for model_type in forecasts[\"track\"].keys():\n                for ml_model in track_model_names:\n                    mframe = forecasts[\"track\"][model_type][group][ml_model]\n                    step_forecasts[model_type + \"_\" + ml_model.replace(\" \", \"-\")] = mframe.loc[\n                        mframe[\"Track_ID\"] == track_id]\n            json_file_name = \"{0}_{1}_{2}_model_track_{3}.json\".format(ensemble_name,\n                                                                       run_date,\n                                                                       member,\n                                                                       track_num)\n            full_json_path = json_data_path + \"/\".join([run_date, member]) + \"/\" + json_file_name\n            with open(full_json_path) as json_file_obj:\n                try:\n                    track_obj = json.load(json_file_obj)\n                except FileNotFoundError:\n                    print(full_json_path + \" not found\")\n                    continue\n            for f, feature in enumerate(track_obj['features']):\n                del feature['properties']['attributes']\n                for model_name, fdata in step_forecasts.items():\n                    ml_model_name = model_name.split(\"_\")[1]\n                    if \"condition\" in model_name:\n                        feature['properties'][model_name] = fdata.values[f]\n                    else:\n                        predcols = []\n                        for col in fdata.columns:\n                            if ml_model_name in col:\n                                predcols.append(col)\n                        feature['properties'][model_name] = fdata.loc[:, predcols].values[f].tolist()\n            full_path = []\n            for part in [run_date, member]:\n                full_path.append(part)\n                if not os.access(out_path + \"/\".join(full_path), os.R_OK):\n                    try:\n                        os.mkdir(out_path + \"/\".join(full_path))\n                    except OSError:\n                        print(\"directory already created\")\n            out_json_filename = out_path + \"/\".join(full_path) + \"/\" + json_file_name\n            with open(out_json_filename, \"w\") as out_json_obj:\n                json.dump(track_obj, out_json_obj, indent=1, sort_keys=True)\n        return", "response": "Output forecasts to geoJSON file format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noutputs hail forecasts to csv files by ensemble member and ensemble name.", "response": "def output_forecasts_csv(self, forecasts, mode, csv_path, run_date_format=\"%Y%m%d-%H%M\"):\n        \"\"\"\n        Output hail forecast values to csv files by run date and ensemble member.\n\n        Args:\n            forecasts:\n            mode:\n            csv_path:\n        Returns:\n        \"\"\"\n        merged_forecasts = pd.merge(forecasts[\"condition\"],\n                                    forecasts[\"dist\"],\n                                    on=[\"Step_ID\",\"Track_ID\",\"Ensemble_Member\",\"Forecast_Hour\"])\n        all_members = self.data[mode][\"combo\"][\"Ensemble_Member\"]\n        members = np.unique(all_members)\n        all_run_dates = pd.DatetimeIndex(self.data[mode][\"combo\"][\"Run_Date\"])\n        run_dates = pd.DatetimeIndex(np.unique(all_run_dates))\n        print(run_dates)\n        for member in members:\n            for run_date in run_dates:\n                mem_run_index = (all_run_dates == run_date) & (all_members == member)\n                member_forecast = merged_forecasts.loc[mem_run_index]\n                member_forecast.to_csv(join(csv_path, \"hail_forecasts_{0}_{1}_{2}.csv\".format(self.ensemble_name,\n                                                                                              member,\n                                                                                              run_date.strftime\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  (run_date_format))))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ativar_sat(self, tipo_certificado, cnpj, codigo_uf):\n        return self.invocar__AtivarSAT(\n                self.gerar_numero_sessao(), tipo_certificado,\n                self._codigo_ativacao, cnpj, codigo_uf)", "response": "AtivarSAT para ER SAT para o estabelecimento de unidade federativa."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef configurar_interface_de_rede(self, configuracao):\n        conf_xml = configuracao \\\n                if isinstance(configuracao, basestring) \\\n                else configuracao.documento()\n\n        return self.invocar__ConfigurarInterfaceDeRede(\n                self.gerar_numero_sessao(), self._codigo_ativacao, conf_xml)", "response": "Configurar o interface de comunica\u00e7\u00e3o do equipamento SAT."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_forecasts(self):\n        forecast_path = self.forecast_json_path + \"/{0}/{1}/\".format(self.run_date.strftime(\"%Y%m%d\"),\n                                                                     self.ensemble_member)\n        forecast_files = sorted(glob(forecast_path + \"*.json\"))\n        for forecast_file in forecast_files:\n            file_obj = open(forecast_file)\n            json_obj = json.load(file_obj)\n            file_obj.close()\n            track_id = json_obj['properties'][\"id\"]\n            obs_track_id = json_obj['properties'][\"obs_track_id\"]\n            forecast_hours = json_obj['properties']['times']\n            duration = json_obj['properties']['duration']\n            for f, feature in enumerate(json_obj['features']):\n                area = np.sum(feature[\"properties\"][\"masks\"])\n                step_id = track_id + \"_{0:02d}\".format(f)\n                for model_type in self.model_types:\n                    for model_name in self.model_names[model_type]:\n                        prediction = feature['properties'][model_type + \"_\" + model_name.replace(\" \", \"-\")]\n                        if model_type == \"condition\":\n                            prediction = [prediction]\n                        row = [track_id, obs_track_id, self.ensemble_name, self.ensemble_member, forecast_hours[f],\n                               f + 1, duration, area] + prediction\n                        self.forecasts[model_type][model_name].loc[step_id] = row", "response": "Loads the forecast files and gathers the forecast information into pandas DataFrames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_obs(self):\n        track_total_file = self.track_data_csv_path + \\\n            \"track_total_{0}_{1}_{2}.csv\".format(self.ensemble_name,\n                                                 self.ensemble_member,\n                                                 self.run_date.strftime(\"%Y%m%d\"))\n        track_step_file = self.track_data_csv_path + \\\n            \"track_step_{0}_{1}_{2}.csv\".format(self.ensemble_name,\n                                                self.ensemble_member,\n                                                self.run_date.strftime(\"%Y%m%d\"))\n        track_total_cols = [\"Track_ID\", \"Translation_Error_X\", \"Translation_Error_Y\", \"Start_Time_Error\"]\n        track_step_cols = [\"Step_ID\", \"Track_ID\", \"Hail_Size\", \"Shape\", \"Location\", \"Scale\"]\n        track_total_data = pd.read_csv(track_total_file, usecols=track_total_cols)\n        track_step_data = pd.read_csv(track_step_file, usecols=track_step_cols)\n        obs_data = pd.merge(track_step_data, track_total_data, on=\"Track_ID\", how=\"left\")\n        self.obs = obs_data", "response": "Loads the track total and step files and merges the information into a single data frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges observations with forecasts.", "response": "def merge_obs(self):\n        \"\"\"\n        Match forecasts and observations.\n        \"\"\"\n        for model_type in self.model_types:\n            self.matched_forecasts[model_type] = {}\n            for model_name in self.model_names[model_type]:\n                self.matched_forecasts[model_type][model_name] = pd.merge(self.forecasts[model_type][model_name],\n                                                                          self.obs, right_on=\"Step_ID\", how=\"left\",\n                                                                          left_index=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the cumulative ranked probability score of the forecasts on the metadata.", "response": "def crps(self, model_type, model_name, condition_model_name, condition_threshold, query=None):\n        \"\"\"\n        Calculates the cumulative ranked probability score (CRPS) on the forecast data.\n\n        Args:\n            model_type: model type being evaluated.\n            model_name: machine learning model being evaluated.\n            condition_model_name: Name of the hail/no-hail model being evaluated\n            condition_threshold: Threshold for using hail size CDF\n            query: pandas query string to filter the forecasts based on the metadata\n\n\n        Returns:\n            a DistributedCRPS object\n        \"\"\"\n\n        def gamma_cdf(x, a, loc, b):\n            if a == 0 or b == 0:\n                cdf = np.ones(x.shape)\n            else:\n                cdf = gamma.cdf(x, a, loc, b)\n            return cdf\n\n        crps_obj = DistributedCRPS(self.dist_thresholds)\n        if query is not None:\n            sub_forecasts = self.matched_forecasts[model_type][model_name].query(query)\n            sub_forecasts = sub_forecasts.reset_index(drop=True)\n            condition_forecasts = self.matched_forecasts[\"condition\"][condition_model_name].query(query)\n            condition_forecasts = condition_forecasts.reset_index(drop=True)\n        else:\n            sub_forecasts = self.matched_forecasts[model_type][model_name]\n            condition_forecasts = self.matched_forecasts[\"condition\"][condition_model_name]\n        if sub_forecasts.shape[0] > 0:\n            if model_type == \"dist\":\n                forecast_cdfs = np.zeros((sub_forecasts.shape[0], self.dist_thresholds.size))\n                for f in range(sub_forecasts.shape[0]):\n                    condition_prob = condition_forecasts.loc[f, self.forecast_bins[\"condition\"][0]]\n                    if condition_prob >= condition_threshold:\n                        f_params = [0, 0, 0]\n                    else:\n                        f_params = sub_forecasts[self.forecast_bins[model_type]].values[f]\n                    forecast_cdfs[f] = gamma_cdf(self.dist_thresholds, f_params[0], f_params[1], f_params[2])\n                obs_cdfs = np.array([gamma_cdf(self.dist_thresholds, *params)\n                                    for params in sub_forecasts[self.type_cols[model_type]].values])\n                crps_obj.update(forecast_cdfs, obs_cdfs)\n            else:\n                crps_obj.update(sub_forecasts[self.forecast_bins[model_type].astype(str)].values,\n                                sub_forecasts[self.type_cols[model_type]].values)\n\n        return crps_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef roc(self, model_type, model_name, intensity_threshold, prob_thresholds, query=None):\n        roc_obj = DistributedROC(prob_thresholds, 0.5)\n        if query is not None:\n            sub_forecasts = self.matched_forecasts[model_type][model_name].query(query)\n            sub_forecasts = sub_forecasts.reset_index(drop=True)\n        else:\n            sub_forecasts = self.matched_forecasts[model_type][model_name]\n        obs_values = np.zeros(sub_forecasts.shape[0])\n        if sub_forecasts.shape[0] > 0:\n            if model_type == \"dist\":\n                forecast_values = np.array([gamma_sf(intensity_threshold, *params)\n                                            for params in sub_forecasts[self.forecast_bins[model_type]].values])\n                obs_probs = np.array([gamma_sf(intensity_threshold, *params)\n                                    for params in sub_forecasts[self.type_cols[model_type]].values])\n                obs_values[obs_probs >= 0.01] = 1\n            elif len(self.forecast_bins[model_type]) > 1:\n                fbin = np.argmin(np.abs(self.forecast_bins[model_type] - intensity_threshold))\n                forecast_values = 1 - sub_forecasts[self.forecast_bins[model_type].astype(str)].values.cumsum(axis=1)[:,\n                                    fbin]\n                obs_values[sub_forecasts[self.type_cols[model_type]].values >= intensity_threshold] = 1\n            else:\n                forecast_values = sub_forecasts[self.forecast_bins[model_type].astype(str)[0]].values\n                obs_values[sub_forecasts[self.type_cols[model_type]].values >= intensity_threshold] = 1\n            roc_obj.update(forecast_values, obs_values)\n        return roc_obj", "response": "Calculates a distributed ROC curve at a specified intensity threshold."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_forecast_max_hail(self, dist_model_name, condition_model_name,\n                                 num_samples, condition_threshold=0.5, query=None):\n        \"\"\"\n        Samples every forecast hail object and returns an empirical distribution of possible maximum hail sizes.\n\n        Hail sizes are sampled from each predicted gamma distribution. The total number of samples equals\n        num_samples * area of the hail object. To get the maximum hail size for each realization, the maximum\n        value within each area sample is used.\n\n        Args:\n            dist_model_name: Name of the distribution machine learning model being evaluated\n            condition_model_name: Name of the hail/no-hail model being evaluated\n            num_samples: Number of maximum hail samples to draw\n            condition_threshold: Threshold for drawing hail samples\n            query: A str that selects a subset of the data for evaluation\n\n        Returns:\n            A numpy array containing maximum hail samples for each forecast object.\n        \"\"\"\n        if query is not None:\n            dist_forecasts = self.matched_forecasts[\"dist\"][dist_model_name].query(query)\n            dist_forecasts = dist_forecasts.reset_index(drop=True)\n            condition_forecasts = self.matched_forecasts[\"condition\"][condition_model_name].query(query)\n            condition_forecasts = condition_forecasts.reset_index(drop=True)\n        else:\n            dist_forecasts = self.matched_forecasts[\"dist\"][dist_model_name]\n            condition_forecasts = self.matched_forecasts[\"condition\"][condition_model_name]\n        max_hail_samples = np.zeros((dist_forecasts.shape[0], num_samples))\n        areas = dist_forecasts[\"Area\"].values\n        for f in np.arange(dist_forecasts.shape[0]):\n            condition_prob = condition_forecasts.loc[f, self.forecast_bins[\"condition\"][0]]\n            if condition_prob >= condition_threshold:\n                max_hail_samples[f] = np.sort(gamma.rvs(*dist_forecasts.loc[f, self.forecast_bins[\"dist\"]].values,\n                                                        size=(num_samples, areas[f])).max(axis=1))\n        return max_hail_samples", "response": "Samples every forecast hail object and returns an empirical distribution of possible maximum hail sizes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting signature and params for the current object", "response": "def get_params(self):\n        \"\"\"Get signature and params\n        \"\"\"\n        params = {\n            'key': self.get_app_key(),\n            'uid': self.user_id,\n            'widget': self.widget_code\n        }\n\n        products_number = len(self.products)\n\n        if self.get_api_type() == self.API_GOODS:\n\n            if isinstance(self.products, list):\n\n                if products_number == 1:\n                    product = self.products[0]\n\n                    if isinstance(product, Product):\n                        post_trial_product = None\n\n                        if isinstance(product.get_trial_product(), Product):\n                            post_trial_product = product\n                            product = product.get_trial_product()\n\n                        params['amount'] = product.get_amount()\n                        params['currencyCode'] = product.get_currency_code()\n                        params['ag_name'] = product.get_name()\n                        params['ag_external_id'] = product.get_id()\n                        params['ag_type'] = product.get_type()\n\n                        if product.get_type() == Product.TYPE_SUBSCRIPTION:\n                            params['ag_period_length'] = product.get_period_length()\n                            params['ag_period_type'] = product.get_period_type()\n\n                            if product.is_recurring():\n                                params['ag_recurring'] = 1 if product.is_recurring() else 0\n\n                                if post_trial_product:\n                                    params['ag_trial'] = 1\n                                    params['ag_post_trial_external_id'] = post_trial_product.get_id()\n                                    params['ag_post_trial_period_length'] = post_trial_product.get_period_length()\n                                    params['ag_post_trial_period_type'] = post_trial_product.get_period_type()\n                                    params['ag_post_trial_name'] = post_trial_product.get_name()\n                                    params['post_trial_amount'] = post_trial_product.get_amount()\n                                    params['post_trial_currencyCode'] = post_trial_product.get_currency_code()\n                    else:\n                        self.append_to_errors('Not a Product instance')\n                else:\n                    self.append_to_errors('Only 1 product is allowed')\n\n        elif self.get_api_type() == self.API_CART:\n            index = 0\n\n            for product in self.products:\n                params['external_ids[' + str(index) + ']'] = product.get_id()\n                if product.get_amount() > 0:\n                    params['prices[' + str(index) + ']'] = product.get_amount()\n                if product.get_currency_code() != '' and product.get_currency_code() is not None:\n                    params['currencies[' + str(index) + ']'] = product.get_currency_code()\n                index += 1\n\n        params['sign_version'] = signature_version = str(self.get_default_widget_signature())\n\n        if not self.is_empty(self.extra_params, 'sign_version'):\n            signature_version = params['sign_version'] = str(self.extra_params['sign_version'])\n\n        params = self.array_merge(params, self.extra_params)\n\n        params['sign'] = self.calculate_signature(params, self.get_secret_key(), int(signature_version))\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tuple of integers representing a partir do total de segundos informado.", "response": "def hms(segundos):  # TODO: mover para util.py\n    \"\"\"\n    Retorna o n\u00famero de horas, minutos e segundos a partir do total de\n    segundos informado.\n\n    .. sourcecode:: python\n\n        >>> hms(1)\n        (0, 0, 1)\n\n        >>> hms(60)\n        (0, 1, 0)\n\n        >>> hms(3600)\n        (1, 0, 0)\n\n        >>> hms(3601)\n        (1, 0, 1)\n\n        >>> hms(3661)\n        (1, 1, 1)\n\n    :param int segundos: O n\u00famero total de segundos.\n\n    :returns: Uma tupla contendo tr\u1ebds elementos representando, respectivamente,\n        o n\u00famero de horas, minutos e segundos calculados a partir do total de\n        segundos.\n\n    :rtype: tuple\n    \"\"\"\n    h = (segundos / 3600)\n    m = (segundos - (3600 * h)) / 60\n    s = (segundos - (3600 * h) - (m * 60));\n    return (h, m, s)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hms_humanizado(segundos): # TODO: mover para util.py\n    p = lambda n, s, p: p if n > 1 else s\n    h, m, s = hms(segundos)\n\n    tokens = [\n            '' if h == 0 else '{:d} {}'.format(h, p(h, 'hora', 'horas')),\n            '' if m == 0 else '{:d} {}'.format(m, p(m, 'minuto', 'minutos')),\n            '' if s == 0 else '{:d} {}'.format(s, p(s, 'segundo', 'segundos'))]\n\n    tokens = [token for token in tokens if token]\n\n    if len(tokens) == 1:\n        return tokens[0]\n\n    if len(tokens) > 1:\n        return '{} e {}'.format(', '.join(tokens[:-1]), tokens[-1])\n\n    return 'zero segundos'", "response": "Descreve o total de horas minutos e segundos."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_grib_name(self, selected_variable):\n        names = self.unknown_names\n        units = self.unknown_units\n        for key, value in names.items():\n            if selected_variable == value:\n                Id = key\n                u = units[key]\n        return Id, u", "response": "Returns the grib2 message number and units of the given variable name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_data(self):\n        file_objects = self.file_objects\n        var = self.variable\n        valid_date = self.valid_dates\n        data = self.data\n        unknown_names = self.unknown_names\n        unknown_units = self.unknown_units\n        member = self.member\n        lat = self.lat\n        lon = self.lon\n      \n        if self.sector_ind_path:\n            inds_file = pd.read_csv(self.sector_ind_path+'sector_data_indices.csv') \n            inds = inds_file.loc[:,'indices']  \n        out_x = self.mapping_data[\"x\"]\n        \n        if not file_objects:\n            print()\n            print(\"No {0} model runs on {1}\".format(member,self.run_date))\n            print()\n            units = None\n            return self.data, units\n\n    \n        for f, file in enumerate(file_objects):\n            grib = pygrib.open(file)\n            if type(var) is int:\n                data_values = grib[var].values\n                #lat, lon = grib[var].latlons()\n                #proj = Proj(grib[var].projparams)\n                if grib[var].units == 'unknown':\n                    Id = grib[var].parameterNumber\n                    units = self.unknown_units[Id] \n                else:\n                    units = grib[var].units\n            elif type(var) is str:\n                if '_' in var:\n                    variable = var.split('_')[0]\n                    level = int(var.split('_')[1])\n                    if variable in unknown_names.values():\n                        Id, units = self.format_grib_name(variable)\n                        data_values = grib.select(parameterNumber=Id, level=level)[0].values\n                        #lat, lon =  grib.select(parameterNumber=Id, level=level)[0].latlons()\n                        #proj = Proj(grib.select(parameterNumber=Id, level=level)[0].projparams)\n\n                    else:\n                        data_values = grib.select(name=variable, level=level)[0].values\n                        units = grib.select(name=variable, level=level)[0].units\n                        #lat, lon  = grib.select(name=variable, level=level)[0].latlons()\n                        #proj = Proj(grib.select(name=variable, level=level)[0].projparams)\n                else:\n                    if var in unknown_names.values():\n                        Id, units = self.format_grib_name(var)\n                        data_values = grib.select(parameterNumber=Id)[0].values\n                        #lat, lon = grib.select(parameterNumber=Id)[0].latlons() \n                        #proj = Proj(grib.select(parameterNumber=Id)[0].projparams)\n\n                    elif len(grib.select(name=var)) > 1:\n                        raise NameError(\"Multiple '{0}' records found. Rename with level:'{0}_level'\".format(var))\n\n                    else:\n                        data_values = grib.select(name=var)[0].values\n                        units = grib.select(name=var)[0].units\n                        #lat, lon = grib.select(name=var)[0].latlons()\n                        #proj = Proj(grib.select(name=var)[0].projparams)\n\n            if data is None:\n                data = np.empty((len(valid_date), out_x.shape[0], out_x.shape[1]), dtype=float)\n                if self.sector_ind_path:\n                    data[f] = data_values[:].flatten()[inds].reshape(out_x.shape)\n                else:\n                    data[f]=data_values[:]\n            else:\n                if self.sector_ind_path:\n                    data[f] = data_values[:].flatten()[inds].reshape(out_x.shape)\n                else:\n                    data[f]=data_values[:]\n        \n        return data, units", "response": "Loads data from grib2 file objects or list of grib2 file objects. Handles specific variable names\n            and grib2 message numbers\n            and grib2 message numbers\n            and grib2 message numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_forecasts(self):\n        run_date_str = self.run_date.strftime(\"%Y%m%d\")\n        for model_name in self.model_names:\n            self.raw_forecasts[model_name] = {}\n            forecast_file = self.forecast_path + run_date_str + \"/\" + \\\n                model_name.replace(\" \", \"-\") + \"_hailprobs_{0}_{1}.nc\".format(self.ensemble_member, run_date_str)\n            forecast_obj = Dataset(forecast_file)\n            forecast_hours = forecast_obj.variables[\"forecast_hour\"][:]\n            valid_hour_indices = np.where((self.start_hour <= forecast_hours) & (forecast_hours <= self.end_hour))[0]\n            for size_threshold in self.size_thresholds:\n                self.raw_forecasts[model_name][size_threshold] = \\\n                    forecast_obj.variables[\"prob_hail_{0:02d}_mm\".format(size_threshold)][valid_hour_indices]\n            forecast_obj.close()", "response": "Load the forecast files into memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_window_forecasts(self):\n        for model_name in self.model_names:\n            self.window_forecasts[model_name] = {}\n            for size_threshold in self.size_thresholds:\n                self.window_forecasts[model_name][size_threshold] = \\\n                    np.array([self.raw_forecasts[model_name][size_threshold][sl].sum(axis=0)\n                              for sl in self.hour_windows])", "response": "Aggregate the forecasts within the specified time windows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading observations and masking grids.", "response": "def load_obs(self,  mask_threshold=0.5):\n        \"\"\"\n        Loads observations and masking grid (if needed).\n\n        :param mask_threshold: Values greater than the threshold are kept, others are masked.\n        :return:\n        \"\"\"\n        start_date = self.run_date + timedelta(hours=self.start_hour)\n        end_date = self.run_date + timedelta(hours=self.end_hour)\n        mrms_grid = MRMSGrid(start_date, end_date, self.mrms_variable, self.mrms_path)\n        mrms_grid.load_data()\n        if len(mrms_grid.data) > 0:\n            self.raw_obs[self.mrms_variable] = np.where(mrms_grid.data > 100, 100, mrms_grid.data)\n            self.window_obs[self.mrms_variable] = np.array([self.raw_obs[self.mrms_variable][sl].max(axis=0)\n                                                            for sl in self.hour_windows])\n            if self.obs_mask:\n                mask_grid = MRMSGrid(start_date, end_date, self.mask_variable, self.mrms_path)\n                mask_grid.load_data()\n                self.raw_obs[self.mask_variable] = np.where(mask_grid.data >= mask_threshold, 1, 0)\n                self.window_obs[self.mask_variable] = np.array([self.raw_obs[self.mask_variable][sl].max(axis=0)\n                                                               for sl in self.hour_windows])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse a dilation filter to grow positive observation areas by a specified number of grid points.", "response": "def dilate_obs(self, dilation_radius):\n        \"\"\"\n        Use a dilation filter to grow positive observation areas by a specified number of grid points\n\n        :param dilation_radius: Number of times to dilate the grid.\n        :return:\n        \"\"\"\n        for s in self.size_thresholds:\n            self.dilated_obs[s] = np.zeros(self.window_obs[self.mrms_variable].shape)\n            for t in range(self.dilated_obs[s].shape[0]):\n                self.dilated_obs[s][t][binary_dilation(self.window_obs[self.mrms_variable][t] >= s, iterations=dilation_radius)] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef roc_curves(self, prob_thresholds):\n        all_roc_curves = {}\n        for model_name in self.model_names:\n            all_roc_curves[model_name] = {}\n            for size_threshold in self.size_thresholds:\n                all_roc_curves[model_name][size_threshold] = {}\n                for h, hour_window in enumerate(self.hour_windows):\n                    hour_range = (hour_window.start, hour_window.stop)\n                    all_roc_curves[model_name][size_threshold][hour_range] = \\\n                        DistributedROC(prob_thresholds, 1)\n                    if self.obs_mask:\n                        all_roc_curves[model_name][size_threshold][hour_range].update(\n                            self.window_forecasts[model_name][size_threshold][h][\n                                self.window_obs[self.mask_variable][h] > 0],\n                            self.dilated_obs[size_threshold][h][self.window_obs[self.mask_variable][h] > 0]\n                        )\n                    else:\n                        all_roc_curves[model_name][size_threshold][hour_range].update(\n                            self.window_forecasts[model_name][size_threshold][h],\n                            self.dilated_obs[size_threshold][h]\n                        )\n        return all_roc_curves", "response": "Generate a dictionary of DistributedROC objects for each machine learning model size threshold and time window."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reliability_curves(self, prob_thresholds):\n        all_rel_curves = {}\n        for model_name in self.model_names:\n            all_rel_curves[model_name] = {}\n            for size_threshold in self.size_thresholds:\n                all_rel_curves[model_name][size_threshold] = {}\n                for h, hour_window in enumerate(self.hour_windows):\n                    hour_range = (hour_window.start, hour_window.stop)\n                    all_rel_curves[model_name][size_threshold][hour_range] = \\\n                        DistributedReliability(prob_thresholds, 1)\n                    if self.obs_mask:\n                        all_rel_curves[model_name][size_threshold][hour_range].update(\n                            self.window_forecasts[model_name][size_threshold][h][\n                                self.window_obs[self.mask_variable][h] > 0],\n                            self.dilated_obs[size_threshold][h][self.window_obs[self.mask_variable][h] > 0]\n                        )\n                    else:\n                        all_rel_curves[model_name][size_threshold][hour_range].update(\n                            self.window_forecasts[model_name][size_threshold][h],\n                            self.dilated_obs[size_threshold][h]\n                        )\n        return all_rel_curves", "response": "Return a dictionary of reliability curves for each machine learning model size threshold and time window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload map coordinates from netCDF or pickle file created by util. makeMapGrids.", "response": "def load_map_coordinates(map_file):\n    \"\"\"\n    Loads map coordinates from netCDF or pickle file created by util.makeMapGrids.\n\n    Args:\n        map_file: Filename for the file containing coordinate information.\n\n    Returns:\n        Latitude and longitude grids as numpy arrays.\n    \"\"\"\n    if map_file[-4:] == \".pkl\":\n        map_data = pickle.load(open(map_file))\n        lon = map_data['lon']\n        lat = map_data['lat']\n    else:\n        map_data = Dataset(map_file)\n        if \"lon\" in map_data.variables.keys():\n            lon = map_data.variables['lon'][:]\n            lat = map_data.variables['lat'][:]\n        else:\n            lon = map_data.variables[\"XLONG\"][0]\n            lat = map_data.variables[\"XLAT\"][0]\n    return lon, lat"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interpolate_mrms_day(start_date, variable, interp_type, mrms_path, map_filename, out_path):\n    try:\n        print(start_date, variable)\n        end_date = start_date + timedelta(hours=23)\n        mrms = MRMSGrid(start_date, end_date, variable, mrms_path)\n        if mrms.data is not None:\n            if map_filename[-3:] == \"map\":\n                mapping_data = make_proj_grids(*read_arps_map_file(map_filename))\n                mrms.interpolate_to_netcdf(mapping_data['lon'], mapping_data['lat'], out_path, interp_type=interp_type)\n            elif map_filename[-3:] == \"txt\":\n                mapping_data = make_proj_grids(*read_ncar_map_file(map_filename))\n                mrms.interpolate_to_netcdf(mapping_data[\"lon\"], mapping_data[\"lat\"], out_path, interp_type=interp_type)\n            else:\n                lon, lat = load_map_coordinates(map_filename)\n                mrms.interpolate_to_netcdf(lon, lat, out_path, interp_type=interp_type)\n    except Exception as e:\n        # This exception catches any errors when run in multiprocessing, prints the stack trace,\n        # and ends the process. Otherwise the process will stall.\n        print(traceback.format_exc())\n        raise e", "response": "This function interpolates hourly MRMS data to a specified latitude and lon grid and saves the interpolated grids to CF - compliant netCDF4 files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_data(self):\n        data = []\n        loaded_dates = []\n        loaded_indices = []\n        for t, timestamp in enumerate(self.all_dates):\n            date_str = timestamp.date().strftime(\"%Y%m%d\")\n            full_path = self.path_start + date_str + \"/\"\n            if self.variable in os.listdir(full_path):\n                full_path += self.variable + \"/\"\n                data_files = sorted(os.listdir(full_path))\n                file_dates = pd.to_datetime([d.split(\"_\")[-1][0:13] for d in data_files])\n                if timestamp in file_dates:\n                    data_file = data_files[np.where(timestamp==file_dates)[0][0]]\n                    print(full_path + data_file)\n                    if data_file[-2:] == \"gz\":\n                        subprocess.call([\"gunzip\", full_path + data_file])\n                        file_obj = Nio.open_file(full_path + data_file[:-3])\n                    else:\n                        file_obj = Nio.open_file(full_path + data_file)\n                    var_name = sorted(file_obj.variables.keys())[0]\n                    data.append(file_obj.variables[var_name][:])\n                    if self.lon is None:\n                        self.lon = file_obj.variables[\"lon_0\"][:]\n                        # Translates longitude values from 0:360 to -180:180\n                        if np.count_nonzero(self.lon > 180) > 0:\n                            self.lon -= 360\n                        self.lat = file_obj.variables[\"lat_0\"][:]\n                    file_obj.close()\n                    if data_file[-2:] == \"gz\":\n                        subprocess.call([\"gzip\", full_path + data_file[:-3]])\n                    else:\n                        subprocess.call([\"gzip\", full_path + data_file])\n                    loaded_dates.append(timestamp)\n                    loaded_indices.append(t)\n        if len(loaded_dates) > 0:\n            self.loaded_dates = pd.DatetimeIndex(loaded_dates)\n            self.data = np.ones((self.all_dates.shape[0], data[0].shape[0], data[0].shape[1])) * -9999\n            self.data[loaded_indices] = np.array(data)", "response": "Loads data from MRMS GRIB2 files and handles compression duties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate_grid(self, in_lon, in_lat):\n        out_data = np.zeros((self.data.shape[0], in_lon.shape[0], in_lon.shape[1]))\n        for d in range(self.data.shape[0]):\n            print(\"Loading \", d, self.variable, self.start_date)\n            if self.data[d].max() > -999:\n                step = self.data[d]\n                step[step < 0] = 0\n                if self.lat[-1] < self.lat[0]:\n                    spline = RectBivariateSpline(self.lat[::-1], self.lon, step[::-1], kx=3, ky=3)\n                else:\n                    spline = RectBivariateSpline(self.lat, self.lon, step, kx=3, ky=3)\n                print(\"Evaluating\", d, self.variable, self.start_date)\n                flat_data = spline.ev(in_lat.ravel(), in_lon.ravel())\n                out_data[d] = flat_data.reshape(in_lon.shape)\n                del spline\n            else:\n                print(d, \" is missing\")\n                out_data[d] = -9999\n        return out_data", "response": "Interpolates MRMS data to a different grid using cubic bivariate splines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the largest value within a given radius of a point on the interpolated grid. Args: in_lon: 2D array of longitude values in_lat: 2D array of latitude values radius: radius of influence for largest neighbor search in degrees Returns: Array of interpolated data", "response": "def max_neighbor(self, in_lon, in_lat, radius=0.05):\n        \"\"\"\n        Finds the largest value within a given radius of a point on the interpolated grid.\n\n        Args:\n            in_lon: 2D array of longitude values\n            in_lat: 2D array of latitude values\n            radius: radius of influence for largest neighbor search in degrees\n\n        Returns:\n            Array of interpolated data\n        \"\"\"\n        out_data = np.zeros((self.data.shape[0], in_lon.shape[0], in_lon.shape[1]))\n        in_tree = cKDTree(np.vstack((in_lat.ravel(), in_lon.ravel())).T)\n        out_indices = np.indices(out_data.shape[1:])\n        out_rows = out_indices[0].ravel()\n        out_cols = out_indices[1].ravel()\n        for d in range(self.data.shape[0]):\n            nz_points = np.where(self.data[d] > 0)\n            if len(nz_points[0]) > 0:\n                nz_vals = self.data[d][nz_points]\n                nz_rank = np.argsort(nz_vals)\n                original_points = cKDTree(np.vstack((self.lat[nz_points[0][nz_rank]], self.lon[nz_points[1][nz_rank]])).T)\n                all_neighbors = original_points.query_ball_tree(in_tree, radius, p=2, eps=0)\n                for n, neighbors in enumerate(all_neighbors):\n                    if len(neighbors) > 0:\n                        out_data[d, out_rows[neighbors], out_cols[neighbors]] = nz_vals[nz_rank][n]\n        return out_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterpolates the MRMS data to a netCDF file.", "response": "def interpolate_to_netcdf(self, in_lon, in_lat, out_path, date_unit=\"seconds since 1970-01-01T00:00\",\n                              interp_type=\"spline\"):\n        \"\"\"\n        Calls the interpolation function and then saves the MRMS data to a netCDF file. It will also create \n        separate directories for each variable if they are not already available.\n        \"\"\"\n        if interp_type == \"spline\":\n            out_data = self.interpolate_grid(in_lon, in_lat)\n        else:\n            out_data = self.max_neighbor(in_lon, in_lat)\n        if not os.access(out_path + self.variable, os.R_OK):\n            try:\n                os.mkdir(out_path + self.variable)\n            except OSError:\n                print(out_path + self.variable + \" already created\")\n        out_file = out_path + self.variable + \"/\" + \"{0}_{1}_{2}.nc\".format(self.variable,\n                                                                            self.start_date.strftime(\"%Y%m%d-%H:%M\"),\n                                                                            self.end_date.strftime(\"%Y%m%d-%H:%M\"))\n        out_obj = Dataset(out_file, \"w\")\n        out_obj.createDimension(\"time\", out_data.shape[0])\n        out_obj.createDimension(\"y\", out_data.shape[1])\n        out_obj.createDimension(\"x\", out_data.shape[2])\n        data_var = out_obj.createVariable(self.variable, \"f4\", (\"time\", \"y\", \"x\"), zlib=True, \n                                          fill_value=-9999.0,\n                                          least_significant_digit=3)\n        data_var[:] = out_data\n        data_var.long_name = self.variable\n        data_var.coordinates = \"latitude longitude\"\n        if \"MESH\" in self.variable or \"QPE\" in self.variable:\n            data_var.units = \"mm\"\n        elif \"Reflectivity\" in self.variable:\n            data_var.units = \"dBZ\"\n        elif \"Rotation\" in self.variable:\n            data_var.units = \"s-1\"\n        else:\n            data_var.units = \"\"\n        out_lon = out_obj.createVariable(\"longitude\", \"f4\", (\"y\", \"x\"), zlib=True)\n        out_lon[:] = in_lon\n        out_lon.units = \"degrees_east\"\n        out_lat = out_obj.createVariable(\"latitude\", \"f4\", (\"y\", \"x\"), zlib=True)\n        out_lat[:] = in_lat\n        out_lat.units = \"degrees_north\"\n        dates = out_obj.createVariable(\"time\", \"i8\", (\"time\",), zlib=True)\n        dates[:] = np.round(date2num(self.all_dates.to_pydatetime(), date_unit)).astype(np.int64)\n        dates.long_name = \"Valid date\"\n        dates.units = date_unit\n        out_obj.Conventions=\"CF-1.6\"\n        out_obj.close()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_data_generator_by_id(hardware_source_id, sync=True):\n    hardware_source = HardwareSourceManager().get_hardware_source_for_hardware_source_id(hardware_source_id)\n    def get_last_data():\n        return hardware_source.get_next_xdatas_to_finish()[0].data.copy()\n    yield get_last_data", "response": "Return a generator for data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the config file for aliases and automatically register them.", "response": "def parse_hardware_aliases_config_file(config_path):\n    \"\"\"\n        Parse config file for aliases and automatically register them.\n\n        Returns True if alias file was found and parsed (successfully or unsuccessfully).\n\n        Returns False if alias file was not found.\n\n        Config file is a standard .ini file with a section\n    \"\"\"\n    if os.path.exists(config_path):\n        logging.info(\"Parsing alias file {:s}\".format(config_path))\n        try:\n            config = configparser.ConfigParser()\n            config.read(config_path)\n            for section in config.sections():\n                device = config.get(section, \"device\")\n                hardware_alias = config.get(section, \"hardware_alias\")\n                display_name = config.get(section, \"display_name\")\n                try:\n                    logging.info(\"Adding alias {:s} for device {:s}, display name: {:s} \".format(hardware_alias, device, display_name))\n                    HardwareSourceManager().make_instrument_alias(device, hardware_alias, _(display_name))\n                except Exception as e:\n                    logging.info(\"Error creating hardware alias {:s} for device {:s} \".format(hardware_alias, device))\n                    logging.info(traceback.format_exc())\n        except Exception as e:\n            logging.info(\"Error reading alias file from: \" + config_path)\n            logging.info(traceback.format_exc())\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_instrument_alias(self, instrument_id, alias_instrument_id, display_name):\n        self.__aliases[alias_instrument_id] = (instrument_id, display_name)\n        for f in self.aliases_updated:\n            f()", "response": "Configure an alias.\n\n            Callers can use the alias to refer to the instrument or hardware source.\n            The alias should be lowercase, no spaces. The display name may be used to display alias to\n            the user. Neither the original instrument or hardware source id and the alias id should ever\n            be visible to end users.\n\n            :param str instrument_id: the hardware source id (lowercase, no spaces)\n            :param str alias_instrument_id: the alias of the hardware source id (lowercase, no spaces)\n            :param str display_name: the display name for the alias"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall from hardware source when new data arrives.", "response": "def update(self, data_and_metadata: DataAndMetadata.DataAndMetadata, state: str, sub_area, view_id) -> None:\n        \"\"\"Called from hardware source when new data arrives.\"\"\"\n        self.__state = state\n        self.__sub_area = sub_area\n\n        hardware_source_id = self.__hardware_source.hardware_source_id\n        channel_index = self.index\n        channel_id = self.channel_id\n        channel_name = self.name\n        metadata = copy.deepcopy(data_and_metadata.metadata)\n        hardware_source_metadata = dict()\n        hardware_source_metadata[\"hardware_source_id\"] = hardware_source_id\n        hardware_source_metadata[\"channel_index\"] = channel_index\n        if channel_id is not None:\n            hardware_source_metadata[\"reference_key\"] = \"_\".join([hardware_source_id, channel_id])\n            hardware_source_metadata[\"channel_id\"] = channel_id\n        else:\n            hardware_source_metadata[\"reference_key\"] = hardware_source_id\n        if channel_name is not None:\n            hardware_source_metadata[\"channel_name\"] = channel_name\n        if view_id:\n            hardware_source_metadata[\"view_id\"] = view_id\n        metadata.setdefault(\"hardware_source\", dict()).update(hardware_source_metadata)\n\n        data = data_and_metadata.data\n        master_data = self.__data_and_metadata.data if self.__data_and_metadata else None\n        data_matches = master_data is not None and data.shape == master_data.shape and data.dtype == master_data.dtype\n        if data_matches and sub_area is not None:\n            top = sub_area[0][0]\n            bottom = sub_area[0][0] + sub_area[1][0]\n            left = sub_area[0][1]\n            right = sub_area[0][1] + sub_area[1][1]\n            if top > 0 or left > 0 or bottom < data.shape[0] or right < data.shape[1]:\n                master_data = numpy.copy(master_data)\n                master_data[top:bottom, left:right] = data[top:bottom, left:right]\n            else:\n                master_data = numpy.copy(data)\n        else:\n            master_data = data  # numpy.copy(data). assume data does not need a copy.\n\n        data_descriptor = data_and_metadata.data_descriptor\n        intensity_calibration = data_and_metadata.intensity_calibration if data_and_metadata else None\n        dimensional_calibrations = data_and_metadata.dimensional_calibrations if data_and_metadata else None\n        timestamp = data_and_metadata.timestamp\n        new_extended_data = DataAndMetadata.new_data_and_metadata(master_data, intensity_calibration=intensity_calibration, dimensional_calibrations=dimensional_calibrations, metadata=metadata, timestamp=timestamp, data_descriptor=data_descriptor)\n\n        self.__data_and_metadata = new_extended_data\n\n        self.data_channel_updated_event.fire(new_extended_data)\n        self.is_dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self):\n        old_start_count = self.__start_count\n        self.__start_count += 1\n        if old_start_count == 0:\n            self.data_channel_start_event.fire()", "response": "Called from hardware source when data starts streaming."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to the data item reference.", "response": "def connect_data_item_reference(self, data_item_reference):\n        \"\"\"Connect to the data item reference, creating a crop graphic if necessary.\n\n        If the data item reference does not yet have an associated data item, add a\n        listener and wait for the data item to be set, then connect.\n        \"\"\"\n        display_item = data_item_reference.display_item\n        data_item = display_item.data_item if display_item else None\n        if data_item and display_item:\n            self.__connect_display(display_item)\n        else:\n            def data_item_reference_changed():\n                self.__data_item_reference_changed_event_listener.close()\n                self.connect_data_item_reference(data_item_reference)  # ugh. recursive mess.\n            self.__data_item_reference_changed_event_listener = data_item_reference.data_item_reference_changed_event.listen(data_item_reference_changed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grab_earliest(self, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        timeout = timeout if timeout is not None else 10.0\n        with self.__buffer_lock:\n            if len(self.__buffer) == 0:\n                done_event = threading.Event()\n                self.__done_events.append(done_event)\n                self.__buffer_lock.release()\n                done = done_event.wait(timeout)\n                self.__buffer_lock.acquire()\n                if not done:\n                    raise Exception(\"Could not grab latest.\")\n            return self.__buffer.pop(0)", "response": "Grab the earliest data from the buffer blocking until one is available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grab_next(self, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        with self.__buffer_lock:\n            self.__buffer = list()\n        return self.grab_latest(timeout)", "response": "Grab the next data from the buffer blocking until one is available."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngrab the next data to start from the buffer blocking until one is available.", "response": "def grab_following(self, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        \"\"\"Grab the next data to start from the buffer, blocking until one is available.\"\"\"\n        self.grab_next(timeout)\n        return self.grab_next(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pause(self) -> None:\n        with self.__state_lock:\n            if self.__state == DataChannelBuffer.State.started:\n                self.__state = DataChannelBuffer.State.paused", "response": "Pause recording.\n\n        Thread safe and UI safe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resume(self) -> None:\n        with self.__state_lock:\n            if self.__state == DataChannelBuffer.State.paused:\n                self.__state = DataChannelBuffer.State.started", "response": "Resume recording after pause."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of n keys associated with the largest values in descending order.", "response": "def nlargest(n, mapping):\n    \"\"\"\n    Takes a mapping and returns the n keys associated with the largest values\n    in descending order. If the mapping has fewer than n items, all its keys\n    are returned.\n\n    Equivalent to:\n        ``next(zip(*heapq.nlargest(mapping.items(), key=lambda x: x[1])))``\n\n    Returns\n    -------\n    list of up to n keys from the mapping\n\n    \"\"\"\n    try:\n        it = mapping.iteritems()\n    except AttributeError:\n        it = iter(mapping.items())\n    pq = minpq()\n    try:\n        for i in range(n):\n            pq.additem(*next(it))\n    except StopIteration:\n        pass\n    try:\n        while it:\n            pq.pushpopitem(*next(it))\n    except StopIteration:\n        pass\n    out = list(pq.popkeys())\n    out.reverse()\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fromkeys(cls, iterable, value, **kwargs):\n        return cls(((k, value) for k in iterable), **kwargs)", "response": "Return a pqict mapping keys from an iterable to the same value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self):\n        return self.__class__(self, key=self._keyfn, precedes=self._precedes)", "response": "Return a shallow copy of a pqdict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving and return the highest priority item from pqdict. Raises KeyError if pqdict is empty.", "response": "def popitem(self):\n        \"\"\"\n        Remove and return the item with highest priority. Raises ``KeyError``\n        if pqdict is empty.\n\n        \"\"\"\n        heap = self._heap\n        position = self._position\n\n        try:\n            end = heap.pop(-1)\n        except IndexError:\n            raise KeyError('pqdict is empty')\n\n        if heap:\n            node = heap[0]\n            heap[0] = end\n            position[end.key] = 0\n            self._sink(0)\n        else:\n            node = end\n        del position[node.key]\n        return node.key, node.value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef topitem(self):\n        try:\n            node = self._heap[0]\n        except IndexError:\n            raise KeyError('pqdict is empty')\n        return node.key, node.value", "response": "Return the highest priority item in pqdict. Raises KeyError if pqdict is empty."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new item to the queue. Raises KeyError if the key is already in the pqdict.", "response": "def additem(self, key, value):\n        \"\"\"\n        Add a new item. Raises ``KeyError`` if key is already in the pqdict.\n\n        \"\"\"\n        if key in self._position:\n            raise KeyError('%s is already in the queue' % repr(key))\n        self[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the priority value of an existing item. Raises KeyError if the key is not in the pqdict.", "response": "def updateitem(self, key, new_val):\n        \"\"\"\n        Update the priority value of an existing item. Raises ``KeyError`` if\n        key is not in the pqdict.\n\n        \"\"\"\n        if key not in self._position:\n            raise KeyError(key)\n        self[key] = new_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replace_key(self, key, new_key):\n        heap = self._heap\n        position = self._position\n        if new_key in self:\n            raise KeyError('%s is already in the queue' % repr(new_key))\n        pos = position.pop(key)  # raises appropriate KeyError\n        position[new_key] = pos\n        heap[pos].key = new_key", "response": "Replaces the key in the existing heap node in place. Raises KeyError if the key is already in the pqdict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef swap_priority(self, key1, key2):\n        heap = self._heap\n        position = self._position\n        if key1 not in self or key2 not in self:\n            raise KeyError\n        pos1, pos2 = position[key1], position[key2]\n        heap[pos1].key, heap[pos2].key = key2, key1\n        position[key1], position[key2] = pos2, pos1", "response": "Swap the priority level of two items in the pqdict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrepair a broken heap.", "response": "def heapify(self, key=__marker):\n        \"\"\"\n        Repair a broken heap. If the state of an item's priority value changes\n        you can re-sort the relevant item only by providing ``key``.\n\n        \"\"\"\n        if key is self.__marker:\n            n = len(self._heap)\n            for pos in reversed(range(n//2)):\n                self._sink(pos)\n        else:\n            try:\n                pos = self._position[key]\n            except KeyError:\n                raise KeyError(key)\n            self._reheapify(pos)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking to make sure _version. py is contained in the package.", "response": "def package_has_version_file(package_name):\n    \"\"\" Check to make sure _version.py is contained in the package \"\"\"\n    version_file_path = helpers.package_file_path('_version.py', package_name)\n    return os.path.isfile(version_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngrab the project name out of setup. py", "response": "def get_project_name():\n    \"\"\" Grab the project name out of setup.py \"\"\"\n    setup_py_content = helpers.get_file_content('setup.py')\n    ret = helpers.value_of_named_argument_in_function(\n        'name', 'setup', setup_py_content, resolve_varname=True\n    )\n    if ret and ret[0] == ret[-1] in ('\"', \"'\"):\n        ret = ret[1:-1]\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_version(package_name, ignore_cache=False):\n    if ignore_cache:\n        with microcache.temporarily_disabled():\n            found = helpers.regex_in_package_file(\n                VERSION_SET_REGEX, '_version.py', package_name, return_match=True\n            )\n    else:\n        found = helpers.regex_in_package_file(\n            VERSION_SET_REGEX, '_version.py', package_name, return_match=True\n        )\n    if found is None:\n        raise ProjectError('found {}, but __version__ is not defined')\n    current_version = found['version']\n    return current_version", "response": "Get the version which is currently configured by the package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the version in _version. py to version_str", "response": "def set_version(package_name, version_str):\n    \"\"\" Set the version in _version.py to version_str \"\"\"\n    current_version = get_version(package_name)\n    version_file_path = helpers.package_file_path('_version.py', package_name)\n    version_file_content = helpers.get_file_content(version_file_path)\n    version_file_content = version_file_content.replace(current_version, version_str)\n    with open(version_file_path, 'w') as version_file:\n        version_file.write(version_file_content)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck to see if the version specified is a valid version", "response": "def version_is_valid(version_str):\n    \"\"\" Check to see if the version specified is a valid as far as pkg_resources is concerned\n\n    >>> version_is_valid('blah')\n    False\n    >>> version_is_valid('1.2.3')\n    True\n    \"\"\"\n    try:\n        packaging.version.Version(version_str)\n    except packaging.version.InvalidVersion:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_uploaded_versions_warehouse(project_name, index_url, requests_verify=True):\n    url = '/'.join((index_url, project_name, 'json'))\n    response = requests.get(url, verify=requests_verify)\n    if response.status_code == 200:\n        return response.json()['releases'].keys()\n    return None", "response": "Query the pypi index at index_url to find all of the releases"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying the pypi index at index_url to find all versions of the specified project", "response": "def _get_uploaded_versions_pypicloud(project_name, index_url, requests_verify=True):\n    \"\"\" Query the pypi index at index_url using pypicloud api to find all versions \"\"\"\n    api_url = index_url\n    for suffix in ('/pypi', '/pypi/', '/simple', '/simple/'):\n        if api_url.endswith(suffix):\n            api_url = api_url[:len(suffix) * -1] + '/api/package'\n            break\n    url = '/'.join((api_url, project_name))\n    response = requests.get(url, verify=requests_verify)\n    if response.status_code == 200:\n        return [p['version'] for p in response.json()['packages']]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck to see if the version specified has already been uploaded to the index", "response": "def version_already_uploaded(project_name, version_str, index_url, requests_verify=True):\n    \"\"\" Check to see if the version specified has already been uploaded to the configured index\n    \"\"\"\n    all_versions = _get_uploaded_versions(project_name, index_url, requests_verify)\n    return version_str in all_versions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to convert a README. md file into README. rst", "response": "def convert_readme_to_rst():\n    \"\"\" Attempt to convert a README.md file into README.rst \"\"\"\n    project_files = os.listdir('.')\n    for filename in project_files:\n        if filename.lower() == 'readme':\n            raise ProjectError(\n                'found {} in project directory...'.format(filename) +\n                'not sure what to do with it, refusing to convert'\n            )\n        elif filename.lower() == 'readme.rst':\n            raise ProjectError(\n                'found {} in project directory...'.format(filename) +\n                'refusing to overwrite'\n            )\n    for filename in project_files:\n        if filename.lower() == 'readme.md':\n            rst_filename = 'README.rst'\n            logger.info('converting {} to {}'.format(filename, rst_filename))\n            try:\n                rst_content = pypandoc.convert(filename, 'rst')\n                with open('README.rst', 'w') as rst_file:\n                    rst_file.write(rst_content)\n                return\n            except OSError as e:\n                raise ProjectError(\n                    'could not convert readme to rst due to pypandoc error:' + os.linesep + str(e)\n                )\n    raise ProjectError('could not find any README.md file to convert')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects all files which have already been packaged", "response": "def get_packaged_files(package_name):\n    \"\"\" Collect relative paths to all files which have already been packaged \"\"\"\n    if not os.path.isdir('dist'):\n        return []\n    return [os.path.join('dist', filename) for filename in os.listdir('dist')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if there are multiple packaged versions of a given package.", "response": "def multiple_packaged_versions(package_name):\n    \"\"\" Look through built package directory and see if there are multiple versions there \"\"\"\n    dist_files = os.listdir('dist')\n    versions = set()\n    for filename in dist_files:\n        version = funcy.re_find(r'{}-(.+).tar.gz'.format(package_name), filename)\n        if version:\n            versions.add(version)\n    return len(versions) > 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the neighborhood probability over the full period of the forecast", "response": "def period_neighborhood_probability(self, radius, smoothing, threshold, stride,start_time,end_time):\n        \"\"\"\n        Calculate the neighborhood probability over the full period of the forecast\n\n        Args:\n            radius: circular radius from each point in km\n            smoothing: width of Gaussian smoother in km\n            threshold: intensity of exceedance\n            stride: number of grid points to skip for reduced neighborhood grid\n\n        Returns:\n            (neighborhood probabilities)\n        \"\"\"\n        neighbor_x = self.x[::stride, ::stride]\n        neighbor_y = self.y[::stride, ::stride]\n        neighbor_kd_tree = cKDTree(np.vstack((neighbor_x.ravel(), neighbor_y.ravel())).T)\n        neighbor_prob = np.zeros((self.data.shape[0], neighbor_x.shape[0], neighbor_x.shape[1]))\n        print('Forecast Hours: {0}-{1}'.format(start_time, end_time))\n        for m in range(len(self.members)):\n            period_max = self.data[m,start_time:end_time,:,:].max(axis=0)\n            valid_i, valid_j = np.where(period_max >= threshold)\n            print(self.members[m], len(valid_i))\n            if len(valid_i) > 0:\n                var_kd_tree = cKDTree(np.vstack((self.x[valid_i, valid_j], self.y[valid_i, valid_j])).T)\n                exceed_points = np.unique(np.concatenate(var_kd_tree.query_ball_tree(neighbor_kd_tree, radius))).astype(int)\n                exceed_i, exceed_j = np.unravel_index(exceed_points, neighbor_x.shape)\n                neighbor_prob[m][exceed_i, exceed_j] = 1\n                if smoothing > 0:\n                    neighbor_prob[m] = gaussian_filter(neighbor_prob[m], smoothing,mode='constant')\n        return neighbor_prob"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analisar(retorno):\n        if '|' not in retorno:\n            raise ErroRespostaSATInvalida('Resposta nao possui pipes '\n                    'separando os campos: {!r}'.format(retorno))\n\n        resposta = _RespostaParcial(*(retorno.split('|')[:2]))\n\n        for faixa, construtor in _RESPOSTAS_POSSIVEIS:\n            if int(resposta.EEEEE) in xrange(faixa, faixa+1000):\n                return construtor(retorno)\n\n        return RespostaConsultarNumeroSessao._pos_analise(retorno)", "response": "Analisar uma ID de un objeto resposta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef comunicar_certificado_icpbrasil(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='ComunicarCertificadoICPBRASIL')\n        if resposta.EEEEE not in ('05000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Comunicar certificado o ICPBRASIL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configurar_interface_de_rede(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='ConfigurarInterfaceDeRede')\n        if resposta.EEEEE not in ('12000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Constr\u00f3i uma retorno de uma tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef atualizar_software_sat(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='AtualizarSoftwareSAT')\n        if resposta.EEEEE not in ('14000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Atualizar a settura seu retorno."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef desbloquear_sat(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='DesbloquearSAT')\n        if resposta.EEEEE not in ('17000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Constr\u00f3i uma :class:`RespostaSAT` para o retorno (unicode) da fun\u00e7\u00e3o\n        :meth:`~satcfe.base.FuncoesSAT.desbloquear_sat`."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trocar_codigo_de_ativacao(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='TrocarCodigoDeAtivacao')\n        if resposta.EEEEE not in ('18000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Retorno para uma string para uma trocar_codigo_de_ativacao."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the variable from the ensemble files then close the files.", "response": "def load_data(self):\n        \"\"\"\n        Load the specified variable from the ensemble files, then close the files.\n        \"\"\"\n        if self.ensemble_name.upper() == \"SSEF\":\n            if self.variable[0:2] == \"rh\":\n                pressure_level = self.variable[2:]\n                relh_vars = [\"sph\", \"tmp\"]\n                relh_vals = {}\n                for var in relh_vars:\n                    mg = SSEFModelGrid(self.member_name,\n                                       self.run_date,\n                                       var + pressure_level,\n                                       self.start_date,\n                                       self.end_date,\n                                       self.path,\n                                       single_step=self.single_step)\n                    relh_vals[var], units = mg.load_data()\n                    mg.close()\n                self.data = relative_humidity_pressure_level(relh_vals[\"tmp\"],\n                                                             relh_vals[\"sph\"],\n                                                             float(pressure_level) * 100)\n                self.units = \"%\"\n            elif self.variable == \"melth\":\n                input_vars = [\"hgtsfc\", \"hgt700\", \"hgt500\", \"tmp700\", \"tmp500\"]\n                input_vals = {}\n                for var in input_vars:\n                    mg = SSEFModelGrid(self.member_name,\n                                       self.run_date,\n                                       var,\n                                       self.start_date,\n                                       self.end_date,\n                                       self.path,\n                                       single_step=self.single_step)\n                    input_vals[var], units = mg.load_data()\n                    mg.close()\n                self.data = melting_layer_height(input_vals[\"hgtsfc\"],\n                                                 input_vals[\"hgt700\"],\n                                                 input_vals[\"hgt500\"],\n                                                 input_vals[\"tmp700\"],\n                                                 input_vals[\"tmp500\"])\n                self.units = \"m\"\n            else:\n                mg = SSEFModelGrid(self.member_name,\n                                   self.run_date,\n                                   self.variable,\n                                   self.start_date,\n                                   self.end_date,\n                                   self.path,\n                                   single_step=self.single_step)\n                self.data, self.units = mg.load_data()\n                mg.close()\n        elif self.ensemble_name.upper() == \"NCAR\":\n            mg = NCARModelGrid(self.member_name,\n                               self.run_date,\n                               self.variable,\n                               self.start_date,\n                               self.end_date,\n                               self.path,\n                               single_step=self.single_step)\n            self.data, self.units = mg.load_data()\n            mg.close()\n        elif self.ensemble_name.upper() == \"HREFV2\":\n            proj_dict, grid_dict = read_ncar_map_file(self.map_file)\n            mapping_data = make_proj_grids(proj_dict, grid_dict)            \n           \n            mg = HREFv2ModelGrid(self.member_name,\n                               self.run_date,\n                               self.variable,\n                               self.start_date,\n                               self.end_date,\n                               self.path,\n                               mapping_data,\n                               self.sector_ind_path,\n                               single_step=self.single_step)\n\n            self.data, self.units = mg.load_data()\n\n        elif self.ensemble_name.upper() == \"VSE\":\n            mg = VSEModelGrid(self.member_name,\n                               self.run_date,\n                               self.variable,\n                               self.start_date,\n                               self.end_date,\n                               self.path,\n                               single_step=self.single_step)\n            self.data, self.units = mg.load_data()\n            mg.close()\n        elif self.ensemble_name.upper() == \"HRRR\":\n            mg = HRRRModelGrid(self.run_date,\n                               self.variable,\n                               self.start_date,\n                               self.end_date,\n                               self.path)\n            self.data, self.units = mg.load_data()\n            mg.close()\n        elif self.ensemble_name.upper() == \"NCARSTORM\":\n            mg = NCARStormEventModelGrid(self.run_date,\n                                         self.variable,\n                                         self.start_date,\n                                         self.end_date,\n                                         self.path)\n            self.data, self.units = mg.load_data()\n            mg.close()\n        else:\n            print(self.ensemble_name + \" not supported.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_map_info(self, map_file):\n        if self.ensemble_name.upper() == \"SSEF\":\n            proj_dict, grid_dict = read_arps_map_file(map_file)\n            self.dx = int(grid_dict[\"dx\"])\n            mapping_data = make_proj_grids(proj_dict, grid_dict)\n            for m, v in mapping_data.items():\n                setattr(self, m, v)\n            self.i, self.j = np.indices(self.lon.shape)\n            self.proj = get_proj_obj(proj_dict)\n        elif self.ensemble_name.upper() in [\"NCAR\", \"NCARSTORM\", \"HRRR\", \"VSE\", \"HREFV2\"]:\n            proj_dict, grid_dict = read_ncar_map_file(map_file)\n            if self.member_name[0:7] == \"1km_pbl\": # Don't just look at the first 3 characters. You have to differentiate '1km_pbl1' and '1km_on_3km_pbl1'\n                grid_dict[\"dx\"] = 1000\n                grid_dict[\"dy\"] = 1000\n                grid_dict[\"sw_lon\"] = 258.697\n                grid_dict[\"sw_lat\"] = 23.999\n                grid_dict[\"ne_lon\"] = 282.868269206236\n                grid_dict[\"ne_lat\"] = 36.4822338520542 \n\n            self.dx = int(grid_dict[\"dx\"])\n            mapping_data = make_proj_grids(proj_dict, grid_dict)\n            for m, v in mapping_data.items():\n                setattr(self, m, v)\n            self.i, self.j = np.indices(self.lon.shape)\n            self.proj = get_proj_obj(proj_dict)", "response": "Load map projection information and create latitude longitude x y i and j grids for the projection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a geojson file containing an STObject and initializes a new STObject from the information in the file.", "response": "def read_geojson(filename):\n    \"\"\"\n    Reads a geojson file containing an STObject and initializes a new STObject from the information in the file.\n\n    Args:\n        filename: Name of the geojson file\n\n    Returns:\n        an STObject\n    \"\"\"\n    json_file = open(filename)\n    data = json.load(json_file)\n    json_file.close()\n    times = data[\"properties\"][\"times\"]\n    main_data = dict(timesteps=[], masks=[], x=[], y=[], i=[], j=[])\n    attribute_data = dict()\n    for feature in data[\"features\"]:\n        for main_name in main_data.keys():\n            main_data[main_name].append(np.array(feature[\"properties\"][main_name]))\n        for k, v in feature[\"properties\"][\"attributes\"].items():\n            if k not in attribute_data.keys():\n                attribute_data[k] = [np.array(v)]\n            else:\n                attribute_data[k].append(np.array(v))\n    kwargs = {}\n    for kw in [\"dx\", \"step\", \"u\", \"v\"]:\n        if kw in data[\"properties\"].keys():\n            kwargs[kw] = data[\"properties\"][kw]\n    sto = STObject(main_data[\"timesteps\"], main_data[\"masks\"], main_data[\"x\"], main_data[\"y\"],\n                   main_data[\"i\"], main_data[\"j\"], times[0], times[-1], **kwargs)\n    for k, v in attribute_data.items():\n        sto.attributes[k] = v\n    return sto"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the center of mass at a given timestep.", "response": "def center_of_mass(self, time):\n        \"\"\"\n        Calculate the center of mass at a given timestep.\n\n        Args:\n            time: Time at which the center of mass calculation is performed\n\n        Returns:\n            The x- and y-coordinates of the center of mass.\n        \"\"\"\n        if self.start_time <= time <= self.end_time:\n            diff = time - self.start_time\n            valid = np.flatnonzero(self.masks[diff] != 0)\n            if valid.size > 0:\n                com_x = 1.0 / self.timesteps[diff].ravel()[valid].sum() * np.sum(self.timesteps[diff].ravel()[valid] *\n                                                                                 self.x[diff].ravel()[valid])\n                com_y = 1.0 / self.timesteps[diff].ravel()[valid].sum() * np.sum(self.timesteps[diff].ravel()[valid] *\n                                                                                 self.y[diff].ravel()[valid])\n            else:\n                com_x = np.mean(self.x[diff])\n                com_y = np.mean(self.y[diff])\n        else:\n            com_x = None\n            com_y = None\n        return com_x, com_y"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trajectory(self):\n        traj = np.zeros((2, self.times.size))\n        for t, time in enumerate(self.times):\n            traj[:, t] = self.center_of_mass(time)\n        return traj", "response": "Calculates the center of mass for each time step and outputs an array"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_corner(self, time):\n        if self.start_time <= time <= self.end_time:\n            diff = time - self.start_time\n            return self.i[diff][0, 0], self.j[diff][0, 0]\n        else:\n            return -1, -1", "response": "Gets the array indices of the STObject at a given time that corresponds \n        to the upper left corner of the bounding box for the STObject."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef size(self, time):\n        if self.start_time <= time <= self.end_time:\n            return self.masks[time - self.start_time].sum()\n        else:\n            return 0", "response": "Gets the size of the object at a given time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef max_size(self):\n        sizes = np.array([m.sum() for m in self.masks])\n        return sizes.max()", "response": "Gets the largest size of the object over all timesteps."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef max_intensity(self, time):\n        ti = np.where(time == self.times)[0][0]\n        return self.timesteps[ti].max()", "response": "Calculate the maximum intensity found at a timestep."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the data from another STObject to this object.", "response": "def extend(self, step):\n        \"\"\"\n        Adds the data from another STObject to this object.\n        \n        Args:\n            step: another STObject being added after the current one in time.\n        \"\"\"\n        self.timesteps.extend(step.timesteps)\n        self.masks.extend(step.masks)\n        self.x.extend(step.x)\n        self.y.extend(step.y)\n        self.i.extend(step.i)\n        self.j.extend(step.j)\n        self.end_time = step.end_time\n        self.times = np.arange(self.start_time, self.end_time + self.step, self.step)\n        self.u = np.concatenate((self.u, step.u))\n        self.v = np.concatenate((self.v, step.v))\n        for attr in self.attributes.keys():\n            if attr in step.attributes.keys():\n                self.attributes[attr].extend(step.attributes[attr])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef boundary_polygon(self, time):\n        ti = np.where(time == self.times)[0][0]\n        com_x, com_y = self.center_of_mass(time)\n        # If at least one point along perimeter of the mask rectangle is unmasked, find_boundaries() works.\n        # But if all perimeter points are masked, find_boundaries() does not find the object.\n        # Therefore, pad the mask with zeroes first and run find_boundaries on the padded array.\n        padded_mask = np.pad(self.masks[ti], 1, 'constant', constant_values=0)\n        chull = convex_hull_image(padded_mask)\n        boundary_image = find_boundaries(chull, mode='inner', background=0)\n        # Now remove the padding.\n        boundary_image = boundary_image[1:-1,1:-1]\n        boundary_x = self.x[ti].ravel()[boundary_image.ravel()]\n        boundary_y = self.y[ti].ravel()[boundary_image.ravel()]\n        r = np.sqrt((boundary_x - com_x) ** 2 + (boundary_y - com_y) ** 2)\n        theta = np.arctan2((boundary_y - com_y), (boundary_x - com_x)) * 180.0 / np.pi + 360\n        polar_coords = np.array([(r[x], theta[x]) for x in range(r.size)], dtype=[('r', 'f4'), ('theta', 'f4')])\n        coord_order = np.argsort(polar_coords, order=['theta', 'r'])\n        ordered_coords = np.vstack([boundary_x[coord_order], boundary_y[coord_order]])\n        return ordered_coords", "response": "Find the coordinates of the object boundary in counter - clockwise order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_motion(self, time, intensity_grid, max_u, max_v):\n        ti = np.where(time == self.times)[0][0]\n        mask_vals = np.where(self.masks[ti].ravel() == 1)\n        i_vals = self.i[ti].ravel()[mask_vals]\n        j_vals = self.j[ti].ravel()[mask_vals]\n        obj_vals = self.timesteps[ti].ravel()[mask_vals]\n        u_shifts = np.arange(-max_u, max_u + 1)\n        v_shifts = np.arange(-max_v, max_v + 1)\n        min_error = 99999999999.0\n        best_u = 0\n        best_v = 0\n        for u in u_shifts:\n            j_shift = j_vals - u\n            for v in v_shifts:\n                i_shift = i_vals - v\n                if np.all((0 <= i_shift) & (i_shift < intensity_grid.shape[0]) &\n                                  (0 <= j_shift) & (j_shift < intensity_grid.shape[1])):\n                    shift_vals = intensity_grid[i_shift, j_shift]\n                else:\n                    shift_vals = np.zeros(i_shift.shape)\n                # This isn't correlation; it is mean absolute error.\n                error = np.abs(shift_vals - obj_vals).mean()\n                if error < min_error:\n                    min_error = error\n                    best_u = u * self.dx\n                    best_v = v * self.dx\n        # 60 seems arbitrarily high\n        #if min_error > 60:\n        #    best_u = 0\n        #    best_v = 0\n        self.u[ti] = best_u\n        self.v[ti] = best_v\n        return best_u, best_v, min_error", "response": "Estimate the motion of the object with cross - correlation on the intensity grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncount the number of points that overlap between this STObject and another STObject. Used for tracking.", "response": "def count_overlap(self, time, other_object, other_time):\n        \"\"\"\n        Counts the number of points that overlap between this STObject and another STObject. Used for tracking.\n        \"\"\"\n        ti = np.where(time == self.times)[0][0]\n        ma = np.where(self.masks[ti].ravel() == 1)\n        oti = np.where(other_time == other_object.times)[0]\n        obj_coords = np.zeros(self.masks[ti].sum(), dtype=[('x', int), ('y', int)])\n        other_obj_coords = np.zeros(other_object.masks[oti].sum(), dtype=[('x', int), ('y', int)])\n        obj_coords['x'] = self.i[ti].ravel()[ma]\n        obj_coords['y'] = self.j[ti].ravel()[ma]\n        other_obj_coords['x'] = other_object.i[oti][other_object.masks[oti] == 1]\n        other_obj_coords['y'] = other_object.j[oti][other_object.masks[oti] == 1]\n        return float(np.intersect1d(obj_coords,\n                                    other_obj_coords).size) / np.maximum(self.masks[ti].sum(),\n                                                                         other_object.masks[oti].sum())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the data from a ModelGrid object within the bounding box of the STObject.", "response": "def extract_attribute_grid(self, model_grid, potential=False, future=False):\n        \"\"\"\n        Extracts the data from a ModelOutput or ModelGrid object within the bounding box region of the STObject.\n        \n        Args:\n            model_grid: A ModelGrid or ModelOutput Object\n            potential: Extracts from the time before instead of the same time as the object\n        \"\"\"\n\n        if potential:\n            var_name = model_grid.variable + \"-potential\"\n            timesteps = np.arange(self.start_time - 1, self.end_time)\n        elif future:\n            var_name = model_grid.variable + \"-future\"\n            timesteps = np.arange(self.start_time + 1, self.end_time + 2)\n        else:\n            var_name = model_grid.variable\n            timesteps = np.arange(self.start_time, self.end_time + 1)\n        self.attributes[var_name] = []\n        for ti, t in enumerate(timesteps):\n            self.attributes[var_name].append(\n                model_grid.data[t - model_grid.start_hour, self.i[ti], self.j[ti]])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract data from a 2D array that has the same dimensions as the grid used to identify the object.", "response": "def extract_attribute_array(self, data_array, var_name):\n        \"\"\"\n        Extracts data from a 2D array that has the same dimensions as the grid used to identify the object.\n\n        Args:\n            data_array: 2D numpy array\n\n        \"\"\"\n        if var_name not in self.attributes.keys():\n            self.attributes[var_name] = []\n        for t in range(self.times.size):\n            self.attributes[var_name].append(data_array[self.i[t], self.j[t]])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the difference in model outputs", "response": "def extract_tendency_grid(self, model_grid):\n        \"\"\"\n        Extracts the difference in model outputs\n\n        Args:\n            model_grid: ModelOutput or ModelGrid object.\n\n        \"\"\"\n        var_name = model_grid.variable + \"-tendency\"\n        self.attributes[var_name] = []\n        timesteps = np.arange(self.start_time, self.end_time + 1)\n        for ti, t in enumerate(timesteps):\n            t_index = t - model_grid.start_hour\n            self.attributes[var_name].append(\n                model_grid.data[t_index, self.i[ti], self.j[ti]] - model_grid.data[t_index - 1, self.i[ti], self.j[ti]]\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_attribute_statistics(self, statistic_name):\n        stats = {}\n        for var, grids in self.attributes.items():\n            if len(grids) > 1:\n                stats[var] = getattr(np.array([getattr(np.ma.array(x, mask=self.masks[t] == 0), statistic_name)()\n                                               for t, x in enumerate(grids)]), statistic_name)()\n            else:\n                stats[var] = getattr(np.ma.array(grids[0], mask=self.masks[0] == 0), statistic_name)()\n        return stats", "response": "Calculates summary statistics over the domains of each attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_attribute_statistic(self, attribute, statistic, time):\n        ti = np.where(self.times == time)[0][0]\n        ma = np.where(self.masks[ti].ravel() == 1)\n        if statistic in ['mean', 'max', 'min', 'std', 'ptp']:\n            stat_val = getattr(self.attributes[attribute][ti].ravel()[ma], statistic)()\n        elif statistic == 'median':\n            stat_val = np.median(self.attributes[attribute][ti].ravel()[ma])\n        elif statistic == \"skew\":\n            stat_val = np.mean(self.attributes[attribute][ti].ravel()[ma]) - \\\n                       np.median(self.attributes[attribute][ti].ravel()[ma])\n        elif 'percentile' in statistic:\n            per = int(statistic.split(\"_\")[1])\n            stat_val = np.percentile(self.attributes[attribute][ti].ravel()[ma], per)\n        elif 'dt' in statistic:\n            stat_name = statistic[:-3]\n            if ti == 0:\n                stat_val = 0\n            else:\n                stat_val = self.calc_attribute_statistic(attribute, stat_name, time) \\\n                    - self.calc_attribute_statistic(attribute, stat_name, time - 1)\n        else:\n            stat_val = np.nan\n        return stat_val", "response": "Calculates the statistic value for an attribute in the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the statistic value for a given time in the current object.", "response": "def calc_timestep_statistic(self, statistic, time):\n        \"\"\"\n        Calculate statistics from the primary attribute of the StObject.\n\n        Args:\n            statistic: statistic being calculated\n            time: Timestep being investigated\n\n        Returns:\n            Value of the statistic\n        \"\"\"\n        ti = np.where(self.times == time)[0][0]\n        ma = np.where(self.masks[ti].ravel() == 1)\n        if statistic in ['mean', 'max', 'min', 'std', 'ptp']:\n            stat_val = getattr(self.timesteps[ti].ravel()[ma], statistic)()\n        elif statistic == 'median':\n            stat_val = np.median(self.timesteps[ti].ravel()[ma])\n        elif 'percentile' in statistic:\n            per = int(statistic.split(\"_\")[1])\n            stat_val = np.percentile(self.timesteps[ti].ravel()[ma], per)\n        elif 'dt' in statistic:\n            stat_name = statistic[:-3]\n            if ti == 0:\n                stat_val = 0\n            else:\n                stat_val = self.calc_timestep_statistic(stat_name, time) -\\\n                    self.calc_timestep_statistic(stat_name, time - 1)\n        else:\n            stat_val = np.nan\n        return stat_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate shape statistics using regionprops applied to the object mask.", "response": "def calc_shape_statistics(self, stat_names):\n        \"\"\"\n        Calculate shape statistics using regionprops applied to the object mask.\n        \n        Args:\n            stat_names: List of statistics to be extracted from those calculated by regionprops.\n\n        Returns:\n            Dictionary of shape statistics\n        \"\"\"\n        stats = {}\n        try:\n            all_props = [regionprops(m) for m in self.masks]\n        except TypeError:\n            print(self.masks)\n            exit()\n        for stat in stat_names:\n            stats[stat] = np.mean([p[0][stat] for p in all_props])\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating shape statistics for a single time step", "response": "def calc_shape_step(self, stat_names, time):\n        \"\"\"\n        Calculate shape statistics for a single time step\n\n        Args:\n            stat_names: List of shape statistics calculated from region props\n            time: Time being investigated\n\n        Returns:\n            List of shape statistics\n\n        \"\"\"\n        ti = np.where(self.times == time)[0][0]\n        props = regionprops(self.masks[ti], self.timesteps[ti])[0]\n        shape_stats = []\n        for stat_name in stat_names:\n            if \"moments_hu\" in stat_name:\n                hu_index = int(stat_name.split(\"_\")[-1])\n                hu_name = \"_\".join(stat_name.split(\"_\")[:-1])\n                hu_val = np.log(props[hu_name][hu_index])\n                if np.isnan(hu_val):\n                    shape_stats.append(0)\n                else:\n                    shape_stats.append(hu_val)\n            else:\n                shape_stats.append(props[stat_name])\n        return shape_stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_geojson(self, filename, proj, metadata=None):\n        if metadata is None:\n            metadata = {}\n        json_obj = {\"type\": \"FeatureCollection\", \"features\": [], \"properties\": {}}\n        json_obj['properties']['times'] = self.times.tolist()\n        json_obj['properties']['dx'] = self.dx\n        json_obj['properties']['step'] = self.step\n        json_obj['properties']['u'] = self.u.tolist()\n        json_obj['properties']['v'] = self.v.tolist()\n        for k, v in metadata.items():\n            json_obj['properties'][k] = v\n        for t, time in enumerate(self.times):\n            feature = {\"type\": \"Feature\",\n                       \"geometry\": {\"type\": \"Polygon\"},\n                       \"properties\": {}}\n            boundary_coords = self.boundary_polygon(time)\n            lonlat = np.vstack(proj(boundary_coords[0], boundary_coords[1], inverse=True))\n            lonlat_list = lonlat.T.tolist()\n            if len(lonlat_list) > 0:\n                lonlat_list.append(lonlat_list[0])\n            feature[\"geometry\"][\"coordinates\"] = [lonlat_list]\n            for attr in [\"timesteps\", \"masks\", \"x\", \"y\", \"i\", \"j\"]:\n                feature[\"properties\"][attr] = getattr(self, attr)[t].tolist()\n            feature[\"properties\"][\"attributes\"] = {}\n            for attr_name, steps in self.attributes.items():\n                feature[\"properties\"][\"attributes\"][attr_name] = steps[t].tolist()\n            json_obj['features'].append(feature)\n        file_obj = open(filename, \"w\")\n        json.dump(json_obj, file_obj, indent=1, sort_keys=True)\n        file_obj.close()\n        return", "response": "Output the data in the STObject to a GeoJSON file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rewrite_properties(self, properties):\n        with self.__library_storage_lock:\n            self.__library_storage = properties\n        self.__write_properties(None)", "response": "Set the properties and write to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef model(self, v=None):\n        \"Returns the model of node v\"\n        if v is None:\n            v = self.estopping\n        hist = self.hist\n        trace = self.trace(v)\n        ins = None\n        if self._base._probability_calibration is not None:\n            node = hist[-1]\n            node.normalize()\n            X = np.array([x.full_array() for x in node.hy]).T\n            y = np.array(self._base._y_klass.full_array())\n            mask = np.ones(X.shape[0], dtype=np.bool)\n            mask[np.array(self._base._mask_ts.index)] = False\n            ins = self._base._probability_calibration().fit(X[mask], y[mask])\n        if self._classifier:\n            nclasses = self._labels.shape[0]\n        else:\n            nclasses = None\n        m = Model(trace, hist, nvar=self._base._nvar,\n                  classifier=self._classifier, labels=self._labels,\n                  probability_calibration=ins, nclasses=nclasses)\n        return m", "response": "Returns the model of node v"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trace(self, n):\n        \"Restore the position in the history of individual v's nodes\"\n        trace_map = {}\n        self._trace(n, trace_map)\n        s = list(trace_map.keys())\n        s.sort()\n        return s", "response": "Restore the position in the history of individual v s nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_population(self):\n        \"Create the initial population\"\n        base = self._base\n        if base._share_inputs:\n            used_inputs_var = SelectNumbers([x for x in range(base.nvar)])\n            used_inputs_naive = used_inputs_var\n        if base._pr_variable == 0:\n            used_inputs_var = SelectNumbers([])\n            used_inputs_naive = SelectNumbers([x for x in range(base.nvar)])\n        elif base._pr_variable == 1:\n            used_inputs_var = SelectNumbers([x for x in range(base.nvar)])\n            used_inputs_naive = SelectNumbers([])\n        else:\n            used_inputs_var = SelectNumbers([x for x in range(base.nvar)])\n            used_inputs_naive = SelectNumbers([x for x in range(base.nvar)])\n        nb_input = Inputs(base, used_inputs_naive, functions=base._input_functions)\n        while ((base._all_inputs and not base.stopping_criteria_tl()) or\n               (self.popsize < base.popsize and\n                not base.stopping_criteria())):\n            if base._all_inputs and used_inputs_var.empty() and used_inputs_naive.empty():\n                base._init_popsize = self.popsize\n                break\n            if nb_input.use_all_variables():\n                v = nb_input.all_variables()\n                if v is None:\n                    continue\n            elif not used_inputs_var.empty() and np.random.random() < base._pr_variable:\n                v = self.variable_input(used_inputs_var)\n                if v is None:\n                    used_inputs_var.pos = used_inputs_var.size\n                    continue\n            elif not used_inputs_naive.empty():\n                v = nb_input.input()\n                if not used_inputs_var.empty() and used_inputs_naive.empty():\n                    base._pr_variable = 1\n                if v is None:\n                    used_inputs_naive.pos = used_inputs_naive.size\n                    if not used_inputs_var.empty():\n                        base._pr_variable = 1\n                    continue\n            else:\n                gen = self.generation\n                self.generation = 0\n                v = base.random_offspring()\n                self.generation = gen\n            self.add(v)", "response": "Create the initial population"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, v):\n        \"Add an individual to the population\"\n        self.population.append(v)\n        self._current_popsize += 1\n        v.position = len(self._hist)\n        self._hist.append(v)\n        self.bsf = v\n        self.estopping = v\n        self._density += self.get_density(v)", "response": "Add an individual to the population"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces an individual selected by negative tournament selection with individual v", "response": "def replace(self, v):\n        \"\"\"Replace an individual selected by negative tournament selection with\n        individual v\"\"\"\n        if self.popsize < self._popsize:\n            return self.add(v)\n        k = self.tournament(negative=True)\n        self.clean(self.population[k])\n        self.population[k] = v\n        v.position = len(self._hist)\n        self._hist.append(v)\n        self.bsf = v\n        self.estopping = v\n        self._inds_replace += 1\n        self._density += self.get_density(v)\n        if self._inds_replace == self._popsize:\n            self._inds_replace = 0\n            self.generation += 1\n            gc.collect()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the directory path if needed.", "response": "def make_directory_if_needed(directory_path):\n    \"\"\"\n        Make the directory path, if needed.\n    \"\"\"\n    if os.path.exists(directory_path):\n        if not os.path.isdir(directory_path):\n            raise OSError(\"Path is not a directory:\", directory_path)\n    else:\n        os.makedirs(directory_path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling an arbitary command and return the exit value stdout and stderr as a tuple", "response": "def call(cmd_args, suppress_output=False):\n    \"\"\" Call an arbitary command and return the exit value, stdout, and stderr as a tuple\n\n    Command can be passed in as either a string or iterable\n\n    >>> result = call('hatchery', suppress_output=True)\n    >>> result.exitval\n    0\n    >>> result = call(['hatchery', 'notreal'])\n    >>> result.exitval\n    1\n    \"\"\"\n    if not funcy.is_list(cmd_args) and not funcy.is_tuple(cmd_args):\n        cmd_args = shlex.split(cmd_args)\n    logger.info('executing `{}`'.format(' '.join(cmd_args)))\n    call_request = CallRequest(cmd_args, suppress_output=suppress_output)\n    call_result = call_request.run()\n    if call_result.exitval:\n        logger.error('`{}` returned error code {}'.format(' '.join(cmd_args), call_result.exitval))\n    return call_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls a setup. py command or list of commands", "response": "def setup(cmd_args, suppress_output=False):\n    \"\"\" Call a setup.py command or list of commands\n\n    >>> result = setup('--name', suppress_output=True)\n    >>> result.exitval\n    0\n    >>> result = setup('notreal')\n    >>> result.exitval\n    1\n    \"\"\"\n    if not funcy.is_list(cmd_args) and not funcy.is_tuple(cmd_args):\n        cmd_args = shlex.split(cmd_args)\n    cmd_args = [sys.executable, 'setup.py'] + [x for x in cmd_args]\n    return call(cmd_args, suppress_output=suppress_output)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_data(self):\n        data = []\n        valid_dates = []\n        mrms_files = np.array(sorted(os.listdir(self.path + self.variable + \"/\")))\n        mrms_file_dates = np.array([m_file.split(\"_\")[-2].split(\"-\")[0]\n            for m_file in mrms_files])\n        old_mrms_file = None\n        file_obj = None\n        for t in range(self.all_dates.shape[0]):\n            file_index = np.where(mrms_file_dates == self.all_dates[t].strftime(\"%Y%m%d\"))[0]\n            if len(file_index) > 0:\n                mrms_file = mrms_files[file_index][0]\n                if mrms_file is not None:\n                    if file_obj is not None:\n                        file_obj.close()\n                    file_obj = Dataset(self.path + self.variable + \"/\" + mrms_file)\n                    #old_mrms_file = mrms_file\n                    \n                    if \"time\" in file_obj.variables.keys():\n                        time_var = \"time\"\n                    else:\n                        time_var = \"date\"\n                    file_valid_dates = pd.DatetimeIndex(num2date(file_obj.variables[time_var][:],\n                                                                 file_obj.variables[time_var].units))\n                else:\n                    file_valid_dates = pd.DatetimeIndex([])\n                time_index = np.where(file_valid_dates.values == self.all_dates.values[t])[0]\n                if len(time_index) > 0:\n                    data.append(file_obj.variables[self.variable][time_index[0]])\n                    valid_dates.append(self.all_dates[t])\n        if file_obj is not None:\n            file_obj.close()\n        self.data = np.array(data)\n        self.data[self.data < 0] = 0\n        self.data[self.data > 150] = 150\n        self.valid_dates = pd.DatetimeIndex(valid_dates)", "response": "Loads the data files and stores the output in the data attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlabels the pixels in the input_grid using enhanced watershed algorithm.", "response": "def label(self, input_grid):\n        \"\"\"\n        Labels input grid using enhanced watershed algorithm.\n\n        Args:\n            input_grid (numpy.ndarray): Grid to be labeled.\n\n        Returns:\n            Array of labeled pixels\n        \"\"\"\n        marked = self.find_local_maxima(input_grid)\n        marked = np.where(marked >= 0, 1, 0)\n        # splabel returns two things in a tuple: an array and an integer\n        # assign the first thing (array) to markers\n        markers = splabel(marked)[0]\n        return markers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the local maxima in the input grid and perform region growing to identify objects.", "response": "def find_local_maxima(self, input_grid):\n        \"\"\"\n        Finds the local maxima in the inputGrid and perform region growing to identify objects.\n\n        Args:\n            input_grid: Raw input data.\n\n        Returns:\n            array with labeled objects.\n        \"\"\"\n        pixels, q_data = self.quantize(input_grid)\n        centers = OrderedDict()\n        for p in pixels.keys():\n            centers[p] = []\n        marked = np.ones(q_data.shape, dtype=int) * self.UNMARKED\n        MIN_INFL = int(np.round(1 + 0.5 * np.sqrt(self.max_size)))\n        MAX_INFL = 2 * MIN_INFL\n        marked_so_far = []\n        # Find the maxima. These are high-values with enough clearance\n        # around them.\n        # Work from high to low bins. The pixels in the highest bin mark their\n        # neighborhoods first. If you did it from low to high the lowest maxima\n        # would mark their neighborhoods first and interfere with the identification of higher maxima.\n        for b in sorted(pixels.keys(),reverse=True):\n            # Square starts large with high intensity bins and gets smaller with low intensity bins.\n            infl_dist = MIN_INFL + int(np.round(float(b) / self.max_bin * (MAX_INFL - MIN_INFL)))\n            for p in pixels[b]:\n                if marked[p] == self.UNMARKED:\n                    ok = False\n                    del marked_so_far[:]\n                    # Temporarily mark unmarked points in square around point (keep track of them in list marked_so_far).\n                    # If none of the points in square were marked already from a higher intensity center, \n                    # this counts as a new center and ok=True and points will remain marked.\n                    # Otherwise ok=False and marked points that were previously unmarked will be unmarked.\n                    for (i, j), v in np.ndenumerate(marked[p[0] - infl_dist:p[0] + infl_dist + 1,\n                                                    p[1] - infl_dist:p[1]+ infl_dist + 1]):\n                        if v == self.UNMARKED:\n                            ok = True\n                            marked[i - infl_dist + p[0],j - infl_dist + p[1]] = b\n                           \n                            marked_so_far.append((i - infl_dist + p[0],j - infl_dist + p[1]))\n                        else:\n                            # neighborhood already taken\n                            ok = False\n                            break\n                    # ok if point and surrounding square were not marked already.\n                    if ok:\n                        # highest point in its neighborhood\n                        centers[b].append(p)\n                    else:\n                        for m in marked_so_far:\n                            marked[m] = self.UNMARKED\n        # Erase marks and start over. You have a list of centers now.\n        marked[:, :] = self.UNMARKED\n        deferred_from_last = []\n        deferred_to_next = []\n        # delta (int): maximum number of increments the cluster is allowed to range over. Larger d results in clusters over larger scales.\n        for delta in range(0, self.delta + 1):\n            # Work from high to low bins.\n            for b in sorted(centers.keys(), reverse=True):\n                bin_lower = b - delta\n                deferred_from_last[:] = deferred_to_next[:]\n                del deferred_to_next[:]\n                foothills = []\n                n_centers = len(centers[b])\n                tot_centers = n_centers + len(deferred_from_last)\n                for i in range(tot_centers):\n                    # done this way to minimize memory overhead of maintaining two lists\n                    if i < n_centers:\n                        center = centers[b][i]\n                    else:\n                        center = deferred_from_last[i - n_centers]\n                    if bin_lower < 0:\n                        bin_lower = 0\n                    if marked[center] == self.UNMARKED:\n                        captured = self.set_maximum(q_data, marked, center, bin_lower, foothills)\n                        if not captured:\n                            # decrement to lower value to see if it'll get big enough\n                            deferred_to_next.append(center)\n                        else:\n                            pass\n                # this is the last one for this bin\n                self.remove_foothills(q_data, marked, b, bin_lower, centers, foothills)\n            del deferred_from_last[:]\n            del deferred_to_next[:]\n        return marked"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_maximum(self, q_data, marked, center, bin_lower, foothills):\n        as_bin = [] # pixels to be included in peak\n        as_glob = []   # pixels to be globbed up as part of foothills\n        marked_so_far = []  # pixels that have already been marked\n        will_be_considered_again = False\n        as_bin.append(center)\n        center_data = q_data[center]\n        while len(as_bin) > 0:\n            p = as_bin.pop(-1) # remove and return last pixel in as_bin\n            if marked[p] != self.UNMARKED: # already processed\n                continue\n            marked[p] = q_data[center]\n            marked_so_far.append(p)\n\n            # check neighbors\n            for index,val in np.ndenumerate(marked[p[0] - 1:p[0] + 2, p[1] - 1:p[1] + 2]):\n                # is neighbor part of peak or part of mountain?\n                if val == self.UNMARKED:\n                    pixel = (index[0] - 1 + p[0],index[1] - 1 + p[1])\n                    p_data = q_data[pixel]\n                    if (not will_be_considered_again) and (p_data >= 0) and (p_data < center_data):\n                        will_be_considered_again = True\n                    if p_data >= bin_lower and (np.abs(center_data - p_data) <= self.delta):\n                        as_bin.append(pixel)\n                    # Do not check that this is the closest: this way, a narrow channel of globbed pixels form\n                    elif p_data >= 0:\n                        as_glob.append(pixel)\n        if bin_lower == 0:\n            will_be_considered_again = False\n        big_enough = len(marked_so_far) >= self.max_size\n        if big_enough:\n            # remove lower values within region of influence\n            foothills.append((center, as_glob))\n        elif will_be_considered_again: # remove the check if you want to ignore regions smaller than max_size\n            for m in marked_so_far:\n                marked[m] = self.UNMARKED\n            del as_bin[:]\n            del as_glob[:]\n            del marked_so_far[:]\n        return big_enough or (not will_be_considered_again)", "response": "This method grows a region at a certain bin level and checks if the maximum size of the object is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_foothills(self, q_data, marked, bin_num, bin_lower, centers, foothills):\n        hills = []\n        for foot in foothills:\n            center = foot[0]\n            hills[:] = foot[1][:]\n            # remove all foothills\n            while len(hills) > 0:\n                # mark this point\n                pt = hills.pop(-1)\n                marked[pt] = self.GLOBBED\n                for s_index, val in np.ndenumerate(marked[pt[0]-1:pt[0]+2,pt[1]-1:pt[1]+2]):\n                    index = (s_index[0] - 1 + pt[0], s_index[1] - 1 + pt[1])\n                    # is neighbor part of peak or part of mountain?\n                    if val == self.UNMARKED:\n                        # will let in even minor peaks\n                        if (q_data[index] >= 0) and \\\n                                (q_data[index] < bin_lower) and \\\n                                ((q_data[index] <= q_data[pt]) or self.is_closest(index, center, centers, bin_num)):\n                            hills.append(index)\n        del foothills[:]", "response": "Removes foothills from a set of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quantize(self, input_grid):\n        pixels = {}\n        for i in range(self.max_bin+1):\n            pixels[i] = []\n\n        data = (np.array(input_grid, dtype=int) - self.min_thresh) / self.data_increment\n        data[data < 0] = -1\n        data[data > self.max_bin] = self.max_bin\n        good_points = np.where(data >= 0)\n        for g in np.arange(good_points[0].shape[0]):\n            pixels[data[(good_points[0][g], good_points[1][g])]].append((good_points[0][g], good_points[1][g]))\n        return pixels, data", "response": "Quantize a grid into discrete steps based on input parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edit(self, **args):\n\t\t'''\n\t\tDoesn't require manual fetching of gistID of a gist\n\t\tpassing gistName will return edit the gist\n\t\t'''\n\t\tself.gist_name = ''\n\t\tif 'description' in args:\n\t\t\tself.description = args['description']\n\t\telse:\n\t\t\tself.description = ''\n\n\n\t\tif 'name' in args and 'id' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = args['id']\n\t\telif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Gist Name/ID must be provided')\n\n\t\tif 'content' in args:\n\t\t\tself.content = args['content']\n\t\telse:\n\t\t\traise Exception('Gist content can\\'t be empty')\n\n\t\tif (self.gist_name == ''):\n\t\t\tself.gist_name = self.getgist(id=self.gist_id)\n\t\t\tdata = {\"description\": self.description,\n  \t\t\t\t\"files\": {\n    \t\t\t\tself.gist_name: {\n      \t\t\t\t\"content\": self.content\n    \t\t\t\t}\n  \t\t\t\t}\n  \t\t}\n\t\telse:\n\t\t\tdata = {\"description\": self.description,\n  \t\t\t\t\"files\": {\n    \t\t\t\tself.gist_name: {\n      \t\t\t\t\"content\": self.content\n    \t\t\t\t}\n  \t\t\t\t}\n  \t\t\t}\n\n\n\t\tif self.gist_id:\n\t\t\tr = requests.patch(\n\t\t\t\t'%s/gists/%s'%(BASE_URL,self.gist_id),\n\t\t\t\theaders=self.gist.header,\n\t\t\t\tdata=json.dumps(data),\n\t\t\t\t)\n\t\t\tif (r.status_code == 200):\n\t\t\t\tr_text = json.loads(r.text)\n\t\t\t\tresponse = {\n\t\t\t\t\t'updated_content': self.content,\n\t\t\t\t\t'created_at': r.json()['created_at'],\n\t\t\t\t\t'comments':r.json()['comments']\n\t\t\t\t}\n\n\t\t\t\treturn response\n\n\t\traise Exception('No such gist found')", "response": "This method will edit the gist with the given name and content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a gist by gistname or gistID", "response": "def delete(self, **args):\n\t\t'''\n\t\tDelete a gist by gistname/gistID\n\t\t'''\n\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Provide GistName to delete')\n\n\t\turl = 'gists'\n\t\tif self.gist_id:\n\t\t\tr = requests.delete(\n\t\t\t\t'%s/%s/%s'%(BASE_URL,url,self.gist_id),\n\t\t\t\theaders=self.gist.header\n\t\t\t\t)\n\t\t\tif (r.status_code == 204):\n\t\t\t\tresponse = {\n\t\t\t\t\t'id': self.gist_id,\n\t\t\t\t}\n\t\t\t\treturn response\n\n\t\traise Exception('Can not delete gist')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of starred gists", "response": "def starred(self, **args):\n\t\t'''\n\t\tList the authenticated user's starred gists\n\t\t'''\n\t\tids =[]\n\t\tr = requests.get(\n\t\t\t'%s/gists/starred'%BASE_URL,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\n\t\tif 'limit' in args:\n\t\t\tlimit = args['limit']\n\t\telse:\n\t\t\tlimit = len(r.json())\n\n\t\tif (r.status_code == 200):\n\t\t\tfor g in range(0,limit ):\n\t\t\t\tids.append('%s/%s/%s' %(GIST_URL,r.json()[g]['user']['login'],r.json()[g]['id']))\n\t\t\treturn ids\n\n\t\traise Exception('Username not found')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Gist URL - Link Clone - Link and Script - Link to embed", "response": "def links(self,**args):\n\t\t'''\n\t\tReturn Gist URL-Link, Clone-Link and Script-Link to embed\n\t\t'''\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Gist Name/ID must be provided')\n\t\tif self.gist_id:\n\t\t\tr = requests.get(\n\t\t\t\t'%s/gists/%s'%(BASE_URL,self.gist_id),\n\t\t\t\theaders=self.gist.header,\n\t\t\t\t)\n\t\t\tif (r.status_code == 200):\n\n\t\t\t\tcontent = {\n\t\t\t\t'Github-User': r.json()['user']['login'],\n\t\t\t\t'GistID': r.json()['id'],\n\t\t\t\t'Gist-Link': '%s/%s/%s' %(GIST_URL,self.gist.username,r.json()['id']),\n\t\t\t\t'Clone-Link': '%s/%s.git' %(GIST_URL,r.json()['id']),\n\t\t\t\t'Embed-Script': '<script src=\"%s/%s/%s.js\"</script>' %(GIST_URL,self.gist.username,r.json()['id'])\n\t\t\t\t}\n\t\t\t\treturn content\n\n\t\traise Exception('No such gist found')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_forecasts(self):\n        run_date_str = self.run_date.strftime(\"%Y%m%d\")\n        forecast_file = self.forecast_path + \"{0}/{1}_{2}_{3}_consensus_{0}.nc\".format(run_date_str,\n                                                                                       self.ensemble_name,\n                                                                                       self.model_name,\n                                                                                       self.forecast_variable)\n        print(\"Forecast file: \" + forecast_file)\n        forecast_data = Dataset(forecast_file)\n        for size_threshold in self.size_thresholds:\n            for smoothing_radius in self.smoothing_radii:\n                for neighbor_radius in self.neighbor_radii:\n                    hour_var = \"neighbor_prob_r_{0:d}_s_{1:d}_{2}_{3:0.2f}\".format(neighbor_radius, smoothing_radius,\n                                                                                   self.forecast_variable,\n                                                                                   float(size_threshold))\n                    period_var = \"neighbor_prob_{0:d}-hour_r_{1:d}_s_{2:d}_{3}_{4:0.2f}\".format(self.end_hour -\n                                                                                                self.start_hour + 1,\n                                                                                                neighbor_radius,\n                                                                                                smoothing_radius,\n                                                                                                self.forecast_variable,\n                                                                                                float(size_threshold))\n\n                    print(\"Loading forecasts {0} {1} {2} {3} {4}\".format(self.run_date, self.model_name,\n                                                                         self.forecast_variable, size_threshold,\n                                                                         smoothing_radius))\n                    if hour_var in forecast_data.variables.keys():\n                        self.hourly_forecasts[hour_var] = forecast_data.variables[hour_var][:]\n                    if period_var in forecast_data.variables.keys():\n                        self.period_forecasts[period_var] = forecast_data.variables[period_var][:]\n        forecast_data.close()", "response": "Load neighborhood probability forecasts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload observations and masking grids.", "response": "def load_obs(self,  mask_threshold=0.5):\n        \"\"\"\n        Loads observations and masking grid (if needed).\n\n        Args:\n            mask_threshold: Values greater than the threshold are kept, others are masked.\n        \"\"\"\n        print(\"Loading obs \", self.run_date, self.model_name, self.forecast_variable)\n        start_date = self.run_date + timedelta(hours=self.start_hour)\n        end_date = self.run_date + timedelta(hours=self.end_hour)\n        mrms_grid = MRMSGrid(start_date, end_date, self.mrms_variable, self.mrms_path)\n        mrms_grid.load_data()\n        if len(mrms_grid.data) > 0:\n            self.raw_obs[self.mrms_variable] = np.where(mrms_grid.data > 100, 100, mrms_grid.data)\n            self.period_obs[self.mrms_variable] = self.raw_obs[self.mrms_variable].max(axis=0)\n            if self.obs_mask:\n                mask_grid = MRMSGrid(start_date, end_date, self.mask_variable, self.mrms_path)\n                mask_grid.load_data()\n                self.raw_obs[self.mask_variable] = np.where(mask_grid.data >= mask_threshold, 1, 0)\n                self.period_obs[self.mask_variable] = self.raw_obs[self.mask_variable].max(axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_coordinates(self):\n        coord_file = Dataset(self.coordinate_file)\n        if \"lon\" in coord_file.variables.keys():\n            self.coordinates[\"lon\"] = coord_file.variables[\"lon\"][:]\n            self.coordinates[\"lat\"] = coord_file.variables[\"lat\"][:]\n        else:\n            self.coordinates[\"lon\"] = coord_file.variables[\"XLONG\"][0]\n            self.coordinates[\"lat\"] = coord_file.variables[\"XLAT\"][0]\n        coord_file.close()", "response": "Loads lat - lon coordinates from a netCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate_hourly_forecasts(self):\n        score_columns = [\"Run_Date\", \"Forecast_Hour\", \"Ensemble Name\", \"Model_Name\", \"Forecast_Variable\",\n                         \"Neighbor_Radius\", \"Smoothing_Radius\", \"Size_Threshold\", \"ROC\", \"Reliability\"]\n        all_scores = pd.DataFrame(columns=score_columns)\n        for h, hour in enumerate(range(self.start_hour, self.end_hour + 1)):\n            for neighbor_radius in self.neighbor_radii:\n                n_filter = disk(neighbor_radius)\n                for s, size_threshold in enumerate(self.size_thresholds):\n                    print(\"Eval hourly forecast {0:02d} {1} {2} {3} {4:d} {5:d}\".format(hour, self.model_name,\n                                                                                        self.forecast_variable,\n                                                                                        self.run_date, neighbor_radius,\n                                                                                        size_threshold))\n                    hour_obs = fftconvolve(self.raw_obs[self.mrms_variable][h] >= self.obs_thresholds[s],\n                                           n_filter, mode=\"same\")\n                    hour_obs[hour_obs > 1] = 1\n                    hour_obs[hour_obs < 1] = 0\n                    if self.obs_mask:\n                        hour_obs = hour_obs[self.raw_obs[self.mask_variable][h] > 0]\n                    for smoothing_radius in self.smoothing_radii:\n                        hour_var = \"neighbor_prob_r_{0:d}_s_{1:d}_{2}_{3:0.2f}\".format(neighbor_radius,\n                                                                                       smoothing_radius,\n                                                                                       self.forecast_variable,\n                                                                                       size_threshold)\n                        if self.obs_mask:\n                            hour_forecast = self.hourly_forecasts[hour_var][h][self.raw_obs[self.mask_variable][h] > 0]\n                        else:\n                            hour_forecast = self.hourly_forecasts[hour_var][h]\n                        roc = DistributedROC(thresholds=self.probability_levels, obs_threshold=0.5)\n                        roc.update(hour_forecast, hour_obs)\n                        rel = DistributedReliability(thresholds=self.probability_levels, obs_threshold=0.5)\n                        rel.update(hour_forecast, hour_obs)\n                        row = [self.run_date, hour, self.ensemble_name, self.model_name, self.forecast_variable,\n                               neighbor_radius,\n                               smoothing_radius, size_threshold, roc, rel]\n                        all_scores.loc[hour_var + \"_{0:d}\".format(hour)] = row\n        return all_scores", "response": "Evaluates the hourly forecasts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef evaluate_period_forecasts(self):\n        score_columns = [\"Run_Date\", \"Ensemble Name\", \"Model_Name\", \"Forecast_Variable\", \"Neighbor_Radius\",\n                         \"Smoothing_Radius\", \"Size_Threshold\",  \"ROC\", \"Reliability\"]\n        all_scores = pd.DataFrame(columns=score_columns)\n        if self.coordinate_file is not None:\n            coord_mask = np.where((self.coordinates[\"lon\"] >= self.lon_bounds[0]) &\n                                  (self.coordinates[\"lon\"] <= self.lon_bounds[1]) &\n                                  (self.coordinates[\"lat\"] >= self.lat_bounds[0]) &\n                                  (self.coordinates[\"lat\"] <= self.lat_bounds[1]) &\n                                  (self.period_obs[self.mask_variable] > 0))\n        else:\n            coord_mask = None\n        for neighbor_radius in self.neighbor_radii:\n            n_filter = disk(neighbor_radius)\n            for s, size_threshold in enumerate(self.size_thresholds):\n                period_obs = fftconvolve(self.period_obs[self.mrms_variable] >= self.obs_thresholds[s],\n                                         n_filter, mode=\"same\")\n                period_obs[period_obs > 1] = 1\n                if self.obs_mask and self.coordinate_file is None:\n                    period_obs = period_obs[self.period_obs[self.mask_variable] > 0]\n                elif self.obs_mask and self.coordinate_file is not None:\n                    period_obs = period_obs[coord_mask[0], coord_mask[1]]\n                else:\n                    period_obs = period_obs.ravel()\n                for smoothing_radius in self.smoothing_radii:\n                    print(\"Eval period forecast {0} {1} {2} {3} {4} {5}\".format(self.model_name,\n                                                                                self.forecast_variable,\n                                                                                self.run_date,\n                                                                                neighbor_radius,\n                                                                                size_threshold, smoothing_radius))\n                    period_var = \"neighbor_prob_{0:d}-hour_r_{1:d}_s_{2:d}_{3}_{4:0.2f}\".format(self.end_hour -\n                                                                                                self.start_hour + 1,\n                                                                                                neighbor_radius,\n                                                                                                smoothing_radius,\n                                                                                                self.forecast_variable,\n                                                                                                size_threshold)\n                    if self.obs_mask and self.coordinate_file is None:\n                        period_forecast = self.period_forecasts[period_var][self.period_obs[self.mask_variable] > 0]\n                    elif self.obs_mask and self.coordinate_file is not None:\n                        period_forecast = self.period_forecasts[period_var][coord_mask[0], coord_mask[1]]\n                    else:\n                        period_forecast = self.period_forecasts[period_var].ravel()\n                    roc = DistributedROC(thresholds=self.probability_levels, obs_threshold=0.5)\n                    roc.update(period_forecast, period_obs)\n                    rel = DistributedReliability(thresholds=self.probability_levels, obs_threshold=0.5)\n                    rel.update(period_forecast, period_obs)\n                    row = [self.run_date, self.ensemble_name, self.model_name, self.forecast_variable, neighbor_radius,\n                           smoothing_radius, size_threshold, roc, rel]\n                    all_scores.loc[period_var] = row\n        return all_scores", "response": "Evaluates the ROC and Reliability scores for the full - period forecasts over the full - period."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _migrate_library(workspace_dir: pathlib.Path, do_logging: bool=True) -> pathlib.Path:\n\n    library_path_11 = workspace_dir / \"Nion Swift Workspace.nslib\"\n    library_path_12 = workspace_dir / \"Nion Swift Library 12.nslib\"\n    library_path_13 = workspace_dir / \"Nion Swift Library 13.nslib\"\n\n    library_paths = (library_path_11, library_path_12)\n    library_path_latest = library_path_13\n\n    if not os.path.exists(library_path_latest):\n        for library_path in reversed(library_paths):\n            if os.path.exists(library_path):\n                if do_logging:\n                    logging.info(\"Migrating library: %s -> %s\", library_path, library_path_latest)\n                shutil.copyfile(library_path, library_path_latest)\n                break\n\n    return library_path_latest", "response": "Migrate the current version of the Nion Swift Library to the latest version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status(self):\n        for valor, rotulo in ESTADOS_OPERACAO:\n            if self.ESTADO_OPERACAO == valor:\n                return rotulo\n        return u'(desconhecido: {})'.format(self.ESTADO_OPERACAO)", "response": "Nome amig\u00e1vel do campo ESTADO_OPERACAO conforme a Tabela de\n        Informa\u00e7\u00f5es do Status do SAT."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analisar(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='ConsultarStatusOperacional',\n                classe_resposta=RespostaConsultarStatusOperacional,\n                campos=RespostaSAT.CAMPOS + (\n                        ('NSERIE', as_clean_unicode),\n                        ('TIPO_LAN', as_clean_unicode),\n                        ('LAN_IP', normalizar_ip),\n                        ('LAN_MAC', unicode),\n                        ('LAN_MASK', normalizar_ip),\n                        ('LAN_GW', normalizar_ip),\n                        ('LAN_DNS_1', normalizar_ip),\n                        ('LAN_DNS_2', normalizar_ip),\n                        ('STATUS_LAN', as_clean_unicode),\n                        ('NIVEL_BATERIA', as_clean_unicode),\n                        ('MT_TOTAL', as_clean_unicode),\n                        ('MT_USADA', as_clean_unicode),\n                        ('DH_ATUAL', as_datetime),\n                        ('VER_SB', as_clean_unicode),\n                        ('VER_LAYOUT', as_clean_unicode),\n                        ('ULTIMO_CF_E_SAT', as_clean_unicode),\n                        ('LISTA_INICIAL', as_clean_unicode),\n                        ('LISTA_FINAL', as_clean_unicode),\n                        ('DH_CFE', as_datetime_or_none),\n                        ('DH_ULTIMA', as_datetime),\n                        ('CERT_EMISSAO', as_date),\n                        ('CERT_VENCIMENTO', as_date),\n                        ('ESTADO_OPERACAO', int),\n                    ),\n                campos_alternativos=[\n                        # se falhar resultar\u00e3o apenas os 5 campos padr\u00e3o\n                        RespostaSAT.CAMPOS,\n                    ]\n            )\n        if resposta.EEEEE not in ('10000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Analisar uma resposta do\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading forecasts from csv files and merges them with the input data from the step csv files.", "response": "def merge_input_csv_forecast_json(input_csv_file, forecast_json_path, condition_models, dist_models):\n    \"\"\"\n    Reads forecasts from json files and merges them with the input data from the step csv files.\n\n    Args:\n        input_csv_file: Name of the input data csv file being processed\n        forecast_json_path: Path to the forecast json files toplevel directory\n        condition_models: List of models used to forecast hail or no hail\n        dist_models: List of models used to forecast the hail size distribution\n\n    Returns:\n\n    \"\"\"\n    try:\n        run_date = input_csv_file[:-4].split(\"_\")[-1]\n        print(run_date)\n        ens_member = \"_\".join(input_csv_file.split(\"/\")[-1][:-4].split(\"_\")[3:-1])\n        ens_name = input_csv_file.split(\"/\")[-1].split(\"_\")[2]\n        input_data = pd.read_csv(input_csv_file, index_col=\"Step_ID\")\n        full_json_path = forecast_json_path + \"{0}/{1}/\".format(run_date, ens_member)\n        track_ids = sorted(input_data[\"Track_ID\"].unique())\n        model_pred_cols = []\n        condition_models_ns = []\n        dist_models_ns = []\n        gamma_params = [\"Shape\", \"Location\", \"Scale\"]\n        for condition_model in condition_models:\n            model_pred_cols.append(condition_model.replace(\" \", \"-\") + \"_Condition\")\n            condition_models_ns.append(condition_model.replace(\" \", \"-\"))\n        for dist_model in dist_models:\n            dist_models_ns.append(dist_model.replace(\" \", \"-\"))\n            for param in gamma_params:\n                model_pred_cols.append(dist_model.replace(\" \", \"-\") + \"_\" + param)\n        pred_data = pd.DataFrame(index=input_data.index, columns=model_pred_cols,\n                                dtype=float)\n        for track_id in track_ids:\n            track_id_num = track_id.split(\"_\")[-1]\n            json_filename = full_json_path + \"{0}_{1}_{2}_model_track_{3}.json\".format(ens_name,\n                                                                                    run_date,\n                                                                                    ens_member,\n                                                                                    track_id_num)\n            json_file = open(json_filename)\n            json_data = json.load(json_file)\n            json_file.close()\n            for s, step in enumerate(json_data[\"features\"]):\n                step_id = track_id + \"_{0:02d}\".format(s)\n                for cond_model in condition_models_ns:\n                    pred_data.loc[step_id, cond_model + \"_Condition\"]  = step[\"properties\"][\"condition_\" + cond_model]\n                for dist_model in dist_models_ns:\n                    pred_data.loc[step_id, [dist_model + \"_\" + p\n                                            for p in gamma_params]] = step[\"properties\"][\"dist_\" + dist_model]\n        out_data = input_data.merge(pred_data, left_index=True, right_index=True)\n        return out_data, ens_name, ens_member\n    except Exception as e:\n        print(traceback.format_exc())\n        raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling from item to indicate its data or metadata has changed.", "response": "def mark_data_dirty(self):\n        \"\"\" Called from item to indicate its data or metadata has changed.\"\"\"\n        self.__cache.set_cached_value_dirty(self.__display_item, self.__cache_property_name)\n        self.__initialize_cache()\n        self.__cached_value_dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __initialize_cache(self):\n        if self.__cached_value_dirty is None:\n            self.__cached_value_dirty = self.__cache.is_cached_value_dirty(self.__display_item, self.__cache_property_name)\n            self.__cached_value = self.__cache.get_cached_value(self.__display_item, self.__cache_property_name)", "response": "Initialize the cache values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the data associated with this processor.", "response": "def recompute_data(self, ui):\n        \"\"\"Compute the data associated with this processor.\n\n        This method is thread safe and may take a long time to return. It should not be called from\n         the UI thread. Upon return, the results will be calculated with the latest data available\n         and the cache will not be marked dirty.\n        \"\"\"\n        self.__initialize_cache()\n        with self.__recompute_lock:\n            if self.__cached_value_dirty:\n                try:\n                    calculated_data = self.get_calculated_data(ui)\n                except Exception as e:\n                    import traceback\n                    traceback.print_exc()\n                    traceback.print_stack()\n                    raise\n                self.__cache.set_cached_value(self.__display_item, self.__cache_property_name, calculated_data)\n                self.__cached_value = calculated_data\n                self.__cached_value_dirty = False\n                self.__cached_value_time = time.time()\n            else:\n                calculated_data = None\n            if calculated_data is None:\n                calculated_data = self.get_default_data()\n                if calculated_data is not None:\n                    # if the default is not None, treat is as valid cached data\n                    self.__cache.set_cached_value(self.__display_item, self.__cache_property_name, calculated_data)\n                    self.__cached_value = calculated_data\n                    self.__cached_value_dirty = False\n                    self.__cached_value_time = time.time()\n                else:\n                    # otherwise remove everything from the cache\n                    self.__cache.remove_cached_value(self.__display_item, self.__cache_property_name)\n                    self.__cached_value = None\n                    self.__cached_value_dirty = None\n                    self.__cached_value_time = 0\n            self.__recompute_lock.release()\n            if callable(self.on_thumbnail_updated):\n                self.on_thumbnail_updated()\n            self.__recompute_lock.acquire()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a ThumbnailSource object for the given UI and display item.", "response": "def thumbnail_source_for_display_item(self, ui, display_item: DisplayItem.DisplayItem) -> ThumbnailSource:\n        \"\"\"Returned ThumbnailSource must be closed.\"\"\"\n        with self.__lock:\n            thumbnail_source = self.__thumbnail_sources.get(display_item)\n            if not thumbnail_source:\n                thumbnail_source = ThumbnailSource(ui, display_item)\n                self.__thumbnail_sources[display_item] = thumbnail_source\n\n                def will_delete(thumbnail_source):\n                    del self.__thumbnail_sources[thumbnail_source._display_item]\n\n                thumbnail_source._on_will_delete = will_delete\n            else:\n                assert thumbnail_source._ui == ui\n            return thumbnail_source.add_ref()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload plug - ins.", "response": "def load_plug_ins(app, root_dir):\n    \"\"\"Load plug-ins.\"\"\"\n    global extensions\n\n    ui = app.ui\n\n    # a list of directories in which sub-directories PlugIns will be searched.\n    subdirectories = []\n\n    # the default location is where the directory main packages are located.\n    if root_dir:\n        subdirectories.append(root_dir)\n\n    # also search the default data location; create directory there if it doesn't exist to make it easier for user.\n    # default data location will be application specific.\n    data_location = ui.get_data_location()\n    if data_location is not None:\n        subdirectories.append(data_location)\n        # create directories here if they don't exist\n        plugins_dir = os.path.abspath(os.path.join(data_location, \"PlugIns\"))\n        if not os.path.exists(plugins_dir):\n            logging.info(\"Creating plug-ins directory %s\", plugins_dir)\n            os.makedirs(plugins_dir)\n\n    # search the Nion/Swift subdirectory of the default document location too,\n    # but don't create directories here - avoid polluting user visible directories.\n    document_location = ui.get_document_location()\n    if document_location is not None:\n        subdirectories.append(os.path.join(document_location, \"Nion\", \"Swift\"))\n        # do not create them in documents if they don't exist. this location is optional.\n\n    # build a list of directories that will be loaded as plug-ins.\n    PlugInDir = collections.namedtuple(\"PlugInDir\", [\"directory\", \"relative_path\"])\n    plugin_dirs = list()\n\n    # track directories that have already been searched.\n    seen_plugin_dirs = list()\n\n    # for each subdirectory, look in PlugIns for sub-directories that represent the plug-ins.\n    for subdirectory in subdirectories:\n        plugins_dir = os.path.abspath(os.path.join(subdirectory, \"PlugIns\"))\n\n        if os.path.exists(plugins_dir) and not plugins_dir in seen_plugin_dirs:\n            logging.info(\"Loading plug-ins from %s\", plugins_dir)\n\n            # add the PlugIns directory to the system import path.\n            sys.path.append(plugins_dir)\n\n            # now build a list of sub-directories representing plug-ins within plugins_dir.\n            sorted_relative_paths = sorted([d for d in os.listdir(plugins_dir) if os.path.isdir(os.path.join(plugins_dir, d))])\n            plugin_dirs.extend([PlugInDir(plugins_dir, sorted_relative_path) for sorted_relative_path in sorted_relative_paths])\n\n            # mark plugins_dir as 'seen' to avoid search it twice.\n            seen_plugin_dirs.append(plugins_dir)\n        else:\n            logging.info(\"NOT Loading plug-ins from %s (missing)\", plugins_dir)\n\n    version_map = dict()\n    module_exists_map = dict()\n\n    plugin_adapters = list()\n\n    import nionswift_plugin\n    for module_info in pkgutil.iter_modules(nionswift_plugin.__path__):\n        plugin_adapters.append(ModuleAdapter(nionswift_plugin.__name__, module_info))\n\n    for directory, relative_path in plugin_dirs:\n        plugin_adapters.append(PlugInAdapter(directory, relative_path))\n\n    progress = True\n    while progress:\n        progress = False\n        plugin_adapters_copy = copy.deepcopy(plugin_adapters)\n        plugin_adapters = list()\n        for plugin_adapter in plugin_adapters_copy:\n            manifest_path = plugin_adapter.manifest_path\n            manifest = plugin_adapter.manifest\n            if manifest:\n                manifest_valid = True\n                if not \"name\" in manifest:\n                    logging.info(\"Invalid manifest (missing 'name'): %s\", manifest_path)\n                    manifest_valid = False\n                if not \"identifier\" in manifest:\n                    logging.info(\"Invalid manifest (missing 'identifier'): %s\", manifest_path)\n                    manifest_valid = False\n                if \"identifier\" in manifest and not re.match(\"[_\\-a-zA-Z][_\\-a-zA-Z0-9.]*$\", manifest[\"identifier\"]):\n                    logging.info(\"Invalid manifest (invalid 'identifier': '%s'): %s\", manifest[\"identifier\"], manifest_path)\n                    manifest_valid = False\n                if not \"version\" in manifest:\n                    logging.info(\"Invalid manifest (missing 'version'): %s\", manifest_path)\n                    manifest_valid = False\n                if \"requires\" in manifest and not isinstance(manifest[\"requires\"], list):\n                    logging.info(\"Invalid manifest ('requires' not a list): %s\", manifest_path)\n                    manifest_valid = False\n                if not manifest_valid:\n                    continue\n                for module in manifest.get(\"modules\", list()):\n                    if module in module_exists_map:\n                        module_exists = module_exists_map.get(module)\n                    else:\n                        module_exists = importlib.util.find_spec(module) is not None\n                        module_exists_map[module] = module_exists\n                    if not module_exists:\n                        logging.info(\"Plug-in '\" + plugin_adapter.module_name + \"' NOT loaded (\" + plugin_adapter.module_path + \").\")\n                        logging.info(\"Cannot satisfy requirement (%s): %s\", module, manifest_path)\n                        manifest_valid = False\n                        break\n                for requirement in manifest.get(\"requires\", list()):\n                    # TODO: see https://packaging.pypa.io/en/latest/\n                    requirement_components = requirement.split()\n                    if len(requirement_components) != 3 or requirement_components[1] != \"~=\":\n                        logging.info(\"Invalid manifest (requirement '%s' invalid): %s\", requirement, manifest_path)\n                        manifest_valid = False\n                        break\n                    identifier, operator, version_specifier = requirement_components[0], requirement_components[1], requirement_components[2]\n                    if identifier in version_map:\n                        if Utility.compare_versions(\"~\" + version_specifier, version_map[identifier]) != 0:\n                            logging.info(\"Plug-in '\" + plugin_adapter.module_name + \"' NOT loaded (\" + plugin_adapter.module_path + \").\")\n                            logging.info(\"Cannot satisfy requirement (%s): %s\", requirement, manifest_path)\n                            manifest_valid = False\n                            break\n                    else:\n                        # requirements not loaded yet; add back to plugin_adapters, but don't mark progress since nothing was loaded.\n                        logging.info(\"Plug-in '\" + plugin_adapter.module_name + \"' delayed (%s) (\" + plugin_adapter.module_path + \").\", requirement)\n                        plugin_adapters.append(plugin_adapter)\n                        manifest_valid = False\n                        break\n                if not manifest_valid:\n                    continue\n                version_map[manifest[\"identifier\"]] = manifest[\"version\"]\n            # read the manifests, if any\n            # repeat loop of plug-ins until no plug-ins left in the list\n            #   if all dependencies satisfied for a plug-in, load it\n            #   otherwise defer until next round\n            #   stop if no plug-ins loaded in the round\n            #   count on the user to have correct dependencies\n            module = plugin_adapter.load()\n            if module:\n                __modules.append(module)\n            progress = True\n    for plugin_adapter in plugin_adapters:\n        logging.info(\"Plug-in '\" + plugin_adapter.module_name + \"' NOT loaded (requirements) (\" + plugin_adapter.module_path + \").\")\n\n    notify_modules(\"run\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getMyID(self,gist_name):\n\t\t'''\n\t\tGetting gistID of a gist in order to make the workflow\n\t\teasy and uninterrupted.\n\t\t'''\n\t\tr = requests.get(\n\t\t\t'%s'%BASE_URL+'/users/%s/gists' % self.user,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\t\tif (r.status_code == 200):\n\t\t\tr_text = json.loads(r.text)\n\t\t\tlimit = len(r.json())\n\n\t\t\tfor g,no in zip(r_text, range(0,limit)):\n\t\t\t\tfor ka,va in r.json()[no]['files'].iteritems():\n\t\t\t\t\tif str(va['filename']) == str(gist_name):\n\t\t\t\t\t\treturn r.json()[no]['id']\n\t\t\treturn 0\n\n\t\traise Exception('Username not found')", "response": "Get the gist ID of a gist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the document controller.", "response": "def close(self):\n        \"\"\"Close the document controller.\n\n        This method must be called to shut down the document controller. There are several\n        paths by which it can be called, though.\n\n           * User quits application via menu item. The menu item will call back to Application.exit which will close each\n             document controller by calling this method.\n           * User quits application using dock menu item. The Qt application will call aboutToClose in the document windows\n           * User closes document window via menu item.\n           * User closes document window via close box.\n\n        The main concept of closing is that it is always triggered by the document window closing. This can be initiated\n        from within Python by calling request_close on the document window. When the window closes, either by explicit request\n        or by the user clicking a close box, it will invoke the about_to_close method on the document window. At this point,\n        the window would still be open, so the about_to_close message can be used to tell the document controller to save anything\n        it needs to save and prepare for closing.\n        \"\"\"\n        assert self.__closed == False\n        self.__closed = True\n        self.finish_periodic()  # required to finish periodic operations during tests\n        # dialogs\n        for weak_dialog in self.__dialogs:\n            dialog = weak_dialog()\n            if dialog:\n                try:\n                    dialog.request_close()\n                except Exception as e:\n                    pass\n        # menus\n        self._file_menu = None\n        self._edit_menu = None\n        self._processing_menu = None\n        self._view_menu = None\n        self._window_menu = None\n        self._help_menu = None\n        self._library_menu = None\n        self._processing_arithmetic_menu = None\n        self._processing_reduce_menu = None\n        self._processing_transform_menu = None\n        self._processing_filter_menu = None\n        self._processing_fourier_menu = None\n        self._processing_graphics_menu = None\n        self._processing_sequence_menu = None\n        self._processing_redimension_menu = None\n        self._display_type_menu = None\n\n        if self.__workspace_controller:\n            self.__workspace_controller.close()\n            self.__workspace_controller = None\n        self.__call_soon_event_listener.close()\n        self.__call_soon_event_listener = None\n        self.__filtered_display_items_model.close()\n        self.__filtered_display_items_model = None\n        self.filter_controller.close()\n        self.filter_controller = None\n        self.__display_items_model.close()\n        self.__display_items_model = None\n        # document_model may be shared between several DocumentControllers, so use reference counting\n        # to determine when to close it.\n        self.document_model.remove_ref()\n        self.document_model = None\n        self.did_close_event.fire(self)\n        self.did_close_event = None\n        super().close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_periodic(self, interval: float, listener_fn):\n        class PeriodicListener:\n            def __init__(self, interval: float, listener_fn):\n                self.interval = interval\n                self.__listener_fn = listener_fn\n                # the call function is very performance critical; make it fast by using a property\n                # instead of a logic statement each time.\n                if callable(listener_fn):\n                    self.call = self.__listener_fn\n                else:\n                    def void(*args, **kwargs):\n                        pass\n                    self.call = void\n                self.next_scheduled_time = time.time() + interval\n            def close(self):\n                self.__listener_fn = None\n                def void(*args, **kwargs):\n                    pass\n                self.call = void\n        listener = PeriodicListener(interval, listener_fn)\n        def remove_listener(weak_listener):\n            with self.__weak_periodic_listeners_mutex:\n                self.__weak_periodic_listeners.remove(weak_listener)\n        weak_listener = weakref.ref(listener, remove_listener)\n        with self.__weak_periodic_listeners_mutex:\n            self.__weak_periodic_listeners.append(weak_listener)\n        return listener", "response": "Add a listener function and return a token. Token can be closed or deleted to unlisten."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the display items model with a new container filter and sorting.", "response": "def __update_display_items_model(self, display_items_model: ListModel.FilteredListModel, data_group: typing.Optional[DataGroup.DataGroup], filter_id: typing.Optional[str]) -> None:\n        \"\"\"Update the data item model with a new container, filter, and sorting.\n\n        This is called when the data item model is created or when the user changes\n        the data group or sorting settings.\n        \"\"\"\n\n        with display_items_model.changes():  # change filter and sort together\n            if data_group is not None:\n                display_items_model.container = data_group\n                display_items_model.filter = ListModel.Filter(True)\n                display_items_model.sort_key = None\n                display_items_model.filter_id = None\n            elif filter_id == \"latest-session\":\n                display_items_model.container = self.document_model\n                display_items_model.filter = ListModel.EqFilter(\"session_id\", self.document_model.session_id)\n                display_items_model.sort_key = DataItem.sort_by_date_key\n                display_items_model.sort_reverse = True\n                display_items_model.filter_id = filter_id\n            elif filter_id == \"temporary\":\n                display_items_model.container = self.document_model\n                display_items_model.filter = ListModel.NotEqFilter(\"category\", \"persistent\")\n                display_items_model.sort_key = DataItem.sort_by_date_key\n                display_items_model.sort_reverse = True\n                display_items_model.filter_id = filter_id\n            elif filter_id == \"none\":  # not intended to be used directly\n                display_items_model.container = self.document_model\n                display_items_model.filter = ListModel.Filter(False)\n                display_items_model.sort_key = DataItem.sort_by_date_key\n                display_items_model.sort_reverse = True\n                display_items_model.filter_id = filter_id\n            else:  # \"all\"\n                display_items_model.container = self.document_model\n                display_items_model.filter = ListModel.EqFilter(\"category\", \"persistent\")\n                display_items_model.sort_key = DataItem.sort_by_date_key\n                display_items_model.sort_reverse = True\n                display_items_model.filter_id = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the data item with keyboard focus.", "response": "def focused_data_item(self) -> typing.Optional[DataItem.DataItem]:\n        \"\"\"Return the data item with keyboard focus.\"\"\"\n        return self.__focused_display_item.data_item if self.__focused_display_item else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef selected_display_item(self) -> typing.Optional[DisplayItem.DisplayItem]:\n        # first check for the [focused] data browser\n        display_item = self.focused_display_item\n        if not display_item:\n            selected_display_panel = self.selected_display_panel\n            display_item = selected_display_panel.display_item if selected_display_panel else None\n        return display_item", "response": "Return the selected display item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_two_data_sources(self):\n        selected_display_items = self.selected_display_items\n        if len(selected_display_items) < 2:\n            selected_display_items = list()\n            display_item = self.selected_display_item\n            if display_item:\n                selected_display_items.append(display_item)\n        if len(selected_display_items) == 1:\n            display_item = selected_display_items[0]\n            data_item = display_item.data_item if display_item else None\n            if display_item and len(display_item.graphic_selection.indexes) == 2:\n                index1 = display_item.graphic_selection.anchor_index\n                index2 = list(display_item.graphic_selection.indexes.difference({index1}))[0]\n                graphic1 = display_item.graphics[index1]\n                graphic2 = display_item.graphics[index2]\n                if data_item:\n                    if data_item.is_datum_1d and isinstance(graphic1, Graphics.IntervalGraphic) and isinstance(graphic2, Graphics.IntervalGraphic):\n                        crop_graphic1 = graphic1\n                        crop_graphic2 = graphic2\n                    elif data_item.is_datum_2d and isinstance(graphic1, Graphics.RectangleTypeGraphic) and isinstance(graphic2, Graphics.RectangleTypeGraphic):\n                        crop_graphic1 = graphic1\n                        crop_graphic2 = graphic2\n                    else:\n                        crop_graphic1 = self.__get_crop_graphic(display_item)\n                        crop_graphic2 = crop_graphic1\n                else:\n                    crop_graphic1 = self.__get_crop_graphic(display_item)\n                    crop_graphic2 = crop_graphic1\n            else:\n                crop_graphic1 = self.__get_crop_graphic(display_item)\n                crop_graphic2 = crop_graphic1\n            return (display_item, crop_graphic1), (display_item, crop_graphic2)\n        if len(selected_display_items) == 2:\n            display_item1 = selected_display_items[0]\n            crop_graphic1 = self.__get_crop_graphic(display_item1)\n            display_item2 = selected_display_items[1]\n            crop_graphic2 = self.__get_crop_graphic(display_item2)\n            return (display_item1, crop_graphic1), (display_item2, crop_graphic2)\n        return None", "response": "Get two sensible data sources which may be the same."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_origin_and_size(canvas_size, data_shape, image_canvas_mode, image_zoom, image_position) -> typing.Tuple[typing.Any, typing.Any]:\n    if data_shape is None:\n        return None, None\n    if image_canvas_mode == \"fill\":\n        data_shape = data_shape\n        scale_h = float(data_shape[1]) / canvas_size[1]\n        scale_v = float(data_shape[0]) / canvas_size[0]\n        if scale_v < scale_h:\n            image_canvas_size = (canvas_size[0], canvas_size[0] * data_shape[1] / data_shape[0])\n        else:\n            image_canvas_size = (canvas_size[1] * data_shape[0] / data_shape[1], canvas_size[1])\n        image_canvas_origin = (canvas_size[0] * 0.5 - image_canvas_size[0] * 0.5, canvas_size[1] * 0.5 - image_canvas_size[1] * 0.5)\n    elif image_canvas_mode == \"fit\":\n        image_canvas_size = canvas_size\n        image_canvas_origin = (0, 0)\n    elif image_canvas_mode == \"1:1\":\n        image_canvas_size = data_shape\n        image_canvas_origin = (canvas_size[0] * 0.5 - image_canvas_size[0] * 0.5, canvas_size[1] * 0.5 - image_canvas_size[1] * 0.5)\n    elif image_canvas_mode == \"2:1\":\n        image_canvas_size = (data_shape[0] * 0.5, data_shape[1] * 0.5)\n        image_canvas_origin = (canvas_size[0] * 0.5 - image_canvas_size[0] * 0.5, canvas_size[1] * 0.5 - image_canvas_size[1] * 0.5)\n    else:\n        image_canvas_size = (canvas_size[0] * image_zoom, canvas_size[1] * image_zoom)\n        canvas_rect = Geometry.fit_to_size(((0, 0), image_canvas_size), data_shape)\n        image_canvas_origin_y = (canvas_size[0] * 0.5) - image_position[0] * canvas_rect[1][0] - canvas_rect[0][0]\n        image_canvas_origin_x = (canvas_size[1] * 0.5) - image_position[1] * canvas_rect[1][1] - canvas_rect[0][1]\n        image_canvas_origin = (image_canvas_origin_y, image_canvas_origin_x)\n    return image_canvas_origin, image_canvas_size", "response": "Calculate origin and size for canvas size data shape and image display parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread data items from the data reference handler and return as a list.", "response": "def read_library(persistent_storage_system, ignore_older_files) -> typing.Dict:\n    \"\"\"Read data items from the data reference handler and return as a list.\n\n    Data items will have persistent_object_context set upon return, but caller will need to call finish_reading\n    on each of the data items.\n    \"\"\"\n    data_item_uuids = set()\n    utilized_deletions = set()  # the uuid's skipped due to being deleted\n    deletions = list()\n\n    reader_info_list, library_updates = auto_migrate_storage_system(persistent_storage_system=persistent_storage_system,\n                                                                    new_persistent_storage_system=persistent_storage_system,\n                                                                    data_item_uuids=data_item_uuids,\n                                                                    deletions=deletions,\n                                                                    utilized_deletions=utilized_deletions,\n                                                                    ignore_older_files=ignore_older_files)\n\n    # next, for each auto migration, create a temporary storage system and read items from that storage system\n    # using auto_migrate_storage_system. the data items returned will have been copied to the current storage\n    # system (persistent object context).\n    for auto_migration in reversed(persistent_storage_system.get_auto_migrations()):\n        old_persistent_storage_system = FileStorageSystem(auto_migration.library_path, auto_migration.paths) if auto_migration.paths else auto_migration.storage_system\n        new_reader_info_list, new_library_updates = auto_migrate_storage_system(persistent_storage_system=old_persistent_storage_system,\n                                                                                new_persistent_storage_system=persistent_storage_system,\n                                                                                data_item_uuids=data_item_uuids,\n                                                                                deletions=deletions,\n                                                                                utilized_deletions=utilized_deletions,\n                                                                                ignore_older_files=ignore_older_files)\n        reader_info_list.extend(new_reader_info_list)\n        library_updates.update(new_library_updates)\n\n    assert len(reader_info_list) == len(data_item_uuids)\n\n    library_storage_properties = persistent_storage_system.library_storage_properties\n\n    for reader_info in reader_info_list:\n        properties = reader_info.properties\n        properties = Utility.clean_dict(copy.deepcopy(properties) if properties else dict())\n        version = properties.get(\"version\", 0)\n        if version == DataItem.DataItem.writer_version:\n            data_item_uuid = uuid.UUID(properties.get(\"uuid\", uuid.uuid4()))\n            library_update = library_updates.get(data_item_uuid, dict())\n            library_storage_properties.setdefault(\"connections\", list()).extend(library_update.get(\"connections\", list()))\n            library_storage_properties.setdefault(\"computations\", list()).extend(library_update.get(\"computations\", list()))\n            library_storage_properties.setdefault(\"display_items\", list()).extend(library_update.get(\"display_items\", list()))\n\n    # mark deletions that need to be tracked because they've been deleted but are also present in older libraries\n    # and would be migrated during reading unless they explicitly are prevented from doing so (via data_item_deletions).\n    # utilized deletions are the ones that were attempted; if nothing was attempted, then no reason to track it anymore\n    # since there is nothing to migrate in the future.\n    library_storage_properties[\"data_item_deletions\"] = [str(uuid_) for uuid_ in utilized_deletions]\n\n    connections_list = library_storage_properties.get(\"connections\", list())\n    assert len(connections_list) == len({connection.get(\"uuid\") for connection in connections_list})\n\n    computations_list = library_storage_properties.get(\"computations\", list())\n    assert len(computations_list) == len({computation.get(\"uuid\") for computation in computations_list})\n\n    # migrations\n\n    if library_storage_properties.get(\"version\", 0) < 2:\n        for data_group_properties in library_storage_properties.get(\"data_groups\", list()):\n            data_group_properties.pop(\"data_groups\")\n            display_item_references = data_group_properties.setdefault(\"display_item_references\", list())\n            data_item_uuid_strs = data_group_properties.pop(\"data_item_uuids\", list())\n            for data_item_uuid_str in data_item_uuid_strs:\n                for display_item_properties in library_storage_properties.get(\"display_items\", list()):\n                    data_item_references = [d.get(\"data_item_reference\", None) for d in display_item_properties.get(\"display_data_channels\", list())]\n                    if data_item_uuid_str in data_item_references:\n                        display_item_references.append(display_item_properties[\"uuid\"])\n        data_item_uuid_to_display_item_uuid_map = dict()\n        data_item_uuid_to_display_item_dict_map = dict()\n        display_to_display_item_map = dict()\n        display_to_display_data_channel_map = dict()\n        for display_item_properties in library_storage_properties.get(\"display_items\", list()):\n            display_to_display_item_map[display_item_properties[\"display\"][\"uuid\"]] = display_item_properties[\"uuid\"]\n            display_to_display_data_channel_map[display_item_properties[\"display\"][\"uuid\"]] = display_item_properties[\"display_data_channels\"][0][\"uuid\"]\n            data_item_references = [d.get(\"data_item_reference\", None) for d in display_item_properties.get(\"display_data_channels\", list())]\n            for data_item_uuid_str in data_item_references:\n                data_item_uuid_to_display_item_uuid_map.setdefault(data_item_uuid_str, display_item_properties[\"uuid\"])\n                data_item_uuid_to_display_item_dict_map.setdefault(data_item_uuid_str, display_item_properties)\n            display_item_properties.pop(\"display\", None)\n        for workspace_properties in library_storage_properties.get(\"workspaces\", list()):\n            def replace1(d):\n                if \"children\" in d:\n                    for dd in d[\"children\"]:\n                        replace1(dd)\n                if \"data_item_uuid\" in d:\n                    data_item_uuid_str = d.pop(\"data_item_uuid\")\n                    display_item_uuid_str = data_item_uuid_to_display_item_uuid_map.get(data_item_uuid_str)\n                    if display_item_uuid_str:\n                        d[\"display_item_uuid\"] = display_item_uuid_str\n            replace1(workspace_properties[\"layout\"])\n        for connection_dict in library_storage_properties.get(\"connections\", list()):\n            source_uuid_str = connection_dict[\"source_uuid\"]\n            if connection_dict[\"type\"] == \"interval-list-connection\":\n                connection_dict[\"source_uuid\"] = display_to_display_item_map.get(source_uuid_str, None)\n            if connection_dict[\"type\"] == \"property-connection\" and connection_dict[\"source_property\"] == \"slice_interval\":\n                connection_dict[\"source_uuid\"] = display_to_display_data_channel_map.get(source_uuid_str, None)\n\n        def fix_specifier(specifier_dict):\n            if specifier_dict.get(\"type\") in (\"data_item\", \"display_xdata\", \"cropped_xdata\", \"cropped_display_xdata\", \"filter_xdata\", \"filtered_xdata\"):\n                if specifier_dict.get(\"uuid\") in data_item_uuid_to_display_item_dict_map:\n                    specifier_dict[\"uuid\"] = data_item_uuid_to_display_item_dict_map[specifier_dict[\"uuid\"]][\"display_data_channels\"][0][\"uuid\"]\n                else:\n                    specifier_dict.pop(\"uuid\", None)\n            if specifier_dict.get(\"type\") == \"data_item\":\n                specifier_dict[\"type\"] = \"data_source\"\n            if specifier_dict.get(\"type\") == \"data_item_object\":\n                specifier_dict[\"type\"] = \"data_item\"\n            if specifier_dict.get(\"type\") == \"region\":\n                specifier_dict[\"type\"] = \"graphic\"\n\n        for computation_dict in library_storage_properties.get(\"computations\", list()):\n            for variable_dict in computation_dict.get(\"variables\", list()):\n                if \"specifier\" in variable_dict:\n                    specifier_dict = variable_dict[\"specifier\"]\n                    if specifier_dict is not None:\n                        fix_specifier(specifier_dict)\n                if \"secondary_specifier\" in variable_dict:\n                    specifier_dict = variable_dict[\"secondary_specifier\"]\n                    if specifier_dict is not None:\n                        fix_specifier(specifier_dict)\n            for result_dict in computation_dict.get(\"results\", list()):\n                fix_specifier(result_dict[\"specifier\"])\n\n        library_storage_properties[\"version\"] = DocumentModel.DocumentModel.library_version\n\n    # TODO: add consistency checks: no duplicated items [by uuid] such as connections or computations or data items\n\n    assert library_storage_properties[\"version\"] == DocumentModel.DocumentModel.library_version\n\n    persistent_storage_system.rewrite_properties(library_storage_properties)\n\n    properties = copy.deepcopy(library_storage_properties)\n\n    for reader_info in reader_info_list:\n        data_item_properties = Utility.clean_dict(reader_info.properties if reader_info.properties else dict())\n        if data_item_properties.get(\"version\", 0) == DataItem.DataItem.writer_version:\n            data_item_properties[\"__large_format\"] = reader_info.large_format\n            data_item_properties[\"__identifier\"] = reader_info.identifier\n            properties.setdefault(\"data_items\", list()).append(data_item_properties)\n\n    def data_item_created(data_item_properties: typing.Mapping) -> str:\n        return data_item_properties.get(\"created\", \"1900-01-01T00:00:00.000000\")\n\n    properties[\"data_items\"] = sorted(properties.get(\"data_items\", list()), key=data_item_created)\n\n    return properties"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmigrating items from the persistent storage system to the object context.", "response": "def auto_migrate_storage_system(*, persistent_storage_system=None, new_persistent_storage_system=None, data_item_uuids=None, deletions: typing.List[uuid.UUID] = None, utilized_deletions: typing.Set[uuid.UUID] = None, ignore_older_files: bool = True):\n    \"\"\"Migrate items from the storage system to the object context.\n\n    Files in data_item_uuids have already been loaded and are ignored (not migrated).\n\n    Files in deletes have been deleted in object context and are ignored (not migrated) and then added\n    to the utilized deletions list.\n\n    Data items will have persistent_object_context set upon return, but caller will need to call finish_reading\n    on each of the data items.\n    \"\"\"\n    storage_handlers = persistent_storage_system.find_data_items()\n    ReaderInfo = collections.namedtuple(\"ReaderInfo\", [\"properties\", \"changed_ref\", \"large_format\", \"storage_handler\", \"identifier\"])\n    reader_info_list = list()\n    for storage_handler in storage_handlers:\n        try:\n            large_format = isinstance(storage_handler, HDF5Handler.HDF5Handler)\n            properties = Migration.transform_to_latest(storage_handler.read_properties())\n            reader_info = ReaderInfo(properties, [False], large_format, storage_handler, storage_handler.reference)\n            reader_info_list.append(reader_info)\n        except Exception as e:\n            logging.debug(\"Error reading %s\", storage_handler.reference)\n            import traceback\n            traceback.print_exc()\n            traceback.print_stack()\n    library_storage_properties = persistent_storage_system.library_storage_properties\n    for deletion in copy.deepcopy(library_storage_properties.get(\"data_item_deletions\", list())):\n        if not deletion in deletions:\n            deletions.append(deletion)\n    preliminary_library_updates = dict()\n    library_updates = dict()\n    if not ignore_older_files:\n        Migration.migrate_to_latest(reader_info_list, preliminary_library_updates)\n    good_reader_info_list = list()\n    count = len(reader_info_list)\n    for index, reader_info in enumerate(reader_info_list):\n        storage_handler = reader_info.storage_handler\n        properties = reader_info.properties\n        try:\n            version = properties.get(\"version\", 0)\n            if version == DataItem.DataItem.writer_version:\n                data_item_uuid = uuid.UUID(properties[\"uuid\"])\n                if not data_item_uuid in data_item_uuids:\n                    if str(data_item_uuid) in deletions:\n                        utilized_deletions.add(data_item_uuid)\n                    else:\n                        auto_migrate_data_item(reader_info, persistent_storage_system, new_persistent_storage_system, index, count)\n                        good_reader_info_list.append(reader_info)\n                        data_item_uuids.add(data_item_uuid)\n                        library_update = preliminary_library_updates.get(data_item_uuid)\n                        if library_update:\n                            library_updates[data_item_uuid] = library_update\n        except Exception as e:\n            logging.debug(\"Error reading %s\", storage_handler.reference)\n            import traceback\n            traceback.print_exc()\n            traceback.print_stack()\n    return good_reader_info_list, library_updates"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rewrite_properties(self, properties):\n        with self.__properties_lock:\n            self.__properties = properties\n        self.__write_properties(None)", "response": "Set the properties and write to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analisar(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='AtivarSAT',\n                classe_resposta=RespostaAtivarSAT,\n                campos=(\n                        ('numeroSessao', int),\n                        ('EEEEE', unicode),\n                        ('mensagem', unicode),\n                        ('cod', unicode),\n                        ('mensagemSEFAZ', unicode),\n                        ('CSR', unicode),\n                    ),\n                campos_alternativos=[\n                        # se a ativa\u00e7\u00e3o falhar espera-se o padr\u00e3o de campos\n                        # no retorno...\n                        RespostaSAT.CAMPOS,\n                    ]\n            )\n        if resposta.EEEEE not in (\n                ATIVADO_CORRETAMENTE,\n                CSR_ICPBRASIL_CRIADO_SUCESSO,):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Analisar uma : class : RespostaAtivarSAT a partir do retorno."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a view task for this hardware source.", "response": "def create_view_task(self, frame_parameters: dict=None, channels_enabled: typing.List[bool]=None, buffer_size: int=1) -> ViewTask:\n        \"\"\"Create a view task for this hardware source.\n\n        .. versionadded:: 1.0\n\n        :param frame_parameters: The frame parameters for the view. Pass None for defaults.\n        :type frame_parameters: :py:class:`FrameParameters`\n        :param channels_enabled: The enabled channels for the view. Pass None for defaults.\n        :type channels_enabled: List of booleans.\n        :param buffer_size: The buffer size if using the grab_earliest method. Default is 1.\n        :type buffer_size: int\n        :return: The :py:class:`ViewTask` object.\n        :rtype: :py:class:`ViewTask`\n\n        Callers should call close on the returned task when finished.\n\n        See :py:class:`ViewTask` for examples of how to use.\n        \"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading configuration from yaml source", "response": "def from_yaml():\n    \"\"\" Load configuration from yaml source(s), cached to only run once \"\"\"\n    default_yaml_str = snippets.get_snippet_content('hatchery.yml')\n    ret = yaml.load(default_yaml_str, Loader=yaml.RoundTripLoader)\n    for config_path in CONFIG_LOCATIONS:\n        config_path = os.path.expanduser(config_path)\n        if os.path.isfile(config_path):\n            with open(config_path) as config_file:\n                config_dict = yaml.load(config_file, Loader=yaml.RoundTripLoader)\n                if config_dict is None:\n                    continue\n                for k, v in config_dict.items():\n                    if k not in ret.keys():\n                        raise ConfigError(\n                            'found garbage key \"{}\" in {}'.format(k, config_path)\n                        )\n                    ret[k] = v\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_pypirc(pypi_repository):\n    ret = {}\n    pypirc_locations = PYPIRC_LOCATIONS\n    for pypirc_path in pypirc_locations:\n        pypirc_path = os.path.expanduser(pypirc_path)\n        if os.path.isfile(pypirc_path):\n            parser = configparser.SafeConfigParser()\n            parser.read(pypirc_path)\n            if 'distutils' not in parser.sections():\n                continue\n            if 'index-servers' not in parser.options('distutils'):\n                continue\n            if pypi_repository not in parser.get('distutils', 'index-servers'):\n                continue\n            if pypi_repository in parser.sections():\n                for option in parser.options(pypi_repository):\n                    ret[option] = parser.get(pypi_repository, option)\n    if not ret:\n        raise ConfigError(\n            'repository does not appear to be configured in pypirc ({})'.format(pypi_repository) +\n            ', remember that it needs an entry in [distutils] and its own section'\n        )\n    return ret", "response": "Load configuration from. pypirc file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pypirc_temp(index_url):\n    pypirc_file = tempfile.NamedTemporaryFile(suffix='.pypirc', delete=False)\n    print(pypirc_file.name)\n    with open(pypirc_file.name, 'w') as fh:\n        fh.write(PYPIRC_TEMPLATE.format(index_name=PYPIRC_TEMP_INDEX_NAME, index_url=index_url))\n    return pypirc_file.name", "response": "Create a temporary pypirc file for interaction with twine"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_api(version: str, ui_version: str=None) -> API_1:\n    ui_version = ui_version if ui_version else \"~1.0\"\n    return _get_api_with_app(version, ui_version, ApplicationModule.app)", "response": "Get a versioned interface matching the given version and ui_version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mask_xdata_with_shape(self, shape: DataAndMetadata.ShapeType) -> DataAndMetadata.DataAndMetadata:\n        mask = self._graphic.get_mask(shape)\n        return DataAndMetadata.DataAndMetadata.from_data(mask)", "response": "Return the extended data mask created by this graphic as extended data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef end(self, value: typing.Union[float, NormPointType]) -> None:\n        self.set_property(\"end\", value)", "response": "Set the end property in relative coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the start property in relative coordinates.", "response": "def start(self, value: typing.Union[float, NormPointType]) -> None:\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        self.set_property(\"start\", value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef data(self, data: numpy.ndarray) -> None:\n        self.__data_item.set_data(numpy.copy(data))", "response": "Set the data.\n\n        :param data: A numpy ndarray.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the extended data of this data item display.", "response": "def display_xdata(self) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Return the extended data of this data item display.\n\n        Display data will always be 1d or 2d and either int, float, or RGB data type.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_data_channel = self.__display_item.display_data_channel\n        return display_data_channel.get_calculated_display_values(True).display_data_and_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_dimensional_calibrations(self, dimensional_calibrations: typing.List[CalibrationModule.Calibration]) -> None:\n        self.__data_item.set_dimensional_calibrations(dimensional_calibrations)", "response": "Set the dimensional calibrations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_metadata_value(self, key: str) -> typing.Any:\n        return self._data_item.get_metadata_value(key)", "response": "Get the metadata value for the given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the metadata value for the given key.", "response": "def set_metadata_value(self, key: str, value: typing.Any) -> None:\n        \"\"\"Set the metadata value for the given key.\n\n        There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n        applications. Please consult reference documentation for valid keys.\n\n        If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n        by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n        Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n        using the ``metadata_value`` methods over directly accessing ``metadata``.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        self._data_item.set_metadata_value(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef graphics(self) -> typing.List[Graphic]:\n        return [Graphic(graphic) for graphic in self.__display_item.graphics]", "response": "Return the graphics attached to this data item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a point graphic to the data item.", "response": "def add_point_region(self, y: float, x: float) -> Graphic:\n        \"\"\"Add a point graphic to the data item.\n\n        :param x: The x coordinate, in relative units [0.0, 1.0]\n        :param y: The y coordinate, in relative units [0.0, 1.0]\n        :return: The :py:class:`nion.swift.Facade.Graphic` object that was added.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        graphic = Graphics.PointGraphic()\n        graphic.position = Geometry.FloatPoint(y, x)\n        self.__display_item.add_graphic(graphic)\n        return Graphic(graphic)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mask of any extended data on this data item as extended data.", "response": "def mask_xdata(self) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Return the mask by combining any mask graphics on this data item as extended data.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_data_channel = self.__display_item.display_data_channel\n        shape = display_data_channel.display_data_shape\n        mask = numpy.zeros(shape)\n        for graphic in self.__display_item.graphics:\n            if isinstance(graphic, (Graphics.SpotGraphic, Graphics.WedgeGraphic, Graphics.RingGraphic, Graphics.LatticeGraphic)):\n                mask = numpy.logical_or(mask, graphic.get_mask(shape))\n        return DataAndMetadata.DataAndMetadata.from_data(mask)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the data item associated with this display panel.", "response": "def data_item(self) -> DataItem:\n        \"\"\"Return the data item associated with this display panel.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_panel = self.__display_panel\n        if not display_panel:\n            return None\n        data_item = display_panel.data_item\n        return DataItem(data_item) if data_item else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the data item associated with this display panel.", "response": "def set_data_item(self, data_item: DataItem) -> None:\n        \"\"\"Set the data item associated with this display panel.\n\n        :param data_item: The :py:class:`nion.swift.Facade.DataItem` object to add.\n\n        This will replace whatever data item, browser, or controller is currently in the display panel with the single\n        data item.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_panel = self.__display_panel\n        if display_panel:\n            display_item = data_item._data_item.container.get_display_item_for_data_item(data_item._data_item) if data_item._data_item.container else None\n            display_panel.set_display_panel_display_item(display_item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a data item to the group.", "response": "def add_data_item(self, data_item: DataItem) -> None:\n        \"\"\"Add a data item to the group.\n\n        :param data_item: The :py:class:`nion.swift.Facade.DataItem` object to add.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_item = data_item._data_item.container.get_display_item_for_data_item(data_item._data_item) if data_item._data_item.container else None\n        if display_item:\n            self.__data_group.append_display_item(display_item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self) -> None:\n        self.__data_channel_buffer.stop()\n        self.__data_channel_buffer.close()\n        self.__data_channel_buffer = None\n        if not self.__was_playing:\n            self.__hardware_source.stop_playing()", "response": "Close the task.\n\n        .. versionadded:: 1.0\n\n        This method must be called when the task is no longer needed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrecord data and return a list of data_and_metadata objects.", "response": "def record(self, frame_parameters: dict=None, channels_enabled: typing.List[bool]=None, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        \"\"\"Record data and return a list of data_and_metadata objects.\n\n        .. versionadded:: 1.0\n\n        :param frame_parameters: The frame parameters for the record. Pass None for defaults.\n        :type frame_parameters: :py:class:`FrameParameters`\n        :param channels_enabled: The enabled channels for the record. Pass None for defaults.\n        :type channels_enabled: List of booleans.\n        :param timeout: The timeout in seconds. Pass None to use default.\n        :return: The list of data and metadata items that were read.\n        :rtype: list of :py:class:`DataAndMetadata`\n        \"\"\"\n        if frame_parameters:\n            self.__hardware_source.set_record_frame_parameters(self.__hardware_source.get_frame_parameters_from_dict(frame_parameters))\n        if channels_enabled is not None:\n            for channel_index, channel_enabled in enumerate(channels_enabled):\n                self.__hardware_source.set_channel_enabled(channel_index, channel_enabled)\n        self.__hardware_source.start_recording()\n        return self.__hardware_source.get_next_xdatas_to_finish(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a record task for this hardware source.", "response": "def create_record_task(self, frame_parameters: dict=None, channels_enabled: typing.List[bool]=None) -> RecordTask:\n        \"\"\"Create a record task for this hardware source.\n\n        .. versionadded:: 1.0\n\n        :param frame_parameters: The frame parameters for the record. Pass None for defaults.\n        :type frame_parameters: :py:class:`FrameParameters`\n        :param channels_enabled: The enabled channels for the record. Pass None for defaults.\n        :type channels_enabled: List of booleans.\n        :return: The :py:class:`RecordTask` object.\n        :rtype: :py:class:`RecordTask`\n\n        Callers should call close on the returned task when finished.\n\n        See :py:class:`RecordTask` for examples of how to use.\n        \"\"\"\n        return RecordTask(self.__hardware_source, frame_parameters, channels_enabled)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_view_task(self, frame_parameters: dict=None, channels_enabled: typing.List[bool]=None, buffer_size: int=1) -> ViewTask:\n        return ViewTask(self.__hardware_source, frame_parameters, channels_enabled, buffer_size)", "response": "Create a view task for this hardware source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grab_next_to_finish(self, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        self.start_playing()\n        return self.__hardware_source.get_next_xdatas_to_finish(timeout)", "response": "Grabs the next frame to finish and returns it as data and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_control_output(self, name: str, value: float, *, options: dict=None) -> None:\n        self.__instrument.set_control_output(name, value, options)", "response": "Set the value of a control asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of a float property.", "response": "def get_property_as_float(self, name: str) -> float:\n        \"\"\"Return the value of a float property.\n\n        :return: The property value (float).\n\n        Raises exception if property with name doesn't exist.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return float(self.__instrument.get_property(name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_property_as_float(self, name: str, value: float) -> None:\n        self.__instrument.set_property(name, float(value))", "response": "Set the value of a float property."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of data items.", "response": "def data_items(self) -> typing.List[DataItem]:\n        \"\"\"Return the list of data items.\n\n        :return: The list of :py:class:`nion.swift.Facade.DataItem` objects.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return [DataItem(data_item) for data_item in self.__document_model.data_items]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_items(self) -> typing.List[Display]:\n        return [Display(display_item) for display_item in self.__document_model.display_items]", "response": "Return the list of display items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of data items that are data sources for the data item.", "response": "def get_source_data_items(self, data_item: DataItem) -> typing.List[DataItem]:\n        \"\"\"Return the list of data items that are data sources for the data item.\n\n        :return: The list of :py:class:`nion.swift.Facade.DataItem` objects.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return [DataItem(data_item) for data_item in self._document_model.get_source_data_items(data_item._data_item)] if data_item else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the dependent data items the data item argument.", "response": "def get_dependent_data_items(self, data_item: DataItem) -> typing.List[DataItem]:\n        \"\"\"Return the dependent data items the data item argument.\n\n        :return: The list of :py:class:`nion.swift.Facade.DataItem` objects.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return [DataItem(data_item) for data_item in self._document_model.get_dependent_data_items(data_item._data_item)] if data_item else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an empty data item in the library.", "response": "def create_data_item(self, title: str=None) -> DataItem:\n        \"\"\"Create an empty data item in the library.\n\n        :param title: The title of the data item (optional).\n        :return: The new :py:class:`nion.swift.Facade.DataItem` object.\n        :rtype: :py:class:`nion.swift.Facade.DataItem`\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        data_item = DataItemModule.DataItem()\n        data_item.ensure_data_source()\n        if title is not None:\n            data_item.title = title\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_data_item_from_data(self, data: numpy.ndarray, title: str=None) -> DataItem:\n        return self.create_data_item_from_data_and_metadata(DataAndMetadata.DataAndMetadata.from_data(data), title)", "response": "Create a data item in the library from an ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_data_item_from_data_and_metadata(self, data_and_metadata: DataAndMetadata.DataAndMetadata, title: str=None) -> DataItem:\n        data_item = DataItemModule.new_data_item(data_and_metadata)\n        if title is not None:\n            data_item.title = title\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)", "response": "Create a new data item in the library from a data and metadata object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_data_item(self, data_item: DataItem) -> DataItem:\n        data_item = copy.deepcopy(data_item._data_item)\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)", "response": "Copy a data item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_or_create_data_group(self, title: str) -> DataGroup:\n        return DataGroup(self.__document_model.get_or_create_data_group(title))", "response": "Get or create a data group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_data_item_for_hardware_source(self, hardware_source, channel_id: str=None, processor_id: str=None, create_if_needed: bool=False, large_format: bool=False) -> DataItem:\n        assert hardware_source is not None\n        hardware_source_id = hardware_source._hardware_source.hardware_source_id\n        document_model = self._document_model\n        data_item_reference_key = document_model.make_data_item_reference_key(hardware_source_id, channel_id, processor_id)\n        return self.get_data_item_for_reference_key(data_item_reference_key, create_if_needed=create_if_needed, large_format=large_format)", "response": "Get the data item associated with a hardware source and channel id and processor id. Optionally create if missing."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_data_item_for_reference_key(self, data_item_reference_key: str=None, create_if_needed: bool=False, large_format: bool=False) -> DataItem:\n        document_model = self._document_model\n        data_item_reference = document_model.get_data_item_reference(data_item_reference_key)\n        data_item = data_item_reference.data_item\n        if data_item is None and create_if_needed:\n            data_item = DataItemModule.DataItem(large_format=large_format)\n            data_item.ensure_data_source()\n            document_model.append_data_item(data_item)\n            document_model.setup_channel(data_item_reference_key, data_item)\n            data_item.session_id = document_model.session_id\n            data_item = document_model.get_data_item_reference(data_item_reference_key).data_item\n        return DataItem(data_item) if data_item else None", "response": "Get the data item associated with a data item reference key. Optionally create if none is found."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the data item with the given UUID.", "response": "def get_data_item_by_uuid(self, data_item_uuid: uuid_module.UUID) -> DataItem:\n        \"\"\"Get the data item with the given UUID.\n\n        .. versionadded:: 1.0\n\n        Status: Provisional\n        Scriptable: Yes\n        \"\"\"\n        data_item = self._document_model.get_data_item_by_uuid(data_item_uuid)\n        return DataItem(data_item) if data_item else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the graphic with the given UUID.", "response": "def get_graphic_by_uuid(self, graphic_uuid: uuid_module.UUID) -> Graphic:\n        \"\"\"Get the graphic with the given UUID.\n\n        .. versionadded:: 1.0\n\n        Status: Provisional\n        Scriptable: Yes\n        \"\"\"\n        for display_item in self._document_model.display_items:\n            for graphic in display_item.graphics:\n                if graphic.uuid == graphic_uuid:\n                    return Graphic(graphic)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether the library value for the given key exists.", "response": "def has_library_value(self, key: str) -> bool:\n        \"\"\"Return whether the library value for the given key exists.\n\n        Please consult the developer documentation for a list of valid keys.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        desc = Metadata.session_key_map.get(key)\n        if desc is not None:\n            field_id = desc['path'][-1]\n            return bool(getattr(ApplicationData.get_session_metadata_model(), field_id, None))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_library_value(self, key: str) -> typing.Any:\n        desc = Metadata.session_key_map.get(key)\n        if desc is not None:\n            field_id = desc['path'][-1]\n            return getattr(ApplicationData.get_session_metadata_model(), field_id)\n        raise KeyError()", "response": "Get the library value for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_library_value(self, key: str, value: typing.Any) -> None:\n        desc = Metadata.session_key_map.get(key)\n        if desc is not None:\n            field_id = desc['path'][-1]\n            setattr(ApplicationData.get_session_metadata_model(), field_id, value)\n            return\n        raise KeyError()", "response": "Set the library value for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the library value for the given key.", "response": "def delete_library_value(self, key: str) -> None:\n        \"\"\"Delete the library value for the given key.\n\n        Please consult the developer documentation for a list of valid keys.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        desc = Metadata.session_key_map.get(key)\n        if desc is not None:\n            field_id = desc['path'][-1]\n            setattr(ApplicationData.get_session_metadata_model(), field_id, None)\n            return\n        raise KeyError()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_display_panels(self) -> typing.List[DisplayPanel]:\n        return [DisplayPanel(display_panel) for display_panel in self.__document_controller.workspace_controller.display_panels]", "response": "Return the list of display panels currently visible."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_display_panel_by_id(self, identifier: str) -> DisplayPanel:\n        display_panel = next(\n            (display_panel for display_panel in self.__document_controller.workspace_controller.display_panels if\n            display_panel.identifier.lower() == identifier.lower()), None)\n        return DisplayPanel(display_panel) if display_panel else None", "response": "Return the display panel with the identifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display_data_item(self, data_item: DataItem, source_display_panel=None, source_data_item=None):\n        for display_panel in self.__document_controller.workspace_controller.display_panels:\n            if display_panel.data_item == data_item._data_item:\n                display_panel.request_focus()\n                return DisplayPanel(display_panel)\n        result_display_panel = self.__document_controller.next_result_display_panel()\n        if result_display_panel:\n            display_item = self.__document_controller.document_model.get_display_item_for_data_item(data_item._data_item)\n            result_display_panel.set_display_panel_display_item(display_item)\n            result_display_panel.request_focus()\n            return DisplayPanel(result_display_panel)\n        return None", "response": "Displays a new data item and gives it keyboard focus."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_get_string_message_box(self, caption: str, text: str, accepted_fn, rejected_fn=None, accepted_text: str=None, rejected_text: str=None) -> None:\n        workspace = self.__document_controller.workspace_controller\n        workspace.pose_get_string_message_box(caption, text, accepted_fn, rejected_fn, accepted_text, rejected_text)", "response": "Show a dialog box and ask for a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a data item in the library from data.", "response": "def create_data_item_from_data(self, data: numpy.ndarray, title: str=None) -> DataItem:\n        \"\"\"Create a data item in the library from data.\n\n        .. versionadded:: 1.0\n        .. deprecated:: 1.1\n           Use :py:meth:`~nion.swift.Facade.Library.create_data_item_from_data` instead.\n\n        Scriptable: No\n        \"\"\"\n        return DataItem(self.__document_controller.add_data(data, title))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_data_item_from_data_and_metadata(self, data_and_metadata: DataAndMetadata.DataAndMetadata, title: str=None) -> DataItem:\n        data_item = DataItemModule.new_data_item(data_and_metadata)\n        if title is not None:\n            data_item.title = title\n        self.__document_controller.document_model.append_data_item(data_item)\n        return DataItem(data_item)", "response": "Create a data item from the data and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting or create a data group.", "response": "def get_or_create_data_group(self, title: str) -> DataGroup:\n        \"\"\"Get (or create) a data group.\n\n        .. versionadded:: 1.0\n        .. deprecated:: 1.1\n           Use :py:meth:`~nion.swift.Facade.Library.create_data_item_from_data` instead.\n\n        Scriptable: No\n        \"\"\"\n        return DataGroup(self.__document_controller.document_model.get_or_create_data_group(title))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the document windows.", "response": "def document_windows(self) -> typing.List[DocumentWindow]:\n        \"\"\"Return the document windows.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return [DocumentWindow(document_controller) for document_controller in self.__application.document_controllers]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_calibration(self, offset: float=None, scale: float=None, units: str=None) -> CalibrationModule.Calibration:\n        return CalibrationModule.Calibration(offset, scale, units)", "response": "Create a new calibration object with the specified offset scale and units."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_data_descriptor(self, is_sequence: bool, collection_dimension_count: int, datum_dimension_count: int) -> DataAndMetadata.DataDescriptor:\n        return DataAndMetadata.DataDescriptor(is_sequence, collection_dimension_count, datum_dimension_count)", "response": "Create a data descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_data_and_metadata(self, data: numpy.ndarray, intensity_calibration: CalibrationModule.Calibration = None,\n                                 dimensional_calibrations: typing.List[CalibrationModule.Calibration] = None, metadata: dict = None,\n                                 timestamp: str = None, data_descriptor: DataAndMetadata.DataDescriptor = None) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Create a data_and_metadata object from data.\n\n        :param data: an ndarray of data.\n        :param intensity_calibration: An optional calibration object.\n        :param dimensional_calibrations: An optional list of calibration objects.\n        :param metadata: A dict of metadata.\n        :param timestamp: A datetime object.\n        :param data_descriptor: A data descriptor describing the dimensions.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        return DataAndMetadata.new_data_and_metadata(data, intensity_calibration, dimensional_calibrations, metadata, timestamp, data_descriptor)", "response": "Create a data_and_metadata object from data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a data_and_metadata object from data.", "response": "def create_data_and_metadata_from_data(self, data: numpy.ndarray, intensity_calibration: CalibrationModule.Calibration=None, dimensional_calibrations: typing.List[CalibrationModule.Calibration]=None, metadata: dict=None, timestamp: str=None) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Create a data_and_metadata object from data.\n\n        .. versionadded:: 1.0\n        .. deprecated:: 1.1\n           Use :py:meth:`~nion.swift.Facade.DataItem.create_data_and_metadata` instead.\n\n        Scriptable: No\n        \"\"\"\n        return self.create_data_and_metadata(numpy.copy(data), intensity_calibration, dimensional_calibrations, metadata, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an I/O handler that reads and writes a single data_and_metadata.", "response": "def create_data_and_metadata_io_handler(self, io_handler_delegate):\n        \"\"\"Create an I/O handler that reads and writes a single data_and_metadata.\n\n        :param io_handler_delegate: A delegate object :py:class:`DataAndMetadataIOHandlerInterface`\n\n        .. versionadded:: 1.0\n\n        Scriptable: No\n        \"\"\"\n        class DelegateIOHandler(ImportExportManager.ImportExportHandler):\n            def __init__(self):\n                super().__init__(io_handler_delegate.io_handler_id, io_handler_delegate.io_handler_name, io_handler_delegate.io_handler_extensions)\n\n            def read_data_elements(self, ui, extension, file_path):\n                data_and_metadata = io_handler_delegate.read_data_and_metadata(extension, file_path)\n                data_element = ImportExportManager.create_data_element_from_extended_data(data_and_metadata)\n                return [data_element]\n\n            def can_write(self, data_and_metadata, extension):\n                return io_handler_delegate.can_write_data_and_metadata(data_and_metadata, extension)\n\n            def write_display_item(self, ui, display_item: DisplayItemModule.DisplayItem, file_path: str, extension: str) -> None:\n                data_item = display_item.data_item\n                if data_item:\n                    self.write_data_item(ui, data_item, file_path, extension)\n\n            def write_data_item(self, ui, data_item, file_path, extension):\n                data_and_metadata = data_item.xdata\n                data = data_and_metadata.data\n                if data is not None:\n                    if hasattr(io_handler_delegate, \"write_data_item\"):\n                        io_handler_delegate.write_data_item(DataItem(data_item), file_path, extension)\n                    else:\n                        assert hasattr(io_handler_delegate, \"write_data_and_metadata\")\n                        io_handler_delegate.write_data_and_metadata(data_and_metadata, file_path, extension)\n\n        class IOHandlerReference:\n\n            def __init__(self):\n                self.__io_handler_delegate = io_handler_delegate\n                self.__io_handler = DelegateIOHandler()\n                ImportExportManager.ImportExportManager().register_io_handler(self.__io_handler)\n\n            def __del__(self):\n                self.close()\n\n            def close(self):\n                if self.__io_handler_delegate:\n                    io_handler_delegate_close_fn = getattr(self.__io_handler_delegate, \"close\", None)\n                    if io_handler_delegate_close_fn:\n                       io_handler_delegate_close_fn()\n                    ImportExportManager.ImportExportManager().unregister_io_handler(self.__io_handler)\n                    self.__io_handler_delegate = None\n\n        return IOHandlerReference()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new panel that can be attached to a window.", "response": "def create_panel(self, panel_delegate):\n        \"\"\"Create a utility panel that can be attached to a window.\n\n        .. versionadded:: 1.0\n\n        Scriptable: No\n\n         The panel_delegate should respond to the following:\n            (property, read-only) panel_id\n            (property, read-only) panel_name\n            (property, read-only) panel_positions (a list from \"top\", \"bottom\", \"left\", \"right\", \"all\")\n            (property, read-only) panel_position (from \"top\", \"bottom\", \"left\", \"right\", \"none\")\n            (method, required) create_panel_widget(ui), returns a widget\n            (method, optional) close()\n        \"\"\"\n\n        panel_id = panel_delegate.panel_id\n        panel_name = panel_delegate.panel_name\n        panel_positions = getattr(panel_delegate, \"panel_positions\", [\"left\", \"right\"])\n        panel_position = getattr(panel_delegate, \"panel_position\", \"none\")\n        properties = getattr(panel_delegate, \"panel_properties\", None)\n\n        workspace_manager = Workspace.WorkspaceManager()\n\n        def create_facade_panel(document_controller, panel_id, properties):\n            panel = Panel(document_controller, panel_id, properties)\n            ui = UserInterface(self.__ui_version, document_controller.ui)\n            document_controller = DocumentWindow(document_controller)\n            panel.widget = panel_delegate.create_panel_widget(ui, document_controller)._widget\n            return panel\n\n        class PanelReference:\n\n            def __init__(self):\n                self.__panel_delegate = panel_delegate\n                workspace_manager.register_panel(create_facade_panel, panel_id, panel_name, panel_positions, panel_position, properties)\n\n            def __del__(self):\n                self.close()\n\n            def close(self):\n                if self.__panel_delegate:\n                    panel_delegate_close_fn = getattr(self.__panel_delegate, \"close\", None)\n                    if panel_delegate_close_fn:\n                       panel_delegate_close_fn()\n                    workspace_manager.unregister_panel(panel_id)\n                    self.__panel_delegate = None\n\n        return PanelReference()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hardware_source_by_id(self, hardware_source_id: str, version: str):\n        actual_version = \"1.0.0\"\n        if Utility.compare_versions(version, actual_version) > 0:\n            raise NotImplementedError(\"Hardware API requested version %s is greater than %s.\" % (version, actual_version))\n        hardware_source = HardwareSourceModule.HardwareSourceManager().get_hardware_source_for_hardware_source_id(hardware_source_id)\n        return HardwareSource(hardware_source) if hardware_source else None", "response": "Return the hardware source API matching the hardware_source_id and version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the library object.", "response": "def library(self) -> Library:\n        \"\"\"Return the library object.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        assert self.__app.document_model\n        return Library(self.__app.document_model)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a cost matrix from a profit matrix by calling inversion_function to invert each value in the profit matrix.", "response": "def make_cost_matrix(profit_matrix, inversion_function):\n    \"\"\"\n    Create a cost matrix from a profit matrix by calling\n    'inversion_function' to invert each value. The inversion\n    function must take one numeric argument (of any type) and return\n    another numeric argument which is presumed to be the cost inverse\n    of the original profit.\n\n    This is a static method. Call it like this:\n\n    .. python::\n\n        cost_matrix = Munkres.make_cost_matrix(matrix, inversion_func)\n\n    For example:\n\n    .. python::\n\n        cost_matrix = Munkres.make_cost_matrix(matrix, lambda x : sys.maxsize - x)\n\n    :Parameters:\n        profit_matrix : list of lists\n            The matrix to convert from a profit to a cost matrix\n\n        inversion_function : function\n            The function to use to invert each entry in the profit matrix\n\n    :rtype: list of lists\n    :return: The converted matrix\n    \"\"\"\n    cost_matrix = []\n    for row in profit_matrix:\n        cost_matrix.append([inversion_function(value) for value in row])\n    return cost_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pad_matrix(self, matrix, pad_value=0):\n        max_columns = 0\n        total_rows = len(matrix)\n\n        for row in matrix:\n            max_columns = max(max_columns, len(row))\n\n        total_rows = max(max_columns, total_rows)\n\n        new_matrix = []\n        for row in matrix:\n            row_len = len(row)\n            new_row = row[:]\n            if total_rows > row_len:\n                # Row too short. Pad it.\n                new_row += [pad_value] * (total_rows - row_len)\n            new_matrix += [new_row]\n\n        while len(new_matrix) < total_rows:\n            new_matrix += [[pad_value] * total_rows]\n\n        return new_matrix", "response": "Pads a possibly non - square matrix to make it square."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the indexes for the lowest - cost pairings between rows and columns in the database.", "response": "def compute(self, cost_matrix):\n        \"\"\"\n        Compute the indexes for the lowest-cost pairings between rows and\n        columns in the database. Returns a list of (row, column) tuples\n        that can be used to traverse the matrix.\n\n        :Parameters:\n            cost_matrix : list of lists\n                The cost matrix. If this cost matrix is not square, it\n                will be padded with zeros, via a call to ``pad_matrix()``.\n                (This method does *not* modify the caller's matrix. It\n                operates on a copy of the matrix.)\n\n                **WARNING**: This code handles square and rectangular\n                matrices. It does *not* handle irregular matrices.\n\n        :rtype: list\n        :return: A list of ``(row, column)`` tuples that describe the lowest\n                 cost path through the matrix\n\n        \"\"\"\n        self.C = self.pad_matrix(cost_matrix)\n        self.n = len(self.C)\n        self.original_length = len(cost_matrix)\n        self.original_width = len(cost_matrix[0])\n        self.row_covered = [False for i in range(self.n)]\n        self.col_covered = [False for i in range(self.n)]\n        self.Z0_r = 0\n        self.Z0_c = 0\n        self.path = self.__make_matrix(self.n * 2, 0)\n        self.marked = self.__make_matrix(self.n, 0)\n\n        done = False\n        step = 1\n\n        steps = { 1 : self.__step1,\n                  2 : self.__step2,\n                  3 : self.__step3,\n                  4 : self.__step4,\n                  5 : self.__step5,\n                  6 : self.__step6 }\n\n        while not done:\n            try:\n                func = steps[step]\n                step = func()\n            except KeyError:\n                done = True\n\n        # Look for the starred columns\n        results = []\n        for i in range(self.original_length):\n            for j in range(self.original_width):\n                if self.marked[i][j] == 1:\n                    results += [(i, j)]\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __make_matrix(self, n, val):\n        matrix = []\n        for i in range(n):\n            matrix += [[val for j in range(n)]]\n        return matrix", "response": "Create an n x n matrix populating it with the specific value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngoing to Step 1.", "response": "def __step1(self):\n        \"\"\"\n        For each row of the matrix, find the smallest element and\n        subtract it from every element in its row. Go to Step 2.\n        \"\"\"\n        C = self.C\n        n = self.n\n        for i in range(n):\n            minval = min(self.C[i])\n            # Find the minimum value for this row and subtract that minimum\n            # from every element in the row.\n            for j in range(n):\n                self.C[i][j] -= minval\n\n        return 2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a zero (Z) in the resulting matrix. If there is no starred zero in its row or column, star Z. Repeat for each element in the matrix. Go to Step 3.", "response": "def __step2(self):\n        \"\"\"\n        Find a zero (Z) in the resulting matrix. If there is no starred\n        zero in its row or column, star Z. Repeat for each element in the\n        matrix. Go to Step 3.\n        \"\"\"\n        n = self.n\n        for i in range(n):\n            for j in range(n):\n                if (self.C[i][j] == 0) and \\\n                        (not self.col_covered[j]) and \\\n                        (not self.row_covered[i]):\n                    self.marked[i][j] = 1\n                    self.col_covered[j] = True\n                    self.row_covered[i] = True\n\n        self.__clear_covers()\n        return 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncovering each column containing a starred zero. If K columns are covered, the starred zeros describe a complete set of unique assignments. In this case, Go to DONE, otherwise, Go to Step 4.", "response": "def __step3(self):\n        \"\"\"\n        Cover each column containing a starred zero. If K columns are\n        covered, the starred zeros describe a complete set of unique\n        assignments. In this case, Go to DONE, otherwise, Go to Step 4.\n        \"\"\"\n        n = self.n\n        count = 0\n        for i in range(n):\n            for j in range(n):\n                if self.marked[i][j] == 1:\n                    self.col_covered[j] = True\n                    count += 1\n\n        if count >= n:\n            step = 7 # done\n        else:\n            step = 4\n\n        return step"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngoes to Step 4.", "response": "def __step4(self):\n        \"\"\"\n        Find a noncovered zero and prime it. If there is no starred zero\n        in the row containing this primed zero, Go to Step 5. Otherwise,\n        cover this row and uncover the column containing the starred\n        zero. Continue in this manner until there are no uncovered zeros\n        left. Save the smallest uncovered value and Go to Step 6.\n        \"\"\"\n        step = 0\n        done = False\n        row = -1\n        col = -1\n        star_col = -1\n        while not done:\n            (row, col) = self.__find_a_zero()\n            if row < 0:\n                done = True\n                step = 6\n            else:\n                self.marked[row][col] = 2\n                star_col = self.__find_star_in_row(row)\n                if star_col >= 0:\n                    col = star_col\n                    self.row_covered[row] = True\n                    self.col_covered[col] = False\n                else:\n                    done = True\n                    self.Z0_r = row\n                    self.Z0_c = col\n                    step = 5\n\n        return step"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a series of alternating primed and starred zeros as follows. Let Z0 represent the uncovered primed zero found in Step 4. Let Z1 denote the starred zero in the column of Z0 (if any). Let Z2 denote the primed zero in the row of Z1 (there will always be one). Continue until the series terminates at a primed zero that has no starred zero in its column. Unstar each starred zero of the series, star each primed zero of the series, erase all primes and uncover every line in the matrix. Return to Step 3", "response": "def __step5(self):\n        \"\"\"\n        Construct a series of alternating primed and starred zeros as\n        follows. Let Z0 represent the uncovered primed zero found in Step 4.\n        Let Z1 denote the starred zero in the column of Z0 (if any).\n        Let Z2 denote the primed zero in the row of Z1 (there will always\n        be one). Continue until the series terminates at a primed zero\n        that has no starred zero in its column. Unstar each starred zero\n        of the series, star each primed zero of the series, erase all\n        primes and uncover every line in the matrix. Return to Step 3\n        \"\"\"\n        count = 0\n        path = self.path\n        path[count][0] = self.Z0_r\n        path[count][1] = self.Z0_c\n        done = False\n        while not done:\n            row = self.__find_star_in_col(path[count][1])\n            if row >= 0:\n                count += 1\n                path[count][0] = row\n                path[count][1] = path[count-1][1]\n            else:\n                done = True\n\n            if not done:\n                col = self.__find_prime_in_row(path[count][0])\n                count += 1\n                path[count][0] = path[count-1][0]\n                path[count][1] = col\n\n        self.__convert_path(path, count)\n        self.__clear_covers()\n        self.__erase_primes()\n        return 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __step6(self):\n        minval = self.__find_smallest()\n        for i in range(self.n):\n            for j in range(self.n):\n                if self.row_covered[i]:\n                    self.C[i][j] += minval\n                if not self.col_covered[j]:\n                    self.C[i][j] -= minval\n        return 4", "response": "Add the value found in Step 4 to every element of each covered\n        row, and subtract it from every element of each uncovered column.\n        Return to Step 4 without altering any stars, primes, or covered\n        lines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the smallest uncovered value in the matrix.", "response": "def __find_smallest(self):\n        \"\"\"Find the smallest uncovered value in the matrix.\"\"\"\n        minval = sys.maxsize\n        for i in range(self.n):\n            for j in range(self.n):\n                if (not self.row_covered[i]) and (not self.col_covered[j]):\n                    if minval > self.C[i][j]:\n                        minval = self.C[i][j]\n        return minval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __find_a_zero(self):\n        row = -1\n        col = -1\n        i = 0\n        n = self.n\n        done = False\n\n        while not done:\n            j = 0\n            while True:\n                if (self.C[i][j] == 0) and \\\n                        (not self.row_covered[i]) and \\\n                        (not self.col_covered[j]):\n                    row = i\n                    col = j\n                    done = True\n                j += 1\n                if j >= n:\n                    break\n            i += 1\n            if i >= n:\n                done = True\n\n        return (row, col)", "response": "Find the first uncovered element with value 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the first starred element in the specified row. Returns - 1 if no starred element was found.", "response": "def __find_star_in_row(self, row):\n        \"\"\"\n        Find the first starred element in the specified row. Returns\n        the column index, or -1 if no starred element was found.\n        \"\"\"\n        col = -1\n        for j in range(self.n):\n            if self.marked[row][j] == 1:\n                col = j\n                break\n\n        return col"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the first starred element in the specified column. Returns - 1 if no starred element was found.", "response": "def __find_star_in_col(self, col):\n        \"\"\"\n        Find the first starred element in the specified row. Returns\n        the row index, or -1 if no starred element was found.\n        \"\"\"\n        row = -1\n        for i in range(self.n):\n            if self.marked[i][col] == 1:\n                row = i\n                break\n\n        return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __find_prime_in_row(self, row):\n        col = -1\n        for j in range(self.n):\n            if self.marked[row][j] == 2:\n                col = j\n                break\n\n        return col", "response": "Find the first prime element in the specified row. Returns - 1 if no prime element was found."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __clear_covers(self):\n        for i in range(self.n):\n            self.row_covered[i] = False\n            self.col_covered[i] = False", "response": "Clear all covered matrix cells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nerasing all prime markings", "response": "def __erase_primes(self):\n        \"\"\"Erase all prime markings\"\"\"\n        for i in range(self.n):\n            for j in range(self.n):\n                if self.marked[i][j] == 2:\n                    self.marked[i][j] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the contingency table with new values.", "response": "def update(self, a, b, c, d):\n        \"\"\"\n        Update contingency table with new values without creating a new object.\n        \"\"\"\n        self.table.ravel()[:] = [a, b, c, d]\n        self.N = self.table.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the frequency bias of the current set of entries.", "response": "def bias(self):\n        \"\"\"\n        Frequency Bias.\n        Formula:  (a+b)/(a+c)\"\"\"\n        return (self.table[0, 0] + self.table[0, 1]) / (self.table[0, 0] + self.table[1, 0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hss(self):\n        return 2 * (self.table[0, 0] * self.table[1, 1] - self.table[0, 1] * self.table[1, 0]) / (\n            (self.table[0, 0] + self.table[0, 1]) * (self.table[0, 1] + self.table[1, 1]) +\n            (self.table[0, 0] + self.table[1, 0]) * (self.table[1, 0] + self.table[1, 1]))", "response": "Doolittle ( Heidke ) Skill Score. 2 ad - bc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pss(self):\n        return (self.table[0, 0] * self.table[1, 1] - self.table[0, 1] * self.table[1, 0]) / \\\n               ((self.table[0, 0] + self.table[1, 0]) * (self.table[0, 1] + self.table[1, 1]))", "response": "Return the PSS of the current Skill Score."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the CSS value of the clayton skill score.", "response": "def css(self):\n        \"\"\"Clayton Skill Score (ad - bc)/((a+b)(c+d))\"\"\"\n        return (self.table[0, 0] * self.table[1, 1] - self.table[0, 1] * self.table[1, 0]) / \\\n               ((self.table[0, 0] + self.table[0, 1]) * (self.table[1, 0] + self.table[1, 1]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the scikit - learn decision tree ensemble object from a pickle file.", "response": "def load_tree_object(filename):\n    \"\"\"\n    Load scikit-learn decision tree ensemble object from file.\n    \n    Parameters\n    ----------\n    filename : str\n        Name of the pickle file containing the tree object.\n    \n    Returns\n    -------\n    tree ensemble object\n    \"\"\"\n    with open(filename) as file_obj:\n        tree_ensemble_obj = pickle.load(file_obj)\n    return tree_ensemble_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output_tree_ensemble(tree_ensemble_obj, output_filename, attribute_names=None):\n    for t, tree in enumerate(tree_ensemble_obj.estimators_):\n        print(\"Writing Tree {0:d}\".format(t))\n        out_file = open(output_filename + \".{0:d}.tree\", \"w\")\n        #out_file.write(\"Tree {0:d}\\n\".format(t))\n        tree_str = print_tree_recursive(tree.tree_, 0, attribute_names)\n        out_file.write(tree_str)\n        #out_file.write(\"\\n\")\n        out_file.close()\n    return", "response": "Writes each decision tree in an ensemble to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_tree_recursive(tree_obj, node_index, attribute_names=None):\n    tree_str = \"\"\n    if node_index == 0:\n        tree_str += \"{0:d}\\n\".format(tree_obj.node_count)\n    if tree_obj.feature[node_index] >= 0:\n        if attribute_names is None:\n            attr_val = \"{0:d}\".format(tree_obj.feature[node_index])\n        else:\n            attr_val = attribute_names[tree_obj.feature[node_index]]\n        tree_str += \"b {0:d} {1} {2:0.4f} {3:d} {4:1.5e}\\n\".format(node_index,\n                                                                   attr_val,\n                                                                   tree_obj.weighted_n_node_samples[node_index],\n                                                                   tree_obj.n_node_samples[node_index],\n                                                                   tree_obj.threshold[node_index])\n    else:\n        if tree_obj.max_n_classes > 1:\n            leaf_value = \"{0:d}\".format(tree_obj.value[node_index].argmax())\n        else:\n            leaf_value = \"{0}\".format(tree_obj.value[node_index][0][0])\n        tree_str += \"l {0:d} {1} {2:0.4f} {3:d}\\n\".format(node_index,\n                                                          leaf_value,\n                                                          tree_obj.weighted_n_node_samples[node_index],\n                                                          tree_obj.n_node_samples[node_index])\n    if tree_obj.children_left[node_index] > 0:\n        tree_str += print_tree_recursive(tree_obj, tree_obj.children_left[node_index], attribute_names)\n    if tree_obj.children_right[node_index] > 0:\n        tree_str += print_tree_recursive(tree_obj, tree_obj.children_right[node_index], attribute_names)\n    return tree_str", "response": "This function writes a string representation of a base decision tree object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_classifier_mask(self, v, base_mask=True):\n        base = self._base\n        v = tonparray(v)\n        a = np.unique(v)\n        if a[0] != -1 or a[1] != 1:\n            raise RuntimeError(\"The labels must be -1 and 1 (%s)\" % a)\n        mask = np.zeros_like(v)\n        cnt = min([(v == x).sum() for x in a]) * base._tr_fraction\n        cnt = int(round(cnt))\n        for i in a:\n            index = np.where((v == i) & base_mask)[0]\n            np.random.shuffle(index)\n            mask[index[:cnt]] = True\n        base._mask = SparseArray.fromlist(mask)\n        return SparseArray.fromlist(v)", "response": "Computes the mask used to create the training and validation set"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the mask used to create the training and validation set", "response": "def set_regression_mask(self, v):\n        \"\"\"Computes the mask used to create the training and validation set\"\"\"\n        base = self._base\n        index = np.arange(v.size())\n        np.random.shuffle(index)\n        ones = np.ones(v.size())\n        ones[index[int(base._tr_fraction * v.size()):]] = 0\n        base._mask = SparseArray.fromlist(ones)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fitness_vs(self, v):\n        base = self._base\n        if base._classifier:\n            if base._multiple_outputs:\n                v.fitness_vs = v._error\n                # if base._fitness_function == 'macro-F1':\n                #     v.fitness_vs = v._error\n                # elif base._fitness_function == 'BER':\n                #     v.fitness_vs = v._error\n                # elif base._fitness_function == 'macro-Precision':\n                #     v.fitness_vs = v._error\n                # elif base._fitness_function == 'accDotMacroF1':\n                #     v.fitness_vs = v._error\n                # elif base._fitness_function == 'macro-RecallF1':\n                #     v.fitness_vs = v._error\n                # elif base._fitness_function == 'F1':\n                #     v.fitness_vs = v._error\n                # else:\n                #     v.fitness_vs = - v._error.dot(base._mask_vs) / base._mask_vs.sum()\n            else:\n                v.fitness_vs = -((base.y - v.hy.sign()).sign().fabs() *\n                                 base._mask_vs).sum()\n        else:\n            mask = base._mask\n            y = base.y\n            hy = v.hy\n            if not isinstance(mask, list):\n                mask = [mask]\n                y = [y]\n                hy = [hy]\n            fit = []\n            for _mask, _y, _hy in zip(mask, y, hy):\n                m = (_mask + -1).fabs()\n                x = _y * m\n                y = _hy * m\n                a = (x - y).sq().sum()\n                b = (x + -x.sum() / x.size()).sq().sum()\n                fit.append(-a / b)\n            v.fitness_vs = np.mean(fit)", "response": "Fitness function in the validation set. In classification it uses BER and RSE in regression"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_fitness(self, v):\n        base = self._base\n        self.fitness(v)\n        if not np.isfinite(v.fitness):\n            self.del_error(v)\n            return False\n        if base._tr_fraction < 1:\n            self.fitness_vs(v)\n            if not np.isfinite(v.fitness_vs):\n                self.del_error(v)\n                return False\n        self.del_error(v)\n        return True", "response": "Set the fitness to a new node. Returns false in case fitness is not finite"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef analisar(retorno):\n        resposta = analisar_retorno(forcar_unicode(retorno),\n                funcao='EnviarDadosVenda',\n                classe_resposta=RespostaCancelarUltimaVenda,\n                campos=(\n                        ('numeroSessao', int),\n                        ('EEEEE', unicode),\n                        ('CCCC', unicode),\n                        ('mensagem', unicode),\n                        ('cod', unicode),\n                        ('mensagemSEFAZ', unicode),\n                        ('arquivoCFeBase64', unicode),\n                        ('timeStamp', as_datetime),\n                        ('chaveConsulta', unicode),\n                        ('valorTotalCFe', Decimal),\n                        ('CPFCNPJValue', unicode),\n                        ('assinaturaQRCODE', unicode),\n                    ),\n                campos_alternativos=[\n                        # se a venda falhar apenas os primeiros seis campos\n                        # especificados na ER dever\u00e3o ser retornados...\n                        (\n                                ('numeroSessao', int),\n                                ('EEEEE', unicode),\n                                ('CCCC', unicode),\n                                ('mensagem', unicode),\n                                ('cod', unicode),\n                                ('mensagemSEFAZ', unicode),\n                        ),\n                        # por via das d\u00favidas, considera o padr\u00e3o de campos,\n                        # caso n\u00e3o haja nenhuma coincid\u00eancia...\n                        RespostaSAT.CAMPOS,\n                    ]\n            )\n        if resposta.EEEEE not in ('07000',):\n            raise ExcecaoRespostaSAT(resposta)\n        return resposta", "response": "Analisar uma resposta a partir do\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_data_element_to_data_and_metadata_1(data_element) -> DataAndMetadata.DataAndMetadata:\n    # data. takes ownership.\n    data = data_element[\"data\"]\n    dimensional_shape = Image.dimensional_shape_from_data(data)\n    is_sequence = data_element.get(\"is_sequence\", False)\n    dimension_count = len(Image.dimensional_shape_from_data(data))\n    adjusted_dimension_count = dimension_count - (1 if is_sequence else 0)\n    collection_dimension_count = data_element.get(\"collection_dimension_count\", 2 if adjusted_dimension_count in (3, 4) else 0)\n    datum_dimension_count = data_element.get(\"datum_dimension_count\", adjusted_dimension_count - collection_dimension_count)\n    data_descriptor = DataAndMetadata.DataDescriptor(is_sequence, collection_dimension_count, datum_dimension_count)\n\n    # dimensional calibrations\n    dimensional_calibrations = None\n    if \"spatial_calibrations\" in data_element:\n        dimensional_calibrations_list = data_element.get(\"spatial_calibrations\")\n        if len(dimensional_calibrations_list) == len(dimensional_shape):\n            dimensional_calibrations = list()\n            for dimension_calibration in dimensional_calibrations_list:\n                offset = float(dimension_calibration.get(\"offset\", 0.0))\n                scale = float(dimension_calibration.get(\"scale\", 1.0))\n                units = dimension_calibration.get(\"units\", \"\")\n                units = str(units) if units is not None else str()\n                if scale != 0.0:\n                    dimensional_calibrations.append(Calibration.Calibration(offset, scale, units))\n                else:\n                    dimensional_calibrations.append(Calibration.Calibration())\n\n    # intensity calibration\n    intensity_calibration = None\n    if \"intensity_calibration\" in data_element:\n        intensity_calibration_dict = data_element.get(\"intensity_calibration\")\n        offset = float(intensity_calibration_dict.get(\"offset\", 0.0))\n        scale = float(intensity_calibration_dict.get(\"scale\", 1.0))\n        units = intensity_calibration_dict.get(\"units\", \"\")\n        units = str(units) if units is not None else str()\n        if scale != 0.0:\n            intensity_calibration = Calibration.Calibration(offset, scale, units)\n\n    # properties (general tags)\n    metadata = dict()\n    if \"metadata\" in data_element:\n        metadata.update(Utility.clean_dict(data_element.get(\"metadata\")))\n    if \"properties\" in data_element and data_element[\"properties\"]:\n        hardware_source_metadata = metadata.setdefault(\"hardware_source\", dict())\n        hardware_source_metadata.update(Utility.clean_dict(data_element.get(\"properties\")))\n\n    # dates are _local_ time and must use this specific ISO 8601 format. 2013-11-17T08:43:21.389391\n    # time zones are offsets (east of UTC) in the following format \"+HHMM\" or \"-HHMM\"\n    # daylight savings times are time offset (east of UTC) in format \"+MM\" or \"-MM\"\n    # timezone is for conversion and is the Olson timezone string.\n    # datetime.datetime.strptime(datetime.datetime.isoformat(datetime.datetime.now()), \"%Y-%m-%dT%H:%M:%S.%f\" )\n    # datetime_modified, datetime_modified_tz, datetime_modified_dst, datetime_modified_tzname is the time at which this image was modified.\n    # datetime_original, datetime_original_tz, datetime_original_dst, datetime_original_tzname is the time at which this image was created.\n    timestamp = data_element.get(\"timestamp\", datetime.datetime.utcnow())\n    datetime_item = data_element.get(\"datetime_modified\", Utility.get_datetime_item_from_utc_datetime(timestamp))\n\n    local_datetime = Utility.get_datetime_from_datetime_item(datetime_item)\n    dst_value = datetime_item.get(\"dst\", \"+00\")\n    tz_value = datetime_item.get(\"tz\", \"+0000\")\n    timezone = datetime_item.get(\"timezone\")\n    time_zone = { \"dst\": dst_value, \"tz\": tz_value}\n    if timezone is not None:\n        time_zone[\"timezone\"] = timezone\n    # note: dst is informational only; tz already include dst\n    tz_adjust = (int(tz_value[1:3]) * 60 + int(tz_value[3:5])) * (-1 if tz_value[0] == '-' else 1)\n    utc_datetime = local_datetime - datetime.timedelta(minutes=tz_adjust)  # tz_adjust already contains dst_adjust\n    timestamp = utc_datetime\n\n    return DataAndMetadata.new_data_and_metadata(data,\n                                                 intensity_calibration=intensity_calibration,\n                                                 dimensional_calibrations=dimensional_calibrations,\n                                                 metadata=metadata,\n                                                 timestamp=timestamp,\n                                                 data_descriptor=data_descriptor,\n                                                 timezone=timezone,\n                                                 timezone_offset=tz_value)", "response": "Convert a data element to xdata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsegmenting forecast tracks to only output data contined within a region in the CONUS, as defined by the mapfile. Args: csv_path(str): Path to the full CONUS csv file. file_dict_key(str): Dictionary key for the csv files, currently either 'track_step' or 'track_total' out_path (str): Path to output new segmented csv files. Returns: Segmented forecast tracks in a csv file.", "response": "def output_sector_csv(self,csv_path,file_dict_key,out_path):\n        \"\"\"\n        Segment forecast tracks to only output data contined within a \n        region in the CONUS, as defined by the mapfile.\n\n        Args:\n            csv_path(str): Path to the full CONUS csv file.\n            file_dict_key(str): Dictionary key for the csv files, \n                currently either 'track_step' or 'track_total'\n            out_path (str): Path to output new segmented csv files.\n        Returns:\n            Segmented forecast tracks in a csv file.\n        \"\"\"\n        csv_file = csv_path + \"{0}_{1}_{2}_{3}.csv\".format(\n                                                        file_dict_key,\n                                                        self.ensemble_name,\n                                                        self.member,\n                                                        self.run_date.strftime(self.date_format))\n        if exists(csv_file):\n            csv_data = pd.read_csv(csv_file)\n            \n            if self.inds is None:\n                lon_obj = csv_data.loc[:,\"Centroid_Lon\"]\n                lat_obj = csv_data.loc[:,\"Centroid_Lat\"]\n            \n                self.inds = np.where((self.ne_lat>=lat_obj)&(self.sw_lat<=lat_obj)\\\n                        &(self.ne_lon>=lon_obj)&(self.sw_lon<=lon_obj))[0]\n            \n            if np.shape(self.inds)[0] > 0:\n                csv_data = csv_data.reindex(np.array(self.inds)) \n                sector_csv_filename = out_path + \"{0}_{1}_{2}_{3}.csv\".format(\n                                                        file_dict_key,\n                                                        self.ensemble_name,\n                                                        self.member,\n                                                        self.run_date.strftime(self.date_format))\n                print(\"Output sector csv file \" + sector_csv_filename)\n                csv_data.to_csv(sector_csv_filename,\n                        na_rep=\"nan\",\n                        float_format=\"%0.5f\",\n                        index=False)\n                os.chmod(sector_csv_filename, 0o666)\n            else:\n                print('No {0} {1} sector data found'.format(self.member,\n                                self.run_date.strftime(\"%Y%m%d\")))\n            \n        else:\n            print('No {0} {1} csv file found'.format(self.member,\n                                self.run_date.strftime(\"%Y%m%d\")))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_sector_netcdf(self,netcdf_path,out_path,patch_radius,config):\n        \n        nc_data = self.load_netcdf_data(netcdf_path,patch_radius)\n    \n        if nc_data is not None:\n            out_filename = out_path + \"{0}_{1}_{2}_model_patches.nc\".format(\n                                                            self.ensemble_name,\n                                                            self.run_date.strftime(self.date_format),\n                                                            self.member)\n            out_file = Dataset(out_filename, \"w\")\n            out_file.createDimension(\"p\", np.shape(nc_data.variables['p'])[0])\n            out_file.createDimension(\"row\", np.shape(nc_data.variables['row'])[0])\n            out_file.createDimension(\"col\", np.shape(nc_data.variables['col'])[0])\n            out_file.createVariable(\"p\", \"i4\", (\"p\",))\n            out_file.createVariable(\"row\", \"i4\", (\"row\",))\n            out_file.createVariable(\"col\", \"i4\", (\"col\",))\n            out_file.variables[\"p\"][:] = nc_data.variables['p'][:]\n            out_file.variables[\"row\"][:] =  nc_data.variables['row'][:]\n            out_file.variables[\"col\"][:] =  nc_data.variables['col'][:]\n            out_file.Conventions = \"CF-1.6\"\n            out_file.title = \"{0} Storm Patches for run {1} member {2}\".format(self.ensemble_name,\n                                                                       self.run_date.strftime(self.date_format),\n                                                                       self.member)\n            out_file.object_variable = config.watershed_variable\n            meta_variables = [\"lon\", \"lat\", \"i\", \"j\", \"x\", \"y\", \"masks\"]\n            meta_units = [\"degrees_east\", \"degrees_north\", \"\", \"\", \"m\", \"m\", \"\"]\n            center_vars = [\"time\", \"centroid_lon\", \"centroid_lat\", \"centroid_i\", \"centroid_j\", \"track_id\", \"track_step\"]\n            center_units = [\"hours since {0}\".format(self.run_date.strftime(\"%Y-%m-%d %H:%M:%S\")),\n                    \"degrees_east\",\n                    \"degrees_north\",\n                    \"\",\n                    \"\",\n                    \"\",\n                    \"\"]\n\n            label_columns = [\"Matched\", \"Max_Hail_Size\", \"Num_Matches\", \"Shape\", \"Location\", \"Scale\"]\n        \n            for m, meta_variable in enumerate(meta_variables):\n                if meta_variable in [\"i\", \"j\", \"masks\"]:\n                    dtype = \"i4\"\n                else:\n                    dtype = \"f4\"\n                m_var = out_file.createVariable(meta_variable, dtype, (\"p\", \"row\", \"col\"), complevel=1, zlib=True)\n                m_var.long_name = meta_variable\n                m_var.units = meta_units[m]\n        \n            for c, center_var in enumerate(center_vars):\n                if center_var in [\"time\", \"track_id\", \"track_step\"]:\n                    dtype = \"i4\"\n                else:\n                    dtype = \"f4\"\n                c_var = out_file.createVariable(center_var, dtype, (\"p\",), zlib=True, complevel=1)\n                c_var.long_name = center_var\n                c_var.units =center_units[c]\n        \n            for storm_variable in config.storm_variables:\n                s_var = out_file.createVariable(storm_variable + \"_curr\", \"f4\", (\"p\", \"row\", \"col\"), complevel=1, zlib=True)\n                s_var.long_name = storm_variable\n                s_var.units = \"\"\n        \n            for potential_variable in config.potential_variables:\n                p_var = out_file.createVariable(potential_variable + \"_prev\", \"f4\", (\"p\", \"row\", \"col\"),\n                                        complevel=1, zlib=True)\n                p_var.long_name = potential_variable\n                p_var.units = \"\"\n        \n            if config.train:\n                for label_column in label_columns:\n                    if label_column in [\"Matched\", \"Num_Matches\"]:\n                        dtype = \"i4\"\n                    else:\n                        dtype = \"f4\"\n                    l_var = out_file.createVariable(label_column, dtype, (\"p\",), zlib=True, complevel=1)\n                    l_var.long_name = label_column\n                    l_var.units = \"\"\n            \n            out_file.variables[\"time\"][:] = nc_data.variables['time'][:]\n    \n            for c_var in [\"lon\", \"lat\"]:\n                out_file.variables[\"centroid_\" + c_var][:] =  nc_data.variables['centroid_' + c_var][:]\n\n            for c_var in [\"i\", \"j\"]:\n                out_file.variables[\"centroid_\" + c_var][:] =  nc_data.variables[\"centroid_\" + c_var][:]\n        \n            out_file.variables[\"track_id\"][:] =  nc_data.variables['track_id'][:]\n            out_file.variables[\"track_step\"][:] =  nc_data.variables['track_step'][:]\n        \n            for meta_var in meta_variables:\n                if meta_var in [\"lon\", \"lat\"]:\n                    out_file.variables[meta_var][:] = nc_data.variables[meta_var][:]\n                else:\n                    out_file.variables[meta_var][:] = nc_data.variables[meta_var][:]\n        \n            for storm_variable in config.storm_variables:\n                out_file.variables[storm_variable + \"_curr\"][:] = nc_data.variables[storm_variable + '_curr'][:]\n        \n            for p_variable in config.potential_variables:\n                out_file.variables[p_variable + \"_prev\"][:] = nc_data.variables[p_variable + '_prev'][:]\n        \n            if config.train:\n                for label_column in label_columns:\n                    try:\n                        out_file.variables[label_column][:] = nc_data.variables[label_column][:]\n                    except:\n                        out_file.variables[label_column][:] = 0\n\n            out_file.close()\n        \n            print(\"Output sector nc file \" + out_filename)\n        else:\n            print('No {0} {1} netcdf file/sector data found'.format(self.member,\n                                self.run_date.strftime(\"%Y%m%d\")))\n\n        return", "response": "This function will output a segmented netcdf file for the specified sector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_dict(d0, clean_item_fn=None):\n    clean_item_fn = clean_item_fn if clean_item_fn else clean_item\n    d = dict()\n    for key in d0:\n        cleaned_item = clean_item_fn(d0[key])\n        if cleaned_item is not None:\n            d[key] = cleaned_item\n    return d", "response": "Returns a json - clean dict. Will log info message for failures."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a json - clean list. Will log info message for failures.", "response": "def clean_list(l0, clean_item_fn=None):\n    \"\"\"\n        Return a json-clean list. Will log info message for failures.\n    \"\"\"\n    clean_item_fn = clean_item_fn if clean_item_fn else clean_item\n    l = list()\n    for index, item in enumerate(l0):\n        cleaned_item = clean_item_fn(item)\n        l.append(cleaned_item)\n    return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a json - clean tuple. Will log info message for failures.", "response": "def clean_tuple(t0, clean_item_fn=None):\n    \"\"\"\n        Return a json-clean tuple. Will log info message for failures.\n    \"\"\"\n    clean_item_fn = clean_item_fn if clean_item_fn else clean_item\n    l = list()\n    for index, item in enumerate(t0):\n        cleaned_item = clean_item_fn(item)\n        l.append(cleaned_item)\n    return tuple(l)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a json - clean item or None. Will log info message for failure.", "response": "def clean_item(i):\n    \"\"\"\n        Return a json-clean item or None. Will log info message for failure.\n    \"\"\"\n    itype = type(i)\n    if itype == dict:\n        return clean_dict(i)\n    elif itype == list:\n        return clean_list(i)\n    elif itype == tuple:\n        return clean_tuple(i)\n    elif itype == numpy.float32:\n        return float(i)\n    elif itype == numpy.float64:\n        return float(i)\n    elif itype == numpy.int16:\n        return int(i)\n    elif itype == numpy.uint16:\n        return int(i)\n    elif itype == numpy.int32:\n        return int(i)\n    elif itype == numpy.uint32:\n        return int(i)\n    elif itype == numpy.int64:\n        return int(i)\n    elif itype == numpy.uint64:\n        return int(i)\n    elif itype == float:\n        return i\n    elif itype == str:\n        return i\n    elif itype == int:\n        return i\n    elif itype == bool:\n        return i\n    elif itype == type(None):\n        return i\n    logging.info(\"[1] Unable to handle type %s\", itype)\n    import traceback\n    traceback.print_stack()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_item_no_list(i):\n    itype = type(i)\n    if itype == dict:\n        return clean_dict(i, clean_item_no_list)\n    elif itype == list:\n        return clean_tuple(i, clean_item_no_list)\n    elif itype == tuple:\n        return clean_tuple(i, clean_item_no_list)\n    elif itype == numpy.float32:\n        return float(i)\n    elif itype == numpy.float64:\n        return float(i)\n    elif itype == numpy.int16:\n        return int(i)\n    elif itype == numpy.uint16:\n        return int(i)\n    elif itype == numpy.int32:\n        return int(i)\n    elif itype == numpy.uint32:\n        return int(i)\n    elif itype == float:\n        return i\n    elif itype == str:\n        return i\n    elif itype == int:\n        return i\n    elif itype == bool:\n        return i\n    elif itype == type(None):\n        return i\n    logging.info(\"[2] Unable to handle type %s\", itype)\n    return None", "response": "Clean a json - clean item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample the stack in a thread and print it at regular intervals.", "response": "def sample_stack_all(count=10, interval=0.1):\n    \"\"\"Sample the stack in a thread and print it at regular intervals.\"\"\"\n\n    def print_stack_all(l, ll):\n        l1 = list()\n        l1.append(\"*** STACKTRACE - START ***\")\n        code = []\n        for threadId, stack in sys._current_frames().items():\n            sub_code = []\n            sub_code.append(\"# ThreadID: %s\" % threadId)\n            for filename, lineno, name, line in traceback.extract_stack(stack):\n                sub_code.append('File: \"%s\", line %d, in %s' % (filename, lineno, name))\n                if line:\n                    sub_code.append(\"  %s\" % (line.strip()))\n            if not \"in select\" in sub_code[-2] and \\\n               not \"in wait\" in sub_code[-2] and \\\n               not \"in print_stack_all\" in sub_code[-2] and \\\n               not \"in sample_stack_all\" in sub_code[-2] and \\\n               not \"in checkcache\" in sub_code[-2] and \\\n               not \"do_sleep\" in sub_code[-2] and \\\n               not \"sleep\" in sub_code[-1] and \\\n               not any([\"in do_sample\" in s for s in sub_code]):\n                code.extend(sub_code)\n        for line in code:\n            l1.append(line)\n        l1.append(\"*** STACKTRACE - END ***\")\n        with l:\n            ll.extend(l1)\n\n    def do_sample():\n        l = threading.RLock()\n        ll = list()\n        for i in range(count):\n            print_stack_all(l, ll)\n            time.sleep(interval)\n        with l:\n            print(\"\\n\".join(ll))\n\n    threading.Thread(target=do_sample).start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decision_function(self, X):\n        \"Decision function i.e. the raw data of the prediction\"\n        self._X = Model.convert_features(X)\n        self._eval()\n        return self._ind[0].hy", "response": "Decision function i. e. the raw data of the prediction"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate a individual using recursion and self. _pos as pointer", "response": "def _eval(self):\n        \"Evaluates a individual using recursion and self._pos as pointer\"\n        pos = self._pos\n        self._pos += 1\n        node = self._ind[pos]\n        if isinstance(node, Function):\n            args = [self._eval() for x in range(node.nargs)]\n            node.eval(args)\n            for x in args:\n                x.hy = None\n                x.hy_test = None\n        else:\n            node.eval(self._X)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grow_use_function(self, depth=0):\n        \"Select either function or terminal in grow method\"\n        if depth == 0:\n            return False\n        if depth == self._depth:\n            return True\n        return np.random.random() < 0.5", "response": "Select either function or terminal in grow method"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate random population using ramped half - and - half method", "response": "def create_population(self, popsize=1000, min_depth=2,\n                          max_depth=4,\n                          X=None):\n        \"Creates random population using ramped half-and-half method\"\n        import itertools\n        args = [x for x in itertools.product(range(min_depth,\n                                                   max_depth+1),\n                                             [True, False])]\n        index = 0\n        output = []\n        while len(output) < popsize:\n            depth, full = args[index]\n            index += 1\n            if index >= len(args):\n                index = 0\n            if full:\n                ind = self.create_random_ind_full(depth=depth)\n            else:\n                ind = self.create_random_ind_grow(depth=depth)\n            flag = True\n            if X is not None:\n                x = Individual(ind)\n                x.decision_function(X)\n                flag = x.individual[0].isfinite()\n            l_vars = (flag, len(output), full, depth, len(ind))\n            l_str = \" flag: %s len(output): %s full: %s depth: %s len(ind): %s\"\n            self._logger.debug(l_str % l_vars)\n            if flag:\n                output.append(ind)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decision_function(self, X, **kwargs):\n        \"Decision function i.e. the raw data of the prediction\"\n        if X is None:\n            return self._hy_test\n        X = self.convert_features(X)\n        if len(X) < self.nvar:\n            _ = 'Number of variables differ, trained with %s given %s' % (self.nvar, len(X))\n            raise RuntimeError(_)\n        hist = self._hist\n        for node in hist:\n            if node.height:\n                node.eval(hist)\n            else:\n                node.eval(X)\n        node.normalize()\n        r = node.hy\n        for i in hist[:-1]:\n            i.hy = None\n            i.hy_test = None\n        gc.collect()\n        return r", "response": "Decision function i. e. the raw data of the prediction"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fitness_vs(self):\n        \"Median Fitness in the validation set\"\n        l = [x.fitness_vs for x in self.models]\n        return np.median(l)", "response": "Median Fitness in the validation set"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the data for the current instance of the class.", "response": "def load_data(self, num_samples=1000, percentiles=None):\n        \"\"\"\n        Args:\n            num_samples: Number of random samples at each grid point\n            percentiles: Which percentiles to extract from the random samples\n\n        Returns:\n        \"\"\"\n        self.percentiles = percentiles\n        self.num_samples = num_samples\n        if self.model_name.lower() in [\"wrf\"]:\n            mo = ModelOutput(self.ensemble_name, self.member, self.run_date, self.variable,\n                             self.start_date, self.end_date, self.path, self.map_file, self.single_step)\n            mo.load_data()\n            self.data = mo.data[:]\n            if mo.units == \"m\":\n                self.data *= 1000\n                self.units = \"mm\"\n            else:\n                self.units = mo.units\n        else:\n            if self.track_forecasts is None:\n                self.load_track_data()\n            self.units = \"mm\"\n            self.data = np.zeros((self.forecast_hours.size,\n                                  self.mapping_data[\"lon\"].shape[0],\n                                  self.mapping_data[\"lon\"].shape[1]), dtype=np.float32)\n            \n            if self.percentiles is not None:\n                self.percentile_data = np.zeros([len(self.percentiles)] + list(self.data.shape))\n            full_condition_name = \"condition_\" + self.condition_model_name.replace(\" \", \"-\")\n            dist_model_name = \"dist\" + \"_\" + self.model_name.replace(\" \", \"-\")\n            for track_forecast in self.track_forecasts:\n                times = track_forecast[\"properties\"][\"times\"]\n                for s, step in enumerate(track_forecast[\"features\"]):\n                    forecast_params = step[\"properties\"][dist_model_name]\n                    if self.condition_model_name is not None:\n                        condition = step[\"properties\"][full_condition_name]\n                    else:\n                        condition = None\n                    forecast_time = self.run_date + timedelta(hours=times[s])\n                    if forecast_time in self.times:\n                        t = np.where(self.times == forecast_time)[0][0]\n                        mask = np.array(step[\"properties\"][\"masks\"], dtype=int).ravel()\n                        rankings = np.argsort(np.array(step[\"properties\"][\"timesteps\"]).ravel()[mask==1])\n                        i = np.array(step[\"properties\"][\"i\"], dtype=int).ravel()[mask == 1][rankings]\n                        j = np.array(step[\"properties\"][\"j\"], dtype=int).ravel()[mask == 1][rankings]\n                        if rankings.size > 0 and forecast_params[0] > 0.1 and 1 < forecast_params[2] < 100:\n                            raw_samples = np.sort(gamma.rvs(forecast_params[0], loc=forecast_params[1],\n                                                            scale=forecast_params[2],\n                                                            size=(num_samples, rankings.size)),\n                                                  axis=1)\n                            if self.percentiles is None:\n                                samples = raw_samples.mean(axis=0)\n                                if condition >= self.condition_threshold:\n                                    self.data[t, i, j] = samples\n                            else:\n                                for p, percentile in enumerate(self.percentiles):\n                                    if percentile != \"mean\":\n                                        if condition >= self.condition_threshold:\n                                            self.percentile_data[p, t, i, j] = np.percentile(raw_samples, percentile,\n                                                                                             axis=0)\n                                    else:\n                                        if condition >= self.condition_threshold:\n                                            self.percentile_data[p, t, i, j] = np.mean(raw_samples, axis=0)\n                                samples = raw_samples.mean(axis=0)\n                                if condition >= self.condition_threshold:\n                                    self.data[t, i, j] = samples"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a probability based on the number of grid points that exceed a threshold and a radius.", "response": "def neighborhood_probability(self, threshold, radius):\n        \"\"\"\n        Calculate a probability based on the number of grid points in an area that exceed a threshold.\n\n        Args:\n            threshold:\n            radius:\n\n        Returns:\n\n        \"\"\"\n        weights = disk(radius, dtype=np.uint8)\n        thresh_data = np.zeros(self.data.shape[1:], dtype=np.uint8)\n        neighbor_prob = np.zeros(self.data.shape, dtype=np.float32)\n        for t in np.arange(self.data.shape[0]):\n            thresh_data[self.data[t] >= threshold] = 1\n            maximized = fftconvolve(thresh_data, weights, mode=\"same\")\n            maximized[maximized > 1] = 1\n            maximized[maximized < 1] = 0\n            neighbor_prob[t] = fftconvolve(maximized, weights, mode=\"same\")\n            thresh_data[:] = 0\n        neighbor_prob[neighbor_prob < 1] = 0\n        neighbor_prob /= weights.sum()\n        return neighbor_prob"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode member percentile data to GRIB2 format.", "response": "def encode_grib2_percentile(self):\n        \"\"\"\n        Encodes member percentile data to GRIB2 format.\n\n        Returns:\n            Series of GRIB2 messages\n        \"\"\"\n        lscale = 1e6\n        grib_id_start = [7, 0, 14, 14, 2]\n        gdsinfo = np.array([0, np.product(self.data.shape[-2:]), 0, 0, 30], dtype=np.int32)\n        lon_0 = self.proj_dict[\"lon_0\"]\n        sw_lon = self.grid_dict[\"sw_lon\"]\n        if lon_0 < 0:\n            lon_0 += 360\n        if sw_lon < 0:\n            sw_lon += 360\n        gdtmp1 = [1, 0, self.proj_dict['a'], 0, float(self.proj_dict['a']), 0, float(self.proj_dict['b']),\n                  self.data.shape[-1], self.data.shape[-2], self.grid_dict[\"sw_lat\"] * lscale,\n                  sw_lon * lscale, 0, self.proj_dict[\"lat_0\"] * lscale,\n                  lon_0 * lscale,\n                  self.grid_dict[\"dx\"] * 1e3, self.grid_dict[\"dy\"] * 1e3, 0b00000000, 0b01000000,\n                  self.proj_dict[\"lat_1\"] * lscale,\n                  self.proj_dict[\"lat_2\"] * lscale, -90 * lscale, 0]\n        pdtmp1 = np.array([1,                # parameter category Moisture\n                           31,               # parameter number Hail\n                           4,                # Type of generating process Ensemble Forecast\n                           0,                # Background generating process identifier\n                           31,               # Generating process or model from NCEP\n                           0,                # Hours after reference time data cutoff\n                           0,                # Minutes after reference time data cutoff\n                           1,                # Forecast time units Hours\n                           0,                # Forecast time\n                           1,                # Type of first fixed surface Ground\n                           1,                # Scale value of first fixed surface\n                           0,                # Value of first fixed surface\n                           1,                # Type of second fixed surface\n                           1,                # Scale value of 2nd fixed surface\n                           0,                # Value of 2nd fixed surface\n                           0,                # Derived forecast type\n                           self.num_samples  # Number of ensemble members\n                           ], dtype=np.int32)\n        grib_objects = pd.Series(index=self.times, data=[None] * self.times.size, dtype=object)\n        drtmp1 = np.array([0, 0, 4, 8, 0], dtype=np.int32)\n        for t, time in enumerate(self.times):\n            time_list = list(self.run_date.utctimetuple()[0:6])\n            if grib_objects[time] is None:\n                grib_objects[time] = Grib2Encode(0, np.array(grib_id_start + time_list + [2, 1], dtype=np.int32))\n                grib_objects[time].addgrid(gdsinfo, gdtmp1)\n            pdtmp1[8] = (time.to_pydatetime() - self.run_date).total_seconds() / 3600.0\n            data = self.percentile_data[:, t] / 1000.0\n            masked_data = np.ma.array(data, mask=data <= 0)\n            for p, percentile in enumerate(self.percentiles):\n                print(\"GRIB {3} Percentile {0}. Max: {1} Min: {2}\".format(percentile, \n                                                                          masked_data[p].max(), \n                                                                          masked_data[p].min(),\n                                                                          time))\n                if percentile in range(1, 100):\n                    pdtmp1[-2] = percentile\n                    grib_objects[time].addfield(6, pdtmp1[:-1], 0, drtmp1, masked_data[p])\n                else:\n                    pdtmp1[-2] = 0\n                    grib_objects[time].addfield(2, pdtmp1, 0, drtmp1, masked_data[p])\n        return grib_objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_grib2_data(self):\n        lscale = 1e6\n        grib_id_start = [7, 0, 14, 14, 2]\n        gdsinfo = np.array([0, np.product(self.data.shape[-2:]), 0, 0, 30], dtype=np.int32)\n        lon_0 = self.proj_dict[\"lon_0\"]\n        sw_lon = self.grid_dict[\"sw_lon\"]\n        if lon_0 < 0:\n            lon_0 += 360\n        if sw_lon < 0:\n            sw_lon += 360\n        gdtmp1 = [1, 0, self.proj_dict['a'], 0, float(self.proj_dict['a']), 0, float(self.proj_dict['b']),\n                  self.data.shape[-1], self.data.shape[-2], self.grid_dict[\"sw_lat\"] * lscale,\n                  sw_lon * lscale, 0, self.proj_dict[\"lat_0\"] * lscale,\n                  lon_0 * lscale,\n                  self.grid_dict[\"dx\"] * 1e3, self.grid_dict[\"dy\"] * 1e3, 0b00000000, 0b01000000,\n                  self.proj_dict[\"lat_1\"] * lscale,\n                  self.proj_dict[\"lat_2\"] * lscale, -90 * lscale, 0]\n        pdtmp1 = np.array([1,                # parameter category Moisture\n                           31,               # parameter number Hail\n                           4,                # Type of generating process Ensemble Forecast\n                           0,                # Background generating process identifier\n                           31,               # Generating process or model from NCEP\n                           0,                # Hours after reference time data cutoff\n                           0,                # Minutes after reference time data cutoff\n                           1,                # Forecast time units Hours\n                           0,                # Forecast time\n                           1,                # Type of first fixed surface Ground\n                           1,                # Scale value of first fixed surface\n                           0,                # Value of first fixed surface\n                           1,                # Type of second fixed surface\n                           1,                # Scale value of 2nd fixed surface\n                           0,                # Value of 2nd fixed surface\n                           0,                # Derived forecast type\n                           1                 # Number of ensemble members\n                           ], dtype=np.int32)\n        grib_objects = pd.Series(index=self.times, data=[None] * self.times.size, dtype=object)\n        drtmp1 = np.array([0, 0, 4, 8, 0], dtype=np.int32)\n        for t, time in enumerate(self.times):\n            time_list = list(self.run_date.utctimetuple()[0:6])\n            if grib_objects[time] is None:\n                grib_objects[time] = Grib2Encode(0, np.array(grib_id_start + time_list + [2, 1], dtype=np.int32))\n                grib_objects[time].addgrid(gdsinfo, gdtmp1)\n            pdtmp1[8] = (time.to_pydatetime() - self.run_date).total_seconds() / 3600.0\n            data = self.data[t] / 1000.0\n            data[np.isnan(data)] = 0\n            masked_data = np.ma.array(data, mask=data<=0)\n            pdtmp1[-2] = 0\n            grib_objects[time].addfield(1, pdtmp1, 0, drtmp1, masked_data)\n        return grib_objects", "response": "Encodes member percentile data to GRIB2 format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_data(self):\n        for m, member in enumerate(self.members):\n            mo = ModelOutput(self.ensemble_name, member, self.run_date, self.variable,\n                             self.start_date, self.end_date, self.path, self.map_file, self.single_step)\n            mo.load_data()\n            if self.data is None:\n                self.data = np.zeros((len(self.members), mo.data.shape[0], mo.data.shape[1], mo.data.shape[2]),\n                                     dtype=np.float32)\n            if mo.units == \"m\":\n                self.data[m] = mo.data * 1000\n                self.units = \"mm\"\n            else:\n                self.data[m] = mo.data\n            if self.units == \"\":\n                self.units = mo.units\n            del mo.data\n            del mo", "response": "Loads data from each ensemble member."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate grid - point statistics across ensemble members.", "response": "def point_consensus(self, consensus_type):\n        \"\"\"\n        Calculate grid-point statistics across ensemble members.\n\n        Args:\n            consensus_type: mean, std, median, max, or percentile_nn\n\n        Returns:\n            EnsembleConsensus containing point statistic\n        \"\"\"\n        if \"mean\" in consensus_type:\n            consensus_data = np.mean(self.data, axis=0)\n        elif \"std\" in consensus_type:\n            consensus_data = np.std(self.data, axis=0)\n        elif \"median\" in consensus_type:\n            consensus_data = np.median(self.data, axis=0)\n        elif \"max\" in consensus_type:\n            consensus_data = np.max(self.data, axis=0)\n        elif \"percentile\" in consensus_type:\n            percentile = int(consensus_type.split(\"_\")[1])\n            consensus_data = np.percentile(self.data, percentile, axis=0)\n        else:\n            consensus_data = np.zeros(self.data.shape[1:])\n        consensus = EnsembleConsensus(consensus_data, consensus_type, self.ensemble_name,\n                                      self.run_date, self.variable, self.start_date, self.end_date, self.units)\n        return consensus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef point_probability(self, threshold):\n        point_prob = np.zeros(self.data.shape[1:])\n        for t in range(self.data.shape[1]):\n            point_prob[t] = np.where(self.data[:, t] >= threshold, 1.0, 0.0).mean(axis=0)\n        return EnsembleConsensus(point_prob, \"point_probability\", self.ensemble_name,\n                                 self.run_date, self.variable + \"_{0:0.2f}_{1}\".format(threshold,\n                                                                                       self.units.replace(\" \", \"_\")),\n                                 self.start_date, self.end_date, \"\")", "response": "Determines the probability of exceeding a threshold at a grid point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef neighborhood_probability(self, threshold, radius, sigmas=None):\n        if sigmas is None:\n            sigmas = [0]\n        weights = disk(radius)\n        filtered_prob = []\n        for sigma in sigmas:\n            filtered_prob.append(EnsembleConsensus(np.zeros(self.data.shape[1:], dtype=np.float32),\n                                                   \"neighbor_prob_r_{0:d}_s_{1:d}\".format(radius, sigma),\n                                                   self.ensemble_name,\n                                                   self.run_date, self.variable + \"_{0:0.2f}\".format(threshold),\n                                                   self.start_date, self.end_date, \"\"))\n        thresh_data = np.zeros(self.data.shape[2:], dtype=np.uint8)\n        neighbor_prob = np.zeros(self.data.shape[2:], dtype=np.float32)\n        for t in range(self.data.shape[1]):\n            for m in range(self.data.shape[0]):\n                thresh_data[self.data[m, t] >= threshold] = 1\n                maximized = fftconvolve(thresh_data, weights, mode=\"same\")\n                maximized[maximized > 1] = 1\n                maximized[maximized < 1] = 0\n                neighbor_prob += fftconvolve(maximized, weights, mode=\"same\")\n                neighbor_prob[neighbor_prob < 1] = 0\n                thresh_data[:] = 0\n            neighbor_prob /= (self.data.shape[0] * float(weights.sum()))\n            for s, sigma in enumerate(sigmas):\n                if sigma > 0:\n                    filtered_prob[s].data[t] = gaussian_filter(neighbor_prob, sigma=sigma)\n                else:\n                    filtered_prob[s].data[t] = neighbor_prob\n            neighbor_prob[:] = 0\n        return filtered_prob", "response": "Calculates the probability of exceeding a threshold based on model values within a specified radius of a point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the maximum neighborhood probability at any time over the period loaded.", "response": "def period_max_neighborhood_probability(self, threshold, radius, sigmas=None):\n        \"\"\"\n        Calculates the neighborhood probability of exceeding a threshold at any time over the period loaded.\n\n        Args:\n            threshold (float): splitting threshold for probability calculatations\n            radius (int): distance from point in number of grid points to include in neighborhood calculation.\n            sigmas (array of ints): Radii for Gaussian filter used to smooth neighborhood probabilities.\n\n        Returns:\n            list of EnsembleConsensus objects\n        \"\"\"\n        if sigmas is None:\n            sigmas = [0]\n        weights = disk(radius)\n        neighborhood_prob = np.zeros(self.data.shape[2:], dtype=np.float32)\n        thresh_data = np.zeros(self.data.shape[2:], dtype=np.uint8)\n        for m in range(self.data.shape[0]):\n            thresh_data[self.data[m].max(axis=0) >= threshold] = 1\n            maximized = fftconvolve(thresh_data, weights, mode=\"same\")\n            maximized[maximized > 1] = 1\n            neighborhood_prob += fftconvolve(maximized, weights, mode=\"same\")\n        neighborhood_prob[neighborhood_prob < 1] = 0\n        neighborhood_prob /= (self.data.shape[0] * float(weights.sum()))\n        consensus_probs = []\n        for sigma in sigmas:\n            if sigma > 0:\n                filtered_prob = gaussian_filter(neighborhood_prob, sigma=sigma)\n            else:\n                filtered_prob = neighborhood_prob\n            ec = EnsembleConsensus(filtered_prob,\n                                   \"neighbor_prob_{0:02d}-hour_r_{1:d}_s_{2:d}\".format(self.data.shape[1],\n                                                                                       radius, sigma),\n                                   self.ensemble_name,\n                                   self.run_date, self.variable + \"_{0:0.2f}\".format(float(threshold)),\n                                   self.start_date, self.end_date, \"\")\n            consensus_probs.append(ec)\n        return consensus_probs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data(self, grid_method=\"gamma\", num_samples=1000, condition_threshold=0.5, zero_inflate=False,\n                  percentile=None):\n        \"\"\"\n        Reads the track forecasts and converts them to grid point values based on random sampling.\n\n        Args:\n            grid_method: \"gamma\" by default\n            num_samples: Number of samples drawn from predicted pdf\n            condition_threshold: Objects are not written to the grid if condition model probability is below this\n                threshold.\n            zero_inflate: Whether to sample zeros from a Bernoulli sampler based on the condition model probability\n            percentile: If None, outputs the mean of the samples at each grid point, otherwise outputs the specified\n                percentile from 0 to 100.\n\n        Returns:\n            0 if tracks are successfully sampled on to grid. If no tracks are found, returns -1.\n        \"\"\"\n        self.percentile = percentile\n        if self.track_forecasts == {}:\n            self.load_track_forecasts()\n        if self.track_forecasts == {}:\n            return -1\n        if self.data is None:\n            self.data = np.zeros((len(self.members), self.times.size, self.grid_shape[0], self.grid_shape[1]),\n                                 dtype=np.float32)\n        else:\n            self.data[:] = 0\n        if grid_method in [\"mean\", \"median\", \"samples\"]:\n            for m, member in enumerate(self.members):\n                print(\"Sampling \" + member)\n                for track_forecast in self.track_forecasts[member]:\n                    times = track_forecast[\"properties\"][\"times\"]\n                    for s, step in enumerate(track_forecast[\"features\"]):\n                        forecast_pdf = np.array(step['properties'][self.variable + \"_\" +\n                                                                   self.ensemble_name.replace(\" \", \"-\")])\n                        forecast_time = self.run_date + timedelta(hours=times[s])\n                        t = np.where(self.times == forecast_time)[0][0]\n                        mask = np.array(step['properties'][\"masks\"], dtype=int)\n                        i = np.array(step['properties'][\"i\"], dtype=int)\n                        i = i[mask == 1]\n                        j = np.array(step['properties'][\"j\"], dtype=int)\n                        j = j[mask == 1]\n                        if grid_method == \"samples\":\n                            intensities = np.array(step[\"properties\"][\"timesteps\"], dtype=float)[mask == 1]\n                            rankings = np.argsort(intensities)\n                            samples = np.random.choice(self.forecast_bins, size=intensities.size, replace=True,\n                                                       p=forecast_pdf)\n                            self.data[m, t, i[rankings], j[rankings]] = samples\n                        else:\n                            if grid_method == \"mean\":\n                                forecast_value = np.sum(forecast_pdf * self.forecast_bins)\n                            elif grid_method == \"median\":\n                                forecast_cdf = np.cumsum(forecast_pdf)\n                                forecast_value = self.forecast_bins[np.argmin(np.abs(forecast_cdf - 0.5))]\n                            else:\n                                forecast_value = 0\n                            self.data[m, t, i, j] = forecast_value\n        if grid_method in [\"gamma\"]:\n            full_condition_name = \"condition_\" + self.condition_model_name.replace(\" \", \"-\")\n            dist_model_name = self.variable + \"_\" + self.ensemble_name.replace(\" \", \"-\")\n            for m, member in enumerate(self.members):\n                for track_forecast in self.track_forecasts[member]:\n                    times = track_forecast[\"properties\"][\"times\"]\n                    for s, step in enumerate(track_forecast[\"features\"]):\n                        forecast_params = step[\"properties\"][dist_model_name]\n                        if self.condition_model_name is not None:\n                            condition = step[\"properties\"][full_condition_name]\n                        else:\n                            condition = None\n                        forecast_time = self.run_date + timedelta(hours=times[s])\n                        if forecast_time in self.times:\n                            t = np.where(self.times == forecast_time)[0][0]\n                            mask = np.array(step[\"properties\"][\"masks\"], dtype=int)\n                            rankings = np.argsort(step[\"properties\"][\"timesteps\"])[mask == 1]\n                            i = np.array(step[\"properties\"][\"i\"], dtype=int)[mask == 1][rankings]\n                            j = np.array(step[\"properties\"][\"j\"], dtype=int)[mask == 1][rankings]\n                            if rankings.size > 0:\n                                raw_samples = np.sort(gamma.rvs(forecast_params[0], loc=forecast_params[1],\n                                                                scale=forecast_params[2],\n                                                                size=(num_samples, rankings.size)),\n                                                      axis=1)\n                                if zero_inflate:\n                                    raw_samples *= bernoulli.rvs(condition,\n                                                                 size=(num_samples, rankings.size))\n                                if percentile is None:\n                                    samples = raw_samples.mean(axis=0)\n                                else:\n                                    samples = np.percentile(raw_samples, percentile, axis=0)\n                                if condition is None or condition >= condition_threshold:\n                                    self.data[m, t, i, j] = samples\n        return 0", "response": "Reads the track forecasts and converts them to grid points based on the condition model probability."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite data to grib2 file. Currently only works for GRIB2.", "response": "def write_grib2(self, path):\n        \"\"\"\n        Writes data to grib2 file. Currently, grib codes are set by hand to hail.\n\n        Args:\n            path: Path to directory containing grib2 files.\n\n        Returns:\n\n        \"\"\"\n        if self.percentile is None:\n            var_type = \"mean\"\n        else:\n            var_type = \"p{0:02d}\".format(self.percentile)\n        lscale = 1e6\n        grib_id_start = [7, 0, 14, 14, 2]\n        gdsinfo = np.array([0, np.product(self.data.shape[-2:]), 0, 0, 30], dtype=np.int32)\n        lon_0 = self.proj_dict[\"lon_0\"]\n        sw_lon = self.grid_dict[\"sw_lon\"]\n        if lon_0 < 0:\n            lon_0 += 360\n        if sw_lon < 0:\n            sw_lon += 360\n        gdtmp1 = np.array([7, 1, self.proj_dict['a'], 1, self.proj_dict['a'], 1, self.proj_dict['b'],\n                           self.data.shape[-2], self.data.shape[-1], self.grid_dict[\"sw_lat\"] * lscale,\n                           sw_lon * lscale, 0, self.proj_dict[\"lat_0\"] * lscale,\n                           lon_0 * lscale,\n                           self.grid_dict[\"dx\"] * 1e3, self.grid_dict[\"dy\"] * 1e3, 0,\n                           self.proj_dict[\"lat_1\"] * lscale,\n                           self.proj_dict[\"lat_2\"] * lscale, 0, 0], dtype=np.int32)\n        pdtmp1 = np.array([1, 31, 2, 0, 116, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 192, 0, self.data.shape[0]], dtype=np.int32)\n        for m, member in enumerate(self.members):\n            pdtmp1[-2] = m\n            for t, time in enumerate(self.times):\n                time_list = list(time.utctimetuple()[0:6])\n                grbe = Grib2Encode(0, np.array(grib_id_start + time_list + [2, 1], dtype=np.int32))\n                grbe.addgrid(gdsinfo, gdtmp1)\n                pdtmp1[8] = (time.to_pydatetime() - self.run_date).total_seconds() / 3600.0\n                drtmp1 = np.array([0, 0, 4, 8, 0], dtype=np.int32)\n                data = self.data[m, t].astype(np.float32) / 1000.0\n                masked_data = np.ma.array(data, mask=data <= 0)\n                grbe.addfield(1, pdtmp1, 0, drtmp1, masked_data)\n                grbe.end()\n                filename = path + \"{0}_{1}_mlhail_{2}_{3}.grib2\".format(self.ensemble_name.replace(\" \", \"-\"), member,\n                                                                        var_type,\n                                                                        time.to_datetime().strftime(\"%Y%m%d%H%M\"))\n                print(\"Writing to \" + filename)\n                grib_file = open(filename, \"wb\")\n                grib_file.write(grbe.msg)\n                grib_file.close()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the netCDF file for writing.", "response": "def init_file(self, filename, time_units=\"seconds since 1970-01-01T00:00\"):\n        \"\"\"\n        Initializes netCDF file for writing\n\n        Args:\n            filename: Name of the netCDF file\n            time_units: Units for the time variable in format \"<time> since <date string>\"\n        Returns:\n            Dataset object\n        \"\"\"\n        if os.access(filename, os.R_OK):\n            out_data = Dataset(filename, \"r+\")\n        else:\n            out_data = Dataset(filename, \"w\")\n            if len(self.data.shape) == 2:\n                for d, dim in enumerate([\"y\", \"x\"]):\n                    out_data.createDimension(dim, self.data.shape[d])\n            else:\n                for d, dim in enumerate([\"y\", \"x\"]):\n                    out_data.createDimension(dim, self.data.shape[d+1])\n            out_data.createDimension(\"time\", len(self.times))\n            time_var = out_data.createVariable(\"time\", \"i8\", (\"time\",))\n            time_var[:] = date2num(self.times.to_pydatetime(), time_units)\n            time_var.units = time_units\n            out_data.Conventions = \"CF-1.6\"\n        return out_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noutput data to a netCDF file.", "response": "def write_to_file(self, out_data):\n        \"\"\"\n        Outputs data to a netCDF file. If the file does not exist, it will be created. Otherwise, additional variables\n        are appended to the current file\n\n        Args:\n            out_data: Full-path and name of output netCDF file\n        \"\"\"\n        full_var_name = self.consensus_type + \"_\" + self.variable\n        if \"-hour\" in self.consensus_type:\n            if full_var_name not in out_data.variables.keys():\n                var = out_data.createVariable(full_var_name, \"f4\", (\"y\", \"x\"), zlib=True, \n                                              least_significant_digit=3, shuffle=True)\n            else:\n                var = out_data.variables[full_var_name]\n            var.coordinates = \"y x\"\n        else:\n            if full_var_name not in out_data.variables.keys():\n                var = out_data.createVariable(full_var_name, \"f4\", (\"time\", \"y\", \"x\"), zlib=True,\n                                              least_significant_digit=3, shuffle=True)\n            else:\n                var = out_data.variables[full_var_name]\n            var.coordinates = \"time y x\"\n        var[:] = self.data\n        var.units = self.units\n        var.long_name = self.consensus_type + \"_\" + self.variable\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restore(self, workspace_uuid):\n        workspace = next((workspace for workspace in self.document_model.workspaces if workspace.uuid == workspace_uuid), None)\n        if workspace is None:\n            workspace = self.new_workspace()\n        self._change_workspace(workspace)", "response": "Restore the workspace to the given workspace_uuid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new workspace and insert it into document_model and return it.", "response": "def new_workspace(self, name=None, layout=None, workspace_id=None, index=None) -> WorkspaceLayout.WorkspaceLayout:\n        \"\"\" Create a new workspace, insert into document_model, and return it. \"\"\"\n        workspace = WorkspaceLayout.WorkspaceLayout()\n        self.document_model.insert_workspace(index if index is not None else len(self.document_model.workspaces), workspace)\n        d = create_image_desc()\n        d[\"selected\"] = True\n        workspace.layout = layout if layout is not None else d\n        workspace.name = name if name is not None else _(\"Workspace\")\n        if workspace_id:\n            workspace.workspace_id = workspace_id\n        return workspace"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_workspace(self, name, layout, workspace_id):\n        workspace = next((workspace for workspace in self.document_model.workspaces if workspace.workspace_id == workspace_id), None)\n        if not workspace:\n            workspace = self.new_workspace(name=name, layout=layout, workspace_id=workspace_id)\n        self._change_workspace(workspace)", "response": "Ensures that a workspace with the given name and layout exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npose a dialog to name and create a workspace.", "response": "def create_workspace(self) -> None:\n        \"\"\" Pose a dialog to name and create a workspace. \"\"\"\n\n        def create_clicked(text):\n            if text:\n                command = Workspace.CreateWorkspaceCommand(self, text)\n                command.perform()\n                self.document_controller.push_undo_command(command)\n\n        self.pose_get_string_message_box(caption=_(\"Enter a name for the workspace\"), text=_(\"Workspace\"),\n                                         accepted_fn=create_clicked, accepted_text=_(\"Create\"),\n                                         message_box_id=\"create_workspace\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_workspace(self) -> None:\n\n        def rename_clicked(text):\n            if len(text) > 0:\n                command = Workspace.RenameWorkspaceCommand(self, text)\n                command.perform()\n                self.document_controller.push_undo_command(command)\n\n        self.pose_get_string_message_box(caption=_(\"Enter new name for workspace\"), text=self.__workspace.name,\n                                         accepted_fn=rename_clicked, accepted_text=_(\"Rename\"),\n                                         message_box_id=\"rename_workspace\")", "response": "Pose a dialog to rename the workspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_workspace(self):\n\n        def confirm_clicked():\n            if len(self.document_model.workspaces) > 1:\n                command = Workspace.RemoveWorkspaceCommand(self)\n                command.perform()\n                self.document_controller.push_undo_command(command)\n\n        caption = _(\"Remove workspace named '{0}'?\").format(self.__workspace.name)\n        self.pose_confirmation_message_box(caption, confirm_clicked, accepted_text=_(\"Remove Workspace\"),\n                                           message_box_id=\"remove_workspace\")", "response": "Pose a dialog to confirm removal then remove workspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone_workspace(self) -> None:\n\n        def clone_clicked(text):\n            if text:\n                command = Workspace.CloneWorkspaceCommand(self, text)\n                command.perform()\n                self.document_controller.push_undo_command(command)\n\n        self.pose_get_string_message_box(caption=_(\"Enter a name for the workspace\"), text=self.__workspace.name,\n                                         accepted_fn=clone_clicked, accepted_text=_(\"Clone\"),\n                                         message_box_id=\"clone_workspace\")", "response": "Pose a dialog to name and clone a workspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the display item with the given display item.", "response": "def __replace_displayed_display_item(self, display_panel, display_item, d=None) -> Undo.UndoableCommand:\n        \"\"\" Used in drag/drop support. \"\"\"\n        self.document_controller.replaced_display_panel_content = display_panel.save_contents()\n        command = DisplayPanel.ReplaceDisplayPanelCommand(self)\n        if display_item:\n            display_panel.set_display_panel_display_item(display_item, detect_controller=True)\n        elif d is not None:\n            display_panel.change_display_panel_content(d)\n        display_panel.request_focus()\n        self.__sync_layout()\n        return command"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bootstrap(score_objs, n_boot=1000):\n    all_samples = np.random.choice(score_objs, size=(n_boot, len(score_objs)), replace=True)\n    return all_samples.sum(axis=1)", "response": "This function performs a\n    bootstrap resampling of the objects and returns the n_boot aggregations of the objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, forecasts, observations):\n        for t, threshold in enumerate(self.thresholds):\n            tp = np.count_nonzero((forecasts >= threshold) & (observations >= self.obs_threshold))\n            fp = np.count_nonzero((forecasts >= threshold) &\n                                  (observations < self.obs_threshold))\n            fn = np.count_nonzero((forecasts < threshold) &\n                                  (observations >= self.obs_threshold))\n            tn = np.count_nonzero((forecasts < threshold) &\n                                  (observations < self.obs_threshold))\n            self.contingency_tables.iloc[t] += [tp, fp, fn, tn]", "response": "Update the ROC curve with a set of forecasts and observations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge(self, other_roc):\n        if other_roc.thresholds.size == self.thresholds.size and np.all(other_roc.thresholds == self.thresholds):\n            self.contingency_tables += other_roc.contingency_tables\n        else:\n            print(\"Input table thresholds do not match.\")", "response": "Merge the values of another DistributedROC object into this one and update the statistics inplace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a ROC curve from the contingency table by calculating the probability of false detection and the probability of detection.", "response": "def roc_curve(self):\n        \"\"\"\n        Generate a ROC curve from the contingency table by calculating the probability of detection (TP/(TP+FN)) and the\n        probability of false detection (FP/(FP+TN)).\n\n        Returns:\n            A pandas.DataFrame containing the POD, POFD, and the corresponding probability thresholds.\n        \"\"\"\n        pod = self.contingency_tables[\"TP\"].astype(float) / (self.contingency_tables[\"TP\"] +\n                                                             self.contingency_tables[\"FN\"])\n        pofd = self.contingency_tables[\"FP\"].astype(float) / (self.contingency_tables[\"FP\"] +\n                                                              self.contingency_tables[\"TN\"])\n        return pd.DataFrame({\"POD\": pod, \"POFD\": pofd, \"Thresholds\": self.thresholds},\n                            columns=[\"POD\", \"POFD\", \"Thresholds\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef performance_curve(self):\n        pod = self.contingency_tables[\"TP\"] / (self.contingency_tables[\"TP\"] + self.contingency_tables[\"FN\"])\n        far = self.contingency_tables[\"FP\"] / (self.contingency_tables[\"FP\"] + self.contingency_tables[\"TP\"])\n        far[(self.contingency_tables[\"FP\"] + self.contingency_tables[\"TP\"]) == 0] = np.nan\n        return pd.DataFrame({\"POD\": pod, \"FAR\": far, \"Thresholds\": self.thresholds},\n                            columns=[\"POD\", \"FAR\", \"Thresholds\"])", "response": "Calculates the Probability of Detection and False Alarm Ratio in order to output a performance diagram."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auc(self):\n        roc_curve = self.roc_curve()\n        return np.abs(np.trapz(roc_curve['POD'], x=roc_curve['POFD']))", "response": "Calculate the Area Under the ROC Curve."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the maximum Critical Success Index across all probability thresholds Returns a float", "response": "def max_csi(self):\n        \"\"\"\n        Calculate the maximum Critical Success Index across all probability thresholds\n\n        Returns:\n            The maximum CSI as a float\n        \"\"\"\n        csi = self.contingency_tables[\"TP\"] / (self.contingency_tables[\"TP\"] + self.contingency_tables[\"FN\"] +\n                                               self.contingency_tables[\"FP\"])\n        return csi.max()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an Array of ContingencyTable objects for each probability threshold.", "response": "def get_contingency_tables(self):\n        \"\"\"\n        Create an Array of ContingencyTable objects for each probability threshold.\n\n        Returns:\n            Array of ContingencyTable objects\n        \"\"\"\n        return np.array([ContingencyTable(*ct) for ct in self.contingency_tables.values])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the DistributedROC string and parses the contingency tables from it.", "response": "def from_str(self, in_str):\n        \"\"\"\n        Read the DistributedROC string and parse the contingency table values from it.\n\n        Args:\n            in_str (str): The string output from the __str__ method\n        \"\"\"\n        parts = in_str.split(\";\")\n        for part in parts:\n            var_name, value = part.split(\":\")\n            if var_name == \"Obs_Threshold\":\n                self.obs_threshold = float(value)\n            elif var_name == \"Thresholds\":\n                self.thresholds = np.array(value.split(), dtype=float)\n                self.contingency_tables = pd.DataFrame(columns=self.contingency_tables.columns,\n                                                       data=np.zeros((self.thresholds.size,\n                                                                     self.contingency_tables.columns.size)))\n            elif var_name in self.contingency_tables.columns:\n                self.contingency_tables[var_name] = np.array(value.split(), dtype=int)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, forecasts, observations):\n        for t, threshold in enumerate(self.thresholds[:-1]):\n            self.frequencies.loc[t, \"Positive_Freq\"] += np.count_nonzero((threshold <= forecasts) &\n                                                                         (forecasts < self.thresholds[t+1]) &\n                                                                         (observations >= self.obs_threshold))\n            self.frequencies.loc[t, \"Total_Freq\"] += np.count_nonzero((threshold <= forecasts) &\n                                                                      (forecasts < self.thresholds[t+1]))", "response": "Update the statistics with a set of forecasts and observations."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ningests another DistributedReliability and add its contents to the current object.", "response": "def merge(self, other_rel):\n        \"\"\"\n        Ingest another DistributedReliability and add its contents to the current object.\n\n        Args:\n            other_rel: a Distributed reliability object.\n        \"\"\"\n        if other_rel.thresholds.size == self.thresholds.size and np.all(other_rel.thresholds == self.thresholds):\n            self.frequencies += other_rel.frequencies\n        else:\n            print(\"Input table thresholds do not match.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the reliability diagram statistics. The columns are Bin_Start Positive_Relative_Freq and Total_Relative_Freq.", "response": "def reliability_curve(self):\n        \"\"\"\n        Calculates the reliability diagram statistics. The key columns are Bin_Start and Positive_Relative_Freq\n\n        Returns:\n            pandas.DataFrame\n        \"\"\"\n        total = self.frequencies[\"Total_Freq\"].sum()\n        curve = pd.DataFrame(columns=[\"Bin_Start\", \"Bin_End\", \"Bin_Center\",\n                                      \"Positive_Relative_Freq\", \"Total_Relative_Freq\"])\n        curve[\"Bin_Start\"] = self.thresholds[:-1]\n        curve[\"Bin_End\"] = self.thresholds[1:]\n        curve[\"Bin_Center\"] = 0.5 * (self.thresholds[:-1] + self.thresholds[1:])\n        curve[\"Positive_Relative_Freq\"] = self.frequencies[\"Positive_Freq\"] / self.frequencies[\"Total_Freq\"]\n        curve[\"Total_Relative_Freq\"] = self.frequencies[\"Total_Freq\"] / total\n        return curve"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef brier_score_components(self):\n        rel_curve = self.reliability_curve()\n        total = self.frequencies[\"Total_Freq\"].sum()\n        climo_freq = float(self.frequencies[\"Positive_Freq\"].sum()) / self.frequencies[\"Total_Freq\"].sum()\n        reliability = np.sum(self.frequencies[\"Total_Freq\"] * (rel_curve[\"Bin_Start\"] -\n                                                               rel_curve[\"Positive_Relative_Freq\"]) ** 2) / total\n        resolution = np.sum(self.frequencies[\"Total_Freq\"] * (rel_curve[\"Positive_Relative_Freq\"] - climo_freq) ** 2) \\\n                     / total\n        uncertainty = climo_freq * (1 - climo_freq)\n        return reliability, resolution, uncertainty", "response": "Calculate the components of the Brier score decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef brier_score(self):\n        reliability, resolution, uncertainty = self.brier_score_components()\n        return reliability - resolution + uncertainty", "response": "Calculate the Brier Score of the current set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the Brier Skill Score", "response": "def brier_skill_score(self):\n        \"\"\"\n        Calculate the Brier Skill Score\n        \"\"\"\n        reliability, resolution, uncertainty = self.brier_score_components()\n        return (resolution - reliability) / uncertainty"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the statistics with the forecasts and observations.", "response": "def update(self, forecasts, observations):\n        \"\"\"\n        Update the statistics with forecasts and observations.\n\n        Args:\n            forecasts: The discrete Cumulative Distribution Functions of\n            observations:\n        \"\"\"\n        if len(observations.shape) == 1:\n            obs_cdfs = np.zeros((observations.size, self.thresholds.size))\n            for o, observation in enumerate(observations):\n                obs_cdfs[o, self.thresholds >= observation] = 1\n        else:\n            obs_cdfs = observations\n        self.errors[\"F_2\"] += np.sum(forecasts ** 2, axis=0)\n        self.errors[\"F_O\"] += np.sum(forecasts * obs_cdfs, axis=0)\n        self.errors[\"O_2\"] += np.sum(obs_cdfs ** 2, axis=0)\n        self.errors[\"O\"] += np.sum(obs_cdfs, axis=0)\n        self.num_forecasts += forecasts.shape[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the continuous ranked probability score.", "response": "def crps(self):\n        \"\"\"\n        Calculates the continuous ranked probability score.\n        \"\"\"\n        return np.sum(self.errors[\"F_2\"].values - self.errors[\"F_O\"].values * 2.0 + self.errors[\"O_2\"].values) / \\\n            (self.thresholds.size * self.num_forecasts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the climatological CRPS.", "response": "def crps_climo(self):\n        \"\"\"\n        Calculate the climatological CRPS.\n        \"\"\"\n        o_bar = self.errors[\"O\"].values / float(self.num_forecasts)\n        crps_c = np.sum(self.num_forecasts * (o_bar ** 2) - o_bar * self.errors[\"O\"].values * 2.0 +\n                        self.errors[\"O_2\"].values) / float(self.thresholds.size * self.num_forecasts)\n        return crps_c"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crpss(self):\n        crps_f = self.crps()\n        crps_c = self.crps_climo()\n        return 1.0 - float(crps_f) / float(crps_c)", "response": "Calculate the continous ranked probability skill score from existing data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checar(cliente_sat):\n\n    resposta = cliente_sat.consultar_status_operacional()\n    alertas = []\n    for classe_alerta in AlertaOperacao.alertas_registrados:\n        alerta = classe_alerta(resposta)\n        if alerta.checar():\n            alertas.append(alerta)\n    return alertas", "response": "Checar um listos os alertas registrados."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the metadata value for the given key exists in the current language.", "response": "def has_metadata_value(metadata_source, key: str) -> bool:\n    \"\"\"Return whether the metadata value for the given key exists.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        d = getattr(metadata_source, \"session_metadata\", dict())\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None:\n            return desc['path'][-1] in d\n    desc = key_map.get(key)\n    if desc is not None:\n        d = getattr(metadata_source, \"metadata\", dict())\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None:\n            return desc['path'][-1] in d\n    raise False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the metadata value for the given key.", "response": "def get_metadata_value(metadata_source, key: str) -> typing.Any:\n    \"\"\"Get the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"session_metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    desc = key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    raise KeyError()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the metadata value for the given key.", "response": "def set_metadata_value(metadata_source, key: str, value: typing.Any) -> None:\n    \"\"\"Set the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"session_metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None:\n            d[desc['path'][-1]] = value\n            metadata_source.session_metadata = d0\n            return\n    desc = key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None:\n            d[desc['path'][-1]] = value\n            metadata_source.metadata = d0\n            return\n    raise KeyError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the metadata value for the given key.", "response": "def delete_metadata_value(metadata_source, key: str) -> None:\n    \"\"\"Delete the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<dotted>.<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'stem.session.instrument' or 'stm.camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"session_metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None and desc['path'][-1] in d:\n            d.pop(desc['path'][-1], None)\n            metadata_source.session_metadata = d0\n            return\n    desc = key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None and desc['path'][-1] in d:\n            d.pop(desc['path'][-1], None)\n            metadata_source.metadata = d0\n            return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the y - axis items dependent on the plot height.", "response": "def calculate_y_ticks(self, plot_height):\n        \"\"\"Calculate the y-axis items dependent on the plot height.\"\"\"\n\n        calibrated_data_min = self.calibrated_data_min\n        calibrated_data_max = self.calibrated_data_max\n        calibrated_data_range = calibrated_data_max - calibrated_data_min\n\n        ticker = self.y_ticker\n        y_ticks = list()\n        for tick_value, tick_label in zip(ticker.values, ticker.labels):\n            if calibrated_data_range != 0.0:\n                y_tick = plot_height - plot_height * (tick_value - calibrated_data_min) / calibrated_data_range\n            else:\n                y_tick = plot_height - plot_height * 0.5\n            if y_tick >= 0 and y_tick <= plot_height:\n                y_ticks.append((y_tick, tick_label))\n\n        return y_ticks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_x_ticks(self, plot_width):\n\n        x_calibration = self.x_calibration\n\n        uncalibrated_data_left = self.__uncalibrated_left_channel\n        uncalibrated_data_right = self.__uncalibrated_right_channel\n\n        calibrated_data_left = x_calibration.convert_to_calibrated_value(uncalibrated_data_left) if x_calibration is not None else uncalibrated_data_left\n        calibrated_data_right = x_calibration.convert_to_calibrated_value(uncalibrated_data_right) if x_calibration is not None else uncalibrated_data_right\n        calibrated_data_left, calibrated_data_right = min(calibrated_data_left, calibrated_data_right), max(calibrated_data_left, calibrated_data_right)\n\n        graph_left, graph_right, tick_values, division, precision = Geometry.make_pretty_range(calibrated_data_left, calibrated_data_right)\n\n        drawn_data_width = self.drawn_right_channel - self.drawn_left_channel\n\n        x_ticks = list()\n        if drawn_data_width > 0.0:\n            for tick_value in tick_values:\n                label = nice_label(tick_value, precision)\n                data_tick = x_calibration.convert_from_calibrated_value(tick_value) if x_calibration else tick_value\n                x_tick = plot_width * (data_tick - self.drawn_left_channel) / drawn_data_width\n                if x_tick >= 0 and x_tick <= plot_width:\n                    x_ticks.append((x_tick, label))\n\n        return x_ticks", "response": "Calculate the x - axis items dependent on the plot width."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size_to_content(self):\n        new_sizing = self.copy_sizing()\n        new_sizing.minimum_height = 0\n        new_sizing.maximum_height = 0\n        axes = self.__axes\n        if axes and axes.is_valid:\n            if axes.x_calibration and axes.x_calibration.units:\n                new_sizing.minimum_height = self.font_size + 4\n                new_sizing.maximum_height = self.font_size + 4\n        self.update_sizing(new_sizing)", "response": "Size the canvas item to the proper height."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsize the canvas item to the proper width the maximum of any label.", "response": "def size_to_content(self, get_font_metrics_fn):\n        \"\"\" Size the canvas item to the proper width, the maximum of any label. \"\"\"\n        new_sizing = self.copy_sizing()\n\n        new_sizing.minimum_width = 0\n        new_sizing.maximum_width = 0\n\n        axes = self.__axes\n        if axes and axes.is_valid:\n\n            # calculate the width based on the label lengths\n            font = \"{0:d}px\".format(self.font_size)\n\n            max_width = 0\n            y_range = axes.calibrated_data_max - axes.calibrated_data_min\n            label = axes.y_ticker.value_label(axes.calibrated_data_max + y_range * 5)\n            max_width = max(max_width, get_font_metrics_fn(font, label).width)\n            label = axes.y_ticker.value_label(axes.calibrated_data_min - y_range * 5)\n            max_width = max(max_width, get_font_metrics_fn(font, label).width)\n\n            new_sizing.minimum_width = max_width\n            new_sizing.maximum_width = max_width\n\n        self.update_sizing(new_sizing)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsizes the canvas item to the proper width.", "response": "def size_to_content(self):\n        \"\"\" Size the canvas item to the proper width. \"\"\"\n        new_sizing = self.copy_sizing()\n        new_sizing.minimum_width = 0\n        new_sizing.maximum_width = 0\n        axes = self.__axes\n        if axes and axes.is_valid:\n            if axes.y_calibration and axes.y_calibration.units:\n                new_sizing.minimum_width = self.font_size + 4\n                new_sizing.maximum_width = self.font_size + 4\n        self.update_sizing(new_sizing)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the content of a snippet file in SNIPPETS_ROOT", "response": "def get_snippet_content(snippet_name, **format_kwargs):\n    \"\"\" Load the content from a snippet file which exists in SNIPPETS_ROOT \"\"\"\n    filename = snippet_name + '.snippet'\n    snippet_file = os.path.join(SNIPPETS_ROOT, filename)\n    if not os.path.isfile(snippet_file):\n        raise ValueError('could not find snippet with name ' + filename)\n    ret = helpers.get_file_content(snippet_file)\n    if format_kwargs:\n        ret = ret.format(**format_kwargs)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_display_properties(self, display_calibration_info, display_properties: typing.Mapping, display_layers: typing.Sequence[typing.Mapping]) -> None:\n\n        # may be called from thread; prevent a race condition with closing.\n        with self.__closing_lock:\n            if self.__closed:\n                return\n\n            displayed_dimensional_scales = display_calibration_info.displayed_dimensional_scales\n            displayed_dimensional_calibrations = display_calibration_info.displayed_dimensional_calibrations\n            self.__data_scale = displayed_dimensional_scales[-1] if len(displayed_dimensional_scales) > 0 else 1\n            self.__displayed_dimensional_calibration = displayed_dimensional_calibrations[-1] if len(displayed_dimensional_calibrations) > 0 else Calibration.Calibration(scale=displayed_dimensional_scales[-1])\n            self.__intensity_calibration = display_calibration_info.displayed_intensity_calibration\n            self.__calibration_style = display_calibration_info.calibration_style\n            self.__y_min = display_properties.get(\"y_min\")\n            self.__y_max = display_properties.get(\"y_max\")\n            self.__y_style = display_properties.get(\"y_style\", \"linear\")\n            self.__left_channel = display_properties.get(\"left_channel\")\n            self.__right_channel = display_properties.get(\"right_channel\")\n            self.__legend_position = display_properties.get(\"legend_position\")\n            self.__display_layers = display_layers\n\n            if self.__display_values_list and len(self.__display_values_list) > 0:\n                self.__xdata_list = [display_values.display_data_and_metadata if display_values else None for display_values in self.__display_values_list]\n                xdata0 = self.__xdata_list[0]\n                if xdata0:\n                    self.__update_frame(xdata0.metadata)\n            else:\n                self.__xdata_list = list()\n\n            # update the cursor info\n            self.__update_cursor_info()\n\n            # mark for update. prepare display will mark children for update if necesssary.\n            self.update()", "response": "Updates the display properties of the items in the canvas item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the view to encompass the channels and data represented by the given intervals.", "response": "def __view_to_intervals(self, data_and_metadata: DataAndMetadata.DataAndMetadata, intervals: typing.List[typing.Tuple[float, float]]) -> None:\n        \"\"\"Change the view to encompass the channels and data represented by the given intervals.\"\"\"\n        left = None\n        right = None\n        for interval in intervals:\n            left = min(left, interval[0]) if left is not None else interval[0]\n            right = max(right, interval[1]) if right is not None else interval[1]\n        left = left if left is not None else 0.0\n        right = right if right is not None else 1.0\n        left_channel = int(max(0.0, left) * data_and_metadata.data_shape[-1])\n        right_channel = int(min(1.0, right) * data_and_metadata.data_shape[-1])\n        data_min = numpy.amin(data_and_metadata.data[..., left_channel:right_channel])\n        data_max = numpy.amax(data_and_metadata.data[..., left_channel:right_channel])\n        if data_min > 0 and data_max > 0:\n            y_min = 0.0\n            y_max = data_max * 1.2\n        elif data_min < 0 and data_max < 0:\n            y_min = data_min * 1.2\n            y_max = 0.0\n        else:\n            y_min = data_min * 1.2\n            y_max = data_max * 1.2\n        extra = (right - left) * 0.5\n        display_left_channel = int(max(0.0, left - extra) * data_and_metadata.data_shape[-1])\n        display_right_channel = int(min(1.0, right + extra) * data_and_metadata.data_shape[-1])\n        # command = self.delegate.create_change_display_command()\n        self.delegate.update_display_properties({\"left_channel\": display_left_channel, \"right_channel\": display_right_channel, \"y_min\": y_min, \"y_max\": y_max})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the view to encompass the selected graphic intervals.", "response": "def __view_to_selected_graphics(self, data_and_metadata: DataAndMetadata.DataAndMetadata) -> None:\n        \"\"\"Change the view to encompass the selected graphic intervals.\"\"\"\n        all_graphics = self.__graphics\n        graphics = [graphic for graphic_index, graphic in enumerate(all_graphics) if self.__graphic_selection.contains(graphic_index)]\n        intervals = list()\n        for graphic in graphics:\n            if isinstance(graphic, Graphics.IntervalGraphic):\n                intervals.append(graphic.interval)\n        self.__view_to_intervals(data_and_metadata, intervals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares the display. This method gets called by the canvas layout/draw engine after being triggered by a call to `update`. When data or display parameters change, the internal state of the line plot gets updated. This method takes that internal state and updates the child canvas items. This method is always run on a thread and should be fast but doesn't need to be instant.", "response": "def prepare_display(self):\n        \"\"\"Prepare the display.\n\n        This method gets called by the canvas layout/draw engine after being triggered by a call to `update`.\n\n        When data or display parameters change, the internal state of the line plot gets updated. This method takes\n        that internal state and updates the child canvas items.\n\n        This method is always run on a thread and should be fast but doesn't need to be instant.\n        \"\"\"\n        displayed_dimensional_calibration = self.__displayed_dimensional_calibration\n        intensity_calibration = self.__intensity_calibration\n        calibration_style = self.__calibration_style\n        y_min = self.__y_min\n        y_max = self.__y_max\n        y_style = self.__y_style\n        left_channel = self.__left_channel\n        right_channel = self.__right_channel\n\n        scalar_xdata_list = None\n\n        def calculate_scalar_xdata(xdata_list):\n            scalar_xdata_list = list()\n            for xdata in xdata_list:\n                if xdata:\n                    scalar_data = Image.scalar_from_array(xdata.data)\n                    scalar_data = Image.convert_to_grayscale(scalar_data)\n                    scalar_intensity_calibration = calibration_style.get_intensity_calibration(xdata)\n                    scalar_dimensional_calibrations = calibration_style.get_dimensional_calibrations(xdata.dimensional_shape, xdata.dimensional_calibrations)\n                    if displayed_dimensional_calibration.units == scalar_dimensional_calibrations[-1].units and intensity_calibration.units == scalar_intensity_calibration.units:\n                        # the data needs to have an intensity scale matching intensity_calibration. convert the data to use the common scale.\n                        scale = scalar_intensity_calibration.scale / intensity_calibration.scale\n                        offset = (scalar_intensity_calibration.offset - intensity_calibration.offset) / intensity_calibration.scale\n                        scalar_data = scalar_data * scale + offset\n                        scalar_xdata_list.append(DataAndMetadata.new_data_and_metadata(scalar_data, scalar_intensity_calibration, scalar_dimensional_calibrations))\n                else:\n                    scalar_xdata_list.append(None)\n            return scalar_xdata_list\n\n        data_scale = self.__data_scale\n        xdata_list = self.__xdata_list\n\n        if data_scale is not None:\n            # update the line graph data\n            left_channel = left_channel if left_channel is not None else 0\n            right_channel = right_channel if right_channel is not None else data_scale\n            left_channel, right_channel = min(left_channel, right_channel), max(left_channel, right_channel)\n\n            scalar_data_list = None\n            if y_min is None or y_max is None and len(xdata_list) > 0:\n                scalar_xdata_list = calculate_scalar_xdata(xdata_list)\n                scalar_data_list = [xdata.data if xdata else None for xdata in scalar_xdata_list]\n            calibrated_data_min, calibrated_data_max, y_ticker = LineGraphCanvasItem.calculate_y_axis(scalar_data_list, y_min, y_max, intensity_calibration, y_style)\n            axes = LineGraphCanvasItem.LineGraphAxes(data_scale, calibrated_data_min, calibrated_data_max, left_channel, right_channel, displayed_dimensional_calibration, intensity_calibration, y_style, y_ticker)\n\n            if scalar_xdata_list is None:\n                if len(xdata_list) > 0:\n                    scalar_xdata_list = calculate_scalar_xdata(xdata_list)\n                else:\n                    scalar_xdata_list = list()\n\n            if self.__display_frame_rate_id:\n                Utility.fps_tick(\"prepare_\"+self.__display_frame_rate_id)\n\n            colors = ('#1E90FF', \"#F00\", \"#0F0\", \"#00F\", \"#FF0\", \"#0FF\", \"#F0F\", \"#888\", \"#800\", \"#080\", \"#008\", \"#CCC\", \"#880\", \"#088\", \"#808\", \"#964B00\")\n\n            display_layers = self.__display_layers\n\n            if len(display_layers) == 0:\n                index = 0\n                for scalar_index, scalar_xdata in enumerate(scalar_xdata_list):\n                    if scalar_xdata and scalar_xdata.is_data_1d:\n                        if index < 16:\n                            display_layers.append({\"fill_color\": colors[index] if index == 0 else None, \"stroke_color\": colors[index] if index > 0 else None, \"data_index\": scalar_index})\n                            index += 1\n                    if scalar_xdata and scalar_xdata.is_data_2d:\n                        for row in range(min(scalar_xdata.data_shape[-1], 16)):\n                            if index < 16:\n                                display_layers.append({\"fill_color\": colors[index] if index == 0 else None, \"stroke_color\": colors[index] if index > 0 else None, \"data_index\": scalar_index, \"data_row\": row})\n                                index += 1\n\n            display_layer_count = len(display_layers)\n\n            self.___has_valid_drawn_graph_data = False\n\n            for index, display_layer in enumerate(display_layers):\n                if index < 16:\n                    fill_color = display_layer.get(\"fill_color\")\n                    stroke_color = display_layer.get(\"stroke_color\")\n                    data_index = display_layer.get(\"data_index\", 0)\n                    data_row = display_layer.get(\"data_row\", 0)\n                    if 0 <= data_index < len(scalar_xdata_list):\n                        scalar_xdata = scalar_xdata_list[data_index]\n                        if scalar_xdata:\n                            data_row = max(0, min(scalar_xdata.dimensional_shape[0] - 1, data_row))\n                            intensity_calibration = scalar_xdata.intensity_calibration\n                            displayed_dimensional_calibration = scalar_xdata.dimensional_calibrations[-1]\n                            if scalar_xdata.is_data_2d:\n                                scalar_data = scalar_xdata.data[data_row:data_row + 1, :].reshape((scalar_xdata.dimensional_shape[-1],))\n                                scalar_xdata = DataAndMetadata.new_data_and_metadata(scalar_data, intensity_calibration, [displayed_dimensional_calibration])\n                        line_graph_canvas_item = self.__line_graph_stack.canvas_items[display_layer_count - (index + 1)]\n                        line_graph_canvas_item.set_fill_color(fill_color)\n                        line_graph_canvas_item.set_stroke_color(stroke_color)\n                        line_graph_canvas_item.set_axes(axes)\n                        line_graph_canvas_item.set_uncalibrated_xdata(scalar_xdata)\n                        self.___has_valid_drawn_graph_data = scalar_xdata is not None\n\n            for index in range(len(display_layers), 16):\n                line_graph_canvas_item = self.__line_graph_stack.canvas_items[index]\n                line_graph_canvas_item.set_axes(None)\n                line_graph_canvas_item.set_uncalibrated_xdata(None)\n\n            legend_position = self.__legend_position\n            LegendEntry = collections.namedtuple(\"LegendEntry\", [\"label\", \"fill_color\", \"stroke_color\"])\n            legend_entries = list()\n            for index, display_layer in enumerate(self.__display_layers):\n                data_index = display_layer.get(\"data_index\", None)\n                data_row = display_layer.get(\"data_row\", None)\n                label = display_layer.get(\"label\", str())\n                if not label:\n                    if data_index is not None and data_row is not None:\n                        label = \"Data {}:{}\".format(data_index, data_row)\n                    elif data_index is not None:\n                        label = \"Data {}\".format(data_index)\n                    else:\n                        label = \"Unknown\"\n                fill_color = display_layer.get(\"fill_color\")\n                stroke_color = display_layer.get(\"stroke_color\")\n                legend_entries.append(LegendEntry(label, fill_color, stroke_color))\n\n            self.__update_canvas_items(axes, legend_position, legend_entries)\n        else:\n            for line_graph_canvas_item in self.__line_graph_stack.canvas_items:\n                line_graph_canvas_item.set_axes(None)\n                line_graph_canvas_item.set_uncalibrated_xdata(None)\n            self.__line_graph_xdata_list = list()\n            self.__update_canvas_items(LineGraphCanvasItem.LineGraphAxes(), None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap the mouse to the 1 - d position within the line graph.", "response": "def __update_cursor_info(self):\n        \"\"\" Map the mouse to the 1-d position within the line graph. \"\"\"\n\n        if not self.delegate:  # allow display to work without delegate\n            return\n\n        if self.__mouse_in and self.__last_mouse:\n            pos_1d = None\n            axes = self.__axes\n            line_graph_canvas_item = self.line_graph_canvas_item\n            if axes and axes.is_valid and line_graph_canvas_item:\n                mouse = self.map_to_canvas_item(self.__last_mouse, line_graph_canvas_item)\n                plot_rect = line_graph_canvas_item.canvas_bounds\n                if plot_rect.contains_point(mouse):\n                    mouse = mouse - plot_rect.origin\n                    x = float(mouse.x) / plot_rect.width\n                    px = axes.drawn_left_channel + x * (axes.drawn_right_channel - axes.drawn_left_channel)\n                    pos_1d = px,\n            self.delegate.cursor_changed(pos_1d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_model_patch_tracks(self):\n        self.model_grid.load_data()\n        tracked_model_objects = []\n        model_objects = []\n        if self.model_grid.data is None:\n            print(\"No model output found\")\n            return tracked_model_objects\n        min_orig = self.model_ew.min_thresh\n        max_orig = self.model_ew.max_thresh\n        data_increment_orig = self.model_ew.data_increment\n        self.model_ew.min_thresh = 0\n        self.model_ew.data_increment = 1\n        self.model_ew.max_thresh = 100\n        for h, hour in enumerate(self.hours):\n            # Identify storms at each time step and apply size filter\n            print(\"Finding {0} objects for run {1} Hour: {2:02d}\".format(self.ensemble_member,\n                                                                         self.run_date.strftime(\"%Y%m%d%H\"), hour))\n            if self.mask is not None:\n                model_data = self.model_grid.data[h] * self.mask\n            else:\n                model_data = self.model_grid.data[h]\n            model_data[:self.patch_radius] = 0\n            model_data[-self.patch_radius:] = 0\n            model_data[:, :self.patch_radius] = 0\n            model_data[:, -self.patch_radius:] = 0\n            scaled_data = np.array(rescale_data(model_data, min_orig, max_orig))\n            hour_labels = label_storm_objects(scaled_data, \"ew\",\n                                              self.model_ew.min_thresh, self.model_ew.max_thresh,\n                                              min_area=self.size_filter, max_area=self.model_ew.max_size,\n                                              max_range=self.model_ew.delta, increment=self.model_ew.data_increment,\n                                              gaussian_sd=self.gaussian_window)\n            model_objects.extend(extract_storm_patches(hour_labels, model_data, self.model_grid.x,\n                                                       self.model_grid.y, [hour],\n                                                       dx=self.model_grid.dx,\n                                                       patch_radius=self.patch_radius))\n            for model_obj in model_objects[-1]:\n                dims = model_obj.timesteps[-1].shape\n                if h > 0:\n                    model_obj.estimate_motion(hour, self.model_grid.data[h-1], dims[1], dims[0])\n            del scaled_data\n            del model_data\n            del hour_labels\n        tracked_model_objects.extend(track_storms(model_objects, self.hours,\n                                                  self.object_matcher.cost_function_components,\n                                                  self.object_matcher.max_values,\n                                                  self.object_matcher.weights))\n        self.model_ew.min_thresh = min_orig\n        self.model_ew.max_thresh = max_orig\n        self.model_ew.data_increment = data_increment_orig\n        return tracked_model_objects", "response": "Finds all model patches in the gridded model output and extracts uniform sized patches around the storm centers of mass."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_model_tracks(self):\n        self.model_grid.load_data()\n        model_objects = []\n        tracked_model_objects = []\n        if self.model_grid.data is None:\n            print(\"No model output found\")\n            return tracked_model_objects\n        for h, hour in enumerate(self.hours):\n            # Identify storms at each time step and apply size filter\n            print(\"Finding {0} objects for run {1} Hour: {2:02d}\".format(self.ensemble_member,\n                                                                         self.run_date.strftime(\"%Y%m%d%H\"), hour))\n            if self.mask is not None:\n                model_data = self.model_grid.data[h] * self.mask\n            else:\n                model_data = self.model_grid.data[h]\n\n            # remember orig values\n            min_orig = self.model_ew.min_thresh\n            max_orig = self.model_ew.max_thresh\n            data_increment_orig = self.model_ew.data_increment\n            # scale to int 0-100.\n            scaled_data = np.array(rescale_data( self.model_grid.data[h], min_orig, max_orig))\n            self.model_ew.min_thresh = 0\n            self.model_ew.data_increment = 1\n            self.model_ew.max_thresh = 100\n            hour_labels = self.model_ew.label(gaussian_filter(scaled_data, self.gaussian_window))\n            hour_labels[model_data < self.model_ew.min_thresh] = 0\n            hour_labels = self.model_ew.size_filter(hour_labels, self.size_filter)\n            # Return to orig values\n            self.model_ew.min_thresh = min_orig\n            self.model_ew.max_thresh = max_orig\n            self.model_ew.data_increment = data_increment_orig\n            obj_slices = find_objects(hour_labels)\n\n            num_slices = len(obj_slices)\n            model_objects.append([])\n            if num_slices > 0:\n                for s, sl in enumerate(obj_slices):\n                    model_objects[-1].append(STObject(self.model_grid.data[h][sl],\n                                                      np.where(hour_labels[sl] == s + 1, 1, 0),\n                                                      self.model_grid.x[sl], \n                                                      self.model_grid.y[sl], \n                                                      self.model_grid.i[sl], \n                                                      self.model_grid.j[sl],\n                                                      hour,\n                                                      hour,\n                                                      dx=self.model_grid.dx))\n                    if h > 0:\n                        dims = model_objects[-1][-1].timesteps[0].shape\n                        model_objects[-1][-1].estimate_motion(hour, self.model_grid.data[h-1], dims[1], dims[0])\n            del hour_labels\n            del scaled_data\n            del model_data\n        for h, hour in enumerate(self.hours):\n            past_time_objs = []\n            for obj in tracked_model_objects:\n                # Potential trackable objects are identified\n                if obj.end_time == hour - 1:\n                    past_time_objs.append(obj)\n            # If no objects existed in the last time step, then consider objects in current time step all new\n            if len(past_time_objs) == 0:\n                tracked_model_objects.extend(model_objects[h])\n            # Match from previous time step with current time step\n            elif len(past_time_objs) > 0 and len(model_objects[h]) > 0:\n                assignments = self.object_matcher.match_objects(past_time_objs, model_objects[h], hour - 1, hour)\n                unpaired = list(range(len(model_objects[h])))\n                for pair in assignments:\n                    past_time_objs[pair[0]].extend(model_objects[h][pair[1]])\n                    unpaired.remove(pair[1])\n                if len(unpaired) > 0:\n                    for up in unpaired:\n                        tracked_model_objects.append(model_objects[h][up])\n            print(\"Tracked Model Objects: {0:03d} Hour: {1:02d}\".format(len(tracked_model_objects), hour))\n\n        return tracked_model_objects", "response": "Finds model tracks at each model time step and links them together with object matching."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding MESH tracks from MRMS timesteps and links them together with object matching.", "response": "def find_mrms_tracks(self):\n        \"\"\"\n        Identify objects from MRMS timesteps and link them together with object matching.\n\n        Returns:\n            List of STObjects containing MESH track information.\n        \"\"\"\n        obs_objects = []\n        tracked_obs_objects = []\n        if self.mrms_ew is not None:\n            self.mrms_grid.load_data()\n            \n            if len(self.mrms_grid.data) != len(self.hours):\n                print('Less than 24 hours of observation data found')\n                \n                return tracked_obs_objects\n         \n            for h, hour in enumerate(self.hours):\n                mrms_data = np.zeros(self.mrms_grid.data[h].shape)\n                mrms_data[:] = np.array(self.mrms_grid.data[h])\n                mrms_data[mrms_data < 0] = 0\n                hour_labels = self.mrms_ew.size_filter(self.mrms_ew.label(gaussian_filter(mrms_data,\n                                                                                      self.gaussian_window)),\n                                                       self.size_filter)\n                hour_labels[mrms_data < self.mrms_ew.min_thresh] = 0\n                obj_slices = find_objects(hour_labels)\n                num_slices = len(obj_slices)\n                obs_objects.append([])\n                if num_slices > 0:\n                    for sl in obj_slices:\n                        obs_objects[-1].append(STObject(mrms_data[sl],\n                                                        np.where(hour_labels[sl] > 0, 1, 0),\n                                                        self.model_grid.x[sl],\n                                                        self.model_grid.y[sl],\n                                                        self.model_grid.i[sl],\n                                                        self.model_grid.j[sl],\n                                                        hour,\n                                                        hour,\n                                                        dx=self.model_grid.dx))\n                        if h > 0:\n                            dims = obs_objects[-1][-1].timesteps[0].shape\n                            obs_objects[-1][-1].estimate_motion(hour, self.mrms_grid.data[h-1], dims[1], dims[0])\n        \n            for h, hour in enumerate(self.hours):\n                past_time_objs = []\n                for obj in tracked_obs_objects:\n                    if obj.end_time == hour - 1:\n                        past_time_objs.append(obj)\n                if len(past_time_objs) == 0:\n                    tracked_obs_objects.extend(obs_objects[h])\n                elif len(past_time_objs) > 0 and len(obs_objects[h]) > 0:\n                    assignments = self.object_matcher.match_objects(past_time_objs, obs_objects[h], hour - 1, hour)\n                    unpaired = list(range(len(obs_objects[h])))\n                    for pair in assignments:\n                        past_time_objs[pair[0]].extend(obs_objects[h][pair[1]])\n                        unpaired.remove(pair[1])\n                    if len(unpaired) > 0:\n                        for up in unpaired:\n                            tracked_obs_objects.append(obs_objects[h][up])\n                print(\"Tracked Obs Objects: {0:03d} Hour: {1:02d}\".format(len(tracked_obs_objects), hour))\n        \n        return tracked_obs_objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match_tracks(self, model_tracks, obs_tracks, unique_matches=True, closest_matches=False):\n        if unique_matches:\n            pairings = self.track_matcher.match_tracks(model_tracks, obs_tracks, closest_matches=closest_matches)\n        else:\n            pairings = self.track_matcher.neighbor_matches(model_tracks, obs_tracks)\n        return pairings", "response": "Match forecast and observed tracks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract model attribute data for each model track.", "response": "def extract_model_attributes(self, tracked_model_objects, storm_variables, potential_variables,\n                                 tendency_variables=None, future_variables=None):\n        \"\"\"\n        Extract model attribute data for each model track. Storm variables are those that describe the model storm\n        directly, such as radar reflectivity or updraft helicity. Potential variables describe the surrounding\n        environmental conditions of the storm, and should be extracted from the timestep before the storm arrives to\n        reduce the chance of the storm contaminating the environmental values. Examples of potential variables include\n        CAPE, shear, temperature, and dewpoint. Future variables are fields that occur in the hour after the extracted\n        field.\n\n        Args:\n            tracked_model_objects: List of STObjects describing each forecasted storm\n            storm_variables: List of storm variable names\n            potential_variables: List of potential variable names.\n            tendency_variables: List of tendency variables\n        \"\"\"\n        if tendency_variables is None:\n            tendency_variables = []\n        if future_variables is None:\n            future_variables = []\n        model_grids = {}\n        for l_var in [\"lon\", \"lat\"]:\n            for model_obj in tracked_model_objects:\n                model_obj.extract_attribute_array(getattr(self.model_grid, l_var), l_var)\n        for storm_var in storm_variables:\n            print(\"Storm {0} {1} {2}\".format(storm_var,self.ensemble_member, self.run_date.strftime(\"%Y%m%d\")))\n            model_grids[storm_var] = ModelOutput(self.ensemble_name, self.ensemble_member,\n                                                 self.run_date, storm_var, self.start_date - timedelta(hours=1),\n                                                 self.end_date + timedelta(hours=1),\n                                                 self.model_path,self.model_map_file, \n                                                 self.sector_ind_path,self.single_step)\n            model_grids[storm_var].load_data()\n            for model_obj in tracked_model_objects:\n                model_obj.extract_attribute_grid(model_grids[storm_var])\n            if storm_var not in potential_variables + tendency_variables + future_variables:\n                del model_grids[storm_var]\n        for potential_var in potential_variables:\n            print(\"Potential {0} {1} {2}\".format(potential_var,self.ensemble_member, self.run_date.strftime(\"%Y%m%d\")))\n            if potential_var not in model_grids.keys():\n                model_grids[potential_var] = ModelOutput(self.ensemble_name, self.ensemble_member,\n                                                         self.run_date, potential_var,\n                                                         self.start_date - timedelta(hours=1),\n                                                         self.end_date + timedelta(hours=1),\n                                                         self.model_path, self.model_map_file, \n                                                         self.sector_ind_path,self.single_step)\n                model_grids[potential_var].load_data()\n            for model_obj in tracked_model_objects:\n                model_obj.extract_attribute_grid(model_grids[potential_var], potential=True)\n            if potential_var not in tendency_variables + future_variables:\n                del model_grids[potential_var]\n        for future_var in future_variables:\n            print(\"Future {0} {1} {2}\".format(future_var, self.ensemble_member, self.run_date.strftime(\"%Y%m%d\")))\n            if future_var not in model_grids.keys():\n                model_grids[future_var] = ModelOutput(self.ensemble_name, self.ensemble_member,\n                                                         self.run_date, future_var,\n                                                         self.start_date - timedelta(hours=1),\n                                                         self.end_date + timedelta(hours=1),\n                                                         self.model_path, self.model_map_file,\n                                                         self.sector_ind_path,self.single_step)\n                model_grids[future_var].load_data()\n            for model_obj in tracked_model_objects:\n                model_obj.extract_attribute_grid(model_grids[future_var], future=True)\n            if future_var not in tendency_variables:\n                del model_grids[future_var]\n        for tendency_var in tendency_variables:\n            print(\"Tendency {0} {1} {2}\".format(tendency_var, self.ensemble_member, self.run_date.strftime(\"%Y%m%d\")))\n            if tendency_var not in model_grids.keys():\n                model_grids[tendency_var] = ModelOutput(self.ensemble_name, self.ensemble_member,\n                                                        self.run_date, tendency_var,\n                                                        self.start_date - timedelta(hours=1),\n                                                        self.end_date,\n                                                        self.model_path, self.model_map_file, \n                                                        self.sector_ind_path,self.single_step)\n            for model_obj in tracked_model_objects:\n                model_obj.extract_tendency_grid(model_grids[tendency_var])\n            del model_grids[tendency_var]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_hail_sizes(model_tracks, obs_tracks, track_pairings):\n        unpaired = list(range(len(model_tracks)))\n        for p, pair in enumerate(track_pairings):\n            model_track = model_tracks[pair[0]]\n            unpaired.remove(pair[0])\n            obs_track = obs_tracks[pair[1]]\n            obs_hail_sizes = np.array([step[obs_track.masks[t] == 1].max()\n                                       for t, step in enumerate(obs_track.timesteps)])\n            if obs_track.times.size > 1 and model_track.times.size > 1:\n                normalized_obs_times = 1.0 / (obs_track.times.max() - obs_track.times.min())\\\n                    * (obs_track.times - obs_track.times.min())\n                normalized_model_times = 1.0 / (model_track.times.max() - model_track.times.min())\\\n                    * (model_track.times - model_track.times.min())\n                hail_interp = interp1d(normalized_obs_times, obs_hail_sizes, kind=\"nearest\",\n                                       bounds_error=False, fill_value=0)\n                model_track.observations = hail_interp(normalized_model_times)\n            elif obs_track.times.size == 1:\n                model_track.observations = np.ones(model_track.times.shape) * obs_hail_sizes[0]\n            elif model_track.times.size == 1:\n                model_track.observations = np.array([obs_hail_sizes.max()])\n            print(pair[0], \"obs\",  obs_hail_sizes)\n            print(pair[0], \"model\", model_track.observations)\n        for u in unpaired:\n            model_tracks[u].observations = np.zeros(model_tracks[u].times.shape)", "response": "This function matches the maximum hail sizes of the model and observed tracks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_track_errors(model_tracks, obs_tracks, track_pairings):\n        columns = ['obs_track_id',\n                   'translation_error_x',\n                   'translation_error_y',\n                   'start_time_difference',\n                   'end_time_difference',\n                   ]\n        track_errors = pd.DataFrame(index=list(range(len(model_tracks))),\n                                    columns=columns)\n        for p, pair in enumerate(track_pairings):\n            model_track = model_tracks[pair[0]]\n            if type(pair[1]) in [int, np.int64]:\n                obs_track = obs_tracks[pair[1]]\n            else:\n                obs_track = obs_tracks[pair[1][0]]\n            model_com = model_track.center_of_mass(model_track.start_time)\n            obs_com = obs_track.center_of_mass(obs_track.start_time)\n            track_errors.loc[pair[0], 'obs_track_id'] = pair[1] if type(pair[1]) in [int, np.int64] else pair[1][0]\n            track_errors.loc[pair[0], 'translation_error_x'] = model_com[0] - obs_com[0]\n            track_errors.loc[pair[0], 'translation_error_y'] = model_com[1] - obs_com[1]\n            track_errors.loc[pair[0], 'start_time_difference'] = model_track.start_time - obs_track.start_time\n            track_errors.loc[pair[0], 'end_time_difference'] = model_track.end_time - obs_track.end_time \n        return track_errors", "response": "Calculates spatial and temporal translation errors between matched\n            forecast and observed tracks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides from PersistentObject. persistent_object_context_changed", "response": "def persistent_object_context_changed(self):\n        \"\"\" Override from PersistentObject. \"\"\"\n        super().persistent_object_context_changed()\n\n        def register():\n            if self.__source is not None and self.__target is not None:\n                assert not self.__binding\n                self.__binding = Binding.PropertyBinding(self.__source, self.source_property)\n                self.__binding.target_setter = self.__set_target_from_source\n                # while reading, the data item in the display data channel will not be connected;\n                # we still set its value here. when the data item becomes valid, it will update.\n                self.__binding.update_target_direct(self.__binding.get_target_value())\n\n        def source_registered(source):\n            self.__source = source\n            register()\n\n        def target_registered(target):\n            self.__target = target\n\n            def property_changed(target, property_name):\n                if property_name == self.target_property:\n                    self.__set_source_from_target(getattr(target, property_name))\n\n            assert self.__target_property_changed_listener is None\n            self.__target_property_changed_listener = target.property_changed_event.listen(functools.partial(property_changed, target))\n            register()\n\n        def unregistered(item=None):\n            if not item or item == self.__source:\n                self.__source = None\n            if not item or item == self.__target:\n                self.__target = None\n            if self.__binding:\n                self.__binding.close()\n                self.__binding = None\n            if self.__target_property_changed_listener:\n                self.__target_property_changed_listener.close()\n                self.__target_property_changed_listener = None\n\n        def change_registration(registered_object, unregistered_object):\n            if registered_object and registered_object.uuid == self.source_uuid:\n                source_registered(registered_object)\n            if registered_object and registered_object.uuid == self.target_uuid:\n                target_registered(registered_object)\n            if unregistered_object and unregistered_object in (self._source, self._target):\n                unregistered(unregistered_object)\n\n        if self.persistent_object_context:\n            self.__registration_listener = self.persistent_object_context.registration_event.listen(change_registration)\n            source = self.persistent_object_context.get_registered_object(self.source_uuid)\n            target = self.persistent_object_context.get_registered_object(self.target_uuid)\n            if source:\n                source_registered(source)\n            if target:\n                target_registered(target)\n        else:\n            unregistered()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling by the persistent object when the context of the current object has changed.", "response": "def persistent_object_context_changed(self):\n        \"\"\" Override from PersistentObject. \"\"\"\n        super().persistent_object_context_changed()\n\n        def detach():\n            for listener in self.__interval_mutated_listeners:\n                listener.close()\n            self.__interval_mutated_listeners = list()\n\n        def reattach():\n            detach()\n            interval_descriptors = list()\n            if self.__source:\n                for region in self.__source.graphics:\n                    if isinstance(region, Graphics.IntervalGraphic):\n                        interval_descriptor = {\"interval\": region.interval, \"color\": \"#F00\"}\n                        interval_descriptors.append(interval_descriptor)\n                        self.__interval_mutated_listeners.append(region.property_changed_event.listen(lambda k: reattach()))\n            if self.__target:\n                self.__target.interval_descriptors = interval_descriptors\n\n        def item_inserted(key, value, before_index):\n            if key == \"graphics\" and self.__target:\n                reattach()\n\n        def item_removed(key, value, index):\n            if key == \"graphics\" and self.__target:\n                reattach()\n\n        def source_registered(source):\n            self.__source = source\n            self.__item_inserted_event_listener = self.__source.item_inserted_event.listen(item_inserted)\n            self.__item_removed_event_listener = self.__source.item_removed_event.listen(item_removed)\n            reattach()\n\n        def target_registered(target):\n            self.__target = target\n            reattach()\n\n        def unregistered(source=None):\n            if self.__item_inserted_event_listener:\n                self.__item_inserted_event_listener.close()\n                self.__item_inserted_event_listener = None\n            if self.__item_removed_event_listener:\n                self.__item_removed_event_listener.close()\n                self.__item_removed_event_listener = None\n\n        if self.persistent_object_context:\n            self.persistent_object_context.subscribe(self.source_uuid, source_registered, unregistered)\n            self.persistent_object_context.subscribe(self.target_uuid, target_registered, unregistered)\n        else:\n            unregistered()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __display_for_tree_node(self, tree_node):\n        keys = tree_node.keys\n        if len(keys) == 1:\n            return \"{0} ({1})\".format(tree_node.keys[-1], tree_node.count)\n        elif len(keys) == 2:\n            months = (_(\"January\"), _(\"February\"), _(\"March\"), _(\"April\"), _(\"May\"), _(\"June\"), _(\"July\"), _(\"August\"),\n                      _(\"September\"), _(\"October\"), _(\"November\"), _(\"December\"))\n            return \"{0} ({1})\".format(months[max(min(tree_node.keys[1]-1, 11), 0)], tree_node.count)\n        else:\n            weekdays = (_(\"Monday\"), _(\"Tuesday\"), _(\"Wednesday\"), _(\"Thursday\"), _(\"Friday\"), _(\"Saturday\"), _(\"Sunday\"))\n            date = datetime.date(tree_node.keys[0], tree_node.keys[1], tree_node.keys[2])\n            return \"{0} - {1} ({2})\".format(tree_node.keys[2], weekdays[date.weekday()], tree_node.count)", "response": "Return the text display for the given tree node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert a new child node into the item model controller.", "response": "def __insert_child(self, parent_tree_node, index, tree_node):\n        \"\"\"\n            Called from the root tree node when a new node is inserted into tree. This method creates properties\n            to represent the node for display and inserts it into the item model controller.\n        \"\"\"\n        # manage the item model\n        parent_item = self.__mapping[id(parent_tree_node)]\n        self.item_model_controller.begin_insert(index, index, parent_item.row, parent_item.id)\n        properties = {\n            \"display\": self.__display_for_tree_node(tree_node),\n            \"tree_node\": tree_node  # used for removal and other lookup\n        }\n        item = self.item_model_controller.create_item(properties)\n        parent_item.insert_child(index, item)\n        self.__mapping[id(tree_node)] = item\n        self.item_model_controller.end_insert()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __remove_child(self, parent_tree_node, index):\n        # get parent and item\n        parent_item = self.__mapping[id(parent_tree_node)]\n        # manage the item model\n        self.item_model_controller.begin_remove(index, index, parent_item.row, parent_item.id)\n        child_item = parent_item.children[index]\n        parent_item.remove_child(child_item)\n        self.__mapping.pop(id(child_item.data[\"tree_node\"]))\n        self.item_model_controller.end_remove()", "response": "This method removes a child from the tree node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates all tree item displays if needed. Usually for count updates.", "response": "def update_all_nodes(self):\n        \"\"\" Update all tree item displays if needed. Usually for count updates. \"\"\"\n        item_model_controller = self.item_model_controller\n        if item_model_controller:\n            if self.__node_counts_dirty:\n                for item in self.__mapping.values():\n                    if \"tree_node\" in item.data:  # don't update the root node\n                        tree_node = item.data[\"tree_node\"]\n                        item.data[\"display\"] = self.__display_for_tree_node(tree_node)\n                        item_model_controller.data_changed(item.row, item.parent.row, item.parent.id)\n                self.__node_counts_dirty = False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by the browser when the selected items of the tree item is changed.", "response": "def date_browser_selection_changed(self, selected_indexes):\n        \"\"\"\n            Called to handle selection changes in the tree widget.\n\n            This method should be connected to the on_selection_changed event. This method builds a list\n            of keys represented by all selected items. It then provides date_filter to filter data items\n            based on the list of keys. It then sets the filter into the document controller.\n\n            :param selected_indexes: The selected indexes\n            :type selected_indexes: list of ints\n        \"\"\"\n        partial_date_filters = list()\n\n        for index, parent_row, parent_id in selected_indexes:\n            item_model_controller = self.item_model_controller\n            tree_node = item_model_controller.item_value(\"tree_node\", index, parent_id)\n            partial_date_filters.append(ListModel.PartialDateFilter(\"created_local\", *tree_node.keys))\n\n        if len(partial_date_filters) > 0:\n            self.__date_filter = ListModel.OrFilter(partial_date_filters)\n        else:\n            self.__date_filter = None\n\n        self.__update_filter()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text_filter_changed(self, text):\n        text = text.strip() if text else None\n\n        if text is not None:\n            self.__text_filter = ListModel.TextFilter(\"text_for_filter\", text)\n        else:\n            self.__text_filter = None\n\n        self.__update_filter()", "response": "Called to handle changes to the text filter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the display filter.", "response": "def __update_filter(self):\n        \"\"\"\n            Create a combined filter. Set the resulting filter into the document controller.\n        \"\"\"\n        filters = list()\n        if self.__date_filter:\n            filters.append(self.__date_filter)\n        if self.__text_filter:\n            filters.append(self.__text_filter)\n        self.document_controller.display_filter = ListModel.AndFilter(filters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the keys associated with this node.", "response": "def __get_keys(self):\n        \"\"\" Return the keys associated with this node by adding its key and then adding parent keys recursively. \"\"\"\n        keys = list()\n        tree_node = self\n        while tree_node is not None and tree_node.key is not None:\n            keys.insert(0, tree_node.key)\n            tree_node = tree_node.parent\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_value(self, keys, value):\n        self.count += 1\n        if not self.key:\n            self.__value_reverse_mapping[value] = keys\n        if len(keys) == 0:\n            self.values.append(value)\n        else:\n            key = keys[0]\n            index = bisect.bisect_left(self.children, TreeNode(key, reversed=self.reversed))\n            if index == len(self.children) or self.children[index].key != key:\n                new_tree_node = TreeNode(key, list(), reversed=self.reversed)\n                new_tree_node.child_inserted = self.child_inserted\n                new_tree_node.child_removed = self.child_removed\n                new_tree_node.tree_node_updated = self.tree_node_updated\n                new_tree_node.__set_parent(self)\n                self.children.insert(index, new_tree_node)\n                if self.child_inserted:\n                    self.child_inserted(self, index, new_tree_node)\n            child = self.children[index]\n            child.insert_value(keys[1:], value)\n            if self.tree_node_updated:\n                self.tree_node_updated(child)", "response": "Insert a value into this tree node and its children."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a value from this tree node and its children.", "response": "def remove_value(self, keys, value):\n        \"\"\"\n            Remove a value (data item) from this tree node and its children.\n            Also updates the tree node's cumulative child count.\n        \"\"\"\n        self.count -= 1\n        if not self.key:\n            keys = self.__value_reverse_mapping[value]\n            del self.__value_reverse_mapping[value]\n        if len(keys) == 0:\n            self.values.remove(value)\n        else:\n            key = keys[0]\n            index = bisect.bisect_left(self.children, TreeNode(key, reversed=self.reversed))\n            assert index != len(self.children) and self.children[index].key == key\n            self.children[index].remove_value(keys[1:], value)\n            if self.tree_node_updated:\n                self.tree_node_updated(self.children[index])\n            if self.children[index].count == 0:\n                del self.children[index]\n                if self.child_removed:\n                    self.child_removed(self, index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the output of extract_storm_objects this method tracks storms through time and merges them into a single set of tracks.", "response": "def track_storms(storm_objects, times, distance_components, distance_maxima, distance_weights, tracked_objects=None):\n    \"\"\"\n    Given the output of extract_storm_objects, this method tracks storms through time and merges individual\n    STObjects into a set of tracks.\n\n    Args:\n        storm_objects: list of list of STObjects that have not been tracked.\n        times: List of times associated with each set of STObjects\n        distance_components: list of function objects that make up components of distance function\n        distance_maxima: array of maximum values for each distance for normalization purposes\n        distance_weights: weight given to each component of the distance function. Should add to 1.\n        tracked_objects: List of STObjects that have already been tracked.\n    Returns:\n        tracked_objects:\n    \"\"\"\n    obj_matcher = ObjectMatcher(distance_components, distance_weights, distance_maxima)\n    if tracked_objects is None:\n        tracked_objects = []\n    for t, time in enumerate(times):\n        past_time_objects = []\n        for obj in tracked_objects:\n            if obj.end_time == time - obj.step:\n                past_time_objects.append(obj)\n        if len(past_time_objects) == 0:\n            tracked_objects.extend(storm_objects[t])\n        elif len(past_time_objects) > 0 and len(storm_objects[t]) > 0:\n            assignments = obj_matcher.match_objects(past_time_objects, storm_objects[t], times[t-1], times[t])\n            unpaired = list(range(len(storm_objects[t])))\n            for pair in assignments:\n                past_time_objects[pair[0]].extend(storm_objects[t][pair[1]])\n                unpaired.remove(pair[1])\n            if len(unpaired) > 0:\n                for up in unpaired:\n                    tracked_objects.append(storm_objects[t][up])\n    return tracked_objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef peirce_skill_score(self):\n        n = float(self.table.sum())\n        nf = self.table.sum(axis=1)\n        no = self.table.sum(axis=0)\n        correct = float(self.table.trace())\n        return (correct / n - (nf * no).sum() / n ** 2) / (1 - (no * no).sum() / n ** 2)", "response": "Multiclass Peirce Skill Score"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shifted_centroid_distance(item_a, time_a, item_b, time_b, max_value):\n    ax, ay = item_a.center_of_mass(time_a)\n    bx, by = item_b.center_of_mass(time_b)\n    if time_a < time_b:\n        bx = bx - item_b.u\n        by = by - item_b.v\n    else:\n        ax = ax - item_a.u\n        ay = ay - item_a.v\n    return np.minimum(np.sqrt((ax - bx) ** 2 + (ay - by) ** 2), max_value) / float(max_value)", "response": "Calculate the shifted centroid distance between two objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ellipse_distance(item_a, time_a, item_b, time_b, max_value):\n    ts = np.array([0, np.pi])\n    ell_a = item_a.get_ellipse_model(time_a)\n    ell_b = item_b.get_ellipse_model(time_b)\n    ends_a = ell_a.predict_xy(ts)\n    ends_b = ell_b.predict_xy(ts)\n    distances = np.sqrt((ends_a[:, 0:1] - ends_b[:, 0:1].T) ** 2 + (ends_a[:, 1:] - ends_b[:, 1:].T) ** 2)\n    return np.minimum(distances[0, 1], max_value) / float(max_value)", "response": "Calculates the distance between two objects in the object matcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the percentage of pixels in each object that do not overlap with the other object", "response": "def nonoverlap(item_a, time_a, item_b, time_b, max_value):\n    \"\"\"\n    Percentage of pixels in each object that do not overlap with the other object\n\n    Args:\n        item_a: STObject from the first set in ObjectMatcher\n        time_a: Time integer being evaluated\n        item_b: STObject from the second set in ObjectMatcher\n        time_b: Time integer being evaluated\n        max_value: Maximum distance value used as scaling value and upper constraint.\n\n    Returns:\n        Distance value between 0 and 1.\n    \"\"\"\n    return np.minimum(1 - item_a.count_overlap(time_a, item_b, time_b), max_value) / float(max_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max_intensity(item_a, time_a, item_b, time_b, max_value):\n    intensity_a = item_a.max_intensity(time_a)\n    intensity_b = item_b.max_intensity(time_b)\n    diff = np.sqrt((intensity_a - intensity_b) ** 2)\n    return np.minimum(diff, max_value) / float(max_value)", "response": "Function that calculates the RMS difference in maximum intensity between two objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mean_min_time_distance(item_a, item_b, max_value):\n    times_a = item_a.times.reshape((item_a.times.size, 1))\n    times_b = item_b.times.reshape((1, item_b.times.size))\n    distance_matrix = (times_a - times_b) ** 2\n    mean_min_distances = np.sqrt(distance_matrix.min(axis=0).mean() + distance_matrix.min(axis=1).mean())\n    return np.minimum(mean_min_distances, max_value) / float(max_value)", "response": "Calculate the mean time difference among the time steps in each object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the distance between the centroids of the first step in each object.", "response": "def start_centroid_distance(item_a, item_b, max_value):\n    \"\"\"\n    Distance between the centroids of the first step in each object.\n\n    Args:\n        item_a: STObject from the first set in TrackMatcher\n        item_b: STObject from the second set in TrackMatcher\n        max_value: Maximum distance value used as scaling value and upper constraint.\n\n    Returns:\n        Distance value between 0 and 1.\n    \"\"\"\n    start_a = item_a.center_of_mass(item_a.times[0])\n    start_b = item_b.center_of_mass(item_b.times[0])\n    start_distance = np.sqrt((start_a[0] - start_b[0]) ** 2 + (start_a[1] - start_b[1]) ** 2)\n    return np.minimum(start_distance, max_value) / float(max_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_time_distance(item_a, item_b, max_value):\n    start_time_diff = np.abs(item_a.times[0] - item_b.times[0])\n    return np.minimum(start_time_diff, max_value) / float(max_value)", "response": "Absolute difference between the starting times of each item."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef duration_distance(item_a, item_b, max_value):\n    duration_a = item_a.times.size\n    duration_b = item_b.times.size\n    return np.minimum(np.abs(duration_a - duration_b), max_value) / float(max_value)", "response": "Absolute difference in the duration of two items\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmatch two sets of STObjects at particular times.", "response": "def match_objects(self, set_a, set_b, time_a, time_b):\n        \"\"\"\n        Match two sets of objects at particular times.\n\n        Args:\n            set_a: list of STObjects\n            set_b: list of STObjects\n            time_a: time at which set_a is being evaluated for matching\n            time_b: time at which set_b is being evaluated for matching\n\n        Returns:\n            List of tuples containing (set_a index, set_b index) for each match\n        \"\"\"\n        costs = self.cost_matrix(set_a, set_b, time_a, time_b) * 100\n        min_row_costs = costs.min(axis=1)\n        min_col_costs = costs.min(axis=0)\n        good_rows = np.where(min_row_costs < 100)[0]\n        good_cols = np.where(min_col_costs < 100)[0]\n        assignments = []\n        if len(good_rows) > 0 and len(good_cols) > 0:\n            munk = Munkres()\n            initial_assignments = munk.compute(costs[tuple(np.meshgrid(good_rows, good_cols, indexing='ij'))].tolist())\n            initial_assignments = [(good_rows[x[0]], good_cols[x[1]]) for x in initial_assignments]\n            for a in initial_assignments:\n                if costs[a[0], a[1]] < 100:\n                    assignments.append(a)\n        return assignments"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cost_matrix(self, set_a, set_b, time_a, time_b):\n        costs = np.zeros((len(set_a), len(set_b)))\n        for a, item_a in enumerate(set_a):\n            for b, item_b in enumerate(set_b):\n                costs[a, b] = self.total_cost_function(item_a, item_b, time_a, time_b)\n        return costs", "response": "Calculates the cost matrix between the items in set a and set b at the specified times."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating total cost function between two items.", "response": "def total_cost_function(self, item_a, item_b, time_a, time_b):\n        \"\"\"\n        Calculate total cost function between two items.\n\n        Args:\n            item_a: STObject\n            item_b: STObject\n            time_a: Timestep in item_a at which cost function is evaluated\n            time_b: Timestep in item_b at which cost function is evaluated\n\n        Returns:\n            The total weighted distance between item_a and item_b\n        \"\"\"\n        distances = np.zeros(len(self.weights))\n        for c, component in enumerate(self.cost_function_components):\n            distances[c] = component(item_a, time_a, item_b, time_b, self.max_values[c])\n        total_distance = np.sum(self.weights * distances)\n        return total_distance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_tracks(self, set_a, set_b, closest_matches=False):\n        costs = self.track_cost_matrix(set_a, set_b) * 100\n        min_row_costs = costs.min(axis=1)\n        min_col_costs = costs.min(axis=0)\n        good_rows = np.where(min_row_costs < 100)[0]\n        good_cols = np.where(min_col_costs < 100)[0]\n        assignments = []\n        if len(good_rows) > 0 and len(good_cols) > 0:\n            if closest_matches:\n                b_matches = costs[np.meshgrid(good_rows, good_cols, indexing='ij')].argmin(axis=1)\n                a_matches = np.arange(b_matches.size)\n                initial_assignments = [(good_rows[a_matches[x]], good_cols[b_matches[x]])\n                                       for x in range(b_matches.size)]\n            else:\n                munk = Munkres()\n                initial_assignments = munk.compute(costs[np.meshgrid(good_rows, good_cols, indexing='ij')].tolist())\n                initial_assignments = [(good_rows[x[0]], good_cols[x[1]]) for x in initial_assignments]\n            for a in initial_assignments:\n                if costs[a[0], a[1]] < 100:\n                    assignments.append(a)\n        return assignments", "response": "This function finds the optimal set of matching assignments between set a and set b."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match(self, set_a, set_b):\n        track_step_matches = [[] * len(set_a)]\n\n        costs = self.cost_matrix(set_a, set_b)\n        valid_costs = np.all(costs < 1, axis=2)\n        set_a_matches, set_b_matches = np.where(valid_costs)\n        s = 0\n        track_pairings = pd.DataFrame(index=np.arange(costs.shape[0]),\n                                      columns=[\"Track\", \"Step\", \"Time\", \"Matched\", \"Pairings\"], dtype=object)\n        set_b_info = []\n        for trb, track_b in enumerate(set_b):\n            for t, time in enumerate(track_b.times):\n                set_b_info.append((trb, t))\n        set_b_info_arr = np.array(set_b_info, dtype=int)\n        for tr, track_a in enumerate(set_a):\n            for t, time in enumerate(track_a.times):\n                track_pairings.loc[s, [\"Track\", \"Step\", \"Time\"]] = [tr, t, time]\n                track_pairings.loc[s, \"Matched\"] = 1 if np.count_nonzero(set_a_matches == s) > 0 else 0\n                if track_pairings.loc[s, \"Matched\"] == 1:\n                    track_pairings.loc[s, \"Pairings\"] = set_b_info_arr[set_b_matches[set_a_matches == s]]\n                else:\n                    track_pairings.loc[s, \"Pairings\"] = np.array([])\n                s += 1\n        return track_pairings", "response": "Returns a pandas. DataFrame representing the number of steps in each track from set_a that meet all the cost function criteria\n        \n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the variable specifier for this variable.", "response": "def variable_specifier(self) -> dict:\n        \"\"\"Return the variable specifier for this variable.\n\n        The specifier can be used to lookup the value of this variable in a computation context.\n        \"\"\"\n        if self.value_type is not None:\n            return {\"type\": \"variable\", \"version\": 1, \"uuid\": str(self.uuid), \"x-name\": self.name, \"x-value\": self.value}\n        else:\n            return self.specifier"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bound_variable(self):\n        class BoundVariable:\n            def __init__(self, variable):\n                self.__variable = variable\n                self.changed_event = Event.Event()\n                self.needs_rebind_event = Event.Event()\n                def property_changed(key):\n                    if key == \"value\":\n                        self.changed_event.fire()\n                self.__variable_property_changed_listener = variable.property_changed_event.listen(property_changed)\n            @property\n            def value(self):\n                return self.__variable.value\n            def close(self):\n                self.__variable_property_changed_listener.close()\n                self.__variable_property_changed_listener = None\n\n        return BoundVariable(self)", "response": "Return an object with a value property and a changed_event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_object_specifier(self, object_specifier, secondary_specifier=None, property_name=None, objects_model=None):\n        variable = self.__computation().resolve_variable(object_specifier)\n        if not variable:\n            return self.__context.resolve_object_specifier(object_specifier, secondary_specifier, property_name, objects_model)\n        elif variable.specifier is None:\n            return variable.bound_variable\n        return None", "response": "Resolve the object specifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_names(cls, expression):\n        names = set()\n        try:\n            ast_node = ast.parse(expression, \"ast\")\n\n            class Visitor(ast.NodeVisitor):\n                def visit_Name(self, node):\n                    names.add(node.id)\n\n            Visitor().visit(ast_node)\n        except Exception:\n            pass\n        return names", "response": "Return the list of identifiers used in the expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbinds a context to this computation.", "response": "def bind(self, context) -> None:\n        \"\"\"Bind a context to this computation.\n\n        The context allows the computation to convert object specifiers to actual objects.\n        \"\"\"\n\n        # make a computation context based on the enclosing context.\n        self.__computation_context = ComputationContext(self, context)\n\n        # re-bind is not valid. be careful to set the computation after the data item is already in document.\n        for variable in self.variables:\n            assert variable.bound_item is None\n        for result in self.results:\n            assert result.bound_item is None\n\n        # bind the variables\n        for variable in self.variables:\n            self.__bind_variable(variable)\n\n        # bind the results\n        for result in self.results:\n            self.__bind_result(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enviar_dados_venda(self, dados_venda):\n        retorno = super(ClienteSATLocal, self).enviar_dados_venda(dados_venda)\n        return RespostaEnviarDadosVenda.analisar(retorno)", "response": "Sobreza o requisi\u00e7\u00e3o do base."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consultar_sat(self):\n        retorno = super(ClienteSATLocal, self).consultar_sat()\n        return RespostaSAT.consultar_sat(retorno)", "response": "Consultar a setar un objeto RespostaSAT."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef consultar_numero_sessao(self, numero_sessao):\n        retorno = super(ClienteSATLocal, self).\\\n                consultar_numero_sessao(numero_sessao)\n        return RespostaConsultarNumeroSessao.analisar(retorno)", "response": "Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.consultar_numero_sessao`.\n\n        :return: Uma resposta SAT que ir\u00e1 depender da sess\u00e3o consultada.\n        :rtype: satcfe.resposta.padrao.RespostaSAT"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef configurar_interface_de_rede(self, configuracao):\n        retorno = super(ClienteSATLocal, self).\\\n                configurar_interface_de_rede(configuracao)\n        return RespostaSAT.configurar_interface_de_rede(retorno)", "response": "Sobreza o configurar interface de rede."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef associar_assinatura(self, sequencia_cnpj, assinatura_ac):\n        retorno = super(ClienteSATLocal, self).\\\n                associar_assinatura(sequencia_cnpj, assinatura_ac)\n        # (!) resposta baseada na reda\u00e7\u00e3o com efeitos at\u00e9 31-12-2016\n        return RespostaSAT.associar_assinatura(retorno)", "response": "Sobreve uma resposta SAT padr\u00e3o."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef atualizar_software_sat(self):\n        retorno = super(ClienteSATLocal, self).atualizar_software_sat()\n        return RespostaSAT.atualizar_software_sat(retorno)", "response": "Sobreve uma resposta SAT para o atualizar avec o tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef desbloquear_sat(self):\n        retorno = super(ClienteSATLocal, self).desbloquear_sat()\n        return RespostaSAT.desbloquear_sat(retorno)", "response": "Desbloquear un objeto SAT."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresult no documento XML como string que pode ou n\u00e3o incluir a XML no in\u00edcio do documento.", "response": "def documento(self, *args, **kwargs):\n        \"\"\"Resulta no documento XML como string, que pode ou n\u00e3o incluir a\n        declara\u00e7\u00e3o XML no in\u00edcio do documento.\n        \"\"\"\n        forcar_unicode = kwargs.pop('forcar_unicode', False)\n        incluir_xml_decl = kwargs.pop('incluir_xml_decl', True)\n        doc = ET.tostring(self._xml(*args, **kwargs),\n                encoding='utf-8').decode('utf-8')\n        if forcar_unicode:\n            if incluir_xml_decl:\n                doc = u'{}\\n{}'.format(constantes.XML_DECL_UNICODE, doc)\n        else:\n            if incluir_xml_decl:\n                doc = '{}\\n{}'.format(constantes.XML_DECL, unidecode(doc))\n            else:\n                doc = unidecode(doc)\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self, skip_choose=False, fixed_workspace_dir=None):\n        logging.getLogger(\"migration\").setLevel(logging.INFO)\n        if fixed_workspace_dir:\n            workspace_dir = fixed_workspace_dir\n        else:\n            documents_dir = self.ui.get_document_location()\n            workspace_dir = os.path.join(documents_dir, \"Nion Swift Libraries\")\n            workspace_dir = self.ui.get_persistent_string(\"workspace_location\", workspace_dir)\n        welcome_message_enabled = fixed_workspace_dir is None\n        profile, is_created = Profile.create_profile(pathlib.Path(workspace_dir), welcome_message_enabled, skip_choose)\n        if not profile:\n            self.choose_library()\n            return True\n        self.workspace_dir = workspace_dir\n        DocumentModel.DocumentModel.computation_min_period = 0.1\n        document_model = DocumentModel.DocumentModel(profile=profile)\n        document_model.create_default_data_groups()\n        document_model.start_dispatcher()\n        # parse the hardware aliases file\n        alias_path = os.path.join(self.workspace_dir, \"aliases.ini\")\n        HardwareSource.parse_hardware_aliases_config_file(alias_path)\n        # create the document controller\n        document_controller = self.create_document_controller(document_model, \"library\")\n        if self.__resources_path is not None:\n            document_model.create_sample_images(self.__resources_path)\n        workspace_history = self.ui.get_persistent_object(\"workspace_history\", list())\n        if workspace_dir in workspace_history:\n            workspace_history.remove(workspace_dir)\n        workspace_history.insert(0, workspace_dir)\n        self.ui.set_persistent_object(\"workspace_history\", workspace_history)\n        self.ui.set_persistent_string(\"workspace_location\", workspace_dir)\n        if welcome_message_enabled:\n            logging.info(\"Welcome to Nion Swift.\")\n        if is_created and len(document_model.display_items) > 0:\n            document_controller.selected_display_panel.set_display_panel_display_item(document_model.display_items[0])\n            document_controller.selected_display_panel.perform_action(\"set_fill_mode\")\n        return True", "response": "Start the application.\n\n            Looks for workspace_location persistent string. If it doesn't find it, uses a default\n            workspace location.\n\n            Then checks to see if that workspace exists. If not and if skip_choose has not been\n            set to True, asks the user for a workspace location. User may choose new folder or\n            existing location. This works by putting up the dialog which will either call start\n            again or exit.\n\n            Creates workspace in location if it doesn't exist.\n\n            Migrates database to latest version.\n\n            Creates document model, resources path, etc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of this library item with any dynamic behavior made static.", "response": "def snapshot(self):\n        \"\"\"Return a new library item which is a copy of this one with any dynamic behavior made static.\"\"\"\n        data_item = self.__class__()\n        # data format (temporary until moved to buffered data source)\n        data_item.large_format = self.large_format\n        data_item.set_data_and_metadata(copy.deepcopy(self.data_and_metadata), self.data_modified)\n        # metadata\n        data_item.created = self.created\n        data_item.timezone = self.timezone\n        data_item.timezone_offset = self.timezone_offset\n        data_item.metadata = self.metadata\n        data_item.title = self.title\n        data_item.caption = self.caption\n        data_item.description = self.description\n        data_item.session_id = self.session_id\n        data_item.session_data = copy.deepcopy(self.session_data)\n        return data_item"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_r_value(self, r_var: str, *, notify_changed=True) -> None:\n        self.r_var = r_var\n        self._description_changed()\n        if notify_changed:  # set to False to set the r-value at startup; avoid marking it as a change\n            self.__notify_description_changed()", "response": "Used to signal changes to the ref var which are kept in document controller. ugh."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the underlying data and metadata to the data_and_metadata.", "response": "def set_data_and_metadata(self, data_and_metadata, data_modified=None):\n        \"\"\"Sets the underlying data and data-metadata to the data_and_metadata.\n\n        Note: this does not make a copy of the data.\n        \"\"\"\n        self.increment_data_ref_count()\n        try:\n            if data_and_metadata:\n                data = data_and_metadata.data\n                data_shape_and_dtype = data_and_metadata.data_shape_and_dtype\n                intensity_calibration = data_and_metadata.intensity_calibration\n                dimensional_calibrations = data_and_metadata.dimensional_calibrations\n                metadata = data_and_metadata.metadata\n                timestamp = data_and_metadata.timestamp\n                data_descriptor = data_and_metadata.data_descriptor\n                timezone = data_and_metadata.timezone or Utility.get_local_timezone()\n                timezone_offset = data_and_metadata.timezone_offset or Utility.TimezoneMinutesToStringConverter().convert(Utility.local_utcoffset_minutes())\n                new_data_and_metadata = DataAndMetadata.DataAndMetadata(self.__load_data, data_shape_and_dtype, intensity_calibration, dimensional_calibrations, metadata, timestamp, data, data_descriptor, timezone, timezone_offset)\n            else:\n                new_data_and_metadata = None\n            self.__set_data_metadata_direct(new_data_and_metadata, data_modified)\n            if self.__data_and_metadata is not None:\n                if self.persistent_object_context and not self.persistent_object_context.is_write_delayed(self):\n                    self.persistent_object_context.write_external_data(self, \"data\", self.__data_and_metadata.data)\n                    self.__data_and_metadata.unloadable = True\n        finally:\n            self.decrement_data_ref_count()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the color map data as a uint8 ndarray with shape 256 and 3.", "response": "def color_map_data(self) -> typing.Optional[numpy.ndarray]:\n        \"\"\"Return the color map data as a uint8 ndarray with shape (256, 3).\"\"\"\n        if self.display_data_shape is None:  # is there display data?\n            return None\n        else:\n            return self.__color_map_data if self.__color_map_data is not None else ColorMaps.get_color_map_data_by_id(\"grayscale\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the display values calculated for this item.", "response": "def get_calculated_display_values(self, immediate: bool=False) -> DisplayValues:\n        \"\"\"Return the display values.\n\n        Return the current (possibly uncalculated) display values unless 'immediate' is specified.\n\n        If 'immediate', return the existing (calculated) values if they exist. Using the 'immediate' values\n        avoids calculation except in cases where the display values haven't already been calculated.\n        \"\"\"\n        if not immediate or not self.__is_master or not self.__last_display_values:\n            if not self.__current_display_values and self.__data_item:\n                self.__current_display_values = DisplayValues(self.__data_item.xdata, self.sequence_index, self.collection_index, self.slice_center, self.slice_width, self.display_limits, self.complex_display_type, self.__color_map_data)\n\n                def finalize(display_values):\n                    self.__last_display_values = display_values\n                    self.display_values_changed_event.fire()\n\n                self.__current_display_values.on_finalize = finalize\n            return self.__current_display_values\n        return self.__last_display_values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef increment_display_ref_count(self, amount: int=1):\n        display_ref_count = self.__display_ref_count\n        self.__display_ref_count += amount\n        if display_ref_count == 0:\n            self.__is_master = True\n        if self.__data_item:\n            for _ in range(amount):\n                self.__data_item.increment_data_ref_count()", "response": "Increment the display reference count to indicate this library item is currently displayed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decrement_display_ref_count(self, amount: int=1):\n        assert not self._closed\n        self.__display_ref_count -= amount\n        if self.__display_ref_count == 0:\n            self.__is_master = False\n        if self.__data_item:\n            for _ in range(amount):\n                self.__data_item.decrement_data_ref_count()", "response": "Decrement the display reference count to indicate this library item is no longer displayed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef auto_display_limits(self):\n        display_data_and_metadata = self.get_calculated_display_values(True).display_data_and_metadata\n        data = display_data_and_metadata.data if display_data_and_metadata else None\n        if data is not None:\n            # The old algorithm was a problem during EELS where the signal data\n            # is a small percentage of the overall data and was falling outside\n            # the included range. This is the new simplified algorithm. Future\n            # feature may allow user to select more complex algorithms.\n            mn, mx = numpy.nanmin(data), numpy.nanmax(data)\n            self.display_limits = mn, mx", "response": "Calculate best display limits and set them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef snapshot(self):\n        display_item = self.__class__()\n        display_item.display_type = self.display_type\n        # metadata\n        display_item._set_persistent_property_value(\"title\", self._get_persistent_property_value(\"title\"))\n        display_item._set_persistent_property_value(\"caption\", self._get_persistent_property_value(\"caption\"))\n        display_item._set_persistent_property_value(\"description\", self._get_persistent_property_value(\"description\"))\n        display_item._set_persistent_property_value(\"session_id\", self._get_persistent_property_value(\"session_id\"))\n        display_item._set_persistent_property_value(\"calibration_style_id\", self._get_persistent_property_value(\"calibration_style_id\"))\n        display_item._set_persistent_property_value(\"display_properties\", self._get_persistent_property_value(\"display_properties\"))\n        display_item.created = self.created\n        for graphic in self.graphics:\n            display_item.add_graphic(copy.deepcopy(graphic))\n        for display_data_channel in self.display_data_channels:\n            display_item.append_display_data_channel(copy.deepcopy(display_data_channel))\n        # this goes after the display data channels so that the layers don't get adjusted\n        display_item._set_persistent_property_value(\"display_layers\", self._get_persistent_property_value(\"display_layers\"))\n        return display_item", "response": "Return a copy of this library item with any dynamic behavior made static."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincrements the display reference count to indicate this library item is currently displayed.", "response": "def increment_display_ref_count(self, amount: int=1):\n        \"\"\"Increment display reference count to indicate this library item is currently displayed.\"\"\"\n        display_ref_count = self.__display_ref_count\n        self.__display_ref_count += amount\n        for display_data_channel in self.display_data_channels:\n            display_data_channel.increment_display_ref_count(amount)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrement_display_ref_count(self, amount: int=1):\n        assert not self._closed\n        self.__display_ref_count -= amount\n        for display_data_channel in self.display_data_channels:\n            display_data_channel.decrement_display_ref_count(amount)", "response": "Decrement the display reference count to indicate this library item is no longer displayed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a graphic from the container.", "response": "def remove_graphic(self, graphic: Graphics.Graphic, *, safe: bool=False) -> typing.Optional[typing.Sequence]:\n        \"\"\"Remove a graphic, but do it through the container, so dependencies can be tracked.\"\"\"\n        return self.remove_model_item(self, \"graphics\", graphic, safe=safe)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshape of the underlying data and metadata.", "response": "def dimensional_shape(self) -> typing.Optional[typing.Tuple[int, ...]]:\n        \"\"\"Shape of the underlying data, if only one.\"\"\"\n        if not self.__data_and_metadata:\n            return None\n        return self.__data_and_metadata.dimensional_shape"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the view to encompass the channels and data represented by the given intervals.", "response": "def view_to_intervals(self, data_and_metadata: DataAndMetadata.DataAndMetadata, intervals: typing.List[typing.Tuple[float, float]]) -> None:\n        \"\"\"Change the view to encompass the channels and data represented by the given intervals.\"\"\"\n        left = None\n        right = None\n        for interval in intervals:\n            left = min(left, interval[0]) if left is not None else interval[0]\n            right = max(right, interval[1]) if right is not None else interval[1]\n        left = left if left is not None else 0.0\n        right = right if right is not None else 1.0\n        extra = (right - left) * 0.5\n        left_channel = int(max(0.0, left - extra) * data_and_metadata.data_shape[-1])\n        right_channel = int(min(1.0, right + extra) * data_and_metadata.data_shape[-1])\n        self.set_display_property(\"left_channel\", left_channel)\n        self.set_display_property(\"right_channel\", right_channel)\n        data_min = numpy.amin(data_and_metadata.data[..., left_channel:right_channel])\n        data_max = numpy.amax(data_and_metadata.data[..., left_channel:right_channel])\n        if data_min > 0 and data_max > 0:\n            self.set_display_property(\"y_min\", 0.0)\n            self.set_display_property(\"y_max\", data_max * 1.2)\n        elif data_min < 0 and data_max < 0:\n            self.set_display_property(\"y_min\", data_min * 1.2)\n            self.set_display_property(\"y_max\", 0.0)\n        else:\n            self.set_display_property(\"y_min\", data_min * 1.2)\n            self.set_display_property(\"y_max\", data_max * 1.2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_local_file(fp, name_bytes, writer, dt):\n    fp.write(struct.pack('I', 0x04034b50))  # local file header\n    fp.write(struct.pack('H', 10))          # extract version (default)\n    fp.write(struct.pack('H', 0))           # general purpose bits\n    fp.write(struct.pack('H', 0))           # compression method\n    msdos_date = int(dt.year - 1980) << 9 | int(dt.month) << 5 | int(dt.day)\n    msdos_time = int(dt.hour) << 11 | int(dt.minute) << 5 | int(dt.second)\n    fp.write(struct.pack('H', msdos_time))  # extract version (default)\n    fp.write(struct.pack('H', msdos_date))  # extract version (default)\n    crc32_pos = fp.tell()\n    fp.write(struct.pack('I', 0))           # crc32 placeholder\n    data_len_pos = fp.tell()\n    fp.write(struct.pack('I', 0))           # compressed length placeholder\n    fp.write(struct.pack('I', 0))           # uncompressed length placeholder\n    fp.write(struct.pack('H', len(name_bytes)))   # name length\n    fp.write(struct.pack('H', 0))           # extra length\n    fp.write(name_bytes)\n    data_start_pos = fp.tell()\n    crc32 = writer(fp)\n    data_end_pos = fp.tell()\n    data_len = data_end_pos - data_start_pos\n    fp.seek(crc32_pos)\n    fp.write(struct.pack('I', crc32))       # crc32\n    fp.seek(data_len_pos)\n    fp.write(struct.pack('I', data_len))    # compressed length placeholder\n    fp.write(struct.pack('I', data_len))    # uncompressed length placeholder\n    fp.seek(data_end_pos)\n    return data_len, crc32", "response": "Writes a zip file containing the local file header structure at the current position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_directory_data(fp, offset, name_bytes, data_len, crc32, dt):\n    fp.write(struct.pack('I', 0x02014b50))  # central directory header\n    fp.write(struct.pack('H', 10))          # made by version (default)\n    fp.write(struct.pack('H', 10))          # extract version (default)\n    fp.write(struct.pack('H', 0))           # general purpose bits\n    fp.write(struct.pack('H', 0))           # compression method\n    msdos_date = int(dt.year - 1980) << 9 | int(dt.month) << 5 | int(dt.day)\n    msdos_time = int(dt.hour) << 11 | int(dt.minute) << 5 | int(dt.second)\n    fp.write(struct.pack('H', msdos_time))  # extract version (default)\n    fp.write(struct.pack('H', msdos_date))  # extract version (default)\n    fp.write(struct.pack('I', crc32))       # crc32\n    fp.write(struct.pack('I', data_len))    # compressed length\n    fp.write(struct.pack('I', data_len))    # uncompressed length\n    fp.write(struct.pack('H', len(name_bytes)))   # name length\n    fp.write(struct.pack('H', 0))           # extra length\n    fp.write(struct.pack('H', 0))           # comments length\n    fp.write(struct.pack('H', 0))           # disk number\n    fp.write(struct.pack('H', 0))           # internal file attributes\n    fp.write(struct.pack('I', 0))           # external file attributes\n    fp.write(struct.pack('I', offset))      # relative offset of file header\n    fp.write(name_bytes)", "response": "Writes the data to the archive as a zip fie directory entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the end of the directory header at the current file position.", "response": "def write_end_of_directory(fp, dir_size, dir_offset, count):\n    \"\"\"\n        Write zip file end of directory header at the current file position\n\n        :param fp: the file point to which to write the header\n        :param dir_size: the total size of the directory\n        :param dir_offset: the start of the first directory header\n        :param count: the count of files\n    \"\"\"\n    fp.write(struct.pack('I', 0x06054b50))  # central directory header\n    fp.write(struct.pack('H', 0))           # disk number\n    fp.write(struct.pack('H', 0))           # disk number\n    fp.write(struct.pack('H', count))       # number of files\n    fp.write(struct.pack('H', count))       # number of files\n    fp.write(struct.pack('I', dir_size))    # central directory size\n    fp.write(struct.pack('I', dir_offset))  # central directory offset\n    fp.write(struct.pack('H', 0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_zip_fp(fp, data, properties, dir_data_list=None):\n    assert data is not None or properties is not None\n    # dir_data_list has the format: local file record offset, name, data length, crc32\n    dir_data_list = list() if dir_data_list is None else dir_data_list\n    dt = datetime.datetime.now()\n    if data is not None:\n        offset_data = fp.tell()\n        def write_data(fp):\n            numpy_start_pos = fp.tell()\n            numpy.save(fp, data)\n            numpy_end_pos = fp.tell()\n            fp.seek(numpy_start_pos)\n            data_c = numpy.require(data, dtype=data.dtype, requirements=[\"C_CONTIGUOUS\"])\n            header_data = fp.read((numpy_end_pos - numpy_start_pos) - data_c.nbytes)  # read the header\n            data_crc32 = binascii.crc32(data_c.data, binascii.crc32(header_data)) & 0xFFFFFFFF\n            fp.seek(numpy_end_pos)\n            return data_crc32\n        data_len, crc32 = write_local_file(fp, b\"data.npy\", write_data, dt)\n        dir_data_list.append((offset_data, b\"data.npy\", data_len, crc32))\n    if properties is not None:\n        json_str = str()\n        try:\n            class JSONEncoder(json.JSONEncoder):\n                def default(self, obj):\n                    if isinstance(obj, Geometry.IntPoint) or isinstance(obj, Geometry.IntSize) or isinstance(obj, Geometry.IntRect) or isinstance(obj, Geometry.FloatPoint) or isinstance(obj, Geometry.FloatSize) or isinstance(obj, Geometry.FloatRect):\n                        return tuple(obj)\n                    else:\n                        return json.JSONEncoder.default(self, obj)\n            json_io = io.StringIO()\n            json.dump(properties, json_io, cls=JSONEncoder)\n            json_str = json_io.getvalue()\n        except Exception as e:\n            # catch exceptions to avoid corrupt zip files\n            import traceback\n            logging.error(\"Exception writing zip file %s\" + str(e))\n            traceback.print_exc()\n            traceback.print_stack()\n        def write_json(fp):\n            json_bytes = bytes(json_str, 'ISO-8859-1')\n            fp.write(json_bytes)\n            return binascii.crc32(json_bytes) & 0xFFFFFFFF\n        offset_json = fp.tell()\n        json_len, json_crc32 = write_local_file(fp, b\"metadata.json\", write_json, dt)\n        dir_data_list.append((offset_json, b\"metadata.json\", json_len, json_crc32))\n    dir_offset = fp.tell()\n    for offset, name_bytes, data_len, crc32 in dir_data_list:\n        write_directory_data(fp, offset, name_bytes, data_len, crc32, dt)\n    dir_size = fp.tell() - dir_offset\n    write_end_of_directory(fp, dir_size, dir_offset, len(dir_data_list))\n    fp.truncate()", "response": "Writes a custom zip file of data and properties to the file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_zip(file_path, data, properties):\n    with open(file_path, \"w+b\") as fp:\n        write_zip_fp(fp, data, properties)", "response": "Writes a custom zip file to the file path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the zip file at fp and return a tuple of the local files directory headers and end of central directory entries.", "response": "def parse_zip(fp):\n    \"\"\"\n        Parse the zip file headers at fp\n\n        :param fp: the file pointer from which to parse the zip file\n        :return: A tuple of local files, directory headers, and end of central directory\n\n        The local files are dictionary where the keys are the local file offset and the\n        values are each a tuple consisting of the name, data position, data length, and crc32.\n\n        The directory headers are a dictionary where the keys are the names of the files\n        and the values are a tuple consisting of the directory header position, and the\n        associated local file position.\n\n        The end of central directory is a tuple consisting of the location of the end of\n        central directory header and the location of the first directory header.\n\n        This method will seek to location 0 of fp and leave fp at end of file.\n    \"\"\"\n    local_files = {}\n    dir_files = {}\n    eocd = None\n    fp.seek(0)\n    while True:\n        pos = fp.tell()\n        signature = struct.unpack('I', fp.read(4))[0]\n        if signature == 0x04034b50:\n            fp.seek(pos + 14)\n            crc32 = struct.unpack('I', fp.read(4))[0]\n            fp.seek(pos + 18)\n            data_len = struct.unpack('I', fp.read(4))[0]\n            fp.seek(pos + 26)\n            name_len = struct.unpack('H', fp.read(2))[0]\n            extra_len = struct.unpack('H', fp.read(2))[0]\n            name_bytes = fp.read(name_len)\n            fp.seek(extra_len, os.SEEK_CUR)\n            data_pos = fp.tell()\n            fp.seek(data_len, os.SEEK_CUR)\n            local_files[pos] = (name_bytes, data_pos, data_len, crc32)\n        elif signature == 0x02014b50:\n            fp.seek(pos + 28)\n            name_len = struct.unpack('H', fp.read(2))[0]\n            extra_len = struct.unpack('H', fp.read(2))[0]\n            comment_len = struct.unpack('H', fp.read(2))[0]\n            fp.seek(pos + 42)\n            pos2 = struct.unpack('I', fp.read(4))[0]\n            name_bytes = fp.read(name_len)\n            fp.seek(pos + 46 + name_len + extra_len + comment_len)\n            dir_files[name_bytes] = (pos, pos2)\n        elif signature == 0x06054b50:\n            fp.seek(pos + 16)\n            pos2 = struct.unpack('I', fp.read(4))[0]\n            eocd = (pos, pos2)\n            break\n        else:\n            raise IOError()\n    return local_files, dir_files, eocd"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_data(fp, local_files, dir_files, name_bytes):\n    if name_bytes in dir_files:\n        fp.seek(local_files[dir_files[name_bytes][1]][1])\n        return numpy.load(fp)\n    return None", "response": "Read a numpy array from the zip file containing the data file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_json(fp, local_files, dir_files, name_bytes):\n    if name_bytes in dir_files:\n        json_pos = local_files[dir_files[name_bytes][1]][1]\n        json_len = local_files[dir_files[name_bytes][1]][2]\n        fp.seek(json_pos)\n        json_properties = fp.read(json_len)\n        return json.loads(json_properties.decode(\"utf-8\"))\n    return None", "response": "Read json properties from the zip file containing the name of the entry in the local_files directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrewrite the json properties in the zip file :param file_path: the file path to the zip file :param properties: the updated properties to write to the zip file This method will attempt to keep the data file within the zip file intact without rewriting it. However, if the data file is not the first item in the zip file, this method will rewrite it. The properties param must not change during this method. Callers should take care to ensure this does not happen.", "response": "def rewrite_zip(file_path, properties):\n    \"\"\"\n        Rewrite the json properties in the zip file\n\n        :param file_path: the file path to the zip file\n        :param properties: the updated properties to write to the zip file\n\n        This method will attempt to keep the data file within the zip\n        file intact without rewriting it. However, if the data file is not the\n        first item in the zip file, this method will rewrite it.\n\n        The properties param must not change during this method. Callers should\n        take care to ensure this does not happen.\n    \"\"\"\n    with open(file_path, \"r+b\") as fp:\n        local_files, dir_files, eocd = parse_zip(fp)\n        # check to make sure directory has two files, named data.npy and metadata.json, and that data.npy is first\n        # TODO: check compression, etc.\n        if len(dir_files) == 2 and b\"data.npy\" in dir_files and b\"metadata.json\" in dir_files and dir_files[b\"data.npy\"][1] == 0:\n            fp.seek(dir_files[b\"metadata.json\"][1])\n            dir_data_list = list()\n            local_file_pos = dir_files[b\"data.npy\"][1]\n            local_file = local_files[local_file_pos]\n            dir_data_list.append((local_file_pos, b\"data.npy\", local_file[2], local_file[3]))\n            write_zip_fp(fp, None, properties, dir_data_list)\n        else:\n            data = None\n            if b\"data.npy\" in dir_files:\n                fp.seek(local_files[dir_files[b\"data.npy\"][1]][1])\n                data = numpy.load(fp)\n            fp.seek(0)\n            write_zip_fp(fp, data, properties)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_matching(cls, file_path):\n        if file_path.endswith(\".ndata\") and os.path.exists(file_path):\n            try:\n                with open(file_path, \"r+b\") as fp:\n                    local_files, dir_files, eocd = parse_zip(fp)\n                    contains_data = b\"data.npy\" in dir_files\n                    contains_metadata = b\"metadata.json\" in dir_files\n                    file_count = contains_data + contains_metadata  # use fact that True is 1, False is 0\n                    # TODO: make sure ndata isn't compressed, or handle it\n                    if len(dir_files) != file_count or file_count == 0:\n                        return False\n                    return True\n            except Exception as e:\n                logging.error(\"Exception parsing ndata file: %s\", file_path)\n                logging.error(str(e))\n        return False", "response": "Return whether the given absolute file path is an ndata file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites data to the ndata file specified by reference.", "response": "def write_data(self, data, file_datetime):\n        \"\"\"\n            Write data to the ndata file specified by reference.\n\n            :param data: the numpy array data to write\n            :param file_datetime: the datetime for the file\n        \"\"\"\n        with self.__lock:\n            assert data is not None\n            absolute_file_path = self.__file_path\n            #logging.debug(\"WRITE data file %s for %s\", absolute_file_path, key)\n            make_directory_if_needed(os.path.dirname(absolute_file_path))\n            properties = self.read_properties() if os.path.exists(absolute_file_path) else dict()\n            write_zip(absolute_file_path, data, properties)\n            # convert to utc time.\n            tz_minutes = Utility.local_utcoffset_minutes(file_datetime)\n            timestamp = calendar.timegm(file_datetime.timetuple()) - tz_minutes * 60\n            os.utime(absolute_file_path, (time.time(), timestamp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_properties(self, properties, file_datetime):\n        with self.__lock:\n            absolute_file_path = self.__file_path\n            #logging.debug(\"WRITE properties %s for %s\", absolute_file_path, key)\n            make_directory_if_needed(os.path.dirname(absolute_file_path))\n            exists = os.path.exists(absolute_file_path)\n            if exists:\n                rewrite_zip(absolute_file_path, Utility.clean_dict(properties))\n            else:\n                write_zip(absolute_file_path, None, Utility.clean_dict(properties))\n            # convert to utc time.\n            tz_minutes = Utility.local_utcoffset_minutes(file_datetime)\n            timestamp = calendar.timegm(file_datetime.timetuple()) - tz_minutes * 60\n            os.utime(absolute_file_path, (time.time(), timestamp))", "response": "Writes the properties to the ndata file specified by reference."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread properties from the ndata file", "response": "def read_properties(self):\n        \"\"\"\n            Read properties from the ndata file reference\n\n            :param reference: the reference from which to read\n            :return: a tuple of the item_uuid and a dict of the properties\n        \"\"\"\n        with self.__lock:\n            absolute_file_path = self.__file_path\n            with open(absolute_file_path, \"rb\") as fp:\n                local_files, dir_files, eocd = parse_zip(fp)\n                properties = read_json(fp, local_files, dir_files, b\"metadata.json\")\n            return properties"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads data from the ndata file and return numpy array of the data.", "response": "def read_data(self):\n        \"\"\"\n            Read data from the ndata file reference\n\n            :param reference: the reference from which to read\n            :return: a numpy array of the data; maybe None\n        \"\"\"\n        with self.__lock:\n            absolute_file_path = self.__file_path\n            #logging.debug(\"READ data file %s\", absolute_file_path)\n            with open(absolute_file_path, \"rb\") as fp:\n                local_files, dir_files, eocd = parse_zip(fp)\n                return read_data(fp, local_files, dir_files, b\"data.npy\")\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the ndata file associated with the current ndata class.", "response": "def remove(self):\n        \"\"\"\n            Remove the ndata file reference\n\n            :param reference: the reference to remove\n        \"\"\"\n        with self.__lock:\n            absolute_file_path = self.__file_path\n            #logging.debug(\"DELETE data file %s\", absolute_file_path)\n            if os.path.isfile(absolute_file_path):\n                os.remove(absolute_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the dynamic menu for the selected display panel.", "response": "def build_menu(self, display_type_menu, document_controller, display_panel):\n        \"\"\"Build the dynamic menu for the selected display panel.\n\n        The user accesses this menu by right-clicking on the display panel.\n\n        The basic menu items are to an empty display panel or a browser display panel.\n\n        After that, each display controller factory is given a chance to add to the menu. The display\n        controllers (for instance, a scan acquisition controller), may add its own menu items.\n        \"\"\"\n        dynamic_live_actions = list()\n\n        def switch_to_display_content(display_panel_type):\n            self.switch_to_display_content(document_controller, display_panel, display_panel_type, display_panel.display_item)\n\n        empty_action = display_type_menu.add_menu_item(_(\"Clear Display Panel\"), functools.partial(switch_to_display_content, \"empty-display-panel\"))\n        display_type_menu.add_separator()\n\n        data_item_display_action = display_type_menu.add_menu_item(_(\"Display Item\"), functools.partial(switch_to_display_content, \"data-display-panel\"))\n        thumbnail_browser_action = display_type_menu.add_menu_item(_(\"Thumbnail Browser\"), functools.partial(switch_to_display_content, \"thumbnail-browser-display-panel\"))\n        grid_browser_action = display_type_menu.add_menu_item(_(\"Grid Browser\"), functools.partial(switch_to_display_content, \"browser-display-panel\"))\n        display_type_menu.add_separator()\n\n        display_panel_type = display_panel.display_panel_type\n\n        empty_action.checked = display_panel_type == \"empty\" and display_panel.display_panel_controller is None\n        data_item_display_action.checked = display_panel_type == \"data_item\"\n        thumbnail_browser_action.checked = display_panel_type == \"horizontal\"\n        grid_browser_action.checked = display_panel_type == \"grid\"\n\n        dynamic_live_actions.append(empty_action)\n        dynamic_live_actions.append(data_item_display_action)\n        dynamic_live_actions.append(thumbnail_browser_action)\n        dynamic_live_actions.append(grid_browser_action)\n\n        for factory in self.__display_controller_factories.values():\n            dynamic_live_actions.extend(factory.build_menu(display_type_menu, display_panel))\n\n        return dynamic_live_actions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef persistent_object_context_changed(self):\n        super().persistent_object_context_changed()\n\n        def source_registered(source):\n            self.__source = source\n\n        def source_unregistered(source=None):\n            pass\n\n        def reference_registered(property_name, reference):\n            self.__referenced_objects[property_name] = reference\n\n        def reference_unregistered(property_name, reference=None):\n            pass\n\n        if self.persistent_object_context:\n            self.persistent_object_context.subscribe(self.source_uuid, source_registered, source_unregistered)\n\n            for property_name, value in self.__properties.items():\n                if isinstance(value, dict) and value.get(\"type\") in {\"data_item\", \"display_item\", \"data_source\", \"graphic\", \"structure\"} and \"uuid\" in value:\n                    self.persistent_object_context.subscribe(uuid.UUID(value[\"uuid\"]), functools.partial(reference_registered, property_name), functools.partial(reference_unregistered, property_name))\n        else:\n            source_unregistered()\n\n            for property_name, value in self.__properties.items():\n                if isinstance(value, dict) and value.get(\"type\") in {\"data_item\", \"display_item\", \"data_source\", \"graphic\", \"structure\"} and \"uuid\" in value:\n                    reference_unregistered(property_name)", "response": "Called by the persistent object when the persistent object context changes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ativar_sat(self, tipo_certificado, cnpj, codigo_uf):\n        resp = self._http_post('ativarsat',\n                tipo_certificado=tipo_certificado,\n                cnpj=cnpj,\n                codigo_uf=codigo_uf)\n        conteudo = resp.json()\n        return RespostaAtivarSAT.analisar(conteudo.get('retorno'))", "response": "Mostrar uma resposta SAT especializada em AtivarSAT."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cancelar_ultima_venda(self, chave_cfe, dados_cancelamento):\n        resp = self._http_post('cancelarultimavenda',\n                chave_cfe=chave_cfe,\n                dados_cancelamento=dados_cancelamento.documento())\n        conteudo = resp.json()\n        return RespostaCancelarUltimaVenda.analisar(conteudo.get('retorno'))", "response": "Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.cancelar_ultima_venda`.\n\n        :return: Uma resposta SAT especializada em ``CancelarUltimaVenda``.\n        :rtype: satcfe.resposta.cancelarultimavenda.RespostaCancelarUltimaVenda"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consultar_numero_sessao(self, numero_sessao):\n        resp = self._http_post('consultarnumerosessao',\n                numero_sessao=numero_sessao)\n        conteudo = resp.json()\n        return RespostaConsultarNumeroSessao.analisar(conteudo.get('retorno'))", "response": "Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.consultar_numero_sessao`.\n\n        :return: Uma resposta SAT que ir\u00e1 depender da sess\u00e3o consultada.\n        :rtype: satcfe.resposta.padrao.RespostaSAT"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef atualizar_software_sat(self):\n        resp = self._http_post('atualizarsoftwaresat')\n        conteudo = resp.json()\n        return RespostaSAT.atualizar_software_sat(conteudo.get('retorno'))", "response": "Sobreve uma resposta SAT atualizar a software."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bloquear_sat(self):\n        resp = self._http_post('bloquearsat')\n        conteudo = resp.json()\n        return RespostaSAT.bloquear_sat(conteudo.get('retorno'))", "response": "Bloquear un objeto RespostaSAT."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef desbloquear_sat(self):\n        resp = self._http_post('desbloquearsat')\n        conteudo = resp.json()\n        return RespostaSAT.desbloquear_sat(conteudo.get('retorno'))", "response": "Desbloquear un objeto RespostaSAT."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trocar_codigo_de_ativacao(self, novo_codigo_ativacao,\n            opcao=constantes.CODIGO_ATIVACAO_REGULAR,\n            codigo_emergencia=None):\n        \"\"\"Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.trocar_codigo_de_ativacao`.\n\n        :return: Uma resposta SAT padr\u00e3o.\n        :rtype: satcfe.resposta.padrao.RespostaSAT\n        \"\"\"\n        resp = self._http_post('trocarcodigodeativacao',\n                novo_codigo_ativacao=novo_codigo_ativacao,\n                opcao=opcao,\n                codigo_emergencia=codigo_emergencia)\n        conteudo = resp.json()\n        return RespostaSAT.trocar_codigo_de_ativacao(conteudo.get('retorno'))", "response": "Trocar codigo de ativacao."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bounds(self) -> typing.Tuple[typing.Tuple[float, float], typing.Tuple[float, float]]:\n        ...", "response": "Return the bounds property in relative coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the end property in relative coordinates.", "response": "def end(self, value: typing.Union[float, typing.Tuple[float, float]]) -> None:\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the start property in relative coordinates.", "response": "def start(self, value: typing.Union[float, typing.Tuple[float, float]]) -> None:\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the vector property in relative coordinates.", "response": "def vector(self) -> typing.Tuple[typing.Tuple[float, float], typing.Tuple[float, float]]:\n        \"\"\"Return the vector property in relative coordinates.\n\n        Vector will be a tuple of tuples ((y_start, x_start), (y_end, x_end)).\"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_data_item_for_hardware_source(self, hardware_source, channel_id: str=None, processor_id: str=None, create_if_needed: bool=False, large_format: bool=False) -> DataItem:\n        ...", "response": "Get the data item associated with a hardware source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the data item associated with the data item reference key. If large_format is True the data item will be large format.", "response": "def get_data_item_for_reference_key(self, data_item_reference_key: str=None, create_if_needed: bool=False, large_format: bool=False) -> DataItem:\n        \"\"\"Get the data item associated with data item reference key. Optionally create if missing.\n\n        :param data_item_reference_key: The data item reference key.\n        :param create_if_needed: Whether to create a new data item if none is found.\n        :return: The associated data item. May be None.\n\n        .. versionadded:: 1.0\n\n        Status: Provisional\n        Scriptable: Yes\n        \"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_get_string_message_box(self, caption: str, text: str, accepted_fn, rejected_fn=None, accepted_text: str=None, rejected_text: str=None) -> None:\n        ...", "response": "Show a dialog box and ask for a string message dialog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a calibration object with offset scale and units.", "response": "def create_calibration(self, offset: float=None, scale: float=None, units: str=None) -> Calibration.Calibration:\n        \"\"\"Create a calibration object with offset, scale, and units.\n\n        :param offset: The offset of the calibration.\n        :param scale: The scale of the calibration.\n        :param units: The units of the calibration as a string.\n        :return: The calibration object.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n\n        Calibrated units and uncalibrated units have the following relationship:\n            :samp:`calibrated_value = offset + value * scale`\n        \"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a data_and_metadata object from data.", "response": "def create_data_and_metadata(self, data: numpy.ndarray, intensity_calibration: Calibration.Calibration=None, dimensional_calibrations: typing.List[Calibration.Calibration]=None, metadata: dict=None, timestamp: str=None, data_descriptor: DataAndMetadata.DataDescriptor=None) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Create a data_and_metadata object from data.\n\n        :param data: an ndarray of data.\n        :param intensity_calibration: An optional calibration object.\n        :param dimensional_calibrations: An optional list of calibration objects.\n        :param metadata: A dict of metadata.\n        :param timestamp: A datetime object.\n        :param data_descriptor: A data descriptor describing the dimensions.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_data_and_metadata_from_data(self, data: numpy.ndarray, intensity_calibration: Calibration.Calibration=None, dimensional_calibrations: typing.List[Calibration.Calibration]=None, metadata: dict=None, timestamp: str=None) -> DataAndMetadata.DataAndMetadata:\n        ...", "response": "Create a data_and_metadata object from data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a data descriptor.", "response": "def create_data_descriptor(self, is_sequence: bool, collection_dimension_count: int, datum_dimension_count: int) -> DataAndMetadata.DataDescriptor:\n        \"\"\"Create a data descriptor.\n\n        :param is_sequence: whether the descriptor describes a sequence of data.\n        :param collection_dimension_count: the number of collection dimensions represented by the descriptor.\n        :param datum_dimension_count: the number of datum dimensions represented by the descriptor.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_calibration_row_widget(ui, calibration_observable, label: str=None):\n    calibration_row = ui.create_row_widget()\n    row_label = ui.create_label_widget(label, properties={\"width\": 60})\n    row_label.widget_id = \"label\"\n    offset_field = ui.create_line_edit_widget(properties={\"width\": 60})\n    offset_field.widget_id = \"offset\"\n    scale_field = ui.create_line_edit_widget(properties={\"width\": 60})\n    scale_field.widget_id = \"scale\"\n    units_field = ui.create_line_edit_widget(properties={\"width\": 60})\n    units_field.widget_id = \"units\"\n    float_point_4_converter = Converter.FloatToStringConverter(format=\"{0:.4f}\")\n    offset_field.bind_text(Binding.PropertyBinding(calibration_observable, \"offset\", converter=float_point_4_converter))\n    scale_field.bind_text(Binding.PropertyBinding(calibration_observable, \"scale\", converter=float_point_4_converter))\n    units_field.bind_text(Binding.PropertyBinding(calibration_observable, \"units\"))\n    # notice the binding of calibration_index below.\n    calibration_row.add(row_label)\n    calibration_row.add_spacing(12)\n    calibration_row.add(offset_field)\n    calibration_row.add_spacing(12)\n    calibration_row.add(scale_field)\n    calibration_row.add_spacing(12)\n    calibration_row.add(units_field)\n    calibration_row.add_stretch()\n    return calibration_row", "response": "Called when an item is inserted into the list widget. Returns a widget."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_widget_to_content(self, widget):\n        self.__section_content_column.add_spacing(4)\n        self.__section_content_column.add(widget)", "response": "Subclasses should call this to add a widget to the section s top level column."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when an item is inserted into the list widget. Returns a widget.", "response": "def __create_list_item_widget(self, ui, calibration_observable):\n        \"\"\"Called when an item (calibration_observable) is inserted into the list widget. Returns a widget.\"\"\"\n        calibration_row = make_calibration_row_widget(ui, calibration_observable)\n        column = ui.create_column_widget()\n        column.add_spacing(4)\n        column.add(calibration_row)\n        return column"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _repaint(self, drawing_context):\n\n        # canvas size\n        canvas_width = self.canvas_size[1]\n        canvas_height = self.canvas_size[0]\n\n        left = self.display_limits[0]\n        right = self.display_limits[1]\n\n        # draw left display limit\n        if left > 0.0:\n            with drawing_context.saver():\n                drawing_context.begin_path()\n                drawing_context.move_to(left * canvas_width, 1)\n                drawing_context.line_to(left * canvas_width, canvas_height-1)\n                drawing_context.line_width = 2\n                drawing_context.stroke_style = \"#000\"\n                drawing_context.stroke()\n\n        # draw right display limit\n        if right < 1.0:\n            with drawing_context.saver():\n                drawing_context.begin_path()\n                drawing_context.move_to(right * canvas_width, 1)\n                drawing_context.line_to(right * canvas_width, canvas_height-1)\n                drawing_context.line_width = 2\n                drawing_context.stroke_style = \"#FFF\"\n                drawing_context.stroke()\n\n        # draw border\n        with drawing_context.saver():\n            drawing_context.begin_path()\n            drawing_context.move_to(0,canvas_height)\n            drawing_context.line_to(canvas_width,canvas_height)\n            drawing_context.line_width = 1\n            drawing_context.stroke_style = \"#444\"\n            drawing_context.stroke()", "response": "Repaint the canvas item. This will occur on a thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _repaint(self, drawing_context):\n\n        # canvas size\n        canvas_width = self.canvas_size[1]\n        canvas_height = self.canvas_size[0]\n\n        # draw background\n        if self.background_color:\n            with drawing_context.saver():\n                drawing_context.begin_path()\n                drawing_context.move_to(0,0)\n                drawing_context.line_to(canvas_width,0)\n                drawing_context.line_to(canvas_width,canvas_height)\n                drawing_context.line_to(0,canvas_height)\n                drawing_context.close_path()\n                drawing_context.fill_style = self.background_color\n                drawing_context.fill()\n\n        # draw the data, if any\n        if (self.data is not None and len(self.data) > 0):\n\n            # draw the histogram itself\n            with drawing_context.saver():\n                drawing_context.begin_path()\n                binned_data = Image.rebin_1d(self.data, int(canvas_width), self.__retained_rebin_1d) if int(canvas_width) != self.data.shape[0] else self.data\n                for i in range(canvas_width):\n                    drawing_context.move_to(i, canvas_height)\n                    drawing_context.line_to(i, canvas_height * (1 - binned_data[i]))\n                drawing_context.line_width = 1\n                drawing_context.stroke_style = \"#444\"\n                drawing_context.stroke()", "response": "Repaint the canvas item. This will occur on a thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef color_map_data(self, data: numpy.ndarray) -> None:\n        self.__color_map_data = data\n        self.update()", "response": "Set the data and mark the canvas item for updating."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _repaint(self, drawing_context: DrawingContext.DrawingContext):\n\n        # canvas size\n        canvas_width = self.canvas_size.width\n        canvas_height = self.canvas_size.height\n\n        with drawing_context.saver():\n            if self.__color_map_data is not None:\n                rgba_image = numpy.empty((4,) + self.__color_map_data.shape[:-1], dtype=numpy.uint32)\n                Image.get_rgb_view(rgba_image)[:] = self.__color_map_data[numpy.newaxis, :, :]  # scalar data assigned to each component of rgb view\n                Image.get_alpha_view(rgba_image)[:] = 255\n                drawing_context.draw_image(rgba_image, 0, 0, canvas_width, canvas_height)", "response": "Repaint the canvas item. This will occur on a thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_params(self):\n        \"Parameters used to initialize the class\"\n        import inspect\n        a = inspect.getargspec(self.__init__)[0]\n        out = dict()\n        for key in a[1:]:\n            value = getattr(self, \"_%s\" % key, None)\n            out[key] = value\n        return out", "response": "Parameters used to initialize the class"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclass containing the population and all the individuals generated", "response": "def population(self):\n        \"Class containing the population and all the individuals generated\"\n        try:\n            return self._p\n        except AttributeError:\n            self._p = self._population_class(base=self,\n                                             tournament_size=self._tournament_size,\n                                             classifier=self.classifier,\n                                             labels=self._labels,\n                                             es_extra_test=self.es_extra_test,\n                                             popsize=self._popsize,\n                                             random_generations=self._random_generations,\n                                             negative_selection=self._negative_selection)\n            return self._p"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_leaf(self):\n        \"Returns a random variable with the associated weight\"\n        for i in range(self._number_tries_feasible_ind):\n            var = np.random.randint(self.nvar)\n            v = self._random_leaf(var)\n            if v is None:\n                continue\n            return v\n        raise RuntimeError(\"Could not find a suitable random leaf\")", "response": "Returns a random variable with the associated weight"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_offspring(self):\n        \"Returns an offspring with the associated weight(s)\"\n        function_set = self.function_set\n        function_selection = self._function_selection_ins\n        function_selection.density = self.population.density\n        function_selection.unfeasible_functions.clear()\n        for i in range(self._number_tries_feasible_ind):\n            if self._function_selection:\n                func_index = function_selection.tournament()\n            else:\n                func_index = function_selection.random_function()\n            func = function_set[func_index]\n            args = self.get_args(func)\n            if args is None:\n                continue\n            args = [self.population.population[x].position for x in args]\n            f = self._random_offspring(func, args)\n            if f is None:\n                function_selection.unfeasible_functions.add(func_index)\n                continue\n            function_selection[func_index] = f.fitness\n            return f\n        raise RuntimeError(\"Could not find a suitable random offpsring\")", "response": "Returns an offspring with the associated weight ( s )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest whether the stopping criteria has been achieved.", "response": "def stopping_criteria(self):\n        \"Test whether the stopping criteria has been achieved.\"\n        if self.stopping_criteria_tl():\n            return True\n        if self.generations < np.inf:\n            inds = self.popsize * self.generations\n            flag = inds <= len(self.population.hist)\n        else:\n            flag = False\n        if flag:\n            return True\n        est = self.population.estopping\n        if self._tr_fraction < 1:\n            if est is not None and est.fitness_vs == 0:\n                return True\n        esr = self._early_stopping_rounds\n        if self._tr_fraction < 1 and esr is not None and est is not None:\n            position = self.population.estopping.position\n            if position < self.init_popsize:\n                position = self.init_popsize\n            return (len(self.population.hist) +\n                    self._unfeasible_counter -\n                    position) > esr\n        return flag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nclasses(self, v):\n        \"Number of classes of v, also sets the labes\"\n        if not self.classifier:\n            return 0\n        if isinstance(v, list):\n            self._labels = np.arange(len(v))\n            return\n        if not isinstance(v, np.ndarray):\n            v = tonparray(v)\n        self._labels = np.unique(v)\n        return self._labels.shape[0]", "response": "Number of classes of v also sets the labes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y, test_set=None):\n        self._init_time = time.time()\n        self.X = X\n        if self._popsize == \"nvar\":\n            self._popsize = self.nvar + len(self._input_functions)\n        if isinstance(test_set, str) and test_set == 'shuffle':\n            test_set = self.shuffle_tr2ts()\n        nclasses = self.nclasses(y)\n        if self.classifier and self._multiple_outputs:\n            pass\n        elif nclasses > 2:\n            assert False\n            self._multiclass = True\n            return self.multiclass(X, y, test_set=test_set)\n        self.y = y\n        if test_set is not None:\n            self.Xtest = test_set\n        for _ in range(self._number_tries_feasible_ind):\n            self._logger.info(\"Starting evolution\")\n            try:\n                self.create_population()\n                if self.stopping_criteria_tl():\n                    break\n            except RuntimeError as err:\n                self._logger.info(\"Done evolution (RuntimeError (%s), hist: %s)\" % (err, len(self.population.hist)))\n                return self\n            self._logger.info(\"Population created (hist: %s)\" % len(self.population.hist))\n            if len(self.population.hist) >= self._tournament_size:\n                break\n        if len(self.population.hist) == 0:\n            raise RuntimeError(\"Could not find a suitable individual\")\n        if len(self.population.hist) < self._tournament_size:\n            self._logger.info(\"Done evolution (hist: %s)\" % len(self.population.hist))\n            return self\n        if self._remove_raw_inputs:\n            for x in range(self.nvar):\n                self._X[x] = None\n        while not self.stopping_criteria():\n            try:\n                a = self.random_offspring()\n            except RuntimeError as err:\n                self._logger.info(\"Done evolution (RuntimeError (%s), hist: %s)\" % (err, len(self.population.hist)))\n                return self\n            self.replace(a)\n        self._logger.info(\"Done evolution (hist: %s)\" % len(self.population.hist))\n        return self", "response": "Fits the evolution process"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serve(application, host='127.0.0.1', port=8080):\n\t\n\t# Instantiate the server with a host/port configuration and our application.\n\tWSGIServer((host, int(port)), application).serve_forever()", "response": "Gevent - based WSGI - HTTP server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the subscribers for a given object.", "response": "def get_subscribers(obj):\n    \"\"\"\n    Returns the subscribers for a given object.\n\n    :param obj: Any object.\n\n    \"\"\"\n    ctype = ContentType.objects.get_for_model(obj)\n    return Subscription.objects.filter(content_type=ctype, object_id=obj.pk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_subscribed(user, obj):\n    if not user.is_authenticated():\n        return False\n\n    ctype = ContentType.objects.get_for_model(obj)\n\n    try:\n        Subscription.objects.get(\n            user=user, content_type=ctype, object_id=obj.pk)\n    except Subscription.DoesNotExist:\n        return False\n\n    return True", "response": "Returns True if the user is subscribed to the given object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _promote(self, name, instantiate=True):\n\t\t\n\t\tmetaclass = type(self.__class__)\n\t\tcontents = self.__dict__.copy()\n\t\tcls = metaclass(str(name), (self.__class__, ), contents)\n\t\t\n\t\tif instantiate:\n\t\t\treturn cls()\n\t\t\n\t\treturn cls", "response": "Create a new subclass of Context which incorporates instance attributes and descriptors and returns an instance of that class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_individual(sim_var, \n                   reference,\n                   neuroml_file,\n                   nml_doc,\n                   still_included,\n                   generate_dir,\n                   target,\n                   sim_time, \n                   dt, \n                   simulator,\n                   cleanup = True,\n                   show=False):\n    \"\"\"\n    Run an individual simulation.\n\n    The candidate data has been flattened into the sim_var dict. The\n    sim_var dict contains parameter:value key value pairs, which are\n    applied to the model before it is simulated.\n\n    \"\"\"\n \n    for var_name in sim_var.keys():\n        \n        individual_var_names = var_name.split('+')\n        \n        for individual_var_name in individual_var_names:\n            words = individual_var_name.split('/')\n            type, id1 = words[0].split(':')\n            if ':' in words[1]:\n                variable, id2 = words[1].split(':')\n            else:\n                variable = words[1]\n                id2 = None\n\n            units = words[2]\n            value = sim_var[var_name]\n\n            pyneuroml.pynml.print_comment_v('  Changing value of %s (%s) in %s (%s) to: %s %s'%(variable, id2, type, id1, value, units))\n\n            if type == 'channel':\n                channel = nml_doc.get_by_id(id1)\n                \n                if channel:\n                    print(\"Setting channel %s\"%(channel))\n                    if variable == 'vShift':\n                        channel.v_shift = '%s %s'%(value, units)\n                else:\n                    \n                    pyneuroml.pynml.print_comment_v('Could not find channel with id %s from expression: %s'%(id1, individual_var_name))\n                    exit()\n                    \n\n            elif type == 'cell':\n                cell = None\n                for c in nml_doc.cells:\n                    if c.id == id1:\n                        cell = c\n\n                if variable == 'channelDensity':\n\n                    chanDens = None\n                    for cd in cell.biophysical_properties.membrane_properties.channel_densities + cell.biophysical_properties.membrane_properties.channel_density_v_shifts:\n                        if cd.id == id2:\n                            chanDens = cd\n\n                    chanDens.cond_density = '%s %s'%(value, units)\n                    \n                elif variable == 'vShift_channelDensity':\n\n                    chanDens = None\n                    for cd in cell.biophysical_properties.membrane_properties.channel_density_v_shifts:\n                        if cd.id == id2:\n                            chanDens = cd\n\n                    chanDens.v_shift = '%s %s'%(value, units)\n\n                elif variable == 'channelDensityNernst':\n\n                    chanDens = None\n                    for cd in cell.biophysical_properties.membrane_properties.channel_density_nernsts:\n                        if cd.id == id2:\n                            chanDens = cd\n\n                    chanDens.cond_density = '%s %s'%(value, units)\n\n                elif variable == 'erev_id': # change all values of erev in channelDensity elements with only this id\n\n                    chanDens = None\n                    for cd in cell.biophysical_properties.membrane_properties.channel_densities + cell.biophysical_properties.membrane_properties.channel_density_v_shifts:\n                        if cd.id == id2:\n                            chanDens = cd\n\n                    chanDens.erev = '%s %s'%(value, units)\n\n                elif variable == 'erev_ion': # change all values of erev in channelDensity elements with this ion\n\n                    chanDens = None\n                    for cd in cell.biophysical_properties.membrane_properties.channel_densities + cell.biophysical_properties.membrane_properties.channel_density_v_shifts:\n                        if cd.ion == id2:\n                            chanDens = cd\n\n                    chanDens.erev = '%s %s'%(value, units)\n\n                elif variable == 'specificCapacitance': \n\n                    specCap = None\n                    for sc in cell.biophysical_properties.membrane_properties.specific_capacitances:\n                        if (sc.segment_groups == None and id2 == 'all') or sc.segment_groups == id2 :\n                            specCap = sc\n\n                    specCap.value = '%s %s'%(value, units)\n\n                elif variable == 'resistivity': \n\n                    resistivity = None\n                    for rs in cell.biophysical_properties.intracellular_properties.resistivities:\n                        if (rs.segment_groups == None and id2 == 'all') or rs.segment_groups == id2 :\n                            resistivity = rs\n\n                    resistivity.value = '%s %s'%(value, units)\n\n                else:\n                    pyneuroml.pynml.print_comment_v('Unknown variable (%s) in variable expression: %s'%(variable, individual_var_name))\n                    exit()\n\n            elif type == 'izhikevich2007Cell':\n                izhcell = None\n                for c in nml_doc.izhikevich2007_cells:\n                    if c.id == id1:\n                        izhcell = c\n\n                izhcell.__setattr__(variable, '%s %s'%(value, units))\n\n            else:\n                pyneuroml.pynml.print_comment_v('Unknown type (%s) in variable expression: %s'%(type, individual_var_name))\n\n\n\n    new_neuroml_file =  '%s/%s'%(generate_dir,os.path.basename(neuroml_file))\n    if new_neuroml_file == neuroml_file:\n        pyneuroml.pynml.print_comment_v('Cannot use a directory for generating into (%s) which is the same location of the NeuroML file (%s)!'% \\\n                  (neuroml_file, generate_dir))\n\n    pyneuroml.pynml.write_neuroml2_file(nml_doc, new_neuroml_file)\n\n    for include in still_included:\n        inc_loc = '%s/%s'%(os.path.dirname(os.path.abspath(neuroml_file)),include)\n        pyneuroml.pynml.print_comment_v(\"Copying non included file %s to %s (%s) beside %s\"%(inc_loc, generate_dir,os.path.abspath(generate_dir), new_neuroml_file))\n        shutil.copy(inc_loc, generate_dir)\n        \n        \n\n    from pyneuroml.tune.NeuroMLSimulation import NeuroMLSimulation\n\n    sim = NeuroMLSimulation(reference, \n                         neuroml_file = new_neuroml_file,\n                         target = target,\n                         sim_time = sim_time, \n                         dt = dt, \n                         simulator = simulator, \n                         generate_dir = generate_dir,\n                         cleanup = cleanup,\n                         nml_doc = nml_doc)\n\n    sim.go()\n\n    if show:\n        sim.show()\n\n    return sim.t, sim.volts", "response": "Runs an individual simulation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self,candidates,parameters):\n\n        traces = []\n        start_time = time.time()\n        \n        if self.num_parallel_evaluations == 1:\n            \n            for candidate_i in range(len(candidates)):\n                \n                candidate = candidates[candidate_i]\n                sim_var = dict(zip(parameters,candidate))\n                pyneuroml.pynml.print_comment_v('\\n\\n  - RUN %i (%i/%i); variables: %s\\n'%(self.count,candidate_i+1,len(candidates),sim_var))\n                self.count+=1\n                t,v = self.run_individual(sim_var)\n                traces.append([t,v])\n\n        else:\n            import pp\n            ppservers = ()\n            job_server = pp.Server(self.num_parallel_evaluations, ppservers=ppservers)\n            pyneuroml.pynml.print_comment_v('Running %i candidates across %i local processes'%(len(candidates),job_server.get_ncpus()))\n            jobs = []\n            \n            for candidate_i in range(len(candidates)):\n                \n                candidate = candidates[candidate_i]\n                sim_var = dict(zip(parameters,candidate))\n                pyneuroml.pynml.print_comment_v('\\n\\n  - PARALLEL RUN %i (%i/%i of curr candidates); variables: %s\\n'%(self.count,candidate_i+1,len(candidates),sim_var))\n                self.count+=1\n                cand_dir = self.generate_dir+\"/CANDIDATE_%s\"%candidate_i\n                if not os.path.exists(cand_dir):\n                    os.mkdir(cand_dir)\n                vars = (sim_var,\n                        self.ref,\n                        self.neuroml_file,\n                        self.nml_doc,\n                        self.still_included,\n                        cand_dir,\n                        self.target,\n                        self.sim_time,\n                        self.dt,\n                        self.simulator)\n                        \n                job = job_server.submit(run_individual, vars, (), (\"pyneuroml.pynml\",'pyneuroml.tune.NeuroMLSimulation','shutil','neuroml'))\n                jobs.append(job)\n            \n            for job_i in range(len(jobs)):\n                job = jobs[job_i]\n                pyneuroml.pynml.print_comment_v(\"Checking parallel job %i/%i; set running so far: %i\"%(job_i,len(jobs),self.count))\n                t,v = job()\n                traces.append([t,v])\n                \n                #pyneuroml.pynml.print_comment_v(\"Obtained: %s\"%result) \n                \n            ####job_server.print_stats()\n            job_server.destroy()\n            print(\"-------------------------------------------\")\n                \n                \n            \n        end_time = time.time()\n        tot = (end_time-start_time)\n        pyneuroml.pynml.print_comment_v('Ran %i candidates in %s seconds (~%ss per job)'%(len(candidates),tot,tot/len(candidates)))\n\n        return traces", "response": "Run simulation for each candidate and return a list of voltage traces."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes prior to processing a request.", "response": "def prepare(self, context):\n\t\t\"\"\"Executed prior to processing a request.\"\"\"\n\t\tif __debug__:\n\t\t\tlog.debug(\"Assigning thread local request context.\")\n\t\t\n\t\tself.local.context = context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(self, kind, handler):\n\t\tif __debug__:  # In production this logging is completely skipped, regardless of logging level.\n\t\t\tif py3 and not pypy:  # Where possible, we shorten things to just the cannonical name.\n\t\t\t\tlog.debug(\"Registering view handler.\", extra=dict(type=name(kind), handler=name(handler)))\n\t\t\telse:  # Canonical name lookup is not entirely reliable on some combinations.\n\t\t\t\tlog.debug(\"Registering view handler.\", extra=dict(type=repr(kind), handler=repr(handler)))\n\t\t\n\t\t# Add the handler to the pool of candidates. This adds to a list instead of replacing the \"dictionary item\".\n\t\tself._map.add(kind, handler)\n\t\t\n\t\treturn handler", "response": "Register a handler for a given type class interface or abstract base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef static(base, mapping=None, far=('js', 'css', 'gif', 'jpg', 'jpeg', 'png', 'ttf', 'woff')):\n\t\n\tbase = abspath(base)\n\t\n\t@staticmethod\n\tdef static_handler(context, *parts, **kw):\n\t\tpath = normpath(pathjoin(base, *parts))\n\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"Attempting to serve static file.\", extra=dict(\n\t\t\t\t\trequest = id(context),\n\t\t\t\t\tbase = base,\n\t\t\t\t\tpath = path\n\t\t\t\t))\n\t\t\n\t\tif not path.startswith(base):  # Ensure we only serve files from the allowed path.\n\t\t\traise HTTPForbidden(\"Cowardly refusing to violate base path policy.\" if __debug__ else None)\n\t\t\n\t\tif not exists(path):  # Do the right thing if the file doesn't actually exist.\n\t\t\traise HTTPNotFound()\n\t\t\n\t\tif not isfile(path):  # Only serve normal files; no UNIX domain sockets, FIFOs, etc., etc.\n\t\t\traise HTTPForbidden(\"Cowardly refusing to open a non-file.\" if __debug__ else None)\n\t\t\n\t\tif far and path.rpartition('.')[2] in far:\n\t\t\tcontext.response.cache_expires = 60*60*24*365\n\t\t\n\t\tif mapping:  # Handle the mapping of filename extensions to 2-tuples. 'Cause why not?\n\t\t\t_, _, extension = basename(path).partition('.')\n\t\t\tif extension in mapping:\n\t\t\t\treturn mapping[extension] + ':' + path, dict()\n\t\t\n\t\treturn open(path, 'rb')\n\t\n\treturn static_handler", "response": "This function is used to serve static files from disk."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing command - line arguments and return a list of dictionaries containing spike time data.", "response": "def process_args():\n    \"\"\" \n    Parse command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"A script for plotting files containing spike time data\")\n    \n    parser.add_argument('spiketimeFiles', \n                        type=str, \n                        metavar='<spiketime file>', \n                        help='List of text file containing spike times', \n                        nargs='+')\n                        \n    parser.add_argument('-format', \n                        type=str,\n                        metavar='<format>',\n                        default=DEFAULTS['format'],\n                        help='How the spiketimes are represented on each line of file:\\n'+\\\n                             'id_t: id of cell, space(s)/tab(s), time of spike (default);\\n'+\\\n                             't_id: time of spike, space(s)/tab(s), id of cell;\\n'+\\\n                             'sonata: SONATA format HDF5 file containing spike times')\n                             \n    parser.add_argument('-rates', \n                        action='store_true',\n                        default=DEFAULTS['rates'],\n                        help='Show a plot of rates')\n                        \n    parser.add_argument('-showPlotsAlready', \n                        action='store_true',\n                        default=DEFAULTS['show_plots_already'],\n                        help='Show plots once generated')\n                        \n    parser.add_argument('-saveSpikePlotTo', \n                        type=str,\n                        metavar='<spiketime plot filename>',\n                        default=DEFAULTS['save_spike_plot_to'],\n                        help='Name of file in which to save spiketime plot')\n                        \n    parser.add_argument('-rateWindow', \n                        type=int,\n                        metavar='<rate window>',\n                        default=DEFAULTS['rate_window'],\n                        help='Window for rate calculation in ms')\n                        \n    parser.add_argument('-rateBins', \n                        type=int,\n                        metavar='<rate bins>',\n                        default=DEFAULTS['rate_bins'],\n                        help='Number of bins for rate histogram')\n                        \n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, *args, **kwargs):\n        method_kwargs = self._get_method_kwargs()\n        try:\n            subscription = Subscription.objects.get(**method_kwargs)\n        except Subscription.DoesNotExist:\n            subscription = Subscription.objects.create(**method_kwargs)\n        return subscription", "response": "Adds a subscription for the given user to the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare(self, context):\n\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"Preparing request context.\", extra=dict(request=id(context)))\n\t\t\n\t\t# Bridge in WebOb `Request` and `Response` objects.\n\t\t# Extensions shouldn't rely on these, using `environ` where possible instead.\n\t\tcontext.request = Request(context.environ)\n\t\tcontext.response = Response(request=context.request)\n\t\t\n\t\t# Record the initial path representing the point where a front-end web server bridged to us.\n\t\tcontext.environ['web.base'] = context.request.script_name\n\t\t\n\t\t# Track the remaining (unprocessed) path elements.\n\t\tcontext.request.remainder = context.request.path_info.split('/')\n\t\tif context.request.remainder and not context.request.remainder[0]:\n\t\t\tdel context.request.remainder[0]\n\t\t\n\t\t# Track the \"breadcrumb list\" of dispatch through distinct controllers.\n\t\tcontext.path = Bread()", "response": "This method is called before the request context is processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dispatch(self, context, consumed, handler, is_endpoint):\n\t\t\n\t\trequest = context.request\n\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"Handling dispatch event.\", extra=dict(\n\t\t\t\t\trequest = id(context),\n\t\t\t\t\tconsumed = consumed,\n\t\t\t\t\thandler = safe_name(handler),\n\t\t\t\t\tendpoint = is_endpoint\n\t\t\t\t))\n\t\t\n\t\t# The leading path element (leading slash) requires special treatment.\n\t\tif not consumed and context.request.path_info_peek() == '':\n\t\t\tconsumed = ['']\n\t\t\n\t\tnConsumed = 0\n\t\tif consumed:\n\t\t\t# Migrate path elements consumed from the `PATH_INFO` to `SCRIPT_NAME` WSGI environment variables.\n\t\t\tif not isinstance(consumed, (list, tuple)):\n\t\t\t\tconsumed = consumed.split('/')\n\t\t\t\n\t\t\tfor element in consumed:\n\t\t\t\tif element == context.request.path_info_peek():\n\t\t\t\t\tcontext.request.path_info_pop()\n\t\t\t\t\tnConsumed += 1\n\t\t\t\telse:\n\t\t\t\t\tbreak\n\t\t\n\t\t# Update the breadcrumb list.\n\t\tcontext.path.append(Crumb(handler, Path(request.script_name)))\n\t\t\n\t\tif consumed:  # Lastly, update the remaining path element list.\n\t\t\trequest.remainder = request.remainder[nConsumed:]", "response": "Called as a dispatch descends into a tier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns binary responses unmodified.", "response": "def render_binary(self, context, result):\n\t\t\"\"\"Return binary responses unmodified.\"\"\"\n\t\tcontext.response.app_iter = iter((result, ))  # This wraps the binary string in a WSGI body iterable.\n\t\treturn True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming appropriate metadata wrangling for returned open file handles.", "response": "def render_file(self, context, result):\n\t\t\"\"\"Perform appropriate metadata wrangling for returned open file handles.\"\"\"\n\t\tif __debug__:\n\t\t\tlog.debug(\"Processing file-like object.\", extra=dict(request=id(context), result=repr(result)))\n\t\t\n\t\tresponse = context.response\n\t\tresponse.conditional_response = True\n\t\t\n\t\tmodified = mktime(gmtime(getmtime(result.name)))\n\t\t\n\t\tresponse.last_modified = datetime.fromtimestamp(modified)\n\t\tct, ce = guess_type(result.name)\n\t\tif not ct: ct = 'application/octet-stream'\n\t\tresponse.content_type, response.content_encoding = ct, ce\n\t\tresponse.etag = unicode(modified)\n\t\t\n\t\tresult.seek(0, 2)  # Seek to the end of the file.\n\t\tresponse.content_length = result.tell()\n\t\t\n\t\tresult.seek(0)  # Seek back to the start of the file.\n\t\tresponse.body_file = result\n\t\t\n\t\treturn True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_generator(self, context, result):\n\t\tcontext.response.encoding = 'utf8'\n\t\tcontext.response.app_iter = (\n\t\t\t\t(i.encode('utf8') if isinstance(i, unicode) else i)  # Stream encode unicode chunks.\n\t\t\t\tfor i in result if i is not None  # Skip None values.\n\t\t\t)\n\t\treturn True", "response": "Attempt to serve generator responses through stream encoding."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serve(application, host='127.0.0.1', port=8080):\n\t\n\t# Instantiate the server with our configuration and application.\n\tserver = CherryPyWSGIServer((host, int(port)), application, server_name=host)\n\t\n\t# Try to be handy as many terminals allow clicking links.\n\tprint(\"serving on http://{0}:{1}\".format(host, port))\n\t\n\t# Bind and launch the server; this is a blocking operation.\n\ttry:\n\t\tserver.start()\n\texcept KeyboardInterrupt:\n\t\tserver.stop()", "response": "Start a CherryPy - based WSGI - HTTP server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the colored string", "response": "def colorize(self, string, rgb=None, ansi=None, bg=None, ansi_bg=None):\n        '''Returns the colored string'''\n        if not isinstance(string, str):\n            string = str(string)\n        if rgb is None and ansi is None:\n            raise TerminalColorMapException(\n                'colorize: must specify one named parameter: rgb or ansi')\n        if rgb is not None and ansi is not None:\n            raise TerminalColorMapException(\n                'colorize: must specify only one named parameter: rgb or ansi')\n        if bg is not None and ansi_bg is not None:\n            raise TerminalColorMapException(\n                'colorize: must specify only one named parameter: bg or ansi_bg')\n\n        if rgb is not None:\n            (closestAnsi, closestRgb) = self.convert(rgb)\n        elif ansi is not None:\n            (closestAnsi, closestRgb) = (ansi, self.colors[ansi])\n\n        if bg is None and ansi_bg is None:\n            return \"\\033[38;5;{ansiCode:d}m{string:s}\\033[0m\".format(ansiCode=closestAnsi, string=string)\n\n        if bg is not None:\n            (closestBgAnsi, unused) = self.convert(bg)\n        elif ansi_bg is not None:\n            (closestBgAnsi, unused) = (ansi_bg, self.colors[ansi_bg])\n\n        return \"\\033[38;5;{ansiCode:d}m\\033[48;5;{bf:d}m{string:s}\\033[0m\".format(ansiCode=closestAnsi, bf=closestBgAnsi, string=string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions that generates the plot of all the available components.", "response": "def main(args=None):\n    \"\"\"Main\"\"\"\n    \n    vs = [(v-100)*0.001 for v in range(200)]\n    \n    for f in ['IM.channel.nml','Kd.channel.nml']:\n        nml_doc = pynml.read_neuroml2_file(f)\n\n        for ct in nml_doc.ComponentType:\n\n            ys = []\n            for v in vs:\n                req_variables = {'v':'%sV'%v,'vShift':'10mV'}\n                vals = pynml.evaluate_component(ct,req_variables=req_variables)\n                print vals\n                if 'x' in vals:\n                    ys.append(vals['x'])\n                if 't' in vals:\n                    ys.append(vals['t'])\n                if 'r' in vals:\n                    ys.append(vals['r'])\n\n            ax = pynml.generate_plot([vs],[ys],          \n                             \"Some traces from %s in %s\"%(ct.name,f),\n                             show_plot_already=False )       \n\n            print vals\n        \n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef root(context):\n\t\n\tresponse = context.response\n\t\n\tparts = []\n\t\n\tfor i in range(12):\n\t\tfor j in range(12):\n\t\t\tparts.append(executor.submit(mul, i, j))\n\t\n\tdef stream(parts, timeout=None):\n\t\ttry:\n\t\t\tfor future in as_completed(parts, timeout):\n\t\t\t\tmime, result = future.result()\n\t\t\t\tresult = result.encode('utf8')\n\t\t\t\t\n\t\t\t\tyield \"!!!!!!=_NextPart_{num}\\nContent-Type: {mime}\\nContent-Length: {length}\\n\\n\".format(\n\t\t\t\t\t\tnum = randint(100000000, 999999999),\n\t\t\t\t\t\tmime = mime,\n\t\t\t\t\t\tlength = len(result)\n\t\t\t\t\t).encode('utf8') + result\n\t\t\n\t\texcept TimeoutError:\n\t\t\tfor future in parts:\n\t\t\t\tfuture.cancel()\n\t\n\tresponse.content_length = None\n\tresponse.app_iter = stream(parts, 0.2)\n\t\n\treturn response", "response": "This function returns a multipart ajax request example. It returns a response object that contains the response body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a file and render the content of the template_file_name with kwargs in a file", "response": "def render_template_with_args_in_file(file, template_file_name, **kwargs):\n    \"\"\"\n    Get a file and render the content of the template_file_name with kwargs in a file\n    :param file: A File Stream to write\n    :param template_file_name: path to route with template name\n    :param **kwargs: Args to be rendered in template\n    \"\"\"\n    template_file_content = \"\".join(\n        codecs.open(\n            template_file_name,\n            encoding='UTF-8'\n        ).readlines()\n    )\n    template_rendered = string.Template(template_file_content).safe_substitute(**kwargs)\n    file.write(template_rendered)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a file or opens a file with file_name name", "response": "def create_or_open(file_name, initial_template_file_name, args):\n    \"\"\"\n    Creates a file or open the file with file_name name\n    :param file_name: String with a filename\n    :param initial_template_file_name: String with path to initial template\n    :param args: from console to determine path to save the files\n    \"\"\"\n    file = None\n    if not os.path.isfile(\n        os.path.join(\n            args['django_application_folder'],\n            file_name\n        )\n    ):\n        # If file_name does not exists, create\n        file = codecs.open(\n            os.path.join(\n                args['django_application_folder'],\n                file_name\n            ),\n            'w+',\n            encoding='UTF-8'\n        )\n        print(\"Creating {}\".format(file_name))\n        if initial_template_file_name:\n            render_template_with_args_in_file(file, initial_template_file_name, **{})\n    else:\n        # If file exists, just load the file\n        file = codecs.open(\n            os.path.join(\n                args['django_application_folder'],\n                file_name\n            ),\n            'a+',\n            encoding='UTF-8'\n        )\n\n    return file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify if the work folder is a Django application folder.", "response": "def sanity_check(args):\n    \"\"\"\n    Verify if the work folder is a django app.\n    A valid django app always must have a models.py file\n    :return: None\n    \"\"\"\n    if not os.path.isfile(\n        os.path.join(\n            args['django_application_folder'],\n            'models.py'\n        )\n    ):\n        print(\"django_application_folder is not a Django application folder\")\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generic_insert_with_folder(folder_name, file_name, template_name, args):\n    # First we make sure views are a package instead a file\n    if not os.path.isdir(\n        os.path.join(\n            args['django_application_folder'],\n            folder_name\n        )\n    ):\n        os.mkdir(os.path.join(args['django_application_folder'], folder_name))\n        codecs.open(\n            os.path.join(\n                args['django_application_folder'],\n                folder_name,\n                '__init__.py'\n            ),\n            'w+'\n        )\n\n    view_file = create_or_open(\n        os.path.join(\n            folder_name,\n            '{}.py'.format(file_name)\n        ), \n        '', \n        args\n    )\n    # Load content from template\n    render_template_with_args_in_file(\n        view_file, \n        os.path.join(\n            BASE_TEMPLATES_DIR, \n            template_name\n        ),\n        model_name=args['model_name'],\n        model_prefix=args['model_prefix'],\n        model_name_lower=args['model_name'].lower(),\n        application_name=args['django_application_folder'].split(\"/\")[-1]\n    )\n    view_file.close()", "response": "Generic method for inserting a new file into a folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the result of the simulation once it s intialized", "response": "def show(self):\n        \"\"\"\n        Plot the result of the simulation once it's been intialized\n        \"\"\"\n\n        from matplotlib import pyplot as plt\n\n        if self.already_run:\n            \n            for ref in self.volts.keys():\n\n                plt.plot(self.t, self.volts[ref], label=ref)\n                plt.title(\"Simulation voltage vs time\")\n                plt.legend()\n                plt.xlabel(\"Time [ms]\")\n                plt.ylabel(\"Voltage [mV]\")\n\n        else:\n            pynml.print_comment(\"First you have to 'go()' the simulation.\", True)\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmultiplying two values together and return the result via JSON.", "response": "def mul(self, a: int = None, b: int = None) -> 'json':\n\t\t\"\"\"Multiply two values together and return the result via JSON.\n\t\t\n\t\tPython 3 function annotations are used to ensure that the arguments are integers. This requires the\n\t\tfunctionality of `web.ext.annotation:AnnotationExtension`.\n\t\t\n\t\tThere are several ways to execute this method:\n\t\t\n\t\t* POST http://localhost:8080/mul\n\t\t* GET http://localhost:8080/mul?a=27&b=42\n\t\t* GET http://localhost:8080/mul/27/42\n\t\t\n\t\tThe latter relies on the fact we can't descend past a callable method so the remaining path elements are\n\t\tused as positional arguments, whereas the others rely on keyword argument assignment from a form-encoded\n\t\trequest body or query string arguments.  (Security note: any data in the request body takes presidence over\n\t\tquery string arguments!)\n\t\t\n\t\tYou can easily test these on the command line using cURL:\n\t\t\n\t\t\tcurl http://localhost:8080/mul/27/42  # HTTP GET\n\t\t\t\n\t\t\tcurl -d a=27 -d b=42 http://localhost:8080/mul  # HTTP POST\n\t\t\"\"\"\n\t\t\n\t\tif not a or not b:\n\t\t\treturn dict(message=\"Pass arguments a and b to multiply them together!\")\n\t\t\n\t\treturn dict(answer=a * b)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the colored string to print on the terminal.", "response": "def colorize(string, rgb=None, ansi=None, bg=None, ansi_bg=None, fd=1):\n    '''Returns the colored string to print on the terminal.\n\n    This function detects the terminal type and if it is supported and the\n    output is not going to a pipe or a file, then it will return the colored\n    string, otherwise it will return the string without modifications.\n\n    string = the string to print. Only accepts strings, unicode strings must\n             be encoded in advance.\n    rgb    = Rgb color for the text; for example 0xFF0000 is red.\n    ansi   = Ansi for the text\n    bg     = Rgb color for the background\n    ansi_bg= Ansi color for the background\n    fd     = The file descriptor that will be used by print, by default is the\n             stdout\n    '''\n\n    #Reinitializes if fd used is different\n    if colorize.fd != fd:\n        colorize.init = False\n        colorize.fd = fd\n\n    #Checks if it is on a terminal, and if the terminal is recognized\n    if not colorize.init:\n        colorize.init = True\n        colorize.is_term = isatty(fd)\n        if 'TERM' in environ:\n            if environ['TERM'].startswith('xterm'):\n                colorize.cmap = XTermColorMap()\n            elif environ['TERM'] == 'vt100':\n                colorize.cmap = VT100ColorMap()\n            else:\n                colorize.is_term = False\n        else:\n            colorize.is_term = False\n\n    if colorize.is_term:\n        string = colorize.cmap.colorize(string, rgb, ansi, bg, ansi_bg)\n\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mutate(self, context, handler, args, kw):\n\t\tdef cast(arg, val):\n\t\t\tif arg not in annotations:\n\t\t\t\treturn\n\t\t\t\n\t\t\tcast = annotations[key]\n\t\t\t\n\t\t\ttry:\n\t\t\t\tval = cast(val)\n\t\t\texcept (ValueError, TypeError) as e:\n\t\t\t\tparts = list(e.args)\n\t\t\t\tparts[0] = parts[0] + \" processing argument '{}'\".format(arg)\n\t\t\t\te.args = tuple(parts)\n\t\t\t\traise\n\t\t\t\n\t\t\treturn val\n\t\t\t\n\t\tannotations = getattr(handler.__func__ if hasattr(handler, '__func__') else handler, '__annotations__', None)\n\t\tif not annotations:\n\t\t\treturn\n\t\t\n\t\targspec = getfullargspec(handler)\n\t\targlist = list(argspec.args)\n\t\t\n\t\tif ismethod(handler):\n\t\t\tdel arglist[0]\n\t\t\n\t\tfor i, value in enumerate(list(args)):\n\t\t\tkey = arglist[i]\n\t\t\tif key in annotations:\n\t\t\t\targs[i] = cast(key, value)\n\t\t\n\t\t# Convert keyword arguments\n\t\tfor key, value in list(items(kw)):\n\t\t\tif key in annotations:\n\t\t\t\tkw[key] = cast(key, value)", "response": "Inspect and potentially mutate the given handler s arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, context, handler, result):\n\t\thandler = handler.__func__ if hasattr(handler, '__func__') else handler\n\t\tannotation = getattr(handler, '__annotations__', {}).get('return', None)\n\t\t\n\t\tif annotation:\n\t\t\treturn (annotation, result)\n\t\t\n\t\treturn result", "response": "Transform the value returned by the controller endpoint. This extension transforms returned values if the endpoint has a return type annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses command - line arguments and return a new object.", "response": "def process_args():\n    \"\"\" \n    Parse command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n                description=(\"A script which can be run to tune a NeuroML 2 model against a number of target properties. Work in progress!\"))\n\n                        \n    parser.add_argument('prefix', \n                        type=str,\n                        metavar='<prefix>', \n                        help=\"Prefix for optimisation run\")\n\n    parser.add_argument('neuromlFile', \n                        type=str,\n                        metavar='<neuromlFile>', \n                        help=\"NeuroML2 file containing model\")\n\n    parser.add_argument('target', \n                        type=str,\n                        metavar='<target>', \n                        help=\"Target in NeuroML2 model\")\n                        \n    parser.add_argument('parameters', \n                        type=str,\n                        metavar='<parameters>', \n                        help=\"List of parameter to adjust\")\n                        \n    parser.add_argument('maxConstraints', \n                        type=str,\n                        metavar='<max_constraints>', \n                        help=\"Max values for parameters\")\n                        \n    parser.add_argument('minConstraints', \n                        type=str,\n                        metavar='<min_constraints>', \n                        help=\"Min values for parameters\")\n                        \n    parser.add_argument('targetData', \n                        type=str,\n                        metavar='<targetData>', \n                        help=\"List of name/value pairs for properties extracted from data to judge fitness against\")\n                        \n    parser.add_argument('weights', \n                        type=str,\n                        metavar='<weights>', \n                        help=\"Weights to assign to each target name/value pair\")\n                        \n    parser.add_argument('-simTime', \n                        type=float,\n                        metavar='<simTime>', \n                        default=DEFAULTS['simTime'],\n                        help=\"Simulation duration\")\n                        \n    parser.add_argument('-dt', \n                        type=float,\n                        metavar='<dt>', \n                        default=DEFAULTS['dt'],\n                        help=\"Simulation timestep\")\n                        \n    parser.add_argument('-analysisStartTime', \n                        type=float,\n                        metavar='<analysisStartTime>', \n                        default=DEFAULTS['analysisStartTime'],\n                        help=\"Analysis start time\")\n                        \n    parser.add_argument('-populationSize', \n                        type=int,\n                        metavar='<populationSize>', \n                        default=DEFAULTS['populationSize'],\n                        help=\"Population size\")\n                        \n    parser.add_argument('-maxEvaluations', \n                        type=int,\n                        metavar='<maxEvaluations>', \n                        default=DEFAULTS['maxEvaluations'],\n                        help=\"Maximum evaluations\")\n                        \n    parser.add_argument('-numSelected', \n                        type=int,\n                        metavar='<numSelected>', \n                        default=DEFAULTS['numSelected'],\n                        help=\"Number selected\")\n                        \n    parser.add_argument('-numOffspring', \n                        type=int,\n                        metavar='<numOffspring>', \n                        default=DEFAULTS['numOffspring'],\n                        help=\"Number offspring\")\n                        \n            \n    parser.add_argument('-mutationRate', \n                        type=float,\n                        metavar='<mutationRate>', \n                        default=DEFAULTS['mutationRate'],\n                        help=\"Mutation rate\")\n                        \n    parser.add_argument('-numElites', \n                        type=int,\n                        metavar='<numElites>', \n                        default=DEFAULTS['numElites'],\n                        help=\"Number of elites\")\n                        \n    parser.add_argument('-numParallelEvaluations', \n                        type=int,\n                        metavar='<numParallelEvaluations>', \n                        default=DEFAULTS['numParallelEvaluations'],\n                        help=\"Number of evaluations to run in parallel\")\n                        \n    parser.add_argument('-seed', \n                        type=int,\n                        metavar='<seed>', \n                        default=DEFAULTS['seed'],\n                        help=\"Seed for optimiser\")\n                        \n    parser.add_argument('-simulator', \n                        type=str,\n                        metavar='<simulator>', \n                        default=DEFAULTS['simulator'],\n                        help=\"Simulator to run\")\n                        \n    parser.add_argument('-knownTargetValues', \n                        type=str,\n                        metavar='<knownTargetValues>', \n                        help=\"List of name/value pairs which represent the known values of the target parameters\")\n                        \n    parser.add_argument('-nogui', \n                        action='store_true',\n                        default=DEFAULTS['nogui'],\n                        help=\"Should GUI elements be supressed?\")\n                        \n    parser.add_argument('-showPlotAlready', \n                        action='store_true',\n                        default=DEFAULTS['showPlotAlready'],\n                        help=\"Should generated plots be suppressed until show() called?\")\n                        \n    parser.add_argument('-verbose', \n                        action='store_true',\n                        default=DEFAULTS['verbose'],\n                        help=\"Verbose mode\")\n                        \n    parser.add_argument('-dryRun', \n                        action='store_true',\n                        default=DEFAULTS['dryRun'],\n                        help=\"Dry run; just print setup information\")\n                        \n    parser.add_argument('-extraReportInfo', \n                        type=str,\n                        metavar='<extraReportInfo>', \n                        default=DEFAULTS['extraReportInfo'],\n                        help='Extra tag/value pairs can be put into the report.json:  -extraReportInfo=[\"tag\":\"value\"]')\n                        \n    parser.add_argument('-cleanup', \n                        action='store_true',\n                        default=DEFAULTS['cleanup'],\n                        help=\"Should (some) generated files, e.g. *.dat, be deleted as optimisation progresses?\")\n                        \n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_args():\n    parser = argparse.ArgumentParser(description=\"A file for overlaying POVRay files generated from NeuroML by NeuroML1ToPOVRay.py with cell activity (e.g. as generated from a neuroConstruct simulation)\")\n\n    parser.add_argument('prefix', \n                        type=str, \n                        metavar='<network prefix>', \n                        help='Prefix for files in PovRay, e.g. use PREFIX is files are PREFIX.pov, PREFIX_net.inc, etc.')\n                        \n    parser.add_argument('-activity',\n                        action='store_true',\n                        default=False,\n                        help=\"If this is specified, overlay network activity (not tested!!)\")\n\n\n    parser.add_argument('-maxV', \n                        type=float,\n                        metavar='<maxV>',\n                        default=50.0,\n                        help='Max voltage for colour scale in mV')\n\n    parser.add_argument('-minV', \n                        type=float,\n                        metavar='<minV>',\n                        default=-90.0,\n                        help='Min voltage for colour scale in mV')\n\n    parser.add_argument('-startTime', \n                        type=float,\n                        metavar='<startTime>',\n                        default=0,\n                        help='Time in ms at which to start overlaying the simulation activity')\n                        \n    parser.add_argument('-endTime', \n                        type=float,\n                        metavar='<endTime>',\n                        default=100,\n                        help='End time of simulation activity in ms')\n                        \n    parser.add_argument('-title', \n                        type=str, \n                        metavar='<title>', \n                        default='Movie generated from neuroConstruct simulation',\n                        help='Title for movie')\n                        \n    parser.add_argument('-left', \n                        type=str, \n                        metavar='<left info>', \n                        default='',\n                        help='Text on left')\n                        \n    parser.add_argument('-frames', \n                        type=int,\n                        metavar='<frames>',\n                        default=100,\n                        help='Number of frames')\n                        \n    parser.add_argument('-name', \n                        type=str, \n                        metavar='<Movie name>', \n                        default='output',\n                        help='Movie name')\n                        \n    return parser.parse_args()", "response": "Parse command - line arguments and return a list of the resulting objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing command line arguments", "response": "def parse_arguments():\n    \"\"\"Parse command line arguments\"\"\"\n\n    import argparse\n\n    parser = argparse.ArgumentParser(\n            description=('pyNeuroML v%s: Python utilities for NeuroML2' % __version__ \n                          + \"\\n    libNeuroML v%s\"%(neuroml.__version__)\n                          + \"\\n    jNeuroML v%s\"%JNEUROML_VERSION),\n            usage=('pynml [-h|--help] [<shared options>] '\n                   '<one of the mutually-exclusive options>'),\n            formatter_class=argparse.RawTextHelpFormatter\n            )\n\n    shared_options = parser.add_argument_group(\n            title='Shared options',\n            description=('These options can be added to any of the '\n                         'mutually-exclusive options')\n            )\n\n    shared_options.add_argument(\n            '-verbose',\n            action='store_true',\n            default=DEFAULTS['v'],\n            help='Verbose output'\n            )\n    shared_options.add_argument(\n            '-java_max_memory',\n            metavar='MAX',\n            default=DEFAULTS['default_java_max_memory'],\n            help=('Java memory for jNeuroML, e.g. 400M, 2G (used in\\n'\n                  '-Xmx argument to java)')\n            )\n    shared_options.add_argument(\n            '-nogui',\n            action='store_true',\n            default=DEFAULTS['nogui'],\n            help=('Suppress GUI,\\n'\n                  'i.e. show no plots, just save results')\n            )\n            \n    shared_options.add_argument(\n            'lems_file',\n            type=str,\n            metavar='<LEMS/NeuroML 2 file>',\n            help='LEMS/NeuroML 2 file to process'\n            )\n            \n    mut_exc_opts_grp = parser.add_argument_group(\n            title='Mutually-exclusive options',\n            description='Only one of these options can be selected'\n            )\n    mut_exc_opts = mut_exc_opts_grp.add_mutually_exclusive_group(required=False)\n     \n    mut_exc_opts.add_argument(\n            '-sedml',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert\\n'\n                  'simulation settings (duration, dt, what to save)\\n'\n                  'to SED-ML format')\n            )\n    mut_exc_opts.add_argument(\n            '-neuron',\n            nargs=argparse.REMAINDER,\n            help=('(Via jNeuroML) Load a LEMS file, and convert it to\\n'\n                  'NEURON format.\\n'\n                  'The full format of the \\'-neuron\\' option is:\\n'\n                  '-neuron [-nogui] [-run] [-outputdir dir] <LEMS file>\\n'\n                  '    -nogui\\n'\n                  '        do not generate gtaphical elements in NEURON,\\n'\n                  '        just run, save data, and quit\\n'\n                  '    -run\\n'\n                  '        compile NMODL files and run the main NEURON\\n'\n                  '        hoc file (Linux only currently)\\n'\n                  '    -outputdir <dir>\\n'\n                  '        generate NEURON files in directory <dir>\\n'\n                  '    <LEMS file>\\n'\n                  '        the LEMS file to use')\n            )\n    mut_exc_opts.add_argument(\n            '-svg',\n            action='store_true',\n            help=('(Via jNeuroML) Convert NeuroML2 file (network & cells)\\n'\n                  'to SVG format view of 3D structure')\n            )\n    mut_exc_opts.add_argument(\n            '-png',\n            action='store_true',\n            help=('(Via jNeuroML) Convert NeuroML2 file (network & cells)\\n'\n                  'to PNG format view of 3D structure')\n            )\n    mut_exc_opts.add_argument(\n            '-dlems',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to dLEMS format, a distilled form of LEMS in JSON')\n            )\n    mut_exc_opts.add_argument(\n            '-vertex',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to VERTEX format')\n            )\n    mut_exc_opts.add_argument(\n            '-xpp',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to XPPAUT format')\n            )\n    mut_exc_opts.add_argument(\n            '-dnsim',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to DNsim format')\n            )\n    mut_exc_opts.add_argument(\n            '-brian',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to Brian format')\n            )\n    mut_exc_opts.add_argument(\n            '-sbml',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to SBML format')\n            )\n    mut_exc_opts.add_argument(\n            '-matlab',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to MATLAB format')\n            )\n    mut_exc_opts.add_argument(\n            '-cvode',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to C format using CVODE package')\n            )\n    mut_exc_opts.add_argument(\n            '-nineml',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to NineML format')\n            )\n    mut_exc_opts.add_argument(\n            '-spineml',\n            action='store_true',\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to SpineML format')\n            )\n    mut_exc_opts.add_argument(\n            '-sbml-import',\n            metavar=('<SBML file>', 'duration', 'dt'),\n            nargs=3,\n            help=('(Via jNeuroML) Load a SBML file, and convert it\\n'\n                  'toLEMS format using values for duration & dt\\n'\n                  'in ms (ignoring SBML units)')\n            )\n    mut_exc_opts.add_argument(\n            '-sbml-import-units',\n            metavar=('<SBML file>', 'duration', 'dt'),\n            nargs=3,\n            help=('(Via jNeuroML) Load a SBML file, and convert it\\n'\n                  'to LEMS format using values for duration & dt\\n'\n                  'in ms (attempt to extract SBML units; ensure units\\n'\n                  'are valid in the SBML!)')\n            )\n    mut_exc_opts.add_argument(\n            '-vhdl',\n            metavar=('neuronid', '<LEMS file>'),\n            nargs=2,\n            help=('(Via jNeuroML) Load a LEMS file, and convert it\\n'\n                  'to VHDL format')\n            )\n    mut_exc_opts.add_argument(\n            '-graph',\n            metavar=('level'),\n            nargs=1,\n            help=('Load a NeuroML file, and convert it to a graph using\\n'\n                  'GraphViz. Detail is set by level (1, 2, etc.)')\n            )\n    mut_exc_opts.add_argument(\n            '-matrix',\n            metavar=('level'),\n            nargs=1,\n            help=('Load a NeuroML file, and convert it to a matrix displaying\\n'\n                  'connectivity. Detail is set by level (1, 2, etc.)')\n            )\n    mut_exc_opts.add_argument(\n            '-validate',\n            action='store_true',\n            help=('(Via jNeuroML) Validate NeuroML2 file(s) against the\\n'\n                  'latest Schema')\n            )\n    mut_exc_opts.add_argument(\n            '-validatev1',\n            action='store_true',\n            help=('(Via jNeuroML) Validate NeuroML file(s) against the\\n'\n                  'v1.8.1 Schema')\n            )\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string that can be used to display a summary of the contents of a NeuroML 2 document.", "response": "def quick_summary(nml2_doc):\n    '''\n    Or better just use nml2_doc.summary(show_includes=False)\n    '''\n    \n    info = 'Contents of NeuroML 2 document: %s\\n'%nml2_doc.id\n    membs = inspect.getmembers(nml2_doc)\n\n    for memb in membs:\n\n        if isinstance(memb[1], list) and len(memb[1])>0 \\\n                and not memb[0].endswith('_'):\n            info+='  %s:\\n    ['%memb[0]\n            for entry in memb[1]:\n                extra = '???'\n                extra = entry.name if hasattr(entry,'name') else extra\n                extra = entry.href if hasattr(entry,'href') else extra\n                extra = entry.id if hasattr(entry,'id') else extra\n                \n                info+=\" %s (%s),\"%(entry, extra)\n            \n            info+=']\\n'\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a command in a specific working directory", "response": "def execute_command_in_dir(command, directory, verbose=DEFAULTS['v'], \n                           prefix=\"Output: \", env=None):\n    \n    \"\"\"Execute a command in specific working directory\"\"\"\n    \n    if os.name == 'nt':\n        directory = os.path.normpath(directory)\n        \n    print_comment(\"Executing: (%s) in directory: %s\" % (command, directory),\n                  verbose)\n    if env is not None:\n        print_comment(\"Extra env variables %s\" % (env), verbose)\n    \n    try:\n        if os.name == 'nt':\n            return_string = subprocess.check_output(command, \n                                                    cwd=directory, \n                                                    shell=True, \n                                                    env=env,\n                                                    close_fds=False)\n        else:\n            return_string = subprocess.check_output(command, \n                                                    cwd=directory, \n                                                    shell=True, \n                                                    stderr=subprocess.STDOUT,\n                                                    env=env,\n                                                    close_fds=True)\n        \n        return_string = return_string.decode(\"utf-8\") # For Python 3\n                                \n        print_comment('Command completed. Output: \\n %s%s' % \\\n                      (prefix,return_string.replace('\\n','\\n '+prefix)), \n                      verbose)\n\n        return return_string\n    \n    except AttributeError:\n        # For python 2.6...\n        print_comment_v('Assuming Python 2.6...')\n        \n        return_string = subprocess.Popen(command, \n                                         cwd=directory, \n                                         shell=True,\n                                         stdout=subprocess.PIPE).communicate()[0]\n        return return_string\n    \n    except subprocess.CalledProcessError as e:        \n        \n        print_comment_v('*** Problem running command: \\n       %s'%e)\n        print_comment_v('%s%s'%(prefix,e.output.decode().replace('\\n','\\n'+prefix)))\n        \n        return None\n        \n    except:\n        print_comment_v('*** Unknown problem running command: %s'%e)\n        \n        return None\n        \n    print_comment(\"Finished execution\", verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate_component(comp_type, req_variables={}, parameter_values={}):\n    \n    print_comment('Evaluating %s with req:%s; params:%s'%(comp_type.name,req_variables,parameter_values))\n    exec_str = ''\n    return_vals = {}\n    from math import exp\n    for p in parameter_values:\n        exec_str+='%s = %s\\n'%(p, get_value_in_si(parameter_values[p]))\n    for r in req_variables:\n        exec_str+='%s = %s\\n'%(r, get_value_in_si(req_variables[r]))\n    for c in comp_type.Constant:\n        exec_str+='%s = %s\\n'%(c.name, get_value_in_si(c.value))\n    for d in comp_type.Dynamics:\n        for dv in d.DerivedVariable:\n            exec_str+='%s = %s\\n'%(dv.name, dv.value)\n            exec_str+='return_vals[\"%s\"] = %s\\n'%(dv.name, dv.name)\n        for cdv in d.ConditionalDerivedVariable:\n            for case in cdv.Case:\n                if case.condition:\n                    cond = case.condition.replace('.neq.','!=').replace('.eq.','==').replace('.gt.','<').replace('.lt.','<')\n                    exec_str+='if ( %s ): %s = %s \\n'%(cond, cdv.name, case.value)\n                else:\n                    exec_str+='else: %s = %s \\n'%(cdv.name, case.value)\n                \n            exec_str+='\\n'\n                \n            exec_str+='return_vals[\"%s\"] = %s\\n'%(cdv.name, cdv.name)\n          \n    '''print_comment_v(exec_str)'''\n    exec(exec_str)\n    \n    return return_vals", "response": "Evaluate the component with the given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef after(self, context, exc=None):\n\t\t\n\t\tduration = context._duration = round((time.time() - context._start_time) * 1000)  # Convert to ms.\n\t\tdelta = unicode(duration)\n\t\t\n\t\t# Default response augmentation.\n\t\tif self.header:\n\t\t\tcontext.response.headers[self.header] = delta\n\t\t\n\t\tif self.log:\n\t\t\tself.log(\"Response generated in \" + delta + \" seconds.\", extra=dict(\n\t\t\t\t\tduration = duration,\n\t\t\t\t\trequest = id(context)\n\t\t\t\t))", "response": "Executed after dispatch has returned and the response populated prior to any other request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a flat namespace transformation to recreate a rich structure.", "response": "def _process_flat_kwargs(source, kwargs):\n\t\t\"\"\"Apply a flat namespace transformation to recreate (in some respects) a rich structure.\n\t\t\n\t\tThis applies several transformations, which may be nested:\n\t\t\n\t\t`foo` (singular): define a simple value named `foo`\n\t\t`foo` (repeated): define a simple value for placement in an array named `foo`\n\t\t`foo[]`: define a simple value for placement in an array, even if there is only one\n\t\t`foo.<id>`: define a simple value to place in the `foo` array at the identified index\n\t\t\n\t\tBy nesting, you may define deeper, more complex structures:\n\t\t\n\t\t`foo.bar`: define a value for the named element `bar` of the `foo` dictionary\n\t\t`foo.<id>.bar`: define a `bar` dictionary element on the array element marked by that ID\n\t\t\n\t\tReferences to `<id>` represent numeric \"attributes\", which makes the parent reference be treated as an array,\n\t\tnot a dictionary. Exact indexes might not be able to be preserved if there are voids; Python lists are not\n\t\tsparse.\n\t\t\n\t\tNo validation of values is performed.\n\t\t\"\"\"\n\t\t\n\t\tordered_arrays = []\n\t\t\n\t\t# Process arguments one at a time and apply them to the kwargs passed in.\n\t\t\n\t\tfor name, value in source.items():\n\t\t\tcontainer = kwargs\n\t\t\t\n\t\t\tif '.' in name:\n\t\t\t\tparts = name.split('.')\n\t\t\t\tname = name.rpartition('.')[2]\n\t\t\t\t\n\t\t\t\tfor target, following in zip(parts[:-1], parts[1:]):\n\t\t\t\t\tif following.isnumeric():  # Prepare any use of numeric IDs.\n\t\t\t\t\t\tcontainer.setdefault(target, [{}])\n\t\t\t\t\t\tif container[target] not in ordered_arrays:\n\t\t\t\t\t\t\tordered_arrays.append(container[target])\n\t\t\t\t\t\tcontainer = container[target][0]\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\n\t\t\t\t\tcontainer = container.setdefault(target, {})\n\t\t\t\n\t\t\tif name.endswith('[]'):  # `foo[]` or `foo.bar[]` etc.\n\t\t\t\tname = name[:-2]\n\t\t\t\tcontainer.setdefault(name, [])\n\t\t\t\tcontainer[name].append(value)\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif name.isnumeric() and container is not kwargs:  # trailing identifiers, `foo.<id>`\n\t\t\t\tcontainer[int(name)] = value\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tif name in container:\n\t\t\t\tif not isinstance(container[name], list):\n\t\t\t\t\tcontainer[name] = [container[name]]\n\t\t\t\t\n\t\t\t\tcontainer[name].append(value)\n\t\t\t\tcontinue\n\t\t\t\n\t\t\tcontainer[name] = value\n\t\t\n\t\tfor container in ordered_arrays:\n\t\t\telements = container[0]\n\t\t\tdel container[:]\n\t\t\tcontainer.extend(value for name, value in sorted(elements.items()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_args():\r\n    parser = argparse.ArgumentParser(description=\"A file for converting NeuroML v2 files into POVRay files for 3D rendering\")\r\n\r\n    parser.add_argument('neuroml_file', type=str, metavar='<NeuroML file>', \r\n                        help='NeuroML (version 2 beta 3+) file to be converted to PovRay format (XML or HDF5 format)')\r\n                        \r\n    parser.add_argument('-split',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, generate separate pov files for cells & network. Default is false\")\r\n\r\n\r\n    parser.add_argument('-background', \r\n                        type=str,\r\n                        metavar='<background colour>',\r\n                        default=_WHITE,\r\n                        help='Colour of background, e.g. <0,0,0,0.55>')\r\n\r\n    parser.add_argument('-movie',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, generate a ini file for generating a sequence of frames for a movie of the 3D structure\")\r\n                        \r\n    parser.add_argument('-inputs',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, show the locations of (synaptic, current clamp, etc.) inputs into the cells of the network\")\r\n                        \r\n    parser.add_argument('-conns',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, show the connections present in the network with lines\")\r\n                        \r\n    parser.add_argument('-conn_points',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, show the end points of the connections present in the network\")\r\n                        \r\n    parser.add_argument('-v',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"Verbose output\")\r\n\r\n    parser.add_argument('-frames', \r\n                        type=int,\r\n                        metavar='<frames>',\r\n                        default=36,\r\n                        help='Number of frames in movie')\r\n                        \r\n    parser.add_argument('-posx', \r\n                        type=float,\r\n                        metavar='<position offset x>',\r\n                        default=0,\r\n                        help='Offset position in x dir (0 is centre, 1 is top)')\r\n    parser.add_argument('-posy', \r\n                        type=float,\r\n                        metavar='<position offset y>',\r\n                        default=0,\r\n                        help='Offset position in y dir (0 is centre, 1 is top)')\r\n    parser.add_argument('-posz', \r\n                        type=float,\r\n                        metavar='<position offset z>',\r\n                        default=0,\r\n                        help='Offset position in z dir (0 is centre, 1 is top)')\r\n                        \r\n    parser.add_argument('-viewx', \r\n                        type=float,\r\n                        metavar='<view offset x>',\r\n                        default=0,\r\n                        help='Offset viewing point in x dir (0 is centre, 1 is top)')\r\n    parser.add_argument('-viewy', \r\n                        type=float,\r\n                        metavar='<view offset y>',\r\n                        default=0,\r\n                        help='Offset viewing point in y dir (0 is centre, 1 is top)')\r\n    parser.add_argument('-viewz', \r\n                        type=float,\r\n                        metavar='<view offset z>',\r\n                        default=0,\r\n                        help='Offset viewing point in z dir (0 is centre, 1 is top)')\r\n\r\n    parser.add_argument('-scalex', \r\n                        type=float,\r\n                        metavar='<scale position x>',\r\n                        default=1,\r\n                        help='Scale position from network in x dir')\r\n    parser.add_argument('-scaley', \r\n                        type=float,\r\n                        metavar='<scale position y>',\r\n                        default=1.5,\r\n                        help='Scale position from network in y dir')\r\n    parser.add_argument('-scalez', \r\n                        type=float,\r\n                        metavar='<scale position z>',\r\n                        default=1,\r\n                        help='Scale position from network in z dir')\r\n\r\n    parser.add_argument('-mindiam', \r\n                        type=float,\r\n                        metavar='<minimum diameter dendrites/axons>',\r\n                        default=0,\r\n                        help='Minimum diameter for dendrites/axons (to improve visualisations)')\r\n\r\n    parser.add_argument('-plane',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"If this is specified, add a 2D plane below cell/network\")\r\n\r\n    parser.add_argument('-segids',\r\n                        action='store_true',\r\n                        default=False,\r\n                        help=\"Show segment ids\")\r\n    \r\n    return parser.parse_args()", "response": "Parse command - line arguments and return a list of arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares the incoming configuration and ensure certain expected values are present.", "response": "def _configure(self, config):\n\t\t\"\"\"Prepare the incoming configuration and ensure certain expected values are present.\n\t\t\n\t\tFor example, this ensures BaseExtension is included in the extension list, and populates the logging config.\n\t\t\"\"\"\n\t\tconfig = config or dict()\n\t\t\n\t\t# We really need this to be there.\n\t\tif 'extensions' not in config: config['extensions'] = list()\n\t\t\n\t\tif not any(isinstance(ext, BaseExtension) for ext in config['extensions']):\n\t\t\t# Always make sure the BaseExtension is present since request/response objects are handy.\n\t\t\tconfig['extensions'].insert(0, BaseExtension())\n\t\t\n\t\tif not any(isinstance(ext, arguments.ArgumentExtension) for ext in config['extensions']):\n\t\t\t# Prepare a default set of argument mutators.\n\t\t\tconfig['extensions'].extend([\n\t\t\t\t\targuments.ValidateArgumentsExtension(),\n\t\t\t\t\targuments.ContextArgsExtension(),\n\t\t\t\t\targuments.RemainderArgsExtension(),\n\t\t\t\t\targuments.QueryStringArgsExtension(),\n\t\t\t\t\targuments.FormEncodedKwargsExtension(),\n\t\t\t\t\targuments.JSONKwargsExtension(),\n\t\t\t\t])\n\t\t\n\t\tconfig['extensions'].append(self)  # Allow the application object itself to register callbacks.\n\t\t\n\t\ttry:\n\t\t\taddLoggingLevel('trace', logging.DEBUG - 5)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\t\n\t\t# Tests are skipped on these as we have no particular need to test Python's own logging mechanism.\n\t\tlevel = config.get('logging', {}).get('level', None)\n\t\tif level:  # pragma: no cover\n\t\t\tlogging.basicConfig(level=getattr(logging, level.upper()))\n\t\telif 'logging' in config:  # pragma: no cover\n\t\t\tlogging.config.dictConfig(config['logging'])\n\t\t\n\t\treturn config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serve(self, service='auto', **options):  # pragma: no cover\n\t\t\n\t\tservice = load(service, 'web.server')  # We don't bother with a full registry for these one-time lookups.\n\t\t\n\t\ttry:\n\t\t\tservice(self, **options)\n\t\texcept KeyboardInterrupt:  # We catch this as SIG_TERM or ^C are basically the only ways to stop most servers.\n\t\t\tpass\n\t\t\n\t\t# Notify extensions that the service has returned and we are exiting.\n\t\tfor ext in self.__context.extension.signal.stop: ext(self.__context)", "response": "Initiate a web server service to serve this application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a single WSGI request/response cycle. This is the WSGI handler for WebCore. Depending on the presence of extensions providing WSGI middleware, the `__call__` attribute of the Application instance will either become this, or become the outermost middleware callable. Most apps won't utilize middleware, the extension interface is preferred for most operations in WebCore. They allow for code injection at various intermediary steps in the processing of a request and response.", "response": "def application(self, environ, start_response):\n\t\t\"\"\"Process a single WSGI request/response cycle.\n\t\t\n\t\tThis is the WSGI handler for WebCore.  Depending on the presence of extensions providing WSGI middleware,\n\t\tthe `__call__` attribute of the Application instance will either become this, or become the outermost\n\t\tmiddleware callable.\n\t\t\n\t\tMost apps won't utilize middleware, the extension interface is preferred for most operations in WebCore.\n\t\tThey allow for code injection at various intermediary steps in the processing of a request and response.\n\t\t\"\"\"\n\t\tcontext = environ['wc.context'] = self.RequestContext(environ=environ)\n\t\tsignals = context.extension.signal\n\t\t\n\t\t# Announce the start of a request cycle. This executes `prepare` and `before` callbacks in the correct order.\n\t\tfor ext in signals.pre: ext(context)\n\t\t\n\t\t# Identify the endpoint for this request.\n\t\tis_endpoint, handler = context.dispatch(context, context.root, context.environ['PATH_INFO'])\n\t\t\n\t\tif is_endpoint:\n\t\t\ttry:\n\t\t\t\tresult = self._execute_endpoint(context, handler, signals)  # Process the endpoint.\n\t\t\texcept Exception as e:\n\t\t\t\tlog.exception(\"Caught exception attempting to execute the endpoint.\")\n\t\t\t\tresult = HTTPInternalServerError(str(e) if __debug__ else \"Please see the logs.\")\n\t\t\t\t\n\t\t\t\tif 'debugger' in context.extension.feature:\n\t\t\t\t\tcontext.response = result\n\t\t\t\t\tfor ext in signals.after: ext(context)  # Allow signals to clean up early.\n\t\t\t\t\traise\n\t\t\n\t\telse:  # If no endpoint could be resolved, that's a 404.\n\t\t\tresult = HTTPNotFound(\"Dispatch failed.\" if __debug__ else None)\n\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"Result prepared, identifying view handler.\", extra=dict(\n\t\t\t\t\trequest = id(context),\n\t\t\t\t\tresult = safe_name(type(result))\n\t\t\t\t))\n\t\t\n\t\t# Identify a view capable of handling this result.\n\t\tfor view in context.view(result):\n\t\t\tif view(context, result): break\n\t\telse:\n\t\t\t# We've run off the bottom of the list of possible views.\n\t\t\traise TypeError(\"No view could be found to handle: \" + repr(type(result)))\n\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"View identified, populating response.\", extra=dict(\n\t\t\t\t\trequest = id(context),\n\t\t\t\t\tview = repr(view),\n\t\t\t\t))\n\t\t\n\t\tfor ext in signals.after: ext(context)\n\t\t\n\t\tdef capture_done(response):\n\t\t\tfor chunk in response:\n\t\t\t\tyield chunk\n\t\t\t\n\t\t\tfor ext in signals.done: ext(context)\n\t\t\n\t\t# This is really long due to the fact we don't want to capture the response too early.\n\t\t# We need anything up to this point to be able to simply replace `context.response` if needed.\n\t\treturn capture_done(context.response.conditional_response_app(environ, start_response))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _swap(self):\n        '''Swaps the alignment so that the reference becomes the query and vice-versa. Swaps their names, coordinates etc. The frame is not changed'''\n        self.ref_start, self.qry_start = self.qry_start, self.ref_start\n        self.ref_end, self.qry_end = self.qry_end, self.ref_end\n        self.hit_length_ref, self.hit_length_qry = self.hit_length_qry, self.hit_length_ref\n        self.ref_length, self.qry_length = self.qry_length, self.ref_length\n        self.ref_name, self.qry_name = self.qry_name, self.ref_name", "response": "Swaps the alignment so that the reference becomes the query and vice - versa."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a pyfastaq. intervals. Interval object of the start and end coordinates in the query sequence", "response": "def qry_coords(self):\n        '''Returns a pyfastaq.intervals.Interval object of the start and end coordinates in the query sequence'''\n        return pyfastaq.intervals.Interval(min(self.qry_start, self.qry_end), max(self.qry_start, self.qry_end))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pyfastaq. intervals. Interval object of the start and end coordinates in the reference sequence", "response": "def ref_coords(self):\n        '''Returns a pyfastaq.intervals.Interval object of the start and end coordinates in the reference sequence'''\n        return pyfastaq.intervals.Interval(min(self.ref_start, self.ref_end), max(self.ref_start, self.ref_end))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true iff the direction of the alignment is the same in the reference and the query", "response": "def on_same_strand(self):\n        '''Returns true iff the direction of the alignment is the same in the reference and the query'''\n        return (self.ref_start < self.ref_end) == (self.qry_start < self.qry_end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_self_hit(self):\n        '''Returns true iff the alignment is of a sequence to itself: names and all coordinates are the same and 100 percent identity'''\n        return self.ref_name == self.qry_name \\\n                and self.ref_start == self.qry_start \\\n                and self.ref_end == self.qry_end \\\n                and self.percent_identity == 100", "response": "Returns true iff the alignment is of a sequence to itself"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges the coordinates as if the query sequence has been reverse complemented", "response": "def reverse_query(self):\n        '''Changes the coordinates as if the query sequence has been reverse complemented'''\n        self.qry_start = self.qry_length - self.qry_start - 1\n        self.qry_end = self.qry_length - self.qry_end - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the coordinates as if the reference sequence has been reversed complemented", "response": "def reverse_reference(self):\n        '''Changes the coordinates as if the reference sequence has been reverse complemented'''\n        self.ref_start = self.ref_length - self.ref_start - 1\n        self.ref_end = self.ref_length - self.ref_end - 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the alignment as a line in MSPcrunch format.", "response": "def to_msp_crunch(self):\n        '''Returns the alignment as a line in MSPcrunch format. The columns are space-separated and are:\n           1. score\n           2. percent identity\n           3. match start in the query sequence\n           4. match end in the query sequence\n           5. query sequence name\n           6. subject sequence start\n           7. subject sequence end\n           8. subject sequence name'''\n\n        # we don't know the alignment score. Estimate it. This approximates 1 for a match.\n        aln_score = int(self.percent_identity * 0.005 * (self.hit_length_ref + self.hit_length_qry))\n\n        return ' '.join(str(x) for x in [\n                aln_score,\n                '{0:.2f}'.format(self.percent_identity),\n                self.qry_start + 1,\n                self.qry_end + 1,\n                self.qry_name,\n                self.ref_start + 1,\n                self.ref_end + 1,\n                self.ref_name\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the nucmer command", "response": "def _nucmer_command(self, ref, qry, outprefix):\n        '''Construct the nucmer command'''\n        if self.use_promer:\n            command = 'promer'\n        else:\n            command = 'nucmer'\n\n        command += ' -p ' + outprefix\n\n        if self.breaklen is not None:\n            command += ' -b ' + str(self.breaklen)\n\n        if self.diagdiff is not None and not self.use_promer:\n            command += ' -D ' + str(self.diagdiff)\n\n        if self.diagfactor:\n            command += ' -d ' + str(self.diagfactor)\n\n        if self.maxgap:\n            command += ' -g ' + str(self.maxgap)\n\n        if self.maxmatch:\n            command += ' --maxmatch'\n\n        if self.mincluster is not None:\n            command += ' -c ' + str(self.mincluster)\n\n        if not self.simplify and not self.use_promer:\n        \tcommand += ' --nosimplify'\n\n        return command + ' ' + ref + ' ' + qry"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing delta - filter command", "response": "def _delta_filter_command(self, infile, outfile):\n        '''Construct delta-filter command'''\n        command = 'delta-filter'\n\n        if self.min_id is not None:\n            command += ' -i ' + str(self.min_id)\n\n        if self.min_length is not None:\n            command += ' -l ' + str(self.min_length)\n\n        return command + ' ' + infile + ' > ' + outfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs show - coords command", "response": "def _show_coords_command(self, infile, outfile):\n        '''Construct show-coords command'''\n        command = 'show-coords -dTlro'\n\n        if not self.coords_header:\n            command += ' -H'\n\n        return command + ' ' + infile + ' > ' + outfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting commands into a bash script", "response": "def _write_script(self, script_name, ref, qry, outfile):\n        '''Write commands into a bash script'''\n        f = pyfastaq.utils.open_file_write(script_name)\n        print(self._nucmer_command(ref, qry, 'p'), file=f)\n        print(self._delta_filter_command('p.delta', 'p.delta.filter'), file=f)\n        print(self._show_coords_command('p.delta.filter', outfile), file=f)\n        if self.show_snps:\n            print(self._show_snps_command('p.delta.filter', outfile + '.snps'), file=f)\n        pyfastaq.utils.close(f)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning bash script containing commands and place results in specified output file", "response": "def run(self):\n        '''\n        Change to a temp directory\n        Run bash script containing commands\n        Place results in specified output file\n        Clean up temp directory\n        '''\n        qry = os.path.abspath(self.qry)\n        ref = os.path.abspath(self.ref)\n        outfile = os.path.abspath(self.outfile)\n        tmpdir = tempfile.mkdtemp(prefix='tmp.run_nucmer.', dir=os.getcwd())\n        original_dir = os.getcwd()\n        os.chdir(tmpdir)\n        script = 'run_nucmer.sh'\n        self._write_script(script, ref, qry, outfile)\n        syscall.run('bash ' + script, verbose=self.verbose)\n        os.chdir(original_dir)\n        shutil.rmtree(tmpdir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a coords file to a crunch file in MSPcrunch format.", "response": "def convert_to_msp_crunch(infile, outfile, ref_fai=None, qry_fai=None):\n    '''Converts a coords file to a file in MSPcrunch format (for use with ACT, most likely).\n       ACT ignores sequence names in the crunch file, and just looks at the numbers.\n       To make a compatible file, the coords all must be shifted appropriately, which\n       can be done by providing both the ref_fai and qry_fai options.\n       Both or neither of these must be used, otherwise an error will be thrown.'''\n    fai_files = {ref_fai, qry_fai}\n    if None in fai_files and len(fai_files) != 1:\n       print(fai_files)\n       raise Error('Error in convert_to_msp_crunch. Must use both of ref_fai and qry_fai, or neither of them')\n\n    if ref_fai is not None:\n        assert qry_fai is not None\n        ref_offsets = pyfastaq.tasks.length_offsets_from_fai(ref_fai)\n        qry_offsets = pyfastaq.tasks.length_offsets_from_fai(qry_fai)\n    \n    file_reader = reader(infile)\n    f_out = pyfastaq.utils.open_file_write(outfile)\n\n    for aln in file_reader:\n        if ref_fai is not None:\n           aln.ref_start += ref_offsets[aln.ref_name] \n           aln.ref_end += ref_offsets[aln.ref_name] \n           aln.qry_start += qry_offsets[aln.qry_name] \n           aln.qry_end += qry_offsets[aln.qry_name] \n\n        print(aln.to_msp_crunch(), file=f_out)\n\n    pyfastaq.utils.close(f_out)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _request(self, method, url, params=None, headers=None, data=None):\n        if not params:\n            params = {}\n\n        # set default headers\n        if not headers:\n            headers = {\n                'accept': '*/*'\n            }\n            if method == 'POST' or method == 'PUT':\n                headers.update({'Content-Type': 'application/json'})\n        try:\n            response = requests.request(method=method, url=self.host + self.key + url, params=params,\n                                        headers=headers, data=data)\n\n            try:\n                response.raise_for_status()\n\n                response_code = response.status_code\n                success = True if response_code // 100 == 2 else False\n                if response.text:\n                    try:\n                        data = response.json()\n                    except ValueError:\n                        data = response.content\n                else:\n                    data = ''\n\n                response_headers = response.headers\n\n                return BurpResponse(success=success, response_code=response_code, data=data,\n                                    response_headers=response_headers)\n            except ValueError as e:\n                return BurpResponse(success=False, message=\"JSON response could not be decoded {}.\".format(e))\n            except requests.exceptions.HTTPError as e:\n                if response.status_code == 400:\n                    return BurpResponse(success=False, response_code=400, message='Bad Request')\n                else:\n                    return BurpResponse(\n                        message='There was an error while handling the request. {}'.format(response.content),\n                        success=False)\n        except Exception as e:\n            return BurpResponse(success=False, message='Eerror is %s' % e)", "response": "Common handler for all the HTTP requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a sphinx role for linking to a user profile.", "response": "def user_role(name, rawtext, text, lineno, inliner, options=None, content=None):\n    \"\"\"Sphinx role for linking to a user profile. Defaults to linking to\n    Github profiles, but the profile URIS can be configured via the\n    ``issues_user_uri`` config value.\n\n    Examples: ::\n\n        :user:`sloria`\n\n    Anchor text also works: ::\n\n        :user:`Steven Loria <sloria>`\n    \"\"\"\n    options = options or {}\n    content = content or []\n    has_explicit_title, title, target = split_explicit_title(text)\n\n    target = utils.unescape(target).strip()\n    title = utils.unescape(title).strip()\n    config = inliner.document.settings.env.app.config\n    if config.issues_user_uri:\n        ref = config.issues_user_uri.format(user=target)\n    else:\n        ref = \"https://github.com/{0}\".format(target)\n    if has_explicit_title:\n        text = title\n    else:\n        text = \"@{0}\".format(target)\n\n    link = nodes.reference(text=text, refuri=ref, **options)\n    return [link], []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cve_role(name, rawtext, text, lineno, inliner, options=None, content=None):\n    options = options or {}\n    content = content or []\n    has_explicit_title, title, target = split_explicit_title(text)\n\n    target = utils.unescape(target).strip()\n    title = utils.unescape(title).strip()\n    ref = \"https://cve.mitre.org/cgi-bin/cvename.cgi?name={0}\".format(target)\n    text = title if has_explicit_title else target\n    link = nodes.reference(text=text, refuri=ref, **options)\n    return [link], []", "response": "Sphinx role for linking to a CVE on https://cve. mitre. org."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the given line of items on the same line.", "response": "def list_line(self, line):\n        \"\"\"\n        Write the given iterable of values (line) to the file as items on the \n        same line. Any argument that stringifies to a string legal as a TSV data\n        item can be written.\n        \n        Does not copy the line or build a big string in memory.\n        \"\"\"\n        \n        if len(line) == 0:\n            return\n        \n        self.stream.write(str(line[0]))\n        \n        for item in line[1:]:\n            self.stream.write(\"\\t\")\n            self.stream.write(str(item))\n        \n        self.stream.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing metadata to obtain list of mustache templates and load those templates.", "response": "def prepare(doc):\n    \"\"\" Parse metadata to obtain list of mustache templates,\n        then load those templates.\n    \"\"\"\n    doc.mustache_files = doc.get_metadata('mustache')\n    if isinstance(doc.mustache_files, basestring):  # process single YAML value stored as string\n        if not doc.mustache_files:\n            doc.mustache_files = None  # switch empty string back to None\n        else:\n            doc.mustache_files = [ doc.mustache_files ]  # put non-empty string in list\n    # with open('debug.txt', 'a') as the_file:\n    #     the_file.write(str(doc.mustache_files))\n    #     the_file.write('\\n')\n    if doc.mustache_files is not None:\n        doc.mustache_hashes = [yaml.load(open(file, 'r').read()) for file in doc.mustache_files]\n        doc.mhash = { k: v for mdict in doc.mustache_hashes for k, v in mdict.items() }  # combine list of dicts into a single dict\n        doc.mrenderer = pystache.Renderer(escape=lambda u: u, missing_tags='strict')\n    else:\n        doc.mhash = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef action(elem, doc):\n    if type(elem) == Str and doc.mhash is not None:\n        elem.text = doc.mrenderer.render(elem.text, doc.mhash)\n        return elem", "response": "Apply combined mustache template to all strings in document.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_callback(self, renderer_context):\n        request = renderer_context.get('request', None)\n        params = request and get_query_params(request) or {}\n        return params.get(self.callback_parameter, self.default_callback)", "response": "Determine the name of the callback to wrap around the json output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render(self, data, accepted_media_type=None, renderer_context=None):\n        renderer_context = renderer_context or {}\n        callback = self.get_callback(renderer_context)\n        json = super(JSONPRenderer, self).render(data, accepted_media_type,\n                                                 renderer_context)\n        return callback.encode(self.charset) + b'(' + json + b');'", "response": "Renders into jsonp wrapping the json output in a callback function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, measurementId):\n        logger.info('Loading raw data for ' + measurementId)\n        measurement = self._measurementController.getMeasurement(measurementId, MeasurementStatus.COMPLETE)\n        if measurement is not None:\n            if measurement.inflate():\n                data = {\n                    name: {\n                        'raw': {\n                            'x': self._jsonify(data.raw('x')),\n                            'y': self._jsonify(data.raw('y')),\n                            'z': self._jsonify(data.raw('z'))\n                        },\n                        'vibration': {\n                            'x': self._jsonify(data.vibration('x')),\n                            'y': self._jsonify(data.vibration('y')),\n                            'z': self._jsonify(data.vibration('z'))\n                        },\n                        'tilt': {\n                            'x': self._jsonify(data.tilt('x')),\n                            'y': self._jsonify(data.tilt('y')),\n                            'z': self._jsonify(data.tilt('z'))\n                        }\n                    }\n                    for name, data in measurement.data.items()\n                }\n                return data, 200\n            else:\n                return None, 404\n        else:\n            return None, 404", "response": "Returns the raw data for the given measurementId."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn Julian day count of given ISO year week and day", "response": "def to_jd(year, week, day):\n    '''Return Julian day count of given ISO year, week, and day'''\n    return day + n_weeks(SUN, gregorian.to_jd(year - 1, 12, 28), week)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning tuple of ISO year week day", "response": "def from_jd(jd):\n    '''Return tuple of ISO (year, week, day) for Julian day'''\n    year = gregorian.from_jd(jd)[0]\n    day = jwday(jd) + 1\n\n    dayofyear = ordinal.from_jd(jd)[1]\n    week = trunc((dayofyear - day + 10) / 7)\n\n    # Reset year\n    if week < 1:\n        week = weeks_per_year(year - 1)\n        year = year - 1\n\n    # Check that year actually has 53 weeks\n    elif week == 53 and weeks_per_year(year) != 53:\n        week = 1\n        year = year + 1\n\n    return year, week, day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnumbering of ISO weeks in a year", "response": "def weeks_per_year(year):\n    '''Number of ISO weeks in a year'''\n    # 53 weeks: any year starting on Thursday and any leap year starting on Wednesday\n    jan1 = jwday(gregorian.to_jd(year, 1, 1))\n\n    if jan1 == THU or (jan1 == WED and isleap(year)):\n        return 53\n    else:\n        return 52"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the GEIS file and return a HDUList object.", "response": "def readgeis(input):\n\n    \"\"\"Input GEIS files \"input\" will be read and a HDUList object will\n       be returned.\n\n       The user can use the writeto method to write the HDUList object to\n       a FITS file.\n    \"\"\"\n\n    global dat\n    cardLen = fits.Card.length\n\n    # input file(s) must be of the form *.??h and *.??d\n    if input[-1] != 'h' or input[-4] != '.':\n        raise \"Illegal input GEIS file name %s\" % input\n\n    data_file = input[:-1]+'d'\n\n    _os = sys.platform\n    if _os[:5] == 'linux' or _os[:5] == 'win32' or _os[:5] == 'sunos' or _os[:3] == 'osf' or _os[:6] == 'darwin':\n        bytes_per_line = cardLen+1\n    else:\n        raise \"Platform %s is not supported (yet).\" % _os\n\n    geis_fmt = {'REAL':'f', 'INTEGER':'i', 'LOGICAL':'i','CHARACTER':'S'}\n    end_card = 'END'+' '* (cardLen-3)\n\n    # open input file\n    im = open(input)\n\n    # Generate the primary HDU\n    cards = []\n    while 1:\n        line = im.read(bytes_per_line)[:cardLen]\n        line = line[:8].upper() + line[8:]\n        if line == end_card:\n            break\n        cards.append(fits.Card.fromstring(line))\n\n    phdr = fits.Header(cards)\n    im.close()\n\n    _naxis0 = phdr.get('NAXIS', 0)\n    _naxis = [phdr['NAXIS'+str(j)] for j in range(1, _naxis0+1)]\n    _naxis.insert(0, _naxis0)\n    _bitpix = phdr['BITPIX']\n    _psize = phdr['PSIZE']\n    if phdr['DATATYPE'][:4] == 'REAL':\n        _bitpix = -_bitpix\n    if _naxis0 > 0:\n        size = reduce(lambda x,y:x*y, _naxis[1:])\n        data_size = abs(_bitpix) * size // 8\n    else:\n        data_size = 0\n    group_size = data_size + _psize // 8\n\n    # decode the group parameter definitions,\n    # group parameters will become extension header\n    groups = phdr['GROUPS']\n    gcount = phdr['GCOUNT']\n    pcount = phdr['PCOUNT']\n\n    formats = []\n    bools = []\n    floats = []\n    _range = range(1, pcount+1)\n    key = [phdr['PTYPE'+str(j)] for j in _range]\n    comm = [phdr.cards['PTYPE'+str(j)].comment for j in _range]\n\n    # delete group parameter definition header keywords\n    _list = ['PTYPE'+str(j) for j in _range] + \\\n            ['PDTYPE'+str(j) for j in _range] + \\\n            ['PSIZE'+str(j) for j in _range] + \\\n            ['DATATYPE', 'PSIZE', 'GCOUNT', 'PCOUNT', 'BSCALE', 'BZERO']\n\n    # Construct record array formats for the group parameters\n    # as interpreted from the Primary header file\n    for i in range(1, pcount+1):\n        ptype = key[i-1]\n        pdtype = phdr['PDTYPE'+str(i)]\n        star = pdtype.find('*')\n        _type = pdtype[:star]\n        _bytes = pdtype[star+1:]\n\n        # collect boolean keywords since they need special attention later\n\n        if _type == 'LOGICAL':\n            bools.append(i)\n        if pdtype == 'REAL*4':\n            floats.append(i)\n\n        fmt = geis_fmt[_type] + _bytes\n        formats.append((ptype,fmt))\n\n    _shape = _naxis[1:]\n    _shape.reverse()\n    _code = fits.BITPIX2DTYPE[_bitpix]\n    _bscale = phdr.get('BSCALE', 1)\n    _bzero = phdr.get('BZERO', 0)\n    if phdr['DATATYPE'][:10] == 'UNSIGNED*2':\n        _uint16 = 1\n        _bzero = 32768\n    else:\n        _uint16 = 0\n\n    # delete from the end, so it will not conflict with previous delete\n    for i in range(len(phdr)-1, -1, -1):\n        if phdr.cards[i].keyword in _list:\n            del phdr[i]\n\n    # clean up other primary header keywords\n    phdr['SIMPLE'] = True\n    phdr['BITPIX'] = 16\n    phdr['GROUPS'] = False\n    _after = 'NAXIS'\n    if _naxis0 > 0:\n        _after += str(_naxis0)\n    phdr.set('EXTEND', value=True, comment=\"FITS dataset may contain extensions\", after=_after)\n    phdr.set('NEXTEND', value=gcount, comment=\"Number of standard extensions\")\n\n    hdulist = fits.HDUList([fits.PrimaryHDU(header=phdr, data=None)])\n\n    # Use copy-on-write for all data types since byteswap may be needed\n    # in some platforms.\n    f1 = open(data_file, mode='rb')\n    dat = f1.read()\n#    dat = memmap(data_file, mode='c')\n    hdulist.mmobject = dat\n\n    errormsg = \"\"\n\n    loc = 0\n    for k in range(gcount):\n        ext_dat = numpy.fromstring(dat[loc:loc+data_size], dtype=_code)\n        ext_dat = ext_dat.reshape(_shape)\n        if _uint16:\n            ext_dat += _bzero\n        # Check to see whether there are any NaN's or infs which might indicate\n        # a byte-swapping problem, such as being written out on little-endian\n        #   and being read in on big-endian or vice-versa.\n        if _code.find('float') >= 0 and \\\n            (numpy.any(numpy.isnan(ext_dat)) or numpy.any(numpy.isinf(ext_dat))):\n            errormsg += \"===================================\\n\"\n            errormsg += \"= WARNING:                        =\\n\"\n            errormsg += \"=  Input image:                   =\\n\"\n            errormsg += input+\"[%d]\\n\"%(k+1)\n            errormsg += \"=  had floating point data values =\\n\"\n            errormsg += \"=  of NaN and/or Inf.             =\\n\"\n            errormsg += \"===================================\\n\"\n        elif _code.find('int') >= 0:\n            # Check INT data for max values\n            ext_dat_frac,ext_dat_exp = numpy.frexp(ext_dat)\n            if ext_dat_exp.max() == int(_bitpix) - 1:\n                # Potential problems with byteswapping\n                errormsg += \"===================================\\n\"\n                errormsg += \"= WARNING:                        =\\n\"\n                errormsg += \"=  Input image:                   =\\n\"\n                errormsg += input+\"[%d]\\n\"%(k+1)\n                errormsg += \"=  had integer data values        =\\n\"\n                errormsg += \"=  with maximum bitvalues.        =\\n\"\n                errormsg += \"===================================\\n\"\n\n        ext_hdu = fits.ImageHDU(data=ext_dat)\n\n        rec = numpy.fromstring(dat[loc+data_size:loc+group_size], dtype=formats)\n\n        loc += group_size\n\n        # Create separate PyFITS Card objects for each entry in 'rec'\n        for i in range(1, pcount+1):\n            #val = rec.field(i-1)[0]\n            val = rec[0][i-1]\n            if val.dtype.kind == 'S':\n                val = val.decode('ascii')\n\n            if i in bools:\n                if val:\n                    val = True\n                else:\n                    val = False\n\n            if i in floats:\n                # use fromstring, format in Card is deprecated in pyfits 0.9\n                _str = '%-8s= %20.7G / %s' % (key[i-1], val, comm[i-1])\n                _card = fits.Card.fromstring(_str)\n            else:\n                _card = fits.Card(keyword=key[i-1], value=val, comment=comm[i-1])\n\n            ext_hdu.header.append(_card)\n\n        # deal with bscale/bzero\n        if (_bscale != 1 or _bzero != 0):\n            ext_hdu.header['BSCALE'] = _bscale\n            ext_hdu.header['BZERO'] = _bzero\n\n        hdulist.append(ext_hdu)\n\n    if errormsg != \"\":\n        errormsg += \"===================================\\n\"\n        errormsg += \"=  This file may have been        =\\n\"\n        errormsg += \"=  written out on a platform      =\\n\"\n        errormsg += \"=  with a different byte-order.   =\\n\"\n        errormsg += \"=                                 =\\n\"\n        errormsg += \"=  Please verify that the values  =\\n\"\n        errormsg += \"=  are correct or apply the       =\\n\"\n        errormsg += \"=  '.byteswap()' method.          =\\n\"\n        errormsg += \"===================================\\n\"\n        print(errormsg)\n\n    f1.close()\n    stsci(hdulist)\n    return hdulist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_path(f1, f2):\n\n    \"\"\"Parse two input arguments and return two lists of file names\"\"\"\n\n    import glob\n\n    # if second argument is missing or is a wild card, point it\n    # to the current directory\n    f2 = f2.strip()\n    if f2 == '' or f2 == '*':\n        f2 = './'\n\n    # if the first argument is a directory, use all GEIS files\n    if os.path.isdir(f1):\n        f1 = os.path.join(f1, '*.??h')\n    list1 = glob.glob(f1)\n    list1 = [name for name in list1 if name[-1] == 'h' and name[-4] == '.']\n\n    # if the second argument is a directory, use file names in the\n    # first argument to construct file names, i.e.\n    # abc.xyh will be converted to abc_xyf.fits\n    if os.path.isdir(f2):\n        list2 = []\n        for file in list1:\n            name = os.path.split(file)[-1]\n            fitsname = name[:-4] + '_' + name[-3:-1] + 'f.fits'\n            list2.append(os.path.join(f2, fitsname))\n    else:\n        list2 = [s.strip() for s in f2.split(\",\")]\n\n    if list1 == [] or list2 == []:\n        err_msg = \"\"\n        if list1 == []:\n            err_msg += \"Input files `{:s}` not usable/available. \".format(f1)\n\n        if list2 == []:\n            err_msg += \"Input files `{:s}` not usable/available. \".format(f2)\n\n        raise IOError(err_msg)\n\n    else:\n        return list1, list2", "response": "Parse two input arguments and return two lists of file names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the filename provided to the function belongs to an association file.", "response": "def checkASN(filename):\n    \"\"\"\n    Determine if the filename provided to the function belongs to\n    an association.\n\n    Parameters\n    ----------\n    filename: string\n\n    Returns\n    -------\n    validASN  : boolean value\n\n    \"\"\"\n    # Extract the file extn type:\n    extnType = filename[filename.rfind('_')+1:filename.rfind('.')]\n\n    # Determine if this extn name is valid for an assocation file\n    if isValidAssocExtn(extnType):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef countinputs(inputlist):\n\n    # Initialize return values\n    numInputs = 0\n    numASNfiles = 0\n\n    # User irafglob to count the number of inputfiles\n    files = irafglob(inputlist, atfile=None)\n\n    # Use the \"len\" ufunc to count the number of entries in the list\n    numInputs = len(files)\n\n    # Loop over the list and see if any of the entries are association files\n    for file in files:\n        if (checkASN(file) == True):\n            numASNfiles += 1\n\n    return numInputs,numASNfiles", "response": "Count the number of inputfiles provided by the user and the number of ASN files that are associated with the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summary(logfile, time_format):\n  \"show a summary of all projects\"\n\n  def output(summary):\n    width = max([len(p[0]) for p in summary]) + 3\n    print '\\n'.join([\n      \"%s%s%s\" % (p[0], ' ' * (width - len(p[0])),\n        colored(minutes_to_txt(p[1]), 'red')) for p in summary])\n\n  output(server.summarize(read(logfile, time_format, only_elapsed=True)))", "response": "show a summary of all projects"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(project, logfile, time_format):\n  \"start tracking for <project>\"\n\n  records = read(logfile, time_format)\n  if records and not records[-1][1][1]:\n    print \"error: there is a project already active\"\n    return\n\n  write(server.start(project, records), logfile, time_format)\n\n  print \"starting work on %s\" % colored(project, attrs=['bold'])\n  print \"  at %s\" % colored(server.date_to_txt(now(), time_format), 'green')", "response": "start tracking for project"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(logfile, time_format):\n  \"stop tracking for the active project\"\n\n  def save_and_output(records):\n    records = server.stop(records)\n    write(records, logfile, time_format)\n\n    def output(r):\n      print \"worked on %s\" % colored(r[0], attrs=['bold'])\n      print \"  from    %s\" % colored(\n        server.date_to_txt(r[1][0], time_format), 'green')\n      print \"  to now, %s\" % colored(\n        server.date_to_txt(r[1][1], time_format), 'green')\n      print \"       => %s elapsed\" % colored(\n        time_elapsed(r[1][0], r[1][1]), 'red')\n\n    output(records[-1])\n\n  save_and_output(read(logfile, time_format))", "response": "stop tracking for the active project"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(logfile, time_format):\n  \"parses a stream with text formatted as a Timed logfile and shows a summary\"\n\n  records = [server.record_from_txt(line, only_elapsed=True,\n    time_format=time_format) for line in sys.stdin.readlines()]\n\n  # TODO: make this code better.\n  def output(summary):\n    width = max([len(p[0]) for p in summary]) + 3\n    print '\\n'.join([\n      \"%s%s%s\" % (p[0], ' ' * (width - len(p[0])),\n        colored(minutes_to_txt(p[1]), 'red')) for p in summary])\n\n  output(server.summarize(records))", "response": "parses a stream with text formatted as a Timed logfile and shows a summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef projects(logfile, time_format):\n  \"prints a newline-separated list of all projects\"\n\n  print '\\n'.join(server.list_projects(read(logfile, time_format)))", "response": "prints a newline - separated list of all projects"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a formatted string with the current local time.", "response": "def getLTime():\n    \"\"\"Returns a formatted string with the current local time.\"\"\"\n\n    _ltime = _time.localtime(_time.time())\n    tlm_str = _time.strftime('%H:%M:%S (%d/%m/%Y)', _ltime)\n    return tlm_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a formatted string with the current date.", "response": "def getDate():\n    \"\"\"Returns a formatted string with the current date.\"\"\"\n\n    _ltime = _time.localtime(_time.time())\n    date_str = _time.strftime('%Y-%m-%dT%H:%M:%S',_ltime)\n\n    return date_str"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convertDate(date):\n\n    d, t = date.split('T')\n    return decimal_date(d, timeobs=t)", "response": "Convert a DATE string into a decimal year."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a DATE - OBS and optional TIME - OBS into a decimal year.", "response": "def decimal_date(dateobs, timeobs=None):\n    \"\"\"Convert DATE-OBS (and optional TIME-OBS) into a decimal year.\"\"\"\n\n    year, month, day = dateobs.split('-')\n    if timeobs is not None:\n        hr, min, sec = timeobs.split(':')\n    else:\n        hr, min, sec = 0, 0, 0\n\n    rdate = datetime.datetime(int(year), int(month), int(day), int(hr),\n                              int(min), int(sec))\n    dday = (float(rdate.strftime(\"%j\")) + rdate.hour / 24.0 +\n            rdate.minute / (60. * 24) + rdate.second / (3600 * 24.)) / 365.25\n    ddate = int(year) + dday\n\n    return ddate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpretDQvalue(input):\n\n    nbits = 16\n    # We will only support integer values up to 2**128\n    for iexp in [16, 32, 64, 128]:\n        # Find out whether the input value is less than 2**iexp\n        if (input // (2 ** iexp)) == 0:\n            # when it finally is, we have identified how many bits can be used to\n            # describe this input bitvalue\n            nbits = iexp\n            break\n\n    # Find out how 'dtype' values are described on this machine\n    a = np.zeros(1, dtype='int16')\n    atype_descr = a.dtype.descr[0][1]\n    # Use this description to build the description we need for our input integer\n    dtype_str = atype_descr[:2] + str(nbits // 8)\n    result = np.zeros(nbits + 1, dtype=dtype_str)\n\n    # For each bit, determine whether it has been set in the input value or not\n    for n in range(nbits + 1):\n        i = 2 ** n\n        if input & i > 0:\n            # record which bit has been set as the power-of-2 integer\n            result[n] = i\n\n    # Return the non-zero unique values as a Python list\n    return np.delete(np.unique(result), 0).tolist()", "response": "Interprets an integer input into its component bit values as a list of the power of 2 integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isFits(input):\n\n    isfits = False\n    fitstype = None\n    names = ['fits', 'fit', 'FITS', 'FIT']\n    #determine if input is a fits file based on extension\n    # Only check type of FITS file if filename ends in valid FITS string\n    f = None\n    fileclose = False\n    if isinstance(input, fits.HDUList):\n        isfits = True\n        f = input\n    else:\n        isfits = True in [input.endswith(l) for l in names]\n\n    # if input is a fits file determine what kind of fits it is\n    #waiver fits len(shape) == 3\n    if isfits:\n        if f is None:\n            try:\n                f = fits.open(input, mode='readonly')\n                fileclose = True\n            except Exception:\n                if f is not None:\n                    f.close()\n                raise\n        data0 = f[0].data\n        if data0 is not None:\n            try:\n                if isinstance(f[1], fits.TableHDU):\n                    fitstype = 'waiver'\n            except IndexError:\n                fitstype = 'simple'\n        else:\n            fitstype = 'mef'\n        if fileclose:\n            f.close()\n\n    return isfits, fitstype", "response": "Returns a tuple of isfits and fitstype of the input image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verifyWriteMode(files):\n\n    # Start by insuring that input is a list of filenames,\n    # if only a single filename has been given as input,\n    # convert it to a list with len == 1.\n    if not isinstance(files, list):\n        files = [files]\n\n    # Keep track of the name of each file which is not writable\n    not_writable = []\n    writable = True\n\n    # Check each file in input list\n    for fname in files:\n        try:\n            f = open(fname,'a')\n            f.close()\n            del f\n        except:\n            not_writable.append(fname)\n            writable = False\n\n    if not writable:\n        print('The following file(s) do not have write permission!')\n        for fname in not_writable:\n            print('    ', fname)\n\n    return writable", "response": "This function checks whether the files are writable and if they are not writable."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a comma - separated string of filter names extracted from the input TIFF header.", "response": "def getFilterNames(header, filternames=None):\n    \"\"\"\n    Returns a comma-separated string of filter names extracted from the input\n    header (PyFITS header object).  This function has been hard-coded to\n    support the following instruments:\n\n        ACS, WFPC2, STIS\n\n    This function relies on the 'INSTRUME' keyword to define what instrument\n    has been used to generate the observation/header.\n\n    The 'filternames' parameter allows the user to provide a list of keyword\n    names for their instrument, in the case their instrument is not supported.\n    \"\"\"\n\n    # Define the keyword names for each instrument\n    _keydict = {\n        'ACS': ['FILTER1', 'FILTER2'],\n        'WFPC2': ['FILTNAM1', 'FILTNAM2'],\n        'STIS': ['OPT_ELEM', 'FILTER'],\n        'NICMOS': ['FILTER', 'FILTER2'],\n        'WFC3': ['FILTER', 'FILTER2']\n    }\n\n    # Find out what instrument the input header came from, based on the\n    # 'INSTRUME' keyword\n    if 'INSTRUME' in header:\n        instrument = header['INSTRUME']\n    else:\n        raise ValueError('Header does not contain INSTRUME keyword.')\n\n    # Check to make sure this instrument is supported in _keydict\n    if instrument in _keydict:\n        _filtlist = _keydict[instrument]\n    else:\n        _filtlist = filternames\n\n    # At this point, we know what keywords correspond to the filter names\n    # in the header.  Now, get the values associated with those keywords.\n    # Build a list of all filter name values, with the exception of the\n    # blank keywords. Values containing 'CLEAR' or 'N/A' are valid.\n    _filter_values = []\n    for _key in _filtlist:\n        if _key in header:\n            _val = header[_key]\n        else:\n            _val = ''\n        if _val.strip() != '':\n            _filter_values.append(header[_key])\n\n    # Return the comma-separated list\n    return ','.join(_filter_values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef buildNewRootname(filename, extn=None, extlist=None):\n\n    # Search known suffixes to replace ('_crj.fits',...)\n    _extlist = copy.deepcopy(EXTLIST)\n    # Also, add a default where '_dth.fits' replaces\n    # whatever extension was there ('.fits','.c1h',...)\n    #_extlist.append('.')\n    # Also append any user-specified extensions...\n    if extlist:\n        _extlist += extlist\n\n    if isinstance(filename, fits.HDUList):\n        try:\n            filename = filename.filename()\n        except:\n            raise ValueError(\"Can't determine the filename of an waivered HDUList object.\")\n    for suffix in _extlist:\n        _indx = filename.find(suffix)\n        if _indx > 0: break\n\n    if _indx < 0:\n         # default to entire rootname\n        _indx = len(filename)\n\n    if extn is None:\n        extn = ''\n\n    return filename[:_indx] + extn", "response": "Build a new rootname for a new file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a new rootname for an existing file and given extension.", "response": "def buildRootname(filename, ext=None):\n    \"\"\"\n    Build a new rootname for an existing file and given extension.\n\n    Any user supplied extensions to use for searching for file need to be\n    provided as a list of extensions.\n\n    Examples\n    --------\n\n    ::\n\n        >>> rootname = buildRootname(filename, ext=['_dth.fits'])  # doctest: +SKIP\n\n    \"\"\"\n\n    if filename in ['' ,' ', None]:\n        return None\n\n    fpath, fname = os.path.split(filename)\n    if ext is not None and '_' in ext[0]:\n        froot = os.path.splitext(fname)[0].split('_')[0]\n    else:\n        froot = fname\n\n    if fpath in ['', ' ', None]:\n        fpath = os.curdir\n    # Get complete list of filenames from current directory\n    flist = os.listdir(fpath)\n\n    #First, assume given filename is complete and verify\n    # it exists...\n    rootname = None\n\n    for name in flist:\n        if name == froot:\n            rootname = froot\n            break\n        elif name == froot + '.fits':\n            rootname = froot + '.fits'\n            break\n\n    # If we have an incomplete filename, try building a default\n    # name and seeing if it exists...\n    #\n    # Set up default list of suffix/extensions to add to rootname\n    _extlist = []\n    for extn in EXTLIST:\n        _extlist.append(extn)\n\n    if rootname is None:\n        # Add any user-specified extension to list of extensions...\n        if ext is not None:\n            for i in ext:\n                _extlist.insert(0,i)\n        # loop over all extensions looking for a filename that matches...\n        for extn in _extlist:\n            # Start by looking for filename with exactly\n            # the same case a provided in ASN table...\n            rname = froot + extn\n            for name in flist:\n                if rname == name:\n                    rootname = name\n                    break\n            if rootname is None:\n                # Try looking for all lower-case filename\n                # instead of a mixed-case filename as required\n                # by the pipeline.\n                rname = froot.lower() + extn\n                for name in flist:\n                    if rname == name:\n                        rootname = name\n                        break\n\n            if rootname is not None:\n                break\n\n    # If we still haven't found the file, see if we have the\n    # info to build one...\n    if rootname is None and ext is not None:\n        # Check to see if we have a full filename to start with...\n        _indx = froot.find('.')\n        if _indx > 0:\n            rootname = froot[:_indx] + ext[0]\n        else:\n            rootname = froot + ext[0]\n\n    if fpath not in ['.', '', ' ', None]:\n        rootname = os.path.join(fpath, rootname)\n    # It will be up to the calling routine to verify\n    # that a valid rootname, rather than 'None', was returned.\n    return rootname"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the value of a keyword in the header of a IRAF recognized image.", "response": "def getKeyword(filename, keyword, default=None, handle=None):\n    \"\"\"\n    General, write-safe method for returning a keyword value from the header of\n    a IRAF recognized image.\n\n    Returns the value as a string.\n    \"\"\"\n    # Insure that there is at least 1 extension specified...\n    if filename.find('[') < 0:\n        filename += '[0]'\n\n    _fname, _extn = parseFilename(filename)\n\n    if not handle:\n        # Open image whether it is FITS or GEIS\n        _fimg = openImage(_fname)\n    else:\n        # Use what the user provides, after insuring\n        # that it is a proper PyFITS object.\n        if isinstance(handle, fits.HDUList):\n            _fimg = handle\n        else:\n            raise ValueError('Handle must be %r object!' % fits.HDUList)\n\n    # Address the correct header\n    _hdr = getExtn(_fimg, _extn).header\n\n    try:\n        value =  _hdr[keyword]\n    except KeyError:\n        _nextn = findKeywordExtn(_fimg, keyword)\n        try:\n            value = _fimg[_nextn].header[keyword]\n        except KeyError:\n            value = ''\n\n    if not handle:\n        _fimg.close()\n        del _fimg\n\n    if value == '':\n        if default is None:\n            value = None\n        else:\n            value = default\n\n    # NOTE:  Need to clean up the keyword.. Occasionally the keyword value\n    # goes right up to the \"/\" FITS delimiter, and iraf.keypar is incapable\n    # of realizing this, so it incorporates \"/\" along with the keyword value.\n    # For example, after running \"pydrizzle\" on the image \"j8e601bkq_flt.fits\",\n    # the CD keywords look like this:\n    #\n    #   CD1_1   = 9.221627430999639E-06/ partial of first axis coordinate w.r.t. x\n    #   CD1_2   = -1.0346992614799E-05 / partial of first axis coordinate w.r.t. y\n    #\n    # so for CD1_1, iraf.keypar returns:\n    #       \"9.221627430999639E-06/\"\n    #\n    # So, the following piece of code CHECKS for this and FIXES the string,\n    # very simply by removing the last character if it is a \"/\".\n    # This fix courtesy of Anton Koekemoer, 2002.\n    elif isinstance(value, string_types):\n        if value[-1:] == '/':\n            value = value[:-1]\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getHeader(filename, handle=None):\n\n    _fname, _extn = parseFilename(filename)\n    # Allow the user to provide an already opened PyFITS object\n    # to derive the header from...\n    #\n    if not handle:\n        # Open image whether it is FITS or GEIS\n        _fimg = openImage(_fname, mode='readonly')\n    else:\n        # Use what the user provides, after insuring\n        # that it is a proper PyFITS object.\n        if isinstance(handle, fits.HDUList):\n            _fimg = handle\n        else:\n            raise ValueError('Handle must be a %r object!' % fits.HDUList)\n\n    _hdr = _fimg['PRIMARY'].header.copy()\n\n    # if the data is not in the primary array delete NAXIS\n    # so that the correct value is read from the extension header\n    if _hdr['NAXIS'] == 0:\n        del _hdr['NAXIS']\n\n    if not (_extn is None or (_extn.isdigit() and int(_extn) == 0)):\n        # Append correct extension/chip/group header to PRIMARY...\n        #for _card in getExtn(_fimg,_extn).header.ascard:\n            #_hdr.ascard.append(_card)\n        for _card in getExtn(_fimg, _extn).header.cards:\n            _hdr.append(_card)\n    if not handle:\n        # Close file handle now...\n        _fimg.close()\n        del _fimg\n\n    return _hdr", "response": "Return a copy of the PRIMARY header along with any group and extension header for this filename specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateKeyword(filename, key, value,show=yes):\n\n    _fname, _extn = parseFilename(filename)\n\n    # Open image whether it is FITS or GEIS\n    _fimg = openImage(_fname, mode='update')\n\n    # Address the correct header\n    _hdr = getExtn(_fimg, _extn).header\n\n    # Assign a new value or add new keyword here.\n    try:\n        _hdr[key] = value\n    except KeyError:\n        if show:\n            print('Adding new keyword ', key, '=', value)\n        _hdr[key] = value\n\n    # Close image\n    _fimg.close()\n    del _fimg", "response": "Add or update a keyword in the header with given value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a new FITS filename for a GEIS input image.", "response": "def buildFITSName(geisname):\n    \"\"\"Build a new FITS filename for a GEIS input image.\"\"\"\n\n    # User wants to make a FITS copy and update it...\n    _indx = geisname.rfind('.')\n    _fitsname = geisname[:_indx] + '_' + geisname[_indx + 1:-1] + 'h.fits'\n\n    return _fitsname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef openImage(filename, mode='readonly', memmap=False, writefits=True,\n              clobber=True, fitsname=None):\n    \"\"\"\n    Opens file and returns PyFITS object.  Works on both FITS and GEIS\n    formatted images.\n\n    Notes\n    -----\n    If a GEIS or waivered FITS image is used as input, it will convert it to a\n    MEF object and only if ``writefits = True`` will write it out to a file. If\n    ``fitsname = None``, the name used to write out the new MEF file will be\n    created using `buildFITSName`.\n\n    Parameters\n    ----------\n    filename: str\n        name of input file\n    mode: str\n        mode for opening file based on PyFITS `mode` parameter values\n    memmap: bool\n        switch for using memory mapping, `False` for no, `True` for yes\n    writefits: bool\n        if `True`, will write out GEIS as multi-extension FITS\n        and return handle to that opened GEIS-derived MEF file\n    clobber: bool\n        overwrite previously written out GEIS-derived MEF file\n    fitsname: str\n        name to use for GEIS-derived MEF file,\n        if None and writefits==`True`, will use 'buildFITSName()' to generate one\n    \"\"\"\n    if not isinstance(filename, fits.HDUList):\n        # Insure that the filename is always fully expanded\n        # This will not affect filenames without paths or\n        # filenames specified with extensions.\n        filename = osfn(filename)\n\n        # Extract the rootname and extension specification\n        # from input image name\n        _fname, _iextn = parseFilename(filename)\n    else:\n        _fname = filename\n\n    # Check whether we have a FITS file and if so what type\n    isfits, fitstype = isFits(_fname)\n\n    if isfits:\n        if fitstype != 'waiver':\n            # Open the FITS file\n            fimg = fits.open(_fname, mode=mode, memmap=memmap)\n            return fimg\n        else:\n            fimg = convertwaiveredfits.convertwaiveredfits(_fname)\n\n            #check for the existence of a data quality file\n            _dqname = buildNewRootname(_fname, extn='_c1f.fits')\n            dqexists = os.path.exists(_dqname)\n            if dqexists:\n                try:\n                    dqfile = convertwaiveredfits.convertwaiveredfits(_dqname)\n                    dqfitsname = buildNewRootname(_dqname, extn='_c1h.fits')\n                except:\n                    print(\"Could not read data quality file %s\" % _dqname)\n            if writefits:\n                # User wants to make a FITS copy and update it\n                # using the filename they have provided\n                if fitsname is None:\n                    rname = buildNewRootname(_fname)\n                    fitsname = buildNewRootname(rname, extn='_c0h.fits')\n\n                # Write out GEIS image as multi-extension FITS.\n                fexists = os.path.exists(fitsname)\n                if (fexists and clobber) or not fexists:\n                    print('Writing out WAIVERED as MEF to ', fitsname)\n                    if ASTROPY_VER_GE13:\n                        fimg.writeto(fitsname, overwrite=clobber)\n                    else:\n                        fimg.writeto(fitsname, clobber=clobber)\n                    if dqexists:\n                        print('Writing out WAIVERED as MEF to ', dqfitsname)\n                        if ASTROPY_VER_GE13:\n                            dqfile.writeto(dqfitsname, overwrite=clobber)\n                        else:\n                            dqfile.writeto(dqfitsname, clobber=clobber)\n                # Now close input GEIS image, and open writable\n                # handle to output FITS image instead...\n                fimg.close()\n                del fimg\n                # Image re-written as MEF, now it needs its WCS updated\n                #updatewcs.updatewcs(fitsname)\n\n                fimg = fits.open(fitsname, mode=mode, memmap=memmap)\n\n        # Return handle for use by user\n        return fimg\n    else:\n        # Input was specified as a GEIS image, but no FITS copy\n        # exists.  Read it in with 'readgeis' and make a copy\n        # then open the FITS copy...\n        try:\n            # Open as a GEIS image for reading only\n            fimg = readgeis.readgeis(_fname)\n        except:\n            raise IOError(\"Could not open GEIS input: %s\" % _fname)\n\n        #check for the existence of a data quality file\n        _dqname = buildNewRootname(_fname, extn='.c1h')\n        dqexists = os.path.exists(_dqname)\n        if dqexists:\n            try:\n                dqfile = readgeis.readgeis(_dqname)\n                dqfitsname = buildFITSName(_dqname)\n            except:\n                print(\"Could not read data quality file %s\" % _dqname)\n\n        # Check to see if user wanted to update GEIS header.\n        # or write out a multi-extension FITS file and return a handle to it\n        if writefits:\n                # User wants to make a FITS copy and update it\n                # using the filename they have provided\n            if fitsname is None:\n                fitsname = buildFITSName(_fname)\n\n            # Write out GEIS image as multi-extension FITS.\n            fexists = os.path.exists(fitsname)\n            if (fexists and clobber) or not fexists:\n                    print('Writing out GEIS as MEF to ', fitsname)\n                    if ASTROPY_VER_GE13:\n                        fimg.writeto(fitsname, overwrite=clobber)\n                    else:\n                        fimg.writeto(fitsname, clobber=clobber)\n                    if dqexists:\n                        print('Writing out GEIS as MEF to ', dqfitsname)\n                        if ASTROPY_VER_GE13:\n                            dqfile.writeto(dqfitsname, overwrite=clobber)\n                        else:\n                            dqfile.writeto(dqfitsname, clobber=clobber)\n            # Now close input GEIS image, and open writable\n            # handle to output FITS image instead...\n            fimg.close()\n            del fimg\n            # Image re-written as MEF, now it needs its WCS updated\n            #updatewcs.updatewcs(fitsname)\n\n            fimg = fits.open(fitsname, mode=mode, memmap=memmap)\n\n        # Return handle for use by user\n        return fimg", "response": "Opens a FITS file and returns a PyFITS object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse out filename from any specified extensions.", "response": "def parseFilename(filename):\n    \"\"\"\n    Parse out filename from any specified extensions.\n\n    Returns rootname and string version of extension name.\n    \"\"\"\n\n    # Parse out any extension specified in filename\n    _indx = filename.find('[')\n    if _indx > 0:\n        # Read extension name provided\n        _fname = filename[:_indx]\n        _extn = filename[_indx + 1:-1]\n    else:\n        _fname = filename\n        _extn = None\n\n    return _fname, _extn"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a string representing a qualified fits extension name and return a tuple of the name and version of the extension.", "response": "def parseExtn(extn=None):\n    \"\"\"\n    Parse a string representing a qualified fits extension name as in the\n    output of `parseFilename` and return a tuple ``(str(extname),\n    int(extver))``, which can be passed to `astropy.io.fits` functions using\n    the 'ext' kw.\n\n    Default return is the first extension in a fits file.\n\n    Examples\n    --------\n\n    ::\n\n        >>> parseExtn('sci, 2')\n        ('sci', 2)\n        >>> parseExtn('2')\n        ('', 2)\n        >>> parseExtn('sci')\n        ('sci', 1)\n\n    \"\"\"\n\n    if not extn:\n        return ('', 0)\n\n    try:\n        lext = extn.split(',')\n    except:\n        return ('', 1)\n\n    if len(lext) == 1 and lext[0].isdigit():\n        return (\"\", int(lext[0]))\n    elif len(lext) == 2:\n        return (lext[0], int(lext[1]))\n    else:\n        return (lext[0], 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of extensions in a FITS file.", "response": "def countExtn(fimg, extname='SCI'):\n    \"\"\"\n    Return the number of 'extname' extensions, defaulting to counting the\n    number of SCI extensions.\n    \"\"\"\n\n    closefits = False\n    if isinstance(fimg, string_types):\n        fimg = fits.open(fimg)\n        closefits = True\n\n    n = 0\n    for e in fimg:\n        if 'extname' in e.header and e.header['extname'] == extname:\n            n += 1\n\n    if closefits:\n        fimg.close()\n\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the PyFITS extension corresponding to the extension specified in the file.", "response": "def getExtn(fimg, extn=None):\n    \"\"\"\n    Returns the PyFITS extension corresponding to extension specified in\n    filename.\n\n    Defaults to returning the first extension with data or the primary\n    extension, if none have data.  If a non-existent extension has been\n    specified, it raises a `KeyError` exception.\n    \"\"\"\n\n    # If no extension is provided, search for first extension\n    # in FITS file with data associated with it.\n    if extn is None:\n        # Set up default to point to PRIMARY extension.\n        _extn = fimg[0]\n        # then look for first extension with data.\n        for _e in fimg:\n            if _e.data is not None:\n                _extn = _e\n                break\n    else:\n        # An extension was provided, so parse it out...\n        if repr(extn).find(',') > 1:\n            if isinstance(extn, tuple):\n                # We have a tuple possibly created by parseExtn(), so\n                # turn it into a list for easier manipulation.\n                _extns = list(extn)\n                if '' in _extns:\n                    _extns.remove('')\n            else:\n                _extns = extn.split(',')\n            # Two values given for extension:\n            #    for example, 'sci,1' or 'dq,1'\n            try:\n                _extn = fimg[_extns[0], int(_extns[1])]\n            except KeyError:\n                _extn = None\n                for e in fimg:\n                    hdr = e.header\n                    if ('extname' in hdr and\n                            hdr['extname'].lower() == _extns[0].lower() and\n                            hdr['extver'] == int(_extns[1])):\n                        _extn = e\n                        break\n        elif repr(extn).find('/') > 1:\n            # We are working with GEIS group syntax\n            _indx = str(extn[:extn.find('/')])\n            _extn = fimg[int(_indx)]\n        elif isinstance(extn, string_types):\n            if extn.strip() == '':\n                _extn = None  # force error since invalid name was provided\n            # Only one extension value specified...\n            elif extn.isdigit():\n                # We only have an extension number specified as a string...\n                _nextn = int(extn)\n            else:\n                # We only have EXTNAME specified...\n                _nextn = None\n                if extn.lower() == 'primary':\n                    _nextn = 0\n                else:\n                    i = 0\n                    for hdu in fimg:\n                        isimg = 'extname' in hdu.header\n                        hdr = hdu.header\n                        if isimg and extn.lower() == hdr['extname'].lower():\n                            _nextn = i\n                            break\n                        i += 1\n\n            if _nextn < len(fimg):\n                _extn = fimg[_nextn]\n            else:\n                _extn = None\n\n        else:\n            # Only integer extension number given, or default of 0 is used.\n            if int(extn) < len(fimg):\n                _extn = fimg[int(extn)]\n            else:\n                _extn = None\n\n    if _extn is None:\n        raise KeyError('Extension %s not found' % extn)\n\n    return _extn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches a directory for full filename with optional path.", "response": "def findFile(input):\n    \"\"\"Search a directory for full filename with optional path.\"\"\"\n\n    # If no input name is provided, default to returning 'no'(FALSE)\n    if not input:\n        return no\n\n    # We use 'osfn' here to insure that any IRAF variables are\n    # expanded out before splitting out the path...\n    _fdir, _fname = os.path.split(osfn(input))\n\n    if _fdir == '':\n        _fdir = os.curdir\n\n    try:\n        flist = os.listdir(_fdir)\n    except OSError:\n        # handle when requested file in on a disconnect network store\n        return no\n\n    _root, _extn = parseFilename(_fname)\n\n    found = no\n    for name in flist:\n        if name == _root:\n            # Check to see if given extension, if any, exists\n            if _extn is None:\n                found = yes\n                continue\n            else:\n                _split = _extn.split(',')\n                _extnum = None\n                _extver = None\n                if  _split[0].isdigit():\n                    _extname = None\n                    _extnum = int(_split[0])\n                else:\n                    _extname = _split[0]\n                    if len(_split) > 1:\n                        _extver = int(_split[1])\n                    else:\n                        _extver = 1\n                f = openImage(_root)\n                f.close()\n                if _extnum is not None:\n                    if _extnum < len(f):\n                        found = yes\n                        del f\n                        continue\n                    else:\n                        del f\n                else:\n                    _fext = findExtname(f, _extname, extver=_extver)\n                    if _fext is not None:\n                        found = yes\n                        del f\n                        continue\n    return found"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkFileExists(filename, directory=None):\n\n    if directory is not None:\n        fname = os.path.join(directory,filename)\n    else:\n        fname = filename\n    _exist = os.path.exists(fname)\n    return _exist", "response": "Checks to see if file specified exists in current or specified directory. Returns 1 if it exists 0 if not found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copyFile(input, output, replace=None):\n\n    _found = findFile(output)\n    if not _found or (_found and replace):\n        shutil.copy2(input, output)", "response": "Copy a file whole from input to output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removeFile(inlist):\n\n    if not isinstance(inlist, string_types):\n    # We do have a list, so delete all filenames in list.\n        # Treat like a list of full filenames\n        _ldir = os.listdir('.')\n        for f in inlist:\n        # Now, check to see if there are wildcards which need to be expanded\n            if f.find('*') >= 0 or f.find('?') >= 0:\n                # We have a wild card specification\n                regpatt = f.replace('?', '.?')\n                regpatt = regpatt.replace('*', '.*')\n                _reg = re.compile(regpatt)\n                for file in _ldir:\n                    if _reg.match(file):\n                        _remove(file)\n            else:\n                # This is just a single filename\n                _remove(f)\n    else:\n        # It must be a string then, so treat as a single filename\n        _remove(inlist)", "response": "Utility function for deleting a list of files or a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findExtname(fimg, extname, extver=None):\n\n    i = 0\n    extnum = None\n    for chip in fimg:\n        hdr = chip.header\n        if 'EXTNAME' in hdr:\n            if hdr['EXTNAME'].strip() == extname.upper():\n                if extver is None or hdr['EXTVER'] == extver:\n                    extnum = i\n                    break\n        i += 1\n    return extnum", "response": "Returns the list number of the extension corresponding to EXTNAME given."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rAsciiLine(ifile):\n\n    _line = ifile.readline().strip()\n    while len(_line) == 0:\n        _line = ifile.readline().strip()\n    return _line", "response": "Returns the next non - blank line in an ASCII file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nundo Python conversion of CL parameter or variable name.", "response": "def untranslateName(s):\n    \"\"\"Undo Python conversion of CL parameter or variable name.\"\"\"\n\n    s = s.replace('DOT', '.')\n    s = s.replace('DOLLAR', '$')\n    # delete 'PY' at start of name components\n    if s[:2] == 'PY': s = s[2:]\n    s = s.replace('.PY', '.')\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting value of IRAF or OS environment variable.", "response": "def envget(var, default=None):\n    \"\"\"Get value of IRAF or OS environment variable.\"\"\"\n\n    if 'pyraf' in sys.modules:\n        #ONLY if pyraf is already loaded, import iraf into the namespace\n        from pyraf import iraf\n    else:\n        # else set iraf to None so it knows to not use iraf's environment\n        iraf = None\n\n    try:\n        if iraf:\n            return iraf.envget(var)\n        else:\n            raise KeyError\n    except KeyError:\n        try:\n            return _varDict[var]\n        except KeyError:\n            try:\n                return os.environ[var]\n            except KeyError:\n                if default is not None:\n                    return default\n                elif var == 'TERM':\n                    # Return a default value for TERM\n                    # TERM gets caught as it is found in the default\n                    # login.cl file setup by IRAF.\n                    print(\"Using default TERM value for session.\")\n                    return 'xterm'\n                else:\n                    raise KeyError(\"Undefined environment variable `%s'\" % var)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts IRAF virtual path name to OS pathname.", "response": "def osfn(filename):\n    \"\"\"Convert IRAF virtual path name to OS pathname.\"\"\"\n\n    # Try to emulate the CL version closely:\n    #\n    # - expands IRAF virtual file names\n    # - strips blanks around path components\n    # - if no slashes or relative paths, return relative pathname\n    # - otherwise return absolute pathname\n    if filename is None:\n        return filename\n\n    ename = Expand(filename)\n    dlist = [part.strip() for part in ename.split(os.sep)]\n    if len(dlist) == 1 and dlist[0] not in [os.curdir, os.pardir]:\n        return dlist[0]\n\n    # I use str.join instead of os.path.join here because\n    # os.path.join(\"\",\"\") returns \"\" instead of \"/\"\n\n    epath = os.sep.join(dlist)\n    fname = os.path.abspath(epath)\n    # append '/' if relative directory was at end or filename ends with '/'\n    if fname[-1] != os.sep and dlist[-1] in ['', os.curdir, os.pardir]:\n        fname = fname + os.sep\n    return fname"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef defvar(varname):\n\n    if 'pyraf' in sys.modules:\n        #ONLY if pyraf is already loaded, import iraf into the namespace\n        from pyraf import iraf\n    else:\n        # else set iraf to None so it knows to not use iraf's environment\n        iraf = None\n\n    if iraf:\n        _irafdef = iraf.envget(varname)\n    else:\n        _irafdef = 0\n    return varname in _varDict or varname in os.environ or _irafdef", "response": "Returns true if CL variable is defined."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(*args, **kw):\n\n    if len(args) == 0:\n        if len(kw) != 0:\n            # normal case is only keyword,value pairs\n            for keyword, value in kw.items():\n                keyword = untranslateName(keyword)\n                svalue = str(value)\n                _varDict[keyword] = svalue\n        else:\n            # set with no arguments lists all variables (using same format\n            # as IRAF)\n            listVars(prefix=\"    \", equals=\"=\")\n    else:\n        # The only other case allowed is the peculiar syntax\n        # 'set @filename', which only gets used in the zzsetenv.def file,\n        # where it reads extern.pkg.  That file also gets read (in full cl\n        # mode) by clpackage.cl.  I get errors if I read this during\n        # zzsetenv.def, so just ignore it here...\n        #\n        # Flag any other syntax as an error.\n        if (len(args) != 1 or len(kw) != 0 or\n                not isinstance(args[0], string_types) or args[0][:1] != '@'):\n            raise SyntaxError(\"set requires name=value pairs\")", "response": "Set IRAF environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint value of IRAF or OS environment variables.", "response": "def show(*args, **kw):\n    \"\"\"Print value of IRAF or OS environment variables.\"\"\"\n\n    if len(kw):\n        raise TypeError('unexpected keyword argument: %r' % list(kw))\n\n    if args:\n        for arg in args:\n            print(envget(arg))\n    else:\n        # print them all\n        listVars(prefix=\"    \", equals=\"=\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Expand(instring, noerror=0):\n\n    # call _expand1 for each entry in comma-separated list\n    wordlist = instring.split(\",\")\n    outlist = []\n    for word in wordlist:\n        outlist.append(os.path.expanduser(_expand1(word, noerror=noerror)))\n    return \",\".join(outlist)", "response": "Expand a string with embedded IRAF variables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _expand1(instring, noerror):\n\n    # first expand names in parentheses\n    # note this works on nested names too, expanding from the\n    # inside out (just like IRAF)\n    mm = __re_var_paren.search(instring)\n    while mm is not None:\n        # remove embedded dollar signs from name\n        varname = mm.group('varname').replace('$','')\n        if defvar(varname):\n            varname = envget(varname)\n        elif noerror:\n            varname = \"\"\n        else:\n            raise ValueError(\"Undefined variable `%s' in string `%s'\" %\n                             (varname, instring))\n\n        instring = instring[:mm.start()] + varname + instring[mm.end():]\n        mm = __re_var_paren.search(instring)\n    # now expand variable name at start of string\n    mm = __re_var_match.match(instring)\n    if mm is None:\n        return instring\n    varname = mm.group('varname')\n    if varname in ['', ' ', None]:\n        mm = __re_var_match2.match(instring)\n        varname = mm.group('varname')\n\n    if defvar(varname):\n        # recursively expand string after substitution\n        return _expand1(envget(varname) + instring[mm.end():], noerror)\n    elif noerror:\n        return _expand1(varname + instring[mm.end():], noerror)\n    else:\n        raise ValueError(\"Undefined variable `%s' in string `%s'\" %\n                         (varname, instring))", "response": "Expand a string with embedded IRAF variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if this is a legal date in the Julian calendar", "response": "def legal_date(year, month, day):\n    '''Check if this is a legal date in the Julian calendar'''\n    daysinmonth = month_length(year, month)\n\n    if not (0 < day <= daysinmonth):\n        raise ValueError(\"Month {} doesn't have a day {}\".format(month, day))\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_jd(jd):\n    '''Calculate Julian calendar date from Julian day'''\n\n    jd += 0.5\n    z = trunc(jd)\n\n    a = z\n    b = a + 1524\n    c = trunc((b - 122.1) / 365.25)\n    d = trunc(365.25 * c)\n    e = trunc((b - d) / 30.6001)\n\n    if trunc(e < 14):\n        month = e - 1\n    else:\n        month = e - 13\n\n    if trunc(month > 2):\n        year = c - 4716\n    else:\n        year = c - 4715\n\n    day = b - d - trunc(30.6001 * e)\n\n    return (year, month, day)", "response": "Calculate Julian calendar date from Julian day"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_jd(year, month, day):\n    '''Convert to Julian day using astronomical years (0 = 1 BC, -1 = 2 BC)'''\n\n    legal_date(year, month, day)\n\n    # Algorithm as given in Meeus, Astronomical Algorithms, Chapter 7, page 61\n\n    if month <= 2:\n        year -= 1\n        month += 12\n\n    return (trunc((365.25 * (year + 4716))) + trunc((30.6001 * (month + 1))) + day) - 1524.5", "response": "Convert to Julian day using astronomical years"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest for delay of start of new year and to avoid'''", "response": "def delay_1(year):\n    '''Test for delay of start of new year and to avoid'''\n    # Sunday, Wednesday, and Friday as start of the new year.\n    months = trunc(((235 * year) - 234) / 19)\n    parts = 12084 + (13753 * months)\n    day = trunc((months * 29) + parts / 25920)\n\n    if ((3 * (day + 1)) % 7) < 3:\n        day += 1\n\n    return day"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delay_2(year):\n    '''Check for delay in start of new year due to length of adjacent years'''\n\n    last = delay_1(year - 1)\n    present = delay_1(year)\n    next_ = delay_1(year + 1)\n\n    if next_ - present == 356:\n        return 2\n    elif present - last == 382:\n        return 1\n    else:\n        return 0", "response": "Check for delay in start of new year due to length of adjacent years"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninputting GEIS files \"input\" will be read and converted to a new GEIS file whose byte-order has been swapped from its original state. Parameters ---------- input - str Full filename with path of input GEIS image header file output - str Full filename with path of output GEIS image header file If None, a default name will be created as input_swap.??h clobber - bool Overwrite any pre-existing output file? [Default: True] Notes ----- This function will automatically read and write out the data file using the GEIS image naming conventions.", "response": "def byteswap(input,output=None,clobber=True):\n\n    \"\"\"Input GEIS files \"input\" will be read and converted to a new GEIS file\n    whose byte-order has been swapped from its original state.\n\n    Parameters\n    ----------\n    input - str\n        Full filename with path of input GEIS image header file\n\n    output - str\n        Full filename with path of output GEIS image header file\n        If None, a default name will be created as input_swap.??h\n\n    clobber - bool\n        Overwrite any pre-existing output file? [Default: True]\n\n    Notes\n    -----\n    This function will automatically read and write out the data file using the\n    GEIS image naming conventions.\n\n    \"\"\"\n\n    global dat\n    cardLen = fits.Card.length\n\n    # input file(s) must be of the form *.??h and *.??d\n    if input[-1] != 'h' or input[-4] != '.':\n        raise \"Illegal input GEIS file name %s\" % input\n\n    data_file = input[:-1]+'d'\n\n    # Create default output name if no output name was specified by the user\n    if output is None:\n        output = input.replace('.','_swap.')\n\n    out_data = output[:-1]+'d'\n    if os.path.exists(output) and not clobber:\n        errstr = 'Output file already exists! Please remove or rename and start again...'\n        raise IOError(errstr)\n\n    _os = sys.platform\n    if _os[:5] == 'linux' or _os[:5] == 'win32' or _os[:5] == 'sunos' or _os[:3] == 'osf' or _os[:6] == 'darwin':\n        bytes_per_line = cardLen+1\n    else:\n        raise \"Platform %s is not supported (yet).\" % _os\n\n    end_card = 'END'+' '* (cardLen-3)\n\n    # open input file\n    im = open(input)\n\n    # Generate the primary HDU so we can have access to keywords which describe\n    # the number of groups and shape of each group's array\n    #\n    cards = []\n    while 1:\n        line = im.read(bytes_per_line)[:cardLen]\n        line = line[:8].upper() + line[8:]\n        if line == end_card:\n            break\n        cards.append(fits.Card.fromstring(line))\n\n    phdr = fits.Header(cards)\n    im.close()\n\n    _naxis0 = phdr.get('NAXIS', 0)\n    _naxis = [phdr['NAXIS'+str(j)] for j in range(1, _naxis0+1)]\n    _naxis.insert(0, _naxis0)\n    _bitpix = phdr['BITPIX']\n    _psize = phdr['PSIZE']\n    if phdr['DATATYPE'][:4] == 'REAL':\n        _bitpix = -_bitpix\n    if _naxis0 > 0:\n        size = reduce(lambda x,y:x*y, _naxis[1:])\n        data_size = abs(_bitpix) * size // 8\n    else:\n        data_size = 0\n    group_size = data_size + _psize // 8\n\n    # decode the group parameter definitions,\n    # group parameters will become extension header\n    groups = phdr['GROUPS']\n    gcount = phdr['GCOUNT']\n    pcount = phdr['PCOUNT']\n\n    formats = []\n    bools = []\n    floats = []\n    _range = list(range(1, pcount+1))\n    key = [phdr['PTYPE'+str(j)] for j in _range]\n    comm = [phdr.cards['PTYPE'+str(j)].comment for j in _range]\n\n    # delete group parameter definition header keywords\n    _list = ['PTYPE'+str(j) for j in _range] + \\\n            ['PDTYPE'+str(j) for j in _range] + \\\n            ['PSIZE'+str(j) for j in _range] + \\\n            ['DATATYPE', 'PSIZE', 'GCOUNT', 'PCOUNT', 'BSCALE', 'BZERO']\n\n    # Construct record array formats for the group parameters\n    # as interpreted from the Primary header file\n    for i in range(1, pcount+1):\n        ptype = key[i-1]\n        pdtype = phdr['PDTYPE'+str(i)]\n        star = pdtype.find('*')\n        _type = pdtype[:star]\n        _bytes = pdtype[star+1:]\n\n        # collect boolean keywords since they need special attention later\n\n        if _type == 'LOGICAL':\n            bools.append(i)\n        if pdtype == 'REAL*4':\n            floats.append(i)\n\n        fmt = geis_fmt[_type] + _bytes\n        formats.append((ptype,fmt))\n\n    _shape = _naxis[1:]\n    _shape.reverse()\n    _code = fits.BITPIX2DTYPE[_bitpix]\n    _bscale = phdr.get('BSCALE', 1)\n    _bzero = phdr.get('BZERO', 0)\n    if phdr['DATATYPE'][:10] == 'UNSIGNED*2':\n        _uint16 = 1\n        _bzero = 32768\n    else:\n        _uint16 = 0\n\n\n    # Use copy-on-write for all data types since byteswap may be needed\n    # in some platforms.\n    f1 = open(data_file, mode='rb')\n    dat = f1.read()\n    f1.close()\n\n    errormsg = \"\"\n\n    loc = 0\n    outdat = b''\n    for k in range(gcount):\n        ext_dat = numpy.fromstring(dat[loc:loc+data_size], dtype=_code)\n        ext_dat = ext_dat.reshape(_shape).byteswap()\n        outdat += ext_dat.tostring()\n\n        ext_hdu = fits.hdu.ImageHDU(data=ext_dat)\n\n        rec = numpy.fromstring(dat[loc+data_size:loc+group_size], dtype=formats).byteswap()\n        outdat += rec.tostring()\n\n        loc += group_size\n\n    if os.path.exists(output):\n        os.remove(output)\n    if os.path.exists(out_data):\n        os.remove(out_data)\n\n    shutil.copy(input,output)\n    outfile = open(out_data,mode='wb')\n    outfile.write(outdat)\n    outfile.close()\n    print('Finished byte-swapping ',input,' to ',output)\n\n#-------------------------------------------------------------------------------\n\n\n    \"\"\"Input GEIS files \"input\" will be read and a HDUList object will\n       be returned that matches the waiver-FITS format written out by 'stwfits' in IRAF.\n\n       The user can use the writeto method to write the HDUList object to\n       a FITS file.\n    \"\"\"\n\n#   global dat  # !!! (looks like this is a function missing its head)\n    cardLen = fits.Card.length\n\n    # input file(s) must be of the form *.??h and *.??d\n    if input[-1] != 'h' or input[-4] != '.':\n        raise \"Illegal input GEIS file name %s\" % input\n\n    data_file = input[:-1]+'d'\n\n    _os = sys.platform\n    if _os[:5] == 'linux' or _os[:5] == 'win32' or _os[:5] == 'sunos' or _os[:3] == 'osf' or _os[:6] == 'darwin':\n        bytes_per_line = cardLen+1\n    else:\n        raise \"Platform %s is not supported (yet).\" % _os\n\n    end_card = 'END'+' '* (cardLen-3)\n\n    # open input file\n    im = open(input)\n\n    # Generate the primary HDU\n    cards = []\n    while 1:\n        line = im.read(bytes_per_line)[:cardLen]\n        line = line[:8].upper() + line[8:]\n        if line == end_card:\n            break\n        cards.append(fits.Card.fromstring(line))\n\n    phdr = fits.Header(cards)\n    im.close()\n\n    phdr.set('FILENAME', value=input, after='DATE')\n\n    # Determine starting point for adding Group Parameter Block keywords to Primary header\n    phdr_indx = phdr.index('PSIZE')\n\n\n    _naxis0 = phdr.get('NAXIS', 0)\n    _naxis = [phdr['NAXIS'+str(j)] for j in range(1, _naxis0+1)]\n    _naxis.insert(0, _naxis0)\n    _bitpix = phdr['BITPIX']\n    _psize = phdr['PSIZE']\n    if phdr['DATATYPE'][:4] == 'REAL':\n        _bitpix = -_bitpix\n    if _naxis0 > 0:\n        size = reduce(lambda x,y:x*y, _naxis[1:])\n        data_size = abs(_bitpix) * size // 8\n    else:\n        data_size = 0\n    group_size = data_size + _psize // 8\n\n    # decode the group parameter definitions,\n    # group parameters will become extension table\n    groups = phdr['GROUPS']\n    gcount = phdr['GCOUNT']\n    pcount = phdr['PCOUNT']\n\n    formats = []\n    bools = []\n    floats = []\n    cols = [] # column definitions used for extension table\n    cols_dict = {} # provides name access to Column defs\n    _range = list(range(1, pcount+1))\n    key = [phdr['PTYPE'+str(j)] for j in _range]\n    comm = [phdr.cards['PTYPE'+str(j)].comment for j in _range]\n\n    # delete group parameter definition header keywords\n    _list = ['PTYPE'+str(j) for j in _range] + \\\n            ['PDTYPE'+str(j) for j in _range] + \\\n            ['PSIZE'+str(j) for j in _range] + \\\n            ['DATATYPE', 'PSIZE', 'GCOUNT', 'PCOUNT', 'BSCALE', 'BZERO']\n\n    # Construct record array formats for the group parameters\n    # as interpreted from the Primary header file\n    for i in range(1, pcount+1):\n        ptype = key[i-1]\n        pdtype = phdr['PDTYPE'+str(i)]\n        star = pdtype.find('*')\n        _type = pdtype[:star]\n        _bytes = pdtype[star+1:]\n\n        # collect boolean keywords since they need special attention later\n\n        if _type == 'LOGICAL':\n            bools.append(i)\n        if pdtype == 'REAL*4':\n            floats.append(i)\n\n        # identify keywords which require conversion to special units\n        if ptype in kw_DOUBLE:\n            _type = 'DOUBLE'\n\n        fmt = geis_fmt[_type] + _bytes\n        formats.append((ptype,fmt))\n\n        # Set up definitions for use in creating the group-parameter block table\n        nrpt = ''\n        nbits = str(int(_bytes)*8)\n        if 'CHAR' in _type:\n            nrpt = _bytes\n            nbits = _bytes\n\n        afmt = cols_fmt[_type]+ nbits\n        if 'LOGICAL' in _type:\n            afmt = cols_fmt[_type]\n        cfmt = cols_pfmt[_type]+nrpt\n\n        #print 'Column format for ',ptype,': ',cfmt,' with dtype of ',afmt\n        cols_dict[ptype] = fits.Column(name=ptype,format=cfmt,array=numpy.zeros(gcount,dtype=afmt))\n        cols.append(cols_dict[ptype]) # This keeps the columns in order\n\n    _shape = _naxis[1:]\n    _shape.reverse()\n    _code = fits.BITPIX2DTYPE[_bitpix]\n    _bscale = phdr.get('BSCALE', 1)\n    _bzero = phdr.get('BZERO', 0)\n    if phdr['DATATYPE'][:10] == 'UNSIGNED*2':\n        _uint16 = 1\n        _bzero = 32768\n    else:\n        _uint16 = 0\n\n    # delete from the end, so it will not conflict with previous delete\n    for i in range(len(phdr)-1, -1, -1):\n        if phdr.cards[i].keyword in _list:\n            del phdr[i]\n\n    # clean up other primary header keywords\n    phdr['SIMPLE'] = True\n    phdr['GROUPS'] = False\n    _after = 'NAXIS'\n    if _naxis0 > 0:\n        _after += str(_naxis0)\n    phdr.set('EXTEND', value=True,\n             comment=\"FITS dataset may contain extensions\",\n             after=_after)\n\n    # Use copy-on-write for all data types since byteswap may be needed\n    # in some platforms.\n    f1 = open(data_file, mode='rb')\n    dat = f1.read()\n    errormsg = \"\"\n\n    # Define data array for all groups\n    arr_shape = _naxis[:]\n    arr_shape[0] = gcount\n    arr_stack = numpy.zeros(arr_shape,dtype=_code)\n\n    loc = 0\n    for k in range(gcount):\n        ext_dat = numpy.fromstring(dat[loc:loc+data_size], dtype=_code)\n        ext_dat = ext_dat.reshape(_shape)\n        if _uint16:\n            ext_dat += _bzero\n        # Check to see whether there are any NaN's or infs which might indicate\n        # a byte-swapping problem, such as being written out on little-endian\n        #   and being read in on big-endian or vice-versa.\n        if _code.find('float') >= 0 and \\\n            (numpy.any(numpy.isnan(ext_dat)) or numpy.any(numpy.isinf(ext_dat))):\n            errormsg += \"===================================\\n\"\n            errormsg += \"= WARNING:                        =\\n\"\n            errormsg += \"=  Input image:                   =\\n\"\n            errormsg += input+\"[%d]\\n\"%(k+1)\n            errormsg += \"=  had floating point data values =\\n\"\n            errormsg += \"=  of NaN and/or Inf.             =\\n\"\n            errormsg += \"===================================\\n\"\n        elif _code.find('int') >= 0:\n            # Check INT data for max values\n            ext_dat_frac,ext_dat_exp = numpy.frexp(ext_dat)\n            if ext_dat_exp.max() == int(_bitpix) - 1:\n                # Potential problems with byteswapping\n                errormsg += \"===================================\\n\"\n                errormsg += \"= WARNING:                        =\\n\"\n                errormsg += \"=  Input image:                   =\\n\"\n                errormsg += input+\"[%d]\\n\"%(k+1)\n                errormsg += \"=  had integer data values        =\\n\"\n                errormsg += \"=  with maximum bitvalues.        =\\n\"\n                errormsg += \"===================================\\n\"\n\n        arr_stack[k] = ext_dat\n\n        rec = numpy.fromstring(dat[loc+data_size:loc+group_size], dtype=formats)\n\n        loc += group_size\n\n        # Add data from this GPB to table\n        for i in range(1, pcount+1):\n            val = rec[0][i-1]\n            if i in bools:\n                if val:\n                    val = 'T'\n                else:\n                    val = 'F'\n            cols[i-1].array[k] = val\n\n        # Based on the first group, add GPB keywords to PRIMARY header\n        if k == 0:\n            # Create separate PyFITS Card objects for each entry in 'rec'\n            # and update Primary HDU with these keywords after PSIZE\n            for i in range(1, pcount+1):\n                #val = rec.field(i-1)[0]\n                val = rec[0][i-1]\n                if val.dtype.kind == 'S':\n                    val = val.decode('ascii')\n\n                if i in bools:\n                    if val:\n                        val = True\n                    else:\n                        val = False\n                if i in floats:\n                    # use fromstring, format in Card is deprecated in pyfits 0.9\n                    _str = '%-8s= %20.13G / %s' % (key[i-1], val, comm[i-1])\n                    _card = fits.Card.fromstring(_str)\n                else:\n                    _card = fits.Card(keyword=key[i-1], value=val, comment=comm[i-1])\n                phdr.insert(phdr_indx+i, _card)\n\n            # deal with bscale/bzero\n            if (_bscale != 1 or _bzero != 0):\n                phdr['BSCALE'] = _bscale\n                phdr['BZERO'] = _bzero\n\n        #hdulist.append(ext_hdu)\n    # Define new table based on Column definitions\n    ext_table = fits.TableHDU.from_columns(cols)\n    ext_table.header.set('EXTNAME', value=input+'.tab', after='TFIELDS')\n    # Add column descriptions to header of table extension to match stwfits output\n    for i in range(len(key)):\n        ext_table.header.append(fits.Card(keyword=key[i], value=comm[i]))\n\n    if errormsg != \"\":\n        errormsg += \"===================================\\n\"\n        errormsg += \"=  This file may have been        =\\n\"\n        errormsg += \"=  written out on a platform      =\\n\"\n        errormsg += \"=  with a different byte-order.   =\\n\"\n        errormsg += \"=                                 =\\n\"\n        errormsg += \"=  Please verify that the values  =\\n\"\n        errormsg += \"=  are correct or apply the       =\\n\"\n        errormsg += \"=  '.byteswap()' method.          =\\n\"\n        errormsg += \"===================================\\n\"\n        print(errormsg)\n\n    f1.close()\n\n    hdulist = fits.HDUList([fits.PrimaryHDU(header=phdr, data=arr_stack)])\n    hdulist.append(ext_table)\n\n    return hdulist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self, measurementId, durationInSeconds=None):\n        logger.info(\">> measurement \" + measurementId +\n                    ((\" for \" + str(durationInSeconds)) if durationInSeconds is not None else \" until break\"))\n        self.failureCode = None\n        self.measurementOverflowed = False\n        self.dataHandler.start(measurementId)\n        self.breakRead = False\n        self.startTime = time.time()\n        self.doInit()\n        # this must follow doInit because doInit sets status to INITIALISED\n        self.status = RecordingDeviceStatus.RECORDING\n        elapsedTime = 0\n        try:\n            self._sampleIdx = 0\n            while True:\n                logger.debug(measurementId + \" provideData \")\n                self.dataHandler.handle(self.provideData())\n                elapsedTime = time.time() - self.startTime\n                if self.breakRead or durationInSeconds is not None and elapsedTime > durationInSeconds:\n                    logger.debug(measurementId + \" breaking provideData\")\n                    self.startTime = 0\n                    break\n        except:\n            self.status = RecordingDeviceStatus.FAILED\n            self.failureCode = str(sys.exc_info())\n            logger.exception(measurementId + \" failed\")\n        finally:\n            expectedSamples = self.fs * (durationInSeconds if durationInSeconds is not None else elapsedTime)\n            if self._sampleIdx < expectedSamples:\n                self.status = RecordingDeviceStatus.FAILED\n                self.failureCode = \"Insufficient samples \" + str(self._sampleIdx) + \" for \" + \\\n                                   str(elapsedTime) + \" second long run, expected \" + str(expectedSamples)\n            self._sampleIdx = 0\n            if self.measurementOverflowed:\n                self.status = RecordingDeviceStatus.FAILED\n                self.failureCode = \"Measurement overflow detected\"\n            if self.status == RecordingDeviceStatus.FAILED:\n                logger.error(\"<< measurement \" + measurementId + \" - FAILED - \" + self.failureCode)\n            else:\n                self.status = RecordingDeviceStatus.INITIALISED\n                logger.info(\"<< measurement \" + measurementId + \" - \" + self.status.name)\n            self.dataHandler.stop(measurementId, self.failureCode)\n            if self.status == RecordingDeviceStatus.FAILED:\n                logger.warning(\"Reinitialising device after measurement failure\")\n                self.doInit()", "response": "Start recording the specified measurement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield the analysed wav data.", "response": "def get(self, targetId):\n        \"\"\"\n        Yields the analysed wav data.\n        :param targetId: \n        :return: \n        \"\"\"\n        result = self._targetController.analyse(targetId)\n        if result:\n            if len(result) == 2:\n                if result[1] == 404:\n                    return result\n                else:\n                    return {'name': targetId, 'data': self._jsonify(result)}, 200\n            else:\n                return None, 404\n        else:\n            return None, 500"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore a new target.", "response": "def put(self, targetId):\n        \"\"\"\n        stores a new target.\n        :param targetId: the target to store.\n        :return:\n        \"\"\"\n        json = request.get_json()\n        if 'hinge' in json:\n            logger.info('Storing target ' + targetId)\n            if self._targetController.storeFromHinge(targetId, json['hinge']):\n                logger.info('Stored target ' + targetId)\n                return None, 200\n            else:\n                return None, 500\n        else:\n            return None, 400"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a datetime for the input floating point Julian Day Count", "response": "def to_datetime(jdc):\n    '''Return a datetime for the input floating point Julian Day Count'''\n    year, month, day = gregorian.from_jd(jdc)\n\n    # in jdc: 0.0 = noon, 0.5 = midnight\n    # the 0.5 changes it to 0.0 = midnight, 0.5 = noon\n    frac = (jdc + 0.5) % 1\n\n    hours = int(24 * frac)\n\n    mfrac = frac * 24 - hours\n    mins = int(60 * round(mfrac, 6))\n\n    sfrac = mfrac * 60 - mins\n    secs = int(60 * round(sfrac, 6))\n\n    msfrac = sfrac * 60 - secs\n\n    # down to ms, which are 1/1000 of a second\n    ms = int(1000 * round(msfrac, 6))\n\n    return datetime(year, month, day, int(hours), int(mins), int(secs), int(ms), tzinfo=utc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_from_qs(qs):\n    ''' Slightly introverted parser for lists of dot-notation nested fields\n        i.e. \"period.di,period.fhr\" => {\"period\": {\"di\": {}, \"fhr\": {}}}\n    '''\n    entries = qs.split(',') if qs.strip() else []\n    entries = [entry.strip() for entry in entries]\n\n    def _dict_from_qs(line, d):\n        if '.' in line:\n            key, value = line.split('.', 1)\n            d.setdefault(key, {})\n            return _dict_from_qs(value, d[key])\n        else:\n            d[line] = {}\n\n    def _default():\n        return defaultdict(_default)\n\n    d = defaultdict(_default)\n    for line in entries:\n        _dict_from_qs(line, d)\n    return d", "response": "Slightly introverted parser for lists of dot - notation nested fields\nAttributeNames = > dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qs_from_dict(qsdict, prefix=\"\"):\n    ''' Same as dict_from_qs, but in reverse\n        i.e. {\"period\": {\"di\": {}, \"fhr\": {}}} => \"period.di,period.fhr\"\n    '''\n    prefix = prefix + '.' if prefix else \"\"\n\n    def descend(qsd):\n        for key, val in sorted(qsd.items()):\n            if val:\n                yield qs_from_dict(val, prefix + key)\n            else:\n                yield prefix + key\n    return \",\".join(descend(qsdict))", "response": "Returns a string that is a list of all the items in the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dbcon(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if self.dbcon is None:\n            # set up connection\n            self.dbcon = sqlite3.connect(self.db)\n            self.dbcur = self.dbcon.cursor()\n            self.dbcur.execute(SQL_SENSOR_TABLE)\n            self.dbcur.execute(SQL_TMPO_TABLE)\n\n            # execute function\n            try:\n                result = func(*args, **kwargs)\n            except Exception as e:\n                # on exception, first close connection and then raise\n                self.dbcon.rollback()\n                self.dbcon.commit()\n                self.dbcon.close()\n                self.dbcon = None\n                self.dbcur = None\n                raise e\n            else:\n                # commit everything and close connection\n                self.dbcon.commit()\n                self.dbcon.close()\n                self.dbcon = None\n                self.dbcur = None\n        else:\n            result = func(*args, **kwargs)\n        return result\n    return wrapper", "response": "Set up connection before executing function commit and close connection after executing function. Unless a connection already has been created a new one is created."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds new sensor to the database", "response": "def add(self, sid, token):\n        \"\"\"\n        Add new sensor to the database\n\n        Parameters\n        ----------\n        sid : str\n            SensorId\n        token : str\n        \"\"\"\n        try:\n            self.dbcur.execute(SQL_SENSOR_INS, (sid, token))\n        except sqlite3.IntegrityError:  # sensor entry exists\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a specific sensor from the database.", "response": "def remove(self, sid):\n        \"\"\"\n        Remove sensor from the database\n\n        Parameters\n        ----------\n        sid : str\n            SensorID\n        \"\"\"\n        self.dbcur.execute(SQL_SENSOR_DEL, (sid,))\n        self.dbcur.execute(SQL_TMPO_DEL, (sid,))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sync(self, *sids):\n        if sids == ():\n            sids = [sid for (sid,) in self.dbcur.execute(SQL_SENSOR_ALL)]\n        for sid in sids:\n            self.dbcur.execute(SQL_TMPO_LAST, (sid,))\n            last = self.dbcur.fetchone()\n            if last:\n                rid, lvl, bid, ext = last\n                self._clean(sid, rid, lvl, bid)\n                # prevent needless polling\n                if time.time() < bid + 256:\n                    return\n            else:\n                rid, lvl, bid = 0, 0, 0\n            self._req_sync(sid, rid, lvl, bid)", "response": "Synchronise data with the specified sensors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, *sids):\n        if sids == ():\n            sids = [sid for (sid,) in self.dbcur.execute(SQL_SENSOR_ALL)]\n        slist = []\n        for sid in sids:\n            tlist = []\n            for tmpo in self.dbcur.execute(SQL_TMPO_ALL, (sid,)):\n                tlist.append(tmpo)\n                sid, rid, lvl, bid, ext, ctd, blk = tmpo\n                self._dprintf(DBG_TMPO_WRITE, ctd, sid, rid, lvl, bid, len(blk))\n            slist.append(tlist)\n        return slist", "response": "List all tmpo - blocks in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating data Series from a record.", "response": "def series(self, sid, recycle_id=None, head=None, tail=None,\n               datetime=True):\n        \"\"\"\n        Create data Series\n\n        Parameters\n        ----------\n        sid : str\n        recycle_id : optional\n        head : int | pandas.Timestamp, optional\n            Start of the interval\n            default earliest available\n        tail : int | pandas.Timestamp, optional\n            End of the interval\n            default max epoch\n        datetime : bool\n            convert index to datetime\n            default True\n\n        Returns\n        -------\n        pandas.Series\n        \"\"\"\n        if head is None:\n            head = 0\n        else:\n            head = self._2epochs(head)\n\n        if tail is None:\n            tail = EPOCHS_MAX\n        else:\n            tail = self._2epochs(tail)\n\n        if recycle_id is None:\n            self.dbcur.execute(SQL_TMPO_RID_MAX, (sid,))\n            recycle_id = self.dbcur.fetchone()[0]\n        tlist = self.list(sid)[0]\n        srlist = []\n        for _sid, rid, lvl, bid, ext, ctd, blk in tlist:\n            if (recycle_id == rid\n            and head < self._blocktail(lvl, bid)\n            and tail >= bid):\n                srlist.append(self._blk2series(ext, blk, head, tail))\n        if len(srlist) > 0:\n            ts = pd.concat(srlist)\n            ts.name = sid\n            if datetime is True:\n                ts.index = pd.to_datetime(ts.index, unit=\"s\", utc=True)\n            return ts\n        else:\n            return pd.Series([], name=sid)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate data frame for a set of sids.", "response": "def dataframe(self, sids, head=0, tail=EPOCHS_MAX, datetime=True):\n        \"\"\"\n        Create data frame\n\n        Parameters\n        ----------\n        sids : list[str]\n        head : int | pandas.Timestamp, optional\n            Start of the interval\n            default earliest available\n        tail : int | pandas.Timestamp, optional\n            End of the interval\n            default max epoch\n        datetime : bool\n            convert index to datetime\n            default True\n\n        Returns\n        -------\n        pandas.DataFrame\n        \"\"\"\n        if head is None:\n            head = 0\n        else:\n            head = self._2epochs(head)\n\n        if tail is None:\n            tail = EPOCHS_MAX\n        else:\n            tail = self._2epochs(tail)\n\n        series = [self.series(sid, head=head, tail=tail, datetime=False)\n                  for sid in sids]\n        df = pd.concat(series, axis=1)\n        if datetime is True:\n            df.index = pd.to_datetime(df.index, unit=\"s\", utc=True)\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the first available timestamp for a sensor.", "response": "def first_timestamp(self, sid, epoch=False):\n        \"\"\"\n        Get the first available timestamp for a sensor\n\n        Parameters\n        ----------\n        sid : str\n            SensorID\n        epoch : bool\n            default False\n            If True return as epoch\n            If False return as pd.Timestamp\n\n        Returns\n        -------\n        pd.Timestamp | int\n        \"\"\"\n        first_block = self.dbcur.execute(SQL_TMPO_FIRST, (sid,)).fetchone()\n        if first_block is None:\n            return None\n\n        timestamp = first_block[2]\n        if not epoch:\n            timestamp = pd.Timestamp.utcfromtimestamp(timestamp)\n            timestamp = timestamp.tz_localize('UTC')\n        return timestamp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the theoretical last timestamp for a sensor.", "response": "def last_timestamp(self, sid, epoch=False):\n        \"\"\"\n        Get the theoretical last timestamp for a sensor\n\n        Parameters\n        ----------\n        sid : str\n            SensorID\n        epoch : bool\n            default False\n            If True return as epoch\n            If False return as pd.Timestamp\n\n        Returns\n        -------\n        pd.Timestamp | int\n        \"\"\"\n        timestamp, value = self.last_datapoint(sid, epoch)\n        return timestamp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the last datapoint for a specific SensorId.", "response": "def last_datapoint(self, sid, epoch=False):\n        \"\"\"\n        Parameters\n        ----------\n        sid : str\n            SensorId\n        epoch : bool\n            default False\n            If True return as epoch\n            If False return as pd.Timestamp\n\n        Returns\n        -------\n        pd.Timestamp | int, float\n        \"\"\"\n        block = self._last_block(sid)\n        if block is None:\n            return None, None\n\n        header = block['h']\n        timestamp, value = header['tail']\n\n        if not epoch:\n            timestamp = pd.Timestamp.utcfromtimestamp(timestamp)\n            timestamp = timestamp.tz_localize('UTC')\n\n        return timestamp, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _npdelta(self, a, delta):\n        for x in np.nditer(a, op_flags=[\"readwrite\"]):\n            delta += x\n            x[...] = delta\n        return a", "response": "Modify a by the delta"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a string and parses it to get a dict of the keyword args and their values.", "response": "def sigStrToKwArgsDict(checkFuncSig):\n    \"\"\" Take a check function signature (string), and parse it to get a dict\n        of the keyword args and their values. \"\"\"\n    p1 = checkFuncSig.find('(')\n    p2 = checkFuncSig.rfind(')')\n    assert p1 > 0 and p2 > 0 and p2 > p1, \"Invalid signature: \"+checkFuncSig\n    argParts = irafutils.csvSplit(checkFuncSig[p1+1:p2], ',', True)\n    argParts = [x.strip() for x in argParts]\n    retval = {}\n    for argPair in argParts:\n        argSpl = argPair.split('=', 1)\n        if len(argSpl) > 1:\n            if argSpl[0] in retval:\n                if isinstance(retval[argSpl[0]], (list,tuple)):\n                    retval[argSpl[0]]+=(irafutils.stripQuotes(argSpl[1]),) # 3rd\n                else: # 2nd in, so convert to tuple\n                    retval[argSpl[0]] = (retval[argSpl[0]],\n                                         irafutils.stripQuotes(argSpl[1]),)\n            else:\n                retval[argSpl[0]] = irafutils.stripQuotes(argSpl[1]) # 1st in\n        else:\n            retval[argSpl[0]] = None #  eg. found \"triggers=, max=6, ...\"\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef separateKeywords(kwArgsDict):\n    standardKws = {}\n    ourKws = {}\n    for k in kwArgsDict:\n        if k in STANDARD_KEYS:\n            standardKws[k]=kwArgsDict[k]\n        else:\n            ourKws[k]=kwArgsDict[k]\n    return (standardKws, ourKws)", "response": "Look through the keywords passed and separate the legal and standard ones."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naltering the passed function signature string to add the given kewords", "response": "def addKwdArgsToSig(sigStr, kwArgsDict):\n    \"\"\" Alter the passed function signature string to add the given kewords \"\"\"\n    retval = sigStr\n    if len(kwArgsDict) > 0:\n        retval = retval.strip(' ,)') # open up the r.h.s. for more args\n        for k in kwArgsDict:\n            if retval[-1] != '(': retval += \", \"\n            retval += str(k)+\"=\"+str(kwArgsDict[k])\n        retval += ')'\n    retval = retval\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gauss_funct(p, fjac=None, x=None, y=None, err=None,\n                 weights=None):\n\n    \"\"\"\n    Defines the gaussian function to be used as the model.\n\n    \"\"\"\n    if p[2] != 0.0:\n        Z = (x - p[1]) / p[2]\n        model = p[0] * np.e ** (-Z ** 2 / 2.0)\n    else:\n        model = np.zeros(np.size(x))\n\n    status = 0\n    if weights is not None:\n        if err is not None:\n            print(\"Warning: Ignoring errors and using weights.\\n\")\n\n        return [status, (y - model) * weights]\n\n    elif err is not None:\n        return [status, (y - model) / err]\n\n    else:\n        return [status, y - model]", "response": "Function to compute the gaussian function for the current state of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gfit1d(y, x=None, err=None, weights=None, par=None, parinfo=None,\n           maxiter=200, quiet=0):\n    \"\"\"\n    Return the gaussian fit as an object.\n\n    Parameters\n    ----------\n    y:   1D Numpy array\n        The data to be fitted\n    x:   1D Numpy array\n        (optional) The x values of the y array. x and y must\n        have the same shape.\n    err: 1D Numpy array\n        (optional) 1D array with measurement errors, must be\n        the same shape as y\n    weights: 1D Numpy array\n        (optiional) 1D array with weights, must be the same\n        shape as y\n    par:  List\n        (optional) Starting values for the parameters to be fitted\n    parinfo: Dictionary of lists\n        (optional) provides additional information for the\n        parameters. For a detailed description see nmpfit.py.\n        Parinfo can be used to limit parameters or keep\n        some of them fixed.\n    maxiter: number\n        Maximum number of iterations to perform\n        Default: 200\n    quiet: number\n        if set to 1, nmpfit does not print to the screen\n        Default: 0\n\n    Examples\n    --------\n    >>> x = np.arange(10,20, 0.1)\n    >>> y= 10*np.e**(-(x-15)**2/4)\n    >>> print(gfit1d(y,x=x, maxiter=20,quiet=1).params)\n    [10.         15.          1.41421356]\n\n    \"\"\"\n    y = y.astype(np.float)\n    if weights is not None:\n        weights = weights.astype(np.float)\n    if err is not None:\n        err = err.astype(np.float)\n    if x is None and len(y.shape) == 1:\n        x = np.arange(len(y)).astype(np.float)\n    if x.shape != y.shape:\n        print(\"input arrays X and Y must be of equal shape.\\n\")\n        return\n\n    fa = {'x': x, 'y': y, 'err': err, 'weights': weights}\n\n    if par is not None:\n        p = par\n    else:\n        ysigma = y.std()\n        ind = np.nonzero(y > ysigma)[0]\n        if len(ind) != 0:\n            xind = int(ind.mean())\n            p2 = x[xind]\n            p1 = y[xind]\n            p3 = 1.0\n        else:\n            ymax = y.max()\n            ymin = y.min()\n            ymean= y.mean()\n            if (ymax - ymean) > (abs(ymin - ymean)):\n                p1 = ymax\n            else: p1 = ymin\n            ind = (np.nonzero(y == p1))[0]\n            p2 = x.mean()\n            p3 = 1.\n\n\n        p = [p1, p2, p3]\n    m = nmpfit.mpfit(_gauss_funct, p,parinfo = parinfo, functkw=fa,\n    maxiter=maxiter, quiet=quiet)\n    if (m.status <= 0): print('error message = ', m.errmsg)\n    return m", "response": "Returns the gaussian fit as an object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, *args, **kwargs):\n        obj = kwargs.pop(self.object_property_name, None)\n        if obj is not None:\n            kwargs['object_hash'] = self.model._compute_hash(obj)\n        return super().filter(*args, **kwargs)", "response": "lets django managers use objects. filter on a hashable object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self):\n        if self.is_file_persisted:\n            self.object_file.open()\n            temp = dill.loads(self.object_file.read())\n            self.set_object(temp)\n            self.object_file.close()", "response": "a private method that loads an estimator object from the filesystem"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an Estimator object given the path of the file relative to the MEDIA_ROOT", "response": "def create_from_file(cls, filename):\n        \"\"\"Return an Estimator object given the path of the file, relative to the MEDIA_ROOT\"\"\"\n        obj = cls()\n        obj.object_file = filename\n        obj.load()\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning our application dir. Create it if it doesn t exist.", "response": "def getAppDir():\n    \"\"\" Return our application dir.  Create it if it doesn't exist. \"\"\"\n    # Be sure the resource dir exists\n    theDir = os.path.expanduser('~/.')+APP_NAME.lower()\n    if not os.path.exists(theDir):\n        try:\n            os.mkdir(theDir)\n        except OSError:\n            print('Could not create \"'+theDir+'\" to save GUI settings.')\n            theDir = \"./\"+APP_NAME.lower()\n    return theDir"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getObjectFromTaskArg(theTask, strict, setAllToDefaults):\n\n    # Already in the form we need (instance of us or of subclass)\n    if isinstance(theTask, ConfigObjPars):\n        if setAllToDefaults:\n            raise RuntimeError('Called getObjectFromTaskArg with existing'+\\\n                  ' object AND setAllToDefaults - is unexpected use case.')\n        # If it is an existing object, make sure it's internal param list is\n        # up to date with it's ConfigObj dict, since the user may have manually\n        # edited the dict before calling us.\n        theTask.syncParamList(False) # use strict somehow?\n        # Note - some validation is done here in IrafPar creation, but it is\n        # not the same validation done by the ConfigObj s/w (no check funcs).\n        # Do we want to do that too here?\n        return theTask\n\n    # For example, a .cfg file\n    if os.path.isfile(str(theTask)):\n        try:\n            return ConfigObjPars(theTask, strict=strict,\n                                 setAllToDefaults=setAllToDefaults)\n        except KeyError:\n            # this might just be caused by a file sitting in the local cwd with\n            # the same exact name as the package we want to import, let's see\n            if theTask.find('.') > 0: # it has an extension, like '.cfg'\n                raise # this really was an error\n            # else we drop down to the next step - try it as a pkg name\n\n    # Else it must be a Python package name to load\n    if isinstance(theTask, str) and setAllToDefaults:\n        # NOTE how we pass the task name string in setAllToDefaults\n        return ConfigObjPars('', setAllToDefaults=theTask, strict=strict)\n    else:\n        return getParsObjForPyPkg(theTask, strict)", "response": "Take the arg and return a ConfigObjPars object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getEmbeddedKeyVal(cfgFileName, kwdName, dflt=None):\n    # Assume this is a ConfigObj file.  Use that s/w to quickly read it and\n    # put it in dict format.  Assume kwd is at top level (not in a section).\n    # The input may also be a .cfgspc file.\n    #\n    # Only use ConfigObj here as a tool to generate a dict from a file - do\n    # not use the returned object as a ConfigObj per se.  As such, we can call\n    # with \"simple\" format, ie. no cfgspc, no val'n, and \"list_values\"=False.\n    try:\n        junkObj = configobj.ConfigObj(cfgFileName, list_values=False)\n    except:\n        if kwdName == TASK_NAME_KEY:\n            raise KeyError('Can not parse as a parameter config file: '+ \\\n                           '\\n\\t'+os.path.realpath(cfgFileName))\n        else:\n            raise KeyError('Unfound key \"'+kwdName+'\" while parsing: '+ \\\n                           '\\n\\t'+os.path.realpath(cfgFileName))\n\n    if kwdName in junkObj:\n        retval = junkObj[kwdName]\n        del junkObj\n        return retval\n    # Not found\n    if dflt is not None:\n        del junkObj\n        return dflt\n    else:\n        if kwdName == TASK_NAME_KEY:\n            raise KeyError('Can not parse as a parameter config file: '+ \\\n                           '\\n\\t'+os.path.realpath(cfgFileName))\n        else:\n            raise KeyError('Unfound key \"'+kwdName+'\" while parsing: '+ \\\n                           '\\n\\t'+os.path.realpath(cfgFileName))", "response": "Read a config file and pull out the value of a given keyword."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlocates the configuration files for a given python package.", "response": "def findCfgFileForPkg(pkgName, theExt, pkgObj=None, taskName=None):\n    \"\"\" Locate the configuration files for/from/within a given python package.\n    pkgName is a string python package name.  This is used unless pkgObj\n    is given, in which case pkgName is taken from pkgObj.__name__.\n    theExt is either '.cfg' or '.cfgspc'. If the task name is known, it is\n    given as taskName, otherwise one is determined using the pkgName.\n    Returns a tuple of (package-object, cfg-file-name). \"\"\"\n    # arg check\n    ext = theExt\n    if ext[0] != '.': ext = '.'+theExt\n\n    # Do the import, if needed\n    pkgsToTry = {}\n    if pkgObj:\n        pkgsToTry[pkgObj.__name__] = pkgObj\n    else:\n        # First try something simple like a regular or dotted import\n        try:\n            fl = []\n            if pkgName.find('.') > 0:\n                fl = [ pkgName[:pkgName.rfind('.')], ]\n            pkgsToTry[str(pkgName)] = __import__(str(pkgName), fromlist=fl)\n        except:\n            throwIt = True\n            # One last case to try is something like \"csc_kill\" from\n            # \"acstools.csc_kill\", but this convenience capability will only be\n            # allowed if the parent pkg (acstools) has already been imported.\n            if isinstance(pkgName, string_types) and pkgName.find('.') < 0:\n                matches = [x for x in sys.modules.keys() \\\n                           if x.endswith(\".\"+pkgName)]\n                if len(matches)>0:\n                    throwIt = False\n                    for mmm in matches:\n                        pkgsToTry[mmm] = sys.modules[mmm]\n            if throwIt:\n                raise NoCfgFileError(\"Unfound package or \"+ext+\" file via: \"+\\\n                                     \"import \"+str(pkgName))\n\n    # Now that we have the package object (or a few of them to try), for each\n    # one find the .cfg or .cfgspc file, and return\n    # Return as soon as ANY match is found.\n    for aPkgName in pkgsToTry:\n        aPkg = pkgsToTry[aPkgName]\n        path = os.path.dirname(aPkg.__file__)\n        if len(path) < 1: path = '.'\n        flist = irafutils.rglob(path, \"*\"+ext)\n        if len(flist) < 1:\n            continue\n\n        # Go through these and find the first one for the assumed or given task\n        # name.  The task name for 'BigBlackBox.drizzle' would be 'drizzle'.\n        if taskName is None:\n            taskName = aPkgName.split(\".\")[-1]\n        flist.sort()\n        for f in flist:\n            # A .cfg file gets checked for _task_name_=val, but a .cfgspc file\n            # will have a string check function signature as the val.\n            if ext == '.cfg':\n                itsTask = getEmbeddedKeyVal(f, TASK_NAME_KEY, '')\n            else: # .cfgspc\n                sigStr  = getEmbeddedKeyVal(f, TASK_NAME_KEY, '')\n                # .cfgspc file MUST have an entry for TASK_NAME_KEY w/ a default\n                itsTask = vtor_checks.sigStrToKwArgsDict(sigStr)['default']\n            if itsTask == taskName:\n                # We've found the correct file in an installation area.  Return\n                # the package object and the found file.\n                return aPkg, f\n\n    # What, are you still here?\n    raise NoCfgFileError('No valid '+ext+' files found in package: \"'+ \\\n                         str(pkgName)+'\" for task: \"'+str(taskName)+'\"')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef findAllCfgTasksUnderDir(aDir):\n    retval = {}\n    for f in irafutils.rglob(aDir, '*.cfg'):\n        retval[f] = getEmbeddedKeyVal(f, TASK_NAME_KEY, '')\n    return retval", "response": "Find all installed tasks under a given directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getParsObjForPyPkg(pkgName, strict):\n    # Get the python package and it's .cfg file - need this no matter what\n    installedPkg, installedFile = findCfgFileForPkg(pkgName, '.cfg')\n    theFile = None\n    tname = getEmbeddedKeyVal(installedFile, TASK_NAME_KEY)\n\n    # See if the user has any of their own .cfg files in the cwd for this task\n    if theFile is None:\n        flist = getCfgFilesInDirForTask(os.getcwd(), tname)\n        if len(flist) > 0:\n            if len(flist) == 1: # can skip file times sort\n                theFile = flist[0]\n            else:\n                # There are a few different choices.  In the absence of\n                # requirements to the contrary, just take the latest.  Set up a\n                # list of tuples of (mtime, fname) so we can sort by mtime.\n                ftups = [ (os.stat(f)[stat.ST_MTIME], f) for f in flist]\n                ftups.sort()\n                theFile = ftups[-1][1]\n\n    # See if the user has any of their own app-dir .cfg files for this task\n    if theFile is None:\n        flist = getCfgFilesInDirForTask(getAppDir(), tname) # verifies tname\n        flist = [f for f in flist if os.path.basename(f) == tname+'.cfg']\n        if len(flist) > 0:\n            theFile = flist[0]\n            assert len(flist) == 1, str(flist) # should never happen\n\n    # Add code to check an env. var defined area?  (speak to users first)\n\n    # Did we find one yet?  If not, use the installed version\n    useInstVer = False\n    if theFile is None:\n        theFile = installedFile\n        useInstVer = True\n\n    # Create a stand-in instance from this file.  Force a read-only situation\n    # if we are dealing with the installed, (expected to be) unwritable file.\n    return ConfigObjPars(theFile, associatedPkg=installedPkg,\n                         forceReadOnly=useInstVer, strict=strict)", "response": "Locate the appropriate ConfigObjPars object for the given python package."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of. cfg files for a given python package.", "response": "def getUsrCfgFilesForPyPkg(pkgName):\n    \"\"\" See if the user has one of their own local .cfg files for this task,\n        such as might be created automatically during the save of a read-only\n        package, and return their names. \"\"\"\n    # Get the python package and it's .cfg file\n    thePkg, theFile = findCfgFileForPkg(pkgName, '.cfg')\n    # See if the user has any of their own local .cfg files for this task\n    tname = getEmbeddedKeyVal(theFile, TASK_NAME_KEY)\n    flist = getCfgFilesInDirForTask(getAppDir(), tname)\n    return flist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the file is read - only and set it to read - only.", "response": "def checkSetReadOnly(fname, raiseOnErr = False):\n    \"\"\" See if we have write-privileges to this file.  If we do, and we\n    are not supposed to, then fix that case. \"\"\"\n    if os.access(fname, os.W_OK):\n        # We can write to this but it is supposed to be read-only. Fix it.\n        # Take away usr-write, leave group and other alone, though it\n        # may be simpler to just force/set it to: r--r--r-- or r--------\n        irafutils.setWritePrivs(fname, False, ignoreErrors= not raiseOnErr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flattenDictTree(aDict):\n    retval = {}\n    for k in aDict:\n        val = aDict[k]\n        if isinstance(val, dict):\n            # This val is a dict, get its data (recursively) into a flat dict\n            subDict = flattenDictTree(val)\n            # Merge its dict of data into ours, watching for NO collisions\n            rvKeySet  = set(retval.keys())\n            sdKeySet = set(subDict.keys())\n            intr = rvKeySet.intersection(sdKeySet)\n            if len(intr) > 0:\n                raise DuplicateKeyError(\"Flattened dict already has \"+ \\\n                    \"key(s): \"+str(list(intr))+\" - cannot flatten this.\")\n\n            else:\n                retval.update(subDict)\n        else:\n            if k in retval:\n                raise DuplicateKeyError(\"Flattened dict already has key: \"+\\\n                                        k+\" - cannot flatten this.\")\n            else:\n                retval[k] = val\n    return retval", "response": "Takes a dict of vals and dicts as input and returns a flat dict of all key - values and dicts as output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the number of times the given par exists in this dict - tree and the given key name.", "response": "def countKey(theDict, name):\n    \"\"\" Return the number of times the given par exists in this dict-tree,\n    since the same key name may be used in different sections/sub-sections. \"\"\"\n\n    retval = 0\n    for key in theDict:\n        val = theDict[key]\n        if isinstance(val, dict):\n            retval += countKey(val, name) # recurse\n        else:\n            if key == name:\n                retval += 1\n                # can't break, even tho we found a hit, other items on\n                # this level will not be named \"name\", but child dicts\n                # may have further counts\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the first par in the given dictionary.", "response": "def findFirstPar(theDict, name, _depth=0):\n    \"\"\" Find the given par.  Return tuple: (its own (sub-)dict, its value).\n    Returns the first match found, without checking whether the given key name\n    is unique or whether it is used in multiple sections. \"\"\"\n\n    for key in theDict:\n        val = theDict[key]\n#       print _depth*'   ', key, str(val)[:40]\n        if isinstance(val, dict):\n            retval = findFirstPar(val, name, _depth=_depth+1) # recurse\n            if retval is not None:\n                return retval\n            # else keep looking\n        else:\n            if key == name:\n                return theDict, theDict[name]\n            # else keep looking\n    # if we get here then we have searched this whole (sub)-section and its\n    # descendants, and found no matches.  only raise if we are at the top.\n    if _depth == 0:\n        raise KeyError(name)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the given par in the given scope.", "response": "def findScopedPar(theDict, scope, name):\n    \"\"\" Find the given par.  Return tuple: (its own (sub-)dict, its value). \"\"\"\n    # Do not search (like findFirstPar), but go right to the correct\n    # sub-section, and pick it up.  Assume it is there as stated.\n    if len(scope):\n        theDict = theDict[scope] # ! only goes one level deep - enhance !\n    return theDict, theDict[name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setPar(theDict, name, value):\n    section, previousVal = findFirstPar(theDict, name)\n    # \"section\" is the actual object, not a copy\n    section[name] = value", "response": "Sets a par s value without having to give its scope."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mergeConfigObj(configObj, inputDict):\n    # Expanded upon Warren's version in astrodrizzle\n\n    # Verify that all inputDict keys in configObj are unique within configObj\n    for key in inputDict:\n        if countKey(configObj, key) > 1:\n            raise DuplicateKeyError(key)\n    # Now update configObj with each inputDict item\n    for key in inputDict:\n        setPar(configObj, key, inputDict[key])", "response": "Merge the inputDict values into an existing configObj instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef findTheLost(config_file, configspec_file, skipHidden=True):\n    # do some sanity checking, but don't (yet) make this a serious error\n    if not os.path.exists(config_file):\n        print(\"ERROR: Config file not found: \"+config_file)\n        return []\n    if not os.path.exists(configspec_file):\n        print(\"ERROR: Configspec file not found: \"+configspec_file)\n        return []\n    tmpObj = configobj.ConfigObj(config_file, configspec=configspec_file)\n    simval = configobj.SimpleVal()\n    test = tmpObj.validate(simval)\n    if test == True:\n        return []\n    # If we get here, there is a dict returned of {key1: bool, key2: bool}\n    # which matches the shape of the config obj.  We need to walk it to\n    # find the Falses, since they are the missing pars.\n    missing = []\n    flattened = configobj.flatten_errors(tmpObj, test)\n    # But, before we move on, skip/eliminate any 'hidden' items from our list,\n    # since hidden items are really supposed to be missing from the .cfg file.\n    if len(flattened) > 0 and skipHidden:\n        keepers = []\n        for tup in flattened:\n            keep = True\n            # hidden section\n            if len(tup[0])>0 and isHiddenName(tup[0][-1]):\n                keep = False\n            # hidden par (in a section, or at the top level)\n            elif tup[1] is not None and isHiddenName(tup[1]):\n                keep = False\n            if keep:\n                keepers.append(tup)\n        flattened = keepers\n    flatStr = flattened2str(flattened, missing=True)\n    return flatStr", "response": "Find any lost or missing parameters in this cfg file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if this string name denotes a hidden par or section", "response": "def isHiddenName(astr):\n    \"\"\" Return True if this string name denotes a hidden par or section \"\"\"\n    if astr is not None and len(astr) > 2 and astr.startswith('_') and \\\n       astr.endswith('_'):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a pretty - printed string version of the output of a list of flattened items.", "response": "def flattened2str(flattened, missing=False, extra=False):\n    \"\"\" Return a pretty-printed multi-line string version of the output of\n    flatten_errors. Know that flattened comes in the form of a list\n    of keys that failed. Each member of the list is a tuple::\n\n        ([list of sections...], key, result)\n\n    so we turn that into a string. Set missing to True if all the input\n    problems are from missing items.  Set extra to True if all the input\n    problems are from extra items. \"\"\"\n\n    if flattened is None or len(flattened) < 1:\n        return ''\n    retval = ''\n    for sections, key, result in flattened:\n        # Name the section and item, to start the message line\n        if sections is None or len(sections) == 0:\n            retval += '\\t\"'+key+'\"'\n        elif len(sections) == 1:\n            if key is None:\n                # a whole section is missing at the top-level; see if hidden\n                junk = sections[0]\n                if isHiddenName(junk):\n                    continue # this missing or extra section is not an error\n                else:\n                    retval += '\\tSection \"'+sections[0]+'\"'\n            else:\n                retval += '\\t\"'+sections[0]+'.'+key+'\"'\n        else: # len > 1\n            joined = '.'.join(sections)\n            joined = '\"'+joined+'\"'\n            if key is None:\n                retval +=  '\\tSection '+joined\n            else:\n                retval +=  '\\t\"'+key+'\" from '+joined\n        # End the msg line with \"what seems to be the trouble\" with this one\n        if missing and result==False:\n            retval += ' is missing.'\n        elif extra:\n            if result:\n                retval += ' is an unexpected section. Is your file out of date?'\n            else:\n                retval += ' is an unexpected parameter. Is your file out of date?'\n        elif isinstance(result, bool):\n            retval += ' has an invalid value'\n        else:\n            retval += ' is invalid, '+result.message\n        retval += '\\n\\n'\n    return retval.rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getDefaultSaveFilename(self, stub=False):\n        if stub:\n            return self._rcDir+os.sep+self.__taskName+'_stub.cfg'\n        else:\n            return self._rcDir+os.sep+self.__taskName+'.cfg'", "response": "Return the name of the default save file for this task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset or reset the internal param list from the dict.", "response": "def syncParamList(self, firstTime, preserve_order=True):\n        \"\"\" Set or reset the internal param list from the dict's contents. \"\"\"\n        # See the note in setParam about this design.\n\n        # Get latest par values from dict.  Make sure we do not\n        # change the id of the __paramList pointer here.\n        new_list = self._getParamsFromConfigDict(self, initialPass=firstTime)\n                                               # dumpCfgspcTo=sys.stdout)\n        # Have to add this odd last one for the sake of the GUI (still?)\n        if self._forUseWithEpar:\n            new_list.append(basicpar.IrafParS(['$nargs','s','h','N']))\n\n        if len(self.__paramList) > 0 and preserve_order:\n            # Here we have the most up-to-date data from the actual data\n            # model, the ConfigObj dict, and we need to use it to fill in\n            # our param list.  BUT, we need to preserve the order our list\n            # has had up until now (by unique parameter name).\n            namesInOrder = [p.fullName() for p in self.__paramList]\n            assert len(namesInOrder) == len(new_list), \\\n                   'Mismatch in num pars, had: '+str(len(namesInOrder))+ \\\n                   ', now we have: '+str(len(new_list))+', '+ \\\n                   str([p.fullName() for p in new_list])\n            self.__paramList[:] = [] # clear list, keep same pointer\n            # create a flat dict view of new_list, for ease of use in next step\n            new_list_dict = {} # can do in one step in v2.7\n            for par in new_list: new_list_dict[par.fullName()] = par\n            # populate\n            for fn in namesInOrder:\n                self.__paramList.append(new_list_dict[fn])\n        else:\n            # Here we just take the data in whatever order it came.\n            self.__paramList[:] = new_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a par list just like ours but with all default values.", "response": "def getDefaultParList(self):\n        \"\"\" Return a par list just like ours, but with all default values. \"\"\"\n        # The code below (create a new set-to-dflts obj) is correct, but it\n        # adds a tenth of a second to startup.  Clicking \"Defaults\" in the\n        # GUI does not call this.  But this can be used to set the order seen.\n\n        # But first check for rare case of no cfg file name\n        if self.filename is None:\n            # this is a .cfgspc-only kind of object so far\n            self.filename = self.getDefaultSaveFilename(stub=True)\n            return copy.deepcopy(self.__paramList)\n\n        tmpObj = ConfigObjPars(self.filename, associatedPkg=self.__assocPkg,\n                               setAllToDefaults=True, strict=False)\n        return tmpObj.getParList()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setParam(self, name, val, scope='', check=1, idxHint=None):\n        theDict, oldVal = findScopedPar(self, scope, name)\n\n        # Set the value, even if invalid.  It needs to be set before\n        # the validation step (next).\n        theDict[name] = val\n\n        # If need be, check the proposed value.  Ideally, we'd like to\n        # (somehow elegantly) only check this one item. For now, the best\n        # shortcut is to only validate this section.\n        if check:\n            ans=self.validate(self._vtor, preserve_errors=True, section=theDict)\n            if ans != True:\n                flatStr = \"All values are invalid!\"\n                if ans != False:\n                    flatStr = flattened2str(configobj.flatten_errors(self, ans))\n                raise RuntimeError(\"Validation error: \"+flatStr)\n\n        # Note - this design needs work.  Right now there are two copies\n        # of the data:  the ConfigObj dict, and the __paramList ...\n        # We rely on the idxHint arg so we don't have to search the __paramList\n        # every time this is called, which could really slows things down.\n        assert idxHint is not None, \"ConfigObjPars relies on a valid idxHint\"\n        assert name == self.__paramList[idxHint].name, \\\n               'Error in setParam, name: \"'+name+'\" != name at idxHint: \"'+\\\n               self.__paramList[idxHint].name+'\", idxHint: '+str(idxHint)\n        self.__paramList[idxHint].set(val)", "response": "Set the value of a parameter in the ConfigObj."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the list of parameters to a file.", "response": "def saveParList(self, *args, **kw):\n        \"\"\"Write parameter data to filename (string or filehandle)\"\"\"\n        if 'filename' in kw:\n            filename = kw['filename']\n        if not filename:\n            filename = self.getFilename()\n        if not filename:\n            raise ValueError(\"No filename specified to save parameters\")\n\n        if hasattr(filename,'write'):\n            fh = filename\n            absFileName = os.path.abspath(fh.name)\n        else:\n            absFileName = os.path.expanduser(filename)\n            absDir = os.path.dirname(absFileName)\n            if len(absDir) and not os.path.isdir(absDir): os.makedirs(absDir)\n            fh = open(absFileName,'w')\n        numpars = len(self.__paramList)\n        if self._forUseWithEpar: numpars -= 1\n        if not self.final_comment: self.final_comment = [''] # force \\n at EOF\n        # Empty the ConfigObj version of section.defaults since that is based\n        # on an assumption incorrect for us, and override with our own list.\n        # THIS IS A BIT OF MONKEY-PATCHING!  WATCH FUTURE VERSION CHANGES!\n        # See Trac ticket #762.\n        while len(self.defaults):\n            self.defaults.pop(-1) # empty it, keeping ref\n        for key in self._neverWrite:\n            self.defaults.append(key)\n        # Note also that we are only overwriting the top/main section's\n        # \"defaults\" list, but EVERY [sub-]section has such an attribute...\n\n        # Now write to file, delegating work to ConfigObj (note that ConfigObj\n        # write() skips any items listed by name in the self.defaults list)\n        self.write(fh)\n        fh.close()\n        retval = str(numpars) + \" parameters written to \" + absFileName\n        self.filename = absFileName # reset our own ConfigObj filename attr\n        self.debug('Keys not written: '+str(self.defaults))\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, *args, **kw):\n        if self._runFunc is not None:\n            # remove the two args sent by EditParDialog which we do not use\n            if 'mode' in kw: kw.pop('mode')\n            if '_save' in kw: kw.pop('_save')\n            return self._runFunc(self, *args, **kw)\n        else:\n            raise taskpars.NoExecError('No way to run task \"'+self.__taskName+\\\n                '\". You must either override the \"run\" method in your '+ \\\n                'ConfigObjPars subclass, or you must supply a \"run\" '+ \\\n                'function in your package.')", "response": "This method is overridden by a subclass to call the run method of the class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef triggerLogicToStr(self):\n        try:\n            import json\n        except ImportError:\n            return \"Cannot dump triggers/dependencies/executes (need json)\"\n        retval = \"TRIGGERS:\\n\"+json.dumps(self._allTriggers, indent=3)\n        retval += \"\\nDEPENDENCIES:\\n\"+json.dumps(self._allDepdcs, indent=3)\n        retval += \"\\nTO EXECUTE:\\n\"+json.dumps(self._allExecutes, indent=3)\n        retval += \"\\n\"\n        return retval", "response": "Print all the trigger logic to a string and return it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _findAssociatedConfigSpecFile(self, cfgFileName):\n\n        # Handle simplest 2 cases first: co-located or local .cfgspc file\n        retval = \".\"+os.sep+self.__taskName+\".cfgspc\"\n        if os.path.isfile(retval): return retval\n\n        retval = os.path.dirname(cfgFileName)+os.sep+self.__taskName+\".cfgspc\"\n        if os.path.isfile(retval): return retval\n\n        # Also try the resource dir\n        retval = self.getDefaultSaveFilename()+'spc' # .cfgspc\n        if os.path.isfile(retval): return retval\n\n        # Now try and see if there is a matching .cfgspc file in/under an\n        # associated package, if one is defined.\n        if self.__assocPkg is not None:\n            x, theFile = findCfgFileForPkg(None, '.cfgspc',\n                                           pkgObj = self.__assocPkg,\n                                           taskName = self.__taskName)\n            return theFile\n\n        # Finally try to import the task name and see if there is a .cfgspc\n        # file in that directory\n        x, theFile = findCfgFileForPkg(self.__taskName, '.cfgspc',\n                                       taskName = self.__taskName)\n        if os.path.exists(theFile):\n            return theFile\n\n        # unfound\n        raise NoCfgFileError('Unfound config-spec file for task: \"'+ \\\n                             self.__taskName+'\"')", "response": "Given a config file find its associated config - spec file and and\n        return the full pathname of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getParamsFromConfigDict(self, cfgObj, scopePrefix='',\n                                 initialPass=False, dumpCfgspcTo=None):\n        \"\"\" Walk the given ConfigObj dict pulling out IRAF-like parameters into\n        a list. Since this operates on a dict this can be called recursively.\n        This is also our chance to find and pull out triggers and such\n        dependencies. \"\"\"\n        # init\n        retval = []\n        if initialPass and len(scopePrefix) < 1:\n            self._posArgs = [] # positional args [2-tuples]: (index,scopedName)\n            # FOR SECURITY: the following 3 chunks of data,\n            #     _allTriggers, _allDepdcs, _allExecutes,\n            # are collected ONLY from the .cfgspc file\n            self._allTriggers = {}\n            self._allDepdcs = {}\n            self._allExecutes = {}\n\n        # start walking (\"tell yer story walkin, buddy\")\n        # NOTE: this relies on the \"in\" operator returning keys in the\n        # order that they exist in the dict (which depends on ConfigObj keeping\n        # the order they were found in the original file)\n        for key in cfgObj:\n            val = cfgObj[key]\n\n            # Do we need to skip this - if not a par, like a rule or something\n            toBeHidden = isHiddenName(key)\n            if toBeHidden:\n                if key not in self._neverWrite and key != TASK_NAME_KEY:\n                    self._neverWrite.append(key)\n                    # yes TASK_NAME_KEY is hidden, but it IS output to the .cfg\n\n            # a section\n            if isinstance(val, dict):\n                if not toBeHidden:\n                    if len(list(val.keys()))>0 and len(retval)>0:\n                        # Here is where we sneak in the section comment\n                        # This is so incredibly kludgy (as the code was), it\n                        # MUST be revamped eventually! This is for the epar GUI.\n                        prevPar = retval[-1]\n                        # Use the key (or its comment?) as the section header\n                        prevPar.set(prevPar.get('p_prompt')+'\\n\\n'+key,\n                                    field='p_prompt', check=0)\n                    if dumpCfgspcTo:\n                        dumpCfgspcTo.write('\\n['+key+']\\n')\n                    # a logical grouping (append its params)\n                    pfx = scopePrefix+'.'+key\n                    pfx = pfx.strip('.')\n                    retval = retval + self._getParamsFromConfigDict(val, pfx,\n                                      initialPass, dumpCfgspcTo) # recurse\n            else:\n                # a param\n                fields = []\n                choicesOrMin = None\n                fields.append(key) # name\n                dtype = 's'\n                cspc = None\n                if cfgObj.configspec:\n                    cspc = cfgObj.configspec.get(key) # None if not found\n                chk_func_name = ''\n                chk_args_dict = {}\n                if cspc:\n                    chk_func_name = cspc[:cspc.find('(')]\n                    chk_args_dict = vtor_checks.sigStrToKwArgsDict(cspc)\n                if chk_func_name.find('option') >= 0:\n                    dtype = 's'\n                    # convert the choices string to a list (to weed out kwds)\n                    x = cspc[cspc.find('(')+1:-1] # just the options() args\n# cspc e.g.: option_kw(\"poly5\",\"nearest\",\"linear\", default=\"poly5\", comment=\"Interpolant (poly5,nearest,linear)\")\n                    x = x.split(',') # tokenize\n                    # but! comment value may have commas in it, find it\n                    # using it's equal sign, rm all after it\n                    has_eq = [i for i in x if i.find('=')>=0]\n                    if len(has_eq) > 0:\n                        x = x[: x.index(has_eq[0]) ]\n                    # rm spaces, extra quotes; rm kywd arg pairs\n                    x = [i.strip(\"' \") for i in x if i.find('=')<0]\n                    choicesOrMin = '|'+'|'.join(x)+'|' # IRAF format for enums\n                elif chk_func_name.find('boolean') >= 0:     dtype = 'b'\n                elif chk_func_name.find('float_or_') >= 0:   dtype = 'r'\n                elif chk_func_name.find('float') >= 0:       dtype = 'R'\n                elif chk_func_name.find('integer_or_') >= 0: dtype = 'i'\n                elif chk_func_name.find('integer') >= 0:     dtype = 'I'\n                elif chk_func_name.find('action') >= 0:      dtype = 'z'\n                fields.append(dtype)\n                fields.append('a')\n                if type(val)==bool:\n                    if val: fields.append('yes')\n                    else:   fields.append('no')\n                else:\n                    fields.append(val)\n                fields.append(choicesOrMin)\n                fields.append(None)\n                # Primarily use description from .cfgspc file (0). But, allow\n                # overrides from .cfg file (1) if different.\n                dscrp0 = chk_args_dict.get('comment','').strip() # ok if missing\n                dscrp1 = cfgObj.inline_comments[key]\n                if dscrp1 is None:\n                    dscrp1 = ''\n\n                while len(dscrp1) > 0 and dscrp1[0] in (' ','#'):\n                    dscrp1 = dscrp1[1:] # .cfg file comments start with '#'\n\n                dscrp1 = dscrp1.strip()\n                # Now, decide what to do/say about the descriptions\n                if len(dscrp1) > 0:\n                    dscrp = dscrp0\n                    if dscrp0 != dscrp1: # allow override if different\n                        dscrp = dscrp1+eparoption.DSCRPTN_FLAG # flag it\n                        if initialPass:\n                            if dscrp0 == '' and cspc is None:\n                                # this is a case where this par isn't in the\n                                # .cfgspc; ignore, it is caught/error later\n                                pass\n                            else:\n                                self.debug('Description of \"'+key+ \\\n                                    '\" overridden, from:  '+repr(dscrp0)+\\\n                                    '  to:  '+repr(dscrp1))\n                    fields.append(dscrp)\n                else:\n                    # set the field for the GUI\n                    fields.append(dscrp0)\n                    # ALSO set it in the dict so it is written to file later\n                    cfgObj.inline_comments[key] = '# '+dscrp0\n                # This little section, while never intended to be used during\n                # normal operation, could save a lot of manual work.\n                if dumpCfgspcTo:\n                    junk = cspc\n                    junk = key+' = '+junk.strip()\n                    if junk.find(' comment=')<0:\n                        junk = junk[:-1]+\", comment=\"+ \\\n                               repr(irafutils.stripQuotes(dscrp1.strip()))+\")\"\n                    dumpCfgspcTo.write(junk+'\\n')\n                # Create the par\n                if not toBeHidden or chk_func_name.find('action')==0:\n                    par = basicpar.parFactory(fields, True)\n                    par.setScope(scopePrefix)\n                    retval.append(par)\n                # else this is a hidden key\n\n                # The next few items require a fully scoped name\n                absKeyName = scopePrefix+'.'+key # assumed to be unique\n                # Check for pars marked to be positional args\n                if initialPass:\n                    pos = chk_args_dict.get('pos')\n                    if pos:\n                        # we'll sort them later, on demand\n                        self._posArgs.append( (int(pos), scopePrefix, key) )\n                # Check for triggers and/or dependencies\n                if initialPass:\n                    # What triggers what? (thats why theres an 's' in the kwd)\n                    # try \"trigger\" (old)\n                    if chk_args_dict.get('trigger'):\n                        print(\"WARNING: outdated version of .cfgspc!! for \"+\n                              self.__taskName+\", 'trigger' unused for \"+\n                              absKeyName)\n                    # try \"triggers\"\n                    trgs = chk_args_dict.get('triggers')\n                    if trgs and len(trgs)>0:\n                        # eg. _allTriggers['STEP2.xy'] == ('_rule1_','_rule3_')\n                        assert absKeyName not in self._allTriggers, \\\n                            'More than 1 of these in .cfgspc?: '+absKeyName\n                        # we force this to always be a sequence\n                        if isinstance(trgs, (list,tuple)):\n                            self._allTriggers[absKeyName] = trgs\n                        else:\n                            self._allTriggers[absKeyName] = (trgs,)\n                    # try \"executes\"\n                    excs = chk_args_dict.get('executes')\n                    if excs and len(excs)>0:\n                        # eg. _allExecutes['STEP2.xy'] == ('_rule1_','_rule3_')\n                        assert absKeyName not in self._allExecutes, \\\n                            'More than 1 of these in .cfgspc?: '+absKeyName\n                        # we force this to always be a sequence\n                        if isinstance(excs, (list,tuple)):\n                            self._allExecutes[absKeyName] = excs\n                        else:\n                            self._allExecutes[absKeyName] = (excs,)\n\n                    # Dependencies? (besides these used here, may someday\n                    # add: 'range_from', 'warn_if', etc.)\n                    depName = None\n                    if not depName:\n                        depType = 'active_if'\n                        depName = chk_args_dict.get(depType) # e.g. =='_rule1_'\n                    if not depName:\n                        depType = 'inactive_if'\n                        depName = chk_args_dict.get(depType)\n                    if not depName:\n                        depType = 'is_set_by'\n                        depName = chk_args_dict.get(depType)\n                    if not depName:\n                        depType = 'set_yes_if'\n                        depName = chk_args_dict.get(depType)\n                    if not depName:\n                        depType = 'set_no_if'\n                        depName = chk_args_dict.get(depType)\n                    if not depName:\n                        depType = 'is_disabled_by'\n                        depName = chk_args_dict.get(depType)\n                    # NOTE - the above few lines stops at the first dependency\n                    # found (depName) for a given par.  If, in the future a\n                    # given par can have >1 dependency than we need to revamp!!\n                    if depName:\n                        # Add to _allDepdcs dict: (val is dict of pars:types)\n                        #\n                        # e.g. _allDepdcs['_rule1_'] == \\\n                        #        {'STEP3.ra':      'active_if',\n                        #         'STEP3.dec':     'active_if',\n                        #         'STEP3.azimuth': 'inactive_if'}\n                        if depName in self._allDepdcs:\n                            thisRulesDict = self._allDepdcs[depName]\n                            assert not absKeyName in thisRulesDict, \\\n                                'Cant yet handle multiple actions for the '+ \\\n                                'same par and the same rule.  For \"'+depName+ \\\n                                '\" dict was: '+str(thisRulesDict)+ \\\n                                ' while trying to add to it: '+\\\n                                str({absKeyName:depType})\n                            thisRulesDict[absKeyName] = depType\n                        else:\n                            self._allDepdcs[depName] = {absKeyName:depType}\n                    # else no dependencies found for this chk_args_dict\n        return retval", "response": "Walks the given ConfigObj dict and returns a list of IRAF - like parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTriggerStrings(self, parScope, parName):\n        # The data structure of _allTriggers was chosen for how easily/quickly\n        # this particular access can be made here.\n        fullName = parScope+'.'+parName\n        return self._allTriggers.get(fullName)", "response": "Returns all strings that are meant to trigger for a given item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all the execute strings for a given item.", "response": "def getExecuteStrings(self, parScope, parName):\n        \"\"\" For a given item (scope + name), return all strings (in a tuple)\n        that it is meant to execute, if any exist.  Returns None is none. \"\"\"\n        # The data structure of _allExecutes was chosen for how easily/quickly\n        # this particular access can be made here.\n        fullName = parScope+'.'+parName\n        return self._allExecutes.get(fullName)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getPosArgs(self):\n        if len(self._posArgs) < 1: return []\n        # The first item in the tuple is the index, so we now sort by it\n        self._posArgs.sort()\n        # Build a return list\n        retval = []\n        for idx, scope, name in self._posArgs:\n            theDict, val = findScopedPar(self, scope, name)\n            retval.append(val)\n        return retval", "response": "Return a list of all parameters marked with pos = N in\n            the. cfgspc file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getKwdArgs(self, flatten = False):\n\n        # Start with a full deep-copy.  What complicates this method is the\n        # idea of sub-sections.  This dict can have dicts as values, and so on.\n        dcopy = self.dict() # ConfigObj docs say this is a deep-copy\n\n        # First go through the dict removing all positional args\n        for idx,scope,name in self._posArgs:\n            theDict, val = findScopedPar(dcopy, scope, name)\n            # 'theDict' may be dcopy, or it may be a dict under it\n            theDict.pop(name)\n\n        # Then go through the dict removing all hidden items ('_item_name_')\n        for k in list(dcopy.keys()):\n            if isHiddenName(k):\n                dcopy.pop(k)\n\n        # Done with the nominal operation\n        if not flatten:\n            return dcopy\n\n        # They have asked us to flatten the structure - to bring all parameters\n        # up to the top level, even if they are in sub-sections.  So we look\n        # for values that are dicts.  We will throw something if we end up\n        # with name collisions at the top level as a result of this.\n        return flattenDictTree(dcopy)", "response": "Return a dict of all normal dict parameters - that is all normal dict parameters NOT marked with pos = N in the. cfgspc file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to set the value of the given item name and scope.", "response": "def tryValue(self, name, val, scope=''):\n        \"\"\" For the given item name (and scope), we are being asked to try\n            the given value to see if it would pass validation.  We are not\n            to set it, but just try it.  We return a tuple:\n            If it fails, we return: (False,  the last known valid value).\n            On success, we return: (True, None). \"\"\"\n\n        # SIMILARITY BETWEEN THIS AND setParam() SHOULD BE CONSOLIDATED!\n\n        # Set the value, even if invalid.  It needs to be set before\n        # the validation step (next).\n        theDict, oldVal = findScopedPar(self, scope, name)\n        if oldVal == val: return (True, None) # assume oldVal is valid\n        theDict[name] = val\n\n        # Check the proposed value.  Ideally, we'd like to\n        # (somehow elegantly) only check this one item. For now, the best\n        # shortcut is to only validate this section.\n        ans=self.validate(self._vtor, preserve_errors=True, section=theDict)\n\n        # No matter what ans is, immediately return the item to its original\n        # value since we are only checking the value here - not setting.\n        theDict[name] = oldVal\n\n        # Now see what the validation check said\n        errStr = ''\n        if ans != True:\n            flatStr = \"All values are invalid!\"\n            if ans != False:\n                flatStr = flattened2str(configobj.flatten_errors(self, ans))\n            errStr = \"Validation error: \"+flatStr # for now this info is unused\n\n        # Done\n        if len(errStr): return (False, oldVal) # was an error\n        else:           return (True, None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef listTheExtras(self, deleteAlso):\n        # get list of extras\n        extras = configobj.get_extra_values(self)\n        # extras is in format: [(sections, key), (sections, key), ]\n        # but we need: [(sections, key, result), ...] - set all results to\n        # a bool just to make it the right shape.  BUT, since we are in\n        # here anyway, make that bool mean something - hide info in it about\n        # whether that extra item is a section (1) or just a single par (0)\n        #\n        # simplified, this is:  expanded = [ (x+(abool,)) for x in extras]\n        expanded = [ (x+ \\\n                       ( bool(len(x[0])<1 and hasattr(self[x[1]], 'keys')), ) \\\n                     ) for x in extras]\n        retval = ''\n        if expanded:\n            retval = flattened2str(expanded, extra=1)\n        # but before we return, delete them (from ourself!) if requested to\n        if deleteAlso:\n            for tup_to_del in extras:\n                target = self\n                # descend the tree to the dict where this items is located.\n                # (this works because target is not a copy (because the dict\n                #  type is mutable))\n                location = tup_to_del[0]\n                for subdict in location: target = target[subdict]\n                # delete it\n                target.pop(tup_to_del[1])\n\n        return retval", "response": "Return a string similar to findTheLost."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreloads the measurements from the backing store.", "response": "def get(self):\n        \"\"\"\n        Reloads the measurements from the backing store.\n        :return: 200 if success.\n        \"\"\"\n        try:\n            self._measurementController.reloadCompletedMeasurements()\n            return None, 200\n        except:\n            logger.exception(\"Failed to reload measurements\")\n            return str(sys.exc_info()), 500"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite each sample to a csv file.", "response": "def handle(self, data):\n        \"\"\"\n        Writes each sample to a csv file.\n        :param data: the samples.\n        :return:\n        \"\"\"\n        self.logger.debug(\"Handling \" + str(len(data)) + \" data items\")\n        for datum in data:\n            if isinstance(datum, dict):\n                # these have to wrapped in a list for python 3.4 due to a change in the implementation\n                # of OrderedDict in python 3.5+ (which means .keys() and .values() are sequences in 3.5+)\n                if self._first:\n                    self._csv.writerow(list(datum.keys()))\n                    self._first = False\n                self._csv.writerow(list(datum.values()))\n            elif isinstance(datum, list):\n                self._csv.writerow(datum)\n            else:\n                self.logger.warning(\"Ignoring unsupported data type \" + str(type(datum)) + \" : \" + str(datum))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a start request to the target to tell it a named measurement is starting.", "response": "def start(self, measurementId):\n        \"\"\"\n        Posts to the target to tell it a named measurement is starting.\n        :param measurementId:\n        \"\"\"\n        self.sendURL = self.rootURL + measurementId + '/' + self.deviceName\n        self.startResponseCode = self._doPut(self.sendURL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the data request", "response": "def handle(self, data):\n        \"\"\"\n        puts the data in the target.\n        :param data: the data to post.\n        :return:\n        \"\"\"\n        self.dataResponseCode.append(self._doPut(self.sendURL + '/data', data=data))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop the measurement with the given id", "response": "def stop(self, measurementId, failureReason=None):\n        \"\"\"\n        informs the target the named measurement has completed\n        :param measurementId: the measurement that has completed.\n        :return:\n        \"\"\"\n        if failureReason is None:\n            self.endResponseCode = self._doPut(self.sendURL + \"/complete\")\n        else:\n            self.endResponseCode = self._doPut(self.sendURL + \"/failed\", data={'failureReason': failureReason})\n        self.sendURL = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dottedQuadToNum(ip):\n\n    # import here to avoid it when ip_addr values are not used\n    import socket, struct\n\n    try:\n        return struct.unpack('!L',\n            socket.inet_aton(ip.strip()))[0]\n    except socket.error:\n        raise ValueError('Not a good dotted-quad IP: %s' % ip)\n    return", "response": "Convert decimal dotted quad string to long integer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a numeric IP number to a dotted quad string.", "response": "def numToDottedQuad(num):\n    \"\"\"\n    Convert long int to dotted quad string\n\n    >>> numToDottedQuad(long(-1))\n    Traceback (most recent call last):\n    ValueError: Not a good numeric IP: -1\n    >>> numToDottedQuad(long(1))\n    '0.0.0.1'\n    >>> numToDottedQuad(long(16777218))\n    '1.0.0.2'\n    >>> numToDottedQuad(long(16908291))\n    '1.2.0.3'\n    >>> numToDottedQuad(long(16909060))\n    '1.2.3.4'\n    >>> numToDottedQuad(long(4294967295))\n    '255.255.255.255'\n    >>> numToDottedQuad(long(4294967296))\n    Traceback (most recent call last):\n    ValueError: Not a good numeric IP: 4294967296\n    \"\"\"\n\n    # import here to avoid it when ip_addr values are not used\n    import socket, struct\n\n    # no need to intercept here, 4294967295 is fine\n    if num > 4294967295 or num < 0:\n        raise ValueError('Not a good numeric IP: %s' % num)\n    try:\n        return socket.inet_ntoa(\n            struct.pack('!L', long(num)))\n    except (socket.error, struct.error, OverflowError):\n        raise ValueError('Not a good numeric IP: %s' % num)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn numbers from inputs or raise VdtParamError.", "response": "def _is_num_param(names, values, to_float=False):\n    \"\"\"\n    Return numbers from inputs or raise VdtParamError.\n\n    Lets ``None`` pass through.\n    Pass in keyword argument ``to_float=True`` to\n    use float for the conversion rather than int.\n\n    >>> _is_num_param(('', ''), (0, 1.0))\n    [0, 1]\n    >>> _is_num_param(('', ''), (0, 1.0), to_float=True)\n    [0.0, 1.0]\n    >>> _is_num_param(('a'), ('a'))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtParamError: passed an incorrect value \"a\" for parameter \"a\".\n    \"\"\"\n    fun = to_float and float or int\n    out_params = []\n    for (name, val) in zip(names, values):\n        if val is None:\n            out_params.append(val)\n        elif isinstance(val, number_or_string_types):\n            try:\n                out_params.append(fun(val))\n            except ValueError:\n                raise VdtParamError(name, val)\n        else:\n            raise VdtParamError(name, val)\n    return out_params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_integer(value, min=None, max=None):\n    (min_val, max_val) = _is_num_param(('min', 'max'), (min, max))\n    if not isinstance(value, int_or_string_types):\n        raise VdtTypeError(value)\n    if isinstance(value, string_types):\n        # if it's a string - does it represent an integer ?\n        try:\n            value = int(value)\n        except ValueError:\n            raise VdtTypeError(value)\n    if (min_val is not None) and (value < min_val):\n        raise VdtValueTooSmallError(value)\n    if (max_val is not None) and (value > max_val):\n        raise VdtValueTooBigError(value)\n    return value", "response": "A test that a given value is an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_boolean(value):\n    if isinstance(value, string_types):\n        try:\n            return bool_dict[value.lower()]\n        except KeyError:\n            raise VdtTypeError(value)\n    # we do an equality test rather than an identity test\n    # this ensures Python 2.2 compatibilty\n    # and allows 0 and 1 to represent True and False\n    if value == False:\n        return False\n    elif value == True:\n        return True\n    else:\n        raise VdtTypeError(value)", "response": "Check if the value represents a boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_ip_addr(value):\n    if not isinstance(value, string_types):\n        raise VdtTypeError(value)\n    value = value.strip()\n    try:\n        dottedQuadToNum(value)\n    except ValueError:\n        raise VdtValueError(value)\n    return value", "response": "Check that the supplied value is an Internet Protocol address v. 4 or dotted - quad string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the value is a list of values.", "response": "def is_list(value, min=None, max=None):\n    \"\"\"\n    Check that the value is a list of values.\n\n    You can optionally specify the minimum and maximum number of members.\n\n    It does no check on list members.\n\n    >>> vtor = Validator()\n    >>> vtor.check('list', ())\n    []\n    >>> vtor.check('list', [])\n    []\n    >>> vtor.check('list', (1, 2))\n    [1, 2]\n    >>> vtor.check('list', [1, 2])\n    [1, 2]\n    >>> vtor.check('list(3)', (1, 2))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtValueTooShortError: the value \"(1, 2)\" is too short.\n    >>> vtor.check('list(max=5)', (1, 2, 3, 4, 5, 6))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtValueTooLongError: the value \"(1, 2, 3, 4, 5, 6)\" is too long.\n    >>> vtor.check('list(min=3, max=5)', (1, 2, 3, 4))  # doctest: +SKIP\n    [1, 2, 3, 4]\n    >>> vtor.check('list', 0)  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"0\" is of the wrong type.\n    >>> vtor.check('list', '12')  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"12\" is of the wrong type.\n    \"\"\"\n    (min_len, max_len) = _is_num_param(('min', 'max'), (min, max))\n    if isinstance(value, string_types):\n        raise VdtTypeError(value)\n    try:\n        num_members = len(value)\n    except TypeError:\n        raise VdtTypeError(value)\n    if min_len is not None and num_members < min_len:\n        raise VdtValueTooShortError(value)\n    if max_len is not None and num_members > max_len:\n        raise VdtValueTooLongError(value)\n    return list(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the value is a list of integers.", "response": "def is_int_list(value, min=None, max=None):\n    \"\"\"\n    Check that the value is a list of integers.\n\n    You can optionally specify the minimum and maximum number of members.\n\n    Each list member is checked that it is an integer.\n\n    >>> vtor = Validator()\n    >>> vtor.check('int_list', ())\n    []\n    >>> vtor.check('int_list', [])\n    []\n    >>> vtor.check('int_list', (1, 2))\n    [1, 2]\n    >>> vtor.check('int_list', [1, 2])\n    [1, 2]\n    >>> vtor.check('int_list', [1, 'a'])  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"a\" is of the wrong type.\n    \"\"\"\n    return [is_integer(mem) for mem in is_list(value, min, max)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the value is a list of booleans.", "response": "def is_bool_list(value, min=None, max=None):\n    \"\"\"\n    Check that the value is a list of booleans.\n\n    You can optionally specify the minimum and maximum number of members.\n\n    Each list member is checked that it is a boolean.\n\n    >>> vtor = Validator()\n    >>> vtor.check('bool_list', ())\n    []\n    >>> vtor.check('bool_list', [])\n    []\n    >>> check_res = vtor.check('bool_list', (True, False))\n    >>> check_res == [True, False]\n    1\n    >>> check_res = vtor.check('bool_list', [True, False])\n    >>> check_res == [True, False]\n    1\n    >>> vtor.check('bool_list', [True, 'a'])  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"a\" is of the wrong type.\n    \"\"\"\n    return [is_boolean(mem) for mem in is_list(value, min, max)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the value is a list of floats.", "response": "def is_float_list(value, min=None, max=None):\n    \"\"\"\n    Check that the value is a list of floats.\n\n    You can optionally specify the minimum and maximum number of members.\n\n    Each list member is checked that it is a float.\n\n    >>> vtor = Validator()\n    >>> vtor.check('float_list', ())\n    []\n    >>> vtor.check('float_list', [])\n    []\n    >>> vtor.check('float_list', (1, 2.0))\n    [1.0, 2.0]\n    >>> vtor.check('float_list', [1, 2.0])\n    [1.0, 2.0]\n    >>> vtor.check('float_list', [1, 'a'])  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"a\" is of the wrong type.\n    \"\"\"\n    return [is_float(mem) for mem in is_list(value, min, max)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_string_list(value, min=None, max=None):\n    if isinstance(value, string_types):\n        raise VdtTypeError(value)\n    return [is_string(mem) for mem in is_list(value, min, max)]", "response": "Check that the value is a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the value is a list of IP addresses.", "response": "def is_ip_addr_list(value, min=None, max=None):\n    \"\"\"\n    Check that the value is a list of IP addresses.\n\n    You can optionally specify the minimum and maximum number of members.\n\n    Each list member is checked that it is an IP address.\n\n    >>> vtor = Validator()\n    >>> vtor.check('ip_addr_list', ())\n    []\n    >>> vtor.check('ip_addr_list', [])\n    []\n    >>> vtor.check('ip_addr_list', ('1.2.3.4', '5.6.7.8'))\n    ['1.2.3.4', '5.6.7.8']\n    >>> vtor.check('ip_addr_list', ['a'])  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtValueError: the value \"a\" is unacceptable.\n    \"\"\"\n    return [is_ip_addr(mem) for mem in is_list(value, min, max)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef force_list(value, min=None, max=None):\n    if not isinstance(value, (list, tuple)):\n        value = [value]\n    return is_list(value, min, max)", "response": "Checks that a value is a list coercing strings into a list with one member."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the value is a list of mixed members.", "response": "def is_mixed_list(value, *args):\n    \"\"\"\n    Check that the value is a list.\n    Allow specifying the type of each member.\n    Work on lists of specific lengths.\n\n    You specify each member as a positional argument specifying type\n\n    Each type should be one of the following strings :\n      'integer', 'float', 'ip_addr', 'string', 'boolean'\n\n    So you can specify a list of two strings, followed by\n    two integers as :\n\n      mixed_list('string', 'string', 'integer', 'integer')\n\n    The length of the list must match the number of positional\n    arguments you supply.\n\n    >>> vtor = Validator()\n    >>> mix_str = \"mixed_list('integer', 'float', 'ip_addr', 'string', 'boolean')\"\n    >>> check_res = vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a', True))\n    >>> check_res == [1, 2.0, '1.2.3.4', 'a', True]\n    1\n    >>> check_res = vtor.check(mix_str, ('1', '2.0', '1.2.3.4', 'a', 'True'))\n    >>> check_res == [1, 2.0, '1.2.3.4', 'a', True]\n    1\n    >>> vtor.check(mix_str, ('b', 2.0, '1.2.3.4', 'a', True))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"b\" is of the wrong type.\n    >>> vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a'))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtValueTooShortError: the value \"(1, 2.0, '1.2.3.4', 'a')\" is too short.\n    >>> vtor.check(mix_str, (1, 2.0, '1.2.3.4', 'a', 1, 'b'))  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtValueTooLongError: the value \"(1, 2.0, '1.2.3.4', 'a', 1, 'b')\" is too long.\n    >>> vtor.check(mix_str, 0)  # doctest: +SKIP\n    Traceback (most recent call last):\n    VdtTypeError: the value \"0\" is of the wrong type.\n\n    This test requires an elaborate setup, because of a change in error string\n    output from the interpreter between Python 2.2 and 2.3 .\n\n    >>> res_seq = (\n    ...     'passed an incorrect value \"',\n    ...     'yoda',\n    ...     '\" for parameter \"mixed_list\".',\n    ... )\n    >>> res_str = \"'\".join(res_seq)\n    >>> try:\n    ...     vtor.check('mixed_list(\"yoda\")', ('a'))\n    ... except VdtParamError as err:\n    ...     str(err) == res_str\n    1\n    \"\"\"\n    try:\n        length = len(value)\n    except TypeError:\n        raise VdtTypeError(value)\n    if length < len(args):\n        raise VdtValueTooShortError(value)\n    elif length > len(args):\n        raise VdtValueTooLongError(value)\n    try:\n        return [fun_dict[arg](val) for arg, val in zip(args, value)]\n    except KeyError as e:\n        raise(VdtParamError('mixed_list', e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_option(value, *options):\n    if not isinstance(value, string_types):\n        raise VdtTypeError(value)\n    if not value in options:\n        raise VdtValueError(value)\n    return value", "response": "This function checks that the value is one of the options in the order they appear."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a value is correct for a given check.", "response": "def check(self, check, value, missing=False):\n        \"\"\"\n        Usage: check(check, value)\n\n        Arguments:\n            check: string representing check to apply (including arguments)\n            value: object to be checked\n        Returns value, converted to correct type if necessary\n\n        If the check fails, raises a ``ValidateError`` subclass.\n\n        >>> vtor = Validator()\n        >>> vtor.check('yoda', '')  # doctest: +SKIP\n        Traceback (most recent call last):\n        VdtUnknownCheckError: the check \"yoda\" is unknown.\n        >>> vtor.check('yoda()', '')  # doctest: +SKIP\n        Traceback (most recent call last):\n        VdtUnknownCheckError: the check \"yoda\" is unknown.\n\n        >>> vtor.check('string(default=\"\")', '', missing=True)\n        ''\n        \"\"\"\n        fun_name, fun_args, fun_kwargs, default = self._parse_with_caching(check)\n\n        if missing:\n            if default is None:\n                # no information needed here - to be handled by caller\n                raise VdtMissingValue()\n            value = self._handle_none(default)\n\n        if value is None:\n            return None\n\n        return self._check_value(value, fun_name, fun_args, fun_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unquote(self, val):\n        if (len(val) >= 2) and (val[0] in (\"'\", '\"')) and (val[0] == val[-1]):\n            val = val[1:-1]\n        return val", "response": "Unquote a value if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _list_handle(self, listmatch):\n        out = []\n        name = listmatch.group(1)\n        args = listmatch.group(2)\n        for arg in self._list_members.findall(args):\n            out.append(self._unquote(arg))\n        return name, out", "response": "Take apart a keyword = list ( val ) type string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_value(self, check):\n        fun_name, fun_args, fun_kwargs, default = self._parse_with_caching(check)\n        if default is None:\n            raise KeyError('Check \"%s\" has no default value.' % check)\n        value = self._handle_none(default)\n        if value is None:\n            return value\n        return self._check_value(value, fun_name, fun_args, fun_kwargs)", "response": "Given a check returns the default value for the check."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_header(cls, header):\n\n        pixvalue = header.get('PIXVALUE')\n        naxis = header.get('NAXIS', 0)\n\n        return (super(_ConstantValueImageBaseHDU, cls).match_header(header) and\n                (isinstance(pixvalue, float) or _is_int(pixvalue)) and\n                naxis == 0)", "response": "Match the header with the constant value image base HDU."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the HDU s data is a constant value array.", "response": "def _check_constant_value_data(self, data):\n        \"\"\"Verify that the HDU's data is a constant value array.\"\"\"\n\n        arrayval = data.flat[0]\n        if np.all(data == arrayval):\n            return arrayval\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn currently selected index or - 1 if no such index exists", "response": "def get_current_index(self):\n        \"\"\" Return currently selected index (or -1) \"\"\"\n\n        # Need to convert to int; currently API returns a tuple of string\n        curSel = self.__lb.curselection()\n        if curSel and len(curSel) > 0:\n            return int(curSel[0])\n        else:\n            return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(input):\n\n    \"\"\"Input GEIS files \"input\" will be read and a HDUList object will\n       be returned that matches the waiver-FITS format written out by 'stwfits' in IRAF.\n\n       The user can use the writeto method to write the HDUList object to\n       a FITS file.\n    \"\"\"\n\n    global dat\n    cardLen = fits.Card.length\n\n    # input file(s) must be of the form *.??h and *.??d\n    if input[-1] != 'h' or input[-4] != '.':\n        raise \"Illegal input GEIS file name %s\" % input\n\n    data_file = input[:-1]+'d'\n\n    _os = sys.platform\n    if _os[:5] == 'linux' or _os[:5] == 'win32' or _os[:5] == 'sunos' or _os[:3] == 'osf' or _os[:6] == 'darwin':\n        bytes_per_line = cardLen+1\n    else:\n        raise \"Platform %s is not supported (yet).\" % _os\n\n    end_card = 'END'+' '* (cardLen-3)\n\n    # open input file\n    im = open(input)\n\n    # Generate the primary HDU\n    cards = []\n    while 1:\n        line = im.read(bytes_per_line)[:cardLen]\n        line = line[:8].upper() + line[8:]\n        if line == end_card:\n            break\n        cards.append(fits.Card.fromstring(line))\n\n    phdr = fits.Header(cards)\n    im.close()\n\n    # Determine starting point for adding Group Parameter Block keywords to Primary header\n    phdr_indx = phdr.index('PSIZE')\n\n    _naxis0 = phdr.get('NAXIS', 0)\n    _naxis = [phdr['NAXIS'+str(j)] for j in range(1, _naxis0+1)]\n    _naxis.insert(0, _naxis0)\n    _bitpix = phdr['BITPIX']\n    _psize = phdr['PSIZE']\n    if phdr['DATATYPE'][:4] == 'REAL':\n        _bitpix = -_bitpix\n    if _naxis0 > 0:\n        size = reduce(lambda x,y:x*y, _naxis[1:])\n        data_size = abs(_bitpix) * size // 8\n    else:\n        data_size = 0\n    group_size = data_size + _psize // 8\n\n    # decode the group parameter definitions,\n    # group parameters will become extension table\n    groups = phdr['GROUPS']\n    gcount = phdr['GCOUNT']\n    pcount = phdr['PCOUNT']\n\n    formats = []\n    bools = []\n    floats = []\n    cols = [] # column definitions used for extension table\n    cols_dict = {} # provides name access to Column defs\n    _range = range(1, pcount+1)\n    key = [phdr['PTYPE'+str(j)] for j in _range]\n    comm = [phdr.cards['PTYPE'+str(j)].comment for j in _range]\n\n    # delete group parameter definition header keywords\n    _list = ['PTYPE'+str(j) for j in _range] + \\\n            ['PDTYPE'+str(j) for j in _range] + \\\n            ['PSIZE'+str(j) for j in _range] + \\\n            ['DATATYPE', 'PSIZE', 'GCOUNT', 'PCOUNT', 'BSCALE', 'BZERO']\n\n    # Construct record array formats for the group parameters\n    # as interpreted from the Primary header file\n    for i in range(1, pcount+1):\n        ptype = key[i-1]\n        pdtype = phdr['PDTYPE'+str(i)]\n        star = pdtype.find('*')\n        _type = pdtype[:star]\n        _bytes = pdtype[star+1:]\n\n        # collect boolean keywords since they need special attention later\n\n        if _type == 'LOGICAL':\n            bools.append(i)\n        if pdtype == 'REAL*4':\n            floats.append(i)\n\n        # identify keywords which require conversion to special units\n        if ptype in kw_DOUBLE:\n            _type = 'DOUBLE'\n\n        fmt = geis_fmt[_type] + _bytes\n        formats.append((ptype,fmt))\n\n        # Set up definitions for use in creating the group-parameter block table\n        nrpt = ''\n        nbits = str(int(_bytes)*8)\n        if 'CHAR' in _type:\n            nrpt = _bytes\n            nbits = _bytes\n\n        afmt = cols_fmt[_type]+ nbits\n        if 'LOGICAL' in _type:\n            afmt = cols_fmt[_type]\n        cfmt = cols_pfmt[_type]+nrpt\n\n        #print 'Column format for ',ptype,': ',cfmt,' with dtype of ',afmt\n        cols_dict[ptype] = fits.Column(name=ptype,format=cfmt,array=numpy.zeros(gcount,dtype=afmt))\n        cols.append(cols_dict[ptype]) # This keeps the columns in order\n\n    _shape = _naxis[1:]\n    _shape.reverse()\n    _code = fits.BITPIX2DTYPE[_bitpix]\n    _bscale = phdr.get('BSCALE', 1)\n    _bzero = phdr.get('BZERO', 0)\n\n    if phdr['DATATYPE'][:10] == 'UNSIGNED*2':\n        _uint16 = 1\n        _bzero = 32768\n    else:\n        _uint16 = 0\n\n    # delete from the end, so it will not conflict with previous delete\n    for i in range(len(phdr)-1, -1, -1):\n        if phdr.cards[i].keyword in _list:\n            del phdr[i]\n\n    # clean up other primary header keywords\n    phdr['SIMPLE'] = True\n    phdr['GROUPS'] = False\n    _after = 'NAXIS'\n    if _naxis0 > 0:\n        _after += str(_naxis0)\n    phdr.set('EXTEND', value=True, comment=\"FITS dataset may contain extensions\", after=_after)\n\n    # Use copy-on-write for all data types since byteswap may be needed\n    # in some platforms.\n    f1 = open(data_file, mode='rb')\n    dat = f1.read()\n    errormsg = \"\"\n\n    # Define data array for all groups\n    arr_shape = _naxis[:]\n    arr_shape[0] = gcount\n    arr_stack = numpy.zeros(arr_shape,dtype=_code)\n\n    loc = 0\n    for k in range(gcount):\n        ext_dat = numpy.fromstring(dat[loc:loc+data_size], dtype=_code)\n        ext_dat = ext_dat.reshape(_shape)\n        if _uint16:\n            ext_dat += _bzero\n        # Check to see whether there are any NaN's or infs which might indicate\n        # a byte-swapping problem, such as being written out on little-endian\n        #   and being read in on big-endian or vice-versa.\n        if _code.find('float') >= 0 and \\\n            (numpy.any(numpy.isnan(ext_dat)) or numpy.any(numpy.isinf(ext_dat))):\n            errormsg += \"===================================\\n\"\n            errormsg += \"= WARNING:                        =\\n\"\n            errormsg += \"=  Input image:                   =\\n\"\n            errormsg += input+\"[%d]\\n\"%(k+1)\n            errormsg += \"=  had floating point data values =\\n\"\n            errormsg += \"=  of NaN and/or Inf.             =\\n\"\n            errormsg += \"===================================\\n\"\n        elif _code.find('int') >= 0:\n            # Check INT data for max values\n            ext_dat_frac,ext_dat_exp = numpy.frexp(ext_dat)\n            if ext_dat_exp.max() == int(_bitpix) - 1:\n                # Potential problems with byteswapping\n                errormsg += \"===================================\\n\"\n                errormsg += \"= WARNING:                        =\\n\"\n                errormsg += \"=  Input image:                   =\\n\"\n                errormsg += input+\"[%d]\\n\"%(k+1)\n                errormsg += \"=  had integer data values        =\\n\"\n                errormsg += \"=  with maximum bitvalues.        =\\n\"\n                errormsg += \"===================================\\n\"\n\n        arr_stack[k] = ext_dat\n        #ext_hdu = fits.hdu.ImageHDU(data=ext_dat)\n\n        rec = numpy.fromstring(dat[loc+data_size:loc+group_size], dtype=formats)\n\n        loc += group_size\n\n        # Add data from this GPB to table\n        for i in range(1, pcount+1):\n            val = rec[0][i-1]\n            if i in bools:\n                if val:\n                    val = 'T'\n                else:\n                    val = 'F'\n            cols[i-1].array[k] = val\n\n        # Based on the first group, add GPB keywords to PRIMARY header\n        if k == 0:\n            # Create separate PyFITS Card objects for each entry in 'rec'\n            # and update Primary HDU with these keywords after PSIZE\n            for i in range(1, pcount+1):\n                #val = rec.field(i-1)[0]\n                val = rec[0][i-1]\n                if val.dtype.kind == 'S':\n                    val = val.decode('ascii')\n\n                if i in bools:\n                    if val:\n                        val = True\n                    else:\n                        val = False\n\n                elif i in floats:\n                    # use fromstring, format in Card is deprecated in pyfits 0.9\n                    _str = '%-8s= %20.13G / %s' % (key[i-1], val, comm[i-1])\n                    _card = fits.Card.fromstring(_str)\n\n                else:\n                    _card = fits.Card(key=key[i-1], value=val, comment=comm[i-1])\n\n                phdr.insert(phdr_indx+i, _card)\n\n            # deal with bscale/bzero\n            if (_bscale != 1 or _bzero != 0):\n                phdr['BSCALE'] = _bscale\n                phdr['BZERO'] = _bzero\n\n        #hdulist.append(ext_hdu)\n    # Define new table based on Column definitions\n    ext_table = fits.TableHDU.from_columns(cols)\n    ext_table.header.set('EXTNAME', value=input+'.tab', after='TFIELDS')\n    # Add column descriptions to header of table extension to match stwfits output\n    for i in range(len(key)):\n        ext_table.header.append(fits.Card(keyword=key[i], value=comm[i]))\n\n    if errormsg != \"\":\n        errormsg += \"===================================\\n\"\n        errormsg += \"=  This file may have been        =\\n\"\n        errormsg += \"=  written out on a platform      =\\n\"\n        errormsg += \"=  with a different byte-order.   =\\n\"\n        errormsg += \"=                                 =\\n\"\n        errormsg += \"=  Please verify that the values  =\\n\"\n        errormsg += \"=  are correct or apply the       =\\n\"\n        errormsg += \"=  '.byteswap()' method.          =\\n\"\n        errormsg += \"===================================\\n\"\n        print(errormsg)\n\n    f1.close()\n\n    hdulist = fits.HDUList([fits.PrimaryHDU(header=phdr, data=arr_stack)])\n    hdulist.append(ext_table)\n\n    stsci2(hdulist,input)\n    return hdulist", "response": "Convert a GEIS file into a list of HDU objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _loadConfig(self):\n        configPath = path.join(self._getConfigPath(), self._name + \".yml\")\n        if os.path.exists(configPath):\n            self.logger.warning(\"Loading config from \" + configPath)\n            with open(configPath, 'r') as yml:\n                return yaml.load(yml, Loader=yaml.FullLoader)\n        defaultConfig = self.loadDefaultConfig()\n        self._storeConfig(defaultConfig, configPath)\n        return defaultConfig", "response": "Loads the config from some predictable locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores the config to the configPath.", "response": "def _storeConfig(self, config, configPath):\n        \"\"\" \n        Writes the config to the configPath.\n        :param config a dict of config.\n        :param configPath the path to the file to write to, intermediate dirs will be created as necessary.\n        \"\"\"\n        self.logger.info(\"Writing to \" + str(configPath))\n        os.makedirs(os.path.dirname(configPath), exist_ok=True)\n        with (open(configPath, 'w')) as yml:\n            yaml.dump(config, yml, default_flow_style=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getConfigPath(self):\n        confHome = environ.get('VIBE_CONFIG_HOME')\n        return confHome if confHome is not None else path.join(path.expanduser(\"~\"), '.vibe')", "response": "Gets the currently configured config path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the python logging system to log to a debug file and to stdout for warn and above.", "response": "def configureLogger(self):\n        \"\"\"\n        Configures the python logging system to log to a debug file and to stdout for warn and above.\n        :return: the base logger.\n        \"\"\"\n        baseLogLevel = logging.DEBUG if self.isDebugLogging() else logging.INFO\n        # create recorder app root logger\n        logger = logging.getLogger(self._name)\n        logger.setLevel(baseLogLevel)\n        # file handler\n        fh = handlers.RotatingFileHandler(path.join(self._getConfigPath(), self._name + '.log'),\n                                          maxBytes=10 * 1024 * 1024, backupCount=10)\n        fh.setLevel(baseLogLevel)\n        # create console handler with a higher log level\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.WARN)\n        # create formatter and add it to the handlers\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s')\n        fh.setFormatter(formatter)\n        ch.setFormatter(formatter)\n        # add the handlers to the logger\n        logger.addHandler(fh)\n        logger.addHandler(ch)\n        return logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntranslates IBM - format floating point numbers to IEEE 754 - format floating point numbers.", "response": "def ibm_to_ieee(ibm):\n    '''\n    Translate IBM-format floating point numbers (as bytes) to IEEE 754\n    64-bit floating point format (as Python float).\n    '''\n    # IBM mainframe:    sign * 0.mantissa * 16 ** (exponent - 64)\n    # Python uses IEEE: sign * 1.mantissa * 2 ** (exponent - 1023)\n\n    # Pad-out to 8 bytes if necessary. We expect 2 to 8 bytes, but\n    # there's no need to check; bizarre sizes will cause a struct\n    # module unpack error.\n    ibm = ibm.ljust(8, b'\\x00')\n\n    # parse the 64 bits of IBM float as one 8-byte unsigned long long\n    ulong, = struct.unpack('>Q', ibm)\n\n    # IBM: 1-bit sign, 7-bits exponent, 56-bits mantissa\n    sign = ulong & 0x8000000000000000\n    exponent = (ulong & 0x7f00000000000000) >> 56\n    mantissa = ulong & 0x00ffffffffffffff\n\n    if mantissa == 0:\n        if ibm[0:1] == b'\\x00':\n            return 0.0\n        elif ibm[0:1] in b'_.ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n            return float('nan')\n        else:\n            raise ValueError('Neither zero nor NaN: %r' % ibm)\n\n    # IBM-format exponent is base 16, so the mantissa can have up to 3\n    # leading zero-bits in the binary mantissa. IEEE format exponent\n    # is base 2, so we don't need any leading zero-bits and will shift\n    # accordingly. This is one of the criticisms of IBM-format, its\n    # wobbling precision.\n    if ulong & 0x0080000000000000:\n        shift = 3\n    elif ulong & 0x0040000000000000:\n        shift = 2\n    elif ulong & 0x0020000000000000:\n        shift = 1\n    else:\n        shift = 0\n    mantissa >>= shift\n\n    # clear the 1 bit to the left of the binary point\n    # this is implicit in IEEE specification\n    mantissa &= 0xffefffffffffffff\n\n    # IBM exponent is excess 64, but we subtract 65, because of the\n    # implicit 1 left of the radix point for the IEEE mantissa\n    exponent -= 65\n    # IBM exponent is base 16, IEEE is base 2, so we multiply by 4\n    exponent <<= 2\n    # IEEE exponent is excess 1023, but we also increment for each\n    # right-shift when aligning the mantissa's first 1-bit\n    exponent += shift + 1023\n\n    # IEEE: 1-bit sign, 11-bits exponent, 52-bits mantissa\n    # We didn't shift the sign bit, so it's already in the right spot\n    ieee = sign | (exponent << 52) | mantissa\n    return struct.unpack(\">d\", struct.pack(\">Q\", ieee))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ieee_to_ibm(ieee):\n    '''\n    Translate Python floating point numbers to IBM-format (as bytes).\n    '''\n    # Python uses IEEE: sign * 1.mantissa * 2 ** (exponent - 1023)\n    # IBM mainframe:    sign * 0.mantissa * 16 ** (exponent - 64)\n\n    if ieee == 0.0:\n        return b'\\x00' * 8\n    if ieee is None or math.isnan(ieee):\n        return b'_' + b'\\x00' * 7\n    if math.isinf(ieee):\n        raise NotImplementedError('Cannot convert infinity')\n\n    bits = struct.pack('>d', ieee)\n    ulong, = struct.unpack('>Q', bits)\n\n    sign = (ulong & (1 << 63)) >> 63                    # 1-bit     sign\n    exponent = ((ulong & (0x7ff << 52)) >> 52) - 1023   # 11-bits   exponent\n    mantissa = ulong & 0x000fffffffffffff               # 52-bits   mantissa/significand\n\n    if exponent > 248:\n        msg = 'Cannot store magnitude more than ~ 16 ** 63 as IBM-format'\n        raise Overflow(msg)\n    if exponent < -260:\n        msg = 'Cannot store magnitude less than ~ 16 ** -65 as IBM-format'\n        raise Underflow(msg)\n\n    # IEEE mantissa has an implicit 1 left of the radix:    1.significand\n    # IBM mantissa has an implicit 0 left of the radix:     0.significand\n    # We must bitwise-or the implicit 1.mmm into the mantissa\n    # later we will increment the exponent to account for this change\n    mantissa = 0x0010000000000000 | mantissa\n\n    # IEEE exponents are for base 2:    mantissa * 2 ** exponent\n    # IBM exponents are for base 16:    mantissa * 16 ** exponent\n    # We must divide the exponent by 4, since 16 ** x == 2 ** (4 * x)\n    quotient, remainder = divmod(exponent, 4)\n    exponent = quotient\n\n    # We don't want to lose information;\n    # the remainder from the divided exponent adjusts the mantissa\n    mantissa <<= remainder\n\n    # Increment exponent, because of earlier adjustment to mantissa\n    # this corresponds to the 1.mantissa vs 0.mantissa implicit bit\n    exponent += 1\n\n    # IBM exponents are excess 64\n    exponent += 64\n\n    # IBM has 1-bit sign, 7-bits exponent, and 56-bits mantissa.\n    # We must shift the sign and exponent into their places.\n    sign <<= 63\n    exponent <<= 56\n\n    # We lose some precision, but who said floats were perfect?\n    return struct.pack('>Q', sign | exponent | mantissa)", "response": "Translate a Python floating point number to IBM - format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize columns to a JSON formatted bytes object.", "response": "def dumps(columns):\n    '''\n    Serialize ``columns`` to a JSON formatted ``bytes`` object.\n    '''\n    fp = BytesIO()\n    dump(columns, fp)\n    fp.seek(0)\n    return fp.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match(header):\n        '''\n        Parse the 3-line (240-byte) header of a SAS XPORT file.\n        '''\n        mo = Library.header_re.match(header)\n        if mo is None:\n            raise ValueError(f'Not a SAS Version 5 or 6 XPORT file')\n        return {\n            'created': strptime(mo['created']),\n            'modified': strptime(mo['modified']),\n            'sas_version': float(mo['version']),\n            'os_version': mo['os'].decode().strip(),\n        }", "response": "Parse the 3 - line header of a SAS XPORT file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef header_match(cls, header):\n        '''\n        Parse the 4-line (320-byte) library member header.\n        '''\n        mo = cls.header_re.match(header)\n        if mo is None:\n            msg = f'Expected {cls.header_re.pattern!r}, got {header!r}'\n            raise ValueError(msg)\n        return {\n            'name': mo['name'].decode().strip(),\n            'label': mo['label'].decode().strip(),\n            'type': mo['type'].decode().strip(),\n            'created': strptime(mo['created']),\n            'modified': strptime(mo['modified']),\n            'sas_version': float(mo['version']),\n            'os_version': mo['os'].decode().strip(),\n            'namestr_size': mo['descriptor_size'],\n        }", "response": "Parse the 4 - line 370 - byte library member header.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a member namestrs header line and return the number of variables that match.", "response": "def header_match(cls, data):\n        '''\n        Parse a member namestrs header (1 line, 80 bytes).\n        '''\n        mo = cls.header_re.match(data)\n        return int(mo['n_variables'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse variable metadata for a XPORT file member.", "response": "def readall(cls, member, size):\n        '''\n        Parse variable metadata for a XPORT file member.\n        '''\n        fp = member.library.fp\n        LINE = member.library.LINE\n\n        n = cls.header_match(fp.read(LINE))\n        namestrs = [fp.read(size) for i in range(n)]\n\n        # Each namestr field is 140 bytes long, but the fields are\n        # streamed together and broken in 80-byte pieces. If the last\n        # byte of the last namestr field does not fall in the last byte\n        # of the 80-byte record, the record is padded with ASCII blanks\n        # to 80 bytes.\n\n        remainder = n * size % LINE\n        if remainder:\n            padding = 80 - remainder\n            fp.read(padding)\n\n        info = [cls.unpack(s) for s in namestrs]\n        for d in info:\n            d['format'] = Format(**d['format'])\n            d['iformat'] = InputFormat(**d['iformat'])\n        return [Variable(**d) for d in info]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_jd(year, month, day):\n    '''Determine Julian day count from Islamic date'''\n    return (day + ceil(29.5 * (month - 1)) + (year - 1) * 354 + trunc((3 + (11 * year)) / 30) + EPOCH) - 1", "response": "Determine Julian day count from Islamic date"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_jd(jd):\n    '''Calculate Islamic date from Julian day'''\n\n    jd = trunc(jd) + 0.5\n    year = trunc(((30 * (jd - EPOCH)) + 10646) / 10631)\n    month = min(12, ceil((jd - (29 + to_jd(year, 1, 1))) / 29.5) + 1)\n    day = int(jd - to_jd(year, month, 1)) + 1\n    return (year, month, day)", "response": "Calculate Islamic date from Julian day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking all the widgets that have been edited and have any triggers that have been triggered by this entry.", "response": "def checkAllTriggers(self, action):\n        \"\"\" Go over all widgets and let them know they have been edited\n            recently and they need to check for any trigger actions.  This\n            would be used right after all the widgets have their values\n            set or forced (e.g. via setAllEntriesFromParList). \"\"\"\n        for entry in self.entryNo:\n            entry.widgetEdited(action=action, skipDups=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef freshenFocus(self):\n        self.top.update_idletasks()\n        self.top.after(10, self.setViewAtTop)", "response": "Called when a new item is added to the top of the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mwl(self, event):\n        if event.num == 4: # up on Linux\n            self.top.f.canvas.yview_scroll(-1*self._tmwm, 'units')\n        elif event.num == 5: # down on Linux\n            self.top.f.canvas.yview_scroll(1*self._tmwm, 'units')\n        else: # assume event.delta has the direction, but reversed sign\n            self.top.f.canvas.yview_scroll(-(event.delta)*self._tmwm, 'units')", "response": "Mouse wheel - under tkinter we need Tk v8. 5 + for this"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset focus to next item in sequence", "response": "def focusNext(self, event):\n        \"\"\"Set focus to next item in sequence\"\"\"\n        try:\n            event.widget.tk_focusNext().focus_set()\n        except TypeError:\n            # see tkinter equivalent code for tk_focusNext to see\n            # commented original version\n            name = event.widget.tk.call('tk_focusNext', event.widget._w)\n            event.widget._nametowidget(str(name)).focus_set()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef focusPrev(self, event):\n        try:\n            event.widget.tk_focusPrev().focus_set()\n        except TypeError:\n            # see tkinter equivalent code for tk_focusPrev to see\n            # commented original version\n            name = event.widget.tk.call('tk_focusPrev', event.widget._w)\n            event.widget._nametowidget(str(name)).focus_set()", "response": "Set focus to previous item in sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscrolls the panel down to ensure the widget with focus to be visible.", "response": "def doScroll(self, event):\n        \"\"\"Scroll the panel down to ensure widget with focus to be visible\n\n        Tracks the last widget that doScroll was called for and ignores\n        repeated calls.  That handles the case where the focus moves not\n        between parameter entries but to someplace outside the hierarchy.\n        In that case the scrolling is not expected.\n\n        Returns false if the scroll is ignored, else true.\n        \"\"\"\n        canvas = self.top.f.canvas\n        widgetWithFocus = event.widget\n        if widgetWithFocus is self.lastFocusWidget:\n            return FALSE\n        self.lastFocusWidget = widgetWithFocus\n        if widgetWithFocus is None:\n            return TRUE\n        # determine distance of widget from top & bottom edges of canvas\n        y1 = widgetWithFocus.winfo_rooty()\n        y2 = y1 + widgetWithFocus.winfo_height()\n        cy1 = canvas.winfo_rooty()\n        cy2 = cy1 + canvas.winfo_height()\n        yinc = self.yscrollincrement\n        if y1<cy1:\n            # this will continue to work when integer division goes away\n            sdist = int((y1-cy1-yinc+1.)/yinc)\n            canvas.yview_scroll(sdist, \"units\")\n        elif cy2<y2:\n            sdist = int((y2-cy2+yinc-1.)/yinc)\n            canvas.yview_scroll(sdist, \"units\")\n        return TRUE"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _handleParListMismatch(self, probStr, extra=False):\n\n        errmsg = 'ERROR: mismatch between default and current par lists ' + \\\n               'for task \"'+self.taskName+'\"'\n        if probStr:\n            errmsg += '\\n\\t'+probStr\n        errmsg += '\\n(try: \"unlearn '+self.taskName+'\")'\n        print(errmsg)\n        return False", "response": "Handle the situation where two par lists do not match."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntoggles the active state of a section.", "response": "def _toggleSectionActiveState(self, sectionName, state, skipList):\n        \"\"\" Make an entire section (minus skipList items) either active or\n            inactive.  sectionName is the same as the param's scope. \"\"\"\n\n        # Get model data, the list of pars\n        theParamList = self._taskParsObj.getParList()\n\n        # Loop over their assoc. entries\n        for i in range(self.numParams):\n            if theParamList[i].scope == sectionName:\n                if skipList and theParamList[i].name in skipList:\n#                   self.entryNo[i].setActiveState(True) # these always active\n                    pass # if it started active, we don't need to reactivate it\n                else:\n                    self.entryNo[i].setActiveState(state)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef saveAs(self, event=None):\n\n        self.debug('Clicked Save as...')\n        # On Linux Pers..Dlg causes the cwd to change, so get a copy of current\n        curdir = os.getcwd()\n\n        # The user wishes to save to a different name\n        writeProtChoice = self._writeProtectOnSaveAs\n        if capable.OF_TKFD_IN_EPAR:\n            # Prompt using native looking dialog\n            fname = asksaveasfilename(parent=self.top,\n                    title='Save Parameter File As',\n                    defaultextension=self._defSaveAsExt,\n                    initialdir=os.path.dirname(self._getSaveAsFilter()))\n        else:\n            # Prompt. (could use tkinter's FileDialog, but this one is prettier)\n            # initWProtState is only used in the 1st call of a session\n            from . import filedlg\n            fd = filedlg.PersistSaveFileDialog(self.top,\n                         \"Save Parameter File As\", self._getSaveAsFilter(),\n                         initWProtState=writeProtChoice)\n            if fd.Show() != 1:\n                fd.DialogCleanup()\n                os.chdir(curdir) # in case file dlg moved us\n                return\n            fname = fd.GetFileName()\n            writeProtChoice = fd.GetWriteProtectChoice()\n            fd.DialogCleanup()\n\n        if not fname: return # canceled\n\n        # First check the child parameters, aborting save if\n        # invalid entries were encountered\n        if self.checkSetSaveChildren():\n            os.chdir(curdir) # in case file dlg moved us\n            return\n\n        # Run any subclass-specific steps right before the save\n        self._saveAsPreSave_Hook(fname)\n\n        # Verify all the entries (without save), keeping track of the invalid\n        # entries which have been reset to their original input values\n        self.badEntriesList = self.checkSetSaveEntries(doSave=False)\n\n        # If there were invalid entries, prepare the message dialog\n        if self.badEntriesList:\n            ansOKCANCEL = self.processBadEntries(self.badEntriesList,\n                          self.taskName)\n            if not ansOKCANCEL:\n                os.chdir(curdir) # in case file dlg moved us\n                return\n\n        # If there were no invalid entries or the user says OK, finally\n        # save to their stated file.  Since we have already processed the\n        # bad entries, there should be none returned.\n        mstr = \"TASKMETA: task=\"+self.taskName+\" package=\"+self.pkgName\n        if self.checkSetSaveEntries(doSave=True, filename=fname, comment=mstr,\n                                    set_ro=writeProtChoice,\n                                    overwriteRO=True):\n            os.chdir(curdir) # in case file dlg moved us\n            raise Exception(\"Unexpected bad entries for: \"+self.taskName)\n\n        # Run any subclass-specific steps right after the save\n        self._saveAsPostSave_Hook(fname)\n\n        os.chdir(curdir)", "response": "Save the parameter settings to a user - specified file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef htmlHelp(self, helpString=None, title=None, istask=False, tag=None):\n        # Check the help string.  If it turns out to be a URL, launch that,\n        # if not, dump it to a quick and dirty tmp html file to make it\n        # presentable, and pass that file name as the URL.\n        if not helpString:\n            helpString = self.getHelpString(self.pkgName+'.'+self.taskName)\n        if not title:\n            title = self.taskName\n        lwr = helpString.lower()\n        if lwr.startswith(\"http:\") or lwr.startswith(\"https:\") or \\\n           lwr.startswith(\"file:\"):\n            url = helpString\n            if tag and url.find('#') < 0:\n                url += '#'+tag\n#           print('LAUNCHING: '+url) # DBG\n            irafutils.launchBrowser(url, subj=title)\n        else:\n            # Write it to a temp HTML file to display\n            (fd, fname) = tempfile.mkstemp(suffix='.html', prefix='editpar_')\n            os.close(fd)\n            f = open(fname, 'w')\n            if istask and self._knowTaskHelpIsHtml:\n                f.write(helpString)\n            else:\n                f.write('<html><head><title>'+title+'</title></head>\\n')\n                f.write('<body><h3>'+title+'</h3>\\n')\n                f.write('<pre>\\n'+helpString+'\\n</pre></body></html>')\n            f.close()\n            irafutils.launchBrowser(\"file://\"+fname, subj=title)", "response": "Displays the help string for the current task."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _showAnyHelp(self, kind, tag=None):\n\n        # sanity check\n        assert kind in ('epar', 'task', 'log'), 'Unknown help kind: '+str(kind)\n\n        #-----------------------------------------\n        # See if they'd like to view in a browser\n        #-----------------------------------------\n        if self._showHelpInBrowser or (kind == 'task' and\n                                       self._knowTaskHelpIsHtml):\n            if kind == 'epar':\n                self.htmlHelp(helpString=self._appHelpString,\n                              title='Parameter Editor Help')\n            if kind == 'task':\n                self.htmlHelp(istask=True, tag=tag)\n            if kind == 'log':\n                self.htmlHelp(helpString='\\n'.join(self._msgHistory),\n                              title=self._appName+' Event Log')\n            return\n\n        #-----------------------------------------\n        # Now try to pop up the regular Tk window\n        #-----------------------------------------\n        wins = {'epar':self.eparHelpWin,\n                'task':self.irafHelpWin,\n                'log': self.logHistWin, }\n        window = wins[kind]\n        try:\n            if window.state() != NORMAL:\n                window.deiconify()\n            window.tkraise()\n            return\n        except (AttributeError, TclError):\n            pass\n\n        #---------------------------------------------------------\n        # That didn't succeed (window is still None), so build it\n        #---------------------------------------------------------\n        if kind == 'epar':\n            self.eparHelpWin = self.makeHelpWin(self._appHelpString,\n                                                title='Parameter Editor Help')\n        if kind == 'task':\n            # Acquire the task help as a string\n            # Need to include the package name for the task to\n            # avoid name conflicts with tasks from other packages. WJH\n            self.irafHelpWin = self.makeHelpWin(self.getHelpString(\n                                                self.pkgName+'.'+self.taskName))\n        if kind == 'log':\n            self.logHistWin = self.makeHelpWin('\\n'.join(self._msgHistory),\n                                               title=self._appName+' Event Log')", "response": "Invoke the task or epar help and put the page in a regular Tk window."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setAllEntriesFromParList(self, aParList, updateModel=False):\n\n        # Get model data, the list of pars\n        theParamList = self._taskParsObj.getParList() # we may modify members\n\n        if len(aParList) != len(theParamList):\n            showwarning(message=\"Attempting to set parameter values from a \"+ \\\n                        \"list of different length (\"+str(len(aParList))+ \\\n                        \") than the number shown here (\"+ \\\n                        str(len(theParamList))+\").  Be aware.\",\n                        title=\"Parameter List Length Mismatch\")\n\n        # LOOP THRU GUI PAR LIST\n        for i in range(self.numParams):\n            par = theParamList[i]\n            if par.type == \"pset\":\n                continue # skip PSET's for now\n            gui_entry = self.entryNo[i]\n\n            # Set the value in the paramList before setting it in the GUI\n            # This may be in the form of a list, or an IrafParList (getValue)\n            if isinstance(aParList, list):\n                # Since \"aParList\" can have them in different order and number\n                # than we do, we'll have to first find the matching param.\n                found = False\n                for newpar in aParList:\n                    if newpar.name==par.name and newpar.scope==par.scope:\n                        par.set(newpar.value) # same as .get(native=1,prompt=0)\n                        found = True\n                        break\n\n                # Now see if newpar was found in our list\n                if not found:\n                    pnm = par.name\n                    if len(par.scope): pnm = par.scope+'.'+par.name\n                    raise UnfoundParamError('Error - Unfound Parameter! \\n\\n'+\\\n                      'Expected parameter \"'+pnm+'\" for task \"'+ \\\n                      self.taskName+'\". \\nThere may be others...')\n\n            else: # assume has getValue()\n                par.set(aParList.getValue(par.name, native=1, prompt=0))\n\n            # gui holds a str, but par.value is native; conversion occurs\n            gui_entry.forceValue(par.value, noteEdited=False) # no triggers yet\n\n        if updateModel:\n            # Update the model values via checkSetSaveEntries\n            self.badEntriesList = self.checkSetSaveEntries(doSave=False)\n\n            # If there were invalid entries, prepare the message dialog\n            if self.badEntriesList:\n                self.processBadEntries(self.badEntriesList,\n                                       self.taskName, canCancel=False)", "response": "Set all the entries in the GUI to the values in the given list of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getValue(self, name, scope=None, native=False):\n\n        # Get model data, the list of pars\n        theParamList = self._taskParsObj.getParList()\n\n        # NOTE: If par scope is given, it will be used, otherwise it is\n        # assumed to be unneeded and the first name-match is returned.\n        fullName = basicpar.makeFullName(scope, name)\n\n        # Loop over the parameters to find the requested par\n        for i in range(self.numParams):\n            par = theParamList[i] # IrafPar or subclass\n            entry = self.entryNo[i] # EparOption or subclass\n            if par.fullName() == fullName or \\\n               (scope is None and par.name == name):\n                if native:\n                    return entry.convertToNative(entry.choice.get())\n                else:\n                    return entry.choice.get()\n        # We didn't find the requested par\n        raise RuntimeError('Could not find par: \"'+fullName+'\"')", "response": "Get the value of the current parameter from the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkSetSaveChildren(self, doSave=True):\n        if self.isChild:\n            return\n\n        # Need to get all the entries and verify them.\n        # Save the children in backwards order to coincide with the\n        # display of the dialogs (LIFO)\n        for n in range (len(self.top.childList)-1, -1, -1):\n            self.badEntriesList = self.top.childList[n]. \\\n                                  checkSetSaveEntries(doSave=doSave)\n            if self.badEntriesList:\n                ansOKCANCEL = self.processBadEntries(self.badEntriesList,\n                              self.top.childList[n].taskName)\n                if not ansOKCANCEL:\n                    return self.badEntriesList\n            # If there were no invalid entries or the user says OK,\n            # close down the child and increment to the next child\n            self.top.childList[n].top.focus_set()\n            self.top.childList[n].top.withdraw()\n            del self.top.childList[n]\n        # all windows saved successfully\n        return", "response": "Check set then save the parameter settings for all child windows."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef showStatus(self, msg, keep=0, cat=None):\n        # prep it, space-wise\n        msg = msg.strip()\n        if len(msg) > 0:\n            # right here is the ideal place to collect a history of messages\n            forhist = msg\n            if cat: forhist = '['+cat+'] '+msg\n            forhist = time.strftime(\"%a %H:%M:%S\")+': '+forhist\n            self._msgHistory.append(forhist)\n            # now set the spacing\n            msg = '  '+msg\n\n        # stop here if it is a category not shown in the GUI\n        if cat == DBG:\n            return\n\n        # see if we can show it\n        now = time.time()\n        if now >= self._leaveStatusMsgUntil: # we are clear, can show a msg\n            # first see if this msg is '' - if so we will show an important\n            # waiting msg instead of the '', and then pop it off our list\n            if len(msg) < 1 and len(self._statusMsgsToShow) > 0:\n                msg, keep = self._statusMsgsToShow[0] # overwrite both args\n                del self._statusMsgsToShow[0]\n            # now actuall print the status out to the status widget\n            self.top.status.config(text = msg)\n            # reset our delay flag\n            self._leaveStatusMsgUntil = 0\n            if keep > 0:\n                self._leaveStatusMsgUntil = now + keep\n        else:\n            # there is a previous message still up, is this one important?\n            if len(msg) > 0 and keep > 0:\n                # Uh-oh, this is an important message that we don't want to\n                # simply skip, but on the other hand we can't show it yet...\n                # So we add it to _statusMsgsToShow and show it later (asap)\n                if (msg,keep) not in self._statusMsgsToShow:\n                    if len(self._statusMsgsToShow) < 7:\n                        self._statusMsgsToShow.append( (msg,keep) ) # tuple\n                        # kick off timer loop to get this one pushed through\n                        if len(self._statusMsgsToShow) == 1:\n                            self._pushMessages()\n                    else:\n                        # should never happen, but just in case\n                        print(\"Lost message!: \"+msg+\" (too far behind...)\")", "response": "Show the given status string but not until any given delay from the previous message has expired."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the GUI session or load a task s ConfigObj.", "response": "def teal(theTask, parent=None, loadOnly=False, returnAs=\"dict\",\n         canExecute=True, strict=False, errorsToTerm=False,\n         autoClose=True, defaults=False):\n#        overrides=None):\n    \"\"\" Start the GUI session, or simply load a task's ConfigObj. \"\"\"\n    if loadOnly: # this forces returnAs=\"dict\"\n        obj = None\n        try:\n            obj = cfgpars.getObjectFromTaskArg(theTask, strict, defaults)\n#           obj.strictUpdate(overrides) # ! would need to re-verify after this !\n        except Exception as re: # catches RuntimeError and KeyError and ...\n            # Since we are loadOnly, don't pop up the GUI for this\n            if strict:\n                raise\n            else:\n                print(re.message.replace('\\n\\n','\\n'))\n        return obj\n    else:\n        assert returnAs in (\"dict\", \"status\", None), \\\n               \"Invalid value for returnAs arg: \"+str(returnAs)\n        dlg = None\n        try:\n            # if setting to all defaults, go ahead and load it here, pre-GUI\n            if defaults:\n                theTask = cfgpars.getObjectFromTaskArg(theTask, strict, True)\n            # now create/run the dialog\n            dlg = ConfigObjEparDialog(theTask, parent=parent,\n                                      autoClose=autoClose,\n                                      strict=strict,\n                                      canExecute=canExecute)\n#                                     overrides=overrides)\n        except cfgpars.NoCfgFileError as ncf:\n            log_last_error()\n            if errorsToTerm:\n                print(str(ncf).replace('\\n\\n','\\n'))\n            else:\n                popUpErr(parent=parent,message=str(ncf),title=\"Unfound Task\")\n        except Exception as re: # catches RuntimeError and KeyError and ...\n            log_last_error()\n            if errorsToTerm:\n                print(re.message.replace('\\n\\n','\\n'))\n            else:\n                popUpErr(parent=parent, message=re.message,\n                         title=\"Bad Parameters\")\n\n        # Return, depending on the mode in which we are operating\n        if returnAs is None:\n            return\n\n        if returnAs == \"dict\":\n            if dlg is None or dlg.canceled():\n                return None\n            else:\n                return dlg.getTaskParsObj()\n\n        # else, returnAs == \"status\"\n        if dlg is None or dlg.canceled():\n            return -1\n        if dlg.executed():\n            return 1\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(theTask, canExecute=True, strict=True, defaults=False):\n    return teal(theTask, parent=None, loadOnly=True, returnAs=\"dict\",\n                canExecute=canExecute, strict=strict, errorsToTerm=True,\n                defaults=defaults)", "response": "Shortcut to load TEAL. cfg files for non - GUI access where\n    loadOnly = True."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nunlearning the user - owned task from the system.", "response": "def unlearn(taskPkgName, deleteAll=False):\n    \"\"\" Find the task named taskPkgName, and delete any/all user-owned\n    .cfg files in the user's resource directory which apply to that task.\n    Like a unix utility, this returns 0 on success (no files found or only\n    1 found but deleted).  For multiple files found, this uses deleteAll,\n    returning the file-name-list if deleteAll is False (to indicate the\n    problem) and without deleting any files.  MUST check return value.\n    This does not prompt the user or print to the screen. \"\"\"\n\n    # this WILL throw an exception if the taskPkgName isn't found\n    flist = cfgpars.getUsrCfgFilesForPyPkg(taskPkgName) # can raise\n    if flist is None or len(flist) == 0:\n        return 0\n    if len(flist) == 1:\n        os.remove(flist[0])\n        return 0\n    # at this point, we know more than one matching file was found\n    if deleteAll:\n        for f in flist:\n            os.remove(f)\n        return 0\n    else:\n        return flist"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diffFromDefaults(theTask, report=False):\n    # get the 2 dicts (trees: dicts of dicts)\n    defaultTree = load(theTask, canExecute=False, strict=True, defaults=True)\n    thisTree    = load(theTask, canExecute=False, strict=True, defaults=False)\n    # they must be flattenable\n    defaultFlat = cfgpars.flattenDictTree(defaultTree)\n    thisFlat    = cfgpars.flattenDictTree(thisTree)\n    # use the \"set\" operations till there is a dict.diff()\n    # thanks to:  http://stackoverflow.com/questions/715234\n    diffFlat = dict( set(thisFlat.items()) - \\\n                     set(defaultFlat.items()) )\n    if report:\n        defaults_of_diffs_only = {}\n#       { k:defaultFlat[k] for k in diffFlat.keys() }\n        for k in diffFlat:\n            defaults_of_diffs_only[k] = defaultFlat[k]\n        msg = 'Non-default values of \"'+str(theTask)+'\":\\n'+ \\\n              _flat2str(diffFlat)+ \\\n              '\\n\\nDefault values:\\n'+ \\\n              _flat2str(defaults_of_diffs_only)\n        print(msg)\n    return diffFlat", "response": "Load the given file and return a dict containing the values which are different from the default values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the given file name is located in an analytical base area.", "response": "def _isInstalled(fullFname):\n    \"\"\" Return True if the given file name is located in an\n    installed area (versus a user-owned file) \"\"\"\n    if not fullFname: return False\n    if not os.path.exists(fullFname): return False\n    instAreas = []\n    try:\n        import site\n        instAreas = site.getsitepackages()\n    except:\n        pass # python 2.6 and lower don't have site.getsitepackages()\n    if len(instAreas) < 1:\n        instAreas = [ os.path.dirname(os.__file__) ]\n    for ia in instAreas:\n        if fullFname.find(ia) >= 0:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a string of embedded code in a specific scope.", "response": "def execEmbCode(SCOPE, NAME, VAL, TEAL, codeStr):\n    \"\"\" .cfgspc embedded code execution is done here, in a relatively confined\n        space.  The variables available to the code to be executed are:\n              SCOPE, NAME, VAL, PARENT, TEAL\n        The code string itself is expected to set a var named OUT\n    \"\"\"\n    # This was all we needed in Python 2.x\n#   OUT = None\n#   exec codeStr\n#   return OUT\n\n    # In Python 3 (& 2.x) be more explicit:  http://bugs.python.org/issue4831\n    PARENT = None\n    if TEAL:\n        PARENT = TEAL.top\n    OUT = None\n    ldict = locals() # will have OUT in it\n    exec(codeStr, globals(), ldict)\n    return ldict['OUT']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_tasknames(pkgName, aDir, term_width=80, always=False,\n                    hidden=None):\n    \"\"\" Print a message listing TEAL-enabled tasks available under a\n        given installation directory (where pkgName resides).\n        If always is True, this will always print when tasks are\n        found; otherwise it will only print found tasks when in interactive\n        mode.\n        The parameter 'hidden' supports a list of input tasknames that should\n        not be reported even though they still exist.\n    \"\"\"\n    # See if we can bail out early\n    if not always:\n        # We can't use the sys.ps1 check if in PyRAF since it changes sys\n        if 'pyraf' not in sys.modules:\n           # sys.ps1 is only defined in interactive mode\n           if not hasattr(sys, 'ps1'):\n               return # leave here, we're in someone's script\n\n    # Check for tasks\n    taskDict = cfgpars.findAllCfgTasksUnderDir(aDir)\n    tasks = [x for x in taskDict.values() if len(x) > 0]\n    if hidden: # could even account for a single taskname as input here if needed\n        for x in hidden:\n            if x in tasks: tasks.remove(x)\n    # only be verbose if there something found\n    if len(tasks) > 0:\n        sortedUniqTasks = sorted(set(tasks))\n        if len(sortedUniqTasks) == 1:\n            tlines = 'The following task in the '+pkgName+\\\n                     ' package can be run with TEAL:\\n'\n        else:\n            tlines = 'The following tasks in the '+pkgName+\\\n                     ' package can be run with TEAL:\\n'\n        tlines += printColsAuto(sortedUniqTasks, term_width=term_width,\n                                min_pad=2)\n        print(tlines)", "response": "Print a message listing TEAL - enabled tasks under a specific installation directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getHelpFileAsString(taskname,taskpath):\n    #get the local library directory where the code is stored\n    pathsplit=os.path.split(taskpath) # taskpath should be task's __file__\n    if taskname.find('.') > -1: # if taskname is given as package.taskname...\n        helpname=taskname.split(\".\")[1]    # taskname should be __taskname__ from task's module\n    else:\n        helpname = taskname\n    localdir = pathsplit[0]\n    if localdir == '':\n       localdir = '.'\n    helpfile=rglob(localdir,helpname+\".help\")[0]\n\n    if os.access(helpfile,os.R_OK):\n        fh=open(helpfile,'r')\n        ss=fh.readlines()\n        fh.close()\n        helpString=\"\"\n        for line in ss:\n            helpString+=line\n    else:\n        helpString= ''\n\n    return helpString", "response": "This function returns useful help as a string read from a file in the task s installed directory called \"<module. help>\"."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cfgGetBool(theObj, name, dflt):\n    strval = theObj.get(name, None)\n    if strval is None:\n        return dflt\n    return strval.lower().strip() == 'true'", "response": "Get a stringified val from a ConfigObj obj and return it as bool"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _overrideMasterSettings(self):\n        # config-obj dict of defaults\n        cod = self._getGuiSettings()\n\n        # our own GUI setup\n        self._appName              = APP_NAME\n        self._appHelpString        = tealHelpString\n        self._useSimpleAutoClose   = self._do_usac\n        self._showExtraHelpButton  = False\n        self._saveAndCloseOnExec   = cfgGetBool(cod, 'saveAndCloseOnExec', True)\n        self._showHelpInBrowser    = cfgGetBool(cod, 'showHelpInBrowser', False)\n        self._writeProtectOnSaveAs = cfgGetBool(cod, 'writeProtectOnSaveAsOpt', True)\n        self._flagNonDefaultVals   = cfgGetBool(cod, 'flagNonDefaultVals', None)\n        self._optFile              = APP_NAME.lower()+\".optionDB\"\n\n        # our own colors\n        # prmdrss teal: #00ffaa, pure cyan (teal) #00ffff (darker) #008080\n        # \"#aaaaee\" is a darker but good blue, but \"#bbbbff\" pops\n        ltblu = \"#ccccff\" # light blue\n        drktl = \"#008888\" # darkish teal\n        self._frmeColor = cod.get('frameColor', drktl)\n        self._taskColor = cod.get('taskBoxColor', ltblu)\n        self._bboxColor = cod.get('buttonBoxColor', ltblu)\n        self._entsColor = cod.get('entriesColor', ltblu)\n        self._flagColor = cod.get('flaggedColor', 'brown')\n\n        # double check _canExecute, but only if it is still set to the default\n        if self._canExecute and self._taskParsObj: # default _canExecute=True\n            self._canExecute = self._taskParsObj.canExecute()\n        self._showExecuteButton = self._canExecute\n\n        # check on the help string - just to see if it is HTML\n        # (could use HTMLParser here if need be, be quick and simple tho)\n        hhh = self.getHelpString(self.pkgName+'.'+self.taskName)\n        if hhh:\n            hhh = hhh.lower()\n            if hhh.find('<html') >= 0 or hhh.find('</html>') > 0:\n                self._knowTaskHelpIsHtml = True\n            elif hhh.startswith('http:') or hhh.startswith('https:'):\n                self._knowTaskHelpIsHtml = True\n            elif hhh.startswith('file:') and \\\n                 (hhh.endswith('.htm') or hhh.endswith('.html')):\n                self._knowTaskHelpIsHtml = True", "response": "Override so that we can run in a different mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride this so we can handle case of file not writable overwriteRO and save the file if it exists.", "response": "def _doActualSave(self, fname, comment, set_ro=False, overwriteRO=False):\n        \"\"\" Override this so we can handle case of file not writable, as\n            well as to make our _lastSavedState copy. \"\"\"\n        self.debug('Saving, file name given: '+str(fname)+', set_ro: '+\\\n                   str(set_ro)+', overwriteRO: '+str(overwriteRO))\n        cantWrite = False\n        inInstArea = False\n        if fname in (None, ''): fname = self._taskParsObj.getFilename()\n        # now do some final checks then save\n        try:\n            if _isInstalled(fname): # check: may be installed but not read-only\n                inInstArea = cantWrite = True\n            else:\n                # in case of save-as, allow overwrite of read-only file\n                if overwriteRO and os.path.exists(fname):\n                    setWritePrivs(fname, True, True) # try make writable\n                # do the save\n                rv=self._taskParsObj.saveParList(filename=fname,comment=comment)\n        except IOError:\n            cantWrite = True\n\n        # User does not have privs to write to this file. Get name of local\n        # choice and try to use that.\n        if cantWrite:\n            fname = self._taskParsObj.getDefaultSaveFilename()\n            # Tell them the context is changing, and where we are saving\n            msg = 'Read-only config file for task \"'\n            if inInstArea:\n                msg = 'Installed config file for task \"'\n            msg += self._taskParsObj.getName()+'\" is not to be overwritten.'+\\\n                  '  Values will be saved to: \\n\\n\\t\"'+fname+'\".'\n            showwarning(message=msg, title=\"Will not overwrite!\")\n            # Try saving to their local copy\n            rv=self._taskParsObj.saveParList(filename=fname, comment=comment)\n\n        # Treat like a save-as (update title for ALL save ops)\n        self._saveAsPostSave_Hook(fname)\n\n        # Limit write privs if requested (only if not in the rc dir)\n        if set_ro and os.path.dirname(os.path.abspath(fname)) != \\\n                                      os.path.abspath(self._rcDir):\n            cfgpars.checkSetReadOnly(fname)\n\n        # Before returning, make a copy so we know what was last saved.\n        # The dict() method returns a deep-copy dict of the keyvals.\n        self._lastSavedState = self._taskParsObj.dict()\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hasUnsavedChanges(self):\n\n        # Sanity check - this case shouldn't occur\n        assert self._lastSavedState is not None, \\\n               \"BUG: Please report this as it should never occur.\"\n\n        # Force the current GUI values into our model in memory, but don't\n        # change anything.  Don't save to file, don't even convert bad\n        # values to their previous state in the gui.  Note that this can\n        # leave the GUI in a half-saved state, but since we are about to exit\n        # this is OK.  We only want prompting to occur if they decide to save.\n        badList = self.checkSetSaveEntries(doSave=False, fleeOnBadVals=True,\n                                           allowGuiChanges=False)\n        if badList:\n            return True\n\n        # Then compare our data to the last known saved state.  MAKE SURE\n        # the LHS is the actual dict (and not 'self') to invoke the dict\n        # comparison only.\n        return self._lastSavedState != self._taskParsObj", "response": "Determine if there are any unsaved changes in the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides to allow us to use an edited callback.", "response": "def _defineEditedCallbackObjectFor(self, parScope, parName):\n        \"\"\" Override to allow us to use an edited callback. \"\"\"\n\n        # We know that the _taskParsObj is a ConfigObjPars\n        triggerStrs = self._taskParsObj.getTriggerStrings(parScope, parName)\n\n        # Some items will have a trigger, but likely most won't\n        if triggerStrs and len(triggerStrs) > 0:\n            return self\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride so we can append read - only status.", "response": "def updateTitle(self, atitle):\n        \"\"\" Override so we can append read-only status. \"\"\"\n        if atitle and os.path.exists(atitle):\n            if _isInstalled(atitle):\n                atitle += '  [installed]'\n            elif not os.access(atitle, os.W_OK):\n                atitle += '  [read only]'\n        super(ConfigObjEparDialog, self).updateTitle(atitle)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findNextSection(self, scope, name):\n        # first find index of starting point\n        plist = self._taskParsObj.getParList()\n        start = 0\n        for i in range(len(plist)):\n            if scope == plist[i].scope and name == plist[i].name:\n                start = i\n                break\n        else:\n            print('WARNING: could not find starting par: '+scope+'.'+name)\n            return (None, None)\n\n        # now find first different (non-null) scope in a par, after start\n        for i in range(start, len(plist)):\n            if len(plist[i].scope) > 0 and plist[i].scope != scope:\n                return (plist[i].scope, plist[i].name)\n        # else didn't find it\n        return (None, None)", "response": "Starts with given scope + name and returns the next section in the list of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setTaskParsObj(self, theTask):\n        # Create the ConfigObjPars obj\n        self._taskParsObj = cfgpars.getObjectFromTaskArg(theTask,\n                                    self._strict, False)\n        # Tell it that we can be used for catching debug lines\n        self._taskParsObj.setDebugLogger(self)\n\n        # Immediately make a copy of it's un-tampered internal dict.\n        # The dict() method returns a deep-copy dict of the keyvals.\n        self._lastSavedState = self._taskParsObj.dict()", "response": "Overridden version for ConfigObj. TheTask can be either a. cfg file name or a ConfigObjPars object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getSaveAsFilter(self):\n        # figure the dir to use, start with the one from the file\n        absRcDir = os.path.abspath(self._rcDir)\n        thedir = os.path.abspath(os.path.dirname(self._taskParsObj.filename))\n        # skip if not writeable, or if is _rcDir\n        if thedir == absRcDir or not os.access(thedir, os.W_OK):\n            thedir = os.path.abspath(os.path.curdir)\n        # create save-as filter string\n        filt = thedir+'/*.cfg'\n        envVarName = APP_NAME.upper()+'_CFG'\n        if envVarName in os.environ:\n            upx = os.environ[envVarName]\n            if len(upx) > 0:  filt = upx+\"/*.cfg\"\n        # done\n        return filt", "response": "Return a string to be used as the filter arg to the save file\n            dialog during Save - As."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngo through all possible sites to find applicable. cfg files. Return as an iterable.", "response": "def _getOpenChoices(self):\n        \"\"\" Go through all possible sites to find applicable .cfg files.\n            Return as an iterable. \"\"\"\n        tsk = self._taskParsObj.getName()\n        taskFiles = set()\n        dirsSoFar = [] # this helps speed this up (skip unneeded globs)\n\n        # last dir\n        aDir = os.path.dirname(self._taskParsObj.filename)\n        if len(aDir) < 1: aDir = os.curdir\n        dirsSoFar.append(aDir)\n        taskFiles.update(cfgpars.getCfgFilesInDirForTask(aDir, tsk))\n\n        # current dir\n        aDir = os.getcwd()\n        if aDir not in dirsSoFar:\n            dirsSoFar.append(aDir)\n            taskFiles.update(cfgpars.getCfgFilesInDirForTask(aDir, tsk))\n\n        # task's python pkg dir (if tsk == python pkg name)\n        try:\n            x, pkgf = cfgpars.findCfgFileForPkg(tsk, '.cfg', taskName=tsk,\n                              pkgObj=self._taskParsObj.getAssocPkg())\n            taskFiles.update( (pkgf,) )\n        except cfgpars.NoCfgFileError:\n            pass # no big deal - maybe there is no python package\n\n        # user's own resourceDir\n        aDir = self._rcDir\n        if aDir not in dirsSoFar:\n            dirsSoFar.append(aDir)\n            taskFiles.update(cfgpars.getCfgFilesInDirForTask(aDir, tsk))\n\n        # extra loc - see if they used the app's env. var\n        aDir = dirsSoFar[0] # flag to skip this if no env var found\n        envVarName = APP_NAME.upper()+'_CFG'\n        if envVarName in os.environ: aDir = os.environ[envVarName]\n        if aDir not in dirsSoFar:\n            dirsSoFar.append(aDir)\n            taskFiles.update(cfgpars.getCfgFilesInDirForTask(aDir, tsk))\n\n        # At the very end, add an option which we will later interpret to mean\n        # to open the file dialog.\n        taskFiles = list(taskFiles) # so as to keep next item at end of seq\n        taskFiles.sort()\n        taskFiles.append(\"Other ...\")\n\n        return taskFiles"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the parameter settings from a user - specified file.", "response": "def pfopen(self, event=None):\n        \"\"\" Load the parameter settings from a user-specified file. \"\"\"\n\n        # Get the selected file name\n        fname = self._openMenuChoice.get()\n\n        # Also allow them to simply find any file - do not check _task_name_...\n        # (could use tkinter's FileDialog, but this one is prettier)\n        if fname[-3:] == '...':\n            if capable.OF_TKFD_IN_EPAR:\n                fname = askopenfilename(title=\"Load Config File\",\n                                        parent=self.top)\n            else:\n                from . import filedlg\n                fd = filedlg.PersistLoadFileDialog(self.top,\n                                                   \"Load Config File\",\n                                                   self._getSaveAsFilter())\n                if fd.Show() != 1:\n                    fd.DialogCleanup()\n                    return\n                fname = fd.GetFileName()\n                fd.DialogCleanup()\n\n        if not fname: return # canceled\n        self.debug('Loading from: '+fname)\n\n        # load it into a tmp object (use associatedPkg if we have one)\n        try:\n            tmpObj = cfgpars.ConfigObjPars(fname, associatedPkg=\\\n                                           self._taskParsObj.getAssocPkg(),\n                                           strict=self._strict)\n        except Exception as ex:\n            showerror(message=ex.message, title='Error in '+os.path.basename(fname))\n            self.debug('Error in '+os.path.basename(fname))\n            self.debug(traceback.format_exc())\n            return\n\n        # check it to make sure it is a match\n        if not self._taskParsObj.isSameTaskAs(tmpObj):\n            msg = 'The current task is \"'+self._taskParsObj.getName()+ \\\n                  '\", but the selected file is for task \"'+ \\\n                  str(tmpObj.getName())+'\".  This file was not loaded.'\n            showerror(message=msg, title=\"Error in \"+os.path.basename(fname))\n            self.debug(msg)\n            self.debug(traceback.format_exc())\n            return\n\n        # Set the GUI entries to these values (let the user Save after)\n        newParList = tmpObj.getParList()\n        try:\n            self.setAllEntriesFromParList(newParList, updateModel=True)\n                # go ahead and updateModel, even though it will take longer,\n                # we need it updated for the copy of the dict we make below\n        except editpar.UnfoundParamError as pe:\n            showwarning(message=str(pe), title=\"Error in \"+os.path.basename(fname))\n        # trip any triggers\n        self.checkAllTriggers('fopen')\n\n        # This new fname is our current context\n        self.updateTitle(fname)\n        self._taskParsObj.filename = fname # !! maybe try setCurrentContext() ?\n        self.freshenFocus()\n        self.showStatus(\"Loaded values from: \"+fname, keep=2)\n\n        # Since we are in a new context (and have made no changes yet), make\n        # a copy so we know what the last state was.\n        # The dict() method returns a deep-copy dict of the keyvals.\n        self._lastSavedState = self._taskParsObj.dict()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride to include ConfigObj filename and specific errors.", "response": "def _handleParListMismatch(self, probStr, extra=False):\n        \"\"\" Override to include ConfigObj filename and specific errors.\n        Note that this only handles \"missing\" pars and \"extra\" pars, not\n        wrong-type pars.  So it isn't that big of a deal. \"\"\"\n\n        # keep down the duplicate errors\n        if extra:\n            return True # the base class is already stating it will be ignored\n\n        # find the actual errors, and then add that to the generic message\n        errmsg = 'Warning: '\n        if self._strict:\n            errmsg = 'ERROR: '\n        errmsg = errmsg+'mismatch between default and current par lists ' + \\\n                 'for task \"'+self.taskName+'\".'\n        if probStr:\n            errmsg += '\\n\\t'+probStr\n        errmsg += '\\nTry editing/deleting: \"' + \\\n                  self._taskParsObj.filename+'\" (or, if in PyRAF: \"unlearn ' + \\\n                  self.taskName+'\").'\n        print(errmsg)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the default parameter settings into the GUI.", "response": "def _setToDefaults(self):\n        \"\"\" Load the default parameter settings into the GUI. \"\"\"\n\n        # Create an empty object, where every item is set to it's default value\n        try:\n            tmpObj = cfgpars.ConfigObjPars(self._taskParsObj.filename,\n                                           associatedPkg=\\\n                                           self._taskParsObj.getAssocPkg(),\n                                           setAllToDefaults=self.taskName,\n                                           strict=False)\n        except Exception as ex:\n            msg = \"Error Determining Defaults\"\n            showerror(message=msg+'\\n\\n'+ex.message, title=\"Error Determining Defaults\")\n            return\n\n        # Set the GUI entries to these values (let the user Save after)\n        tmpObj.filename = self._taskParsObj.filename = '' # name it later\n        newParList = tmpObj.getParList()\n        try:\n            self.setAllEntriesFromParList(newParList) # needn't updateModel yet\n            self.checkAllTriggers('defaults')\n            self.updateTitle('')\n            self.showStatus(\"Loaded default \"+self.taskName+\" values via: \"+ \\\n                 os.path.basename(tmpObj._original_configspec), keep=1)\n        except editpar.UnfoundParamError as pe:\n            showerror(message=str(pe), title=\"Error Setting to Default Values\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the current parameter settings from the GUI.", "response": "def getDict(self):\n        \"\"\" Retrieve the current parameter settings from the GUI.\"\"\"\n        # We are going to have to return the dict so let's\n        # first make sure all of our models are up to date with the values in\n        # the GUI right now.\n        badList = self.checkSetSaveEntries(doSave=False)\n        if badList:\n            self.processBadEntries(badList, self.taskName, canCancel=False)\n        return self._taskParsObj.dict()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the parameter settings from a given dictionary into the GUI.", "response": "def loadDict(self, theDict):\n        \"\"\" Load the parameter settings from a given dict into the GUI. \"\"\"\n        # We are going to have to merge this info into ourselves so let's\n        # first make sure all of our models are up to date with the values in\n        # the GUI right now.\n        badList = self.checkSetSaveEntries(doSave=False)\n        if badList:\n            if not self.processBadEntries(badList, self.taskName):\n                return\n        # now, self._taskParsObj is up-to-date\n        # So now we update _taskParsObj with the input dict\n        cfgpars.mergeConfigObj(self._taskParsObj, theDict)\n        # now sync the _taskParsObj dict with its par list model\n        #    '\\n'.join([str(jj) for jj in self._taskParsObj.getParList()])\n        self._taskParsObj.syncParamList(False)\n\n        # Set the GUI entries to these values (let the user Save after)\n        try:\n            self.setAllEntriesFromParList(self._taskParsObj.getParList(),\n                                          updateModel=True)\n            self.checkAllTriggers('fopen')\n            self.freshenFocus()\n            self.showStatus('Loaded '+str(len(theDict))+ \\\n                ' user par values for: '+self.taskName, keep=1)\n        except Exception as ex:\n            showerror(message=ex.message, title=\"Error Setting to Loaded Values\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getGuiSettings(self):\n        # Put the settings into a ConfigObj dict (don't use a config-spec)\n        rcFile = self._rcDir+os.sep+APP_NAME.lower()+'.cfg'\n        if os.path.exists(rcFile):\n            try:\n                return configobj.ConfigObj(rcFile)\n            except:\n                raise RuntimeError('Error parsing: '+os.path.realpath(rcFile))\n\n            # tho, for simple types, unrepr=True eliminates need for .cfgspc\n            # also, if we turn unrepr on, we don't need cfgGetBool\n        else:\n            return {}", "response": "Return a dict of all user settings found in rcFile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _saveGuiSettings(self):\n        # Put the settings into a ConfigObj dict (don't use a config-spec)\n        rcFile = self._rcDir+os.sep+APP_NAME.lower()+'.cfg'\n        #\n        if os.path.exists(rcFile): os.remove(rcFile)\n        co = configobj.ConfigObj(rcFile) # can skip try-block, won't read file\n\n        co['showHelpInBrowser']       = self._showHelpInBrowser\n        co['saveAndCloseOnExec']      = self._saveAndCloseOnExec\n        co['writeProtectOnSaveAsOpt'] = self._writeProtectOnSaveAs\n        co['flagNonDefaultVals']      = self._flagNonDefaultVals\n        co['frameColor']              = self._frmeColor\n        co['taskBoxColor']            = self._taskColor\n        co['buttonBoxColor']          = self._bboxColor\n        co['entriesColor']            = self._entsColor\n        co['flaggedColor']            = self._flagColor\n\n        co.initial_comment = ['Automatically generated by '+\\\n            APP_NAME+'.  All edits will eventually be overwritten.']\n        co.initial_comment.append('To use platform default colors, delete each color line below.')\n        co.final_comment = [''] # ensure \\n at EOF\n        co.write()", "response": "Save the settings of the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads an association table from a FITS file and returns a dictionary that can be used by pydrizzle and multidrizzle.", "response": "def readASNTable(fname, output=None, prodonly=False):\n    \"\"\"\n    Given a fits filename repesenting an association table reads in the table as a\n    dictionary which can be used by pydrizzle and multidrizzle.\n\n    An association table is a FITS binary table with 2 required columns: 'MEMNAME',\n    'MEMTYPE'. It checks 'MEMPRSNT' column and removes all files for which its value is 'no'.\n\n    Parameters\n    ----------\n    fname : str\n        name of association table\n    output : str\n        name of output product - if not specified by the user,\n        the first PROD-DTH name is used if present,\n        if not, the first PROD-RPT name is used if present,\n        if not, the rootname of the input association table is used.\n    prodonly : bool\n        what files should be considered as input\n        if True - select only MEMTYPE=PROD* as input\n        if False - select only MEMTYPE=EXP as input\n\n    Returns\n    -------\n    asndict : dict\n        A dictionary-like object with all the association information.\n\n    Examples\n    --------\n    An association table can be read from a file using the following commands::\n\n    >>> from stsci.tools import asnutil\n    >>> asntab = asnutil.readASNTable('j8bt06010_shifts_asn.fits', prodonly=False)  # doctest: +SKIP\n\n    The `asntab` object can now be passed to other code to provide relationships\n    between input and output images defined by the association table.\n\n    \"\"\"\n\n    try:\n        f = fits.open(fu.osfn(fname))\n    except:\n        raise IOError(\"Can't open file %s\\n\" % fname)\n\n    colnames = f[1].data.names\n    try:\n        colunits = f[1].data.units\n    except AttributeError: pass\n\n    hdr = f[0].header\n\n    if 'MEMNAME' not in colnames or 'MEMTYPE' not in colnames:\n        msg = 'Association table incomplete: required column(s) MEMNAME/MEMTYPE NOT found!'\n        raise ValueError(msg)\n\n    d = {}\n    for n in colnames:\n        d[n]=f[1].data.field(n)\n    f.close()\n\n    valid_input = d['MEMPRSNT'].copy()\n    memtype = d['MEMTYPE'].copy()\n    prod_dth = (memtype.find('PROD-DTH')==0).nonzero()[0]\n    prod_rpt = (memtype.find('PROD-RPT')==0).nonzero()[0]\n    prod_crj = (memtype.find('PROD-CRJ')==0).nonzero()[0]\n\n    # set output name\n    if output is None:\n        if prod_dth:\n            output = d['MEMNAME'][prod_dth[0]]\n        elif prod_rpt:\n            output = d['MEMNAME'][prod_rpt[0]]\n        elif prod_crj:\n            output = d['MEMNAME'][prod_crj[0]]\n        else:\n            output = fname.split('_')[0]\n\n    if prodonly:\n        input = d['MEMTYPE'].find('PROD')==0\n        if prod_dth:\n            input[prod_dth] = False\n    else:\n        input = (d['MEMTYPE'].find('EXP')==0)\n    valid_input *= input\n\n    for k in d:\n        d[k] = d[k][valid_input]\n\n    infiles = list(d['MEMNAME'].lower())\n    if not infiles:\n        print(\"No valid input specified\")\n        return None\n\n    if ('XOFFSET' in colnames and d['XOFFSET'].any()) or ('YOFFSET' in colnames and d['YOFFSET'].any()):\n        abshift = True\n        dshift = False\n        try:\n            units=colunits[colnames.index('XOFFSET')]\n        except: units='pixels'\n        xshifts = list(d['XOFFSET'])\n        yshifts = list(d['YOFFSET'])\n    elif ('XDELTA' in colnames and d['XDELTA'].any()) or  ('YDELTA' in colnames and d['YDELTA'].any()):\n        abshift = False\n        dshift = True\n        try:\n            units=colunits[colnames.index('XDELTA')]\n        except: units='pixels'\n        xshifts = list(d['XDELTA'])\n        yshifts = list(d['YDELTA'])\n    else:\n        abshift = False\n        dshift = False\n    members = {}\n\n    if not abshift and not dshift:\n        asndict = ASNTable(infiles,output=output)\n        asndict.create()\n        return asndict\n    else:\n        try:\n            refimage = hdr['refimage']\n        except KeyError: refimage = None\n        try:\n            frame = hdr['shframe']\n        except KeyError: frame = 'input'\n        if 'ROTATION' in colnames:\n            rots = list(d['ROTATION'])\n        if 'SCALE' in colnames:\n            scales = list(d['SCALE'])\n\n        for r in range(len(infiles)):\n            row = r\n            xshift = xshifts[r]\n            yshift = yshifts[r]\n            if rots: rot = rots[r]\n            if scales: scale = scales[r]\n            members[infiles[r]] = ASNMember(row=row, dshift=dshift, abshift=abshift, rot=rot, xshift=xshift,\n                                      yshift=yshift, scale=scale, refimage=refimage, shift_frame=frame,\n                                      shift_units=units)\n\n\n        asndict= ASNTable(infiles, output=output)\n        asndict.create()\n        asndict['members'].update(members)\n        return asndict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the association table to a FITS file.", "response": "def write(self, output=None):\n        \"\"\"\n        Write association table to a file.\n\n        \"\"\"\n        if not output:\n            outfile = self['output']+'_asn.fits'\n            output = self['output']\n        else:\n            outfile = output\n\n        # Delete the file if it exists.\n        if os.path.exists(outfile):\n            warningmsg =  \"\\n#########################################\\n\"\n            warningmsg += \"#                                       #\\n\"\n            warningmsg += \"# WARNING:                              #\\n\"\n            warningmsg += \"#  The existing association table,      #\\n\"\n            warningmsg += \"           \" + str(outfile) + '\\n'\n            warningmsg += \"#  is being replaced.                   #\\n\"\n            warningmsg += \"#                                       #\\n\"\n            warningmsg += \"#########################################\\n\\n\"\n        fasn = fits.HDUList()\n\n        # Compute maximum length of MEMNAME for table column definition\n        _maxlen = 0\n        for _fname in self['order']:\n            if len(_fname) > _maxlen: _maxlen = len(_fname)\n        # Enforce a mimimum size of 24\n        if _maxlen < 24: _maxlen = 24\n        namelen_str = str(_maxlen+2)+'A'\n        self.buildPrimary(fasn, output=output)\n\n        mname = self['order'][:]\n        mname.append(output)\n        mtype = ['EXP-DTH' for l in self['order']]\n        mtype.append('PROD-DTH')\n        mprsn = [True for l in self['order']]\n        mprsn.append(False)\n        xoff = [self['members'][l]['xoff'] for l in self['order']]\n        xoff.append(0.0)\n        yoff = [self['members'][l]['yoff'] for l in self['order']]\n        yoff.append(0.0)\n        xsh = [self['members'][l]['xshift'] for l in self['order']]\n        xsh.append(0.0)\n        ysh = [self['members'][l]['yshift'] for l in self['order']]\n        ysh.append(0.0)\n        rot = [self['members'][l]['rot'] for l in self['order']]\n        rot.append(0.0)\n        scl = [self['members'][l]['scale'] for l in self['order']]\n        scl.append(1.0)\n\n        memname = fits.Column(name='MEMNAME',format=namelen_str,array=N.char.array(mname))\n        memtype = fits.Column(name='MEMTYPE',format='14A',array=N.char.array(mtype))\n        memprsn = fits.Column(name='MEMPRSNT', format='L', array=N.array(mprsn).astype(N.uint8))\n        xoffset = fits.Column(name='XOFFSET', format='E', array=N.array(xoff))\n        yoffset = fits.Column(name='YOFFSET', format='E', array=N.array(yoff))\n        xdelta = fits.Column(name='XDELTA', format='E', array=N.array(xsh))\n        ydelta = fits.Column(name='YDELTA', format='E', array=N.array(ysh))\n        rotation = fits.Column(name='ROTATION', format='E', array=N.array(rot))\n        scale = fits.Column(name='SCALE', format='E', array=N.array(scl))\n        cols = fits.ColDefs([memname,memtype,memprsn,xoffset,yoffset,xdelta,ydelta,rotation,scale])\n        hdu = fits.BinTableHDU.from_columns(cols)\n        fasn.append(hdu)\n        if ASTROPY_VER_GE13:\n            fasn.writeto(outfile, overwrite=True)\n        else:\n            fasn.writeto(outfile, clobber=True)\n        fasn.close()\n        mem0 = self['order'][0]\n        refimg = self['members'][mem0]['refimage']\n        if refimg is not None:\n            whdu = wcsutil.WCSObject(refimg)\n            whdu.createReferenceWCS(outfile,overwrite=False)\n            ftab = fits.open(outfile)\n            ftab['primary'].header['refimage'] = outfile+\"[wcs]\"\n            ftab.close()\n        del whdu"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readShiftFile(self, filename):\n        order = []\n        fshift = open(filename,'r')\n        flines = fshift.readlines()\n        fshift.close()\n\n        common = [f.strip('#').strip() for f in flines if f.startswith('#')]\n        c=[line.split(': ') for line in common]\n\n        # Remove any line comments in the shift file - lines starting with '#'\n        # but not part of the common block.\n        for l in c:\n            if l[0] not in ['frame', 'refimage', 'form', 'units']:\n                c.remove(l)\n\n        for line in c: line[1]=line[1].strip()\n        self.update(c)\n\n        files = [f.strip().split(' ',1) for f in flines if not (f.startswith('#') or f.strip() == '')]\n        for f in files:\n            order.append(f[0])\n\n        self['order'] = order\n\n        for f in files:\n            # Check to see if filename provided is a full filename that corresponds\n            # to a file on the path.  If not, try to convert given rootname into\n            # a valid filename based on available files.  This may or may not\n            # define the correct filename, which is why it prints out what it is\n            # doing, so that the user can verify and edit the shiftfile if needed.\n            #NOTE:\n            # Supporting the specification of only rootnames in the shiftfile with this\n            # filename expansion is NOT to be documented, but provided solely as\n            # an undocumented, dangerous and not fully supported helper function for\n            # some backwards compatibility.\n            if not os.path.exists(f[0]):\n                f[0] = fu.buildRootname(f[0])\n                print('Defining filename in shiftfile as: ', f[0])\n\n            f[1] = f[1].split()\n            try:\n                f[1] = [float(s) for s in f[1]]\n            except:\n                msg = 'Cannot read in ', s, ' from shiftfile ', filename, ' as a float number'\n                raise ValueError(msg)\n            msg = \"At least 2 and at most 4 shift values should be provided in a shiftfile\"\n            if len(f[1]) < 2:\n                raise ValueError(msg)\n            elif len(f[1]) == 3:\n                f[1].append(1.0)\n            elif len(f[1]) == 2:\n                f[1].extend([0.0, 1.0])\n            elif len(f[1]) > 4:\n                raise ValueError(msg)\n\n        fdict = dict(files)\n        self.update(fdict)", "response": "Reads a shift file from disk and populates the internal dictionary with the information from that file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a shift file object to a file on disk using the convention for shift file format.", "response": "def writeShiftFile(self, filename=\"shifts.txt\"):\n        \"\"\"\n        Writes a shift file object to a file on disk using the convention for shift file format.\n        \"\"\"\n        lines = ['# frame: ', self['frame'], '\\n',\n                 '# refimage: ', self['refimage'], '\\n',\n                 '# form: ', self['form'], '\\n',\n                 '# units: ', self['units'], '\\n']\n\n        for o in self['order']:\n            ss = \" \"\n            for shift in self[o]:\n                ss += str(shift) + \" \"\n            line = str(o) + ss + \"\\n\"\n            lines.append(line)\n\n        fshifts= open(filename, 'w')\n        fshifts.writelines(lines)\n        fshifts.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef weave(target, aspects, **options):\n    if not callable(aspects):\n        if not hasattr(aspects, '__iter__'):\n            raise ExpectedAdvice('%s must be an `Aspect` instance, a callable or an iterable of.' % aspects)\n        for obj in aspects:\n            if not callable(obj):\n                raise ExpectedAdvice('%s must be an `Aspect` instance or a callable.' % obj)\n    assert target, \"Can't weave falsy value %r.\" % target\n    logdebug(\"weave (target=%s, aspects=%s, **options=%s)\", target, aspects, options)\n\n    bag = options.setdefault('bag', ObjectBag())\n\n    if isinstance(target, (list, tuple)):\n        return Rollback([\n            weave(item, aspects, **options) for item in target\n        ])\n    elif isinstance(target, basestring):\n        parts = target.split('.')\n        for part in parts:\n            _check_name(part)\n\n        if len(parts) == 1:\n            return weave_module(_import_module(part), aspects, **options)\n\n        for pos in reversed(range(1, len(parts))):\n            owner, name = '.'.join(parts[:pos]), '.'.join(parts[pos:])\n            try:\n                owner = _import_module(owner)\n            except ImportError:\n                continue\n            else:\n                break\n        else:\n            raise ImportError(\"Could not import %r. Last try was for %s\" % (target, owner))\n\n        if '.' in name:\n            path, name = name.rsplit('.', 1)\n            path = deque(path.split('.'))\n            while path:\n                owner = getattr(owner, path.popleft())\n\n        logdebug(\"@ patching %s from %s ...\", name, owner)\n        obj = getattr(owner, name)\n\n        if isinstance(obj, (type, ClassType)):\n            logdebug(\"   .. as a class %r.\", obj)\n            return weave_class(\n                obj, aspects,\n                owner=owner, name=name, **options\n            )\n        elif callable(obj):  # or isinstance(obj, FunctionType) ??\n            logdebug(\"   .. as a callable %r.\", obj)\n            if bag.has(obj):\n                return Nothing\n            return patch_module_function(owner, obj, aspects, force_name=name, **options)\n        else:\n            return weave(obj, aspects, **options)\n\n    name = getattr(target, '__name__', None)\n    if name and getattr(__builtin__, name, None) is target:\n        if bag.has(target):\n            return Nothing\n        return patch_module_function(__builtin__, target, aspects, **options)\n    elif PY3 and ismethod(target):\n        if bag.has(target):\n            return Nothing\n        inst = target.__self__\n        name = target.__name__\n        logdebug(\"@ patching %r (%s) as instance method.\", target, name)\n        func = target.__func__\n        setattr(inst, name, _checked_apply(aspects, func).__get__(inst, type(inst)))\n        return Rollback(lambda: delattr(inst, name))\n    elif PY3 and isfunction(target):\n        if bag.has(target):\n            return Nothing\n        owner = _import_module(target.__module__)\n        path = deque(target.__qualname__.split('.')[:-1])\n        while path:\n            owner = getattr(owner, path.popleft())\n        name = target.__name__\n        logdebug(\"@ patching %r (%s) as a property.\", target, name)\n        func = owner.__dict__[name]\n        return patch_module(owner, name, _checked_apply(aspects, func), func, **options)\n    elif PY2 and isfunction(target):\n        if bag.has(target):\n            return Nothing\n        return patch_module_function(_import_module(target.__module__), target, aspects, **options)\n    elif PY2 and ismethod(target):\n        if target.im_self:\n            if bag.has(target):\n                return Nothing\n            inst = target.im_self\n            name = target.__name__\n            logdebug(\"@ patching %r (%s) as instance method.\", target, name)\n            func = target.im_func\n            setattr(inst, name, _checked_apply(aspects, func).__get__(inst, type(inst)))\n            return Rollback(lambda: delattr(inst, name))\n        else:\n            klass = target.im_class\n            name = target.__name__\n            return weave(klass, aspects, methods='%s$' % name, **options)\n    elif isclass(target):\n        return weave_class(target, aspects, **options)\n    elif ismodule(target):\n        return weave_module(target, aspects, **options)\n    elif type(target).__module__ not in ('builtins', '__builtin__') or InstanceType and isinstance(target, InstanceType):\n        return weave_instance(target, aspects, **options)\n    else:\n        raise UnsupportedType(\"Can't weave object %s of type %s\" % (target, type(target)))", "response": "Send a message to a recipient that weave the target object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef weave_instance(instance, aspect, methods=NORMAL_METHODS, lazy=False, bag=BrokenBag, **options):\n    if bag.has(instance):\n        return Nothing\n\n    entanglement = Rollback()\n    method_matches = make_method_matcher(methods)\n    logdebug(\"weave_instance (module=%r, aspect=%s, methods=%s, lazy=%s, **options=%s)\",\n             instance, aspect, methods, lazy, options)\n\n    def fixup(func):\n        return func.__get__(instance, type(instance))\n    fixed_aspect = aspect + [fixup] if isinstance(aspect, (list, tuple)) else [aspect, fixup]\n\n    for attr in dir(instance):\n        func = getattr(instance, attr)\n        if method_matches(attr):\n            if ismethod(func):\n                if hasattr(func, '__func__'):\n                    realfunc = func.__func__\n                else:\n                    realfunc = func.im_func\n                entanglement.merge(\n                    patch_module(instance, attr, _checked_apply(fixed_aspect, realfunc, module=None), **options)\n                )\n    return entanglement", "response": "Low-level weaver for instances.\n\n    .. warning:: You should not use this directly.\n\n    :returns: An :obj:`aspectlib.Rollback` object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weave_module(module, aspect, methods=NORMAL_METHODS, lazy=False, bag=BrokenBag, **options):\n    if bag.has(module):\n        return Nothing\n\n    entanglement = Rollback()\n    method_matches = make_method_matcher(methods)\n    logdebug(\"weave_module (module=%r, aspect=%s, methods=%s, lazy=%s, **options=%s)\",\n             module, aspect, methods, lazy, options)\n\n    for attr in dir(module):\n        func = getattr(module, attr)\n        if method_matches(attr):\n            if isroutine(func):\n                entanglement.merge(patch_module_function(module, func, aspect, force_name=attr, **options))\n            elif isclass(func):\n                entanglement.merge(\n                    weave_class(func, aspect, owner=module, name=attr, methods=methods, lazy=lazy, bag=bag, **options),\n                    #  it's not consistent with the other ways of weaving a class (it's never weaved as a routine).\n                    #  therefore it's disabled until it's considered useful.\n                    #  #patch_module_function(module, getattr(module, attr), aspect, force_name=attr, **options),\n                )\n    return entanglement", "response": "Low-level weaver for \"whole module weaving\".\n\n    .. warning:: You should not use this directly.\n\n    :returns: An :obj:`aspectlib.Rollback` object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npatches an object in a module.", "response": "def patch_module(module, name, replacement, original=UNSPECIFIED, aliases=True, location=None, **_bogus_options):\n    \"\"\"\n    Low-level attribute patcher.\n\n    :param module module: Object to patch.\n    :param str name: Attribute to patch\n    :param replacement: The replacement value.\n    :param original: The original value (in case the object beeing patched uses descriptors or is plain weird).\n    :param bool aliases: If ``True`` patch all the attributes that have the same original value.\n\n    :returns: An :obj:`aspectlib.Rollback` object.\n    \"\"\"\n    rollback = Rollback()\n    seen = False\n    original = getattr(module, name) if original is UNSPECIFIED else original\n    location = module.__name__ if hasattr(module, '__name__') else type(module).__module__\n    target = module.__name__ if hasattr(module, '__name__') else type(module).__name__\n    try:\n        replacement.__module__ = location\n    except (TypeError, AttributeError):\n        pass\n    for alias in dir(module):\n        logdebug(\"alias:%s (%s)\", alias, name)\n        if hasattr(module, alias):\n            obj = getattr(module, alias)\n            logdebug(\"- %s:%s (%s)\", obj, original, obj is original)\n            if obj is original:\n                if aliases or alias == name:\n                    logdebug(\"= saving %s on %s.%s ...\", replacement, target, alias)\n                    setattr(module, alias, replacement)\n                    rollback.merge(lambda alias=alias: setattr(module, alias, original))\n                if alias == name:\n                    seen = True\n            elif alias == name:\n                if ismethod(obj):\n                    logdebug(\"= saving %s on %s.%s ...\", replacement, target, alias)\n                    setattr(module, alias, replacement)\n                    rollback.merge(lambda alias=alias: setattr(module, alias, original))\n                    seen = True\n                else:\n                    raise AssertionError(\"%s.%s = %s is not %s.\" % (module, alias, obj, original))\n\n    if not seen:\n        warnings.warn('Setting %s.%s to %s. There was no previous definition, probably patching the wrong module.' % (\n            target, name, replacement\n        ))\n        logdebug(\"= saving %s on %s.%s ...\", replacement, target, name)\n        setattr(module, name, replacement)\n        rollback.merge(lambda: setattr(module, name, original))\n    return rollback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npatching a module function with a target function.", "response": "def patch_module_function(module, target, aspect, force_name=None, bag=BrokenBag, **options):\n    \"\"\"\n    Low-level patcher for one function from a specified module.\n\n    .. warning:: You should not use this directly.\n\n    :returns: An :obj:`aspectlib.Rollback` object.\n    \"\"\"\n    logdebug(\"patch_module_function (module=%s, target=%s, aspect=%s, force_name=%s, **options=%s\",\n             module, target, aspect, force_name, options)\n    name = force_name or target.__name__\n    return patch_module(module, name, _checked_apply(aspect, target, module=module), original=target, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unframe(packet):\n\n    if is_framed(packet):\n        return packet.lstrip(CHR_DLE).rstrip(CHR_ETX).rstrip(CHR_DLE)\n    else:\n        raise ValueError('packet does not contain leading DLE and trailing DLE/ETX')", "response": "Unframe a TSIP packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stuff(packet):\n\n    if is_framed(packet):\n        raise ValueError('packet contains leading DLE and trailing DLE/ETX')\n    else:\n        return packet.replace(CHR_DLE, CHR_DLE + CHR_DLE)", "response": "Add byte stuffing to TSIP packet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving byte stuffing from a TSIP packet.", "response": "def unstuff(packet):\n    \"\"\"\n    Remove byte stuffing from a TSIP packet.\n\n    :param packet: TSIP packet with byte stuffing. The packet must already\n        have been stripped or `ValueError` will be raised.\n    :type packet: Binary string.\n    :return: Packet without byte stuffing.\n\n    \"\"\"\n\n    if is_framed(packet):\n        raise ValueError('packet contains leading DLE and trailing DLE/ETX')\n    else:\n        return packet.replace(CHR_DLE + CHR_DLE, CHR_DLE)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parseFilename(filename):\n    # Parse out any extension specified in filename\n    _indx = filename.find('[')\n    if _indx > 0:\n        # Read extension name provided\n        _fname = filename[:_indx]\n        extn = filename[_indx+1:-1]\n\n        # An extension was provided, so parse it out...\n        if repr(extn).find(',') > 1:\n            _extns = extn.split(',')\n            # Two values given for extension:\n            #    for example, 'sci,1' or 'dq,1'\n            _extn = [_extns[0],int(_extns[1])]\n        elif repr(extn).find('/') > 1:\n            # We are working with GEIS group syntax\n            _indx = str(extn[:extn.find('/')])\n            _extn = [int(_indx)]\n        elif isinstance(extn, str):\n            # Only one extension value specified...\n            if extn.isdigit():\n                # We only have an extension number specified as a string...\n                _nextn = int(extn)\n            else:\n                # We only have EXTNAME specified...\n                _nextn = extn\n            _extn = [_nextn]\n        else:\n            # Only integer extension number given, or default of 0 is used.\n            _extn = [int(extn)]\n\n    else:\n        _fname = filename\n        _extn = None\n    return _fname,_extn", "response": "Parse out a filename from any specified extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the shape of the data array associated with this file.", "response": "def _shape(self):\n        \"\"\" Returns the shape of the data array associated with this file.\"\"\"\n        hdu = self.open()\n        _shape = hdu.shape\n        if not self.inmemory:\n            self.close()\n            del hdu\n        return _shape"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the data array associated with this file and extenstion.", "response": "def _data(self):\n        \"\"\" Returns the data array associated with this file/extenstion.\"\"\"\n        hdu = self.open()\n        _data = hdu.data.copy()\n        if not self.inmemory:\n            self.close()\n            del hdu\n        return _data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the shape of the data array associated with this file.", "response": "def type(self):\n        \"\"\" Returns the shape of the data array associated with this file.\"\"\"\n        hdu = self.open()\n        _type = hdu.data.dtype.name\n        if not self.inmemory:\n            self.close()\n            del hdu\n        return _type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen the file for subsequent access.", "response": "def open(self):\n        \"\"\" Opens the file for subsequent access. \"\"\"\n\n        if self.handle is None:\n            self.handle = fits.open(self.fname, mode='readonly')\n\n        if self.extn:\n            if len(self.extn) == 1:\n                hdu = self.handle[self.extn[0]]\n            else:\n                hdu = self.handle[self.extn[0],self.extn[1]]\n        else:\n            hdu = self.handle[0]\n        if isinstance(hdu,fits.hdu.compressed.CompImageHDU):\n            self.compress = True\n        return hdu"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overlapsWith(self, targetStartTime, duration):\n        targetEndTime = targetStartTime + datetime.timedelta(days=0, seconds=duration)\n        return (self.startTime <= targetStartTime <= self.endTime) \\\n               or (targetStartTime <= self.startTime <= targetEndTime)", "response": "Tests if the given times overlap with this measurement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef updateDeviceStatus(self, deviceName, state, reason=None):\n        logger.info('Updating recording device state for ' + deviceName + ' to ' + state.name +\n                    ('' if reason is None else '[reason: ' + reason + ']'))\n        currentState = self.recordingDevices.get(deviceName)\n        count = 0\n        if currentState is not None:\n            if currentState['state'] == MeasurementStatus.RECORDING.name:\n                count = currentState['count']\n        self.recordingDevices[deviceName] = {\n            'state': state.name,\n            'reason': reason,\n            'time': datetime.datetime.utcnow().strftime(DATETIME_FORMAT),\n            'count': count\n        }", "response": "Updates the current status of the given device."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stillRecording(self, deviceId, dataCount):\n        status = self.recordingDevices[deviceId]\n        if status is not None:\n            if status['state'] == MeasurementStatus.RECORDING.name:\n                status['last'] = datetime.datetime.utcnow().strftime(DATETIME_FORMAT)\n                status['count'] = status['count'] + dataCount", "response": "Updates the last timestamp so we now when we last received data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inflate(self):\n        if self.measurementParameters['accelerometerEnabled']:\n            if len(self.data) == 0:\n                logger.info('Loading measurement data for ' + self.name)\n                self.data = {name: self._loadXYZ(name) for name, value in self.recordingDevices.items()}\n            return True\n        else:\n            # TODO error handling\n            return False", "response": "Loads the recording into memory and returns it as a Signal\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sweep(self):\n        while self.running:\n            for am in list(self.activeMeasurements):\n                now = datetime.datetime.utcnow()\n                # devices were allocated and have completed == complete\n                recordingDeviceCount = len(am.recordingDevices)\n                if recordingDeviceCount > 0:\n                    if all(entry['state'] == RecordStatus.COMPLETE.name for entry in am.recordingDevices.values()):\n                        logger.info(\"Detected completedmeasurement \" + am.id)\n                        self._moveToComplete(am)\n\n                # we have reached the end time and we have either all failed devices or no devices == kill\n                if now > (am.endTime + datetime.timedelta(days=0, seconds=1)):\n                    allFailed = all(entry['state'] == RecordStatus.FAILED.name\n                                    for entry in am.recordingDevices.values())\n                    if (recordingDeviceCount > 0 and allFailed) or recordingDeviceCount == 0:\n                        logger.warning(\"Detected failed measurement \" + am.id + \" with \" + str(recordingDeviceCount)\n                                       + \" devices, allFailed: \" + str(allFailed))\n                        self._moveToFailed(am)\n\n                # we are well past the end time and we have failed devices or an ongoing recording == kill or deathbed\n                if now > (am.endTime + datetime.timedelta(days=0, seconds=self.maxTimeTilDeathbedSeconds)):\n                    if any(entry['state'] == RecordStatus.FAILED.name for entry in am.recordingDevices.values()):\n                        logger.warning(\"Detected failed and incomplete measurement \" + am.id + \", assumed dead\")\n                        self._moveToFailed(am)\n                    elif all(entry['state'] == RecordStatus.RECORDING.name for entry in am.recordingDevices.values()):\n                        self._handleDeathbed(am)\n            time.sleep(0.1)\n        logger.warning(\"MeasurementCaretaker is now shutdown\")", "response": "Sweep the activeMeasurements and check their state and passes them to the completed set if they are still running and if they are still in the deathbed set then evicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef schedule(self, name, duration, startTime, description=None):\n        if self._clashes(startTime, duration):\n            return False, MEASUREMENT_TIMES_CLASH\n        else:\n            am = ActiveMeasurement(name, startTime, duration, self.targetStateProvider.state, description=description)\n            logger.info(\"Scheduling measurement \" + am.id + \" for \" + str(duration) + \"s\")\n            self.activeMeasurements.append(am)\n            devices = self.deviceController.scheduleMeasurement(am.id, am.duration, am.startTime)\n            anyFail = False\n            for device, status in devices.items():\n                if status == 200:\n                    deviceStatus = RecordStatus.SCHEDULED\n                else:\n                    deviceStatus = RecordStatus.FAILED\n                    anyFail = True\n                am.updateDeviceStatus(device.deviceId, deviceStatus)\n            if anyFail:\n                am.status = MeasurementStatus.FAILED\n            else:\n                if am.status is MeasurementStatus.NEW:\n                    am.status = MeasurementStatus.SCHEDULED\n            return True, None", "response": "Schedules a new measurement with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that this measurement does not clash with an already scheduled measurement.", "response": "def _clashes(self, startTime, duration):\n        \"\"\"\n        verifies that this measurement does not clash with an already scheduled measurement.\n        :param startTime: the start time.\n        :param duration: the duration.\n        :return: true if the measurement is allowed.\n        \"\"\"\n        return [m for m in self.activeMeasurements if m.overlapsWith(startTime, duration)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the measurement for the device.", "response": "def startMeasurement(self, measurementId, deviceId):\n        \"\"\"\n        Starts the measurement for the device.\n        :param deviceId: the device that is starting.\n        :param measurementId: the measurement that is started.\n        :return: true if it started (i.e. device and measurement exists).\n        \"\"\"\n        am, handler = self.getDataHandler(measurementId, deviceId)\n        if am is not None:\n            am.status = MeasurementStatus.RECORDING\n            am.updateDeviceStatus(deviceId, RecordStatus.RECORDING)\n            handler.start(am.idAsPath)\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the active measurement and handler for the given device", "response": "def getDataHandler(self, measurementId, deviceId):\n        \"\"\"\n        finds the handler.\n        :param measurementId: the measurement\n        :param deviceId: the device.\n        :return: active measurement and handler\n        \"\"\"\n        am = next((m for m in self.activeMeasurements if m.id == measurementId), None)\n        if am is None:\n            return None, None\n        else:\n            device = self.deviceController.getDevice(deviceId)\n            if device is None:\n                return None, None\n            else:\n                return am, device.dataHandler"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npasses the data to the handler.", "response": "def recordData(self, measurementId, deviceId, data):\n        \"\"\"\n        Passes the data to the handler.\n        :param deviceId: the device the data comes from.\n        :param measurementId: the measurement id.\n        :param data: the data.\n        :return: true if the data was handled.\n        \"\"\"\n        am, handler = self.getDataHandler(measurementId, deviceId)\n        if handler is not None:\n            am.stillRecording(deviceId, len(data))\n            handler.handle(data)\n            return True\n        else:\n            logger.error('Received data for unknown handler ' + deviceId + '/' + measurementId)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef completeMeasurement(self, measurementId, deviceId):\n        am, handler = self.getDataHandler(measurementId, deviceId)\n        if handler is not None:\n            handler.stop(measurementId)\n            am.updateDeviceStatus(deviceId, RecordStatus.COMPLETE)\n            return True\n        else:\n            return False", "response": "Completes the measurement session."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef failMeasurement(self, measurementId, deviceName, failureReason=None):\n        am, handler = self.getDataHandler(measurementId, deviceName)\n        if handler is not None:\n            am.updateDeviceStatus(deviceName, RecordStatus.FAILED, reason=failureReason)\n            handler.stop(measurementId)\n            return True\n        else:\n            return False", "response": "Fail the measurement session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _deleteCompletedMeasurement(self, measurementId):\n        message, count, deleted = self.deleteFrom(measurementId, self.completeMeasurements)\n        if count is 0:\n            message, count, deleted = self.deleteFrom(measurementId, self.failedMeasurements)\n        return message, count, deleted", "response": "Deletes the named measurement from the completed measurement store if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreload the completed measurements from the backing store.", "response": "def reloadCompletedMeasurements(self):\n        \"\"\"\n        Reloads the completed measurements from the backing store.\n        \"\"\"\n        from pathlib import Path\n        reloaded = [self.load(x.resolve()) for x in Path(self.dataDir).glob('*/*/*') if x.is_dir()]\n        logger.info('Reloaded ' + str(len(reloaded)) + ' completed measurements')\n        self.completeMeasurements = [x for x in reloaded if x is not None and x.status == MeasurementStatus.COMPLETE]\n        self.failedMeasurements = [x for x in reloaded if x is not None and x.status == MeasurementStatus.FAILED]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getMeasurements(self, measurementStatus=None):\n        if measurementStatus is None:\n            return self.activeMeasurements + self.completeMeasurements + self.failedMeasurements\n        elif measurementStatus == MeasurementStatus.COMPLETE:\n            return self.completeMeasurements\n        elif measurementStatus == MeasurementStatus.FAILED:\n            return self.failedMeasurements\n        elif measurementStatus == MeasurementStatus.DYING:\n            return list(self.deathBed.keys())\n        else:\n            return [x for x in self.activeMeasurements if x.status == measurementStatus]", "response": "Gets all available measurements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getMeasurement(self, measurementId, measurementStatus=None):\n        return next((x for x in self.getMeasurements(measurementStatus) if x.id == measurementId), None)", "response": "Gets the measurement with the given id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store(self, measurement):\n        os.makedirs(self._getPathToMeasurementMetaDir(measurement.idAsPath), exist_ok=True)\n        output = marshal(measurement, measurementFields)\n        with open(self._getPathToMeasurementMetaFile(measurement.idAsPath), 'w') as outfile:\n            json.dump(output, outfile)\n        return output", "response": "Stores the metadata of the active measurement in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, path):\n        meta = self._loadMetaFromJson(path)\n        return CompleteMeasurement(meta, self.dataDir) if meta is not None else None", "response": "Loads a CompletedMeasurement from the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the json meta into memory.", "response": "def _loadMetaFromJson(self, path):\n        \"\"\"\n        Reads the json meta into memory.\n        :return: the meta.\n        \"\"\"\n        try:\n            with (path / 'metadata.json').open() as infile:\n                return json.load(infile)\n        except FileNotFoundError:\n            logger.error('Metadata does not exist at ' + str(path))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef editMeasurement(self, measurementId, data):\n        oldMeasurement = self.getMeasurement(measurementId, measurementStatus=MeasurementStatus.COMPLETE)\n        if oldMeasurement:\n            import copy\n            newMeasurement = copy.deepcopy(oldMeasurement)\n            deleteOld = False\n            createdFilteredCopy = False\n            newName = data.get('name', None)\n            newDesc = data.get('description', None)\n            newStart = float(data.get('start', 0))\n            newEnd = float(data.get('end', oldMeasurement.duration))\n            newDuration = newEnd - newStart\n            newDevices = data.get('devices', None)\n            if newName:\n                logger.info('Updating name from ' + oldMeasurement.name + ' to ' + newName)\n                newMeasurement.updateName(newName)\n                createdFilteredCopy = True\n                deleteOld = True\n            if newDesc:\n                logger.info('Updating description from ' + str(oldMeasurement.description) + ' to ' + str(newDesc))\n                newMeasurement.description = newDesc\n            if newDuration != oldMeasurement.duration:\n                logger.info('Copying measurement to allow support new duration ' + str(newDuration))\n                if oldMeasurement.name == newMeasurement.name:\n                    newMeasurement.updateName(newMeasurement.name + '-' + str(int(time.time())))\n                newMeasurement.duration = newDuration\n                createdFilteredCopy = True\n            if createdFilteredCopy:\n                logger.info('Copying measurement data from ' + oldMeasurement.idAsPath + ' to ' + newMeasurement.idAsPath)\n                newMeasurementPath = self._getPathToMeasurementMetaDir(newMeasurement.idAsPath)\n                dataSearchPattern = self._getPathToMeasurementMetaDir(oldMeasurement.idAsPath) + '/**/data.out'\n                newDataCountsByDevice = [self._filterCopy(dataFile, newStart, newEnd, newMeasurementPath)\n                                         for dataFile in glob.glob(dataSearchPattern)]\n                for device, count in newDataCountsByDevice:\n                    newMeasurement.recordingDevices.get(device)['count'] = count\n            self.store(newMeasurement)\n            if newDevices:\n                for renames in newDevices:\n                    logger.info('Updating device name from ' + str(renames[0]) + ' to ' + str(renames[1]))\n                    deviceState = newMeasurement.recordingDevices.get(renames[0])\n                    newMeasurement.recordingDevices[renames[1]] = deviceState\n                    del newMeasurement.recordingDevices[renames[0]]\n                    os.rename(os.path.join(self._getPathToMeasurementMetaDir(newMeasurement.idAsPath), renames[0]),\n                              os.path.join(self._getPathToMeasurementMetaDir(newMeasurement.idAsPath), renames[1]))\n                self.store(newMeasurement)\n            if deleteOld or createdFilteredCopy or newDevices:\n                self.completeMeasurements.append(newMeasurement)\n            if deleteOld:\n                self.delete(oldMeasurement.id)\n            return True\n        else:\n            return False", "response": "Edits the specified measurement with the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filterCopy(self, dataFile, newStart, newEnd, newDataDir):\n        import csv\n        pathToData = os.path.split(dataFile)\n        dataFileName = pathToData[1]\n        dataDeviceName = os.path.split(pathToData[0])[1]\n        os.makedirs(os.path.join(newDataDir, dataDeviceName), exist_ok=True)\n        outputFile = os.path.join(newDataDir, dataDeviceName, dataFileName)\n        dataCount = 0\n        rowNum = 0\n        with open(dataFile, mode='rt', newline='') as dataIn, open(outputFile, mode='wt', newline='') as dataOut:\n            writer = csv.writer(dataOut, delimiter=',')\n            for row in csv.reader(dataIn, delimiter=','):\n                if len(row) > 0:\n                    time = float(row[0])\n                    if newStart <= time <= newEnd:\n                        newRow = row[:]\n                        if newStart > 0:\n                            newRow[0] = \"{0:.3f}\".format(time - newStart)\n                        writer.writerow(newRow)\n                        dataCount += 1\n                else:\n                    logger.warning('Ignoring empty row ' + str(rowNum) + ' in ' + str(dataFile))\n                rowNum += 1\n        return dataDeviceName, dataCount", "response": "This function copies the data file to a new file in the tmp dir and adjusting \n        the times as appropriate so it starts from newStart and newEnd are adjusted so it starts from 0."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkFiles(filelist,ivmlist = None):\n    toclose = False\n    if isinstance(filelist[0], str):\n        toclose = True\n    newfilelist, ivmlist = checkFITSFormat(filelist, ivmlist)\n\n    # check for STIS association files. This must be done before\n    # the other checks in order to handle correctly stis\n    # assoc files\n    newfilelist, ivmlist = checkStisFiles(newfilelist, ivmlist)\n    if newfilelist == []:\n        return [], []\n    removed_expt_files = check_exptime(newfilelist)\n\n    newfilelist, ivmlist = update_input(newfilelist, ivmlist, removed_expt_files)\n    if newfilelist == []:\n        return [], []\n    removed_ngood_files = checkNGOODPIX(newfilelist)\n    newfilelist, ivmlist = update_input(newfilelist, ivmlist, removed_ngood_files)\n    if newfilelist == []:\n        return [], []\n\n    removed_pav3_files = checkPA_V3(newfilelist)\n    newfilelist, ivmlist = update_input(newfilelist, ivmlist, removed_pav3_files)\n\n    newfilelist, ivmlist = update_input(newfilelist, ivmlist,[])\n\n    if newfilelist == []:\n        return [], []\n\n    if toclose:\n        newfilelist = [hdul.filename() for hdul in newfilelist]\n    return newfilelist, ivmlist", "response": "Checks for waiver fits science and data quality files and splits them into single image sets and returns a list of new files and the list of ivm files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the EXPTIME is set and remove all files with EXPTIME == 0 from filelist.", "response": "def check_exptime(filelist):\n    \"\"\"\n    Removes files with EXPTIME==0 from filelist.\n    \"\"\"\n    toclose = False\n    removed_files = []\n    for f in filelist:\n        if isinstance(f, str):\n            f = fits.open(f)\n            toclose = True\n\n        try:\n            exptime = f[0].header['EXPTIME']\n        except KeyError:\n            removed_files.append(f)\n            print(\"Warning:  There are files without keyword EXPTIME\")\n            continue\n        if exptime <= 0:\n            removed_files.append(f)\n            print(\"Warning:  There are files with zero exposure time: keyword EXPTIME = 0.0\")\n\n    if removed_files != []:\n        print(\"Warning:  Removing the following files from input list\")\n        for f in removed_files:\n            print('\\t',f.filename() or \"\")\n    return removed_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking NGOODPIX for all files in filelist.", "response": "def checkNGOODPIX(filelist):\n    \"\"\"\n    Only for ACS, WFC3 and STIS, check NGOODPIX\n    If all pixels are 'bad' on all chips, exclude this image\n    from further processing.\n    Similar checks requiring comparing 'driz_sep_bits' against\n    WFPC2 c1f.fits arrays and NICMOS DQ arrays will need to be\n    done separately (and later).\n    \"\"\"\n    toclose = False\n    removed_files = []\n    supported_instruments = ['ACS','STIS','WFC3']\n    for inputfile in filelist:\n        if isinstance(inputfile, str):\n            if fileutil.getKeyword(inputfile,'instrume') in supported_instruments:\n                inputfile = fits.open(inputfile)\n                toclose = True\n        elif inputfile[0].header['instrume'] not in supported_instruments:\n            continue\n\n        ngood = 0\n        for extn in inputfile:\n            if 'EXTNAME' in extn.header and extn.header['EXTNAME'] == 'SCI':\n                ngood += extn.header['NGOODPIX']\n        if (ngood == 0):\n            removed_files.append(inputfile)\n        if toclose:\n            inputfile.close()\n\n    if removed_files != []:\n        print(\"Warning:  Files without valid pixels detected: keyword NGOODPIX = 0.0\")\n        print(\"Warning:  Removing the following files from input list\")\n        for f in removed_files:\n            print('\\t',f.filename() or \"\")\n\n    return removed_files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stisObsCount(input):\n    count = 0\n    toclose = False\n    if isinstance(input, str):\n        input = fits.open(input)\n        toclose = True\n    for ext in input:\n        if 'extname' in ext.header:\n            if (ext.header['extname'].upper() == 'SCI'):\n                count += 1\n    if toclose:\n        input.close()\n    return count", "response": "This function returns the number of stis science extensions in a multiextension file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits a STIS association file into multiple imset MEF files.", "response": "def splitStis(stisfile, sci_count):\n    \"\"\"\n    Split a STIS association file into multiple imset MEF files.\n\n    Split the corresponding spt file if present into single spt files.\n    If an spt file can't be split or is missing a Warning is printed.\n\n    Returns\n    -------\n    names: list\n        a list with the names of the new flt files.\n\n    \"\"\"\n    newfiles = []\n\n    toclose = False\n    if isinstance(stisfile, str):\n        f = fits.open(stisfile)\n        toclose = True\n    else:\n        f = stisfile\n    hdu0 = f[0].copy()\n    stisfilename = stisfile.filename()\n\n    for count in range(1,sci_count+1):\n        fitsobj = fits.HDUList()\n        fitsobj.append(hdu0)\n        hdu = f[('sci',count)].copy()\n        fitsobj.append(hdu)\n        rootname = hdu.header['EXPNAME']\n        newfilename = fileutil.buildNewRootname(rootname, extn='_flt.fits')\n        try:\n            # Verify error array exists\n            if f[('err', count)].data is None:\n                raise ValueError\n            # Verify dq array exists\n            if f[('dq', count)].data is None:\n                raise ValueError\n            # Copy the err extension\n            hdu = f[('err',count)].copy()\n            fitsobj.append(hdu)\n            # Copy the dq extension\n            hdu = f[('dq',count)].copy()\n            fitsobj.append(hdu)\n            fitsobj[1].header['EXTVER'] = 1\n            fitsobj[2].header['EXTVER'] = 1\n            fitsobj[3].header['EXTVER'] = 1\n\n        except ValueError:\n            print('\\nWarning:')\n            print('Extension version %d of the input file %s does not' %(count, stisfile))\n            print('contain all required image extensions. Each must contain')\n            print('populates SCI, ERR and DQ arrays.')\n\n            continue\n\n\n        # Determine if the file you wish to create already exists on the disk.\n        # If the file does exist, replace it.\n        if (os.path.exists(newfilename)):\n            os.remove(newfilename)\n            print(\"       Replacing \"+newfilename+\"...\")\n\n            # Write out the new file\n        fitsobj.writeto(newfilename)\n        # Insure returned HDUList is associated with a file\n        fitsobj.close()\n        fitsobj = fits.open(newfilename, mode='update')\n        newfiles.append(fitsobj) # Return HDUList, not filename\n\n    f.close()\n\n    sptfilename = fileutil.buildNewRootname(stisfilename, extn='_spt.fits')\n    try:\n        sptfile = fits.open(sptfilename)\n    except IOError:\n        print('SPT file not found %s \\n' % sptfilename)\n        return newfiles\n\n    if sptfile:\n        hdu0 = sptfile[0].copy()\n        try:\n            for count in range(1,sci_count+1):\n                fitsobj = fits.HDUList()\n                fitsobj.append(hdu0)\n                hdu = sptfile[count].copy()\n                fitsobj.append(hdu)\n                rootname = hdu.header['EXPNAME']\n                newfilename = fileutil.buildNewRootname(rootname, extn='_spt.fits')\n                fitsobj[1].header['EXTVER'] = 1\n                if (os.path.exists(newfilename)):\n                    os.remove(newfilename)\n                    print(\"       Replacing \"+newfilename+\"...\")\n\n                # Write out the new file\n                fitsobj.writeto(newfilename)\n        except:\n            print(\"Warning: Unable to split spt file %s \" % sptfilename)\n        if toclose:\n            sptfile.close()\n\n    return newfiles"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stisExt2PrimKw(stisfiles):\n\n    kw_list = ['DATE-OBS', 'EXPEND', 'EXPSTART', 'EXPTIME']\n\n    for sfile in stisfiles:\n        toclose = False\n\n        if isinstance(sfile, str):\n            sfile = fits.open(sfile, mode='update')\n            toclose = True\n\n        #d = {}\n        for k in kw_list:\n            sfile[0].header[k] = sfile[1].header[k]\n            sfile[0].header.comments[k] = \"Copied from extension header\"\n        if toclose:\n            sfile.close()", "response": "Copy the kw which are usually in the primary header for STIS and the extension header for convenience."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert2fits(sci_ivm):\n    removed_files = []\n    translated_names = []\n    newivmlist = []\n\n    for file in sci_ivm:\n        #find out what the input is\n        # if science file is not found on disk, add it to removed_files for removal\n        try:\n            imgfits,imgtype = fileutil.isFits(file[0])\n        except IOError:\n            print(\"Warning:  File %s could not be found\" %file[0])\n            print(\"Warning:  Removing file %s from input list\" %file[0])\n            removed_files.append(file[0])\n            continue\n\n        # Check for existence of waiver FITS input, and quit if found.\n        # Or should we print a warning and continue but not use that file\n        if imgfits and imgtype == 'waiver':\n            newfilename = waiver2mef(file[0], convert_dq=True)\n            if newfilename is None:\n                print(\"Removing file %s from input list - could not convert WAIVER format to MEF\\n\" %file[0])\n                removed_files.append(file[0])\n            else:\n                removed_files.append(file[0])\n                translated_names.append(newfilename)\n                newivmlist.append(file[1])\n\n        # If a GEIS image is provided as input, create a new MEF file with\n        # a name generated using 'buildFITSName()'\n        # Convert the corresponding data quality file if present\n        if not imgfits:\n            newfilename = geis2mef(file[0], convert_dq=True)\n            if newfilename is None:\n                print(\"Removing file %s from input list - could not convert GEIS format to MEF\\n\" %file[0])\n                removed_files.append(file[0])\n            else:\n                removed_files.append(file[0])\n                translated_names.append(newfilename)\n                newivmlist.append(file[1])\n\n    return removed_files, translated_names, newivmlist", "response": "Checks if a file is in WAIVER of GEIS format and converts it to MEF"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a WAIVERED GEIS science file and its corresponding data quality file to MEF format.", "response": "def waiver2mef(sciname, newname=None, convert_dq=True, writefits=True):\n    \"\"\"\n    Converts a GEIS science file and its corresponding\n    data quality file (if present) to MEF format\n    Writes out both files to disk.\n    Returns the new name of the science image.\n    \"\"\"\n\n    if isinstance(sciname, fits.HDUList):\n        filename = sciname.filename()\n    else:\n        filename = sciname\n\n    try:\n        clobber = True\n        fimg = convertwaiveredfits.convertwaiveredfits(filename)\n\n        #check for the existence of a data quality file\n        _dqname = fileutil.buildNewRootname(filename, extn='_c1f.fits')\n        dqexists = os.path.exists(_dqname)\n        if convert_dq and dqexists:\n            try:\n                dqfile = convertwaiveredfits.convertwaiveredfits(_dqname)\n                dqfitsname = fileutil.buildNewRootname(_dqname, extn='_c1h.fits')\n            except Exception:\n                print(\"Could not read data quality file %s\" % _dqname)\n        if writefits:\n            # User wants to make a FITS copy and update it\n            # using the filename they have provided\n            rname = fileutil.buildNewRootname(filename)\n            fitsname = fileutil.buildNewRootname(rname, extn='_c0h.fits')\n\n            # Write out GEIS image as multi-extension FITS.\n            fexists = os.path.exists(fitsname)\n            if (fexists and clobber) or not fexists:\n                print('Writing out WAIVERED as MEF to ', fitsname)\n                if ASTROPY_VER_GE13:\n                    fimg.writeto(fitsname, overwrite=clobber)\n                else:\n                    fimg.writeto(fitsname, clobber=clobber)\n                if dqexists:\n                    print('Writing out WAIVERED as MEF to ', dqfitsname)\n                    if ASTROPY_VER_GE13:\n                        dqfile.writeto(dqfitsname, overwrite=clobber)\n                    else:\n                        dqfile.writeto(dqfitsname, clobber=clobber)\n        # Now close input GEIS image, and open writable\n        # handle to output FITS image instead...\n        fimg.close()\n        del fimg\n\n        fimg = fits.open(fitsname, mode='update', memmap=False)\n\n        return fimg\n    except IOError:\n        print('Warning: File %s could not be found' % sciname)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a GEIS science file and its corresponding data quality file to MEF format.", "response": "def geis2mef(sciname, convert_dq=True):\n    \"\"\"\n    Converts a GEIS science file and its corresponding\n    data quality file (if present) to MEF format\n    Writes out both files to disk.\n    Returns the new name of the science image.\n    \"\"\"\n    clobber = True\n    mode = 'update'\n    memmap = True\n    # Input was specified as a GEIS image, but no FITS copy\n    # exists.  Read it in with 'readgeis' and make a copy\n    # then open the FITS copy...\n    try:\n        # Open as a GEIS image for reading only\n        fimg = readgeis.readgeis(sciname)\n    except Exception:\n        raise IOError(\"Could not open GEIS input: %s\" % sciname)\n\n    #check for the existence of a data quality file\n    _dqname = fileutil.buildNewRootname(sciname, extn='.c1h')\n    dqexists = os.path.exists(_dqname)\n    if dqexists:\n        try:\n            dqfile = readgeis.readgeis(_dqname)\n            dqfitsname = fileutil.buildFITSName(_dqname)\n        except Exception:\n            print(\"Could not read data quality file %s\" % _dqname)\n\n    # Check to see if user wanted to update GEIS header.\n    # or write out a multi-extension FITS file and return a handle to it\n    # User wants to make a FITS copy and update it\n    # using the filename they have provided\n    fitsname = fileutil.buildFITSName(sciname)\n\n    # Write out GEIS image as multi-extension FITS.\n    fexists = os.path.exists(fitsname)\n    if (fexists and clobber) or not fexists:\n            print('Writing out GEIS as MEF to ', fitsname)\n            if ASTROPY_VER_GE13:\n                fimg.writeto(fitsname, overwrite=clobber)\n            else:\n                fimg.writeto(fitsname, clobber=clobber)\n            if dqexists:\n                print('Writing out GEIS as MEF to ', dqfitsname)\n                if ASTROPY_VER_GE13:\n                    dqfile.writeto(dqfitsname, overwrite=clobber)\n                else:\n                    dqfile.writeto(dqfitsname, clobber=clobber)\n    # Now close input GEIS image, and open writable\n    # handle to output FITS image instead...\n    fimg.close()\n    del fimg\n    fimg = fits.open(fitsname, mode=mode, memmap=memmap)\n\n    return fimg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving journals attribute for this very Issue", "response": "def journals(self):\n        \"\"\"\n        Retrieve journals attribute for this very Issue\n        \"\"\"\n        try:\n            target = self._item_path\n            json_data = self._redmine.get(target % str(self.id),\n                                          parms={'include': 'journals'})\n            data = self._redmine.unwrap_json(None, json_data)\n            journals = [Journal(redmine=self._redmine,\n                                data=journal,\n                                type='issue_journal')\n                        for journal in data['issue']['journals']]\n\n            return journals\n\n        except Exception:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves all changes back to Redmine with optional notes.", "response": "def save(self, notes=None):\n        '''Save all changes back to Redmine with optional notes.'''\n        # Capture the notes if given\n        if notes:\n            self._changes['notes'] = notes\n\n        # Call the base-class save function\n        super(Issue, self).save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave all changes and set to the given new_status", "response": "def set_status(self, new_status, notes=None):\n        '''Save all changes and set to the given new_status'''\n        self.status_id = new_status\n        try:\n            self.status['id'] = self.status_id\n            # We don't have the id to name mapping, so blank the name\n            self.status['name'] = None\n        except:\n            pass\n        self.save(notes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve(self, notes=None):\n        '''Save all changes and resolve this issue'''\n        self.set_status(self._redmine.ISSUE_STATUS_ID_RESOLVED, notes=notes)", "response": "Save all changes and resolve this issue"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self, notes=None):\n        '''Save all changes and close this issue'''\n        self.set_status(self._redmine.ISSUE_STATUS_ID_CLOSED, notes=notes)", "response": "Save all changes and close this issue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _objectify(self, json_data=None, data={}):\n        '''Return an object derived from the given json data.'''\n        if json_data:\n            # Parse the data\n            try:\n                data = json.loads(json_data)\n            except ValueError:\n                # If parsing failed, then raise the string which likely\n                # contains an error message instead of data\n                raise RedmineError(json_data)\n        # Check to see if there is a data wrapper\n        # Some replies will have {'issue':{<data>}} instead of just {<data>}\n        try:\n            data = data[self._item_type]\n        except KeyError:\n            pass\n\n        # If there's no ID but a source path\n        if ('id' not in data) and ('_source_path' in data):\n            # use the path between /projects/ and .json as the ID\n            data['id'] = data['_source_path']\\\n                .partition('/projects/')[2]\\\n                .partition('.json')[0]\n\n        # Call the base class objectify method\n        return super(Redmine_Wiki_Pages_Manager, self)._objectify(data=data)", "response": "Return an object derived from the given json data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new item at the given page_name with the provided dict information at the given page_name. Returns the new item.", "response": "def new(self, page_name, **dict):\n        '''\n        Create a new item with the provided dict information\n        at the given page_name.  Returns the new item.\n\n        As of version 2.2 of Redmine, this doesn't seem to function.\n        '''\n        self._item_new_path = '/projects/%s/wiki/%s.json' % \\\n            (self._project.identifier, page_name)\n        # Call the base class new method\n        return super(Redmine_Wiki_Pages_Manager, self).new(**dict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_version(self, version):\n        '''\n        Set up this object based on the capabilities of the\n        known versions of Redmine\n        '''\n        # Store the version we are evaluating\n        self.version = version or None\n        # To evaluate the version capabilities,\n        # assume the best-case if no version is provided.\n        version_check = version or 9999.0\n\n        if version_check < 1.0:\n            raise RedmineError('This library will only work with '\n                               'Redmine version 1.0 and higher.')\n\n        ## SECURITY AUGMENTATION\n        # All versions support the key in the request\n        #  (http://server/stuff.json?key=blah)\n        # But versions 1.1 and higher can put the key in a header field\n        # for better security.\n        # If no version was provided (0.0) then assume we should\n        # set the key with the request.\n        self.key_in_header = version >= 1.1\n        # it puts the key in the header or\n        # it gets the hose, but not for 1.0.\n\n        self.impersonation_supported = version_check >= 2.2\n        self.has_project_memberships = version_check >= 1.4\n        self.has_project_versions = version_check >= 1.3\n        self.has_wiki_pages = version_check >= 2.2\n\n        ## ITEM MANAGERS\n        # Step through all the item managers by version\n        # and instatiate and item manager for that item.\n        for manager_version in self._item_managers_by_version:\n            if version_check >= manager_version:\n                managers = self._item_managers_by_version[manager_version]\n                for attribute_name, item in managers.iteritems():\n                    setattr(self, attribute_name,\n                            Redmine_Items_Manager(self, item))", "response": "Set up this object based on the capabilities of the Redmine object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loadTargetState(targetStateConfig, existingTargetState=None):\n    from analyser.common.targetstatecontroller import TargetState\n    targetState = TargetState() if existingTargetState is None else existingTargetState\n    # FIXFIX validate\n    if targetStateConfig is not None:\n        val = targetStateConfig.get('fs')\n        if val is not None:\n            targetState.fs = val\n        val = targetStateConfig.get('samplesPerBatch')\n        if val is not None:\n            targetState.samplesPerBatch = val\n        val = targetStateConfig.get('gyroEnabled')\n        if val is not None:\n            targetState.gyroEnabled = val\n        val = targetStateConfig.get('gyroSens')\n        if val is not None:\n            targetState.gyroSens = val\n        val = targetStateConfig.get('accelerometerEnabled')\n        if val is not None:\n            targetState.accelerometerEnabled = val\n        val = targetStateConfig.get('accelerometerSens')\n        if val is not None:\n            targetState.accelerometerSens = val\n    return targetState", "response": "Loads a new TargetState object from the specified configuration dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpret the input bit flags and returns a single integer bitmask or None.", "response": "def interpret_bit_flags(bit_flags, flip_bits=None):\n    \"\"\"\n    Converts input bit flags to a single integer value (bitmask) or `None`.\n\n    When input is a list of flags (either a Python list of integer flags or a\n    sting of comma- or '+'-separated list of flags), the returned bitmask\n    is obtained by summing input flags.\n\n    .. note::\n        In order to flip the bits of the returned bitmask,\n        for input of `str` type, prepend '~' to the input string. '~' must\n        be prepended to the *entire string* and not to each bit flag! For\n        input that is already a bitmask or a Python list of bit flags, set\n        `flip_bits` for `True` in order to flip the bits of the returned\n        bitmask.\n\n    Parameters\n    ----------\n    bit_flags : int, str, list, None\n        An integer bitmask or flag, `None`, a string of comma- or\n        '+'-separated list of integer bit flags, or a Python list of integer\n        bit flags. If `bit_flags` is a `str` and if it is prepended with '~',\n        then the output bitmask will have its bits flipped (compared to simple\n        sum of input flags). For input `bit_flags` that is already a bitmask\n        or a Python list of bit flags, bit-flipping can be controlled through\n        `flip_bits` parameter.\n\n    flip_bits : bool, None\n        Indicates whether or not to flip the bits of the returned bitmask\n        obtained from input bit flags. This parameter must be set to `None`\n        when input `bit_flags` is either `None` or a Python list of flags.\n\n    Returns\n    -------\n    bitmask : int or None\n        Returns and integer bit mask formed from the input bit value\n        or `None` if input `bit_flags` parameter is `None` or an empty string.\n        If input string value was prepended with '~' (or `flip_bits` was\n        set to `True`), then returned value will have its bits flipped\n        (inverse mask).\n\n    Examples\n    --------\n        >>> from stsci.tools.bitmask import interpret_bit_flags\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags(28))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('4,8,16'))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('~4,8,16'))\n        '1111111111100011'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags('~(4+8+16)'))\n        '1111111111100011'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags([4, 8, 16]))\n        '0000000000011100'\n        >>> \"{0:016b}\".format(0xFFFF & interpret_bit_flags([4, 8, 16], flip_bits=True))\n        '1111111111100011'\n\n    \"\"\"\n    has_flip_bits = flip_bits is not None\n    flip_bits = bool(flip_bits)\n    allow_non_flags = False\n\n    if _is_int(bit_flags):\n        return (~int(bit_flags) if flip_bits else int(bit_flags))\n\n    elif bit_flags is None:\n        if has_flip_bits:\n            raise TypeError(\n                \"Keyword argument 'flip_bits' must be set to 'None' when \"\n                \"input 'bit_flags' is None.\"\n            )\n        return None\n\n    elif isinstance(bit_flags, six.string_types):\n        if has_flip_bits:\n            raise TypeError(\n                \"Keyword argument 'flip_bits' is not permitted for \"\n                \"comma-separated string lists of bit flags. Prepend '~' to \"\n                \"the string to indicate bit-flipping.\"\n            )\n\n        bit_flags = str(bit_flags).strip()\n\n        if bit_flags.upper() in ['', 'NONE', 'INDEF']:\n            return None\n\n        # check whether bitwise-NOT is present and if it is, check that it is\n        # in the first position:\n        bitflip_pos = bit_flags.find('~')\n        if bitflip_pos == 0:\n            flip_bits = True\n            bit_flags = bit_flags[1:].lstrip()\n        else:\n            if bitflip_pos > 0:\n                raise ValueError(\"Bitwise-NOT must precede bit flag list.\")\n            flip_bits = False\n\n        # basic check for correct use of parenthesis:\n        while True:\n            nlpar = bit_flags.count('(')\n            nrpar = bit_flags.count(')')\n\n            if nlpar == 0 and nrpar == 0:\n                break\n\n            if nlpar != nrpar:\n                raise ValueError(\"Unbalanced parantheses in bit flag list.\")\n\n            lpar_pos = bit_flags.find('(')\n            rpar_pos = bit_flags.rfind(')')\n            if lpar_pos > 0 or rpar_pos < (len(bit_flags) - 1):\n                raise ValueError(\"Incorrect syntax (incorrect use of \"\n                                 \"parenthesis) in bit flag list.\")\n\n            bit_flags = bit_flags[1:-1].strip()\n\n        if ',' in bit_flags:\n            bit_flags = bit_flags.split(',')\n\n        elif '+' in bit_flags:\n            bit_flags = bit_flags.split('+')\n\n        else:\n            if bit_flags == '':\n                raise ValueError(\n                    \"Empty bit flag lists not allowed when either bitwise-NOT \"\n                    \"or parenthesis are present.\"\n                )\n            bit_flags = [bit_flags]\n\n        allow_non_flags = len(bit_flags) == 1\n\n    elif hasattr(bit_flags, '__iter__'):\n        if not all([_is_int(flag) for flag in bit_flags]):\n            raise TypeError(\"Each bit flag in a list must be an integer.\")\n\n    else:\n        raise TypeError(\"Unsupported type for argument 'bit_flags'.\")\n\n    bitset = set(map(int, bit_flags))\n    if len(bitset) != len(bit_flags):\n        warnings.warn(\"Duplicate bit flags will be ignored\")\n\n    bitmask = 0\n    for v in bitset:\n        if not is_bit_flag(v) and not allow_non_flags:\n            raise ValueError(\"Input list contains invalid (not powers of two) \"\n                             \"bit flags\")\n        bitmask += v\n\n    if flip_bits:\n        bitmask = ~bitmask\n\n    return bitmask"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an array of bit fields to a boolean mask.", "response": "def bitfield_to_boolean_mask(bitfield, ignore_flags=0, flip_bits=None,\n                             good_mask_value=True, dtype=np.bool_):\n    \"\"\"\n    bitfield_to_boolean_mask(bitfield, ignore_flags=None, flip_bits=None, \\\ngood_mask_value=True, dtype=numpy.bool\\_)\n    Converts an array of bit fields to a boolean (or integer) mask array\n    according to a bitmask constructed from the supplied bit flags (see\n    ``ignore_flags`` parameter).\n\n    This function is particularly useful to convert data quality arrays to\n    boolean masks with selective filtering of DQ flags.\n\n    Parameters\n    ----------\n    bitfield : numpy.ndarray\n        An array of bit flags. By default, values different from zero are\n        interpreted as \"bad\" values and values equal to zero are considered\n        as \"good\" values. However, see ``ignore_flags`` parameter on how to\n        selectively ignore some bits in the ``bitfield`` array data.\n\n    ignore_flags : int, str, list, None (Default = 0)\n        An integer bitmask, a Python list of bit flags, a comma- or\n        '+'-separated string list of integer bit flags that indicate what\n        bits in the input ``bitfield`` should be *ignored* (i.e., zeroed), or\n        `None`.\n\n        | Setting ``ignore_flags`` to `None` effectively will make\n          `bitfield_to_boolean_mask` interpret all ``bitfield`` elements\n          as \"good\" regardless of their value.\n\n        | When ``ignore_flags`` argument is an integer bitmask, it will be\n          combined using bitwise-NOT and bitwise-AND with each element of the\n          input ``bitfield`` array (``~ignore_flags & bitfield``). If the\n          resultant bitfield element is non-zero, that element will be\n          interpreted as a \"bad\" in the output boolean mask and it will be\n          interpreted as \"good\" otherwise. ``flip_bits`` parameter may be used\n          to flip the bits (``bitwise-NOT``) of the bitmask thus effectively\n          changing the meaning of the ``ignore_flags`` parameter from \"ignore\"\n          to \"use only\" these flags.\n\n        .. note::\n\n            Setting ``ignore_flags`` to 0 effectively will assume that all\n            non-zero elements in the input ``bitfield`` array are to be\n            interpreted as \"bad\".\n\n        | When ``ignore_flags`` argument is an Python list of integer bit\n          flags, these flags are added together to create an integer bitmask.\n          Each item in the list must be a flag, i.e., an integer that is an\n          integer power of 2. In order to flip the bits of the resultant\n          bitmask, use ``flip_bits`` parameter.\n\n        | Alternatively, ``ignore_flags`` may be a string of comma- or\n          '+'-separated list of integer bit flags that should be added together\n          to create an integer bitmask. For example, both ``'4,8'`` and\n          ``'4+8'`` are equivalent and indicate that bit flags 4 and 8 in\n          the input ``bitfield`` array should be ignored when generating\n          boolean mask.\n\n        .. note::\n\n            ``'None'``, ``'INDEF'``, and empty (or all white space) strings\n            are special values of string ``ignore_flags`` that are\n            interpreted as `None`.\n\n        .. note::\n\n            Each item in the list must be a flag, i.e., an integer that is an\n            integer power of 2. In addition, for convenience, an arbitrary\n            **single** integer is allowed and it will be interpretted as an\n            integer bitmask. For example, instead of ``'4,8'`` one could\n            simply provide string ``'12'``.\n\n        .. note::\n\n            When ``ignore_flags`` is a `str` and when it is prepended with\n            '~', then the meaning of ``ignore_flags`` parameters will be\n            reversed: now it will be interpreted as a list of bit flags to be\n            *used* (or *not ignored*) when deciding which elements of the\n            input ``bitfield`` array are \"bad\". Following this convention,\n            an ``ignore_flags`` string value of ``'~0'`` would be equivalent\n            to setting ``ignore_flags=None``.\n\n        .. warning::\n\n            Because prepending '~' to a string ``ignore_flags`` is equivalent\n            to setting ``flip_bits`` to `True`, ``flip_bits`` cannot be used\n            with string ``ignore_flags`` and it must be set to `None`.\n\n    flip_bits : bool, None (Default = None)\n        Specifies whether or not to invert the bits of the bitmask either\n        supplied directly through ``ignore_flags`` parameter or built from the\n        bit flags passed through ``ignore_flags`` (only when bit flags are\n        passed as Python lists of integer bit flags). Occasionally, it may be\n        useful to *consider only specific bit flags* in the ``bitfield``\n        array when creating a boolean mask as opposite to *ignoring* specific\n        bit flags as ``ignore_flags`` behaves by default. This can be achieved\n        by inverting/flipping the bits of the bitmask created from\n        ``ignore_flags`` flags which effectively changes the meaning of the\n        ``ignore_flags`` parameter from \"ignore\" to \"use only\" these flags.\n        Setting ``flip_bits`` to `None` means that no bit flipping will be\n        performed. Bit flipping for string lists of bit flags must be\n        specified by prepending '~' to string bit flag lists\n        (see documentation for ``ignore_flags`` for more details).\n\n        .. warning::\n            This parameter can be set to either `True` or `False` **ONLY** when\n            ``ignore_flags`` is either an integer bitmask or a Python\n            list of integer bit flags. When ``ignore_flags`` is either\n            `None` or a string list of flags, ``flip_bits`` **MUST** be set\n            to `None`.\n\n    good_mask_value : int, bool (Default = True)\n        This parameter is used to derive the values that will be assigned to\n        the elements in the output boolean mask array that correspond to the\n        \"good\" bit fields (that are 0 after zeroing bits specified by\n        ``ignore_flags``) in the input ``bitfield`` array. When\n        ``good_mask_value`` is non-zero or `True` then values in the output\n        boolean mask array corresponding to \"good\" bit fields in ``bitfield``\n        will be `True` (if ``dtype`` is `numpy.bool_`) or 1 (if ``dtype`` is\n        of numerical type) and values of corresponding to \"bad\" flags will be\n        `False` (or 0). When ``good_mask_value`` is zero or `False` then the\n        values in the output boolean mask array corresponding to \"good\" bit\n        fields in ``bitfield`` will be `False` (if ``dtype`` is `numpy.bool_`)\n        or 0 (if ``dtype`` is of numerical type) and values of corresponding\n        to \"bad\" flags will be `True` (or 1).\n\n    dtype : data-type (Default = numpy.bool\\_)\n        The desired data-type for the output binary mask array.\n\n    Returns\n    -------\n    mask : numpy.ndarray\n        Returns an array of the same dimensionality as the input ``bitfield``\n        array whose elements can have two possible values,\n        e.g., `True` or `False` (or 1 or 0 for integer ``dtype``) according to\n        values of to the input ``bitfield`` elements, ``ignore_flags``\n        parameter, and the ``good_mask_value`` parameter.\n\n    Examples\n    --------\n        >>> from stsci.tools import bitmask\n        >>> import numpy as np\n        >>> dqbits = np.asarray([[0,0,1,2,0,8,12,0],[10,4,0,0,0,16,6,0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=0, dtype=int)\n        array([[1, 1, 0, 0, 1, 0, 0, 1],\n               [0, 0, 1, 1, 1, 0, 0, 1]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=0, dtype=bool)\n        array([[ True,  True, False, False,  True, False, False,  True],\n               [False, False,  True,  True,  True, False, False,  True]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=6, good_mask_value=0, dtype=int)\n        array([[0, 0, 1, 0, 0, 1, 1, 0],\n               [1, 0, 0, 0, 0, 1, 0, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=~6, good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=6, flip_bits=True, good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags='~(2+4)', good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n        >>> bitmask.bitfield_to_boolean_mask(dqbits, ignore_flags=[2, 4], flip_bits=True, good_mask_value=0, dtype=int)\n        array([[0, 0, 0, 1, 0, 0, 1, 0],\n               [1, 1, 0, 0, 0, 0, 1, 0]])\n\n    \"\"\"\n    bitfield = np.asarray(bitfield)\n    if not np.issubdtype(bitfield.dtype, np.integer):\n        raise TypeError(\"Input bitfield array must be of integer type.\")\n\n    ignore_mask = interpret_bit_flags(ignore_flags, flip_bits=flip_bits)\n\n    if ignore_mask is None:\n        if good_mask_value:\n            mask = np.ones_like(bitfield, dtype=dtype)\n        else:\n            mask = np.zeros_like(bitfield, dtype=dtype)\n        return mask\n\n    # filter out bits beyond the maximum supported by the data type:\n    ignore_mask = ignore_mask & SUPPORTED_FLAGS\n\n    # invert the \"ignore\" mask:\n    ignore_mask = np.bitwise_not(ignore_mask, dtype=bitfield.dtype,\n                                 casting='unsafe')\n\n    mask = np.empty_like(bitfield, dtype=np.bool_)\n    np.bitwise_and(bitfield, ignore_mask, out=mask, casting='unsafe')\n\n    if good_mask_value:\n        np.logical_not(mask, out=mask)\n\n    return mask.astype(dtype=dtype, subok=False, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts input bits value from string to integer bit mask or None.", "response": "def interpret_bits_value(val):\n    \"\"\"\n    Converts input bits value from string to a single integer value or None.\n    If a comma- or '+'-separated set of values are provided, they are summed.\n\n    .. note::\n        In order to flip the bits of the final result (after summation),\n        for input of `str` type, prepend '~' to the input string. '~' must\n        be prepended to the *entire string* and not to each bit flag!\n\n    Parameters\n    ----------\n    val : int, str, None\n        An integer bit mask or flag, `None`, or a comma- or '+'-separated\n        string list of integer bit values. If `val` is a `str` and if\n        it is prepended with '~', then the output bit mask will have its\n        bits flipped (compared to simple sum of input val).\n\n    Returns\n    -------\n    bitmask : int or None\n        Returns and integer bit mask formed from the input bit value\n        or `None` if input `val` parameter is `None` or an empty string.\n        If input string value was prepended with '~', then returned\n        value will have its bits flipped (inverse mask).\n\n    \"\"\"\n    if isinstance(val, int) or val is None:\n        return val\n\n    else:\n        val = str(val).strip()\n\n        if val.startswith('~'):\n            flip_bits = True\n            val = val[1:].lstrip()\n        else:\n            flip_bits = False\n\n        if val.startswith('('):\n            if val.endswith(')'):\n                val = val[1:-1].strip()\n            else:\n                raise ValueError('Unbalanced parantheses or incorrect syntax.')\n\n        if ',' in val:\n            valspl = val.split(',')\n            bitmask = 0\n            for v in valspl:\n                bitmask += int(v)\n\n        elif '+' in val:\n            valspl = val.split('+')\n            bitmask = 0\n            for v in valspl:\n                bitmask += int(v)\n\n        elif val.upper() in ['', 'NONE', 'INDEF']:\n            return None\n\n        else:\n            bitmask = int(val)\n\n        if flip_bits:\n            bitmask = ~bitmask\n\n    return bitmask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bitmask2mask(bitmask, ignore_bits, good_mask_value=1, dtype=np.bool_):\n    if not np.issubdtype(bitmask.dtype, np.integer):\n        raise TypeError(\"Input 'bitmask' array must be of integer type.\")\n\n    ignore_bits = interpret_bits_value(ignore_bits)\n\n    if ignore_bits is None:\n        if good_mask_value:\n            mask = np.ones_like(bitmask, dtype=dtype)\n        else:\n            mask = np.zeros_like(bitmask, dtype=dtype)\n        return mask\n\n    ignore_bits = ~bitmask.dtype.type(ignore_bits)\n\n    mask = np.empty_like(bitmask, dtype=np.bool_)\n    np.bitwise_and(bitmask, ignore_bits, out=mask, casting='unsafe')\n\n    if good_mask_value:\n        np.logical_not(mask, out=mask)\n\n    return mask.astype(dtype=dtype, subok=False, copy=False)", "response": "Interprets an array of bit flags and converts it to a binary mask array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output_text(account, all_data, show_hourly=False):\n    print(\"\"\"\n#################################\n# Hydro Quebec data for account #\n# {}\n#################################\"\"\".format(account))\n    for contract, data in all_data.items():\n        data['contract'] = contract\n        if data['period_total_bill'] is None:\n            data['period_total_bill'] = 0.0\n        if data['period_projection'] is None:\n            data['period_projection'] = 0.0\n        if data['period_mean_daily_bill'] is None:\n            data['period_mean_daily_bill'] = 0.0\n        output = (\"\"\"\n----------------------------------------------------------------\n\nContract: {d[contract]}\n===================\n\nBalance: {d[balance]:.2f} $\n\nPeriod Info\n===========\nPeriod day number:      {d[period_length]:d}\nPeriod total days:      {d[period_total_days]:d} days\n\nPeriod current bill\n===================\nTotal Bill:             {d[period_total_bill]:.2f} $\nProjection bill:        {d[period_projection]:.2f} $\nMean Daily Bill:        {d[period_mean_daily_bill]:.2f} $\n\nTotal period consumption\n========================\nLower price:            {d[period_lower_price_consumption]:.2f} kWh\nHigher price:           {d[period_higher_price_consumption]:.2f} kWh\nTotal:                  {d[period_total_consumption]:.2f} kWh\nMean daily:             {d[period_mean_daily_consumption]:.2f} kWh\"\"\")\n        print(output.format(d=data))\n        if data.get(\"period_average_temperature\") is not None:\n            output2 = (\"\"\"Temperature:            {d[period_average_temperature]:d} \u00b0C\"\"\")\n            print(output2.format(d=data))\n        if data.get(\"yesterday_average_temperature\") is not None:\n            output3 = (\"\"\"\nYesterday consumption\n=====================\nTemperature:            {d[yesterday_average_temperature]:d} \u00b0C\nLower price:            {d[yesterday_lower_price_consumption]:.2f} kWh\nHigher price:           {d[yesterday_higher_price_consumption]:.2f} kWh\nTotal:                  {d[yesterday_total_consumption]:.2f} kWh\"\"\")\n            print(output3.format(d=data))\n        if show_hourly:\n            msg = (\"\"\"\nYesterday consumption details\n-----------------------------\n   Hour  | Temperature | Lower price consumption | Higher price consumption | total comsumption\n\"\"\")\n            for hdata in data['yesterday_hourly_consumption']:\n                msg += (\"{d[hour]} | {d[temp]:8d} \u00b0C | {d[lower]:19.2f} kWh | \"\n                        \"{d[high]:20.2f} kWh | {d[total]:.2f} kWh\\n\").format(d=hdata)\n            print(msg)\n\n        if data['annual_total_bill']:\n            output3 = (\"\"\"\n    Annual Total\n    ============\n\n    Start date:             {d[annual_date_start]}\n    End date:               {d[annual_date_end]}\n    Total bill:             {d[annual_total_bill]} $\n    Mean daily bill:        {d[annual_mean_daily_bill]} $\n    Total consumption:      {d[annual_total_consumption]} kWh\n    Mean dailyconsumption:  {d[annual_mean_daily_consumption]} kWh\n    kWh price:              {d[annual_kwh_price_cent]} \u00a2\n    \"\"\")\n            print(output3.format(d=data))", "response": "Format data to get a readable output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef output_influx(data):\n    for contract in data:\n\n        # Pop yesterdays data\n        yesterday_data = data[contract]['yesterday_hourly_consumption']\n        del data[contract]['yesterday_hourly_consumption']\n\n        # Print general data\n        out = \"pyhydroquebec,contract=\" + contract + \" \"\n\n        for index, key in enumerate(data[contract]):\n            if index != 0:\n                out = out + \",\"\n            if key in (\"annual_date_start\", \"annual_date_end\"):\n                out += key + \"=\\\"\" + str(data[contract][key]) + \"\\\"\"\n            else:\n                out += key + \"=\" + str(data[contract][key])\n\n        out += \" \" + str(int(datetime.datetime.now(HQ_TIMEZONE).timestamp() * 1000000000))\n        print(out)\n\n        # Print yesterday values\n        yesterday = datetime.datetime.now(HQ_TIMEZONE) - datetime.timedelta(days=1)\n        yesterday = yesterday.replace(minute=0, hour=0, second=0, microsecond=0)\n\n        for hour in yesterday_data:\n            msg = \"pyhydroquebec,contract={} {} {}\"\n\n            data = \",\".join([\"{}={}\".format(key, value) for key, value in hour.items()\n                             if key != 'hour'])\n\n            datatime = datetime.datetime.strptime(hour['hour'], '%H:%M:%S')\n            yesterday = yesterday.replace(hour=datatime.hour)\n            yesterday_str = str(int(yesterday.timestamp() * 1000000000))\n\n            print(msg.format(contract, data, yesterday_str))", "response": "Print data using influxDB format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn one of x11 Aqua or Aqua depending on the platform.", "response": "def which_darwin_linkage(force_otool_check=False):\n    \"\"\" Convenience function.  Returns one of ('x11', 'aqua') in answer to the\n    question of whether this is an X11-linked Python/tkinter, or a natively\n    built (framework, Aqua) one.  This is only for OSX.\n    This relies on the assumption that on OSX, PyObjC is installed\n    in the Framework builds of Python.  If it doesn't find PyObjC,\n    this inspects the actual tkinter library binary via otool.\n\n    One driving requirement here is to try to make the determination quickly\n    and quietly without actually importing/loading any GUI libraries.  We\n    even want to avoid importing tkinter if we can.\n    \"\"\"\n\n    # sanity check\n    assert sys.platform=='darwin', 'Incorrect usage, not on OSX'\n\n    # If not forced to run otool, then make some quick and dirty\n    # simple checks/assumptions, which do not add to startup time and do not\n    # attempt to initialize any graphics.\n    if not force_otool_check:\n\n        # There will (for now) only ever be an aqua-linked Python/tkinter\n        # when using Ureka on darwin, so this is an easy short-circuit check.\n        if 'UR_DIR' in os.environ:\n            return \"aqua\"\n\n        # There will *usually* be PyObjC modules on sys.path on the natively-\n        # linked Python. This is assumed to be always correct on Python 2.x, as\n        # of 2012.  This is kludgy but quick and effective.\n        sp = \",\".join(sys.path)\n        sp = sp.lower().strip(',')\n        if '/pyobjc' in sp or 'pyobjc,' in sp or 'pyobjc/' in sp or sp.endswith('pyobjc'):\n            return \"aqua\"\n\n        # Try one more thing - look for the physical PyObjC install dir under site-packages\n        # The assumption above using sys.path does not seem to be correct as of the\n        # combination of Python2.7.9/PyObjC3.0.4/2015.\n        sitepacksloc = os.path.split(os.__file__)[0]+'/site-packages/objc'\n        if os.path.exists(sitepacksloc):\n            return \"aqua\"\n\n        # OK, no trace of PyObjC found - need to fall through to the forced otool check.\n\n    # Use otool shell command\n    if PY3K:\n        import tkinter as TKNTR\n    else:\n        import Tkinter as TKNTR\n    import subprocess\n    try:\n        tk_dyn_lib = TKNTR._tkinter.__file__\n    except AttributeError: # happens on Ureka\n        if 'UR_DIR' in os.environ:\n            return 'aqua'\n        else:\n            return 'unknown'\n    libs = subprocess.check_output(('/usr/bin/otool', '-L', tk_dyn_lib)).decode('ascii')\n    if libs.find('/libX11.') >= 0:\n        return \"x11\"\n    else:\n        return \"aqua\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dc_owner(raises, mask_if_self):\n    try:\n        from pwd import getpwuid\n        owner_uid = os.stat('/dev/console').st_uid\n        self_uid  = os.getuid()\n        if mask_if_self and owner_uid == self_uid:\n            return \"<self>\"\n        owner_name = getpwuid(owner_uid).pw_name\n        return owner_name\n    except Exception as e:\n        if raises:\n            raise e\n        else:\n            return str(e)", "response": "Convenience function to return owner of the DC."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining Julian day from Bahai date", "response": "def to_jd(year, month, day):\n    '''Determine Julian day from Bahai date'''\n    gy = year - 1 + EPOCH_GREGORIAN_YEAR\n\n    if month != 20:\n        m = 0\n    else:\n        if isleap(gy + 1):\n            m = -14\n        else:\n            m = -15\n    return gregorian.to_jd(gy, 3, 20) + (19 * (month - 1)) + m + day"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_jd(jd):\n    '''Calculate Bahai date from Julian day'''\n\n    jd = trunc(jd) + 0.5\n    g = gregorian.from_jd(jd)\n    gy = g[0]\n\n    bstarty = EPOCH_GREGORIAN_YEAR\n\n    if jd <= gregorian.to_jd(gy, 3, 20):\n        x = 1\n    else:\n        x = 0\n    # verify this next line...\n    bys = gy - (bstarty + (((gregorian.to_jd(gy, 1, 1) <= jd) and x)))\n\n    year = bys + 1\n    days = jd - to_jd(year, 1, 1)\n    bld = to_jd(year, 20, 1)\n\n    if jd >= bld:\n        month = 20\n    else:\n        month = trunc(days / 19) + 1\n    day = int((jd + 1) - to_jd(year, month, 1))\n\n    return year, month, day", "response": "Calculate Bahai date from Julian day"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines Julian day from Mayan long count", "response": "def to_jd(baktun, katun, tun, uinal, kin):\n    '''Determine Julian day from Mayan long count'''\n    return EPOCH + (baktun * 144000) + (katun * 7200) + (tun * 360) + (uinal * 20) + kin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating Mayan long count from Julian day", "response": "def from_jd(jd):\n    '''Calculate Mayan long count from Julian day'''\n    d = jd - EPOCH\n    baktun = trunc(d / 144000)\n    d = (d % 144000)\n    katun = trunc(d / 7200)\n    d = (d % 7200)\n    tun = trunc(d / 360)\n    d = (d % 360)\n    uinal = trunc(d / 20)\n    kin = int((d % 20))\n\n    return (baktun, katun, tun, uinal, kin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines Mayan Haab month and day from Julian day", "response": "def to_haab(jd):\n    '''Determine Mayan Haab \"month\" and day from Julian day'''\n    # Number of days since the start of the long count\n    lcount = trunc(jd) + 0.5 - EPOCH\n    # Long Count begins 348 days after the start of the cycle\n    day = (lcount + 348) % 365\n\n    count = day % 20\n    month = trunc(day / 20)\n\n    return int(count), HAAB_MONTHS[month]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine Mayan Tzolkin month and day from Julian day", "response": "def to_tzolkin(jd):\n    '''Determine Mayan Tzolkin \"month\" and day from Julian day'''\n    lcount = trunc(jd) + 0.5 - EPOCH\n    day = amod(lcount + 4, 13)\n    name = amod(lcount + 20, 20)\n    return int(day), TZOLKIN_NAMES[int(name) - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _haab_count(day, month):\n    '''Return the count of the given haab in the cycle. e.g. 0 Pop == 1, 5 Wayeb' == 365'''\n    if day < 0 or day > 19:\n        raise IndexError(\"Invalid day number\")\n\n    try:\n        i = HAAB_MONTHS.index(month)\n    except ValueError:\n        raise ValueError(\"'{0}' is not a valid Haab' month\".format(month))\n\n    return min(i * 20, 360) + day", "response": "Return the count of the given haab in the cycle. e. g. 0 Pop == 1 5 Wayeb == 365"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a generator that yields a tzolkin from the given number and name combination.", "response": "def tzolkin_generator(number=None, name=None):\n    '''For a given tzolkin name/number combination, return a generator\n    that gives cycle, starting with the input'''\n\n    # By default, it will start at the beginning\n    number = number or 13\n    name = name or \"Ajaw\"\n\n    if number > 13:\n        raise ValueError(\"Invalid day number\")\n\n    if name not in TZOLKIN_NAMES:\n        raise ValueError(\"Invalid day name\")\n\n    count = _tzolkin_count(number, name)\n\n    ranged = itertools.chain(list(range(count, 260)), list(range(1, count)))\n\n    for i in ranged:\n        yield _tzolkin_from_count(i)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating long counts starting with input", "response": "def longcount_generator(baktun, katun, tun, uinal, kin):\n    '''Generate long counts, starting with input'''\n    j = to_jd(baktun, katun, tun, uinal, kin)\n\n    while True:\n        yield from_jd(j)\n        j = j + 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next_haab(month, jd):\n    '''For a given haab month and a julian day count, find the next start of that month on or after the JDC'''\n    if jd < EPOCH:\n        raise IndexError(\"Input day is before Mayan epoch.\")\n\n    hday, hmonth = to_haab(jd)\n\n    if hmonth == month:\n        days = 1 - hday\n\n    else:\n        count1 = _haab_count(hday, hmonth)\n        count2 = _haab_count(1, month)\n\n        # Find number of days between haab of given jd and desired haab\n        days = (count2 - count1) % 365\n\n    # add in the number of days and return new jd\n    return jd + days", "response": "For a given haab month and a julian day count find the next haab of that month on or after the JDC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next_tzolkin(tzolkin, jd):\n    '''For a given tzolk'in day, and a julian day count, find the next occurrance of that tzolk'in after the date'''\n    if jd < EPOCH:\n        raise IndexError(\"Input day is before Mayan epoch.\")\n\n    count1 = _tzolkin_count(*to_tzolkin(jd))\n    count2 = _tzolkin_count(*tzolkin)\n\n    add_days = (count2 - count1) % 260\n    return jd + add_days", "response": "Given a tzolk in day and a julian day count find the next occurrance of that tzolk in after the date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next_tzolkin_haab(tzolkin, haab, jd):\n    '''For a given haab-tzolk'in combination, and a Julian day count, find the next occurrance of the combination after the date'''\n    # get H & T of input jd, and their place in the 18,980 day cycle\n    haabcount = _haab_count(*to_haab(jd))\n    haab_desired_count = _haab_count(*haab)\n\n    # How many days between the input day and the desired day?\n    haab_days = (haab_desired_count - haabcount) % 365\n\n    possible_haab = set(h + haab_days for h in range(0, 18980, 365))\n\n    tzcount = _tzolkin_count(*to_tzolkin(jd))\n    tz_desired_count = _tzolkin_count(*tzolkin)\n    # How many days between the input day and the desired day?\n    tzolkin_days = (tz_desired_count - tzcount) % 260\n\n    possible_tz = set(t + tzolkin_days for t in range(0, 18980, 260))\n\n    try:\n        return possible_tz.intersection(possible_haab).pop() + jd\n    except KeyError:\n        raise IndexError(\"That Haab'-Tzolk'in combination isn't possible\")", "response": "For a given haab - tzolk in combination and a Julian day count find the next occurrance of the combination after the date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef haab_monthcalendar(baktun=None, katun=None, tun=None, uinal=None, kin=None, jdc=None):\n    '''For a given long count, return a calender of the current haab month, divided into tzolkin \"weeks\"'''\n    if not jdc:\n        jdc = to_jd(baktun, katun, tun, uinal, kin)\n\n    haab_number, haab_month = to_haab(jdc)\n    first_j = jdc - haab_number + 1\n\n    tzolkin_start_number, tzolkin_start_name = to_tzolkin(first_j)\n\n    gen_longcount = longcount_generator(*from_jd(first_j))\n    gen_tzolkin = tzolkin_generator(tzolkin_start_number, tzolkin_start_name)\n\n    # 13 day long tzolkin 'weeks'\n    lpad = tzolkin_start_number - 1\n    rpad = 13 - (tzolkin_start_number + 19 % 13)\n\n    monlen = month_length(haab_month)\n\n    days = [None] * lpad + list(range(1, monlen + 1)) + rpad * [None]\n\n    def g(x, generate):\n        if x is None:\n            return None\n        return next(generate)\n\n    return [[(k, g(k, gen_tzolkin), g(k, gen_longcount)) for k in days[i:i + 13]] for i in range(0, len(days), 13)]", "response": "For a given long count return a calender of the current haab month divided into tzolkin weeks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the data in this object.", "response": "def _update_data(self, data={}):\n        '''Update the data in this object.'''\n\n        # Store the changes to prevent this update from affecting it\n        pending_changes = self._changes or {}\n        try:\n            del self._changes\n        except:\n            pass\n\n        # Map custom fields into our custom fields object\n        try:\n            custom_field_data = data.pop('custom_fields')\n        except KeyError:\n            pass\n        else:\n            self.custom_fields = Custom_Fields(custom_field_data)\n\n        # Map all other dictionary data to object attributes\n        for key, value in data.iteritems():\n            lookup_key = self._field_type.get(key, key)\n\n            # if it's a datetime object, turn into proper DT object\n            if lookup_key == 'datetime' or lookup_key == 'date':\n                self.__dict__[key] = datetime_parse(value)\n            else:\n                # Check to see if there's cache data for this item.\n                # Will return an object if it's recognized as one.\n                self.__dict__[key] = self._redmine.check_cache(lookup_key, value)\n\n\n        # Set the changes dict to track all changes from here on out\n        self._changes = pending_changes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _remap_tag_to_tag_id(cls, tag, new_data):\n        '''Remaps a given changed field from tag to tag_id.'''\n        try:\n            value = new_data[tag]\n        except:\n            # If tag wasn't changed, just return\n            return\n\n        tag_id = tag + '_id'\n        try:\n            # Remap the ID change to the required field\n            new_data[tag_id] = value['id']\n        except:\n            try:\n                # Try and grab the id of the object\n                new_data[tag_id] = value.id\n            except AttributeError:\n                # If the changes field is not a dict or object, just use whatever value was given\n                new_data[tag_id] = value\n\n        # Remove the tag from the changed data\n        del new_data[tag]", "response": "Remaps a given changed field from tag to tag_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an item manager to this object.", "response": "def _add_item_manager(self, key, item_class, **paths):\n        '''\n        Add an item manager to this object.\n        '''\n        updated_paths = {}\n        for path_type, path_value in paths.iteritems():\n            updated_paths[path_type] = path_value.format(**self.__dict__)\n\n        manager = Redmine_Items_Manager(self._redmine, item_class,\n                                        **updated_paths)\n        self.__dict__[key] = manager"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving all changes on this item back to Redmine.", "response": "def save(self):\n        '''Save all changes on this item (if any) back to Redmine.'''\n        self._check_custom_fields()\n\n        if not self._changes:\n            return None\n\n        for tag in self._remap_to_id:\n            self._remap_tag_to_tag_id(tag, self._changes)\n\n        # Check for custom handlers for tags\n        for tag, type in self._field_type.items():\n            try:\n                raw_data = self._changes[tag]\n            except:\n                continue\n\n            # Convert datetime type to a datetime string that Redmine expects\n            if type == 'datetime':\n                try:\n                    self._changes[tag] = raw_data.strftime('%Y-%m-%dT%H:%M:%S%z')\n                except AttributeError:\n                    continue\n\n            # Convert date type to a date string that Redmine expects\n            if type == 'date':\n                try:\n                    self._changes[tag] = raw_data.strftime('%Y-%m-%d')\n                except AttributeError:\n                    continue\n\n\n        try:\n            self._update(self._changes)\n        except:\n            raise\n        else:\n            # Successful save, woot! Now clear the changes dict\n            self._changes.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrefresh this item from data on the server. Will save any unsaved data first.", "response": "def refresh(self):\n        '''Refresh this item from data on the server.\n        Will save any unsaved data first.'''\n\n        if not self._item_path:\n            raise AttributeError('refresh is not available for %s' % self._type)\n        if not self.id:\n            raise RedmineError('%s did not come from the Redmine server - no link.' % self._type)\n\n        try:\n            self.save()\n        except:\n            pass\n\n        # Mimic the Redmine_Item_Manager.get command\n        target = self._item_path % self.id\n        json_data = self._redmine.get(target)\n        data = self._redmine.unwrap_json(self._type, json_data)\n        self._update_data(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all changed values.", "response": "def _get_changes(self):\n        '''Get all changed values.'''\n        result = dict( (f['id'], f.get('value','')) for f in self._data if f.get('changed', False) )\n        self._clear_changes\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iteritems(self, **options):\n        '''Return a query interator with (id, object) pairs.'''\n        iter = self.query(**options)\n        while True:\n            obj = iter.next()\n            yield (obj.id, obj)", "response": "Return a query interator with ( id object pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _objectify(self, json_data=None, data={}):\n        '''Return an object derived from the given json data.'''\n        if json_data:\n            # Parse the data\n            try:\n                data = json.loads(json_data)\n            except ValueError:\n                # If parsing failed, then raise the string which likely contains an error message instead of data\n                raise RedmineError(json_data)\n        # Check to see if there is a data wrapper\n        # Some replies will have {'issue':{<data>}} instead of just {<data>}\n        try:\n            data = data[self._item_type]\n        except KeyError:\n            pass\n\n        # Either returns a new item or updates the item in the cache and returns that\n        return self._redmine.check_cache(self._item_type, data, self._object)", "response": "Return an object derived from the given json data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new item with the provided dict information. Returns the new item.", "response": "def new(self, **dict):\n        '''Create a new item with the provided dict information.  Returns the new item.'''\n        if not self._item_new_path:\n            raise AttributeError('new is not available for %s' % self._item_name)\n\n        # Remap various tag to tag_id\n        for tag in self._object._remap_to_id:\n            self._object._remap_tag_to_tag_id(tag, dict)\n\n        target = self._item_new_path\n        payload = json.dumps({self._item_type:dict})\n        json_data = self._redmine.post(target, payload)\n        data = self._redmine.unwrap_json(self._item_type, json_data)\n        data['_source_path'] = target\n        return self._objectify(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, id, **options):\n        '''Get a single item with the given ID'''\n        if not self._item_path:\n            raise AttributeError('get is not available for %s' % self._item_name)\n        target = self._item_path % id\n        json_data = self._redmine.get(target, **options)\n        data = self._redmine.unwrap_json(self._item_type, json_data)\n        data['_source_path'] = target\n        return self._objectify(data=data)", "response": "Get a single item with the given ID"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a given item with the passed data.", "response": "def update(self, id, **dict):\n        '''Update a given item with the passed data.'''\n        if not self._item_path:\n            raise AttributeError('update is not available for %s' % self._item_name)\n        target = (self._update_path or self._item_path) % id\n        payload = json.dumps({self._item_type:dict})\n        self._redmine.put(target, payload)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, id):\n        '''Delete a single item with the given ID'''\n        if not self._item_path:\n            raise AttributeError('delete is not available for %s' % self._item_name)\n        target = self._item_path % id\n        self._redmine.delete(target)\n        return None", "response": "Delete a single item with the given ID"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self, **options):\n        '''Return an iterator for the given items.'''\n        if not self._query_path:\n            raise AttributeError('query is not available for %s' % self._item_name)\n        last_item = 0\n        offset = 0\n        current_item = None\n        limit = options.get('limit', 25)\n        options['limit'] = limit\n        target = self._query_path\n        while True:\n            options['offset'] = offset\n            # go get the data with the given offset\n            json_data = self._redmine.get(target, options)\n            # Try and read the json\n            try:\n                data = json.loads(json_data)\n            except:\n                raise RedmineError(json_data)\n\n            # The data is enclosed in the _query_container item\n            # That is, {'issues':{(issue1),(issue2)...}, 'total_count':##}\n            data_container = data[self._query_container]\n            for item_data in data_container:\n                yield(self._objectify(data=item_data))\n\n            # If the container was empty, we requested past the end, just exit\n            if not data_container:\n                break\n            try:\n                if int(data['total_count']) > ( offset + len(data_container) ):\n                    # moar data!\n                    offset += limit\n                else:\n                    break\n            except:\n                # If we don't even have a 'total_count', we're done.\n                break", "response": "Return an iterator for the given items."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_authentication(self, username, password):\n        '''Create the authentication object with the given credentials.'''\n\n        ## BUG WORKAROUND\n        if self.version < 1.1:\n            # Version 1.0 had a bug when using the key parameter.\n            # Later versions have the opposite bug (a key in the username doesn't function)\n            if not username:\n                username = self._key\n                self._key = None\n\n        if not username:\n            return\n        if not password:\n            password = '12345'  #the same combination on my luggage!  (required dummy value)\n\n        #realm = 'Redmine API' - doesn't always work\n        # create a password manager\n        password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()\n\n        password_mgr.add_password(None, self._url, username, password )\n        handler = urllib2.HTTPBasicAuthHandler( password_mgr )\n\n        # create \"opener\" (OpenerDirector instance)\n        self._opener = urllib2.build_opener( handler )\n\n        # set the opener when we fetch the URL\n        self._opener.open( self._url )\n\n        # Install the opener.\n        urllib2.install_opener( self._opener )", "response": "Create the authentication object with the given credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_raw(self, page, parms=None, payload=None, HTTPrequest=None, payload_type='application/json' ):\n        '''Opens a page from the server with optional XML.  Returns a response file-like object'''\n        if not parms:\n            parms={}\n\n        # if we're using a key, but it's not going in the header, add it to the parms array\n        if self._key and not self.key_in_header:\n            parms['key'] = self._key\n\n        # encode any data\n        urldata = ''\n        if parms:\n            urldata = '?' + urllib.urlencode( parms )\n\n\n        fullUrl = self._url + page\n\n        #debug\n        if self.debug:\n            print fullUrl + urldata\n\n        # register this url to be used with the opener\n        # must be registered for each unique path\n        try:\n            self._opener.open( fullUrl )\n        except AttributeError:\n            # No authentication\n            pass\n\n        # Set up the request\n        if HTTPrequest:\n            request = HTTPrequest( fullUrl + urldata )\n        else:\n            request = urllib2.Request( fullUrl + urldata )\n\n        # If the key is set and in the header, add it\n        if self._key and self.key_in_header:\n            request.add_header('X-Redmine-API-Key', self._key)\n\n        # If impersonation is set, add header\n        if self.impersonate and self.impersonation_supported:\n            request.add_header('X-Redmine-Switch-User', self.impersonate)\n\n        # get the data and return XML object\n        if payload:\n            request.add_header('Content-Type', payload_type)\n            response = urllib2.urlopen( request, payload )\n        else:\n            response = urllib2.urlopen( request )\n\n        return response", "response": "Opens a page from the server with optional XML. Returns a response file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, page, parms=None, payload=None, HTTPrequest=None ):\n        '''Opens a page from the server with optional content.  Returns the string response.'''\n        response = self.open_raw( page, parms, payload, HTTPrequest )\n        return response.read()", "response": "Opens a page from the server with optional content. Returns the string response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nposting a string payload to the server - used to make new Redmine items. Returns an JSON string.", "response": "def post(self, page, payload, parms=None ):\n        '''Posts a string payload to the server - used to make new Redmine items.  Returns an JSON string or error.'''\n        if self.readonlytest:\n            print 'Redmine read only test: Pretending to create: ' + page\n            return payload\n        else:\n            return self.open( page, parms, payload )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(self, page, payload, parms=None ):\n        '''Puts an XML object on the server - used to update Redmine items.  Returns nothing useful.'''\n        if self.readonlytest:\n            print 'Redmine read only test: Pretending to update: ' + page\n        else:\n            return self.open( page, parms, payload, HTTPrequest=self.PUT_Request )", "response": "Puts an XML object on the server - used to update items. Returns nothing useful."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, page ):\n        '''Deletes a given object on the server - used to remove items from Redmine.  Use carefully!'''\n        if self.readonlytest:\n            print 'Redmine read only test: Pretending to delete: ' + page\n        else:\n            return self.open( page, HTTPrequest=self.DELETE_Request )", "response": "Deletes a given object on the server - used to remove items from Redmine. Use carefully!"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a json string and unwraps any type it finds within.", "response": "def unwrap_json(self, type, json_data):\n        '''Decodes a json string, and unwraps any 'type' it finds within.'''\n        # Parse the data\n        try:\n            data = json.loads(json_data)\n        except ValueError:\n            # If parsing failed, then raise the string which likely contains an error message instead of data\n            raise RedmineError(json_data)\n        # Check to see if there is a data wrapper\n        # Some replies will have {'issue':{<data>}} instead of just {<data>}\n        try:\n            data = data[type]\n        except KeyError:\n            pass\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding and stores a reference to all Redmine_Item subclasses for later use.", "response": "def find_all_item_classes(self):\n        '''Finds and stores a reference to all Redmine_Item subclasses for later use.'''\n        # This is a circular import, but performed after the class is defined and an object is instatiated.\n        # We do this in order to get references to any objects definitions in the redmine.py file\n        # without requiring anyone editing the file to do anything other than create a class with the proper name.\n        import redmine as public_classes\n\n        item_class = {}\n        for key, value in public_classes.__dict__.items():\n            try:\n                if issubclass(value, Redmine_Item):\n                    item_class[key.lower()] = value\n            except:\n                continue\n        self.item_class = item_class"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the updated cached version of the given dict", "response": "def check_cache(self, type, data, obj=None):\n        '''Returns the updated cached version of the given dict'''\n        try:\n            id = data['id']\n        except:\n            # Not an identifiable item\n            #print 'don\\'t know this item %r:%r' % (type, data)\n            return data\n\n        # If obj was passed in, its type takes precedence\n        try:\n            type = obj._get_type()\n        except:\n            pass\n\n        # Find the item in the cache, update and return if it's there\n        try:\n            hit = self.item_cache[type][id]\n        except KeyError:\n            pass\n        else:\n            hit._update_data(data)\n            #print 'cache hit for %s at %s' % (type, id)\n            return hit\n\n        # Not there? Let's make us a new item\n        # If we weren't given the object ref, find the name in the global scope\n        if not obj:\n            # Default to Redmine_Item if it's not found\n            obj = self.item_class.get(type, Redmine_Item)\n\n        new_item = obj(redmine=self, data=data, type=type)\n\n        # Store it\n        self.item_cache.setdefault(type, {})[id] = new_item\n        #print 'set new %s at %s' % (type, id)\n\n        return new_item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef substract(self, pt):\n        if isinstance(pt, Point):\n                return Point(pt.x - self.x, pt.y - self.y, pt.z - self.z)\n        else:\n            raise TypeError", "response": "Return a Point instance as the displacement of two points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Point instance from a given list", "response": "def from_list(cls, l):\n        \"\"\"Return a Point instance from a given list\"\"\"\n        if len(l) == 3:\n                x, y, z = map(float, l)\n                return cls(x, y, z)\n        elif len(l) == 2:\n            x, y = map(float, l)\n            return cls(x, y)\n        else:\n            raise AttributeError"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multiply(self, number):\n        return self.from_list([x * number for x in self.to_list()])", "response": "Return a Vector as the product of the vector and a real number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef magnitude(self):\n        return math.sqrt(\n            reduce(lambda x, y: x + y, [x ** 2 for x in self.to_list()])\n        )", "response": "Return the magnitude of the vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Vector instance as the vector sum of two vectors.", "response": "def sum(self, vector):\n        \"\"\"Return a Vector instance as the vector sum of two vectors.\"\"\"\n        return self.from_list(\n            [x + vector.vector[i] for i, x in self.to_list()]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dot(self, vector, theta=None):\n        if theta is not None:\n            return (self.magnitude() * vector.magnitude() *\n                    math.degrees(math.cos(theta)))\n        return (reduce(lambda x, y: x + y,\n                [x * vector.vector[i] for i, x in self.to_list()()]))", "response": "Return the dot product of two tables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Vector instance as the cross product of two vectors", "response": "def cross(self, vector):\n        \"\"\"Return a Vector instance as the cross product of two vectors\"\"\"\n        return Vector((self.y * vector.z - self.z * vector.y),\n                      (self.z * vector.x - self.x * vector.z),\n                      (self.x * vector.y - self.y * vector.x))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unit(self):\n        return Vector(\n            (self.x / self.magnitude()),\n            (self.y / self.magnitude()),\n            (self.z / self.magnitude())\n        )", "response": "Return a Vector instance of the unit vector"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the angle between two vectors in degrees.", "response": "def angle(self, vector):\n        \"\"\"Return the angle between two vectors in degrees.\"\"\"\n        return math.degrees(\n            math.acos(\n                self.dot(vector) /\n                (self.magnitude() * vector.magnitude())\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef non_parallel(self, vector):\n        if (self.is_parallel(vector) is not True and\n                self.is_perpendicular(vector) is not True):\n            return True\n        return False", "response": "Return True if vectors are non - parallel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rotate(self, angle, axis=(0, 0, 1)):\n        if not all(isinstance(a, int) for a in axis):\n            raise ValueError\n        x, y, z = self.x, self.y, self.z\n\n        # Z axis rotation\n        if(axis[2]):\n            x = (self.x * math.cos(angle) - self.y * math.sin(angle))\n            y = (self.x * math.sin(angle) + self.y * math.cos(angle))\n\n        # Y axis rotation\n        if(axis[1]):\n            x = self.x * math.cos(angle) + self.z * math.sin(angle)\n            z = -self.x * math.sin(angle) + self.z * math.cos(angle)\n\n        # X axis rotation\n        if(axis[0]):\n            y = self.y * math.cos(angle) - self.z * math.sin(angle)\n            z = self.y * math.sin(angle) + self.z * math.cos(angle)\n\n        return Vector(x, y, z)", "response": "Returns the rotated vector. Assumes angle is in radians"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_points(cls, point1, point2):\n        if isinstance(point1, Point) and isinstance(point2, Point):\n            displacement = point1.substract(point2)\n            return cls(displacement.x, displacement.y, displacement.z)\n        raise TypeError", "response": "Return a Vector instance from two given points."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Vector instance from spherical coordinates.", "response": "def spherical(cls, mag, theta, phi=0):\n        '''Returns a Vector instance from spherical coordinates'''\n        return cls(\n            mag * math.sin(phi) * math.cos(theta),  # X\n            mag * math.sin(phi) * math.sin(theta),  # Y\n            mag * math.cos(phi)  # Z\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Vector instance from cylindircal coordinates.", "response": "def cylindrical(cls, mag, theta, z=0):\n        '''Returns a Vector instance from cylindircal coordinates'''\n        return cls(\n            mag * math.cos(theta),  # X\n            mag * math.sin(theta),  # Y\n            z  # Z\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_weekday(weekday, jd, direction, offset):\n    '''Determine the Julian date for the next or previous weekday'''\n    return weekday_before(weekday, jd + (direction * offset))", "response": "Determine the Julian date for the next or previous weekday"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the nth day of the month in a given year.", "response": "def nth_day_of_month(n, weekday, month, year):\n    \"\"\"\n    Return (year, month, day) tuple that represents nth weekday of month in year.\n    If n==0, returns last weekday of month. Weekdays: Monday=0\n    \"\"\"\n    if not (0 <= n <= 5):\n        raise IndexError(\"Nth day of month must be 0-5. Received: {}\".format(n))\n\n    if not (0 <= weekday <= 6):\n        raise IndexError(\"Weekday must be 0-6\")\n\n    firstday, daysinmonth = calendar.monthrange(year, month)\n\n    # Get first WEEKDAY of month\n    first_weekday_of_kind = 1 + (weekday - firstday) % 7\n\n    if n == 0:\n        # find last weekday of kind, which is 5 if these conditions are met, else 4\n        if first_weekday_of_kind in [1, 2, 3] and first_weekday_of_kind + 28 < daysinmonth:\n            n = 5\n        else:\n            n = 4\n\n    day = first_weekday_of_kind + ((n - 1) * 7)\n\n    if day > daysinmonth:\n        raise IndexError(\"No {}th day of month {}\".format(n, month))\n\n    return (year, month, day)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef irafglob(inlist, atfile=None):\n\n    # Sanity check\n    if inlist is None or len(inlist) == 0:\n        return []\n\n    # Determine which form of input was provided:\n    if isinstance(inlist, list):\n        #  python list\n        flist = []\n        for f in inlist:\n            flist += irafglob(f)\n    elif ',' in inlist:\n        #  comma-separated string list\n        flist = []\n        for f in inlist.split(','):\n            f = f.strip()\n            flist += irafglob(f)\n    elif inlist[0] == '@':\n        #  file list\n        flist = []\n        for f in open(inlist[1:], 'r').readlines():\n            f = f.rstrip()\n            # hook for application specific atfiles.\n            if atfile:\n                f = atfile(f)\n            flist += irafglob(f)\n    else:\n        #  shell globbing\n        if osfn:\n            inlist = osfn(inlist)\n        flist = glob.glob(inlist)\n\n    return flist", "response": "Returns a list of filenames based on the type of input."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pack(self):\n\n        # Possible structs for packet ID.\n        #\n        try:\n            structs_ = get_structs_for_fields([self.fields[0]])\n        except (TypeError):\n            # TypeError, if self.fields[0] is a wrong argument to `chr()`.\n            raise PackError(self)\n\n\n        # Possible structs for packet ID + subcode\n        #\n        if structs_ == []:\n            try:\n                structs_ = get_structs_for_fields([self.fields[0], self.fields[1]])\n            except (IndexError, TypeError):\n                # IndexError, if no self.fields[1]\n                # TypeError, if self.fields[1] is a wrong argument to `chr()`.\n                raise PackError(self)\n\n\n        # Try to pack the packet with any of the possible structs.\n        #\n        for struct_ in structs_:\n            try:\n                return struct_.pack(*self.fields)\n            except struct.error:\n                pass\n\n        # We only get here if the ``return`` inside the``for`` loop\n        # above wasn't reached, i.e. none of the `structs_` matched.\n        #\n        raise PackError(self)", "response": "Return binary format of the packet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ch_handler(offset=0, length=-1, **kw):\n    global _lastSel\n\n    offset = int(offset)\n    length = int(length)\n    if length < 0: length = len(_lastSel)\n    return _lastSel[offset:offset+length]", "response": "Handle standard PRIMARY clipboard access."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nputting the given string into the given clipboard.", "response": "def put(text, cbname):\n    \"\"\" Put the given string into the given clipboard. \"\"\"\n    global _lastSel\n    _checkTkInit()\n    if cbname == 'CLIPBOARD':\n        _theRoot.clipboard_clear()\n        if text:\n            # for clipboard_append, kwds can be -displayof, -format, or -type\n            _theRoot.clipboard_append(text)\n        return\n    if cbname == 'PRIMARY':\n        _lastSel = text\n        _theRoot.selection_handle(ch_handler, selection='PRIMARY')\n        # we need to claim/own it so that ch_handler is used\n        _theRoot.selection_own(selection='PRIMARY')\n        # could add command arg for a func to be called when we lose ownership\n        return\n    raise RuntimeError(\"Unexpected clipboard name: \"+str(cbname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cbname):\n    _checkTkInit()\n    if cbname == 'PRIMARY':\n        try:\n            return _theRoot.selection_get(selection='PRIMARY')\n        except:\n            return None\n    if cbname == 'CLIPBOARD':\n        try:\n            return _theRoot.selection_get(selection='CLIPBOARD')\n        except:\n            return None\n    raise RuntimeError(\"Unexpected clipboard name: \"+str(cbname))", "response": "Get the contents of the given clipboard."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createDevice(self, deviceCfg):\n        ioCfg = deviceCfg['io']\n        type = deviceCfg['type']\n        if type == 'mpu6050':\n            fs = deviceCfg.get('fs')\n            name = deviceCfg.get('name')\n            if ioCfg['type'] == 'mock':\n                provider = ioCfg.get('provider')\n                if provider is not None and provider == 'white noise':\n                    dataProvider = WhiteNoiseProvider()\n                else:\n                    raise ValueError(provider + \" is not a supported mock io data provider\")\n                self.logger.warning(\"Loading mock data provider for mpu6050\")\n                io = mock_io(dataProvider=dataProvider.provide)\n            elif ioCfg['type'] == 'smbus':\n                busId = ioCfg['busId']\n                self.logger.warning(\"Loading smbus %d\", busId)\n                io = smbus_io(busId)\n            else:\n                raise ValueError(ioCfg['type'] + \" is not a supported io provider\")\n            self.logger.warning(\"Loading mpu6050 \" + name + \"/\" + str(fs))\n            return mpu6050(io, name=name, fs=fs) if name is not None else mpu6050(io, fs=fs)\n        else:\n            raise ValueError(type + \" is not a supported device\")", "response": "Creates a new measurement device from the input configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _loadRecordingDevices(self):\n        return {device.name: device for device in\n                [self.createDevice(deviceCfg) for deviceCfg in self.config['accelerometers']]}", "response": "Loads the recordingDevices specified in the configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createHandler(self, handler):\n        target = handler['target']\n        if handler['type'] == 'log':\n            self.logger.warning(\"Initialising csvlogger to log data to \" + target)\n            return CSVLogger('recorder', handler['name'], target)\n        elif handler['type'] == 'post':\n            self.logger.warning(\"Initialising http logger to log data to \" + target)\n            return HttpPoster(handler['name'], target)", "response": "Creates a data handler from the input configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a dictionary of named handler instances", "response": "def _loadHandlers(self):\n        \"\"\"\n        creates a dictionary of named handler instances\n        :return: the dictionary\n        \"\"\"\n        return {handler.name: handler for handler in map(self.createHandler, self.config['handlers'])}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores a chunk of new file, this is a nop if the file already exists. :param filename: the filename. :param chunkIdx: the chunk idx. :param totalChunks: the no of chunks expected. :return: the no of bytes written and 200 or 400 if nothing was written.", "response": "def put(self, filename, chunkIdx, totalChunks):\n        \"\"\"\n        stores a chunk of new file, this is a nop if the file already exists.\n        :param filename: the filename.\n        :param chunkIdx: the chunk idx.\n        :param totalChunks: the no of chunks expected.\n        :return: the no of bytes written and 200 or 400 if nothing was written.\n        \"\"\"\n        logger.info('handling chunk ' + chunkIdx + ' of ' + totalChunks + ' for ' + filename)\n        import flask\n        bytesWritten = self._uploadController.writeChunk(flask.request.stream, filename, int(chunkIdx))\n        return str(bytesWritten), 200 if bytesWritten > 0 else 400"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, name):\n        try:\n            result = self._uploadController.delete(name)\n            return None, 200 if result is not None else 404\n        except Exception as e:\n            return str(e), 500", "response": "Deletes the named file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring a new target.", "response": "def put(self, name, start, end):\n        \"\"\"\n        Stores a new target.\n        :param name: the name.\n        :param start: start time.\n        :param end: end time.\n        :return:\n        \"\"\"\n        entry = self._uploadController.getEntry(name)\n        if entry is not None:\n            return None, 200 if self._targetController.storeFromWav(entry, start, end) else 500\n        else:\n            return None, 404"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an analysed file from the specified frequency range.", "response": "def get(self, name, start, end, resolution, window):\n        \"\"\"\n        :param name:\n        :param start:\n        :param end:\n        :param resolution:\n        :param window:\n        :return: an analysed file.\n        \"\"\"\n        logger.info(\n            'Analysing ' + name + ' from ' + start + ' to ' + end + ' at ' + resolution + 'x resolution using ' + window + ' window')\n        signal = self._uploadController.loadSignal(name,\n                                                   start=start if start != 'start' else None,\n                                                   end=end if end != 'end' else None)\n        if signal is not None:\n            window = tuple(filter(None, window.split(' ')))\n            if len(window) == 2:\n                window = (window[0], float(window[1]))\n            import time\n            data = {\n                'spectrum': self._jsonify(\n                    signal.spectrum(ref=SPECLAB_REFERENCE, segmentLengthMultiplier=int(resolution), window=window)\n                ),\n                'peakSpectrum': self._jsonify(\n                    signal.peakSpectrum(ref=SPECLAB_REFERENCE, segmentLengthMultiplier=int(resolution), window=window)\n                ),\n                'analysedAt': int(time.time() * 1000)\n            }\n            return data, 200\n        else:\n            return None, 404"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncomplete the specified upload.", "response": "def put(self, filename, totalChunks, status):\n        \"\"\"\n        Completes the specified upload.\n        :param filename: the filename.\n        :param totalChunks: the no of chunks.\n        :param status: the status of the upload.\n        :return: 200.\n        \"\"\"\n        logger.info('Completing ' + filename + ' - ' + status)\n        self._uploadController.finalise(filename, int(totalChunks), status)\n        return None, 200"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the metadata associated with the new measurement.", "response": "def patch(self, measurementId):\n        \"\"\"\n        Patches the metadata associated with the new measurement, if this impacts the measurement length then a new \n        measurement is created otherwise it just updates it in place.\n        :param measurementId: \n        :return: \n        \"\"\"\n        data = request.get_json()\n        if data is not None:\n            logger.debug('Received payload for ' + measurementId + ' - ' + str(data))\n            if self._measurementController.editMeasurement(measurementId, data):\n                return None, 200\n            else:\n                logger.warning('Unable to edit payload ' + measurementId)\n                return None, 404\n        else:\n            logger.error('Invalid data payload received ' + measurementId)\n            return None, 400"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nschedule a new measurement.", "response": "def put(self, measurementId):\n        \"\"\"\n        Initiates a new measurement. Accepts a json payload with the following attributes;\n\n        * duration: in seconds\n        * startTime OR delay: a date in YMD_HMS format or a delay in seconds\n        * description: some free text information about the measurement\n\n        :return:\n        \"\"\"\n        json = request.get_json()\n        try:\n            start = self._calculateStartTime(json)\n        except ValueError:\n            return 'invalid date format in request', 400\n        duration = json['duration'] if 'duration' in json else 10\n        if start is None:\n            # should never happen but just in case\n            return 'no start time', 400\n        else:\n            scheduled, message = self._measurementController.schedule(measurementId, duration, start,\n                                                                      description=json.get('description'))\n            return message, 200 if scheduled else 400"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _calculateStartTime(self, json):\n        start = json['startTime'] if 'startTime' in json else None\n        delay = json['delay'] if 'delay' in json else None\n        if start is None and delay is None:\n            return self._getAbsoluteTime(datetime.datetime.utcnow(), 2)\n        elif start is not None:\n            target = datetime.datetime.strptime(start, DATETIME_FORMAT)\n            if target <= datetime.datetime.utcnow():\n                time = self._getAbsoluteTime(datetime.datetime.utcnow(), 2)\n                logger.warning('Date requested is in the past (' + start + '), defaulting to ' +\n                               time.strftime(DATETIME_FORMAT))\n                return time\n            else:\n                return target\n        elif delay is not None:\n            return self._getAbsoluteTime(datetime.datetime.utcnow(), delay)\n        else:\n            return None", "response": "Calculates an absolute start time from the json payload."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the absolute time for the specified point in time.", "response": "def _getAbsoluteTime(self, start, delay):\n        \"\"\"\n        Adds the delay in seconds to the start time.\n        :param start:\n        :param delay:\n        :return: a datetimem for the specified point in time.\n        \"\"\"\n        return start + datetime.timedelta(days=0, seconds=delay)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, measurementId):\n        message, count, deleted = self._measurementController.delete(measurementId)\n        if count == 0:\n            return message, 404\n        elif deleted is None:\n            return message, 500\n        else:\n            return deleted, 200", "response": "Deletes the named measurement."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put(self, measurementId, deviceId):\n        logger.info('Starting measurement ' + measurementId + ' for ' + deviceId)\n        if self._measurementController.startMeasurement(measurementId, deviceId):\n            logger.info('Started measurement ' + measurementId + ' for ' + deviceId)\n            return None, 200\n        else:\n            logger.warning('Failed to start measurement ' + measurementId + ' for ' + deviceId)\n            return None, 404", "response": "Starts the measurement session from the given device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring a bunch of data for this measurement session.", "response": "def put(self, measurementId, deviceId):\n        \"\"\"\n        Store a bunch of data for this measurement session.\n        :param measurementId:\n        :param deviceId:\n        :return:\n        \"\"\"\n        data = request.get_json()\n        if data is not None:\n            parsedData = json.loads(data)\n            logger.debug('Received payload ' + measurementId + '/' + deviceId + ': ' +\n                         str(len(parsedData)) + ' records')\n            if self._measurementController.recordData(measurementId, deviceId, parsedData):\n                return None, 200\n            else:\n                logger.warning('Unable to record payload ' + measurementId + '/' + deviceId)\n                return None, 404\n        else:\n            logger.error('Invalid data payload received ' + measurementId + '/' + deviceId)\n            return None, 400"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put(self, measurementId, deviceId):\n        payload = request.get_json()\n        failureReason = json.loads(payload).get('failureReason') if payload is not None else None\n        logger.warning('Failing measurement ' + measurementId + ' for ' + deviceId + ' because ' + str(failureReason))\n        if self._measurementController.failMeasurement(measurementId, deviceId, failureReason=failureReason):\n            logger.warning('Failed measurement ' + measurementId + ' for ' + deviceId)\n            return None, 200\n        else:\n            logger.error('Unable to fail measurement ' + measurementId + ' for ' + deviceId)\n            return None, 404", "response": "Fail the measurement for this device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put(self, deviceId):\n        device = request.get_json()\n        logger.debug(\"Received /devices/\" + deviceId + \" - \" + str(device))\n        self._deviceController.accept(deviceId, device)\n        return None, 200", "response": "Put a new device into the device store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef printColsAuto(in_strings, term_width=80, min_pad=1):\n    # sanity check\n    assert in_strings and len(in_strings)>0, 'Unexpected: '+repr(in_strings)\n\n    # get max width in input\n    maxWidth = len(max(in_strings, key=len)) + (2*min_pad) # width with pad\n    numCols = term_width//maxWidth # integer div\n    # set numCols so we take advantage of the whole line width\n    numCols = min(numCols, len(in_strings))\n\n    # easy case - single column or too big\n    if numCols < 2:\n        # one or some items are too big but print one item per line anyway\n        lines = [x.center(term_width) for x in in_strings]\n        return '\\n'.join(lines)\n\n    # normal case - 2 or more columns\n    colWidth = term_width//numCols # integer div\n    # colWidth is guaranteed to be larger than all items in input\n    retval = ''\n    for i in range(len(in_strings)):\n        retval+=in_strings[i].center(colWidth)\n        if (i+1)%numCols == 0:\n            retval += '\\n'\n    return retval.rstrip()", "response": "Print a list of strings centered in columns and lines on the fly."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints elements of list in cols columns", "response": "def printCols(strlist,cols=5,width=80):\n\n    \"\"\"Print elements of list in cols columns\"\"\"\n\n    # This may exist somewhere in the Python standard libraries?\n    # Should probably rewrite this, it is pretty crude.\n\n    nlines = (len(strlist)+cols-1)//cols\n    line = nlines*[\"\"]\n    for i in range(len(strlist)):\n        c, r = divmod(i,nlines)\n        nwid = c*width//cols - len(line[r])\n        if nwid>0:\n            line[r] = line[r] + nwid*\" \" + strlist[i]\n        else:\n            line[r] = line[r] + \" \" + strlist[i]\n    for s in line:\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stripQuotes(value):\n\n    \"\"\"Strip single or double quotes off string; remove embedded quote pairs\"\"\"\n\n    if value[:1] == '\"':\n        value = value[1:]\n        if value[-1:] == '\"':\n            value = value[:-1]\n        # replace \"\" with \"\n        value = re.sub(_re_doubleq2, '\"', value)\n    elif value[:1] == \"'\":\n        value = value[1:]\n        if value[-1:] == \"'\":\n            value = value[:-1]\n        # replace '' with '\n        value = re.sub(_re_singleq2, \"'\", value)\n    return value", "response": "Strip single or double quotes off string ; remove embedded quote pairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a string as input and splits it into tokens separated by commas ignoring embedded inside quoted sections.", "response": "def csvSplit(line, delim=',', allowEol=True):\n    \"\"\" Take a string as input (e.g. a line in a csv text file), and break\n    it into tokens separated by commas while ignoring commas embedded inside\n    quoted sections.  This is exactly what the 'csv' module is meant for, so\n    we *should* be using it, save that it has two bugs (described next) which\n    limit our use of it.  When these bugs are fixed, this function should be\n    forsaken in favor of direct use of the csv module (or similar).\n\n    The basic use case is to split a function signature string, so for:\n        afunc(arg1='str1', arg2='str, with, embedded, commas', arg3=7)\n    we want a 3 element sequence:\n        [\"arg1='str1'\", \"arg2='str, with, embedded, commas'\", \"arg3=7\"]\n\n    but:\n    >>> import csv\n    >>> y = \"arg1='str1', arg2='str, with, embedded, commas', arg3=7\"\n    >>> rdr = csv.reader( (y,), dialect='excel', quotechar=\"'\", skipinitialspace=True)\n    >>> l = rdr.next(); print(len(l), str(l))  # doctest: +SKIP\n    6 [\"arg1='str1'\", \"arg2='str\", 'with', 'embedded', \"commas'\", \"arg3=7\"]\n\n    which we can see is not correct - we wanted 3 tokens.  This occurs in\n    Python 2.5.2 and 2.6.  It seems to be due to the text at the start of each\n    token (\"arg1=\") i.e. because the quote isn't for the whole token.  If we\n    were to remove the names of the args and the equal signs, it works:\n\n    >>> x = \"'str1', 'str, with, embedded, commas', 7\"\n    >>> rdr = csv.reader( (x,), dialect='excel', quotechar=\"'\", skipinitialspace=True)\n    >>> l = rdr.next(); print(len(l), str(l))  # doctest: +SKIP\n    3 ['str1', 'str, with, embedded, commas', '7']\n\n    But even this usage is delicate - when we turn off skipinitialspace, it\n    fails:\n\n    >>> x = \"'str1', 'str, with, embedded, commas', 7\"\n    >>> rdr = csv.reader( (x,), dialect='excel', quotechar=\"'\")\n    >>> l = rdr.next(); print(len(l), str(l))  # doctest: +SKIP\n    6 ['str1', \" 'str\", ' with', ' embedded', \" commas'\", ' 7']\n\n    So, for now, we'll roll our own.\n    \"\"\"\n    # Algorithm:  read chars left to right, go from delimiter to delimiter,\n    # but as soon as a single/double/triple quote is hit, scan forward\n    # (ignoring all else) until its matching end-quote is found.\n    # For now, we will not specially handle escaped quotes.\n    tokens = []\n    ldl = len(delim)\n    keepOnRollin = line is not None and len(line) > 0\n    while keepOnRollin:\n        tok = _getCharsUntil(line, delim, True, allowEol=allowEol)\n        # len of token should always be > 0 because it includes end delimiter\n        # except on last token\n        if len(tok) > 0:\n            # append it, but without the delimiter\n            if tok[-ldl:] == delim:\n                tokens.append(tok[:-ldl])\n            else:\n                tokens.append(tok) # tok goes to EOL - has no delimiter\n                keepOnRollin = False\n            line = line[len(tok):]\n        else:\n            # This is the case of the empty end token\n            tokens.append('')\n            keepOnRollin = False\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rglob(root, pattern):\n    # Thanks to Alex Martelli for basics on Stack Overflow\n    retlist = []\n    if None not in (pattern, root):\n        for base, dirs, files in os.walk(root):\n            goodfiles = fnmatch.filter(files, pattern)\n            retlist.extend(os.path.join(base, f) for f in goodfiles)\n    return retlist", "response": "Globs the root directory for all files matching the pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a file named fname to be writable by user.", "response": "def setWritePrivs(fname, makeWritable, ignoreErrors=False):\n    \"\"\" Set a file named fname to be writable (or not) by user, with the\n    option to ignore errors.  There is nothing ground-breaking here, but I\n    was annoyed with having to repeate this little bit of code. \"\"\"\n    privs = os.stat(fname).st_mode\n    try:\n        if makeWritable:\n            os.chmod(fname, privs | stat.S_IWUSR)\n        else:\n            os.chmod(fname, privs & (~ stat.S_IWUSR))\n    except OSError:\n        if ignoreErrors:\n            pass # just try, don't whine\n        else:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removeEscapes(value, quoted=0):\n\n    \"\"\"Remove escapes from in front of quotes (which IRAF seems to\n    just stick in for fun sometimes.)  Remove \\-newline too.\n    If quoted is true, removes all blanks following \\-newline\n    (which is a nasty thing IRAF does for continuations inside\n    quoted strings.)\n    XXX Should we remove \\\\ too?\n    \"\"\"\n\n    i = value.find(r'\\\"')\n    while i>=0:\n        value = value[:i] + value[i+1:]\n        i = value.find(r'\\\"',i+1)\n    i = value.find(r\"\\'\")\n    while i>=0:\n        value = value[:i] + value[i+1:]\n        i = value.find(r\"\\'\",i+1)\n    # delete backslash-newlines\n    i = value.find(\"\\\\\\n\")\n    while i>=0:\n        j = i+2\n        if quoted:\n            # ignore blanks and tabs following \\-newline in quoted strings\n            for c in value[i+2:]:\n                if c not in ' \\t':\n                    break\n                j = j+1\n        value = value[:i] + value[j:]\n        i = value.find(\"\\\\\\n\",i+1)\n    return value", "response": "Remove escapes from in front of quotes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating CL parameter or variable name to Python - acceptable name", "response": "def translateName(s, dot=0):\n\n    \"\"\"Convert CL parameter or variable name to Python-acceptable name\n\n    Translate embedded dollar signs to 'DOLLAR'\n    Add 'PY' prefix to components that are Python reserved words\n    Add 'PY' prefix to components start with a number\n    If dot != 0, also replaces '.' with 'DOT'\n    \"\"\"\n\n    s = s.replace('$', 'DOLLAR')\n    sparts = s.split('.')\n    for i in range(len(sparts)):\n        if sparts[i] == \"\" or sparts[i][0] in string.digits or \\\n          keyword.iskeyword(sparts[i]):\n            sparts[i] = 'PY' + sparts[i]\n    if dot:\n        return 'DOT'.join(sparts)\n    else:\n        return '.'.join(sparts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_tk_default_root(withdraw=True):\n\n    \"\"\" In case the _default_root value is required, you may\n    safely call this ahead of time to ensure that it has been\n    initialized.  If it has already been, this is a no-op.\n    \"\"\"\n    if not capable.OF_GRAPHICS:\n        raise RuntimeError(\"Cannot run this command without graphics\")\n\n    if not TKNTR._default_root: # TKNTR imported above\n        junk = TKNTR.Tk()\n\n    # tkinter._default_root is now populated (== junk)\n    retval = TKNTR._default_root\n    if withdraw and retval:\n        retval.withdraw()\n\n    return retval", "response": "Initialize the _default_root value of the TK."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading a line from file while running Tk mainloop.", "response": "def tkreadline(file=None):\n\n    \"\"\"Read a line from file while running Tk mainloop.\n\n    If the file is not line-buffered then the Tk mainloop will stop\n    running after one character is typed.  The function will still work\n    but Tk widgets will stop updating.  This should work OK for stdin and\n    other line-buffered filehandles.  If file is omitted, reads from\n    sys.stdin.\n\n    The file must have a readline method.  If it does not have a fileno\n    method (which can happen e.g. for the status line input on the\n    graphics window) then the readline method is simply called directly.\n    \"\"\"\n\n    if file is None:\n        file = sys.stdin\n    if not hasattr(file, \"readline\"):\n        raise TypeError(\"file must be a filehandle with a readline method\")\n\n    # Call tkread now...\n    # BUT, if we get in here for something not GUI-related (e.g. terminal-\n    # focused code in a sometimes-GUI app) then skip tkread and simply call\n    # readline on the input eg. stdin.  Otherwise we'd fail in _TkRead().read()\n\n    try:\n        fd = file.fileno()\n    except:\n        fd = None\n\n    if (fd and capable.OF_GRAPHICS):\n        tkread(fd, 0)\n        # if EOF was encountered on a tty, avoid reading again because\n        # it actually requests more data\n        if not select.select([fd],[],[],0)[0]:\n            return ''\n    return file.readline()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef launchBrowser(url, brow_bin='mozilla', subj=None):\n\n    if not subj: subj = url\n\n    # Tries to use webbrowser module on most OSes, unless a system command\n    # is needed.  (E.g. win, linux, sun, etc)\n    if sys.platform not in ('os2warp, iphone'): # try webbrowser w/ everything?\n        import webbrowser\n        if not webbrowser.open(url):\n            print(\"Error opening URL: \"+url)\n        else:\n            print('Help on \"'+subj+'\" is now being displayed in a web browser')\n        return\n\n    # Go ahead and fork a subprocess to call the correct binary\n    pid = os.fork()\n    if pid == 0: # child\n        if sys.platform == 'darwin':\n            if 0 != os.system('open \"'+url+'\"'): # does not seem to keep '#.*'\n                print(\"Error opening URL: \"+url)\n        os._exit(0)\n#       The following retries if \"-remote\" doesnt work, opening a new browser\n#       cmd = brow_bin+\" -remote 'openURL(\"+url+\")' '\"+url+\"' 1> /dev/null 2>&1\"\n#       if 0 != os.system(cmd)\n#           print \"Running \"+brow_bin+\" for HTML help...\"\n#           os.execvp(brow_bin,[brow_bin,url])\n#       os._exit(0)\n\n    else: # parent\n        if not subj:\n            subj = url\n        print('Help on \"'+subj+'\" is now being displayed in a browser')", "response": "Tries to open a URL in a browser and launches it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, file, nbytes):\n        if not capable.OF_GRAPHICS:\n            raise RuntimeError(\"Cannot run this command without graphics\")\n        if isinstance(file, int):\n            fd = file\n        else:\n            # Otherwise, assume we have Python file object\n            try:\n                fd = file.fileno()\n\n            except:\n                raise TypeError(\"file must be an integer or a filehandle/socket\")\n        init_tk_default_root() # harmless if already done\n        self.widget = TKNTR._default_root\n        if not self.widget:\n            # no Tk widgets yet, so no need for mainloop\n            # (shouldnt happen now with init_tk_default_root)\n            s = []\n            while nbytes>0:\n                snew = os.read(fd, nbytes) # returns bytes in PY3K\n                if snew:\n                    if PY3K: snew = snew.decode('ascii','replace')\n                    s.append(snew)\n                    nbytes -= len(snew)\n                else:\n                    # EOF -- just return what we have so far\n                    break\n            return \"\".join(s)\n        else:\n            self.nbytes = nbytes\n            self.value = []\n            self.widget.tk.createfilehandler(fd,\n                                    TKNTR.READABLE | TKNTR.EXCEPTION,\n                                    self._read)\n            try:\n                self.widget.mainloop()\n            finally:\n                self.widget.tk.deletefilehandler(fd)\n            return \"\".join(self.value)", "response": "Read nbytes characters from file while running Tk mainloop"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread waiting data and terminate Tk mainloop if done", "response": "def _read(self, fd, mask):\n        \"\"\"Read waiting data and terminate Tk mainloop if done\"\"\"\n        try:\n            # if EOF was encountered on a tty, avoid reading again because\n            # it actually requests more data\n            if select.select([fd],[],[],0)[0]:\n                snew = os.read(fd, self.nbytes) # returns bytes in PY3K\n                if PY3K: snew = snew.decode('ascii','replace')\n                self.value.append(snew)\n                self.nbytes -= len(snew)\n            else:\n                snew = ''\n            if (self.nbytes <= 0 or len(snew) == 0) and self.widget:\n                # stop the mainloop\n                self.widget.quit()\n        except OSError:\n            raise IOError(\"Error reading from %s\" % (fd,))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loadSignalFromDelimitedFile(filename, timeColumnIdx=0, dataColumnIdx=1, delimiter=',', skipHeader=0) -> Signal:\n    data = np.genfromtxt(filename, delimiter=delimiter, skip_header=skipHeader)\n    columnCount = data.shape[1]\n    if columnCount < timeColumnIdx + 1:\n        raise ValueError(\n            filename + ' has only ' + columnCount + ' columns,  time values can\\'t be at column ' + timeColumnIdx)\n    if columnCount < dataColumnIdx + 1:\n        raise ValueError(\n            filename + ' has only ' + columnCount + ' columns,  data values can\\'t be at column ' + dataColumnIdx)\n\n    t = data[:, [timeColumnIdx]]\n    samples = data[:, [dataColumnIdx]]\n    # calculate fs as the interval between the time samples\n    fs = int(round(1 / (np.diff(t, n=1, axis=0).mean()), 0))\n    source = Signal(samples.ravel(), fs)\n    return source", "response": "reads a delimited file and converts it into a Signal instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a wav file into a Signal and scales it to a range of + 1 to - 1", "response": "def loadSignalFromWav(inputSignalFile, calibrationRealWorldValue=None, calibrationSignalFile=None, start=None,\n                      end=None) -> Signal:\n    \"\"\" reads a wav file into a Signal and scales the input so that the sample are expressed in real world values\n    (as defined by the calibration signal).\n    :param inputSignalFile: a path to the input signal file\n    :param calibrationSignalFile: a path the calibration signal file\n    :param calibrationRealWorldValue: the real world value represented by the calibration signal\n    :param bitDepth: the bit depth of the input signal, used to rescale the value to a range of +1 to -1\n    :returns: a Signal\n    \"\"\"\n    inputSignal = readWav(inputSignalFile, start=start, end=end)\n    if calibrationSignalFile is not None:\n        calibrationSignal = readWav(calibrationSignalFile)\n        scalingFactor = calibrationRealWorldValue / np.max(calibrationSignal.samples)\n        return Signal(inputSignal.samples * scalingFactor, inputSignal.fs)\n    else:\n        return inputSignal"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a wav file into a Seq object", "response": "def readWav(inputSignalFile, selectedChannel=1, start=None, end=None) -> Signal:\n    \"\"\" reads a wav file into a Signal.\n    :param inputSignalFile: a path to the input signal file\n    :param selectedChannel: the channel to read.\n    :param start: the time to start reading from in HH:mm:ss.SSS format.\n    :param end: the time to end reading from in HH:mm:ss.SSS format.\n    :returns: Signal.\n    \"\"\"\n\n    def asFrames(time, fs):\n        hours, minutes, seconds = (time.split(\":\"))[-3:]\n        hours = int(hours)\n        minutes = int(minutes)\n        seconds = float(seconds)\n        millis = int((3600000 * hours) + (60000 * minutes) + (1000 * seconds))\n        return int(millis * (fs / 1000))\n\n    import soundfile as sf\n    if start is not None or end is not None:\n        info = sf.info(inputSignalFile)\n        startFrame = 0 if start is None else asFrames(start, info.samplerate)\n        endFrame = None if end is None else asFrames(end, info.samplerate)\n        ys, frameRate = sf.read(inputSignalFile, start=startFrame, stop=endFrame)\n    else:\n        ys, frameRate = sf.read(inputSignalFile)\n    return Signal(ys[::selectedChannel], frameRate)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loadTriAxisSignalFromFile(filename, timeColumnIdx=0, xIdx=1, yIdx=2, zIdx=3, delimiter=',',\n                              skipHeader=0) -> TriAxisSignal:\n    \"\"\"\n    A factory method for loading a tri axis measurement from a single file.\n    :param filename: the file to load from.\n    :param timeColumnIdx: the column containing time data.\n    :param xIdx: the column containing x axis data.\n    :param yIdx: the column containing y axis data.\n    :param zIdx: the column containing z axis data.\n    :param delimiter: the delimiter.\n    :param skipHeader: how many rows of headers to skip.\n    :return: the measurement\n    \"\"\"\n    return TriAxisSignal(\n        x=loadSignalFromDelimitedFile(filename, timeColumnIdx=timeColumnIdx, dataColumnIdx=xIdx,\n                                      delimiter=delimiter, skipHeader=skipHeader),\n        y=loadSignalFromDelimitedFile(filename, timeColumnIdx=timeColumnIdx, dataColumnIdx=yIdx,\n                                      delimiter=delimiter, skipHeader=skipHeader),\n        z=loadSignalFromDelimitedFile(filename, timeColumnIdx=timeColumnIdx, dataColumnIdx=zIdx,\n                                      delimiter=delimiter, skipHeader=skipHeader))", "response": "A factory method for loading a tri axis measurement from a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef psd(self, ref=None, segmentLengthMultiplier=1, mode=None, **kwargs):\n\n        def analysisFunc(x, nperseg, **kwargs):\n            f, Pxx_den = signal.welch(self.samples, self.fs, nperseg=nperseg, detrend=False, **kwargs)\n            if ref is not None:\n                Pxx_den = librosa.power_to_db(Pxx_den, ref)\n            return f, Pxx_den\n\n        if mode == 'cq':\n            return self._cq(analysisFunc, segmentLengthMultiplier)\n        else:\n            return analysisFunc(0, self.getSegmentLength() * segmentLengthMultiplier, **kwargs)", "response": "Analyzes the source and returns a PSD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spectrum(self, ref=None, segmentLengthMultiplier=1, mode=None, **kwargs):\n\n        def analysisFunc(x, nperseg, **kwargs):\n            f, Pxx_spec = signal.welch(self.samples, self.fs, nperseg=nperseg, scaling='spectrum', detrend=False,\n                                       **kwargs)\n            Pxx_spec = np.sqrt(Pxx_spec)\n            # it seems a 3dB adjustment is required to account for the change in nperseg\n            if x > 0:\n                Pxx_spec = Pxx_spec / (10 ** ((3 * x) / 20))\n            if ref is not None:\n                Pxx_spec = librosa.amplitude_to_db(Pxx_spec, ref)\n            return f, Pxx_spec\n\n        if mode == 'cq':\n            return self._cq(analysisFunc, segmentLengthMultiplier)\n        else:\n            return analysisFunc(0, self.getSegmentLength() * segmentLengthMultiplier, **kwargs)", "response": "Analyzes the linear spectrum of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peakSpectrum(self, ref=None, segmentLengthMultiplier=1, mode=None, window='hann'):\n\n        def analysisFunc(x, nperseg):\n            freqs, _, Pxy = signal.spectrogram(self.samples,\n                                               self.fs,\n                                               window=window,\n                                               nperseg=int(nperseg),\n                                               noverlap=int(nperseg // 2),\n                                               detrend=False,\n                                               scaling='spectrum')\n            Pxy_max = np.sqrt(Pxy.max(axis=-1).real)\n            if x > 0:\n                Pxy_max = Pxy_max / (10 ** ((3 * x) / 20))\n            if ref is not None:\n                Pxy_max = librosa.amplitude_to_db(Pxy_max, ref=ref)\n            return freqs, Pxy_max\n\n        if mode == 'cq':\n            return self._cq(analysisFunc, segmentLengthMultiplier)\n        else:\n            return analysisFunc(0, self.getSegmentLength() * segmentLengthMultiplier)", "response": "Analyzes the peak spectrum of the source and returns the max values per bin per segment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spectrogram(self, ref=None, segmentLengthMultiplier=1, window='hann'):\n        t, f, Sxx = signal.spectrogram(self.samples,\n                                       self.fs,\n                                       window=window,\n                                       nperseg=self.getSegmentLength() * segmentLengthMultiplier,\n                                       detrend=False,\n                                       scaling='spectrum')\n        Sxx = np.sqrt(Sxx)\n        if ref is not None:\n            Sxx = librosa.amplitude_to_db(Sxx, ref)\n        return t, f, Sxx", "response": "Analyses the source to generate a spectrogram of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a copy of the current signal with the low pass applied.", "response": "def lowPass(self, *args):\n        \"\"\"\n        Creates a copy of the signal with the low pass applied, args specifed are passed through to _butter. \n        :return: \n        \"\"\"\n        return Signal(self._butter(self.samples, 'low', *args), fs=self.fs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef highPass(self, *args):\n        return Signal(self._butter(self.samples, 'high', *args), fs=self.fs)", "response": "Returns a copy of the current signal with the high pass applied."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply a digital butterworth filter via filtfilt at the specified f3 and order.", "response": "def _butter(self, data, btype, f3=2, order=2):\n        \"\"\"\n        Applies a digital butterworth filter via filtfilt at the specified f3 and order. Default values are set to \n        correspond to apparently sensible filters that distinguish between vibration and tilt from an accelerometer.\n        :param data: the data to filter.\n        :param btype: high or low.\n        :param f3: the f3 of the filter.\n        :param order: the filter order.\n        :return: the filtered signal.\n        \"\"\"\n        b, a = signal.butter(order, f3 / (0.5 * self.fs), btype=btype)\n        y = signal.filtfilt(b, a, data)\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getAnalysis(self, axis, analysis, ref=None):\n        cache = self.cache.get(str(ref))\n        if cache is None:\n            cache = {'x': {}, 'y': {}, 'z': {}, 'sum': {}}\n            self.cache[str(ref)] = cache\n        if axis in cache:\n            data = self.cache['raw'].get(axis, None)\n            cachedAxis = cache.get(axis)\n            if cachedAxis.get(analysis) is None:\n                if axis == 'sum':\n                    if self._canSum(analysis):\n                        fx, Pxx = self._getAnalysis('x', analysis)\n                        fy, Pxy = self._getAnalysis('y', analysis)\n                        fz, Pxz = self._getAnalysis('z', analysis)\n                        # calculate the sum of the squares with an additional weighting for x and y\n                        Psum = (((Pxx * 2.2) ** 2) + ((Pxy * 2.4) ** 2) + (Pxz ** 2)) ** 0.5\n                        if ref is not None:\n                            Psum = librosa.amplitude_to_db(Psum, ref)\n                        cachedAxis[analysis] = (fx, Psum)\n                    else:\n                        return None\n                else:\n                    cachedAxis[analysis] = getattr(data.highPass(), analysis)(ref=ref)\n            return cachedAxis[analysis]\n        else:\n            return None", "response": "get the analysis on the given axis and cache the result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef legal_date(year, month, day):\n    '''Checks if a given date is a legal positivist date'''\n    try:\n        assert year >= 1\n        assert 0 < month <= 14\n        assert 0 < day <= 28\n        if month == 14:\n            if isleap(year + YEAR_EPOCH - 1):\n                assert day <= 2\n            else:\n                assert day == 1\n\n    except AssertionError:\n        raise ValueError(\"Invalid Positivist date: ({}, {}, {})\".format(year, month, day))\n\n    return True", "response": "Checks if a given date is a legal positivist date."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_jd(year, month, day):\n    '''Convert a Positivist date to Julian day count.'''\n    legal_date(year, month, day)\n    gyear = year + YEAR_EPOCH - 1\n\n    return (\n        gregorian.EPOCH - 1 + (365 * (gyear - 1)) +\n        floor((gyear - 1) / 4) + (-floor((gyear - 1) / 100)) +\n        floor((gyear - 1) / 400) + (month - 1) * 28 + day\n    )", "response": "Convert a Positivist date to Julian day count."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_jd(jd):\n    '''Convert a Julian day count to Positivist date.'''\n    try:\n        assert jd >= EPOCH\n    except AssertionError:\n        raise ValueError('Invalid Julian day')\n\n    depoch = floor(jd - 0.5) + 0.5 - gregorian.EPOCH\n\n    quadricent = floor(depoch / gregorian.INTERCALATION_CYCLE_DAYS)\n    dqc = depoch % gregorian.INTERCALATION_CYCLE_DAYS\n\n    cent = floor(dqc / gregorian.LEAP_SUPPRESSION_DAYS)\n    dcent = dqc % gregorian.LEAP_SUPPRESSION_DAYS\n\n    quad = floor(dcent / gregorian.LEAP_CYCLE_DAYS)\n    dquad = dcent % gregorian.LEAP_CYCLE_DAYS\n\n    yindex = floor(dquad / gregorian.YEAR_DAYS)\n    year = (\n        quadricent * gregorian.INTERCALATION_CYCLE_YEARS +\n        cent * gregorian.LEAP_SUPPRESSION_YEARS +\n        quad * gregorian.LEAP_CYCLE_YEARS + yindex\n    )\n\n    if yindex == 4:\n        yearday = 365\n        year = year - 1\n\n    else:\n        yearday = (\n            depoch -\n            quadricent * gregorian.INTERCALATION_CYCLE_DAYS -\n            cent * gregorian.LEAP_SUPPRESSION_DAYS -\n            quad * gregorian.LEAP_CYCLE_DAYS -\n            yindex * gregorian.YEAR_DAYS\n        )\n\n    month = floor(yearday / 28)\n\n    return (year - YEAR_EPOCH + 2, month + 1, int(yearday - (month * 28)) + 1)", "response": "Convert a Julian day count to Positivist date."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the name of the month and day for a given date.", "response": "def dayname(year, month, day):\n    '''\n    Give the name of the month and day for a given date.\n\n    Returns:\n        tuple month_name, day_name\n    '''\n    legal_date(year, month, day)\n\n    yearday = (month - 1) * 28 + day\n\n    if isleap(year + YEAR_EPOCH - 1):\n        dname = data.day_names_leap[yearday - 1]\n    else:\n        dname = data.day_names[yearday - 1]\n\n    return MONTHS[month - 1], dname"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the lastUpdateTime is more than maxAge seconds ago.", "response": "def hasExpired(self):\n        \"\"\"\n        :return: true if the lastUpdateTime is more than maxAge seconds ago.\n        \"\"\"\n        return (datetime.datetime.utcnow() - self.lastUpdateTime).total_seconds() > self.maxAgeSeconds"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the named device to the store.", "response": "def accept(self, deviceId, device):\n        \"\"\"\n        Adds the named device to the store.\n        :param deviceId:\n        :param device:\n        :return:\n        \"\"\"\n        storedDevice = self.devices.get(deviceId)\n        if storedDevice is None:\n            logger.info('Initialising device ' + deviceId)\n            storedDevice = Device(self.maxAgeSeconds)\n            storedDevice.deviceId = deviceId\n            # this uses an async handler to decouple the recorder put (of the data) from the analyser handling that data\n            # thus the recorder will become free as soon as it has handed off the data. This means delivery is only\n            # guaranteed as long as the analyser stays up but this is not a system that sits on top of a bulletproof\n            # message bus so unlucky :P\n            storedDevice.dataHandler = AsyncHandler('analyser', CSVLogger('analyser', deviceId, self.dataDir))\n        else:\n            logger.debug('Pinged by device ' + deviceId)\n        storedDevice.payload = device\n        storedDevice.lastUpdateTime = datetime.datetime.utcnow()\n        # TODO if device has FAILED, do something?\n        self.devices.update({deviceId: storedDevice})\n        self.targetStateController.updateDeviceState(storedDevice.payload)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the devices in the given state.", "response": "def getDevices(self, status=None):\n        \"\"\"\n        The devices in the given state or all devices is the arg is none.\n        :param status: the state to match against.\n        :return: the devices\n        \"\"\"\n        return [d for d in self.devices.values() if status is None or d.payload.get('status') == status]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getDevice(self, id):\n        return next(iter([d for d in self.devices.values() if d.deviceId == id]), None)", "response": "get the named device."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nschedules the requested measurement session with all INITIALISED devices.", "response": "def scheduleMeasurement(self, measurementId, duration, start):\n        \"\"\"\n        Schedules the requested measurement session with all INITIALISED devices.\n        :param measurementId:\n        :param duration:\n        :param start:\n        :return: a dict of device vs status.\n        \"\"\"\n        # TODO subtract 1s from start and format\n        results = {}\n        for device in self.getDevices(RecordingDeviceStatus.INITIALISED.name):\n            logger.info('Sending measurement ' + measurementId + ' to ' + device.payload['serviceURL'])\n            try:\n                resp = self.httpclient.put(device.payload['serviceURL'] + '/measurements/' + measurementId,\n                                           json={'duration': duration, 'at': start.strftime(DATETIME_FORMAT)})\n                logger.info('Response for ' + measurementId + ' from ' + device.payload['serviceURL'] + ' is ' +\n                            str(resp.status_code))\n                results[device] = resp.status_code\n            except Exception as e:\n                logger.exception(e)\n                results[device] = 500\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch(self):\n        # TODO block until all devices have updated?\n        json = request.get_json()\n        logger.info(\"Updating target state with \" + str(json))\n        self._targetStateController.updateTargetState(json)\n        return None, 200", "response": "Updates the target state with the current state of the target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_parse(name_list):\n\n    if name_list and name_list[0] == '@':\n        value = name_list[1:]\n        if not os.path.exists(value):\n            log.warning('The file %s does not exist' % value)\n            return\n        try:\n            return [v.strip() for v in open(value, 'r').readlines()]\n        except IOError as e:\n            log.warning('reading %s failed: %s; ignoring this file' %\n                        (value, e))\n    else:\n        return [v.strip() for v in name_list.split(',')]", "response": "Parse a comma - separated list of values or a filename starting with @"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _mmInit(self):\n        # cache references to speed up loop a bit\n        mmkeys = {}\n        mmkeysGet = mmkeys.setdefault\n        minkeylength = self.minkeylength\n        for key in self.data.keys():\n            # add abbreviations as short as minkeylength\n            # always add at least one entry (even for key=\"\")\n            lenkey = len(key)\n            start = min(minkeylength,lenkey)\n            for i in range(start,lenkey+1):\n                mmkeysGet(key[0:i],[]).append(key)\n        self.mmkeys = mmkeys", "response": "Create the minimum match dictionary of keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve(self, key, keylist):\n        raise AmbiguousKeyError(\"Ambiguous key \"+ repr(key) +\n                \", could be any of \" + str(sorted(keylist)))", "response": "Hook to resolve ambiguities in selected keys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, key, item):\n        mmkeys = self.mmkeys\n        if mmkeys is not None and not (key in self.data):\n            # add abbreviations as short as minkeylength\n            # always add at least one entry (even for key=\"\")\n            lenkey = len(key)\n            start = min(self.minkeylength,lenkey)\n            # cache references to speed up loop a bit\n            mmkeysGet = mmkeys.setdefault\n            for i in range(start,lenkey+1):\n                mmkeysGet(key[0:i],[]).append(key)\n        self.data[key] = item", "response": "Add a new key - item pair to the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, key, failobj=None, exact=0):\n        if not exact:\n            key = self.getfullkey(key,new=1)\n        return self.data.get(key,failobj)", "response": "Returns the value of the object with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _has(self, key, exact=0):\n        if not exact:\n            key = self.getfullkey(key,new=1)\n        return key in self.data", "response": "Returns True if the key is in the cache False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all the matching values for key and containing a single entry for unambiguous matches and multiple entries for ambiguous matches and multiple entries for ambiguous matches.", "response": "def getall(self, key, failobj=None):\n        \"\"\"Returns a list of all the matching values for key,\n        containing a single entry for unambiguous matches and\n        multiple entries for ambiguous matches.\"\"\"\n        if self.mmkeys is None: self._mmInit()\n        k = self.mmkeys.get(key)\n        if not k: return failobj\n        return list(map(self.data.get, k))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of the full key names for all the matching values for key.", "response": "def getallkeys(self, key, failobj=None):\n        \"\"\"Returns a list of the full key names (not the items)\n        for all the matching values for key.  The list will\n        contain a single entry for unambiguous matches and\n        multiple entries for ambiguous matches.\"\"\"\n        if self.mmkeys is None: self._mmInit()\n        return self.mmkeys.get(key, failobj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a single entry from the cache if the key is not found or is ambiguous", "response": "def get(self, key, failobj=None, exact=0):\n\n        \"\"\"Returns failobj if key is not found or is ambiguous\"\"\"\n\n        if not exact:\n            try:\n                key = self.getfullkey(key)\n            except KeyError:\n                return failobj\n        return self.data.get(key,failobj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if the key is found in the data.", "response": "def _has(self, key, exact=0):\n\n        \"\"\"Returns false if key is not found or is ambiguous\"\"\"\n\n        if not exact:\n            try:\n                key = self.getfullkey(key)\n                return 1\n            except KeyError:\n                return 0\n        else:\n            return key in self.data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning true if this parameter is learned by the user.", "response": "def isLearned(self, mode=None):\n        \"\"\"Return true if this parameter is learned\n\n        Hidden parameters are not learned; automatic parameters inherit\n        behavior from package/cl; other parameters are learned.\n        If mode is set, it determines how automatic parameters behave.\n        If not set, cl.mode parameter determines behavior.\n        \"\"\"\n        if \"l\" in self.mode: return 1\n        if \"h\" in self.mode: return 0\n        if \"a\" in self.mode:\n            if mode is None: mode = 'ql' # that is, iraf.cl.mode\n            if \"h\" in mode and \"l\" not in mode:\n                return 0\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, field=None, index=None, lpar=0, prompt=1, native=0, mode=\"h\"):\n\n        if field and field != \"p_value\":\n            # note p_value comes back to this routine, so shortcut that case\n            return self._getField(field,native=native,prompt=prompt)\n\n        # may prompt for value if prompt flag is set\n        if prompt: self._optionalPrompt(mode)\n\n        if index is not None:\n            raise SyntaxError(\"Parameter \"+self.name+\" is not an array\")\n\n        if native:\n            rv = self.value\n        else:\n            rv = self.toString(self.value)\n        return rv", "response": "Return value of this parameter as a string or in native format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, value, field=None, index=None, check=1):\n\n        if index is not None:\n            raise SyntaxError(\"Parameter \"+self.name+\" is not an array\")\n\n        if field:\n            self._setField(value,field,check=check)\n        else:\n            if check:\n                self.value = self.checkValue(value)\n            else:\n                self.value = self._coerceValue(value)\n            self.setChanged()", "response": "Set value of this parameter from a string or other value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking and converts a parameter value. Returns an exception if the value is not permitted for this parameter.", "response": "def checkValue(self,value,strict=0):\n        \"\"\"Check and convert a parameter value.\n\n        Raises an exception if the value is not permitted for this\n        parameter.  Otherwise returns the value (converted to the\n        right type.)\n        \"\"\"\n        v = self._coerceValue(value,strict)\n        return self.checkOneValue(v,strict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkOneValue(self,v,strict=0):\n        if v in [None, INDEF] or (isinstance(v,str) and v[:1] == \")\"):\n            return v\n        elif v == \"\":\n            # most parameters treat null string as omitted value\n            return None\n        elif self.choice is not None and v not in self.choiceDict:\n            schoice = list(map(self.toString, self.choice))\n            schoice = \"|\".join(schoice)\n            raise ValueError(\"Parameter %s: \"\n                    \"value %s is not in choice list (%s)\" %\n                    (self.name, str(v), schoice))\n        elif (self.min not in [None, INDEF] and v<self.min):\n            raise ValueError(\"Parameter %s: \"\n                    \"value `%s' is less than minimum `%s'\" %\n                    (self.name, str(v), str(self.min)))\n        elif (self.max not in [None, INDEF] and v>self.max):\n            raise ValueError(\"Parameter %s: \"\n                    \"value `%s' is greater than maximum `%s'\" %\n                    (self.name, str(v), str(self.max)))\n        return v", "response": "Checks a single value to see if it is in range or choice list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dpar(self, cl=1):\n        sval = self.toString(self.value, quoted=1)\n        if not cl:\n            if sval == \"\": sval = \"None\"\n        s = \"%s = %s\" % (self.name, sval)\n        return s", "response": "Return a dpar - style executable assignment for the current parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pretty(self,verbose=0):\n        # split prompt lines and add blanks in later lines to align them\n        plines = self.prompt.split('\\n')\n        for i in range(len(plines)-1): plines[i+1] = 32*' ' + plines[i+1]\n        plines = '\\n'.join(plines)\n        namelen = min(len(self.name), 12)\n        pvalue = self.get(prompt=0,lpar=1)\n        alwaysquoted = ['s', 'f', '*gcur', '*imcur', '*ukey', 'pset']\n        if self.type in alwaysquoted and self.value is not None: pvalue = '\"' + pvalue + '\"'\n        if self.mode == \"h\":\n            s = \"%13s = %-15s %s\" % (\"(\"+self.name[:namelen],\n                                    pvalue+\")\", plines)\n        else:\n            s = \"%13s = %-15s %s\" % (self.name[:namelen],\n                                    pvalue, plines)\n        if not verbose: return s\n\n        if self.choice is not None:\n            s = s + \"\\n\" + 32*\" \" + \"|\"\n            nline = 33\n            for i in range(len(self.choice)):\n                sch = str(self.choice[i]) + \"|\"\n                s = s + sch\n                nline = nline + len(sch) + 1\n                if nline > 80:\n                    s = s + \"\\n\" + 32*\" \" + \"|\"\n                    nline = 33\n        elif self.min not in [None, INDEF] or self.max not in [None, INDEF]:\n            s = s + \"\\n\" + 32*\" \"\n            if self.min not in [None, INDEF]:\n                s = s + str(self.min) + \" <= \"\n            s = s + self.name\n            if self.max not in [None, INDEF]:\n                s = s + \" <= \" + str(self.max)\n        return s", "response": "Return pretty list description of parameter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning par format string for this parameter.", "response": "def save(self, dolist=0):\n        \"\"\"Return .par format string for this parameter\n\n        If dolist is set, returns fields as a list of strings.  Default\n        is to return a single string appropriate for writing to a file.\n        \"\"\"\n        quoted = not dolist\n        fields = 7*[\"\"]\n        fields[0] = self.name\n        fields[1] = self.type\n        fields[2] = self.mode\n        fields[3] = self.toString(self.value,quoted=quoted)\n        if self.choice is not None:\n            schoice = list(map(self.toString, self.choice))\n            schoice.insert(0,'')\n            schoice.append('')\n            fields[4] = repr('|'.join(schoice))\n        elif self.min not in [None,INDEF]:\n            fields[4] = self.toString(self.min,quoted=quoted)\n        if self.max not in [None,INDEF]:\n            fields[5] = self.toString(self.max,quoted=quoted)\n        if self.prompt:\n            if quoted:\n                sprompt = repr(self.prompt)\n            else:\n                sprompt = self.prompt\n            # prompt can have embedded newlines (which are printed)\n            sprompt = sprompt.replace(r'\\012', '\\n')\n            sprompt = sprompt.replace(r'\\n', '\\n')\n            fields[6] = sprompt\n        # delete trailing null parameters\n        for i in [6,5,4]:\n            if fields[i] != \"\": break\n            del fields[i]\n        if dolist:\n            return fields\n        else:\n            return ','.join(fields)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setChoice(self,s,strict=0):\n        clist = _getChoice(s,strict)\n        self.choice = list(map(self._coerceValue, clist))\n        self._setChoiceDict()", "response": "Set the choice parameter from string s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setChoiceDict(self):\n        # value is name of choice parameter (same as key)\n        self.choiceDict = {}\n        for c in self.choice: self.choiceDict[c] = c", "response": "Create dictionary for choice list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the p_filename field for this parameter", "response": "def _getPFilename(self,native,prompt):\n        \"\"\"Get p_filename field for this parameter\n\n        Same as get for non-list params\n        \"\"\"\n        return self.get(native=native,prompt=prompt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a parameter field value", "response": "def _getField(self, field, native=0, prompt=1):\n        \"\"\"Get a parameter field value\"\"\"\n        try:\n            # expand field name using minimum match\n            field = _getFieldDict[field]\n        except KeyError as e:\n            # re-raise the exception with a bit more info\n            raise SyntaxError(\"Cannot get field \" + field +\n                    \" for parameter \" + self.name + \"\\n\" + str(e))\n        if field == \"p_value\":\n            # return value of parameter\n            # Note that IRAF returns the filename for list parameters\n            # when p_value is used.  I consider this a bug, and it does\n            # not appear to be used by any cl scripts or SPP programs\n            # in either IRAF or STSDAS.  It is also in conflict with\n            # the IRAF help documentation.  I am making p_value exactly\n            # the same as just a simple CL parameter reference.\n            return self.get(native=native,prompt=prompt)\n        elif field == \"p_name\": return self.name\n        elif field == \"p_xtype\": return self.type\n        elif field == \"p_type\": return self._getPType()\n        elif field == \"p_mode\": return self.mode\n        elif field == \"p_prompt\": return self.prompt\n        elif field == \"p_scope\": return self.scope\n        elif field == \"p_default\" or field == \"p_filename\":\n            # these all appear to be equivalent -- they just return the\n            # current PFilename of the parameter (which is the same as the value\n            # for non-list parameters, and is the filename for list parameters)\n            return self._getPFilename(native,prompt)\n        elif field == \"p_maximum\":\n            if native:\n                return self.max\n            else:\n                return self.toString(self.max)\n        elif field == \"p_minimum\":\n            if self.choice is not None:\n                if native:\n                    return self.choice\n                else:\n                    schoice = list(map(self.toString, self.choice))\n                    return \"|\" + \"|\".join(schoice) + \"|\"\n            else:\n                if native:\n                    return self.min\n                else:\n                    return self.toString(self.min)\n        else:\n            # XXX unimplemented fields:\n            # p_length: maximum string length in bytes -- what to do with it?\n            raise RuntimeError(\"Program bug in IrafPar._getField()\\n\" +\n                    \"Requested field \" + field + \" for parameter \" + self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setField(self, value, field, check=1):\n        try:\n            # expand field name using minimum match\n            field = _setFieldDict[field]\n        except KeyError as e:\n            raise SyntaxError(\"Cannot set field \" + field +\n                    \" for parameter \" + self.name + \"\\n\" + str(e))\n        if field == \"p_prompt\":\n            self.prompt = irafutils.removeEscapes(irafutils.stripQuotes(value))\n        elif field == \"p_value\":\n            self.set(value,check=check)\n        elif field == \"p_filename\":\n            # this is only relevant for list parameters (*imcur, *gcur, etc.)\n            self.set(value,check=check)\n        elif field == \"p_scope\":\n            self.scope = value\n        elif field == \"p_maximum\":\n            self.max = self._coerceOneValue(value)\n        elif field == \"p_minimum\":\n            if isinstance(value,str) and '|' in value:\n                self._setChoice(irafutils.stripQuotes(value))\n            else:\n                self.min = self._coerceOneValue(value)\n        elif field == \"p_mode\":\n            # not doing any type or value checking here -- setting mode is\n            # rare, so assume that it is being done correctly\n            self.mode = irafutils.stripQuotes(value)\n        else:\n            raise RuntimeError(\"Program bug in IrafPar._setField()\" +\n                    \"Requested field \" + field + \" for parameter \" + self.name)", "response": "Set a parameter field value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns par format string for this parameter.", "response": "def save(self, dolist=0):\n        \"\"\"Return .par format string for this parameter\n\n        If dolist is set, returns fields as a list of strings.  Default\n        is to return a single string appropriate for writing to a file.\n        \"\"\"\n        quoted = not dolist\n        array_size = 1\n        for d in self.shape:\n            array_size = d*array_size\n        ndim = len(self.shape)\n        fields = (7+2*ndim+len(self.value))*[\"\"]\n        fields[0] = self.name\n        fields[1] = self.type\n        fields[2] = self.mode\n        fields[3] = str(ndim)\n        next = 4\n        for d in self.shape:\n            fields[next] = str(d); next += 1\n            fields[next] = '1';    next += 1\n        nvstart = 7+2*ndim\n        if self.choice is not None:\n            schoice = list(map(self.toString, self.choice))\n            schoice.insert(0,'')\n            schoice.append('')\n            fields[nvstart-3] = repr('|'.join(schoice))\n        elif self.min not in [None,INDEF]:\n            fields[nvstart-3] = self.toString(self.min,quoted=quoted)\n        # insert an escaped line break before min field\n        if quoted:\n            fields[nvstart-3] = '\\\\\\n' + fields[nvstart-3]\n        if self.max not in [None,INDEF]:\n            fields[nvstart-2] = self.toString(self.max,quoted=quoted)\n        if self.prompt:\n            if quoted:\n                sprompt = repr(self.prompt)\n            else:\n                sprompt = self.prompt\n            # prompt can have embedded newlines (which are printed)\n            sprompt = sprompt.replace(r'\\012', '\\n')\n            sprompt = sprompt.replace(r'\\n', '\\n')\n            fields[nvstart-1] = sprompt\n        for i in range(len(self.value)):\n            fields[nvstart+i] = self.toString(self.value[i],quoted=quoted)\n        # insert an escaped line break before value fields\n        if dolist:\n            return fields\n        else:\n            fields[nvstart] = '\\\\\\n' + fields[nvstart]\n            return ','.join(fields)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dpar(self, cl=1):\n        sval = list(map(self.toString, self.value, len(self.value)*[1]))\n        for i in range(len(sval)):\n            if sval[i] == \"\":\n                sval[i] = \"None\"\n        s = \"%s = [%s]\" % (self.name, ', '.join(sval))\n        return s", "response": "Return dpar - style executable assignment for the current parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn value of this parameter as a string or in native format.", "response": "def get(self, field=None, index=None, lpar=0, prompt=1, native=0, mode=\"h\"):\n        \"\"\"Return value of this parameter as a string (or in native format\n        if native is non-zero.)\"\"\"\n\n        if field: return self._getField(field,native=native,prompt=prompt)\n\n        # may prompt for value if prompt flag is set\n        #XXX should change _optionalPrompt so we prompt for each element of\n        #XXX the array separately?  I think array parameters are\n        #XXX not useful as non-hidden params.\n\n        if prompt: self._optionalPrompt(mode)\n\n        if index is not None:\n            sumindex = self._sumindex(index)\n            try:\n                if native:\n                    return self.value[sumindex]\n                else:\n                    return self.toString(self.value[sumindex])\n            except IndexError:\n                # should never happen\n                raise SyntaxError(\"Illegal index [\" + repr(sumindex) +\n                        \"] for array parameter \" + self.name)\n        elif native:\n            # return object itself for an array because it is\n            # indexable, can have values assigned, etc.\n            return self\n        else:\n            # return blank-separated string of values for array\n            return str(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset value of this parameter from a string or other value.", "response": "def set(self, value, field=None, index=None, check=1):\n        \"\"\"Set value of this parameter from a string or other value.\n        Field is optional parameter field (p_prompt, p_minimum, etc.)\n        Index is optional array index (zero-based).  Set check=0 to\n        assign the value without checking to see if it is within\n        the min-max range or in the choice list.\"\"\"\n        if index is not None:\n            sumindex = self._sumindex(index)\n            try:\n                value = self._coerceOneValue(value)\n                if check:\n                    self.value[sumindex] = self.checkOneValue(value)\n                else:\n                    self.value[sumindex] = value\n                return\n            except IndexError:\n                # should never happen\n                raise SyntaxError(\"Illegal index [\" + repr(sumindex) +\n                        \"] for array parameter \" + self.name)\n        if field:\n            self._setField(value,field,check=check)\n        else:\n            if check:\n                self.value = self.checkValue(value)\n            else:\n                self.value = self._coerceValue(value)\n            self.setChanged()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck and convert a parameter value.", "response": "def checkValue(self,value,strict=0):\n        \"\"\"Check and convert a parameter value.\n\n        Raises an exception if the value is not permitted for this\n        parameter.  Otherwise returns the value (converted to the\n        right type.)\n        \"\"\"\n        v = self._coerceValue(value,strict)\n        for i in range(len(v)):\n            self.checkOneValue(v[i],strict=strict)\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sumindex(self, index=None):\n        try:\n            ndim = len(index)\n        except TypeError:\n            # turn index into a 1-tuple\n            index = (index,)\n            ndim = 1\n        if len(self.shape) != ndim:\n            raise ValueError(\"Index to %d-dimensional array %s has too %s dimensions\" %\n                (len(self.shape), self.name, [\"many\",\"few\"][len(self.shape) > ndim]))\n        sumindex = 0\n        for i in range(ndim-1,-1,-1):\n            index1 = index[i]\n            if index1 < 0 or index1 >= self.shape[i]:\n                raise ValueError(\"Dimension %d index for array %s is out of bounds (value=%d)\" %\n                    (i+1, self.name, index1))\n            sumindex = index1 + sumindex*self.shape[i]\n        return sumindex", "response": "Convert tuple index to 1 - D index into value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncoerce parameter to appropriate type.", "response": "def _coerceValue(self,value,strict=0):\n        \"\"\"Coerce parameter to appropriate type\n\n        Should accept None or null string.  Must be an array.\n        \"\"\"\n        try:\n            if isinstance(value,str):\n                # allow single blank-separated string as input\n                value = value.split()\n            if len(value) != len(self.value):\n                raise IndexError\n            v = len(self.value)*[0]\n            for i in range(len(v)):\n                v[i] = self._coerceOneValue(value[i],strict)\n            return v\n        except (IndexError, TypeError):\n            raise ValueError(\"Value must be a \" + repr(len(self.value)) +\n                    \"-element array for \" + self.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setChoiceDict(self):\n        # value is full name of choice parameter\n        self.choiceDict = minmatch.MinMatchDict()\n        for c in self.choice: self.choiceDict.add(c, c)", "response": "Create min - match dictionary for choice list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks initial attributes to make sure they are legal", "response": "def _checkAttribs(self, strict):\n        \"\"\"Check initial attributes to make sure they are legal\"\"\"\n        if self.min:\n            warning(\"Minimum value not allowed for boolean-type parameter \" +\n                    self.name, strict)\n            self.min = None\n        if self.max:\n            if not self.prompt:\n                warning(\"Maximum value not allowed for boolean-type parameter \" +\n                                self.name + \" (probably missing comma)\",\n                                strict)\n                # try to recover by assuming max string is prompt\n                self.prompt = self.max\n            else:\n                warning(\"Maximum value not allowed for boolean-type parameter \" +\n                        self.name, strict)\n            self.max = None\n        if self.choice:\n            warning(\"Choice values not allowed for boolean-type parameter \" +\n                    self.name, strict)\n            self.choice = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks initial attributes to make sure they are legal", "response": "def _checkAttribs(self, strict):\n        \"\"\"Check initial attributes to make sure they are legal\"\"\"\n        if self.choice:\n            warning(\"Choice values not allowed for real-type parameter \" +\n                    self.name, strict)\n            self.choice = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the device with the given data.", "response": "def patch(self, deviceId):\n        \"\"\"\n        Updates the device with the given data. Supports a json payload like\n        {\n            fs: newFs\n            samplesPerBatch: samplesPerBatch\n            gyroEnabled: true\n            gyroSensitivity: 500\n            accelerometerEnabled: true\n            accelerometerSensitivity: 2\n        }\n        A heartbeat is sent on completion of the request to ensure the analyser gets a rapid update.\n        :return: the device and 200 if the update was ok, 400 if not.\n        \"\"\"\n        try:\n            device = self.recordingDevices.get(deviceId)\n            if device.status == RecordingDeviceStatus.INITIALISED:\n                errors = self._handlePatch(device)\n                if len(errors) == 0:\n                    return device, 200\n                else:\n                    return device, 500\n            else:\n                return device, 400\n        finally:\n            logger.info(\"Sending adhoc heartbeat on device state update\")\n            self.heartbeater.sendHeartbeat()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if this is a legal date in the Gregorian calendar", "response": "def legal_date(year, month, day):\n    '''Check if this is a legal date in the Gregorian calendar'''\n    if month == 2:\n        daysinmonth = 29 if isleap(year) else 28\n    else:\n        daysinmonth = 30 if month in HAVE_30_DAYS else 31\n\n    if not (0 < day <= daysinmonth):\n        raise ValueError(\"Month {} doesn't have a day {}\".format(month, day))\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_jd(jd):\n    '''Return Gregorian date in a (Y, M, D) tuple'''\n    wjd = floor(jd - 0.5) + 0.5\n    depoch = wjd - EPOCH\n\n    quadricent = floor(depoch / INTERCALATION_CYCLE_DAYS)\n    dqc = depoch % INTERCALATION_CYCLE_DAYS\n\n    cent = floor(dqc / LEAP_SUPPRESSION_DAYS)\n    dcent = dqc % LEAP_SUPPRESSION_DAYS\n\n    quad = floor(dcent / LEAP_CYCLE_DAYS)\n    dquad = dcent % LEAP_CYCLE_DAYS\n\n    yindex = floor(dquad / YEAR_DAYS)\n    year = (\n        quadricent * INTERCALATION_CYCLE_YEARS +\n        cent * LEAP_SUPPRESSION_YEARS +\n        quad * LEAP_CYCLE_YEARS + yindex\n    )\n\n    if not (cent == 4 or yindex == 4):\n        year += 1\n\n    yearday = wjd - to_jd(year, 1, 1)\n\n    leap = isleap(year)\n\n    if yearday < 58 + leap:\n        leap_adj = 0\n    elif leap:\n        leap_adj = 1\n    else:\n        leap_adj = 2\n\n    month = floor((((yearday + leap_adj) * 12) + 373) / 367)\n    day = int(wjd - to_jd(year, month, 1)) + 1\n\n    return (year, month, day)", "response": "Return Gregorian date in a ( Y M D ) tuple"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring a new item in the cache if it is allowed in the hinge points. Returns True if it is stored False otherwise.", "response": "def storeFromHinge(self, name, hingePoints):\n        \"\"\"\n        Stores a new item in the cache if it is allowed in.\n        :param name: the name.\n        :param hingePoints: the hinge points. \n        :return: true if it is stored.\n        \"\"\"\n        if name not in self._cache:\n            if self._valid(hingePoints):\n                self._cache[name] = {'name': name, 'type': 'hinge', 'hinge': hingePoints}\n                self.writeCache()\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef storeFromWav(self, uploadCacheEntry, start, end):\n        prefix = uploadCacheEntry['name'] + '_' + start + '_' + end\n        match = next((x for x in self._cache.values() if x['type'] == 'wav' and x['name'].startswith(prefix)), None)\n        if match is None:\n            cached = [\n                {\n                    'name': prefix + '_' + n,\n                    'analysis': n,\n                    'start': start,\n                    'end': end,\n                    'type': 'wav',\n                    'filename': uploadCacheEntry['name']\n                } for n in ['spectrum', 'peakSpectrum']\n            ]\n            for cache in cached:\n                self._cache[cache['name']] = cache\n            self.writeCache()\n            return True\n        return False", "response": "Stores a new item in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the named entry in the cache.", "response": "def delete(self, name):\n        \"\"\"\n        Deletes the named entry in the cache.\n        :param name: the name.\n        :return: true if it is deleted.\n        \"\"\"\n        if name in self._cache:\n            del self._cache[name]\n            self.writeCache()\n            # TODO clean files\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef analyse(self, name):\n        if name in self._cache:\n            target = self._cache[name]\n            if target['type'] == 'wav':\n                signal = self._uploadController.loadSignal(target['filename'],\n                                                           start=target['start'] if target['start'] != 'start' else None,\n                                                           end=target['end'] if target['end'] != 'end' else None)\n                if signal is not None:\n                    # TODO allow user defined window\n                    return getattr(signal, target['analysis'])(ref=1.0)\n                else:\n                    return None, 404\n                pass\n            elif target['type'] == 'hinge':\n                hingePoints = np.array(target['hinge']).astype(np.float64)\n                x = hingePoints[:, 1]\n                y = hingePoints[:, 0]\n                # extend as straight line from 0 to 500\n                if x[0] != 0:\n                    x = np.insert(x, 0, 0.0000001)\n                    y = np.insert(y, 0, y[0])\n                if x[-1] != 500:\n                    x = np.insert(x, len(x), 500.0)\n                    y = np.insert(y, len(y), y[-1])\n                # convert the y axis dB values into a linear value\n                y = 10 ** (y / 10)\n                # perform a logspace interpolation\n                f = self.log_interp1d(x, y)\n                # remap to 0-500\n                xnew = np.linspace(x[0], x[-1], num=500, endpoint=False)\n                # and convert back to dB\n                return xnew, 10 * np.log10(f(xnew))\n            else:\n                logger.error('Unknown target type with name ' + name)\n        return None", "response": "This function reads the specified file and returns the frequency and Pxx analysis of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a log space 1d interpolation.", "response": "def log_interp1d(self, xx, yy, kind='linear'):\n        \"\"\"\n        Performs a log space 1d interpolation.\n        :param xx: the x values.\n        :param yy: the y values.\n        :param kind: the type of interpolation to apply (as per scipy interp1d)\n        :return: the interpolation function.\n        \"\"\"\n        logx = np.log10(xx)\n        logy = np.log10(yy)\n        lin_interp = interp1d(logx, logy, kind=kind)\n        log_interp = lambda zz: np.power(10.0, lin_interp(np.log10(zz)))\n        return log_interp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the Julian date equivalent for this date", "response": "def to_jd(year, month, day):\n    \"Retrieve the Julian date equivalent for this date\"\n    return day + (month - 1) * 30 + (year - 1) * 365 + floor(year / 4) + EPOCH - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_jd(jdc):\n    \"Create a new date from a Julian date.\"\n    cdc = floor(jdc) + 0.5 - EPOCH\n    year = floor((cdc - floor((cdc + 366) / 1461)) / 365) + 1\n\n    yday = jdc - to_jd(year, 1, 1)\n\n    month = floor(yday / 30) + 1\n    day = yday - (month - 1) * 30 + 1\n    return year, month, day", "response": "Create a new date from a Julian date."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the roll angle at the target position based on the Colin Cox algorithm.", "response": "def troll(roll, dec, v2, v3):\n    \"\"\" Computes the roll angle at the target position based on::\n\n            the roll angle at the V1 axis(roll),\n            the dec of the target(dec), and\n            the V2/V3 position of the aperture (v2,v3) in arcseconds.\n\n        Based on the algorithm provided by Colin Cox that is used in\n        Generic Conversion at STScI.\n    \"\"\"\n    # Convert all angles to radians\n    _roll = DEGTORAD(roll)\n    _dec = DEGTORAD(dec)\n    _v2 = DEGTORAD(v2 / 3600.)\n    _v3 = DEGTORAD(v3 / 3600.)\n\n    # compute components\n    sin_rho = sqrt((pow(sin(_v2),2)+pow(sin(_v3),2)) - (pow(sin(_v2),2)*pow(sin(_v3),2)))\n    rho = asin(sin_rho)\n    beta = asin(sin(_v3)/sin_rho)\n    if _v2 < 0: beta = pi - beta\n    gamma = asin(sin(_v2)/sin_rho)\n    if _v3 < 0: gamma = pi - gamma\n    A = pi/2. + _roll - beta\n    B = atan2( sin(A)*cos(_dec), (sin(_dec)*sin_rho - cos(_dec)*cos(rho)*cos(A)))\n\n    # compute final value\n    troll = RADTODEG(pi - (gamma+B))\n\n    return troll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_archive(self,format=True):\n        if len(list(self.orig_wcs.keys())) > 0:\n            block  = 'Original WCS keywords for ' + self.rootname+ '\\n'\n            block += '    backed up on '+repr(self.orig_wcs['WCSCDATE'])+'\\n'\n            if not format:\n                for key in self.wcstrans.keys():\n                    block += key.upper() + \" = \" + repr(self.get_archivekw(key)) + '\\n'\n                block = 'PA_V3: '+repr(self.pa_obs)+'\\n'\n\n            else:\n                block += 'CD_11  CD_12: '+repr(self.get_archivekw('CD1_1'))+'  '+repr(self.get_archivekw('CD1_2')) +'\\n'\n                block += 'CD_21  CD_22: '+repr(self.get_archivekw('CD2_1'))+'  '+repr(self.get_archivekw('CD2_2')) +'\\n'\n                block += 'CRVAL       : '+repr(self.get_archivekw('CRVAL1'))+'  '+repr(self.get_archivekw('CRVAL2')) + '\\n'\n                block += 'CRPIX       : '+repr(self.get_archivekw('CRPIX1'))+'  '+repr(self.get_archivekw('CRPIX2')) + '\\n'\n                block += 'NAXIS       : '+repr(int(self.get_archivekw('NAXIS1')))+'  '+repr(int(self.get_archivekw('NAXIS2'))) + '\\n'\n                block += 'Plate Scale : '+repr(self.get_archivekw('pixel scale'))+'\\n'\n                block += 'ORIENTAT    : '+repr(self.get_archivekw('ORIENTAT'))+'\\n'\n\n            print(block)", "response": "Prints out the archived WCS keywords."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_pscale(self):\n        if self.new:\n            self.pscale = 1.0\n        else:\n            self.pscale = self.compute_pscale(self.cd11,self.cd21)", "response": "Compute the pixel scale based on active WCS values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_pscale(self,cd11,cd21):\n        return N.sqrt(N.power(cd11,2)+N.power(cd21,2)) * 3600.", "response": "Compute the pixel scale based on active WCS values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_orient(self):\n        self.orient = RADTODEG(N.arctan2(self.cd12,self.cd22))", "response": "Return the computed orientation based on CD matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the WCS of a specific entry in the WCS.", "response": "def updateWCS(self, pixel_scale=None, orient=None,refpos=None,refval=None,size=None):\n        \"\"\"\n        Create a new CD Matrix from the absolute pixel scale\n        and reference image orientation.\n        \"\"\"\n        # Set up parameters necessary for updating WCS\n        # Check to see if new value is provided,\n        # If not, fall back on old value as the default\n\n        _updateCD = no\n        if orient is not None and orient != self.orient:\n            pa = DEGTORAD(orient)\n            self.orient = orient\n            self._orient_lin = orient\n            _updateCD = yes\n        else:\n            # In case only pixel_scale was specified\n            pa = DEGTORAD(self.orient)\n\n        if pixel_scale is not None and pixel_scale != self.pscale:\n            _ratio = pixel_scale / self.pscale\n            self.pscale = pixel_scale\n            _updateCD = yes\n        else:\n            # In case, only orient was specified\n            pixel_scale = self.pscale\n            _ratio = None\n\n        # If a new plate scale was given,\n        # the default size should be revised accordingly\n        # along with the default reference pixel position.\n        # Added 31 Mar 03, WJH.\n        if _ratio is not None:\n            self.naxis1 /= _ratio\n            self.naxis2 /= _ratio\n            self.crpix1 = self.naxis1/2.\n            self.crpix2 = self.naxis2/2.\n\n        # However, if the user provides a given size,\n        # set it to use that no matter what.\n        if size is not None:\n            self.naxis1 = size[0]\n            self.naxis2 = size[1]\n\n        # Insure that naxis1,2 always return as integer values.\n        self.naxis1 = int(self.naxis1)\n        self.naxis2 = int(self.naxis2)\n\n        if refpos is not None:\n            self.crpix1 = refpos[0]\n            self.crpix2 = refpos[1]\n        if self.crpix1 is None:\n            self.crpix1 = self.naxis1/2.\n            self.crpix2 = self.naxis2/2.\n\n        if refval is not None:\n            self.crval1 = refval[0]\n            self.crval2 = refval[1]\n\n        # Reset WCS info now...\n        if _updateCD:\n            # Only update this should the pscale or orientation change...\n            pscale = pixel_scale / 3600.\n\n            self.cd11 = -pscale * N.cos(pa)\n            self.cd12 = pscale * N.sin(pa)\n            self.cd21 = self.cd12\n            self.cd22 = -self.cd11\n\n        # Now make sure that all derived values are really up-to-date based\n        # on these changes\n        self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale_WCS(self,pixel_scale,retain=True):\n        ''' Scale the WCS to a new pixel_scale. The 'retain' parameter\n        [default value: True] controls whether or not to retain the original\n        distortion solution in the CD matrix.\n        '''\n        _ratio = pixel_scale / self.pscale\n\n        # Correct the size of the image and CRPIX values for scaled WCS\n        self.naxis1 /= _ratio\n        self.naxis2 /= _ratio\n        self.crpix1 = self.naxis1/2.\n        self.crpix2 = self.naxis2/2.\n\n        if retain:\n            # Correct the WCS while retaining original distortion information\n            self.cd11 *= _ratio\n            self.cd12 *= _ratio\n            self.cd21 *= _ratio\n            self.cd22 *= _ratio\n        else:\n            pscale = pixel_scale / 3600.\n            self.cd11 = -pscale * N.cos(pa)\n            self.cd12 = pscale * N.sin(pa)\n            self.cd21 = self.cd12\n            self.cd22 = -self.cd11\n\n        # Now make sure that all derived values are really up-to-date based\n        # on these changes\n        self.update()", "response": "Scale the WCS to a new pixel_scale. The retain parameter controls whether or not to retain the original WCS."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotateCD(self,orient):\n        # Determine where member CRVAL position falls in ref frame\n        # Find out whether this needs to be rotated to align with\n        # reference frame.\n\n        _delta = self.get_orient() - orient\n        if _delta == 0.:\n            return\n\n        # Start by building the rotation matrix...\n        _rot = fileutil.buildRotMatrix(_delta)\n        # ...then, rotate the CD matrix and update the values...\n        _cd = N.array([[self.cd11,self.cd12],[self.cd21,self.cd22]],dtype=N.float64)\n        _cdrot = N.dot(_cd,_rot)\n        self.cd11 = _cdrot[0][0]\n        self.cd12 = _cdrot[0][1]\n        self.cd21 = _cdrot[1][0]\n        self.cd22 = _cdrot[1][1]\n        self.orient = orient", "response": "Rotates the WCS CD matrix to new orientation given by orient."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self,fitsname=None,wcs=None,archive=True,overwrite=False,quiet=True):\n        ## Start by making sure all derived values are in sync with CD matrix\n        self.update()\n\n        image = self.rootname\n        _fitsname = fitsname\n\n        if image.find('.fits') < 0 and _fitsname is not None:\n            # A non-FITS image was provided, and openImage made a copy\n            # Update attributes to point to new copy instead\n            self.geisname = image\n            image = self.rootname = _fitsname\n\n        # Open image as writable FITS object\n        fimg = fileutil.openImage(image, mode='update', fitsname=_fitsname)\n\n        _root,_iextn = fileutil.parseFilename(image)\n        _extn = fileutil.getExtn(fimg,_iextn)\n\n        # Write out values to header...\n        if wcs:\n            _wcsobj = wcs\n        else:\n            _wcsobj = self\n\n        for key in _wcsobj.wcstrans.keys():\n            _dkey = _wcsobj.wcstrans[key]\n            if _dkey != 'pscale':\n                _extn.header[key] = _wcsobj.__dict__[_dkey]\n\n        # Close the file\n        fimg.close()\n        del fimg\n        if archive:\n            self.write_archive(fitsname=fitsname,overwrite=overwrite,quiet=quiet)", "response": "Write out the values of the WCS keywords to the file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore(self):\n        # If there are no backup keys, do nothing...\n        if len(list(self.backup.keys())) == 0:\n            return\n        for key in self.backup.keys():\n            if key != 'WCSCDATE':\n                self.__dict__[self.wcstrans[key]] = self.orig_wcs[self.backup[key]]\n\n        self.update()", "response": "Reset the active WCS keywords to values stored in the the\n            backup keywords."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates backup copies of the WCS keywords with the given prepended string.", "response": "def archive(self,prepend=None,overwrite=no,quiet=yes):\n        \"\"\" Create backup copies of the WCS keywords with the given prepended\n            string.\n            If backup keywords are already present, only update them if\n            'overwrite' is set to 'yes', otherwise, do warn the user and do nothing.\n            Set the WCSDATE at this time as well.\n        \"\"\"\n        # Verify that existing backup values are not overwritten accidentally.\n        if len(list(self.backup.keys())) > 0 and overwrite == no:\n            if not quiet:\n                print('WARNING: Backup WCS keywords already exist! No backup made.')\n                print('         The values can only be overridden if overwrite=yes.')\n            return\n\n        # Establish what prepend string to use...\n        if prepend is None:\n            if self.prepend is not None:\n                _prefix = self.prepend\n            else:\n                _prefix = DEFAULT_PREFIX\n        else:\n            _prefix = prepend\n\n        # Update backup and orig_wcs dictionaries\n        # We have archive keywords and a defined prefix\n        # Go through and append them to self.backup\n        self.prepend = _prefix\n        for key in self.wcstrans.keys():\n            if key != 'pixel scale':\n                _archive_key = self._buildNewKeyname(key,_prefix)\n            else:\n                _archive_key = self.prepend.lower()+'pscale'\n#            if key != 'pixel scale':\n            self.orig_wcs[_archive_key] = self.__dict__[self.wcstrans[key]]\n            self.backup[key] = _archive_key\n            self.revert[_archive_key] = key\n\n        # Setup keyword to record when these keywords were backed up.\n        self.orig_wcs['WCSCDATE']= fileutil.getLTime()\n        self.backup['WCSCDATE'] = 'WCSCDATE'\n        self.revert['WCSCDATE'] = 'WCSCDATE'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting a copy of WCS keywords from an open file header and set the prefix used for those keywords.", "response": "def read_archive(self,header,prepend=None):\n        \"\"\" Extract a copy of WCS keywords from an open file header,\n            if they have already been created and remember the prefix\n            used for those keywords. Otherwise, setup the current WCS\n            keywords as the archive values.\n        \"\"\"\n        # Start by looking for the any backup WCS keywords to\n        # determine whether archived values are present and to set\n        # the prefix used.\n        _prefix = None\n        _archive = False\n        if header is not None:\n            for kw in header.items():\n                if kw[0][1:] in self.wcstrans.keys():\n                    _prefix = kw[0][0]\n                    _archive = True\n                    break\n\n        if not _archive:\n            self.archive(prepend=prepend)\n            return\n\n        # We have archive keywords and a defined prefix\n        # Go through and append them to self.backup\n        if _prefix is not None:\n            self.prepend = _prefix\n        else:\n            self.prepend = DEFAULT_PREFIX\n\n        for key in self.wcstrans.keys():\n            _archive_key = self._buildNewKeyname(key,_prefix)\n            if key!= 'pixel scale':\n                if _archive_key in header:\n                    self.orig_wcs[_archive_key] = header[_archive_key]\n                else:\n                    self.orig_wcs[_archive_key] = header[key]\n                self.backup[key] = _archive_key\n                self.revert[_archive_key] = key\n\n        # Establish plate scale value\n        _cd11str = self.prepend+'CD1_1'\n        _cd21str = self.prepend+'CD2_1'\n        pscale = self.compute_pscale(self.orig_wcs[_cd11str],self.orig_wcs[_cd21str])\n        _archive_key = self.prepend.lower()+'pscale'\n        self.orig_wcs[_archive_key] = pscale\n        self.backup['pixel scale'] = _archive_key\n        self.revert[_archive_key] = 'pixel scale'\n\n        # Setup keyword to record when these keywords were backed up.\n        if 'WCSCDATE' in header:\n            self.orig_wcs['WCSCDATE'] = header['WCSCDATE']\n        else:\n            self.orig_wcs['WCSCDATE'] = fileutil.getLTime()\n        self.backup['WCSCDATE'] = 'WCSCDATE'\n        self.revert['WCSCDATE'] = 'WCSCDATE'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting out the WCS keywords to the archive.", "response": "def write_archive(self,fitsname=None,overwrite=no,quiet=yes):\n        \"\"\" Saves a copy of the WCS keywords from the image header\n            as new keywords with the user-supplied 'prepend'\n            character(s) prepended to the old keyword names.\n\n            If the file is a GEIS image and 'fitsname' is not None, create\n            a FITS copy and update that version; otherwise, raise\n            an Exception and do not update anything.\n\n        \"\"\"\n        _fitsname = fitsname\n\n        # Open image in update mode\n        #    Copying of GEIS images handled by 'openImage'.\n        fimg = fileutil.openImage(self.rootname,mode='update',fitsname=_fitsname)\n        if self.rootname.find('.fits') < 0 and _fitsname is not None:\n            # A non-FITS image was provided, and openImage made a copy\n            # Update attributes to point to new copy instead\n            self.geisname = self.rootname\n            self.rootname = _fitsname\n\n        # extract the extension ID being updated\n        _root,_iextn = fileutil.parseFilename(self.rootname)\n        _extn = fileutil.getExtn(fimg,_iextn)\n        if not quiet:\n            print('Updating archive WCS keywords for ',_fitsname)\n\n        # Write out values to header...\n        for key in self.orig_wcs.keys():\n            _comment = None\n            _dkey = self.revert[key]\n\n            # Verify that archive keywords will not be overwritten,\n            # unless overwrite=yes.\n            _old_key = key in _extn.header\n            if  _old_key == True and overwrite == no:\n                if not quiet:\n                    print('WCS keyword',key,' already exists! Not overwriting.')\n                continue\n\n            # No archive keywords exist yet in file, or overwrite=yes...\n            # Extract the value for the original keyword\n            if _dkey in _extn.header:\n\n                # Extract any comment string for the keyword as well\n                _indx_key = _extn.header.index(_dkey)\n                _full_key = _extn.header.cards[_indx_key]\n                if not quiet:\n                    print('updating ',key,' with value of: ',self.orig_wcs[key])\n                _extn.header[key] = (self.orig_wcs[key], _full_key.comment)\n\n        key = 'WCSCDATE'\n        if key not in _extn.header:\n            # Print out history keywords to record when these keywords\n            # were backed up.\n            _extn.header[key] = (self.orig_wcs[key], \"Time WCS keywords were copied.\")\n\n        # Close the now updated image\n        fimg.close()\n        del fimg"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef createReferenceWCS(self,refname,overwrite=yes):\n        hdu = self.createWcsHDU()\n        # If refname already exists, delete it to make way for new file\n        if os.path.exists(refname):\n            if overwrite==yes:\n                # Remove previous version and re-create with new header\n                os.remove(refname)\n                hdu.writeto(refname)\n            else:\n                # Append header to existing file\n                wcs_append = True\n                oldhdu = fits.open(refname, mode='append')\n                for e in oldhdu:\n                    if 'extname' in e.header and e.header['extname'] == 'WCS':\n                        wcs_append = False\n                if wcs_append == True:\n                    oldhdu.append(hdu)\n                oldhdu.close()\n                del oldhdu\n        else:\n            # No previous file, so generate new one from scratch\n            hdu.writeto(refname)\n\n        # Clean up\n        del hdu", "response": "Create a new WCS file for the NEW\n            specified image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef createWcsHDU(self):\n        hdu = fits.ImageHDU()\n        hdu.header['EXTNAME'] = 'WCS'\n        hdu.header['EXTVER'] = 1\n        # Now, update original image size information\n        hdu.header['WCSAXES'] = (2, \"number of World Coordinate System axes\")\n        hdu.header['NPIX1'] = (self.naxis1, \"Length of array axis 1\")\n        hdu.header['NPIX2'] = (self.naxis2, \"Length of array axis 2\")\n        hdu.header['PIXVALUE'] = (0.0, \"values of pixels in array\")\n\n        # Write out values to header...\n        excluded_keys = ['naxis1','naxis2']\n        for key in self.wcskeys:\n            _dkey = self.wcstrans[key]\n            if _dkey not in excluded_keys:\n                hdu.header[key] = self.__dict__[_dkey]\n\n\n        return hdu", "response": "Create a new WCS HDU that can be used to populate a reference WCS HDU."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(args=None):\n    logger = cfg.configureLogger()\n    for root, dirs, files in os.walk(cfg.dataDir):\n        for dir in dirs:\n            newDir = os.path.join(root, dir)\n            try:\n                os.removedirs(newDir)\n                logger.info(\"Deleted empty dir \" + str(newDir))\n            except:\n                pass\n\n    if cfg.useTwisted:\n        import logging\n        logger = logging.getLogger('analyser.twisted')\n        from twisted.internet import reactor\n        from twisted.web.resource import Resource\n        from twisted.web import static, server\n        from twisted.web.wsgi import WSGIResource\n        from twisted.application import service\n        from twisted.internet import endpoints\n\n        class ReactApp:\n            \"\"\"\n            Handles the react app (excluding the static dir).\n            \"\"\"\n\n            def __init__(self, path):\n                # TODO allow this to load when in debug mode even if the files don't exist\n                self.publicFiles = {f: static.File(os.path.join(path, f)) for f in os.listdir(path) if\n                                    os.path.exists(os.path.join(path, f))}\n                self.indexHtml = ReactIndex(os.path.join(path, 'index.html'))\n\n            def getFile(self, path):\n                \"\"\"\n                overrides getChild so it always just serves index.html unless the file does actually exist (i.e. is an\n                icon or something like that)\n                \"\"\"\n                return self.publicFiles.get(path.decode('utf-8'), self.indexHtml)\n\n        class ReactIndex(static.File):\n            \"\"\"\n            a twisted File which overrides getChild so it always just serves index.html (NB: this is a bit of a hack, \n            there is probably a more correct way to do this but...)\n            \"\"\"\n\n            def getChild(self, path, request):\n                return self\n\n        class FlaskAppWrapper(Resource):\n            \"\"\"\n            wraps the flask app as a WSGI resource while allow the react index.html (and its associated static content)\n            to be served as the default page.\n            \"\"\"\n\n            def __init__(self):\n                super().__init__()\n                self.wsgi = WSGIResource(reactor, reactor.getThreadPool(), app)\n                import sys\n                if getattr(sys, 'frozen', False):\n                    # pyinstaller lets you copy files to arbitrary locations under the _MEIPASS root dir\n                    uiRoot = sys._MEIPASS\n                else:\n                    # release script moves the ui under the analyser package because setuptools doesn't seem to include\n                    # files from outside the package\n                    uiRoot = os.path.dirname(__file__)\n                logger.info('Serving ui from ' + str(uiRoot))\n                self.react = ReactApp(os.path.join(uiRoot, 'ui'))\n                self.static = static.File(os.path.join(uiRoot, 'ui', 'static'))\n\n            def getChild(self, path, request):\n                \"\"\"\n                Overrides getChild to allow the request to be routed to the wsgi app (i.e. flask for the rest api \n                calls),\n                the static dir (i.e. for the packaged css/js etc), the various concrete files (i.e. the public \n                dir from react-app) or to index.html (i.e. the react app) for everything else.\n                :param path: \n                :param request: \n                :return: \n                \"\"\"\n                if path == b'api':\n                    request.prepath.pop()\n                    request.postpath.insert(0, path)\n                    return self.wsgi\n                elif path == b'static':\n                    return self.static\n                else:\n                    return self.react.getFile(path)\n\n            def render(self, request):\n                return self.wsgi.render(request)\n\n        application = service.Application('analyser')\n        site = server.Site(FlaskAppWrapper())\n        endpoint = endpoints.TCP4ServerEndpoint(reactor, cfg.getPort(), interface='0.0.0.0')\n        endpoint.listen(site)\n        reactor.run()\n    else:\n        # get config from a flask standard place not our config yml\n        app.run(debug=cfg.runInDebug(), host='0.0.0.0', port=cfg.getPort())", "response": "This is the main entry point for the react app."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef leap(year, method=None):\n    '''\n    Determine if this is a leap year in the FR calendar using one of three methods: 4, 100, 128\n    (every 4th years, every 4th or 400th but not 100th, every 4th but not 128th)\n    '''\n\n    method = method or 'equinox'\n\n    if year in (3, 7, 11):\n        return True\n    elif year < 15:\n        return False\n\n    if method in (4, 'continuous') or (year <= 16 and method in (128, 'madler', 4, 'continuous')):\n        return year % 4 == 3\n\n    elif method in (100, 'romme'):\n        return (year % 4 == 0 and year % 100 != 0) or year % 400 == 0\n\n    elif method in (128, 'madler'):\n        return year % 4 == 0 and year % 128 != 0\n\n    elif method == 'equinox':\n        # Is equinox on 366th day after (year, 1, 1)\n        startjd = to_jd(year, 1, 1, method='equinox')\n        if premier_da_la_annee(startjd + 367) - startjd == 366.0:\n            return True\n    else:\n        raise ValueError(\"Unknown leap year method. Try: continuous, romme, madler or equinox\")\n\n    return False", "response": "Determine if this is a leap year in the FR calendar."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef premier_da_la_annee(jd):\n    '''Determine the year in the French revolutionary calendar in which a given Julian day falls.\n    Returns Julian day number containing fall equinox (first day of the FR year)'''\n    p = ephem.previous_fall_equinox(dublin.from_jd(jd))\n    previous = trunc(dublin.to_jd(p) - 0.5) + 0.5\n\n    if previous + 364 < jd:\n        # test if current day is the equinox if the previous equinox was a long time ago\n        n = ephem.next_fall_equinox(dublin.from_jd(jd))\n        nxt = trunc(dublin.to_jd(n) - 0.5) + 0.5\n\n        if nxt <= jd:\n            return nxt\n\n    return previous", "response": "Determine the year in the French revolutionary calendar in which a given Julian day falls."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_jd(year, month, day, method=None):\n    '''Obtain Julian day from a given French Revolutionary calendar date.'''\n    method = method or 'equinox'\n\n    if day < 1 or day > 30:\n        raise ValueError(\"Invalid day for this calendar\")\n\n    if month > 13:\n        raise ValueError(\"Invalid month for this calendar\")\n\n    if month == 13 and day > 5 + leap(year, method=method):\n        raise ValueError(\"Invalid day for this month in this calendar\")\n\n    if method == 'equinox':\n        return _to_jd_equinox(year, month, day)\n\n    else:\n        return _to_jd_schematic(year, month, day, method)", "response": "Obtain Julian day from a given French Revolutionary calendar date."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates JD using various leap - year calculation methods.", "response": "def _to_jd_schematic(year, month, day, method):\n    '''Calculate JD using various leap-year calculation methods'''\n\n    y0, y1, y2, y3, y4, y5 = 0, 0, 0, 0, 0, 0\n\n    intercal_cycle_yrs, over_cycle_yrs, leap_suppression_yrs = None, None, None\n\n    # Use the every-four-years method below year 16 (madler) or below 15 (romme)\n    if ((method in (100, 'romme') and year < 15) or\n            (method in (128, 'madler') and year < 17)):\n        method = 4\n\n    if method in (4, 'continuous'):\n        # Leap years: 15, 19, 23, ...\n        y5 = -365\n\n    elif method in (100, 'romme'):\n        year = year - 13\n        y5 = DAYS_IN_YEAR * 12 + 3\n\n        leap_suppression_yrs = 100.\n        leap_suppression_days = 36524  # leap_cycle_days * 25 - 1\n\n        intercal_cycle_yrs = 400.\n        intercal_cycle_days = 146097  # leap_suppression_days * 4 + 1\n\n        over_cycle_yrs = 4000.\n        over_cycle_days = 1460969  # intercal_cycle_days * 10 - 1\n\n    elif method in (128, 'madler'):\n        year = year - 17\n        y5 = DAYS_IN_YEAR * 16 + 4\n\n        leap_suppression_days = 46751  # 32 * leap_cycle_days - 1\n        leap_suppression_yrs = 128\n\n    else:\n        raise ValueError(\"Unknown leap year method. Try: continuous, romme, madler or equinox\")\n\n    if over_cycle_yrs:\n        y0 = trunc(year / over_cycle_yrs) * over_cycle_days\n        year = year % over_cycle_yrs\n\n    # count intercalary cycles in days (400 years long or None)\n    if intercal_cycle_yrs:\n        y1 = trunc(year / intercal_cycle_yrs) * intercal_cycle_days\n        year = year % intercal_cycle_yrs\n\n    # count leap suppresion cycles in days (100 or 128 years long)\n    if leap_suppression_yrs:\n        y2 = trunc(year / leap_suppression_yrs) * leap_suppression_days\n        year = year % leap_suppression_yrs\n\n    y3 = trunc(year / LEAP_CYCLE_YEARS) * LEAP_CYCLE_DAYS\n    year = year % LEAP_CYCLE_YEARS\n\n    # Adjust 'year' by one to account for lack of year 0\n    y4 = year * DAYS_IN_YEAR\n\n    yj = y0 + y1 + y2 + y3 + y4 + y5\n\n    mj = (month - 1) * 30\n\n    return EPOCH + yj + mj + day - 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_jd(jd, method=None):\n    '''Calculate date in the French Revolutionary\n    calendar from Julian day.  The five or six\n    \"sansculottides\" are considered a thirteenth\n    month in the results of this function.'''\n    method = method or 'equinox'\n\n    if method == 'equinox':\n        return _from_jd_equinox(jd)\n\n    else:\n        return _from_jd_schematic(jd, method)", "response": "Calculate date in the French Revolutionary\n    calendar from Julian day."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _from_jd_schematic(jd, method):\n    '''Convert from JD using various leap-year calculation methods'''\n    if jd < EPOCH:\n        raise ValueError(\"Can't convert days before the French Revolution\")\n\n    # days since Epoch\n    J = trunc(jd) + 0.5 - EPOCH\n\n    y0, y1, y2, y3, y4, y5 = 0, 0, 0, 0, 0, 0\n    intercal_cycle_days = leap_suppression_days = over_cycle_days = None\n\n    # Use the every-four-years method below year 17\n    if (J <= DAYS_IN_YEAR * 12 + 3 and\n            method in (100, 'romme')) or (J <= DAYS_IN_YEAR * 17 + 4 and method in (128, 'madler')):\n        method = 4\n\n    # set p and r in Hatcher algorithm\n    if method in (4, 'continuous'):\n        # Leap years: 15, 19, 23, ...\n        # Reorganize so that leap day is last day of cycle\n        J = J + 365\n        y5 = - 1\n\n    elif method in (100, 'romme'):\n        # Year 15 is not a leap year\n        # Year 16 is leap, then multiples of 4, not multiples of 100, yes multiples of 400\n        y5 = 12\n        J = J - DAYS_IN_YEAR * 12 - 3\n\n        leap_suppression_yrs = 100.\n        leap_suppression_days = 36524  # LEAP_CYCLE_DAYS * 25 - 1\n\n        intercal_cycle_yrs = 400.\n        intercal_cycle_days = 146097  # leap_suppression_days * 4 + 1\n\n        over_cycle_yrs = 4000.\n        over_cycle_days = 1460969  # intercal_cycle_days * 10 - 1\n\n    elif method in (128, 'madler'):\n        # Year 15 is a leap year, then year 20 and multiples of 4, not multiples of 128\n        y5 = 16\n        J = J - DAYS_IN_YEAR * 16 - 4\n\n        leap_suppression_yrs = 128\n        leap_suppression_days = 46751  # 32 * leap_cycle_days - 1\n\n    else:\n        raise ValueError(\"Unknown leap year method. Try: continuous, romme, madler or equinox\")\n\n    if over_cycle_days:\n        y0 = trunc(J / over_cycle_days) * over_cycle_yrs\n        J = J % over_cycle_days\n\n    if intercal_cycle_days:\n        y1 = trunc(J / intercal_cycle_days) * intercal_cycle_yrs\n        J = J % intercal_cycle_days\n\n    if leap_suppression_days:\n        y2 = trunc(J / leap_suppression_days) * leap_suppression_yrs\n        J = J % leap_suppression_days\n\n    y3 = trunc(J / LEAP_CYCLE_DAYS) * LEAP_CYCLE_YEARS\n\n    if J % LEAP_CYCLE_DAYS == LEAP_CYCLE_DAYS - 1:\n        J = 1460\n    else:\n        J = J % LEAP_CYCLE_DAYS\n\n    # 0 <= J <= 1460\n    # J needs to be 365 here on leap days ONLY\n\n    y4 = trunc(J / DAYS_IN_YEAR)\n\n    if J == DAYS_IN_YEAR * 4:\n        y4 = y4 - 1\n        J = 365.0\n    else:\n        J = J % DAYS_IN_YEAR\n\n    year = y0 + y1 + y2 + y3 + y4 + y5\n\n    month = trunc(J / 30.)\n    J = J - month * 30\n\n    return year + 1, month + 1, trunc(J) + 1", "response": "Convert from Julian day using various leap - year calculation methods"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the FR day using the equinox as day 1", "response": "def _from_jd_equinox(jd):\n    '''Calculate the FR day using the equinox as day 1'''\n    jd = trunc(jd) + 0.5\n    equinoxe = premier_da_la_annee(jd)\n\n    an = gregorian.from_jd(equinoxe)[0] - YEAR_EPOCH\n    mois = trunc((jd - equinoxe) / 30.) + 1\n    jour = int((jd - equinoxe) % 30) + 1\n\n    return (an, mois, jour)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain Julian day for Indian Civil date", "response": "def to_jd(year, month, day):\n    '''Obtain Julian day for Indian Civil date'''\n\n    gyear = year + 78\n    leap = isleap(gyear)\n    # // Is this a leap year ?\n\n    # 22 - leap = 21 if leap, 22 non-leap\n    start = gregorian.to_jd(gyear, 3, 22 - leap)\n    if leap:\n        Caitra = 31\n    else:\n        Caitra = 30\n\n    if month == 1:\n        jd = start + (day - 1)\n    else:\n        jd = start + Caitra\n        m = month - 2\n        m = min(m, 5)\n        jd += m * 31\n        if month >= 8:\n            m = month - 7\n            jd += m * 30\n\n        jd += day - 1\n\n    return jd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_jd(jd):\n    '''Calculate Indian Civil date from Julian day\n    Offset in years from Saka era to Gregorian epoch'''\n\n    start = 80\n    # Day offset between Saka and Gregorian\n\n    jd = trunc(jd) + 0.5\n    greg = gregorian.from_jd(jd)  # Gregorian date for Julian day\n    leap = isleap(greg[0])  # Is this a leap year?\n    # Tentative year in Saka era\n    year = greg[0] - SAKA_EPOCH\n    # JD at start of Gregorian year\n    greg0 = gregorian.to_jd(greg[0], 1, 1)\n    yday = jd - greg0  # Day number (0 based) in Gregorian year\n\n    if leap:\n        Caitra = 31  # Days in Caitra this year\n    else:\n        Caitra = 30\n\n    if yday < start:\n        # Day is at the end of the preceding Saka year\n        year -= 1\n        yday += Caitra + (31 * 5) + (30 * 3) + 10 + start\n\n    yday -= start\n    if yday < Caitra:\n        month = 1\n        day = yday + 1\n    else:\n        mday = yday - Caitra\n        if (mday < (31 * 5)):\n            month = trunc(mday / 31) + 2\n            day = (mday % 31) + 1\n        else:\n            mday -= 31 * 5\n            month = trunc(mday / 30) + 7\n            day = (mday % 30) + 1\n\n    return (year, month, int(day))", "response": "Calculate Indian Civil date from Julian day in Saka era to Gregorian epoch"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_stack(skip=0, length=6, _sep=os.path.sep):\n    return ' < '.join(\"%s:%s:%s\" % (\n        '/'.join(f.f_code.co_filename.split(_sep)[-2:]),\n        f.f_lineno,\n        f.f_code.co_name\n    ) for f in islice(frame_iterator(sys._getframe(1 + skip)), length))", "response": "Returns a one - line string with the current callstack."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecorate `func` to have logging. Args func (function): Function to decorate. If missing log returns a partial which you can use as a decorator. stacktrace (int): Number of frames to show. stacktrace_align (int): Column to align the framelist to. attributes (list): List of instance attributes to show, in case the function is a instance method. module (bool): Show the module. call (bool): If ``True``, then show calls. If ``False`` only show the call details on exceptions (if ``exception`` is enabled) (default: ``True``) call_args (bool): If ``True``, then show call arguments. (default: ``True``) call_args_repr (bool): Function to convert one argument to a string. (default: ``repr``) result (bool): If ``True``, then show result. (default: ``True``) exception (bool): If ``True``, then show exceptions. (default: ``True``) exception_repr (function): Function to convert an exception to a string. (default: ``repr``) result_repr (function): Function to convert the result object to a string. (default: ``strip_non_ascii`` - like ``str`` but nonascii characters are replaced with dots.) use_logging (string): Emit log messages with the given loglevel. (default: ``\"CRITICAL\"``) print_to (fileobject): File object to write to, in case you don't want to use logging module. (default: ``None`` - printing is disabled) Returns: A decorator or a wrapper. Example:: >>> @log(print_to=sys.stdout) ... def a(weird=False): ... if weird: ... raise RuntimeError('BOOM!') >>> a() a() <<< ... a => None >>> try: ... a(weird=True) ... except Exception: ... pass # naughty code! a(weird=True) <<< ... a ~ raised RuntimeError('BOOM!',) You can conveniently use this to logs just errors, or just results, example:: >>> import aspectlib >>> with aspectlib.weave(float, log(call=False, result=False, print_to=sys.stdout)): ... try: ... float('invalid') ... except Exception as e: ... pass # naughty code! float('invalid') <<< ... float ~ raised ValueError(...float...invalid...) This makes debugging naughty code easier. PS: Without the weaving it looks like this:: >>> try: ... log(call=False, result=False, print_to=sys.stdout)(float)('invalid') ... except Exception: ... pass # naughty code! float('invalid') <<< ... float ~ raised ValueError(...float...invalid...) .. versionchanged:: 0.5.0 Renamed `arguments` to `call_args`. Renamed `arguments_repr` to `call_args_repr`. Added `call` option.", "response": "def log(func=None,\n        stacktrace=10,\n        stacktrace_align=60,\n        attributes=(),\n        module=True,\n        call=True,\n        call_args=True,\n        call_args_repr=repr,\n        result=True,\n        exception=True,\n        exception_repr=repr,\n        result_repr=strip_non_ascii,\n        use_logging='CRITICAL',\n        print_to=None):\n    \"\"\"\n    Decorates `func` to have logging.\n\n    Args\n        func (function):\n            Function to decorate. If missing log returns a partial which you can use as a decorator.\n        stacktrace (int):\n            Number of frames to show.\n        stacktrace_align (int):\n            Column to align the framelist to.\n        attributes (list):\n            List of instance attributes to show, in case the function is a instance method.\n        module (bool):\n            Show the module.\n        call (bool):\n            If ``True``, then show calls. If ``False`` only show the call details on exceptions (if ``exception`` is\n            enabled) (default: ``True``)\n        call_args (bool):\n            If ``True``, then show call arguments. (default: ``True``)\n        call_args_repr (bool):\n            Function to convert one argument to a string. (default: ``repr``)\n        result (bool):\n            If ``True``, then show result. (default: ``True``)\n        exception (bool):\n            If ``True``, then show exceptions. (default: ``True``)\n        exception_repr (function):\n            Function to convert an exception to a string. (default: ``repr``)\n        result_repr (function):\n            Function to convert the result object to a string. (default: ``strip_non_ascii`` - like ``str`` but nonascii\n            characters are replaced with dots.)\n        use_logging (string):\n            Emit log messages with the given loglevel. (default: ``\"CRITICAL\"``)\n        print_to (fileobject):\n            File object to write to, in case you don't want to use logging module. (default: ``None`` - printing is\n            disabled)\n\n    Returns:\n        A decorator or a wrapper.\n\n    Example::\n\n        >>> @log(print_to=sys.stdout)\n        ... def a(weird=False):\n        ...     if weird:\n        ...         raise RuntimeError('BOOM!')\n        >>> a()\n        a()                                                           <<< ...\n        a => None\n        >>> try:\n        ...     a(weird=True)\n        ... except Exception:\n        ...     pass # naughty code!\n        a(weird=True)                                                 <<< ...\n        a ~ raised RuntimeError('BOOM!',)\n\n    You can conveniently use this to logs just errors, or just results, example::\n\n        >>> import aspectlib\n        >>> with aspectlib.weave(float, log(call=False, result=False, print_to=sys.stdout)):\n        ...     try:\n        ...         float('invalid')\n        ...     except Exception as e:\n        ...         pass # naughty code!\n        float('invalid')                                              <<< ...\n        float ~ raised ValueError(...float...invalid...)\n\n    This makes debugging naughty code easier.\n\n    PS: Without the weaving it looks like this::\n\n        >>> try:\n        ...     log(call=False, result=False, print_to=sys.stdout)(float)('invalid')\n        ... except Exception:\n        ...     pass # naughty code!\n        float('invalid')                                              <<< ...\n        float ~ raised ValueError(...float...invalid...)\n\n\n    .. versionchanged:: 0.5.0\n\n        Renamed `arguments` to `call_args`.\n        Renamed `arguments_repr` to `call_args_repr`.\n        Added `call` option.\n    \"\"\"\n\n    loglevel = use_logging and (\n        logging._levelNames if hasattr(logging, '_levelNames') else logging._nameToLevel\n    ).get(use_logging, logging.CRITICAL)\n    _missing = object()\n\n    def dump(buf):\n        try:\n            if use_logging:\n                logger._log(loglevel, buf, ())\n            if print_to:\n                buf += '\\n'\n                print_to.write(buf)\n        except Exception as exc:\n            logger.critical('Failed to log a message: %s', exc, exc_info=True)\n\n    class __logged__(Aspect):\n        __slots__ = 'cutpoint_function', 'final_function', 'binding', '__name__', '__weakref__'\n\n        bind = False\n\n        def __init__(self, cutpoint_function, binding=None):\n            mimic(self, cutpoint_function)\n            self.cutpoint_function = cutpoint_function\n            self.final_function = super(__logged__, self).__call__(cutpoint_function)\n            self.binding = binding\n\n        def __get__(self, instance, owner):\n            return __logged__(self.cutpoint_function.__get__(instance, owner), instance)\n\n        def __call__(self, *args, **kwargs):\n            return self.final_function(*args, **kwargs)\n\n        def advising_function(self, *args, **kwargs):\n            name = self.cutpoint_function.__name__\n            instance = self.binding\n            if instance is not None:\n                if isinstance(instance, InstanceType):\n                    instance_type = instance.__class__\n                else:\n                    instance_type = type(instance)\n\n                info = []\n                for key in attributes:\n                    if key.endswith('()'):\n                        callarg = key = key.rstrip('()')\n                    else:\n                        callarg = False\n                    val = getattr(instance, key, _missing)\n                    if val is not _missing and key != name:\n                        info.append(' %s=%s' % (\n                            key, call_args_repr(val() if callarg else val)\n                        ))\n                sig = buf = '{%s%s%s}.%s' % (\n                    instance_type.__module__ + '.' if module else '',\n                    instance_type.__name__,\n                    ''.join(info),\n                    name\n                )\n            else:\n                sig = buf = name\n            if call_args:\n                buf += '(%s%s)' % (\n                    ', '.join(repr(i) for i in (args if call_args is True else args[:call_args])),\n                    ((', ' if args else '') + ', '.join('%s=%r' % i for i in kwargs.items()))\n                    if kwargs and call_args is True\n                    else '',\n                )\n            if stacktrace:\n                buf = (\"%%-%ds  <<< %%s\" % stacktrace_align) % (buf, format_stack(skip=1, length=stacktrace))\n            if call:\n                dump(buf)\n            try:\n                res = yield\n            except Exception as exc:\n                if exception:\n                    if not call:\n                        dump(buf)\n                    dump('%s ~ raised %s' % (sig, exception_repr(exc)))\n                raise\n\n            if result:\n                dump('%s => %s' % (sig, result_repr(res)))\n\n    if func:\n        return __logged__(func)\n    else:\n        return __logged__"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwire handlers to the active device.", "response": "def wireHandlers(cfg):\n    \"\"\"\n    If the device is configured to run against a remote server, ping that device on a scheduled basis with our current\n    state.\n    :param cfg: the config object.\n    :return:\n    \"\"\"\n    logger = logging.getLogger('recorder')\n    httpPoster = cfg.handlers.get('remote')\n    csvLogger = cfg.handlers.get('local')\n    activeHandler = None\n    if httpPoster is None:\n        if csvLogger is None:\n            logger.warning(\"App is running with discard handler only, ALL DATA WILL BE DISCARDED!!!\")\n        else:\n            logger.info(\"App is running in standalone mode, logging data to local filesystem\")\n            activeHandler = csvLogger\n    else:\n        logger.info(\"App is running against remote server, logging data to \" + httpPoster.target)\n        activeHandler = httpPoster\n        heartbeater.serverURL = httpPoster.target\n        heartbeater.ping()\n\n    if activeHandler is not None:\n        for device in cfg.recordingDevices.values():\n            if activeHandler is httpPoster:\n                httpPoster.deviceName = device.name\n            copied = copy.copy(activeHandler)\n            device.dataHandler = copied if not cfg.useAsyncHandlers else AsyncHandler('recorder', copied)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, deviceId):\n        measurementsByName = self.measurements.get(deviceId)\n        if measurementsByName is None:\n            return []\n        else:\n            return list(measurementsByName.values())", "response": "Returns a list of all known active measurements for a device."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the specific measurement details", "response": "def get(self, deviceId, measurementId):\n        \"\"\"\n        details the specific measurement.\n        \"\"\"\n        record = self.measurements.get(deviceId)\n        if record is not None:\n            return record.get(measurementId)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscheduling a new measurement at the specified time.", "response": "def put(self, deviceId, measurementId):\n        \"\"\"\n        Schedules a new measurement at the specified time.\n        :param deviceId: the device to measure.\n        :param measurementId: the name of the measurement.\n        :return: 200 if it was scheduled, 400 if the device is busy, 500 if the device is bad.\n        \"\"\"\n        record = self.measurements.get(deviceId)\n        if record is not None:\n            measurement = record.get(measurementId)\n            if measurement is not None:\n                if len([x.name for x in measurement.statuses if x.name is 'COMPLETE' or x.name is 'FAILED']) > 0:\n                    logger.info('Overwriting existing completed measurement ' + x.name)\n                    measurement = None\n            if measurement is None:\n                logger.info('Initiating measurement ' + measurementId)\n                measurement = ScheduledMeasurement(measurementId, self.recordingDevices.get(deviceId))\n                body = request.get_json()\n                duration_ = body['duration']\n                def _cleanup():\n                    logger.info('Removing ' + measurementId + ' from ' + deviceId)\n                    record.pop(measurementId)\n                measurement.schedule(duration_, at=body.get('at'), delay=body.get('delay'), callback=_cleanup)\n                # a quick hack to enable the measurement to be cleaned up by the ScheduledMeasurement\n                record[measurementId] = measurement\n                return measurement, 200\n            else:\n                return measurement, 400\n        else:\n            return 'unknown device ' + deviceId, 400"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a stored measurement.", "response": "def delete(self, deviceId, measurementId):\n        \"\"\"\n        Deletes a stored measurement.\n        :param deviceId: the device to measure.\n        :param measurementId: the name of the measurement.\n        :return: 200 if it was deleted, 400 if no such measurement (or device).\n        \"\"\"\n        record = self.measurements.get(deviceId)\n        if record is not None:\n            popped = record.pop(measurementId, None)\n            return popped, 200 if popped else 400\n        return None, 400"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsignals a stop for the given measurement.", "response": "def get(self, deviceId, measurementId):\n        \"\"\"\n        signals a stop for the given measurement.\n        :param deviceId: the device to measure.\n        :param measurementId: the name of the measurement.\n        :return: 200 if stop is signalled, 400 if it doesn't exist or is not running.\n        \"\"\"\n        record = self.measurements.get(deviceId)\n        if record is not None:\n            measurement = record.get(measurementId)\n            if measurement.recording:\n                device = self.recordingDevices.get(deviceId)\n                device.signalStop()\n                return measurement, 200\n            else:\n                return measurement, 400\n        return '', 400"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nschedule the measurement asynchronously.", "response": "def schedule(self, duration, at=None, delay=None, callback=None):\n        \"\"\"\n        schedules the measurement (to execute asynchronously).\n        :param duration: how long to run for.\n        :param at: the time to start at.\n        :param delay: the time to wait til starting (use at or delay).\n        :param callback: a callback.\n        :return: nothing.\n        \"\"\"\n        delay = self.calculateDelay(at, delay)\n        self.callback = callback\n        logger.info('Initiating measurement ' + self.name + ' for ' + str(duration) + 's in ' + str(delay) + 's')\n        self.statuses.append({'name': ScheduledMeasurementStatus.SCHEDULED.name, 'time': datetime.utcnow()})\n        threading.Timer(delay, self.execute, [duration]).start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes the measurement recording the event status.", "response": "def execute(self, duration):\n        \"\"\"\n        Executes the measurement, recording the event status.\n        :param duration: the time to run for.\n        :return: nothing.\n        \"\"\"\n        self.statuses.append({'name': ScheduledMeasurementStatus.RUNNING.name, 'time': datetime.utcnow()})\n        try:\n            self.recording = True\n            self.device.start(self.name, durationInSeconds=duration)\n        finally:\n            self.recording = False\n        if self.device.status == RecordingDeviceStatus.FAILED:\n            self.statuses.append({'name': ScheduledMeasurementStatus.FAILED.name,\n                                  'time': datetime.utcnow(),\n                                  'reason': self.device.failureCode})\n        else:\n            self.statuses.append({'name': ScheduledMeasurementStatus.COMPLETE.name, 'time': datetime.utcnow()})\n        # this is a bit of a hack, need to remove this at some point by refactoring the way measurements are stored\n        if self.callback is not None:\n            self.callback()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculateDelay(self, at, delay):\n        if at is not None:\n            return max((datetime.strptime(at, DATETIME_FORMAT) - datetime.utcnow()).total_seconds(), 0)\n        elif delay is not None:\n            return delay\n        else:\n            return 0", "response": "Calculates the delay from now til the specified start time uses at if available."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of multiprocessing.Process objects which have not yet been started, this function launches them and blocks until the last finishes. This makes sure that only <pool_size> processes are ever working at any one time (this number does not include the main process which called this function, since that will not tax the CPU). The idea here is roughly analogous to multiprocessing.Pool with the exceptions that: 1 - The caller will get to use the multiprocessing.Process model of using shared memory (inheritance) to pass arg data to the child, 2 - maxtasksperchild is always 1, 3 - no function return value is kept/tranferred (not yet implemented)", "response": "def launch_and_wait(mp_proc_list, pool_size):\n    \"\"\" Given a list of multiprocessing.Process objects which have not yet\n    been started, this function launches them and blocks until the last\n    finishes.  This makes sure that only <pool_size> processes are ever\n    working at any one time (this number does not include the main process\n    which called this function, since that will not tax the CPU).\n    The idea here is roughly analogous to multiprocessing.Pool\n    with the exceptions that:\n        1 - The caller will get to use the multiprocessing.Process model of\n            using shared memory (inheritance) to pass arg data to the child,\n        2 - maxtasksperchild is always 1,\n        3 - no function return value is kept/tranferred (not yet implemented)\n    \"\"\"\n\n    # Sanity check\n    if len(mp_proc_list) < 1:\n        return\n\n    # Create or own list with easy state watching\n    procs = []\n    for p in mp_proc_list:\n        procs.append(WatchedProcess(p))\n\n    # Launch all of them, but only so pool_size are running at any time\n    keep_going = True\n    while (keep_going):\n        # Before we start any more, find out how many are running.  First go\n        # through the list of those started and see if alive.  Update state.\n        for p in procs:\n            if p.state == 1: # been started\n                if not p.process.is_alive():\n                    p.state = 2 # process has finished or been terminated\n                    assert p.process.exitcode is not None, \\\n                           \"Process is not alive but has no exitcode? \"+ \\\n                           str(p.process)\n\n        # now figure num_running\n        num_running = len([p for p in procs if p.state == 1])\n\n        # Start some.  Only as many as pool_size should ever be running.\n        num_avail_cpus = pool_size - num_running\n        num_to_start = len([p for p in procs if p.state == 0])\n        if num_to_start < 1:\n            # all have been started, can finally leave loop and go wait\n            break\n        if num_avail_cpus > 0 and num_to_start > 0:\n            num_to_start_now = min(num_avail_cpus, num_to_start)\n            started_now = 0\n            for p in procs:\n                if started_now < num_to_start_now and p.state == 0:\n                    p.start_process()\n                    # debug \"launch_and_wait: started: \"+str(p.process)\n                    started_now += 1\n        # else: otherwise, all cpus are in use, just wait ...\n\n        # sleep to tame loop activity, but also must sleep a bit after each\n        # start call so that the call to is_alive() woorks correctly\n        time.sleep(1)\n\n    # Out of the launching loop, can now wait on all procs left.\n    for p in procs:\n        p.join_process()\n\n    # Check all exit codes before returning\n    for p in procs:\n        if 0 != p.process.exitcode:\n            raise RuntimeError(\"Problem during: \"+str(p.process.name)+ \\\n                  ', exitcode: '+str(p.process.exitcode)+'. Check log.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines and return the best layout of tiles for a given pool_size.", "response": "def best_tile_layout(pool_size):\n    \"\"\" Determine and return the best layout of \"tiles\" for fastest\n    overall parallel processing of a rectangular image broken up into N\n    smaller equally-sized rectangular tiles, given as input the number\n    of processes/chunks which can be run/worked at the same time (pool_size).\n\n    This attempts to return a layout whose total number of tiles is as\n    close as possible to pool_size, without going over (and thus not\n    really taking advantage of pooling).  Since we can vary the\n    size of the rectangles, there is not much (any?) benefit to pooling.\n\n    Returns a tuple of ( <num tiles in X dir>, <num in Y direction> )\n\n    This assumes the image in question is relatively close to square, and\n    so the returned tuple attempts to give a layout which is as\n    squarishly-blocked as possible, except in cases where speed would be\n    sacrificed.\n\n    EXAMPLES:\n\n    For pool_size of 4, the best result is 2x2.\n\n    For pool_size of 6, the best result is 2x3.\n\n    For pool_size of 5, a result of 1x5 is better than a result of\n    2x2 (which would leave one core unused), and 1x5 is also better than\n    a result of 2x3 (which would require one core to work twice while all\n    others wait).\n\n    For higher, odd pool_size values (say 39), it is deemed best to\n    sacrifice a few unused cores to satisfy our other constraints, and thus\n    the result of 6x6 is best (giving 36 tiles and 3 unused cores).\n    \"\"\"\n    # Easy answer sanity-checks\n    if pool_size < 2:\n        return (1, 1)\n\n    # Next, use a small mapping of hard-coded results.  While we agree\n    # that many of these are unlikely pool_size values, they are easy\n    # to accomodate.\n    mapping = { 0:(1,1), 1:(1,1), 2:(1,2), 3:(1,3), 4:(2,2), 5:(1,5),\n                6:(2,3), 7:(2,3), 8:(2,4), 9:(3,3), 10:(2,5), 11:(2,5),\n                14:(2,7), 18:(3,6), 19:(3,6), 28:(4,7), 29:(4,7),\n                32:(4,8), 33:(4,8), 34:(4,8), 40:(4,10), 41:(4,10) }\n    if pool_size in mapping:\n        return mapping[pool_size]\n\n    # Next, take a guess using the square root and (for the sake of\n    # simplicity), go with it.  We *could* get much fancier here...\n    # Use floor-rounding (not ceil) so that the total number of resulting\n    # tiles is <= pool_size.\n    xnum = int(math.sqrt(pool_size))\n    ynum = int((1.*pool_size)/xnum)\n    return (xnum, ynum)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _accept(self):\n        logger.warning(\"Reactor \" + self._name + \" is starting\")\n        while self.running:\n            try:\n                self._completeTask()\n            except:\n                logger.exception(\"Unexpected exception during request processing\")\n        logger.warning(\"Reactor \" + self._name + \" is terminating\")", "response": "This method is called by the daemon when the thread is listening and the thread is shutting down."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noffering a new request to the reactor.", "response": "def offer(self, requestType, *args):\n        \"\"\"\n        public interface to the reactor.\n        :param requestType:\n        :param args:\n        :return:\n        \"\"\"\n        if self._funcsByRequest.get(requestType) is not None:\n            self._workQueue.put((requestType, list(*args)))\n        else:\n            logger.error(\"Ignoring unknown request on reactor \" + self._name + \" \" + requestType)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clicked(self):\n        try:\n            from . import teal\n        except:\n            teal = None\n        try:\n            # start drilling down into the tpo to get the code\n            tealGui = self._mainGuiObj\n            tealGui.showStatus('Clicked \"'+self.getButtonLabel()+'\"', keep=1)\n            pscope = self.paramInfo.scope\n            pname = self.paramInfo.name\n            tpo = tealGui._taskParsObj\n            tup = tpo.getExecuteStrings(pscope, pname)\n            code = ''\n            if not tup:\n                if teal:\n                    teal.popUpErr(tealGui.top, \"No action to perform\",\n                                  \"Action Button Error\")\n                return\n            for exname in tup:\n                if '_RULES_' in tpo and exname in tpo['_RULES_'].configspec:\n                    ruleSig = tpo['_RULES_'].configspec[exname]\n                    chkArgsDict = vtor_checks.sigStrToKwArgsDict(ruleSig)\n                    code = chkArgsDict.get('code') # a string or None\n                    # now go ahead and execute it\n                    teal.execEmbCode(pscope, pname, self.getButtonLabel(),\n                                     tealGui, code)\n            # done\n            tealGui.debug('Finished: \"'+self.getButtonLabel()+'\"')\n        except Exception as ex:\n            msg = 'Error executing: \"'+self.getButtonLabel()+'\"\\n'+ex.message\n            msgFull = msg+'\\n'+''.join(traceback.format_exc())\n            msgFull+= \"CODE:\\n\"+code\n            if tealGui:\n                if teal: teal.popUpErr(tealGui.top, msg, \"Action Button Error\")\n                tealGui.debug(msgFull)\n            else:\n                if teal: teal.popUpErr(None, msg, \"Action Button Error\")\n                print(msgFull)", "response": "Called when the button is clicked. Execute code from. cfgspc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting numpy array to string.", "response": "def ndarr2str(arr, encoding='ascii'):\n    \"\"\" This is used to ensure that the return value of arr.tostring()\n    is actually a string.  This will prevent lots of if-checks in calling\n    code.  As of numpy v1.6.1 (in Python 3.2.3), the tostring() function\n    still returns type 'bytes', not 'str' as it advertises. \"\"\"\n    # be fast, don't check - just assume 'arr' is a numpy array - the tostring\n    # call will fail anyway if not\n    retval = arr.tostring()\n    # would rather check \"if isinstance(retval, bytes)\", but support 2.5.\n    # could rm the if PY3K check, but it makes this faster on 2.x.\n    if PY3K and not isinstance(retval, str):\n        return retval.decode(encoding)\n    else: # is str\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting numpy array to bytes.", "response": "def ndarr2bytes(arr, encoding='ascii'):\n    \"\"\" This is used to ensure that the return value of arr.tostring()\n    is actually a *bytes* array in PY3K.  See notes in ndarr2str above.  Even\n    though we consider it a bug that numpy's tostring() function returns\n    a bytes array in PY3K, there are actually many instances where that is what\n    we want - bytes, not unicode.  So we use this function in those\n    instances to ensure that when/if this numpy \"bug\" is \"fixed\", that\n    our calling code still gets bytes where it needs/expects them. \"\"\"\n    # be fast, don't check - just assume 'arr' is a numpy array - the tostring\n    # call will fail anyway if not\n    retval = arr.tostring()\n    # would rather check \"if not isinstance(retval, bytes)\", but support 2.5.\n    if PY3K and isinstance(retval, str):\n        # Take note if this ever gets used.  If this ever occurs, it\n        # is likely wildly inefficient since numpy.tostring() is now\n        # returning unicode and numpy surely has a tobytes() func by now.\n        # If so, add a code path to call its tobytes() func at our start.\n        return retval.encode(encoding)\n    else: # is str==bytes in 2.x\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts string s to bytes type in all Pythons before 3. 0 and 2. 6 on Pythons before 3. 0.", "response": "def tobytes(s, encoding='ascii'):\n    \"\"\" Convert string s to the 'bytes' type, in all Pythons, even\n    back before Python 2.6.  What 'str' means varies by PY3K or not.\n    In Pythons before 3.0, this is technically the same as the str type\n    in terms of the character data in memory. \"\"\"\n    # NOTE: after we abandon 2.5, we might simply instead use \"bytes(s)\"\n    # NOTE: after we abandon all 2.*, del this and prepend byte strings with 'b'\n    if PY3K:\n        if isinstance(s, bytes):\n            return s\n        else:\n            return s.encode(encoding)\n    else:\n        # for py2.6 on (before 3.0), bytes is same as str;  2.5 has no bytes\n        # but handle if unicode is passed\n        if isinstance(s, unicode):\n            return s.encode(encoding)\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting string - like - thing s to the str type in all Pythons before 3. 0 bytes.", "response": "def tostr(s, encoding='ascii'):\n    \"\"\" Convert string-like-thing s to the 'str' type, in all Pythons, even\n    back before Python 2.6.  What 'str' means varies by PY3K or not.\n    In Pythons before 3.0, str and bytes are the same type.\n    In Python 3+, this may require a decoding step. \"\"\"\n    if PY3K:\n        if isinstance(s, str): # str == unicode in PY3K\n            return s\n        else: # s is type bytes\n            return s.decode(encoding)\n    else:\n        # for py2.6 on (before 3.0), bytes is same as str;  2.5 has no bytes\n        # but handle if unicode is passed\n        if isinstance(s, unicode):\n            return s.encode(encoding)\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef retry(func=None, retries=5, backoff=None, exceptions=(IOError, OSError, EOFError), cleanup=None, sleep=time.sleep):\n\n    @Aspect(bind=True)\n    def retry_aspect(cutpoint, *args, **kwargs):\n        for count in range(retries + 1):\n            try:\n                if count and cleanup:\n                    cleanup(*args, **kwargs)\n                yield\n                break\n            except exceptions as exc:\n                if count == retries:\n                    raise\n                if not backoff:\n                    timeout = 0\n                elif isinstance(backoff, (int, float)):\n                    timeout = backoff\n                else:\n                    timeout = backoff(count)\n                logger.exception(\"%s(%s, %s) raised exception %s. %s retries left. Sleeping %s secs.\",\n                                 cutpoint.__name__, args, kwargs, exc, retries - count, timeout)\n                sleep(timeout)\n\n    return retry_aspect if func is None else retry_aspect(func)", "response": "Decorator that retries the given function if it raises exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loadSignal(self, name, start=None, end=None):\n        entry = self._getCacheEntry(name)\n        if entry is not None:\n            from analyser.common.signal import loadSignalFromWav\n            return loadSignalFromWav(entry['path'], start=start, end=end)\n        else:\n            return None", "response": "Loads the named entry from the upload cache as a signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the entry with the given name or None if no entry exists.", "response": "def _getCacheEntry(self, name):\n        \"\"\"\n        :param name: the name of the cache entry.\n        :return: the entry or none.\n        \"\"\"\n        return next((x for x in self._uploadCache if x['name'] == name), None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a temporary file to the upload dir resampling it if necessary and then deleting the tmp entries.", "response": "def _convertTmp(self, tmpCacheEntry):\n        \"\"\"\n        Moves a tmp file to the upload dir, resampling it if necessary, and then deleting the tmp entries.\n        :param tmpCacheEntry: the cache entry.\n        :return:\n        \"\"\"\n        from analyser.common.signal import loadSignalFromWav\n        tmpCacheEntry['status'] = 'converting'\n        logger.info(\"Loading \" + tmpCacheEntry['path'])\n        signal = loadSignalFromWav(tmpCacheEntry['path'])\n        logger.info(\"Loaded \" + tmpCacheEntry['path'])\n        if Path(tmpCacheEntry['path']).exists():\n            logger.info('Deleting ' + tmpCacheEntry['path'])\n            os.remove(tmpCacheEntry['path'])\n        else:\n            logger.warning('Tmp cache file does not exist: ' + tmpCacheEntry['path'])\n        self._tmpCache.remove(tmpCacheEntry)\n        self._conversionCache.append(tmpCacheEntry)\n        srcFs = signal.fs\n        completeSamples = signal.samples\n        outputFileName = os.path.join(self._uploadDir, tmpCacheEntry['name'])\n        if srcFs > 1024:\n            self.writeOutput(outputFileName, completeSamples, srcFs, 1000)\n        else:\n            self.writeOutput(outputFileName, completeSamples, srcFs, srcFs)\n        tmpCacheEntry['status'] = 'loaded'\n        self._conversionCache.remove(tmpCacheEntry)\n        self._uploadCache.append(self._extractMeta(outputFileName, 'loaded'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef writeChunk(self, stream, filename, chunkIdx=None):\n        import io\n        more = True\n        outputFileName = filename if chunkIdx is None else filename + '.' + str(chunkIdx)\n        outputDir = self._uploadDir if chunkIdx is None else self._tmpDir\n        chunkFilePath = os.path.join(outputDir, outputFileName)\n        if os.path.exists(chunkFilePath) and os.path.isfile(chunkFilePath):\n            logger.error('Uploaded file already exists: ' + chunkFilePath)\n            return -1\n        else:\n            chunkFile = open(chunkFilePath, 'xb')\n        count = 0\n        while more:\n            chunk = stream.read(io.DEFAULT_BUFFER_SIZE)\n            chunkLen = len(chunk)\n            count += chunkLen\n            if chunkLen == 0:\n                more = False\n            else:\n                chunkFile.write(chunk)\n        return count", "response": "Streams an uploaded chunk to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfinalise the upload which means converting to a single 1kHz sample rate file output file.", "response": "def finalise(self, filename, totalChunks, status):\n        \"\"\"\n        Completes the upload which means converting to a single 1kHz sample rate file output file.   \n        :param filename: \n        :param totalChunks: \n        :param status: \n        :return: \n        \"\"\"\n\n        def getChunkIdx(x):\n            try:\n                return int(x.suffix[1:])\n            except ValueError:\n                return -1\n\n        def isChunkFile(x):\n            return x.is_file() and -1 < getChunkIdx(x) <= totalChunks\n\n        asSingleFile = os.path.join(self._tmpDir, filename)\n        if status.lower() == 'true':\n            chunks = [(getChunkIdx(file), str(file)) for file in\n                      Path(self._tmpDir).glob(filename + '.*') if isChunkFile(file)]\n            # TODO if len(chunks) != totalChunks then error\n            with open(asSingleFile, 'xb') as wfd:\n                for f in [x[1] for x in sorted(chunks, key=lambda tup: tup[0])]:\n                    with open(f, 'rb') as fd:\n                        logger.info(\"cat \" + f + \" with \" + asSingleFile)\n                        shutil.copyfileobj(fd, wfd, 1024 * 1024 * 10)\n        self.cleanupChunks(filename, isChunkFile, status)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef writeOutput(self, filename, samples, srcFs, targetFs):\n        import librosa\n        inputLength = samples.shape[-1]\n        if srcFs != targetFs:\n            if inputLength < targetFs:\n                logger.info(\"Input signal is too short (\" + str(inputLength) +\n                            \" samples) for resampling to \" + str(targetFs) + \"Hz\")\n                outputSamples = samples\n                targetFs = srcFs\n            else:\n                logger.info(\"Resampling \" + str(inputLength) + \" samples from \" + str(srcFs) + \"Hz to \" +\n                            str(targetFs) + \"Hz\")\n                outputSamples = librosa.resample(samples, srcFs, targetFs, res_type='kaiser_fast')\n        else:\n            outputSamples = samples\n        logger.info(\"Writing output to \" + filename)\n        maxv = np.iinfo(np.int32).max\n        librosa.output.write_wav(filename, (outputSamples * maxv).astype(np.int32), targetFs)\n        logger.info(\"Output written to \" + filename)", "response": "Resamples the signal to the targetFs and writes it to the filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, name):\n        i, entry = next(((i, x) for i, x in enumerate(self._uploadCache) if x['name'] == name), (None, None))\n        if entry is not None:\n            logger.info(\"Deleting \" + name)\n            os.remove(str(entry['path']))\n            del self._uploadCache[i]\n            return entry\n        else:\n            logger.info(\"Unable to delete \" + name + \", not found\")\n            return None", "response": "Deletes the named entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an EparOption item of appropriate type for the parameter param", "response": "def eparOptionFactory(master, statusBar, param, defaultParam,\n                      doScroll, fieldWidths,\n                      plugIn=None, editedCallbackObj=None,\n                      helpCallbackObj=None, mainGuiObj=None,\n                      defaultsVerb=\"Default\", bg=None, indent=False,\n                      flagging=False, flaggedColor=None):\n\n    \"\"\"Return EparOption item of appropriate type for the parameter param\"\"\"\n\n    # Allow passed-in overrides\n    if plugIn is not None:\n        eparOption = plugIn\n\n    # If there is an enumerated list, regardless of datatype use EnumEparOption\n    elif param.choice is not None:\n        eparOption = EnumEparOption\n\n    else:\n        # Use String for types not in the dictionary\n        eparOption = _eparOptionDict.get(param.type, StringEparOption)\n\n    # Create it\n    eo = eparOption(master, statusBar, param, defaultParam, doScroll,\n                    fieldWidths, defaultsVerb, bg,\n                    indent=indent, helpCallbackObj=helpCallbackObj,\n                    mainGuiObj=mainGuiObj)\n    eo.setEditedCallbackObj(editedCallbackObj)\n    eo.setIsFlagging(flagging, False)\n    if flaggedColor:\n        eo.setFlaggedColor(flaggedColor)\n    return eo"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extraBindingsForSelectableText(self):\n        # See notes in watchTextSelection\n        self.entry.bind('<FocusIn>', self.watchTextSelection, \"+\")\n        self.entry.bind('<ButtonRelease-1>', self.watchTextSelection, \"+\")\n        self.entry.bind('<B1-Motion>', self.watchTextSelection, \"+\")\n        self.entry.bind('<Shift_L>', self.watchTextSelection, \"+\")\n        self.entry.bind('<Left>', self.watchTextSelection, \"+\")\n        self.entry.bind('<Right>', self.watchTextSelection, \"+\")", "response": "Collect in 1 place the bindings needed for the watchTextSelection method"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef focusOut(self, event=None):\n        # do nothing if this isn't a text-enabled widget\n        if not self.isSelectable:\n            return\n        if self.entryCheck(event) is None:\n            # Entry value is OK\n            # Save the last selection so it can be restored if we\n            # come right back to this widget.  Then clear the selection\n            # before moving on.\n            entry = self.entry\n            try:\n                if not entry.selection_present():\n                    self.lastSelection = None\n                else:\n                    self.lastSelection = (entry.index(SEL_FIRST),\n                                          entry.index(SEL_LAST))\n            except AttributeError:\n                pass\n            if USING_X and sys.platform == 'darwin':\n                pass # do nothing here - we need it left selected for cut/paste\n            else:\n                entry.selection_clear()\n        else:\n            return \"break\"", "response": "Clear selection ( if text is selected in this widget"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting all text on taking focus", "response": "def focusIn(self, event=None):\n        \"\"\"Select all text (if applicable) on taking focus\"\"\"\n        try:\n            # doScroll returns false if the call was ignored because the\n            # last call also came from this widget.  That avoids unwanted\n            # scrolls and text selection when the focus moves in and out\n            # of the window.\n            if self.doScroll(event):\n                self.entry.selection_range(0, END) # select all text in widget\n            else:\n                # restore selection to what it was on the last FocusOut\n                if self.lastSelection:\n                    self.entry.selection_range(*self.lastSelection)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef widgetEdited(self, event=None, val=None, action='entry', skipDups=True):\n\n        # be as lightweight as possible if obj doesn't care about this stuff\n        if not self._editedCallbackObj and not self._flagNonDefaultVals:\n            return\n\n        # get the current value\n        curVal = val # take this first, if it is given\n        if curVal is None:\n            curVal = self.choice.get()\n\n        # do any flagging\n        self.flagThisPar(curVal, False)\n\n        # see if this is a duplicate successive call for the same value\n        if skipDups and curVal==self._lastWidgetEditedVal: return\n\n        # pull trigger\n        if not self._editedCallbackObj: return\n        self._editedCallbackObj.edited(self.paramInfo.scope,\n                                       self.paramInfo.name,\n                                       self.previousValue, curVal,\n                                       action)\n        # for our duplicate checker\n        self._lastWidgetEditedVal = curVal", "response": "This method is called by the GUI when a value has been set. It is called by the GUI when the user has clicked on the entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fileBrowser(self):\n        if capable.OF_TKFD_IN_EPAR:\n           fname = askopenfilename(parent=self.entry, title=\"Select File\")\n        else:\n            from . import filedlg\n            self.fd = filedlg.PersistLoadFileDialog(self.entry,\n                              \"Select File\", \"*\")\n            if self.fd.Show() != 1:\n                self.fd.DialogCleanup()\n                return\n            fname = self.fd.GetFileName()\n            self.fd.DialogCleanup()\n        if not fname: return # canceled\n\n        self.choice.set(fname)\n        # don't select when we go back to widget to reduce risk of\n        # accidentally typing over the filename\n        self.lastSelection = None", "response": "Invoke a tkinter file dialog"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dirBrowser(self):\n        if capable.OF_TKFD_IN_EPAR:\n            fname = askdirectory(parent=self.entry, title=\"Select Directory\")\n        else:\n            raise NotImplementedError('Fix popupChoices() logic.')\n\n        if not fname:\n            return # canceled\n\n        self.choice.set(fname)\n        # don't select when we go back to widget to reduce risk of\n        # accidentally typing over the filename\n        self.lastSelection = None", "response": "Invoke a tkinter directory dialog"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forceValue(self, newVal, noteEdited=False):\n        if newVal is None:\n            newVal = \"\"\n        self.choice.set(newVal)\n        if noteEdited:\n            self.widgetEdited(val=newVal, skipDups=False)", "response": "Force - set a parameter entry to the given value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unlearnValue(self):\n        defaultValue = self.defaultParamInfo.get(field = \"p_filename\",\n                            native = 0, prompt = 0)\n        self.choice.set(defaultValue)", "response": "Unlearn a parameter value by setting it back to its default value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setActiveState(self, active):\n        st = DISABLED\n        if active: st = NORMAL\n        self.entry.configure(state=st)\n        self.inputLabel.configure(state=st)\n        self.promptLabel.configure(state=st)", "response": "Configure the active state of the entry input label prompt label and entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flagThisPar(self, currentVal, force):\n\n        # Get out ASAP if we can\n        if (not force) and (not self._flagNonDefaultVals): return\n\n        # handle simple case before comparing values (quick return)\n        if force and not self._flagNonDefaultVals:\n            self._flagged = False\n            self.promptLabel.configure(fg=\"black\")\n            return\n\n        # Get/format values to compare\n        currentNative = self.convertToNative(currentVal)\n        defaultNative = self.convertToNative(self.defaultParamInfo.value)\n        # par.value is same as par.get(native=1,prompt=0)\n\n        # flag or unflag as needed\n        if currentNative != defaultNative:\n            if not self._flagged or force:\n                self._flagged = True\n                self.promptLabel.configure(fg=self._flaggedColor) # was red\n        else: # same as def\n            if self._flagged or force:\n                self._flagged = False\n                self.promptLabel.configure(fg=\"black\")", "response": "Flag this parameter as needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nallow keys typed in widget to select items", "response": "def keypress(self, event):\n        \"\"\"Allow keys typed in widget to select items\"\"\"\n        try:\n            self.choice.set(self.shortcuts[event.keysym])\n        except KeyError:\n            # key not found (probably a bug, since we intend to catch\n            # only events from shortcut keys, but ignore it anyway)\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postcmd(self):\n        value = self.choice.get()\n        try:\n            index = self.paramInfo.choice.index(value)\n            self.entry.menu.activate(index)\n        except ValueError:\n            # initial null value may not be in list\n            pass", "response": "Make sure proper entry is activated when menu is posted"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convertToNative(self, aVal):\n        if aVal is None:\n            return None\n        if isinstance(aVal, bool): return aVal\n        # otherwise interpret strings\n        return str(aVal).lower() in ('1','on','yes','true')", "response": "Convert to native bool ; interpret certain strings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntoggle value between Yes and No", "response": "def toggle(self, event=None):\n        \"\"\"Toggle value between Yes and No\"\"\"\n        if self.choice.get() == \"yes\":\n            self.rbno.select()\n        else:\n            self.rbyes.select()\n        self.widgetEdited()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if an entry is uppercase and set the choice to the current value.", "response": "def entryCheck(self, event = None, repair = True):\n        \"\"\" Ensure any INDEF entry is uppercase, before base class behavior \"\"\"\n        valupr = self.choice.get().upper()\n        if valupr.strip() == 'INDEF':\n            self.choice.set(valupr)\n        return EparOption.entryCheck(self, event, repair = repair)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the current record of the packet size per sample and the max bytes per fifo read.", "response": "def _setSampleSizeBytes(self):\n        \"\"\"\n        updates the current record of the packet size per sample and the relationship between this and the fifo reads. \n        \"\"\"\n        self.sampleSizeBytes = self.getPacketSize()\n        if self.sampleSizeBytes > 0:\n            self.maxBytesPerFifoRead = (32 // self.sampleSizeBytes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the current packet size based on the enabled registers", "response": "def getPacketSize(self):\n        \"\"\"\n        the current packet size.\n        :return: the current packet size based on the enabled registers.\n        \"\"\"\n        size = 0\n        if self.isAccelerometerEnabled():\n            size += 6\n        if self.isGyroEnabled():\n            size += 6\n        if self.isTemperatureEnabled():\n            size += 2\n        return size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitialising the device with the specified parameters", "response": "def initialiseDevice(self):\n        \"\"\"\n        performs initialisation of the device\n        :param batchSize: the no of samples that each provideData call should yield\n        :return:\n        \"\"\"\n        logger.debug(\"Initialising device\")\n        self.getInterruptStatus()\n        self.setAccelerometerSensitivity(self._accelerationFactor * 32768.0)\n        self.setGyroSensitivity(self._gyroFactor * 32768.0)\n        self.setSampleRate(self.fs)\n        for loop in self.ZeroRegister:\n            self.i2c_io.write(self.MPU6050_ADDRESS, loop, 0)\n        # Sets clock source to gyro reference w/ PLL\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_PWR_MGMT_1, 0b00000010)\n        # Controls frequency of wakeups in accel low power mode plus the sensor standby modes\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_PWR_MGMT_2, 0x00)\n        # Enables any I2C master interrupt source to generate an interrupt\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_INT_ENABLE, 0x01)\n        # enable the FIFO\n        self.enableFifo()\n        logger.debug(\"Initialised device\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling the acceleration sensor for this device.", "response": "def enableAccelerometer(self):\n        \"\"\"\n        Specifies the device should write acceleration values to the FIFO, is not applied until enableFIFO is called.\n        :return:\n        \"\"\"\n        logger.debug(\"Enabling acceleration sensor\")\n        self.fifoSensorMask |= self.enableAccelerometerMask\n        self._accelEnabled = True\n        self._setSampleSizeBytes()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling the acceleration sensor for the current process.", "response": "def disableAccelerometer(self):\n        \"\"\"\n        Specifies the device should NOT write acceleration values to the FIFO, is not applied until enableFIFO is\n        called.\n        :return: \n        \"\"\"\n        logger.debug(\"Disabling acceleration sensor\")\n        self.fifoSensorMask &= ~self.enableAccelerometerMask\n        self._accelEnabled = False\n        self._setSampleSizeBytes()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable the gyro sensor for this software.", "response": "def enableGyro(self):\n        \"\"\"\n        Specifies the device should write gyro values to the FIFO, is not applied until enableFIFO is called.\n        :return: \n        \"\"\"\n        logger.debug(\"Enabling gyro sensor\")\n        self.fifoSensorMask |= self.enableGyroMask\n        self._gyroEnabled = True\n        self._setSampleSizeBytes()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disableGyro(self):\n        logger.debug(\"Disabling gyro sensor\")\n        self.fifoSensorMask &= ~self.enableGyroMask\n        self._gyroEnabled = False\n        self._setSampleSizeBytes()", "response": "Disables the gyro sensor for the current process."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nenables the temperature sensor for the current process.", "response": "def enableTemperature(self):\n        \"\"\"\n        Specifies the device should write temperature values to the FIFO, is not applied until enableFIFO is called.\n        :return: \n        \"\"\"\n        logger.debug(\"Enabling temperature sensor\")\n        self.fifoSensorMask |= self.enableTemperatureMask\n        self._setSampleSizeBytes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disableTemperature(self):\n        logger.debug(\"Disabling temperature sensor\")\n        self.fifoSensorMask &= ~self.enableTemperatureMask\n        self._setSampleSizeBytes()", "response": "Disables the temperature sensor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the gyro sensitivity value for the current locale.", "response": "def setGyroSensitivity(self, value):\n        \"\"\"\n        Sets the gyro sensitivity to 250, 500, 1000 or 2000 according to the given value (and implicitly disables the\n        self\n        tests)\n        :param value: the target sensitivity.\n        \"\"\"\n        try:\n            self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_GYRO_CONFIG,\n                              {250: 0, 500: 8, 1000: 16, 2000: 24}[value])\n            self._gyroFactor = value / 32768.0\n            self.gyroSensitivity = value\n            logger.debug(\"Set gyro sensitivity = %d\", value)\n        except KeyError:\n            raise ArgumentError(value + \" is not a valid sensitivity (250,500,1000,2000)\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setAccelerometerSensitivity(self, value):\n        # note that this implicitly disables the self tests on each axis\n        # i.e. the full byte is actually 000[accel]000 where the 1st 3 are the accelerometer self tests, the next two\n        # values are the actual sensitivity and the last 3 are unused\n        # the 2 [accel] bits are translated by the device as follows; 00 = 2g, 01 = 4g, 10 = 8g, 11 = 16g\n        # in binary we get 2 = 0, 4 = 1000, 8 = 10000, 16 = 11000\n        # so the 1st 3 bits are always 0\n        try:\n            self.i2c_io.write(self.MPU6050_ADDRESS,\n                              self.MPU6050_RA_ACCEL_CONFIG,\n                              {2: 0, 4: 8, 8: 16, 16: 24}[value])\n            self._accelerationFactor = value / 32768.0\n            self.accelerometerSensitivity = value\n            logger.debug(\"Set accelerometer sensitivity = %d\", value)\n        except KeyError:\n            raise ArgumentError(value + \" is not a valid sensitivity (2,4,8,18)\")", "response": "Sets the accelerometer sensitivity to the given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setSampleRate(self, targetSampleRate):\n        sampleRateDenominator = int((8000 / min(targetSampleRate, 1000)) - 1)\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_SMPLRT_DIV, sampleRateDenominator)\n        self.fs = 8000.0 / (sampleRateDenominator + 1.0)\n        logger.debug(\"Set sample rate = %d\", self.fs)", "response": "Sets the internal sample rate of the MPU - 6050."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resetFifo(self):\n        logger.debug(\"Resetting FIFO\")\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_USER_CTRL, 0b00000000)\n        pass\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_USER_CTRL, 0b00000100)\n        pass\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_USER_CTRL, 0b01000000)\n        self.getInterruptStatus()", "response": "Resets the FIFO by first disabling the FIFO then sending a FIFO_RESET and then re - enabling the FIFO."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enableFifo(self):\n        logger.debug(\"Enabling FIFO\")\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_FIFO_EN, 0)\n        self.resetFifo()\n        self.i2c_io.write(self.MPU6050_ADDRESS, self.MPU6050_RA_FIFO_EN, self.fifoSensorMask)\n        logger.debug(\"Enabled FIFO\")", "response": "Enables the FIFO and resets it and sets which values should be written to the FIFO."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getFifoCount(self):\n        bytes = self.i2c_io.readBlock(self.MPU6050_ADDRESS, self.MPU6050_RA_FIFO_COUNTH, 2)\n        count = (bytes[0] << 8) + bytes[1]\n        logger.debug(\"FIFO Count: %d\", count)\n        return count", "response": "Gets the number of bytes available on the FIFO right now."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the specified number of bytes from the FIFO and returns them as a bytearray.", "response": "def getDataFromFIFO(self, bytesToRead):\n        \"\"\"\n        reads the specified number of bytes from the FIFO, should be called after a call to getFifoCount to ensure there\n        is new data available (to avoid reading duplicate data).\n        :param bytesToRead: the number of bytes to read.\n        :return: the bytes read.\n        \"\"\"\n        return self.i2c_io.readBlock(self.MPU6050_ADDRESS, self.MPU6050_RA_FIFO_R_W, bytesToRead)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a batchSize batch of data from the FIFO while attempting to optimise the number of times we have to read from the device itself. :return: a list of data where each item is a single sample of data converted into real values and stored as a dict.", "response": "def provideData(self):\n        \"\"\"\n        reads a batchSize batch of data from the FIFO while attempting to optimise the number of times we have to read\n        from the device itself.\n        :return: a list of data where each item is a single sample of data converted into real values and stored as a\n        dict.\n        \"\"\"\n        samples = []\n        fifoBytesAvailable = 0\n        fifoWasReset = False\n        logger.debug(\">> provideData target %d samples\", self.samplesPerBatch)\n        iterations = 0\n        # allow 1.5x the expected duration of the batch\n        breakTime = time() + ((self.samplesPerBatch / self.fs) * 1.5)\n        overdue = False\n        while len(samples) < self.samplesPerBatch and not overdue:\n            iterations += 1\n            if iterations > self.samplesPerBatch and iterations % 100 == 0:\n                if time() > breakTime:\n                    logger.warning(\"Breaking measurement after %d iterations, batch overdue\", iterations)\n                    overdue = True\n            if fifoBytesAvailable < self.sampleSizeBytes or fifoWasReset:\n                interrupt = self.getInterruptStatus()\n                fifoBytesAvailable = self.getFifoCount()\n                fifoWasReset = False\n            logger.debug(\"Start sample loop [available: %d , required: %d]\", fifoBytesAvailable, self.sampleSizeBytes)\n            if interrupt & 0x10:\n                logger.error(\"FIFO OVERFLOW, RESETTING [available: %d , interrupt: %d]\", fifoBytesAvailable, interrupt)\n                self.measurementOverflowed = True\n                self.resetFifo()\n                fifoWasReset = True\n            elif fifoBytesAvailable == 1024:\n                logger.error(\"FIFO FULL, RESETTING [available: %d , interrupt: %d]\", fifoBytesAvailable, interrupt)\n                self.measurementOverflowed = True\n                self.resetFifo()\n                fifoWasReset = True\n            elif interrupt & 0x02 or interrupt & 0x01:\n                # wait for at least 1 sample to arrive, should be a VERY short wait\n                while fifoBytesAvailable < self.sampleSizeBytes:\n                    logger.debug(\"Waiting for sample [available: %d , required: %d]\", fifoBytesAvailable,\n                                 self.sampleSizeBytes)\n                    fifoBytesAvailable = self.getFifoCount()\n                logger.debug(\"Processing data [available: %d , required: %d]\", fifoBytesAvailable, self.sampleSizeBytes)\n                fifoReadBytes = self.sampleSizeBytes\n                # TODO this chunk of code is a bit messy, tidy it up\n                # if we have more than 1 sample available then ensure we read as many as we can at once (albeit within\n                # the limits of the max i2c read size of 32 bytes)\n                if fifoBytesAvailable > self.sampleSizeBytes:\n                    fifoReadBytes = min(fifoBytesAvailable // self.sampleSizeBytes,\n                                        self.maxBytesPerFifoRead) * self.sampleSizeBytes\n                    logger.debug(\"Excess bytes to read [available: %d , reading: %d]\", fifoBytesAvailable,\n                                 fifoReadBytes)\n                # but don't read more than we need to fulfil the batch\n                samplesToRead = fifoReadBytes // self.sampleSizeBytes\n                excessSamples = self.samplesPerBatch - len(samples) - samplesToRead\n                if excessSamples < 0:\n                    samplesToRead += excessSamples\n                    fifoReadBytes = int(samplesToRead * self.sampleSizeBytes)\n                    logger.debug(\"Excess samples to read [available: %d , reading: %d]\", fifoBytesAvailable,\n                                 fifoReadBytes)\n                else:\n                    logger.debug(\"Reading [available: %d , reading: %d]\", fifoBytesAvailable, fifoReadBytes)\n                # read the bytes from the fifo, break it into sample sized chunks and convert to the actual values\n                fifoBytes = self.getDataFromFIFO(fifoReadBytes)\n                samples.extend([self.unpackSample(fifoBytes[i:i + self.sampleSizeBytes])\n                                for i in range(0, len(fifoBytes), self.sampleSizeBytes)])\n                # track the count here so we can avoid going back to the FIFO each time\n                fifoBytesAvailable -= fifoReadBytes\n                logger.debug(\"End sample loop [available: %d , required: %d]\", fifoBytesAvailable, self.sampleSizeBytes)\n        logger.debug(\"<< provideData %d samples\", len(samples))\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpackSample(self, rawData):\n        length = len(rawData)\n        # TODO error if not multiple of 2\n        # logger.debug(\">> unpacking sample %d length %d\", self._sampleIdx, length)\n        unpacked = struct.unpack(\">\" + ('h' * (length // 2)), memoryview(bytearray(rawData)).tobytes())\n        # store the data in a dictionary\n        mpu6050 = collections.OrderedDict()\n        mpu6050[SAMPLE_TIME] = self._sampleIdx / self.fs\n        sensorIdx = 0\n        if self.isAccelerometerEnabled():\n            mpu6050[ACCEL_X] = unpacked[sensorIdx] * self._accelerationFactor\n            sensorIdx += 1\n            mpu6050[ACCEL_Y] = unpacked[sensorIdx] * self._accelerationFactor\n            sensorIdx += 1\n            mpu6050[ACCEL_Z] = unpacked[sensorIdx] * self._accelerationFactor\n            sensorIdx += 1\n\n        if self.isTemperatureEnabled():\n            mpu6050[TEMP] = unpacked[sensorIdx] * self._temperatureGain + self._temperatureOffset\n            sensorIdx += 1\n\n        if self.isGyroEnabled():\n            mpu6050[GYRO_X] = unpacked[sensorIdx] * self._gyroFactor\n            sensorIdx += 1\n            mpu6050[GYRO_Y] = unpacked[sensorIdx] * self._gyroFactor\n            sensorIdx += 1\n            mpu6050[GYRO_Z] = unpacked[sensorIdx] * self._gyroFactor\n            sensorIdx += 1\n        # TODO should we send as a dict so the keys are available?\n        output = list(mpu6050.values())\n        self._sampleIdx += 1\n        # logger.debug(\"<< unpacked sample length %d into vals size %d\", length, len(output))\n        return output", "response": "This function unpacks a single sample of data into a list of the most recent available sensors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping text in a single line.", "response": "def wrap(text, width, *args, **kwargs):\n    \"\"\"\n    Like :func:`textwrap.wrap` but preserves existing newlines which\n    :func:`textwrap.wrap` does not otherwise handle well.\n\n    See Also\n    --------\n    :func:`textwrap.wrap`\n    \"\"\"\n\n    return sum([textwrap.wrap(line, width, *args, **kwargs)\n                if line else [''] for line in text.splitlines()], [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noutputs line-wrapped text wrapped in a box drawn with a repeated (usually ASCII) character. For example: >>> print(textbox('Text to wrap', width=16)) ################ # # # Text to wrap # # # ################ Parameters ---------- text : string The text to wrap width : int The width of the entire box, including the perimeter and the indentation space. Because the wrapped text is padded with an additional column of whitespace on each side, the minimum width is 5--any width less than that is is automatically increased to 5 (default: 78) boxchar : string (No pun intended.) The character to draw the box with. May also be a string of multiple characters (default: '#') indent : int Amount of space by which the box should be indented. (default: 0)", "response": "def textbox(text, width=78, boxchar='#', indent=0):\n    \"\"\"\n    Outputs line-wrapped text wrapped in a box drawn with a repeated (usually\n    ASCII) character.\n\n    For example:\n\n        >>> print(textbox('Text to wrap', width=16))\n        ################\n        #              #\n        # Text to wrap #\n        #              #\n        ################\n\n    Parameters\n    ----------\n    text : string\n        The text to wrap\n\n    width : int\n        The width of the entire box, including the perimeter and\n        the indentation space.  Because the\n        wrapped text is padded with an additional column of whitespace on each\n        side, the minimum width is 5--any width less than that is\n        is automatically increased to 5 (default: 78)\n\n    boxchar : string\n        (No pun intended.) The character to draw the box with.  May also\n        be a string of multiple characters (default: '#')\n\n    indent : int\n        Amount of space by which the box should be indented. (default: 0)\n\n    \"\"\"\n\n    min_width = len(boxchar) * 2 + 3\n    width = max(width-indent, min_width)\n    indentspace = indent * ' '\n\n    wrap_width = width - min_width + 1\n\n    q, r = divmod(width, len(boxchar))\n    # The top/bottom border\n    top_border = indentspace + boxchar * q + boxchar[:r]\n    top_padding  = indentspace + boxchar + ' ' * (width - len(boxchar) * 2) + boxchar\n\n    lines = ['%s%s %s %s' % (indentspace, boxchar, line.ljust(wrap_width),\n                             boxchar)\n             for line in wrap(text, wrap_width)]\n    top = [top_border, top_padding]\n    bottom = [top_padding, top_border]\n    return '\\n'.join(top + lines + bottom)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef independence_day(year, observed=None):\n    '''July 4th'''\n    day = 4\n\n    if observed:\n        if calendar.weekday(year, JUL, 4) == SAT:\n            day = 3\n\n        if calendar.weekday(year, JUL, 4) == SUN:\n            day = 5\n\n    return (year, JUL, day)", "response": "July 4th or SUN"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the columbus day of the given year.", "response": "def columbus_day(year, country='usa'):\n    '''in USA: 2nd Monday in Oct\n       Elsewhere: Oct 12'''\n    if country == 'usa':\n        return nth_day_of_month(2, MON, OCT, year)\n    else:\n        return (year, OCT, 12)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the first thanksgiving entry for a given year.", "response": "def thanksgiving(year, country='usa'):\n    '''USA: last Thurs. of November, Canada: 2nd Mon. of October'''\n    if country == 'usa':\n        if year in [1940, 1941]:\n            return nth_day_of_month(3, THU, NOV, year)\n        elif year == 1939:\n            return nth_day_of_month(4, THU, NOV, year)\n        else:\n            return nth_day_of_month(0, THU, NOV, year)\n\n    if country == 'canada':\n        return nth_day_of_month(2, MON, OCT, year)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linefit(x, y, weights=None):\n\n    \"\"\"\n    Parameters\n    ----------\n    y: 1D numpy array\n        The data to be fitted\n    x: 1D numpy array\n        The x values of the y array. x and y must\n        have the same shape.\n    weights:   1D numpy array, must have the same shape as x and y\n        weight values\n\n    Examples\n    --------\n    >>> import numpy as N\n    >>> from numpy.core import around\n    >>> x = N.array([-5, -4 ,-3 ,-2 ,-1, 0, 1, 2, 3, 4, 5])\n    >>> y = N.array([1, 5, 4, 7, 10, 8, 9, 13, 14, 13, 18])\n    >>> around(linefit(x,y), decimals=5)\n    array([9.27273, 1.43636])\n    >>> x = N.array([1.3,1.3,2.0,2.0,2.7,3.3,3.3,3.7,3.7,4.,4.,4.,4.7,4.7,5.,5.3,5.3,5.3,5.7,6.,6.,6.3,6.7])\n    >>> y = N.array([2.3,1.8,2.8,1.5,2.2,3.8,1.8,3.7,1.7,2.8,2.8,2.2,3.2,1.9,1.8,3.5,2.8,2.1,3.4,3.2,3.,3.,5.9])\n    >>> around(linefit(x,y), decimals=5)\n    array([1.42564, 0.31579])\n    \"\"\"\n    if len(x) != len(y):\n        print(\"Error: X and Y must have equal size\\n\")\n        return\n    n = len(x)\n    w = N.zeros((n,n)).astype(N.float)\n    if weights is None:\n        for i in N.arange(n):\n            w[i,i] = 1\n    else:\n        if len(weights) != n:\n            print(\"Error: Weights must have the same size as X and Y.\\n\")\n            return\n        for i in N.arange(n):\n            w[i,i] = weights[i]\n    x = x.astype(N.float)\n    y = y.astype(N.float)\n    # take the weighted avg for calculatiing the covarince\n    Xavg = N.sum(N.dot(w,x)) / N.sum(w.diagonal())\n    Yavg = N.sum(N.dot(w,y)) / N.sum(w.diagonal())\n\n    xm = x - Xavg\n    xmt = N.transpose(xm)\n    ym = y - Yavg\n\n    b1 = N.dot(xmt,N.dot(w,ym)) / N.dot(xmt ,N.dot(w,xm))\n    b0 = Yavg - b1 * Xavg\n\n    return b0, b1", "response": "This function calculates the linefit of a single language language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the data for the given measurementId.", "response": "def get(self, measurementId):\n        \"\"\"\n        Analyses the measurement with the given parameters\n        :param measurementId:\n        :return:\n        \"\"\"\n        logger.info('Analysing ' + measurementId)\n        measurement = self._measurementController.getMeasurement(measurementId, MeasurementStatus.COMPLETE)\n        if measurement is not None:\n            if measurement.inflate():\n                data = {\n                    name: {\n                        'spectrum': {\n                            'x': self._jsonify(data.spectrum('x')),\n                            'y': self._jsonify(data.spectrum('y')),\n                            'z': self._jsonify(data.spectrum('z')),\n                            'sum': self._jsonify(data.spectrum('sum'))\n                        },\n                        'psd': {\n                            'x': self._jsonify(data.psd('x')),\n                            'y': self._jsonify(data.psd('y')),\n                            'z': self._jsonify(data.psd('z'))\n                        },\n                        'peakSpectrum': {\n                            'x': self._jsonify(data.peakSpectrum('x')),\n                            'y': self._jsonify(data.peakSpectrum('y')),\n                            'z': self._jsonify(data.peakSpectrum('z')),\n                            'sum': self._jsonify(data.peakSpectrum('sum'))\n                        }\n                    }\n                    for name, data in measurement.data.items()\n                }\n                return data, 200\n            else:\n                return None, 404\n        else:\n            return None, 404"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply the target state to the current state of the current state.", "response": "def _applyTargetState(targetState, md, httpclient):\n    \"\"\"\n    compares the current device state against the targetStateProvider and issues updates as necessary to ensure the\n    device is\n    at that state.\n    :param md:\n    :param targetState: the target state.\n    :param httpclient: the http client\n    :return:\n    \"\"\"\n    anyUpdate = False\n    if md['fs'] != targetState.fs:\n        logger.info(\"Updating fs from \" + str(md['fs']) + \" to \" + str(targetState.fs) + \" for \" + md['name'])\n        anyUpdate = True\n\n    if md['samplesPerBatch'] != targetState.samplesPerBatch:\n        logger.info(\"Updating samplesPerBatch from \" + str(md['samplesPerBatch']) + \" to \" + str(\n            targetState.samplesPerBatch) + \" for \" + md['name'])\n        anyUpdate = True\n\n    if md['gyroEnabled'] != targetState.gyroEnabled:\n        logger.info(\"Updating gyroEnabled from \" + str(md['gyroEnabled']) + \" to \" + str(\n            targetState.gyroEnabled) + \" for \" + md['name'])\n        anyUpdate = True\n\n    if md['gyroSens'] != targetState.gyroSens:\n        logger.info(\n            \"Updating gyroSens from \" + str(md['gyroSens']) + \" to \" + str(targetState.gyroSens) + \" for \" + md[\n                'name'])\n        anyUpdate = True\n\n    if md['accelerometerEnabled'] != targetState.accelerometerEnabled:\n        logger.info(\"Updating accelerometerEnabled from \" + str(md['accelerometerEnabled']) + \" to \" + str(\n            targetState.accelerometerEnabled) + \" for \" + md['name'])\n        anyUpdate = True\n\n    if md['accelerometerSens'] != targetState.accelerometerSens:\n        logger.info(\"Updating accelerometerSens from \" + str(md['accelerometerSens']) + \" to \" + str(\n            targetState.accelerometerSens) + \" for \" + md['name'])\n        anyUpdate = True\n\n    if anyUpdate:\n        payload = marshal(targetState, targetStateFields)\n        logger.info(\"Applying target state change \" + md['name'] + \" - \" + str(payload))\n        if RecordingDeviceStatus.INITIALISED.name == md.get('status'):\n            try:\n                httpclient.patch(md['serviceURL'], json=payload)\n            except Exception as e:\n                logger.exception(e)\n        else:\n            logger.warning(\"Ignoring target state change until \" + md['name'] + \" is idle, currently \" + md['status'])\n    else:\n        logger.debug(\"Device \" + md['name'] + \" is at target state, we continue\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the target state of the specified device.", "response": "def updateDeviceState(self, device):\n        \"\"\"\n        Updates the target state on the specified device.\n        :param targetState: the target state to reach.\n        :param device: the device to update.\n        :return:\n        \"\"\"\n        # this is only threadsafe because the targetstate is effectively immutable, if it becomes mutable in future then\n        # funkiness may result\n        self._reactor.offer(REACH_TARGET_STATE, [self._targetStateProvider.state, device, self._httpclient])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the system target state and propagates that to all devices.", "response": "def updateTargetState(self, newState):\n        \"\"\"\n        Updates the system target state and propagates that to all devices.\n        :param newState:\n        :return:\n        \"\"\"\n        self._targetStateProvider.state = loadTargetState(newState, self._targetStateProvider.state)\n        for device in self.deviceController.getDevices():\n            self.updateDeviceState(device.payload)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(input, width=132, output=None, keep=False):\n\n    \"\"\"Input ASCII trailer file \"input\" will be read.\n\n    The contents will then be written out to a FITS file in the same format\n    as used by 'stwfits' from IRAF.\n\n    Parameters\n    ===========\n    input : str\n        Filename of input ASCII trailer file\n\n    width : int\n        Number of characters wide to use for defining output FITS column\n        [Default: 132]\n\n    output : str\n        Filename to use for writing out converted FITS trailer file\n        If None, input filename will be converted from *.tra -> *_trl.fits\n        [Default: None]\n\n    keep : bool\n        Specifies whether or not to keep any previously written FITS files\n        [Default: False]\n\n    \"\"\"\n    # open input trailer file\n    trl = open(input)\n\n    # process all lines\n    lines = np.array([i for text in trl.readlines() for i in textwrap.wrap(text,width=width)])\n\n    # close ASCII trailer file now that we have processed all the lines\n    trl.close()\n\n    if output is None:\n        # create fits file\n        rootname,suffix = os.path.splitext(input)\n        s = suffix[1:].replace('ra','rl')\n        fitsname = \"{}_{}{}fits\".format(rootname,s,os.path.extsep)\n    else:\n        fitsname = output\n    full_name = os.path.abspath(os.path.join(os.path.curdir,fitsname))\n\n    old_file = os.path.exists(full_name)\n    if old_file:\n        if keep:\n            print(\"ERROR: Trailer file already written out as: {}\".format(full_name))\n            raise IOError\n        else:\n            os.remove(full_name)\n\n    # Build FITS table and write it out\n    line_fmt = \"{}A\".format(width)\n    tbhdu = fits.BinTableHDU.from_columns([fits.Column(name='TEXT_FILE',format=line_fmt,array=lines)])\n    tbhdu.writeto(fitsname)\n\n    print(\"Created output FITS filename for trailer:{}    {}\".format(os.linesep,full_name))\n\n    os.remove(input)", "response": "Convert an ASCII trailer file from IRAF to FITS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_extra_values(conf, _prepend=()):\n    out = []\n\n    out.extend([(_prepend, name) for name in conf.extra_values])\n    for name in conf.sections:\n        if name not in conf.extra_values:\n            out.extend(get_extra_values(conf[name], _prepend + (name,)))\n    return out", "response": "Get all the extra values from a validated\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef popitem(self):\n        sequence = (self.scalars + self.sections)\n        if not sequence:\n            raise KeyError(\": 'popitem(): dictionary is empty'\")\n        key = sequence[0]\n        val =  self[key]\n        del self[key]\n        return key, val", "response": "Removes the first element from the dictionary and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict(self):\n        newdict = {}\n        for entry in self:\n            this_entry = self[entry]\n            if isinstance(this_entry, Section):\n                this_entry = this_entry.dict()\n            elif isinstance(this_entry, list):\n                # create a copy rather than a reference\n                this_entry = list(this_entry)\n            elif isinstance(this_entry, tuple):\n                # create a copy rather than a reference\n                this_entry = tuple(this_entry)\n            newdict[entry] = this_entry\n        return newdict", "response": "Return a deepcopy of self as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming an entry in the dictionary with the given keyname.", "response": "def rename(self, oldkey, newkey):\n        \"\"\"\n        Change a keyname to another, without changing position in sequence.\n\n        Implemented so that transformations can be made on keys,\n        as well as on values. (used by encode and decode)\n\n        Also renames comments.\n        \"\"\"\n        if oldkey in self.scalars:\n            the_list = self.scalars\n        elif oldkey in self.sections:\n            the_list = self.sections\n        else:\n            raise KeyError('Key \"%s\" not found.' % oldkey)\n        pos = the_list.index(oldkey)\n        #\n        val = self[oldkey]\n        dict.__delitem__(self, oldkey)\n        dict.__setitem__(self, newkey, val)\n        the_list.remove(oldkey)\n        the_list.insert(pos, newkey)\n        comm = self.comments[oldkey]\n        inline_comment = self.inline_comments[oldkey]\n        del self.comments[oldkey]\n        del self.inline_comments[oldkey]\n        self.comments[newkey] = comm\n        self.inline_comments[newkey] = inline_comment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walk(self, function, raise_errors=True,\n            call_on_sections=False, **keywargs):\n        \"\"\"\n        Walk every member and call a function on the keyword and value.\n\n        Return a dictionary of the return values\n\n        If the function raises an exception, raise the errror\n        unless ``raise_errors=False``, in which case set the return value to\n        ``False``.\n\n        Any unrecognised keyword arguments you pass to walk, will be pased on\n        to the function you pass in.\n\n        Note: if ``call_on_sections`` is ``True`` then - on encountering a\n        subsection, *first* the function is called for the *whole* subsection,\n        and then recurses into it's members. This means your function must be\n        able to handle strings, dictionaries and lists. This allows you\n        to change the key of subsections as well as for ordinary members. The\n        return value when called on the whole subsection has to be discarded.\n\n        See  the encode and decode methods for examples, including functions.\n\n        admonition:: caution\n\n        You can use ``walk`` to transform the names of members of a section\n        but you mustn't add or delete members.\n\n        >>> config = '''[XXXXsection]\n        ... XXXXkey = XXXXvalue'''.splitlines()\n        >>> cfg = ConfigObj(config)\n        >>> cfg\n        ConfigObj({'XXXXsection': {'XXXXkey': 'XXXXvalue'}})\n        >>> def transform(section, key):\n        ...     val = section[key]\n        ...     newkey = key.replace('XXXX', 'CLIENT1')\n        ...     section.rename(key, newkey)\n        ...     if isinstance(val, (tuple, list, dict)):\n        ...         pass\n        ...     else:\n        ...         val = val.replace('XXXX', 'CLIENT1')\n        ...         section[newkey] = val\n        >>> cfg.walk(transform, call_on_sections=True)\n        {'CLIENT1section': {'CLIENT1key': None}}\n        >>> cfg\n        ConfigObj({'CLIENT1section': {'CLIENT1key': 'CLIENT1value'}})\n        \"\"\"\n        out = {}\n        # scalars first\n        for i in range(len(self.scalars)):\n            entry = self.scalars[i]\n            try:\n                val = function(self, entry, **keywargs)\n                # bound again in case name has changed\n                entry = self.scalars[i]\n                out[entry] = val\n            except Exception:\n                if raise_errors:\n                    raise\n                else:\n                    entry = self.scalars[i]\n                    out[entry] = False\n        # then sections\n        for i in range(len(self.sections)):\n            entry = self.sections[i]\n            if call_on_sections:\n                try:\n                    function(self, entry, **keywargs)\n                except Exception:\n                    if raise_errors:\n                        raise\n                    else:\n                        entry = self.sections[i]\n                        out[entry] = False\n                # bound again in case name has changed\n                entry = self.sections[i]\n            # previous result is discarded\n            out[entry] = self[entry].walk(\n                function,\n                raise_errors=raise_errors,\n                call_on_sections=call_on_sections,\n                **keywargs)\n        return out", "response": "Walks the configuration tree and calls a function on the keyword and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_bool(self, key):\n        val = self[key]\n        if val == True:\n            return True\n        elif val == False:\n            return False\n        else:\n            try:\n                if not isinstance(val, string_types):\n                    # TODO: Why do we raise a KeyError here?\n                    raise KeyError()\n                else:\n                    return self.main._bools[val.lower()]\n            except KeyError:\n                raise ValueError('Value \"%s\" is neither True nor False' % val)", "response": "Returns a value from the ConfigObj corresponding to the given key as a boolean."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore_default(self, key):\n        default = self.default_values[key]\n        dict.__setitem__(self, key, default)\n        if key not in self.defaults:\n            self.defaults.append(key)\n        return default", "response": "Restore the default value for the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restore_defaults(self):\n        for key in self.default_values:\n            self.restore_default(key)\n\n        for section in self.sections:\n            self[section].restore_defaults()", "response": "Recursively restore default values to all members\n        that have them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_bom(self, infile):\n        if ((self.encoding is not None) and\n            (self.encoding.lower() not in BOM_LIST)):\n            # No need to check for a BOM\n            # the encoding specified doesn't have one\n            # just decode\n            return self._decode(infile, self.encoding)\n\n        if isinstance(infile, (list, tuple)):\n            line = infile[0]\n        else:\n            line = infile\n        if self.encoding is not None:\n            # encoding explicitly supplied\n            # And it could have an associated BOM\n            # TODO: if encoding is just UTF16 - we ought to check for both\n            # TODO: big endian and little endian versions.\n            enc = BOM_LIST[self.encoding.lower()]\n            if enc == 'utf_16':\n                # For UTF16 we try big endian and little endian\n                for BOM, (encoding, final_encoding) in list(BOMS.items()):\n                    if not final_encoding:\n                        # skip UTF8\n                        continue\n                    if infile.startswith(BOM):\n                        ### BOM discovered\n                        ##self.BOM = True\n                        # Don't need to remove BOM\n                        return self._decode(infile, encoding)\n\n                # If we get this far, will *probably* raise a DecodeError\n                # As it doesn't appear to start with a BOM\n                return self._decode(infile, self.encoding)\n\n            # Must be UTF8\n            BOM = BOM_SET[enc]\n            if not line.startswith(BOM):\n                return self._decode(infile, self.encoding)\n\n            newline = line[len(BOM):]\n\n            # BOM removed\n            if isinstance(infile, (list, tuple)):\n                infile[0] = newline\n            else:\n                infile = newline\n            self.BOM = True\n            return self._decode(infile, self.encoding)\n\n        # No encoding specified - so we need to check for UTF8/UTF16\n        for BOM, (encoding, final_encoding) in list(BOMS.items()):\n            if not isinstance(BOM, str) or not line.startswith(BOM):\n                continue\n            else:\n                # BOM discovered\n                self.encoding = final_encoding\n                if not final_encoding:\n                    self.BOM = True\n                    # UTF8\n                    # remove BOM\n                    newline = line[len(BOM):]\n                    if isinstance(infile, (list, tuple)):\n                        infile[0] = newline\n                    else:\n                        infile = newline\n                    # UTF8 - don't decode\n                    if isinstance(infile, string_types):\n                        return infile.splitlines(True)\n                    else:\n                        return infile\n                # UTF16 - have to decode\n                return self._decode(infile, encoding)\n\n        # No BOM discovered and no encoding specified, just return\n        if isinstance(infile, string_types):\n            # infile read from a file will be a single string\n            return infile.splitlines(True)\n        return infile", "response": "Handle any BOM and decode if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _decode(self, infile, encoding):\n        if isinstance(infile, string_types):\n            # can't be unicode\n            # NOTE: Could raise a ``UnicodeDecodeError``\n            return infile.decode(encoding).splitlines(True)\n        for i, line in enumerate(infile):\n            # NOTE: The isinstance test here handles mixed lists of unicode/string\n            # NOTE: But the decode will break on any non-string values\n            # NOTE: Or could raise a ``UnicodeDecodeError``\n            if PY3K:\n                if not isinstance(line, str):\n                    infile[i] = line.decode(encoding)\n            else:\n                if not isinstance(line, unicode):\n                    infile[i] = line.decode(encoding)\n        return infile", "response": "Decode infile to unicode using the specified encoding."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding element to unicode if necessary.", "response": "def _decode_element(self, line):\n        \"\"\"Decode element to unicode if necessary.\"\"\"\n        if not self.encoding:\n            return line\n        if isinstance(line, str) and self.default_encoding:\n            return line.decode(self.default_encoding)\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _match_depth(self, sect, depth):\n        while depth < sect.depth:\n            if sect is sect.parent:\n                # we've reached the top level already\n                raise SyntaxError()\n            sect = sect.parent\n        if sect.depth == depth:\n            return sect\n        # shouldn't get here\n        raise SyntaxError()", "response": "Given a section and a depth level return a reference to the right section."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling an error according to the error settings.", "response": "def _handle_error(self, text, ErrorClass, infile, cur_index):\n        \"\"\"\n        Handle an error according to the error settings.\n\n        Either raise the error or store it.\n        The error will have occured at ``cur_index``\n        \"\"\"\n        line = infile[cur_index]\n        cur_index += 1\n        message = text % cur_index\n        error = ErrorClass(message, cur_index, line)\n        if self.raise_errors:\n            # raise the error - parsing stops here\n            raise error\n        # store the error\n        # reraise when parsing has finished\n        self._errors.append(error)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _unquote(self, value):\n        if not value:\n            # should only happen during parsing of lists\n            raise SyntaxError\n        if (value[0] == value[-1]) and (value[0] in ('\"', \"'\")):\n            value = value[1:-1]\n        return value", "response": "Return an unquoted version of a value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _quote(self, value, multiline=True):\n        if multiline and self.write_empty_values and value == '':\n            # Only if multiline is set, so that it is used for values not\n            # keys, and not values that are part of a list\n            return ''\n\n        if multiline and isinstance(value, (list, tuple)):\n            if not value:\n                return ','\n            elif len(value) == 1:\n                return self._quote(value[0], multiline=False) + ','\n            return ', '.join([self._quote(val, multiline=False)\n                for val in value])\n        if not isinstance(value, string_types):\n            if self.stringify:\n                value = str(value)\n            else:\n                raise TypeError('Value \"%s\" is not a string.' % value)\n\n        if not value:\n            return '\"\"'\n\n        no_lists_no_quotes = not self.list_values and '\\n' not in value and '#' not in value\n        need_triple = multiline and (((\"'\" in value) and ('\"' in value)) or ('\\n' in value ))\n        hash_triple_quote = multiline and not need_triple and (\"'\" in value) and ('\"' in value) and ('#' in value)\n        check_for_single = (no_lists_no_quotes or not need_triple) and not hash_triple_quote\n\n        if check_for_single:\n            if not self.list_values:\n                # we don't quote if ``list_values=False``\n                quot = noquot\n            # for normal values either single or double quotes will do\n            elif '\\n' in value:\n                # will only happen if multiline is off - e.g. '\\n' in key\n                raise ConfigObjError('Value \"%s\" cannot be safely quoted.' % value)\n            elif ((value[0] not in wspace_plus) and\n                    (value[-1] not in wspace_plus) and\n                    (',' not in value)):\n                quot = noquot\n            else:\n                quot = self._get_single_quote(value)\n        else:\n            # if value has '\\n' or \"'\" *and* '\"', it will need triple quotes\n            quot = self._get_triple_quote(value)\n\n        if quot == noquot and '#' in value and self.list_values:\n            quot = self._get_single_quote(value)\n\n        return quot % value", "response": "Quote a value for a config object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_value(self, value):\n        if self._inspec:\n            # Parsing a configspec so don't handle comments\n            return (value, '')\n        # do we look for lists in values ?\n        if not self.list_values:\n            mat = self._nolistvalue.match(value)\n            if mat is None:\n                raise SyntaxError()\n            # NOTE: we don't unquote here\n            return mat.groups()\n        #\n        mat = self._valueexp.match(value)\n        if mat is None:\n            # the value is badly constructed, probably badly quoted,\n            # or an invalid list\n            raise SyntaxError()\n        (list_values, single, empty_list, comment) = mat.groups()\n        if (list_values == '') and (single is None):\n            # change this if you want to accept empty values\n            raise SyntaxError()\n        # NOTE: note there is no error handling from here if the regex\n        # is wrong: then incorrect values will slip through\n        if empty_list is not None:\n            # the single comma - meaning an empty list\n            return ([], comment)\n        if single is not None:\n            # handle empty values\n            if list_values and not single:\n                # FIXME: the '' is a workaround because our regex now matches\n                #   '' at the end of a list if it has a trailing comma\n                single = None\n            else:\n                single = single or '\"\"'\n                single = self._unquote(single)\n        if list_values == '':\n            # not a list value\n            return (single, comment)\n        the_list = self._listvalueexp.findall(list_values)\n        the_list = [self._unquote(val) for val in the_list]\n        if single is not None:\n            the_list += [single]\n        return (the_list, comment)", "response": "Handle a value string and return a tuple of the list of the names and comments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _multiline(self, value, infile, cur_index, maxline):\n        quot = value[:3]\n        newvalue = value[3:]\n        single_line = self._triple_quote[quot][0]\n        multi_line = self._triple_quote[quot][1]\n        mat = single_line.match(value)\n        if mat is not None:\n            retval = list(mat.groups())\n            retval.append(cur_index)\n            return retval\n        elif newvalue.find(quot) != -1:\n            # somehow the triple quote is missing\n            raise SyntaxError()\n        #\n        while cur_index < maxline:\n            cur_index += 1\n            newvalue += '\\n'\n            line = infile[cur_index]\n            if line.find(quot) == -1:\n                newvalue += line\n            else:\n                # end of multiline, process it\n                break\n        else:\n            # we've got to the end of the config, oops...\n            raise SyntaxError()\n        mat = multi_line.match(line)\n        if mat is None:\n            # a badly formed line\n            raise SyntaxError()\n        (value, comment) = mat.groups()\n        return (newvalue + value, comment, cur_index)", "response": "Extract the value where we are in a multiline situation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the configspec and parse it into a ConfigObj object.", "response": "def _handle_configspec(self, configspec):\n        \"\"\"Parse the configspec.\"\"\"\n        # FIXME: Should we check that the configspec was created with the\n        #        correct settings ? (i.e. ``list_values=False``)\n        if not isinstance(configspec, ConfigObj):\n            try:\n                configspec = ConfigObj(configspec,\n                                       raise_errors=True,\n                                       file_error=True,\n                                       _inspec=True)\n            except ConfigObjError as e:\n                # FIXME: Should these errors have a reference\n                #        to the already parsed ConfigObj ?\n                raise ConfigspecError('Parsing configspec failed: %s' % e)\n            except IOError as e:\n                raise IOError('Reading configspec failed: %s' % e)\n\n        self.configspec = configspec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the configspec on the section.", "response": "def _set_configspec(self, section, copy):\n        \"\"\"\n        Called by validate. Handles setting the configspec on subsections\n        including sections to be validated by __many__\n        \"\"\"\n        configspec = section.configspec\n        many = configspec.get('__many__')\n        if isinstance(many, dict):\n            for entry in section.sections:\n                if entry not in configspec:\n                    section[entry].configspec = many\n\n        for entry in configspec.sections:\n            if entry == '__many__':\n                continue\n            if entry not in section:\n                section[entry] = {}\n                section[entry]._created = True\n                if copy:\n                    # copy comments\n                    section.comments[entry] = configspec.comments.get(entry, [])\n                    section.inline_comments[entry] = configspec.inline_comments.get(entry, '')\n\n            # Could be a scalar when we expect a section\n            if isinstance(section[entry], Section):\n                section[entry].configspec = configspec[entry]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting an individual line for the write method.", "response": "def _write_line(self, indent_string, entry, this_entry, comment):\n        \"\"\"Write an individual line, for the write method\"\"\"\n        # NOTE: the calls to self._quote here handles non-StringType values.\n        if not self.unrepr:\n            val = self._decode_element(self._quote(this_entry))\n        else:\n            val = repr(this_entry)\n        return '%s%s%s%s%s' % (indent_string,\n                               self._decode_element(self._quote(entry, multiline=False)),\n                               self._a_to_u(' = '),\n                               val,\n                               self._decode_element(comment))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_marker(self, indent_string, depth, entry, comment):\n        return '%s%s%s%s%s' % (indent_string,\n                               self._a_to_u('[' * depth),\n                               self._quote(self._decode_element(entry), multiline=False),\n                               self._a_to_u(']' * depth),\n                               self._decode_element(comment))", "response": "Write a section marker line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_comment(self, comment):\n        if not comment:\n            return ''\n        start = self.indent_type\n        if not comment.startswith('#'):\n            start += self._a_to_u(' # ')\n        return (start + comment)", "response": "Deal with a comment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, outfile=None, section=None):\n        if self.indent_type is None:\n            # this can be true if initialised from a dictionary\n            self.indent_type = DEFAULT_INDENT_TYPE\n\n        out = []\n        cs = self._a_to_u('#')\n        csp = self._a_to_u('# ')\n        if section is None:\n            int_val = self.interpolation\n            self.interpolation = False\n            section = self\n            for line in self.initial_comment:\n                line = self._decode_element(line)\n                stripped_line = line.strip()\n                if stripped_line and not stripped_line.startswith(cs):\n                    line = csp + line\n                out.append(line)\n\n        indent_string = self.indent_type * section.depth\n        for entry in (section.scalars + section.sections):\n            if entry in section.defaults:\n                # don't write out default values\n                continue\n            for comment_line in section.comments[entry]:\n                comment_line = self._decode_element(comment_line.lstrip())\n                if comment_line and not comment_line.startswith(cs):\n                    comment_line = csp + comment_line\n                out.append(indent_string + comment_line)\n            this_entry = section[entry]\n            comment = self._handle_comment(section.inline_comments[entry])\n\n            if isinstance(this_entry, dict):\n                # a section\n                out.append(self._write_marker(\n                    indent_string,\n                    this_entry.depth,\n                    entry,\n                    comment))\n                out.extend(self.write(section=this_entry))\n            else:\n                out.append(self._write_line(\n                    indent_string,\n                    entry,\n                    this_entry,\n                    comment))\n\n        if section is self:\n            for line in self.final_comment:\n                line = self._decode_element(line)\n                stripped_line = line.strip()\n                if stripped_line and not stripped_line.startswith(cs):\n                    line = csp + line\n                out.append(line)\n            self.interpolation = int_val\n\n        if section is not self:\n            return out\n\n        if (self.filename is None) and (outfile is None):\n            # output a list of lines\n            # might need to encode\n            # NOTE: This will *screw* UTF16, each line will start with the BOM\n            if self.encoding:\n                out = [l.encode(self.encoding) for l in out]\n            if (self.BOM and ((self.encoding is None) or\n                (BOM_LIST.get(self.encoding.lower()) == 'utf_8'))):\n                # Add the UTF8 BOM\n                if not out:\n                    out.append('')\n                out[0] = BOM_UTF8 + out[0]\n            return out\n\n        # Turn the list to a string, joined with correct newlines\n        newline = self.newlines or os.linesep\n        if (getattr(outfile, 'mode', None) is not None and outfile.mode == 'w'\n            and sys.platform == 'win32' and newline == '\\r\\n'):\n            # Windows specific hack to avoid writing '\\r\\r\\n'\n            newline = '\\n'\n        output = self._a_to_u(newline).join(out)\n        if self.encoding:\n            output = output.encode(self.encoding)\n        if self.BOM and ((self.encoding is None) or match_utf8(self.encoding)):\n            # Add the UTF8 BOM\n            output = BOM_UTF8 + output\n\n        if not output.endswith(newline):\n            output += newline\n        if outfile is not None:\n            outfile.write(output)\n        else:\n            # !!! write mode was 'wb' but that fails in PY3K and we dont need\n            h = open(self.filename, 'w')\n            h.write(output)\n            h.close()", "response": "Write the current ConfigObj as a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the current ConfigObj against a given validator.", "response": "def validate(self, validator, preserve_errors=False, copy=False,\n                 section=None):\n        \"\"\"\n        Test the ConfigObj against a configspec.\n\n        It uses the ``validator`` object from *validate.py*.\n\n        To run ``validate`` on the current ConfigObj, call: ::\n\n            test = config.validate(validator)\n\n        (Normally having previously passed in the configspec when the ConfigObj\n        was created - you can dynamically assign a dictionary of checks to the\n        ``configspec`` attribute of a section though).\n\n        It returns ``True`` if everything passes, or a dictionary of\n        pass/fails (True/False). If every member of a subsection passes, it\n        will just have the value ``True``. (It also returns ``False`` if all\n        members fail).\n\n        In addition, it converts the values from strings to their native\n        types if their checks pass (and ``stringify`` is set).\n\n        If ``preserve_errors`` is ``True`` (``False`` is default) then instead\n        of a marking a fail with a ``False``, it will preserve the actual\n        exception object. This can contain info about the reason for failure.\n        For example the ``VdtValueTooSmallError`` indicates that the value\n        supplied was too small. If a value (or section) is missing it will\n        still be marked as ``False``.\n\n        You must have the validate module to use ``preserve_errors=True``.\n\n        You can then use the ``flatten_errors`` function to turn your nested\n        results dictionary into a flattened list of failures - useful for\n        displaying meaningful error messages.\n        \"\"\"\n        if section is None:\n            if self.configspec is None:\n                raise ValueError('No configspec supplied.')\n            if preserve_errors:\n                # We do this once to remove a top level dependency on the validate module\n                # Which makes importing configobj faster\n                from .validate import VdtMissingValue\n                self._vdtMissingValue = VdtMissingValue\n\n            section = self\n\n            if copy:\n                section.initial_comment = section.configspec.initial_comment\n                section.final_comment = section.configspec.final_comment\n                section.encoding = section.configspec.encoding\n                section.BOM = section.configspec.BOM\n                section.newlines = section.configspec.newlines\n                section.indent_type = section.configspec.indent_type\n\n        #\n        # section.default_values.clear() #??\n        configspec = section.configspec\n        self._set_configspec(section, copy)\n\n\n        def validate_entry(entry, spec, val, missing, ret_true, ret_false):\n            section.default_values.pop(entry, None)\n\n            try:\n                section.default_values[entry] = validator.get_default_value(configspec[entry])\n            except (KeyError, AttributeError, validator.baseErrorClass):\n                # No default, bad default or validator has no 'get_default_value'\n                # (e.g. SimpleVal)\n                pass\n\n            try:\n                check = validator.check(spec,\n                                        val,\n                                        missing=missing\n                                        )\n            except validator.baseErrorClass as e:\n                if not preserve_errors or isinstance(e, self._vdtMissingValue):\n                    out[entry] = False\n                else:\n                    # preserve the error\n                    out[entry] = e\n                    ret_false = False\n                ret_true = False\n            else:\n                ret_false = False\n                out[entry] = True\n                if self.stringify or missing:\n                    # if we are doing type conversion\n                    # or the value is a supplied default\n                    if not self.stringify:\n                        if isinstance(check, (list, tuple)):\n                            # preserve lists\n                            check = [self._str(item) for item in check]\n                        elif missing and check is None:\n                            # convert the None from a default to a ''\n                            check = ''\n                        else:\n                            check = self._str(check)\n                    if (check != val) or missing:\n                        section[entry] = check\n                if not copy and missing and entry not in section.defaults:\n                    section.defaults.append(entry)\n            return ret_true, ret_false\n\n        #\n        out = {}\n        ret_true = True\n        ret_false = True\n\n        unvalidated = [k for k in section.scalars if k not in configspec]\n        incorrect_sections = [k for k in configspec.sections if k in section.scalars]\n        incorrect_scalars = [k for k in configspec.scalars if k in section.sections]\n\n        for entry in configspec.scalars:\n            if entry in ('__many__', '___many___'):\n                # reserved names\n                continue\n            if (not entry in section.scalars) or (entry in section.defaults):\n                # missing entries\n                # or entries from defaults\n                missing = True\n                val = None\n                if copy and entry not in section.scalars:\n                    # copy comments\n                    section.comments[entry] = (\n                        configspec.comments.get(entry, []))\n                    section.inline_comments[entry] = (\n                        configspec.inline_comments.get(entry, ''))\n                #\n            else:\n                missing = False\n                val = section[entry]\n\n            ret_true, ret_false = validate_entry(entry, configspec[entry], val,\n                                                 missing, ret_true, ret_false)\n\n        many = None\n        if '__many__' in configspec.scalars:\n            many = configspec['__many__']\n        elif '___many___' in configspec.scalars:\n            many = configspec['___many___']\n\n        if many is not None:\n            for entry in unvalidated:\n                val = section[entry]\n                ret_true, ret_false = validate_entry(entry, many, val, False,\n                                                     ret_true, ret_false)\n            unvalidated = []\n\n        for entry in incorrect_scalars:\n            ret_true = False\n            if not preserve_errors:\n                out[entry] = False\n            else:\n                ret_false = False\n                msg = 'Value %r was provided as a section' % entry\n                out[entry] = validator.baseErrorClass(msg)\n        for entry in incorrect_sections:\n            ret_true = False\n            if not preserve_errors:\n                out[entry] = False\n            else:\n                ret_false = False\n                msg = 'Section %r was provided as a single value' % entry\n                out[entry] = validator.baseErrorClass(msg)\n\n        # Missing sections will have been created as empty ones when the\n        # configspec was read.\n        for entry in section.sections:\n            # FIXME: this means DEFAULT is not copied in copy mode\n            if section is self and entry == 'DEFAULT':\n                continue\n            if section[entry].configspec is None:\n                unvalidated.append(entry)\n                continue\n            if copy:\n                section.comments[entry] = configspec.comments.get(entry, [])\n                section.inline_comments[entry] = configspec.inline_comments.get(entry, '')\n            check = self.validate(validator, preserve_errors=preserve_errors, copy=copy, section=section[entry])\n            out[entry] = check\n            if check == False:\n                ret_true = False\n            elif check == True:\n                ret_false = False\n            else:\n                ret_true = False\n\n        section.extra_values = unvalidated\n        if preserve_errors and not section._created:\n            # If the section wasn't created (i.e. it wasn't missing)\n            # then we can't return False, we need to preserve errors\n            ret_false = False\n        #\n        if ret_false and preserve_errors and out:\n            # If we are preserving errors, but all\n            # the failures are from missing sections / values\n            # then we can return False. Otherwise there is a\n            # real failure that we need to preserve.\n            ret_false = not any(out.values())\n        if ret_true:\n            return True\n        elif ret_false:\n            return False\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n        self.clear()\n        self._initialise()\n        # FIXME: Should be done by '_initialise', but ConfigObj constructor (and reload)\n        #        requires an empty dictionary\n        self.configspec = None\n        # Just to be sure ;-)\n        self._original_configspec = None", "response": "Clear ConfigObj instance and restore to freshly created state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload(self):\n        if not isinstance(self.filename, string_types):\n            raise ReloadError()\n\n        filename = self.filename\n        current_options = {}\n        for entry in OPTION_DEFAULTS:\n            if entry == 'configspec':\n                continue\n            current_options[entry] = getattr(self, entry)\n\n        configspec = self._original_configspec\n        current_options['configspec'] = configspec\n\n        self.clear()\n        self._initialise(current_options)\n        self._load(filename, configspec)", "response": "Reload a ConfigObj from file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the command line arguments for the getnova - tree - structure conversion and convertwaiveredfits. py module.", "response": "def _processCommandLineArgs():\n    \"\"\"\n        Get the command line arguments\n\n        Parameters: NONE\n\n        Returns:\n\n           files            list of file specifications to be converted\n\n           outputFileNames  list of output file specifications\n                             (one per input file)\n                             Default: a list of None values (one per input file)\n\n           conversionFormat string indicating the conversion format requested\n                             Default: \"mulitextension\"\n\n           verbose          flag indicating if verbose output is desired\n                             Default: False\n\n        Exceptions: NONE\n    \"\"\"\n\n    import getopt\n\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], \"hvmo:\",\n                                   [\"help\",\n                                    \"verbose\",\n                                    \"multiExtensionConversion\",\n                                    \"outputFileName\"])\n    except getopt.GetoptError as e:\n        print(str(e))\n        _usage()\n        sys.exit(1)\n\n    conversionFormat = \"\"\n    outputFileNames = []\n    verbose = False\n\n    for o, a in opts:\n        if o in (\"-h\", \"--help\"):\n            _usage()\n            print(\"       Convert the waivered FITS Files (FILEs) to various formats.\")\n            print(\"       The default conversion format is multi-extension FITS.\")\n            print(\"       Options:\")\n            print(\"         -h,  --help                       display this help message and exit\")\n            print(\"         -v,  --verbose                    provide verbose output\")\n            print(\"         -m,  --multiExtensionConversion   convert to multiExtension FITS format\")\n            print(\"         -o,  --outputFileName             comma separated list of output file\")\n            print(\"                                           specifications (one per input FILE)\")\n            sys.exit()\n\n        if o in (\"-v\", \"--verbose\"):\n            verbose = True\n\n        if o in (\"-m\", \"--multiExtensionConversion\"):\n            if conversionFormat != \"\":\n                print(\"convertwaiveredfits.py: only one conversion format allowed\")\n                _usage()\n                sys.exit(1)\n\n            conversionFormat = \"multiExtension\"\n\n        if o in (\"-o\", \"--outputFileName\"):\n            outputFileNames = a.split(',')\n\n    if conversionFormat == \"\":\n        #\n        # Set the default conversion format if none was provided\n        #\n        conversionFormat = \"multiExtension\"\n\n    if not args:\n        print(\"convertwaiveredfits.py: nothing to convert\")\n        _usage()\n        sys.exit(1)\n    else:\n        files = args\n\n        if outputFileNames:\n            if len(files) != len(outputFileNames):\n                print(\"convertwaiveredfits.py: number of output file names does not match\")\n                print(\"                        the number of FILEs to convert\")\n                _usage()\n                sys.exit(1)\n        else:\n            for i in range(0,len(files)):\n                outputFileNames.append(None)\n\n    return files,outputFileNames,conversionFormat,verbose"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _verify(waiveredHdul):\n\n    if len(waiveredHdul) == 2:\n        #\n        # There must be exactly 2 HDU's\n        #\n        if waiveredHdul[0].header['NAXIS'] > 0:\n            #\n            # The Primary HDU must have some data\n            #\n            if isinstance(waiveredHdul[1], fits.TableHDU):\n                #\n                # The Alternate HDU must be a TableHDU\n                #\n                if waiveredHdul[0].data.shape[0] == \\\n                   waiveredHdul[1].data.shape[0] or \\\n                   waiveredHdul[1].data.shape[0] == 1:\n                    #\n                    # The number of arrays in the Primary HDU must match\n                    # the number of rows in the TableHDU.  This includes\n                    # the case where there is only a single array and row.\n                    #\n                    return\n    #\n    # Not a valid waivered Fits file\n    #\n    raise ValueError(\"Input object does not represent a valid waivered\" + \\\n                      \" FITS file\")", "response": "Verify that the input HDUList is for a waivered FITS file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the input waivered FITS object to a multi - extension FITS file.", "response": "def toMultiExtensionFits(waiveredObject,\n                         multiExtensionFileName=None,\n                         forceFileOutput=False,\n                         verbose=False):\n    \"\"\"\n        Convert the input waivered FITS object to a multi-extension FITS\n        HDUList object.  Generate an output multi-extension FITS file if\n        requested.\n\n        Parameters:\n\n          waiveredObject  input object representing a waivered FITS file;\n                          either a astroyp.io.fits.HDUList object, a file object, or a\n                          file specification\n\n          outputFileName  file specification for the output file\n                          Default: None - do not generate an output file\n\n          forceFileOutput force the generation of an output file when the\n                          outputFileName parameter is None; the output file\n                          specification will be the same as the input file\n                          specification with the last character of the base\n                          name replaced with the character 'h'.\n                          Default: False\n\n          verbose         provide verbose output\n                          Default: False\n\n        Returns:\n\n          mhdul           an HDUList object in multi-extension FITS format.\n\n        Exceptions:\n\n          TypeError       Input object is not a HDUList, a file object or a\n                          file name\n    \"\"\"\n\n    if isinstance(waiveredObject, fits.HDUList):\n        whdul = waiveredObject\n        inputObjectDescription = \"HDUList object\"\n    else:\n        try:\n            whdul = fits.open(waiveredObject)\n            if isinstance(waiveredObject, string_types):\n                inputObjectDescription = \"file \" + waiveredObject\n            else:\n                inputObjectDescription = \"file \" + waiveredObject.name\n        except TypeError:\n            raise TypeError(\"Input object must be HDUList, file object, \" + \\\n                            \"or file name\")\n\n    _verify(whdul)\n\n    undesiredPrimaryHeaderKeywords = ['ORIGIN','FITSDATE','FILENAME',\n                                      'ALLG-MAX','ALLG-MIN','ODATTYPE',\n                                      'SDASMGNU','OPSIZE','CTYPE2',\n                                      'CD2_2','CD2_1','CD1_2','CTYPE3',\n                                      'CD3_3','CD3_1','CD1_3','CD2_3',\n                                      'CD3_2']\n    #\n    # Create the multi-extension primary header as a copy of the\n    # wavered file primary header\n    #\n    mPHeader = whdul[0].header\n    originalDataType =  whdul[0].header.get('ODATTYPE','')\n    #\n    # Remove primary header cards with keywords matching the\n    # list of undesired primary header keywords\n    #\n    for keyword in undesiredPrimaryHeaderKeywords:\n        #\n        # Be careful only to delete the first card that matches\n        # the keyword, not all of the cards\n        #\n        if keyword in mPHeader:\n            del mPHeader[mPHeader.index(keyword)]\n    #\n    # Get the columns from the secondary HDU table\n    #\n    wcols = whdul[1].columns\n    #\n    # Remove primary header cards with keywords matching the\n    # column names in the secondary HDU table\n    #\n    for keyword in wcols.names:\n        if keyword in mPHeader:\n            del mPHeader[keyword]\n    #\n    # Create the PrimaryHDU\n    #\n    mPHdu = fits.PrimaryHDU(header=mPHeader)\n    #\n    # Add the EXTEND card\n    #\n    mPHdu.header.set('EXTEND', value=True, after='NAXIS')\n    #\n    # Add the NEXTEND card.  There will be one extension\n    # for each row in the wavered Fits file table HDU.\n    #\n    mPHdu.header['NEXTEND'] = (whdul[1].data.shape[0],\n                               'Number of standard extensions')\n    #\n    # Create the multi-extension file HDUList from the primary header\n    #\n    mhdul = fits.HDUList([mPHdu])\n    #\n    # Create the extension HDUs for the multi-extension file.  There\n    # will be one extension for each row in the wavered file's table.\n    #\n    instrument = mPHeader.get('INSTRUME', '')\n    nrows = whdul[1].data.shape[0]\n\n    for i in range(0,nrows):\n        #\n        # Create the basic HDU from the data\n        #\n        if nrows == 1:\n            #\n            # Handle case where there is only one row in the table\n            #\n            data = whdul[0].data\n        else:\n            data = whdul[0].data[i]\n\n        mhdul.append(fits.ImageHDU(data))\n        #\n        # Add cards to the header for each keyword in the column\n        # names of the secondary HDU table from the wavered file\n        #\n        for keyword,format,unit in zip(wcols.names,wcols.formats,wcols.units):\n            if unit == 'LOGICAL-':\n                #\n                # Handle logical values\n                #\n                if whdul[1].data.field(keyword)[i].strip() == 'T':\n                    d = True\n                else:\n                    d = False\n            elif format[0] == 'E':\n                #\n                # Handle floating point values\n                #\n                fmt = '%'+format[1:]+'G'\n                d = eval(fmt % float(whdul[1].data.field(keyword)[i]))\n            else:\n                d = whdul[1].data.field(keyword)[i]\n\n            kw_descr = \"\"\n            if keyword in whdul[1].header:\n                kw_descr = whdul[1].header[keyword]\n            mhdul[i+1].header[keyword] = (d, kw_descr)\n        #\n        # If original data is unsigned short then scale the data.\n        #\n        if originalDataType == 'USHORT':\n            mhdul[i+1].scale('int16','',bscale=1,bzero=32768)\n            mhdul[i+1].header.set('BSCALE', value=1, before='BZERO')\n        #\n        # For WFPC2 and FOS instruments require additional header cards\n        #\n        if instrument in ('WFPC2','FOC'):\n            #\n            # Add EXTNAME card to header\n            #\n            mhdul[i+1].header['EXTNAME'] = (mPHeader.get('FILETYPE',''),\n                                            'extension name')\n            #\n            # Add EXTVER card to the header\n            #\n            mhdul[i+1]._extver = i+1\n            mhdul[i+1].header.set('EXTVER', value=i+1,\n                                  comment='extension version number',\n                                  after='EXTNAME')\n            #\n            # Add the EXPNAME card to the header\n            #\n            mhdul[i+1].header.set('EXPNAME',\n                                  mPHeader.get('ROOTNAME', ''),\n                                  '9 character exposure identifier',\n                                  before='EXTVER')\n            #\n            # Add the INHERIT card to the header.\n            #\n            mhdul[i+1].header.set('INHERIT', True,\n                                  'inherit the primary header',\n                                  after='EXTVER')\n            #\n            # Add the ROOTNAME card to the header\n            #\n            mhdul[i+1].header.set('ROOTNAME',\n                                  mPHeader.get('ROOTNAME', ''),\n                                  'rootname of the observationset',\n                                  after='INHERIT')\n\n    if not multiExtensionFileName and forceFileOutput:\n        base,ext = os.path.splitext(whdul[0]._file.name)\n        multiExtensionFileName = base[:-1]+'h'+ext\n\n    verboseString = \"Input \" + inputObjectDescription + \\\n                    \" converted to multi-extension FITS format.\"\n\n    if multiExtensionFileName:\n        if instrument in ('WFPC2','FOC'):\n            #\n            # write the FILENAME card to the header for the WFPC2 and FOC\n            # instruments\n            #\n            head,tail = os.path.split(multiExtensionFileName)\n            mhdul[0].header.set('FILENAME', value=tail, after='NEXTEND')\n\n        if ASTROPY_VER_GE13:\n            mhdul.writeto(multiExtensionFileName, overwrite=True)\n        else:\n            mhdul.writeto(multiExtensionFileName, clobber=True)\n\n        verboseString = verboseString[:-1] + \" and written to \" + \\\n                        multiExtensionFileName + \".\"\n\n    if verbose:\n        print(verboseString)\n\n    return mhdul"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a waivered FITS object to various formats.", "response": "def convertwaiveredfits(waiveredObject,\n                        outputFileName=None,\n                        forceFileOutput=False,\n                        convertTo='multiExtension',\n                        verbose=False):\n    \"\"\"\n        Convert the input waivered FITS object to various formats.  The\n        default conversion format is multi-extension FITS.  Generate an output\n        file in the desired format if requested.\n\n        Parameters:\n\n          waiveredObject  input object representing a waivered FITS file;\n                          either a astropy.io.fits.HDUList object, a file object, or a\n                          file specification\n\n          outputFileName  file specification for the output file\n                          Default: None - do not generate an output file\n\n          forceFileOutput force the generation of an output file when the\n                          outputFileName parameter is None; the output file\n                          specification will be the same as the input file\n                          specification with the last character of the base\n                          name replaced with the character `h` in\n                          multi-extension FITS format.\n\n                          Default: False\n\n          convertTo       target conversion type\n                          Default: 'multiExtension'\n\n          verbose         provide verbose output\n                          Default: False\n\n        Returns:\n\n          hdul            an HDUList object in the requested format.\n\n        Exceptions:\n\n           ValueError       Conversion type is unknown\n    \"\"\"\n\n    if convertTo == 'multiExtension':\n        func = toMultiExtensionFits\n    else:\n        raise ValueError('Conversion type ' + convertTo + ' unknown')\n\n    return func(*(waiveredObject,outputFileName,forceFileOutput,verbose))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_jd(year, month, day):\n    '''Determine Julian day from Persian date'''\n\n    if year >= 0:\n        y = 474\n    else:\n        y = 473\n    epbase = year - y\n    epyear = 474 + (epbase % 2820)\n\n    if month <= 7:\n        m = (month - 1) * 31\n    else:\n        m = (month - 1) * 30 + 6\n\n    return day + m + trunc(((epyear * 682) - 110) / 2816) + (epyear - 1) * 365 + trunc(epbase / 2820) * 1029983 + (EPOCH - 1)", "response": "Determine Julian day from Persian date"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_jd(jd):\n    '''Calculate Persian date from Julian day'''\n    jd = trunc(jd) + 0.5\n\n    depoch = jd - to_jd(475, 1, 1)\n    cycle = trunc(depoch / 1029983)\n    cyear = (depoch % 1029983)\n\n    if cyear == 1029982:\n        ycycle = 2820\n    else:\n        aux1 = trunc(cyear / 366)\n        aux2 = cyear % 366\n        ycycle = trunc(((2134 * aux1) + (2816 * aux2) + 2815) / 1028522) + aux1 + 1\n\n    year = ycycle + (2820 * cycle) + 474\n\n    if (year <= 0):\n        year -= 1\n\n    yday = (jd - to_jd(year, 1, 1)) + 1\n\n    if yday <= 186:\n        month = ceil(yday / 31)\n    else:\n        month = ceil((yday - 6) / 30)\n\n    day = int(jd - to_jd(year, month, 1)) + 1\n\n    return (year, month, day)", "response": "Calculate Persian date from Julian day"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize capture of stdout stderr Python warnings and exceptions and redirect them to the loggers for the modules from which they originated them.", "response": "def setup_global_logging():\n    \"\"\"\n    Initializes capture of stdout/stderr, Python warnings, and exceptions;\n    redirecting them to the loggers for the modules from which they originated.\n    \"\"\"\n\n    global global_logging_started\n\n    if not PY3K:\n        sys.exc_clear()\n\n    if global_logging_started:\n        return\n\n    orig_logger_class = logging.getLoggerClass()\n    logging.setLoggerClass(StreamTeeLogger)\n    try:\n        stdout_logger = logging.getLogger(__name__ + '.stdout')\n        stderr_logger = logging.getLogger(__name__ + '.stderr')\n    finally:\n        logging.setLoggerClass(orig_logger_class)\n\n    stdout_logger.setLevel(logging.INFO)\n    stderr_logger.setLevel(logging.ERROR)\n    stdout_logger.set_stream(sys.stdout)\n    stderr_logger.set_stream(sys.stderr)\n    sys.stdout = stdout_logger\n    sys.stderr = stderr_logger\n\n    exception_logger = logging.getLogger(__name__ + '.exc')\n    sys.excepthook = LoggingExceptionHook(exception_logger)\n\n    logging.captureWarnings(True)\n\n    rawinput = 'input' if PY3K else 'raw_input'\n    builtins._original_raw_input = getattr(builtins, rawinput)\n    setattr(builtins, rawinput, global_logging_raw_input)\n\n    global_logging_started = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef teardown_global_logging():\n\n    global global_logging_started\n    if not global_logging_started:\n        return\n\n    stdout_logger = logging.getLogger(__name__ + '.stdout')\n    stderr_logger = logging.getLogger(__name__ + '.stderr')\n    if sys.stdout is stdout_logger:\n        sys.stdout = sys.stdout.stream\n    if sys.stderr is stderr_logger:\n        sys.stderr = sys.stderr.stream\n\n    # If we still have an unhandled exception go ahead and handle it with the\n    # replacement excepthook before deleting it\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    if exc_type is not None:\n        sys.excepthook(exc_type, exc_value, exc_traceback)\n    del exc_type\n    del exc_value\n    del exc_traceback\n    if not PY3K:\n        sys.exc_clear()\n\n    del sys.excepthook\n    logging.captureWarnings(False)\n\n    rawinput = 'input' if PY3K else 'raw_input'\n    if hasattr(builtins, '_original_raw_input'):\n        setattr(builtins, rawinput, builtins._original_raw_input)\n        del builtins._original_raw_input\n\n    global_logging_started = False", "response": "Disable global logging of stdio warnings and exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a logger object for the specified name.", "response": "def create_logger(name, format='%(levelname)s: %(message)s', datefmt=None,\n                  stream=None, level=logging.INFO, filename=None, filemode='w',\n                  filelevel=None, propagate=True):\n    \"\"\"\n    Do basic configuration for the logging system. Similar to\n    logging.basicConfig but the logger ``name`` is configurable and both a file\n    output and a stream output can be created. Returns a logger object.\n\n    The default behaviour is to create a logger called ``name`` with a null\n    handled, and to use the \"%(levelname)s: %(message)s\" format string, and add\n    the handler to the ``name`` logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    :param name: Logger name\n    :param format: handler format string\n    :param datefmt: handler date/time format specifier\n    :param stream: add a StreamHandler using ``stream``\n                    (None disables the stream, default=None)\n    :param level: logger level (default=INFO).\n    :param filename: add a FileHandler using ``filename`` (default=None)\n    :param filemode: open ``filename`` with specified filemode ('w' or 'a')\n    :param filelevel: logger level for file logger (default=``level``)\n    :param propagate: propagate message to parent (default=True)\n\n    :returns: logging.Logger object\n    \"\"\"\n\n    # Get a logger for the specified name\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    fmt = logging.Formatter(format, datefmt)\n    logger.propagate = propagate\n\n    # Remove existing handlers, otherwise multiple handlers can accrue\n    for hdlr in logger.handlers:\n        logger.removeHandler(hdlr)\n\n    # Add handlers. Add NullHandler if no file or stream output so that\n    # modules don't emit a warning about no handler.\n    if not (filename or stream):\n        logger.addHandler(logging.NullHandler())\n\n    if filename:\n        hdlr = logging.FileHandler(filename, filemode)\n        if filelevel is None:\n            filelevel = level\n        hdlr.setLevel(filelevel)\n        hdlr.setFormatter(fmt)\n        logger.addHandler(hdlr)\n\n    if stream:\n        hdlr = logging.StreamHandler(stream)\n        hdlr.setLevel(level)\n        hdlr.setFormatter(fmt)\n        logger.addHandler(hdlr)\n\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_stream(self, stream):\n\n        for handler in self.handlers[:]:\n            if isinstance(handler, logging.StreamHandler):\n                self.handlers.remove(handler)\n\n        if stream is not None:\n            stream_handler = logging.StreamHandler(stream)\n            stream_handler.addFilter(_StreamHandlerEchoFilter())\n            stream_handler.setFormatter(logging.Formatter('%(message)s'))\n            self.addHandler(stream_handler)\n\n        self.stream = stream", "response": "Sets the stream that this logger will be meant to replace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a message to the log system.", "response": "def write(self, message):\n        \"\"\"\n        Buffers each message until a newline is reached.  Each complete line is\n        then published to the logging system through ``self.log()``.\n        \"\"\"\n\n        self.__thread_local_ctx.write_count += 1\n\n        try:\n            if self.__thread_local_ctx.write_count > 1:\n                return\n\n            # For each line in the buffer ending with \\n, output that line to\n            # the logger\n            msgs = (self.buffer + message).split('\\n')\n            self.buffer = msgs.pop(-1)\n            for m in msgs:\n                self.log_orig(m, echo=True)\n        finally:\n            self.__thread_local_ctx.write_count -= 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the actual caller of the current object.", "response": "def find_actual_caller(self):\n        \"\"\"\n        Returns the full-qualified module name, full pathname, line number, and\n        function in which `StreamTeeLogger.write()` was called.  For example,\n        if this instance is used to replace `sys.stdout`, this will return the\n        location of any print statement.\n        \"\"\"\n\n        # Gleaned from code in the logging module itself...\n        try:\n            f = sys._getframe(1)\n            ##f = inspect.currentframe(1)\n        except Exception:\n            f = None\n        # On some versions of IronPython, currentframe() returns None if\n         # IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown module)\", \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            mod = inspect.getmodule(f)\n\n            if mod is None:\n                modname = '__main__'\n            else:\n                modname = mod.__name__\n\n            if modname == __name__:\n                # Crawl back until the first frame outside of this module\n                f = f.f_back\n                continue\n\n            rv = (modname, filename, f.f_lineno, co.co_name)\n            break\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(fp):\n    '''\n    Deserialize ``fp`` (a ``.read()``-supporting file-like object\n    containing a XPORT document) to a Python object.\n    '''\n    reader = reading.Reader(fp)\n    keys = reader.fields\n    columns = {k: [] for k in keys}\n    for row in reader:\n        for key, value in zip(keys, row):\n            columns[key].append(value)\n    return columns", "response": "Deserialize a file - like object containing a Python object containing a list of all the IA - O tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngoes to the login page.", "response": "def _get_login_page(self):\n        \"\"\"Go to the login page.\"\"\"\n        try:\n            raw_res = yield from self._session.get(HOME_URL,\n                                                   timeout=self._timeout)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not connect to login page\")\n        # Get login url\n        content = yield from raw_res.text()\n        soup = BeautifulSoup(content, 'html.parser')\n        form_node = soup.find('form', {'name': 'fm'})\n        if form_node is None:\n            raise PyHydroQuebecError(\"No login form find\")\n        login_url = form_node.attrs.get('action')\n        if login_url is None:\n            raise PyHydroQuebecError(\"Can not found login url\")\n        return login_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlogin to HydroQuebec website.", "response": "def _post_login_page(self, login_url):\n        \"\"\"Login to HydroQuebec website.\"\"\"\n        data = {\"login\": self.username,\n                \"_58_password\": self.password}\n\n        try:\n            raw_res = yield from self._session.post(login_url,\n                                                    data=data,\n                                                    timeout=self._timeout,\n                                                    allow_redirects=False)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not submit login form\")\n        if raw_res.status != 302:\n            raise PyHydroQuebecError(\"Login error: Bad HTTP status code. \"\n                                     \"Please check your username/password.\")\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_p_p_id_and_contract(self):\n        contracts = {}\n        try:\n            raw_res = yield from self._session.get(PROFILE_URL,\n                                                   timeout=self._timeout)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not get profile page\")\n        # Parse html\n        content = yield from raw_res.text()\n        soup = BeautifulSoup(content, 'html.parser')\n        # Search contracts\n        for node in soup.find_all('span', {\"class\": \"contrat\"}):\n            rematch = re.match(\"C[a-z]* ([0-9]{4} [0-9]{5})\", node.text)\n            if rematch is not None:\n                contracts[rematch.group(1).replace(\" \", \"\")] = None\n        # search for links\n        for node in soup.find_all('a', {\"class\": \"big iconLink\"}):\n            for contract in contracts:\n                if contract in node.attrs.get('href'):\n                    contracts[contract] = node.attrs.get('href')\n        # Looking for p_p_id\n        p_p_id = None\n        for node in soup.find_all('span'):\n            node_id = node.attrs.get('id', \"\")\n            if node_id.startswith(\"p_portraitConsommation_WAR\"):\n                p_p_id = node_id[2:]\n                break\n\n        if p_p_id is None:\n            raise PyHydroQuebecError(\"Could not get p_p_id\")\n\n        return p_p_id, contracts", "response": "Get id of consumption profile and contracts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_lonely_contract(self):\n        contracts = {}\n        try:\n            raw_res = yield from self._session.get(MAIN_URL,\n                                                   timeout=self._timeout)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not get main page\")\n        # Parse html\n        content = yield from raw_res.text()\n        soup = BeautifulSoup(content, 'html.parser')\n        info_node = soup.find(\"div\", {\"class\": \"span3 contrat\"})\n        if info_node is None:\n            raise PyHydroQuebecError(\"Can not found contract\")\n        research = re.search(\"Contrat ([0-9]{4} [0-9]{5})\", info_node.text)\n        if research is not None:\n            contracts[research.group(1).replace(\" \", \"\")] = None\n\n        if contracts == {}:\n            raise PyHydroQuebecError(\"Can not found contract\")\n\n        return contracts", "response": "Get contract number when we have only one contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all balances. .. todo:: IT SEEMS balances are shown (MAIN_URL) in the same order that contracts in profile page (PROFILE_URL). Maybe we should ensure that.", "response": "def _get_balances(self):\n        \"\"\"Get all balances.\n\n        .. todo::\n\n            IT SEEMS balances are shown (MAIN_URL) in the same order\n            that contracts in profile page (PROFILE_URL).\n            Maybe we should ensure that.\n        \"\"\"\n        balances = []\n        try:\n            raw_res = yield from self._session.get(MAIN_URL,\n                                                   timeout=self._timeout)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not get main page\")\n        # Parse html\n        content = yield from raw_res.text()\n        soup = BeautifulSoup(content, 'html.parser')\n        solde_nodes = soup.find_all(\"div\", {\"class\": \"solde-compte\"})\n        if solde_nodes == []:\n            raise PyHydroQuebecError(\"Can not found balance\")\n        for solde_node in solde_nodes:\n            try:\n                balance = solde_node.find(\"p\").text\n            except AttributeError:\n                raise PyHydroQuebecError(\"Can not found balance\")\n            balances.append(float(balance[:-2]\n                            .replace(\",\", \".\")\n                            .replace(\"\\xa0\", \"\")))\n\n        return balances"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_contract_page(self, contract_url):\n        try:\n            yield from self._session.get(contract_url,\n                                         timeout=self._timeout)\n        except OSError:\n            raise PyHydroQuebecError(\"Can not get profile page for a \"\n                                     \"specific contract\")", "response": "Load the profile page of a specific contract when we have multiple contracts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_data_detailled_energy_use(self, start_date=None, end_date=None):\n        if start_date is None:\n            start_date = datetime.datetime.now(HQ_TIMEZONE) - datetime.timedelta(days=1)\n        if end_date is None:\n            end_date = datetime.datetime.now(HQ_TIMEZONE)\n        # Get http session\n        yield from self._get_httpsession()\n        # Get login page\n        login_url = yield from self._get_login_page()\n        # Post login page\n        yield from self._post_login_page(login_url)\n        # Get p_p_id and contracts\n        p_p_id, contracts = yield from self._get_p_p_id_and_contract()\n        # If we don't have any contrats that means we have only\n        # onecontract. Let's get it\n        if contracts == {}:\n            contracts = yield from self._get_lonely_contract()\n        # For all contracts\n        for contract, contract_url in contracts.items():\n            if contract_url:\n                yield from self._load_contract_page(contract_url)\n\n            data = {}\n            dates = [(start_date + datetime.timedelta(n))\n                     for n in range(int((end_date - start_date).days))]\n\n            for date in dates:\n                # Get Hourly data\n                day_date = date.strftime(\"%Y-%m-%d\")\n                hourly_data = yield from self._get_hourly_data(day_date, p_p_id)\n                data[day_date] = hourly_data['raw_hourly_data']\n\n            # Add contract\n            self._data[contract] = data", "response": "Get detailled energy use from a specific contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the latest data from HydroQuebec.", "response": "def fetch_data(self):\n        \"\"\"Get the latest data from HydroQuebec.\"\"\"\n        # Get http session\n        yield from self._get_httpsession()\n        # Get login page\n        login_url = yield from self._get_login_page()\n        # Post login page\n        yield from self._post_login_page(login_url)\n        # Get p_p_id and contracts\n        p_p_id, contracts = yield from self._get_p_p_id_and_contract()\n        # If we don't have any contrats that means we have only\n        # onecontract. Let's get it\n        if contracts == {}:\n            contracts = yield from self._get_lonely_contract()\n\n        # Get balance\n        balances = yield from self._get_balances()\n        balances_len = len(balances)\n        balance_id = 0\n        # For all contracts\n        for contract, contract_url in contracts.items():\n            if contract_url:\n                yield from self._load_contract_page(contract_url)\n\n            # Get Hourly data\n            try:\n                yesterday = datetime.datetime.now(HQ_TIMEZONE) - datetime.timedelta(days=1)\n                day_date = yesterday.strftime(\"%Y-%m-%d\")\n                hourly_data = yield from self._get_hourly_data(day_date, p_p_id)\n                hourly_data = hourly_data['processed_hourly_data']\n            except Exception:  # pylint: disable=W0703\n                # We don't have hourly data for some reason\n                hourly_data = {}\n\n            # Get Annual data\n            try:\n                annual_data = yield from self._get_annual_data(p_p_id)\n            except PyHydroQuebecAnnualError:\n                # We don't have annual data, which is possible if your\n                # contract is younger than 1 year\n                annual_data = {}\n            # Get Monthly data\n            monthly_data = yield from self._get_monthly_data(p_p_id)\n            monthly_data = monthly_data[0]\n            # Get daily data\n            start_date = monthly_data.get('dateDebutPeriode')\n            end_date = monthly_data.get('dateFinPeriode')\n            try:\n                daily_data = yield from self._get_daily_data(p_p_id, start_date, end_date)\n            except Exception:  # pylint: disable=W0703\n                daily_data = []\n            # We have to test daily_data because it's empty\n            # At the end/starts of a period\n            if daily_data:\n                daily_data = daily_data[0]['courant']\n            # format data\n            contract_data = {\"balance\": balances[balance_id]}\n            for key1, key2 in MONTHLY_MAP:\n                contract_data[key1] = monthly_data[key2]\n            for key1, key2 in ANNUAL_MAP:\n                contract_data[key1] = annual_data.get(key2, \"\")\n            # We have to test daily_data because it's empty\n            # At the end/starts of a period\n            if daily_data:\n                for key1, key2 in DAILY_MAP:\n                    contract_data[key1] = daily_data[key2]\n            # Hourly\n            if hourly_data:\n                contract_data['yesterday_hourly_consumption'] = hourly_data\n            # Add contract\n            self._data[contract] = contract_data\n            balance_count = balance_id + 1\n            if balance_count < balances_len:\n                balance_id += 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npinging the server for the current state of each device.", "response": "def ping(self):\n        \"\"\"\n        Posts the current state of each device to the server and schedules the next call in n seconds.\n        :param serverURL:\n        :return:\n        \"\"\"\n        from datetime import datetime\n        nextRun = datetime.utcnow().timestamp() + self.cfg.getPingInterval()\n        self.sendHeartbeat()\n        self.scheduleNextHeartbeat(nextRun)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends heartbeat to the server.", "response": "def sendHeartbeat(self):\n        \"\"\"\n        Posts the current state to the server.\n        :param serverURL: the URL to ping.\n        :return:\n        \"\"\"\n        for name, md in self.cfg.recordingDevices.items():\n            try:\n                data = marshal(md, recordingDeviceFields)\n                data['serviceURL'] = self.cfg.getServiceURL() + API_PREFIX + '/devices/' + name\n                targetURL = self.serverURL + API_PREFIX + '/devices/' + name\n                logger.info(\"Pinging \" + targetURL)\n                resp = self.httpclient.put(targetURL, json=data)\n                if resp.status_code != 200:\n                    logger.warning(\"Unable to ping server at \" + targetURL + \" with \" + str(data.keys()) +\n                                   \", response is \" + str(resp.status_code))\n                else:\n                    logger.info(\"Pinged server at \" + targetURL + \" with \" + str(data.items()))\n            except:\n                logger.exception(\"Unable to ping server\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nschedules the next heartbeat.", "response": "def scheduleNextHeartbeat(self, nextRun):\n        \"\"\"\n        Schedules the next ping.\n        :param nextRun: when we should run next.\n        :param serverURL: the URL to ping.\n        :return:\n        \"\"\"\n        import threading\n        from datetime import datetime\n        tilNextTime = max(nextRun - datetime.utcnow().timestamp(), 0)\n        logging.getLogger('recorder').info(\"Scheduling next ping in \" + str(round(tilNextTime, 3)) + \" seconds\")\n        threading.Timer(tilNextTime, self.ping).start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates a type or matcher argument to the constructor.", "response": "def _validate_argument(self, arg):\n        \"\"\"Validate a type or matcher argument to the constructor.\"\"\"\n        if arg is None:\n            return arg\n\n        if isinstance(arg, type):\n            return InstanceOf(arg)\n        if not isinstance(arg, BaseMatcher):\n            raise TypeError(\n                \"argument of %s can be a type or a matcher (got %r)\" % (\n                    self.__class__.__name__, type(arg)))\n\n        return arg"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize(self, *args, **kwargs):\n        self.items = None\n        self.keys = None\n        self.values = None\n\n        if args:\n            if len(args) != 2:\n                raise TypeError(\"expected exactly two positional arguments, \"\n                                \"got %s\" % len(args))\n            if kwargs:\n                raise TypeError(\n                    \"expected positional or keyword arguments, not both\")\n\n            # got positional arguments only\n            self.keys, self.values = map(self._validate_argument, args)\n        elif kwargs:\n            has_kv = 'keys' in kwargs and 'values' in kwargs\n            has_of = 'of' in kwargs\n            if not (has_kv or has_of):\n                raise TypeError(\"expected keys/values or items matchers, \"\n                                \"but got: %s\" % list(kwargs.keys()))\n            if has_kv and has_of:\n                raise TypeError(\n                    \"expected keys & values, or items matchers, not both\")\n\n            if has_kv:\n                # got keys= and values= matchers\n                self.keys = self._validate_argument(kwargs['keys'])\n                self.values = self._validate_argument(kwargs['values'])\n            else:\n                # got of= matcher, which can be a tuple of matchers,\n                # or a single matcher for dictionary items\n                of = kwargs['of']\n                if isinstance(of, tuple):\n                    try:\n                        # got of= as tuple of matchers\n                        self.keys, self.values = \\\n                            map(self._validate_argument, of)\n                    except ValueError:\n                        raise TypeError(\n                            \"of= tuple has to be a pair of matchers/types\" % (\n                                self.__class__.__name__,))\n                else:\n                    # got of= as a single matcher\n                    self.items = self._validate_argument(of)", "response": "Initialize the mapping matcher with constructor arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef docs(ctx, output='html', rebuild=False, show=True, verbose=True):\n    sphinx_build = ctx.run(\n        'sphinx-build -b {output} {all} {verbose} docs docs/_build'.format(\n            output=output,\n            all='-a -E' if rebuild else '',\n            verbose='-v' if verbose else ''))\n    if not sphinx_build.ok:\n        fatal(\"Failed to build the docs\", cause=sphinx_build)\n\n    if show:\n        path = os.path.join(DOCS_OUTPUT_DIR, 'index.html')\n        if sys.platform == 'darwin':\n            path = 'file://%s' % os.path.abspath(path)\n        webbrowser.open_new_tab(path)", "response": "Build the docs and show them in default web browser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupload the package to PyPI.", "response": "def upload(ctx, yes=False):\n    \"\"\"Upload the package to PyPI.\"\"\"\n    import callee\n    version = callee.__version__\n\n    # check the packages version\n    # TODO: add a 'release' to automatically bless a version as release one\n    if version.endswith('-dev'):\n        fatal(\"Can't upload a development version (%s) to PyPI!\", version)\n\n    # run the upload if it has been confirmed by the user\n    if not yes:\n        answer = input(\"Do you really want to upload to PyPI [y/N]? \")\n        yes = answer.strip().lower() == 'y'\n    if not yes:\n        logging.warning(\"Aborted -- not uploading to PyPI.\")\n        return -2\n\n    logging.debug(\"Uploading version %s to PyPI...\", version)\n    setup_py_upload = ctx.run('python setup.py sdist upload')\n    if not setup_py_upload.ok:\n        fatal(\"Failed to upload version %s to PyPI!\", version,\n              cause=setup_py_upload)\n    logging.info(\"PyPI upload completed successfully.\")\n\n    # add a Git tag and push\n    git_tag = ctx.run('git tag %s' % version)\n    if not git_tag.ok:\n        fatal(\"Failed to add a Git tag for uploaded version %s\", version,\n              cause=git_tag)\n    git_push = ctx.run('git push && git push --tags')\n    if not git_push.ok:\n        fatal(\"Failed to push the release upstream.\", cause=git_push)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog an error message and exit.", "response": "def fatal(*args, **kwargs):\n    \"\"\"Log an error message and exit.\n\n    Following arguments are keyword-only.\n\n    :param exitcode: Optional exit code to use\n    :param cause: Optional Invoke's Result object, i.e.\n                  result of a subprocess invocation\n    \"\"\"\n    # determine the exitcode to return to the operating system\n    exitcode = None\n    if 'exitcode' in kwargs:\n        exitcode = kwargs.pop('exitcode')\n    if 'cause' in kwargs:\n        cause = kwargs.pop('cause')\n        if not isinstance(cause, Result):\n            raise TypeError(\n                \"invalid cause of fatal error: expected %r, got %r\" % (\n                    Result, type(cause)))\n        exitcode = exitcode or cause.return_code\n\n    logging.error(*args, **kwargs)\n    raise Exit(exitcode or -1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rungtd1d(time: Union[datetime, str, np.ndarray],\n             altkm: np.ndarray,\n             glat: float, glon: float) -> xarray.Dataset:\n    \"\"\"\n    This is the \"atomic\" function looped by other functions\n    \"\"\"\n    time = todt64(time)\n    # %% get solar parameters for date\n    f107Ap = gi.getApF107(time, smoothdays=81)\n    f107a = f107Ap['f107s'].item()\n    f107 = f107Ap['f107'].item()\n    Ap = f107Ap['Ap'].item()\n# %% dimensions\n    altkm = np.atleast_1d(altkm)\n    assert altkm.ndim == 1\n    assert isinstance(glon, (int, float))\n    assert isinstance(glat, (int, float))\n    assert isinstance(time, np.datetime64) or (time.size == 1 and isinstance(\n        time[0], np.datetime64)), 'if you have multiple times, for loop over them'\n\n# don't check ap, too complicated\n    assert isinstance(MASS, (float, int))\n    assert len(TSELECOPS) == 25\n# %%\n    gtd7.tselec(TSELECOPS)  # like the msis_driver example\n\n    iyd, utsec, stl = datetime2gtd(time, glon)\n    altkm = np.atleast_1d(altkm)\n\n    gtd7.meters(1)  # makes output in m^-3 and kg/m^-3\n# %%\n    if isinstance(Ap, (float, int)):\n        Ap = [Ap]*7  # even if SW(9) == 1 due to f2py needs for array\n\n    dens = np.empty((altkm.size, len(species)))\n    temp = np.empty((altkm.size, len(ttypes)))\n    for i, a in enumerate(altkm):\n        dens[i, :], temp[i, :] = gtd7.gtd7(iyd, utsec, a, glat, glon, stl, f107a, f107, Ap, MASS)\n\n    dsf = {k: (('time', 'alt_km', 'lat', 'lon'), v[None, :, None, None]) for (k, v) in zip(species, dens.T)}\n    dsf.update({'Tn':  (('time', 'alt_km', 'lat', 'lon'), temp[:, 1][None, :, None, None]),\n                'Texo': (('time', 'alt_km', 'lat', 'lon'), temp[:, 0][None, :, None, None])})\n\n    atmos = xarray.Dataset(dsf,\n                           coords={'time': time.astype(datetime), 'alt_km': altkm, 'lat': [glat], 'lon': [glon], },\n                           attrs={'Ap': Ap, 'f107': f107, 'f107a': f107a,\n                                  'species': species})\n\n    return atmos", "response": "This is the atomic function looped by other functions that have a single time. It is the atomic function looped by other functions that have a single time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef form_valid(self, form):\n        form.save()\n        return super(SubscriptionView, self).form_valid(form)", "response": "Call form. save and super itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_request_parameters(func):\n\n        # The function the decorator returns\n        async def decorated_func(*args, handle_ratelimit=None, max_tries=None, request_timeout=None, **kwargs):\n            return await func(*args, handle_ratelimit=handle_ratelimit, max_tries=max_tries,\n                              request_timeout=request_timeout, **kwargs)\n\n        # We return the decorated func\n        return decorated_func", "response": "Adds the ratelimit and request timeout parameters to a function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_stats(self, battletag: str, regions=(EUROPE, KOREA, AMERICAS, CHINA, JAPAN, ANY),\n                        platform=None, _session=None, handle_ratelimit=None, max_tries=None, request_timeout=None):\n        \"\"\"Returns the stats for the profiles on the specified regions and platform. The format for regions without a matching user, the format is the same as get_profile.\n        The stats are returned in a dictionary with a similar format to what https://github.com/SunDwarf/OWAPI/blob/master/api.md#get-apiv3ubattletagstats specifies.\"\"\"\n\n        if platform is None:\n            platform = self.default_platform\n        try:\n            blob_dict = await self._base_request(battletag, \"stats\", _session, platform=platform,\n                                                 handle_ratelimit=handle_ratelimit, max_tries=max_tries,\n                                                 request_timeout=request_timeout)\n        except ProfileNotFoundError as e:\n            # The battletag doesn't exist\n            blob_dict = {}\n        existing_regions = {key: val for key, val in blob_dict.items() if ((val is not None) and (key != \"_request\"))}\n        return {key: [inner_val for inner_key, inner_val in val.items() if inner_key == \"stats\"][0] for key, val in\n                existing_regions.items() if key in regions}", "response": "Returns the stats for the profiles on the specified regions and platform."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing a request to some endpoint. This is also where ratelimit logic is handled.", "response": "async def _base_request(self, battle_tag: str, endpoint_name: str, session: aiohttp.ClientSession, *, platform=None,\n                            handle_ratelimit=None, max_tries=None, request_timeout=None):\n        \"\"\"Does a request to some endpoint. This is also where ratelimit logic is handled.\"\"\"\n        # We check the different optional arguments, and if they're not passed (are none) we set them to the default for the client object\n        if platform is None:\n            platform = self.default_platform\n        if handle_ratelimit is None:\n            handle_ratelimit = self.default_handle_ratelimit\n        if max_tries is None:\n            max_tries = self.default_max_tries\n        if request_timeout is None:\n            request_timeout = self.default_request_timeout\n\n        # The battletag with #s removed\n        san_battle_tag = self.sanitize_battletag(battle_tag)\n\n        # The ratelimit logic\n        for _ in range(max_tries):\n            # We execute a request\n            try:\n                resp_json, status = await self._async_get(\n                    session,\n                    self.server_url + self._api_urlpath + \"{battle_tag}/{endpoint}\".format(\n                        battle_tag=san_battle_tag,\n                        endpoint=endpoint_name\n                    ),\n                    params={\"platform\": platform},\n                    # Passed to _async_get and indicates what platform we're searching on\n                    headers={\"User-Agent\": \"overwatch_python_api\"},\n                    # According to https://github.com/SunDwarf/OWAPI/blob/master/owapi/v3/v3_util.py#L18 we have to customise our User-Agent, so we do\n                    _async_timeout_seconds=request_timeout\n                )\n                if status == 429 and resp_json[\"msg\"] == \"you are being ratelimited\":\n                    raise RatelimitError\n            except RatelimitError as e:\n                # This excepts both RatelimitErrors and TimeoutErrors, ratelimiterrors for server returning a ratelimit, timeouterrors for the connection not being done in with in the timeout\n                # We are ratelimited, so we check if we handle ratelimiting logic\n                # If so, we wait and then execute the next iteration of the loop\n                if handle_ratelimit:\n                    # We wait to remedy ratelimiting, and we wait a bit more than the response says we should\n                    await asyncio.sleep(resp_json[\"retry\"] + 1)\n                    continue\n                else:\n                    raise\n            else:\n                # We didn't get an error, so we exit the loop because it was a successful request\n                break\n        else:\n            # The loop didn't stop because it got breaked, which means that we got ratelimited until the maximum number of tries were finished\n            raise RatelimitError(\"Got ratelimited for each requests until the maximum number of retries were reached.\")\n\n        # Validate the response\n        if status != 200:\n            if status == 404 and resp_json[\"msg\"] == \"profile not found\":\n                raise ProfileNotFoundError(\n                    \"Got HTTP 404, profile not found. This is caused by the given battletag not existing on the specified platform.\")\n            if status == 429 and resp_json[\"msg\"] == \"you are being ratelimited\":\n                raise RatelimitError(\n                    \"Got HTTP 429, you are being ratelimited. This is caused by calls to the api too frequently.\")\n            raise ConnectionError(\"Did not get HTTP status 200, got: {0}\".format(status))\n        return resp_json"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _async_get(self, session: aiohttp.ClientSession, *args, _async_timeout_seconds: int = 5,\n                         **kwargs):\n        \"\"\"Uses aiohttp to make a get request asynchronously. \n        Will raise asyncio.TimeoutError if the request could not be completed \n        within _async_timeout_seconds (default 5) seconds.\"\"\"\n\n        # Taken almost directly from the aiohttp tutorial\n        with async_timeout.timeout(_async_timeout_seconds):\n            async with session.get(*args, **kwargs) as response:\n                return await response.json(), response.status", "response": "Uses aiohttp to make a get request asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if argument is a method.", "response": "def is_method(arg, min_arity=None, max_arity=None):\n    \"\"\"Check if argument is a method.\n\n    Optionally, we can also check if minimum or maximum arities\n    (number of accepted arguments) match given minimum and/or maximum.\n    \"\"\"\n    if not callable(arg):\n        return False\n\n    if not any(is_(arg) for is_ in (inspect.ismethod,\n                                    inspect.ismethoddescriptor,\n                                    inspect.isbuiltin)):\n        return False\n\n    try:\n        argnames, varargs, kwargs, defaults = getargspec(arg)\n    except TypeError:\n        # On CPython 2.x, built-in methods of file aren't inspectable,\n        # so if it's file.read() or file.write(), we can't tell it for sure.\n        # Given how this check is being used, assuming the best is probably\n        # all we can do here.\n        return True\n    else:\n        if argnames and argnames[0] == 'self':\n            argnames = argnames[1:]\n\n    if min_arity is not None:\n        actual_min_arity = len(argnames) - len(defaults or ())\n        assert actual_min_arity >= 0, (\n            \"Minimum arity of %r found to be negative (got %s)!\" % (\n                arg, actual_min_arity))\n        if int(min_arity) != actual_min_arity:\n            return False\n\n    if max_arity is not None:\n        actual_max_arity = sys.maxsize if varargs or kwargs else len(argnames)\n        if int(max_arity) != actual_max_arity:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_readable(self, obj):\n        try:\n            read = getattr(obj, 'read')\n        except AttributeError:\n            return False\n        else:\n            return is_method(read, max_arity=1)", "response": "Check if the argument is a readable file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the argument is a writable file - like object.", "response": "def _is_writable(self, obj):\n        \"\"\"Check if the argument is a writable file-like object.\"\"\"\n        try:\n            write = getattr(obj, 'write')\n        except AttributeError:\n            return False\n        else:\n            return is_method(write, min_arity=1, max_arity=1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(time: datetime, altkm: float,\n        glat: Union[float, np.ndarray], glon: Union[float, np.ndarray], *,\n        f107a: float = None, f107: float = None, Ap: int = None) -> xarray.Dataset:\n    \"\"\"\n    loops the rungtd1d function below. Figure it's easier to troubleshoot in Python than Fortran.\n    \"\"\"\n    glat = np.atleast_2d(glat)\n    glon = np.atleast_2d(glon)  # has to be here\n# %% altitude 1-D\n    if glat.size == 1 and glon.size == 1 and isinstance(time, (str, date, datetime, np.datetime64)):\n        atmos = rungtd1d(time, altkm, glat.squeeze()[()], glon.squeeze()[()],\n                         f107a=f107a, f107=f107, Ap=Ap)\n# %% lat/lon grid at 1 altitude\n    else:\n        atmos = loopalt_gtd(time, glat, glon, altkm,\n                            f107a=f107a, f107=f107, Ap=Ap)\n\n    return atmos", "response": "rungtd1d function below. Figure it s easier to troubleshoot in Python than Fortran."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loopalt_gtd(time: datetime,\n                glat: Union[float, np.ndarray], glon: Union[float, np.ndarray],\n                altkm: Union[float, List[float], np.ndarray], *,\n                f107a: float = None, f107: float = None, Ap: int = None) -> xarray.Dataset:\n    \"\"\"\n    loop over location and time\n\n    time: datetime or numpy.datetime64 or list of datetime or np.ndarray of datetime\n    glat: float or 2-D np.ndarray\n    glon: float or 2-D np.ndarray\n    altkm: float or list or 1-D np.ndarray\n    \"\"\"\n    glat = np.atleast_2d(glat)\n    glon = np.atleast_2d(glon)\n    assert glat.ndim == glon.ndim == 2\n\n    times = np.atleast_1d(time)\n    assert times.ndim == 1\n\n    atmos = xarray.Dataset()\n\n    for k, t in enumerate(times):\n        print('computing', t)\n        for i in range(glat.shape[0]):\n            for j in range(glat.shape[1]):\n                # atmos = xarray.concat((atmos, rungtd1d(t, altkm, glat[i,j], glon[i,j])),\n                #                      data_vars='minimal',coords='minimal',dim='lon')\n                atm = rungtd1d(t, altkm, glat[i, j], glon[i, j],\n                               f107a=f107a, f107=f107, Ap=Ap)\n                atmos = xarray.merge((atmos, atm))\n\n    atmos.attrs = atm.attrs\n\n    return atmos", "response": "loop over location and time and return a new dataset with the altkm"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_desc(self, desc):\n        if desc is None:\n            return desc\n\n        if not isinstance(desc, STRING_TYPES):\n            raise TypeError(\n                \"predicate description for Matching must be a string, \"\n                \"got %r\" % (type(desc),))\n\n        # Python 2 mandates __repr__ to be an ASCII string,\n        # so if Unicode is passed (usually due to unicode_literals),\n        # it should be ASCII-encodable.\n        if not IS_PY3 and isinstance(desc, unicode):\n            try:\n                desc = desc.encode('ascii', errors='strict')\n            except UnicodeEncodeError:\n                raise TypeError(\"predicate description must be \"\n                                \"an ASCII string in Python 2\")\n\n        return desc", "response": "Validate the predicate description for Matching."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_placeholder_repr(self):\n        placeholder = '...'\n        if self.TRANSFORM is not None:\n            placeholder = '%s(%s)' % (self.TRANSFORM.__name__, placeholder)\n        return placeholder", "response": "Return the placeholder part of matcher s __repr__."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_class_definition(meta, classname, bases, dict_):\n        # let the BaseMatcher class be created without hassle\n        if meta._is_base_matcher_class_definition(classname, dict_):\n            return\n\n        # ensure that no important magic methods are being overridden\n        for name, member in dict_.items():\n            if not (name.startswith('__') and name.endswith('__')):\n                continue\n\n            # check if it's not a whitelisted magic method name\n            name = name[2:-2]\n            if not name:\n                continue  # unlikely case of a ``____`` function\n            if name not in meta._list_magic_methods(BaseMatcher):\n                continue\n            if name in meta.USER_OVERRIDABLE_MAGIC_METHODS:\n                continue\n\n            # non-function attributes, like __slots__, are harmless\n            if not inspect.isfunction(member):\n                continue\n\n            # classes in this very module are exempt, since they define\n            # the very behavior of matchers we want to protect\n            if member.__module__ == __name__:\n                continue\n\n            raise RuntimeError(\n                \"matcher class %s cannot override the __%s__ method\" % (\n                    classname, name))", "response": "Ensure the matcher class definition is acceptable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_base_matcher_class_definition(meta, classname, dict_):\n        if classname != 'BaseMatcher':\n            return False\n        methods = list(filter(inspect.isfunction, dict_.values()))\n        return methods and all(m.__module__ == __name__ for m in methods)", "response": "Checks whether given class name and dictionary contains the BaseMatcher."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _list_magic_methods(meta, class_):\n        return [\n            name[2:-2] for name, member in class_.__dict__.items()\n            if len(name) > 4 and name.startswith('__') and name.endswith('__')\n            and inspect.isfunction(member)\n        ]", "response": "Return names of magic methods defined by a class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new version object for the given version.", "response": "def semver(version, loose):\n    if isinstance(version, SemVer):\n        if version.loose == loose:\n            return version\n        else:\n            version = version.version\n    elif not isinstance(version, str):  # xxx:\n        raise InvalidTypeIncluded(\"must be str, but {!r}\".format(version))\n\n    \"\"\"\n    if (!(this instanceof SemVer))\n       return new SemVer(version, loose);\n    \"\"\"\n    return SemVer(version, loose)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef autodoc_process_docstring(app, what, name, obj, options, lines):\n    # check that only symbols that can be directly imported from ``callee``\n    # package are being documented\n    _, symbol = name.rsplit('.', 1)\n    if symbol not in callee.__all__:\n        raise SphinxError(\n            \"autodoc'd '%s' is not a part of the public API!\" % name)\n\n    # for classes exempt from automatic merging of class & __init__ docs,\n    # pretend their __init__ methods have no docstring at all,\n    # so that nothing will be appended to the class's docstring\n    if what == 'class' and name in autoclass_content_exceptions:\n        # amusingly, when autodoc reads the constructor's docstring\n        # for appending it to class docstring, it will report ``what``\n        # as 'class' (again!); hence we must check what it actually read\n        ctor_docstring_lines = prepare_docstring(obj.__init__.__doc__)\n        if lines == ctor_docstring_lines:\n            lines[:] = []", "response": "Process the docstring of a class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nraising ValidationError if the email is already subscribed.", "response": "def clean_email(self):\n        \"\"\" Raise ValidationError if the contact exists. \"\"\"\n        contacts = self.api.lists.contacts(id=self.list_id)['result']\n\n        for contact in contacts:\n            if contact['email'] == self.cleaned_data['email']:\n                raise forms.ValidationError(\n                    _(u'This email is already subscribed'))\n\n        return self.cleaned_data['email']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_contact(self):\n        self.api.lists.addcontact(\n            contact=self.cleaned_data['email'], id=self.list_id, method='POST')", "response": "Create a contact with using the email on the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets or create an Api instance.", "response": "def api(self):\n        \"\"\" Get or create an Api() instance using django settings. \"\"\"\n        api = getattr(self, '_api', None)\n\n        if api is None:\n            self._api = mailjet.Api()\n\n        return self._api"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_id(self):\n        list_id = getattr(self, '_list_id', None)\n\n        if list_id is None:\n            for l in self.api.lists.all()['lists']:\n                if l['name'] == self.list_name:\n                    self._list_id = l['id']\n\n            if not getattr(self, '_list_id', None):\n                self._list_id = self.api.lists.create(\n                    label=self.list_label, name=self.list_name,\n                    method='POST')['list_id']\n\n        return self._list_id", "response": "Get or create the list id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the values of magic tags defined in the given Python file.", "response": "def read_tags(filename):\n    \"\"\"Reads values of \"magic tags\" defined in the given Python file.\n\n    :param filename: Python filename to read the tags from\n    :return: Dictionary of tags\n    \"\"\"\n    with open(filename) as f:\n        ast_tree = ast.parse(f.read(), filename)\n\n    res = {}\n    for node in ast.walk(ast_tree):\n        if type(node) is not ast.Assign:\n            continue\n\n        target = node.targets[0]\n        if type(target) is not ast.Name:\n            continue\n        if not (target.id.startswith('__') and target.id.endswith('__')):\n            continue\n\n        name = target.id[2:-2]\n        res[name] = ast.literal_eval(node.value)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing any unicode characters to ascii equivalent", "response": "def normalize_unicode(text):\n    \"\"\"\n    Normalize any unicode characters to ascii equivalent\n    https://docs.python.org/2/library/unicodedata.html#unicodedata.normalize\n    \"\"\"\n    if isinstance(text, six.text_type):\n        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n    else:\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef word_tokenize(text, stopwords=_stopwords, ngrams=None, min_length=0, ignore_numeric=True):\n    if ngrams is None:\n        ngrams = 1\n\n    text = re.sub(re.compile('\\'s'), '', text)  # Simple heuristic\n    text = re.sub(_re_punctuation, '', text)\n\n    matched_tokens = re.findall(_re_token, text.lower())\n    for tokens in get_ngrams(matched_tokens, ngrams):\n        for i in range(len(tokens)):\n            tokens[i] = tokens[i].strip(punctuation)\n\n            if len(tokens[i]) < min_length or tokens[i] in stopwords:\n                break\n            if ignore_numeric and isnumeric(tokens[i]):\n                break\n        else:\n            yield tuple(tokens)", "response": "Parses the given text and yields tokens which represent words within the given text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to build using CMake > = 3", "response": "def cmake_setup():\n    \"\"\"\n    attempt to build using CMake >= 3\n    \"\"\"\n    cmake_exe = shutil.which('cmake')\n    if not cmake_exe:\n        raise FileNotFoundError('CMake not available')\n\n    wopts = ['-G', 'MinGW Makefiles', '-DCMAKE_SH=\"CMAKE_SH-NOTFOUND'] if os.name == 'nt' else []\n\n    subprocess.check_call([cmake_exe] + wopts + [str(SRCDIR)],\n                          cwd=BINDIR)\n\n    ret = subprocess.run([cmake_exe, '--build', str(BINDIR)],\n                         stderr=subprocess.PIPE,\n                         universal_newlines=True)\n\n    result(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef meson_setup():\n    meson_exe = shutil.which('meson')\n    ninja_exe = shutil.which('ninja')\n\n    if not meson_exe or not ninja_exe:\n        raise FileNotFoundError('Meson or Ninja not available')\n\n    if not (BINDIR / 'build.ninja').is_file():\n        subprocess.check_call([meson_exe, str(SRCDIR)], cwd=BINDIR)\n\n    ret = subprocess.run(ninja_exe, cwd=BINDIR, stderr=subprocess.PIPE,\n                         universal_newlines=True)\n\n    result(ret)", "response": "attempt to build with Meson + Ninja\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_term_occurrence(self, term, document):\n        if document not in self._documents:\n            self._documents[document] = 0\n\n        if term not in self._terms:\n            if self._freeze:\n                return\n            else:\n                self._terms[term] = collections.Counter()\n\n        if document not in self._terms[term]:\n            self._terms[term][document] = 0\n\n        self._documents[document] += 1\n        self._terms[term][document] += 1", "response": "Adds an occurrence of the specified term in the specified document."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_total_term_frequency(self, term):\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n\n        return sum(self._terms[term].values())", "response": "Gets the total frequency of the specified term in the entire corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the frequency of the term specified in the document.", "response": "def get_term_frequency(self, term, document, normalized=False):\n        \"\"\"\n        Returns the frequency of the term specified in the document.\n        \"\"\"\n        if document not in self._documents:\n            raise IndexError(DOCUMENT_DOES_NOT_EXIST)\n\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n\n        result = self._terms[term].get(document, 0)\n        if normalized:\n            result /= self.get_document_length(document)\n\n        return float(result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_document_frequency(self, term):\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n        else:\n            return len(self._terms[term])", "response": "Returns the number of documents the specified term appears in."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_document_length(self, document):\n        if document in self._documents:\n            return self._documents[document]\n        else:\n            raise IndexError(DOCUMENT_DOES_NOT_EXIST)", "response": "Returns the number of terms found within the specified document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_documents(self, term):\n        if term not in self._terms:\n            raise IndexError(TERM_DOES_NOT_EXIST)\n        else:\n            return self._terms[term]", "response": "Returns all documents related to the specified term in the Counter object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the Term - Frequency Inverse - Document - Frequency value for the given term in the specified document.", "response": "def get_tfidf(self, term, document, normalized=False):\n        \"\"\"\n        Returns the Term-Frequency Inverse-Document-Frequency value for the given\n        term in the specified document. If normalized is True, term frequency will\n        be divided by the document length.\n        \"\"\"\n        tf = self.get_term_frequency(term, document)\n\n        # Speeds up performance by avoiding extra calculations\n        if tf != 0.0:\n            # Add 1 to document frequency to prevent divide by 0\n            # (Laplacian Correction)\n            df = 1 + self.get_document_frequency(term)\n            n = 2 + len(self._documents)\n\n            if normalized:\n                tf /= self.get_document_length(document)\n\n            return tf * math.log10(n / df)\n        else:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a document vector for the specified document.", "response": "def generate_document_vector(self, doc, mode='tfidf'):\n        \"\"\"\n        Returns a representation of the specified document as a feature vector\n        weighted according the mode specified (by default tf-dif).\n\n        A custom weighting function can also be passed which receives the hashedindex\n        instance, the selected term and document as parameters.\n\n        The result will be returned in the form of a list. This can be converted\n        into a numpy array if required using the `np.asarray` method\n        Available built-in modes:\n          * tfidf: Term Frequency Inverse Document Frequency\n          * ntfidf: Normalized Term Frequency Inverse Document Frequency\n          * tf: Term Frequency\n          * ntf: Normalized Term Frequency\n        \"\"\"\n        if mode == 'tfidf':\n            selected_function = HashedIndex.get_tfidf\n        elif mode == 'ntfidf':\n            selected_function = functools.partial(HashedIndex.get_tfidf, normalized=True)\n        elif mode == 'tf':\n            selected_function = HashedIndex.get_term_frequency\n        elif mode == 'ntf':\n            selected_function = functools.partial(HashedIndex.get_term_frequency, normalized=True)\n        elif hasattr(mode, '__call__'):\n            selected_function = mode\n        else:\n            raise ValueError('Unexpected mode: %s', mode)\n\n        result = []\n        for term in self._terms:\n            result.append(selected_function(self, term, doc))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_feature_matrix(self, mode='tfidf'):\n        result = []\n\n        for doc in self._documents:\n            result.append(self.generate_document_vector(doc, mode))\n\n        return result", "response": "Generates a feature matrix for the given term - count and document - count."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_class_in_list(klass, lst):\n    filtered = list(filter(lambda x: x.__class__ == klass, lst))\n    if filtered:\n        return filtered[0]\n    return None", "response": "Returns the first occurrence of an instance of type klass in \n    the given list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_classes_in_list(klasses, lst):\n    if not isinstance(klasses, list):\n        klasses = [klasses]\n    return tuple(map(lambda klass: find_class_in_list(klass, lst), klasses))", "response": "Returns a tuple containing an entry corresponding to each of the requested class types in the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a PARMLIST string value acceptable to the Payflow Pro API.", "response": "def _build_parmlist(self, parameters):\n        \"\"\"\n        Converts a dictionary of name and value pairs into a \n        PARMLIST string value acceptable to the Payflow Pro API.\n\n        \"\"\"\n\n        args = []\n        for key, value in parameters.items():\n            if not value is None:\n                # We always use the explicit-length keyname format, to reduce the chance\n                # of requests failing due to unusual characters in parameter values.\n\n                try:\n                    classinfo = unicode\n                except NameError:\n                    classinfo = str\n\n                if isinstance(value, classinfo):\n                    key = '%s[%d]' % (key.upper(), len(value.encode('utf-8')))\n                else:\n                    key = '%s[%d]' % (key.upper(), len(str(value)))\n                args.append('%s=%s' % (key, value))\n        args.sort()\n        parmlist = '&'.join(args)        \n        return parmlist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a PARMLIST string into a dictionary of name and value pairs.", "response": "def _parse_parmlist(self, parmlist):\n        \"\"\"\n        Parses a PARMLIST string into a dictionary of name and value \n        pairs. The parsing is complicated by the following:\n        \n         - parameter keynames may or may not include a length \n           specification\n         - delimiter characters (=, &) may appear inside parameter\n           values, provided the parameter has an explicit length.\n        \n        For example, the following parmlist values are possible:\n        \n          A=B&C=D\n          A[1]=B&C[1]=D\n          A=B&C[1]=D\n          A[3]=B&B&C[1]=D  (Here, the value of A is \"B&B\")\n          A[1]=B&C[3]=D=7  (Here, the value of C is \"D=7\")\n          \n        \"\"\"\n        parmlist = \"&\" + parmlist\n        name_re = re.compile(r'\\&([A-Z0-9_]+)(\\[\\d+\\])?=')\n        \n        results = {}\n        offset = 0\n        match = name_re.search(parmlist, offset)\n        while match:\n            name, len_suffix = match.groups()\n            offset = match.end()\n            if len_suffix:\n                val_len = int(len_suffix[1:-1])\n            else:\n                next_match = name_re.search(parmlist, offset)\n                if next_match:\n                    val_len = next_match.start() - match.end()\n                else:\n                    # At end of parmlist\n                    val_len = len(parmlist) - match.end()\n            value = parmlist[match.end() : match.end() + val_len]\n            results[name.lower()] = value\n                                    \n            match = name_re.search(parmlist, offset)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, url):\n        self.logger.debug('url:\\n' + url)\n        try:\n            response = urlopen(url)\n            return json.loads(response.read().decode('utf-8'))\n        except URLError:\n            self.logger.info('Server connection problem')\n        except Exception:\n            self.logger.info('Server format problem')", "response": "Send a http request to the given url and return the decoded result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(self, address, acceptlanguage=None, limit=20,\n              countrycodes=None):\n        \"\"\"\n        Issue a geocoding query for *address* to the\n        Nominatim instance and return the decoded results\n\n        :param address: a query string with an address\n                        or presumed parts of an address\n        :type address: str or (if python2) unicode\n        :param acceptlanguage: rfc2616 language code\n        :type acceptlanguage: str or None\n        :param limit: limit the number of results\n        :type limit: int or None\n        :param countrycodes: restrict the search to countries\n             given by their ISO 3166-1alpha2 codes (cf.\n             https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 )\n        :type countrycodes: str iterable\n        :returns: a list of search results (each a dict)\n        :rtype: list or None\n        \"\"\"\n        url = self.url + '&q=' + quote_plus(address)\n        if acceptlanguage:\n            url += '&accept-language=' + acceptlanguage\n        if limit:\n            url += '&limit=' + str(limit)\n        if countrycodes:\n            url += '&countrycodes=' + ','.join(countrycodes)\n        return self.request(url)", "response": "Issue a geocoding query to the Nominatim instance and return the decoded results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nissue a reverse geocoding query for a place given by latitude and longitude.", "response": "def query(self, lat=None, lon=None, osm_id=None, osm_type=None,\n              acceptlanguage='', zoom=18):\n        \"\"\"\n        Issue a reverse geocoding query for a place given\n        by *lat* and *lon*, or by *osm_id* and *osm_type*\n        to the Nominatim instance and return the decoded results\n\n        :param lat: the geograpical latitude of the place\n        :param lon: the geograpical longitude of the place\n        :param osm_id: openstreetmap identifier osm_id\n        :type osm_id: str\n        :param osm_type: openstreetmap type osm_type\n        :type osm_type: str\n        :param acceptlanguage: rfc2616 language code\n        :type acceptlanguage: str or None\n        :param zoom: zoom factor between from 0 to 18\n        :type zoom: int or None or a key in :data:`zoom_aliases`\n        :param countrycodes: restrict the search to countries\n             given by their ISO 3166-1alpha2 codes (cf.\n             https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 )\n        :type countrycodes: str iterable\n        :returns: a list of search results (each a dict)\n        :rtype: list or None\n        :raise: NominatimException if invalid zoom value\n        \"\"\"\n        url = self.url\n        if osm_id is not None and osm_type not in ('N', 'W', 'R'):\n            raise NominatimException('invalid osm_type')\n        if osm_id is not None and osm_type is not None:\n            url += '&osm_id=' + osm_id + '&osm_type=' + osm_type\n        elif lat is not None and lon is not None:\n            url += '&lat=' + str(lat) + '&lon=' + str(lon)\n        else:\n            return None\n        if acceptlanguage:\n            url += '&accept-language=' + acceptlanguage\n        if zoom in zoom_aliases:\n            zoom = zoom_aliases[zoom]\n        if not isinstance(zoom, int) or zoom < 0 or zoom > 18:\n            raise NominatimException('zoom must effectively be betwen 0 and 18')\n        url +='&zoom=' + str(zoom)\n        return self.request(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new CTMGrid object from a given model name and keyword arguments.", "response": "def from_model(cls, model_name, **kwargs):\n        \"\"\"\n        Define a grid using the specifications of a given model.\n\n        Parameters\n        ----------\n        model_name : string\n            Name the model (see :func:`get_supported_models` for available\n            model names).\n            Supports multiple formats (e.g., 'GEOS5', 'GEOS-5' or 'GEOS_5').\n        **kwargs : string\n            Parameters that override the model  or default grid\n          settings (See Other Parameters below).\n\n        Returns\n        -------\n        A :class:`CTMGrid` object.\n\n        Other Parameters\n        ----------------\n        resolution : (float, float)\n            Horizontal grid resolution (lon, lat) or (DI, DJ) [degrees]\n        Psurf : float\n            Average surface pressure [hPa] (default: 1013.15)\n\n        Notes\n        -----\n        Regridded vertical models may have several valid names (e.g.,\n        'GEOS5_47L' and 'GEOS5_REDUCED' refer to the same model).\n\n        \"\"\"\n        settings = _get_model_info(model_name)\n        model = settings.pop('model_name')\n        for k, v in list(kwargs.items()):\n            if k in ('resolution', 'Psurf'):\n                settings[k] = v\n\n        return cls(model, **settings)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new instance of the class cls with the specified model and parameters.", "response": "def copy_from_model(cls, model_name, reference, **kwargs):\n        \"\"\"\n        Set-up a user-defined grid using specifications of a reference\n        grid model.\n\n        Parameters\n        ----------\n        model_name : string\n            name of the user-defined grid model.\n        reference : string or :class:`CTMGrid` instance\n            Name of the reference model (see :func:`get_supported_models`),\n            or a :class:`CTMGrid` object from which grid set-up is copied.\n        **kwargs\n            Any set-up parameter which will override the settings of the\n            reference model (see :class:`CTMGrid` parameters).\n\n        Returns\n        -------\n        A :class:`CTMGrid` object.\n\n        \"\"\"\n        if isinstance(reference, cls):\n            settings = reference.__dict__.copy()\n            settings.pop('model')\n        else:\n            settings = _get_model_info(reference)\n            settings.pop('model_name')\n\n        settings.update(kwargs)\n        settings['reference'] = reference\n\n        return cls(model_name, **settings)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_layers(self, Psurf=1013.25, Ptop=0.01, **kwargs):\n\n        Psurf = np.asarray(Psurf)\n        output_ndims = Psurf.ndim + 1\n        if output_ndims > 3:\n            raise ValueError(\"`Psurf` argument must be a float or an array\"\n                             \" with <= 2 dimensions (or None)\")\n\n        # Compute all variables: takes not much memory, fast\n        # and better for code reading\n        SIGe = None\n        SIGc = None\n        ETAe = None\n        ETAc = None\n\n        if self.hybrid:\n            try:\n                Ap = broadcast_1d_array(self.Ap, output_ndims)\n                Bp = broadcast_1d_array(self.Bp, output_ndims)\n            except KeyError:\n                raise ValueError(\"Impossible to compute vertical levels,\"\n                                 \" data is missing (Ap, Bp)\")\n            Cp = 0.\n        else:\n            try:\n                Bp = SIGe = broadcast_1d_array(self.esig, output_ndims)\n                SIGc = broadcast_1d_array(self.csig, output_ndims)\n            except KeyError:\n                raise ValueError(\"Impossible to compute vertical levels,\"\n                                 \" data is missing (esig, csig)\")\n            Ap = Cp = Ptop\n\n        Pe = Ap + Bp * (Psurf - Cp)\n        Pc = 0.5 * (Pe[0:-1] + Pe[1:])\n\n        if self.hybrid:\n            ETAe = (Pe - Ptop)/(Psurf - Ptop)\n            ETAc = (Pc - Ptop)/(Psurf - Ptop)\n        else:\n            SIGe = SIGe * np.ones_like(Psurf)\n            SIGc = SIGc * np.ones_like(Psurf)\n\n        Ze = prof_altitude(Pe, **kwargs)\n        Zc = prof_altitude(Pc, **kwargs)\n\n        all_vars = {'eta_edges': ETAe,\n                    'eta_centers': ETAc,\n                    'sigma_edges': SIGe,\n                    'sigma_centers': SIGc,\n                    'pressure_edges': Pe,\n                    'pressure_centers': Pc,\n                    'altitude_edges': Ze,\n                    'altitude_centers': Zc}\n\n        return all_vars", "response": "Compute scalars or coordinates associated to the vertical layers of a given model coordinate system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_lonlat(self):\n\n        rlon, rlat = self.resolution\n\n        # Compute number of grid cells in each direction\n        Nlon = int(360. / rlon)\n        Nlat = int(180. / rlat) + self.halfpolar\n\n        # Compute grid cell edges\n        elon = np.arange(Nlon + 1) * rlon - np.array(180.)\n        elon -= rlon / 2. * self.center180\n        elat = np.arange(Nlat + 1) * rlat - np.array(90.)\n        elat -= rlat / 2. * self.halfpolar\n        elat[0] = -90.\n        elat[-1] = 90.\n\n        # Compute grid cell centers\n        clon = (elon - (rlon / 2.))[1:]\n        clat = np.arange(Nlat) * rlat - np.array(90.)\n\n        # Fix grid boundaries if halfpolar\n        if self.halfpolar:\n            clat[0] = (elat[0] + elat[1]) / 2.\n            clat[-1] = -clat[0]\n        else:\n            clat += (elat[1] - elat[0]) / 2.\n\n        return {\n            \"lon_centers\": clon, \"lat_centers\": clat,\n            \"lon_edges\": elon, \"lat_edges\": elat\n        }", "response": "Returns a dictionary of longitude - latitude grids for a specified resolution and configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_template_dirs():\n    return filter(lambda x: os.path.exists(x), [\n        # user dir\n        os.path.join(os.path.expanduser('~'), '.py2pack', 'templates'),\n        # system wide dir\n        os.path.join('/', 'usr', 'share', 'py2pack', 'templates'),\n        # usually inside the site-packages dir\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates'),\n    ])", "response": "Returns a list of directories where to search for jinja2 templates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _license_from_classifiers(data):\n    classifiers = data.get('classifiers', [])\n    found_license = None\n    for c in classifiers:\n        if c.startswith(\"License :: OSI Approved :: \"):\n            found_license = c.replace(\"License :: OSI Approved :: \", \"\")\n    return found_license", "response": "try to get a license from the classifiers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_license(data):\n    license = data.get('license', None)\n    if not license:\n        # try to get license from classifiers\n        license = _license_from_classifiers(data)\n    if license:\n        if license in SDPX_LICENSES.keys():\n            data['license'] = SDPX_LICENSES[license]\n        else:\n            data['license'] = \"%s (FIXME:No SPDX)\" % (license)\n    else:\n        data['license'] = \"\"", "response": "normalize the license of the data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping an IPython s Prompt class to inject escape sequences at right positions for shell integrations.", "response": "def wrap_prompts_class(Klass):\n    \"\"\"\n    Wrap an IPython's Prompt class\n\n    This is needed in order for Prompt to inject the correct escape sequences\n    at the right positions for shell integrations.\n\n    \"\"\"\n\n    try:\n        from prompt_toolkit.token import ZeroWidthEscape\n    except ImportError:\n        return Klass\n\n    class ITerm2IPythonPrompt(Klass):\n\n        def in_prompt_tokens(self, cli=None):\n            return  [\n                     (ZeroWidthEscape, last_status(self.shell)+BEFORE_PROMPT),\n                    ]+\\\n                    super(ITerm2IPythonPrompt, self).in_prompt_tokens(cli)+\\\n                    [(ZeroWidthEscape, AFTER_PROMPT)]\n\n\n    return ITerm2IPythonPrompt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing the star in a format string fmt so that struct. calcsize is equal to the given size.", "response": "def _replace_star(fmt, size):\n    \"\"\"\n    Replace the `*` placeholder in a format string (fmt), so that\n    struct.calcsize(fmt) is equal to the given `size` using the format\n    following the placeholder.\n\n    Raises `ValueError` if number of `*` is larger than 1. If no `*`\n    in `fmt`, returns `fmt` without checking its size!\n\n    Examples\n    --------\n    >>> _replace_star('ii*fi', 40)\n    'ii7fi'\n    \"\"\"\n    n_stars = fmt.count('*')\n\n    if n_stars > 1:\n        raise ValueError(\"More than one `*` in format (%s).\" % fmt)\n\n    if n_stars:\n        i = fmt.find('*')\n        s = struct.calcsize(fmt.replace(fmt[i:i + 2], ''))\n        n = old_div((size - s), struct.calcsize(fmt[i + 1]))\n\n        fmt = fmt.replace('*', str(n))\n\n    return fmt"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads pre - or suffix of line at current position with given format fmt.", "response": "def _fix(self, fmt='i'):\n        \"\"\"\n        Read pre- or suffix of line at current position with given\n        format `fmt` (default 'i').\n        \"\"\"\n        fmt = self.endian + fmt\n        fix = self.read(struct.calcsize(fmt))\n        if fix:\n            return struct.unpack(fmt, fix)[0]\n        else:\n            raise EOFError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread next unformatted line.", "response": "def readline(self, fmt=None):\n        \"\"\"\n        Return next unformatted \"line\". If format is given, unpack content,\n        otherwise return byte string.\n        \"\"\"\n        prefix_size = self._fix()\n\n        if fmt is None:\n            content = self.read(prefix_size)\n        else:\n            fmt = self.endian + fmt\n            fmt = _replace_star(fmt, prefix_size)\n            content = struct.unpack(fmt, self.read(prefix_size))\n\n        try:\n            suffix_size = self._fix()\n        except EOFError:\n            # when endian is invalid and prefix_size > total file size\n            suffix_size = -1\n\n        if prefix_size != suffix_size:\n            raise IOError(_FIX_ERROR)\n\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nskip the next line and returns position and size of line. Raises IOError if the prefix and suffix of line do not match.", "response": "def skipline(self):\n        \"\"\"\n        Skip the next line and returns position and size of line.\n        Raises IOError if pre- and suffix of line do not match.\n        \"\"\"\n        position = self.tell()\n        prefix = self._fix()\n        self.seek(prefix, 1)  # skip content\n        suffix = self._fix()\n\n        if prefix != suffix:\n            raise IOError(_FIX_ERROR)\n\n        return position, prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef writeline(self, fmt, *args):\n        fmt = self.endian + fmt\n        size = struct.calcsize(fmt)\n\n        fix = struct.pack(self.endian + 'i', size)\n        line = struct.pack(fmt, *args)\n\n        self.write(fix)\n        self.write(line)\n        self.write(fix)", "response": "Writes a single line of data with given format to file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef writelines(self, lines, fmt):\n        if isinstance(fmt, basestring):\n            fmt = [fmt] * len(lines)\n        for f, line in zip(fmt, lines):\n            self.writeline(f, line, self.endian)", "response": "Write lines with given format."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a variable length integer from the stream.", "response": "def read_varint(stream):\n    \"\"\"Read while the most significant bit is set, then put the 7 least\n    significant bits of all read bytes together to create a number.\n\n    \"\"\"\n    value = 0\n    while True:\n        byte = ord(stream.read(1))\n        if not byte & 0b10000000:\n            return value << 7 | byte\n        value = value << 7 | (byte & 0b01111111)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a GEOS - Chem BPCH file output as an xarray Dataset.", "response": "def open_bpchdataset(filename, fields=[], categories=[],\n                     tracerinfo_file='tracerinfo.dat',\n                     diaginfo_file='diaginfo.dat',\n                     endian=\">\", decode_cf=True,\n                     memmap=True, dask=True, return_store=False):\n    \"\"\" Open a GEOS-Chem BPCH file output as an xarray Dataset.\n\n    Parameters\n    ----------\n    filename : string\n        Path to the output file to read in.\n    {tracerinfo,diaginfo}_file : string, optional\n        Path to the metadata \"info\" .dat files which are used to decipher\n        the metadata corresponding to each variable in the output dataset.\n        If not provided, will look for them in the current directory or\n        fall back on a generic set.\n    fields : list, optional\n        List of a subset of variable names to return. This can substantially\n        improve read performance. Note that the field here is just the tracer\n        name - not the category, e.g. 'O3' instead of 'IJ-AVG-$_O3'.\n    categories : list, optional\n        List a subset of variable categories to look through. This can\n        substantially improve read performance.\n    endian : {'=', '>', '<'}, optional\n        Endianness of file on disk. By default, \"big endian\" (\">\") is assumed.\n    decode_cf : bool\n        Enforce CF conventions for variable names, units, and other metadata\n    default_dtype : numpy.dtype, optional\n        Default datatype for variables encoded in file on disk (single-precision\n        float by default).\n    memmap : bool\n        Flag indicating that data should be memory-mapped from disk instead of\n        eagerly loaded into memory\n    dask : bool\n        Flag indicating that data reading should be deferred (delayed) to\n        construct a task-graph for later execution\n    return_store : bool\n        Also return the underlying DataStore to the user\n\n    Returns\n    -------\n    ds : xarray.Dataset\n        Dataset containing the requested fields (or the entire file), with data\n        contained in proxy containers for access later.\n    store : xarray.AbstractDataStore\n        Underlying DataStore which handles the loading and processing of\n        bpch files on disk\n\n    \"\"\"\n\n    store = BPCHDataStore(\n        filename, fields=fields, categories=categories,\n        tracerinfo_file=tracerinfo_file,\n        diaginfo_file=diaginfo_file, endian=endian,\n        use_mmap=memmap, dask_delayed=dask\n    )\n    ds = xr.Dataset.load_store(store)\n    # Record what the file object underlying the store which we culled this\n    # Dataset from is so that we can clean it up later\n    ds._file_obj = store._bpch\n\n    # Handle CF corrections\n    if decode_cf:\n        decoded_vars = OrderedDict()\n        rename_dict = {}\n        for v in ds.variables:\n            cf_name = cf.get_valid_varname(v)\n            rename_dict[v] = cf_name\n            new_var = cf.enforce_cf_variable(ds[v])\n            decoded_vars[cf_name] = new_var\n        ds = xr.Dataset(decoded_vars, attrs=ds.attrs.copy())\n\n        # ds.rename(rename_dict, inplace=True)\n\n        # TODO: There's a bug with xr.decode_cf which eagerly loads data.\n        #       Re-enable this once that bug is fixed\n        # Note that we do not need to decode the times because we explicitly\n        # kept track of them as we parsed the data.\n        # ds = xr.decode_cf(ds, decode_times=False)\n\n    # Set attributes for CF conventions\n    ts = get_timestamp()\n    ds.attrs.update(dict(\n        Conventions='CF1.6',\n        source=filename,\n        tracerinfo=tracerinfo_file,\n        diaginfo=diaginfo_file,\n        filetype=store._bpch.filetype,\n        filetitle=store._bpch.filetitle,\n        history=(\n            \"{}: Processed/loaded by xbpch-{} from {}\"\n            .format(ts, ver, filename)\n        ),\n    ))\n\n    # To immediately load the data from the BPCHDataProxy paylods, need\n    # to execute ds.data_vars for some reason...\n    if return_store:\n        return ds, store\n    else:\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open_mfbpchdataset(paths, concat_dim='time', compat='no_conflicts',\n                       preprocess=None, lock=None, **kwargs):\n    \"\"\" Open multiple bpch files as a single dataset.\n\n    You must have dask installed for this to work, as this greatly\n    simplifies issues relating to multi-file I/O.\n\n    Also, please note that this is not a very performant routine. I/O is still\n    limited by the fact that we need to manually scan/read through each bpch\n    file so that we can figure out what its contents are, since that metadata\n    isn't saved anywhere. So this routine will actually sequentially load\n    Datasets for each bpch file, then concatenate them along the \"time\" axis.\n    You may wish to simply process each file individually, coerce to NetCDF,\n    and then ingest through xarray as normal.\n\n    Parameters\n    ----------\n    paths : list of strs\n        Filenames to load; order doesn't matter as they will be\n        lexicographically sorted before we read in the data\n    concat_dim : str, default='time'\n        Dimension to concatenate Datasets over. We default to \"time\" since this\n        is how GEOS-Chem splits output files\n    compat : str (optional)\n        String indicating how to compare variables of the same name for\n        potential conflicts when merging:\n\n        - 'broadcast_equals': all values must be equal when variables are\n          broadcast against each other to ensure common dimensions.\n        - 'equals': all values and dimensions must be the same.\n        - 'identical': all values, dimensions and attributes must be the\n          same.\n        - 'no_conflicts': only values which are not null in both datasets\n          must be equal. The returned dataset then contains the combination\n          of all non-null values.\n    preprocess : callable (optional)\n        A pre-processing function to apply to each Dataset prior to\n        concatenation\n    lock : False, True, or threading.Lock (optional)\n        Passed to :py:func:`dask.array.from_array`. By default, xarray\n        employs a per-variable lock when reading data from NetCDF files,\n        but this model has not yet been extended or implemented for bpch files\n        and so this is not actually used. However, it is likely necessary\n        before dask's multi-threaded backend can be used\n    **kwargs : optional\n        Additional arguments to pass to :py:func:`xbpch.open_bpchdataset`.\n    \n    \"\"\"\n\n    from xarray.backends.api import _MultiFileCloser\n\n    # TODO: Include file locks?\n\n    # Check for dask\n    dask = kwargs.pop('dask', False)\n    if not dask:\n        raise ValueError(\"Reading multiple files without dask is not supported\")\n    kwargs['dask'] = True\n\n    # Add th\n\n    if isinstance(paths, basestring):\n        paths = sorted(glob(paths))\n    if not paths:\n        raise IOError(\"No paths to files were passed into open_mfbpchdataset\")\n\n    datasets = [open_bpchdataset(filename, **kwargs)\n                for filename in paths]\n    bpch_objs = [ds._file_obj for ds in datasets]\n\n    if preprocess is not None:\n        datasets = [preprocess(ds) for ds in datasets]\n\n    # Concatenate over time\n    combined = xr.auto_combine(datasets, compat=compat, concat_dim=concat_dim)\n\n    combined._file_obj = _MultiFileCloser(bpch_objs)\n    combined.attrs = datasets[0].attrs\n    ts = get_timestamp()\n    fns_str = \" \".join(paths)\n    combined.attrs['history'] = (\n        \"{}: Processed/loaded by xbpch-{} from {}\"\n        .format(ts, ver, fns_str)\n    )\n\n\n    return combined", "response": "Open multiple bpch files as a single dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Remember to use orm['appname.ModelName'] rather than \"from appname.models...\"\n        for entry in orm['multilingual_news.NewsEntry'].objects.all():\n            self.migrate_placeholder(\n                orm, entry, 'excerpt', 'multilingual_news_excerpt', 'excerpt')\n            self.migrate_placeholder(\n                orm, entry, 'content', 'multilingual_news_content', 'content')", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a bytes string that displays image given by bytes b in the terminal Taxonomy.", "response": "def image_bytes(b, filename=None, inline=1, width='auto', height='auto',\n    preserve_aspect_ratio=None):\n    \"\"\"\n    Return a bytes string that displays image given by bytes b in the terminal\n\n    If filename=None, the filename defaults to \"Unnamed file\"\n\n    width and height are strings, following the format\n\n        N: N character cells.\n\n        Npx: N pixels.\n\n        N%: N percent of the session's width or height.\n\n        'auto': The image's inherent size will be used to determine an appropriate\n                dimension.\n\n    preserve_aspect_ratio sets whether the aspect ratio of the image is\n    preserved. The default (None) is True unless both width and height are\n    set.\n\n    See https://www.iterm2.com/documentation-images.html\n    \"\"\"\n    if preserve_aspect_ratio is None:\n        if width != 'auto' and height != 'auto':\n            preserve_aspect_ratio = False\n        else:\n            preserve_aspect_ratio = True\n    data = {\n        'name': base64.b64encode((filename or 'Unnamed file').encode('utf-8')).decode('ascii'),\n        'inline': inline,\n        'size': len(b),\n        'base64_img': base64.b64encode(b).decode('ascii'),\n        'width': width,\n        'height': height,\n        'preserve_aspect_ratio': int(preserve_aspect_ratio),\n        }\n    # IMAGE_CODE is a string because bytes doesn't support formatting\n    return IMAGE_CODE.format(**data).encode('ascii')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display_image_bytes(b, filename=None, inline=1, width='auto',\n    height='auto', preserve_aspect_ratio=None):\n    \"\"\"\n    Display the image given by the bytes b in the terminal.\n\n    If filename=None the filename defaults to \"Unnamed file\".\n\n    width and height are strings, following the format\n\n        N: N character cells.\n\n        Npx: N pixels.\n\n        N%: N percent of the session's width or height.\n\n        'auto': The image's inherent size will be used to determine an appropriate\n                dimension.\n\n    preserve_aspect_ratio sets whether the aspect ratio of the image is\n    preserved. The default (None) is True unless both width and height are\n    set.\n\n    See https://www.iterm2.com/documentation-images.html\n\n    \"\"\"\n    sys.stdout.buffer.write(image_bytes(b, filename=filename, inline=inline,\n        width=width, height=height, preserve_aspect_ratio=preserve_aspect_ratio))\n    sys.stdout.write('\\n')", "response": "Display the image given by the bytes b in the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying an image in the terminal.", "response": "def display_image_file(fn, width='auto', height='auto', preserve_aspect_ratio=None):\n    \"\"\"\n    Display an image in the terminal.\n\n    A newline is not printed.\n\n    width and height are strings, following the format\n\n        N: N character cells.\n\n        Npx: N pixels.\n\n        N%: N percent of the session's width or height.\n\n        'auto': The image's inherent size will be used to determine an appropriate\n                dimension.\n\n    preserve_aspect_ratio sets whether the aspect ratio of the image is\n    preserved. The default (None) is True unless both width and height are\n    set.\n\n    See https://www.iterm2.com/documentation-images.html\n\n    \"\"\"\n    with open(os.path.realpath(os.path.expanduser(fn)), 'rb') as f:\n        sys.stdout.buffer.write(image_bytes(f.read(), filename=fn,\n            width=width, height=height,\n            preserve_aspect_ratio=preserve_aspect_ratio))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_requirements(*args):\n    requirements = set()\n    contents = get_contents(*args)\n    for line in contents.splitlines():\n        # Strip comments.\n        line = re.sub(r'^#.*|\\s#.*', '', line)\n        # Ignore empty lines\n        if line and not line.isspace():\n            requirements.add(re.sub(r'\\s+', '', line))\n    return sorted(requirements)", "response": "Get requirements from pip requirement files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_holidays(self, division=None, year=None):\n        if division:\n            holidays = self.data[division]\n        else:\n            holidays = self.data[self.ENGLAND_AND_WALES]\n            dates_in_common = six.moves.reduce(\n                set.intersection,\n                (set(map(lambda holiday: holiday['date'], division_holidays))\n                 for division, division_holidays in six.iteritems(self.data))\n            )\n            holidays = filter(lambda holiday: holiday['date'] in dates_in_common, holidays)\n        if year:\n            holidays = filter(lambda holiday: holiday['date'].year == year, holidays)\n        return list(holidays)", "response": "Gets a list of all known bank holidays optionally filtered by division and year"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the next known bank holiday in the given division", "response": "def get_next_holiday(self, division=None, date=None):\n        \"\"\"\n        Returns the next known bank holiday\n        :param division: see division constants; defaults to common holidays\n        :param date: search starting from this date; defaults to today\n        :return: dict\n        \"\"\"\n        date = date or datetime.date.today()\n        for holiday in self.get_holidays(division=division):\n            if holiday['date'] > date:\n                return holiday"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the date is a known bank holiday False otherwise.", "response": "def is_holiday(self, date, division=None):\n        \"\"\"\n        True if the date is a known bank holiday\n        :param date: the date to check\n        :param division: see division constants; defaults to common holidays\n        :return: bool\n        \"\"\"\n        return date in (holiday['date'] for holiday in self.get_holidays(division=division))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the next work day in the given division", "response": "def get_next_work_day(self, division=None, date=None):\n        \"\"\"\n        Returns the next work day, skipping weekends and bank holidays\n        :param division: see division constants; defaults to common holidays\n        :param date: search starting from this date; defaults to today\n        :return: datetime.date; NB: get_next_holiday returns a dict\n        \"\"\"\n        date = date or datetime.date.today()\n        one_day = datetime.timedelta(days=1)\n        holidays = set(holiday['date'] for holiday in self.get_holidays(division=division))\n        while True:\n            date += one_day\n            if date.weekday() not in self.weekend and date not in holidays:\n                return date"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the date is a weekend or a known bank holiday", "response": "def is_work_day(self, date, division=None):\n        \"\"\"\n        True if the date is not a weekend or a known bank holiday\n        :param date: the date to check\n        :param division: see division constants; defaults to common holidays\n        :return: bool\n        \"\"\"\n        return date.weekday() not in self.weekend and date not in (\n            holiday['date'] for holiday in self.get_holidays(division=division)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_regions_with_tiles(self):\n        for key in self.get_all_keys():\n            (layer, rx, ry) = struct.unpack('>BHH', key)\n            if layer == 1:\n                yield (rx, ry)", "response": "Generator which yields a set of tuples which describe\n        all regions which the world has tile data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_entity_uuid_coords(self, uuid):\n        if uuid in self._entity_to_region_map:\n            coords = self._entity_to_region_map[uuid]\n            entities = self.get_entities(*coords)\n            for entity in entities:\n                if 'uniqueId' in entity.data and entity.data['uniqueId'] == uuid:\n                    return tuple(entity.data['tilePosition'])\n        return None", "response": "Returns the coordinates of the given entity UUID inside this world or None if the UUID is not found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary that maps UUIDs to region coordinates.", "response": "def _entity_to_region_map(self):\n        \"\"\"\n        A dict whose keys are the UUIDs (or just IDs, in some cases) of\n        entities, and whose values are the `(rx, ry)` coordinates in which that\n        entity can be found. This can be used to easily locate particular\n        entities inside the world.\n        \"\"\"\n        entity_to_region = {}\n        for key in self.get_all_keys():\n            layer, rx, ry = struct.unpack('>BHH', key)\n            if layer != 4:\n                continue\n            stream = io.BytesIO(self.get(layer, rx, ry))\n            num_entities = sbon.read_varint(stream)\n            for _ in range(num_entities):\n                uuid = sbon.read_string(stream)\n                if uuid in entity_to_region:\n                    raise ValueError('Duplicate UUID {}'.format(uuid))\n                entity_to_region[uuid] = (rx, ry)\n        return entity_to_region"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a string into a fuzzy regular expression pattern.", "response": "def create_fuzzy_pattern(pattern):\n    \"\"\"\n    Convert a string into a fuzzy regular expression pattern.\n\n    :param pattern: The input pattern (a string).\n    :returns: A compiled regular expression object.\n\n    This function works by adding ``.*`` between each of the characters in the\n    input pattern and compiling the resulting expression into a case\n    insensitive regular expression.\n    \"\"\"\n    return re.compile(\".*\".join(map(re.escape, pattern)), re.IGNORECASE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filtered_entries(self):\n        return [\n            e for e in self.entries if not any(fnmatch.fnmatch(e.name.lower(), p.lower()) for p in self.exclude_list)\n        ]", "response": "A list of : class : PasswordEntry objects that don t match the exclude list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fuzzy_search(self, *filters):\n        matches = []\n        logger.verbose(\n            \"Performing fuzzy search on %s (%s) ..\", pluralize(len(filters), \"pattern\"), concatenate(map(repr, filters))\n        )\n        patterns = list(map(create_fuzzy_pattern, filters))\n        for entry in self.filtered_entries:\n            if all(p.search(entry.name) for p in patterns):\n                matches.append(entry)\n        logger.log(\n            logging.INFO if matches else logging.VERBOSE,\n            \"Matched %s using fuzzy search.\",\n            pluralize(len(matches), \"password\"),\n        )\n        return matches", "response": "Perform a fuzzy search on the password names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_entry(self, *arguments):\n        matches = self.smart_search(*arguments)\n        if len(matches) > 1:\n            logger.info(\"More than one match, prompting for choice ..\")\n            labels = [entry.name for entry in matches]\n            return matches[labels.index(prompt_for_choice(labels))]\n        else:\n            logger.info(\"Matched one entry: %s\", matches[0].name)\n            return matches[0]", "response": "Select a password from the available choices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a simple search for case insensitive substring matches.", "response": "def simple_search(self, *keywords):\n        \"\"\"\n        Perform a simple search for case insensitive substring matches.\n\n        :param keywords: The string(s) to search for.\n        :returns: The matched password names (a generator of strings).\n\n        Only passwords whose names matches *all*  of the given keywords are\n        returned.\n        \"\"\"\n        matches = []\n        keywords = [kw.lower() for kw in keywords]\n        logger.verbose(\n            \"Performing simple search on %s (%s) ..\",\n            pluralize(len(keywords), \"keyword\"),\n            concatenate(map(repr, keywords)),\n        )\n        for entry in self.filtered_entries:\n            normalized = entry.name.lower()\n            if all(kw in normalized for kw in keywords):\n                matches.append(entry)\n        logger.log(\n            logging.INFO if matches else logging.VERBOSE,\n            \"Matched %s using simple search.\",\n            pluralize(len(matches), \"password\"),\n        )\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smart_search(self, *arguments):\n        matches = self.simple_search(*arguments)\n        if not matches:\n            logger.verbose(\"Falling back from substring search to fuzzy search ..\")\n            matches = self.fuzzy_search(*arguments)\n        if not matches:\n            if len(self.filtered_entries) > 0:\n                raise NoMatchingPasswordError(\n                    format(\"No passwords matched the given arguments! (%s)\", concatenate(map(repr, arguments)))\n                )\n            else:\n                msg = \"You don't have any passwords yet! (no *.gpg files found)\"\n                raise EmptyPasswordStoreError(msg)\n        return matches", "response": "Perform a smart search on the given keywords or patterns."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new execution context object with the values set to the value of context.", "response": "def context(self):\n        \"\"\"\n        An execution context created using :mod:`executor.contexts`.\n\n        The value of :attr:`context` defaults to a\n        :class:`~executor.contexts.LocalContext` object with the following\n        characteristics:\n\n        - The working directory of the execution context is set to the\n          value of :attr:`directory`.\n\n        - The environment variable given by :data:`DIRECTORY_VARIABLE` is set\n          to the value of :attr:`directory`.\n\n        :raises: :exc:`.MissingPasswordStoreError` when :attr:`directory`\n                 doesn't exist.\n        \"\"\"\n        # Make sure the directory exists.\n        self.ensure_directory_exists()\n        # Prepare the environment variables.\n        environment = {DIRECTORY_VARIABLE: self.directory}\n        try:\n            # Try to enable the GPG agent in headless sessions.\n            environment.update(get_gpg_variables())\n        except Exception:\n            # If we failed then let's at least make sure that the\n            # $GPG_TTY environment variable is set correctly.\n            environment.update(GPG_TTY=execute(\"tty\", capture=True, check=False, tty=True, silent=True))\n        return LocalContext(directory=self.directory, environment=environment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the directory of the log entry.", "response": "def directory(self, value):\n        \"\"\"Normalize the value of :attr:`directory` when it's set.\"\"\"\n        # Normalize the value of `directory'.\n        set_property(self, \"directory\", parse_path(value))\n        # Clear the computed values of `context' and `entries'.\n        clear_property(self, \"context\")\n        clear_property(self, \"entries\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_directory_exists(self):\n        if not os.path.isdir(self.directory):\n            msg = \"The password storage directory doesn't exist! (%s)\"\n            raise MissingPasswordStoreError(msg % self.directory)", "response": "Ensures that the password storage directory exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the entry s text for viewing on a terminal.", "response": "def format_text(self, include_password=True, use_colors=None, padding=True, filters=()):\n        \"\"\"\n        Format :attr:`text` for viewing on a terminal.\n\n        :param include_password: :data:`True` to include the password in the\n                                 formatted text, :data:`False` to exclude the\n                                 password from the formatted text.\n        :param use_colors: :data:`True` to use ANSI escape sequences,\n                           :data:`False` otherwise. When this is :data:`None`\n                           :func:`~humanfriendly.terminal.terminal_supports_colors()`\n                           will be used to detect whether ANSI escape sequences\n                           are supported.\n        :param padding: :data:`True` to add empty lines before and after the\n                        entry and indent the entry's text with two spaces,\n                        :data:`False` to skip the padding.\n        :param filters: An iterable of regular expression patterns (defaults to\n                        an empty tuple). If a line in the entry's text matches\n                        one of these patterns it won't be shown on the\n                        terminal.\n        :returns: The formatted entry (a string).\n        \"\"\"\n        # Determine whether we can use ANSI escape sequences.\n        if use_colors is None:\n            use_colors = terminal_supports_colors()\n        # Extract the password (first line) from the entry.\n        lines = self.text.splitlines()\n        password = lines.pop(0).strip()\n        # Compile the given patterns to case insensitive regular expressions\n        # and use them to ignore lines that match any of the given filters.\n        patterns = [coerce_pattern(f, re.IGNORECASE) for f in filters]\n        lines = [l for l in lines if not any(p.search(l) for p in patterns)]\n        text = trim_empty_lines(\"\\n\".join(lines))\n        # Include the password in the formatted text?\n        if include_password:\n            text = \"Password: %s\\n%s\" % (password, text)\n        # Add the name to the entry (only when there's something to show).\n        if text and not text.isspace():\n            title = \" / \".join(split(self.name, \"/\"))\n            if use_colors:\n                title = ansi_wrap(title, bold=True)\n            text = \"%s\\n\\n%s\" % (title, text)\n        # Highlight the entry's text using ANSI escape sequences.\n        lines = []\n        for line in text.splitlines():\n            # Check for a \"Key: Value\" line.\n            match = KEY_VALUE_PATTERN.match(line)\n            if match:\n                key = \"%s:\" % match.group(1).strip()\n                value = match.group(2).strip()\n                if use_colors:\n                    # Highlight the key.\n                    key = ansi_wrap(key, color=HIGHLIGHT_COLOR)\n                    # Underline hyperlinks in the value.\n                    tokens = value.split()\n                    for i in range(len(tokens)):\n                        if \"://\" in tokens[i]:\n                            tokens[i] = ansi_wrap(tokens[i], underline=True)\n                    # Replace the line with a highlighted version.\n                    line = key + \" \" + \" \".join(tokens)\n            if padding:\n                line = \"  \" + line\n            lines.append(line)\n        text = \"\\n\".join(lines)\n        text = trim_empty_lines(text)\n        if text and padding:\n            text = \"\\n%s\\n\" % text\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_diaginfo(diaginfo_file):\n\n    widths = [rec.width for rec in diag_recs]\n    col_names = [rec.name for rec in diag_recs]\n    dtypes = [rec.type for rec in diag_recs]\n    usecols = [name for name in col_names if not name.startswith('-')]\n\n    diag_df = pd.read_fwf(diaginfo_file, widths=widths, names=col_names,\n                          dtypes=dtypes, comment=\"#\", header=None,\n                          usecols=usecols)\n    diag_desc = {diag.name: diag.desc for diag in diag_recs\n                 if not diag.name.startswith('-')}\n\n    return diag_df, diag_desc", "response": "Read an output s diaginfo. dat file and parse into a DataFrame for selecting and parsing categories."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading an output s tracerinfo. dat file and parse into a DataFrame for selecting and parsing categories.", "response": "def get_tracerinfo(tracerinfo_file):\n    \"\"\"\n    Read an output's tracerinfo.dat file and parse into a DataFrame for\n    use in selecting and parsing categories.\n\n    Parameters\n    ----------\n    tracerinfo_file : str\n        Path to tracerinfo.dat\n\n    Returns\n    -------\n    DataFrame containing the tracer information.\n\n    \"\"\"\n\n    widths = [rec.width for rec in tracer_recs]\n    col_names = [rec.name for rec in tracer_recs]\n    dtypes = [rec.type for rec in tracer_recs]\n    usecols = [name for name in col_names if not name.startswith('-')]\n\n    tracer_df = pd.read_fwf(tracerinfo_file, widths=widths, names=col_names,\n                            dtypes=dtypes, comment=\"#\", header=None,\n                            usecols=usecols)\n\n    # Check an edge case related to a bug in GEOS-Chem v12.0.3 which \n    # erroneously dropped short/long tracer names in certain tracerinfo.dat outputs.\n    # What we do here is figure out which rows were erroneously processed (they'll \n    # have NaNs in them) and raise a warning if there are any\n    na_free = tracer_df.dropna(subset=['tracer', 'scale'])\n    only_na = tracer_df[~tracer_df.index.isin(na_free.index)]\n    if len(only_na) > 0:\n        warn(\"At least one row in {} wasn't decoded correctly; we strongly\"\n             \" recommend you manually check that file to see that all\"\n             \" tracers are properly recorded.\"\n             .format(tracerinfo_file)) \n\n    tracer_desc = {tracer.name: tracer.desc for tracer in tracer_recs\n                   if not tracer.name.startswith('-')}\n\n    # Process some of the information about which variables are hydrocarbons\n    # and chemical tracers versus other diagnostics.\n    def _assign_hydrocarbon(row):\n        if row['C'] != 1:\n            row['hydrocarbon'] = True\n            row['molwt'] = C_MOLECULAR_WEIGHT\n        else:\n            row['hydrocarbon'] = False\n        return row\n\n    tracer_df = (\n        tracer_df\n            .apply(_assign_hydrocarbon, axis=1)\n            .assign(chemical=lambda x: x['molwt'].astype(bool))\n    )\n\n    return tracer_df, tracer_desc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a chunk of data from a bpch output file.", "response": "def read_from_bpch(filename, file_position, shape, dtype, endian,\n                   use_mmap=False):\n    \"\"\" Read a chunk of data from a bpch output file.\n\n    Parameters\n    ----------\n    filename : str\n        Path to file on disk containing the  data\n    file_position : int\n        Position (bytes) where desired data chunk begins\n    shape : tuple of ints\n        Resultant (n-dimensional) shape of requested data; the chunk\n        will be read sequentially from disk and then re-shaped\n    dtype : dtype\n        Dtype of data; for best results, pass a dtype which includes\n        an endian indicator, e.g. `dtype = np.dtype('>f4')`\n    endian : str\n        Endianness of data; should be consistent with `dtype`\n    use_mmap : bool\n        Memory map the chunk of data to the file on disk, else read\n        immediately\n\n    Returns\n    -------\n    Array with shape `shape` and dtype `dtype` containing the requested\n    chunk of data from `filename`.\n\n    \"\"\"\n    offset = file_position + 4\n    if use_mmap:\n        d = np.memmap(filename, dtype=dtype, mode='r', shape=shape,\n                      offset=offset, order='F')\n    else:\n        with FortranFile(filename, 'rb', endian) as ff:\n            ff.seek(file_position)\n            d = np.array(ff.readline('*f'))\n            d = d.reshape(shape, order='F')\n\n    # As a sanity check, *be sure* that the resulting data block has the\n    # correct shape, and fail early if it doesn't.\n    if (d.shape != shape):\n        raise IOError(\"Data chunk read from {} does not have the right shape,\"\n                      \" (expected {} but got {})\"\n                      .format(filename, shape, d.shape))\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses this bpch file.", "response": "def close(self):\n        \"\"\" Close this bpch file.\n\n        \"\"\"\n\n        if not self.fp.closed:\n            for v in list(self.var_data):\n                del self.var_data[v]\n\n            self.fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the main metadata packaged within a bpch file.", "response": "def _read_metadata(self):\n        \"\"\" Read the main metadata packaged within a bpch file, indicating\n        the output filetype and its title.\n\n        \"\"\"\n\n        filetype = self.fp.readline().strip()\n        filetitle = self.fp.readline().strip()\n        # Decode to UTF string, if possible\n        try:\n            filetype = str(filetype, 'utf-8')\n            filetitle = str(filetitle, 'utf-8')\n        except:\n            # TODO: Handle this edge-case of converting file metadata more elegantly.\n            pass\n\n        self.__setattr__('filetype', filetype)\n        self.__setattr__('filetitle', filetitle)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the header information from the file and update the attributes.", "response": "def _read_header(self):\n        \"\"\" Process the header information (data model / grid spec) \"\"\"\n\n        self._header_pos = self.fp.tell()\n\n        line = self.fp.readline('20sffii')\n        modelname, res0, res1, halfpolar, center180 = line\n        self._attributes.update({\n            \"modelname\": str(modelname, 'utf-8').strip(),\n            \"halfpolar\": halfpolar,\n            \"center180\": center180,\n            \"res\": (res0, res1)\n        })\n        self.__setattr__('modelname', modelname)\n        self.__setattr__('res', (res0, res1))\n        self.__setattr__('halfpolar', halfpolar)\n        self.__setattr__('center180', center180)\n\n        # Re-wind the file\n        self.fp.seek(self._header_pos)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the block of this bpch file and return handlers for access to the data contained in the contained base class.", "response": "def _read_var_data(self):\n        \"\"\" Iterate over the block of this bpch file and return handlers\n        in the form of `BPCHDataBundle`s for access to the data contained\n        therein.\n\n        \"\"\"\n\n        var_bundles = OrderedDict()\n        var_attrs = OrderedDict()\n\n        n_vars = 0\n\n        while self.fp.tell() < self.fsize:\n\n            var_attr = OrderedDict()\n\n            # read first and second header lines\n            line = self.fp.readline('20sffii')\n            modelname, res0, res1, halfpolar, center180 = line\n\n            line = self.fp.readline('40si40sdd40s7i')\n            category_name, number, unit, tau0, tau1, reserved = line[:6]\n            dim0, dim1, dim2, dim3, dim4, dim5, skip = line[6:]\n            var_attr['number'] = number\n\n            # Decode byte-strings to utf-8\n            category_name = str(category_name, 'utf-8')\n            var_attr['category'] = category_name.strip()\n            unit = str(unit, 'utf-8')\n\n            # get additional metadata from tracerinfo / diaginfo\n            try:\n                cat_df = self.diaginfo_df[\n                    self.diaginfo_df.name == category_name.strip()\n                ]\n                # TODO: Safer logic for handling case where more than one\n                #       tracer metadata match was made\n                # if len(cat_df > 1):\n                #     raise ValueError(\n                #         \"More than one category matching {} found in \"\n                #         \"diaginfo.dat\".format(\n                #             category_name.strip()\n                #         )\n                #     )\n                # Safe now to select the only row in the DataFrame\n                cat = cat_df.T.squeeze()\n\n                tracer_num = int(cat.offset) + int(number)\n                diag_df = self.tracerinfo_df[\n                    self.tracerinfo_df.tracer == tracer_num\n                ]\n                # TODO: Safer logic for handling case where more than one\n                #       tracer metadata match was made\n                # if len(diag_df > 1):\n                #     raise ValueError(\n                #         \"More than one tracer matching {:d} found in \"\n                #         \"tracerinfo.dat\".format(tracer_num)\n                #     )\n                # Safe now to select only row in the DataFrame\n                diag = diag_df.T.squeeze()\n                diag_attr = diag.to_dict()\n\n                if not unit.strip():  # unit may be empty in bpch\n                    unit = diag_attr['unit']  # but not in tracerinfo\n                var_attr.update(diag_attr)\n            except:\n                diag = {'name': '', 'scale': 1}\n                var_attr.update(diag)\n            var_attr['unit'] = unit\n\n            vname = diag['name']\n            fullname = category_name.strip() + \"_\" + vname\n\n            # parse metadata, get data or set a data proxy\n            if dim2 == 1:\n                data_shape = (dim0, dim1)         # 2D field\n            else:\n                data_shape = (dim0, dim1, dim2)\n            var_attr['original_shape'] = data_shape\n\n            # Add proxy time dimension to shape\n            data_shape = tuple([1, ] + list(data_shape))\n            origin = (dim3, dim4, dim5)\n            var_attr['origin'] = origin\n\n            timelo, timehi = cf.tau2time(tau0), cf.tau2time(tau1)\n\n            pos = self.fp.tell()\n            # Note that we don't pass a dtype, and assume everything is\n            # single-fp floats with the correct endian, as hard-coded\n            var_bundle = BPCHDataBundle(\n                data_shape, self.endian, self.filename, pos, [timelo, timehi],\n                metadata=var_attr,\n                use_mmap=self.use_mmap, dask_delayed=self.dask_delayed\n            )\n            self.fp.skipline()\n\n            # Save the data as a \"bundle\" for concatenating in the final step\n            if fullname in var_bundles:\n                var_bundles[fullname].append(var_bundle)\n            else:\n                var_bundles[fullname] = [var_bundle, ]\n                var_attrs[fullname] = var_attr\n                n_vars += 1\n\n        self.var_data = var_bundles\n        self.var_attrs = var_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef broadcast_1d_array(arr, ndim, axis=1):\n    ext_arr = arr\n    for i in range(ndim - 1):\n        ext_arr = np.expand_dims(ext_arr, axis=axis)\n    return ext_arr", "response": "Broadcasts 1 - d array arr to ndim dimensions on the first axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_timestamp(time=True, date=True, fmt=None):\n\n    time_format = \"%H:%M:%S\"\n    date_format = \"%m-%d-%Y\"\n\n    if fmt is None:\n        if time and date:\n            fmt = time_format + \" \" + date_format\n        elif time:\n            fmt = time_format\n        elif date:\n            fmt = date_format\n        else:\n            raise ValueError(\"One of `date` or `time` must be True!\")\n\n    return datetime.now().strftime(fmt)", "response": "Returns the current timestamp in machine local time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_attr_encoding(ds):\n\n    def _maybe_del_attr(da, attr):\n        \"\"\" Possibly delete an attribute on a DataArray if it's present \"\"\"\n        if attr in da.attrs:\n            del da.attrs[attr]\n        return da\n\n    def _maybe_decode_attr(da, attr):\n        # TODO: Fix this so that bools get written as attributes just fine\n        \"\"\" Possibly coerce an attribute on a DataArray to an easier type\n        to write to disk. \"\"\"\n        # bool -> int\n        if (attr in da.attrs) and (type(da.attrs[attr] == bool)):\n            da.attrs[attr] = int(da.attrs[attr])\n        return da\n\n    for v in ds.data_vars:\n        da = ds[v]\n        da = _maybe_del_attr(da, 'scale_factor')\n        da = _maybe_del_attr(da, 'units')\n        da = _maybe_decode_attr(da, 'hydrocarbon')\n        da = _maybe_decode_attr(da, 'chemical')\n    # Also delete attributes on time.\n    if hasattr(ds, 'time'):\n        times = ds.time\n        times = _maybe_del_attr(times, 'units')\n\n    return ds", "response": "This function handles the attribute encoding of the data array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef after_output(command_status):\n    if command_status not in range(256):\n        raise ValueError(\"command_status must be an integer in the range 0-255\")\n    sys.stdout.write(AFTER_OUTPUT.format(command_status=command_status))\n    # Flushing is important as the command timing feature maybe based on\n    # AFTER_OUTPUT in the future.\n    sys.stdout.flush()", "response": "Shell sequence to be run after the command output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Remember to use orm['appname.ModelName'] rather than \"from appname.models...\"\n        for entry_title in orm.NewsEntryTitle.objects.all():\n            entry = NewsEntry.objects.get(pk=entry_title.entry.pk)\n            entry.translate(entry_title.language)\n            entry.title = entry_title.title\n            entry.slug = entry_title.slug\n            entry.is_published = entry_title.is_published\n            entry.save()", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cfcompliant_units(units, prefix='', suffix=''):\n    compliant_units = units\n\n    for gcunits, udunits in UNITS_MAP_CTM2CF:\n        compliant_units = str.replace(compliant_units, gcunits, udunits)\n\n    return prefix + compliant_units + suffix", "response": "Returns a string representation of the units that are CF - compliant with the udunits2 library."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_valid_varname(varname):\n    vname = varname\n    for s, r in VARNAME_MAP_CHAR:\n        vname = vname.replace(s, r)\n\n    return vname", "response": "Returns a valid variable name for the given variable name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenforcing a variable to be CF - compliant.", "response": "def enforce_cf_variable(var, mask_and_scale=True):\n    \"\"\" Given a Variable constructed from GEOS-Chem output, enforce\n    CF-compliant metadata and formatting.\n\n    Until a bug with lazily-loaded data and masking/scaling is resolved in\n    xarray, you have the option to manually mask and scale the data here.\n\n    Parameters\n    ----------\n    var : xarray.Variable\n        A variable holding information decoded from GEOS-Chem output.\n    mask_and_scale : bool\n        Flag to scale and mask the data given the unit conversions provided\n\n    Returns\n    -------\n    out : xarray.Variable\n        The original variable processed to conform to CF standards\n\n    .. note::\n\n        This method borrows heavily from the ideas in ``xarray.decode_cf_variable``\n\n    \"\"\"\n    var = as_variable(var)\n    data = var._data  # avoid loading by accessing _data instead of data\n    dims = var.dims\n    attrs = var.attrs.copy()\n    encoding = var.encoding.copy()\n    orig_dtype = data.dtype\n\n    # Process masking/scaling coordinates. We only expect a \"scale\" value\n    # for the units with this output.\n    if 'scale' in attrs:\n        scale = attrs.pop('scale')\n        attrs['scale_factor'] = scale\n        encoding['scale_factor'] = scale\n\n        # TODO: Once the xr.decode_cf bug is fixed, we won't need to manually\n        #       handle masking/scaling\n        if mask_and_scale:\n            data = scale*data\n\n    # Process units\n    # TODO: How do we want to handle parts-per-* units? These are not part of\n    #       the udunits standard, and the CF conventions suggest using units\n    #       like 1e-6 for parts-per-million. But we potentially mix mass and\n    #       volume/molar mixing ratios in GEOS-Chem output, so we need a way\n    #       to handle that edge case.\n    if 'unit' in attrs:\n        unit = attrs.pop('unit')\n        unit = get_cfcompliant_units(unit)\n        attrs['units'] = unit\n\n    # TODO: Once the xr.decode_cf bug is fixed, we won't need to manually\n    #       handle masking/scaling\n    return Variable(dims, data, attrs, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef published(self, check_language=True, language=None, kwargs=None,\n                  exclude_kwargs=None):\n        \"\"\"\n        Returns all entries, which publication date has been hit or which have\n        no date and which language matches the current language.\n\n        \"\"\"\n        if check_language:\n            qs = NewsEntry.objects.language(language or get_language()).filter(\n                is_published=True)\n        else:\n            qs = self.get_queryset()\n        qs = qs.filter(\n            models.Q(pub_date__lte=now()) | models.Q(pub_date__isnull=True)\n        )\n        if kwargs is not None:\n            qs = qs.filter(**kwargs)\n        if exclude_kwargs is not None:\n            qs = qs.exclude(**exclude_kwargs)\n        return qs.distinct().order_by('-pub_date')", "response": "Returns all entries which have a published date."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns recently published new entries.", "response": "def recent(self, check_language=True, language=None, limit=3, exclude=None,\n               kwargs=None, category=None):\n        \"\"\"\n        Returns recently published new entries.\n\n        \"\"\"\n        if category:\n            if not kwargs:\n                kwargs = {}\n            kwargs['categories__in'] = [category]\n        qs = self.published(check_language=check_language, language=language,\n                            kwargs=kwargs)\n        if exclude:\n            qs = qs.exclude(pk=exclude.pk)\n        return qs[:limit]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the meta description for the given entry.", "response": "def get_newsentry_meta_description(newsentry):\n    \"\"\"Returns the meta description for the given entry.\"\"\"\n    if newsentry.meta_description:\n        return newsentry.meta_description\n\n    # If there is no seo addon found, take the info from the placeholders\n    text = newsentry.get_description()\n\n    if len(text) > 160:\n        return u'{}...'.format(text[:160])\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_news_placeholder(context, obj, name=False, truncate=False):  # pragma: nocover  # NOQA\n    warnings.warn(\n        \"render_news_placeholder is deprecated. Use render_placeholder\"\n        \" instead\", DeprecationWarning, stacklevel=2)\n    result = ''\n    if context.get('request'):\n        if isinstance(name, int):\n            # If the user doesn't want to use a placeholder name, but a cut, we\n            # need to check if the user has used the name as a number\n            truncate = name\n            name = False\n        if name:\n            # If the name of the placeholder slot is given, get, render and\n            # return it!\n            try:\n                result = safe(getattr(obj, name).render(context, None))\n            except AttributeError:\n                pass\n        else:\n            # If no name is provided get the first placeholder with content\n            for name in ['excerpt', 'content']:\n                rendered = ''\n                try:\n                    rendered = safe(getattr(obj, name).render(context, None))\n                except AttributeError:\n                    pass\n                if rendered:\n                    result = rendered\n                    break\n    if truncate:\n        return truncatewords_html(result, truncate)\n    return result", "response": "Render a NewsEntry placeholder from an object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _requirement_filter_by_marker(req):\n    # type: (pkg_resources.Requirement) -> bool\n    \"\"\"Check if the requirement is satisfied by the marker.\n\n    This function checks for a given Requirement whether its environment marker\n    is satisfied on the current platform. Currently only the python version and\n    system platform are checked.\n    \"\"\"\n    if hasattr(req, 'marker') and req.marker:\n        marker_env = {\n            'python_version': '.'.join(map(str, sys.version_info[:2])),\n            'sys_platform': sys.platform\n        }\n        if not req.marker.evaluate(environment=marker_env):\n            return False\n    return True", "response": "Checks if a given Requirement is satisfied by the marker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _requirement_find_lowest_possible(req):\n    # type: (pkg_resources.Requirement) -> List[str]\n    \"\"\"Find lowest required version.\n\n    Given a single Requirement, this function calculates the lowest required\n    version to satisfy it. If the requirement excludes a specific version, then\n    this version will not be used as the minimal supported version.\n\n    Examples\n    --------\n\n    >>> req = pkg_resources.Requirement.parse(\"foobar>=1.0,>2\")\n    >>> _requirement_find_lowest_possible(req)\n    ['foobar', '>=', '1.0']\n    >>> req = pkg_resources.Requirement.parse(\"baz>=1.3,>3,!=1.5\")\n    >>> _requirement_find_lowest_possible(req)\n    ['baz', '>=', '1.3']\n\n    \"\"\"\n    version_dep = None  # type: Optional[str]\n    version_comp = None  # type: Optional[str]\n    for dep in req.specs:\n        version = pkg_resources.parse_version(dep[1])\n        # we don't want to have a not supported version as minimal version\n        if dep[0] == '!=':\n            continue\n        # try to use the lowest version available\n        # i.e. for \">=0.8.4,>=0.9.7\", select \"0.8.4\"\n        if (not version_dep or\n                version < pkg_resources.parse_version(version_dep)):\n            version_dep = dep[1]\n            version_comp = dep[0]\n\n    assert (version_dep is None and version_comp is None) or \\\n        (version_dep is not None and version_comp is not None)\n\n    return [\n        x for x in (req.unsafe_name, version_comp, version_dep)\n        if x is not None]", "response": "Find the lowest required version of a single requirement."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncleaning a list of requirement strings to only contain entries valid for this platform and with the lowest required version only.", "response": "def _requirements_sanitize(req_list):\n    # type: (List[str]) -> List[str]\n    \"\"\"\n    Cleanup a list of requirement strings (e.g. from requirements.txt) to only\n    contain entries valid for this platform and with the lowest required version\n    only.\n\n    Example\n    -------\n\n    >>> from sys import version_info\n    >>> _requirements_sanitize([\n    ...     'foo>=3.0',\n    ...     \"monotonic>=1.0,>0.1;python_version=='2.4'\",\n    ...     \"bar>1.0;python_version=='{}.{}'\".format(version_info[0], version_info[1])\n    ... ])\n    ['foo >= 3.0', 'bar > 1.0']\n    \"\"\"\n    filtered_req_list = (\n        _requirement_find_lowest_possible(req) for req in\n        (pkg_resources.Requirement.parse(s) for s in req_list)\n        if _requirement_filter_by_marker(req)\n    )\n    return [\" \".join(req) for req in filtered_req_list]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ensure_coroutine_function(func):\n    if asyncio.iscoroutinefunction(func):\n        return func\n    else:\n        @asyncio.coroutine\n        def coroutine_function(evt):\n            func(evt)\n            yield\n        return coroutine_function", "response": "Return a coroutine function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef location(self):\n        if self._location is None:\n            self._location = \"{}/{}-{}\".format(\n                self.stream,\n                self.type,\n                self.sequence,\n            )\n        return self._location", "response": "Return a string uniquely identifying the event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the first event matching predicate.", "response": "async def find_backwards(self, stream_name, predicate, predicate_label='predicate'):\n        \"\"\"Return first event matching predicate, or None if none exists.\n\n        Note: 'backwards', both here and in Event Store, means 'towards the\n        event emitted furthest in the past'.\n        \"\"\"\n        logger = self._logger.getChild(predicate_label)\n        logger.info('Fetching first matching event')\n        uri = self._head_uri\n        try:\n            page = await self._fetcher.fetch(uri)\n        except HttpNotFoundError as e:\n            raise StreamNotFoundError() from e\n        while True:\n            evt = next(page.iter_events_matching(predicate), None)\n            if evt is not None:\n                return evt\n\n            uri = page.get_link(\"next\")\n            if uri is None:\n                logger.warning(\"No matching event found\")\n                return None\n\n            page = await self._fetcher.fetch(uri)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    # Initialize logging to the terminal.\n    coloredlogs.install()\n    # Prepare for command line argument parsing.\n    action = show_matching_entry\n    program_opts = dict(exclude_list=[])\n    show_opts = dict(filters=[], use_clipboard=is_clipboard_supported())\n    verbosity = 0\n    # Parse the command line arguments.\n    try:\n        options, arguments = getopt.gnu_getopt(\n            sys.argv[1:],\n            \"elnp:f:x:vqh\",\n            [\"edit\", \"list\", \"no-clipboard\", \"password-store=\", \"filter=\", \"exclude=\", \"verbose\", \"quiet\", \"help\"],\n        )\n        for option, value in options:\n            if option in (\"-e\", \"--edit\"):\n                action = edit_matching_entry\n            elif option in (\"-l\", \"--list\"):\n                action = list_matching_entries\n            elif option in (\"-n\", \"--no-clipboard\"):\n                show_opts[\"use_clipboard\"] = False\n            elif option in (\"-p\", \"--password-store\"):\n                stores = program_opts.setdefault(\"stores\", [])\n                stores.append(PasswordStore(directory=value))\n            elif option in (\"-f\", \"--filter\"):\n                show_opts[\"filters\"].append(value)\n            elif option in (\"-x\", \"--exclude\"):\n                program_opts[\"exclude_list\"].append(value)\n            elif option in (\"-v\", \"--verbose\"):\n                coloredlogs.increase_verbosity()\n                verbosity += 1\n            elif option in (\"-q\", \"--quiet\"):\n                coloredlogs.decrease_verbosity()\n                verbosity -= 1\n            elif option in (\"-h\", \"--help\"):\n                usage(__doc__)\n                return\n            else:\n                raise Exception(\"Unhandled option! (programming error)\")\n        if not (arguments or action == list_matching_entries):\n            usage(__doc__)\n            return\n    except Exception as e:\n        warning(\"Error: %s\", e)\n        sys.exit(1)\n    # Execute the requested action.\n    try:\n        show_opts[\"quiet\"] = verbosity < 0\n        kw = show_opts if action == show_matching_entry else {}\n        action(QuickPass(**program_opts), arguments, **kw)\n    except PasswordStoreError as e:\n        # Known issues don't get a traceback.\n        logger.error(\"%s\", e)\n        sys.exit(1)\n    except KeyboardInterrupt:\n        # If the user interrupted an interactive prompt they most likely did so\n        # intentionally, so there's no point in generating more output here.\n        sys.exit(1)", "response": "Command line interface for the qpass program."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edit_matching_entry(program, arguments):\n    entry = program.select_entry(*arguments)\n    entry.context.execute(\"pass\", \"edit\", entry.name)", "response": "Edit the matching entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the entries matching the given keywords and patterns.", "response": "def list_matching_entries(program, arguments):\n    \"\"\"List the entries matching the given keywords/patterns.\"\"\"\n    output(\"\\n\".join(entry.name for entry in program.smart_search(*arguments)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_matching_entry(program, arguments, use_clipboard=True, quiet=False, filters=()):\n    entry = program.select_entry(*arguments)\n    if not quiet:\n        formatted_entry = entry.format_text(include_password=not use_clipboard, filters=filters)\n        if formatted_entry and not formatted_entry.isspace():\n            output(formatted_entry)\n    if use_clipboard:\n        entry.copy_password()", "response": "Show the matching entry on the terminal and copy the password to the clipboard."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a set of Payflow Pro response parameter name and value pairs into a list of PayflowProObjects and returns a tuple containing the object list and a dictionary containing any unconsumed data.", "response": "def parse_parameters(payflowpro_response_data):\n    \"\"\"\n    Parses a set of Payflow Pro response parameter name and value pairs into \n    a list of PayflowProObjects, and returns a tuple containing the object\n    list and a dictionary containing any unconsumed data. \n    \n    The first item in the object list will always be the Response object, and\n    the RecurringPayments object (if any) will be last.\n\n    The presence of any unconsumed data in the resulting dictionary probably\n    indicates an error or oversight in the PayflowProObject definitions.\n    \"\"\"\n    def build_class(klass, unconsumed_data):\n        known_att_names_set = set(klass.base_fields.keys())\n        available_atts_set = known_att_names_set.intersection(unconsumed_data)\n        if available_atts_set:\n            available_atts = dict()\n            for name in available_atts_set:\n                available_atts[name] = unconsumed_data[name]\n                del unconsumed_data[name]                    \n            return klass(**available_atts)\n        return None\n\n    unconsumed_data = payflowpro_response_data.copy()\n\n    # Parse the response data first\n    response = build_class(Response, unconsumed_data)\n    result_objects = [response]\n    \n    # Parse the remaining data\n    for klass in object.__class__.__subclasses__(PayflowProObject):\n        obj = build_class(klass, unconsumed_data)\n        if obj:\n            result_objects.append(obj)\n    \n    # Special handling of RecurringPayments\n    p_count = 1\n    payments = []\n    while (\"p_result%d\" % p_count) in unconsumed_data:\n        payments.append(RecurringPayment(\n            p_result = unconsumed_data.pop(\"p_result%d\" % p_count, None),\n            p_pnref = unconsumed_data.pop(\"p_pnref%d\" % p_count, None),\n            p_transtate = unconsumed_data.pop(\"p_transtate%d\" % p_count, None),\n            p_tender = unconsumed_data.pop(\"p_tender%d\" % p_count, None),\n            p_transtime = unconsumed_data.pop(\"p_transtime%d\" % p_count, None),\n            p_amt = unconsumed_data.pop(\"p_amt%d\" % p_count, None)))\n        p_count += 1\n    if payments:\n        result_objects.append(RecurringPayments(payments=payments))\n        \n    return (result_objects, unconsumed_data,)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting items stored in canvas to SVG document.", "response": "def convert(document, canvas, items=None, tounicode=None):\n\t\"\"\"\n\tConvert 'items' stored in 'canvas' to SVG 'document'.\n\tIf 'items' is None, then all items are convered.\n\n\ttounicode is a function that get text and returns\n\tit's unicode representation. It should be used when\n\tnational characters are used on canvas.\n\n\tReturn list of XML elements\n\t\"\"\"\n\ttk = canvas.tk\n\tglobal segment\n\n\tif items is None:\t# default: all items\n\t\titems = canvas.find_all()\n\n\tsupported_item_types = \\\n\t\tset([\"line\", \"oval\", \"polygon\", \"rectangle\", \"text\", \"arc\"])\n\t\n\tif tounicode is None:\n\t\ttry:\n\t\t\t# python3\n\t\t\tbytes\n\t\t\ttounicode = lambda x: x\n\t\texcept NameError:\n\t\t\t# python2\n\t\t\ttounicode  = lambda text: str(text).encode(\"utf-8\")\n\n\telements = []\n\tfor item in items:\n\n\t\t# skip unsupported items\n\t\titemtype = canvas.type(item)\n\t\tif itemtype not in supported_item_types:\n\t\t\temit_warning(\"Items of type '%s' are not supported.\" % itemtype)\n\t\t\tcontinue\n\n\t\t# get item coords\n\t\tcoords = canvas.coords(item)\n\n\t\t# get item options;\n\t\t# options is a dict: opt. name -> opt. actual value\n\t\ttmp     = canvas.itemconfigure(item)\n\t\toptions = dict((v0, v4) for v0, v1, v2, v3, v4 in tmp.values())\n\t\t\n\t\t# get state of item\n\t\tstate = options['state']\n\t\tif 'current' in options['tags']:\n\t\t\toptions['state'] = ACTIVE\n\t\telif options['state'] == '':\n\t\t\toptions['state'] = 'normal'\n\t\telse:\n\t\t\t# left state unchanged\n\t\t\tassert options['state'] in ['normal', DISABLED, 'hidden']\n\n\t\t# skip hidden items\n\t\tif options['state'] == 'hidden': continue\n\n\t\tdef get(name, default=\"\"):\n\t\t\tif state == ACTIVE and options.get(state + name):\n\t\t\t\treturn options.get(state + name)\n\t\t\tif state == DISABLED and options.get(state + name):\n\t\t\t\treturn options.get(state + name)\n\n\t\t\tif options.get(name):\n\t\t\t\treturn options.get(name)\n\t\t\telse:\n\t\t\t\treturn default\n\t\t\n\t\t\n\t\tif itemtype == 'line':\n\t\t\toptions['outline'] \t\t\t= ''\n\t\t\toptions['activeoutline'] \t= ''\n\t\t\toptions['disabledoutline'] \t= ''\n\t\telif itemtype == 'arc' and options['style'] == ARC:\n\t\t\toptions['fill'] \t\t\t= ''\n\t\t\toptions['activefill'] \t\t= ''\n\t\t\toptions['disabledfill'] \t= ''\n\n\t\tstyle = {}\n\t\tstyle[\"stroke\"] = HTMLcolor(canvas, get(\"outline\"))\n\n\t\tif get(\"fill\"):\n\t\t\tstyle[\"fill\"] = HTMLcolor(canvas, get(\"fill\"))\n\t\telse:\n\t\t\tstyle[\"fill\"] = \"none\"\n\n\t\t\n\t\twidth = float(options['width'])\n\t\tif state == ACTIVE:\n\t\t\twidth = max(float(options['activewidth']), width)\n\t\telif state == DISABLED:\n\t\t\ttry:\n\t\t\t\tdisabledwidth = options['disabledwidth']\n\t\t\texcept KeyError:\n\t\t\t\t# Text item might not have 'disabledwidth' option. This raises\n\t\t\t\t# the exception in course of processing of such item.\n\t\t\t\t# Default value is 0. Hence, it shall not affect width.\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif float(disabledwidth) > 0:\n\t\t\t\t\twidth = disabledwidth\n\n\t\n\t\tif width != 1.0:\n\t\t\tstyle['stroke-width'] = width\n\t\n\t\t\n\t\tif width:\n\t\t\tdash = canvas.itemcget(item, 'dash')\n\t\t\tif state == DISABLED and canvas.itemcget(item, 'disableddash'):\n\t\t\t\tdash = canvas.itemcget(item, 'disableddash')\n\t\t\telif state == ACTIVE and canvas.itemcget(item, 'activedash'):\n\t\t\t\tdash = canvas.itemcget(item, 'activedash')\n\n\t\t\tif dash != '':\n\t\t\t\ttry:\n\t\t\t\t\tdash = tuple(map(int, dash.split()))\n\t\t\t\texcept ValueError:\n\t\t\t\t\t# int can't parse literal, dash defined with -.,_\n\t\t\t\t\tlinewidth = float(get('width'))\n\t\t\t\t\tdash = parse_dash(dash, linewidth)\n\n\t\t\t\tstyle['stroke-dasharray']  = \",\".join(map(str, dash))\n\t\t\t\tstyle['stroke-dashoffset'] = options['dashoffset']\n\n\n\t\tif itemtype == 'line':\n\t\t\t# in this case, outline is set with fill property\n\t\t\tstyle[\"fill\"], style[\"stroke\"] = \"none\", style[\"fill\"]\n\t\t\n\t\t\tstyle['stroke-linecap'] = cap_style[options['capstyle']]\n\n\t\t\tif options['smooth'] in ['1', 'bezier', 'true']:\n\t\t\t\telement = smoothline(document, coords)\n\t\t\telif options['smooth'] == 'raw':\n\t\t\t\telement = cubic_bezier(document, coords)\n\t\t\telif options['smooth'] == '0':\n\t\t\t\tif len(coords) == 4:\n\t\t\t\t\t# segment\n\t\t\t\t\telement = segment(document, coords)\n\t\t\t\telse:\n\t\t\t\t\t# polyline\n\t\t\t\t\telement = polyline(document, coords)\n\t\t\t\t\tstyle['fill'] = \"none\"\n\t\t\t\t\tstyle['stroke-linejoin'] = join_style[options['joinstyle']]\n\t\t\telse:\n\t\t\t\temit_warning(\"Unknown smooth type: %s. Falling back to smooth=0\" % options['smooth'])\n\t\t\t\telement = polyline(coords)\n\t\t\t\tstyle['stroke-linejoin'] = join_style[options['joinstyle']]\n\n\t\t\telements.append(element)\n\t\t\tif options['arrow'] in [FIRST, BOTH]:\n\t\t\t\tarrow = arrow_head(document, coords[2], coords[3], coords[0], coords[1], options['arrowshape'])\n\t\t\t\tarrow.setAttribute('fill', style['stroke'])\n\t\t\t\telements.append(arrow)\n\t\t\tif options['arrow'] in [LAST, BOTH]:\n\t\t\t\tarrow = arrow_head(document, coords[-4], coords[-3], coords[-2], coords[-1], options['arrowshape'])\n\t\t\t\tarrow.setAttribute('fill', style['stroke'])\n\t\t\t\telements.append(arrow)\n\n\t\telif itemtype == 'polygon':\n\t\t\tif options['smooth'] in ['1', 'bezier', 'true']:\n\t\t\t\telement = smoothpolygon(document, coords)\n\t\t\telif options['smooth'] == '0':\n\t\t\t\telement = polygon(document, coords)\n\t\t\telse:\n\t\t\t\temit_warning(\"Unknown smooth type: %s. Falling back to smooth=0\" % options['smooth'])\n\t\t\t\telement = polygon(document, coords)\n\n\t\t\telements.append(element)\n\n\t\t\tstyle['fill-rule'] = 'evenodd'\n\t\t\tstyle['stroke-linejoin'] = join_style[options['joinstyle']]\n\t\t\n\t\telif itemtype == 'oval':\n\t\t\telement = oval(document, coords)\n\t\t\telements.append(element)\n\n\t\telif itemtype == 'rectangle':\n\t\t\telement = rectangle(document, coords)\n\t\t\telements.append(element)\n\n\t\telif itemtype == 'arc':\n\t\t\telement = arc(document, coords, options['start'], options['extent'], options['style'])\n\t\t\tif options['style'] == ARC:\n\t\t\t\tstyle['fill'] = \"none\"\n\n\t\t\telements.append(element)\n\n\t\telif itemtype == 'text':\n\t\t\tstyle['stroke'] = '' # no stroke\n\t\t\t\n\t\t\t# setup geometry\n\t\t\txmin, ymin, xmax, ymax = canvas.bbox(item)\n\t\t\t\n\t\t\tx = coords[0]\n\n\t\t\t# set y at 'dominant-baseline'\n\t\t\ty = ymin + font_metrics(tk, options['font'], 'ascent') \n\t\t\t\n\t\t\telement = setattribs(\n\t\t\t\tdocument.createElement('text'),\n\t\t\t\tx = x, y = y \n\t\t\t)\n\t\t\telements.append(element)\n\n\t\t\telement.appendChild(document.createTextNode(\n\t\t\t\ttounicode(canvas.itemcget(item, 'text'))\n\t\t\t))\n\n\t\t\t# 2. Setup style\n\t\t\tactual = font_actual(tk, options['font'])\n\n\t\t\tstyle['fill'] = HTMLcolor(canvas, get('fill'))\n\t\t\tstyle[\"text-anchor\"] = text_anchor[options[\"anchor\"]]\n\t\t\tstyle['font-family'] = actual['family']\n\n\t\t\t# size\n\t\t\tsize = float(actual['size'])\n\t\t\tif size > 0: # size in points\n\t\t\t\tstyle['font-size'] = \"%spt\" % size\n\t\t\telse:        # size in pixels\n\t\t\t\tstyle['font-size'] = \"%s\" % (-size)\n\n\t\t\tstyle['font-style']  = font_style[actual['slant']]\n\t\t\tstyle['font-weight'] = font_weight[actual['weight']]\n\n\t\t\t# overstrike/underline\n\t\t\tif actual['overstrike'] and actual['underline']:\n\t\t\t\tstyle['text-decoration'] = 'underline line-through'\n\t\t\telif actual['overstrike']:\n\t\t\t\tstyle['text-decoration'] = 'line-through'\n\t\t\telif actual['underline']:\n\t\t\t\tstyle['text-decoration'] = 'underline'\n\n\n\t\tfor attr, value in style.items():\n\t\t\tif value != '': # create only nonempty attributes\n\t\t\t\telement.setAttribute(attr, str(value))\n\n\treturn elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SVGdocument():\n\t\"Create default SVG document\"\n\n\timport xml.dom.minidom\n\timplementation = xml.dom.minidom.getDOMImplementation()\n\tdoctype = implementation.createDocumentType(\n\t\t\"svg\", \"-//W3C//DTD SVG 1.1//EN\",\n\t\t\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\n\t)\n\tdocument= implementation.createDocument(None, \"svg\", doctype)\n\tdocument.documentElement.setAttribute(\n\t\t'xmlns', 'http://www.w3.org/2000/svg'\n\t)\n\treturn document", "response": "Create default SVG document"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef oval(document, coords):\n\t\"circle/ellipse\"\n\tx1, y1, x2, y2 = coords\n\n\t# circle\n\tif x2-x1 == y2-y1:\n\t\treturn setattribs(document.createElement('circle'),\n\t\t\tcx = (x1+x2)/2,\n\t\t\tcy = (y1+y2)/2,\n\t\t\tr  = abs(x2-x1)/2,\n\t\t)\n\t\n\t# ellipse\n\telse:\n\t\treturn setattribs(document.createElement('ellipse'),\n\t\t\tcx = (x1+x2)/2,\n\t\t\tcy = (y1+y2)/2,\n\t\t\trx = abs(x2-x1)/2,\n\t\t\try = abs(y2-y1)/2,\n\t\t)\n\t\n\treturn element", "response": "Oval a circle or ellipse."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arc(document, bounding_rect, start, extent, style):\n\t\"arc, pieslice (filled), arc with chord (filled)\"\n\t(x1, y1, x2, y2) = bounding_rect\n\timport math\n\t\n\tcx = (x1 + x2)/2.0\n\tcy = (y1 + y2)/2.0\n\n\trx = (x2 - x1)/2.0\n\try = (y2 - y1)/2.0\n\t\n\tstart  = math.radians(float(start))\n\textent = math.radians(float(extent))\n\n\t# from SVG spec:\n\t# http://www.w3.org/TR/SVG/implnote.html#ArcImplementationNotes\n\tx1 =  rx * math.cos(start) + cx\n\ty1 = -ry * math.sin(start) + cy # XXX: ry is negated here\n\n\tx2 =  rx * math.cos(start + extent) + cx\n\ty2 = -ry * math.sin(start + extent) + cy # XXX: ry is negated here\n\n\tif abs(extent) > math.pi:\n\t\tfa = 1\n\telse:\n\t\tfa = 0\n\n\tif extent > 0.0:\n\t\tfs = 0\n\telse:\n\t\tfs = 1\n\t\n\tpath = []\n\t# common: arc\n\tpath.append('M%s,%s' % (x1, y1))\n\tpath.append('A%s,%s 0 %d %d %s,%s' % (rx, ry, fa, fs, x2, y2))\n\t\n\tif style == ARC:\n\t\tpass\n\t\n\telif style == CHORD:\n\t\tpath.append('z')\n\n\telse: # default: pieslice\n\t\tpath.append('L%s,%s' % (cx, cy))\n\t\tpath.append('z')\n\n\treturn setattribs(document.createElement('path'), d = ''.join(path))", "response": "arc pieslice arc with chord ( filled"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HTMLcolor(canvas, color):\n\t\"returns Tk color in form '#rrggbb' or '#rgb'\"\n\tif color:\n\t\t# r, g, b \\in [0..2**16]\n\n\t\tr, g, b = [\"%02x\" % (c // 256) for c in canvas.winfo_rgb(color)]\n\n\t\tif (r[0] == r[1]) and (g[0] == g[1]) and (b[0] == b[1]):\n\t\t\t# shorter form #rgb\n\t\t\treturn \"#\" + r[0] + g[0] + b[0]\n\t\telse:\n\t\t\treturn \"#\" + r + g + b\n\telse:\n\t\treturn color", "response": "returns Tk color in form r g b or rgb"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arrow_head(document, x0, y0, x1, y1, arrowshape):\n\t\"make arrow head at (x1,y1), arrowshape is tuple (d1, d2, d3)\"\n\timport math\n\t \n\tdx = x1 - x0\n\tdy = y1 - y0\n\n\tpoly = document.createElement('polygon')\n\t\n\td = math.sqrt(dx*dx + dy*dy)\n\tif d == 0.0: # XXX: equal, no \"close enough\"\n\t\treturn poly\n\n\ttry:\n\t\td1, d2, d3 = list(map(float, arrowshape))\n\texcept ValueError:\n\t\td1, d2, d3 = map(float, arrowshape.split())\n\tP0 = (x0, y0)\n\tP1 = (x1, y1)\n\t\n\txa, ya = lerp(P1, P0, d1/d)\n\txb, yb = lerp(P1, P0, d2/d)\n\n\tt = d3/d\n\txc, yc = dx*t, dy*t\n\n\tpoints = [\n\t\tx1, y1,\n\t\txb - yc, yb + xc,\n\t\txa, ya,\n\t\txb + yc, yb - xc,\n\t]\n\tpoly.setAttribute('points', ' '.join(map(str, points)))\n\treturn poly", "response": "make arrow head at ( x1 y1 arrowshape is tuple ( d1 d2 d3 )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing dash pattern specified with string", "response": "def parse_dash(string, width):\n\t\"parse dash pattern specified with string\"\n\t\n\t# DashConvert from {tk-sources}/generic/tkCanvUtil.c\n\tw = max(1, int(width + 0.5))\n\n\tn = len(string)\n\tresult = []\n\tfor i, c in enumerate(string):\n\t\tif c == \" \" and len(result):\n\t\t\tresult[-1] += w + 1\n\t\telif c == \"_\":\n\t\t\tresult.append(8*w)\n\t\t\tresult.append(4*w)\n\t\telif c == \"-\":\n\t\t\tresult.append(6*w)\n\t\t\tresult.append(4*w)\n\t\telif c == \",\":\n\t\t\tresult.append(4*w)\n\t\t\tresult.append(4*w)\n\t\telif c == \".\":\n\t\t\tresult.append(2*w)\n\t\t\tresult.append(4*w)\n\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prof_altitude(pressure, p_coef=(-0.028389, -0.0493698, 0.485718, 0.278656,\n                                    -17.5703, 48.0926)):\n    \"\"\"\n    Return altitude for given pressure.\n\n    This function evaluates a polynomial at log10(pressure) values.\n\n    Parameters\n    ----------\n    pressure : array-like\n        pressure values [hPa].\n    p_coef : array-like\n        coefficients of the polynomial (default values are for the US\n        Standard Atmosphere).\n\n    Returns\n    -------\n    altitude : array-like\n        altitude values [km] (same shape than the pressure input array).\n\n    See Also\n    --------\n    prof_pressure : Returns pressure for\n        given altitude.\n    prof_temperature : Returns air temperature for\n        given altitude.\n\n    Notes\n    -----\n    Default coefficient values represent a 5th degree polynomial which had\n    been fitted to USSA data from 0-100 km. Accuracy is on the order of 1% for\n    0-100 km and 0.5% below 30 km. This function, with default values, may thus\n    produce bad results with pressure less than about 3e-4 hPa.\n\n    Examples\n    --------\n    >>> prof_altitude([1000, 800, 600])\n    array([ 0.1065092 ,  1.95627858,  4.2060627 ])\n\n    \"\"\"\n    pressure = np.asarray(pressure)\n    altitude = np.polyval(p_coef, np.log10(pressure.flatten()))\n    return altitude.reshape(pressure.shape)", "response": "Return altitude for given pressure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prof_pressure(altitude, z_coef=(1.94170e-9, -5.14580e-7, 4.57018e-5,\n                                    -1.55620e-3, -4.61994e-2, 2.99955)):\n    \"\"\"\n    Return pressure for given altitude.\n\n    This function evaluates a polynomial at altitudes values.\n\n    Parameters\n    ----------\n    altitude : array-like\n        altitude values [km].\n    z_coef : array-like\n        coefficients of the polynomial (default values are for the US\n        Standard Atmosphere).\n\n    Returns\n    -------\n    pressure : array-like\n        pressure values [hPa] (same shape than the altitude input array).\n\n    See Also\n    --------\n    prof_altitude : Returns altitude for\n        given pressure.\n    prof_temperature : Returns air temperature for\n        given altitude.\n\n    Notes\n    -----\n    Default coefficient values represent a 5th degree polynomial which had\n    been fitted to USA data from 0-100 km. Accuracy is on the order of 1% for\n    0-100 km and 0.5% below 30 km. This function, with default values, may thus\n    produce bad results with altitude > 100 km.\n\n    Examples\n    --------\n    >>> prof_pressure([0, 10, 20])\n    array([ 998.96437334,  264.658697  ,   55.28114631])\n\n    \"\"\"\n    altitude = np.asarray(altitude)\n    pressure = np.power(10, np.polyval(z_coef, altitude.flatten()))\n    return pressure.reshape(altitude.shape)", "response": "Return pressure for given altitude."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_references(model_name, references=None):\n    references = references or []\n    references.append(model_name)\n\n    ref = MODELS[model_name].get('reference')\n    if ref is not None:\n        _find_references(ref, references)\n\n    parent_models = [m for m in references]\n    parent_models.reverse()\n\n    return parent_models", "response": "Find all parent models that are referenced by model_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the grid specifications for a given model name.", "response": "def _get_model_info(model_name):\n    \"\"\"\n    Get the grid specifications for a given model.\n\n    Parameters\n    ----------\n    model_name : string\n        Name of the model. Supports multiple formats\n        (e.g., 'GEOS5', 'GEOS-5' or 'GEOS_5').\n\n    Returns\n    -------\n    specifications : dict\n        Grid specifications as a dictionary.\n\n    Raises\n    ------\n    ValueError\n        If the model is not supported (see `models`) or if the given\n        `model_name` corresponds to several entries in the list of\n        supported models.\n\n    \"\"\"\n    # trying to get as much as possible a valid model name from the given\n    # `model_name`, using regular expressions.\n    split_name = re.split(r'[\\-_\\s]', model_name.strip().upper())\n    sep_chars = ('', ' ', '-', '_')\n    gen_seps = itertools.combinations_with_replacement(\n        sep_chars, len(split_name) - 1\n    )\n    test_names = (\"\".join((n for n in itertools.chain(*list(zip(split_name,\n                                                           s + ('',))))))\n                  for s in gen_seps)\n    match_names = list([name for name in test_names if name\n                        in _get_supported_models()])\n\n    if not len(match_names):\n        raise ValueError(\"Model '{0}' is not supported\".format(model_name))\n    elif len(match_names) > 1:\n        raise ValueError(\"Multiple matched models for given model name '{0}'\"\n                         .format(model_name))\n\n    valid_model_name = match_names[0]\n    parent_models = _find_references(valid_model_name)\n\n    model_spec = dict()\n    for m in parent_models:\n        model_spec.update(MODELS[m])\n    model_spec.pop('reference')\n    model_spec['model_family'] = parent_models[0]\n    model_spec['model_name'] = valid_model_name\n\n    return model_spec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the list of files from a tar or zip archive.", "response": "def _get_archive_filelist(filename):\n    # type: (str) -> List[str]\n    \"\"\"Extract the list of files from a tar or zip archive.\n\n    Args:\n        filename: name of the archive\n\n    Returns:\n        Sorted list of files in the archive, excluding './'\n\n    Raises:\n        ValueError: when the file is neither a zip nor a tar archive\n        FileNotFoundError: when the provided file does not exist (for Python 3)\n        IOError: when the provided file does not exist (for Python 2)\n    \"\"\"\n    names = []  # type: List[str]\n    if tarfile.is_tarfile(filename):\n        with tarfile.open(filename) as tar_file:\n            names = sorted(tar_file.getnames())\n    elif zipfile.is_zipfile(filename):\n        with zipfile.ZipFile(filename) as zip_file:\n            names = sorted(zip_file.namelist())\n    else:\n        raise ValueError(\"Can not get filenames from '{!s}'. \"\n                         \"Not a tar or zip file\".format(filename))\n    if \"./\" in names:\n        names.remove(\"./\")\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _augment_book(self, uuid, event):\n        try:\n            if not isbnmeta:\n                self.log(\n                    \"No isbntools found! Install it to get full \"\n                    \"functionality!\",\n                    lvl=warn)\n                return\n\n            new_book = objectmodels['book'].find_one({'uuid': uuid})\n            try:\n                if len(new_book.isbn) != 0:\n\n                    self.log('Got a lookup candidate: ', new_book._fields)\n\n                    try:\n                        meta = isbnmeta(\n                            new_book.isbn,\n                            service=self.config.isbnservice\n                        )\n\n                        mapping = libraryfieldmapping[\n                            self.config.isbnservice\n                        ]\n\n                        new_meta = {}\n\n                        for key in meta.keys():\n                            if key in mapping:\n                                if isinstance(mapping[key], tuple):\n                                    name, conv = mapping[key]\n                                    try:\n                                        new_meta[name] = conv(meta[key])\n                                    except ValueError:\n                                        self.log(\n                                            'Bad value from lookup:',\n                                            name, conv, key\n                                        )\n                                else:\n                                    new_meta[mapping[key]] = meta[key]\n\n                        new_book.update(new_meta)\n                        new_book.save()\n\n                        self._notify_result(event, new_book)\n                        self.log(\"Book successfully augmented from \",\n                                 self.config.isbnservice)\n                    except Exception as e:\n                        self.log(\"Error during meta lookup: \", e, type(e),\n                                 new_book.isbn, lvl=error, exc=True)\n                        error_response = {\n                            'component': 'hfos.alert.manager',\n                            'action': 'notify',\n                            'data': {\n                                'type': 'error',\n                                'message': 'Could not look up metadata, sorry:' + str(e)\n                            }\n                        }\n                        self.log(event, event.client, pretty=True)\n                        self.fireEvent(send(event.client.uuid, error_response))\n\n            except Exception as e:\n                self.log(\"Error during book update.\", e, type(e),\n                         exc=True, lvl=error)\n\n        except Exception as e:\n            self.log(\"Book creation notification error: \", uuid, e, type(e),\n                     lvl=error, exc=True)", "response": "Augment the object with the contents of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the serial ports available on the system", "response": "def serial_ports():\n    \"\"\" Lists serial port names\n\n        :raises EnvironmentError:\n            On unsupported or unknown platforms\n        :returns:\n            A list of the serial ports available on the system\n\n        Courtesy: Thomas ( http://stackoverflow.com/questions/12090503\n        /listing-available-com-ports-with-python )\n    \"\"\"\n    if sys.platform.startswith('win'):\n        ports = ['COM%s' % (i + 1) for i in range(256)]\n    elif sys.platform.startswith('linux') or sys.platform.startswith('cygwin'):\n        # this excludes your current terminal \"/dev/tty\"\n        ports = glob.glob('/dev/tty[A-Za-z]*')\n    elif sys.platform.startswith('darwin'):\n        ports = glob.glob('/dev/tty.*')\n    else:\n        raise EnvironmentError('Unsupported platform')\n\n    result = []\n    for port in ports:\n        try:\n            s = serial.Serial(port)\n            s.close()\n            result.append(port)\n        except (OSError, serial.SerialException):\n            pass\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef opened(self, *args):\n        self._serial_open = True\n\n        self.log(\"Opened: \", args, lvl=debug)\n        self._send_command(b'l,1')  # Saying hello, shortly\n        self.log(\"Turning off engine, pump and neutralizing rudder\")\n        self._send_command(b'v')\n        self._handle_servo(self._machine_channel, 0)\n        self._handle_servo(self._rudder_channel, 127)\n        self._set_digital_pin(self._pump_channel, 0)\n        # self._send_command(b'h')\n        self._send_command(b'l,0')\n        self._send_command(b'm,HFOS Control')", "response": "Initiates communication with the remote controlled device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_machinerequest(self, event):\n        self.log(\"Updating new machine power: \", event.controlvalue)\n        self._handle_servo(self._machine_channel, event.controlvalue)", "response": "Updates the current machine power."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_rudderrequest(self, event):\n        self.log(\"Updating new rudder angle: \", event.controlvalue)\n        self._handle_servo(self._rudder_channel, event.controlvalue)", "response": "Called when a new rudder angle is set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_pumprequest(self, event):\n        self.log(\"Updating pump status: \", event.controlvalue)\n        self._set_digital_pin(self._pump_channel, event.controlvalue)", "response": "Called when a Pump request is received."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef provisionList(items, database_name, overwrite=False, clear=False, skip_user_check=False):\n\n    log('Provisioning', items, database_name, lvl=debug)\n    system_user = None\n\n    def get_system_user():\n        \"\"\"Retrieves the node local system user\"\"\"\n\n        user = objectmodels['user'].find_one({'name': 'System'})\n\n        try:\n            log('System user uuid: ', user.uuid, lvl=verbose)\n            return user.uuid\n        except AttributeError as e:\n            log('No system user found:', e, lvl=warn)\n            log('Please install the user provision to setup a system user or check your database configuration',\n                lvl=error)\n            return False\n\n    # TODO: Do not check this on specific objects but on the model (i.e. once)\n    def needs_owner(obj):\n        \"\"\"Determines whether a basic object has an ownership field\"\"\"\n        for privilege in obj._fields.get('perms', None):\n            if 'owner' in obj._fields['perms'][privilege]:\n                return True\n\n        return False\n\n    import pymongo\n    from hfos.database import objectmodels, dbhost, dbport, dbname\n\n    database_object = objectmodels[database_name]\n\n    log(dbhost, dbname)\n    # TODO: Fix this to make use of the dbhost\n\n    client = pymongo.MongoClient(dbhost, dbport)\n    db = client[dbname]\n\n    if not skip_user_check:\n        system_user = get_system_user()\n\n        if not system_user:\n            return\n    else:\n        # TODO: Evaluate what to do instead of using a hardcoded UUID\n        # This is usually only here for provisioning the system user\n        # One way to avoid this, is to create (instead of provision)\n        # this one upon system installation.\n        system_user = '0ba87daa-d315-462e-9f2e-6091d768fd36'\n\n    col_name = database_object.collection_name()\n\n    if clear is True:\n        log(\"Clearing collection for\", col_name, lvl=warn)\n        db.drop_collection(col_name)\n    counter = 0\n\n    for no, item in enumerate(items):\n        new_object = None\n        item_uuid = item['uuid']\n        log(\"Validating object (%i/%i):\" % (no + 1, len(items)), item_uuid, lvl=debug)\n\n        if database_object.count({'uuid': item_uuid}) > 0:\n            log('Object already present', lvl=warn)\n            if overwrite is False:\n                log(\"Not updating item\", item, lvl=warn)\n            else:\n                log(\"Overwriting item: \", item_uuid, lvl=warn)\n                new_object = database_object.find_one({'uuid': item_uuid})\n                new_object._fields.update(item)\n        else:\n            new_object = database_object(item)\n\n        if new_object is not None:\n            try:\n                if needs_owner(new_object):\n                    if not hasattr(new_object, 'owner'):\n                        log('Adding system owner to object.', lvl=verbose)\n                        new_object.owner = system_user\n            except Exception as e:\n                log('Error during ownership test:', e, type(e),\n                    exc=True, lvl=error)\n            try:\n                new_object.validate()\n                new_object.save()\n                counter += 1\n            except ValidationError as e:\n                raise ValidationError(\n                    \"Could not provision object: \" + str(item_uuid), e)\n\n    log(\"Provisioned %i out of %i items successfully.\" % (counter, len(items)))", "response": "Provisions a list of items according to their schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a default extension for a given schema and form", "response": "def DefaultExtension(schema_obj, form_obj, schemata=None):\n    \"\"\"Create a default field\"\"\"\n\n    if schemata is None:\n        schemata = ['systemconfig', 'profile', 'client']\n\n    DefaultExtends = {\n        'schema': {\n            \"properties/modules\": [\n                schema_obj\n            ]\n        },\n        'form': {\n            'modules': {\n                'items/': form_obj\n            }\n        }\n    }\n\n    output = {}\n\n    for schema in schemata:\n        output[schema] = DefaultExtends\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copytree(root_src_dir, root_dst_dir, hardlink=True):\n\n    for src_dir, dirs, files in os.walk(root_src_dir):\n        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n        if not os.path.exists(dst_dir):\n            os.makedirs(dst_dir)\n        for file_ in files:\n            src_file = os.path.join(src_dir, file_)\n            dst_file = os.path.join(dst_dir, file_)\n            try:\n                if os.path.exists(dst_file):\n                    if hardlink:\n                        hfoslog('Removing frontend link:', dst_file,\n                                emitter='BUILDER', lvl=verbose)\n                        os.remove(dst_file)\n                    else:\n                        hfoslog('Overwriting frontend file:', dst_file,\n                                emitter='BUILDER', lvl=verbose)\n\n                hfoslog('Hardlinking ', src_file, dst_dir, emitter='BUILDER',\n                        lvl=verbose)\n\n                if hardlink:\n                    os.link(src_file, dst_file)\n                else:\n                    copy(src_file, dst_dir)\n            except PermissionError as e:\n                hfoslog(\n                    \" No permission to remove/create target %s for \"\n                    \"frontend:\" % ('link' if hardlink else 'copy'),\n                    dst_dir, e, emitter='BUILDER', lvl=error)\n            except Exception as e:\n                hfoslog(\"Error during\", 'link' if hardlink else 'copy',\n                        \"creation:\", type(e), e, emitter='BUILDER',\n                        lvl=error)\n\n            hfoslog('Done linking', root_dst_dir, emitter='BUILDER',\n                    lvl=verbose)", "response": "Copies a whole directory tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and installs the frontend", "response": "def install_frontend(instance='default', forcereload=False, forcerebuild=False,\n                     forcecopy=True, install=True, development=False, build_type='dist'):\n    \"\"\"Builds and installs the frontend\"\"\"\n\n    hfoslog(\"Updating frontend components\", emitter='BUILDER')\n    components = {}\n    loadable_components = {}\n    # TODO: Fix this up, it is probably not a sane way to get at the real root\n    if development:\n        frontendroot = os.path.abspath(os.path.dirname(os.path.realpath(\n            __file__)) + \"../../../frontend\")\n    else:\n        frontendroot = '/opt/hfos/frontend'\n    frontendtarget = os.path.join('/var/lib/hfos', instance, 'frontend')\n\n    if install:\n        cmdline = [\"npm\", \"install\"]\n\n        hfoslog(\"Running\", cmdline, lvl=verbose,\n                emitter='BUILDER')\n        npminstall = Popen(cmdline, cwd=frontendroot)\n        out, err = npminstall.communicate()\n\n        npminstall.wait()\n\n        hfoslog(\"Frontend dependency installing done: \", out,\n                err, lvl=debug, emitter='BUILDER')\n\n    if True:  # try:\n        from pkg_resources import iter_entry_points\n\n        entry_point_tuple = (\n            iter_entry_points(group='hfos.base', name=None),\n            iter_entry_points(group='hfos.sails', name=None),\n            iter_entry_points(group='hfos.components', name=None)\n        )\n\n        for iterator in entry_point_tuple:\n            for entry_point in iterator:\n                try:\n                    name = entry_point.name\n                    location = entry_point.dist.location\n                    loaded = entry_point.load()\n\n                    hfoslog(\"Entry point: \", entry_point,\n                            name,\n                            entry_point.resolve().__module__, lvl=debug,\n                            emitter='BUILDER')\n                    component_name = entry_point.resolve().__module__.split('.')[1]\n\n                    hfoslog(\"Loaded: \", loaded, lvl=verbose, emitter='BUILDER')\n                    comp = {\n                        'location': location,\n                        'version': str(entry_point.dist.parsed_version),\n                        'description': loaded.__doc__\n                    }\n\n                    frontend = os.path.join(location, 'frontend')\n                    hfoslog(\"Checking component frontend parts: \",\n                            frontend, lvl=verbose, emitter='BUILDER')\n                    if os.path.isdir(\n                            frontend) and frontend != frontendroot:\n                        comp['frontend'] = frontend\n                    else:\n                        hfoslog(\"Component without frontend \"\n                                \"directory:\", comp, lvl=debug,\n                                emitter='BUILDER')\n\n                    components[component_name] = comp\n                    loadable_components[component_name] = loaded\n\n                    hfoslog(\"Loaded component:\", comp, lvl=verbose,\n                            emitter='BUILDER')\n\n                except Exception as e:\n                    hfoslog(\"Could not inspect entrypoint: \", e,\n                            type(e), entry_point, iterator, lvl=error,\n                            exc=True, emitter='BUILDER')\n\n    # except Exception as e:\n    #    hfoslog(\"Error: \", e, type(e), lvl=error, exc=True, emitter='BUILDER')\n    #    return\n\n    hfoslog('Components after lookup:', sorted(list(components.keys())), emitter='BUILDER')\n\n    def _update_frontends(install=True):\n        hfoslog(\"Checking unique frontend locations: \",\n                loadable_components, lvl=debug, emitter='BUILDER')\n\n        importlines = []\n        modules = []\n\n        for name, component in components.items():\n            if 'frontend' in component:\n                origin = component['frontend']\n\n                target = os.path.join(frontendroot, 'src', 'components',\n                                      name)\n                target = os.path.normpath(target)\n\n                if install:\n                    reqfile = os.path.join(origin, 'requirements.txt')\n\n                    if os.path.exists(reqfile):\n                        # TODO: Speed this up by collecting deps first then doing one single install call\n                        hfoslog(\"Installing package dependencies\", lvl=debug,\n                                emitter='BUILDER')\n                        with open(reqfile, 'r') as f:\n                            cmdline = [\"npm\", \"install\"]\n                            for line in f.readlines():\n                                cmdline.append(line.replace(\"\\n\", \"\"))\n\n                            hfoslog(\"Running\", cmdline, lvl=verbose,\n                                    emitter='BUILDER')\n                            npminstall = Popen(cmdline, cwd=frontendroot)\n                            out, err = npminstall.communicate()\n\n                            npminstall.wait()\n\n                            hfoslog(\"Frontend installing done: \", out,\n                                    err, lvl=debug, emitter='BUILDER')\n\n                # if target in ('/', '/boot', '/usr', '/home', '/root',\n                # '/var'):\n                #    hfoslog(\"Unsafe frontend deletion target path, \"\n                #            \"NOT proceeding! \", target, lvl=critical,\n                #            emitter='BUILDER')\n\n                hfoslog(\"Copying:\", origin, target, lvl=debug,\n                        emitter='BUILDER')\n\n                copytree(origin, target)\n\n                for modulefilename in glob(target + '/*.module.js'):\n                    modulename = os.path.basename(modulefilename).split(\n                        \".module.js\")[0]\n                    line = u\"import {s} from './components/{p}/{\" \\\n                           u\"s}.module';\\nmodules.push({s});\\n\".format(\n                        s=modulename, p=name)\n                    if modulename not in modules:\n                        importlines += line\n                        modules.append(modulename)\n            else:\n                hfoslog(\"Module without frontend:\", name, component,\n                        lvl=debug, emitter='BUILDER')\n\n        with open(os.path.join(frontendroot, 'src', 'main.tpl.js'),\n                  \"r\") as f:\n            main = \"\".join(f.readlines())\n\n        parts = main.split(\"/* COMPONENT SECTION */\")\n        if len(parts) != 3:\n            hfoslog(\"Frontend loader seems damaged! Please check!\",\n                    lvl=critical, emitter='BUILDER')\n            return\n\n        try:\n            with open(os.path.join(frontendroot, 'src', 'main.js'),\n                      \"w\") as f:\n                f.write(parts[0])\n                f.write(\"/* COMPONENT SECTION:BEGIN */\\n\")\n                for line in importlines:\n                    f.write(line)\n                f.write(\"/* COMPONENT SECTION:END */\\n\")\n                f.write(parts[2])\n        except Exception as e:\n            hfoslog(\"Error during frontend package info writing. Check \"\n                    \"permissions! \", e, lvl=error, emitter='BUILDER')\n\n    def _rebuild_frontend():\n        hfoslog(\"Starting frontend build.\", lvl=warn, emitter='BUILDER')\n\n        npmbuild = Popen([\"npm\", \"run\", build_type], cwd=frontendroot)\n        out, err = npmbuild.communicate()\n        try:\n            npmbuild.wait()\n        except Exception as e:\n            hfoslog(\"Error during frontend build\", e, type(e),\n                    exc=True, lvl=error, emitter='BUILDER')\n            return\n\n        hfoslog(\"Frontend build done: \", out, err, lvl=debug, emitter='BUILDER')\n\n        copytree(os.path.join(frontendroot, build_type),\n                 frontendtarget, hardlink=False)\n        copytree(os.path.join(frontendroot, 'assets'),\n                 os.path.join(frontendtarget, 'assets'),\n                 hardlink=False)\n\n        hfoslog(\"Frontend deployed\", emitter='BUILDER')\n\n    hfoslog(\"Checking component frontend bits in \", frontendroot,\n            lvl=verbose, emitter='BUILDER')\n\n    _update_frontends(install=install)\n    if forcerebuild:\n        _rebuild_frontend()\n\n    hfoslog(\"Done: Install Frontend\", emitter='BUILDER')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(ctx, componentname):\n    col = ctx.obj['col']\n\n    if col.count({'name': componentname}) > 1:\n        log('More than one component configuration of this name! Try '\n            'one of the uuids as argument. Get a list with \"config '\n            'list\"')\n        return\n\n    log('Deleting component configuration', componentname,\n        emitter='MANAGE')\n\n    configuration = col.find_one({'name': componentname})\n\n    if configuration is None:\n        configuration = col.find_one({'uuid': componentname})\n\n    if configuration is None:\n        log('Component configuration not found:', componentname,\n            emitter='MANAGE')\n        return\n\n    configuration.delete()\n    log('Done')", "response": "Delete an existing component configuration. This will trigger\n    creation of its default configuration upon next restart."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show(ctx, component):\n\n    col = ctx.obj['col']\n\n    if col.count({'name': component}) > 1:\n        log('More than one component configuration of this name! Try '\n            'one of the uuids as argument. Get a list with \"config '\n            'list\"')\n        return\n\n    if component is None:\n        configurations = col.find()\n        for configuration in configurations:\n            log(\"%-15s : %s\" % (configuration.name,\n                                configuration.uuid),\n                emitter='MANAGE')\n    else:\n        configuration = col.find_one({'name': component})\n\n        if configuration is None:\n            configuration = col.find_one({'uuid': component})\n\n        if configuration is None:\n            log('No component with that name or uuid found.')\n            return\n\n        print(json.dumps(configuration.serializablefields(), indent=4))", "response": "Show the stored active configuration of a component."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nseparating a string into a list of two lists.", "response": "def separate_string(string):\n    \"\"\"\n    >>> separate_string(\"test <2>\")\n    (['test ', ''], ['2'])\n    \"\"\"\n    string_list = regex.split(r'<(?![!=])', regex.sub(r'>', '<', string))\n    return string_list[::2], string_list[1::2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef overlapping(start1, end1, start2, end2):\n    return not ((start1 <= start2 and start1 <= end2 and end1 <= end2 and end1 <= start2) or\n                (start1 >= start2 and start1 >= end2 and end1 >= end2 and end1 >= start2))", "response": "Returns True if start1 < = end1 and start1 < = end2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_lower_overlapping(current, higher):\n    for (match, h_start, h_end) in higher:\n        overlaps = list(overlapping_at(h_start, h_end, current))\n        for overlap in overlaps:\n            del current[overlap]\n        if len(overlaps) > 0:\n            # Keeps order in place\n            current.insert(overlaps[0], (match, h_start, h_end))\n        else:\n            current.append((match, h_start, h_end))\n\n    return current", "response": "Remove all lower overlapping entries from the current list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stdin_read(self, data):\n\n        data = data.strip().decode(\"utf-8\")\n        self.log(\"Incoming:\", data, lvl=verbose)\n\n        if len(data) == 0:\n            self.log('Use /help to get a list of enabled cli hooks')\n            return\n\n        if data[0] == \"/\":\n            cmd = data[1:]\n            args = []\n            if ' ' in cmd:\n                cmd, args = cmd.split(' ', maxsplit=1)\n                args = args.split(' ')\n            if cmd in self.hooks:\n                self.log('Firing hooked event:', cmd, args, lvl=debug)\n                self.fireEvent(self.hooks[cmd](*args))\n            # TODO: Move these out, so we get a simple logic here\n            elif cmd == 'frontend':\n                self.log(\"Sending %s frontend rebuild event\" %\n                         (\"(forced)\" if 'force' in args else ''))\n                self.fireEvent(\n                    frontendbuildrequest(force='force' in args,\n                                         install='install' in args),\n                    \"setup\")\n            elif cmd == 'backend':\n                self.log(\"Sending backend reload event\")\n                self.fireEvent(componentupdaterequest(force=False), \"setup\")\n            else:\n                self.log('Unknown Command:', cmd, '. Use /help to get a list of enabled '\n                                                  'cli hooks')", "response": "This is the event handler for reading from stdin. This is the event handler for reading from the ISO - 8601 file. This is triggered each time stdin has data that it has read. This is triggered each time stdin has data that it has read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a new command line interface event hook as command", "response": "def register_event(self, event):\n        \"\"\"Registers a new command line interface event hook as command\"\"\"\n\n        self.log('Registering event hook:', event.cmd, event.thing,\n                 pretty=True, lvl=verbose)\n        self.hooks[event.cmd] = event.thing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_user_events():\n\n    global AuthorizedEvents\n    global AnonymousEvents\n\n    def inheritors(klass):\n        \"\"\"Find inheritors of a specified object class\"\"\"\n\n        subclasses = {}\n        subclasses_set = set()\n        work = [klass]\n        while work:\n            parent = work.pop()\n            for child in parent.__subclasses__():\n                if child not in subclasses_set:\n                    # pprint(child.__dict__)\n                    name = child.__module__ + \".\" + child.__name__\n                    if name.startswith('hfos'):\n\n                        subclasses_set.add(child)\n                        event = {\n                            'event': child,\n                            'name': name,\n                            'doc': child.__doc__,\n                            'args': []\n                        }\n\n                        if child.__module__ in subclasses:\n                            subclasses[child.__module__][\n                                child.__name__] = event\n                        else:\n                            subclasses[child.__module__] = {\n                                child.__name__: event\n                            }\n                    work.append(child)\n        return subclasses\n\n    # TODO: Change event system again, to catch authorized (i.e. \"user\") as\n    # well as normal events, so they can be processed by Automat\n\n    # NormalEvents = inheritors(Event)\n    AuthorizedEvents = inheritors(authorizedevent)\n    AnonymousEvents = inheritors(anonymousevent)", "response": "Populate the internal state of the internal state of the authorized and anonymous events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef db(ctx):\n\n    from hfos import database\n    database.initialize(ctx.obj['dbhost'], ctx.obj['dbname'])\n    ctx.obj['db'] = database", "response": "A context manager for managing database management operations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear an entire database collection irrevocably. Use with caution!", "response": "def clear(ctx, schema):\n    \"\"\"Clears an entire database collection irrevocably. Use with caution!\"\"\"\n\n    response = _ask('Are you sure you want to delete the collection \"%s\"' % (\n        schema), default='N', data_type='bool')\n    if response is True:\n        host, port = ctx.obj['dbhost'].split(':')\n\n        client = pymongo.MongoClient(host=host, port=int(port))\n        database = client[ctx.obj['dbname']]\n\n        log(\"Clearing collection for\", schema, lvl=warn,\n            emitter='MANAGE')\n        result = database.drop_collection(schema)\n        if not result['ok']:\n            log(\"Could not drop collection:\", lvl=error)\n            log(result, pretty=True, lvl=error)\n        else:\n            log(\"Done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprovisions a basic system configuration", "response": "def provision_system_config(items, database_name, overwrite=False, clear=False, skip_user_check=False):\n    \"\"\"Provision a basic system configuration\"\"\"\n\n    from hfos.provisions.base import provisionList\n    from hfos.database import objectmodels\n\n    default_system_config_count = objectmodels['systemconfig'].count({\n        'name': 'Default System Configuration'})\n\n    if default_system_config_count == 0 or (clear or overwrite):\n        provisionList([SystemConfiguration], 'systemconfig', overwrite, clear, skip_user_check)\n        hfoslog('Provisioning: System: Done.', emitter='PROVISIONS')\n    else:\n        hfoslog('Default system configuration already present.', lvl=warn,\n                emitter='PROVISIONS')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ICALImporter(ctx, filename, all, owner, calendar, create_calendar, clear_calendar, dry, execfilter):\n\n    log('iCal importer running')\n\n    objectmodels = ctx.obj['db'].objectmodels\n\n    if objectmodels['user'].count({'name': owner}) > 0:\n        owner_object = objectmodels['user'].find_one({'name': owner})\n    elif objectmodels['user'].count({'uuid': owner}) > 0:\n        owner_object = objectmodels['user'].find_one({'uuid': owner})\n    else:\n        log('User unknown. Specify either uuid or name.', lvl=warn)\n        return\n\n    log('Found user')\n\n    if objectmodels['calendar'].count({'name': calendar}) > 0:\n        calendar = objectmodels['calendar'].find_one({'name': calendar})\n    elif objectmodels['calendar'].count({'uuid': owner}) > 0:\n        calendar = objectmodels['calendar'].find_one({'uuid': calendar})\n    elif create_calendar:\n        calendar = objectmodels['calendar']({\n            'uuid': std_uuid(),\n            'name': calendar\n        })\n    else:\n        log('Calendar unknown and no --create-calendar specified. Specify either uuid or name of an existing calendar.',\n            lvl=warn)\n        return\n\n    log('Found calendar')\n\n    if clear_calendar is True:\n        log('Clearing calendar events')\n        for item in objectmodels['event'].find({'calendar': calendar.uuid}):\n            item.delete()\n\n    with open(filename, 'rb') as file_object:\n        caldata = Calendar.from_ical(file_object.read())\n\n    keys = {\n        'class': 'str',\n        'created': 'dt',\n        'description': 'str',\n        'dtstart': 'dt',\n        'dtend': 'dt',\n        'timestamp': 'dt',\n        'modified': 'dt',\n        'location': 'str',\n        'status': 'str',\n        'summary': 'str',\n        'uid': 'str'\n    }\n    mapping = {\n        'description': 'summary',\n        'summary': 'name'\n    }\n\n    imports = []\n\n    def ical_import_filter(original, logfacilty):\n        log('Passthrough filter')\n        return original\n\n    if execfilter is not None:\n        import os\n\n        textFilePath = os.path.abspath(os.path.join(os.path.curdir, execfilter))\n        textFileFolder = os.path.dirname(textFilePath)\n\n        from importlib.machinery import SourceFileLoader\n\n        filter_module = SourceFileLoader(\"importfilter\", textFilePath).load_module()\n        ical_import_filter = filter_module.ical_import_filter\n\n    for event in caldata.walk():\n        if event.name == 'VEVENT':\n            log(event, lvl=verbose, pretty=True)\n            initializer = {\n                'uuid': std_uuid(),\n                'calendar': calendar.uuid,\n            }\n\n            for item in keys:\n                thing = event.get(item, None)\n                if thing is None:\n                    thing = 'NO-' + item\n                else:\n                    if keys[item] == 'str':\n                        thing = str(thing)\n                    else:\n                        thing = parser.parse(str(thing.dt))\n                        thing = thing.isoformat()\n\n                if item in mapping:\n                    item_assignment = mapping[item]\n                else:\n                    item_assignment = item\n\n                initializer[item_assignment] = thing\n            new_event = objectmodels['event'](initializer)\n\n            new_event = ical_import_filter(new_event, log)\n\n            imports.append(new_event)\n\n            log(new_event, lvl=debug)\n\n    for ev in imports:\n        log(ev.summary)\n\n    if not dry:\n        log('Bulk creating events')\n        objectmodels['event'].bulk_create(imports)\n\n        calendar.save()\n    else:\n        log('Dry run - nothing stored.', lvl=warn)", "response": "ICAL Importer for ICAL files"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a migration data structure for a specified schema.", "response": "def make_migrations(schema=None):\n    \"\"\"Create migration data for a specified schema\"\"\"\n\n    entrypoints = {}\n    old = {}\n\n    def apply_migrations(migrations, new_model):\n        \"\"\"Apply migration data to compile an up to date model\"\"\"\n\n        def get_path(raw_path):\n            \"\"\"Get local path of schema definition\"\"\"\n\n            print(\"RAW PATH:\", raw_path, type(raw_path))\n            path = []\n            for item in raw_path.split(\"[\"):\n                print(item)\n                item = item.rstrip(\"]\")\n                item = item.replace('\"', '')\n                item = item.replace(\"'\", '')\n                try:\n                    item = int(item)\n                except ValueError:\n                    pass\n                path.append(item)\n            path.remove('root')\n            print(\"PATH:\", path)\n            return path\n\n        def apply_entry(changetype, change, result):\n            \"\"\"Upgrade with a single migration\"\"\"\n\n            def apply_removes(removes, result):\n                \"\"\"Delete removed fields\"\"\"\n\n                for remove in removes:\n                    path = get_path(remove)\n                    amount = dpath.util.delete(result, path)\n                    assert amount == 1\n                return result\n\n            def apply_additions(additions, result):\n                \"\"\"Add newly added fields\"\"\"\n\n                for addition in additions:\n                    path = get_path(addition)\n                    entry = additions[addition]\n                    hfoslog('Adding:', entry, 'at', path)\n                    dpath.util.new(result, path, entry)\n                return result\n\n            if changetype == 'type_changes':\n                hfoslog('Creating new object')\n                result = change['root']['new_value']\n                return result\n\n            if changetype == 'dictionary_item_added':\n                hfoslog('Adding items')\n                result = apply_additions(change, result)\n            elif changetype == 'dictionary_item_removed':\n                hfoslog('Removing items')\n                result = apply_removes(change, result)\n            elif changetype == 'values_changed':\n                hfoslog(\"Changing items' types\")\n                for item in change:\n                    path = get_path(item)\n                    hfoslog('Changing', path, 'from',\n                            change[item]['old_value'], ' to',\n                            change[item]['new_value'])\n                    assert dpath.util.get(result, path) == change[item][\n                        'old_value']\n                    amount = dpath.util.set(result, path, change[item][\n                        'new_value'])\n                    assert amount == 1\n\n            return result\n\n        def get_renames(migrations):\n            \"\"\"Check migrations for renamed fields\"\"\"\n\n            hfoslog('Checking for rename operations:')\n            pprint(migrations)\n            for entry in migrations:\n\n                added = entry.get('dictionary_item_added', None)\n                removed = entry.get('dictionary_item_removed', None)\n\n            renames = []\n\n            if added and removed:\n                for addition in added:\n                    path = get_path(addition)\n                    for removal in removed:\n                        removed_path = get_path(removal)\n                        if path[:-1] == removed_path[:-1]:\n                            hfoslog('Possible rename detected:', removal, '->',\n                                    addition)\n                            renames.append((removed_path, path))\n            return renames\n\n        result = {}\n        for no, migration in enumerate(migrations):\n            hfoslog('Migrating', no)\n            hfoslog('Migration:', migration, lvl=debug)\n            renamed = get_renames(migrations)\n\n            for entry in migration:\n                result = apply_entry(entry, migration[entry], result)\n\n        pprint(result)\n        return result\n\n    def write_migration(schema, counter, path, previous, current):\n        \"\"\"Write out complete migration data\"\"\"\n\n        filename = \"%s_%04i.json\" % (schema, counter)\n        migration = DeepDiff(previous, current, verbose_level=2).json\n        if migration == \"{}\":\n            hfoslog('Nothing changed - no new migration data.', lvl=warn)\n            return\n\n        print('Writing migration: ', os.path.join(path, filename))\n        pprint(migration)\n\n        with open(os.path.join(path, filename), 'w') as f:\n            f.write(migration)\n\n    for schema_entrypoint in iter_entry_points(group='hfos.schemata',\n                                               name=None):\n        try:\n            hfoslog(\"Schemata found: \", schema_entrypoint.name, lvl=debug,\n                    emitter='DB')\n            if schema is not None and schema_entrypoint.name != schema:\n                continue\n\n            entrypoints[schema_entrypoint.name] = schema_entrypoint\n            pprint(schema_entrypoint.dist.location)\n            schema_top = schema_entrypoint.dist.location\n            schema_migrations = schema_entrypoint.module_name.replace(\n                'schemata', 'migrations').replace('.', '/')\n            path = os.path.join(schema_top, schema_migrations)\n            new_model = schema_entrypoint.load()['schema']\n\n            migrations = []\n\n            try:\n                for file in sorted(os.listdir(path)):\n                    if not file.endswith('.json'):\n                        continue\n                    fullpath = os.path.join(path, file)\n                    hfoslog('Importing migration', fullpath)\n                    with open(fullpath, 'r') as f:\n                        migration = DeepDiff.from_json(f.read())\n                    migrations.append(migration)\n                    hfoslog('Successfully imported')\n\n                if len(migrations) == 0:\n                    raise ImportError\n                pprint(migrations)\n                model = apply_migrations(migrations, new_model)\n                write_migration(schema, len(migrations) + 1, path, model,\n                                new_model)\n            except ImportError as e:\n                hfoslog('No previous migrations for', schema, e,\n                        type(e), exc=True)\n\n            if len(migrations) == 0:\n                write_migration(schema, 1, path, None, new_model)\n\n        except (ImportError, DistributionNotFound) as e:\n            hfoslog(\"Problematic schema: \", e, type(e),\n                    schema_entrypoint.name, exc=True, lvl=warn,\n                    emitter='SCHEMATA')\n\n    hfoslog(\"Found schemata: \", sorted(entrypoints.keys()), lvl=debug,\n            emitter='SCHEMATA')\n\n    pprint(entrypoints)\n\n    def make_single_migration(old, new):\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef userlogin(self, event):\n\n        try:\n            user_uuid = event.useruuid\n            user = objectmodels['user'].find_one({'uuid': user_uuid})\n\n            if user_uuid not in self.lastlogs:\n                self.log('Setting up lastlog for a new user.', lvl=debug)\n                lastlog = objectmodels['chatlastlog']({\n                    'owner': user_uuid,\n                    'uuid': std_uuid(),\n                    'channels': {}\n                })\n                lastlog.save()\n                self.lastlogs[user_uuid] = lastlog\n\n            self.users[user_uuid] = user\n            self.user_attention[user_uuid] = None\n            self._send_status(user_uuid, event.clientuuid)\n        except Exception as e:\n            self.log('Error during chat setup of user:', e, type(e), exc=True)", "response": "Provides the newly authenticated user with a backlog and general\n        channel status information"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join(self, event):\n\n        try:\n            channel_uuid = event.data\n            user_uuid = event.user.uuid\n\n            if channel_uuid in self.chat_channels:\n                self.log('User joins a known channel', lvl=debug)\n                if user_uuid in self.chat_channels[channel_uuid].users:\n                    self.log('User already joined', lvl=warn)\n                else:\n                    self.chat_channels[channel_uuid].users.append(user_uuid)\n                    self.chat_channels[channel_uuid].save()\n                    packet = {\n                        'component': 'hfos.chat.host',\n                        'action': 'join',\n                        'data': channel_uuid\n                    }\n                    self.fireEvent(send(event.client.uuid, packet))\n            else:\n                self.log('Request to join unavailable channel', lvl=warn)\n        except Exception as e:\n            self.log('Join error:', e, type(e), exc=True, lvl=error)", "response": "This method is called when a chat message is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsay a chat message", "response": "def say(self, event):\n        \"\"\"Chat event handler for incoming events\n        :param event: say-event with incoming chat message\n        \"\"\"\n\n        try:\n            userid = event.user.uuid\n            recipient = self._get_recipient(event)\n            content = self._get_content(event)\n\n            message = objectmodels['chatmessage']({\n                'timestamp': time(),\n                'recipient': recipient,\n                'sender': userid,\n                'content': content,\n                'uuid': std_uuid()\n            })\n\n            message.save()\n\n            chat_packet = {\n                'component': 'hfos.chat.host',\n                'action': 'say',\n                'data': message.serializablefields()\n            }\n\n            if recipient in self.chat_channels:\n                for useruuid in self.users:\n                    if useruuid in self.chat_channels[recipient].users:\n                        self.log('User in channel', lvl=debug)\n                        self.update_lastlog(useruuid, recipient)\n\n                        self.log('Sending message', lvl=debug)\n                        self.fireEvent(send(useruuid, chat_packet,\n                                            sendtype='user'))\n\n        except Exception as e:\n            self.log(\"Error: '%s' %s\" % (e, type(e)), exc=True, lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding and installs the complete HFOS documentation.", "response": "def install_docs(instance, clear_target):\n    \"\"\"Builds and installs the complete HFOS documentation.\"\"\"\n\n    _check_root()\n\n    def make_docs():\n        \"\"\"Trigger a Sphinx make command to build the documentation.\"\"\"\n        log(\"Generating HTML documentation\")\n\n        try:\n            build = Popen(\n                [\n                    'make',\n                    'html'\n                ],\n                cwd='docs/'\n            )\n\n            build.wait()\n        except Exception as e:\n            log(\"Problem during documentation building: \", e, type(e),\n                exc=True, lvl=error)\n            return False\n        return True\n\n    make_docs()\n\n    # If these need changes, make sure they are watertight and don't remove\n    # wanted stuff!\n    target = os.path.join('/var/lib/hfos', instance, 'frontend/docs')\n    source = 'docs/build/html'\n\n    log(\"Updating documentation directory:\", target)\n\n    if not os.path.exists(os.path.join(os.path.curdir, source)):\n        log(\n            \"Documentation not existing yet. Run python setup.py \"\n            \"build_sphinx first.\", lvl=error)\n        return\n\n    if os.path.exists(target):\n        log(\"Path already exists: \" + target)\n        if clear_target:\n            log(\"Cleaning up \" + target, lvl=warn)\n            shutil.rmtree(target)\n\n    log(\"Copying docs to \" + target)\n    copy_tree(source, target)\n    log(\"Done: Install Docs\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef var(ctx, clear_target, clear_all):\n\n    install_var(str(ctx.obj['instance']), clear_target, clear_all)", "response": "Install variable data to the var folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling required folders in the var folder.", "response": "def install_var(instance, clear_target, clear_all):\n    \"\"\"Install required folders in /var\"\"\"\n    _check_root()\n\n    log(\"Checking frontend library and cache directories\",\n        emitter='MANAGE')\n\n    uid = pwd.getpwnam(\"hfos\").pw_uid\n    gid = grp.getgrnam(\"hfos\").gr_gid\n\n    join = os.path.join\n\n    # If these need changes, make sure they are watertight and don't remove\n    # wanted stuff!\n    target_paths = (\n        '/var/www/challenges',  # For LetsEncrypt acme certificate challenges\n        join('/var/lib/hfos', instance),\n        join('/var/local/hfos', instance),\n        join('/var/local/hfos', instance, 'backup'),\n        join('/var/cache/hfos', instance),\n        join('/var/cache/hfos', instance, 'tilecache'),\n        join('/var/cache/hfos', instance, 'rastertiles'),\n        join('/var/cache/hfos', instance, 'rastercache')\n    )\n    logfile = \"/var/log/hfos-\" + instance + \".log\"\n\n    for item in target_paths:\n        if os.path.exists(item):\n            log(\"Path already exists: \" + item)\n            if clear_all or (clear_target and 'cache' in item):\n                log(\"Cleaning up: \" + item, lvl=warn)\n                shutil.rmtree(item)\n\n        if not os.path.exists(item):\n            log(\"Creating path: \" + item)\n            os.mkdir(item)\n            os.chown(item, uid, gid)\n\n    # Touch logfile to make sure it exists\n    open(logfile, \"a\").close()\n    os.chown(logfile, uid, gid)\n\n    log(\"Done: Install Var\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef provisions(ctx, provision, clear_existing, overwrite, list_provisions):\n\n    install_provisions(ctx, provision, clear_existing, overwrite, list_provisions)", "response": "Install default provisioning data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef install_provisions(ctx, provision, clear_provisions=False, overwrite=False, list_provisions=False):\n\n    log(\"Installing HFOS default provisions\")\n\n    # from hfos.logger import verbosity, events\n    # verbosity['console'] = verbosity['global'] = events\n\n    from hfos import database\n    database.initialize(ctx.obj['dbhost'], ctx.obj['dbname'])\n\n    from hfos.provisions import build_provision_store\n\n    provision_store = build_provision_store()\n\n    def sort_dependencies(items):\n        \"\"\"Topologically sort the dependency tree\"\"\"\n\n        g = networkx.DiGraph()\n        log('Sorting dependencies')\n\n        for key, item in items:\n            log('key: ', key, 'item:', item, pretty=True, lvl=debug)\n            dependencies = item.get('dependencies', [])\n            if isinstance(dependencies, str):\n                dependencies = [dependencies]\n\n            if key not in g:\n                g.add_node(key)\n\n            for link in dependencies:\n                g.add_edge(key, link)\n\n        if not networkx.is_directed_acyclic_graph(g):\n            log('Cycles in provosioning dependency graph detected!', lvl=error)\n            log('Involved provisions:', list(networkx.simple_cycles(g)), lvl=error)\n\n        topology = list(networkx.algorithms.topological_sort(g))\n        topology.reverse()\n\n        log(topology, pretty=True)\n\n        return topology\n\n    if list_provisions:\n        sort_dependencies(provision_store.items())\n        exit()\n\n    def provision_item(item):\n        \"\"\"Provision a single provisioning element\"\"\"\n\n        method = item.get('method', provisionList)\n        model = item.get('model')\n        data = item.get('data')\n\n        method(data, model, overwrite=overwrite, clear=clear_provisions)\n\n    if provision is not None:\n        if provision in provision_store:\n            log(\"Provisioning \", provision, pretty=True)\n            provision_item(provision_store[provision])\n        else:\n            log(\"Unknown provision: \", provision, \"\\nValid provisions are\",\n                list(provision_store.keys()),\n                lvl=error,\n                emitter='MANAGE')\n    else:\n        for name in sort_dependencies(provision_store.items()):\n            log(\"Provisioning\", name, pretty=True)\n            provision_item(provision_store[name])\n\n    log(\"Done: Install Provisions\")", "response": "Installs default provisioning data for all the items in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install_modules(wip):\n\n    def install_module(hfos_module):\n        \"\"\"Install a single module via setuptools\"\"\"\n        try:\n            setup = Popen(\n                [\n                    sys.executable,\n                    'setup.py',\n                    'develop'\n                ],\n                cwd='modules/' + hfos_module + \"/\"\n            )\n\n            setup.wait()\n        except Exception as e:\n            log(\"Problem during module installation: \", hfos_module, e,\n                type(e), exc=True, lvl=error)\n            return False\n        return True\n\n    # TODO: Sort module dependencies via topological sort or let pip do this in future.\n    # # To get the module dependencies:\n    # packages = {}\n    # for provision_entrypoint in iter_entry_points(group='hfos.provisions',\n    #                                               name=None):\n    #     log(\"Found packages: \", provision_entrypoint.dist.project_name, lvl=warn)\n    #\n    #     _package_name = provision_entrypoint.dist.project_name\n    #     _package = pkg_resources.working_set.by_key[_package_name]\n    #\n    #     print([str(r) for r in _package.requires()])  # retrieve deps from setup.py\n\n    modules_production = [\n        # TODO: Poor man's dependency management, as long as the modules are\n        # installed from local sources and they're not available on pypi,\n        # which would handle real dependency management for us:\n        'navdata',\n\n        # Now all the rest:\n        'alert',\n        'automat',\n        'busrepeater',\n        'calendar',\n        'countables',\n        'dash',\n        # 'dev',\n        'enrol',\n        'mail',\n        'maps',\n        'nmea',\n        'nodestate',\n        'project',\n        'webguides',\n        'wiki'\n    ]\n\n    modules_wip = [\n        'calc',\n        'camera',\n        'chat',\n        'comms',\n        'contacts',\n        'crew',\n        'equipment',\n        'filemanager',\n        'garden',\n        'heroic',\n        'ldap',\n        'library',\n        'logbook',\n        'protocols',\n        'polls',\n        'mesh',\n        'robot',\n        'switchboard',\n        'shareables',\n    ]\n\n    installables = modules_production\n\n    if wip:\n        installables.extend(modules_wip)\n\n    success = []\n    failed = []\n\n    for installable in installables:\n        log('Installing module ', installable)\n        if install_module(installable):\n            success.append(installable)\n        else:\n            failed.append(installable)\n\n    log('Installed modules: ', success)\n    if len(failed) > 0:\n        log('Failed modules: ', failed)\n    log('Done: Install Modules')", "response": "Installs the plugin modules in the current working set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef service(ctx):\n\n    install_service(ctx.obj['instance'], ctx.obj['dbhost'], ctx.obj['dbname'], ctx.obj['port'])", "response": "Install systemd service configuration"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall systemd service configuration", "response": "def install_service(instance, dbhost, dbname, port):\n    \"\"\"Install systemd service configuration\"\"\"\n\n    _check_root()\n\n    log(\"Installing systemd service\")\n\n    launcher = os.path.realpath(__file__).replace('manage', 'launcher')\n    executable = sys.executable + \" \" + launcher\n    executable += \" --instance \" + instance\n    executable += \" --dbname \" + dbname + \" --dbhost \" + dbhost\n    executable += \" --port \" + port\n    executable += \" --dolog --logfile /var/log/hfos-\" + instance + \".log\"\n    executable += \" --logfileverbosity 30 -q\"\n\n    definitions = {\n        'instance': instance,\n        'executable': executable\n    }\n    service_name = 'hfos-' + instance + '.service'\n\n    write_template_file(os.path.join('dev/templates', service_template),\n                        os.path.join('/etc/systemd/system/', service_name),\n                        definitions)\n\n    Popen([\n        'systemctl',\n        'enable',\n        service_name\n    ])\n\n    log('Launching service')\n\n    Popen([\n        'systemctl',\n        'start',\n        service_name\n    ])\n\n    log(\"Done: Install Service\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef install_cert(selfsigned):\n\n    _check_root()\n\n    if selfsigned:\n        log('Generating self signed (insecure) certificate/key '\n            'combination')\n\n        try:\n            os.mkdir('/etc/ssl/certs/hfos')\n        except FileExistsError:\n            pass\n        except PermissionError:\n            log(\"Need root (e.g. via sudo) to generate ssl certificate\")\n            sys.exit(1)\n\n        def create_self_signed_cert():\n            \"\"\"Create a simple self signed SSL certificate\"\"\"\n\n            # create a key pair\n            k = crypto.PKey()\n            k.generate_key(crypto.TYPE_RSA, 1024)\n\n            if os.path.exists(cert_file):\n                try:\n                    certificate = open(cert_file, \"rb\").read()\n                    old_cert = crypto.load_certificate(crypto.FILETYPE_PEM,\n                                                       certificate)\n                    serial = old_cert.get_serial_number() + 1\n                except (crypto.Error, OSError) as e:\n                    log('Could not read old certificate to increment '\n                        'serial:', type(e), e, exc=True, lvl=warn)\n                    serial = 1\n            else:\n                serial = 1\n\n            # create a self-signed certificate\n            certificate = crypto.X509()\n            certificate.get_subject().C = \"DE\"\n            certificate.get_subject().ST = \"Berlin\"\n            certificate.get_subject().L = \"Berlin\"\n            # noinspection PyPep8\n            certificate.get_subject().O = \"Hackerfleet\"\n            certificate.get_subject().OU = \"Hackerfleet\"\n            certificate.get_subject().CN = gethostname()\n            certificate.set_serial_number(serial)\n            certificate.gmtime_adj_notBefore(0)\n            certificate.gmtime_adj_notAfter(10 * 365 * 24 * 60 * 60)\n            certificate.set_issuer(certificate.get_subject())\n            certificate.set_pubkey(k)\n            certificate.sign(k, b'sha512')\n\n            open(key_file, \"wt\").write(str(\n                crypto.dump_privatekey(crypto.FILETYPE_PEM, k),\n                encoding=\"ASCII\"))\n\n            open(cert_file, \"wt\").write(str(\n                crypto.dump_certificate(crypto.FILETYPE_PEM, certificate),\n                encoding=\"ASCII\"))\n\n            open(combined_file, \"wt\").write(str(\n                crypto.dump_certificate(crypto.FILETYPE_PEM, certificate),\n                encoding=\"ASCII\") + str(\n                crypto.dump_privatekey(crypto.FILETYPE_PEM, k),\n                encoding=\"ASCII\"))\n\n        create_self_signed_cert()\n\n        log('Done: Install Cert')\n    else:\n\n        # TODO\n\n        log('Not implemented yet. You can build your own certificate and '\n            'store it in /etc/ssl/certs/hfos/server-cert.pem - it should '\n            'be a certificate with key, as this is used server side and '\n            'there is no way to enter a separate key.', lvl=error)", "response": "Install a local SSL certificate and a self - signed key pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and install frontend", "response": "def frontend(ctx, dev, rebuild, no_install, build_type):\n    \"\"\"Build and install frontend\"\"\"\n\n    install_frontend(instance=ctx.obj['instance'],\n                     forcerebuild=rebuild,\n                     development=dev,\n                     install=not no_install,\n                     build_type=build_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting - Install everything installable", "response": "def install_all(ctx, clear_all):\n    \"\"\"Default-Install everything installable\n\n    \\b\n    This includes\n    * System user (hfos.hfos)\n    * Self signed certificate\n    * Variable data locations (/var/lib/hfos and /var/cache/hfos)\n    * All the official modules in this repository\n    * Default module provisioning data\n    * Documentation\n    * systemd service descriptor\n\n    It does NOT build and install the HTML5 frontend.\"\"\"\n\n    _check_root()\n\n    instance = ctx.obj['instance']\n    dbhost = ctx.obj['dbhost']\n    dbname = ctx.obj['dbname']\n    port = ctx.obj['port']\n\n    install_system_user()\n    install_cert(selfsigned=True)\n\n    install_var(instance, clear_target=clear_all, clear_all=clear_all)\n    install_modules(wip=False)\n    install_provisions(provision=None, clear_provisions=clear_all)\n    install_docs(instance, clear_target=clear_all)\n\n    install_service(instance, dbhost, dbname, port)\n    install_nginx(instance, dbhost, dbname, port)\n\n    log('Done')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(ctx, no_restart, no_rebuild):\n\n    # 0. (NOT YET! MAKE A BACKUP OF EVERYTHING)\n    # 1. update repository\n    # 2. update frontend repository\n    # 3. (Not yet: update venv)\n    # 4. rebuild frontend\n    # 5. restart service\n\n    instance = ctx.obj['instance']\n\n    log('Pulling github updates')\n    run_process('.', ['git', 'pull', 'origin', 'master'])\n    run_process('./frontend', ['git', 'pull', 'origin', 'master'])\n\n    if not no_rebuild:\n        log('Rebuilding frontend')\n        install_frontend(instance, forcerebuild=True, install=False, development=True)\n\n    if not no_restart:\n        log('Restaring service')\n        if instance != 'hfos':\n            instance = 'hfos-' + instance\n\n        run_process('.', ['sudo', 'systemctl', 'restart', instance])\n\n    log('Done')", "response": "Update a HFOS node"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle incoming raw sensor data from the bus", "response": "def raw_data(self, event):\n        \"\"\"Handles incoming raw sensor data\n        :param event: Raw sentences incoming data\n        \"\"\"\n\n        self.log('Received raw data from bus', lvl=events)\n        if not parse:\n            return\n\n        nmea_time = event.data[0]\n        try:\n            parsed_data = parse(event.data[1])\n        except Exception as e:\n            self.log('Unparseable sentence:', event.data[1], e, type(e),\n                     exc=True, lvl=warn)\n            self.unparsable += event\n            return\n\n        bus = event.bus\n\n        sensor_data_package = self._handle(parsed_data)\n\n        self.log(\"Sensor data:\", sensor_data_package, lvl=verbose)\n\n        if sensor_data_package:\n            # pprint(sensor_data_package)\n            self.fireEvent(sensordata(sensor_data_package, nmea_time, bus),\n                           \"navdata\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_model_factories(store):\n\n    result = {}\n\n    for schemaname in store:\n\n        schema = None\n\n        try:\n            schema = store[schemaname]['schema']\n        except KeyError:\n            schemata_log(\"No schema found for \", schemaname, lvl=critical, exc=True)\n\n        try:\n            result[schemaname] = warmongo.model_factory(schema)\n        except Exception as e:\n            schemata_log(\"Could not create factory for schema \", schemaname, schema, lvl=critical, exc=True)\n\n    return result", "response": "Generate factories to construct objects from schemata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_collections(store):\n\n    result = {}\n\n    client = pymongo.MongoClient(host=dbhost, port=dbport)\n    db = client[dbname]\n\n    for schemaname in store:\n\n        schema = None\n        indices = None\n\n        try:\n            schema = store[schemaname]['schema']\n            indices = store[schemaname].get('indices', None)\n        except KeyError:\n            db_log(\"No schema found for \", schemaname, lvl=critical)\n\n        try:\n            result[schemaname] = db[schemaname]\n        except Exception:\n            db_log(\"Could not get collection for schema \", schemaname, schema, lvl=critical, exc=True)\n\n        if indices is not None:\n            col = db[schemaname]\n            db_log('Adding indices to', schemaname, lvl=debug)\n            i = 0\n            keys = list(indices.keys())\n\n            while i < len(indices):\n                index_name = keys[i]\n                index = indices[index_name]\n\n                index_type = index.get('type', None)\n                index_unique = index.get('unique', False)\n                index_sparse = index.get('sparse', True)\n                index_reindex = index.get('reindex', False)\n\n                if index_type in (None, 'text'):\n                    index_type = pymongo.TEXT\n                elif index_type == '2dsphere':\n                    index_type = pymongo.GEOSPHERE\n\n                def do_index():\n                    col.ensure_index([(index_name, index_type)],\n                                     unique=index_unique,\n                                     sparse=index_sparse)\n\n                db_log('Enabling index of type', index_type, 'on', index_name, lvl=debug)\n                try:\n                    do_index()\n                    i += 1\n                except pymongo.errors.OperationFailure:\n                    db_log(col.list_indexes().__dict__, pretty=True, lvl=verbose)\n                    if not index_reindex:\n                        db_log('Index was not created!', lvl=warn)\n                        i += 1\n                    else:\n                        try:\n                            col.drop_index(index_name)\n                            do_index()\n                            i += 1\n                        except pymongo.errors.OperationFailure as e:\n                            db_log('Index recreation problem:', exc=True, lvl=error)\n                            col.drop_indexes()\n                            i = 0\n\n                            # for index in col.list_indexes():\n                            #    db_log(\"Index: \", index)\n    return result", "response": "Generate database collections with indices from the schemastore"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize(address='127.0.0.1:27017', database_name='hfos', instance_name=\"default\", reload=False):\n\n    global schemastore\n    global l10n_schemastore\n    global objectmodels\n    global collections\n    global dbhost\n    global dbport\n    global dbname\n    global instance\n    global initialized\n\n    if initialized and not reload:\n        hfoslog('Already initialized and not reloading.', lvl=warn, emitter=\"DB\", frame_ref=2)\n        return\n\n    dbhost = address.split(':')[0]\n    dbport = int(address.split(\":\")[1]) if \":\" in address else 27017\n    dbname = database_name\n\n    db_log(\"Using database:\", dbname, '@', dbhost, ':', dbport)\n\n    try:\n        client = pymongo.MongoClient(host=dbhost, port=dbport)\n        db = client[dbname]\n        db_log(\"Database: \", db.command('buildinfo'), lvl=debug)\n    except Exception as e:\n        db_log(\"No database available! Check if you have mongodb > 3.0 \"\n               \"installed and running as well as listening on port 27017 \"\n               \"of localhost. (Error: %s) -> EXIT\" % e, lvl=critical)\n        sys.exit(5)\n\n    warmongo.connect(database_name)\n\n    schemastore = _build_schemastore_new()\n    l10n_schemastore = _build_l10n_schemastore(schemastore)\n    objectmodels = _build_model_factories(schemastore)\n    collections = _build_collections(schemastore)\n    instance = instance_name\n    initialized = True", "response": "Initializes the database connectivity schemata and object models"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef profile(schemaname='sensordata', profiletype='pjs'):\n\n    db_log(\"Profiling \", schemaname)\n\n    schema = schemastore[schemaname]['schema']\n\n    db_log(\"Schema: \", schema, lvl=debug)\n\n    testclass = None\n\n    if profiletype == 'warmongo':\n        db_log(\"Running Warmongo benchmark\")\n        testclass = warmongo.model_factory(schema)\n    elif profiletype == 'pjs':\n        db_log(\"Running PJS benchmark\")\n        try:\n            import python_jsonschema_objects as pjs\n        except ImportError:\n            db_log(\"PJS benchmark selected but not available. Install \"\n                   \"python_jsonschema_objects (PJS)\")\n            return\n\n        db_log()\n        builder = pjs.ObjectBuilder(schema)\n        ns = builder.build_classes()\n        pprint(ns)\n        testclass = ns[schemaname]\n        db_log(\"ns: \", ns, lvl=warn)\n\n    if testclass is not None:\n        db_log(\"Instantiating elements...\")\n        for i in range(100):\n            testclass()\n    else:\n        db_log(\"No Profiletype available!\")\n\n    db_log(\"Profiling done\")", "response": "Profile a schemata into a single object model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport all collections to a JSON - file.", "response": "def backup(schema, uuid, export_filter, export_format, filename, pretty, export_all, omit):\n    \"\"\"Exports all collections to (JSON-) files.\"\"\"\n\n    export_format = export_format.upper()\n\n    if pretty:\n        indent = 4\n    else:\n        indent = 0\n\n    f = None\n\n    if filename:\n        try:\n            f = open(filename, 'w')\n        except (IOError, PermissionError) as e:\n            backup_log('Could not open output file for writing:', exc=True, lvl=error)\n            return\n\n    def output(what, convert=False):\n        \"\"\"Output the backup in a specified format.\"\"\"\n\n        if convert:\n            if export_format == 'JSON':\n                data = json.dumps(what, indent=indent)\n            else:\n                data = \"\"\n        else:\n            data = what\n\n        if not filename:\n            print(data)\n        else:\n            f.write(data)\n\n    if schema is None:\n        if export_all is False:\n            backup_log('No schema given.', lvl=warn)\n            return\n        else:\n            schemata = objectmodels.keys()\n    else:\n        schemata = [schema]\n\n    all_items = {}\n\n    for schema_item in schemata:\n        model = objectmodels[schema_item]\n\n        if uuid:\n            obj = model.find({'uuid': uuid})\n        elif export_filter:\n            obj = model.find(literal_eval(export_filter))\n        else:\n            obj = model.find()\n\n        items = []\n        for item in obj:\n            fields = item.serializablefields()\n            for field in omit:\n                try:\n                    fields.pop(field)\n                except KeyError:\n                    pass\n            items.append(fields)\n\n        all_items[schema_item] = items\n\n        # if pretty is True:\n        #    output('\\n// Objectmodel: ' + schema_item + '\\n\\n')\n        # output(schema_item + ' = [\\n')\n\n    output(all_items, convert=True)\n\n    if f is not None:\n        f.flush()\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_collections(self):\n\n        self.collection_sizes = {}\n        self.collection_total = 0\n        for col in self.db.collection_names(include_system_collections=False):\n            self.collection_sizes[col] = self.db.command('collstats', col).get(\n                'storageSize', 0)\n            self.collection_total += self.collection_sizes[col]\n\n        sorted_x = sorted(self.collection_sizes.items(),\n                          key=operator.itemgetter(1))\n\n        for item in sorted_x:\n            self.log(\"Collection size (%s): %.2f MB\" % (\n                item[0], item[1] / 1024.0 / 1024),\n                     lvl=verbose)\n\n        self.log(\"Total collection sizes: %.2f MB\" % (self.collection_total /\n                                                      1024.0 / 1024))", "response": "Checks node local collection storage sizes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_free_space(self):\n\n        def get_folder_size(path):\n            \"\"\"Aggregates used size of a specified path, recursively\"\"\"\n\n            total_size = 0\n            for item in walk(path):\n                for file in item[2]:\n                    try:\n                        total_size = total_size + getsize(join(item[0], file))\n                    except (OSError, PermissionError) as e:\n                        self.log(\"error with file:  \" + join(item[0], file), e)\n            return total_size\n\n        for name, checkpoint in self.config.locations.items():\n            try:\n                stats = statvfs(checkpoint['location'])\n            except (OSError, PermissionError) as e:\n                self.log('Location unavailable:', name, e, type(e),\n                         lvl=error, exc=True)\n                continue\n            free_space = stats.f_frsize * stats.f_bavail\n            used_space = get_folder_size(\n                checkpoint['location']\n            ) / 1024.0 / 1024\n\n            self.log('Location %s uses %.2f MB' % (name, used_space))\n\n            if free_space < checkpoint['minimum']:\n                self.log('Short of free space on %s: %.2f MB left' % (\n                    name, free_space / 1024.0 / 1024 / 1024),\n                         lvl=warn)", "response": "Checks the used filesystem storage sizes of a specified path and checks if the free space is less than the minimum."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_mail_worker(config, mail, event):\n    log = \"\"\n\n    try:\n        if config.mail_ssl:\n            server = SMTP_SSL(config.mail_server, port=config.mail_server_port, timeout=30)\n        else:\n            server = SMTP(config.mail_server, port=config.mail_server_port, timeout=30)\n\n        if config.mail_tls:\n            log += 'Starting TLS\\n'\n            server.starttls()\n\n        if config.mail_username != '':\n            log += 'Logging in with ' + str(config.mail_username) + \"\\n\"\n            server.login(config.mail_username, config.mail_password)\n        else:\n            log += 'No username, trying anonymous access\\n'\n\n        log += 'Sending Mail\\n'\n        response_send = server.send_message(mail)\n        server.quit()\n\n    except timeout as e:\n        log += 'Could not send email to enrollee, mailserver timeout: ' + str(e) + \"\\n\"\n        return False, log, event\n\n    log += 'Server response:' + str(response_send)\n    return True, log, event", "response": "This function is a worker task to send an email to the enrollee."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reload_configuration(self, event):\n\n        super(EnrolManager, self).reload_configuration(event)\n        self.log('Reloaded configuration.')\n        self._setup()", "response": "Reload the current configuration and set up everything depending on it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invite(self, event):\n\n        self.log('Inviting new user to enrol')\n        name = event.data['name']\n        email = event.data['email']\n        method = event.data['method']\n\n        self._invite(name, method, email, event.client.uuid, event)", "response": "Invites a new user to enrol by an admin user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntells the client that we re currently open for enrollment", "response": "def status(self, event):\n        \"\"\"An anonymous client wants to know if we're open for enrollment\"\"\"\n\n        self.log('Registration status requested')\n\n        response = {\n            'component': 'hfos.enrol.enrolmanager',\n            'action': 'status',\n            'data': self.config.allow_registration\n        }\n\n        self.fire(send(event.client.uuid, response))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_reset(self, event):\n\n        self.log('Password reset request received:', event.__dict__, lvl=hilight)\n\n        user_object = objectmodels['user']\n\n        email = event.data.get('email', None)\n        email_user = None\n\n        if email is not None and user_object.count({'mail': email}) > 0:\n            email_user = user_object.find_one({'mail': email})\n\n        if email_user is None:\n            self._fail(event, msg=\"Mail address unknown\")\n            return", "response": "An anonymous client requests a password reset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef captcha_transmit(self, captcha, uuid):\n\n        self.log('Transmitting captcha')\n\n        response = {\n            'component': 'hfos.enrol.enrolmanager',\n            'action': 'captcha',\n            'data': b64encode(captcha['image'].getvalue()).decode('utf-8')\n        }\n        self.fire(send(uuid, response))", "response": "Delayed transmission of a captcha"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _invite(self, name, method, email, uuid, event, password=\"\"):\n\n        props = {\n            'uuid': std_uuid(),\n            'status': 'Open',\n            'name': name,\n            'method': method,\n            'email': email,\n            'password': password,\n            'timestamp': std_now()\n        }\n        enrollment = objectmodels['enrollment'](props)\n        enrollment.save()\n\n        self.log('Enrollment stored', lvl=debug)\n\n        self._send_invitation(enrollment, event)\n\n        packet = {\n            'component': 'hfos.enrol.enrolmanager',\n            'action': 'invite',\n            'data': [True, email]\n        }\n        self.fireEvent(send(uuid, packet))", "response": "Actually invite a given user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new user and all initial data", "response": "def _create_user(self, username, password, mail, method, uuid):\n        \"\"\"Create a new user and all initial data\"\"\"\n\n        try:\n            if method == 'Invited':\n                config_role = self.config.group_accept_invited\n            else:\n                config_role = self.config.group_accept_enrolled\n\n            roles = []\n            if ',' in config_role:\n                for item in config_role.split(','):\n                    roles.append(item.lstrip().rstrip())\n            else:\n                roles = [config_role]\n\n            newuser = objectmodels['user']({\n                'name': username,\n                'passhash': std_hash(password, self.salt),\n                'mail': mail,\n                'uuid': std_uuid(),\n                'roles': roles,\n                'created': std_now()\n            })\n\n            if method == 'Invited':\n                newuser.needs_password_change = True\n\n            newuser.save()\n        except Exception as e:\n            self.log(\"Problem creating new user: \", type(e), e,\n                     lvl=error)\n            return\n\n        try:\n            newprofile = objectmodels['profile']({\n                'uuid': std_uuid(),\n                'owner': newuser.uuid\n            })\n            self.log(\"New profile uuid: \", newprofile.uuid,\n                     lvl=verbose)\n\n            newprofile.save()\n\n            packet = {\n                'component': 'hfos.enrol.enrolmanager',\n                'action': 'enrol',\n                'data': [True, mail]\n            }\n            self.fireEvent(send(uuid, packet))\n\n            # TODO: Notify crew-admins\n        except Exception as e:\n            self.log(\"Problem creating new profile: \", type(e),\n                     e, lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend an invitation mail to an open enrolment", "response": "def _send_invitation(self, enrollment, event):\n        \"\"\"Send an invitation mail to an open enrolment\"\"\"\n\n        self.log('Sending enrollment status mail to user')\n\n        self._send_mail(self.config.invitation_subject, self.config.invitation_mail, enrollment, event)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_acceptance(self, enrollment, password, event):\n\n        self.log('Sending acceptance status mail to user')\n\n        if password is not \"\":\n            password_hint = '\\n\\nPS: Your new password is ' + password + ' - please change it after your first login!'\n\n            acceptance_text = self.config.acceptance_mail + password_hint\n        else:\n            acceptance_text = self.config.acceptance_mail\n\n        self._send_mail(self.config.acceptance_subject, acceptance_text, enrollment, event)", "response": "Send an acceptance mail to an open enrolment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_mail(self, subject, template, enrollment, event):\n\n        context = {\n            'name': enrollment.name,\n            'invitation_url': self.invitation_url,\n            'node_name': self.node_name,\n            'node_url': self.node_url,\n            'uuid': enrollment.uuid\n        }\n\n        mail = render(template, context)\n        self.log('Mail:', mail, lvl=verbose)\n        mime_mail = MIMEText(mail)\n        mime_mail['Subject'] = render(subject, context)\n        mime_mail['From'] = render(self.config.mail_from, {'hostname': self.hostname})\n        mime_mail['To'] = enrollment.email\n\n        self.log('MimeMail:', mime_mail, lvl=verbose)\n        if self.config.mail_send is True:\n            self.log('Sending mail to', enrollment.email)\n\n            self.fireEvent(task(send_mail_worker, self.config, mime_mail, event), \"enrolworkers\")\n        else:\n            self.log('Not sending mail, here it is for debugging info:', mail, pretty=True)", "response": "Send an email to the envelopers mail server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaying the user who is who he is", "response": "def say(self, event):\n        \"\"\"Chat event handler for incoming events\n        :param event: say-event with incoming chat message\n        \"\"\"\n\n        try:\n            userid = event.user.uuid\n            recipient = self._get_recipient(event)\n            content = self._get_content(event)\n\n            if self.config.name in content:\n                self.log('I think, someone mentioned me:', content)\n\n        except Exception as e:\n            self.log(\"Error: '%s' %s\" % (e, type(e)), exc=True, lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_auth_hook(self, event):\n\n        self.log('Adding authentication hook for', event.authenticator_name)\n        self.auth_hooks[event.authenticator_name] = event.event", "response": "Register event hook on reception of add_auth_hook - event"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fail(self, event, message='Invalid credentials'):\n\n        notification = {\n            'component': 'auth',\n            'action': 'fail',\n            'data': message\n        }\n\n        ip = event.sock.getpeername()[0]\n\n        self.failing_clients[ip] = event\n        Timer(3, Event.create('notify_fail', event.clientuuid, notification, ip)).register(self)", "response": "Sends a failure message to the requesting client"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _login(self, event, user_account, user_profile, client_config):\n\n        user_account.lastlogin = std_now()\n        user_account.save()\n\n        user_account.passhash = \"\"\n        self.fireEvent(\n            authentication(user_account.name, (\n                user_account, user_profile, client_config),\n                           event.clientuuid,\n                           user_account.uuid,\n                           event.sock),\n            \"auth\")", "response": "Send login notification to client"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles authentication requests from clients", "response": "def authenticationrequest(self, event):\n        \"\"\"Handles authentication requests from clients\n        :param event: AuthenticationRequest with user's credentials\n        \"\"\"\n\n        if event.sock.getpeername()[0] in self.failing_clients:\n            self.log('Client failed a login and has to wait', lvl=debug)\n            return\n\n        if event.auto:\n            self._handle_autologin(event)\n        else:\n            self._handle_login(event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_autologin(self, event):\n\n        self.log(\"Verifying automatic login request\")\n\n        # TODO: Check for a common secret\n\n        # noinspection PyBroadException\n        try:\n            client_config = objectmodels['client'].find_one({\n                'uuid': event.requestedclientuuid\n            })\n        except Exception:\n            client_config = None\n\n        if client_config is None or client_config.autologin is False:\n            self.log(\"Autologin failed:\", event.requestedclientuuid,\n                     lvl=error)\n            self._fail(event)\n            return\n\n        try:\n            user_account = objectmodels['user'].find_one({\n                'uuid': client_config.owner\n            })\n            if user_account is None:\n                raise AuthenticationError\n            self.log(\"Autologin for\", user_account.name, lvl=debug)\n        except Exception as e:\n            self.log(\"No user object due to error: \", e, type(e),\n                     lvl=error)\n            self._fail(event)\n            return\n\n        if user_account.active is False:\n            self.log(\"Account deactivated.\")\n            self._fail(event, 'Account deactivated.')\n            return\n\n        user_profile = self._get_profile(user_account)\n\n        self._login(event, user_account, user_profile, client_config)\n\n        self.log(\"Autologin successful!\", lvl=warn)", "response": "Handle automatic login requests for the user account and client configurations that allow it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a login request.", "response": "def _handle_login(self, event):\n        \"\"\"Manual password based login\"\"\"\n\n        # TODO: Refactor to simplify\n\n        self.log(\"Auth request for \", event.username, 'client:',\n                 event.clientuuid)\n\n        # TODO: Define the requirements for secure passwords etc.\n        # They're also required in the Enrol module..!\n\n        if (len(event.username) < 1) or (len(event.password) < 5):\n            self.log(\"Illegal username or password received, login cancelled\", lvl=warn)\n            self._fail(event, 'Password or username too short')\n            return\n\n        client_config = None\n\n        try:\n            user_account = objectmodels['user'].find_one({\n                'name': event.username\n            })\n            # self.log(\"Account: %s\" % user_account._fields, lvl=debug)\n            if user_account is None:\n                raise AuthenticationError\n        except Exception as e:\n            self.log(\"No userobject due to error: \", e, type(e),\n                     lvl=error)\n            self._fail(event)\n            return\n\n        self.log(\"User found.\", lvl=debug)\n\n        if user_account.active is False:\n            self.log(\"Account deactivated.\")\n            self._fail(event, 'Account deactivated.')\n            return\n\n        if not std_hash(event.password, self.salt) == user_account.passhash:\n            self.log(\"Password was wrong!\", lvl=warn)\n            self._fail(event)\n            return\n\n        self.log(\"Passhash matches, checking client and profile.\",\n                 lvl=debug)\n\n        requested_client_uuid = event.requestedclientuuid\n        if requested_client_uuid is not None:\n            client_config = objectmodels['client'].find_one({\n                'uuid': requested_client_uuid\n            })\n\n        if client_config:\n            self.log(\"Checking client configuration permissions\",\n                     lvl=debug)\n            # TODO: Shareable client configurations?\n            if client_config.owner != user_account.uuid:\n                client_config = None\n                self.log(\"Unauthorized client configuration \"\n                         \"requested\",\n                         lvl=warn)\n        else:\n            self.log(\"Unknown client configuration requested: \",\n                     requested_client_uuid, event.__dict__,\n                     lvl=warn)\n\n        if not client_config:\n            self.log(\"Creating new default client configuration\")\n            # Either no configuration was found or not requested\n            # -> Create a new client configuration\n            uuid = event.clientuuid if event.clientuuid is not None else str(uuid4())\n\n            client_config = objectmodels['client']({'uuid': uuid})\n\n            client_config.name = std_human_uid(kind='place')\n\n            client_config.description = \"New client configuration from \" + user_account.name\n            client_config.owner = user_account.uuid\n\n            # TODO: Get client configuration storage done right, this one is too simple\n            client_config.save()\n\n        user_profile = self._get_profile(user_account)\n\n        self._login(event, user_account, user_profile, client_config)\n        self.log(\"Done with Login request\", lvl=debug)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_profile(self, user_account):\n\n        try:\n            # TODO: Load active profile, not just any\n            user_profile = objectmodels['profile'].find_one(\n                {'owner': str(user_account.uuid)})\n            self.log(\"Profile: \", user_profile,\n                     user_account.uuid, lvl=debug)\n        except Exception as e:\n            self.log(\"No profile due to error: \", e, type(e),\n                     lvl=error)\n            user_profile = None\n\n        if not user_profile:\n            default = {\n                'uuid': std_uuid(),\n                'owner': user_account.uuid,\n                'userdata': {\n                    'notes': 'Default profile of ' + user_account.name\n                }\n            }\n            user_profile = objectmodels['profile'](default)\n            user_profile.save()\n\n        return user_profile", "response": "Retrieves a user s profile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the raw data from the device and return a list of tuples.", "response": "def _parse(self, bus, data):\n        \"\"\"\n        Called when a sensor sends a new raw data to this serial connector.\n\n        The data is sanitized and sent to the registered protocol\n        listeners as time/raw/bus sentence tuple.\n        \"\"\"\n\n        sen_time = time.time()\n\n        try:\n            # Split up multiple sentences\n            if isinstance(data, bytes):\n                data = data.decode('ascii')\n\n            dirtysentences = data.split(\"\\n\")\n            sentences = [(sen_time, x) for x in dirtysentences if x]\n\n            def unique(it):\n                s = set()\n                for el in it:\n                    if el not in s:\n                        s.add(el)\n                        yield el\n                    else:\n                        # TODO: Make sure, this is not identical but new data\n                        self.log(\"Duplicate sentence received: \", el,\n                                 lvl=debug)\n\n            sentences = list(unique(sentences))\n            return sentences\n        except Exception as e:\n            self.log(\"Error during data unpacking: \", e, type(e), lvl=error,\n                     exc=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle incoming raw sensor data", "response": "def serial_packet(self, event):\n        \"\"\"Handles incoming raw sensor data\n        :param data: raw incoming data\n        \"\"\"\n\n        self.log('Incoming serial packet:', event.__dict__, lvl=verbose)\n\n        if self.scanning:\n            pass\n        else:\n            # self.log(\"Incoming data: \", '%.50s ...' % event.data, lvl=debug)\n            sanitized_data = self._parse(event.bus, event.data)\n            self.log('Sanitized data:', sanitized_data, lvl=verbose)\n            if sanitized_data is not None:\n                self._broadcast(event.bus, sanitized_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n\n    desc = 'Converts between geodetic, modified apex, quasi-dipole and MLT'\n    parser = argparse.ArgumentParser(description=desc, prog='apexpy')\n\n    parser.add_argument('source', metavar='SOURCE',\n                        choices=['geo', 'apex', 'qd', 'mlt'],\n                        help='Convert from {geo, apex, qd, mlt}')\n    parser.add_argument('dest', metavar='DEST',\n                        choices=['geo', 'apex', 'qd', 'mlt'],\n                        help='Convert to {geo, apex, qd, mlt}')\n    desc = 'YYYY[MM[DD[HHMMSS]]] date/time for IGRF coefficients, time part '\n    desc += 'required for MLT calculations'\n    parser.add_argument('date', metavar='DATE', help=desc)\n    parser.add_argument('--height', dest='height', default=0, metavar='HEIGHT',\n                        type=float, help='height for conversion')\n    parser.add_argument('--refh', dest='refh', metavar='REFH', type=float,\n                        default=0,\n                        help='reference height for modified apex coordinates')\n\n    parser.add_argument('-i', '--input', dest='file_in', metavar='FILE_IN',\n                        type=argparse.FileType('r'), default=STDIN,\n                        help='input file (stdin if none specified)')\n    parser.add_argument('-o', '--output', dest='file_out', metavar='FILE_OUT',\n                        type=argparse.FileType('wb'), default=STDOUT,\n                        help='output file (stdout if none specified)')\n\n    args = parser.parse_args()\n\n    array = np.loadtxt(args.file_in, ndmin=2)\n\n    if 'mlt' in [args.source, args.dest] and len(args.date) < 14:\n        desc = 'full date/time YYYYMMDDHHMMSS required for MLT calculations'\n        raise ValueError(desc)\n    if 9 <= len(args.date) <= 13:\n        desc = 'full date/time must be given as YYYYMMDDHHMMSS, not ' + \\\n               'YYYYMMDDHHMMSS'[:len(args.date)]\n        raise ValueError(desc)\n    datetime = dt.datetime.strptime(args.date,\n                                    '%Y%m%d%H%M%S'[:len(args.date)-2])\n    A = apexpy.Apex(date=datetime, refh=args.refh)\n    lats, lons = A.convert(array[:, 0], array[:, 1], args.source, args.dest,\n                           args.height, datetime=datetime)\n    np.savetxt(args.file_out, np.column_stack((lats, lons)), fmt='%.8f')", "response": "Entry point for the script"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes an external process via subprocess. Popen", "response": "def run_process(cwd, args):\n    \"\"\"Executes an external process via subprocess.Popen\"\"\"\n    try:\n        process = check_output(args, cwd=cwd, stderr=STDOUT)\n\n        return process\n    except CalledProcessError as e:\n        log('Uh oh, the teapot broke again! Error:', e, type(e), lvl=verbose, pretty=True)\n        log(e.cmd, e.returncode, e.output, lvl=verbose)\n        return e.output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ask_password():\n\n    password = \"Foo\"\n    password_trial = \"\"\n\n    while password != password_trial:\n        password = getpass.getpass()\n        password_trial = getpass.getpass(prompt=\"Repeat:\")\n        if password != password_trial:\n            print(\"\\nPasswords do not match!\")\n\n    return password", "response": "Securely and interactively ask for a password"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nobtain user credentials by arguments or asking the user", "response": "def _get_credentials(username=None, password=None, dbhost=None):\n    \"\"\"Obtain user credentials by arguments or asking the user\"\"\"\n\n    # Database salt\n    system_config = dbhost.objectmodels['systemconfig'].find_one({\n        'active': True\n    })\n\n    try:\n        salt = system_config.salt.encode('ascii')\n    except (KeyError, AttributeError):\n        log('No systemconfig or it is without a salt! '\n            'Reinstall the system provisioning with'\n            'hfos_manage.py install provisions -p system')\n        sys.exit(3)\n\n    if username is None:\n        username = _ask(\"Please enter username: \")\n    else:\n        username = username\n\n    if password is None:\n        password = _ask_password()\n    else:\n        password = password\n\n    try:\n        password = password.encode('utf-8')\n    except UnicodeDecodeError:\n        password = password\n\n    passhash = hashlib.sha512(password)\n    passhash.update(salt)\n\n    return username, passhash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ask(question, default=None, data_type='str', show_hint=False):\n\n    data = default\n\n    if data_type == 'bool':\n        data = None\n        default_string = \"Y\" if default else \"N\"\n\n        while data not in ('Y', 'J', 'N', '1', '0'):\n            data = input(\"%s? [%s]: \" % (question, default_string)).upper()\n\n            if data == '':\n                return default\n\n        return data in ('Y', 'J', '1')\n\n    elif data_type in ('str', 'unicode'):\n        if show_hint:\n            msg = \"%s? [%s] (%s): \" % (question, default, data_type)\n        else:\n            msg = question\n\n        data = input(msg)\n\n        if len(data) == 0:\n            data = default\n    elif data_type == 'int':\n        if show_hint:\n            msg = \"%s? [%s] (%s): \" % (question, default, data_type)\n        else:\n            msg = question\n\n        data = input(msg)\n\n        if len(data) == 0:\n            data = int(default)\n        else:\n            data = int(data)\n\n    return data", "response": "Interactively ask the user for data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checklat(lat, name='lat'):\n\n    if np.all(np.float64(lat) >= -90) and np.all(np.float64(lat) <= 90):\n        return lat\n\n    if np.isscalar(lat):\n        if lat > 90 and np.isclose(lat, 90, rtol=0, atol=1e-4):\n            lat = 90\n            return lat\n        elif lat < -90 and np.isclose(lat, -90, rtol=0, atol=1e-4):\n            lat = -90\n            return lat\n    else:\n        lat = np.float64(lat)  # make sure we have an array, not list\n        lat[(lat > 90) & (np.isclose(lat, 90, rtol=0, atol=1e-4))] = 90\n        lat[(lat < -90) & (np.isclose(lat, -90, rtol=0, atol=1e-4))] = -90\n        if np.all(lat >= -90) and np.all(lat <= 90):\n            return lat\n\n    # we haven't returned yet, so raise exception\n    raise ValueError(name + ' must be in [-90, 90]')", "response": "Checks that the latitude is inside the range - 90 - 90 and clipping close values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes sinIm from modified apex latitude.", "response": "def getsinIm(alat):\n    \"\"\"Computes sinIm from modified apex latitude.\n\n    Parameters\n    ==========\n    alat : array_like\n        Modified apex latitude\n\n    Returns\n    =======\n    sinIm : ndarray or float\n\n    \"\"\"\n\n    alat = np.float64(alat)\n\n    return 2*np.sin(np.radians(alat))/np.sqrt(4 - 3*np.cos(np.radians(alat))**2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute cosIm from modified apex latitude.", "response": "def getcosIm(alat):\n    \"\"\"Computes cosIm from modified apex latitude.\n\n    Parameters\n    ==========\n    alat : array_like\n        Modified apex latitude\n\n    Returns\n    =======\n    cosIm : ndarray or float\n\n    \"\"\"\n\n    alat = np.float64(alat)\n\n    return np.cos(np.radians(alat))/np.sqrt(4 - 3*np.cos(np.radians(alat))**2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a date object to a decimal year.", "response": "def toYearFraction(date):\n    \"\"\"Converts :class:`datetime.date` or :class:`datetime.datetime` to decimal\n    year.\n\n    Parameters\n    ==========\n    date : :class:`datetime.date` or :class:`datetime.datetime`\n\n    Returns\n    =======\n    year : float\n        Decimal year\n\n    Notes\n    =====\n    The algorithm is taken from http://stackoverflow.com/a/6451892/2978652\n\n    \"\"\"\n\n    def sinceEpoch(date):\n        \"\"\"returns seconds since epoch\"\"\"\n        return time.mktime(date.timetuple())\n    year = date.year\n    startOfThisYear = dt.datetime(year=year, month=1, day=1)\n    startOfNextYear = dt.datetime(year=year+1, month=1, day=1)\n\n    yearElapsed = sinceEpoch(date) - sinceEpoch(startOfThisYear)\n    yearDuration = sinceEpoch(startOfNextYear) - sinceEpoch(startOfThisYear)\n    fraction = yearElapsed/yearDuration\n\n    return date.year + fraction"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts geocentric latitude to geodetic latitude using WGS84.", "response": "def gc2gdlat(gclat):\n    \"\"\"Converts geocentric latitude to geodetic latitude using WGS84.\n\n    Parameters\n    ==========\n    gclat : array_like\n        Geocentric latitude\n\n    Returns\n    =======\n    gdlat : ndarray or float\n        Geodetic latitude\n\n    \"\"\"\n    WGS84_e2 = 0.006694379990141317  # WGS84 first eccentricity squared\n    return np.rad2deg(-np.arctan(np.tan(np.deg2rad(gclat))/(WGS84_e2 - 1)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding subsolar geocentric latitude and longitude.", "response": "def subsol(datetime):\n    \"\"\"Finds subsolar geocentric latitude and longitude.\n\n    Parameters\n    ==========\n    datetime : :class:`datetime.datetime`\n\n    Returns\n    =======\n    sbsllat : float\n        Latitude of subsolar point\n    sbsllon : float\n        Longitude of subsolar point\n\n    Notes\n    =====\n    Based on formulas in Astronomical Almanac for the year 1996, p. C24.\n    (U.S. Government Printing Office, 1994). Usable for years 1601-2100,\n    inclusive. According to the Almanac, results are good to at least 0.01\n    degree latitude and 0.025 degrees longitude between years 1950 and 2050.\n    Accuracy for other years has not been tested. Every day is assumed to have\n    exactly 86400 seconds; thus leap seconds that sometimes occur on December\n    31 are ignored (their effect is below the accuracy threshold of the\n    algorithm).\n\n    After Fortran code by A. D. Richmond, NCAR. Translated from IDL\n    by K. Laundal.\n\n    \"\"\"\n    # convert to year, day of year and seconds since midnight\n    year = datetime.year\n    doy = datetime.timetuple().tm_yday\n    ut = datetime.hour * 3600 + datetime.minute * 60 + datetime.second\n\n    if not 1601 <= year <= 2100:\n        raise ValueError('Year must be in [1601, 2100]')\n\n    yr = year - 2000\n\n    nleap = int(np.floor((year - 1601.0) / 4.0))\n    nleap -= 99\n    if year <= 1900:\n        ncent = int(np.floor((year - 1601.0) / 100.0))\n        ncent = 3 - ncent\n        nleap = nleap + ncent\n\n    l0 = -79.549 + (-0.238699 * (yr - 4.0 * nleap) + 3.08514e-2 * nleap)\n    g0 = -2.472 + (-0.2558905 * (yr - 4.0 * nleap) - 3.79617e-2 * nleap)\n\n    # Days (including fraction) since 12 UT on January 1 of IYR:\n    df = (ut / 86400.0 - 1.5) + doy\n\n    # Mean longitude of Sun:\n    lmean = l0 + 0.9856474 * df\n\n    # Mean anomaly in radians:\n    grad = np.radians(g0 + 0.9856003 * df)\n\n    # Ecliptic longitude:\n    lmrad = np.radians(lmean + 1.915 * np.sin(grad)\n                       + 0.020 * np.sin(2.0 * grad))\n    sinlm = np.sin(lmrad)\n\n    # Obliquity of ecliptic in radians:\n    epsrad = np.radians(23.439 - 4e-7 * (df + 365 * yr + nleap))\n\n    # Right ascension:\n    alpha = np.degrees(np.arctan2(np.cos(epsrad) * sinlm, np.cos(lmrad)))\n\n    # Declination, which is also the subsolar latitude:\n    sslat = np.degrees(np.arcsin(np.sin(epsrad) * sinlm))\n\n    # Equation of time (degrees):\n    etdeg = lmean - alpha\n    nrot = round(etdeg / 360.0)\n    etdeg = etdeg - 360.0 * nrot\n\n    # Subsolar longitude:\n    sslon = 180.0 - (ut / 240.0 + etdeg) # Earth rotates one degree every 240 s.\n    nrot = round(sslon / 360.0)\n    sslon = sslon - 360.0 * nrot\n\n    return sslat, sslon"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the authentication headers needed to use the Appveyor API.", "response": "def make_auth_headers():\n    \"\"\"Make the authentication headers needed to use the Appveyor API.\"\"\"\n    if not os.path.exists(\".appveyor.token\"):\n        raise RuntimeError(\n            \"Please create a file named `.appveyor.token` in the current directory. \"\n            \"You can get the token from https://ci.appveyor.com/api-token\"\n        )\n    with open(\".appveyor.token\") as f:\n        token = f.read().strip()\n\n    headers = {\n        'Authorization': 'Bearer {}'.format(token),\n    }\n    return headers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the details of the latest Appveyor build.", "response": "def get_project_build(account_project):\n    \"\"\"Get the details of the latest Appveyor build.\"\"\"\n    url = make_url(\"/projects/{account_project}\", account_project=account_project)\n    response = requests.get(url, headers=make_auth_headers())\n    return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_url(url, filename, headers):\n    ensure_dirs(filename)\n    response = requests.get(url, headers=headers, stream=True)\n    if response.status_code == 200:\n        with open(filename, 'wb') as f:\n            for chunk in response.iter_content(16 * 1024):\n                f.write(chunk)", "response": "Download a file from url to filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_schemata_list(self, *args):\n\n        self.log('Registered schemata languages:', \",\".join(sorted(l10n_schemastore.keys())))\n        self.log('Registered Schemata:', \",\".join(sorted(schemastore.keys())))\n        if '-c' in args or '-config' in args:\n            self.log('Registered Configuration Schemata:', \",\".join(sorted(configschemastore.keys())), pretty=True)", "response": "Display a list of registered schemata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays a schemata s form definition", "response": "def cli_form(self, *args):\n        \"\"\"Display a schemata's form definition\"\"\"\n\n        if args[0] == '*':\n            for schema in schemastore:\n                self.log(schema, ':', schemastore[schema]['form'], pretty=True)\n        else:\n            self.log(schemastore[args[0]]['form'], pretty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays a single schema definition", "response": "def cli_schema(self, *args):\n        \"\"\"Display a single schema definition\"\"\"\n\n        key = None\n        if len(args) > 1:\n            key = args[1]\n            args = list(args)\n\n        if '-config' in args or '-c' in args:\n            store = configschemastore\n            try:\n                args.remove('-c')\n                args.remove('-config')\n            except ValueError:\n                pass\n        else:\n            store = schemastore\n\n        def output(schema):\n            self.log(\"%s :\" % schema)\n            if key == 'props':\n                self.log(store[schema]['schema']['properties'], pretty=True)\n            elif key == 'perms':\n                try:\n                    self.log(store[schema]['schema']['roles_create'], pretty=True)\n                except KeyError:\n                    self.log('Schema', schema, 'has no role for creation', lvl=warn)\n                try:\n                    self.log(store[schema]['schema']['properties']['perms']['properties'], pretty=True)\n                except KeyError:\n                    self.log('Schema', schema, 'has no permissions', lvl=warn)\n            else:\n                self.log(store[schema]['schema'], pretty=True)\n\n        if '*' in args:\n            for schema in store:\n                output(schema)\n        else:\n            output(args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all available form definitions", "response": "def cli_forms(self, *args):\n        \"\"\"List all available form definitions\"\"\"\n\n        forms = []\n        missing = []\n\n        for key, item in schemastore.items():\n            if 'form' in item and len(item['form']) > 0:\n                forms.append(key)\n            else:\n                missing.append(key)\n\n        self.log('Schemata with form:', forms)\n        self.log('Missing forms:', missing)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli_default_perms(self, *args):\n\n        for key, item in schemastore.items():\n            # self.log(item, pretty=True)\n            if item['schema'].get('no_perms', False):\n                self.log('Schema without permissions:', key)\n                continue\n            try:\n                perms = item['schema']['properties']['perms']['properties']\n                if perms == {}:\n                    self.log('Schema:', item, pretty=True)\n\n                self.log(\n                    'Schema:', key,\n                    'read', perms['read']['default'],\n                    'write', perms['write']['default'],\n                    'list', perms['list']['default'],\n                    'create', item['schema']['roles_create']\n                )\n            except KeyError as e:\n                self.log('Fishy schema found:', key, e, lvl=error)\n                self.log(item, pretty=True)", "response": "Show default permissions for all schemata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up the application after startup.", "response": "def ready(self):\n        \"\"\"Sets up the application after startup.\"\"\"\n\n        self.log('Got', len(schemastore), 'data and',\n                 len(configschemastore), 'component schemata.', lvl=debug)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef all(self, event):\n\n        self.log(\"Schemarequest for all schemata from\",\n                 event.user, lvl=debug)\n        response = {\n            'component': 'hfos.events.schemamanager',\n            'action': 'all',\n            'data': l10n_schemastore[event.client.language]\n        }\n        self.fireEvent(send(event.client.uuid, response))", "response": "Return all known schemata to the requesting client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, event):\n        self.log(\"Schemarequest for\", event.data, \"from\",\n                 event.user, lvl=debug)\n        if event.data in schemastore:\n            response = {\n                'component': 'hfos.events.schemamanager',\n                'action': 'get',\n                'data': l10n_schemastore[event.client.language][event.data]\n            }\n            self.fireEvent(send(event.client.uuid, response))\n        else:\n            self.log(\"Unavailable schema requested!\", lvl=warn)", "response": "Return a single schema"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configuration(self, event):\n\n        try:\n            self.log(\"Schemarequest for all configuration schemata from\",\n                     event.user.account.name, lvl=debug)\n            response = {\n                'component': 'hfos.events.schemamanager',\n                'action': 'configuration',\n                'data': configschemastore\n            }\n            self.fireEvent(send(event.client.uuid, response))\n        except Exception as e:\n            self.log(\"ERROR:\", e)", "response": "Return all configurable components schemata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uuid_object(title=\"Reference\", description=\"Select an object\", default=None, display=True):\n\n    uuid = {\n        'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{'\n                   '4}-['\n                   'a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n        'type': 'string',\n        'title': title,\n        'description': description,\n    }\n\n    if not display:\n        uuid['x-schema-form'] = {\n            'condition': \"false\"\n        }\n\n    if default is not None:\n        uuid['default'] = default\n\n    return uuid", "response": "Generates a regular expression controlled UUID field"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a basic object with RBAC properties", "response": "def base_object(name,\n                no_perms=False,\n                has_owner=True,\n                hide_owner=True,\n                has_uuid=True,\n                roles_write=None,\n                roles_read=None,\n                roles_list=None,\n                roles_create=None,\n                all_roles=None):\n    \"\"\"Generates a basic object with RBAC properties\"\"\"\n    base_schema = {\n        'id': '#' + name,\n        'type': 'object',\n        'name': name,\n        'properties': {}\n    }\n\n    if not no_perms:\n        if all_roles:\n            roles_create = ['admin', all_roles]\n            roles_write = ['admin', all_roles]\n            roles_read = ['admin', all_roles]\n            roles_list = ['admin', all_roles]\n        else:\n            if roles_write is None:\n                roles_write = ['admin']\n            if roles_read is None:\n                roles_read = ['admin']\n            if roles_list is None:\n                roles_list = ['admin']\n            if roles_create is None:\n                roles_create = ['admin']\n\n        if isinstance(roles_create, str):\n            roles_create = [roles_create]\n        if isinstance(roles_write, str):\n            roles_write = [roles_write]\n        if isinstance(roles_read, str):\n            roles_read = [roles_read]\n        if isinstance(roles_list, str):\n            roles_list = [roles_list]\n\n        if has_owner:\n            roles_write.append('owner')\n            roles_read.append('owner')\n            roles_list.append('owner')\n\n        base_schema['roles_create'] = roles_create\n        base_schema['properties'].update({\n            'perms': {\n                'id': '#perms',\n                'type': 'object',\n                'name': 'perms',\n                'properties': {\n                    'write': {\n                        'type': 'array',\n                        'default': roles_write,\n                        'items': {\n                            'type': 'string',\n                        }\n                    },\n                    'read': {\n                        'type': 'array',\n                        'default': roles_read,\n                        'items': {\n                            'type': 'string',\n                        }\n                    },\n                    'list': {\n                        'type': 'array',\n                        'default': roles_list,\n                        'items': {\n                            'type': 'string',\n                        }\n                    }\n                },\n                'default': {},\n                'x-schema-form': {\n                    'condition': \"false\"\n                }\n            },\n            'name': {\n                'type': 'string',\n                'description': 'Name of ' + name\n            }\n        })\n\n        if has_owner:\n            # TODO: Schema should allow specification of non-local owners as\n            #  well as special accounts like admin or even system perhaps\n            # base_schema['required'] = base_schema.get('required', [])\n            # base_schema['required'].append('owner')\n            base_schema['properties'].update({\n                'owner': uuid_object(title='Unique Owner ID', display=hide_owner)\n            })\n    else:\n        base_schema['no_perms'] = True\n\n    # TODO: Using this causes all sorts of (obvious) problems with the object\n    # manager\n    if has_uuid:\n        base_schema['properties'].update({\n            'uuid': uuid_object(title='Unique ' + name + ' ID', display=False)\n        })\n        base_schema['required'] = [\"uuid\"]\n\n    return base_schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the serial ports available on the system", "response": "def serial_ports():\n    \"\"\" Lists serial port names\n\n        :raises EnvironmentError:\n            On unsupported or unknown platforms\n        :returns:\n            A list of the serial ports available on the system\n\n        Courtesy: Thomas ( http://stackoverflow.com/questions/12090503\n        /listing-available-com-ports-with-python )\n    \"\"\"\n    if sys.platform.startswith('win'):\n        ports = ['COM%s' % (i + 1) for i in range(256)]\n    elif sys.platform.startswith('linux') or sys.platform.startswith('cygwin'):\n        # this excludes your current terminal \"/dev/tty\"\n        ports = glob.glob('/dev/tty[A-Za-z]*')\n    elif sys.platform.startswith('darwin'):\n        ports = glob.glob('/dev/tty.*')\n    else:\n        raise EnvironmentError('Unsupported platform')\n\n    result = []\n    for port in ports:\n        try:\n            s = serial.Serial(port)\n            s.close()\n            result.append(port)\n        except (OSError, serial.SerialException) as e:\n            hfoslog('Could not open serial port:', port, e, type(e),\n                    exc=True, lvl=warn)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sensordata(self, event):\n\n        if len(self.datatypes) == 0:\n            return\n\n        data = event.data\n        timestamp = event.timestamp\n        # bus = event.bus\n\n        # TODO: What about multiple busses? That is prepared, but how exactly\n        # should they be handled?\n\n        self.log(\"New incoming navdata:\", data, lvl=verbose)\n\n        for name, value in data.items():\n            if name in self.datatypes:\n                ref = self.datatypes[name]\n                self.sensed[name] = ref\n\n                if ref.lastvalue != str(value):\n                    # self.log(\"Reference outdated:\", ref._fields)\n\n                    item = {\n                        'value': value,\n                        'timestamp': timestamp,\n                        'type': name\n                    }\n\n                    self.referenceframe[name] = value\n                    self.referenceages[name] = timestamp\n\n                    # self.log(\"Subscriptions:\", self.subscriptions, ref.name)\n                    if ref.name in self.subscriptions:\n\n                        packet = {\n                            'component': 'hfos.navdata.sensors',\n                            'action': 'update',\n                            'data': item\n                        }\n\n                        self.log(\"Serving update: \", packet, lvl=verbose)\n                        for uuid in self.subscriptions[ref.name]:\n                            self.log(\"Serving to \", uuid, lvl=events)\n                            self.fireEvent(send(uuid, packet),\n                                           'hfosweb')\n\n                    # self.log(\"New item: \", item)\n                    sensordata = objectmodels['sensordata'](item)\n                    # self.log(\"Value entry:\", sensordata._fields)\n\n                    if ref.record:\n                        self.log(\"Recording updated reference:\",\n                                 sensordata._fields)\n                        sensordata.save()\n\n                    ref.lastvalue = str(value)\n                    ref.timestamp = timestamp\n            else:\n                self.log(\"Unknown sensor data received!\", data, lvl=warn)", "response": "Generates a new reference frame from incoming sensordata"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npush the current referenceframe out to clients.", "response": "def navdatapush(self):\n        \"\"\"\n        Pushes the current :referenceframe: out to clients.\n\n        :return:\n        \"\"\"\n\n        try:\n            self.fireEvent(referenceframe({\n                'data': self.referenceframe, 'ages': self.referenceages\n            }), \"navdata\")\n            self.intervalcount += 1\n\n            if self.intervalcount == self.passiveinterval and len(\n                    self.referenceframe) > 0:\n                self.fireEvent(broadcast('users', {\n                    'component': 'hfos.navdata.sensors',\n                    'action': 'update',\n                    'data': {\n                        'data': self.referenceframe,\n                        'ages': self.referenceages\n                    }\n                }), \"hfosweb\")\n                self.intervalcount = 0\n                # self.log(\"Reference frame successfully pushed.\",\n                # lvl=verbose)\n        except Exception as e:\n            self.log(\"Could not push referenceframe: \", e, type(e),\n                     lvl=critical)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport stored objects Warning! This functionality is work in progress and you may destroy live data by using it! Be very careful when using the export/import functionality!", "response": "def db_export(schema, uuid, object_filter, export_format, filename, pretty, all_schemata, omit):\n    \"\"\"Export stored objects\n\n    Warning! This functionality is work in progress and you may destroy live data by using it!\n    Be very careful when using the export/import functionality!\"\"\"\n\n    internal_backup(schema, uuid, object_filter, export_format, filename, pretty, all_schemata, omit)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef db_import(ctx, schema, uuid, object_filter, import_format, filename, all_schemata, dry):\n\n    import_format = import_format.upper()\n\n    with open(filename, 'r') as f:\n        json_data = f.read()\n    data = json.loads(json_data)  # , parse_float=True, parse_int=True)\n\n    if schema is None:\n        if all_schemata is False:\n            log('No schema given. Read the help', lvl=warn)\n            return\n        else:\n            schemata = data.keys()\n    else:\n        schemata = [schema]\n\n    from hfos import database\n    database.initialize(ctx.obj['dbhost'], ctx.obj['dbname'])\n\n    all_items = {}\n    total = 0\n\n    for schema_item in schemata:\n        model = database.objectmodels[schema_item]\n\n        objects = data[schema_item]\n        if uuid:\n            for item in objects:\n                if item['uuid'] == uuid:\n                    items = [model(item)]\n        else:\n            items = []\n            for item in objects:\n                thing = model(item)\n                items.append(thing)\n\n        schema_total = len(items)\n        total += schema_total\n\n        if dry:\n            log('Would import', schema_total, 'items of', schema_item)\n        all_items[schema_item] = items\n\n    if dry:\n        log('Would import', total, 'objects.')\n    else:\n        log('Importing', total, 'objects.')\n        for schema_name, item_list in all_items.items():\n            log('Importing', len(item_list), 'objects of type', schema_name)\n            for item in item_list:\n                item._fields['_id'] = bson.objectid.ObjectId(item._fields['_id'])\n                item.save()", "response": "Import objects from file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the newly created object is a wikipage and rerenders the automatic index.", "response": "def _page_update(self, event):\n        \"\"\"\n        Checks if the newly created object is a wikipage..\n        If so, rerenders the automatic index.\n\n        :param event: objectchange or objectcreation event\n        \"\"\"\n        try:\n            if event.schema == 'wikipage':\n                self._update_index()\n\n        except Exception as e:\n            self.log(\"Page creation notification error: \", event, e,\n                     type(e), lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_privileges(uid_name='hfos', gid_name='hfos'):\n\n    if os.getuid() != 0:\n        hfoslog(\"Not root, cannot drop privileges\", lvl=warn, emitter='CORE')\n        return\n\n    try:\n        # Get the uid/gid from the name\n        running_uid = pwd.getpwnam(uid_name).pw_uid\n        running_gid = grp.getgrnam(gid_name).gr_gid\n\n        # Remove group privileges\n        os.setgroups([])\n\n        # Try setting the new uid/gid\n        os.setgid(running_gid)\n        os.setuid(running_uid)\n\n        # Ensure a very conservative umask\n        # old_umask = os.umask(22)\n        hfoslog('Privileges dropped', emitter='CORE')\n    except Exception as e:\n        hfoslog('Could not drop privileges:', e, type(e), exc=True, lvl=error, emitter='CORE')", "response": "Attempt to drop privileges and change user to hfos user or group"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef construct_graph(args):\n\n    app = Core(args)\n\n    setup_root(app)\n\n    if args['debug']:\n        from circuits import Debugger\n        hfoslog(\"Starting circuits debugger\", lvl=warn, emitter='GRAPH')\n        dbg = Debugger().register(app)\n        # TODO: Make these configurable from modules, navdata is _very_ noisy\n        # but should not be listed _here_\n        dbg.IgnoreEvents.extend([\n            \"read\", \"_read\", \"write\", \"_write\",\n            \"stream_success\", \"stream_complete\",\n            \"serial_packet\", \"raw_data\", \"stream\",\n            \"navdatapush\", \"referenceframe\",\n            \"updateposition\", \"updatesubscriptions\",\n            \"generatevesseldata\", \"generatenavdata\", \"sensordata\",\n            \"reset_flood_offenders\", \"reset_flood_counters\",  # Flood counters\n            \"task_success\", \"task_done\",  # Thread completion\n            \"keepalive\"  # IRC Gateway\n        ])\n\n    hfoslog(\"Beginning graph assembly.\", emitter='GRAPH')\n\n    if args['drawgraph']:\n        from circuits.tools import graph\n\n        graph(app)\n\n    if args['opengui']:\n        import webbrowser\n        # TODO: Fix up that url:\n        webbrowser.open(\"http://%s:%i/\" % (args['host'], args['port']))\n\n    hfoslog(\"Graph assembly done.\", emitter='GRAPH')\n\n    return app", "response": "Construct a preliminary HFOS application Launcher."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef launch(run=True, **args):\n\n    verbosity['console'] = args['log'] if not args['quiet'] else 100\n    verbosity['global'] = min(args['log'], args['logfileverbosity'])\n    verbosity['file'] = args['logfileverbosity'] if args['dolog'] else 100\n    set_logfile(args['logfilepath'], args['instance'])\n\n    if args['livelog'] is True:\n        from hfos import logger\n        logger.live = True\n\n    hfoslog(\"Running with Python\", sys.version.replace(\"\\n\", \"\"),\n            sys.platform, lvl=debug, emitter='CORE')\n    hfoslog(\"Interpreter executable:\", sys.executable, emitter='CORE')\n    if args['cert'] is not None:\n        hfoslog(\"Warning! Using SSL without nginx is currently not broken!\",\n                lvl=critical, emitter='CORE')\n\n    hfoslog(\"Initializing database access\", emitter='CORE', lvl=debug)\n    initialize(args['dbhost'], args['dbname'], args['instance'])\n\n    server = construct_graph(args)\n    if run and not args['norun']:\n        server.run()\n\n    return server", "response": "Launch the Core\n    component."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ready(self, source):\n\n        from hfos.database import configschemastore\n        configschemastore[self.name] = self.configschema\n\n        self._start_server()\n\n        if not self.insecure:\n            self._drop_privileges()\n\n        self.fireEvent(cli_register_event('components', cli_components))\n        self.fireEvent(cli_register_event('drop_privileges', cli_drop_privileges))\n        self.fireEvent(cli_register_event('reload_db', cli_reload_db))\n        self.fireEvent(cli_register_event('reload', cli_reload))\n        self.fireEvent(cli_register_event('quit', cli_quit))\n        self.fireEvent(cli_register_event('info', cli_info))", "response": "Called when all components have been initialized and the component s configuration schema - store is ready to be used."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trigger_frontend_build(self, event):\n\n        from hfos.database import instance\n        install_frontend(instance=instance,\n                         forcerebuild=event.force,\n                         install=event.install,\n                         development=self.development\n                         )", "response": "Hook to trigger a new frontend build"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_reload(self, event):\n\n        self.log('Reloading all components.')\n\n        self.update_components(forcereload=True)\n        initialize()\n\n        from hfos.debugger import cli_compgraph\n        self.fireEvent(cli_compgraph())", "response": "Experimental call to reload the component tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli_info(self, event):\n\n        self.log('Instance:', self.instance,\n                 'Dev:', self.development,\n                 'Host:', self.host,\n                 'Port:', self.port,\n                 'Insecure:', self.insecure,\n                 'Frontend:', self.frontendtarget)", "response": "Provides information about the running instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_server(self, *args):\n\n        self.log(\"Starting server\", args)\n        secure = self.certificate is not None\n        if secure:\n            self.log(\"Running SSL server with cert:\", self.certificate)\n        else:\n            self.log(\"Running insecure server without SSL. Do not use without SSL proxy in production!\", lvl=warn)\n\n        try:\n            self.server = Server(\n                (self.host, self.port),\n                secure=secure,\n                certfile=self.certificate  # ,\n                # inherit=True\n            ).register(self)\n        except PermissionError:\n            self.log('Could not open (privileged?) port, check '\n                     'permissions!', lvl=critical)", "response": "Start the node local server"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the loadable components of all known entry points.", "response": "def update_components(self, forcereload=False, forcerebuild=False,\n                          forcecopy=True, install=False):\n        \"\"\"Check all known entry points for components. If necessary,\n        manage configuration updates\"\"\"\n\n        # TODO: See if we can pull out major parts of the component handling.\n        # They are also used in the manage tool to instantiate the\n        # component frontend bits.\n\n        self.log(\"Updating components\")\n        components = {}\n\n        if True:  # try:\n\n            from pkg_resources import iter_entry_points\n\n            entry_point_tuple = (\n                iter_entry_points(group='hfos.base', name=None),\n                iter_entry_points(group='hfos.sails', name=None),\n                iter_entry_points(group='hfos.components', name=None)\n            )\n\n            for iterator in entry_point_tuple:\n                for entry_point in iterator:\n                    try:\n                        name = entry_point.name\n                        location = entry_point.dist.location\n                        loaded = entry_point.load()\n\n                        self.log(\"Entry point: \", entry_point,\n                                 name,\n                                 entry_point.resolve(), lvl=verbose)\n\n                        self.log(\"Loaded: \", loaded, lvl=verbose)\n                        comp = {\n                            'package': entry_point.dist.project_name,\n                            'location': location,\n                            'version': str(entry_point.dist.parsed_version),\n                            'description': loaded.__doc__\n                        }\n\n                        components[name] = comp\n                        self.loadable_components[name] = loaded\n\n                        self.log(\"Loaded component:\", comp, lvl=verbose)\n\n                    except Exception as e:\n                        self.log(\"Could not inspect entrypoint: \", e,\n                                 type(e), entry_point, iterator, lvl=error,\n                                 exc=True)\n\n                        # for name in components.keys():\n                        #     try:\n                        #         self.log(self.loadable_components[name])\n                        #         configobject = {\n                        #             'type': 'object',\n                        #             'properties':\n                        # self.loadable_components[name].configprops\n                        #         }\n                        #         ComponentBaseConfigSchema['schema'][\n                        # 'properties'][\n                        #             'settings'][\n                        #             'oneOf'].append(configobject)\n                        #     except (KeyError, AttributeError) as e:\n                        #         self.log('Problematic configuration\n                        # properties in '\n                        #                  'component ', name, exc=True)\n                        #\n                        # schemastore['component'] = ComponentBaseConfigSchema\n\n        # except Exception as e:\n        #    self.log(\"Error: \", e, type(e), lvl=error, exc=True)\n        #    return\n\n        self.log(\"Checking component frontend bits in \", self.frontendroot,\n                 lvl=verbose)\n\n        # pprint(self.config._fields)\n        diff = set(components) ^ set(self.config.components)\n        if diff or forcecopy and self.config.frontendenabled:\n            self.log(\"Old component configuration differs:\", diff, lvl=debug)\n            self.log(self.config.components, components, lvl=verbose)\n            self.config.components = components\n        else:\n            self.log(\"No component configuration change. Proceeding.\")\n\n        if forcereload:\n            self.log(\"Restarting all components.\", lvl=warn)\n            self._instantiate_components(clear=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the frontend is enabled and start the frontend http & websocket", "response": "def _start_frontend(self, restart=False):\n        \"\"\"Check if it is enabled and start the frontend http & websocket\"\"\"\n\n        self.log(self.config, self.config.frontendenabled, lvl=verbose)\n        if self.config.frontendenabled and not self.frontendrunning or restart:\n            self.log(\"Restarting webfrontend services on\",\n                     self.frontendtarget)\n\n            self.static = Static(\"/\",\n                                 docroot=self.frontendtarget).register(\n                self)\n            self.websocket = WebSocketsDispatcher(\"/websocket\").register(self)\n            self.frontendrunning = True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninspect all loadable components and run them", "response": "def _instantiate_components(self, clear=True):\n        \"\"\"Inspect all loadable components and run them\"\"\"\n\n        if clear:\n            import objgraph\n            from copy import deepcopy\n            from circuits.tools import kill\n            from circuits import Component\n            for comp in self.runningcomponents.values():\n                self.log(comp, type(comp), isinstance(comp, Component), pretty=True)\n                kill(comp)\n            # removables = deepcopy(list(self.runningcomponents.keys()))\n            #\n            # for key in removables:\n            #     comp = self.runningcomponents[key]\n            #     self.log(comp)\n            #     comp.unregister()\n            #     comp.stop()\n            #     self.runningcomponents.pop(key)\n            #\n            #     objgraph.show_backrefs([comp],\n            #                            max_depth=5,\n            #                            filter=lambda x: type(x) not in [list, tuple, set],\n            #                            highlight=lambda x: type(x) in [ConfigurableComponent],\n            #                            filename='backref-graph_%s.png' % comp.uniquename)\n            #     del comp\n            # del removables\n            self.runningcomponents = {}\n\n        self.log('Not running blacklisted components: ',\n                 self.component_blacklist,\n                 lvl=debug)\n\n        running = set(self.loadable_components.keys()).difference(\n            self.component_blacklist)\n        self.log('Starting components: ', sorted(running))\n        for name, componentdata in self.loadable_components.items():\n            if name in self.component_blacklist:\n                continue\n            self.log(\"Running component: \", name, lvl=verbose)\n            try:\n                if name in self.runningcomponents:\n                    self.log(\"Component already running: \", name,\n                             lvl=warn)\n                else:\n                    runningcomponent = componentdata()\n                    runningcomponent.register(self)\n                    self.runningcomponents[name] = runningcomponent\n            except Exception as e:\n                self.log(\"Could not register component: \", name, e,\n                         type(e), lvl=error, exc=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the application after startup.", "response": "def started(self, component):\n        \"\"\"Sets up the application after startup.\"\"\"\n\n        self.log(\"Running.\")\n        self.log(\"Started event origin: \", component, lvl=verbose)\n        populate_user_events()\n\n        from hfos.events.system import AuthorizedEvents\n        self.log(len(AuthorizedEvents), \"authorized event sources:\",\n                 list(AuthorizedEvents.keys()), lvl=debug)\n\n        self._instantiate_components()\n        self._start_frontend()\n        self.fire(ready(), \"hfosweb\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user(ctx, username, password):\n\n    ctx.obj['username'] = username\n    ctx.obj['password'] = password", "response": "[GROUP] User management operations"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_user(ctx):\n\n    username, passhash = _get_credentials(ctx.obj['username'],\n                                          ctx.obj['password'],\n                                          ctx.obj['db'])\n\n    if ctx.obj['db'].objectmodels['user'].count({'name': username}) > 0:\n        raise KeyError()\n\n    new_user = ctx.obj['db'].objectmodels['user']({\n        'uuid': str(uuid4()),\n        'created': std_now()\n    })\n\n    new_user.name = username\n    new_user.passhash = passhash\n\n    return new_user", "response": "Internal method to create a normal user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new local user", "response": "def create_user(ctx):\n    \"\"\"Creates a new local user\"\"\"\n\n    try:\n        new_user = _create_user(ctx)\n\n        new_user.save()\n        log(\"Done\")\n    except KeyError:\n        log('User already exists', lvl=warn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_admin(ctx):\n\n    try:\n        admin = _create_user(ctx)\n        admin.roles.append('admin')\n\n        admin.save()\n        log(\"Done\")\n    except KeyError:\n        log('User already exists', lvl=warn)", "response": "Creates a new local user and assigns admin role"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_user(ctx, yes):\n\n    if ctx.obj['username'] is None:\n        username = _ask(\"Please enter username:\")\n    else:\n        username = ctx.obj['username']\n\n    del_user = ctx.obj['db'].objectmodels['user'].find_one({'name': username})\n    if yes or _ask('Confirm deletion', default=False, data_type='bool'):\n        try:\n            del_user.delete()\n            log(\"Done\")\n        except AttributeError:\n            log('User not found', lvl=warn)\n    else:\n        log(\"Cancelled\")", "response": "Delete a local user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_password(ctx):\n\n    username, passhash = _get_credentials(ctx.obj['username'],\n                                          ctx.obj['password'],\n                                          ctx.obj['db'])\n\n    change_user = ctx.obj['db'].objectmodels['user'].find_one({\n        'name': username\n    })\n    if change_user is None:\n        log('No such user', lvl=warn)\n        return\n\n    change_user.passhash = passhash\n    change_user.save()\n\n    log(\"Done\")", "response": "Change password of an existing user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_users(ctx, search, uuid, active):\n\n    users = ctx.obj['db'].objectmodels['user']\n\n    for found_user in users.find():\n        if not search or (search and search in found_user.name):\n            # TODO: Not 2.x compatible\n            print(found_user.name, end=' ' if active or uuid else '\\n')\n            if uuid:\n                print(found_user.uuid, end=' ' if active else '\\n')\n            if active:\n                print(found_user.active)\n\n    log(\"Done\")", "response": "List all locally known users"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling an existing user", "response": "def disable(ctx):\n    \"\"\"Disable an existing user\"\"\"\n\n    if ctx.obj['username'] is None:\n        log('Specify the username with \"iso db user --username ...\"')\n        return\n\n    change_user = ctx.obj['db'].objectmodels['user'].find_one({\n        'name': ctx.obj['username']\n    })\n\n    change_user.active = False\n    change_user.save()\n    log('Done')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable(ctx):\n\n    if ctx.obj['username'] is None:\n        log('Specify the username with \"iso db user --username ...\"')\n        return\n\n    change_user = ctx.obj['db'].objectmodels['user'].find_one({\n        'name': ctx.obj['username']\n    })\n\n    change_user.active = True\n    change_user.save()\n    log('Done')", "response": "Enable an existing user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngranting a role to an existing user", "response": "def add_role(ctx, role):\n    \"\"\"Grant a role to an existing user\"\"\"\n\n    if role is None:\n        log('Specify the role with --role')\n        return\n    if ctx.obj['username'] is None:\n        log('Specify the username with --username')\n        return\n\n    change_user = ctx.obj['db'].objectmodels['user'].find_one({\n        'name': ctx.obj['username']\n    })\n    if role not in change_user.roles:\n        change_user.roles.append(role)\n        change_user.save()\n        log('Done')\n    else:\n        log('User already has that role!', lvl=warn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_expression(testing_framework, expression_dict):\n    expression_sub = get_expression_sub()\n    \n    for expression_type_name, expression_type in expression_dict.items():\n        for name, expression_object in expression_type.items():\n            if 'Matches' in expression_object.keys():\n                for test in expression_object['Matches'].split('|'):\n                    # Substitute and check to make sure that the entire string matches\n                    result = expression_sub(expression_object['Expression'], '', test.strip()) == ''\n                    testing_framework.assertTrue(result, match_error_msg.format(expression_type_name, name, test))\n            if 'Non-Matches' in expression_object.keys():\n                for test in expression_object['Non-Matches'].split('|'):\n                    result = expression_sub(expression_object['Expression'], '', test.strip()) == ''\n                    testing_framework.assertFalse(result, non_match_error_msg.format(expression_type_name, name, test))", "response": "Check that the expression is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_future_tasks(self):\n\n        self.alerts = {}\n        now = std_now()\n\n        for task in objectmodels['task'].find({'alert_time': {'$gt': now}}):\n            self.alerts[task.alert_time] = task\n\n        self.log('Found', len(self.alerts), 'future tasks')", "response": "Assemble a list of future alerts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling incoming data and broadcasts it to all connected tcp clients and udp endpoints", "response": "def read(self, data):\n        \"\"\"Handles incoming raw sensor data and broadcasts it to specified\n        udp servers and connected tcp clients\n        :param data: NMEA raw sentences incoming data\n        \"\"\"\n\n        self.log('Received NMEA data:', data, lvl=debug)\n        # self.log(data, pretty=True)\n\n        if self._tcp_socket is not None and \\\n                len(self._connected_tcp_endpoints) > 0:\n            self.log('Publishing data on tcp server', lvl=debug)\n            for endpoint in self._connected_tcp_endpoints:\n                self.fireEvent(\n                    write(\n                        endpoint,\n                        bytes(data, 'ascii')),\n                    self.channel + '_tcp'\n                )\n\n        if self._udp_socket is not None and \\\n                len(self.config.udp_endpoints) > 0:\n            self.log('Publishing data to udp endpoints', lvl=debug)\n            for endpoint in self.config.udp_endpoints:\n                host, port = endpoint.split(\":\")\n                self.log('Transmitting to', endpoint, lvl=verbose)\n                self.fireEvent(\n                    write(\n                        (host, int(port)),\n                        bytes(data, 'ascii')\n                    ),\n                    self.channel +\n                    '_udp'\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmdmap(xdot):\n    # TODO: Integrate the output into documentation\n\n    from copy import copy\n\n    def print_commands(command, map_output, groups=None, depth=0):\n        if groups is None:\n            groups = []\n        if 'commands' in command.__dict__:\n            if len(groups) > 0:\n                if xdot:\n                    line = \"    %s -> %s [weight=1.0];\\n\" % (groups[-1], command.name)\n                else:\n                    line = \"    \" * (depth - 1) + \"%s %s\\n\" % (groups[-1], command.name)\n                map_output.append(line)\n\n            for item in command.commands.values():\n                subgroups = copy(groups)\n                subgroups.append(command.name)\n                print_commands(item, map_output, subgroups, depth + 1)\n        else:\n            if xdot:\n                line = \"    %s -> %s [weight=%1.1f];\\n\" % (groups[-1], command.name, len(groups))\n            else:\n                line = \"    \" * (len(groups) - 3 + depth) + \"%s %s\\n\" % (groups[-1], command.name)\n            map_output.append(line)\n\n    output = []\n    print_commands(cli, output)\n\n    output = [line.replace(\"cli\", \"isomer\") for line in output]\n\n    if xdot:\n        with open('iso.dot', 'w') as f:\n            f.write('strict digraph {\\n')\n            f.writelines(sorted(output))\n            f.write('}')\n\n        run_process('.', ['xdot', 'iso.dot'])\n    else:\n        print(\"\".join(output))", "response": "Generates a command map for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender a given pystache template with given content", "response": "def format_template(template, content):\n    \"\"\"Render a given pystache template\n    with given content\"\"\"\n\n    import pystache\n    result = u\"\"\n    if True:  # try:\n        result = pystache.render(template, content, string_encoding='utf-8')\n    # except (ValueError, KeyError) as e:\n    #    print(\"Templating error: %s %s\" % (e, type(e)))\n\n    # pprint(result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering a given pystache template file with given content", "response": "def format_template_file(filename, content):\n    \"\"\"Render a given pystache template file with given content\"\"\"\n\n    with open(filename, 'r') as f:\n        template = f.read()\n        if type(template) != str:\n            template = template.decode('utf-8')\n\n    return format_template(template, content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a new file from a given pystache template file and content", "response": "def write_template_file(source, target, content):\n    \"\"\"Write a new file from a given pystache template file and content\"\"\"\n\n    # print(formatTemplateFile(source, content))\n    print(target)\n    data = format_template_file(source, content)\n    with open(target, 'w') as f:\n        for line in data:\n            if type(line) != str:\n                line = line.encode('utf-8')\n            f.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_nginx_service(definition):  # pragma: no cover\n\n    config_file = '/etc/nginx/sites-available/hfos.conf'\n    splitter = \"### SERVICE DEFINITIONS ###\"\n\n    with open(config_file, 'r') as f:\n        old_config = \"\".join(f.readlines())\n\n    pprint(old_config)\n\n    if definition in old_config:\n        print(\"Service definition already inserted\")\n        return\n\n    parts = old_config.split(splitter)\n    print(len(parts))\n    if len(parts) != 3:\n        print(\"Nginx configuration seems to be changed and cannot be \"\n              \"extended automatically anymore!\")\n        pprint(parts)\n        return\n\n    try:\n        with open(config_file, \"w\") as f:\n            f.write(parts[0])\n            f.write(splitter + \"\\n\")\n            f.write(parts[1])\n            for line in definition:\n                f.write(line)\n            f.write(\"\\n    \" + splitter)\n            f.write(parts[2])\n    except Exception as e:\n        print(\"Error during Nginx configuration extension:\", type(e), e)", "response": "Insert a new nginx service definition into the nginx config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rbac(ctx, schema, object_filter, action, role, all_schemata):\n\n    database = ctx.obj['db']\n\n    if schema is None:\n        if all_schemata is False:\n            log('No schema given. Read the RBAC group help', lvl=warn)\n            sys.exit()\n        else:\n            schemata = database.objectmodels.keys()\n    else:\n        schemata = [schema]\n\n    things = []\n\n    if object_filter is None:\n        parsed_filter = {}\n    else:\n        parsed_filter = json.loads(object_filter)\n\n    for schema in schemata:\n        for obj in database.objectmodels[schema].find(parsed_filter):\n            things.append(obj)\n\n    if len(things) == 0:\n        log('No objects matched the criteria.', lvl=warn)\n        sys.exit()\n\n    ctx.obj['objects'] = things\n    ctx.obj['action'] = action\n    ctx.obj['role'] = role", "response": "Manage the RBAC group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_action_role(ctx):\n\n    objects = ctx.obj['objects']\n    action = ctx.obj['action']\n    role = ctx.obj['role']\n\n    if action is None or role is None:\n        log('You need to specify an action or role to the RBAC command group for this to work.', lvl=warn)\n        return\n\n    for item in objects:\n        if role not in item.perms[action]:\n            item.perms[action].append(role)\n            item.save()\n\n    log(\"Done\")", "response": "Adds a role to an action on objects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_action_role(ctx):\n\n    objects = ctx.obj['objects']\n    action = ctx.obj['action']\n    role = ctx.obj['role']\n\n    if action is None or role is None:\n        log('You need to specify an action or role to the RBAC command group for this to work.', lvl=warn)\n        return\n\n    for item in objects:\n        if role in item.perms[action]:\n            item.perms[action].remove(role)\n            item.save()\n\n    log(\"Done\")", "response": "Deletes a role from an action on objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the ownership of objects", "response": "def change_owner(ctx, owner, uuid):\n    \"\"\"Changes the ownership of objects\"\"\"\n\n    objects = ctx.obj['objects']\n    database = ctx.obj['db']\n\n    if uuid is True:\n        owner_filter = {'uuid': owner}\n    else:\n        owner_filter = {'name': owner}\n\n    owner = database.objectmodels['user'].find_one(owner_filter)\n    if owner is None:\n        log('User unknown.', lvl=error)\n        return\n\n    for item in objects:\n        item.owner = owner.uuid\n        item.save()\n\n    log('Done')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing configuration list requests", "response": "def getlist(self, event):\n        \"\"\"Processes configuration list requests\n\n        :param event:\n        \"\"\"\n\n        try:\n\n            componentlist = model_factory(Schema).find({})\n            data = []\n            for comp in componentlist:\n                try:\n                    data.append({\n                        'name': comp.name,\n                        'uuid': comp.uuid,\n                        'class': comp.componentclass,\n                        'active': comp.active\n                    })\n                except AttributeError:\n                    self.log('Bad component without component class encountered:', lvl=warn)\n                    self.log(comp.serializablefields(), pretty=True, lvl=warn)\n\n            data = sorted(data, key=lambda x: x['name'])\n\n            response = {\n                'component': 'hfos.ui.configurator',\n                'action': 'getlist',\n                'data': data\n            }\n            self.fireEvent(send(event.client.uuid, response))\n            return\n        except Exception as e:\n            self.log(\"List error: \", e, type(e), lvl=error, exc=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore a given configuration", "response": "def put(self, event):\n        \"\"\"Store a given configuration\"\"\"\n\n        self.log(\"Configuration put request \",\n                 event.user)\n\n        try:\n            component = model_factory(Schema).find_one({\n                'uuid': event.data['uuid']\n            })\n\n            component.update(event.data)\n            component.save()\n\n            response = {\n                'component': 'hfos.ui.configurator',\n                'action': 'put',\n                'data': True\n            }\n            self.log('Updated component configuration:',\n                     component.name)\n\n            self.fireEvent(reload_configuration(component.name))\n        except (KeyError, ValueError, ValidationError, PermissionError) as e:\n            response = {\n                'component': 'hfos.ui.configurator',\n                'action': 'put',\n                'data': False\n            }\n            self.log('Storing component configuration failed: ',\n                     type(e), e, exc=True, lvl=error)\n\n        self.fireEvent(send(event.client.uuid, response))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, event):\n\n        try:\n            comp = event.data['uuid']\n        except KeyError:\n            comp = None\n\n        if not comp:\n            self.log('Invalid get request without schema or component',\n                     lvl=error)\n            return\n\n        self.log(\"Config data get  request for \", event.data, \"from\",\n                 event.user)\n\n        component = model_factory(Schema).find_one({\n            'uuid': comp\n        })\n        response = {\n            'component': 'hfos.ui.configurator',\n            'action': 'get',\n            'data': component.serializablefields()\n        }\n        self.fireEvent(send(event.client.uuid, response))", "response": "Get a stored configuration"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rec(self):\n\n        try:\n            self._snapshot()\n        except Exception as e:\n            self.log(\"Timer error: \", e, type(e), lvl=error)", "response": "Records a single snapshot"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntoggles the camera system recording state", "response": "def _toggle_filming(self):\n        \"\"\"Toggles the camera system recording state\"\"\"\n\n        if self._filming:\n            self.log(\"Stopping operation\")\n            self._filming = False\n            self.timer.stop()\n        else:\n            self.log(\"Starting operation\")\n            self._filming = True\n            self.timer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when a client has disconnected.", "response": "def client_disconnect(self, event):\n        \"\"\"\n        A client has disconnected, update possible subscriptions accordingly.\n\n        :param event:\n        \"\"\"\n        self.log(\"Removing disconnected client from subscriptions\", lvl=debug)\n        client_uuid = event.clientuuid\n        self._unsubscribe(client_uuid)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, event):\n\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        object_filter = self._get_filter(event)\n\n        if 'subscribe' in data:\n            do_subscribe = data['subscribe'] is True\n        else:\n            do_subscribe = False\n\n        try:\n            uuid = str(data['uuid'])\n        except (KeyError, TypeError):\n            uuid = \"\"\n\n        opts = schemastore[schema].get('options', {})\n        hidden = opts.get('hidden', [])\n\n        if object_filter == {}:\n            if uuid == \"\":\n                self.log('Object with no filter/uuid requested:', schema,\n                         data,\n                         lvl=warn)\n                return\n            object_filter = {'uuid': uuid}\n\n        storage_object = None\n        storage_object = objectmodels[schema].find_one(object_filter)\n\n        if not storage_object:\n            self._cancel_by_error(event, uuid + '(' + str(object_filter) + ') of ' + schema +\n                                  ' unavailable')\n            return\n\n        if storage_object:\n            self.log(\"Object found, checking permissions: \", data, lvl=verbose)\n\n            if not self._check_permissions(user, 'read',\n                                           storage_object):\n                self._cancel_by_permission(schema, data, event)\n                return\n\n            for field in hidden:\n                storage_object._fields.pop(field, None)\n\n            if do_subscribe and uuid != \"\":\n                self._add_subscription(uuid, event)\n\n            result = {\n                'component': 'hfos.events.objectmanager',\n                'action': 'get',\n                'data': {\n                    'schema': schema,\n                    'uuid': uuid,\n                    'object': storage_object.serializablefields()\n                }\n            }\n            self._respond(None, result, event)", "response": "Get a specific object from the object manager"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch for an object in the database", "response": "def search(self, event):\n        \"\"\"Search for an object\"\"\"\n\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        # object_filter['$text'] = {'$search': str(data['search'])}\n        if data.get('fulltext', False) is True:\n            object_filter = {\n                'name': {\n                    '$regex': str(data['search']),\n                    '$options': '$i'\n                }\n            }\n        else:\n            if isinstance(data['search'], dict):\n                object_filter = data['search']\n            else:\n                object_filter = {}\n\n        if 'fields' in data:\n            fields = data['fields']\n        else:\n            fields = []\n\n        skip = data.get('skip', 0)\n        limit = data.get('limit', 0)\n        sort = data.get('sort', None)\n        # page = data.get('page', 0)\n        # count = data.get('count', 0)\n        #\n        # if page > 0 and count > 0:\n        #     skip = page * count\n        #     limit = count\n\n        if 'subscribe' in data:\n            self.log('Subscription:', data['subscribe'], lvl=verbose)\n            do_subscribe = data['subscribe'] is True\n        else:\n            do_subscribe = False\n\n        object_list = []\n\n        size = objectmodels[schema].count(object_filter)\n\n        if size > WARNSIZE and (limit > 0 and limit > WARNSIZE):\n            self.log(\"Getting a very long (\", size, \") list of items for \", schema,\n                     lvl=warn)\n\n        opts = schemastore[schema].get('options', {})\n        hidden = opts.get('hidden', [])\n\n        self.log(\"object_filter: \", object_filter, ' Schema: ', schema,\n                 \"Fields: \", fields,\n                 lvl=verbose)\n\n        options = {}\n        if skip > 0:\n            options['skip'] = skip\n        if limit > 0:\n            options['limit'] = limit\n        if sort is not None:\n            options['sort'] = []\n            for item in sort:\n                key = item[0]\n                direction = item[1]\n                direction = ASCENDING if direction == 'asc' else DESCENDING\n                options['sort'].append([key, direction])\n\n        cursor = objectmodels[schema].find(object_filter, **options)\n\n        for item in cursor:\n            if not self._check_permissions(user, 'list', item):\n                continue\n            self.log(\"Search found item: \", item, lvl=verbose)\n\n            try:\n                list_item = {'uuid': item.uuid}\n                if fields in ('*', ['*']):\n                    item_fields = item.serializablefields()\n                    for field in hidden:\n                        item_fields.pop(field, None)\n                    object_list.append(item_fields)\n                else:\n                    if 'name' in item._fields:\n                        list_item['name'] = item.name\n\n                    for field in fields:\n                        if field in item._fields and field not in hidden:\n                            list_item[field] = item._fields[field]\n                        else:\n                            list_item[field] = None\n\n                    object_list.append(list_item)\n\n                if do_subscribe:\n                    self._add_subscription(item.uuid, event)\n            except Exception as e:\n                self.log(\"Faulty object or field: \", e, type(e),\n                         item._fields, fields, lvl=error, exc=True)\n        # self.log(\"Generated object search list: \", object_list)\n\n        result = {\n            'component': 'hfos.events.objectmanager',\n            'action': 'search',\n            'data': {\n                'schema': schema,\n                'list': object_list,\n                'size': size\n            }\n        }\n\n        self._respond(None, result, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef objectlist(self, event):\n\n        self.log('LEGACY LIST FUNCTION CALLED!', lvl=warn)\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        object_filter = self._get_filter(event)\n        self.log('Object list for', schema, 'requested from',\n                 user.account.name, lvl=debug)\n\n        if 'fields' in data:\n            fields = data['fields']\n        else:\n            fields = []\n\n        object_list = []\n\n        opts = schemastore[schema].get('options', {})\n        hidden = opts.get('hidden', [])\n\n        if objectmodels[schema].count(object_filter) > WARNSIZE:\n            self.log(\"Getting a very long list of items for \", schema,\n                     lvl=warn)\n\n        try:\n            for item in objectmodels[schema].find(object_filter):\n                try:\n                    if not self._check_permissions(user, 'list', item):\n                        continue\n                    if fields in ('*', ['*']):\n                        item_fields = item.serializablefields()\n                        for field in hidden:\n                            item_fields.pop(field, None)\n                        object_list.append(item_fields)\n                    else:\n                        list_item = {'uuid': item.uuid}\n\n                        if 'name' in item._fields:\n                            list_item['name'] = item._fields['name']\n\n                        for field in fields:\n                            if field in item._fields and field not in hidden:\n                                list_item[field] = item._fields[field]\n                            else:\n                                list_item[field] = None\n\n                        object_list.append(list_item)\n                except Exception as e:\n                    self.log(\"Faulty object or field: \", e, type(e),\n                             item._fields, fields, lvl=error, exc=True)\n        except ValidationError as e:\n            self.log('Invalid object in database encountered!', e, exc=True,\n                     lvl=warn)\n        # self.log(\"Generated object list: \", object_list)\n\n        result = {\n            'component': 'hfos.events.objectmanager',\n            'action': 'getlist',\n            'data': {\n                'schema': schema,\n                'list': object_list\n            }\n        }\n\n        self._respond(None, result, event)", "response": "Get a list of objects for a given event"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change(self, event):\n\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        try:\n            uuid = data['uuid']\n            change = data['change']\n            field = change['field']\n            new_data = change['value']\n        except KeyError as e:\n            self.log(\"Update request with missing arguments!\", data, e,\n                     lvl=critical)\n            self._cancel_by_error(event, 'missing_args')\n            return\n\n        storage_object = None\n\n        try:\n            storage_object = objectmodels[schema].find_one({'uuid': uuid})\n        except Exception as e:\n            self.log('Change for unknown object requested:', schema, data, lvl=warn)\n\n        if storage_object is None:\n            self._cancel_by_error(event, 'not_found')\n            return\n\n        if not self._check_permissions(user, 'write', storage_object):\n            self._cancel_by_permission(schema, data, event)\n            return\n\n        self.log(\"Changing object:\", storage_object._fields, lvl=debug)\n        storage_object._fields[field] = new_data\n\n        self.log(\"Storing object:\", storage_object._fields, lvl=debug)\n        try:\n            storage_object.validate()\n        except ValidationError:\n            self.log(\"Validation of changed object failed!\",\n                     storage_object, lvl=warn)\n            self._cancel_by_error(event, 'invalid_object')\n            return\n\n        storage_object.save()\n\n        self.log(\"Object stored.\")\n\n        result = {\n            'component': 'hfos.events.objectmanager',\n            'action': 'change',\n            'data': {\n                'schema': schema,\n                'uuid': uuid\n            }\n        }\n\n        self._respond(None, result, event)", "response": "Change an existing object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self, event):\n\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        try:\n            clientobject = data['obj']\n            uuid = clientobject['uuid']\n        except KeyError as e:\n            self.log(\"Put request with missing arguments!\", e, data,\n                     lvl=critical)\n            return\n\n        try:\n            model = objectmodels[schema]\n            created = False\n            storage_object = None\n\n            if uuid != 'create':\n                storage_object = model.find_one({'uuid': uuid})\n            if uuid == 'create' or model.count({'uuid': uuid}) == 0:\n                if uuid == 'create':\n                    uuid = str(uuid4())\n                created = True\n                clientobject['uuid'] = uuid\n                clientobject['owner'] = user.uuid\n                storage_object = model(clientobject)\n                if not self._check_create_permission(user, schema):\n                    self._cancel_by_permission(schema, data, event)\n                    return\n\n            if storage_object is not None:\n                if not self._check_permissions(user, 'write', storage_object):\n                    self._cancel_by_permission(schema, data, event)\n                    return\n\n                self.log(\"Updating object:\", storage_object._fields, lvl=debug)\n                storage_object.update(clientobject)\n\n            else:\n                storage_object = model(clientobject)\n                if not self._check_permissions(user, 'write', storage_object):\n                    self._cancel_by_permission(schema, data, event)\n                    return\n\n                self.log(\"Storing object:\", storage_object._fields, lvl=debug)\n                try:\n                    storage_object.validate()\n                except ValidationError:\n                    self.log(\"Validation of new object failed!\", clientobject,\n                             lvl=warn)\n\n            storage_object.save()\n\n            self.log(\"Object %s stored.\" % schema)\n\n            # Notify backend listeners\n\n            if created:\n                notification = objectcreation(\n                    storage_object.uuid, schema, client\n                )\n            else:\n                notification = objectchange(\n                    storage_object.uuid, schema, client\n                )\n\n            self._update_subscribers(schema, storage_object)\n\n            result = {\n                'component': 'hfos.events.objectmanager',\n                'action': 'put',\n                'data': {\n                    'schema': schema,\n                    'object': storage_object.serializablefields(),\n                    'uuid': storage_object.uuid,\n                }\n            }\n\n            self._respond(notification, result, event)\n\n        except Exception as e:\n            self.log(\"Error during object storage:\", e, type(e), data,\n                     lvl=error, exc=True, pretty=True)", "response": "Put an object in the backend"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an existing object", "response": "def delete(self, event):\n        \"\"\"Delete an existing object\"\"\"\n\n        try:\n            data, schema, user, client = self._get_args(event)\n        except AttributeError:\n            return\n\n        try:\n            uuids = data['uuid']\n\n            if not isinstance(uuids, list):\n                uuids = [uuids]\n\n            if schema not in objectmodels.keys():\n                self.log(\"Unknown schema encountered: \", schema, lvl=warn)\n                return\n\n            for uuid in uuids:\n                self.log(\"Looking for object to be deleted:\", uuid, lvl=debug)\n                storage_object = objectmodels[schema].find_one({'uuid': uuid})\n\n                if not storage_object:\n                    self._cancel_by_error(event, 'not found')\n                    return\n\n                self.log(\"Found object.\", lvl=debug)\n\n                if not self._check_permissions(user, 'write', storage_object):\n                    self._cancel_by_permission(schema, data, event)\n                    return\n\n                # self.log(\"Fields:\", storage_object._fields, \"\\n\\n\\n\",\n                #         storage_object.__dict__)\n\n                storage_object.delete()\n\n                self.log(\"Deleted. Preparing notification.\", lvl=debug)\n                notification = objectdeletion(uuid, schema, client)\n\n                if uuid in self.subscriptions:\n                    deletion = {\n                        'component': 'hfos.events.objectmanager',\n                        'action': 'deletion',\n                        'data': {\n                            'schema': schema,\n                            'uuid': uuid,\n                        }\n                    }\n                    for recipient in self.subscriptions[uuid]:\n                        self.fireEvent(send(recipient, deletion))\n\n                    del (self.subscriptions[uuid])\n\n                result = {\n                    'component': 'hfos.events.objectmanager',\n                    'action': 'delete',\n                    'data': {\n                        'schema': schema,\n                        'uuid': storage_object.uuid\n                    }\n                }\n\n                self._respond(notification, result, event)\n\n        except Exception as e:\n            self.log(\"Error during delete request: \", e, type(e),\n                     lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes to an object s future changes", "response": "def subscribe(self, event):\n        \"\"\"Subscribe to an object's future changes\"\"\"\n        uuids = event.data\n\n        if not isinstance(uuids, list):\n            uuids = [uuids]\n\n        subscribed = []\n        for uuid in uuids:\n            try:\n                self._add_subscription(uuid, event)\n                subscribed.append(uuid)\n            except KeyError:\n                continue\n\n        result = {\n            'component': 'hfos.events.objectmanager',\n            'action': 'subscribe',\n            'data': {\n                'uuid': subscribed, 'success': True\n            }\n        }\n        self._respond(None, result, event)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_subscriptions(self, event):\n\n        # self.log(\"Event: '%s'\" % event.__dict__)\n        try:\n            self._update_subscribers(event.schema, event.data)\n\n        except Exception as e:\n            self.log(\"Error during subscription update: \", type(e), e,\n                     exc=True)", "response": "Update the subscriptions for the current object models"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprojects Importer for Github Repository Issues", "response": "def GithubImporter(ctx, repository, all, owner, project, ignore_labels, no_tags, username, password):\n    \"\"\"Project Importer for Github Repository Issues\n\n    Argument REPOSITORY must be given as 'username/repository'\n\n    Owner and project have to be UUIDs\n    \"\"\"\n\n    db = ctx.obj['db']\n\n    if project is not None:\n        project_obj = db.objectmodels['project'].find_one({'uuid': project})\n\n        if project_obj is None:\n            project_obj = db.objectmodels['project'].find_one({'name': project})\n\n        if project_obj is None:\n            log('Project not found.', lvl=error)\n            return\n        else:\n            project_uuid = project_obj.uuid\n    else:\n        project_uuid = None\n\n    tags = {}\n    if not ignore_labels:\n        for tag in db.objectmodels['tag'].find():\n            tags[tag.name.lower()] = tag\n\n    def write_issue(issue):\n        \"\"\"Stores a single github issue as task\"\"\"\n\n        if 'pull_request' not in issue:\n\n            issue_tags = []\n            if not ignore_labels:\n                for l in issue['labels']:\n                    if l['name'].lower() not in tags:\n                        initial = {\n                            'uuid': std_uuid(),\n                            'name': l['name']\n                        }\n                        new_tag = db.objectmodels['tag'](initial)\n                        new_tag.save()\n\n                        tags[new_tag.name] = new_tag\n\n                        issue_tags.append(new_tag.uuid)\n                    else:\n                        issue_tags.append(tags[l['name'].lower()].uuid)\n\n            date = issue['created_at'].split('T')[0]\n\n            initial = {\n                'uuid': std_uuid(),\n                'name': issue['title'],\n                'notes': str(issue['state']) + \"\\n\\n\" + issue['html_url'],\n                'created': date,\n                'project': project_uuid\n            }\n            if len(issue_tags) > 0:\n                initial['tags'] = issue_tags\n\n            task = db.objectmodels['task'](initial)\n            task.save()\n        else:\n            log('Pull request issue:', issue, lvl=debug)\n\n    def write_issues(r):\n        \"\"\"Parses JSON response and stores all issues.\"\"\"\n\n        if r.status_code != 200:\n            raise Exception(r.status_code)\n        for issue in r.json():\n            write_issue(issue)\n\n    def get_issues(name, state, auth):\n        \"\"\"Requests issues from GitHub API\"\"\"\n\n        url = 'https://api.github.com/repos/{}/issues?state={}'.format(name, state)\n        r = requests.get(url, auth=auth)\n\n        write_issues(r)\n\n        # Multiple requests are required if response is paged\n        if 'link' in r.headers:\n            pages = {rel[6:-1]: url[url.index('<') + 1:-1] for url, rel in\n                     (link.split(';') for link in\n                      r.headers['link'].split(','))}\n            while 'last' in pages and 'next' in pages:\n                pages = {rel[6:-1]: url[url.index('<') + 1:-1] for url, rel in\n                         (link.split(';') for link in\n                          r.headers['link'].split(','))}\n                r = requests.get(pages['next'], auth=auth)\n                write_issues(r)\n                if pages['next'] == pages['last']:\n                    break\n\n    # username = input(\"Username for 'https://github.com': \")\n    # password = getpass(\"Password for 'https://{}@github.com': \".format(username))\n    # auth = (username, password)\n    # for repository in args.repositories:\n    #    get_issues(repository)\n\n    if all:\n        state = 'all'\n    else:\n        state = 'open'\n\n    auth = (username, password)\n\n    get_issues(repository, state, auth)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef all_languages():\n\n    rv = []\n\n    for lang in os.listdir(localedir):\n        base = lang.split('_')[0].split('.')[0].split('@')[0]\n        if 2 <= len(base) <= 3 and all(c.islower() for c in base):\n            if base != 'all':\n                rv.append(lang)\n    rv.sort()\n    rv.append('en')\n    l10n_log('Registered languages:', rv, lvl=verbose)\n\n    return rv", "response": "Compile a list of all available language translations"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a descriptive title for all languages", "response": "def language_token_to_name(languages):\n    \"\"\"Get a descriptive title for all languages\"\"\"\n\n    result = {}\n\n    with open(os.path.join(localedir, 'languages.json'), 'r') as f:\n        language_lookup = json.load(f)\n\n    for language in languages:\n        language = language.lower()\n        try:\n            result[language] = language_lookup[language]\n        except KeyError:\n            l10n_log('Language token lookup not found:', language, lvl=warn)\n            result[language] = language\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndebugging function to print all messages in a domain", "response": "def print_messages(domain, msg):\n    \"\"\"Debugging function to print all message language variants\"\"\"\n\n    domain = Domain(domain)\n    for lang in all_languages():\n        print(lang, ':', domain.get(lang, msg))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef i18n(msg, event=None, lang='en', domain='backend'):\n\n    if event is not None:\n        language = event.client.language\n    else:\n        language = lang\n\n    domain = Domain(domain)\n    return domain.get(language, msg)", "response": "Gettext function wrapper to return a message in a specified language by domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a cryptographically strong hash with this nodes salt added.", "response": "def std_hash(word, salt):\n    \"\"\"Generates a cryptographically strong (sha512) hash with this nodes\n    salt added.\"\"\"\n\n    try:\n        password = word.encode('utf-8')\n    except UnicodeDecodeError:\n        password = word\n\n    word_hash = sha512(password)\n    word_hash.update(salt)\n    hex_hash = word_hash.hexdigest()\n\n    return hex_hash"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef std_human_uid(kind=None):\n\n    kind_list = alphabet\n\n    if kind == 'animal':\n        kind_list = animals\n    elif kind == 'place':\n        kind_list = places\n\n    name = \"{color} {adjective} {kind} of {attribute}\".format(\n        color=choice(colors),\n        adjective=choice(adjectives),\n        kind=choice(kind_list),\n        attribute=choice(attributes)\n    )\n\n    return name", "response": "Return a random generated human - friendly phrase as low - probability unique id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef std_table(rows):\n\n    result = \"\"\n    if len(rows) > 1:\n        headers = rows[0]._fields\n        lens = []\n        for i in range(len(rows[0])):\n            lens.append(len(max([x[i] for x in rows] + [headers[i]],\n                                key=lambda x: len(str(x)))))\n        formats = []\n        hformats = []\n        for i in range(len(rows[0])):\n            if isinstance(rows[0][i], int):\n                formats.append(\"%%%dd\" % lens[i])\n            else:\n                formats.append(\"%%-%ds\" % lens[i])\n            hformats.append(\"%%-%ds\" % lens[i])\n        pattern = \" | \".join(formats)\n        hpattern = \" | \".join(hformats)\n        separator = \"-+-\".join(['-' * n for n in lens])\n        result += hpattern % tuple(headers) + \" \\n\"\n        result += separator + \"\\n\"\n\n        for line in rows:\n            result += pattern % tuple(t for t in line) + \"\\n\"\n    elif len(rows) == 1:\n        row = rows[0]\n        hwidth = len(max(row._fields, key=lambda x: len(x)))\n        for i in range(len(row)):\n            result += \"%*s = %s\" % (hwidth, row._fields[i], row[i]) + \"\\n\"\n\n    return result", "response": "Return a formatted table of given rows"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef std_salt(length=16, lowercase=True):\n\n    alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    if lowercase is True:\n        alphabet += \"abcdefghijklmnopqrstuvwxyz\"\n\n    chars = []\n    for i in range(length):\n        chars.append(choice(alphabet))\n\n    return \"\".join(chars)", "response": "Generates a cryptographically sane salt of length"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a translation for a given language", "response": "def _get_translation(self, lang):\n        \"\"\"Add a new translation language to the live gettext translator\"\"\"\n\n        try:\n            return self._translations[lang]\n        except KeyError:\n            # The fact that `fallback=True` is not the default is a serious design flaw.\n            rv = self._translations[lang] = gettext.translation(self._domain, localedir=localedir, languages=[lang],\n                                                                fallback=True)\n            return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an Event Handler This decorator can be applied to methods of classes derived from :class:`circuits.core.components.BaseComponent`. It marks the method as a handler for the events passed as arguments to the ``@handler`` decorator. The events are specified by their name. The decorated method's arguments must match the arguments passed to the :class:`circuits.core.events.Event` on creation. Optionally, the method may have an additional first argument named *event*. If declared, the event object that caused the handler to be invoked is assigned to it. By default, the handler is invoked by the component's root :class:`~.manager.Manager` for events that are propagated on the channel determined by the BaseComponent's *channel* attribute. This may be overridden by specifying a different channel as a keyword parameter of the decorator (``channel=...``). Keyword argument ``priority`` influences the order in which handlers for a specific event are invoked. The higher the priority, the earlier the handler is executed. If you want to override a handler defined in a base class of your component, you must specify ``override=True``, else your method becomes an additional handler for the event. **Return value** Normally, the results returned by the handlers for an event are simply collected in the :class:`circuits.core.events.Event`'s :attr:`value` attribute. As a special case, a handler may return a :class:`types.GeneratorType`. This signals to the dispatcher that the handler isn't ready to deliver a result yet. Rather, it has interrupted it's execution with a ``yield None`` statement, thus preserving its current execution state. The dispatcher saves the returned generator object as a task. All tasks are reexamined (i.e. their :meth:`next()` method is invoked) when the pending events have been executed. This feature avoids an unnecessarily complicated chaining of event handlers. Imagine a handler A that needs the results from firing an event E in order to complete. Then without this feature, the final action of A would be to fire event E, and another handler for an event ``SuccessE`` would be required to complete handler A's operation, now having the result from invoking E available (actually it's even a bit more complicated). Using this \"suspend\" feature, the handler simply fires event E and then yields ``None`` until e.g. it finds a result in E's :attr:`value` attribute. For the simplest scenario, there even is a utility method :meth:`circuits.core.manager.Manager.callEvent` that combines firing and waiting.", "response": "def handler(*names, **kwargs):\n    \"\"\"Creates an Event Handler\n\n    This decorator can be applied to methods of classes derived from\n    :class:`circuits.core.components.BaseComponent`. It marks the method as a\n    handler for the events passed as arguments to the ``@handler`` decorator.\n    The events are specified by their name.\n\n    The decorated method's arguments must match the arguments passed to the\n    :class:`circuits.core.events.Event` on creation. Optionally, the\n    method may have an additional first argument named *event*. If declared,\n    the event object that caused the handler to be invoked is assigned to it.\n\n    By default, the handler is invoked by the component's root\n    :class:`~.manager.Manager` for events that are propagated on the channel\n    determined by the BaseComponent's *channel* attribute.\n    This may be overridden by specifying a different channel as a keyword\n    parameter of the decorator (``channel=...``).\n\n    Keyword argument ``priority`` influences the order in which handlers\n    for a specific event are invoked. The higher the priority, the earlier\n    the handler is executed.\n\n    If you want to override a handler defined in a base class of your\n    component, you must specify ``override=True``, else your method becomes\n    an additional handler for the event.\n\n    **Return value**\n\n    Normally, the results returned by the handlers for an event are simply\n    collected in the :class:`circuits.core.events.Event`'s :attr:`value`\n    attribute. As a special case, a handler may return a\n    :class:`types.GeneratorType`. This signals to the dispatcher that the\n    handler isn't ready to deliver a result yet.\n    Rather, it has interrupted it's execution with a ``yield None``\n    statement, thus preserving its current execution state.\n\n    The dispatcher saves the returned generator object as a task.\n    All tasks are reexamined (i.e. their :meth:`next()` method is invoked)\n    when the pending events have been executed.\n\n    This feature avoids an unnecessarily complicated chaining of event\n    handlers. Imagine a handler A that needs the results from firing an\n    event E in order to complete. Then without this feature, the final\n    action of A would be to fire event E, and another handler for\n    an event ``SuccessE`` would be required to complete handler A's\n    operation, now having the result from invoking E available\n    (actually it's even a bit more complicated).\n\n    Using this \"suspend\" feature, the handler simply fires event E and\n    then yields ``None`` until e.g. it finds a result in E's :attr:`value`\n    attribute. For the simplest scenario, there even is a utility\n    method :meth:`circuits.core.manager.Manager.callEvent` that combines\n    firing and waiting.\n    \"\"\"\n\n    def wrapper(f):\n        if names and isinstance(names[0], bool) and not names[0]:\n            f.handler = False\n            return f\n\n        if len(names) > 0 and inspect.isclass(names[0]) and \\\n                issubclass(names[0], hfosEvent):\n            f.names = (str(names[0].realname()),)\n        else:\n            f.names = names\n\n        f.handler = True\n\n        f.priority = kwargs.get(\"priority\", 0)\n        f.channel = kwargs.get(\"channel\", None)\n        f.override = kwargs.get(\"override\", False)\n\n        args = inspect.getargspec(f)[0]\n\n        if args and args[0] == \"self\":\n            del args[0]\n        f.event = getattr(f, \"event\", bool(args and args[0] == \"event\"))\n\n        return f\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(self, *args, **kwargs):\n\n        func = inspect.currentframe().f_back.f_code\n        # Dump the message + the name of this function to the log.\n\n        if 'exc' in kwargs and kwargs['exc'] is True:\n            exc_type, exc_obj, exc_tb = exc_info()\n            line_no = exc_tb.tb_lineno\n            # print('EXCEPTION DATA:', line_no, exc_type, exc_obj, exc_tb)\n            args += traceback.extract_tb(exc_tb),\n        else:\n            line_no = func.co_firstlineno\n\n        sourceloc = \"[%.10s@%s:%i]\" % (\n            func.co_name,\n            func.co_filename,\n            line_no\n        )\n        hfoslog(sourceloc=sourceloc, emitter=self.uniquename, *args, **kwargs)", "response": "Log a statement from this component to the log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a configurable component in the configuration schema store", "response": "def register(self, *args):\n        \"\"\"Register a configurable component in the configuration schema\n        store\"\"\"\n\n        super(ConfigurableMeta, self).register(*args)\n        from hfos.database import configschemastore\n        # self.log('ADDING SCHEMA:')\n        # pprint(self.configschema)\n        configschemastore[self.name] = self.configschema"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unregister(self):\n        self.names.remove(self.uniquename)\n        super(ConfigurableMeta, self).unregister()", "response": "Removes the unique name from the system unique name list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_config(self):\n\n        try:\n            self.config = self.componentmodel.find_one(\n                {'name': self.uniquename})\n        except ServerSelectionTimeoutError:  # pragma: no cover\n            self.log(\"No database access! Check if mongodb is running \"\n                     \"correctly.\", lvl=critical)\n        if self.config:\n            self.log(\"Configuration read.\", lvl=verbose)\n        else:\n            self.log(\"No configuration found.\", lvl=warn)", "response": "Read this component s configuration from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_config(self):\n\n        if not self.config:\n            self.log(\"Unable to write non existing configuration\", lvl=error)\n            return\n\n        self.config.save()\n        self.log(\"Configuration stored.\")", "response": "Write this component s configuration back to the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_config(self, config=None):\n        if not config:\n            config = {}\n\n        try:\n            # pprint(self.configschema)\n            self.config = self.componentmodel(config)\n            # self.log(\"Config schema:\", lvl=critical)\n            # pprint(self.config.__dict__)\n\n            # pprint(self.config._fields)\n\n            try:\n                name = self.config.name\n                self.log(\"Name set to: \", name, lvl=verbose)\n            except (AttributeError, KeyError):  # pragma: no cover\n                self.log(\"Has no name.\", lvl=verbose)\n\n            try:\n                self.config.name = self.uniquename\n            except (AttributeError, KeyError) as e:  # pragma: no cover\n                self.log(\"Cannot set component name for configuration: \", e,\n                         type(e), self.name, exc=True, lvl=critical)\n\n            try:\n                uuid = self.config.uuid\n                self.log(\"UUID set to: \", uuid, lvl=verbose)\n            except (AttributeError, KeyError):\n                self.log(\"Has no UUID\", lvl=verbose)\n                self.config.uuid = str(uuid4())\n\n            try:\n                notes = self.config.notes\n                self.log(\"Notes set to: \", notes, lvl=verbose)\n            except (AttributeError, KeyError):\n                self.log(\"Has no notes, trying docstring\", lvl=verbose)\n\n                notes = self.__doc__\n                if notes is None:\n                    notes = \"No notes.\"\n                else:\n                    notes = notes.lstrip().rstrip()\n                    self.log(notes)\n                self.config.notes = notes\n\n            try:\n                componentclass = self.config.componentclass\n                self.log(\"Componentclass set to: \", componentclass,\n                         lvl=verbose)\n            except (AttributeError, KeyError):\n                self.log(\"Has no component class\", lvl=verbose)\n                self.config.componentclass = self.name\n\n        except ValidationError as e:\n            self.log(\"Not setting invalid component configuration: \", e,\n                     type(e), exc=True, lvl=error)", "response": "Set this component s initial configuration"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reload_configuration(self, event):\n\n        if event.target == self.uniquename:\n            self.log('Reloading configuration')\n            self._read_config()", "response": "Event triggered configuration reload"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling out the template information", "response": "def _augment_info(info):\n    \"\"\"Fill out the template information\"\"\"\n\n    info['description_header'] = \"=\" * len(info['description'])\n    info['component_name'] = info['plugin_name'].capitalize()\n    info['year'] = time.localtime().tm_year\n    info['license_longtext'] = ''\n\n    info['keyword_list'] = u\"\"\n    for keyword in info['keywords'].split(\" \"):\n        print(keyword)\n        info['keyword_list'] += u\"\\'\" + str(keyword) + u\"\\', \"\n    print(info['keyword_list'])\n    if len(info['keyword_list']) > 0:\n        # strip last comma\n        info['keyword_list'] = info['keyword_list'][:-2]\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a module from templates and user supplied information", "response": "def _construct_module(info, target):\n    \"\"\"Build a module from templates and user supplied information\"\"\"\n\n    for path in paths:\n        real_path = os.path.abspath(os.path.join(target, path.format(**info)))\n        log(\"Making directory '%s'\" % real_path)\n        os.makedirs(real_path)\n\n    # pprint(info)\n    for item in templates.values():\n        source = os.path.join('dev/templates', item[0])\n        filename = os.path.abspath(\n            os.path.join(target, item[1].format(**info)))\n        log(\"Creating file from template '%s'\" % filename,\n            emitter='MANAGE')\n        write_template_file(source, filename, info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nasks questions to fill out a HFOS plugin template", "response": "def _ask_questionnaire():\n    \"\"\"Asks questions to fill out a HFOS plugin template\"\"\"\n\n    answers = {}\n    print(info_header)\n    pprint(questions.items())\n\n    for question, default in questions.items():\n        response = _ask(question, default, str(type(default)), show_hint=True)\n        if type(default) == unicode and type(response) != str:\n            response = response.decode('utf-8')\n        answers[question] = response\n\n    return answers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_module(clear_target, target):\n\n    if os.path.exists(target):\n        if clear_target:\n            shutil.rmtree(target)\n        else:\n            log(\"Target exists! Use --clear to delete it first.\",\n                emitter='MANAGE')\n            sys.exit(2)\n\n    done = False\n    info = None\n\n    while not done:\n        info = _ask_questionnaire()\n        pprint(info)\n        done = _ask('Is the above correct', default='y', data_type='bool')\n\n    augmented_info = _augment_info(info)\n\n    log(\"Constructing module %(plugin_name)s\" % info)\n    _construct_module(augmented_info, target)", "response": "Creates a new template HFOS plugin module"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_field(key, lookup_type=None, placeholder=None, html_class=\"div\",\n                 select_type=\"strapselect\", mapping=\"uuid\"):\n    \"\"\"Generates a lookup field for form definitions\"\"\"\n\n    if lookup_type is None:\n        lookup_type = key\n\n    if placeholder is None:\n        placeholder = \"Select a \" + lookup_type\n\n    result = {\n        'key': key,\n        'htmlClass': html_class,\n        'type': select_type,\n        'placeholder': placeholder,\n        'options': {\n            \"type\": lookup_type,\n            \"asyncCallback\": \"$ctrl.getFormData\",\n            \"map\": {'valueProperty': mapping, 'nameProperty': 'name'}\n        }\n    }\n\n    return result", "response": "Generates a lookup field for form definitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fieldset(title, items, options=None):\n    result = {\n        'title': title,\n        'type': 'fieldset',\n        'items': items\n    }\n    if options is not None:\n        result.update(options)\n\n    return result", "response": "A field set with a title and sub items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef section(rows, columns, items, label=None):\n\n    # TODO: Integrate label\n\n    sections = []\n\n    column_class = \"section-column col-sm-%i\" % (12 / columns)\n\n    for vertical in range(columns):\n        column_items = []\n        for horizontal in range(rows):\n            try:\n                item = items[horizontal][vertical]\n                column_items.append(item)\n            except IndexError:\n                hfoslog('Field in', label, 'omitted, due to missing row/column:', vertical, horizontal,\n                        lvl=warn, emitter='FORMS', tb=True, frame=2)\n\n        column = {\n            'type': 'section',\n            'htmlClass': column_class,\n            'items': column_items\n        }\n        sections.append(column)\n\n    result = {\n        'type': 'section',\n        'htmlClass': 'row',\n        'items': sections\n    }\n\n    return result", "response": "A section consisting of rows and columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emptyArray(key, add_label=None):\n\n    result = {\n        'key': key,\n        'startEmpty': True\n    }\n    if add_label is not None:\n        result['add'] = add_label\n        result['style'] = {'add': 'btn-success'}\n    return result", "response": "An array that starts empty"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tabset(titles, contents):\n\n    tabs = []\n    for no, title in enumerate(titles):\n        tab = {\n            'title': title,\n        }\n        content = contents[no]\n        if isinstance(content, list):\n            tab['items'] = content\n        else:\n            tab['items'] = [content]\n        tabs.append(tab)\n\n    result = {\n        'type': 'tabs',\n        'tabs': tabs\n    }\n\n    return result", "response": "A tabbed container widget"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef country_field(key='country'):\n\n    country_list = list(countries)\n    title_map = []\n    for item in country_list:\n        title_map.append({'value': item.alpha_3, 'name': item.name})\n\n    widget = {\n        'key': key,\n        'type': 'uiselect',\n        'titleMap': title_map\n    }\n\n    return widget", "response": "Provides a select box for country selection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef area_field(key='area'):\n\n    area_list = list(subdivisions)\n    title_map = []\n    for item in area_list:\n        title_map.append({'value': item.code, 'name': item.name})\n\n    widget = {\n        'key': key,\n        'type': 'uiselect',\n        'titleMap': title_map\n    }\n\n    return widget", "response": "Provides a select box for country selection"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef timed_connectivity_check(self, event):\n        self.status = self._can_connect()\n        self.log('Timed connectivity check:', self.status, lvl=verbose)\n\n        if self.status:\n            if not self.old_status:\n                self.log('Connectivity gained')\n                self.fireEvent(backend_nodestate_toggle(STATE_UUID_CONNECTIVITY, on=True, force=True))\n        else:\n            if self.old_status:\n                self.log('Connectivity lost', lvl=warn)\n                self.old_status = False\n                self.fireEvent(backend_nodestate_toggle(STATE_UUID_CONNECTIVITY, off=True, force=True))\n\n        self.old_status = self.status", "response": "Tests internet connectivity in regular intervals and updates the nodestate accordingly"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _can_connect(self):\n        self.log('Trying to reach configured connectivity check endpoint', lvl=verbose)\n\n        try:\n            socket.setdefaulttimeout(self.config.timeout)\n            socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((self.config.host, self.config.port))\n            return True\n        except Exception as ex:\n            self.log(ex, pretty=True, lvl=debug)\n            return False", "response": "Tries to connect to the configured host : port and returns True if the connection was established"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef referenceframe(self, event):\n\n        self.log(\"Got a reference frame update! \", event, lvl=verbose)\n\n        self.referenceframe = event.data", "response": "Handles navigational reference frame updates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef activityrequest(self, event):\n\n        # self.log(\"Event: '%s'\" % event.__dict__)\n\n        try:\n            action = event.action\n            data = event.data\n            self.log(\"Activityrequest: \", action, data)\n\n        except Exception as e:\n            self.log(\"Error: '%s' %s\" % (e, type(e)), lvl=error)", "response": "Handler for incoming activity monitor events"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmodify field values of objects", "response": "def modify(ctx, schema, uuid, object_filter, field, value):\n    \"\"\"Modify field values of objects\"\"\"\n    database = ctx.obj['db']\n\n    model = database.objectmodels[schema]\n    obj = None\n\n    if uuid:\n        obj = model.find_one({'uuid': uuid})\n    elif object_filter:\n        obj = model.find_one(literal_eval(object_filter))\n    else:\n        log('No object uuid or filter specified.',\n            lvl=error)\n\n    if obj is None:\n        log('No object found',\n            lvl=error)\n        return\n\n    log('Object found, modifying', lvl=debug)\n    try:\n        new_value = literal_eval(value)\n    except ValueError:\n        log('Interpreting value as string')\n        new_value = str(value)\n\n    obj._fields[field] = new_value\n    obj.validate()\n    log('Changed object validated', lvl=debug)\n    obj.save()\n    log('Done')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes stored objects (CAUTION!)", "response": "def delete(ctx, schema, uuid, object_filter, yes):\n    \"\"\"Delete stored objects (CAUTION!)\"\"\"\n\n    database = ctx.obj['db']\n\n    if schema is None:\n        log('No schema given. Read the help', lvl=warn)\n        return\n\n    model = database.objectmodels[schema]\n\n    if uuid:\n        count = model.count({'uuid': uuid})\n        obj = model.find({'uuid': uuid})\n    elif object_filter:\n        count = model.count(literal_eval(object_filter))\n        obj = model.find(literal_eval(object_filter))\n    else:\n        count = model.count()\n        obj = model.find()\n\n    if count == 0:\n        log('No objects to delete found')\n        return\n\n    if not yes and not _ask(\"Are you sure you want to delete %i objects\" % count,\n                            default=False, data_type=\"bool\", show_hint=True):\n        return\n\n    for item in obj:\n        item.delete()\n\n    log('Done')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(ctx, schema, all_schemata):\n\n    database = ctx.obj['db']\n\n    if schema is None:\n        if all_schemata is False:\n            log('No schema given. Read the help', lvl=warn)\n            return\n        else:\n            schemata = database.objectmodels.keys()\n    else:\n        schemata = [schema]\n\n    for schema in schemata:\n        try:\n            things = database.objectmodels[schema]\n            with click.progressbar(things.find(), length=things.count(),\n                                   label='Validating %15s' % schema) as object_bar:\n                for obj in object_bar:\n                    obj.validate()\n        except Exception as e:\n\n            log('Exception while validating:',\n                schema, e, type(e),\n                '\\n\\nFix this object and rerun validation!',\n                emitter='MANAGE', lvl=error)\n\n    log('Done')", "response": "Validates all objects or all objects of a given schema."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_field(ctx, search, by_type, obj):\n\n    # TODO: Fix this to work recursively on all possible subschemes\n    if search is not None:\n        search = search\n    else:\n        search = _ask(\"Enter search term\")\n\n    database = ctx.obj['db']\n\n    def find(search_schema, search_field, find_result=None, key=\"\"):\n        \"\"\"Examine a schema to find fields by type or name\"\"\"\n\n        if find_result is None:\n            find_result = []\n        fields = search_schema['properties']\n        if not by_type:\n            if search_field in fields:\n                find_result.append(key)\n                # log(\"Found queried fieldname in \", model)\n        else:\n            for field in fields:\n                try:\n                    if \"type\" in fields[field]:\n                        # log(fields[field], field)\n                        if fields[field][\"type\"] == search_field:\n                            find_result.append((key, field))\n                            # log(\"Found field\", field, \"in\", model)\n                except KeyError as e:\n                    log(\"Field access error:\", e, type(e), exc=True,\n                        lvl=debug)\n\n        if 'properties' in fields:\n            # log('Sub properties checking:', fields['properties'])\n            find_result.append(find(fields['properties'], search_field,\n                                    find_result, key=fields['name']))\n\n        for field in fields:\n            if 'items' in fields[field]:\n                if 'properties' in fields[field]['items']:\n                    # log('Sub items checking:', fields[field])\n                    find_result.append(find(fields[field]['items'], search_field,\n                                            find_result, key=field))\n                else:\n                    pass\n                    # log('Items without proper definition!')\n\n        return find_result\n\n    if obj is not None:\n        schema = database.objectmodels[obj]._schema\n        result = find(schema, search, [], key=\"top\")\n        if result:\n            # log(args.object, result)\n            print(obj)\n            pprint(result)\n    else:\n        for model, thing in database.objectmodels.items():\n            schema = thing._schema\n\n            result = find(schema, search, [], key=\"top\")\n            if result:\n                print(model)\n                # log(model, result)\n                print(result)", "response": "Find fields in registered data models."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Distance(lat1, lon1, lat2, lon2):\n\n    az12, az21, dist = wgs84_geod.inv(lon1, lat1, lon2, lat2)\n    return az21, dist", "response": "Get distance between two points in a single series of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays known details about a given client", "response": "def client_details(self, *args):\n        \"\"\"Display known details about a given client\"\"\"\n\n        self.log(_('Client details:', lang='de'))\n        client = self._clients[args[0]]\n\n        self.log('UUID:', client.uuid, 'IP:', client.ip, 'Name:', client.name, 'User:', self._users[client.useruuid],\n                 pretty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef client_list(self, *args):\n        if len(self._clients) == 0:\n            self.log('No clients connected')\n        else:\n            self.log(self._clients, pretty=True)", "response": "Display a list of connected clients"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays a list of connected users", "response": "def users_list(self, *args):\n        \"\"\"Display a list of connected users\"\"\"\n        if len(self._users) == 0:\n            self.log('No users connected')\n        else:\n            self.log(self._users, pretty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a list of all registered events", "response": "def sourcess_list(self, *args):\n        \"\"\"Display a list of all registered events\"\"\"\n\n        from pprint import pprint\n\n        sources = {}\n        sources.update(self.authorized_events)\n        sources.update(self.anonymous_events)\n\n        for source in sources:\n            pprint(source)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef events_list(self, *args):\n\n        def merge(a, b, path=None):\n            \"merges b into a\"\n            if path is None: path = []\n            for key in b:\n                if key in a:\n                    if isinstance(a[key], dict) and isinstance(b[key], dict):\n                        merge(a[key], b[key], path + [str(key)])\n                    elif a[key] == b[key]:\n                        pass  # same leaf value\n                    else:\n                        raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))\n                else:\n                    a[key] = b[key]\n            return a\n\n        events = {}\n        sources = merge(self.authorized_events, self.anonymous_events)\n\n        for source, source_events in sources.items():\n            events[source] = []\n            for item in source_events:\n                events[source].append(item)\n\n        self.log(events, pretty=True)", "response": "Display a list of all registered events"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays a table of connected users and clients", "response": "def who(self, *args):\n        \"\"\"Display a table of connected users and clients\"\"\"\n        if len(self._users) == 0:\n            self.log('No users connected')\n            if len(self._clients) == 0:\n                self.log('No clients connected')\n                return\n\n        Row = namedtuple(\"Row\", ['User', 'Client', 'IP'])\n        rows = []\n\n        for user in self._users.values():\n            for key, client in self._clients.items():\n                if client.useruuid == user.uuid:\n                    row = Row(user.account.name, key, client.ip)\n                    rows.append(row)\n\n        for key, client in self._clients.items():\n            if client.useruuid is None:\n                row = Row('ANON', key, client.ip)\n                rows.append(row)\n\n        self.log(\"\\n\" + std_table(rows))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog out a client and possibly associated user", "response": "def _logoutclient(self, useruuid, clientuuid):\n        \"\"\"Log out a client and possibly associated user\"\"\"\n\n        self.log(\"Cleaning up client of logged in user.\", lvl=debug)\n        try:\n            self._users[useruuid].clients.remove(clientuuid)\n            if len(self._users[useruuid].clients) == 0:\n                self.log(\"Last client of user disconnected.\", lvl=verbose)\n\n                self.fireEvent(userlogout(useruuid, clientuuid))\n                del self._users[useruuid]\n\n            self._clients[clientuuid].useruuid = None\n        except Exception as e:\n            self.log(\"Error during client logout: \", e, type(e),\n                     clientuuid, useruuid, lvl=error,\n                     exc=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister new sockets and their clients and allocates uuids", "response": "def connect(self, *args):\n        \"\"\"Registers new sockets and their clients and allocates uuids\"\"\"\n\n        self.log(\"Connect \", args, lvl=verbose)\n\n        try:\n            sock = args[0]\n            ip = args[1]\n\n            if sock not in self._sockets:\n                self.log(\"New client connected:\", ip, lvl=debug)\n                clientuuid = str(uuid4())\n                self._sockets[sock] = Socket(ip, clientuuid)\n                # Key uuid is temporary, until signin, will then be replaced\n                #  with account uuid\n\n                self._clients[clientuuid] = Client(\n                    sock=sock,\n                    ip=ip,\n                    clientuuid=clientuuid,\n                )\n\n                self.log(\"Client connected:\", clientuuid, lvl=debug)\n            else:\n                self.log(\"Old IP reconnected!\", lvl=warn)\n                #     self.fireEvent(write(sock, \"Another client is\n                # connecting from your IP!\"))\n                #     self._sockets[sock] = (ip, uuid.uuid4())\n        except Exception as e:\n            self.log(\"Error during connect: \", e, type(e), lvl=critical)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a packet to one of his clients by the user s name or uuid.", "response": "def send(self, event):\n        \"\"\"Sends a packet to an already known user or one of his clients by\n        UUID\"\"\"\n\n        try:\n            jsonpacket = json.dumps(event.packet, cls=ComplexEncoder)\n            if event.sendtype == \"user\":\n                # TODO: I think, caching a user name <-> uuid table would\n                # make sense instead of looking this up all the time.\n\n                if event.uuid is None:\n                    userobject = objectmodels['user'].find_one({\n                        'name': event.username\n                    })\n                else:\n                    userobject = objectmodels['user'].find_one({\n                        'uuid': event.uuid\n                    })\n\n                if userobject is None:\n                    self.log(\"No user by that name known.\", lvl=warn)\n                    return\n                else:\n                    uuid = userobject.uuid\n\n                self.log(\"Broadcasting to all of users clients: '%s': '%s\" % (\n                    uuid, str(event.packet)[:20]), lvl=network)\n                if uuid not in self._users:\n                    self.log(\"User not connected!\", event, lvl=critical)\n                    return\n                clients = self._users[uuid].clients\n\n                for clientuuid in clients:\n                    sock = self._clients[clientuuid].sock\n\n                    if not event.raw:\n                        self.log(\"Sending json to client\", jsonpacket[:50],\n                                 lvl=network)\n\n                        self.fireEvent(write(sock, jsonpacket), \"wsserver\")\n                    else:\n                        self.log(\"Sending raw data to client\")\n                        self.fireEvent(write(sock, event.packet), \"wsserver\")\n            else:  # only to client\n                self.log(\"Sending to user's client: '%s': '%s'\" % (\n                    event.uuid, jsonpacket[:20]), lvl=network)\n                if event.uuid not in self._clients:\n                    if not event.fail_quiet:\n                        self.log(\"Unknown client!\", event.uuid, lvl=critical)\n                        self.log(\"Clients:\", self._clients, lvl=debug)\n                    return\n\n                sock = self._clients[event.uuid].sock\n                if not event.raw:\n                    self.fireEvent(write(sock, jsonpacket), \"wsserver\")\n                else:\n                    self.log(\"Sending raw data to client\", lvl=network)\n                    self.fireEvent(write(sock, event.packet[:20]), \"wsserver\")\n\n        except Exception as e:\n            self.log(\"Exception during sending: %s (%s)\" % (e, type(e)),\n                     lvl=critical, exc=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef broadcast(self, event):\n        try:\n            if event.broadcasttype == \"users\":\n                if len(self._users) > 0:\n                    self.log(\"Broadcasting to all users:\",\n                             event.content, lvl=network)\n                    for useruuid in self._users.keys():\n                        self.fireEvent(\n                            send(useruuid, event.content, sendtype=\"user\"))\n                        # else:\n                        #    self.log(\"Not broadcasting, no users connected.\",\n                        #            lvl=debug)\n\n            elif event.broadcasttype == \"clients\":\n                if len(self._clients) > 0:\n                    self.log(\"Broadcasting to all clients: \",\n                             event.content, lvl=network)\n                    for client in self._clients.values():\n                        self.fireEvent(write(client.sock, event.content),\n                                       \"wsserver\")\n                        # else:\n                        #    self.log(\"Not broadcasting, no clients\n                        # connected.\",\n                        #            lvl=debug)\n            elif event.broadcasttype == \"socks\":\n                if len(self._sockets) > 0:\n                    self.log(\"Emergency?! Broadcasting to all sockets: \",\n                             event.content)\n                    for sock in self._sockets:\n                        self.fireEvent(write(sock, event.content), \"wsserver\")\n                        # else:\n                        #    self.log(\"Not broadcasting, no sockets\n                        # connected.\",\n                        #            lvl=debug)\n\n        except Exception as e:\n            self.log(\"Error during broadcast: \", e, type(e), lvl=critical)", "response": "Broadcasts an event to all users or clients depending on the event flag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the user has in any role that allows to fire the event.", "response": "def _checkPermissions(self, user, event):\n        \"\"\"Checks if the user has in any role that allows to fire the event.\"\"\"\n\n        for role in user.account.roles:\n            if role in event.roles:\n                self.log('Access granted', lvl=verbose)\n                return True\n\n        self.log('Access denied', lvl=verbose)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handleAuthorizedEvents(self, component, action, data, user, client):\n\n        try:\n            if component == \"debugger\":\n                self.log(component, action, data, user, client, lvl=info)\n\n            if not user and component in self.authorized_events.keys():\n                self.log(\"Unknown client tried to do an authenticated \"\n                         \"operation: %s\",\n                         component, action, data, user)\n                return\n\n            event = self.authorized_events[component][action]['event'](user, action, data, client)\n\n            self.log('Authorized event roles:', event.roles, lvl=verbose)\n            if not self._checkPermissions(user, event):\n                result = {\n                    'component': 'hfos.ui.clientmanager',\n                    'action': 'Permission',\n                    'data': _('You have no role that allows this action.', lang='de')\n                }\n                self.fireEvent(send(event.client.uuid, result))\n                return\n\n            self.log(\"Firing authorized event: \", component, action,\n                     str(data)[:100], lvl=debug)\n            # self.log(\"\", (user, action, data, client), lvl=critical)\n            self.fireEvent(event)\n        except Exception as e:\n            self.log(\"Critical error during authorized event handling:\",\n                     component, action, e,\n                     type(e), lvl=critical, exc=True)", "response": "Isolated communication link for authorized events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handleAnonymousEvents(self, component, action, data, client):\n        try:\n            event = self.anonymous_events[component][action]['event']\n\n            self.log(\"Firing anonymous event: \", component, action,\n                     str(data)[:20], lvl=network)\n            # self.log(\"\", (user, action, data, client), lvl=critical)\n            self.fireEvent(event(action, data, client))\n        except Exception as e:\n            self.log(\"Critical error during anonymous event handling:\",\n                     component, action, e,\n                     type(e), lvl=critical, exc=True)", "response": "Handler for anonymous events"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handleAuthenticationEvents(self, requestdata, requestaction,\n                                    clientuuid, sock):\n        \"\"\"Handler for authentication events\"\"\"\n\n        # TODO: Move this stuff over to ./auth.py\n        if requestaction in (\"login\", \"autologin\"):\n            try:\n                self.log(\"Login request\", lvl=verbose)\n\n                if requestaction == \"autologin\":\n                    username = password = None\n                    requestedclientuuid = requestdata\n                    auto = True\n\n                    self.log(\"Autologin for\", requestedclientuuid, lvl=debug)\n                else:\n                    username = requestdata['username']\n                    password = requestdata['password']\n\n                    if 'clientuuid' in requestdata:\n                        requestedclientuuid = requestdata['clientuuid']\n                    else:\n                        requestedclientuuid = None\n                    auto = False\n\n                    self.log(\"Auth request by\", username, lvl=verbose)\n\n                self.fireEvent(authenticationrequest(\n                    username,\n                    password,\n                    clientuuid,\n                    requestedclientuuid,\n                    sock,\n                    auto,\n                ), \"auth\")\n                return\n            except Exception as e:\n                self.log(\"Login failed: \", e, type(e), lvl=warn, exc=True)\n        elif requestaction == \"logout\":\n            self.log(\"User logged out, refreshing client.\", lvl=network)\n            try:\n                if clientuuid in self._clients:\n                    client = self._clients[clientuuid]\n                    user_id = client.useruuid\n                    if client.useruuid:\n                        self.log(\"Logout client uuid: \", clientuuid)\n                        self._logoutclient(client.useruuid, clientuuid)\n                    self.fireEvent(clientdisconnect(clientuuid))\n                else:\n                    self.log(\"Client is not connected!\", lvl=warn)\n            except Exception as e:\n                self.log(\"Error during client logout: \", e, type(e),\n                         lvl=error, exc=True)\n        else:\n            self.log(\"Unsupported auth action requested:\",\n                     requestaction, lvl=warn)", "response": "Handler for authentication events"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reset_flood_offenders(self, *args):\n\n        offenders = []\n        # self.log('Resetting flood offenders')\n\n        for offender, offence_time in self._flooding.items():\n            if time() - offence_time < 10:\n                self.log('Removed offender from flood list:', offender)\n                offenders.append(offender)\n\n        for offender in offenders:\n            del self._flooding[offender]", "response": "Resets the list of flood offenders on event trigger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if any clients have been flooding the node", "response": "def _check_flood_protection(self, component, action, clientuuid):\n        \"\"\"Checks if any clients have been flooding the node\"\"\"\n\n        if clientuuid not in self._flood_counter:\n            self._flood_counter[clientuuid] = 0\n\n        self._flood_counter[clientuuid] += 1\n\n        if self._flood_counter[clientuuid] > 100:\n            packet = {\n                'component': 'hfos.ui.clientmanager',\n                'action': 'Flooding',\n                'data': True\n            }\n            self.fireEvent(send(clientuuid, packet))\n            self.log('Flooding from', clientuuid)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles raw client requests and distributes them to the appropriate components", "response": "def read(self, *args):\n        \"\"\"Handles raw client requests and distributes them to the\n        appropriate components\"\"\"\n\n        self.log(\"Beginning new transaction: \", args, lvl=network)\n        try:\n            sock, msg = args[0], args[1]\n            user = password = client = clientuuid = useruuid = requestdata = \\\n                requestaction = None\n            # self.log(\"\", msg)\n\n            clientuuid = self._sockets[sock].clientuuid\n        except Exception as e:\n            self.log(\"Receiving error: \", e, type(e), lvl=error)\n\n        if clientuuid in self._flooding:\n            return\n\n        try:\n            msg = json.loads(msg)\n            self.log(\"Message from client received: \", msg, lvl=network)\n        except Exception as e:\n            self.log(\"JSON Decoding failed! %s (%s of %s)\" % (msg, e, type(e)))\n            return\n\n        try:\n            requestcomponent = msg['component']\n            requestaction = msg['action']\n        except (KeyError, AttributeError) as e:\n            self.log(\"Unpacking error: \", msg, e, type(e), lvl=error)\n            return\n\n        if self._check_flood_protection(requestcomponent, requestaction,\n                                        clientuuid):\n            self.log('Flood protection triggered')\n            self._flooding[clientuuid] = time()\n\n        try:\n            # TODO: Do not unpickle or decode anything from unsafe events\n            requestdata = msg['data']\n            if isinstance(requestdata, (dict, list)) and 'raw' in requestdata:\n                # self.log(requestdata['raw'], lvl=critical)\n                requestdata['raw'] = b64decode(requestdata['raw'])\n                # self.log(requestdata['raw'])\n        except (KeyError, AttributeError) as e:\n            self.log(\"No payload.\", lvl=network)\n            requestdata = None\n\n        if requestcomponent == \"auth\":\n            self._handleAuthenticationEvents(requestdata, requestaction,\n                                             clientuuid, sock)\n            return\n\n        try:\n            client = self._clients[clientuuid]\n        except KeyError as e:\n            self.log('Could not get client for request!', e, type(e), lvl=warn)\n            return\n\n        if requestcomponent in self.anonymous_events and requestaction in \\\n            self.anonymous_events[requestcomponent]:\n            self.log('Executing anonymous event:', requestcomponent,\n                     requestaction)\n            try:\n                self._handleAnonymousEvents(requestcomponent, requestaction,\n                                            requestdata, client)\n            except Exception as e:\n                self.log(\"Anonymous request failed:\", e, type(e), lvl=warn,\n                         exc=True)\n            return\n\n        elif requestcomponent in self.authorized_events:\n            try:\n                useruuid = client.useruuid\n                self.log(\"Authenticated operation requested by \",\n                         useruuid, client.config, lvl=network)\n            except Exception as e:\n                self.log(\"No useruuid!\", e, type(e), lvl=critical)\n                return\n\n            self.log('Checking if user is logged in', lvl=verbose)\n\n            try:\n                user = self._users[useruuid]\n            except KeyError:\n                if not (requestaction == 'ping' and requestcomponent == 'hfos.ui.clientmanager'):\n                    self.log(\"User not logged in.\", lvl=warn)\n\n                return\n\n            self.log('Handling event:', requestcomponent, requestaction, lvl=verbose)\n            try:\n                self._handleAuthorizedEvents(requestcomponent, requestaction,\n                                             requestdata, user, client)\n            except Exception as e:\n                self.log(\"User request failed: \", e, type(e), lvl=warn,\n                         exc=True)\n        else:\n            self.log('Invalid event received:', requestcomponent, requestaction, lvl=warn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlinks the client to the user and socket and notifies the client", "response": "def authentication(self, event):\n        \"\"\"Links the client to the granted account and profile,\n        then notifies the client\"\"\"\n\n        try:\n            self.log(\"Authorization has been granted by DB check:\",\n                     event.username, lvl=debug)\n\n            account, profile, clientconfig = event.userdata\n\n            useruuid = event.useruuid\n            originatingclientuuid = event.clientuuid\n            clientuuid = clientconfig.uuid\n\n            if clientuuid != originatingclientuuid:\n                self.log(\"Mutating client uuid to request id:\",\n                         clientuuid, lvl=network)\n            # Assign client to user\n            if useruuid in self._users:\n                signedinuser = self._users[useruuid]\n            else:\n                signedinuser = User(account, profile, useruuid)\n                self._users[account.uuid] = signedinuser\n\n            if clientuuid in signedinuser.clients:\n                self.log(\"Client configuration already logged in.\",\n                         lvl=critical)\n                # TODO: What now??\n                # Probably senseful would be to add the socket to the\n                # client's other socket\n                # The clients would be identical then - that could cause\n                # problems\n                # which could be remedied by duplicating the configuration\n            else:\n                signedinuser.clients.append(clientuuid)\n                self.log(\"Active client (\", clientuuid, \") registered to \"\n                                                        \"user\", useruuid,\n                         lvl=debug)\n\n            # Update socket..\n            socket = self._sockets[event.sock]\n            socket.clientuuid = clientuuid\n            self._sockets[event.sock] = socket\n\n            # ..and client lists\n\n            try:\n                language = clientconfig.language\n            except AttributeError:\n                language = \"en\"\n\n            # TODO: Rewrite and simplify this:\n            newclient = Client(\n                sock=event.sock,\n                ip=socket.ip,\n                clientuuid=clientuuid,\n                useruuid=useruuid,\n                name=clientconfig.name,\n                config=clientconfig,\n                language=language\n            )\n\n            del (self._clients[originatingclientuuid])\n            self._clients[clientuuid] = newclient\n\n            authpacket = {\"component\": \"auth\", \"action\": \"login\",\n                          \"data\": account.serializablefields()}\n            self.log(\"Transmitting Authorization to client\", authpacket,\n                     lvl=network)\n            self.fireEvent(\n                write(event.sock, json.dumps(authpacket)),\n                \"wsserver\"\n            )\n\n            profilepacket = {\"component\": \"profile\", \"action\": \"get\",\n                             \"data\": profile.serializablefields()}\n            self.log(\"Transmitting Profile to client\", profilepacket,\n                     lvl=network)\n            self.fireEvent(write(event.sock, json.dumps(profilepacket)),\n                           \"wsserver\")\n\n            clientconfigpacket = {\"component\": \"clientconfig\", \"action\": \"get\",\n                                  \"data\": clientconfig.serializablefields()}\n            self.log(\"Transmitting client configuration to client\",\n                     clientconfigpacket, lvl=network)\n            self.fireEvent(write(event.sock, json.dumps(clientconfigpacket)),\n                           \"wsserver\")\n\n            self.fireEvent(userlogin(clientuuid, useruuid, clientconfig, signedinuser))\n\n            self.log(\"User configured: Name\",\n                     signedinuser.account.name, \"Profile\",\n                     signedinuser.profile.uuid, \"Clients\",\n                     signedinuser.clients,\n                     lvl=debug)\n\n        except Exception as e:\n            self.log(\"Error (%s, %s) during auth grant: %s\" % (\n                type(e), e, event), lvl=error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores client s selection of a new translation", "response": "def selectlanguage(self, event):\n        \"\"\"Store client's selection of a new translation\"\"\"\n\n        self.log('Language selection event:', event.client, pretty=True)\n\n        if event.data not in all_languages():\n            self.log('Unavailable language selected:', event.data, lvl=warn)\n            language = None\n        else:\n            language = event.data\n\n        if language is None:\n            language = 'en'\n\n        event.client.language = language\n\n        if event.client.config is not None:\n            event.client.config.language = language\n            event.client.config.save()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getlanguages(self, event):\n\n        self.log('Client requests all languages.', lvl=verbose)\n        result = {\n            'component': 'hfos.ui.clientmanager',\n            'action': 'getlanguages',\n            'data': language_token_to_name(all_languages())\n        }\n        self.fireEvent(send(event.client.uuid, result))", "response": "Compile and return a human readable list of registered translations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a ping to measure client < - node latency", "response": "def ping(self, event):\n        \"\"\"Perform a ping to measure client <-> node latency\"\"\"\n\n        self.log('Client ping received:', event.data, lvl=verbose)\n        response = {\n            'component': 'hfos.ui.clientmanager',\n            'action': 'pong',\n            'data': [event.data, time() * 1000]\n        }\n\n        self.fire(send(event.client.uuid, response))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting between geodetic modified apex quasi - dipole and MLT.", "response": "def convert(self, lat, lon, source, dest, height=0, datetime=None,\n                precision=1e-10, ssheight=50*6371):\n        \"\"\"Converts between geodetic, modified apex, quasi-dipole and MLT.\n\n        Parameters\n        ==========\n        lat : array_like\n            Latitude\n        lon : array_like\n            Longitude/MLT\n        source : {'geo', 'apex', 'qd', 'mlt'}\n            Input coordinate system\n        dest : {'geo', 'apex', 'qd', 'mlt'}\n            Output coordinate system\n        height : array_like, optional\n            Altitude in km\n        datetime : :class:`datetime.datetime`\n            Date and time for MLT conversions (required for MLT conversions)\n        precision : float, optional\n            Precision of output (degrees) when converting to geo. A negative\n            value of this argument produces a low-precision calculation of\n            geodetic lat/lon based only on their spherical harmonic\n            representation.\n            A positive value causes the underlying Fortran routine to iterate\n            until feeding the output geo lat/lon into geo2qd (APXG2Q) reproduces\n            the input QD lat/lon to within the specified precision (all\n            coordinates being converted to geo are converted to QD first and\n            passed through APXG2Q).\n        ssheight : float, optional\n            Altitude in km to use for converting the subsolar point from\n            geographic to magnetic coordinates. A high altitude is used\n            to ensure the subsolar point is mapped to high latitudes, which\n            prevents the South-Atlantic Anomaly (SAA) from influencing the MLT.\n\n        Returns\n        =======\n        lat : ndarray or float\n            Converted latitude (if converting to MLT, output latitude is apex)\n        lat : ndarray or float\n            Converted longitude/MLT\n\n        \"\"\"\n\n        if datetime is None and ('mlt' in [source, dest]):\n            raise ValueError('datetime must be given for MLT calculations')\n\n        lat = helpers.checklat(lat)\n\n        if source == dest:\n            return lat, lon\n        # from geo\n        elif source == 'geo' and dest == 'apex':\n            lat, lon = self.geo2apex(lat, lon, height)\n        elif source == 'geo' and dest == 'qd':\n            lat, lon = self.geo2qd(lat, lon, height)\n        elif source == 'geo' and dest == 'mlt':\n            lat, lon = self.geo2apex(lat, lon, height)\n            lon = self.mlon2mlt(lon, datetime, ssheight=ssheight)\n        # from apex\n        elif source == 'apex' and dest == 'geo':\n            lat, lon, _ = self.apex2geo(lat, lon, height, precision=precision)\n        elif source == 'apex' and dest == 'qd':\n            lat, lon = self.apex2qd(lat, lon, height=height)\n        elif source == 'apex' and dest == 'mlt':\n            lon = self.mlon2mlt(lon, datetime, ssheight=ssheight)\n        # from qd\n        elif source == 'qd' and dest == 'geo':\n            lat, lon, _ = self.qd2geo(lat, lon, height, precision=precision)\n        elif source == 'qd' and dest == 'apex':\n            lat, lon = self.qd2apex(lat, lon, height=height)\n        elif source == 'qd' and dest == 'mlt':\n            lat, lon = self.qd2apex(lat, lon, height=height)\n            lon = self.mlon2mlt(lon, datetime, ssheight=ssheight)\n        # from mlt (input latitude assumed apex)\n        elif source == 'mlt' and dest == 'geo':\n            lon = self.mlt2mlon(lon, datetime, ssheight=ssheight)\n            lat, lon, _ = self.apex2geo(lat, lon, height, precision=precision)\n        elif source == 'mlt' and dest == 'apex':\n            lon = self.mlt2mlon(lon, datetime, ssheight=ssheight)\n        elif source == 'mlt' and dest == 'qd':\n            lon = self.mlt2mlon(lon, datetime, ssheight=ssheight)\n            lat, lon = self.apex2qd(lat, lon, height=height)\n        # no other transformations are implemented\n        else:\n            estr = 'Unknown coordinate transformation: '\n            estr += '{} -> {}'.format(source, dest)\n            raise NotImplementedError(estr)\n\n        return lat, lon"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts geodetic coordinates to modified apex coordinates.", "response": "def geo2apex(self, glat, glon, height):\n        \"\"\"Converts geodetic to modified apex coordinates.\n\n        Parameters\n        ==========\n        glat : array_like\n            Geodetic latitude\n        glon : array_like\n            Geodetic longitude\n        height : array_like\n            Altitude in km\n\n        Returns\n        =======\n        alat : ndarray or float\n            Modified apex latitude\n        alon : ndarray or float\n            Modified apex longitude\n\n        \"\"\"\n\n        glat = helpers.checklat(glat, name='glat')\n\n        alat, alon = self._geo2apex(glat, glon, height)\n\n        if np.any(np.float64(alat) == -9999):\n            warnings.warn('Apex latitude set to -9999 where undefined '\n                          '(apex height may be < reference height)')\n\n        # if array is returned, dtype is object, so convert to float\n        return np.float64(alat), np.float64(alon)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apex2geo(self, alat, alon, height, precision=1e-10):\n\n        alat = helpers.checklat(alat, name='alat')\n\n        qlat, qlon = self.apex2qd(alat, alon, height=height)\n        glat, glon, error = self.qd2geo(qlat, qlon, height, precision=precision)\n\n        return glat, glon, error", "response": "Converts modified apex to geodetic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert geodetic coordinates to quasi - dipole coordinates.", "response": "def geo2qd(self, glat, glon, height):\n        \"\"\"Converts geodetic to quasi-dipole coordinates.\n\n        Parameters\n        ==========\n        glat : array_like\n            Geodetic latitude\n        glon : array_like\n            Geodetic longitude\n        height : array_like\n            Altitude in km\n\n        Returns\n        =======\n        qlat : ndarray or float\n            Quasi-dipole latitude\n        qlon : ndarray or float\n            Quasi-dipole longitude\n\n        \"\"\"\n\n        glat = helpers.checklat(glat, name='glat')\n\n        qlat, qlon = self._geo2qd(glat, glon, height)\n\n        # if array is returned, dtype is object, so convert to float\n        return np.float64(qlat), np.float64(qlon)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef qd2geo(self, qlat, qlon, height, precision=1e-10):\n\n        qlat = helpers.checklat(qlat, name='qlat')\n\n        glat, glon, error = self._qd2geo(qlat, qlon, height, precision)\n\n        # if array is returned, dtype is object, so convert to float\n        return np.float64(glat), np.float64(glon), np.float64(error)", "response": "Converts quasi - dipole to geodetic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _apex2qd_nonvectorized(self, alat, alon, height):\n\n        alat = helpers.checklat(alat, name='alat')\n\n        # convert modified apex to quasi-dipole:\n        qlon = alon\n\n        # apex height\n        hA = self.get_apex(alat)\n\n        if hA < height:\n            if np.isclose(hA, height, rtol=0, atol=1e-5):\n                # allow for values that are close\n                hA = height\n            else:\n                estr = 'height {:.3g} is > apex height '.format(np.max(height))\n                estr += '{:.3g} for alat {:.3g}'.format(hA, alat)\n                raise ApexHeightError(estr)\n\n        qlat = np.sign(alat) * np.degrees(np.arccos(np.sqrt((self.RE + height) /\n                                                            (self.RE + hA))))\n\n        return qlat, qlon", "response": "Convert from apex to quasi - dipole"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts modified apex coordinates to quasi - dipole coordinates.", "response": "def apex2qd(self, alat, alon, height):\n        \"\"\"Converts modified apex to quasi-dipole coordinates.\n\n        Parameters\n        ==========\n        alat : array_like\n            Modified apex latitude\n        alon : array_like\n            Modified apex longitude\n        height : array_like\n            Altitude in km\n\n        Returns\n        =======\n        qlat : ndarray or float\n            Quasi-dipole latitude\n        qlon : ndarray or float\n            Quasi-dipole longitude\n\n        Raises\n        ======\n        ApexHeightError\n            if `height` > apex height\n\n        \"\"\"\n\n        qlat, qlon = self._apex2qd(alat, alon, height)\n\n        # if array is returned, the dtype is object, so convert to float\n        return np.float64(qlat), np.float64(qlon)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qd2apex(self, qlat, qlon, height):\n\n        alat, alon = self._qd2apex(qlat, qlon, height)\n\n        # if array is returned, the dtype is object, so convert to float\n        return np.float64(alat), np.float64(alon)", "response": "Converts quasi - dipole to modified apex coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mlon2mlt(self, mlon, datetime, ssheight=50*6371):\n        ssglat, ssglon = helpers.subsol(datetime)\n        ssalat, ssalon = self.geo2apex(ssglat, ssglon, ssheight)\n\n        # np.float64 will ensure lists are converted to arrays\n        return (180 + np.float64(mlon) - ssalon)/15 % 24", "response": "Computes the magnetic local time at the specified magnetic longitude and UT."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the magnetic longitude at the specified magnetic local time and UT.", "response": "def mlt2mlon(self, mlt, datetime, ssheight=50*6371):\n        \"\"\"Computes the magnetic longitude at the specified magnetic local time\n        and UT.\n\n        Parameters\n        ==========\n        mlt : array_like\n            Magnetic local time\n        datetime : :class:`datetime.datetime`\n            Date and time\n        ssheight : float, optional\n            Altitude in km to use for converting the subsolar point from\n            geographic to magnetic coordinates. A high altitude is used\n            to ensure the subsolar point is mapped to high latitudes, which\n            prevents the South-Atlantic Anomaly (SAA) from influencing the MLT.\n\n        Returns\n        =======\n        mlon : ndarray or float\n            Magnetic longitude [0, 360) (apex and quasi-dipole longitude are\n            always equal)\n\n        Notes\n        =====\n        To compute the magnetic longitude, we find the apex longitude of the\n        subsolar point at the given time. Then the magnetic longitude of the\n        given point will be computed from the separation in magnetic local time\n        from this point (1 hour = 15 degrees).\n        \"\"\"\n\n        ssglat, ssglon = helpers.subsol(datetime)\n        ssalat, ssalon = self.geo2apex(ssglat, ssglon, ssheight)\n\n        # np.float64 will ensure lists are converted to arrays\n        return (15*np.float64(mlt) - 180 + ssalon + 360) % 360"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming mapping of points along the magnetic field to the closest or conjugate hemisphere. Parameters ========== glat : array_like Geodetic latitude glon : array_like Geodetic longitude height : array_like Source altitude in km newheight : array_like Destination altitude in km conjugate : bool, optional Map to `newheight` in the conjugate hemisphere instead of the closest hemisphere precision : float, optional Precision of output (degrees). A negative value of this argument produces a low-precision calculation of geodetic lat/lon based only on their spherical harmonic representation. A positive value causes the underlying Fortran routine to iterate until feeding the output geo lat/lon into geo2qd (APXG2Q) reproduces the input QD lat/lon to within the specified precision. Returns ======= newglat : ndarray or float Geodetic latitude of mapped point newglon : ndarray or float Geodetic longitude of mapped point error : ndarray or float The angular difference (degrees) between the input QD coordinates and the qlat/qlon produced by feeding the output glat and glon into geo2qd (APXG2Q) Notes ===== The mapping is done by converting glat/glon/height to modified apex lat/lon, and converting back to geographic using newheight (if conjugate, use negative apex latitude when converting back)", "response": "def map_to_height(self, glat, glon, height, newheight, conjugate=False,\n                      precision=1e-10):\n        \"\"\"Performs mapping of points along the magnetic field to the closest\n        or conjugate hemisphere.\n\n        Parameters\n        ==========\n        glat : array_like\n            Geodetic latitude\n        glon : array_like\n            Geodetic longitude\n        height : array_like\n            Source altitude in km\n        newheight : array_like\n            Destination altitude in km\n        conjugate : bool, optional\n            Map to `newheight` in the conjugate hemisphere instead of the\n            closest hemisphere\n        precision : float, optional\n            Precision of output (degrees). A negative value of this argument\n            produces a low-precision calculation of geodetic lat/lon based only\n            on their spherical harmonic representation. A positive value causes\n            the underlying Fortran routine to iterate until feeding the output\n            geo lat/lon into geo2qd (APXG2Q) reproduces the input QD lat/lon to\n            within the specified precision.\n\n        Returns\n        =======\n        newglat : ndarray or float\n            Geodetic latitude of mapped point\n        newglon : ndarray or float\n            Geodetic longitude of mapped point\n        error : ndarray or float\n            The angular difference (degrees) between the input QD coordinates\n            and the qlat/qlon produced by feeding the output glat and glon\n            into geo2qd (APXG2Q)\n\n        Notes\n        =====\n        The mapping is done by converting glat/glon/height to modified apex\n        lat/lon, and converting back to geographic using newheight (if\n        conjugate, use negative apex latitude when converting back)\n\n        \"\"\"\n\n        alat, alon = self.geo2apex(glat, glon, height)\n        if conjugate:\n            alat = -alat\n        try:\n            newglat, newglon, error = self.apex2geo(alat, alon, newheight,\n                                                    precision=precision)\n        except ApexHeightError:\n            raise ApexHeightError(\"newheight is > apex height\")\n\n        return newglat, newglon, error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map_E_to_height(self, alat, alon, height, newheight, E):\n\n        return self._map_EV_to_height(alat, alon, height, newheight, E, 'E')", "response": "This function maps an electric field along the magnetic field to a new height."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform mapping of electric drift velocity along the magnetic field.", "response": "def map_V_to_height(self, alat, alon, height, newheight, V):\n        \"\"\"Performs mapping of electric drift velocity along the magnetic field.\n\n        It is assumed that the electric field is perpendicular to B.\n\n        Parameters\n        ==========\n        alat : (N,) array_like or float\n            Modified apex latitude\n        alon : (N,) array_like or float\n            Modified apex longitude\n        height : (N,) array_like or float\n            Source altitude in km\n        newheight : (N,) array_like or float\n            Destination altitude in km\n        V : (3,) or (3, N) array_like\n            Electric drift velocity (at `alat`, `alon`, `height`) in geodetic\n            east, north, and up components\n\n        Returns\n        =======\n        V : (3, N) or (3,) ndarray\n            The electric drift velocity at `newheight` (geodetic east, north,\n            and up components)\n\n        \"\"\"\n\n        return self._map_EV_to_height(alat, alon, height, newheight, V, 'V')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the quasi - dipole base vectors f1 and f2 at the specified location.", "response": "def basevectors_qd(self, lat, lon, height, coords='geo', precision=1e-10):\n        \"\"\"Returns quasi-dipole base vectors f1 and f2 at the specified\n        coordinates.\n\n        The vectors are described by Richmond [1995] [2]_ and\n        Emmert et al. [2010] [3]_.  The vector components are geodetic east and\n        north.\n\n        Parameters\n        ==========\n        lat : (N,) array_like or float\n            Latitude\n        lon : (N,) array_like or float\n            Longitude\n        height : (N,) array_like or float\n            Altitude in km\n        coords : {'geo', 'apex', 'qd'}, optional\n            Input coordinate system\n        precision : float, optional\n            Precision of output (degrees) when converting to geo. A negative\n            value of this argument produces a low-precision calculation of\n            geodetic lat/lon based only on their spherical harmonic\n            representation.\n            A positive value causes the underlying Fortran routine to iterate\n            until feeding the output geo lat/lon into geo2qd (APXG2Q) reproduces\n            the input QD lat/lon to within the specified precision (all\n            coordinates being converted to geo are converted to QD first and\n            passed through APXG2Q).\n\n        Returns\n        =======\n        f1 : (2, N) or (2,) ndarray\n        f2 : (2, N) or (2,) ndarray\n\n        References\n        ==========\n        .. [2] Richmond, A. D. (1995), Ionospheric Electrodynamics Using\n               Magnetic Apex Coordinates, Journal of geomagnetism and\n               geoelectricity, 47(2), 191\u2013212, :doi:`10.5636/jgg.47.191`.\n\n        .. [3] Emmert, J. T., A. D. Richmond, and D. P. Drob (2010),\n               A computationally compact representation of Magnetic-Apex\n               and Quasi-Dipole coordinates with smooth base vectors,\n               J. Geophys. Res., 115(A8), A08322, :doi:`10.1029/2010JA015326`.\n\n        \"\"\"\n\n        glat, glon = self.convert(lat, lon, coords, 'geo', height=height,\n                                  precision=precision)\n\n        f1, f2 = self._basevec(glat, glon, height)\n\n        # if inputs are not scalar, each vector is an array of arrays,\n        # so reshape to a single array\n        if f1.dtype == object:\n            f1 = np.vstack(f1).T\n            f2 = np.vstack(f2).T\n\n        return f1, f2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning base vectors in quasi - dipole and apex coordinates.", "response": "def basevectors_apex(self, lat, lon, height, coords='geo', precision=1e-10):\n        \"\"\"Returns base vectors in quasi-dipole and apex coordinates.\n\n        The vectors are described by Richmond [1995] [4]_ and\n        Emmert et al. [2010] [5]_.  The vector components are geodetic east,\n        north, and up (only east and north for `f1` and `f2`).\n\n        Parameters\n        ==========\n        lat, lon : (N,) array_like or float\n            Latitude\n        lat : (N,) array_like or float\n            Longitude\n        height : (N,) array_like or float\n            Altitude in km\n        coords : {'geo', 'apex', 'qd'}, optional\n            Input coordinate system\n        return_all : bool, optional\n            Will also return f3, g1, g2, and g3, and f1 and f2 have 3 components\n            (the last component is zero). Requires `lat`, `lon`, and `height`\n            to be broadcast to 1D (at least one of the parameters must be 1D\n            and the other two parameters must be 1D or 0D).\n        precision : float, optional\n            Precision of output (degrees) when converting to geo. A negative\n            value of this argument produces a low-precision calculation of\n            geodetic lat/lon based only on their spherical harmonic\n            representation.\n            A positive value causes the underlying Fortran routine to iterate\n            until feeding the output geo lat/lon into geo2qd (APXG2Q) reproduces\n            the input QD lat/lon to within the specified precision (all\n            coordinates being converted to geo are converted to QD first and\n            passed through APXG2Q).\n\n        Returns\n        =======\n        f1, f2 : (2, N) or (2,) ndarray\n        f3, g1, g2, g3, d1, d2, d3, e1, e2, e3 : (3, N) or (3,) ndarray\n\n        Note\n        ====\n        `f3`, `g1`, `g2`, and `g3` are not part of the Fortran code\n        by Emmert et al. [2010] [5]_. They are calculated by this\n        Python library according to the following equations in\n        Richmond [1995] [4]_:\n\n        * `g1`: Eqn. 6.3\n        * `g2`: Eqn. 6.4\n        * `g3`: Eqn. 6.5\n        * `f3`: Eqn. 6.8\n\n        References\n        ==========\n\n        .. [4] Richmond, A. D. (1995), Ionospheric Electrodynamics Using\n               Magnetic Apex Coordinates, Journal of geomagnetism and\n               geoelectricity, 47(2), 191\u2013212, :doi:`10.5636/jgg.47.191`.\n\n        .. [5] Emmert, J. T., A. D. Richmond, and D. P. Drob (2010),\n               A computationally compact representation of Magnetic-Apex\n               and Quasi-Dipole coordinates with smooth base vectors,\n               J. Geophys. Res., 115(A8), A08322, :doi:`10.1029/2010JA015326`.\n\n        \"\"\"\n\n        glat, glon = self.convert(lat, lon, coords, 'geo', height=height,\n                                  precision=precision)\n\n        returnvals = self._geo2apexall(glat, glon, height)\n        qlat = np.float64(returnvals[0])\n        alat = np.float64(returnvals[2])\n        f1, f2 = returnvals[4:6]\n        d1, d2, d3 = returnvals[7:10]\n        e1, e2, e3 = returnvals[11:14]\n\n        # if inputs are not scalar, each vector is an array of arrays,\n        # so reshape to a single array\n        if f1.dtype == object:\n            f1 = np.vstack(f1).T\n            f2 = np.vstack(f2).T\n            d1 = np.vstack(d1).T\n            d2 = np.vstack(d2).T\n            d3 = np.vstack(d3).T\n            e1 = np.vstack(e1).T\n            e2 = np.vstack(e2).T\n            e3 = np.vstack(e3).T\n\n        # make sure arrays are 2D\n        f1 = f1.reshape((2, f1.size//2))\n        f2 = f2.reshape((2, f2.size//2))\n        d1 = d1.reshape((3, d1.size//3))\n        d2 = d2.reshape((3, d2.size//3))\n        d3 = d3.reshape((3, d3.size//3))\n        e1 = e1.reshape((3, e1.size//3))\n        e2 = e2.reshape((3, e2.size//3))\n        e3 = e3.reshape((3, e3.size//3))\n\n        # compute f3, g1, g2, g3\n        F1 = np.vstack((f1, np.zeros_like(f1[0])))\n        F2 = np.vstack((f2, np.zeros_like(f2[0])))\n        F = np.cross(F1.T, F2.T).T[-1]\n        cosI = helpers.getcosIm(alat)\n        k = np.array([0, 0, 1], dtype=np.float64).reshape((3, 1))\n        g1 = ((self.RE + np.float64(height)) / (self.RE + self.refh))**(3/2) \\\n             * d1 / F\n        g2 = -1.0 / (2.0 * F * np.tan(np.radians(qlat))) * \\\n             (k + ((self.RE + np.float64(height)) / (self.RE + self.refh))\n              * d2 / cosI)\n        g3 = k*F\n        f3 = np.cross(g1.T, g2.T).T\n\n        if np.any(alat == -9999):\n            warnings.warn(('Base vectors g, d, e, and f3 set to -9999 where '\n                           'apex latitude is undefined (apex height may be < '\n                           'reference height)'))\n            f3 = np.where(alat == -9999, -9999, f3)\n            g1 = np.where(alat == -9999, -9999, g1)\n            g2 = np.where(alat == -9999, -9999, g2)\n            g3 = np.where(alat == -9999, -9999, g3)\n            d1 = np.where(alat == -9999, -9999, d1)\n            d2 = np.where(alat == -9999, -9999, d2)\n            d3 = np.where(alat == -9999, -9999, d3)\n            e1 = np.where(alat == -9999, -9999, e1)\n            e2 = np.where(alat == -9999, -9999, e2)\n            e3 = np.where(alat == -9999, -9999, e3)\n\n        return tuple(np.squeeze(x) for x in\n                     [f1, f2, f3, g1, g2, g3, d1, d2, d3, e1, e2, e3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the apex height of the specified location.", "response": "def get_apex(self, lat, height=None):\n        \"\"\" Calculate apex height\n\n        Parameters\n        -----------\n        lat : (float)\n            Latitude in degrees\n        height : (float or NoneType)\n            Height above the surface of the earth in km or NoneType to use\n            reference height (default=None)\n\n        Returns\n        ----------\n        apex_height : (float)\n            Height of the field line apex in km\n        \"\"\"\n        lat = helpers.checklat(lat, name='alat')\n        if height is None:\n            height = self.refh\n\n        cos_lat_squared = np.cos(np.radians(lat))**2\n        apex_height = (self.RE + height) / cos_lat_squared - self.RE\n\n        return apex_height"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the epoch for all subsequent conversions.", "response": "def set_epoch(self, year):\n        \"\"\"Updates the epoch for all subsequent conversions.\n\n        Parameters\n        ==========\n        year : float\n            Decimal year\n\n        \"\"\"\n\n        fa.loadapxsh(self.datafile, np.float(year))\n        self.year = year"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alt_parser(patterns):\n    from reparse.util import remove_lower_overlapping\n    get_first = lambda items: [i[0] for i in items]\n    get_second = lambda items: [i[1] for i in items]\n\n    def parse(line):\n        output = []\n        for pattern in patterns:\n            results = pattern.scan(line)\n            if results and any(results):\n                output.append((pattern.order, results))\n        return get_first(reduce(remove_lower_overlapping, get_second(sorted(output)), []))\n\n    return parse", "response": "A parser that can handle multiple different patterns\nTaxonomy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_tree_parser(patterns):\n    def output():\n        for pattern in patterns:\n            yield (pattern.build_full_tree(), pattern.regex)\n    return list(output())", "response": "This function builds a parser_type that returns an array of tuples where each element is a tree and each element is a regex."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parser(parser_type=basic_parser, functions=None, patterns=None, expressions=None, patterns_yaml_path=None,\n           expressions_yaml_path=None):\n    \"\"\" A Reparse parser description.\n        Simply provide the functions, patterns, & expressions to build.\n        If you are using YAML for expressions + patterns, you can use\n        ``expressions_yaml_path`` & ``patterns_yaml_path`` for convenience.\n\n        The default parser_type is the basic ordered parser.\n    \"\"\"\n    from reparse.builders import build_all\n    from reparse.validators import validate\n\n    def _load_yaml(file_path):\n        import yaml\n        with open(file_path) as f:\n            return yaml.safe_load(f)\n\n    assert expressions or expressions_yaml_path, \"Reparse can't build a parser without expressions\"\n    assert patterns or patterns_yaml_path, \"Reparse can't build a parser without patterns\"\n    assert functions, \"Reparse can't build without a functions\"\n\n    if patterns_yaml_path:\n        patterns = _load_yaml(patterns_yaml_path)\n    if expressions_yaml_path:\n        expressions = _load_yaml(expressions_yaml_path)\n    validate(patterns, expressions)\n\n    return parser_type(build_all(patterns, expressions, functions))", "response": "A basic ordered parser description."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _translate(self, input_filename, output_filename):\n        command = [\n            self.translate_binary,\n            '-f', 'GeoJSON',\n            output_filename,\n            input_filename\n        ]\n\n        result = self._runcommand(command)\n        self.log('Result (Translate): ', result, lvl=debug)", "response": "Translate KML file to geojson for import"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_guide(self, guide, update=False, clear=True):\n\n        kml_filename = os.path.join(self.cache_path, guide + '.kml')\n        geojson_filename = os.path.join(self.cache_path, guide + '.geojson')\n\n        if not os.path.exists(geojson_filename) or update:\n            try:\n                data = request.urlopen(self.guides[guide]).read().decode(\n                    'utf-8')\n            except (request.URLError, request.HTTPError) as e:\n                self.log('Could not get web guide data:', e, type(e), lvl=warn)\n                return\n\n            with open(kml_filename, 'w') as f:\n                f.write(data)\n\n            self._translate(kml_filename, geojson_filename)\n\n        with open(geojson_filename, 'r') as f:\n            json_data = json.loads(f.read())\n\n        if len(json_data['features']) == 0:\n            self.log('No features found!', lvl=warn)\n            return\n\n        layer = objectmodels['layer'].find_one({'name': guide})\n\n        if clear and layer is not None:\n            layer.delete()\n            layer = None\n\n        if layer is None:\n            layer_uuid = std_uuid()\n            layer = objectmodels['layer']({\n                'uuid': layer_uuid,\n                'name': guide,\n                'type': 'geoobjects'\n            })\n            layer.save()\n        else:\n            layer_uuid = layer.uuid\n\n        if clear:\n            for item in objectmodels['geoobject'].find({'layer': layer_uuid}):\n                self.log('Deleting old guide location', lvl=debug)\n                item.delete()\n\n        locations = []\n\n        for item in json_data['features']:\n            self.log('Adding new guide location:', item, lvl=verbose)\n            location = objectmodels['geoobject']({\n                'uuid': std_uuid(),\n                'layer': layer_uuid,\n                'geojson': item,\n                'type': 'Skipperguide',\n                'name': 'Guide for %s' % (item['properties']['Name'])\n            })\n            locations.append(location)\n\n        self.log('Bulk inserting guide locations', lvl=debug)\n        objectmodels['geoobject'].bulk_create(locations)", "response": "Update a single specified guide"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_mail(self, event):\n\n        mime_mail = MIMEText(event.text)\n        mime_mail['Subject'] = event.subject\n\n        if event.account == 'default':\n            account_name = self.config.default_account\n        else:\n            account_name = event.account\n\n        account = list(filter(lambda account: account['name'] == account_name, self.config.accounts))[0]\n\n        mime_mail['From'] = render(account['mail_from'], {'server': account['server'], 'hostname': self.hostname})\n        mime_mail['To'] = event.to_address\n\n        self.log('MimeMail:', mime_mail, lvl=verbose)\n        if self.config.mail_send is True:\n            self.log('Sending mail to', event.to_address)\n\n            self.fireEvent(task(send_mail_worker, account, mime_mail, event), \"mail-transmit-workers\")\n        else:\n            self.log('Not sending mail, here it is for debugging info:', mime_mail, pretty=True)", "response": "Connect to mail server and send actual email"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef provision_system_user(items, database_name, overwrite=False, clear=False, skip_user_check=False):\n\n    from hfos.provisions.base import provisionList\n    from hfos.database import objectmodels\n\n    # TODO: Add a root user and make sure owner can access it later.\n    # Setting up details and asking for a password here is not very useful,\n    # since this process is usually run automated.\n\n    if overwrite is True:\n        hfoslog('Refusing to overwrite system user!', lvl=warn,\n                emitter='PROVISIONS')\n        overwrite = False\n\n    system_user_count = objectmodels['user'].count({'name': 'System'})\n    if system_user_count == 0 or clear is False:\n        provisionList(Users, 'user', overwrite, clear,  skip_user_check=True)\n        hfoslog('Provisioning: Users: Done.', emitter=\"PROVISIONS\")\n    else:\n        hfoslog('System user already present.', lvl=warn, emitter='PROVISIONS')", "response": "Provision a system user"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngroup expressions using the OR character.", "response": "def AlternatesGroup(expressions, final_function, name=\"\"):\n    \"\"\" Group expressions using the OR character ``|``\n    >>> from collections import namedtuple\n    >>> expr = namedtuple('expr', 'regex group_lengths run')('(1)', [1], None)\n    >>> grouping = AlternatesGroup([expr, expr], lambda f: None, 'yeah')\n    >>> grouping.regex  # doctest: +IGNORE_UNICODE\n    '(?:(1))|(?:(1))'\n    >>> grouping.group_lengths\n    [1, 1]\n    \"\"\"\n    inbetweens = [\"|\"] * (len(expressions) + 1)\n    inbetweens[0] = \"\"\n    inbetweens[-1] = \"\"\n    return Group(expressions, final_function, inbetweens, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Group(expressions, final_function, inbetweens, name=\"\"):\n    lengths = []\n    functions = []\n    regex = \"\"\n    i = 0\n    for expression in expressions:\n        regex += inbetweens[i]\n        regex += \"(?:\" + expression.regex + \")\"\n        lengths.append(sum(expression.group_lengths))\n        functions.append(expression.run)\n        i += 1\n    regex += inbetweens[i]\n\n    return Expression(regex, functions, lengths, final_function, name)", "response": "Group expressions together with inbetweens and return a new Expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findall(self, string):\n        output = []\n        for match in self.pattern.findall(string):\n            if hasattr(match, 'strip'):\n                match = [match]\n            self._list_add(output, self.run(match))\n        return output", "response": "Parse string and return all outputs as parsed by functions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes findall but also returns matching start and end string locations", "response": "def scan(self, string):\n        \"\"\" Like findall, but also returning matching start and end string locations\n        \"\"\"\n        return list(self._scanner_to_matches(self.pattern.scanner(string), self.run))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns group functions over matches", "response": "def run(self, matches):\n        \"\"\" Run group functions over matches\n        \"\"\"\n        def _run(matches):\n            group_starting_pos = 0\n            for current_pos, (group_length, group_function) in enumerate(zip(self.group_lengths, self.group_functions)):\n                start_pos = current_pos + group_starting_pos\n                end_pos = current_pos + group_starting_pos + group_length\n                yield group_function(matches[start_pos:end_pos])\n                group_starting_pos += group_length - 1\n        return self.final_function(list(_run(matches)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_muted(what):\n\n    state = False\n\n    for item in solo:\n        if item not in what:\n            state = True\n        else:\n            state = False\n            break\n\n    for item in mute:\n        if item in what:\n            state = True\n            break\n\n    return state", "response": "Checks if a logged event is to be muted for debugging purposes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging all what arguments.", "response": "def hfoslog(*what, **kwargs):\n    \"\"\"Logs all *what arguments.\n\n    :param *what: Loggable objects (i.e. they have a string representation)\n    :param lvl: Debug message level\n    :param exc: Switch to better handle exceptions, use if logging in an\n                except clause\n    :param emitter: Optional log source, where this can't be determined\n                    automatically\n    :param sourceloc: Give specific source code location hints, used internally\n    \"\"\"\n\n    # Count all messages (missing numbers give a hint at too high log level)\n    global count\n    global verbosity\n\n    count += 1\n\n    lvl = kwargs.get('lvl', info)\n\n    if lvl < verbosity['global']:\n        return\n\n    emitter = kwargs.get('emitter', 'UNKNOWN')\n    traceback = kwargs.get('tb', False)\n    frame_ref = kwargs.get('frame_ref', 0)\n\n    output = None\n\n    timestamp = time.time()\n    runtime = timestamp - start\n\n    callee = None\n\n    exception = kwargs.get('exc', False)\n\n    if exception:\n        exc_type, exc_obj, exc_tb = sys.exc_info()  # NOQA\n\n    if verbosity['global'] <= debug or traceback:\n        # Automatically log the current function details.\n\n        if 'sourceloc' not in kwargs:\n            frame = kwargs.get('frame', frame_ref)\n\n            # Get the previous frame in the stack, otherwise it would\n            # be this function\n            current_frame = inspect.currentframe()\n            while frame > 0:\n                frame -= 1\n                current_frame = current_frame.f_back\n\n            func = current_frame.f_code\n            # Dump the message + the name of this function to the log.\n\n            if exception:\n                line_no = exc_tb.tb_lineno\n                if lvl <= error:\n                    lvl = error\n            else:\n                line_no = func.co_firstlineno\n\n            callee = \"[%.10s@%s:%i]\" % (\n                func.co_name,\n                func.co_filename,\n                line_no\n            )\n        else:\n            callee = kwargs['sourceloc']\n\n    now = time.asctime()\n\n    msg = \"[%s] : %5s : %.5f : %3i : [%5s]\" % (now,\n                                               lvldata[lvl][0],\n                                               runtime,\n                                               count,\n                                               emitter)\n\n    content = \"\"\n\n    if callee:\n        if not uncut and lvl > 10:\n            msg += \"%-60s\" % callee\n        else:\n            msg += \"%s\" % callee\n\n    for thing in what:\n        content += \" \"\n        if kwargs.get('pretty', False):\n            content += pprint.pformat(thing)\n        else:\n            content += str(thing)\n\n    msg += content\n\n    if exception:\n        msg += \"\\n\" + \"\".join(format_exception(exc_type, exc_obj, exc_tb))\n\n    if is_muted(msg):\n        return\n\n    if not uncut and lvl > 10 and len(msg) > 1000:\n        msg = msg[:1000]\n\n    if lvl >= verbosity['file']:\n        try:\n            f = open(logfile, \"a\")\n            f.write(msg + '\\n')\n            f.flush()\n            f.close()\n        except IOError:\n            print(\"Can't open logfile %s for writing!\" % logfile)\n            # sys.exit(23)\n\n    if is_marked(msg):\n        lvl = hilight\n\n    if lvl >= verbosity['console']:\n        output = str(msg)\n        if six.PY3 and color:\n            output = lvldata[lvl][1] + output + terminator\n        try:\n            print(output)\n        except UnicodeEncodeError as e:\n            print(output.encode(\"utf-8\"))\n            hfoslog(\"Bad encoding encountered on previous message:\", e,\n                    lvl=error)\n        except BlockingIOError:\n            hfoslog(\"Too long log line encountered:\", output[:20], lvl=warn)\n\n    if live:\n        item = [now, lvl, runtime, count, emitter, str(content)]\n        LiveLog.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tagged(self, event):\n        self.log(\"Tagged objects request for\", event.data, \"from\",\n                 event.user, lvl=debug)\n        if event.data in self.tags:\n            tagged = self._get_tagged(event.data)\n\n            response = {\n                'component': 'hfos.events.schemamanager',\n                'action': 'get',\n                'data': tagged\n            }\n            self.fireEvent(send(event.client.uuid, response))\n        else:\n            self.log(\"Unavailable schema requested!\", lvl=warn)", "response": "Return a list of tagged objects for a schema"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprovisioning the default system vessel", "response": "def provision_system_vessel(items, database_name, overwrite=False, clear=False, skip_user_check=False):\n    \"\"\"Provisions the default system vessel\"\"\"\n\n    from hfos.provisions.base import provisionList\n    from hfos.database import objectmodels\n\n    vessel = objectmodels['vessel'].find_one({'name': 'Default System Vessel'})\n    if vessel is not None:\n        if overwrite is False:\n            hfoslog('Default vessel already existing. Skipping provisions.')\n            return\n        else:\n            vessel.delete()\n\n    provisionList([SystemVessel], 'vessel', overwrite, clear, skip_user_check)\n\n    sysconfig = objectmodels['systemconfig'].find_one({'active': True})\n    hfoslog('Adapting system config for default vessel:', sysconfig)\n    sysconfig.vesseluuid = SystemVessel['uuid']\n    sysconfig.save()\n\n    hfoslog('Provisioning: Vessel: Done.', emitter='PROVISIONS')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tile(url):\n    log = \"\"\n    connection = None\n\n    try:\n        if six.PY3:\n            connection = urlopen(url=url, timeout=2)  # NOQA\n        else:\n            connection = urlopen(url=url)\n    except Exception as e:\n        log += \"MTST: ERROR Tilegetter error: %s \" % str([type(e), e, url])\n\n    content = \"\"\n\n    # Read and return requested content\n    if connection:\n        try:\n            content = connection.read()\n        except (socket.timeout, socket.error) as e:\n            log += \"MTST: ERROR Tilegetter error: %s \" % str([type(e), e])\n\n        connection.close()\n    else:\n        log += \"MTST: ERROR Got no connection.\"\n\n    return content, log", "response": "Threadable function to retrieve map tiles from the internet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking and caches a requested tile to disk then delivers it to client", "response": "def tilecache(self, event, *args, **kwargs):\n        \"\"\"Checks and caches a requested tile to disk, then delivers it to\n        client\"\"\"\n        request, response = event.args[:2]\n        self.log(request.path, lvl=verbose)\n        try:\n            filename, url = self._split_cache_url(request.path, 'tilecache')\n        except UrlError:\n            return\n\n        # self.log('CACHE QUERY:', filename, url)\n\n        # Do we have the tile already?\n        if os.path.isfile(filename):\n            self.log(\"Tile exists in cache\", lvl=verbose)\n            # Don't set cookies for static content\n            response.cookie.clear()\n            try:\n                yield serve_file(request, response, filename)\n            finally:\n                event.stop()\n        else:\n            # We will have to get it first.\n            self.log(\"Tile not cached yet. Tile data: \", filename, url,\n                     lvl=verbose)\n            if url in self._tiles:\n                self.log(\"Getting a tile for the second time?!\", lvl=error)\n            else:\n                self._tiles += url\n            try:\n                tile, log = yield self.call(task(get_tile, url), \"tcworkers\")\n                if log:\n                    self.log(\"Thread error: \", log, lvl=error)\n            except Exception as e:\n                self.log(\"[MTS]\", e, type(e))\n                tile = None\n\n            tile_path = os.path.dirname(filename)\n\n            if tile:\n                try:\n                    os.makedirs(tile_path)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        self.log(\n                            \"Couldn't create path: %s (%s)\" % (e, type(e)), lvl=error)\n\n                self.log(\"Caching tile.\", lvl=verbose)\n                try:\n                    with open(filename, \"wb\") as tile_file:\n                        try:\n                            tile_file.write(bytes(tile))\n                        except Exception as e:\n                            self.log(\"Writing error: %s\" % str([type(e), e]), lvl=error)\n\n                except Exception as e:\n                    self.log(\"Open error on %s - %s\" % (filename, str([type(e), e])), lvl=error)\n                    return\n                finally:\n                    event.stop()\n\n                try:\n                    self.log(\"Delivering tile.\", lvl=verbose)\n                    yield serve_file(request, response, filename)\n                except Exception as e:\n                    self.log(\"Couldn't deliver tile: \", e, lvl=error)\n                    event.stop()\n                self.log(\"Tile stored and delivered.\", lvl=verbose)\n            else:\n                self.log(\"Got no tile, serving default tile: %s\" % url)\n                if self.default_tile:\n                    try:\n                        yield serve_file(request, response, self.default_tile)\n                    except Exception as e:\n                        self.log('Cannot deliver default tile:', e, type(e),\n                                 exc=True, lvl=error)\n                    finally:\n                        event.stop()\n                else:\n                    yield"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef todegdec(origin):\n\n    # if the input is already a float (or can be converted to float)\n    try:\n        return float(origin)\n    except ValueError:\n        pass\n\n    # DMS format\n    m = dms_re.search(origin)\n    if m:\n        degrees = int(m.group('degrees'))\n        minutes = float(m.group('minutes'))\n        seconds = float(m.group('seconds'))\n\n        return degrees + minutes / 60 + seconds / 3600\n\n    # Degree + Minutes format\n    m = mindec_re.search(origin)\n    if m:\n        degrees = int(m.group('degrees'))\n        minutes = float(m.group('minutes'))\n\n        return degrees + minutes / 60", "response": "Convert from [ + ] DDD\u00b0MMM. MMMM or [ + ] DDD\u00b0MMM. MMMM or [ + ] DDDD. DDDDD\u00b0MMMM. MMMM or [ + ] DDDD. DDDDD\u00b0MMMM. MMMM or [ + ] DDDD. DDDDD\u00b0MMMM. MMMM or [ + ] DDDD. DDDDD\u00b0MMMM. MMMM or [ +"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting [ + - ] DDD. DDDDD to a tuple", "response": "def tomindec(origin):\n    \"\"\"\n    Convert [+/-]DDD.DDDDD to a tuple (degrees, minutes)\n    \"\"\"\n\n    origin = float(origin)\n    degrees = int(origin)\n    minutes = (origin % 1) * 60\n\n    return degrees, minutes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a number from a UTC time to a Minutely - readable string.", "response": "def tomindecstr(origin):\n    \"\"\"\n    Convert [+/-]DDD.DDDDD to [+/-]DDD\u00b0MMM.MMMM'\n    \"\"\"\n\n    degrees, minutes = tomindec(origin)\n    return u'%d\u00b0%f\\'' % (degrees, minutes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef todms(origin):\n\n    degrees, minutes = tomindec(origin)\n    seconds = (minutes % 1) * 60\n\n    return degrees, int(minutes), seconds", "response": "Convert [ + - ] DDDD. DDDDD to a tuple"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef todmsstr(origin):\n\n    degrees, minutes, seconds = todms(origin)\n    return u'%d\u00b0%d\\'%f\"' % (degrees, minutes, seconds)", "response": "Convert a number to a string in the format of YYYYMMDD. DDDDD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef towgs84(E, N, pkm=False, presentation=None):\n\n    _lng0 = lng0pkm if pkm else lng0\n\n    E /= 1000.0\n    N /= 1000.0\n    epsilon = (N-N0) / (k0*A)\n    eta = (E-E0) / (k0*A)\n\n    epsilonp = epsilon - beta1*sin(2*1*epsilon)*cosh(2*1*eta) - \\\n                         beta2*sin(2*2*epsilon)*cosh(2*2*eta) - \\\n                         beta3*sin(2*3*epsilon)*cosh(2*3*eta)\n    etap = eta - beta1*cos(2*1*epsilon)*sinh(2*1*eta) - \\\n                 beta2*cos(2*2*epsilon)*sinh(2*2*eta) - \\\n                 beta3*cos(2*3*epsilon)*sinh(2*3*eta)\n    sigmap = 1 - 2*1*beta1*cos(2*1*epsilon)*cosh(2*1*eta) - \\\n                 2*2*beta2*cos(2*2*epsilon)*cosh(2*2*eta) - \\\n                 2*3*beta3*cos(2*3*epsilon)*cosh(2*3*eta)\n    taup = 2*1*beta1*sin(2*1*epsilon)*sinh(2*1*eta) + \\\n           2*2*beta2*sin(2*2*epsilon)*sinh(2*2*eta) + \\\n           2*3*beta3*sin(2*3*epsilon)*sinh(2*3*eta)\n\n    chi = asin(sin(epsilonp) / cosh(etap))\n\n    latitude = chi + delta1*sin(2*1*chi) + \\\n                     delta2*sin(2*2*chi) + \\\n                     delta3*sin(2*3*chi)\n\n    longitude = _lng0 + atan(sinh(etap) / cos(epsilonp))\n\n    func = None\n    presentation = 'to%s' % presentation if presentation else None\n    if presentation in presentations:\n        func = getattr(sys.modules[__name__], presentation)\n\n    if func and func != 'todegdec':\n        return func(degrees(latitude)), func(degrees(longitude))\n\n    return (degrees(latitude), degrees(longitude))", "response": "Convert coordintes from TWD97 to WGS84"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert coordinates from WGS84 to TWD97", "response": "def fromwgs84(lat, lng, pkm=False):\n    \"\"\"\n    Convert coordintes from WGS84 to TWD97\n\n    pkm true for Penghu, Kinmen and Matsu area\n    The latitude and longitude can be in the following formats:\n        [+/-]DDD\u00b0MMM'SSS.SSSS\" (unicode)\n        [+/-]DDD\u00b0MMM.MMMM' (unicode)\n        [+/-]DDD.DDDDD (string, unicode or float)\n    The returned coordinates are in meters\n    \"\"\"\n\n    _lng0 = lng0pkm if pkm else lng0\n\n    lat = radians(todegdec(lat))\n    lng = radians(todegdec(lng))\n\n    t = sinh((atanh(sin(lat)) - 2*pow(n,0.5)/(1+n)*atanh(2*pow(n,0.5)/(1+n)*sin(lat))))\n    epsilonp = atan(t/cos(lng-_lng0))\n    etap = atan(sin(lng-_lng0) / pow(1+t*t, 0.5))\n\n    E = E0 + k0*A*(etap + alpha1*cos(2*1*epsilonp)*sinh(2*1*etap) + \n                          alpha2*cos(2*2*epsilonp)*sinh(2*2*etap) +\n                          alpha3*cos(2*3*epsilonp)*sinh(2*3*etap))\n    N = N0 + k0*A*(epsilonp + alpha1*sin(2*1*epsilonp)*cosh(2*1*etap) +\n                              alpha2*sin(2*2*epsilonp)*cosh(2*2*etap) +\n                              alpha3*sin(2*3*epsilonp)*cosh(2*3*etap))\n\n    return E*1000, N*1000"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clipValue(self, value, minValue, maxValue):\n        '''\n        Makes sure that value is within a specific range.\n        If not, then the lower or upper bounds is returned\n        '''\n        return min(max(value, minValue), maxValue)", "response": "Makes sure that the value is within a specific range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the ground resolution for based on latitude and zoom level", "response": "def getGroundResolution(self, latitude, level):\n        '''\n        returns the ground resolution for based on latitude and zoom level.\n        '''\n        latitude = self.clipValue(latitude, self.min_lat, self.max_lat);\n        mapSize = self.getMapDimensionsByZoomLevel(level)\n        return math.cos(\n            latitude * math.pi / 180) * 2 * math.pi * self.earth_radius / \\\n               mapSize"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the map scale on the dpi of the screen", "response": "def getMapScale(self, latitude, level, dpi=96):\n        '''\n        returns the map scale on the dpi of the screen\n        '''\n        dpm = dpi / 0.0254  # convert to dots per meter\n        return self.getGroundResolution(latitude, level) * dpm"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convertLatLngToPixelXY(self, lat, lng, level):\n        '''\n        returns the x and y values of the pixel corresponding to a latitude\n        and longitude.\n        '''\n        mapSize = self.getMapDimensionsByZoomLevel(level)\n\n        lat = self.clipValue(lat, self.min_lat, self.max_lat)\n        lng = self.clipValue(lng, self.min_lng, self.max_lng)\n\n        x = (lng + 180) / 360\n        sinlat = math.sin(lat * math.pi / 180)\n        y = 0.5 - math.log((1 + sinlat) / (1 - sinlat)) / (4 * math.pi)\n\n        pixelX = int(self.clipValue(x * mapSize + 0.5, 0, mapSize - 1))\n        pixelY = int(self.clipValue(y * mapSize + 0.5, 0, mapSize - 1))\n        return (pixelX, pixelY)", "response": "converts a latitude and longitude to the x and y values of the pixel corresponding to a latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convertPixelXYToLngLat(self, pixelX, pixelY, level):\n        '''\n        converts a pixel x, y to a latitude and longitude.\n        '''\n        mapSize = self.getMapDimensionsByZoomLevel(level)\n        x = (self.clipValue(pixelX, 0, mapSize - 1) / mapSize) - 0.5\n        y = 0.5 - (self.clipValue(pixelY, 0, mapSize - 1) / mapSize)\n\n        lat = 90 - 360 * math.atan(math.exp(-y * 2 * math.pi)) / math.pi\n        lng = 360 * x\n\n        return (lng, lat)", "response": "converts a pixel x y to a longitude and latitude."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tileXYZToQuadKey(self, x, y, z):\n        '''\n        Computes quadKey value based on tile x, y and z values.\n        '''\n        quadKey = ''\n        for i in range(z, 0, -1):\n            digit = 0\n            mask = 1 << (i - 1)\n            if (x & mask) != 0:\n                digit += 1\n            if (y & mask) != 0:\n                digit += 2\n            quadKey += str(digit)\n        return quadKey", "response": "Computes the quadKey value based on tile x y z values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a quadKey to a tile x y z value based on the quadKey.", "response": "def quadKeyToTileXYZ(self, quadKey):\n        '''\n        Computes tile x, y and z values based on quadKey.\n        '''\n        tileX = 0\n        tileY = 0\n        tileZ = len(quadKey)\n\n        for i in range(tileZ, 0, -1):\n            mask = 1 << (i - 1)\n            value = quadKey[tileZ - i]\n\n            if value == '0':\n                continue\n\n            elif value == '1':\n                tileX |= mask\n\n            elif value == '2':\n                tileY |= mask\n\n            elif value == '3':\n                tileX |= mask\n                tileY |= mask\n\n            else:\n                raise Exception('Invalid QuadKey')\n\n        return (tileX, tileY, tileZ)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the upper - left hand corner lat / lng for a tile.", "response": "def getTileOrigin(self, tileX, tileY, level):\n        '''\n        Returns the upper-left hand corner lat/lng for a tile\n        '''\n        pixelX, pixelY = self.convertTileXYToPixelXY(tileX, tileY)\n        lng, lat = self.convertPixelXYToLngLat(pixelX, pixelY, level)\n        return (lat, lng)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of tile urls by extent.", "response": "def getTileUrlsByLatLngExtent(self, xmin, ymin, xmax, ymax, level):\n        '''\n        Returns a list of tile urls by extent\n        '''\n        # Upper-Left Tile\n        tileXMin, tileYMin = self.tileUtils.convertLngLatToTileXY(xmin, ymax,\n                                                                  level)\n\n        # Lower-Right Tile\n        tileXMax, tileYMax = self.tileUtils.convertLngLatToTileXY(xmax, ymin,\n                                                                  level)\n\n        tileUrls = []\n\n        for y in range(tileYMax, tileYMin - 1, -1):\n            for x in range(tileXMin, tileXMax + 1, 1):\n                tileUrls.append(self.createTileUrl(x, y, level))\n\n        return tileUrls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createTileUrl(self, x, y, z):\n        '''\n        returns new tile url based on template\n        '''\n        return self.tileTemplate.replace('{{x}}', str(x)).replace('{{y}}', str(\n            y)).replace('{{z}}', str(z))", "response": "returns new tile url based on template\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle navigational reference frame updates.", "response": "def referenceframe(self, event):\n        \"\"\"Handles navigational reference frame updates.\n        These are necessary to assign geo coordinates to alerts and other\n        misc things.\n\n        :param event with incoming referenceframe message\n        \"\"\"\n\n        self.log(\"Got a reference frame update! \", event, lvl=events)\n\n        self.reference_frame = event.data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef userlogin(self, event):\n\n        client_uuid = event.clientuuid\n\n        self.log(event.user, pretty=True, lvl=verbose)\n\n        self.log('Adding client')\n        self.clients[event.clientuuid] = event.user\n\n        for topic, alert in self.alerts.items():\n            self.alert(client_uuid, alert)", "response": "Checks if an alert is ongoing and alerts the newly connected\n        client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trigger(self, event):\n\n        topic = event.data.get('topic', None)\n        if topic is None:\n            self.log('No alert topic to trigger', lvl=warn)\n            return\n\n        alert = {\n            'topic': topic,\n            'message': event.data.get('msg', 'Alert has been triggered'),\n            'role': event.data.get('role', 'all')\n        }\n\n        self._trigger(event, alert)", "response": "Trigger an alert for the given event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cancel(self, event):\n\n        topic = event.data.get('topic', None)\n        if topic is None:\n            self.log('No alert topic to cancel', lvl=warn)\n            return\n        self._cancel(topic)", "response": "Cancels an alert from the given topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(ctx, instance, quiet, verbose, log_level, dbhost, dbname):\n\n    ctx.obj['instance'] = instance\n\n    if dbname == db_default and instance != 'default':\n        dbname = instance\n\n    ctx.obj['quiet'] = quiet\n    ctx.obj['verbose'] = verbose\n    verbosity['console'] = log_level\n    verbosity['global'] = log_level\n\n    ctx.obj['dbhost'] = dbhost\n    ctx.obj['dbname'] = dbname", "response": "This is the main isomer management tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    from datetime import datetime\n\n    # Initialize Command-Line and User-Config Settings, Log\n    # -----------------------------------------------------\n\n    beg_time = datetime.now()\n    # Process command-line arguments to determine action\n    # If no subcommand (e.g. 'import') is given, returns 'None' --> exit\n    args, sub_clargs = load_command_line_args()\n    if args is None:\n        return\n\n    # Create a logging object\n    log = load_log(args)\n\n    # Run configuration/setup interactive script\n    if args.command == 'setup':\n        setup_user_config(log)\n        return\n\n    # Make sure configuration file exists, or that's what we're doing\n    # (with the 'setup' subcommand)\n    if not os.path.isfile(_CONFIG_PATH):\n        raise RuntimeError(\"'{}' does not exist.  \"\n                           \"Run `astrocats setup` to configure.\"\n                           \"\".format(_CONFIG_PATH))\n\n    git_vers = get_git()\n    title_str = \"Astrocats, version: {}, SHA: {}\".format(__version__, git_vers)\n    log.warning(\"\\n\\n{}\\n{}\\n{}\\n\".format(title_str, '=' * len(title_str),\n                                          beg_time.ctime()))\n\n    # Load the user settings from the home directory\n    args = load_user_config(args, log)\n\n    # Choose Catalog and Operation(s) to perform\n    # ------------------------------------------\n    mod_name = args.command\n    log.debug(\"Importing specified module: '{}'\".format(mod_name))\n    # Try to import the specified module\n    try:\n        mod = importlib.import_module('.' + mod_name, package='astrocats')\n    except Exception as err:\n        log.error(\"Import of specified module '{}' failed.\".format(mod_name))\n        log_raise(log, str(err), type(err))\n\n    # Run the `main.main` method of the specified module\n    log.debug(\"Running `main.main()`\")\n    mod.main.main(args, sub_clargs, log)\n\n    end_time = datetime.now()\n    log.warning(\"\\nAll complete at {}, After {}\".format(end_time, end_time -\n                                                        beg_time))\n    return", "response": "Entry point for the AstroCats main function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup_user_config(log):\n    log.warning(\"AstroCats Setup\")\n    log.warning(\"Configure filepath: '{}'\".format(_CONFIG_PATH))\n\n    # Create path to configuration file as needed\n    config_path_dir = os.path.split(_CONFIG_PATH)[0]\n    if not os.path.exists(config_path_dir):\n        log.debug(\"Creating config directory '{}'\".format(config_path_dir))\n        os.makedirs(config_path_dir)\n\n    if not os.path.isdir(config_path_dir):\n        log_raise(log, \"Configure path error '{}'\".format(config_path_dir))\n\n    # Determine default settings\n\n    # Get this containing directory and use that as default data path\n    def_base_path = os.path.abspath(os.path.dirname(os.path.abspath(__file__)))\n    log.warning(\"Setting '{}' to default path: '{}'\".format(_BASE_PATH_KEY,\n                                                            def_base_path))\n    config = {_BASE_PATH_KEY: def_base_path}\n\n    # Write settings to configuration file\n    json.dump(config, open(_CONFIG_PATH, 'w'))\n    if not os.path.exists(def_base_path):\n        log_raise(log, \"Problem creating configuration file.\")\n\n    return", "response": "Setup a configuration file in the user s home directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_user_config(args, log):\n    if not os.path.exists(_CONFIG_PATH):\n        err_str = (\n            \"Configuration file does not exists ({}).\\n\".format(_CONFIG_PATH) +\n            \"Run `python -m astrocats setup` to configure.\")\n        log_raise(log, err_str)\n\n    config = json.load(open(_CONFIG_PATH, 'r'))\n    setattr(args, _BASE_PATH_KEY, config[_BASE_PATH_KEY])\n    log.debug(\"Loaded configuration: {}: {}\".format(_BASE_PATH_KEY, config[\n        _BASE_PATH_KEY]))\n    return args", "response": "Load settings from the user s confiuration file and add them to the args object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading and parse command - line arguments.", "response": "def load_command_line_args(clargs=None):\n    \"\"\"Load and parse command-line arguments.\n\n    Arguments\n    ---------\n    args : str or None\n        'Faked' commandline arguments passed to `argparse`.\n\n    Returns\n    -------\n    args : `argparse.Namespace` object\n        Namespace in which settings are stored - default values modified by the\n        given command-line arguments.\n\n    \"\"\"\n    import argparse\n    git_vers = get_git()\n\n    parser = argparse.ArgumentParser(\n        prog='astrocats',\n        description='Generate catalogs for astronomical data.')\n\n    parser.add_argument('command', nargs='?', default=None)\n\n    parser.add_argument(\n        '--version',\n        action='version',\n        version='AstroCats v{}, SHA: {}'.format(__version__, git_vers))\n    parser.add_argument(\n        '--verbose',\n        '-v',\n        dest='verbose',\n        default=False,\n        action='store_true',\n        help='Print more messages to the screen.')\n    parser.add_argument(\n        '--debug',\n        '-d',\n        dest='debug',\n        default=False,\n        action='store_true',\n        help='Print excessive messages to the screen.')\n    parser.add_argument(\n        '--include-private',\n        dest='private',\n        default=False,\n        action='store_true',\n        help='Include private data in import.')\n    parser.add_argument(\n        '--travis',\n        '-t',\n        dest='travis',\n        default=False,\n        action='store_true',\n        help='Run import script in test mode for Travis.')\n    parser.add_argument(\n        '--clone-depth',\n        dest='clone_depth',\n        default=0,\n        type=int,\n        help=('When cloning git repos, only clone out to this depth '\n              '(default: 0 = all levels).'))\n    parser.add_argument(\n        '--purge-outputs',\n        dest='purge_outputs',\n        default=False,\n        action='store_true',\n        help=('Purge git outputs after cloning.'))\n    parser.add_argument(\n        '--log',\n        dest='log_filename',\n        default=None,\n        help='Filename to which to store logging information.')\n\n    # If output files should be written or not\n    # ----------------------------------------\n    write_group = parser.add_mutually_exclusive_group()\n    write_group.add_argument(\n        '--write',\n        action='store_true',\n        dest='write_entries',\n        default=True,\n        help='Write entries to files [default].')\n    write_group.add_argument(\n        '--no-write',\n        action='store_false',\n        dest='write_entries',\n        default=True,\n        help='do not write entries to file.')\n\n    # If previously cleared output files should be deleted or not\n    # -----------------------------------------------------------\n    delete_group = parser.add_mutually_exclusive_group()\n    delete_group.add_argument(\n        '--predelete',\n        action='store_true',\n        dest='delete_old',\n        default=True,\n        help='Delete all old event files to begin [default].')\n    delete_group.add_argument(\n        '--no-predelete',\n        action='store_false',\n        dest='delete_old',\n        default=True,\n        help='Do not delete all old event files to start.')\n\n    args, sub_clargs = parser.parse_known_args(args=clargs)\n    # Print the help information if no command is given\n    if args.command is None:\n        parser.print_help()\n        return None, None\n\n    return args, sub_clargs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_log(args):\n    from astrocats.catalog.utils import logger\n\n    # Determine verbosity ('None' means use default)\n    log_stream_level = None\n    if args.debug:\n        log_stream_level = logger.DEBUG\n    elif args.verbose:\n        log_stream_level = logger.INFO\n\n    # Create log\n    log = logger.get_logger(\n        stream_level=log_stream_level, tofile=args.log_filename)\n    log._verbose = args.verbose\n    log._debug = args.debug\n    return log", "response": "Load a logging. Logger object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_dicts(old_full, new_full, old_data, new_data, depth=0):\n    depth = depth + 1\n    indent = \"  \"*depth\n\n    # Print with an indentation matching the nested-dictionary depth\n    def my_print(str):\n        print(\"{}{}\".format(indent, str))\n\n    old_keys = list(old_data.keys())\n    # Compare data key by key, in *this* dictionary level\n    # Note: since we're comparing by keys explicity, order doesnt matter\n    for key in old_keys:\n        # Remove elements as we go\n        old_vals = old_data.pop(key)\n        # Current key\n        my_print(\"{}\".format(key))\n        # If `new_data` doesnt also have this key, return False\n        if key not in new_data:\n            my_print(\"Key '{}' not in new_data.\".format(key))\n            my_print(\"Old:\")\n            my_print(pprint(new_data))\n            my_print(\"New:\")\n            my_print(pprint(new_data))\n            return False\n\n        # If it does have the key, extract the values (remove as we go)\n        new_vals = new_data.pop(key)\n        # If these values are a sub-dictionary, compare those\n        if isinstance(old_vals, dict) and isinstance(new_vals, dict):\n            # If the sub-dictionary are not the same, return False\n            if not compare_dicts(old_full, new_full, old_vals, new_vals, depth=depth):\n                return False\n        # If these values are a list of sub-dictionaries, compare each of those\n        elif (isinstance(old_vals, list) and isinstance(old_vals[0], dict) and\n              isinstance(old_vals, list) and isinstance(old_vals[0], dict)):\n            for old_elem, new_elem in zip_longest(old_vals, new_vals):\n                # If one or the other has extra elements, print message, but\n                # continue on\n                if old_elem is None or new_elem is None:\n                    my_print(\"Missing element!\")\n                    my_print(\"\\tOld: '{}'\".format(old_elem))\n                    my_print(\"\\tNew: '{}'\".format(new_elem))\n                else:\n                    if not compare_dicts(old_full, new_full, old_elem, new_elem, depth=depth):\n                        return False\n\n        # At the lowest-dictionary level, compare the values themselves\n        else:\n            # Turn everything into a list for convenience (most things should be\n            # already)\n            if  (not isinstance(old_vals, list) and\n                 not isinstance(new_vals, list)):\n                old_vals = [old_vals]\n                new_vals = [new_vals]\n\n            # Sort both lists\n            old_vals = sorted(old_vals)\n            new_vals = sorted(new_vals)\n\n            for oldv, newv in zip_longest(old_vals, new_vals):\n                # If one or the other has extra elements, print message, but\n                # continue on\n                if oldv is None or newv is None:\n                    my_print(\"Missing element!\")\n                    my_print(\"\\tOld: '{}'\".format(oldv))\n                    my_print(\"\\tNew: '{}'\".format(newv))\n                # If values match, continue\n                elif oldv == newv:\n                    my_print(\"Good Match: '{}'\".format(key))\n                # If values dont match, return False\n                else:\n                    my_print(\"Bad  Match: '{}'\".format(key))\n                    my_print(\"\\tOld: '{}'\".format(oldv))\n                    my_print(\"\\tNew: '{}'\".format(newv))\n                    return False\n\n    return True", "response": "Function compares dictionaries by key - value recursively."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclip a line to a rectangular area. This implements the Cohen-Sutherland line clipping algorithm. xmin, ymax, xmax and ymin denote the clipping area, into which the line defined by x1, y1 (start point) and x2, y2 (end point) will be clipped. If the line does not intersect with the rectangular clipping area, four None values will be returned as tuple. Otherwise a tuple of the clipped line points will be returned in the form (cx1, cy1, cx2, cy2).", "response": "def cohensutherland(xmin, ymax, xmax, ymin, x1, y1, x2, y2):\n    \"\"\"Clips a line to a rectangular area.\n\n    This implements the Cohen-Sutherland line clipping algorithm.  xmin,\n    ymax, xmax and ymin denote the clipping area, into which the line\n    defined by x1, y1 (start point) and x2, y2 (end point) will be\n    clipped.\n\n    If the line does not intersect with the rectangular clipping area,\n    four None values will be returned as tuple. Otherwise a tuple of the\n    clipped line points will be returned in the form (cx1, cy1, cx2, cy2).\n    \"\"\"\n    INSIDE, LEFT, RIGHT, LOWER, UPPER = 0, 1, 2, 4, 8\n\n    def _getclip(xa, ya):\n        #if dbglvl>1: print('point: '),; print(xa,ya)\n        p = INSIDE  # default is inside\n\n        # consider x\n        if xa < xmin:\n            p |= LEFT\n        elif xa > xmax:\n            p |= RIGHT\n\n        # consider y\n        if ya < ymin:\n            p |= LOWER  # bitwise OR\n        elif ya > ymax:\n            p |= UPPER  # bitwise OR\n        return p\n\n# check for trivially outside lines\n    k1 = _getclip(x1, y1)\n    k2 = _getclip(x2, y2)\n\n# %% examine non-trivially outside points\n    # bitwise OR |\n    while (k1 | k2) != 0:  # if both points are inside box (0000) , ACCEPT trivial whole line in box\n\n        # if line trivially outside window, REJECT\n        if (k1 & k2) != 0:  # bitwise AND &\n            #if dbglvl>1: print('  REJECT trivially outside box')\n            # return nan, nan, nan, nan\n            return None, None, None, None\n\n        # non-trivial case, at least one point outside window\n        # this is not a bitwise or, it's the word \"or\"\n        opt = k1 or k2  # take first non-zero point, short circuit logic\n        if opt & UPPER:  # these are bitwise ANDS\n            x = x1 + (x2 - x1) * (ymax - y1) / (y2 - y1)\n            y = ymax\n        elif opt & LOWER:\n            x = x1 + (x2 - x1) * (ymin - y1) / (y2 - y1)\n            y = ymin\n        elif opt & RIGHT:\n            y = y1 + (y2 - y1) * (xmax - x1) / (x2 - x1)\n            x = xmax\n        elif opt & LEFT:\n            y = y1 + (y2 - y1) * (xmin - x1) / (x2 - x1)\n            x = xmin\n        else:\n            raise RuntimeError('Undefined clipping state')\n\n        if opt == k1:\n            x1, y1 = x, y\n            k1 = _getclip(x1, y1)\n            #if dbglvl>1: print('checking k1: ' + str(x) + ',' + str(y) + '    ' + str(k1))\n        elif opt == k2:\n            #if dbglvl>1: print('checking k2: ' + str(x) + ',' + str(y) + '    ' + str(k2))\n            x2, y2 = x, y\n            k2 = _getclip(x2, y2)\n    return x1, y1, x2, y2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setupuv(rc):\n    if cv is not None:\n        (r, c) = rc\n        u = cv.CreateMat(r, c, cv.CV_32FC1)\n        v = cv.CreateMat(r, c, cv.CV_32FC1)\n        return (u, v)\n    else:\n        return [None]*2", "response": "Horn Schunck legacy OpenCV function requires we use these old - fashioned cv matrices not numpy array\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_cat_dict(self, cat_dict_class, key_in_self, **kwargs):\n        # Catch errors associated with crappy, but not unexpected data\n        try:\n            new_entry = cat_dict_class(self, key=key_in_self, **kwargs)\n        except CatDictError as err:\n            if err.warn:\n                self._log.info(\"'{}' Not adding '{}': '{}'\".format(self[\n                    self._KEYS.NAME], key_in_self, str(err)))\n            return None\n        return new_entry", "response": "Initialize a CatDict object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a CatDict to this Entry.", "response": "def _add_cat_dict(self,\n                      cat_dict_class,\n                      key_in_self,\n                      check_for_dupes=True,\n                      **kwargs):\n        \"\"\"Add a CatDict to this Entry if initialization succeeds and it\n        doesn't already exist within the Entry.\n        \"\"\"\n        # Try to create a new instance of this subclass of `CatDict`\n        new_entry = self._init_cat_dict(cat_dict_class, key_in_self, **kwargs)\n        if new_entry is None:\n            return False\n\n        # Compare this new entry with all previous entries to make sure is new\n        if cat_dict_class != Error:\n            for item in self.get(key_in_self, []):\n                if new_entry.is_duplicate_of(item):\n                    item.append_sources_from(new_entry)\n                    # Return the entry in case we want to use any additional\n                    # tags to augment the old entry\n                    return new_entry\n\n        self.setdefault(key_in_self, []).append(new_entry)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pbar(iter, desc='', **kwargs):\n    return tqdm(\n        iter,\n        desc=('<' + str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) + '> ' +\n              desc),\n        dynamic_ncols=True,\n        **kwargs)", "response": "Wrapper for tqdm progress bar.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pbar_strings(files, desc='', **kwargs):\n    return tqdm(\n        sorted(files, key=lambda s: s.lower()),\n        desc=('<' + str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) + '> ' +\n              desc),\n        dynamic_ncols=True,\n        **kwargs)", "response": "Wrapper for tqdm which also sorts list of strings\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the task priority corresponding to the given task_priority.", "response": "def _get_task_priority(tasks, task_priority):\n    \"\"\"Get the task `priority` corresponding to the given `task_priority`.\n\n    If `task_priority` is an integer or 'None', return it.\n    If `task_priority` is a str, return the priority of the task it matches.\n    Otherwise, raise `ValueError`.\n    \"\"\"\n    if task_priority is None:\n        return None\n    if is_integer(task_priority):\n        return task_priority\n    if isinstance(task_priority, basestring):\n        if task_priority in tasks:\n            return tasks[task_priority].priority\n\n    raise ValueError(\"Unrecognized task priority '{}'\".format(task_priority))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_data(self):\n\n        tasks_list = self.load_task_list()\n        warnings.filterwarnings(\n            'ignore', r'Warning: converting a masked element to nan.')\n        # FIX\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n\n        # Delete all old (previously constructed) output files\n        if self.args.delete_old:\n            self.log.warning(\"Deleting all old entry files.\")\n            self.delete_old_entry_files()\n\n        # In update mode, load all entry stubs.\n        if self.args.load_stubs or self.args.update:\n            self.load_stubs()\n\n        if self.args.travis:\n            self.log.warning(\"Running in `travis` mode.\")\n\n        prev_priority = 0\n        prev_task_name = ''\n        # for task, task_obj in tasks_list.items():\n        for task_name, task_obj in tasks_list.items():\n            if not task_obj.active:\n                continue\n            self.log.warning(\"Task: '{}'\".format(task_name))\n\n            nice_name = task_obj.nice_name\n            mod_name = task_obj.module\n            func_name = task_obj.function\n            priority = task_obj.priority\n\n            # Make sure things are running in the correct order\n            if priority < prev_priority and priority > 0:\n                raise RuntimeError(\"Priority for '{}': '{}', less than prev,\"\n                                   \"'{}': '{}'.\\n{}\"\n                                   .format(task_name, priority, prev_task_name,\n                                           prev_priority, task_obj))\n\n            self.log.debug(\"\\t{}, {}, {}, {}\".format(nice_name, priority,\n                                                     mod_name, func_name))\n            mod = importlib.import_module('.' + mod_name, package='astrocats')\n            self.current_task = task_obj\n            getattr(mod, func_name)(self)\n\n            num_events, num_stubs = self.count()\n            self.log.warning(\"Task finished.  Events: {},  Stubs: {}\".format(\n                num_events, num_stubs))\n            self.journal_entries()\n            num_events, num_stubs = self.count()\n            self.log.warning(\"Journal finished.  Events: {}, Stubs: {}\".format(\n                num_events, num_stubs))\n\n            prev_priority = priority\n            prev_task_name = task_name\n\n        process = psutil.Process(os.getpid())\n        memory = process.memory_info().rss\n        self.log.warning('Memory used (MBs): '\n                         '{:,}'.format(memory / 1024. / 1024.))\n        return", "response": "Run all of the import tasks. A task is loaded and the function is imported."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_task_list(self):\n        # In update mode, do not delete old files\n        if self.args.update:\n            self.log.info(\"Disabling `pre-delete` for 'update' mode.\")\n            self.args.delete_old = False\n\n        # Dont allow both a 'min' and 'max' task priority\n        # FIX: this is probably unnecessary... having both could be useful\n        if ((self.args.min_task_priority is not None and\n             self.args.max_task_priority is not None)):\n            raise ValueError(\"Can only use *either* 'min' *or* 'max' priority\")\n\n        # Load tasks data from input json file\n        tasks, task_names = self._load_task_list_from_file()\n\n        # Make sure 'active' modification lists are all valid\n        args_lists = [\n            self.args.args_task_list, self.args.yes_task_list,\n            self.args.no_task_list\n        ]\n        args_names = ['--tasks', '--yes', '--no']\n        for arglist, lname in zip(args_lists, args_names):\n            if arglist is not None:\n                for tname in arglist:\n                    if tname not in task_names:\n                        raise ValueError(\n                            \"Value '{}' in '{}' list does not match\"\n                            \" any tasks\".format(tname, lname))\n\n        # Process min/max priority specification ('None' if none given)\n        min_priority = _get_task_priority(tasks, self.args.min_task_priority)\n        max_priority = _get_task_priority(tasks, self.args.max_task_priority)\n        task_groups = self.args.task_groups\n        if task_groups is not None:\n            if not isinstance(task_groups, list):\n                task_groups = [task_groups]\n\n        # Iterate over all tasks to determine which should be (in)active\n        # --------------------------------------------------------------\n        for key in tasks:\n            # If in update mode, only run update tasks\n            if self.args.update:\n                if not tasks[key].update:\n                    tasks[key].active = False\n\n            # If specific list of tasks is given, make only those active\n            if self.args.args_task_list is not None:\n                if key in self.args.args_task_list:\n                    tasks[key].active = True\n                else:\n                    tasks[key].active = False\n\n            # Only run tasks above minimum priority\n            # (doesn't modify negtive priority tasks)\n            if min_priority is not None and tasks[key].priority >= 0:\n                tasks[key].active = False\n                if tasks[key].priority >= min_priority:\n                    tasks[key].active = True\n\n            # Only run tasks below maximum priority\n            # (doesnt modify negative priority tasks)\n            if max_priority is not None and tasks[key].priority >= 0:\n                tasks[key].active = False\n                if tasks[key].priority <= max_priority:\n                    tasks[key].active = True\n\n            # Set 'yes' tasks to *active*\n            if self.args.yes_task_list is not None:\n                if key in self.args.yes_task_list:\n                    tasks[key].active = True\n            # Set 'no' tasks to *inactive*\n            if self.args.no_task_list is not None:\n                if key in self.args.no_task_list:\n                    tasks[key].active = False\n            # Set tasks in target 'groups' to *active*\n            if task_groups is not None and tasks[key].groups is not None:\n                # Go through each group defined in the command line\n                for given_group in task_groups:\n                    # If this task is a member of any of those groups\n                    if given_group in tasks[key].groups:\n                        tasks[key].active = True\n                        break\n\n        # Sort entries as positive values, then negative values\n        #    [0, 1, 2, 2, 10, -100, -10, -1]\n        # Tuples are sorted by first element (here: '0' if positive), then\n        # second (here normal order)\n        tasks = OrderedDict(\n            sorted(\n                tasks.items(),\n                key=lambda t: (t[1].priority < 0, t[1].priority, t[1].name)))\n\n        # Find the first task that has \"always_journal\" set to True\n        for key in tasks:\n            if tasks[key].active and tasks[key].always_journal:\n                self.min_journal_priority = tasks[key].priority\n                break\n\n        names_act = []\n        names_inact = []\n        for key, val in tasks.items():\n            if val.active:\n                names_act.append(key)\n            else:\n                names_inact.append(key)\n\n        self.log.info(\"Active Tasks:\\n\\t\" + \", \".join(nn for nn in names_act))\n        self.log.debug(\"Inactive Tasks:\\n\\t\" + \", \".join(nn for nn in\n                                                         names_inact))\n        return tasks", "response": "Load the list of tasks in this catalog s input / tasks. json file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_entry(self, name, load=True, delete=True):\n        newname = self.clean_entry_name(name)\n\n        if not newname:\n            raise (ValueError('Fatal: Attempted to add entry with no name.'))\n\n        # If entry already exists, return\n        if newname in self.entries:\n            self.log.debug(\"`newname`: '{}' (name: '{}') already exists.\".\n                           format(newname, name))\n            # If this is a stub, we need to continue, possibly load file\n            if self.entries[newname]._stub:\n                self.log.debug(\"'{}' is a stub\".format(newname))\n            # If a full (non-stub) event exists, return its name\n            else:\n                self.log.debug(\"'{}' is not a stub, returning\".format(newname))\n                return newname\n\n        # If entry is alias of another entry in `entries`, find and return that\n        match_name = self.find_entry_name_of_alias(newname)\n        if match_name is not None:\n            self.log.debug(\n                \"`newname`: '{}' (name: '{}') already exists as alias for \"\n                \"'{}'.\".format(newname, name, match_name))\n            newname = match_name\n\n        # Load entry from file\n        if load:\n            loaded_name = self.load_entry_from_name(newname, delete=delete)\n            if loaded_name:\n                return loaded_name\n\n        # If we match an existing event, return that\n        if match_name is not None:\n            return match_name\n\n        # Create new entry\n        new_entry = self.proto(catalog=self, name=newname)\n        new_entry[self.proto._KEYS.SCHEMA] = self.SCHEMA.URL\n        self.log.log(self.log._LOADED,\n                     \"Created new entry for '{}'\".format(newname))\n        # Add entry to dictionary\n        self.entries[newname] = new_entry\n        return newname", "response": "Add an entry to the entries dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the first entry name with the given alias included in its list of aliases.", "response": "def find_entry_name_of_alias(self, alias):\n        \"\"\"Return the first entry name with the given 'alias' included in its\n        list of aliases.\n\n        Returns\n        -------\n        name of matching entry (str) or 'None' if no matches\n\n        \"\"\"\n        if alias in self.aliases:\n            name = self.aliases[alias]\n            if name in self.entries:\n                return name\n            else:\n                # Name wasn't found, possibly merged or deleted. Now look\n                # really hard.\n                for name, entry in self.entries.items():\n                    aliases = entry.get_aliases(includename=False)\n                    if alias in aliases:\n                        if (ENTRY.DISTINCT_FROM not in entry or\n                                alias not in entry[ENTRY.DISTINCT_FROM]):\n                            return name\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies an entry from one entry to another.", "response": "def copy_entry_to_entry(self,\n                            fromentry,\n                            destentry,\n                            check_for_dupes=True,\n                            compare_to_existing=True):\n        \"\"\"Used by `merge_duplicates`\n        \"\"\"\n        self.log.info(\"Copy entry object '{}' to '{}'\".format(fromentry[\n            fromentry._KEYS.NAME], destentry[destentry._KEYS.NAME]))\n\n        newsourcealiases = {}\n        if self.proto._KEYS.SOURCES in fromentry:\n            for source in fromentry[self.proto._KEYS.SOURCES]:\n                alias = source.pop(SOURCE.ALIAS)\n                newsourcealiases[alias] = source\n\n        newmodelaliases = {}\n        if self.proto._KEYS.MODELS in fromentry:\n            for model in fromentry[self.proto._KEYS.MODELS]:\n                alias = model.pop(MODEL.ALIAS)\n                newmodelaliases[alias] = model\n\n        if self.proto._KEYS.ERRORS in fromentry:\n            for err in fromentry[self.proto._KEYS.ERRORS]:\n                destentry.setdefault(self.proto._KEYS.ERRORS, []).append(err)\n\n        for rkey in fromentry:\n            key = fromentry._KEYS.get_key_by_name(rkey)\n            if key.no_source:\n                continue\n            for item in fromentry[key]:\n                # isd = False\n                if 'source' not in item:\n                    raise ValueError(\"Item has no source!\")\n\n                nsid = []\n                for sid in item['source'].split(','):\n                    if sid in newsourcealiases:\n                        source = newsourcealiases[sid]\n                        nsid.append(destentry.add_source(**source))\n                    else:\n                        raise ValueError(\"Couldn't find source alias!\")\n                item['source'] = uniq_cdl(nsid)\n\n                if 'model' in item:\n                    nmid = []\n                    for mid in item['model'].split(','):\n                        if mid in newmodelaliases:\n                            model = newmodelaliases[mid]\n                            nmid.append(destentry.add_model(**model))\n                        else:\n                            raise ValueError(\"Couldn't find model alias!\")\n                    item['model'] = uniq_cdl(nmid)\n\n                if key == ENTRY.PHOTOMETRY:\n                    destentry.add_photometry(\n                        compare_to_existing=compare_to_existing,\n                        **item)\n                elif key == ENTRY.SPECTRA:\n                    destentry.add_spectrum(\n                        compare_to_existing=compare_to_existing,\n                        **item)\n                elif key == ENTRY.ERRORS:\n                    destentry.add_error(**item)\n                elif key == ENTRY.MODELS:\n                    continue\n                else:\n                    destentry.add_quantity(\n                        compare_to_existing=compare_to_existing,\n                        check_for_dupes=False, quantities=key, **item)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_duplicates(self):\n        if len(self.entries) == 0:\n            self.log.error(\"WARNING: `entries` is empty, loading stubs\")\n            if self.args.update:\n                self.log.warning(\n                    \"No sources changed, entry files unchanged in update.\"\n                    \"  Skipping merge.\")\n                return\n            self.entries = self.load_stubs()\n\n        task_str = self.get_current_task_str()\n\n        keys = list(sorted(self.entries.keys()))\n        n1 = 0\n        mainpbar = tqdm(total=len(keys), desc=task_str)\n        while n1 < len(keys):\n            name1 = keys[n1]\n            if name1 not in self.entries:\n                self.log.info(\"Entry for {} not found, likely already \"\n                              \"deleted in merging process.\".format(name1))\n                n1 = n1 + 1\n                mainpbar.update(1)\n                continue\n            allnames1 = set(self.entries[name1].get_aliases() + self.entries[\n                name1].extra_aliases())\n\n            # Search all later names\n            for name2 in keys[n1 + 1:]:\n                if name1 == name2:\n                    continue\n                if name1 not in self.entries:\n                    self.log.info(\"Entry for {} not found, likely already \"\n                                  \"deleted in merging process.\".format(name1))\n                    continue\n                if name2 not in self.entries:\n                    self.log.info(\"Entry for {} not found, likely already \"\n                                  \"deleted in merging process.\".format(name2))\n                    continue\n\n                allnames2 = set(self.entries[name2].get_aliases() +\n                                self.entries[name2].extra_aliases())\n\n                # If there are any common names or aliases, merge\n                if len(allnames1 & allnames2):\n                    self.log.warning(\"Found two entries with common aliases \"\n                                     \"('{}' and '{}'), merging.\".format(name1,\n                                                                        name2))\n\n                    load1 = self.proto.init_from_file(self, name=name1)\n                    load2 = self.proto.init_from_file(self, name=name2)\n                    if load1 is not None and load2 is not None:\n                        # Delete old files\n                        self._delete_entry_file(entry=load1)\n                        self._delete_entry_file(entry=load2)\n                        self.entries[name1] = load1\n                        self.entries[name2] = load2\n                        priority1 = 0\n                        priority2 = 0\n                        for an in allnames1:\n                            if an.startswith(self.entries[name1]\n                                             .priority_prefixes()):\n                                priority1 += 1\n                        for an in allnames2:\n                            if an.startswith(self.entries[name2]\n                                             .priority_prefixes()):\n                                priority2 += 1\n\n                        if priority1 > priority2:\n                            self.copy_to_entry_in_catalog(name2, name1)\n                            keys.append(name1)\n                            del self.entries[name2]\n                        else:\n                            self.copy_to_entry_in_catalog(name1, name2)\n                            keys.append(name2)\n                            del self.entries[name1]\n                    else:\n                        self.log.warning('Duplicate already deleted')\n\n                    # if len(self.entries) != 1:\n                    #     self.log.error(\n                    #         \"WARNING: len(entries) = {}, expected 1.  \"\n                    #         \"Still journaling...\".format(len(self.entries)))\n                    self.journal_entries()\n\n            if self.args.travis and n1 > self.TRAVIS_QUERY_LIMIT:\n                break\n            n1 = n1 + 1\n            mainpbar.update(1)\n        mainpbar.close()", "response": "Merge and remove duplicate entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all events in their stub form.", "response": "def load_stubs(self, log_mem=False):\n        \"\"\"Load all events in their `stub` (name, alias, etc only) form.\n\n        Used in `update` mode.\n        \"\"\"\n        # Initialize parameter related to diagnostic output of memory usage\n        if log_mem:\n            import psutil\n            process = psutil.Process(os.getpid())\n            rss = process.memory_info().rss\n            LOG_MEMORY_INT = 1000\n            MEMORY_LIMIT = 1000.0\n\n        def _add_stub_manually(_fname):\n            \"\"\"Create and add a 'stub' by manually loading parameters from\n            JSON files.\n\n            Previously this was done by creating a full `Entry` instance, then\n            using the `Entry.get_stub()` method to trim it down.  This was very\n            slow and memory intensive, hence this improved approach.\n            \"\"\"\n            # FIX: should this be ``fi.endswith(``.gz')`` ?\n            fname = uncompress_gz(_fname) if '.gz' in _fname else _fname\n\n            stub = None\n            stub_name = None\n            with codecs.open(fname, 'r') as jfil:\n                # Load the full JSON file\n                data = json.load(jfil, object_pairs_hook=OrderedDict)\n                # Extract the top-level keys (should just be the name of the\n                # entry)\n                stub_name = list(data.keys())\n                # Make sure there is only a single top-level entry\n                if len(stub_name) != 1:\n                    err = \"json file '{}' has multiple keys: {}\".format(\n                        fname, list(stub_name))\n                    self._log.error(err)\n                    raise ValueError(err)\n                stub_name = stub_name[0]\n\n                # Make sure a non-stub entry doesnt already exist with this\n                # name\n                if stub_name in self.entries and not self.entries[\n                        stub_name]._stub:\n                    err_str = (\n                        \"ERROR: non-stub entry already exists with name '{}'\"\n                        .format(stub_name))\n                    self.log.error(err_str)\n                    raise RuntimeError(err_str)\n\n                # Remove the outmost dict level\n                data = data[stub_name]\n                # Create a new `Entry` (subclass) instance\n                proto = self.proto\n                stub = proto(catalog=self, name=stub_name, stub=True)\n                # Add stub parameters if they are available\n                if proto._KEYS.ALIAS in data:\n                    stub[proto._KEYS.ALIAS] = data[proto._KEYS.ALIAS]\n                if proto._KEYS.DISTINCT_FROM in data:\n                    stub[proto._KEYS.DISTINCT_FROM] = data[\n                        proto._KEYS.DISTINCT_FROM]\n                if proto._KEYS.RA in data:\n                    stub[proto._KEYS.RA] = data[proto._KEYS.RA]\n                if proto._KEYS.DEC in data:\n                    stub[proto._KEYS.DEC] = data[proto._KEYS.DEC]\n                if proto._KEYS.DISCOVER_DATE in data:\n                    stub[proto._KEYS.DISCOVER_DATE] = data[\n                        proto._KEYS.DISCOVER_DATE]\n                if proto._KEYS.SOURCES in data:\n                    stub[proto._KEYS.SOURCES] = data[\n                        proto._KEYS.SOURCES]\n\n            # Store the stub\n            self.entries[stub_name] = stub\n            self.log.debug(\"Added stub for '{}'\".format(stub_name))\n\n        currenttask = 'Loading entry stubs'\n        files = self.PATHS.get_repo_output_file_list()\n        for ii, _fname in enumerate(pbar(files, currenttask)):\n            # Run normally\n            # _add_stub(_fname)\n\n            # Run 'manually' (extract stub parameters directly from JSON)\n            _add_stub_manually(_fname)\n\n            if log_mem:\n                rss = process.memory_info().rss / 1024 / 1024\n                if ii % LOG_MEMORY_INT == 0 or rss > MEMORY_LIMIT:\n                    log_memory(self.log, \"\\nLoaded stub {}\".format(ii),\n                               logging.INFO)\n                    if rss > MEMORY_LIMIT:\n                        err = (\n                            \"Memory usage {}, has exceeded {} on file {} '{}'\".\n                            format(rss, MEMORY_LIMIT, ii, _fname))\n                        self.log.error(err)\n                        raise RuntimeError(err)\n\n        return self.entries"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _delete_entry_file(self, entry_name=None, entry=None):\n        if entry_name is None and entry is None:\n            raise RuntimeError(\"Either `entry_name` or `entry` must be given.\")\n        elif entry_name is not None and entry is not None:\n            raise RuntimeError(\"Cannot use both `entry_name` and `entry`.\")\n\n        if entry_name is not None:\n            entry = self.entries[entry_name]\n        else:\n            entry_name = entry[ENTRY.NAME]\n\n        # FIX: do we also need to check for gzipped files??\n        entry_filename = self.entry_filename(entry_name)\n\n        if self.args.write_entries:\n            self.log.info(\"Deleting entry file '{}' of entry '{}'\".format(\n                entry_filename, entry_name))\n            if not os.path.exists(entry_filename):\n                self.log.error(\n                    \"Filename '{}' does not exist\".format(entry_filename))\n            os.remove(entry_filename)\n        else:\n            self.log.debug(\"Not deleting '{}' because `write_entries`\"\n                           \" is False\".format(entry_filename))\n\n        return", "response": "Delete the file associated with the given entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef journal_entries(self,\n                        clear=True,\n                        gz=False,\n                        bury=False,\n                        write_stubs=False,\n                        final=False):\n        \"\"\"Write all entries in `entries` to files, and clear.  Depending on\n        arguments and `tasks`.\n\n        Iterates over all elements of `entries`, saving (possibly 'burying')\n        and deleting.\n        -   If ``clear == True``, then each element of `entries` is deleted,\n            and a `stubs` entry is added\n        \"\"\"\n\n        # if (self.current_task.priority >= 0 and\n        #        self.current_task.priority < self.min_journal_priority):\n        #    return\n\n        # Write it all out!\n        # NOTE: this needs to use a `list` wrapper to allow modification of\n        # dict\n        for name in list(self.entries.keys()):\n            if self.args.write_entries:\n                # If this is a stub and we aren't writing stubs, skip\n                if self.entries[name]._stub and not write_stubs:\n                    continue\n\n                # Bury non-SN entries here if only claimed type is non-SN type,\n                # or if primary name starts with a non-SN prefix.\n                bury_entry = False\n                save_entry = True\n                if bury:\n                    (bury_entry, save_entry) = self.should_bury(name)\n\n                if save_entry:\n                    save_name = self.entries[name].save(\n                        bury=bury_entry, final=final)\n                    self.log.info(\n                        \"Saved {} to '{}'.\".format(name.ljust(20), save_name))\n                    if (gz and os.path.getsize(save_name) >\n                            self.COMPRESS_ABOVE_FILESIZE):\n                        save_name = compress_gz(save_name)\n                        self.log.debug(\n                            \"Compressed '{}' to '{}'\".format(name, save_name))\n                        # FIX: use subprocess\n                        outdir, filename = os.path.split(save_name)\n                        filename = filename.split('.')[0]\n                        os.system('cd ' + outdir + '; git rm --cached ' +\n                                  filename + '.json; git add -f ' + filename +\n                                  '.json.gz; cd ' + self.PATHS.PATH_BASE)\n\n            if clear:\n                self.entries[name] = self.entries[name].get_stub()\n                self.log.debug(\"Entry for '{}' converted to stub\".format(name))\n\n        return", "response": "Write all entries in entries to files and clear them if clear == True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchoosing between each entry given name and its possible aliases for the best one.", "response": "def set_preferred_names(self):\n        \"\"\"Choose between each entries given name and its possible aliases for\n        the best one.\n        \"\"\"\n        if len(self.entries) == 0:\n            self.log.error(\"WARNING: `entries` is empty, loading stubs\")\n            self.load_stubs()\n\n        task_str = self.get_current_task_str()\n        for ni, oname in enumerate(pbar(self.entries, task_str)):\n            name = self.add_entry(oname)\n            self.entries[name].set_preferred_name()\n\n            if self.args.travis and ni > self.TRAVIS_QUERY_LIMIT:\n                break\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prep_git_add_file_list(self,\n                                repo,\n                                size_limit,\n                                fail=True,\n                                file_types=None):\n        \"\"\"Get a list of files which should be added to the given repository.\n\n        Notes\n        -----\n        * Finds files in the *root* of the given repository path.\n        * If `file_types` is given, only use those file types.\n        * If an uncompressed file is above the `size_limit`, it is compressed.\n        * If a compressed file is above the file limit, an error is raised\n          (if `fail = True`) or it is skipped (if `fail == False`).\n\n        Arguments\n        ---------\n        repo : str\n            Path to repository\n        size_limit : scalar\n        fail : bool\n            Raise an error if a compressed file is still above the size limit.\n        file_types : list of str or None\n            Exclusive list of file types to add. 'None' to add all filetypes.\n\n        \"\"\"\n        add_files = []\n        if file_types is None:\n            file_patterns = ['*']\n        else:\n            self.log.error(\n                \"WARNING: uncertain behavior with specified file types!\")\n            file_patterns = ['*.' + ft for ft in file_types]\n\n        # Construct glob patterns for each file-type\n        file_patterns = [os.path.join(repo, fp) for fp in file_patterns]\n        for pattern in file_patterns:\n            file_list = glob(pattern)\n            for ff in file_list:\n                fsize = os.path.getsize(ff)\n                fname = str(ff)\n                comp_failed = False\n                # If the found file is too large\n                if fsize > size_limit:\n                    self.log.debug(\"File '{}' size '{}' MB.\".format(\n                        fname, fsize / 1028 / 1028))\n                    # If the file is already compressed... fail or skip\n                    if ff.endswith('.gz'):\n                        self.log.error(\n                            \"File '{}' is already compressed.\".format(fname))\n                        comp_failed = True\n                    # Not yet compressed - compress it\n                    else:\n                        fname = compress_gz(fname)\n                        fsize = os.path.getsize(fname)\n                        self.log.info(\"Compressed to '{}', size '{}' MB\".\n                                      format(fname, fsize / 1028 / 1028))\n                        # If still too big, fail or skip\n                        if fsize > size_limit:\n                            comp_failed = True\n\n                # If compressed file is too large, skip file or raise error\n                if comp_failed:\n                    # Raise an error\n                    if fail:\n                        raise RuntimeError(\n                            \"File '{}' cannot be added!\".format(fname))\n                    # Skip file without adding it\n                    self.log.info(\"Skipping file.\")\n                    continue\n\n                # If everything is good, add file to list\n                add_files.append(fname)\n\n        return add_files", "response": "Prepare a list of files which should be added to the given repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_url(self,\n                 url,\n                 fname,\n                 repo=None,\n                 timeout=120,\n                 post=None,\n                 fail=False,\n                 write=True,\n                 json_sort=None,\n                 cache_only=False,\n                 archived_mode=None,\n                 archived_task=None,\n                 update_mode=None,\n                 verify=False):\n        \"\"\"Load the given URL, or a cached-version.\n\n        Load page from url or cached file, depending on the current settings.\n        'archived' mode applies when `args.archived` is true (from\n        `--archived` CL argument), and when this task has `Task.archived`\n        also set to True.\n\n        'archived' mode:\n            * Try to load from cached file.\n            * If cache does not exist, try to load from web.\n            * If neither works, raise an error if ``fail == True``,\n              otherwise return None\n        non-'archived' mode:\n            * Try to load from url, save to cache file.\n            * If url fails, try to load existing cache file.\n            * If neither works, raise an error if ``fail == True``,\n              otherwise return None\n\n        'update' mode:\n            * In update mode, try to compare URL to cached file.\n            * If URL fails, return None\n              (cannot update)\n            * If URL data matches cached data, return None\n              (dont need to update)\n            * If URL is different from data, return url data\n              (proceed with update)\n\n        Arguments\n        ---------\n        self\n        url : str\n            URL to download.\n        fname : str\n            Filename to which to save/load cached file.  Inludes suffix.\n            NOTE: in general, this should be the source's BIBCODE.\n        repo : str or None\n            The full path of the data-repository the cached file should be\n            saved/loaded from.  If 'None', then the current task is used to\n            determine the repo.\n        timeout : int\n            Time (in seconds) after which a URL query should exit.\n        post : dict\n            List of arguments to post to URL when requesting it.\n        archived : bool\n            Load a previously archived version of the file.\n        fail : bool\n            If the file/url cannot be loaded, raise an error.\n        write : bool\n            Save a new copy of the cached file.\n        json_sort : str or None\n            If data is being saved to a json file, sort first by this str.\n        quiet : bool\n            Whether to emit error messages upon being unable to find files.\n        verify : bool\n            Whether to check for valid SSL cert when downloading\n\n        \"\"\"\n        file_txt = None\n        url_txt = None\n\n        # Load default settings if needed\n        # -------------------------------\n        # Determine if we are running in archived mode\n        if archived_mode is None:\n            archived_mode = self.args.archived\n        # Determine if this task is one which uses archived files\n        if archived_task is None:\n            archived_task = self.current_task.archived\n        # Determine if running in update mode\n        if update_mode is None:\n            update_mode = self.args.update\n\n        # Construct the cached filename\n        if repo is None:\n            repo = self.get_current_task_repo()\n        cached_path = os.path.join(repo, fname)\n\n        # Load cached file if it exists\n        # ----------------------------\n        if os.path.isfile(cached_path):\n            with codecs.open(cached_path, 'r', encoding='utf8') as infile:\n                file_txt = infile.read()\n                self.log.debug(\"Task {}: Loaded from '{}'.\".format(\n                    self.current_task.name, cached_path))\n\n        # In `archived` mode and task - try to return the cached page\n        if archived_mode or (archived_task and not update_mode):\n            if file_txt is not None:\n                return file_txt\n\n            # If this flag is set, don't even attempt to download from web\n            if cache_only:\n                return None\n\n            # If file does not exist, log error, continue\n            else:\n                self.log.error(\"Task {}: Cached file '{}' does not exist.\".\n                               format(self.current_task.name, cached_path))\n\n        # Load url.  'None' is returned on failure - handle that below\n        url_txt = self.download_url(\n            url, timeout, fail=False, post=post, verify=verify)\n\n        # At this point, we might have both `url_txt` and `file_txt`\n        # If either of them failed, then they are set to None\n\n        # If URL download failed, error or return cached data\n        # ---------------------------------------------------\n        if url_txt is None:\n            # Both sources failed\n            if file_txt is None:\n                err_str = \"Both url and file retrieval failed!\"\n                # If we should raise errors on failure\n                if fail:\n                    err_str += \" `fail` set.\"\n                    self.log.error(err_str)\n                    raise RuntimeError(err_str)\n                # Otherwise warn and return None\n                self.log.warning(err_str)\n                return None\n\n            # Otherwise, if only url failed, return file data\n            else:\n                # If we are trying to update, but the url failed, then return\n                # None\n                if update_mode:\n                    self.log.error(\n                        \"Cannot check for updates, url download failed.\")\n                    return None\n                # Otherwise, return file data\n                self.log.warning(\"URL download failed, using cached data.\")\n                return file_txt\n\n        # Here: `url_txt` exists, `file_txt` may exist or may be None\n        # Determine if update should happen, and if file should be resaved\n\n        # Write new url_txt to cache file\n        # -------------------------------\n        if write:\n            self.log.info(\n                \"Writing `url_txt` to file '{}'.\".format(cached_path))\n            self._write_cache_file(url_txt, cached_path, json_sort=json_sort)\n        # If `file_txt` doesnt exist but were not writing.. warn\n        elif file_txt is None:\n            err_str = \"Warning: cached file '{}' does not exist.\".format(\n                cached_path)\n            err_str += \" And is not being saved.\"\n            self.log.warning(err_str)\n\n        # Check if we need to update this data\n        # ------------------------------------\n        # If both `url_txt` and `file_txt` exist and update mode check MD5\n        if file_txt is not None and update_mode:\n            from hashlib import md5\n            url_md5 = md5(url_txt.encode('utf-8')).hexdigest()\n            file_md5 = md5(file_txt.encode('utf-8')).hexdigest()\n            self.log.debug(\"URL: '{}', File: '{}'.\".format(url_md5, file_md5))\n            # If the data is the same, no need to parse (update), return None\n            if url_md5 == file_md5:\n                self.log.info(\n                    \"Skipping file '{}', no changes.\".format(cached_path))\n                return None\n            else:\n                self.log.info(\"File '{}' has been updated\".format(cached_path))\n                # Warn if we didnt save a new copy\n                if not write:\n                    err_str = \"Warning: updated data not saved to file.\"\n                    self.log.warning(err_str)\n\n        return url_txt", "response": "Load a cached - version from a URL or a cached - version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload text from the given url.", "response": "def download_url(self, url, timeout, fail=False, post=None, verify=True):\n        \"\"\"Download text from the given url.\n\n        Returns `None` on failure.\n\n        Arguments\n        ---------\n        self\n        url : str\n            URL web address to download.\n        timeout : int\n            Duration after which URL request should terminate.\n        fail : bool\n            If `True`, then an error will be raised on failure.\n            If `False`, then 'None' is returned on failure.\n        post : dict\n            List of arguments to post to URL when requesting it.\n        verify : bool\n            Whether to check for valid SSL cert when downloading\n\n        Returns\n        -------\n        url_txt : str or None\n            On success the text of the url is returned.  On failure `None` is\n            returned.\n\n        \"\"\"\n        _CODE_ERRORS = [500, 307, 404]\n        import requests\n        session = requests.Session()\n\n        try:\n            headers = {\n                'User-Agent':\n                'Mozilla/5.0 (Macintosh; Intel Mac OS X '\n                '10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) '\n                'Chrome/39.0.2171.95 Safari/537.36'\n            }\n            if post:\n                response = session.post(\n                    url,\n                    timeout=timeout,\n                    headers=headers,\n                    data=post,\n                    verify=verify)\n            else:\n                response = session.get(\n                    url, timeout=timeout, headers=headers, verify=verify)\n            response.raise_for_status()\n            # Look for errors\n            for xx in response.history:\n                xx.raise_for_status()\n                if xx.status_code in _CODE_ERRORS:\n                    self.log.error(\"URL response returned status code '{}'\".\n                                   format(xx.status_code))\n                    raise\n\n            url_txt = response.text\n            self.log.debug(\"Task {}: Loaded `url_txt` from '{}'.\".format(\n                self.current_task.name, url))\n\n        except (KeyboardInterrupt, SystemExit):\n            raise\n\n        except Exception as err:\n            err_str = (\"URL Download of '{}' failed ('{}').\"\n                       .format(url, str(err)))\n            # Raise an error on failure\n            if fail:\n                err_str += \" and `fail` is set.\"\n                self.log.error(err_str)\n                raise RuntimeError(err_str)\n            # Log a warning on error, and return None\n            else:\n                self.log.warning(err_str)\n                return None\n\n        return url_txt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge the source alias lists of two CatDicts.", "response": "def append_sources_from(self, other):\n        \"\"\"Merge the source alias lists of two CatDicts.\"\"\"\n        # Get aliases lists from this `CatDict` and other\n        self_aliases = self[self._KEYS.SOURCE].split(',')\n        other_aliases = other[self._KEYS.SOURCE].split(',')\n\n        # Store alias to `self`\n        self[self._KEYS.SOURCE] = uniq_cdl(self_aliases + other_aliases)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current task string for the current action for progress - bar output.", "response": "def current_task(self, args):\n        \"\"\"Name of current action for progress-bar output.\n\n        The specific task string is depends on the configuration via `args`.\n\n        Returns\n        -------\n        ctask : str\n            String representation of this task.\n        \"\"\"\n        ctask = self.nice_name if self.nice_name is not None else self.name\n        if args is not None:\n            if args.update:\n                ctask = ctask.replace('%pre', 'Updating')\n            else:\n                ctask = ctask.replace('%pre', 'Loading')\n        return ctask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_archive(self, args):\n        import warnings\n        warnings.warn(\"`Task.load_archive()` is deprecated!  \"\n                      \"`Catalog.load_url` handles the same functionality.\")\n        return self.archived or args.archived", "response": "Whether previously archived data should be loaded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting SHA of the current node.", "response": "def get_sha(path=None, log=None, short=False, timeout=None):\n    \"\"\"Use `git rev-parse HEAD <REPO>` to get current SHA.\n    \"\"\"\n    # git_command = \"git rev-parse HEAD {}\".format(repo_name).split()\n    # git_command = \"git rev-parse HEAD\".split()\n    git_command = [\"git\", \"rev-parse\"]\n    if short:\n        git_command.append(\"--short\")\n    git_command.append(\"HEAD\")\n\n    kwargs = {}\n    if path is not None:\n        kwargs['cwd'] = path\n    if timeout is not None:\n        kwargs['timeout'] = timeout\n\n    if log is not None:\n        log.debug(\"{} {}\".format(git_command, str(kwargs)))\n\n    sha = subprocess.check_output(git_command, **kwargs)\n    try:\n        sha = sha.decode('ascii').strip()\n    except:\n        if log is not None:\n            log.debug(\"decode of '{}' failed\".format(sha))\n\n    return sha"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef git_add_commit_push_all_repos(cat):\n    log = cat.log\n    log.debug(\"gitter.git_add_commit_push_all_repos()\")\n\n    # Do not commit/push private repos\n    all_repos = cat.PATHS.get_all_repo_folders(private=False)\n    for repo in all_repos:\n        log.info(\"Repo in: '{}'\".format(repo))\n        # Get the initial git SHA\n        sha_beg = get_sha(repo)\n        log.debug(\"Current SHA: '{}'\".format(sha_beg))\n\n        # Get files that should be added, compress and check sizes\n        add_files = cat._prep_git_add_file_list(repo,\n                                                cat.COMPRESS_ABOVE_FILESIZE)\n        log.info(\"Found {} Files to add.\".format(len(add_files)))\n        if len(add_files) == 0:\n            continue\n\n        try:\n            # Add all files in the repository directory tree\n            git_comm = [\"git\", \"add\"]\n            if cat.args.travis:\n                git_comm.append(\"-f\")\n            git_comm.extend(add_files)\n            _call_command_in_repo(\n                git_comm, repo, cat.log, fail=True, log_flag=False)\n\n            # Commit these files\n            commit_msg = \"'push' - adding all files.\"\n            commit_msg = \"{} : {}\".format(cat._version_long, commit_msg)\n            log.info(commit_msg)\n            git_comm = [\"git\", \"commit\", \"-am\", commit_msg]\n            _call_command_in_repo(git_comm, repo, cat.log)\n\n            # Add all files in the repository directory tree\n            git_comm = [\"git\", \"push\"]\n            if not cat.args.travis:\n                _call_command_in_repo(git_comm, repo, cat.log, fail=True)\n        except Exception as err:\n            try:\n                git_comm = [\"git\", \"reset\", \"HEAD\"]\n                _call_command_in_repo(git_comm, repo, cat.log, fail=True)\n            except:\n                pass\n\n            raise err\n\n    return", "response": "Add all files in each data repository tree commit and push."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a git pull in each data repository.", "response": "def git_pull_all_repos(cat, strategy_recursive=True, strategy='theirs'):\n    \"\"\"Perform a 'git pull' in each data repository.\n\n    > `git pull -s recursive -X theirs`\n    \"\"\"\n    # raise RuntimeError(\"THIS DOESNT WORK YET!\")\n    log = cat.log\n    log.debug(\"gitter.git_pull_all_repos()\")\n    log.warning(\"WARNING: using experimental `git_pull_all_repos()`!\")\n\n    all_repos = cat.PATHS.get_all_repo_folders()\n    for repo_name in all_repos:\n        log.info(\"Repo in: '{}'\".format(repo_name))\n        # Get the initial git SHA\n        sha_beg = get_sha(repo_name)\n        log.debug(\"Current SHA: '{}'\".format(sha_beg))\n\n        # Initialize the git repository\n        repo = git.Repo(repo_name)\n        # Construct the command to call\n        git_comm = \"git pull --verbose\"\n        if strategy_recursive:\n            git_comm += \" -s recursive\"\n        if strategy is not None:\n            git_comm += \" -X {:s}\".format(strategy)\n        log.debug(\"Calling '{}'\".format(git_comm))\n        # Call git command (do this manually to use desired options)\n        #    Set `with_exceptions=False` to handle errors ourselves (below)\n        code, out, err = repo.git.execute(\n            git_comm.split(),\n            with_stdout=True,\n            with_extended_output=True,\n            with_exceptions=False)\n        # Handle output of git command\n        if len(out):\n            log.info(out)\n        if len(err):\n            log.info(err)\n        # Hangle error-codes\n        if code != 0:\n            err_str = \"Command '{}' returned exit code '{}'!\".format(git_comm,\n                                                                     code)\n            err_str += \"\\n\\tout: '{}'\\n\\terr: '{}'\".format(out, err)\n            log.error(err_str)\n            raise RuntimeError(err_str)\n\n        sha_end = get_sha(repo_name)\n        if sha_end != sha_beg:\n            log.info(\"Updated SHA: '{}'\".format(sha_end))\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a git clone for each data repository that does not exist.", "response": "def git_clone_all_repos(cat):\n    \"\"\"Perform a 'git clone' for each data repository that doesnt exist.\n    \"\"\"\n    log = cat.log\n    log.debug(\"gitter.git_clone_all_repos()\")\n\n    all_repos = cat.PATHS.get_all_repo_folders()\n    out_repos = cat.PATHS.get_repo_output_folders()\n    for repo in all_repos:\n        log.info(\"Repo in: '{}'\".format(repo))\n\n        if os.path.isdir(repo):\n            log.info(\"Directory exists.\")\n        else:\n            log.debug(\"Cloning directory...\")\n            clone(repo, cat.log, depth=max(cat.args.clone_depth, 1))\n\n        if cat.args.purge_outputs and repo in out_repos:\n            for fil in glob(os.path.join(repo, '*.json')):\n                os.remove(fil)\n\n        grepo = git.cmd.Git(repo)\n        try:\n            grepo.status()\n        except git.GitCommandError:\n            log.error(\"Repository does not exist!\")\n            raise\n\n        # Get the initial git SHA\n        sha_beg = get_sha(repo)\n        log.debug(\"Current SHA: '{}'\".format(sha_beg))\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef git_reset_all_repos(cat, hard=True, origin=False, clean=True):\n    log = cat.log\n    log.debug(\"gitter.git_reset_all_repos()\")\n\n    all_repos = cat.PATHS.get_all_repo_folders()\n    for repo in all_repos:\n        log.warning(\"Repo in: '{}'\".format(repo))\n        # Get the initial git SHA\n        sha_beg = get_sha(repo)\n        log.debug(\"Current SHA: '{}'\".format(sha_beg))\n\n        grepo = git.cmd.Git(repo)\n        # Fetch first\n        log.info(\"fetching\")\n        grepo.fetch()\n\n        args = []\n        if hard:\n            args.append('--hard')\n        if origin:\n            args.append('origin/master')\n        log.info(\"resetting\")\n        retval = grepo.reset(*args)\n        if len(retval):\n            log.warning(\"Git says: '{}'\".format(retval))\n\n        # Clean\n        if clean:\n            log.info(\"cleaning\")\n            # [q]uiet, [f]orce, [d]irectories\n            retval = grepo.clean('-qdf')\n            if len(retval):\n                log.warning(\"Git says: '{}'\".format(retval))\n\n        sha_end = get_sha(repo)\n        if sha_end != sha_beg:\n            log.debug(\"Updated SHA: '{}'\".format(sha_end))\n\n    return", "response": "Perform a git reset in each data repository."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a git status in each data repository.", "response": "def git_status_all_repos(cat, hard=True, origin=False, clean=True):\n    \"\"\"Perform a 'git status' in each data repository.\n    \"\"\"\n    log = cat.log\n    log.debug(\"gitter.git_status_all_repos()\")\n\n    all_repos = cat.PATHS.get_all_repo_folders()\n    for repo_name in all_repos:\n        log.info(\"Repo in: '{}'\".format(repo_name))\n        # Get the initial git SHA\n        sha_beg = get_sha(repo_name)\n        log.debug(\"Current SHA: '{}'\".format(sha_beg))\n\n        log.info(\"Fetching\")\n        fetch(repo_name, log=cat.log)\n\n        git_comm = [\"git\", \"status\"]\n        _call_command_in_repo(\n            git_comm, repo_name, cat.log, fail=True, log_flag=True)\n\n        sha_end = get_sha(repo_name)\n        if sha_end != sha_beg:\n            log.info(\"Updated SHA: '{}'\".format(sha_end))\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of repositories make sure they re all cloned.", "response": "def clone(repo, log, depth=1):\n    \"\"\"Given a list of repositories, make sure they're all cloned.\n\n    Should be called from the subclassed `Catalog` objects, passed a list\n    of specific repository names.\n\n    Arguments\n    ---------\n    all_repos : list of str\n        *Absolute* path specification of each target repository.\n\n    \"\"\"\n    kwargs = {}\n    if depth > 0:\n        kwargs['depth'] = depth\n\n    try:\n        repo_name = os.path.split(repo)[-1]\n        repo_name = \"https://github.com/astrocatalogs/\" + repo_name + \".git\"\n        log.warning(\"Cloning '{}' (only needs to be done \".format(repo) +\n                    \"once, may take few minutes per repo).\")\n        grepo = git.Repo.clone_from(repo_name, repo, **kwargs)\n    except:\n        log.error(\"CLONING '{}' INTERRUPTED\".format(repo))\n        raise\n\n    return grepo"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses subprocess to call a command in a certain directory.", "response": "def _call_command_in_repo(comm, repo, log, fail=False, log_flag=True):\n    \"\"\"Use `subprocess` to call a command in a certain (repo) directory.\n\n    Logs the output (both `stderr` and `stdout`) to the log, and checks the\n    return codes to make sure they're valid.  Raises error if not.\n\n    Raises\n    ------\n    exception `subprocess.CalledProcessError`: if the command fails\n\n    \"\"\"\n    if log_flag:\n        log.debug(\"Running '{}'.\".format(\" \".join(comm)))\n    process = subprocess.Popen(\n        comm, cwd=repo, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (stdout, stderr) = process.communicate()\n    if stderr is not None:\n        err_msg = stderr.decode('ascii').strip().splitlines()\n        for em in err_msg:\n            log.error(em)\n    if stdout is not None:\n        out_msg = stdout.decode('ascii').strip().splitlines()\n        for om in out_msg:\n            log.warning(om)\n    # Raises an error if the command failed.\n    if fail:\n        if process.returncode:\n            raise subprocess.CalledProcessError\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check(self):\n        # Run the super method\n        super(Spectrum, self)._check()\n\n        err_str = None\n        has_data = self._KEYS.DATA in self\n        has_wave = self._KEYS.WAVELENGTHS in self\n        has_flux = self._KEYS.FLUXES in self\n        has_filename = self._KEYS.FILENAME in self\n\n        if not has_data:\n            if (not has_wave or not has_flux) and not has_filename:\n                err_str = (\n                    \"If `{}` not given\".format(self._KEYS.DATA) +\n                    \"; `{}` or `{}` needed\".format(\n                        self._KEYS.WAVELENGTHS, self._KEYS.FLUXES))\n\n        if err_str is not None:\n            raise ValueError(err_str)\n\n        return", "response": "Check that the current object has legal combination of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_duplicate_of(self, other):\n        if super(Spectrum, self).is_duplicate_of(other):\n            return True\n        row_matches = 0\n        for ri, row in enumerate(self.get(self._KEYS.DATA, [])):\n            lambda1, flux1 = tuple(row[0:2])\n            if (self._KEYS.DATA not in other or\n                    ri > len(other[self._KEYS.DATA])):\n                break\n            lambda2, flux2 = tuple(other[self._KEYS.DATA][ri][0:2])\n            minlambdalen = min(len(lambda1), len(lambda2))\n            minfluxlen = min(len(flux1), len(flux2))\n            if (lambda1[:minlambdalen + 1] == lambda2[:minlambdalen + 1] and\n                    flux1[:minfluxlen + 1] == flux2[:minfluxlen + 1] and\n                    float(flux1[:minfluxlen + 1]) != 0.0):\n                row_matches += 1\n            # Five row matches should be enough to be sure spectrum is a dupe.\n            if row_matches >= 5:\n                return True\n            # Matches need to happen in the first 10 rows.\n            if ri >= 10:\n                break\n        return False", "response": "Check if spectrum is duplicate of another."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_func(self, key):\n        if key == self._KEYS.TIME:\n            return 'aaa'\n        if key == self._KEYS.DATA:\n            return 'zzy'\n        if key == self._KEYS.SOURCE:\n            return 'zzz'\n        return key", "response": "Logic for sorting keys in a Spectrum relative to one another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting logic for Quantity objects.", "response": "def sort_func(self, key):\n        \"\"\"Sorting logic for `Quantity` objects.\"\"\"\n        if key == self._KEYS.VALUE:\n            return 'aaa'\n        if key == self._KEYS.SOURCE:\n            return 'zzz'\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning this class s attribute names.", "response": "def keys(cls):\n        \"\"\"Return this class's attribute names (those not stating with '_').\n\n        Also retrieves the attributes from base classes, e.g.\n        For: ``ENTRY(KeyCollection)``, ``ENTRY.keys()`` gives just the\n             attributes of `ENTRY` (`KeyCollection` has no keys).\n        For: ``SUPERNOVA(ENTRY)``, ``SUPERNOVA.keys()`` gives both the\n             attributes of `SUPERNOVAE` itself, and of `ENTRY`.\n\n        Returns\n        -------\n        _keys : list of str\n            List of names of internal attributes.  Order is effectiely random.\n        \"\"\"\n        if cls._keys:\n            return cls._keys\n\n        # If `_keys` is not yet defined, create it\n        # ----------------------------------------\n        _keys = []\n        # get the keys from all base-classes aswell (when this is subclasses)\n        for mro in cls.__bases__:\n            # base classes below `KeyCollection` (e.g. `object`) wont work\n            if issubclass(mro, KeyCollection):\n                _keys.extend(mro.keys())\n\n        # Get the keys from this particular subclass\n        # Only non-hidden (no '_') and variables (non-callable)\n        _keys.extend([\n            kk for kk in vars(cls).keys()\n            if not kk.startswith('_') and not callable(getattr(cls, kk))\n        ])\n        # Store for future retrieval\n        cls._keys = _keys\n        return cls._keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning this class s attribute values.", "response": "def vals(cls):\n        \"\"\"Return this class's attribute values (those not stating with '_').\n\n        Returns\n        -------\n        _vals : list of objects\n            List of values of internal attributes.  Order is effectiely random.\n        \"\"\"\n        if cls._vals:\n            return cls._vals\n\n        # If `_vals` is not yet defined, create it\n        # ----------------------------------------\n        _vals = []\n        # get the keys from all base-classes aswell (when this is subclasses)\n        for mro in cls.__bases__:\n            # base classes below `KeyCollection` (e.g. `object`) wont work\n            if issubclass(mro, KeyCollection):\n                _vals.extend(mro.vals())\n\n        # Get the keys from this particular subclass\n        # Only non-hidden (no '_') and variables (non-callable)\n        _vals.extend([\n            vv for kk, vv in vars(cls).items()\n            if not kk.startswith('_') and not callable(getattr(cls, kk))\n        ])\n        # Store for future retrieval\n        cls._vals = _vals\n        return cls._vals"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare_vals(cls, sort=True):\n        if cls._compare_vals:\n            return cls._compare_vals\n\n        # If `_compare_vals` is not yet defined, create it\n        # ----------------------------------------\n        _compare_vals = []\n        # get the keys from all base-classes aswell (when this is subclasses)\n        for mro in cls.__bases__:\n            # base classes below `KeyCollection` (e.g. `object`) wont work\n            if issubclass(mro, KeyCollection):\n                _compare_vals.extend(mro.compare_vals(sort=False))\n\n        # Get the keys from this particular subclass\n        # Only non-hidden (no '_') and variables (non-callable)\n        _compare_vals.extend([\n            vv for kk, vv in vars(cls).items()\n            if (not kk.startswith('_') and not callable(getattr(cls, kk)) and\n                vv.compare)\n        ])\n\n        # Sort keys based on priority, high priority values first\n        if sort:\n            _compare_vals = sorted(\n                _compare_vals,\n                reverse=True,\n                key=lambda key: (key.priority, key.name))\n\n        # Store for future retrieval\n        cls._compare_vals = _compare_vals\n        return cls._compare_vals", "response": "Return this class s attribute values but only for attributes with compare set to True."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pretty(self):\n        retval = (\"Key(name={}, type={}, listable={}, compare={}, \"\n                  \"priority={}, kind_preference={}, \"\n                  \"replace_better={})\").format(\n                      self.name, self.type, self.listable, self.compare,\n                      self.priority, self.kind_preference, self.replace_better)\n        return retval", "response": "Return a pretty string representation of this Key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if given value is consistent with this Key specification.", "response": "def check(self, val):\n        \"\"\"Make sure given value is consistent with this `Key` specification.\n\n        NOTE: if `type` is 'None', then `listable` also is *not* checked.\n        \"\"\"\n        # If there is no `type` requirement, everything is allowed\n        if self.type is None:\n            return True\n\n        is_list = isinstance(val, list)\n        # If lists are not allowed, and this is a list --> false\n        if not self.listable and is_list:\n            return False\n\n        # `is_number` already checks for either list or single value\n        if self.type == KEY_TYPES.NUMERIC and not is_number(val):\n            return False\n        elif (self.type == KEY_TYPES.TIME and\n              not is_number(val) and '-' not in val and '/' not in val):\n            return False\n        elif self.type == KEY_TYPES.STRING:\n            # If its a list, check first element\n            if is_list:\n                if not isinstance(val[0], basestring):\n                    return False\n            # Otherwise, check it\n            elif not isinstance(val, basestring):\n                return False\n        elif self.type == KEY_TYPES.BOOL:\n            if is_list and not isinstance(val[0], bool):\n                return False\n            elif not isinstance(val, bool):\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logger(name=None, stream_fmt=None, file_fmt=None, date_fmt=None,\n               stream_level=None, file_level=None,\n               tofile=None, tostr=True):\n    \"\"\"Create a standard logger object which logs to file and or stdout stream.\n\n    If a logger has already been created in this session, it is returned\n    (unless `name` is given).\n\n    Arguments\n    ---------\n    name : str,\n        Handle for this logger, must be distinct for a distinct logger.\n    stream_fmt : str or `None`,\n        Format of log messages to stream (stdout).  If `None`, default settings\n        are used.\n    file_fmt : str or `None`,\n        Format of log messages to file.  If `None`, default settings are used.\n    date_fmt : str or `None`\n        Format of time stamps to stream and/or file.  If `None`, default\n        settings are used.\n    stream_level : int,\n        Logging level for stream.\n    file_level : int,\n        Logging level for file.\n    tofile : str or `None`,\n        Filename to log to (turned off if `None`).\n    tostr : bool,\n        Log to stdout stream.\n\n    Returns\n    -------\n    logger : ``logging.Logger`` object,\n        Logger object to use for logging.\n\n    \"\"\"\n    if tofile is None and not tostr:\n        raise ValueError(\n            \"Must log to something: `tofile` or `tostr` must be `True`.\")\n\n    logger = logging.getLogger(name)\n    # Add a custom attribute to this `logger` so that we know when an existing\n    # one is being returned\n    if hasattr(logger, '_OSC_LOGGER'):\n        return logger\n    else:\n        logger._OSC_LOGGER = True\n\n    # Set other custom parameters\n    logger._LOADED = _LOADED_LEVEL\n\n    # Make sure handlers don't get duplicated (ipython issue)\n    while len(logger.handlers) > 0:\n        logger.handlers.pop()\n    # Prevents duplication or something something...\n    logger.propagate = 0\n\n    # Determine and Set Logging Levels\n    if file_level is None:\n        file_level = _FILE_LEVEL_DEF\n    if stream_level is None:\n        stream_level = _STREAM_LEVEL_DEF\n    # Logger object must be at minimum level\n    logger.setLevel(int(np.min([file_level, stream_level])))\n\n    if date_fmt is None:\n        date_fmt = '%Y/%m/%d %H:%M:%S'\n\n    # Log to file\n    # -----------\n    if tofile is not None:\n        if file_fmt is None:\n            file_fmt = \"%(asctime)s %(levelname)8.8s [%(filename)20.20s:\"\n            file_fmt += \"%(funcName)-20.20s]%(indent)s%(message)s\"\n\n        fileFormatter = IndentFormatter(file_fmt, datefmt=date_fmt)\n        fileHandler = logging.FileHandler(tofile, 'w')\n        fileHandler.setFormatter(fileFormatter)\n        fileHandler.setLevel(file_level)\n        logger.addHandler(fileHandler)\n        #     Store output filename to `logger` object\n        logger.filename = tofile\n\n    # Log To stdout\n    # -------------\n    if tostr:\n        if stream_fmt is None:\n            stream_fmt = \"%(indent)s%(message)s\"\n\n        strFormatter = IndentFormatter(stream_fmt, datefmt=date_fmt)\n        strHandler = logging.StreamHandler()\n        strHandler.setFormatter(strFormatter)\n        strHandler.setLevel(stream_level)\n        logger.addHandler(strHandler)\n\n    return logger", "response": "Create a logger object which logs to file and or stdout stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_raise(log, err_str, err_type=RuntimeError):\n    log.error(err_str)\n    # Make sure output is flushed\n    # (happens automatically to `StreamHandlers`, but not `FileHandlers`)\n    for handle in log.handlers:\n        handle.flush()\n    # Raise given error\n    raise err_type(err_str)", "response": "Log an error message and raise an error."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_memory(log, pref=None, lvl=logging.DEBUG, raise_flag=True):\n    import os\n    import sys\n    cyc_str = \"\"\n    KB = 1024.0\n    if pref is not None:\n        cyc_str += \"{}: \".format(pref)\n\n    # Linux returns units in Bytes; OSX in kilobytes\n    UNIT = KB*KB if sys.platform == 'darwin' else KB\n\n    good = False\n    # Use the `resource` module to check the maximum memory usage of this process\n    try:\n        import resource\n        max_self = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        max_child = resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss\n        _str = \"RSS Max Self: {:7.2f} [MB], Child: {:7.2f} [MB]\".format(\n            max_self/UNIT, max_child/UNIT)\n        cyc_str += _str\n    except Exception as err:\n        log.log(lvl, \"resource.getrusage failed.  '{}'\".format(str(err)))\n        if raise_flag:\n            raise\n    else:\n        good = True\n\n    # Use the `psutil` module to check the current memory/cpu usage of this process\n    try:\n        import psutil\n        process = psutil.Process(os.getpid())\n        rss = process.memory_info().rss\n        cpu_perc = process.cpu_percent()\n        mem_perc = process.memory_percent()\n        num_thr = process.num_threads()\n        _str = \"; RSS: {:7.2f} [MB], {:7.2f}%; Threads: {:3d}, CPU: {:7.2f}%\".format(\n            rss/UNIT, mem_perc, num_thr, cpu_perc)\n        cyc_str += _str\n    except Exception as err:\n        log.log(lvl, \"psutil.Process failed.  '{}'\".format(str(err)))\n        if raise_flag:\n            raise\n    else:\n        good = True\n\n    if good:\n        log.log(lvl, cyc_str)\n\n    return", "response": "Log the current memory usage of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef doblob(morphed, blobdet, img, anno=True):\n    keypoints = blobdet.detect(morphed)\n    nkey = len(keypoints)\n    kpsize = asarray([k.size for k in keypoints])\n    final = img.copy()  # is the .copy necessary?\n\n    final = cv2.drawKeypoints(img, keypoints, outImage=final,\n                              flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n# %% plot count of blobs\n    if anno:\n        cv2.putText(final, text=str(nkey), org=(int(img.shape[1]*.9), 25),\n                    fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=2,\n                    color=(0, 255, 0), thickness=2)\n\n    return final, nkey, kpsize", "response": "This function will draw the morphed blob and return the resulting image and the count of the blobs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses arguments and return configuration settings.", "response": "def load_args(self, args, clargs):\n        \"\"\"Parse arguments and return configuration settings.\n        \"\"\"\n        # Parse All Arguments\n        args = self.parser.parse_args(args=clargs, namespace=args)\n\n        # Print the help information if no subcommand is given\n        # subcommand is required for operation\n        if args.subcommand is None:\n            self.parser.print_help()\n            args = None\n\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an argparse instance and setup with appropriate parameters.", "response": "def _setup_argparse(self):\n        \"\"\"Create `argparse` instance, and setup with appropriate parameters.\n        \"\"\"\n        parser = argparse.ArgumentParser(\n            prog='catalog', description='Parent Catalog class for astrocats.')\n\n        subparsers = parser.add_subparsers(\n            description='valid subcommands', dest='subcommand')\n\n        # Data Import\n        # -----------\n        # Add the 'import' command, and related arguments\n        self._add_parser_arguments_import(subparsers)\n\n        # Git Subcommands\n        # ---------------\n        self._add_parser_arguments_git(subparsers)\n\n        # Analyze Catalogs\n        # ----------------\n        # Add the 'analyze' command, and related arguments\n        self._add_parser_arguments_analyze(subparsers)\n\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating parser for import subcommand and associated arguments.", "response": "def _add_parser_arguments_import(self, subparsers):\n        \"\"\"Create parser for 'import' subcommand, and associated arguments.\n        \"\"\"\n        import_pars = subparsers.add_parser(\n            \"import\", help=\"Import data.\")\n\n        import_pars.add_argument(\n            '--update', '-u', dest='update',\n            default=False, action='store_true',\n            help='Only update catalog using live sources.')\n        import_pars.add_argument(\n            '--load-stubs', dest='load_stubs',\n            default=False, action='store_true',\n            help='Load stubs before running.')\n        import_pars.add_argument(\n            '--archived', '-a', dest='archived',\n            default=False, action='store_true',\n            help='Always use task caches.')\n\n        # Control which 'tasks' are executed\n        # ----------------------------------\n        import_pars.add_argument(\n            '--tasks', dest='args_task_list', nargs='*', default=None,\n            help='space delimited list of tasks to perform.')\n        import_pars.add_argument(\n            '--yes', dest='yes_task_list', nargs='+', default=None,\n            help='space delimited list of tasks to turn on.')\n        import_pars.add_argument(\n            '--no', dest='no_task_list', nargs='+', default=None,\n            help='space delimited list of tasks to turn off.')\n        import_pars.add_argument(\n            '--min-task-priority', dest='min_task_priority',\n            default=None,\n            help='minimum priority for a task to run')\n        import_pars.add_argument(\n            '--max-task-priority', dest='max_task_priority',\n            default=None,\n            help='maximum priority for a task to run')\n        import_pars.add_argument(\n            '--task-groups', dest='task_groups',\n            default=None,\n            help='predefined group(s) of tasks to run.')\n\n        return import_pars"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a sub - parsers for git subcommands.", "response": "def _add_parser_arguments_git(self, subparsers):\n        \"\"\"Create a sub-parsers for git subcommands.\n        \"\"\"\n        subparsers.add_parser(\n            \"git-clone\",\n            help=\"Clone all defined data repositories if they dont exist.\")\n\n        subparsers.add_parser(\n            \"git-push\",\n            help=\"Add all files to data repositories, commit, and push.\")\n\n        subparsers.add_parser(\n            \"git-pull\",\n            help=\"'Pull' all data repositories.\")\n\n        subparsers.add_parser(\n            \"git-reset-local\",\n            help=\"Hard reset all data repositories using local 'HEAD'.\")\n\n        subparsers.add_parser(\n            \"git-reset-origin\",\n            help=\"Hard reset all data repositories using 'origin/master'.\")\n\n        subparsers.add_parser(\n            \"git-status\",\n            help=\"Get the 'git status' of all data repositories.\")\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a parser for the analyze subcommand.", "response": "def _add_parser_arguments_analyze(self, subparsers):\n        \"\"\"Create a parser for the 'analyze' subcommand.\n        \"\"\"\n        lyze_pars = subparsers.add_parser(\n            \"analyze\",\n            help=\"Perform basic analysis on this catalog.\")\n\n        lyze_pars.add_argument(\n            '--count', '-c', dest='count',\n            default=False, action='store_true',\n            help='Determine counts of entries, files, etc.')\n\n        return lyze_pars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompressing the file with the given name and delete the uncompressed file.", "response": "def compress_gz(fname):\n    \"\"\"Compress the file with the given name and delete the uncompressed file.\n\n    The compressed filename is simply the input filename with '.gz' appended.\n\n    Arguments\n    ---------\n    fname : str\n        Name of the file to compress and delete.\n\n    Returns\n    -------\n    comp_fname : str\n        Name of the compressed file produced.  Equal to `fname + '.gz'`.\n\n    \"\"\"\n    import shutil\n    import gzip\n    comp_fname = fname + '.gz'\n    with codecs.open(fname, 'rb') as f_in, gzip.open(\n            comp_fname, 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)\n    os.remove(fname)\n    return comp_fname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_flow(img, flow, step=16, dtype=uint8):\n    maxval = iinfo(img.dtype).max\n\n    # scaleFact = 1. #arbitary factor to make flow visible\n    canno = (0, maxval, 0)  # green color\n    h, w = img.shape[:2]\n    y, x = mgrid[step//2:h:step, step//2:w:step].reshape(2, -1)\n    fx, fy = flow[y, x].T\n    # create line endpoints\n    lines = vstack([x, y, (x+fx), (y+fy)]).T.reshape(-1, 2, 2)\n    lines = int32(lines + 0.5)\n    # create image\n    if img.ndim == 2:  # assume gray\n        vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    else:  # already RGB\n        vis = img\n    # draw line\n    cv2.polylines(vis, lines, isClosed=False, color=canno, thickness=1, lineType=8)\n    # draw filled green circles\n    for (x1, y1), (x2, y2) in lines:\n        cv2.circle(vis, center=(x1, y1), radius=1, color=canno, thickness=-1)\n    return vis", "response": "draws flow vectors on image\n    draws flow vectors on image\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_hsv(mag, ang, dtype=uint8, fn=None):\n    assert mag.shape == ang.shape\n    assert mag.ndim == 2\n    maxval = iinfo(dtype).max\n\n    hsv = dstack(((degrees(ang)/2).astype(dtype),  # /2 to keep less than 255\n                  ones_like(mag)*maxval,  # maxval must be after in 1-D case\n                  cv2.normalize(mag, alpha=0, beta=maxval, norm_type=cv2.NORM_MINMAX)))\n    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\n    if fn is not None:\n        print('writing ' + fn)\n        cv2.imwrite(fn, rgb)\n\n    return rgb", "response": "mag must be uint8, uint16, uint32 and 2-D\n    ang is in radians (float)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a 2D flow to a 2D magnitude and angle", "response": "def flow2magang(flow, dtype=uint8):\n    \"\"\"\n    flow dimensions y,x,2  3-D.  flow[...,0] is magnitude, flow[...,1] is angle\n    \"\"\"\n    fx, fy = flow[..., 0], flow[..., 1]\n    return hypot(fx, fy).astype(dtype), arctan2(fy, fx) + pi"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef IOC(dir, type, nr, size):\n    assert dir <= _IOC_DIRMASK, dir\n    assert type <= _IOC_TYPEMASK, type\n    assert nr <= _IOC_NRMASK, nr\n    assert size <= _IOC_SIZEMASK, size\n    return (dir << _IOC_DIRSHIFT) | (type << _IOC_TYPESHIFT) | (nr << _IOC_NRSHIFT) | (size << _IOC_SIZESHIFT)", "response": "Return a 16 - bit IOC register value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the size of given type and check its suitability for use in an anatomical ioctl command number.", "response": "def IOC_TYPECHECK(t):\n    \"\"\"\n    Returns the size of given type, and check its suitability for use in an\n    ioctl command number.\n    \"\"\"\n    result = ctypes.sizeof(t)\n    assert result <= _IOC_SIZEMASK, result\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef IOWR(type, nr, size):\n    return IOC(IOC_READ | IOC_WRITE, type, nr, IOC_TYPECHECK(size))", "response": "A helper function for creating a new IOC with both read and write parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_last_dirs(path, num=1):\n    head, tail = os.path.split(path)\n    last_path = str(tail)\n    for ii in range(num):\n        head, tail = os.path.split(head)\n        last_path = os.path.join(tail, last_path)\n\n    last_path = \"...\" + last_path\n    return last_path", "response": "Get a path including only the trailing num directories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the analysis routines determined from the given args.", "response": "def analyze(self, args):\n        \"\"\"Run the analysis routines determined from the given `args`.\n        \"\"\"\n        self.log.info(\"Running catalog analysis\")\n\n        if args.count:\n            self.count()\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count(self):\n        self.log.info(\"Running 'count'\")\n        retvals = {}\n\n        # Numbers of 'tasks'\n        num_tasks = self._count_tasks()\n        retvals['num_tasks'] = num_tasks\n\n        # Numbers of 'files'\n        num_files = self._count_repo_files()\n        retvals['num_files'] = num_files\n\n        return retvals", "response": "Analyze the counts of...things.\n        Returns ------- dict\n            Dictionary of property - name : counts pairs for further processing\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncount the number of tasks in the json and directory.", "response": "def _count_tasks(self):\n        \"\"\"Count the number of tasks, both in the json and directory.\n\n        Returns\n        -------\n        num_tasks : int\n            The total number of all tasks included in the `tasks.json` file.\n\n        \"\"\"\n        self.log.warning(\"Tasks:\")\n        tasks, task_names = self.catalog._load_task_list_from_file()\n        # Total number of all tasks\n        num_tasks = len(tasks)\n        # Number which are active by default\n        num_tasks_act = len([tt for tt, vv in tasks.items() if vv.active])\n        # Number of python files in the tasks directory\n        num_task_files = os.path.join(self.catalog.PATHS.tasks_dir, '*.py')\n        num_task_files = len(glob(num_task_files))\n        tasks_str = \"{} ({} default active) with {} task-files.\".format(\n            num_tasks, num_tasks_act, num_task_files)\n        self.log.warning(tasks_str)\n        return num_tasks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _count_repo_files(self):\n        self.log.warning(\"Files:\")\n        num_files = 0\n        repos = self.catalog.PATHS.get_all_repo_folders()\n        num_type = np.zeros(len(self._COUNT_FILE_TYPES), dtype=int)\n        num_ign = 0\n        for rep in repos:\n            # Get the last portion of the filepath for this repo\n            last_path = _get_last_dirs(rep, 2)\n            # Get counts for different file types\n            n_all = self._count_files_by_type(rep, '*')\n            n_type = np.zeros(len(self._COUNT_FILE_TYPES), dtype=int)\n            for ii, ftype in enumerate(self._COUNT_FILE_TYPES):\n                n_type[ii] = self._count_files_by_type(rep, '*.' + ftype)\n            # Get the number of ignored files\n            # (total including ignore, minus 'all')\n            n_ign = self._count_files_by_type(rep, '*', ignore=False)\n            n_ign -= n_all\n            f_str = self._file_nums_str(n_all, n_type, n_ign)\n            f_str = \"{}: {}\".format(last_path, f_str)\n            self.log.warning(f_str)\n            # Update cumulative counts\n            num_files += n_all\n            num_type += n_type\n            num_ign += n_ign\n\n        f_str = self._file_nums_str(num_files, num_type, num_ign)\n        self.log.warning(f_str)\n        return num_files", "response": "Counts the number of files in the data repositories."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _file_nums_str(self, n_all, n_type, n_ign):\n        # 'other' is the difference between all and named\n        n_oth = n_all - np.sum(n_type)\n\n        f_str = \"{} Files\".format(n_all) + \" (\"\n        if len(n_type):\n            f_str += \", \".join(\"{} {}\".format(name, num) for name, num in\n                               zip(self._COUNT_FILE_TYPES, n_type))\n            f_str += \", \"\n        f_str += \"other {}; {} ignored)\".format(n_oth, n_ign)\n        return f_str", "response": "Construct a string showing the number of different file types."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts the number of files in the given path with the given pattern.", "response": "def _count_files_by_type(self, path, pattern, ignore=True):\n        \"\"\"Count files in the given path, with the given pattern.\n\n        If `ignore = True`, skip files in the `_IGNORE_FILES` list.\n\n        Returns\n        -------\n        num_files : int\n\n        \"\"\"\n        # Get all files matching the given path and pattern\n        files = glob(os.path.join(path, pattern))\n        # Count the files\n        files = [ff for ff in files\n                 if os.path.split(ff)[-1] not in self._IGNORE_FILES\n                 or not ignore]\n        num_files = len(files)\n        return num_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bibcode_from_url(cls, url):\n        try:\n            code = url.split('/abs/')\n            code = code[1].strip()\n            return code\n        except:\n            return None", "response": "Try to find the ADS bibcode from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_save_path(self, bury=False):\n        filename = self.get_filename(self[self._KEYS.NAME])\n\n        # Put objects that shouldn't belong in this catalog in the boneyard\n        if bury:\n            outdir = self.catalog.get_repo_boneyard()\n\n        # Get normal repository save directory\n        else:\n            repo_folders = self.catalog.PATHS.get_repo_output_folders()\n            # If no repo folders exist, raise an error -- cannot save\n            if not len(repo_folders):\n                err_str = (\n                    \"No output data repositories found. Cannot save.\\n\"\n                    \"Make sure that repo names are correctly configured \"\n                    \"in the `input/repos.json` file, and either manually or \"\n                    \"automatically (using `astrocats CATALOG git-clone`) \"\n                    \"clone the appropriate data repositories.\")\n                self.catalog.log.error(err_str)\n                raise RuntimeError(err_str)\n\n            outdir = repo_folders[0]\n\n        return outdir, filename", "response": "Return the path that this Entry should be saved to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the object into a plain OrderedDict.", "response": "def _ordered(self, odict):\n        \"\"\"Convert the object into a plain OrderedDict.\"\"\"\n        ndict = OrderedDict()\n\n        if isinstance(odict, CatDict) or isinstance(odict, Entry):\n            key = odict.sort_func\n        else:\n            key = None\n\n        nkeys = list(sorted(odict.keys(), key=key))\n        for key in nkeys:\n            if isinstance(odict[key], OrderedDict):\n                odict[key] = self._ordered(odict[key])\n            if isinstance(odict[key], list):\n                if (not (odict[key] and\n                         not isinstance(odict[key][0], OrderedDict))):\n                    nlist = []\n                    for item in odict[key]:\n                        if isinstance(item, OrderedDict):\n                            nlist.append(self._ordered(item))\n                        else:\n                            nlist.append(item)\n                    odict[key] = nlist\n            ndict[key] = odict[key]\n\n        return ndict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a unique hash associated with the listed keys.", "response": "def get_hash(self, keys=[]):\n        \"\"\"Return a unique hash associated with the listed keys.\"\"\"\n        if not len(keys):\n            keys = list(self.keys())\n\n        string_rep = ''\n        oself = self._ordered(deepcopy(self))\n        for key in keys:\n            string_rep += json.dumps(oself.get(key, ''), sort_keys=True)\n\n        return hashlib.sha512(string_rep.encode()).hexdigest()[:16]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean quantity value before it is added to entry.", "response": "def _clean_quantity(self, quantity):\n        \"\"\"Clean quantity value before it is added to entry.\"\"\"\n        value = quantity.get(QUANTITY.VALUE, '').strip()\n        error = quantity.get(QUANTITY.E_VALUE, '').strip()\n        unit = quantity.get(QUANTITY.U_VALUE, '').strip()\n        kind = quantity.get(QUANTITY.KIND, '')\n\n        if isinstance(kind, list) and not isinstance(kind, string_types):\n            kind = [x.strip() for x in kind]\n        else:\n            kind = kind.strip()\n\n        if not value:\n            return False\n\n        if is_number(value):\n            value = '%g' % Decimal(value)\n        if error:\n            error = '%g' % Decimal(error)\n\n        if value:\n            quantity[QUANTITY.VALUE] = value\n        if error:\n            quantity[QUANTITY.E_VALUE] = error\n        if unit:\n            quantity[QUANTITY.U_VALUE] = unit\n        if kind:\n            quantity[QUANTITY.KIND] = kind\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an ordered dictionary into Entry or its derivative classes.", "response": "def _convert_odict_to_classes(self,\n                                  data,\n                                  clean=False,\n                                  merge=True,\n                                  pop_schema=True,\n                                  compare_to_existing=True,\n                                  filter_on={}):\n        \"\"\"Convert `OrderedDict` into `Entry` or its derivative classes.\"\"\"\n        self._log.debug(\"_convert_odict_to_classes(): {}\".format(self.name()))\n        self._log.debug(\"This should be a temporary fix.  Dont be lazy.\")\n\n        # Setup filters. Currently only used for photometry.\n        fkeys = list(filter_on.keys())\n\n        # Handle 'name'\n        name_key = self._KEYS.NAME\n        if name_key in data:\n            self[name_key] = data.pop(name_key)\n\n        # Handle 'schema'\n        schema_key = self._KEYS.SCHEMA\n        if schema_key in data:\n            # Schema should be re-added every execution (done elsewhere) so\n            # just delete the old entry\n            if pop_schema:\n                data.pop(schema_key)\n            else:\n                self[schema_key] = data.pop(schema_key)\n\n        # Cleanup 'internal' repository stuff\n        if clean:\n            # Add data to `self` in ways accomodating 'internal' formats and\n            # leeway.  Removes each added entry from `data` so the remaining\n            # stuff can be handled normally\n            data = self.clean_internal(data)\n\n        # Handle 'sources'\n        # ----------------\n        src_key = self._KEYS.SOURCES\n        if src_key in data:\n            # Remove from `data`\n            sources = data.pop(src_key)\n            self._log.debug(\"Found {} '{}' entries\".format(\n                len(sources), src_key))\n            self._log.debug(\"{}: {}\".format(src_key, sources))\n\n            for src in sources:\n                self.add_source(allow_alias=True, **src)\n\n        # Handle `photometry`\n        # -------------------\n        photo_key = self._KEYS.PHOTOMETRY\n        if photo_key in data:\n            photoms = data.pop(photo_key)\n            self._log.debug(\"Found {} '{}' entries\".format(\n                len(photoms), photo_key))\n            phcount = 0\n            for photo in photoms:\n                skip = False\n                for fkey in fkeys:\n                    if fkey in photo and photo[fkey] not in filter_on[fkey]:\n                        skip = True\n                if skip:\n                    continue\n                self._add_cat_dict(\n                    Photometry,\n                    self._KEYS.PHOTOMETRY,\n                    compare_to_existing=compare_to_existing,\n                    **photo)\n                phcount += 1\n            self._log.debug(\"Added {} '{}' entries\".format(\n                phcount, photo_key))\n\n        # Handle `spectra`\n        # ---------------\n        spec_key = self._KEYS.SPECTRA\n        if spec_key in data:\n            # When we are cleaning internal data, we don't always want to\n            # require all of the normal spectrum data elements.\n            spectra = data.pop(spec_key)\n            self._log.debug(\"Found {} '{}' entries\".format(\n                len(spectra), spec_key))\n            for spec in spectra:\n                self._add_cat_dict(\n                    Spectrum,\n                    self._KEYS.SPECTRA,\n                    compare_to_existing=compare_to_existing,\n                    **spec)\n\n        # Handle `error`\n        # --------------\n        err_key = self._KEYS.ERRORS\n        if err_key in data:\n            errors = data.pop(err_key)\n            self._log.debug(\"Found {} '{}' entries\".format(\n                len(errors), err_key))\n            for err in errors:\n                self._add_cat_dict(Error, self._KEYS.ERRORS, **err)\n\n        # Handle `models`\n        # ---------------\n        model_key = self._KEYS.MODELS\n        if model_key in data:\n            # When we are cleaning internal data, we don't always want to\n            # require all of the normal spectrum data elements.\n            model = data.pop(model_key)\n            self._log.debug(\"Found {} '{}' entries\".format(\n                len(model), model_key))\n            for mod in model:\n                self._add_cat_dict(\n                    Model,\n                    self._KEYS.MODELS,\n                    compare_to_existing=compare_to_existing,\n                    **mod)\n\n        # Handle everything else --- should be `Quantity`s\n        # ------------------------------------------------\n        if len(data):\n            self._log.debug(\"{} remaining entries, assuming `Quantity`\".format(\n                len(data)))\n            # Iterate over remaining keys\n            for key in list(data.keys()):\n                vals = data.pop(key)\n                # All quantities should be in lists of that quantity\n                #    E.g. `aliases` is a list of alias quantities\n                if not isinstance(vals, list):\n                    vals = [vals]\n                self._log.debug(\"{}: {}\".format(key, vals))\n                for vv in vals:\n                    self._add_cat_dict(\n                        Quantity,\n                        key,\n                        check_for_dupes=merge,\n                        compare_to_existing=compare_to_existing,\n                        **vv)\n\n        if merge and self.dupe_of:\n            self.merge_dupes()\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that a source exists and that a quantity isn t erroneous.", "response": "def _check_cat_dict_source(self, cat_dict_class, key_in_self, **kwargs):\n        \"\"\"Check that a source exists and that a quantity isn't erroneous.\"\"\"\n        # Make sure that a source is given\n        source = kwargs.get(cat_dict_class._KEYS.SOURCE, None)\n        if source is None:\n            raise CatDictError(\n                \"{}: `source` must be provided!\".format(self[self._KEYS.NAME]),\n                warn=True)\n        # Check that source is a list of integers\n        for x in source.split(','):\n            if not is_integer(x):\n                raise CatDictError(\n                    \"{}: `source` is comma-delimited list of \"\n                    \" integers!\".format(self[self._KEYS.NAME]),\n                    warn=True)\n        # If this source/data is erroneous, skip it\n        if self.is_erroneous(key_in_self, source):\n            self._log.info(\"This source is erroneous, skipping\")\n            return None\n        # If this source/data is private, skip it\n        if (self.catalog.args is not None and not self.catalog.args.private and\n                self.is_private(key_in_self, source)):\n            self._log.info(\"This source is private, skipping\")\n            return None\n        return source"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a CatDict to this Entry.", "response": "def _add_cat_dict(self,\n                      cat_dict_class,\n                      key_in_self,\n                      check_for_dupes=True,\n                      compare_to_existing=True,\n                      **kwargs):\n        \"\"\"Add a `CatDict` to this `Entry`.\n\n        CatDict only added if initialization succeeds and it\n        doesn't already exist within the Entry.\n        \"\"\"\n        # Make sure that a source is given, and is valid (nor erroneous)\n        if cat_dict_class != Error:\n            try:\n                source = self._check_cat_dict_source(cat_dict_class,\n                                                     key_in_self, **kwargs)\n            except CatDictError as err:\n                if err.warn:\n                    self._log.info(\"'{}' Not adding '{}': '{}'\".format(self[\n                        self._KEYS.NAME], key_in_self, str(err)))\n                return False\n\n            if source is None:\n                return False\n\n        # Try to create a new instance of this subclass of `CatDict`\n        new_entry = self._init_cat_dict(cat_dict_class, key_in_self, **kwargs)\n        if new_entry is None:\n            return False\n\n        # Compare this new entry with all previous entries to make sure is new\n        if compare_to_existing and cat_dict_class != Error:\n            for item in self.get(key_in_self, []):\n                if new_entry.is_duplicate_of(item):\n                    item.append_sources_from(new_entry)\n                    # Return the entry in case we want to use any additional\n                    # tags to augment the old entry\n                    return new_entry\n\n        # If this is an alias, add it to the parent catalog's reverse\n        # dictionary linking aliases to names for fast lookup.\n        if key_in_self == self._KEYS.ALIAS:\n            # Check if this adding this alias makes us a dupe, if so mark\n            # ourselves as a dupe.\n            if (check_for_dupes and 'aliases' in dir(self.catalog) and\n                    new_entry[QUANTITY.VALUE] in self.catalog.aliases):\n                possible_dupe = self.catalog.aliases[new_entry[QUANTITY.VALUE]]\n                # print(possible_dupe)\n                if (possible_dupe != self[self._KEYS.NAME] and\n                        possible_dupe in self.catalog.entries):\n                    self.dupe_of.append(possible_dupe)\n            if 'aliases' in dir(self.catalog):\n                self.catalog.aliases[new_entry[QUANTITY.VALUE]] = self[\n                    self._KEYS.NAME]\n\n        self.setdefault(key_in_self, []).append(new_entry)\n\n        if (key_in_self == self._KEYS.ALIAS and check_for_dupes and\n                self.dupe_of):\n            self.merge_dupes()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_from_file(cls,\n                       catalog,\n                       name=None,\n                       path=None,\n                       clean=False,\n                       merge=True,\n                       pop_schema=True,\n                       ignore_keys=[],\n                       compare_to_existing=True,\n                       try_gzip=False,\n                       filter_on={}):\n        \"\"\"Construct a new `Entry` instance from an input file.\n\n        The input file can be given explicitly by `path`, or a path will\n        be constructed appropriately if possible.\n\n        Arguments\n        ---------\n        catalog : `astrocats.catalog.catalog.Catalog` instance\n            The parent catalog object of which this entry belongs.\n        name : str or 'None'\n            The name of this entry, e.g. `SN1987A` for a `Supernova` entry.\n            If no `path` is given, a path is constructed by trying to find\n            a file in one of the 'output' repositories with this `name`.\n            note: either `name` or `path` must be provided.\n        path : str or 'None'\n            The absolutely path of the input file.\n            note: either `name` or `path` must be provided.\n        clean : bool\n            Whether special sanitization processing should be done on the input\n            data.  This is mostly for input files from the 'internal'\n            repositories.\n\n        \"\"\"\n        if not catalog:\n            from astrocats.catalog.catalog import Catalog\n            log = logging.getLogger()\n            catalog = Catalog(None, log)\n\n        catalog.log.debug(\"init_from_file()\")\n        if name is None and path is None:\n            err = (\"Either entry `name` or `path` must be specified to load \"\n                   \"entry.\")\n            log.error(err)\n            raise ValueError(err)\n\n        # If the path is given, use that to load from\n        load_path = ''\n        if path is not None:\n            load_path = path\n            name = ''\n        # If the name is given, try to find a path for it\n        else:\n            repo_paths = catalog.PATHS.get_repo_output_folders()\n            for rep in repo_paths:\n                filename = cls.get_filename(name)\n                newpath = os.path.join(rep, filename + '.json')\n                if os.path.isfile(newpath):\n                    load_path = newpath\n                    break\n\n        if load_path is None or not os.path.isfile(load_path):\n            # FIX: is this warning worthy?\n            return None\n\n        # Create a new `Entry` instance\n        new_entry = cls(catalog, name)\n\n        # Check if .gz file\n        if try_gzip and not load_path.endswith('.gz'):\n            try_gzip = False\n\n        # Fill it with data from json file\n        new_entry._load_data_from_json(\n            load_path,\n            clean=clean,\n            merge=merge,\n            pop_schema=pop_schema,\n            ignore_keys=ignore_keys,\n            compare_to_existing=compare_to_existing,\n            gzip=try_gzip,\n            filter_on=filter_on)\n\n        return new_entry", "response": "Construct an Entry instance from an input file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_alias(self, alias, source, clean=True):\n        if clean:\n            alias = self.catalog.clean_entry_name(alias)\n        self.add_quantity(self._KEYS.ALIAS, alias, source)\n        return alias", "response": "Adds an alias to the store."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_error(self, value, **kwargs):\n        kwargs.update({ERROR.VALUE: value})\n        self._add_cat_dict(Error, self._KEYS.ERRORS, **kwargs)\n        return", "response": "Add an Error instance to this entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a Photometry instance to this entry.", "response": "def add_photometry(self, compare_to_existing=True, **kwargs):\n        \"\"\"Add a `Photometry` instance to this entry.\"\"\"\n        self._add_cat_dict(\n            Photometry,\n            self._KEYS.PHOTOMETRY,\n            compare_to_existing=compare_to_existing,\n            **kwargs)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges two entries that correspond to the same entry.", "response": "def merge_dupes(self):\n        \"\"\"Merge two entries that correspond to the same entry.\"\"\"\n        for dupe in self.dupe_of:\n            if dupe in self.catalog.entries:\n                if self.catalog.entries[dupe]._stub:\n                    # merge = False to avoid infinite recursion\n                    self.catalog.load_entry_from_name(\n                        dupe, delete=True, merge=False)\n                self.catalog.copy_entry_to_entry(self.catalog.entries[dupe],\n                                                 self)\n                del self.catalog.entries[dupe]\n        self.dupe_of = []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_quantity(self,\n                     quantities,\n                     value,\n                     source,\n                     check_for_dupes=True,\n                     compare_to_existing=True,\n                     **kwargs):\n        \"\"\"Add an `Quantity` instance to this entry.\"\"\"\n        success = True\n        for quantity in listify(quantities):\n            kwargs.update({QUANTITY.VALUE: value, QUANTITY.SOURCE: source})\n            cat_dict = self._add_cat_dict(\n                Quantity,\n                quantity,\n                compare_to_existing=compare_to_existing,\n                check_for_dupes=check_for_dupes,\n                **kwargs)\n            if isinstance(cat_dict, CatDict):\n                self._append_additional_tags(quantity, source, cat_dict)\n                success = False\n\n        return success", "response": "Adds an Quantity instance to this entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_self_source(self):\n        return self.add_source(\n            bibcode=self.catalog.OSC_BIBCODE,\n            name=self.catalog.OSC_NAME,\n            url=self.catalog.OSC_URL,\n            secondary=True)", "response": "Add a source that refers to the catalog itself."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a Source instance to this entry.", "response": "def add_source(self, allow_alias=False, **kwargs):\n        \"\"\"Add a `Source` instance to this entry.\"\"\"\n        if not allow_alias and SOURCE.ALIAS in kwargs:\n            err_str = \"`{}` passed in kwargs, this shouldn't happen!\".format(\n                SOURCE.ALIAS)\n            self._log.error(err_str)\n            raise RuntimeError(err_str)\n\n        # Set alias number to be +1 of current number of sources\n        if SOURCE.ALIAS not in kwargs:\n            kwargs[SOURCE.ALIAS] = str(self.num_sources() + 1)\n        source_obj = self._init_cat_dict(Source, self._KEYS.SOURCES, **kwargs)\n        if source_obj is None:\n            return None\n\n        for item in self.get(self._KEYS.SOURCES, ''):\n            if source_obj.is_duplicate_of(item):\n                return item[item._KEYS.ALIAS]\n\n        self.setdefault(self._KEYS.SOURCES, []).append(source_obj)\n        return source_obj[source_obj._KEYS.ALIAS]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a Model instance to this entry.", "response": "def add_model(self, allow_alias=False, **kwargs):\n        \"\"\"Add a `Model` instance to this entry.\"\"\"\n        if not allow_alias and MODEL.ALIAS in kwargs:\n            err_str = \"`{}` passed in kwargs, this shouldn't happen!\".format(\n                SOURCE.ALIAS)\n            self._log.error(err_str)\n            raise RuntimeError(err_str)\n\n        # Set alias number to be +1 of current number of models\n        if MODEL.ALIAS not in kwargs:\n            kwargs[MODEL.ALIAS] = str(self.num_models() + 1)\n        model_obj = self._init_cat_dict(Model, self._KEYS.MODELS, **kwargs)\n        if model_obj is None:\n            return None\n\n        for item in self.get(self._KEYS.MODELS, ''):\n            if model_obj.is_duplicate_of(item):\n                return item[item._KEYS.ALIAS]\n\n        self.setdefault(self._KEYS.MODELS, []).append(model_obj)\n        return model_obj[model_obj._KEYS.ALIAS]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_spectrum(self, compare_to_existing=True, **kwargs):\n        spec_key = self._KEYS.SPECTRA\n        # Make sure that a source is given, and is valid (nor erroneous)\n        source = self._check_cat_dict_source(Spectrum, spec_key, **kwargs)\n        if source is None:\n            return None\n\n        # Try to create a new instance of `Spectrum`\n        new_spectrum = self._init_cat_dict(Spectrum, spec_key, **kwargs)\n        if new_spectrum is None:\n            return None\n\n        is_dupe = False\n        for item in self.get(spec_key, []):\n            # Only the `filename` should be compared for duplicates. If a\n            # duplicate is found, that means the previous `exclude` array\n            # should be saved to the new object, and the old deleted\n            if new_spectrum.is_duplicate_of(item):\n                if SPECTRUM.EXCLUDE in new_spectrum:\n                    item[SPECTRUM.EXCLUDE] = new_spectrum[SPECTRUM.EXCLUDE]\n                elif SPECTRUM.EXCLUDE in item:\n                    item.update(new_spectrum)\n                is_dupe = True\n                break\n\n        if not is_dupe:\n            self.setdefault(spec_key, []).append(new_spectrum)\n        return", "response": "Add a Spectrum instance to this entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the entry has the required fields.", "response": "def check(self):\n        \"\"\"Check that the entry has the required fields.\"\"\"\n        # Make sure there is a schema key in dict\n        if self._KEYS.SCHEMA not in self:\n            self[self._KEYS.SCHEMA] = self.catalog.SCHEMA.URL\n        # Make sure there is a name key in dict\n        if (self._KEYS.NAME not in self or len(self[self._KEYS.NAME]) == 0):\n            raise ValueError(\"Entry name is empty:\\n\\t{}\".format(\n                json.dumps(\n                    self, indent=2)))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of aliases of this object.", "response": "def get_aliases(self, includename=True):\n        \"\"\"Retrieve the aliases of this object as a list of strings.\n\n        Arguments\n        ---------\n        includename : bool\n            Include the 'name' parameter in the list of aliases.\n        \"\"\"\n        # empty list if doesnt exist\n        alias_quanta = self.get(self._KEYS.ALIAS, [])\n        aliases = [aq[QUANTITY.VALUE] for aq in alias_quanta]\n        if includename and self[self._KEYS.NAME] not in aliases:\n            aliases = [self[self._KEYS.NAME]] + aliases\n        return aliases"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the raw text from a file.", "response": "def get_entry_text(self, fname):\n        \"\"\"Retrieve the raw text from a file.\"\"\"\n        if fname.split('.')[-1] == 'gz':\n            with gz.open(fname, 'rt') as f:\n                filetext = f.read()\n        else:\n            with codecs.open(fname, 'r') as f:\n                filetext = f.read()\n        return filetext"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an alias find the corresponding source in this entry.", "response": "def get_source_by_alias(self, alias):\n        \"\"\"Given an alias, find the corresponding source in this entry.\n\n        If the given alias doesn't exist (e.g. there are no sources), then a\n        `ValueError` is raised.\n\n        Arguments\n        ---------\n        alias : str\n            The str-integer (e.g. '8') of the target source.\n\n        Returns\n        -------\n        source : `astrocats.catalog.source.Source` object\n            The source object corresponding to the passed alias.\n\n        \"\"\"\n        for source in self.get(self._KEYS.SOURCES, []):\n            if source[self._KEYS.ALIAS] == alias:\n                return source\n        raise ValueError(\"Source '{}': alias '{}' not found!\".format(self[\n            self._KEYS.NAME], alias))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a new Entry which contains the stub of this one.", "response": "def get_stub(self):\n        \"\"\"Get a new `Entry` which contains the 'stub' of this one.\n\n        The 'stub' is only the name and aliases.\n\n        Usage:\n        -----\n        To convert a normal entry into a stub (for example), overwrite the\n        entry in place, i.e.\n        >>> entries[name] = entries[name].get_stub()\n\n        Returns\n        -------\n        stub : `astrocats.catalog.entry.Entry` subclass object\n            The type of the returned object is this instance's type.\n\n        \"\"\"\n        stub = type(self)(self.catalog, self[self._KEYS.NAME], stub=True)\n        if self._KEYS.ALIAS in self:\n            stub[self._KEYS.ALIAS] = self[self._KEYS.ALIAS]\n        if self._KEYS.DISTINCT_FROM in self:\n            stub[self._KEYS.DISTINCT_FROM] = self[self._KEYS.DISTINCT_FROM]\n        if self._KEYS.RA in self:\n            stub[self._KEYS.RA] = self[self._KEYS.RA]\n        if self._KEYS.DEC in self:\n            stub[self._KEYS.DEC] = self[self._KEYS.DEC]\n        if self._KEYS.DISCOVER_DATE in self:\n            stub[self._KEYS.DISCOVER_DATE] = self[self._KEYS.DISCOVER_DATE]\n        if self._KEYS.SOURCES in self:\n            stub[self._KEYS.SOURCES] = self[self._KEYS.SOURCES]\n        return stub"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_erroneous(self, field, sources):\n        if self._KEYS.ERRORS in self:\n            my_errors = self[self._KEYS.ERRORS]\n            for alias in sources.split(','):\n                source = self.get_source_by_alias(alias)\n                bib_err_values = [\n                    err[ERROR.VALUE] for err in my_errors\n                    if err[ERROR.KIND] == SOURCE.BIBCODE and\n                    err[ERROR.EXTRA] == field\n                ]\n                if (SOURCE.BIBCODE in source and\n                        source[SOURCE.BIBCODE] in bib_err_values):\n                    return True\n\n                name_err_values = [\n                    err[ERROR.VALUE] for err in my_errors\n                    if err[ERROR.KIND] == SOURCE.NAME and err[ERROR.EXTRA] ==\n                    field\n                ]\n                if (SOURCE.NAME in source and\n                        source[SOURCE.NAME] in name_err_values):\n                    return True\n\n        return False", "response": "Check if the attribute has been marked as being erroneous."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_private(self, key, sources):\n        # aliases are always public.\n        if key == ENTRY.ALIAS:\n            return False\n        return all([\n            SOURCE.PRIVATE in self.get_source_by_alias(x)\n            for x in sources.split(',')\n        ])", "response": "Check if attribute is private."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sanitize(self):\n        name = self[self._KEYS.NAME]\n\n        aliases = self.get_aliases(includename=False)\n        if name not in aliases:\n            # Assign the first source to alias, if not available assign us.\n            if self._KEYS.SOURCES in self:\n                self.add_quantity(self._KEYS.ALIAS, name, '1')\n                if self._KEYS.ALIAS not in self:\n                    source = self.add_self_source()\n                    self.add_quantity(self._KEYS.ALIAS, name, source)\n            else:\n                source = self.add_self_source()\n                self.add_quantity(self._KEYS.ALIAS, name, source)\n\n        if self._KEYS.ALIAS in self:\n            self[self._KEYS.ALIAS].sort(\n                key=lambda key: alias_priority(name, key[QUANTITY.VALUE]))\n        else:\n            self._log.error(\n                'There should be at least one alias for `{}`.'.format(name))\n\n        if self._KEYS.PHOTOMETRY in self:\n            self[self._KEYS.PHOTOMETRY].sort(\n                key=lambda x: ((float(x[PHOTOMETRY.TIME]) if\n                                isinstance(x[PHOTOMETRY.TIME],\n                                           (basestring, float, int))\n                                else min([float(y) for y in\n                                          x[PHOTOMETRY.TIME]])) if\n                               PHOTOMETRY.TIME in x else 0.0,\n                               x[PHOTOMETRY.BAND] if PHOTOMETRY.BAND in\n                               x else '',\n                               float(x[PHOTOMETRY.MAGNITUDE]) if\n                               PHOTOMETRY.MAGNITUDE in x else ''))\n\n        if (self._KEYS.SPECTRA in self and list(\n                filter(None, [\n                    SPECTRUM.TIME in x for x in self[self._KEYS.SPECTRA]\n                ]))):\n            self[self._KEYS.SPECTRA].sort(\n                key=lambda x: (float(x[SPECTRUM.TIME]) if\n                               SPECTRUM.TIME in x else 0.0,\n                               x[SPECTRUM.FILENAME] if\n                               SPECTRUM.FILENAME in x else '')\n            )\n\n        if self._KEYS.SOURCES in self:\n            # Remove orphan sources\n            source_aliases = [\n                x[SOURCE.ALIAS] for x in self[self._KEYS.SOURCES]\n            ]\n            # Sources with the `PRIVATE` attribute are always retained\n            source_list = [\n                x[SOURCE.ALIAS] for x in self[self._KEYS.SOURCES]\n                if SOURCE.PRIVATE in x\n            ]\n            for key in self:\n                # if self._KEYS.get_key_by_name(key).no_source:\n                if (key in [\n                        self._KEYS.NAME, self._KEYS.SCHEMA, self._KEYS.SOURCES,\n                        self._KEYS.ERRORS\n                ]):\n                    continue\n                for item in self[key]:\n                    source_list += item[item._KEYS.SOURCE].split(',')\n            new_src_list = sorted(\n                list(set(source_aliases).intersection(source_list)))\n            new_sources = []\n            for source in self[self._KEYS.SOURCES]:\n                if source[SOURCE.ALIAS] in new_src_list:\n                    new_sources.append(source)\n                else:\n                    self._log.info('Removing orphaned source from `{}`.'\n                                   .format(name))\n\n            if not new_sources:\n                del self[self._KEYS.SOURCES]\n\n            self[self._KEYS.SOURCES] = new_sources", "response": "Sanitize the data before writing it to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, bury=False, final=False):\n        outdir, filename = self._get_save_path(bury=bury)\n\n        if final:\n            self.sanitize()\n\n        # FIX: use 'dump' not 'dumps'\n        jsonstring = json.dumps(\n            {\n                self[self._KEYS.NAME]: self._ordered(self)\n            },\n            indent='\\t' if sys.version_info[0] >= 3 else 4,\n            separators=(',', ':'),\n            ensure_ascii=False)\n        if not os.path.isdir(outdir):\n            raise RuntimeError(\"Output directory '{}' for event '{}' does \"\n                               \"not exist.\".format(outdir, self[\n                                   self._KEYS.NAME]))\n        save_name = os.path.join(outdir, filename + '.json')\n        with codecs.open(save_name, 'w', encoding='utf8') as sf:\n            sf.write(jsonstring)\n\n        if not os.path.exists(save_name):\n            raise RuntimeError(\"File '{}' was not saved!\".format(save_name))\n\n        return save_name", "response": "Save the current object to a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse to sort keys when writing Entry to JSON format.", "response": "def sort_func(self, key):\n        \"\"\"Used to sort keys when writing Entry to JSON format.\n\n        Should be supplemented/overridden by inheriting classes.\n        \"\"\"\n        if key == self._KEYS.SCHEMA:\n            return 'aaa'\n        if key == self._KEYS.NAME:\n            return 'aab'\n        if key == self._KEYS.SOURCES:\n            return 'aac'\n        if key == self._KEYS.ALIAS:\n            return 'aad'\n        if key == self._KEYS.MODELS:\n            return 'aae'\n        if key == self._KEYS.PHOTOMETRY:\n            return 'zzy'\n        if key == self._KEYS.SPECTRA:\n            return 'zzz'\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_pd_mag_from_counts(photodict,\n                           c='',\n                           ec='',\n                           lec='',\n                           uec='',\n                           zp=DEFAULT_ZP,\n                           sig=DEFAULT_UL_SIGMA):\n    \"\"\"Set photometry dictionary from a counts measurement.\"\"\"\n    with localcontext() as ctx:\n        if lec == '' or uec == '':\n            lec = ec\n            uec = ec\n        prec = max(\n            get_sig_digits(str(c), strip_zeroes=False),\n            get_sig_digits(str(lec), strip_zeroes=False),\n            get_sig_digits(str(uec), strip_zeroes=False)) + 1\n        ctx.prec = prec\n        dlec = Decimal(str(lec))\n        duec = Decimal(str(uec))\n        if c != '':\n            dc = Decimal(str(c))\n        dzp = Decimal(str(zp))\n        dsig = Decimal(str(sig))\n        photodict[PHOTOMETRY.ZERO_POINT] = str(zp)\n        if c == '' or float(c) < float(sig) * float(uec):\n            photodict[PHOTOMETRY.UPPER_LIMIT] = True\n            photodict[PHOTOMETRY.UPPER_LIMIT_SIGMA] = str(sig)\n            photodict[PHOTOMETRY.MAGNITUDE] = str(dzp - (D25 * (dsig * duec\n                                                                ).log10()))\n            dnec = Decimal('10.0') ** (\n                (dzp - Decimal(photodict[PHOTOMETRY.MAGNITUDE])) / D25)\n            photodict[PHOTOMETRY.E_UPPER_MAGNITUDE] = str(D25 * (\n                (dnec + duec).log10() - dnec.log10()))\n        else:\n            photodict[PHOTOMETRY.MAGNITUDE] = str(dzp - D25 * dc.log10())\n            photodict[PHOTOMETRY.E_UPPER_MAGNITUDE] = str(D25 * (\n                (dc + duec).log10() - dc.log10()))\n            photodict[PHOTOMETRY.E_LOWER_MAGNITUDE] = str(D25 * (\n                dc.log10() - (dc - dlec).log10()))", "response": "Set the photometry dictionary from a counts measurement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_pd_mag_from_flux_density(photodict,\n                                 fd='',\n                                 efd='',\n                                 lefd='',\n                                 uefd='',\n                                 sig=DEFAULT_UL_SIGMA):\n    \"\"\"Set photometry dictionary from a flux density measurement.\n\n    `fd` is assumed to be in microjanskys.\n    \"\"\"\n    with localcontext() as ctx:\n        if lefd == '' or uefd == '':\n            lefd = efd\n            uefd = efd\n        prec = max(\n            get_sig_digits(str(fd), strip_zeroes=False),\n            get_sig_digits(str(lefd), strip_zeroes=False),\n            get_sig_digits(str(uefd), strip_zeroes=False)) + 1\n        ctx.prec = prec\n        dlefd = Decimal(str(lefd))\n        duefd = Decimal(str(uefd))\n        if fd != '':\n            dfd = Decimal(str(fd))\n        dsig = Decimal(str(sig))\n        if fd == '' or float(fd) < DEFAULT_UL_SIGMA * float(uefd):\n            photodict[PHOTOMETRY.UPPER_LIMIT] = True\n            photodict[PHOTOMETRY.UPPER_LIMIT_SIGMA] = str(sig)\n            photodict[PHOTOMETRY.MAGNITUDE] = str(Decimal('23.9') - D25 * (\n                dsig * duefd).log10())\n            if fd:\n                photodict[PHOTOMETRY.E_UPPER_MAGNITUDE] = str(D25 * (\n                    (dfd + duefd).log10() - dfd.log10()))\n        else:\n            photodict[PHOTOMETRY.MAGNITUDE] = str(Decimal('23.9') - D25 *\n                                                  dfd.log10())\n            photodict[PHOTOMETRY.E_UPPER_MAGNITUDE] = str(D25 * (\n                (dfd + duefd).log10() - dfd.log10()))\n            photodict[PHOTOMETRY.E_LOWER_MAGNITUDE] = str(D25 * (\n                dfd.log10() - (dfd - dlefd).log10()))", "response": "Set the photometry dictionary from a flux density measurement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the entry attributes are legal.", "response": "def _check(self):\n        \"\"\"Check that entry attributes are legal.\"\"\"\n        # Run the super method\n        super(Photometry, self)._check()\n\n        err_str = None\n        has_flux = self._KEYS.FLUX in self\n        has_flux_dens = self._KEYS.FLUX_DENSITY in self\n        has_u_flux = self._KEYS.U_FLUX in self\n        has_u_flux_dens = self._KEYS.U_FLUX_DENSITY in self\n\n        has_freq = self._KEYS.FREQUENCY in self\n        has_band = self._KEYS.BAND in self\n        has_ener = self._KEYS.ENERGY in self\n        has_u_freq = self._KEYS.U_FREQUENCY in self\n        has_u_ener = self._KEYS.U_ENERGY in self\n\n        if has_flux or has_flux_dens:\n            if not any([has_freq, has_band, has_ener]):\n                err_str = (\"Has `{}` or `{}`\".format(self._KEYS.FLUX,\n                                                     self._KEYS.FLUX_DENSITY) +\n                           \" but None of `{}`, `{}`, `{}`\".format(\n                               self._KEYS.FREQUENCY, self._KEYS.BAND,\n                               self._KEYS.ENERGY))\n            elif has_flux and not has_u_flux:\n                err_str = \"`{}` provided without `{}`.\".format(\n                    self._KEYS.FLUX, self._KEYS.U_FLUX)\n            elif has_flux_dens and not has_u_flux_dens:\n                err_str = \"`{}` provided without `{}`.\".format(\n                    self._KEYS.FLUX_DENSITY, self._KEYS.U_FLUX_DENSITY)\n            elif has_freq and not has_u_freq:\n                err_str = \"`{}` provided without `{}`.\".format(\n                    self._KEYS.FREQUENCY, self._KEYS.U_FREQUENCY)\n            elif has_ener and not has_u_ener:\n                err_str = \"`{}` provided without `{}`.\".format(\n                    self._KEYS.ENERGY, self._KEYS.U_ENERGY)\n\n        if err_str is not None:\n            raise ValueError(err_str)\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nspecify order for attributes.", "response": "def sort_func(self, key):\n        \"\"\"Specify order for attributes.\"\"\"\n        if key == self._KEYS.TIME:\n            return 'aaa'\n        if key == self._KEYS.MODEL:\n            return 'zzy'\n        if key == self._KEYS.SOURCE:\n            return 'zzz'\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_migrations_offline():\n    url = get_url()\n    context.configure(\n        url=url,\n        version_table=\"alembic_ziggurat_foundations_version\",\n        transaction_per_migration=True,\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()", "response": "Run migrations in offline mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun migrations in online mode.", "response": "def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    engine = create_engine(get_url())\n\n    connection = engine.connect()\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata,\n        version_table=\"alembic_ziggurat_foundations_version\",\n        transaction_per_migration=True,\n    )\n\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all instances by user name perm name and resource id", "response": "def by_resource_user_and_perm(\n        cls, user_id, perm_name, resource_id, db_session=None\n    ):\n        \"\"\"\n        return all instances by user name, perm name and resource id\n\n        :param user_id:\n        :param perm_name:\n        :param resource_id:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n\n        query = db_session.query(cls.model).filter(cls.model.user_id == user_id)\n        query = query.filter(cls.model.resource_id == resource_id)\n        query = query.filter(cls.model.perm_name == perm_name)\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the next sensor while iterating.", "response": "def tdSensor(self):\n        \"\"\"Get the next sensor while iterating.\n\n        :return: a dict with the keys: protocol, model, id, datatypes.\n        \"\"\"\n        protocol = create_string_buffer(20)\n        model = create_string_buffer(20)\n        sid = c_int()\n        datatypes = c_int()\n\n        self._lib.tdSensor(protocol, sizeof(protocol), model, sizeof(model),\n                           byref(sid), byref(datatypes))\n        return {'protocol': self._to_str(protocol),\n                'model': self._to_str(model),\n                'id': sid.value, 'datatypes': datatypes.value}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the sensor value for a given sensor.", "response": "def tdSensorValue(self, protocol, model, sid, datatype):\n        \"\"\"Get the sensor value for a given sensor.\n\n        :return: a dict with the keys: value, timestamp.\n        \"\"\"\n        value = create_string_buffer(20)\n        timestamp = c_int()\n\n        self._lib.tdSensorValue(protocol, model, sid, datatype,\n                                value, sizeof(value), byref(timestamp))\n        return {'value': self._to_str(value), 'timestamp': timestamp.value}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tdController(self):\n        cid = c_int()\n        ctype = c_int()\n        name = create_string_buffer(255)\n        available = c_int()\n\n        self._lib.tdController(byref(cid), byref(ctype), name, sizeof(name),\n                               byref(available))\n        return {'id': cid.value, 'type': ctype.value,\n                'name': self._to_str(name), 'available': available.value}", "response": "Get the next controller while iterating."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a cryptcontext object for use with the password manager.", "response": "def make_passwordmanager(schemes=None):\n    \"\"\"\n    schemes contains a list of replace this list with the hash(es) you wish\n    to support.\n    this example sets pbkdf2_sha256 as the default,\n    with support for legacy bcrypt hashes.\n\n    :param schemes:\n    :return: CryptContext()\n    \"\"\"\n    from passlib.context import CryptContext\n\n    if not schemes:\n        schemes = [\"pbkdf2_sha256\", \"bcrypt\"]\n    pwd_context = CryptContext(schemes=schemes, deprecated=\"auto\")\n    return pwd_context"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef messages(request, year=None, month=None, day=None,\n             template=\"gnotty/messages.html\"):\n    \"\"\"\n    Show messages for the given query or day.\n    \"\"\"\n\n    query = request.REQUEST.get(\"q\")\n    prev_url, next_url = None, None\n    messages = IRCMessage.objects.all()\n    if hide_joins_and_leaves(request):\n        messages = messages.filter(join_or_leave=False)\n    if query:\n        search = Q(message__icontains=query) | Q(nickname__icontains=query)\n        messages = messages.filter(search).order_by(\"-message_time\")\n    elif year and month and day:\n        messages = messages.filter(message_time__year=year,\n                                   message_time__month=month,\n                                   message_time__day=day)\n        day_delta = timedelta(days=1)\n        this_date = date(int(year), int(month), int(day))\n        prev_date = this_date - day_delta\n        next_date = this_date + day_delta\n        prev_url = reverse(\"gnotty_day\", args=prev_date.timetuple()[:3])\n        next_url = reverse(\"gnotty_day\", args=next_date.timetuple()[:3])\n    else:\n        return redirect(\"gnotty_year\", year=datetime.now().year)\n\n    context = dict(settings)\n    context[\"messages\"] = messages\n    context[\"prev_url\"] = prev_url\n    context[\"next_url\"] = next_url\n    return render(request, template, context)", "response": "Show messages for the given year month and day."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calendar(request, year=None, month=None, template=\"gnotty/calendar.html\"):\n\n    try:\n        year = int(year)\n    except TypeError:\n        year = datetime.now().year\n    lookup = {\"message_time__year\": year}\n    if month:\n        lookup[\"message_time__month\"] = month\n    if hide_joins_and_leaves(request):\n        lookup[\"join_or_leave\"] = False\n\n    messages = IRCMessage.objects.filter(**lookup)\n    try:\n        dates = messages.datetimes(\"message_time\", \"day\")\n    except AttributeError:\n        dates = messages.dates(\"message_time\", \"day\")\n    days = [d.date() for d in dates]\n    months = []\n\n    if days:\n        min_date, max_date = days[0], days[-1]\n        days = set(days)\n        calendar = Calendar(SUNDAY)\n        for m in range(1, 13) if not month else [int(month)]:\n            lt_max = m <= max_date.month or year < max_date.year\n            gt_min = m >= min_date.month or year > min_date.year\n            if lt_max and gt_min:\n                weeks = calendar.monthdatescalendar(year, m)\n                for w, week in enumerate(weeks):\n                    for d, day in enumerate(week):\n                        weeks[w][d] = {\n                            \"date\": day,\n                            \"in_month\": day.month == m,\n                            \"has_messages\": day in days,\n                        }\n                months.append({\"month\": date(year, m, 1), \"weeks\": weeks})\n\n    context = dict(settings)\n    context[\"months\"] = months\n    return render(request, template, context)", "response": "Show calendar months for the given year and month."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decorate_client(api_client, func, name):\n    client_attr = getattr(api_client, name)\n    if not callable(client_attr):\n        return client_attr\n\n    return OperationDecorator(client_attr, functools.partial(func, name))", "response": "A helper for creating a class which wraps all calls to the specified API client."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all expired mutex locks if a ttl is provided.", "response": "def delete_expired_locks(self):\n        \"\"\"\n        Deletes all expired mutex locks if a ttl is provided.\n        \"\"\"\n        ttl_seconds = self.get_mutex_ttl_seconds()\n        if ttl_seconds is not None:\n            DBMutex.objects.filter(creation_time__lte=timezone.now() - timedelta(seconds=ttl_seconds)).delete()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        # Delete any expired locks first\n        self.delete_expired_locks()\n        try:\n            with transaction.atomic():\n                self.lock = DBMutex.objects.create(lock_id=self.lock_id)\n        except IntegrityError:\n            raise DBMutexError('Could not acquire lock: {0}'.format(self.lock_id))", "response": "Acquires the db mutex lock. Takes the necessary steps to delete any stale locks. Returns a DBMutexError if it can t acquire the lock."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(self):\n        if not DBMutex.objects.filter(id=self.lock.id).exists():\n            raise DBMutexTimeoutError('Lock {0} expired before function completed'.format(self.lock_id))\n        else:\n            self.lock.delete()", "response": "Removes the db mutex lock."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decorate_callable(self, func):\n        def wrapper(*args, **kwargs):\n            try:\n                with self:\n                    result = func(*args, **kwargs)\n                return result\n            except DBMutexError as e:\n                if self.suppress_acquisition_exceptions:\n                    LOG.error(e)\n                else:\n                    raise e\n        functools.update_wrapper(wrapper, func)\n        return wrapper", "response": "Decorates a function with the db_mutex decorator by using this class as a context manager around\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef groupfinder(userid, request):\n    if userid and hasattr(request, \"user\") and request.user:\n        groups = [\"group:%s\" % g.id for g in request.user.groups]\n        return groups\n    return []", "response": "Default groupfinder implementaion for pyramid applications"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npositions nodes using ForceAtlas2 force-directed algorithm Parameters ---------- graph: NetworkX graph A position will be assigned to every node in G. pos_list : dict or None optional (default=None) Initial positions for nodes as a dictionary with node as keys and values as a coordinate list or tuple. If None, then use random initial positions. node_masses : dict or None optional (default=None) Predefined masses for nodes with node as keys and masses as values. If None, then use degree of nodes. iterations : int optional (default=50) Number of iterations outbound_attraction_distribution : boolean Distributes attraction along outbound edges. Hubs attract less and thus are pushed to the borders. This mode is meant to grant authorities (nodes with a high indegree) a more central position than hubs (nodes with a high outdegree). This is useful for social networks and web networks, where authorities are sometimes considered more important than hubs lin_log_mode: boolean Switch ForceAtlas model from lin-lin to lin-log (tribute to Andreas Noack). Makes clusters more tight prevent_overlapping: boolean With this mode enabled, the repulsion is modified so that the nodes do not overlap. The goal is to produce a more readable and aesthetically pleasing image. edge_weight_influence: float How much influence you give to the edges weight. 0 is \u201cno influence\u201d and 1 is \u201cnormal\u201d. jitter_tolerance: float How much swinging you allow. Above 1 discouraged. Lower gives less speed and more precision barnes_hut_optimize: boolean Barnes Hut optimization: n\u00b2 complexity to n.ln(n) ; allows larger graphs. barnes_hut_theta: float Theta of the Barnes Hut optimization scaling_ratio: float How much repulsion you want. More makes a more sparse graph. strong_gravity_mode: boolean The \u201cStrong gravity\u201d option sets a force that attracts the nodes that are distant from the center more ( is this distance). This force has the drawback of being so strong that it is sometimes stronger than the other forces. It may result in a biased placement of the nodes. However, its advantage is to force a very compact layout, which may be useful for certain purposes. multithread: boolean gravity: float Attracts nodes to the center. Prevents islands from drifting away. Returns ------- pos : dict A dictionary of positions keyed by node", "response": "def force_atlas2_layout(graph,\n                        pos_list=None,\n                        node_masses=None,\n                        iterations=100,\n\n                        outbound_attraction_distribution=False,\n                        lin_log_mode=False,\n                        prevent_overlapping=False,\n                        edge_weight_influence=1.0,\n\n                        jitter_tolerance=1.0,\n                        barnes_hut_optimize=False,\n                        barnes_hut_theta=1.2,\n\n                        scaling_ratio=2.0,\n                        strong_gravity_mode=False,\n                        multithread=False,\n                        gravity=1.0):\n    \"\"\"\n    Position nodes using ForceAtlas2 force-directed algorithm\n\n    Parameters\n    ----------\n    graph: NetworkX graph\n        A position will be assigned to every node in G.\n\n    pos_list : dict or None  optional (default=None)\n        Initial positions for nodes as a dictionary with node as keys\n        and values as a coordinate list or tuple.  If None, then use\n        random initial positions.\n\n    node_masses : dict or None  optional (default=None)\n        Predefined masses for nodes with node as keys and masses as values.\n        If None, then use degree of nodes.\n\n    iterations : int  optional (default=50)\n        Number of iterations\n\n    outbound_attraction_distribution : boolean\n        Distributes attraction along outbound edges. Hubs attract less and thus are pushed to the borders.\n        This mode is meant to grant authorities (nodes with a high indegree) a more central position than hubs (nodes with a high outdegree).\n        This is useful for social networks and web networks, where authorities are sometimes considered more important than hubs\n\n    lin_log_mode: boolean\n        Switch ForceAtlas model from lin-lin to lin-log (tribute to Andreas Noack). Makes clusters more tight\n\n    prevent_overlapping: boolean\n        With this mode enabled, the repulsion is modified so that the nodes do not overlap.\n        The goal is to produce a more readable and aesthetically pleasing image.\n\n    edge_weight_influence: float\n        How much influence you give to the edges weight. 0 is \u201cno influence\u201d and 1 is \u201cnormal\u201d.\n\n    jitter_tolerance: float\n        How much swinging you allow. Above 1 discouraged. Lower gives less speed and more precision\n\n    barnes_hut_optimize: boolean\n        Barnes Hut optimization: n\u00b2 complexity to n.ln(n) ; allows larger graphs.\n\n    barnes_hut_theta: float\n        Theta of the Barnes Hut optimization\n\n    scaling_ratio: float\n        How much repulsion you want. More makes a more sparse graph.\n\n    strong_gravity_mode: boolean\n        The \u201cStrong gravity\u201d option sets a force that attracts the nodes that are distant from the center more ( is this distance).\n        This force has the drawback of being so strong that it is sometimes stronger than the other forces.\n        It may result in a biased placement of the nodes.\n        However, its advantage is to force a very compact layout, which may be useful for certain purposes.\n\n    multithread: boolean\n\n    gravity: float\n        Attracts nodes to the center. Prevents islands from drifting away.\n\n    Returns\n    -------\n    pos : dict\n        A dictionary of positions keyed by node\n    \"\"\"\n\n    assert isinstance(graph, networkx.classes.graph.Graph), \"Not a networkx graph\"\n    assert isinstance(pos_list, dict) or (pos_list is None), \"pos must be specified as a dictionary, as in networkx\"\n\n    assert multithread is False, \"Not implemented yet\"\n\n    G = numpy.asarray(networkx.to_numpy_matrix(graph))\n\n    pos = None\n\n    if pos_list is not None:\n        pos = numpy.asarray([pos_list[i] for i in graph.nodes()])\n\n    masses = None\n\n    if node_masses is not None:\n        masses = numpy.asarray([node_masses[node] for node in graph.nodes()])\n\n    assert G.shape == (G.shape[0], G.shape[0]), \"G is not 2D square\"\n    assert numpy.all(G.T == G), \"G is not symmetric.\"\n\n    # speed and speed efficiency describe a scaling factor of dx and dy\n    # before x and y are adjusted.  These are modified as the\n    # algorithm runs to help ensure convergence.\n    speed = 1\n    speed_efficiency = 1\n\n    nodes = []\n    for i in range(0, G.shape[0]):\n        n = Node()\n        if node_masses is None:\n            n.mass = 1 + numpy.count_nonzero(G[i])\n        else:\n            n.mass = masses[i]\n        n.old_dx = 0\n        n.old_dy = 0\n        n.dx = 0\n        n.dy = 0\n        if pos is None:\n            n.x = random.random()\n            n.y = random.random()\n        else:\n            n.x = pos[i][0]\n            n.y = pos[i][1]\n        nodes.append(n)\n\n    edges = []\n    es = numpy.asarray(G.nonzero()).T\n    for e in es:\n        if e[1] <= e[0]: continue  # Avoid duplicate edges\n        edge = Edge()\n        edge.node1 = e[0]  # The index of the first node in `nodes`\n        edge.node2 = e[1]  # The index of the second node in `nodes`\n        edge.weight = G[tuple(e)]\n        edges.append(edge)\n\n    repulsion = get_repulsion(prevent_overlapping, scaling_ratio)\n\n    if strong_gravity_mode:\n        gravity_force = get_strong_gravity(scaling_ratio)\n    else:\n        gravity_force = repulsion\n\n    if outbound_attraction_distribution:\n        outbound_att_compensation = numpy.mean([n.mass for n in nodes])\n\n    attraction_coef = outbound_att_compensation if outbound_attraction_distribution else 1\n    attraction = get_attraction(lin_log_mode, outbound_attraction_distribution, prevent_overlapping,\n                                attraction_coef)\n    # Main loop\n\n    for _i in range(0, iterations):\n        for n in nodes:\n            n.old_dx = n.dx\n            n.old_dy = n.dy\n            n.dx = 0\n            n.dy = 0\n\n        # Barnes Hut optimization\n        root_region = None\n\n        if barnes_hut_optimize:\n            root_region = Quadtree(nodes)\n            root_region.build()\n\n        apply_repulsion(repulsion, nodes, barnes_hut_optimize=barnes_hut_optimize, barnes_hut_theta=barnes_hut_theta,\n                        region=root_region)\n        apply_gravity(gravity_force, nodes, gravity, scaling_ratio)\n\n        apply_attraction(attraction, nodes, edges, edge_weight_influence)\n\n        # Auto adjust speed.\n        total_swinging = 0.0  # How much irregular movement\n        total_effective_traction = 0.0  # How much useful movement\n        for n in nodes:\n            swinging = math.sqrt((n.old_dx - n.dx) * (n.old_dx - n.dx) + (n.old_dy - n.dy) * (n.old_dy - n.dy))\n            total_swinging += n.mass * swinging\n            total_effective_traction += .5 * n.mass * math.sqrt(\n                (n.old_dx + n.dx) * (n.old_dx + n.dx) + (n.old_dy + n.dy) * (n.old_dy + n.dy))\n\n        # Optimize jitter tolerance.\n        # The 'right' jitter tolerance for this network.\n        # Bigger networks need more tolerance. Denser networks need less tolerance.\n        # Totally empiric.\n\n        estimated_optimal_jitter_tolerance = .05 * math.sqrt(len(nodes))\n        min_jt = math.sqrt(estimated_optimal_jitter_tolerance)\n        max_jt = 10\n        jt = jitter_tolerance * max(min_jt, min(max_jt, estimated_optimal_jitter_tolerance * total_effective_traction /\n                                                (len(nodes) ** 2)))\n\n        min_speed_efficiency = 0.05\n\n        # Protective against erratic behavior\n        if total_swinging / total_effective_traction > 2.0:\n            if speed_efficiency > min_speed_efficiency:\n                speed_efficiency *= .5\n            jt = max(jt, jitter_tolerance)\n\n        target_speed = jt * speed_efficiency * total_effective_traction / total_swinging\n\n        if total_swinging > jt * total_effective_traction:\n            if speed_efficiency > min_speed_efficiency:\n                speed_efficiency *= .7\n        elif speed < 1000:\n            speed_efficiency *= 1.3\n\n        # But the speed shoudn't rise too much too quickly, since it would\n        # make the convergence drop dramatically.\n        max_rise = .5\n        speed = speed + min(target_speed - speed, max_rise * speed)\n\n        # Apply forces.\n        if prevent_overlapping:\n            for n in nodes:\n                swinging = n.mass * math.sqrt(\n                    (n.old_dx - n.dx) * (n.old_dx - n.dx) + (n.old_dy - n.dy) * (n.old_dy - n.dy))\n                factor = 0.1 * speed / (1 + math.sqrt(speed * swinging))\n\n                df = math.sqrt(math.pow(n.dx, 2) + n.dy ** 2)\n                factor = min(factor * df, 10.) / df\n\n                x = n.dx * factor\n                y = n.dy * factor\n        else:\n            for n in nodes:\n                swinging = n.mass * math.sqrt(\n                    (n.old_dx - n.dx) * (n.old_dx - n.dx) + (n.old_dy - n.dy) * (n.old_dy - n.dy))\n                factor = speed / (1.0 + math.sqrt(speed * swinging))\n                n.x = n.x + (n.dx * factor)\n                n.y = n.y + (n.dy * factor)\n    positions = [(n.x, n.y) for n in nodes]\n    return dict(zip(graph.nodes(), positions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_repulsion(repulsion, nodes, barnes_hut_optimize=False, region=None, barnes_hut_theta=1.2):\n    if not barnes_hut_optimize:\n        for i in range(0, len(nodes)):\n            for j in range(0, i):\n                repulsion.apply_node_to_node(nodes[i], nodes[j])\n    else:\n        for i in range(0, len(nodes)):\n            region.apply_force(nodes[i], repulsion, barnes_hut_theta)", "response": "Apply forces to the given nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the gravity to the nodes or edges of a given node.", "response": "def apply_gravity(repulsion, nodes, gravity, scaling_ratio):\n    \"\"\"\n    Iterate through the nodes or edges and apply the gravity directly to the node objects.\n    \"\"\"\n    for i in range(0, len(nodes)):\n        repulsion.apply_gravitation(nodes[i], gravity / scaling_ratio)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(cls, external_id, local_user_id, provider_name, db_session=None):\n        db_session = get_db_session(db_session)\n        return db_session.query(cls.model).get(\n            [external_id, local_user_id, provider_name]\n        )", "response": "Fetch a specific entry from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an ExternalIdentity instance based on search params", "response": "def by_external_id_and_provider(cls, external_id, provider_name, db_session=None):\n        \"\"\"\n        Returns ExternalIdentity instance based on search params\n\n        :param external_id:\n        :param provider_name:\n        :param db_session:\n        :return: ExternalIdentity\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(cls.model.external_id == external_id)\n        query = query.filter(cls.model.provider_name == provider_name)\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns User instance based on search params", "response": "def user_by_external_id_and_provider(\n        cls, external_id, provider_name, db_session=None\n    ):\n        \"\"\"\n        Returns User instance based on search params\n\n        :param external_id:\n        :param provider_name:\n        :param db_session:\n        :return: User\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.models_proxy.User)\n        query = query.filter(cls.model.external_id == external_id)\n        query = query.filter(cls.model.provider_name == provider_name)\n        query = query.filter(cls.models_proxy.User.id == cls.model.local_user_id)\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef by_user_and_perm(cls, user_id, perm_name, db_session=None):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(cls.model.user_id == user_id)\n        query = query.filter(cls.model.perm_name == perm_name)\n        return query.first()", "response": "return a record set by user and permission name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef node_is_subclass(cls, *subclass_names):\n    if not isinstance(cls, (ClassDef, Instance)):\n        return False\n\n    # if cls.bases == YES:\n    #     return False\n    for base_cls in cls.bases:\n        try:\n            for inf in base_cls.inferred():  # pragma no branch\n                if inf.qname() in subclass_names:\n                    return True\n\n                if inf != cls and node_is_subclass(  # pragma no branch\n                        inf, *subclass_names):\n                    # check up the hierarchy in case we are a subclass of\n                    # a subclass of a subclass ...\n                    return True\n        except InferenceError:  # pragma no cover\n            continue\n\n    return False", "response": "Checks if cls node has parent with subclass_name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_field_method(node):\n    name = node.attrname\n    parent = node.last_child()\n    inferred = safe_infer(parent)\n\n    if not inferred:\n        return False\n\n    for cls_name, inst in FIELD_TYPES.items():\n        if node_is_instance(inferred, cls_name) and hasattr(inst, name):\n            return True\n\n    return False", "response": "Checks if a call to a field instance method is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_node_parent_class(node):\n\n    while node.parent:  # pragma no branch\n        if isinstance(node, ClassDef):\n            return node\n\n        node = node.parent", "response": "Gets the parent class of a node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_field_embedded_doc(node):\n\n    definition = get_field_definition(node)\n    cls_name = definition.last_child().last_child()\n    cls = next(cls_name.infer())\n    return cls", "response": "Returns de ClassDef for the related embedded document in a\n    embedded document field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a node is a valid field or method in an embedded document.", "response": "def node_is_embedded_doc_attr(node):\n    \"\"\"Checks if a node is a valid field or method in a embedded document.\n    \"\"\"\n    embedded_doc = get_field_embedded_doc(node.last_child())\n    name = node.attrname\n    try:\n        r = bool(embedded_doc.lookup(name)[1][0])\n    except IndexError:\n        r = False\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message_channel(self, message):\n        self.log(None, message)\n        super(BaseBot, self).message_channel(message)", "response": "Log a message to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog any public messages and also handle the command event.", "response": "def on_pubmsg(self, connection, event):\n        \"\"\"\n        Log any public messages, and also handle the command event.\n        \"\"\"\n        for message in event.arguments():\n            self.log(event, message)\n            command_args = filter(None, message.split())\n            command_name = command_args.pop(0)\n            for handler in self.events[\"command\"]:\n                if handler.event.args[\"command\"] == command_name:\n                    self.handle_command_event(event, handler, command_args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_command_event(self, event, command, args):\n        argspec = getargspec(command)\n        num_all_args = len(argspec.args) - 2  # Ignore self/event args\n        num_pos_args = num_all_args - len(argspec.defaults or [])\n        if num_pos_args <= len(args) <= num_all_args:\n            response = command(self, event, *args)\n        elif num_all_args == num_pos_args:\n            s = \"s are\" if num_all_args != 1 else \" is\"\n            response = \"%s arg%s required\" % (num_all_args, s)\n        else:\n            bits = (num_pos_args, num_all_args)\n            response = \"between %s and %s args are required\" % bits\n        response = \"%s: %s\" % (self.get_nickname(event), response)\n        self.message_channel(response)", "response": "Handles a command event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning each timer handler in a separate greenlet thread.", "response": "def handle_timer_event(self, handler):\n        \"\"\"\n        Runs each timer handler in a separate greenlet thread.\n        \"\"\"\n        while True:\n            handler(self)\n            sleep(handler.event.args[\"seconds\"])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DeviceFactory(id, lib=None):\n    lib = lib or Library()\n    if lib.tdGetDeviceType(id) == const.TELLSTICK_TYPE_GROUP:\n        return DeviceGroup(id, lib=lib)\n    return Device(id, lib=lib)", "response": "Create a correct Device instance based on device type and return it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndispatch a single callback in the current thread.", "response": "def process_callback(self, block=True):\n        \"\"\"Dispatch a single callback in the current thread.\n\n        :param boolean block: If True, blocks waiting for a callback to come.\n        :return: True if a callback was processed; otherwise False.\n        \"\"\"\n        try:\n            (callback, args) = self._queue.get(block=block)\n            try:\n                callback(*args)\n            finally:\n                self._queue.task_done()\n        except queue.Empty:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef devices(self):\n        devices = []\n        count = self.lib.tdGetNumberOfDevices()\n        for i in range(count):\n            device = DeviceFactory(self.lib.tdGetDeviceId(i), lib=self.lib)\n            devices.append(device)\n        return devices", "response": "Return all known devices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all known sensors.", "response": "def sensors(self):\n        \"\"\"Return all known sensors.\n\n        :return: list of :class:`Sensor` instances.\n        \"\"\"\n        sensors = []\n        try:\n            while True:\n                sensor = self.lib.tdSensor()\n                sensors.append(Sensor(lib=self.lib, **sensor))\n        except TelldusError as e:\n            if e.error != const.TELLSTICK_ERROR_DEVICE_NOT_FOUND:\n                raise\n        return sensors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef controllers(self):\n        controllers = []\n        try:\n            while True:\n                controller = self.lib.tdController()\n                del controller[\"name\"]\n                del controller[\"available\"]\n                controllers.append(Controller(lib=self.lib, **controller))\n        except TelldusError as e:\n            if e.error != const.TELLSTICK_ERROR_NOT_FOUND:\n                raise\n        return controllers", "response": "Return all known controllers. Requires Telldus core library version >= 2. 1. 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new device.", "response": "def add_device(self, name, protocol, model=None, **parameters):\n        \"\"\"Add a new device.\n\n        :return: a :class:`Device` or :class:`DeviceGroup` instance.\n        \"\"\"\n        device = Device(self.lib.tdAddDevice(), lib=self.lib)\n        try:\n            device.name = name\n            device.protocol = protocol\n            if model:\n                device.model = model\n            for key, value in parameters.items():\n                device.set_parameter(key, value)\n\n            # Return correct type\n            return DeviceFactory(device.id, lib=self.lib)\n        except Exception:\n            import sys\n            exc_info = sys.exc_info()\n            try:\n                device.remove()\n            except:\n                pass\n\n            if \"with_traceback\" in dir(Exception):\n                raise exc_info[0].with_traceback(exc_info[1], exc_info[2])\n            else:\n                exec(\"raise exc_info[0], exc_info[1], exc_info[2]\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new group.", "response": "def add_group(self, name, devices):\n        \"\"\"Add a new device group.\n\n        :return: a :class:`DeviceGroup` instance.\n        \"\"\"\n        device = self.add_device(name, \"group\")\n        device.add_to_group(devices)\n        return device"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget dict with all set parameters.", "response": "def parameters(self):\n        \"\"\"Get dict with all set parameters.\"\"\"\n        parameters = {}\n        for name in self.PARAMETERS:\n            try:\n                parameters[name] = self.get_parameter(name)\n            except AttributeError:\n                pass\n        return parameters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_to_group(self, devices):\n        ids = {d.id for d in self.devices_in_group()}\n        ids.update(self._device_ids(devices))\n        self._set_group(ids)", "response": "Add devices to the group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove devices from the group.", "response": "def remove_from_group(self, devices):\n        \"\"\"Remove device(s) from the group.\"\"\"\n        ids = {d.id for d in self.devices_in_group()}\n        ids.difference_update(self._device_ids(devices))\n        self._set_group(ids)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching list of devices in group.", "response": "def devices_in_group(self):\n        \"\"\"Fetch list of devices in group.\"\"\"\n        try:\n            devices = self.get_parameter('devices')\n        except AttributeError:\n            return []\n\n        ctor = DeviceFactory\n        return [ctor(int(x), lib=self.lib) for x in devices.split(',') if x]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value(self, datatype):\n        value = self.lib.tdSensorValue(\n            self.protocol, self.model, self.id, datatype)\n        return SensorValue(datatype, value['value'], value['timestamp'])", "response": "Return the sensor value for the given data type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepPointsForSegments(points):\n    while 1:\n        point = points[-1]\n        if point.segmentType:\n            break\n        else:\n            point = points.pop()\n            points.insert(0, point)\n            continue\n        break", "response": "Move any off curves at the end of the contour to the beginning of the contour. This makes sure that the contour is not empty."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreversing the points in a .", "response": "def _reversePoints(points):\n    \"\"\"\n    Reverse the points. This differs from the\n    reversal point pen in RoboFab in that it doesn't\n    worry about maintaing the start point position.\n    That has no benefit within the context of this module.\n    \"\"\"\n    # copy the points\n    points = _copyPoints(points)\n    # find the first on curve type and recycle\n    # it for the last on curve type\n    firstOnCurve = None\n    for index, point in enumerate(points):\n        if point.segmentType is not None:\n            firstOnCurve = index\n            break\n    lastSegmentType = points[firstOnCurve].segmentType\n    # reverse the points\n    points = reversed(points)\n    # work through the reversed remaining points\n    final = []\n    for point in points:\n        segmentType = point.segmentType\n        if segmentType is not None:\n            point.segmentType = lastSegmentType\n            lastSegmentType = segmentType\n        final.append(point)\n    # move any offcurves at the end of the points\n    # to the start of the points\n    _prepPointsForSegments(final)\n    # done\n    return final"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _convertPointsToSegments(points, willBeReversed=False):\n    # get the last on curve\n    previousOnCurve = None\n    for point in reversed(points):\n        if point.segmentType is not None:\n            previousOnCurve = point.coordinates\n            break\n    assert previousOnCurve is not None\n    # gather the segments\n    offCurves = []\n    segments = []\n    for point in points:\n        # off curve, hold.\n        if point.segmentType is None:\n            offCurves.append(point)\n        else:\n            segment = InputSegment(\n                points=offCurves + [point],\n                previousOnCurve=previousOnCurve,\n                willBeReversed=willBeReversed\n            )\n            segments.append(segment)\n            offCurves = []\n            previousOnCurve = point.coordinates\n    assert not offCurves\n    return segments", "response": "Convert a list of points into InputSegment objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tValueForPointOnCubicCurve(point, cubicCurve, isHorizontal=0):\n    pt1, pt2, pt3, pt4 = cubicCurve\n    a, b, c, d = bezierTools.calcCubicParameters(pt1, pt2, pt3, pt4)\n    solutions = bezierTools.solveCubic(a[isHorizontal], b[isHorizontal], c[isHorizontal],\n        d[isHorizontal] - point[isHorizontal])\n    solutions = [t for t in solutions if 0 <= t < 1]\n    if not solutions and not isHorizontal:\n        # can happen that a horizontal line doens intersect, try the vertical\n        return _tValueForPointOnCubicCurve(point, (pt1, pt2, pt3, pt4), isHorizontal=1)\n    if len(solutions) > 1:\n        intersectionLenghts = {}\n        for t in solutions:\n            tp = _getCubicPoint(t, pt1, pt2, pt3, pt4)\n            dist = _distance(tp, point)\n            intersectionLenghts[dist] = t\n        minDist = min(intersectionLenghts.keys())\n        solutions = [intersectionLenghts[minDist]]\n    return solutions", "response": "Find a t value on a curve from a point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscaling the points in a node tree.", "response": "def _scalePoints(points, scale=1, convertToInteger=True):\n    \"\"\"\n    Scale points and optionally convert them to integers.\n    \"\"\"\n    if convertToInteger:\n        points = [\n            (int(round(x * scale)), int(round(y * scale)))\n            for (x, y) in points\n        ]\n    else:\n        points = [(x * scale, y * scale) for (x, y) in points]\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscales a single point in a node.", "response": "def _scaleSinglePoint(point, scale=1, convertToInteger=True):\n    \"\"\"\n    Scale a single point\n    \"\"\"\n    x, y = point\n    if convertToInteger:\n        return int(round(x * scale)), int(round(y * scale))\n    else:\n        return (x * scale, y * scale)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nflatten a curve segment into a list of points.", "response": "def _flattenSegment(segment, approximateSegmentLength=_approximateSegmentLength):\n    \"\"\"\n    Flatten the curve segment int a list of points.\n    The first and last points in the segment must be\n    on curves. The returned list of points will not\n    include the first on curve point.\n    false curves (where the off curves are not any\n    different from the on curves) must not be sent here.\n    duplicate points must not be sent here.\n    \"\"\"\n    onCurve1, offCurve1, offCurve2, onCurve2 = segment\n    if _pointOnLine(onCurve1, onCurve2, offCurve1) and _pointOnLine(onCurve1, onCurve2, offCurve2):\n        return [onCurve2]\n    est = _estimateCubicCurveLength(onCurve1, offCurve1, offCurve2, onCurve2) / approximateSegmentLength\n    flat = []\n    minStep = 0.1564\n    step = 1.0 / est\n    if step > .3:\n        step = minStep\n    t = step\n    while t < 1:\n        pt = _getCubicPoint(t, onCurve1, offCurve1, offCurve2, onCurve2)\n        # ignore when point is in the same direction as the on - off curve line\n        if not _pointOnLine(offCurve2, onCurve2, pt) and not _pointOnLine(onCurve1, offCurve1, pt):\n            flat.append(pt)\n        t += step\n    flat.append(onCurve2)\n    return flat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating the length of this curve by iterating through it averaging the length of the flat bits.", "response": "def _estimateCubicCurveLength(pt0, pt1, pt2, pt3, precision=10):\n    \"\"\"\n    Estimate the length of this curve by iterating\n    through it and averaging the length of the flat bits.\n    \"\"\"\n    points = []\n    length = 0\n    step = 1.0 / precision\n    factors = range(0, precision + 1)\n    for i in factors:\n        points.append(_getCubicPoint(i * step, pt0, pt1, pt2, pt3))\n    for i in range(len(points) - 1):\n        pta = points[i]\n        ptb = points[i + 1]\n        length += _distance(pta, ptb)\n    return length"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mid(pt1, pt2):\n    (x0, y0), (x1, y1) = pt1, pt2\n    return 0.5 * (x0 + x1), 0.5 * (y0 + y1)", "response": "Return the midpoint of a segment between two input points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(self, tValues):\n        if self.segmentType == \"curve\":\n            on1 = self.previousOnCurve\n            off1 = self.points[0].coordinates\n            off2 = self.points[1].coordinates\n            on2 = self.points[2].coordinates\n            return bezierTools.splitCubicAtT(on1, off1, off2, on2, *tValues)\n        elif self.segmentType == \"line\":\n            segments = []\n            x1, y1 = self.previousOnCurve\n            x2, y2 = self.points[0].coordinates\n            dx = x2 - x1\n            dy = y2 - y1\n            pp = x1, y1\n            for t in tValues:\n                np = (x1+dx*t, y1+dy*t)\n                segments.append([pp, np])\n                pp = np\n            segments.append([pp, (x2, y2)])\n            return segments\n        elif self.segmentType == \"qcurve\":\n            raise NotImplementedError\n        else:\n            raise NotImplementedError", "response": "Split the segment according the t values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a t value for a given point", "response": "def tValueForPoint(self, point):\n        \"\"\"\n        get a t values for a given point\n        required:\n            the point must be a point on the curve.\n            in an overlap cause the point will be an intersection points wich is alwasy a point on the curve\n        \"\"\"\n        if self.segmentType == \"curve\":\n            on1 = self.previousOnCurve\n            off1 = self.points[0].coordinates\n            off2 = self.points[1].coordinates\n            on2 = self.points[2].coordinates\n            return _tValueForPointOnCubicCurve(point, (on1, off1, off2, on2))\n        elif self.segmentType == \"line\":\n            return _tValueForPointOnLine(point, (self.previousOnCurve, self.points[0].coordinates))\n        elif self.segmentType == \"qcurve\":\n            raise NotImplementedError\n        else:\n            raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of normalized InputPoint objects for the contour drawn with this pen.", "response": "def getData(self):\n        \"\"\"\n        Return a list of normalized InputPoint objects\n        for the contour drawn with this pen.\n        \"\"\"\n        # organize the points into segments\n        # 1. make sure there is an on curve\n        haveOnCurve = False\n        for point in self._points:\n            if point.segmentType is not None:\n                haveOnCurve = True\n                break\n        # 2. move the off curves to front of the list\n        if haveOnCurve:\n            _prepPointsForSegments(self._points)\n        # 3. ignore double points on start and end\n        firstPoint = self._points[0]\n        lastPoint = self._points[-1]\n        if firstPoint.segmentType is not None and lastPoint.segmentType is not None:\n            if firstPoint.coordinates == lastPoint.coordinates:\n                if (firstPoint.segmentType in [\"line\", \"move\"]):\n                    del self._points[0]\n                else:\n                    raise AssertionError(\"Unhandled point type sequence\")\n        # done\n        return self._points"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reCurveFromEntireInputContour(self, inputContour):\n        if self.clockwise:\n            inputFlat = inputContour.clockwiseFlat\n        else:\n            inputFlat = inputContour.counterClockwiseFlat\n        outputFlat = []\n        for segment in self.segments:\n            # XXX this could be expensive\n            assert segment.segmentType == \"flat\"\n            outputFlat += segment.points\n        # test lengths\n        haveMatch = False\n        if len(inputFlat) == len(outputFlat):\n            if inputFlat == outputFlat:\n                haveMatch = True\n            else:\n                inputStart = inputFlat[0]\n                if inputStart in outputFlat:\n                    # there should be only one occurance of the point\n                    # but handle it just in case\n                    if outputFlat.count(inputStart) > 1:\n                        startIndexes = [index for index, point in enumerate(outputFlat) if point == inputStart]\n                    else:\n                        startIndexes = [outputFlat.index(inputStart)]\n                    # slice and dice to test possible orders\n                    for startIndex in startIndexes:\n                        test = outputFlat[startIndex:] + outputFlat[:startIndex]\n                        if inputFlat == test:\n                            haveMatch = True\n                            break\n        if haveMatch:\n            # clear out the flat points\n            self.segments = []\n            # replace with the appropriate points from the input\n            if self.clockwise:\n                inputSegments = inputContour.clockwiseSegments\n            else:\n                inputSegments = inputContour.counterClockwiseSegments\n            for inputSegment in inputSegments:\n                self.segments.append(\n                    OutputSegment(\n                        segmentType=inputSegment.segmentType,\n                        points=[\n                            OutputPoint(\n                                coordinates=point.coordinates,\n                                segmentType=point.segmentType,\n                                smooth=point.smooth,\n                                name=point.name,\n                                kwargs=point.kwargs\n                            )\n                            for point in inputSegment.points\n                        ],\n                        final=True\n                    )\n                )\n                inputSegment.used = True\n            # reset the direction of the final contour\n            self.clockwise = inputContour.clockwise\n            return True\n        return False", "response": "Re - curve from an entire input contour to the output contour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_custom_qs_manager(funcdef):\n\n    decors = getattr(funcdef, 'decorators', None)\n    if decors:\n        for dec in decors.get_children():\n            try:\n                if dec.name == 'queryset_manager':  # pragma no branch\n                    return True\n            except AttributeError:\n                continue\n\n    return False", "response": "Checks if a function definition is a queryset manager created\n    with the queryset_manager decorator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_call2custom_manager(node):\n    called = safe_infer(node.func)\n    funcdef = getattr(called, '_proxied', None)\n    return _is_custom_qs_manager(funcdef)", "response": "Checks if the call is being done to a custom queryset manager."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_custom_manager_attribute(node):\n\n    attrname = node.attrname\n    if not name_is_from_qs(attrname):\n        return False\n\n    for attr in node.get_children():\n        inferred = safe_infer(attr)\n        funcdef = getattr(inferred, '_proxied', None)\n        if _is_custom_qs_manager(funcdef):\n            return True\n\n    return False", "response": "Checks if the attribute is a valid attribute for a queryset manager."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef by_group_and_perm(cls, group_id, perm_name, db_session=None):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(cls.model.group_id == group_id)\n        query = query.filter(cls.model.perm_name == perm_name)\n        return query.first()", "response": "return by group_id and perm_name and permission name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serve_forever(django=False):\n    logger = getLogger(\"irc.dispatch\")\n    logger.setLevel(settings.LOG_LEVEL)\n    logger.addHandler(StreamHandler())\n    app = IRCApplication(django)\n    server = SocketIOServer((settings.HTTP_HOST, settings.HTTP_PORT), app)\n    print \"%s [Bot: %s] listening on %s:%s\" % (\n        settings.GNOTTY_VERSION_STRING,\n        app.bot.__class__.__name__,\n        settings.HTTP_HOST,\n        settings.HTTP_PORT,\n    )\n    server.serve_forever()", "response": "Starts the gevent - socketio server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkills a previously started daemon.", "response": "def kill(pid_file):\n    \"\"\"\n    Attempts to shut down a previously started daemon.\n    \"\"\"\n    try:\n        with open(pid_file) as f:\n            os.kill(int(f.read()), 9)\n        os.remove(pid_file)\n    except (IOError, OSError):\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run():\n    settings.parse_args()\n    pid_name = \"gnotty-%s-%s.pid\" % (settings.HTTP_HOST, settings.HTTP_PORT)\n    pid_file = settings.PID_FILE or os.path.join(gettempdir(), pid_name)\n    if settings.KILL:\n        if kill(pid_file):\n            print \"Daemon killed\"\n        else:\n            print \"Could not kill any daemons\"\n        return\n    elif kill(pid_file):\n        print \"Running daemon killed\"\n    if settings.DAEMON:\n        daemonize(pid_file)\n    serve_forever()", "response": "Main entry point for the gevent - socketio server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_start(self, host, port, channel, nickname, password):\n        self.client = WebSocketIRCClient(host, port, channel, nickname,\n                                         password, self)\n        self.spawn(self.client.start)", "response": "Called when the IRC client is started."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisconnect from the IRC server.", "response": "def disconnect(self, *args, **kwargs):\n        \"\"\"\n        WebSocket was disconnected - leave the IRC channel.\n        \"\"\"\n        quit_message = \"%s %s\" % (settings.GNOTTY_VERSION_STRING,\n                                  settings.GNOTTY_PROJECT_URL)\n        self.client.connection.quit(quit_message)\n        super(IRCNamespace, self).disconnect(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nthreads that will try and reconnect the bot if it s not connected.", "response": "def bot_watcher(self):\n        \"\"\"\n        Thread (greenlet) that will try and reconnect the bot if\n        it's not connected.\n        \"\"\"\n        default_interval = 5\n        interval = default_interval\n        while True:\n            if not self.bot.connection.connected:\n                if self.bot.reconnect():\n                    interval = default_interval\n                else:\n                    interval *= 2\n            sleep(interval)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npass the request onto a bot with a webhook.", "response": "def respond_webhook(self, environ):\n        \"\"\"\n        Passes the request onto a bot with a webhook if the webhook\n        path is requested.\n        \"\"\"\n        request = FieldStorage(fp=environ[\"wsgi.input\"], environ=environ)\n        url = environ[\"PATH_INFO\"]\n        params = dict([(k, request[k].value) for k in request])\n        try:\n            if self.bot is None:\n                raise NotImplementedError\n            response = self.bot.handle_webhook_event(environ, url, params)\n        except NotImplementedError:\n            return 404\n        except:\n            self.logger.debug(format_exc())\n            return 500\n        return response or 200"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserves a static file when Django isn t being used.", "response": "def respond_static(self, environ):\n        \"\"\"\n        Serves a static file when Django isn't being used.\n        \"\"\"\n        path = os.path.normpath(environ[\"PATH_INFO\"])\n        if path == \"/\":\n            content = self.index()\n            content_type = \"text/html\"\n        else:\n            path = os.path.join(os.path.dirname(__file__), path.lstrip(\"/\"))\n            try:\n                with open(path, \"r\") as f:\n                    content = f.read()\n            except IOError:\n                return 404\n            content_type = guess_type(path)[0]\n        return (200, [(\"Content-Type\", content_type)], content)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(self):\n        root_dir = os.path.dirname(__file__)\n        template_dir = os.path.join(root_dir, \"templates\", \"gnotty\")\n        with open(os.path.join(template_dir, \"base.html\"), \"r\") as f:\n            base = f.read()\n        with open(os.path.join(template_dir, \"chat.html\"), \"r\") as f:\n            base = base.replace(\"{% block content %}\", f.read())\n        replace = {\n            \"{% block content %}\": \"\",\n            \"{% block extrahead %}\": \"\",\n            \"{% endblock %}\": \"\",\n            \"{% load gnotty_tags %}\": \"\",\n            \"{% extends \\\"gnotty/base.html\\\" %}\": \"\",\n            \"{% url gnotty_chat %}\": \"/\",\n            \"{% gnotty_nav %}\": \"\",\n            \"{% templatetag openvariable %}\": \"{{\",\n            \"{% templatetag closevariable %}\": \"}}\",\n        }\n        for k, v in replace.items():\n            base = base.replace(k, v)\n        for k, v in settings.items():\n            base = base.replace(\"{{ %s }}\" % k, unicode(v or \"\"))\n        return base", "response": "Loads the chat interface template when Django isn t being\n        used manually dealing with the Django template bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authorized(self, environ):\n        if self.django and settings.LOGIN_REQUIRED:\n            try:\n                from django.conf import settings as django_settings\n                from django.contrib.auth import SESSION_KEY\n                from django.contrib.auth.models import User\n                from django.contrib.sessions.models import Session\n                from django.core.exceptions import ObjectDoesNotExist\n                cookie = SimpleCookie(environ[\"HTTP_COOKIE\"])\n                cookie_name = django_settings.SESSION_COOKIE_NAME\n                session_key = cookie[cookie_name].value\n                session = Session.objects.get(session_key=session_key)\n                user_id = session.get_decoded().get(SESSION_KEY)\n                user = User.objects.get(id=user_id)\n            except (ImportError, KeyError, ObjectDoesNotExist):\n                return False\n        return True", "response": "Check if the user is authenticated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Remember to use orm['appname.ModelName'] rather than \"from appname.models...\"\n        if not db.dry_run:\n            orm['gnotty.IRCMessage'].objects.filter(message=\"joins\").update(join_or_leave=True)\n            orm['gnotty.IRCMessage'].objects.filter(message=\"leaves\").update(join_or_leave=True)", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a list of permissions to legacy acl format kept for bw. compatibility", "response": "def permission_to_04_acls(permissions):\n    \"\"\"\n    Legacy acl format kept for bw. compatibility\n    :param permissions:\n    :return:\n    \"\"\"\n    acls = []\n    for perm in permissions:\n        if perm.type == \"user\":\n            acls.append((perm.user.id, perm.perm_name))\n        elif perm.type == \"group\":\n            acls.append((\"group:%s\" % perm.group.id, perm.perm_name))\n    return acls"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of permissions in a format understood by pyramid", "response": "def permission_to_pyramid_acls(permissions):\n    \"\"\"\n    Returns a list of permissions in a format understood by pyramid\n    :param permissions:\n    :return:\n    \"\"\"\n    acls = []\n    for perm in permissions:\n        if perm.type == \"user\":\n            acls.append((Allow, perm.user.id, perm.perm_name))\n        elif perm.type == \"group\":\n            acls.append((Allow, \"group:%s\" % perm.group.id, perm.perm_name))\n    return acls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ChunkBy(self, f=None):\n    if f is None:\n        return _chunk(self)\n    if is_to_destruct(f):\n        f = destruct_func(f)\n    return _chunk(self, f)", "response": "Returns a list of items in a sequence that is a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GroupBy(self: Iterable, f=None):\n    if f and is_to_destruct(f):\n        f = destruct_func(f)\n    return _group_by(self, f)", "response": "Groups the elements of the list by a function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Take(self: Iterable, n):\n\n    for i, e in enumerate(self):\n        if i == n:\n            break\n        yield e", "response": "Take the n elements from the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields the elements from self while f returns True.", "response": "def TakeWhile(self: Iterable, f):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3, 4, 5],\n            'f': lambda x: x < 4,\n            'assert': lambda ret: list(ret)  == [1, 2, 3]\n         }\n    ]\n    \"\"\"\n    if is_to_destruct(f):\n        f = destruct_func(f)\n\n    for e in self:\n        if not f(e):\n            break\n        yield e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndropping n items from the list.", "response": "def Drop(self: Iterable, n):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3, 4, 5],\n            'n': 3,\n            'assert': lambda ret: list(ret) == [1, 2]\n         }\n    ]\n    \"\"\"\n    con = tuple(self)\n    n = len(con) - n\n    if n <= 0:\n        yield from con\n    else:\n        for i, e in enumerate(con):\n            if i == n:\n                break\n            yield e"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nskips the first n items from the list.", "response": "def Skip(self: Iterable, n):\n    \"\"\"\n        [\n            {\n                'self': [1, 2, 3, 4, 5],\n                'n': 3,\n                'assert': lambda ret: list(ret) == [4, 5]\n             }\n        ]\n        \"\"\"\n    con = iter(self)\n    for i, _ in enumerate(con):\n        if i == n:\n            break\n    return con"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Shift(self, n):\n    headn = tuple(Take(self, n))\n    yield from self\n    yield from headn", "response": "Yields the n most recent elements of the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a generator that returns the elements of self and others.", "response": "def Concat(self: Iterable, *others):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3],\n            ':args': [[4, 5, 6], [7, 8, 9]],\n            'assert': lambda ret: list(ret) == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n         }\n    ]\n    \"\"\"\n    return concat_generator(self, *[unbox_if_flow(other) for other in others])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if an attribute is being accessed throught the default queryset manager ie : MyClass. objects. filter ( some = value", "response": "def _called_thru_default_qs(self, node):\n        \"\"\"Checks if an attribute is being accessed throught the default\n        queryset manager, ie: MyClass.objects.filter(some='value')\"\"\"\n        last_child = node.last_child()\n        if not last_child:\n            return False\n\n        # the default qs manager is called 'objects', we check for it here\n        attrname = getattr(last_child, 'attrname', None)\n        if attrname != 'objects':\n            return False\n\n        base_cls = last_child.last_child()\n        base_classes = DOCUMENT_BASES\n        for cls in base_cls.inferred():\n            if node_is_subclass(cls, *base_classes):\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all objects of specific type", "response": "def all(cls, klass, db_session=None):\n        \"\"\"\n        returns all objects of specific type - will work correctly with\n        sqlalchemy inheritance models, you should normally use models\n        base_query()  instead of this function its for bw. compat purposes\n\n        :param klass:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        return db_session.query(klass)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef base_query(cls, db_session=None):\n        db_session = get_db_session(db_session)\n        return db_session.query(cls.model)", "response": "returns base query for specific service\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on(event, *args, **kwargs):\n    def wrapper(func):\n        for i, arg in args:\n            kwargs[i] = arg\n        func.event = Event(event, kwargs)\n        return func\n    return wrapper", "response": "A decorator for bot mixins."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of keys and values corresponding to this model s neccesary necces", "response": "def get_dict(self, exclude_keys=None, include_keys=None):\n        \"\"\"\n        return dictionary of keys and values corresponding to this model's\n        data - if include_keys is null the function will return all keys\n\n        :param exclude_keys: (optional) is a list of columns from model that\n        should not be returned by this function\n        :param include_keys: (optional) is a list of columns from model that\n        should be returned by this function\n        :return:\n        \"\"\"\n        d = {}\n        exclude_keys_list = exclude_keys or []\n        include_keys_list = include_keys or []\n        for k in self._get_keys():\n            if k not in exclude_keys_list and (\n                k in include_keys_list or not include_keys\n            ):\n                d[k] = getattr(self, k)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_appstruct(self):\n        result = []\n        for k in self._get_keys():\n            result.append((k, getattr(self, k)))\n        return result", "response": "return list of tuples keys and values corresponding to this model s\n        data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npopulate the instance properties of the object with the values from the passed dictionary.", "response": "def populate_obj(self, appstruct, exclude_keys=None, include_keys=None):\n        \"\"\"\n        updates instance properties *for column names that exist*\n        for this model and are keys present in passed dictionary\n\n        :param appstruct: (dictionary)\n        :param exclude_keys: (optional) is a list of columns from model that\n        should not be updated by this function\n        :param include_keys: (optional) is a list of columns from model that\n        should be updated by this function\n        :return:\n        \"\"\"\n        exclude_keys_list = exclude_keys or []\n        include_keys_list = include_keys or []\n        for k in self._get_keys():\n            if (\n                k in appstruct\n                and k not in exclude_keys_list\n                and (k in include_keys_list or not include_keys)\n            ):\n                setattr(self, k, appstruct[k])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npopulate the instance properties with the values from the passed object.", "response": "def populate_obj_from_obj(self, instance, exclude_keys=None, include_keys=None):\n        \"\"\"\n        updates instance properties *for column names that exist*\n        for this model and are properties present in passed dictionary\n\n        :param instance:\n        :param exclude_keys: (optional) is a list of columns from model that\n        should not be updated by this function\n        :param include_keys: (optional) is a list of columns from model that\n        should be updated by this function\n        :return:\n        \"\"\"\n        exclude_keys_list = exclude_keys or []\n        include_keys_list = include_keys or []\n        for k in self._get_keys():\n            if (\n                hasattr(instance, k)\n                and k not in exclude_keys_list\n                and (k in include_keys_list or not include_keys)\n            ):\n                setattr(self, k, getattr(instance, k))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, db_session=None):\n        db_session = get_db_session(db_session, self)\n        db_session.delete(self)", "response": "Deletes the object from storage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn off the HX711", "response": "def power_down(self):\n        \"\"\"\n        turn off the HX711\n        :return: always True\n        :rtype bool\n        \"\"\"\n        GPIO.output(self._pd_sck, False)\n        GPIO.output(self._pd_sck, True)\n        time.sleep(0.01)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npowers up the HX711", "response": "def power_up(self):\n        \"\"\"\n        power up the HX711\n\n        :return: always True\n        :rtype bool\n        \"\"\"\n        GPIO.output(self._pd_sck, False)\n        time.sleep(0.01)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self):\n        logging.debug(\"power down\")\n        self.power_down()\n        logging.debug(\"power up\")\n        self.power_up()\n        logging.debug(\"read some raw data\")\n        result = self.get_raw_data(6)\n        if result is False:\n            raise GenericHX711Exception(\"failed to reset HX711\")\n        else:\n            return True", "response": "reset the HX711 and prepare it for the next reading\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_measure_count(self, times):\n        if not self.min_measures <= times <= self.max_measures:\n            raise ParameterValidationError(\n                \"{times} is not within the borders defined in the class\".format(\n                    times=times\n                )\n            )", "response": "check if the count of the class is within the borders defined in the class\nCOOKIE"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_gain_A_value(self, gain_A):\n        if gain_A not in self._valid_gains_for_channel_A:\n            raise ParameterValidationError(\"{gain_A} is not a valid gain\".format(gain_A=gain_A))", "response": "Validate a given value for gain_A."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the data is ready for reading.", "response": "def _ready(self):\n        \"\"\"\n        check if ther is som data is ready to get read.\n        :return True if there is some date\n        :rtype bool\n        \"\"\"\n        # if DOUT pin is low, data is ready for reading\n        _is_ready = GPIO.input(self._dout) == 0\n        logging.debug(\"check data ready for reading: {result}\".format(\n            result=\"YES\" if _is_ready is True else \"NO\"\n        ))\n        return _is_ready"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the channel gain for the current HX711.", "response": "def _set_channel_gain(self, num):\n        \"\"\"\n        Finish data transmission from HX711 by setting\n        next required gain and channel\n\n        Only called from the _read function.\n        :param num: how often so do the set (1...3)\n        :type num: int\n        :return True on success\n        :rtype bool\n        \"\"\"\n        if not 1 <= num <= 3:\n            raise AttributeError(\n                \"\"\"\"num\" has to be in the range of 1 to 3\"\"\"\n            )\n\n        for _ in range(num):\n            logging.debug(\"_set_channel_gain called\")\n            start_counter = time.perf_counter()  # start timer now.\n            GPIO.output(self._pd_sck, True)  # set high\n            GPIO.output(self._pd_sck, False)  # set low\n            end_counter = time.perf_counter()  # stop timer\n            time_elapsed = float(end_counter - start_counter)\n            # check if HX711 did not turn off...\n            # if pd_sck pin is HIGH for 60 \u00b5s and more the HX 711 enters power down mode.\n            if time_elapsed >= 0.00006:\n                logging.warning(\n                    'setting gain and channel took more than 60\u00b5s. '\n                    'Time elapsed: {:0.8f}'.format(time_elapsed)\n                )\n                # hx711 has turned off. First few readings are inaccurate.\n                # Despite this reading was ok and data can be used.\n                result = self.get_raw_data(times=6)  # set for the next reading.\n                if result is False:\n                    raise GenericHX711Exception(\"channel was not set properly\")\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read(self, max_tries=40):\n        # start by setting the pd_sck to false\n        GPIO.output(self._pd_sck, False)\n        # init the counter\n        ready_counter = 0\n\n        # loop until HX711 is ready\n        # halt when maximum number of tires is reached\n        while self._ready() is False:\n            time.sleep(0.01)  # sleep for 10 ms before next try\n            ready_counter += 1  # increment counter\n            # check loop count\n            # and stop when defined maximum is reached\n            if ready_counter >= max_tries:\n                logging.debug('self._read() not ready after 40 trials\\n')\n                return False\n\n        data_in = 0  # 2's complement data from hx 711\n        # read first 24 bits of data\n        for i in range(24):\n            # start timer\n            start_counter = time.perf_counter()\n            # request next bit from HX711\n            GPIO.output(self._pd_sck, True)\n            GPIO.output(self._pd_sck, False)\n            # stop timer\n            end_counter = time.perf_counter()\n            time_elapsed = float(end_counter - start_counter)\n\n            # check if the hx 711 did not turn off:\n            # if pd_sck pin is HIGH for 60 us and more than the HX 711 enters power down mode.\n            if time_elapsed >= 0.00006:\n                logging.debug('Reading data took longer than 60\u00b5s. Time elapsed: {:0.8f}'.format(time_elapsed))\n                return False\n\n            # Shift the bits as they come to data_in variable.\n            # Left shift by one bit then bitwise OR with the new bit.\n            data_in = (data_in << 1) | GPIO.input(self._dout)\n\n        if self.channel == 'A' and self.channel_a_gain == 128:\n            self._set_channel_gain(num=1)  # send one bit\n        elif self.channel == 'A' and self.channel_a_gain == 64:\n            self._set_channel_gain(num=3)  # send three bits\n        else:\n            self._set_channel_gain(num=2)  # send two bits\n\n        logging.debug('Binary value as it has come: ' + str(bin(data_in)))\n\n        # check if data is valid\n        # 0x800000 is the lowest\n        # 0x7fffff is the highest possible value from HX711\n        if data_in == 0x7fffff or data_in == 0x800000:\n            logging.debug('Invalid data detected: ' + str(data_in))\n            return False\n\n        # calculate int from 2's complement\n        signed_data = 0\n        if (data_in & 0x800000):  # 0b1000 0000 0000 0000 0000 0000 check if the sign bit is 1. Negative number.\n            signed_data = -((data_in ^ 0xffffff) + 1)  # convert from 2's complement to int\n        else:  # else do not do anything the value is positive number\n            signed_data = data_in\n\n        logging.debug('Converted 2\\'s complement value: ' + str(signed_data))\n\n        return signed_data", "response": "read the bit stream from HX711 and convert it to an int."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the raw data of the current log entry", "response": "def get_raw_data(self, times=5):\n        \"\"\"\n        do some readings and aggregate them using the defined statistics function\n\n        :param times: how many measures to aggregate\n        :type times: int\n        :return: the aggregate of the measured values\n        :rtype float\n        \"\"\"\n\n        self._validate_measure_count(times)\n\n        data_list = []\n        while len(data_list) < times:\n            data = self._read()\n            if data not in [False, -1]:\n                data_list.append(data)\n\n        return data_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a subtree of the node_id to the resource tree.", "response": "def build_subtree_strut(self, result, *args, **kwargs):\n        \"\"\"\n        Returns a dictionary in form of\n        {node:Resource, children:{node_id: Resource}}\n\n        :param result:\n        :return:\n        \"\"\"\n        return self.service.build_subtree_strut(result=result, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path_upper(\n        self, object_id, limit_depth=1000000, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        This returns you path to root node starting from object_id\n            currently only for postgresql\n\n        :param object_id:\n        :param limit_depth:\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.path_upper(\n            object_id=object_id,\n            limit_depth=limit_depth,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )", "response": "This method returns you path to root node starting from object_id\n            currently only for postgresql."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove the node to the new location in the tree.", "response": "def move_to_position(\n        self,\n        resource_id,\n        to_position,\n        new_parent_id=noop,\n        db_session=None,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Moves node to new location in the tree\n\n        :param resource_id: resource to move\n        :param to_position: new position\n        :param new_parent_id: new parent id\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.move_to_position(\n            resource_id=resource_id,\n            to_position=to_position,\n            new_parent_id=new_parent_id,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshifting ordering to close gaps after node deletion or being moved to another branch.", "response": "def shift_ordering_down(\n        self, parent_id, position, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Shifts ordering to \"close gaps\" after node deletion or being moved\n        to another branch, begins the shift from given position\n\n        :param parent_id:\n        :param position:\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.shift_ordering_down(\n            parent_id=parent_id,\n            position=position,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshift ordering to open a gap for node insertion", "response": "def shift_ordering_up(self, parent_id, position, db_session=None, *args, **kwargs):\n        \"\"\"\n        Shifts ordering to \"open a gap\" for node insertion,\n        begins the shift from given position\n\n        :param parent_id:\n        :param position:\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.shift_ordering_up(\n            parent_id=parent_id,\n            position=position,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the position of the node in the tree", "response": "def set_position(self, resource_id, to_position, db_session=None, *args, **kwargs):\n        \"\"\"\n        Sets node position for new node in the tree\n\n        :param resource_id: resource to move\n        :param to_position: new position\n        :param db_session:\n        :return:def count_children(cls, resource_id, db_session=None):\n        \"\"\"\n        return self.service.set_position(\n            resource_id=resource_id,\n            to_position=to_position,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if parent destination is valid for node node.", "response": "def check_node_parent(\n        self, resource_id, new_parent_id, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Checks if parent destination is valid for node\n\n        :param resource_id:\n        :param new_parent_id:\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.check_node_parent(\n            resource_id=resource_id,\n            new_parent_id=new_parent_id,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_children(self, resource_id, db_session=None, *args, **kwargs):\n        return self.service.count_children(\n            resource_id=resource_id, db_session=db_session, *args, **kwargs\n        )", "response": "Counts the number of children of a resource node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if node position for given parent is valid raises exception if not valid", "response": "def check_node_position(\n        self, parent_id, position, on_same_branch, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Checks if node position for given parent is valid, raises exception if\n        this is not the case\n\n        :param parent_id:\n        :param position:\n        :param on_same_branch: indicates that we are checking same branch\n        :param db_session:\n        :return:\n        \"\"\"\n        return self.service.check_node_position(\n            parent_id=parent_id,\n            position=position,\n            on_same_branch=on_same_branch,\n            db_session=db_session,\n            *args,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of lists l and returns a flat list", "response": "def flatten_list(l: List[list]) -> list:\n    \"\"\" takes a list of lists, l and returns a flat list\n    \"\"\"\n    return [v for inner_l in l for v in inner_l]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading in NEM file and return meter readings named tuple", "response": "def read_nem_file(file_path: str) -> NEMFile:\n    \"\"\" Read in NEM file and return meter readings named tuple\n\n    :param file_path: The NEM file to process\n    :returns: The file that was created\n    \"\"\"\n\n    _, file_extension = os.path.splitext(file_path)\n    if file_extension.lower() == '.zip':\n        with zipfile.ZipFile(file_path, 'r') as archive:\n            for csv_file in archive.namelist():\n                with archive.open(csv_file) as csv_text:\n                    # Zip file is open in binary mode\n                    # So decode then convert back to list\n                    nmi_file = csv_text.read().decode('utf-8').splitlines()\n                    reader = csv.reader(nmi_file, delimiter=',')\n                    return parse_nem_rows(reader, file_name=csv_file)\n\n    with open(file_path) as nmi_file:\n        return parse_nem_file(nmi_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses NEM file and return meter readings named tuple", "response": "def parse_nem_file(nem_file) -> NEMFile:\n    \"\"\" Parse NEM file and return meter readings named tuple \"\"\"\n    reader = csv.reader(nem_file, delimiter=',')\n    return parse_nem_rows(reader, file_name=nem_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse NEM files and return a dictionary of meter readings and transactions named tuple", "response": "def parse_nem_rows(nem_list: Iterable, file_name=None) -> NEMFile:\n    \"\"\" Parse NEM row iterator and return meter readings named tuple \"\"\"\n\n    header = HeaderRecord(None, None, None, None, file_name)\n    readings = dict()  # readings nested by NMI then channel\n    trans = dict()  # transactions nested by NMI then channel\n    nmi_d = None  # current NMI details block that readings apply to\n\n    for i, row in enumerate(nem_list):\n        record_indicator = int(row[0])\n\n        if i == 0 and record_indicator != 100:\n            raise ValueError(\"NEM Files must start with a 100 row\")\n\n        if record_indicator == 100:\n            header = parse_100_row(row, file_name)\n            if header.version_header not in ['NEM12', 'NEM13']:\n                raise ValueError(\"Invalid NEM version {}\".format(\n                    header.version_header))\n\n        elif record_indicator == 900:\n            for nmi in readings:\n                for suffix in readings[nmi]:\n                    readings[nmi][suffix] = flatten_list(readings[nmi][suffix])\n            break  # End of file\n\n        elif header.version_header == 'NEM12' and record_indicator == 200:\n            try:\n                nmi_details = parse_200_row(row)\n            except ValueError:\n                logging.error('Error passing 200 row:')\n                logging.error(row)\n                raise\n            nmi_d = nmi_details\n\n            if nmi_d.nmi not in readings:\n                readings[nmi_d.nmi] = {}\n            if nmi_d.nmi_suffix not in readings[nmi_d.nmi]:\n                readings[nmi_d.nmi][nmi_d.nmi_suffix] = []\n            if nmi_d.nmi not in trans:\n                trans[nmi_d.nmi] = {}\n            if nmi_d.nmi_suffix not in trans[nmi_d.nmi]:\n                trans[nmi_d.nmi][nmi_d.nmi_suffix] = []\n\n        elif header.version_header == 'NEM12' and record_indicator == 300:\n            num_intervals = int(24 * 60 / nmi_d.interval_length)\n            assert len(row) > num_intervals, \"Incomplete 300 Row in {}\".format(\n                file_name)\n            interval_record = parse_300_row(row, nmi_d.interval_length,\n                                            nmi_d.uom)\n            # don't flatten the list of interval readings at this stage,\n            # as they may need to be adjusted by a 400 row\n            readings[nmi_d.nmi][nmi_d.nmi_suffix].append(\n                interval_record.interval_values)\n\n        elif header.version_header == 'NEM12' and record_indicator == 400:\n            event_record = parse_400_row(row)\n            readings[nmi_d.nmi][nmi_d.nmi_suffix][-1] = update_reading_events(\n                readings[nmi_d.nmi][nmi_d.nmi_suffix][-1], event_record)\n\n        elif header.version_header == 'NEM12' and record_indicator == 500:\n            b2b_details = parse_500_row(row)\n            trans[nmi_d.nmi][nmi_d.nmi_suffix].append(b2b_details)\n\n        elif header.version_header == 'NEM13' and record_indicator == 550:\n            b2b_details = parse_550_row(row)\n            trans[nmi_d.nmi][nmi_d.nmi_suffix].append(b2b_details)\n\n        elif header.version_header == 'NEM13' and record_indicator == 250:\n            basic_data = parse_250_row(row)\n            reading = calculate_manual_reading(basic_data)\n\n            nmi_d = basic_data\n\n            if basic_data.nmi not in readings:\n                readings[nmi_d.nmi] = {}\n            if nmi_d.nmi_suffix not in readings[nmi_d.nmi]:\n                readings[nmi_d.nmi][nmi_d.nmi_suffix] = []\n            if nmi_d.nmi not in trans:\n                trans[nmi_d.nmi] = {}\n            if nmi_d.nmi_suffix not in trans[nmi_d.nmi]:\n                trans[nmi_d.nmi][nmi_d.nmi_suffix] = []\n\n            readings[nmi_d.nmi][nmi_d.nmi_suffix].append([reading])\n\n        else:\n            logging.warning(\n                \"Record indicator %s not supported and was skipped\",\n                record_indicator)\n    return NEMFile(header, readings, trans)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_manual_reading(basic_data: BasicMeterData) -> Reading:\n    t_start = basic_data.previous_register_read_datetime\n    t_end = basic_data.current_register_read_datetime\n    read_start = basic_data.previous_register_read\n    read_end = basic_data.current_register_read\n    value = basic_data.quantity\n\n    uom = basic_data.uom\n    quality_method = basic_data.current_quality_method\n\n    return Reading(t_start, t_end, value, uom, quality_method, \"\", \"\",\n                      read_start, read_end)", "response": "Calculates the interval between two manual readings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses header record (100)", "response": "def parse_100_row(row: list, file_name: str) -> HeaderRecord:\n    \"\"\" Parse header record (100) \"\"\"\n    return HeaderRecord(\n        row[1],\n        parse_datetime(row[2]),\n        row[3],\n        row[4],\n        file_name,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses NMI data details record 200", "response": "def parse_200_row(row: list) -> NmiDetails:\n    \"\"\" Parse NMI data details record (200) \"\"\"\n    return NmiDetails(row[1], row[2], row[3], row[4], row[5], row[6],\n                         row[7], int(row[8]), parse_datetime(row[9]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_250_row(row: list) -> BasicMeterData:\n    return BasicMeterData(row[1], row[2], row[3], row[4], row[5],\n                             row[6], row[7], float(row[8]),\n                             parse_datetime(row[9]), row[10], row[11], row[12],\n                             float(row[13]), parse_datetime(\n                                 row[14]), row[15], row[16], row[17],\n                             float(row[18]), row[19], row[20],\n                             parse_datetime(row[21]), parse_datetime(row[22]))", "response": "Parse basic meter data record (250)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_300_row(row: list, interval: int, uom: str) -> IntervalRecord:\n\n    num_intervals = int(24 * 60 / interval)\n    interval_date = parse_datetime(row[1])\n    last_interval = 2 + num_intervals\n    quality_method = row[last_interval]\n\n    interval_values = parse_interval_records(\n        row[2:last_interval], interval_date, interval, uom, quality_method)\n\n    return IntervalRecord(interval_date, interval_values,\n                             row[last_interval + 0], row[last_interval + 1],\n                             row[last_interval + 2],\n                             parse_datetime(row[last_interval + 3]),\n                             parse_datetime(row[last_interval + 4]))", "response": "Parse a 300 row of data into an IntervalRecord object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_interval_records(interval_record, interval_date, interval, uom,\n                           quality_method) -> List[Reading]:\n    \"\"\" Convert interval values into tuples with datetime\n    \"\"\"\n    interval_delta = timedelta(minutes=interval)\n    return [\n        Reading(\n            t_start=interval_date + (i * interval_delta),\n            t_end=interval_date + (i * interval_delta) + interval_delta,\n            read_value=parse_reading(val),\n            uom=uom,\n            quality_method=quality_method,\n            event_code=\"\",  # event is unknown at time of reading\n            event_desc=\"\",  # event is unknown at time of reading\n            read_start=None,\n            read_end=None  # No before and after readings for intervals\n        ) for i, val in enumerate(interval_record)\n    ]", "response": "Convert interval records into tuples with datetime\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting reading value to float if possible", "response": "def parse_reading(val: str) -> Optional[float]:\n    \"\"\" Convert reading value to float (if possible) \"\"\"\n    try:\n        return float(val)\n    except ValueError:\n        logging.warning('Reading of \"%s\" is not a number', val)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_400_row(row: list) -> tuple:\n\n    return EventRecord(int(row[1]), int(row[2]), row[3], row[4], row[5])", "response": "Parses a 400 row into an interval event record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_reading_events(readings, event_record):\n    # event intervals are 1-indexed\n    for i in range(event_record.start_interval - 1, event_record.end_interval):\n        readings[i] = Reading(\n            t_start=readings[i].t_start,\n            t_end=readings[i].t_end,\n            read_value=readings[i].read_value,\n            uom=readings[i].uom,\n            quality_method=event_record.quality_method,\n            event_code=event_record.reason_code,\n            event_desc=event_record.reason_description,\n            read_start=readings[i].read_start,\n            read_end=readings[i].read_end)\n    return readings", "response": "Updates the list of readings from a 300 row to reflect any events found in the event_record."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a datetime string into a python datetime object", "response": "def parse_datetime(record: str) -> Optional[datetime]:\n    \"\"\" Parse a datetime string into a python datetime object \"\"\"\n    # NEM defines Date8, DateTime12 and DateTime14\n    format_strings = {8: '%Y%m%d', 12: '%Y%m%d%H%M', 14: '%Y%m%d%H%M%S'}\n    if record == '':\n        return None\n    return datetime.strptime(record.strip(),\n                                          format_strings[len(record.strip())])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_args(self):\n        options, _ = parser.parse_args()\n        file_settings = {}\n        if options.CONF_FILE:\n            execfile(options.CONF_FILE, {}, file_settings)\n        for option in self.option_list:\n            if option.dest:\n                file_value = file_settings.get(\"GNOTTY_%s\" % option.dest, None)\n                # optparse doesn't seem to provide a way to determine if\n                # an option's value was provided as a CLI arg, or if the\n                # default is being used, so we manually check sys.argv,\n                # since provided CLI args should take precedence over\n                # any settings defined in a conf module.\n                flags = option._short_opts + option._long_opts\n                in_argv = set(flags) & set(sys.argv)\n                options_value = getattr(options, option.dest)\n                if file_value and not in_argv:\n                    self[option.dest] = file_value\n                elif in_argv:\n                    self[option.dest] = options_value\n                else:\n                    self[option.dest] = self.get(option.dest, options_value)\n        self.set_max_message_length()\n        self[\"STATIC_URL\"] = \"/static/\"\n        self[\"LOG_LEVEL\"] = getattr(logging, self[\"LOG_LEVEL\"])", "response": "Called from gnotty. server. run and parses any CLI args that were provided by the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef color(nickname):\n    _hex = md5(nickname).hexdigest()[:6]\n    darken = lambda s: str(int(round(int(s, 16) * .7)))\n    return \"rgb(%s)\" % \",\".join([darken(_hex[i:i+2]) for i in range(6)[::2]])", "response": "Provides a consistent color for a nickname."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_welcome(self, connection, event):\n        connection.join(self.channel, key=settings.IRC_CHANNEL_KEY or \"\")", "response": "Join the channel once connected to the IRC server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when a nickname is in use.", "response": "def on_nicknameinuse(self, connection, event):\n        \"\"\"\n        Increment a digit on the nickname if it's in use, and\n        re-connect.\n        \"\"\"\n        digits = \"\"\n        while self.nickname[-1].isdigit():\n            digits = self.nickname[-1] + digits\n            self.nickname = self.nickname[:-1]\n        digits = 1 if not digits else int(digits) + 1\n        self.nickname += str(digits)\n        self.connect(self.host, self.port, self.nickname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to a channel.", "response": "def message_channel(self, message):\n        \"\"\"\n        Nicer shortcut for sending a message to a channel. Also\n        irclib doesn't handle unicode so we bypass its\n        privmsg -> send_raw methods and use its socket directly.\n        \"\"\"\n        data = \"PRIVMSG %s :%s\\r\\n\" % (self.channel, message)\n        self.connection.socket.send(data.encode(\"utf-8\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to the channel.", "response": "def emit_message(self, message):\n        \"\"\"\n        Send a message to the channel. We also emit the message\n        back to the sender's WebSocket.\n        \"\"\"\n        try:\n            nickname_color = self.nicknames[self.nickname]\n        except KeyError:\n            # Only accept messages if we've joined.\n            return\n        message = message[:settings.MAX_MESSAGE_LENGTH]\n        # Handle IRC commands.\n        if message.startswith(\"/\"):\n            self.connection.send_raw(message.lstrip(\"/\"))\n            return\n        self.message_channel(message)\n        self.namespace.emit(\"message\", self.nickname, message, nickname_color)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending the nickname list to the Websocket. Called whenever the thenicknames list changes.", "response": "def emit_nicknames(self):\n        \"\"\"\n        Send the nickname list to the Websocket. Called whenever the\n        nicknames list changes.\n        \"\"\"\n        nicknames = [{\"nickname\": name, \"color\": color(name)}\n                     for name in sorted(self.nicknames.keys())]\n        self.namespace.emit(\"nicknames\", nicknames)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when a user sends a list of nicknames.", "response": "def on_namreply(self, connection, event):\n        \"\"\"\n        Initial list of nicknames received - remove op/voice prefixes,\n        and send the list to the WebSocket.\n        \"\"\"\n        for nickname in event.arguments()[-1].split():\n            nickname = nickname.lstrip(\"@+\")\n            self.nicknames[nickname] = color(nickname)\n        self.emit_nicknames()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_quit(self, connection, event):\n        nickname = self.get_nickname(event)\n        nickname_color = self.nicknames[nickname]\n        del self.nicknames[nickname]\n        self.namespace.emit(\"message\", nickname, \"leaves\", nickname_color)\n        self.emit_nicknames()", "response": "Called when the channel is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles messages received in the channel - send them to the WebSocket.", "response": "def on_pubmsg(self, connection, event):\n        \"\"\"\n        Messages received in the channel - send them to the WebSocket.\n        \"\"\"\n        for message in event.arguments():\n            nickname = self.get_nickname(event)\n            nickname_color = self.nicknames[nickname]\n            self.namespace.emit(\"message\", nickname, message, nickname_color)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a resource_id from the database", "response": "def get(cls, resource_id, db_session=None):\n        \"\"\"\n        Fetch row using primary key -\n        will use existing object in session if already present\n\n        :param resource_id:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        return db_session.query(cls.model).get(resource_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perms_for_user(cls, instance, user, db_session=None):\n        db_session = get_db_session(db_session, instance)\n        query = db_session.query(\n            cls.models_proxy.GroupResourcePermission.group_id.label(\"owner_id\"),\n            cls.models_proxy.GroupResourcePermission.perm_name,\n            sa.literal(\"group\").label(\"type\"),\n        )\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.group_id.in_(\n                [gr.id for gr in user.groups]\n            )\n        )\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.resource_id == instance.resource_id\n        )\n\n        query2 = db_session.query(\n            cls.models_proxy.UserResourcePermission.user_id.label(\"owner_id\"),\n            cls.models_proxy.UserResourcePermission.perm_name,\n            sa.literal(\"user\").label(\"type\"),\n        )\n        query2 = query2.filter(\n            cls.models_proxy.UserResourcePermission.user_id == user.id\n        )\n        query2 = query2.filter(\n            cls.models_proxy.UserResourcePermission.resource_id == instance.resource_id\n        )\n        query = query.union(query2)\n\n        groups_dict = dict([(g.id, g) for g in user.groups])\n        perms = [\n            PermissionTuple(\n                user,\n                row.perm_name,\n                row.type,\n                groups_dict.get(row.owner_id) if row.type == \"group\" else None,\n                instance,\n                False,\n                True,\n            )\n            for row in query\n        ]\n\n        # include all perms if user is the owner of this resource\n        if instance.owner_user_id == user.id:\n            perms.append(\n                PermissionTuple(\n                    user, ALL_PERMISSIONS, \"user\", None, instance, True, True\n                )\n            )\n        groups_dict = dict([(g.id, g) for g in user.groups])\n        if instance.owner_group_id in groups_dict:\n            perms.append(\n                PermissionTuple(\n                    user,\n                    ALL_PERMISSIONS,\n                    \"group\",\n                    groups_dict.get(instance.owner_group_id),\n                    instance,\n                    True,\n                    True,\n                )\n            )\n\n        return perms", "response": "returns all permissions that given user has for this resource\n            from groups and directly set ones too"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns permissions that given user has for this resource without ones inherited from groups that user belongs to the resource", "response": "def direct_perms_for_user(cls, instance, user, db_session=None):\n        \"\"\"\n        returns permissions that given user has for this resource\n            without ones inherited from groups that user belongs to\n\n        :param instance:\n        :param user:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session, instance)\n        query = db_session.query(\n            cls.models_proxy.UserResourcePermission.user_id,\n            cls.models_proxy.UserResourcePermission.perm_name,\n        )\n        query = query.filter(cls.models_proxy.UserResourcePermission.user_id == user.id)\n        query = query.filter(\n            cls.models_proxy.UserResourcePermission.resource_id == instance.resource_id\n        )\n\n        perms = [\n            PermissionTuple(user, row.perm_name, \"user\", None, instance, False, True)\n            for row in query\n        ]\n\n        # include all perms if user is the owner of this resource\n        if instance.owner_user_id == user.id:\n            perms.append(\n                PermissionTuple(user, ALL_PERMISSIONS, \"user\", None, instance, True)\n            )\n        return perms"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns permissions that given user has for this resource that are inherited from groups", "response": "def group_perms_for_user(cls, instance, user, db_session=None):\n        \"\"\"\n        returns permissions that given user has for this resource\n            that are inherited from groups\n\n        :param instance:\n        :param user:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session, instance)\n        perms = resource_permissions_for_users(\n            cls.models_proxy,\n            ANY_PERMISSION,\n            resource_ids=[instance.resource_id],\n            user_ids=[user.id],\n            db_session=db_session,\n        )\n        perms = [p for p in perms if p.type == \"group\"]\n        # include all perms if user is the owner of this resource\n        groups_dict = dict([(g.id, g) for g in user.groups])\n        if instance.owner_group_id in groups_dict:\n            perms.append(\n                PermissionTuple(\n                    user,\n                    ALL_PERMISSIONS,\n                    \"group\",\n                    groups_dict.get(instance.owner_group_id),\n                    instance,\n                    True,\n                    True,\n                )\n            )\n        return perms"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of PermissionTuple objects for users AND groups that have given permission for the resource", "response": "def users_for_perm(\n        cls,\n        instance,\n        perm_name,\n        user_ids=None,\n        group_ids=None,\n        limit_group_permissions=False,\n        skip_group_perms=False,\n        db_session=None,\n    ):\n        \"\"\"\n        return PermissionTuples for users AND groups that have given\n        permission for the resource, perm_name is __any_permission__ then\n        users with any permission will be listed\n\n        :param instance:\n        :param perm_name:\n        :param user_ids: limits the permissions to specific user ids\n        :param group_ids: limits the permissions to specific group ids\n        :param limit_group_permissions: should be used if we do not want to have\n        user objects returned for group permissions, this might cause performance\n        issues for big groups\n        :param skip_group_perms: do not attach group permissions to the resultset\n        :param db_session:\n        :return:\n        \"\"\"  # noqa\n        db_session = get_db_session(db_session, instance)\n        users_perms = resource_permissions_for_users(\n            cls.models_proxy,\n            [perm_name],\n            [instance.resource_id],\n            user_ids=user_ids,\n            group_ids=group_ids,\n            limit_group_permissions=limit_group_permissions,\n            skip_group_perms=skip_group_perms,\n            db_session=db_session,\n        )\n        if instance.owner_user_id:\n            users_perms.append(\n                PermissionTuple(\n                    instance.owner, ALL_PERMISSIONS, \"user\", None, instance, True, True\n                )\n            )\n        if instance.owner_group_id and not skip_group_perms:\n            for user in instance.owner_group.users:\n                users_perms.append(\n                    PermissionTuple(\n                        user,\n                        ALL_PERMISSIONS,\n                        \"group\",\n                        instance.owner_group,\n                        instance,\n                        True,\n                        True,\n                    )\n                )\n\n        return users_perms"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the resouce by id", "response": "def by_resource_id(cls, resource_id, db_session=None):\n        \"\"\"\n        fetch the resouce by id\n\n        :param resource_id:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(\n            cls.model.resource_id == int(resource_id)\n        )\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef perm_by_group_and_perm_name(\n        cls, resource_id, group_id, perm_name, db_session=None\n    ):\n        \"\"\"\n        fetch permissions by group and permission name\n\n        :param resource_id:\n        :param group_id:\n        :param perm_name:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.models_proxy.GroupResourcePermission)\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.group_id == group_id\n        )\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.perm_name == perm_name\n        )\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.resource_id == resource_id\n        )\n        return query.first()", "response": "fetch permissions by group and permission name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of PermissionTuples for groups that have given permission for the resource", "response": "def groups_for_perm(\n        cls,\n        instance,\n        perm_name,\n        group_ids=None,\n        limit_group_permissions=False,\n        db_session=None,\n    ):\n        \"\"\"\n        return PermissionTuples for groups that have given\n        permission for the resource, perm_name is __any_permission__ then\n        users with any permission will be listed\n\n        :param instance:\n        :param perm_name:\n        :param group_ids: limits the permissions to specific group ids\n        :param limit_group_permissions: should be used if we do not want to have\n        user objects returned for group permissions, this might cause performance\n        issues for big groups\n        :param db_session:\n        :return:\n        \"\"\"  # noqa\n        db_session = get_db_session(db_session, instance)\n        group_perms = resource_permissions_for_users(\n            cls.models_proxy,\n            [perm_name],\n            [instance.resource_id],\n            group_ids=group_ids,\n            limit_group_permissions=limit_group_permissions,\n            skip_user_perms=True,\n            db_session=db_session,\n        )\n        if instance.owner_group_id:\n            for user in instance.owner_group.users:\n                group_perms.append(\n                    PermissionTuple(\n                        user,\n                        ALL_PERMISSIONS,\n                        \"group\",\n                        instance.owner_group,\n                        instance,\n                        True,\n                        True,\n                    )\n                )\n\n        return group_perms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lock_resource_for_update(cls, resource_id, db_session):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(cls.model.resource_id == resource_id)\n        query = query.with_for_update()\n        return query.first()", "response": "Select a resource for update - locking access for other transactions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(cls, user_id, db_session=None):\n        db_session = get_db_session(db_session)\n        return db_session.query(cls.model).get(user_id)", "response": "Fetch user s nagios entry table info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef permissions(cls, instance, db_session=None):\n        db_session = get_db_session(db_session, instance)\n        query = db_session.query(\n            cls.models_proxy.GroupPermission.group_id.label(\"owner_id\"),\n            cls.models_proxy.GroupPermission.perm_name.label(\"perm_name\"),\n            sa.literal(\"group\").label(\"type\"),\n        )\n        query = query.filter(\n            cls.models_proxy.GroupPermission.group_id\n            == cls.models_proxy.UserGroup.group_id\n        )\n        query = query.filter(\n            cls.models_proxy.User.id == cls.models_proxy.UserGroup.user_id\n        )\n        query = query.filter(cls.models_proxy.User.id == instance.id)\n\n        query2 = db_session.query(\n            cls.models_proxy.UserPermission.user_id.label(\"owner_id\"),\n            cls.models_proxy.UserPermission.perm_name.label(\"perm_name\"),\n            sa.literal(\"user\").label(\"type\"),\n        )\n        query2 = query2.filter(cls.models_proxy.UserPermission.user_id == instance.id)\n        query = query.union(query2)\n        groups_dict = dict([(g.id, g) for g in instance.groups])\n        return [\n            PermissionTuple(\n                instance,\n                row.perm_name,\n                row.type,\n                groups_dict.get(row.owner_id) if row.type == \"group\" else None,\n                None,\n                False,\n                True,\n            )\n            for row in query\n        ]", "response": "returns all non - resource permissions based on what groups user\n            belongs and directly set ones for this user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resources_with_perms(\n        cls, instance, perms, resource_ids=None, resource_types=None, db_session=None\n    ):\n        \"\"\"\n        returns all resources that user has perms for\n            (note that at least one perm needs to be met)\n\n        :param instance:\n        :param perms:\n        :param resource_ids: restricts the search to specific resources\n        :param resource_types:\n        :param db_session:\n        :return:\n        \"\"\"\n        # owned entities have ALL permissions so we return those resources too\n        # even without explict perms set\n        # TODO: implement admin superrule perm - maybe return all apps\n        db_session = get_db_session(db_session, instance)\n        query = db_session.query(cls.models_proxy.Resource).distinct()\n        group_ids = [gr.id for gr in instance.groups]\n        # if user has some groups lets try to join based on their permissions\n        if group_ids:\n            join_conditions = (\n                cls.models_proxy.GroupResourcePermission.group_id.in_(group_ids),\n                cls.models_proxy.Resource.resource_id\n                == cls.models_proxy.GroupResourcePermission.resource_id,\n                cls.models_proxy.GroupResourcePermission.perm_name.in_(perms),\n            )\n            query = query.outerjoin(\n                (cls.models_proxy.GroupResourcePermission, sa.and_(*join_conditions))\n            )\n            # ensure outerjoin permissions are correct -\n            # dont add empty rows from join\n            # conditions are - join ON possible group permissions\n            # OR owning group/user\n            query = query.filter(\n                sa.or_(\n                    cls.models_proxy.Resource.owner_user_id == instance.id,\n                    cls.models_proxy.Resource.owner_group_id.in_(group_ids),\n                    cls.models_proxy.GroupResourcePermission.perm_name != None,\n                )  # noqa\n            )\n        else:\n            # filter just by username\n            query = query.filter(cls.models_proxy.Resource.owner_user_id == instance.id)\n        # lets try by custom user permissions for resource\n        query2 = db_session.query(cls.models_proxy.Resource).distinct()\n        query2 = query2.filter(\n            cls.models_proxy.UserResourcePermission.user_id == instance.id\n        )\n        query2 = query2.filter(\n            cls.models_proxy.Resource.resource_id\n            == cls.models_proxy.UserResourcePermission.resource_id\n        )\n        query2 = query2.filter(\n            cls.models_proxy.UserResourcePermission.perm_name.in_(perms)\n        )\n        if resource_ids:\n            query = query.filter(\n                cls.models_proxy.Resource.resource_id.in_(resource_ids)\n            )\n            query2 = query2.filter(\n                cls.models_proxy.Resource.resource_id.in_(resource_ids)\n            )\n\n        if resource_types:\n            query = query.filter(\n                cls.models_proxy.Resource.resource_type.in_(resource_types)\n            )\n            query2 = query2.filter(\n                cls.models_proxy.Resource.resource_type.in_(resource_types)\n            )\n        query = query.union(query2)\n        query = query.order_by(cls.models_proxy.Resource.resource_name)\n        return query", "response": "returns all resources that user has permission for the given resource_ids and perms"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of groups users belongs to with eager loaded resources owned by those groups", "response": "def groups_with_resources(cls, instance):\n        \"\"\"\n        Returns a list of groups users belongs to with eager loaded\n        resources owned by those groups\n\n        :param instance:\n        :return:\n        \"\"\"\n        return instance.groups_dynamic.options(\n            sa.orm.eagerload(cls.models_proxy.Group.resources)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn list of permissions and resources for this user", "response": "def resources_with_possible_perms(\n        cls, instance, resource_ids=None, resource_types=None, db_session=None\n    ):\n        \"\"\"\n        returns list of permissions and resources for this user\n\n        :param instance:\n        :param resource_ids: restricts the search to specific resources\n        :param resource_types: restricts the search to specific resource types\n        :param db_session:\n        :return:\n        \"\"\"\n        perms = resource_permissions_for_users(\n            cls.models_proxy,\n            ANY_PERMISSION,\n            resource_ids=resource_ids,\n            resource_types=resource_types,\n            user_ids=[instance.id],\n            db_session=db_session,\n        )\n        for resource in instance.resources:\n            perms.append(\n                PermissionTuple(\n                    instance, ALL_PERMISSIONS, \"user\", None, resource, True, True\n                )\n            )\n        for group in cls.groups_with_resources(instance):\n            for resource in group.resources:\n                perms.append(\n                    PermissionTuple(\n                        instance, ALL_PERMISSIONS, \"group\", group, resource, True, True\n                    )\n                )\n\n        return perms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn user gravatar url", "response": "def gravatar_url(cls, instance, default=\"mm\", **kwargs):\n        \"\"\"\n        returns user gravatar url\n\n        :param instance:\n        :param default:\n        :param kwargs:\n        :return:\n        \"\"\"\n        # construct the url\n        hash = hashlib.md5(instance.email.encode(\"utf8\").lower()).hexdigest()\n        if \"d\" not in kwargs:\n            kwargs[\"d\"] = default\n        params = \"&\".join(\n            [\n                six.moves.urllib.parse.urlencode({key: value})\n                for key, value in kwargs.items()\n            ]\n        )\n        return \"https://secure.gravatar.com/avatar/{}?{}\".format(hash, params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_password(cls, instance, raw_password):\n        # support API for both passlib 1.x and 2.x\n        hash_callable = getattr(\n            instance.passwordmanager, \"hash\", instance.passwordmanager.encrypt\n        )\n        password = hash_callable(raw_password)\n        if six.PY2:\n            instance.user_password = password.decode(\"utf8\")\n        else:\n            instance.user_password = password\n        cls.regenerate_security_code(instance)", "response": "Sets the user password on a user using password manager"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking string with users password hash using password manager", "response": "def check_password(cls, instance, raw_password, enable_hash_migration=True):\n        \"\"\"\n        checks string with users password hash using password manager\n\n        :param instance:\n        :param raw_password:\n        :param enable_hash_migration: if legacy hashes should be migrated\n        :return:\n        \"\"\"\n        verified, replacement_hash = instance.passwordmanager.verify_and_update(\n            raw_password, instance.user_password\n        )\n        if enable_hash_migration and replacement_hash:\n            if six.PY2:\n                instance.user_password = replacement_hash.decode(\"utf8\")\n            else:\n                instance.user_password = replacement_hash\n        return verified"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef by_id(cls, user_id, db_session=None):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(cls.model.id == user_id)\n        query = query.options(sa.orm.eagerload(\"groups\"))\n        return query.first()", "response": "fetch user by user id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef by_user_name_and_security_code(cls, user_name, security_code, db_session=None):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(\n            sa.func.lower(cls.model.user_name) == (user_name or \"\").lower()\n        )\n        query = query.filter(cls.model.security_code == security_code)\n        return query.first()", "response": "fetch user objects by user name and security code"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches user objects by user names", "response": "def by_user_names(cls, user_names, db_session=None):\n        \"\"\"\n        fetch user objects by user names\n\n        :param user_names:\n        :param db_session:\n        :return:\n        \"\"\"\n        user_names = [(name or \"\").lower() for name in user_names]\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(sa.func.lower(cls.model.user_name).in_(user_names))\n        # q = q.options(sa.orm.eagerload(cls.groups))\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef user_names_like(cls, user_name, db_session=None):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(\n            sa.func.lower(cls.model.user_name).like((user_name or \"\").lower())\n        )\n        query = query.order_by(cls.model.user_name)\n        # q = q.options(sa.orm.eagerload('groups'))\n        return query", "response": "fetch users with similar names using LIKE clause"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching user object by email", "response": "def by_email(cls, email, db_session=None):\n        \"\"\"\n        fetch user object by email\n\n        :param email:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(\n            sa.func.lower(cls.model.email) == (email or \"\").lower()\n        )\n        query = query.options(sa.orm.eagerload(\"groups\"))\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning users hat have one of given permissions", "response": "def users_for_perms(cls, perm_names, db_session=None):\n        \"\"\"\n        return users hat have one of given permissions\n\n        :param perm_names:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(\n            cls.models_proxy.User.id == cls.models_proxy.UserGroup.user_id\n        )\n        query = query.filter(\n            cls.models_proxy.UserGroup.group_id\n            == cls.models_proxy.GroupPermission.group_id\n        )\n        query = query.filter(cls.models_proxy.GroupPermission.perm_name.in_(perm_names))\n\n        query2 = db_session.query(cls.model)\n        query2 = query2.filter(\n            cls.models_proxy.User.id == cls.models_proxy.UserPermission.user_id\n        )\n        query2 = query2.filter(\n            cls.models_proxy.UserPermission.perm_name.in_(perm_names)\n        )\n        users = query.union(query2).order_by(cls.model.id)\n        return users"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore join times for current nicknames when we first join.", "response": "def handle_joined(self, connection, event):\n        \"\"\"\n        Store join times for current nicknames when we first join.\n        \"\"\"\n        nicknames = [s.lstrip(\"@+\") for s in event.arguments()[-1].split()]\n        for nickname in nicknames:\n            self.joined[nickname] = datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore the join time for a nickname when it joins.", "response": "def handle_join(self, connection, event):\n        \"\"\"\n        Store join time for a nickname when it joins.\n        \"\"\"\n        nickname = self.get_nickname(event)\n        self.joined[nickname] = datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_quit(self, connection, event):\n        nickname = self.get_nickname(event)\n        self.quit[nickname] = datetime.now()\n        del self.joined[nickname]", "response": "Store quit time for a nickname when it quits."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a human friendly version of the timespan between now and the given datetime.", "response": "def timesince(self, when):\n        \"\"\"\n        Returns human friendly version of the timespan between now\n        and the given datetime.\n        \"\"\"\n        units = (\n            (\"year\",   60 * 60 * 24 * 365),\n            (\"week\",   60 * 60 * 24 * 7),\n            (\"day\",    60 * 60 * 24),\n            (\"hour\",   60 * 60),\n            (\"minute\", 60),\n            (\"second\", 1),\n        )\n        delta = datetime.now() - when\n        total_seconds = delta.days * 60 * 60 * 24 + delta.seconds\n        parts = []\n        for name, seconds in units:\n            value = total_seconds / seconds\n            if value > 0:\n                total_seconds %= seconds\n                s = \"s\" if value != 1 else \"\"\n                parts.append(\"%s %s%s\" % (value, name, s))\n        return \" and \".join(\", \".join(parts).rsplit(\", \", 1))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef commands(self, event):\n        commands = sorted(self.commands_dict().keys())\n        return \"Available commands: %s\" % \" \".join(commands)", "response": "Lists all available commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the help message for the bot. Takes an optional command name which will show the help message for that command.", "response": "def help(self, event, command_name=None):\n        \"\"\"\n        Shows the help message for the bot. Takes an optional command name\n        which when given, will show help for that command.\n        \"\"\"\n        if command_name is None:\n            return (\"Type !commands for a list of all commands. Type \"\n                    \"!help [command] to see help for a specific command.\")\n        try:\n            command = self.commands_dict()[command_name]\n        except KeyError:\n            return \"%s is not a command\" % command_name\n\n        argspec = getargspec(command)\n        args = argspec.args[2:]\n        defaults = argspec.defaults or []\n        for i in range(-1, -len(defaults) - 1, -1):\n            args[i] = \"%s [default: %s]\" % (args[i], defaults[i])\n        args = \", \".join(args)\n        help = getdoc(command).replace(\"\\n\", \" \")\n        return \"help for %s: (args: %s) %s\" % (command_name, args, help)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uptime(self, event, nickname=None):\n        if nickname and nickname != self.nickname:\n            try:\n                uptime = self.timesince(self.joined[nickname])\n            except KeyError:\n                return \"%s is not in the channel\" % nickname\n            else:\n                if nickname == self.get_nickname(event):\n                    prefix = \"you have\"\n                else:\n                    prefix = \"%s has\" % nickname\n                return \"%s been here for %s\" % (prefix, uptime)\n        uptime = self.timesince(self.joined[self.nickname])\n        return \"I've been here for %s\" % uptime", "response": "Shows the amount of time since the given nickname has beenhere in the channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing the amount of time since the given nickname was last seen in the channel.", "response": "def seen(self, event, nickname):\n        \"\"\"\n        Shows the amount of time since the given nickname was last\n        seen in the channel.\n        \"\"\"\n        try:\n            self.joined[nickname]\n        except KeyError:\n            pass\n        else:\n            if nickname == self.get_nickname(event):\n                prefix = \"you are\"\n            else:\n                prefix = \"%s is\" % nickname\n            return \"%s here right now\" % prefix\n        try:\n            seen = self.timesince(self.quit[nickname])\n        except KeyError:\n            return \"%s has never been seen\" % nickname\n        else:\n            return \"%s was last seen %s ago\" % (nickname,  seen)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the last login date of the user s account", "response": "def last_login_date(self):\n        \"\"\" Date of user's last login \"\"\"\n        return sa.Column(\n            sa.TIMESTAMP(timezone=False),\n            default=lambda x: datetime.utcnow(),\n            server_default=sa.func.now(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the date of the user s security code update", "response": "def security_code_date(self):\n        \"\"\" Date of user's security code update \"\"\"\n        return sa.Column(\n            sa.TIMESTAMP(timezone=False),\n            default=datetime(2000, 1, 1),\n            server_default=\"2000-01-01 01:01\",\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef groups_dynamic(self):\n        return sa.orm.relationship(\n            \"Group\",\n            secondary=\"users_groups\",\n            lazy=\"dynamic\",\n            passive_deletes=True,\n            passive_updates=True,\n        )", "response": "returns dynamic relationship for groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all resources owned by user", "response": "def resources(self):\n        \"\"\" Returns all resources directly owned by user, can be used to assign\n        ownership of new resources::\n\n            user.resources.append(resource) \"\"\"\n        return sa.orm.relationship(\n            \"Resource\",\n            cascade=\"all\",\n            passive_deletes=True,\n            passive_updates=True,\n            backref=\"owner\",\n            lazy=\"dynamic\",\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resources_dynamic(self):\n        return sa.orm.relationship(\n            \"Resource\",\n            cascade=\"all\",\n            passive_deletes=True,\n            passive_updates=True,\n            lazy=\"dynamic\",\n        )", "response": "Returns all resources owned by group"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates if group can get assigned with permission", "response": "def validate_permission(self, key, permission):\n        \"\"\" validates if group can get assigned with permission\"\"\"\n        if permission.perm_name not in self.__possible_permissions__:\n            raise AssertionError(\n                \"perm_name is not one of {}\".format(self.__possible_permissions__)\n            )\n        return permission"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the feeds and sends any items to the channel that have not yet been parsed.", "response": "def parse_feeds(self, message_channel=True):\n        \"\"\"\n        Iterates through each of the feed URLs, parses their items, and\n        sends any items to the channel that have not been previously\n        been parsed.\n        \"\"\"\n        if parse:\n            for feed_url in self.feeds:\n                feed = parse(feed_url)\n                for item in feed.entries:\n                    if item[\"id\"] not in self.feed_items:\n                        self.feed_items.add(item[\"id\"])\n                        if message_channel:\n                            message = self.format_item_message(feed, item)\n                            self.message_channel(message)\n                            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ChunkBy(self: dict, f=None):\n    if f is None:\n        return _chunk(self.items())\n    if is_to_destruct(f):\n        f = destruct_func(f)\n    return _chunk(self.items(), f)", "response": "Returns a list of items in a single chunk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GroupBy(self: dict, f=None):\n    if f and is_to_destruct(f):\n        f = destruct_func(f)\n    return _group_by(self.items(), f)", "response": "Groups the dictionary by a function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake the n most recent items from the dict.", "response": "def Take(self: dict, n):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3],\n            'n': 2,\n            'assert': lambda ret: list(ret)  == [1, 2]\n         }\n    ]\n    \"\"\"\n\n    for i, e in enumerate(self.items()):\n        if i == n:\n            break\n        yield e"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef TakeIf(self: dict, f):\n    if is_to_destruct(f):\n        f = destruct_func(f)\n\n    return (e for e in self.items() if f(e))", "response": "TakeIf - Returns a list of items from the dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef TakeWhile(self: dict, f):\n    if is_to_destruct(f):\n        f = destruct_func(f)\n\n    for e in self.items():\n        if not f(e):\n            break\n        yield e", "response": "Yields the items in the dictionary while f returns True."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Drop(self: dict, n):\n    n = len(self) - n\n    if n <= 0:\n        yield from self.items()\n    else:\n        for i, e in enumerate(self.items()):\n            if i == n:\n                break\n            yield e", "response": "Drop n items from the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nskip the n - th entry in the dict.", "response": "def Skip(self: dict, n):\n    \"\"\"\n        [\n            {\n                'self': [1, 2, 3, 4, 5],\n                'n': 3,\n                'assert': lambda ret: list(ret) == [4, 5]\n             }\n        ]\n        \"\"\"\n\n    con = self.items()\n    for i, _ in enumerate(con):\n        if i == n:\n            break\n    return con"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noutputs readings for specified number of rows to console", "response": "def print_meter_record(file_path, rows=5):\n    \"\"\" Output readings for specified number of rows to console \"\"\"\n    m = nr.read_nem_file(file_path)\n    print('Header:', m.header)\n    print('Transactions:', m.transactions)\n    for nmi in m.readings:\n        for channel in m.readings[nmi]:\n            print(nmi, 'Channel', channel)\n            for reading in m.readings[nmi][channel][-rows:]:\n                print('', reading)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all users that have permissions for this resource", "response": "def users(self):\n        \"\"\" returns all users that have permissions for this resource\"\"\"\n        return sa.orm.relationship(\n            \"User\",\n            secondary=\"users_resources_permissions\",\n            passive_deletes=True,\n            passive_updates=True,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_parent_deeper(\n        cls, parent_id=None, limit_depth=1000000, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        This returns you subtree of ordered objects relative\n        to the start parent_id (currently only implemented in postgresql)\n\n        :param resource_id:\n        :param limit_depth:\n        :param db_session:\n        :return:\n        \"\"\"\n\n        if parent_id:\n            limiting_clause = \"res.parent_id = :parent_id\"\n        else:\n            limiting_clause = \"res.parent_id is null\"\n        tablename = cls.model.__table__.name\n        raw_q = \"\"\"\n            WITH RECURSIVE subtree AS (\n                    SELECT res.*, 1 AS depth, LPAD(res.ordering::CHARACTER VARYING, 7, '0') AS sorting,\n                    res.resource_id::CHARACTER VARYING AS path\n                    FROM {tablename} AS res WHERE {limiting_clause}\n                  UNION ALL\n                    SELECT res_u.*, depth+1 AS depth,\n                    (st.sorting::CHARACTER VARYING || '/' || LPAD(res_u.ordering::CHARACTER VARYING, 7, '0') ) AS sorting,\n                    (st.path::CHARACTER VARYING || '/' || res_u.resource_id::CHARACTER VARYING ) AS path\n                    FROM {tablename} res_u, subtree st\n                    WHERE res_u.parent_id = st.resource_id\n            )\n            SELECT * FROM subtree WHERE depth<=:depth ORDER BY sorting;\n        \"\"\".format(\n            tablename=tablename, limiting_clause=limiting_clause\n        )  # noqa\n        db_session = get_db_session(db_session)\n        text_obj = sa.text(raw_q)\n        query = db_session.query(cls.model, \"depth\", \"sorting\", \"path\")\n        query = query.from_statement(text_obj)\n        query = query.params(parent_id=parent_id, depth=limit_depth)\n        return query", "response": "This method returns you subtree of ordered objects relative to the start parent_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a subtree of the resource tree.", "response": "def build_subtree_strut(self, result, *args, **kwargs):\n        \"\"\"\n        Returns a dictionary in form of\n        {node:Resource, children:{node_id: Resource}}\n\n        :param result:\n        :return:\n        \"\"\"\n        items = list(result)\n        root_elem = {\"node\": None, \"children\": OrderedDict()}\n        if len(items) == 0:\n            return root_elem\n        for _, node in enumerate(items):\n            new_elem = {\"node\": node.Resource, \"children\": OrderedDict()}\n            path = list(map(int, node.path.split(\"/\")))\n            parent_node = root_elem\n            normalized_path = path[:-1]\n            if normalized_path:\n                for path_part in normalized_path:\n                    parent_node = parent_node[\"children\"][path_part]\n            parent_node[\"children\"][new_elem[\"node\"].resource_id] = new_elem\n        return root_elem"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path_upper(\n        cls, object_id, limit_depth=1000000, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        This returns you path to root node starting from object_id\n            currently only for postgresql\n\n        :param object_id:\n        :param limit_depth:\n        :param db_session:\n        :return:\n        \"\"\"\n        tablename = cls.model.__table__.name\n        raw_q = \"\"\"\n            WITH RECURSIVE subtree AS (\n                    SELECT res.*, 1 as depth FROM {tablename} res\n                    WHERE res.resource_id = :resource_id\n                  UNION ALL\n                    SELECT res_u.*, depth+1 as depth\n                    FROM {tablename} res_u, subtree st\n                    WHERE res_u.resource_id = st.parent_id\n            )\n            SELECT * FROM subtree WHERE depth<=:depth;\n        \"\"\".format(\n            tablename=tablename\n        )\n        db_session = get_db_session(db_session)\n        q = (\n            db_session.query(cls.model)\n            .from_statement(sa.text(raw_q))\n            .params(resource_id=object_id, depth=limit_depth)\n        )\n        return q", "response": "This returns you path to root node starting from object_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving the node to a new location in the tree.", "response": "def move_to_position(\n        cls,\n        resource_id,\n        to_position,\n        new_parent_id=noop,\n        db_session=None,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Moves node to new location in the tree\n\n        :param resource_id: resource to move\n        :param to_position: new position\n        :param new_parent_id: new parent id\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        # lets lock rows to prevent bad tree states\n        resource = ResourceService.lock_resource_for_update(\n            resource_id=resource_id, db_session=db_session\n        )\n        ResourceService.lock_resource_for_update(\n            resource_id=resource.parent_id, db_session=db_session\n        )\n        same_branch = False\n\n        # reset if parent is same as old\n        if new_parent_id == resource.parent_id:\n            new_parent_id = noop\n\n        if new_parent_id is not noop:\n            cls.check_node_parent(resource_id, new_parent_id, db_session=db_session)\n        else:\n            same_branch = True\n\n        if new_parent_id is noop:\n            # it is not guaranteed that parent exists\n            parent_id = resource.parent_id if resource else None\n        else:\n            parent_id = new_parent_id\n\n        cls.check_node_position(\n            parent_id, to_position, on_same_branch=same_branch, db_session=db_session\n        )\n        # move on same branch\n        if new_parent_id is noop:\n            order_range = list(sorted((resource.ordering, to_position)))\n            move_down = resource.ordering > to_position\n\n            query = db_session.query(cls.model)\n            query = query.filter(cls.model.parent_id == parent_id)\n            query = query.filter(cls.model.ordering.between(*order_range))\n            if move_down:\n                query.update(\n                    {cls.model.ordering: cls.model.ordering + 1},\n                    synchronize_session=False,\n                )\n            else:\n                query.update(\n                    {cls.model.ordering: cls.model.ordering - 1},\n                    synchronize_session=False,\n                )\n            db_session.flush()\n            db_session.expire(resource)\n            resource.ordering = to_position\n        # move between branches\n        else:\n            cls.shift_ordering_down(\n                resource.parent_id, resource.ordering, db_session=db_session\n            )\n            cls.shift_ordering_up(new_parent_id, to_position, db_session=db_session)\n            db_session.expire(resource)\n            resource.parent_id = new_parent_id\n            resource.ordering = to_position\n            db_session.flush()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shift_ordering_up(cls, parent_id, position, db_session=None, *args, **kwargs):\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model)\n        query = query.filter(cls.model.parent_id == parent_id)\n        query = query.filter(cls.model.ordering >= position)\n        query.update(\n            {cls.model.ordering: cls.model.ordering + 1}, synchronize_session=False\n        )\n        db_session.flush()", "response": "Shifts the ordering to open a gap for node insertion"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the node position for new node in the tree.", "response": "def set_position(cls, resource_id, to_position, db_session=None, *args, **kwargs):\n        \"\"\"\n        Sets node position for new node in the tree\n\n        :param resource_id: resource to move\n        :param to_position: new position\n        :param db_session:\n        :return:def count_children(cls, resource_id, db_session=None):\n        \"\"\"\n        db_session = get_db_session(db_session)\n        # lets lock rows to prevent bad tree states\n        resource = ResourceService.lock_resource_for_update(\n            resource_id=resource_id, db_session=db_session\n        )\n        cls.check_node_position(\n            resource.parent_id, to_position, on_same_branch=True, db_session=db_session\n        )\n        cls.shift_ordering_up(resource.parent_id, to_position, db_session=db_session)\n        db_session.flush()\n        db_session.expire(resource)\n        resource.ordering = to_position\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the parent destination is valid for node.", "response": "def check_node_parent(\n        cls, resource_id, new_parent_id, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Checks if parent destination is valid for node\n\n        :param resource_id:\n        :param new_parent_id:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        new_parent = ResourceService.lock_resource_for_update(\n            resource_id=new_parent_id, db_session=db_session\n        )\n        # we are not moving to \"root\" so parent should be found\n        if not new_parent and new_parent_id is not None:\n            raise ZigguratResourceTreeMissingException(\"New parent node not found\")\n        else:\n            result = cls.path_upper(new_parent_id, db_session=db_session)\n            path_ids = [r.resource_id for r in result]\n            if resource_id in path_ids:\n                raise ZigguratResourceTreePathException(\n                    \"Trying to insert node into itself\"\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count_children(cls, resource_id, db_session=None, *args, **kwargs):\n        query = db_session.query(cls.model.resource_id)\n        query = query.filter(cls.model.parent_id == resource_id)\n        return query.count()", "response": "Count the number of children of a resource node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the position for given parent is valid raises exception if it is not the case", "response": "def check_node_position(\n        cls, parent_id, position, on_same_branch, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Checks if node position for given parent is valid, raises exception if\n        this is not the case\n\n        :param parent_id:\n        :param position:\n        :param on_same_branch: indicates that we are checking same branch\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        if not position or position < 1:\n            raise ZigguratResourceOutOfBoundaryException(\n                \"Position is lower than {}\", value=1\n            )\n        item_count = cls.count_children(parent_id, db_session=db_session)\n        max_value = item_count if on_same_branch else item_count + 1\n        if position > max_value:\n            raise ZigguratResourceOutOfBoundaryException(\n                \"Maximum resource ordering is {}\", value=max_value\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef egcd(b, n):\n    '''\n    Given two integers (b, n), returns (gcd(b, n), a, m) such that\n    a*b + n*m = gcd(b, n).\n    \n    Adapted from several sources:\n      https://brilliant.org/wiki/extended-euclidean-algorithm/\n      https://rosettacode.org/wiki/Modular_inverse\n      https://en.wikibooks.org/wiki/Algorithm_Implementation/Mathematics/Extended_Euclidean_algorithm\n      https://en.wikipedia.org/wiki/Euclidean_algorithm\n      \n    >>> egcd(1, 1)\n    (1, 0, 1)\n    >>> egcd(12, 8)\n    (4, 1, -1)\n    >>> egcd(23894798501898, 23948178468116)\n    (2, 2437250447493, -2431817869532)\n    >>> egcd(pow(2, 50), pow(3, 50))\n    (1, -260414429242905345185687, 408415383037561)\n\n    '''\n    (x0, x1, y0, y1) = (1, 0, 0, 1)\n    while n != 0:\n        (q, b, n) = (b // n, n, b % n)\n        (x0, x1) = (x1, x0 - q * x1)\n        (y0, y1) = (y1, y0 - q * y1)\n    return (b, x0, y0)", "response": "Given two integers b and n returns a tuple of the gcd of b and m."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister the needed transformations and supressions.", "response": "def register(linter):\n    \"\"\"Add the needed transformations and supressions.\n    \"\"\"\n\n    linter.register_checker(MongoEngineChecker(linter))\n    add_transform('mongoengine')\n    add_transform('mongomotor')\n    suppress_qs_decorator_messages(linter)\n    suppress_fields_attrs_messages(linter)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_as_csv(file_name, nmi=None, output_file=None):\n\n    m = read_nem_file(file_name)\n    if nmi is None:\n        nmi = list(m.readings.keys())[0]  # Use first NMI\n\n    channels = list(m.transactions[nmi].keys())\n    num_records = len(m.readings[nmi][channels[0]])\n    last_date = m.readings[nmi][channels[0]][-1].t_end\n    if output_file is None:\n        output_file = '{}_{}_transposed.csv'.format(\n            nmi, last_date.strftime('%Y%m%d'))\n    with open(output_file, 'w', newline='') as csvfile:\n        cwriter = csv.writer(\n            csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        heading_list = ['period_start', 'period_end']\n        for channel in channels:\n            heading_list.append(channel)\n        heading_list.append('quality_method')\n        cwriter.writerow(heading_list)\n\n        for i in range(0, num_records):\n            t_start = m.readings[nmi][channels[0]][i].t_start\n            t_end = m.readings[nmi][channels[0]][i].t_end\n            quality_method = m.readings[nmi][channels[0]][i].quality_method\n            row_list = [t_start, t_end]\n            for ch in channels:\n                val = m.readings[nmi][ch][i].read_value\n                row_list.append(val)\n            row_list.append(quality_method)\n            cwriter.writerow(row_list)\n    return output_file", "response": "Transpose all channels and output a csv that is easier to read and do charting on the NMI."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(cls, group_id, db_session=None):\n        db_session = get_db_session(db_session)\n        return db_session.query(cls.model).get(group_id)", "response": "Fetch a specific record from the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch group by group_name", "response": "def by_group_name(cls, group_name, db_session=None):\n        \"\"\"\n        fetch group by name\n\n        :param group_name:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session)\n        query = db_session.query(cls.model).filter(cls.model.group_name == group_name)\n        return query.first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_user_paginator(\n        cls,\n        instance,\n        page=1,\n        item_count=None,\n        items_per_page=50,\n        user_ids=None,\n        GET_params=None,\n    ):\n        \"\"\"\n        returns paginator over users belonging to the group\n\n        :param instance:\n        :param page:\n        :param item_count:\n        :param items_per_page:\n        :param user_ids:\n        :param GET_params:\n        :return:\n        \"\"\"\n        if not GET_params:\n            GET_params = {}\n        GET_params.pop(\"page\", None)\n        query = instance.users_dynamic\n        if user_ids:\n            query = query.filter(cls.models_proxy.UserGroup.user_id.in_(user_ids))\n        return SqlalchemyOrmPage(\n            query,\n            page=page,\n            item_count=item_count,\n            items_per_page=items_per_page,\n            **GET_params\n        )", "response": "returns paginator over users belonging to the group\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resources_with_possible_perms(\n        cls,\n        instance,\n        perm_names=None,\n        resource_ids=None,\n        resource_types=None,\n        db_session=None,\n    ):\n        \"\"\"\n        returns list of permissions and resources for this group,\n            resource_ids restricts the search to specific resources\n\n        :param instance:\n        :param perm_names:\n        :param resource_ids:\n        :param resource_types:\n        :param db_session:\n        :return:\n        \"\"\"\n        db_session = get_db_session(db_session, instance)\n\n        query = db_session.query(\n            cls.models_proxy.GroupResourcePermission.perm_name,\n            cls.models_proxy.Group,\n            cls.models_proxy.Resource,\n        )\n        query = query.filter(\n            cls.models_proxy.Resource.resource_id\n            == cls.models_proxy.GroupResourcePermission.resource_id\n        )\n        query = query.filter(\n            cls.models_proxy.Group.id\n            == cls.models_proxy.GroupResourcePermission.group_id\n        )\n        if resource_ids:\n            query = query.filter(\n                cls.models_proxy.GroupResourcePermission.resource_id.in_(resource_ids)\n            )\n\n        if resource_types:\n            query = query.filter(\n                cls.models_proxy.Resource.resource_type.in_(resource_types)\n            )\n\n        if perm_names not in ([ANY_PERMISSION], ANY_PERMISSION) and perm_names:\n            query = query.filter(\n                cls.models_proxy.GroupResourcePermission.perm_name.in_(perm_names)\n            )\n        query = query.filter(\n            cls.models_proxy.GroupResourcePermission.group_id == instance.id\n        )\n\n        perms = [\n            PermissionTuple(\n                None, row.perm_name, \"group\", instance, row.Resource, False, True\n            )\n            for row in query\n        ]\n        for resource in instance.resources:\n            perms.append(\n                PermissionTuple(\n                    None, ALL_PERMISSIONS, \"group\", instance, resource, True, True\n                )\n            )\n        return perms", "response": "returns list of permissions and resources for this group and resource_ids restricts the search to specific resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new pool", "response": "def create_pool(\n        database,\n        minsize=1,\n        maxsize=10,\n        echo=False,\n        loop=None,\n        **kwargs\n):\n    \"\"\"\n    \u521b\u5efa\u652f\u6301\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684pool\n    \"\"\"\n    coro = _create_pool(\n        database=database,\n        minsize=minsize,\n        maxsize=maxsize,\n        echo=echo,\n        loop=loop,\n        **kwargs\n    )\n    return _PoolContextManager(coro)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for all free resources to be closed.", "response": "def wait_closed(self):\n        \"\"\"\n        Wait for closing all pool's connections.\n        \"\"\"\n        if self._closed:\n            return\n        if not self._closing:\n            raise RuntimeError(\n                \".wait_closed() should be called \"\n                \"after .close()\"\n            )\n\n        while self._free:\n            conn = self._free.popleft()\n            if not conn.closed:\n                yield from conn.close()\n            else:\n                # pragma: no cover\n                pass\n        with (yield from self._cond):\n            while self.size > self.freesize:\n                yield from self._cond.wait()\n        self._used.clear()\n        self._closed = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fill_free_pool(self, override_min):\n        while self.size < self.minsize:\n            self._acquiring += 1\n            try:\n                conn = yield from connect(\n                    database=self._database,\n                    echo=self._echo,\n                    loop=self._loop,\n                    **self._conn_kwargs\n                )\n                self._free.append(conn)\n                self._cond.notify()\n            finally:\n                self._acquiring -= 1\n        if self._free:\n            return\n\n        if override_min and self.size < self.maxsize:\n            self._acquiring += 1\n            try:\n                conn = yield from connect(\n                    database=self._database,\n                    echo=self._echo,\n                    loop=self._loop,\n                    **self._conn_kwargs\n                )\n                self._free.append(conn)\n                self._cond.notify()\n            finally:\n                self._acquiring -= 1", "response": "Fill the free pool with new ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the function to the list of registered functions.", "response": "def add_function(self, function):\n        \"\"\"\n        Adds the function to the list of registered functions.\n        \"\"\"\n        function = self.build_function(function)\n        if function.name in self.functions:\n            raise FunctionAlreadyRegistered(function.name)\n        self.functions[function.name] = function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a function from the registry.", "response": "def get_one(self, context, name):\n        \"\"\"\n        Returns a function if it is registered, the context is ignored.\n        \"\"\"\n        try:\n            return self.functions[name]\n        except KeyError:\n            raise FunctionNotFound(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subfield_get(self, obj, type=None):\n    if obj is None:\n        return self\n    return obj.__dict__[self.field.name]", "response": "Returns the value of the subfield."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _preprocess_kwargs(self, initial_kwargs):\n        kwargs = initial_kwargs.copy()\n        generic_key_related_kwargs = self._get_generic_key_related_kwargs(initial_kwargs)\n        for key, value in generic_key_related_kwargs.items():\n            # delete old kwarg that was related to generic key\n            del kwargs[key]\n            try:\n                suffix = key.split('__')[1]\n            except IndexError:\n                suffix = None\n            # add new kwargs that related to object_id and content_type fields\n            new_kwargs = self._get_filter_object_id_and_content_type_filter_kwargs(value, suffix)\n            kwargs.update(new_kwargs)\n\n        return kwargs", "response": "Replace generic key related attribute with filters by object_id and content_type fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef categorize(\n    data, col_name: str = None, new_col_name: str = None,\n    categories: dict = None, max_categories: float = 0.15\n):\n    \"\"\"\n    :param data:\n    :param col_name:\n    :param new_col_name:\n    :param categories:\n    :param max_categories: max proportion threshold of categories\n    :return: new categories\n    :rtype dict:\n    \"\"\"\n\n    _categories = {}\n    if col_name is None:\n        if categories is not None:\n            raise Exception(\n                'col_name is None when categories was defined.'\n            )\n        # create a list of cols with all object columns\n        cols = [\n            k for k in data.keys()\n            if data[k].dtype == 'object' and\n            (data[k].unique() / data[k].count()) <= max_categories\n        ]\n    else:\n        # create a list with col_name\n        if new_col_name is not None:\n            data[new_col_name] = data[col_name]\n            col_name = new_col_name\n\n        cols = [col_name]\n\n    for c in cols:\n        if categories is not None:\n            # assert all keys is a number\n            assert all(type(k) in (int, float) for k in categories.keys())\n            # replace values using given categories dict\n            data[c].replace(categories, inplace=True)\n            # change column to categorical type\n            data[c] = data[c].astype('category')\n            # update categories information\n            _categories.update({c: categories})\n        else:\n            # change column to categorical type\n            data[c] = data[c].astype('category')\n            # change column to categorical type\n            _categories.update({\n                c: dict(enumerate(\n                    data[c].cat.categories,\n                ))\n            })\n    return _categories", "response": "Categorize the given data dict into a new list of categorical categories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dropna(data: pd.DataFrame, axis: int, **params):\n    if axis == 0:\n        dropna_rows(data=data, **params)\n    else:\n        dropna_columns(data=data, **params)", "response": "Remove columns with more NA values than threshold level\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves columns with more NA values than threshold level", "response": "def dropna_columns(data: pd.DataFrame, max_na_values: int=0.15):\n    \"\"\"\n    Remove columns with more NA values than threshold level\n\n    :param data:\n    :param max_na_values: proportion threshold of max na values\n    :return:\n\n    \"\"\"\n    size = data.shape[0]\n    df_na = (data.isnull().sum()/size) >= max_na_values\n    data.drop(df_na[df_na].index, axis=1, inplace=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves rows with more NA values than threshold level", "response": "def dropna_rows(data: pd.DataFrame, columns_name: str=None):\n    \"\"\"\n    Remove columns with more NA values than threshold level\n\n    :param data:\n    :param columns_name:\n    :return:\n\n    \"\"\"\n    params = {}\n    if columns_name is not None:\n        params.update({'subset': columns_name.split(',')})\n    data.dropna(inplace=True, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_columns_with_unique_values(\n    data: pd.DataFrame, max_unique_values: int = 0.25\n):\n    \"\"\"\n    Remove columns when the proportion\n    of the total of unique values is more than the max_unique_values\n    threshold, just for columns with type as object or category\n\n    :param data:\n    :param max_unique_values:\n    :return:\n\n    \"\"\"\n    size = data.shape[0]\n    df_uv = data.apply(\n        lambda se: (\n            (se.dropna().unique().shape[0]/size) > max_unique_values and\n            se.dtype in ['object', 'category']\n        )\n    )\n    data.drop(df_uv[df_uv].index, axis=1, inplace=True)", "response": "Remove columns when the proportion of the total of unique values is more than the max_unique_values\n    threshold just for columns with type as object or category."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list(self, request, *args, **kwargs):\n        return super(QuotaViewSet, self).list(request, *args, **kwargs)", "response": "Returns a list of objects that are in the quotas set and are visible to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retrieve(self, request, *args, **kwargs):\n        return super(QuotaViewSet, self).retrieve(request, *args, **kwargs)", "response": "This method is deprecated in favor of the base retrieve method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_representation(self, obj):\n        kwargs = None\n        for field in self.lookup_fields:\n            if hasattr(obj, field):\n                kwargs = {field: getattr(obj, field)}\n                break\n        if kwargs is None:\n            raise AttributeError('Related object does not have any of lookup_fields')\n        request = self._get_request()\n        return request.build_absolute_uri(reverse(self._get_url(obj), kwargs=kwargs))", "response": "Serializes any object to his url representation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestore model instance from its url.", "response": "def to_internal_value(self, data):\n        \"\"\"\n        Restores model instance from its url\n        \"\"\"\n        if not data:\n            return None\n        request = self._get_request()\n        user = request.user\n        try:\n            obj = core_utils.instance_from_url(data, user=user)\n            model = obj.__class__\n        except ValueError:\n            raise serializers.ValidationError(_('URL is invalid: %s.') % data)\n        except (Resolver404, AttributeError, MultipleObjectsReturned, ObjectDoesNotExist):\n            raise serializers.ValidationError(_(\"Can't restore object from url: %s\") % data)\n        if model not in self.related_models:\n            raise serializers.ValidationError(_('%s object does not support such relationship.') % six.text_type(obj))\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the start and end values are in the correct format.", "response": "def validate(self, data):\n        \"\"\"\n        Check that the start is before the end.\n        \"\"\"\n        if 'start' in data and 'end' in data and data['start'] >= data['end']:\n            raise serializers.ValidationError(_('End must occur after start.'))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_task(app_label, task_name):\n\n    def delay(*args, **kwargs):\n        full_task_name = 'waldur_core.%s.%s' % (app_label, task_name)\n        send_celery_task(full_task_name, args, kwargs, countdown=2)\n\n    return delay", "response": "A helper function to deal with waldur_core high - level tasks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds description to celery log output", "response": "def log_celery_task(request):\n    \"\"\" Add description to celery log output \"\"\"\n    task = request.task\n    description = None\n    if isinstance(task, Task):\n        try:\n            description = task.get_description(*request.args, **request.kwargs)\n        except NotImplementedError:\n            pass\n        except Exception as e:\n            # Logging should never break workflow.\n            logger.exception('Cannot get description for task %s. Error: %s' % (task.__class__.__name__, e))\n\n    return '{0.name}[{0.id}]{1}{2}{3}'.format(\n        request,\n        ' {0}'.format(description) if description else '',\n        ' eta:[{0}]'.format(request.eta) if request.eta else '',\n        ' expires:[{0}]'.format(request.expires) if request.expires else '',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, serialized_instance, *args, **kwargs):\n        try:\n            instance = utils.deserialize_instance(serialized_instance)\n        except ObjectDoesNotExist:\n            message = ('Cannot restore instance from serialized object %s. Probably it was deleted.' %\n                       serialized_instance)\n            six.reraise(ObjectDoesNotExist, message)\n\n        self.args = args\n        self.kwargs = kwargs\n\n        self.pre_execute(instance)\n        result = self.execute(instance, *self.args, **self.kwargs)\n        self.post_execute(instance)\n        if result and isinstance(result, django_models.Model):\n            result = utils.serialize_instance(result)\n        return result", "response": "Deserialize input data and execute backend operation execution"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_previous_task_processing(self, *args, **kwargs):\n        app = self._get_app()\n        inspect = app.control.inspect()\n        active = inspect.active() or {}\n        scheduled = inspect.scheduled() or {}\n        reserved = inspect.reserved() or {}\n        uncompleted = sum(list(active.values()) + list(scheduled.values()) + reserved.values(), [])\n        return any(self.is_equal(task, *args, **kwargs) for task in uncompleted)", "response": "Return True if exist task that is equal to current and is uncompleted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing not run background task if previous task is uncompleted", "response": "def apply_async(self, args=None, kwargs=None, **options):\n        \"\"\" Do not run background task if previous task is uncompleted \"\"\"\n        if self.is_previous_task_processing(*args, **kwargs):\n            message = 'Background task %s was not scheduled, because its predecessor is not completed yet.' % self.name\n            logger.info(message)\n            # It is expected by Celery that apply_async return AsyncResult, otherwise celerybeat dies\n            return self.AsyncResult(options.get('task_id') or str(uuid4()))\n        return super(BackgroundTask, self).apply_async(args=args, kwargs=kwargs, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_cache_key(self, args, kwargs):\n        hash_input = json.dumps({'name': self.name, 'args': args, 'kwargs': kwargs}, sort_keys=True)\n        # md5 is used for internal caching, not need to care about security\n        return hashlib.md5(hash_input).hexdigest()", "response": "Returns the key used in cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the task must be skipped and decreases the counter in that case.", "response": "def apply_async(self, args=None, kwargs=None, **options):\n        \"\"\"\n        Checks whether task must be skipped and decreases the counter in that case.\n        \"\"\"\n        key = self._get_cache_key(args, kwargs)\n        counter, penalty = cache.get(key, (0, 0))\n        if not counter:\n            return super(PenalizedBackgroundTask, self).apply_async(args=args, kwargs=kwargs, **options)\n\n        cache.set(key, (counter - 1, penalty), self.CACHE_LIFETIME)\n        logger.info('The task %s will not be executed due to the penalty.' % self.name)\n        return self.AsyncResult(options.get('task_id') or str(uuid4()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nincrease penalty for the task and reset the penalty.", "response": "def on_failure(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"\n        Increases penalty for the task and resets the counter.\n        \"\"\"\n        key = self._get_cache_key(args, kwargs)\n        _, penalty = cache.get(key, (0, 0))\n        if penalty < self.MAX_PENALTY:\n            penalty += 1\n\n        logger.debug('The task %s is penalized and will be executed on %d run.' % (self.name, penalty))\n        cache.set(key, (penalty, penalty), self.CACHE_LIFETIME)\n        return super(PenalizedBackgroundTask, self).on_failure(exc, task_id, args, kwargs, einfo)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_success(self, retval, task_id, args, kwargs):\n        key = self._get_cache_key(args, kwargs)\n        if cache.get(key) is not None:\n            cache.delete(key)\n            logger.debug('Penalty for the task %s has been removed.' % self.name)\n\n        return super(PenalizedBackgroundTask, self).on_success(retval, task_id, args, kwargs)", "response": "Remove the cache entry for the task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log_backend_action(action=None):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapped(self, instance, *args, **kwargs):\n            action_name = func.func_name.replace('_', ' ') if action is None else action\n\n            logger.debug('About to %s `%s` (PK: %s).', action_name, instance, instance.pk)\n            result = func(self, instance, *args, **kwargs)\n            logger.debug('Action `%s` was executed successfully for `%s` (PK: %s).',\n                         action_name, instance, instance.pk)\n            return result\n        return wrapped\n    return decorator", "response": "Decorator to log backend action."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_services(cls, request=None):\n        return {service['name']: reverse(service['list_view'], request=request)\n                for service in cls._registry.values()}", "response": "Get a list of services endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_resources(cls, request=None):\n        return {'.'.join([service['name'], resource['name']]): reverse(resource['list_view'], request=request)\n                for service in cls._registry.values()\n                for resource in service['resources'].values()}", "response": "Get a list of resources endpoints."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of services and resources endpoints.", "response": "def get_services_with_resources(cls, request=None):\n        \"\"\" Get a list of services and resources endpoints.\n            {\n                ...\n                \"GitLab\": {\n                    \"url\": \"/api/gitlab/\",\n                    \"service_project_link_url\": \"/api/gitlab-service-project-link/\",\n                    \"resources\": {\n                        \"Project\": \"/api/gitlab-projects/\",\n                        \"Group\": \"/api/gitlab-groups/\"\n                    }\n                },\n                ...\n            }\n        \"\"\"\n        from django.apps import apps\n\n        data = {}\n        for service in cls._registry.values():\n            service_model = apps.get_model(service['model_name'])\n            service_project_link = service_model.projects.through\n            service_project_link_url = reverse(cls.get_list_view_for_model(service_project_link), request=request)\n\n            data[service['name']] = {\n                'url': reverse(service['list_view'], request=request),\n                'service_project_link_url': service_project_link_url,\n                'resources': {resource['name']: reverse(resource['list_view'], request=request)\n                              for resource in service['resources'].values()},\n                'properties': {resource['name']: reverse(resource['list_view'], request=request)\n                               for resource in service.get('properties', {}).values()},\n                'is_public_service': cls.is_public_service(service_model)\n            }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of service models.", "response": "def get_service_models(cls):\n        \"\"\" Get a list of service models.\n            {\n                ...\n                'gitlab': {\n                    \"service\": nodeconductor_gitlab.models.GitLabService,\n                    \"service_project_link\": nodeconductor_gitlab.models.GitLabServiceProjectLink,\n                    \"resources\": [\n                        nodeconductor_gitlab.models.Group,\n                        nodeconductor_gitlab.models.Project\n                    ],\n                },\n                ...\n            }\n\n        \"\"\"\n        from django.apps import apps\n\n        data = {}\n        for key, service in cls._registry.items():\n            service_model = apps.get_model(service['model_name'])\n            service_project_link = service_model.projects.through\n            data[key] = {\n                'service': service_model,\n                'service_project_link': service_project_link,\n                'resources': [apps.get_model(r) for r in service['resources'].keys()],\n                'properties': [apps.get_model(r) for r in service['properties'].keys() if '.' in r],\n            }\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of resource models.", "response": "def get_resource_models(cls):\n        \"\"\" Get a list of resource models.\n            {\n                'DigitalOcean.Droplet': waldur_digitalocean.models.Droplet,\n                'JIRA.Project': waldur_jira.models.Project,\n                'OpenStack.Tenant': waldur_openstack.models.Tenant\n            }\n\n        \"\"\"\n        from django.apps import apps\n\n        return {'.'.join([service['name'], attrs['name']]): apps.get_model(resource)\n                for service in cls._registry.values()\n                for resource, attrs in service['resources'].items()}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets resource models by service model", "response": "def get_service_resources(cls, model):\n        \"\"\" Get resource models by service model \"\"\"\n        key = cls.get_model_key(model)\n        return cls.get_service_name_resources(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_service_name_resources(cls, service_name):\n        from django.apps import apps\n\n        resources = cls._registry[service_name]['resources'].keys()\n        return [apps.get_model(resource) for resource in resources]", "response": "Get resource models by service name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_name_for_model(cls, model):\n        key = cls.get_model_key(model)\n        model_str = cls._get_model_str(model)\n        service = cls._registry[key]\n        if model_str in service['resources']:\n            return '{}.{}'.format(service['name'], service['resources'][model_str]['name'])\n        else:\n            return service['name']", "response": "Get a name for given class or model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a dictionary with related structure models for given class or model", "response": "def get_related_models(cls, model):\n        \"\"\" Get a dictionary with related structure models for given class or model:\n\n            >> SupportedServices.get_related_models(gitlab_models.Project)\n            {\n                'service': nodeconductor_gitlab.models.GitLabService,\n                'service_project_link': nodeconductor_gitlab.models.GitLabServiceProjectLink,\n                'resources': [\n                    nodeconductor_gitlab.models.Group,\n                    nodeconductor_gitlab.models.Project,\n                ]\n            }\n        \"\"\"\n        from waldur_core.structure.models import ServiceSettings\n\n        if isinstance(model, ServiceSettings):\n            model_str = cls._registry.get(model.type, {}).get('model_name', '')\n        else:\n            model_str = cls._get_model_str(model)\n\n        for models in cls.get_service_models().values():\n            if model_str == cls._get_model_str(models['service']) or \\\n               model_str == cls._get_model_str(models['service_project_link']):\n                return models\n\n            for resource_model in models['resources']:\n                if model_str == cls._get_model_str(resource_model):\n                    return models"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if model app name is in list of INSTALLED_APPS", "response": "def _is_active_model(cls, model):\n        \"\"\" Check is model app name is in list of INSTALLED_APPS \"\"\"\n        # We need to use such tricky way to check because of inconsistent apps names:\n        # some apps are included in format \"<module_name>.<app_name>\" like \"waldur_core.openstack\"\n        # other apps are included in format \"<app_name>\" like \"nodecondcutor_sugarcrm\"\n        return ('.'.join(model.__module__.split('.')[:2]) in settings.INSTALLED_APPS or\n                '.'.join(model.__module__.split('.')[:1]) in settings.INSTALLED_APPS)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process(self, event):\n\n        conf = settings.WALDUR_CORE.get('GOOGLE_API') or {}\n        keys = conf.get(dict(self.Type.CHOICES)[self.type])\n\n        if not keys or not self.token:\n            return\n\n        endpoint = 'https://gcm-http.googleapis.com/gcm/send'\n        headers = {\n            'Content-Type': 'application/json',\n            'Authorization': 'key=%s' % keys['server_key'],\n        }\n        payload = {\n            'to': self.token,\n            'notification': {\n                'body': event.get('message', 'New event'),\n                'title': conf.get('NOTIFICATION_TITLE', 'Waldur notification'),\n                'image': 'icon',\n            },\n            'data': {\n                'event': event\n            },\n        }\n        if self.type == self.Type.IOS:\n            payload['content-available'] = '1'\n        logger.debug('Submitting GCM push notification with headers %s, payload: %s' % (headers, payload))\n        requests.post(endpoint, json=payload, headers=headers)", "response": "Send events as push notification via Google Cloud Messaging API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_context_data_from_headers(request, headers_schema):\n\n    if not headers_schema:\n        return None\n\n    env = request.parsed_data.xml.xpath(\n        '/soap:Envelope', namespaces=SoapProtocol.namespaces)[0]\n\n    header = env.xpath(\n        './soap:Header/*', namespaces=SoapProtocol.namespaces)\n    if len(header) < 1:\n        return None\n\n    return headers_schema.validate(xml2obj(header[0], headers_schema))", "response": "Extracts context data from request headers according to specified schema."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lru_cache(maxsize=128, key_fn=None):\n\n    def decorator(fn):\n        cache = LRUCache(maxsize)\n        argspec = inspect2.getfullargspec(fn)\n        arg_names = argspec.args[1:] + argspec.kwonlyargs  # remove self\n        kwargs_defaults = get_kwargs_defaults(argspec)\n\n        cache_key = key_fn\n        if cache_key is None:\n\n            def cache_key(args, kwargs):\n                return get_args_tuple(args, kwargs, arg_names, kwargs_defaults)\n\n        @functools.wraps(fn)\n        def wrapper(*args, **kwargs):\n            key = cache_key(args, kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                value = fn(*args, **kwargs)\n                cache[key] = value\n                return value\n\n        wrapper.clear = cache.clear\n\n        return wrapper\n\n    return decorator", "response": "A decorator that adds an LRU cache of size maxsize to a function that returns the value of the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cached_per_instance():\n\n    def cache_fun(fun):\n        argspec = inspect2.getfullargspec(fun)\n        arg_names = argspec.args[1:] + argspec.kwonlyargs  # remove self\n        kwargs_defaults = get_kwargs_defaults(argspec)\n        cache = {}\n\n        def cache_key(args, kwargs):\n            return get_args_tuple(args, kwargs, arg_names, kwargs_defaults)\n\n        def clear_cache(instance_key, ref):\n            del cache[instance_key]\n\n        @functools.wraps(fun)\n        def new_fun(self, *args, **kwargs):\n            instance_key = id(self)\n            if instance_key not in cache:\n                ref = weakref.ref(self, functools.partial(clear_cache, instance_key))\n                cache[instance_key] = (ref, {})\n            instance_cache = cache[instance_key][1]\n\n            k = cache_key(args, kwargs)\n            if k not in instance_cache:\n                instance_cache[k] = fun(self, *args, **kwargs)\n            return instance_cache[k]\n\n        # just so unit tests can check that this is cleaned up correctly\n        new_fun.__cached_per_instance_cache__ = cache\n        return new_fun\n\n    return cache_fun", "response": "Decorator that adds caching to an instance method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a cache key from the passed in arguments.", "response": "def get_args_tuple(args, kwargs, arg_names, kwargs_defaults):\n    \"\"\"Generates a cache key from the passed in arguments.\"\"\"\n    args_list = list(args)\n    args_len = len(args)\n    all_args_len = len(arg_names)\n    try:\n        while args_len < all_args_len:\n            arg_name = arg_names[args_len]\n            if arg_name in kwargs_defaults:\n                args_list.append(kwargs.get(arg_name, kwargs_defaults[arg_name]))\n            else:\n                args_list.append(kwargs[arg_name])\n            args_len += 1\n    except KeyError as e:\n        raise TypeError(\"Missing argument %r\" % (e.args[0],))\n    return tuple(args_list)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_kwargs_defaults(argspec):\n    arg_names = tuple(argspec.args)\n    defaults = argspec.defaults or ()\n    num_args = len(argspec.args) - len(defaults)\n    kwargs_defaults = {}\n    for i, default_value in enumerate(defaults):\n        kwargs_defaults[arg_names[num_args + i]] = default_value\n    if getattr(argspec, \"kwonlydefaults\", None):\n        kwargs_defaults.update(argspec.kwonlydefaults)\n    return kwargs_defaults", "response": "Computes a kwargs_defaults dictionary for use by get_args_tuple given an argspec."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef memoize(fun):\n    argspec = inspect2.getfullargspec(fun)\n    arg_names = argspec.args + argspec.kwonlyargs\n    kwargs_defaults = get_kwargs_defaults(argspec)\n\n    def cache_key(args, kwargs):\n        return get_args_tuple(args, kwargs, arg_names, kwargs_defaults)\n\n    @functools.wraps(fun)\n    def new_fun(*args, **kwargs):\n        k = cache_key(args, kwargs)\n        if k not in new_fun.__cache:\n            new_fun.__cache[k] = fun(*args, **kwargs)\n        return new_fun.__cache[k]\n\n    def clear_cache():\n        \"\"\"Removes all cached values for this function.\"\"\"\n        new_fun.__cache.clear()\n\n    new_fun.__cache = {}\n    new_fun.clear_cache = clear_cache\n    return new_fun", "response": "Memoizes return values of the decorated function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef memoize_with_ttl(ttl_secs=60 * 60 * 24):\n\n    error_msg = (\n        \"Incorrect usage of qcore.caching.memoize_with_ttl: \"\n        \"ttl_secs must be a positive integer.\"\n    )\n    assert_is_instance(ttl_secs, six.integer_types, error_msg)\n    assert_gt(ttl_secs, 0, error_msg)\n\n    def cache_fun(fun):\n        argspec = inspect2.getfullargspec(fun)\n        arg_names = argspec.args + argspec.kwonlyargs\n        kwargs_defaults = get_kwargs_defaults(argspec)\n\n        def cache_key(args, kwargs):\n            return repr(get_args_tuple(args, kwargs, arg_names, kwargs_defaults))\n\n        @functools.wraps(fun)\n        def new_fun(*args, **kwargs):\n            k = cache_key(args, kwargs)\n            current_time = int(time.time())\n\n            # k is not in the cache; perform the function and cache the result.\n            if k not in new_fun.__cache or k not in new_fun.__cache_times:\n                new_fun.__cache[k] = fun(*args, **kwargs)\n                new_fun.__cache_times[k] = current_time\n                return new_fun.__cache[k]\n\n            # k is in the cache at this point. Check if the ttl has expired;\n            # if so, recompute the value and cache it.\n            cache_time = new_fun.__cache_times[k]\n            if current_time - cache_time > ttl_secs:\n                new_fun.__cache[k] = fun(*args, **kwargs)\n                new_fun.__cache_times[k] = current_time\n\n            # finally, return the cached result.\n            return new_fun.__cache[k]\n\n        def clear_cache():\n            \"\"\"Removes all cached values for this function.\"\"\"\n            new_fun.__cache.clear()\n            new_fun.__cache_times.clear()\n\n        def dirty(*args, **kwargs):\n            \"\"\"Dirties the function for a given set of arguments.\"\"\"\n            k = cache_key(args, kwargs)\n            new_fun.__cache.pop(k, None)\n            new_fun.__cache_times.pop(k, None)\n\n        new_fun.__cache = {}\n        new_fun.__cache_times = {}\n        new_fun.clear_cache = clear_cache\n        new_fun.dirty = dirty\n        return new_fun\n\n    return cache_fun", "response": "Memoizes return values of a decorated function for a given time - to - live."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value of the constant.", "response": "def get_value(self):\n        \"\"\"Returns the value of the constant.\"\"\"\n        if self.value is not_computed:\n            self.value = self.value_provider()\n            if self.value is not_computed:\n                return None\n        return self.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the value. Does not look at the cache.", "response": "def compute(self):\n        \"\"\"Computes the value. Does not look at the cache.\"\"\"\n        self.value = self.value_provider()\n        if self.value is not_computed:\n            return None\n        else:\n            return self.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, key, default=miss):\n        if key not in self._dict:\n            return default\n\n        # invokes __getitem__, which updates the item\n        return self[key]", "response": "Return the value for given key if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nempty the cache and optionally invoke item_evicted callback.", "response": "def clear(self, omit_item_evicted=False):\n        \"\"\"Empty the cache and optionally invoke item_evicted callback.\"\"\"\n        if not omit_item_evicted:\n            items = self._dict.items()\n            for key, value in items:\n                self._evict_item(key, value)\n        self._dict.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_task(coro, loop):\n    # pragma: no cover\n    \"\"\"Compatibility wrapper for the loop.create_task() call introduced in\n    3.4.2.\"\"\"\n    if hasattr(loop, 'create_task'):\n        return loop.create_task(coro)\n    return asyncio.Task(coro, loop=loop)", "response": "Compatibility wrapper for asyncio. create_task"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of objects that can be used to search objects available to user.", "response": "def get_permitted_objects_uuids(cls, user):\n        \"\"\"\n        Return query dictionary to search objects available to user.\n        \"\"\"\n        uuids = filter_queryset_for_user(cls.objects.all(), user).values_list('uuid', flat=True)\n        key = core_utils.camel_case_to_underscore(cls.__name__) + '_uuid'\n        return {key: uuids}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether user has role in entity.", "response": "def has_user(self, user, role=None, timestamp=False):\n        \"\"\"\n        Checks whether user has role in entity.\n        `timestamp` can have following values:\n            - False - check whether user has role in entity at the moment.\n            - None - check whether user has permanent role in entity.\n            - Datetime object - check whether user will have role in entity at specific timestamp.\n        \"\"\"\n        permissions = self.permissions.filter(user=user, is_active=True)\n\n        if role is not None:\n            permissions = permissions.filter(role=role)\n\n        if timestamp is None:\n            permissions = permissions.filter(expiration_time=None)\n        elif timestamp:\n            permissions = permissions.filter(Q(expiration_time=None) | Q(expiration_time__gte=timestamp))\n\n        return permissions.exists()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nidentifies the contents of buf and return it.", "response": "def from_buffer(self, buf):\n        \"\"\"\n        Identify the contents of `buf`\n        \"\"\"\n        with self.lock:\n            try:\n                # if we're on python3, convert buf to bytes\n                # otherwise this string is passed as wchar*\n                # which is not what libmagic expects\n                if isinstance(buf, str) and str != bytes:\n                    buf = buf.encode('utf-8', errors='replace')\n                return maybe_decode(magic_buffer(self.cookie, buf))\n            except MagicException as e:\n                return self._handle509Bug(e)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self, timeout=None, root_object=None):\n\n        if self._is_running:\n            return\n\n        if timeout:\n            self._timeout = timeout\n            self._start_time = int(time())\n\n        pushcenter_logger.debug(\"[NURESTPushCenter] Starting push center on url %s ...\" % self.url)\n        self._is_running = True\n        self.__root_object = root_object\n\n        from .nurest_session import NURESTSession\n        current_session = NURESTSession.get_current_session()\n        args_session = {'session': current_session}\n\n        self._thread = StoppableThread(target=self._listen, name='push-center', kwargs=args_session)\n        self._thread.daemon = True\n        self._thread.start()", "response": "Starts listening to events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop listening for events.", "response": "def stop(self):\n        \"\"\" Stops listening for events. \"\"\"\n\n        if not self._is_running:\n            return\n\n        pushcenter_logger.debug(\"[NURESTPushCenter] Stopping...\")\n\n        self._thread.stop()\n        self._thread.join()\n\n        self._is_running = False\n        self._current_connection = None\n        self._start_time = None\n        self._timeout = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait_until_exit(self):\n\n        if self._timeout is None:\n            raise Exception(\"Thread will never exit. Use stop or specify timeout when starting it!\")\n\n        self._thread.join()\n        self.stop()", "response": "Wait until thread exits"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by the event loop when a new event is received from the connection.", "response": "def _did_receive_event(self, connection):\n        \"\"\" Receive an event from connection \"\"\"\n\n        if not self._is_running:\n            return\n\n        if connection.has_timeouted:\n            return\n\n        response = connection.response\n        data = None\n\n        if response.status_code != 200:\n            pushcenter_logger.error(\"[NURESTPushCenter]: Connection failure [%s] %s\" % (response.status_code, response.errors))\n\n        else:\n            data = response.data\n\n            if len(self._delegate_methods) > 0:\n                for m in self._delegate_methods:\n                    try:\n                        m(data)\n                    except Exception as exc:\n                        pushcenter_logger.error(\"[NURESTPushCenter] Delegate method %s failed:\\n%s\" % (m, exc))\n            elif data:\n                events = data['events']\n                self.nb_events_received += len(events)\n                self.nb_push_received += 1\n\n                pushcenter_logger.info(\"[NURESTPushCenter] Received Push #%s (total=%s, latest=%s)\\n%s\" % (self.nb_push_received, self.nb_events_received, len(events), json.dumps(events, indent=4)))\n                self._last_events.extend(events)\n\n        if self._is_running:\n            uuid = None\n            if data and 'uuid' in data:\n                uuid = data['uuid']\n\n            self._listen(uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _listen(self, uuid=None, session=None):\n\n        if self.url is None:\n            raise Exception(\"NURESTPushCenter needs to have a valid URL. please use setURL: before starting it.\")\n\n        events_url = \"%s/events\" % self.url\n        if uuid:\n            events_url = \"%s?uuid=%s\" % (events_url, uuid)\n\n        request = NURESTRequest(method='GET', url=events_url)\n\n        # Force async to False so the push center will have only 1 thread running\n        connection = NURESTConnection(request=request, async=True, callback=self._did_receive_event, root_object=self._root_object)\n\n        if self._timeout:\n            if int(time()) - self._start_time >= self._timeout:\n                pushcenter_logger.debug(\"[NURESTPushCenter] Timeout (timeout=%ss).\" % self._timeout)\n                return\n\n            else:\n                connection.timeout = self._timeout\n\n        pushcenter_logger.info('Bambou Sending >>>>>>\\n%s %s' % (request.method, request.url))\n\n        # connection.ignore_request_idle = True\n        connection.start()", "response": "Listen a connection uuid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_delegate(self, callback):\n\n        if callback in self._delegate_methods:\n            return\n\n        self._delegate_methods.append(callback)", "response": "Registers a new delegate callback"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_delegate(self, callback):\n\n        if callback not in self._delegate_methods:\n            return\n\n        self._delegate_methods.remove(callback)", "response": "Unregisters a registered delegate function or a method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_config(cls):\n\n        cls._config_parser = configparser.ConfigParser()\n        cls._config_parser.read(cls._default_attribute_values_configuration_file_path)", "response": "Reads the configuration file if any\n        is True."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_attribute_value(cls, object_class, property_name, attr_type=str):\n\n        if not cls._default_attribute_values_configuration_file_path:\n            return None\n\n        if not cls._config_parser:\n            cls._read_config()\n\n        class_name = object_class.__name__\n\n        if not cls._config_parser.has_section(class_name):\n            return None\n\n        if not cls._config_parser.has_option(class_name, property_name):\n            return None\n\n        if sys.version_info < (3,):\n            integer_types = (int, long,)\n        else:\n            integer_types = (int,)\n\n        if isinstance(attr_type, integer_types):\n            return cls._config_parser.getint(class_name, property_name)\n        elif attr_type is bool:\n            return cls._config_parser.getboolean(class_name, property_name)\n        else:\n            return cls._config_parser.get(class_name, property_name)", "response": "Gets the default value of a given property for a given object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter each resource separately using its own filter", "response": "def filter(self, request, queryset, view):\n        \"\"\" Filter each resource separately using its own filter \"\"\"\n        summary_queryset = queryset\n        filtered_querysets = []\n        for queryset in summary_queryset.querysets:\n            filter_class = self._get_filter(queryset)\n            queryset = filter_class(request.query_params, queryset=queryset).qs\n            filtered_querysets.append(queryset)\n\n        summary_queryset.querysets = filtered_querysets\n        return summary_queryset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef TypeFactory(type_):\n    if isinstance(type_, type) and issubclass(type_, Type):\n        return type_\n    for x in __types__:\n        if x.represents(type_):\n            return x.get(type_)\n    raise UnknownType(type_)", "response": "This function creates a standard form type from a simplified form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dummy_image(filetype='gif'):\n    # 1x1px Transparent GIF\n    GIF = 'R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7'\n    tmp_file = tempfile.NamedTemporaryFile(suffix='.%s' % filetype)\n    tmp_file.write(base64.b64decode(GIF))\n    return open(tmp_file.name, 'rb')", "response": "Generate empty image in temporary file for testing"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef utime_delta(days=0, hours=0, minutes=0, seconds=0):\n    return (days * DAY) + (hours * HOUR) + (minutes * MINUTE) + (seconds * SECOND)", "response": "Gets time delta in microseconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_with_timeout(\n    fn,\n    args=None,\n    kwargs=None,\n    timeout=None,\n    fail_if_no_timer=True,\n    signal_type=_default_signal_type,\n    timer_type=_default_timer_type,\n    timeout_exception_cls=TimeoutError,\n):\n    \"\"\"\n    Executes specified function with timeout. Uses SIGALRM to interrupt it.\n\n    :type fn: function\n    :param fn: function to execute\n\n    :type args: tuple\n    :param args: function args\n\n    :type kwargs: dict\n    :param kwargs: function kwargs\n\n    :type timeout: float\n    :param timeout: timeout, seconds; 0 or None means no timeout\n\n    :type fail_if_no_timer: bool\n    :param fail_if_no_timer: fail, if timer is nor available; normally it's available only in the\n    main thread\n\n    :type signal_type: signalnum\n    :param signal_type: type of signal to use (see signal module)\n\n    :type timer_type: signal.ITIMER_REAL, signal.ITIMER_VIRTUAL or signal.ITIMER_PROF\n    :param timer_type: type of timer to use (see signal module)\n\n    :type timeout_exception_cls: class\n    :param timeout_exception_cls: exception to throw in case of timeout\n\n    :return: fn call result.\n\n    \"\"\"\n    if args is None:\n        args = empty_tuple\n    if kwargs is None:\n        kwargs = empty_dict\n\n    if timeout is None or timeout == 0 or signal_type is None or timer_type is None:\n        return fn(*args, **kwargs)\n\n    def signal_handler(signum, frame):\n        raise timeout_exception_cls(inspection.get_function_call_str(fn, args, kwargs))\n\n    old_signal_handler = none\n    timer_is_set = False\n    try:\n        try:\n            old_signal_handler = signal.signal(signal_type, signal_handler)\n            signal.setitimer(timer_type, timeout)\n            timer_is_set = True\n        except ValueError:\n            if fail_if_no_timer:\n                raise NotSupportedError(\n                    \"Timer is not available; the code is probably invoked from outside the main \"\n                    \"thread.\"\n                )\n        return fn(*args, **kwargs)\n    finally:\n        if timer_is_set:\n            signal.setitimer(timer_type, 0)\n        if old_signal_handler is not none:\n            signal.signal(signal_type, old_signal_handler)", "response": "Executes specified function with timeout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_original_fn(fn):\n\n    fn_type = type(fn)\n    if fn_type is classmethod or fn_type is staticmethod:\n        return get_original_fn(fn.__func__)\n    if hasattr(fn, \"original_fn\"):\n        return fn.original_fn\n    if hasattr(fn, \"fn\"):\n        fn.original_fn = get_original_fn(fn.fn)\n        return fn.original_fn\n    return fn", "response": "Gets the very original function of a decorated one."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget full class or function name.", "response": "def get_full_name(src):\n    \"\"\"Gets full class or function name.\"\"\"\n\n    if hasattr(src, \"_full_name_\"):\n        return src._full_name_\n    if hasattr(src, \"is_decorator\"):\n        # Our own decorator or binder\n        if hasattr(src, \"decorator\"):\n            # Our own binder\n            _full_name_ = str(src.decorator)\n            # It's a short-living object, so we don't cache result\n        else:\n            # Our own decorator\n            _full_name_ = str(src)\n            try:\n                src._full_name_ = _full_name_\n            except AttributeError:\n                pass\n            except TypeError:\n                pass\n    elif hasattr(src, \"im_class\"):\n        # Bound method\n        cls = src.im_class\n        _full_name_ = get_full_name(cls) + \".\" + src.__name__\n        # It's a short-living object, so we don't cache result\n    elif hasattr(src, \"__module__\") and hasattr(src, \"__name__\"):\n        # Func or class\n        _full_name_ = (\n            (\"<unknown module>\" if src.__module__ is None else src.__module__)\n            + \".\"\n            + src.__name__\n        )\n        try:\n            src._full_name_ = _full_name_\n        except AttributeError:\n            pass\n        except TypeError:\n            pass\n    else:\n        # Something else\n        _full_name_ = str(get_original_fn(src))\n    return _full_name_"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_function_call_str(fn, args, kwargs):\n\n    def str_converter(v):\n        try:\n            return str(v)\n        except Exception:\n            try:\n                return repr(v)\n            except Exception:\n                return \"<n/a str raised>\"\n\n    result = get_full_name(fn) + \"(\"\n    first = True\n    for v in args:\n        if first:\n            first = False\n        else:\n            result += \",\"\n        result += str_converter(v)\n    for k, v in kwargs.items():\n        if first:\n            first = False\n        else:\n            result += \",\"\n        result += str(k) + \"=\" + str_converter(v)\n    result += \")\"\n    return result", "response": "Converts a function call to a str(... - like string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_function_call_repr(fn, args, kwargs):\n\n    result = get_full_name(fn) + \"(\"\n    first = True\n    for v in args:\n        if first:\n            first = False\n        else:\n            result += \",\"\n        result += repr(v)\n    for k, v in kwargs.items():\n        if first:\n            first = False\n        else:\n            result += \",\"\n        result += str(k) + \"=\" + repr(v)\n    result += \")\"\n    return result", "response": "Converts a function call to a repr(... - like string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getargspec(func):\n    if inspect.ismethod(func):\n        func = func.__func__\n    # Cythonized functions have a .__code__, but don't pass inspect.isfunction()\n    try:\n        code = func.__code__\n    except AttributeError:\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n    if hasattr(code, \"co_kwonlyargcount\") and code.co_kwonlyargcount > 0:\n        raise ValueError(\"keyword-only arguments are not supported by getargspec()\")\n    args, varargs, varkw = inspect.getargs(code)\n    return inspect.ArgSpec(args, varargs, varkw, func.__defaults__)", "response": "Variation of inspect. getargspec that works for more functions but\n    is not accepted by getargspec."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_cython_or_generator(fn):\n    if hasattr(fn, \"__func__\"):\n        fn = fn.__func__  # Class method, static method\n    if inspect.isgeneratorfunction(fn):\n        return True\n    name = type(fn).__name__\n    return (\n        name == \"generator\"\n        or name == \"method_descriptor\"\n        or name == \"cython_function_or_method\"\n        or name == \"builtin_function_or_method\"\n    )", "response": "Returns whether this function is either a Cythonized function or a Cythonized function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a function is compiled w / Cython.", "response": "def is_cython_function(fn):\n    \"\"\"Checks if a function is compiled w/Cython.\"\"\"\n    if hasattr(fn, \"__func__\"):\n        fn = fn.__func__  # Class method, static method\n    name = type(fn).__name__\n    return (\n        name == \"method_descriptor\"\n        or name == \"cython_function_or_method\"\n        or name == \"builtin_function_or_method\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_classmethod(fn):\n    # This is True for bound methods\n    if not inspect.ismethod(fn):\n        return False\n    if not hasattr(fn, \"__self__\"):\n        return False\n    im_self = fn.__self__\n    # This is None for instance methods on classes, but True\n    # for instance methods on instances.\n    if im_self is None:\n        return False\n    # This is True for class methods of new- and old-style classes, respectively\n    return isinstance(im_self, six.class_types)", "response": "Returns whether f is a classmethod."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wraps(\n    wrapped, assigned=functools.WRAPPER_ASSIGNMENTS, updated=functools.WRAPPER_UPDATES\n):\n    \"\"\"Cython-compatible functools.wraps implementation.\"\"\"\n    if not is_cython_function(wrapped):\n        return functools.wraps(wrapped, assigned, updated)\n    else:\n        return lambda wrapper: wrapper", "response": "Cython - compatible functools. wraps implementation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subclass_tree(cls, ensure_unique=True):\n    subclasses = []\n    # cls.__subclasses__() fails on classes inheriting from type\n    for subcls in type.__subclasses__(cls):\n        subclasses.append(subcls)\n        subclasses.extend(get_subclass_tree(subcls, ensure_unique))\n    return list(set(subclasses)) if ensure_unique else subclasses", "response": "Returns all subclasses of cls recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DictOf(name, *fields):\n    ret = type(name, (Dict,), {'fields': []})\n    #noinspection PyUnresolvedReferences\n    ret.add_fields(*fields)\n    return ret", "response": "Returns a new Dict type with the specified name and fields."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ListOf(element_type, element_none_value=None):\n    from pyws.functions.args.types import TypeFactory\n    element_type = TypeFactory(element_type)\n    return type(element_type.__name__ + 'List', (List,), {\n        'element_type': element_type,\n        'element_none_value': element_none_value})", "response": "This function creates a list type with element type element_type and an optional element value element_none_value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_actions(self, request, view):\n        metadata = OrderedDict()\n        actions = self.get_resource_actions(view)\n\n        resource = view.get_object()\n        for action_name, action in actions.items():\n            if action_name == 'update':\n                view.request = clone_request(request, 'PUT')\n            else:\n                view.action = action_name\n\n            data = ActionSerializer(action, action_name, request, view, resource)\n            metadata[action_name] = data.serialize()\n            if not metadata[action_name]['enabled']:\n                continue\n            fields = self.get_action_fields(view, action_name, resource)\n            if not fields:\n                metadata[action_name]['type'] = 'button'\n            else:\n                metadata[action_name]['type'] = 'form'\n                metadata[action_name]['fields'] = fields\n\n            view.action = None\n            view.request = request\n\n        return metadata", "response": "Return metadata for resource - specific actions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_action_fields(self, view, action_name, resource):\n        serializer = view.get_serializer(resource)\n        fields = OrderedDict()\n        if not isinstance(serializer, view.serializer_class) or action_name == 'update':\n            fields = self.get_fields(serializer.fields)\n        return fields", "response": "Get fields exposed by action s serializer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving an instance of a serializer return a dictionary of metadata about its fields.", "response": "def get_serializer_info(self, serializer):\n        \"\"\"\n        Given an instance of a serializer, return a dictionary of metadata\n        about its fields.\n        \"\"\"\n        if hasattr(serializer, 'child'):\n            # If this is a `ListSerializer` then we want to examine the\n            # underlying child serializer instance instead.\n            serializer = serializer.child\n        return self.get_fields(serializer.fields)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting fields metadata skipping empty fields", "response": "def get_fields(self, serializer_fields):\n        \"\"\"\n        Get fields metadata skipping empty fields\n        \"\"\"\n        fields = OrderedDict()\n        for field_name, field in serializer_fields.items():\n            # Skip tags field in action because it is needed only for resource creation\n            # See also: WAL-1223\n            if field_name == 'tags':\n                continue\n            info = self.get_field_info(field, field_name)\n            if info:\n                fields[field_name] = info\n        return fields"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of metadata about the given instance of a serializer field.", "response": "def get_field_info(self, field, field_name):\n        \"\"\"\n        Given an instance of a serializer field, return a dictionary\n        of metadata about it.\n        \"\"\"\n        field_info = OrderedDict()\n        field_info['type'] = self.label_lookup[field]\n        field_info['required'] = getattr(field, 'required', False)\n\n        attrs = [\n            'label', 'help_text', 'default_value', 'placeholder', 'required',\n            'min_length', 'max_length', 'min_value', 'max_value', 'many'\n        ]\n\n        if getattr(field, 'read_only', False):\n            return None\n\n        for attr in attrs:\n            value = getattr(field, attr, None)\n            if value is not None and value != '':\n                field_info[attr] = force_text(value, strings_only=True)\n\n        if 'label' not in field_info:\n            field_info['label'] = field_name.replace('_', ' ').title()\n\n        if hasattr(field, 'view_name'):\n            list_view = field.view_name.replace('-detail', '-list')\n            base_url = reverse(list_view, request=self.request)\n            field_info['type'] = 'select'\n            field_info['url'] = base_url\n            if hasattr(field, 'query_params'):\n                field_info['url'] += '?%s' % urlencode(field.query_params)\n            field_info['value_field'] = getattr(field, 'value_field', 'url')\n            field_info['display_name_field'] = getattr(field, 'display_name_field', 'display_name')\n\n        if hasattr(field, 'choices') and not hasattr(field, 'queryset'):\n            field_info['choices'] = [\n                {\n                    'value': choice_value,\n                    'display_name': force_text(choice_name, strings_only=True)\n                }\n                for choice_value, choice_name in field.choices.items()\n            ]\n\n        return field_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(\n        database: str,\n        loop: asyncio.BaseEventLoop = None,\n        executor: concurrent.futures.Executor = None,\n        timeout: int = 5,\n        echo: bool = False,\n        isolation_level: str = '',\n        check_same_thread: bool = False,\n        **kwargs: dict\n):\n    \"\"\"\n    \u628aasync\u65b9\u6cd5\u6267\u884c\u540e\u7684\u5bf9\u8c61\u521b\u5efa\u4e3aasync\u4e0a\u4e0b\u6587\u6a21\u5f0f\n    \"\"\"\n    coro = _connect(\n        database,\n        loop=loop,\n        executor=executor,\n        timeout=timeout,\n        echo=echo,\n        isolation_level=isolation_level,\n        check_same_thread=check_same_thread,\n        **kwargs\n    )\n    return _ContextManager(coro)", "response": "Connect to a database and return a new context manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recalculate_estimate(recalculate_total=False):\n    # Celery does not import server.urls and does not discover cost tracking modules.\n    # So they should be discovered implicitly.\n    CostTrackingRegister.autodiscover()\n    # Step 1. Recalculate resources estimates.\n    for resource_model in CostTrackingRegister.registered_resources:\n        for resource in resource_model.objects.all():\n            _update_resource_consumed(resource, recalculate_total=recalculate_total)\n    # Step 2. Move from down to top and recalculate consumed estimate for each\n    #         object based on its children.\n    ancestors_models = [m for m in models.PriceEstimate.get_estimated_models()\n                        if not issubclass(m, structure_models.ResourceMixin)]\n    for model in ancestors_models:\n        for ancestor in model.objects.all():\n            _update_ancestor_consumed(ancestor)", "response": "Recalculate consumed price of consumables that were used by resource until now."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nneeds to return the string source for the module.", "response": "def get_data(self, path):\n        \"\"\"Needs to return the string source for the module.\"\"\"\n        return LineCacheNotebookDecoder(\n            code=self.code, raw=self.raw, markdown=self.markdown\n        ).decode(self.decode(), self.path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a lazy loader source file loader.", "response": "def loader(self):\n        \"\"\"Create a lazy loader source file loader.\"\"\"\n        loader = super().loader\n        if self._lazy and (sys.version_info.major, sys.version_info.minor) != (3, 4):\n            loader = LazyLoader.factory(loader)\n        # Strip the leading underscore from slots\n        return partial(\n            loader, **{object.lstrip(\"_\"): getattr(self, object) for object in self.__slots__}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(cls, filename, dir=None, main=False, **kwargs):\n        name = main and \"__main__\" or Path(filename).stem\n        loader = cls(name, str(filename), **kwargs)\n        module = module_from_spec(FileModuleSpec(name, loader, origin=loader.path))\n        cwd = str(Path(loader.path).parent)\n        try:\n            with ExitStack() as stack:\n                sys.path.append(cwd)\n                loader.name != \"__main__\" and stack.enter_context(_installed_safely(module))\n                loader.exec_module(module)\n        finally:\n            sys.path.pop()\n\n        return module", "response": "Import a notebook as a module from a filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the current source to ast", "response": "def source_to_code(self, nodes, path, *, _optimize=-1):\n        \"\"\"* Convert the current source to ast \n        * Apply ast transformers.\n        * Compile the code.\"\"\"\n        if not isinstance(nodes, ast.Module):\n            nodes = ast.parse(nodes, self.path)\n        if self._markdown_docstring:\n            nodes = update_docstring(nodes)\n        return super().source_to_code(\n            ast.fix_missing_locations(self.visit(nodes)), path, _optimize=_optimize\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_urls(self):\n        urls = []\n\n        for action in self.get_extra_actions():\n            regex = r'^{}/$'.format(self._get_action_href(action))\n            view = self.admin_site.admin_view(action)\n            urls.append(url(regex, view))\n\n        return urls + super(ExtraActionsMixin, self).get_urls()", "response": "Inject extra action URLs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninjects extra links into template context.", "response": "def changelist_view(self, request, extra_context=None):\n        \"\"\"\n        Inject extra links into template context.\n        \"\"\"\n        links = []\n\n        for action in self.get_extra_actions():\n            links.append({\n                'label': self._get_action_label(action),\n                'href': self._get_action_href(action)\n            })\n\n        extra_context = extra_context or {}\n        extra_context['extra_links'] = links\n\n        return super(ExtraActionsMixin, self).changelist_view(\n            request, extra_context=extra_context,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self):\n\n        if NURESTSession.session_stack:\n            bambou_logger.critical(\"Starting a session inside a with statement is not supported.\")\n            raise Exception(\"Starting a session inside a with statement is not supported.\")\n\n        NURESTSession.current_session = self\n\n        self._authenticate()\n        return self", "response": "Starts the session.\n\n            Starting the session will actually get the API key of the current user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing new instances quotas", "response": "def init_quotas(sender, instance, created=False, **kwargs):\n    \"\"\" Initialize new instances quotas \"\"\"\n    if not created:\n        return\n    for field in sender.get_quotas_fields():\n        try:\n            field.get_or_create_quota(scope=instance)\n        except CreationConditionFailedQuotaError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate handler that will recalculate count_quota on creation or deletion.", "response": "def count_quota_handler_factory(count_quota_field):\n    \"\"\" Creates handler that will recalculate count_quota on creation/deletion \"\"\"\n\n    def recalculate_count_quota(sender, instance, **kwargs):\n        signal = kwargs['signal']\n        if signal == signals.post_save and kwargs.get('created'):\n            count_quota_field.add_usage(instance, delta=1)\n        elif signal == signals.post_delete:\n            count_quota_field.add_usage(instance, delta=-1, fail_silently=True)\n\n    return recalculate_count_quota"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall aggregated quotas fields update methods", "response": "def handle_aggregated_quotas(sender, instance, **kwargs):\n    \"\"\" Call aggregated quotas fields update methods \"\"\"\n    quota = instance\n    # aggregation is not supported for global quotas.\n    if quota.scope is None:\n        return\n    quota_field = quota.get_field()\n    # usage aggregation should not count another usage aggregator field to avoid calls duplication.\n    if isinstance(quota_field, fields.UsageAggregatorQuotaField) or quota_field is None:\n        return\n    signal = kwargs['signal']\n    for aggregator_quota in quota_field.get_aggregator_quotas(quota):\n        field = aggregator_quota.get_field()\n        if signal == signals.post_save:\n            field.post_child_quota_save(aggregator_quota.scope, child_quota=quota, created=kwargs.get('created'))\n        elif signal == signals.pre_delete:\n            field.pre_child_quota_delete(aggregator_quota.scope, child_quota=quota)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_settings(self, link):\n        return reverse(\n            'servicesettings-detail', kwargs={'uuid': link.service.settings.uuid}, request=self.context['request'])", "response": "Get the URL of the service settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_url(self, link):\n        view_name = SupportedServices.get_detail_view_for_model(link.service)\n        return reverse(view_name, kwargs={'uuid': link.service.uuid.hex}, request=self.context['request'])", "response": "Get the URL of the service with the given link."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_resources_count(self, link):\n        total = 0\n        for model in SupportedServices.get_service_resources(link.service):\n            # Format query path from resource to service project link\n            query = {model.Permissions.project_path.split('__')[0]: link}\n            total += model.objects.filter(**query).count()\n        return total", "response": "Count the number of all resources connected to link"}
