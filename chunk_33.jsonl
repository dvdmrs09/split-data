{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _algebraic_rules_state():\n    u = wc(\"u\", head=SCALAR_TYPES)\n    v = wc(\"v\", head=SCALAR_TYPES)\n\n    n = wc(\"n\", head=(int, str, SymbolicLabelBase))\n    m = wc(\"m\", head=(int, str, SymbolicLabelBase))\n    k = wc(\"k\", head=(int, str, SymbolicLabelBase))\n\n    A = wc(\"A\", head=Operator)\n    A__ = wc(\"A__\", head=Operator)\n    B = wc(\"B\", head=Operator)\n\n    A_times = wc(\"A\", head=OperatorTimes)\n    A_local = wc(\"A\", head=LocalOperator)\n    B_local = wc(\"B\", head=LocalOperator)\n\n    Psi_sym = wc(\"Psi\", head=KetSymbol)\n    Psi = wc(\"Psi\", head=State)\n    Phi = wc(\"Phi\", head=State)\n    Psi_local = wc(\"Psi\", head=LocalKet)\n    Psi_tensor = wc(\"Psi\", head=TensorKet)\n    Phi_tensor = wc(\"Phi\", head=TensorKet)\n\n    ls = wc(\"ls\", head=LocalSpace)\n\n    basisket = wc('basisket', BasisKet, kwargs={'hs': ls})\n    ket_a = wc('a', BasisKet)\n    ket_b = wc('b', BasisKet)\n\n    indranges__ = wc(\"indranges__\", head=IndexRangeBase)\n    sum = wc('sum', head=KetIndexedSum)\n    sum2 = wc('sum2', head=KetIndexedSum)\n\n    ScalarTimesKet._rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(1, Psi),\n            lambda Psi: Psi)),\n        ('R002', (\n            pattern_head(0, Psi),\n            lambda Psi: ZeroKet)),\n        ('R003', (\n            pattern_head(u, ZeroKet),\n            lambda u: ZeroKet)),\n        ('R004', (\n            pattern_head(u, pattern(ScalarTimesKet, v, Psi)),\n            lambda u, v, Psi: (u * v) * Psi))\n    ]))\n\n    def local_rule(A, B, Psi):\n        return OperatorTimes.create(*A) * (B * Psi)\n\n    OperatorTimesKet._rules.update(check_rules_dict([\n        ('R001', (  # Id * Psi = Psi\n            pattern_head(IdentityOperator, Psi),\n            lambda Psi: Psi)),\n        ('R002', (  # 0 * Psi = 0\n            pattern_head(ZeroOperator, Psi),\n            lambda Psi: ZeroKet)),\n        ('R003', (  # A * 0 = 0\n            pattern_head(A, ZeroKet),\n            lambda A: ZeroKet)),\n        ('R004', (  # A * v * Psi = v * A * Psi (pull out scalar)\n            pattern_head(A, pattern(ScalarTimesKet, v, Psi)),\n            lambda A, v, Psi:  v * (A * Psi))),\n\n        ('R005', (  # |n><m| * |k> = delta_mk * |n>\n            pattern_head(\n                pattern(LocalSigma, n, m, hs=ls),\n                pattern(BasisKet, k, hs=ls)),\n            lambda ls, n, m, k:\n                KroneckerDelta(\n                    BasisKet(m, hs=ls).index, BasisKet(k, hs=ls).index) *\n                BasisKet(n, hs=ls))),\n\n        # harmonic oscillator\n        ('R006', (  # a^+ |n> = sqrt(n+1) * |n+1>\n            pattern_head(pattern(Create, hs=ls), basisket),\n            lambda basisket, ls:\n                sqrt(basisket.index + 1) * basisket.next())),\n        ('R007', (  # a |n> = sqrt(n) * |n-1>\n            pattern_head(pattern(Destroy, hs=ls), basisket),\n            lambda basisket, ls:\n                sqrt(basisket.index) * basisket.prev())),\n        ('R008', (  # a |alpha> = alpha * |alpha> (eigenstate of annihilator)\n            pattern_head(\n                pattern(Destroy, hs=ls),\n                pattern(CoherentStateKet, u, hs=ls)),\n            lambda ls, u: u * CoherentStateKet(u, hs=ls))),\n\n        # spin\n        ('R009', (\n            pattern_head(pattern(Jplus, hs=ls), basisket),\n            lambda basisket, ls:\n                Jpjmcoeff(basisket.space, basisket.index, shift=True) *\n                basisket.next())),\n        ('R010', (\n            pattern_head(pattern(Jminus, hs=ls), basisket),\n            lambda basisket, ls:\n                Jmjmcoeff(basisket.space, basisket.index, shift=True) *\n                basisket.prev())),\n        ('R011', (\n            pattern_head(pattern(Jz, hs=ls), basisket),\n            lambda basisket, ls:\n                Jzjmcoeff(basisket.space, basisket.index, shift=True) *\n                basisket)),\n\n        ('R012', (\n            pattern_head(A_local, Psi_tensor),\n            lambda A, Psi: act_locally(A, Psi))),\n        ('R013', (\n            pattern_head(A_times, Psi_tensor),\n            lambda A, Psi: act_locally_times_tensor(A, Psi))),\n        ('R014', (\n            pattern_head(A, pattern(OperatorTimesKet, B, Psi)),\n            lambda A, B, Psi: (\n                (A * B) * Psi\n                if (B * Psi) == OperatorTimesKet(B, Psi)\n                else A * (B * Psi)))),\n        ('R015', (\n            pattern_head(pattern(OperatorTimes, A__, B_local), Psi_local),\n            local_rule)),\n        ('R016', (\n            pattern_head(pattern(ScalarTimesOperator, u, A), Psi),\n            lambda u, A, Psi: u * (A * Psi))),\n        ('R017', (\n            pattern_head(\n                pattern(Displace, u, hs=ls),\n                pattern(BasisKet, 0, hs=ls)),\n            lambda ls, u: CoherentStateKet(u, hs=ls))),\n        ('R018', (\n            pattern_head(\n                pattern(Displace, u, hs=ls),\n                pattern(CoherentStateKet, v, hs=ls)),\n            lambda ls, u, v:\n                ((Displace(u, hs=ls) * Displace(v, hs=ls)) *\n                 BasisKet(0, hs=ls)))),\n        ('R019', (\n            pattern_head(\n                pattern(Phase, u, hs=ls), pattern(BasisKet, m, hs=ls)),\n            lambda ls, u, m: exp(I * u * m) * BasisKet(m, hs=ls))),\n        ('R020', (\n            pattern_head(\n                pattern(Phase, u, hs=ls),\n                pattern(CoherentStateKet, v, hs=ls)),\n            lambda ls, u, v: CoherentStateKet(v * exp(I * u), hs=ls))),\n\n        ('R021', (\n            pattern_head(A, sum),\n            lambda A, sum: KetIndexedSum.create(A * sum.term, *sum.ranges))),\n    ]))\n\n    TensorKet._binary_rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(pattern(ScalarTimesKet, u, Psi), Phi),\n            lambda u, Psi, Phi: u * (Psi * Phi))),\n        ('R002', (\n            pattern_head(Psi, pattern(ScalarTimesKet, u, Phi)),\n            lambda Psi, u, Phi: u * (Psi * Phi))),\n        ('R003', (  # delegate to __mul__\n            pattern_head(sum, sum2),\n            lambda sum, sum2: sum * sum2)),\n        ('R004', (  # delegate to __mul__\n            pattern_head(Psi, sum),\n            lambda Psi, sum: Psi * sum)),\n        ('R005', (  # delegate to __mul__\n            pattern_head(sum, Psi),\n            lambda sum, Psi: sum * Psi)),\n    ]))\n\n    BraKet._rules.update(check_rules_dict([\n        # All rules must result in scalars or objects in the TrivialSpace\n        ('R001', (\n            pattern_head(Phi, ZeroKet),\n            lambda Phi: Zero)),\n        ('R002', (\n            pattern_head(ZeroKet, Phi),\n            lambda Phi: Zero)),\n        ('R003', (\n            pattern_head(ket_a, ket_b),\n            lambda a, b: KroneckerDelta(a.index, b.index))),\n        ('R004', (\n            pattern_head(Psi_sym, Psi_sym),\n            lambda Psi: One)),\n            # we're assuming every KetSymbol is normalized. If we ever want\n            # to allow non-normalized states, the best thing to dou would be to\n            # add a `norm` attribute\n        ('R005', (\n            pattern_head(Psi_tensor, Phi_tensor),\n            lambda Psi, Phi: tensor_decompose_kets(Psi, Phi, BraKet.create))),\n        ('R006', (\n            pattern_head(pattern(ScalarTimesKet, u, Psi), Phi),\n            lambda u, Psi, Phi: u.conjugate() * (Psi.adjoint() * Phi))),\n        ('R007', (\n            pattern_head(pattern(OperatorTimesKet, A, Psi), Phi),\n            lambda A, Psi, Phi: (Psi.adjoint() * (A.dag() * Phi)))),\n        ('R008', (\n            pattern_head(Psi, pattern(ScalarTimesKet, u, Phi)),\n            lambda Psi, u, Phi: u * (Psi.adjoint() * Phi))),\n        ('R009', (  # delegate to __mul__\n            pattern_head(sum, sum2),\n            lambda sum, sum2: Bra.create(sum) * sum2)),\n        ('R010', (  # delegate to __mul__\n            pattern_head(Psi, sum),\n            lambda Psi, sum: Bra.create(Psi) * sum)),\n        ('R011', (  # delegate to __mul__\n            pattern_head(sum, Psi),\n            lambda sum, Psi: Bra.create(sum) * Psi)),\n    ]))\n\n    KetBra._rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(\n                pattern(BasisKet, m, hs=ls),\n                pattern(BasisKet, n, hs=ls)),\n            lambda ls, m, n: LocalSigma(m, n, hs=ls))),\n        ('R002', (\n            pattern_head(pattern(CoherentStateKet, u, hs=ls), Phi),\n            lambda ls, u, Phi: (\n                Displace(u, hs=ls) * (BasisKet(0, hs=ls) * Phi.adjoint())))),\n        ('R003', (\n            pattern_head(Phi, pattern(CoherentStateKet, u, hs=ls)),\n            lambda ls, u, Phi: (\n                (Phi * BasisKet(0, hs=ls).adjoint()) * Displace(-u, hs=ls)))),\n        ('R004', (\n            pattern_head(Psi_tensor, Phi_tensor),\n            lambda Psi, Phi: tensor_decompose_kets(Psi, Phi, KetBra.create))),\n        ('R005', (\n            pattern_head(pattern(OperatorTimesKet, A, Psi), Phi),\n            lambda A, Psi, Phi: A * (Psi * Phi.adjoint()))),\n        ('R006', (\n            pattern_head(Psi, pattern(OperatorTimesKet, A, Phi)),\n            lambda Psi, A, Phi: (Psi * Phi.adjoint()) * A.adjoint())),\n        ('R007', (\n            pattern_head(pattern(ScalarTimesKet, u, Psi), Phi),\n            lambda u, Psi, Phi: u * (Psi * Phi.adjoint()))),\n        ('R008', (\n            pattern_head(Psi, pattern(ScalarTimesKet, u, Phi)),\n            lambda Psi, u, Phi: u.conjugate() * (Psi * Phi.adjoint()))),\n        ('R009', (  # delegate to __mul__\n            pattern_head(sum, sum2),\n            lambda sum, sum2: sum * Bra.create(sum2))),\n        ('R010', (  # delegate to __mul__\n            pattern_head(Psi, sum),\n            lambda Psi, sum: Psi * Bra.create(sum))),\n        ('R011', (  # delegate to __mul__\n            pattern_head(sum, Psi),\n            lambda sum, Psi: sum * Bra.create(Psi))),\n    ]))\n\n    def pull_constfactor_from_sum(u, Psi, indranges):\n        bound_symbols = set([r.index_symbol for r in indranges])\n        if len(u.free_symbols.intersection(bound_symbols)) == 0:\n            return u * KetIndexedSum.create(Psi, *indranges)\n        else:\n            raise CannotSimplify()\n\n    KetIndexedSum._rules.update(check_rules_dict([\n        ('R001', (  # sum over zero -> zero\n            pattern_head(ZeroKet, indranges__),\n            lambda indranges: ZeroKet)),\n        ('R002', (  # pull constant prefactor out of sum\n            pattern_head(pattern(ScalarTimesKet, u, Psi), indranges__),\n            lambda u, Psi, indranges:\n                pull_constfactor_from_sum(u, Psi, indranges))),\n    ]))", "response": "Set the default algebraic rules for the operations defined in this\n    module"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the maximal common block structure for two Circuit objects.", "response": "def _get_common_block_structure(lhs_bs, rhs_bs):\n    \"\"\"For two block structures ``aa = (a1, a2, ..., an)``, ``bb = (b1, b2,\n    ..., bm)`` generate the maximal common block structure so that every block\n    from aa and bb is contained in exactly one block of the resulting\n    structure.  This is useful for determining how to apply the distributive\n    law when feeding two concatenated Circuit objects into each other.\n\n    Examples:\n        ``(1, 1, 1), (2, 1) -> (2, 1)``\n        ``(1, 1, 2, 1), (2, 1, 2) -> (2, 3)``\n\n    Args:\n        lhs_bs (tuple): first block structure\n        rhs_bs (tuple): second block structure\n    \"\"\"\n\n    # for convenience the arguments may also be Circuit objects\n    if isinstance(lhs_bs, Circuit):\n        lhs_bs = lhs_bs.block_structure\n    if isinstance(rhs_bs, Circuit):\n        rhs_bs = rhs_bs.block_structure\n\n    if sum(lhs_bs) != sum(rhs_bs):\n        raise IncompatibleBlockStructures(\n            'Blockstructures have different total channel numbers.')\n\n    if len(lhs_bs) == len(rhs_bs) == 0:\n        return ()\n\n    i = j = 1\n    lsum = 0\n    while True:\n        lsum = sum(lhs_bs[:i])\n        rsum = sum(rhs_bs[:j])\n        if lsum < rsum:\n            i += 1\n        elif rsum < lsum:\n            j += 1\n        else:\n            break\n\n    return (lsum, ) + _get_common_block_structure(lhs_bs[i:], rhs_bs[j:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tensor_decompose_series(lhs, rhs):\n    if isinstance(rhs, CPermutation):\n        raise CannotSimplify()\n    lhs_structure = lhs.block_structure\n    rhs_structure = rhs.block_structure\n    res_struct = _get_common_block_structure(lhs_structure, rhs_structure)\n    if len(res_struct) > 1:\n        blocks, oblocks = (\n            lhs.get_blocks(res_struct),\n            rhs.get_blocks(res_struct))\n        parallel_series = [SeriesProduct.create(lb, rb)\n                           for (lb, rb) in zip(blocks, oblocks)]\n        return Concatenation.create(*parallel_series)\n    raise CannotSimplify()", "response": "Simplification method for lhs << rhs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npulls out a permutation from the Feedback of a SeriesProduct with itself.", "response": "def _pull_out_perm_lhs(lhs, rest, out_port, in_port):\n    \"\"\"Pull out a permutation from the Feedback of a SeriesProduct with itself.\n\n    Args:\n        lhs (CPermutation): The permutation circuit\n        rest (tuple): The other SeriesProduct operands\n        out_port (int): The feedback output port index\n        in_port (int): The feedback input port index\n\n    Returns:\n        Circuit: The simplified circuit\n    \"\"\"\n    out_inv, lhs_red = lhs._factor_lhs(out_port)\n    return lhs_red << Feedback.create(SeriesProduct.create(*rest),\n                                      out_port=out_inv, in_port=in_port)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninvert a series self - feedback twice to get rid of unnecessary permutations.", "response": "def _series_feedback(series, out_port, in_port):\n    \"\"\"Invert a series self-feedback twice to get rid of unnecessary\n    permutations.\"\"\"\n    series_s = series.series_inverse().series_inverse()\n    if series_s == series:\n        raise CannotSimplify()\n    return series_s.feedback(out_port=out_port, in_port=in_port)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the default algebraic rules for the operations defined in this module", "response": "def _algebraic_rules_circuit():\n    \"\"\"Set the default algebraic rules for the operations defined in this\n    module\"\"\"\n    A_CPermutation = wc(\"A\", head=CPermutation)\n    B_CPermutation = wc(\"B\", head=CPermutation)\n    C_CPermutation = wc(\"C\", head=CPermutation)\n    D_CPermutation = wc(\"D\", head=CPermutation)\n\n    A_Concatenation = wc(\"A\", head=Concatenation)\n    B_Concatenation = wc(\"B\", head=Concatenation)\n\n    A_SeriesProduct = wc(\"A\", head=SeriesProduct)\n\n    A_Circuit = wc(\"A\", head=Circuit)\n    B_Circuit = wc(\"B\", head=Circuit)\n    C_Circuit = wc(\"C\", head=Circuit)\n\n    A__Circuit = wc(\"A__\", head=Circuit)\n    B__Circuit = wc(\"B__\", head=Circuit)\n    C__Circuit = wc(\"C__\", head=Circuit)\n\n    A_SLH = wc(\"A\", head=SLH)\n    B_SLH = wc(\"B\", head=SLH)\n\n    j_int = wc(\"j\", head=int)\n    k_int = wc(\"k\", head=int)\n\n    SeriesProduct._binary_rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(A_CPermutation, B_CPermutation),\n            lambda A, B: A.series_with_permutation(B))),\n        ('R002', (\n            pattern_head(A_SLH, B_SLH),\n            lambda A, B: A.series_with_slh(B))),\n        ('R003', (\n            pattern_head(A_Circuit, B_Circuit),\n            lambda A, B: _tensor_decompose_series(A, B))),\n        ('R004', (\n            pattern_head(A_CPermutation, B_Circuit),\n            lambda A, B: _factor_permutation_for_blocks(A, B))),\n        ('R005', (\n            pattern_head(A_Circuit, pattern(SeriesInverse, A_Circuit)),\n            lambda A: cid(A.cdim))),\n        ('R006', (\n            pattern_head(pattern(SeriesInverse, A_Circuit), A_Circuit),\n            lambda A: cid(A.cdim))),\n    ]))\n\n    Concatenation._binary_rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(A_SLH, B_SLH),\n            lambda A, B: A.concatenate_slh(B))),\n        ('R002', (\n            pattern_head(A_CPermutation, B_CPermutation),\n            lambda A, B: CPermutation.create(\n                concatenate_permutations(A.permutation, B.permutation)))),\n        ('R003', (\n            pattern_head(A_CPermutation, CIdentity),\n            lambda A: CPermutation.create(\n                concatenate_permutations(A.permutation, (0,))))),\n        ('R004', (\n            pattern_head(CIdentity, B_CPermutation),\n            lambda B: CPermutation.create(\n                concatenate_permutations((0,), B.permutation)))),\n        ('R005', (\n            pattern_head(\n                pattern(SeriesProduct, A__Circuit, B_CPermutation),\n                pattern(SeriesProduct, C__Circuit, D_CPermutation)),\n            lambda A, B, C, D: (\n                (SeriesProduct.create(*A) + SeriesProduct.create(*C)) <<\n                (B + D)))),\n        ('R006', (\n            pattern_head(\n                pattern(SeriesProduct, A__Circuit, B_CPermutation), C_Circuit),\n            lambda A, B, C: (\n                (SeriesProduct.create(*A) + C) << (B + cid(C.cdim))))),\n        ('R007', (\n            pattern_head(\n                A_Circuit, pattern(SeriesProduct, B__Circuit, C_CPermutation)),\n            lambda A, B, C: ((A + SeriesProduct.create(*B)) <<\n                             (cid(A.cdim) + C)))),\n    ]))\n\n    Feedback._rules.update(check_rules_dict([\n        ('R001', (\n            pattern_head(A_SeriesProduct, out_port=j_int, in_port=k_int),\n            lambda A, j, k: _series_feedback(A, out_port=j, in_port=k))),\n        ('R002', (\n            pattern_head(\n                pattern(SeriesProduct, A_CPermutation, B__Circuit),\n                out_port=j_int, in_port=k_int),\n            lambda A, B, j, k: _pull_out_perm_lhs(A, B, j, k))),\n        ('R003', (\n            pattern_head(\n                pattern(SeriesProduct, A_Concatenation, B__Circuit),\n                out_port=j_int, in_port=k_int),\n            lambda A, B, j, k: _pull_out_unaffected_blocks_lhs(A, B, j, k))),\n        ('R004', (\n            pattern_head(\n                pattern(SeriesProduct, A__Circuit, B_CPermutation),\n                out_port=j_int, in_port=k_int),\n            lambda A, B, j, k: _pull_out_perm_rhs(A, B, j, k))),\n        ('R005', (\n            pattern_head(\n                pattern(SeriesProduct, A__Circuit, B_Concatenation),\n                out_port=j_int, in_port=k_int),\n            lambda A, B, j, k: _pull_out_unaffected_blocks_rhs(A, B, j, k))),\n    ]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef properties_for_args(cls, arg_names='_arg_names'):\n    from qnet.algebra.core.scalar_algebra import Scalar\n    scalar_args = False\n    if hasattr(cls, '_scalar_args'):\n        scalar_args = cls._scalar_args\n    for arg_name in getattr(cls, arg_names):\n        def get_arg(self, name):\n            val = getattr(self, \"_%s\" % name)\n            if scalar_args:\n                assert isinstance(val, Scalar)\n            return val\n        prop = property(partial(get_arg, name=arg_name))\n        doc = \"The `%s` argument\" % arg_name\n        if scalar_args:\n            doc += \", as a :class:`.Scalar` instance.\"\n        else:\n            doc += \".\"\n        prop.__doc__ = doc\n        setattr(cls, arg_name, prop)\n    cls._has_properties_for_args = True\n    return cls", "response": "A class decorator that adds properties for each of the given argument names."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the category object for the given slug.", "response": "def get_category(self, slug):\n        \"\"\"\n        Get the category object\n        \"\"\"\n        try:\n            return get_category_for_slug(slug)\n        except ObjectDoesNotExist as e:\n            raise Http404(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_unique_slug(self, cleaned_data):\n        date_kwargs = {}\n        error_msg = _(\"The slug is not unique\")\n\n        # The /year/month/slug/ URL determines when a slug can be unique.\n        pubdate = cleaned_data['publication_date'] or now()\n        if '{year}' in appsettings.FLUENT_BLOGS_ENTRY_LINK_STYLE:\n            date_kwargs['year'] = pubdate.year\n            error_msg = _(\"The slug is not unique within it's publication year.\")\n        if '{month}' in appsettings.FLUENT_BLOGS_ENTRY_LINK_STYLE:\n            date_kwargs['month'] = pubdate.month\n            error_msg = _(\"The slug is not unique within it's publication month.\")\n        if '{day}' in appsettings.FLUENT_BLOGS_ENTRY_LINK_STYLE:\n            date_kwargs['day'] = pubdate.day\n            error_msg = _(\"The slug is not unique within it's publication day.\")\n\n        date_range = get_date_range(**date_kwargs)\n\n        # Base filters are configurable for translation support.\n        dup_filters = self.get_unique_slug_filters(cleaned_data)\n        if date_range:\n            dup_filters['publication_date__range'] = date_range\n\n        dup_qs = EntryModel.objects.filter(**dup_filters)\n\n        if self.instance and self.instance.pk:\n            dup_qs = dup_qs.exclude(pk=self.instance.pk)\n\n        # Test whether the slug is unique in the current month\n        # Note: doesn't take changes to FLUENT_BLOGS_ENTRY_LINK_STYLE into account.\n        if dup_qs.exists():\n            raise ValidationError(error_msg)", "response": "Validate that the slug is unique within a given time period."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubstitute symbols or sub - expressions with the given replacements and re - evaluate the result .", "response": "def substitute(expr, var_map):\n    \"\"\"Substitute symbols or (sub-)expressions with the given replacements and\n    re-evalute the result\n\n    Args:\n        expr: The expression in which to perform the substitution\n        var_map (dict): The substitution dictionary.\n    \"\"\"\n    try:\n        if isinstance(expr, SympyBasic):\n            sympy_var_map = {\n                k: v for (k, v) in var_map.items()\n                if isinstance(k, SympyBasic)}\n            return expr.subs(sympy_var_map)\n        else:\n            return expr.substitute(var_map)\n    except AttributeError:\n        if expr in var_map:\n            return var_map[expr]\n        return expr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_rules_no_recurse(expr, rules):\n    try:\n        # `rules` is an OrderedDict key => (pattern, replacement)\n        items = rules.items()\n    except AttributeError:\n        # `rules` is a list of (pattern, replacement) tuples\n        items = enumerate(rules)\n    for key, (pat, replacement) in items:\n        matched = pat.match(expr)\n        if matched:\n            try:\n                return replacement(**matched)\n            except CannotSimplify:\n                pass\n    return expr", "response": "Non - recursively match expr again all rules"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_rules(expr, rules):\n    if LOG:\n        logger = logging.getLogger('QNET.create')\n    stack = []\n    path = []\n    if isinstance(expr, Expression):\n        stack.append(ProtoExpr.from_expr(expr))\n        path.append(0)\n        if LOG:\n            logger.debug(\n                \"Starting at level 1: placing expr on stack: %s\", expr)\n        while True:\n            i = path[-1]\n            try:\n                arg = stack[-1][i]\n                if LOG:\n                    logger.debug(\n                        \"At level %d: considering arg %d: %s\",\n                        len(stack), i+1, arg)\n            except IndexError:\n                # done at this level\n                path.pop()\n                expr = stack.pop().instantiate()\n                expr = _apply_rules_no_recurse(expr, rules)\n                if len(stack) == 0:\n                    if LOG:\n                        logger.debug(\n                            \"Complete level 1: returning simplified expr: %s\",\n                            expr)\n                    return expr\n                else:\n                    stack[-1][path[-1]] = expr\n                    path[-1] += 1\n                    if LOG:\n                        logger.debug(\n                            \"Complete level %d. At level %d, setting arg %d \"\n                            \"to simplified expr: %s\", len(stack)+1, len(stack),\n                            path[-1], expr)\n            else:\n                if isinstance(arg, Expression):\n                    stack.append(ProtoExpr.from_expr(arg))\n                    path.append(0)\n                    if LOG:\n                        logger.debug(\"   placing arg on stack\")\n                else:  # scalar\n                    stack[-1][i] = _apply_rules_no_recurse(arg, rules)\n                    if LOG:\n                        logger.debug(\n                            \"   arg is leaf, replacing with simplified expr: \"\n                            \"%s\", stack[-1][i])\n                    path[-1] += 1\n    else:\n        return _apply_rules_no_recurse(expr, rules)", "response": "Recursively re - instantiates the expression expr while applying all of the given rules to all of the context managers that are encountered in the sequence of context managers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rules_attr(cls):\n        from qnet.algebra.core.algebraic_properties import (\n            match_replace, match_replace_binary)\n        if match_replace in cls.simplifications:\n            return '_rules'\n        elif match_replace_binary in cls.simplifications:\n            return '_binary_rules'\n        else:\n            raise TypeError(\n                \"class %s does not have match_replace or \"\n                \"match_replace_binary in its simplifications\" % cls.__name__)", "response": "Return the name of the attribute with rules for create"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_rule(cls, name, pattern, replacement, attr=None):\n        from qnet.utils.check_rules import check_rules_dict\n        if attr is None:\n            attr = cls._rules_attr()\n        if name in getattr(cls, attr):\n            raise ValueError(\n                \"Duplicate key '%s': rule already exists\" % name)\n        getattr(cls, attr).update(check_rules_dict(\n            [(name, (pattern, replacement))]))", "response": "Add an algebraic rule for the given class to the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint a summary of the algebraic rules used by the given class.", "response": "def show_rules(cls, *names, attr=None):\n        \"\"\"Print algebraic rules used by :class:`create`\n\n        Print a summary of the algebraic rules with the given names, or all\n        rules if not names a given.\n\n        Args:\n            names (str): Names of rules to show\n            attr (None or str): Name of the class attribute from which to get\n                the rules. Cf. :meth:`add_rule`.\n\n        Raises:\n            AttributeError: If invalid `attr`\n        \"\"\"\n        from qnet.printing import srepr\n        try:\n            if attr is None:\n                attr = cls._rules_attr()\n            rules = getattr(cls, attr)\n        except TypeError:\n            rules = {}\n        for (name, rule) in rules.items():\n            if len(names) > 0 and name not in names:\n                continue\n            pat, repl = rule\n            print(name)\n            print(\"    PATTERN:\")\n            print(textwrap.indent(\n                textwrap.dedent(srepr(pat, indented=True)),\n                prefix=\" \"*8))\n            print(\"    REPLACEMENT:\")\n            print(textwrap.indent(\n                textwrap.dedent(inspect.getsource(repl).rstrip()),\n                prefix=\" \"*8))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef del_rules(cls, *names, attr=None):\n        if attr is None:\n            attr = cls._rules_attr()\n        if len(names) == 0:\n            getattr(cls, attr)  # raise AttributeError if wrong attr\n            setattr(cls, attr, OrderedDict())\n        else:\n            for name in names:\n                del getattr(cls, attr)[name]", "response": "Delete the rules with the given names or all rules if no names are given."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rules(cls, attr=None):\n        try:\n            if attr is None:\n                attr = cls._rules_attr()\n            return getattr(cls, attr).keys()\n        except TypeError:\n            return ()", "response": "Returns an iterable of rule names used by create."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubstitute sub - expressions of the current object with the new object.", "response": "def substitute(self, var_map):\n        \"\"\"Substitute sub-expressions\n\n        Args:\n            var_map (dict): Dictionary with entries of the form\n                ``{expr: substitution}``\n        \"\"\"\n        if self in var_map:\n            return var_map[self]\n        return self._substitute(var_map)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef doit(self, classes=None, recursive=True, **kwargs):\n        in_classes = (\n            (classes is None) or\n            any([isinstance(self, cls) for cls in classes]))\n        if in_classes:\n            new = self._doit(**kwargs)\n        else:\n            new = self\n        if (new == self) or recursive:\n            new_args = []\n            for arg in new.args:\n                if isinstance(arg, Expression):\n                    new_args.append(arg.doit(\n                        classes=classes, recursive=recursive, **kwargs))\n                else:\n                    new_args.append(arg)\n            new_kwargs = OrderedDict([])\n            for (key, val) in new.kwargs.items():\n                if isinstance(val, Expression):\n                    new_kwargs[key] = val.doit(\n                        classes=classes, recursive=recursive, **kwargs)\n                else:\n                    new_kwargs[key] = val\n            new = new.__class__.create(*new_args, **new_kwargs)\n            if new != self and recursive:\n                new = new.doit(classes=classes, recursive=True, **kwargs)\n        return new", "response": "Rewrite sub - expressions in a more explicit form of a more explicit form of a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrebuild the expression while applying a list of rules to the instantiated expression.", "response": "def apply_rules(self, rules, recursive=True):\n        \"\"\"Rebuild the expression while applying a list of rules\n\n        The rules are applied against the instantiated expression, and any\n        sub-expressions if `recursive` is True. Rule application is best though\n        of as a pattern-based substitution. This is different from the\n        *automatic* rules that :meth:`create` uses (see :meth:`add_rule`),\n        which are applied *before* expressions are instantiated.\n\n        Args:\n            rules (list or ~collections.OrderedDict): List of rules or\n                dictionary mapping names to rules, where each rule is a tuple\n                (:class:`Pattern`, replacement callable), cf.\n                :meth:`apply_rule`\n            recursive (bool): If true (default), apply rules to all arguments\n                and keyword arguments of the expression. Otherwise, only the\n                expression itself will be re-instantiated.\n\n        If `rules` is a dictionary, the keys (rules names) are used only for\n        debug logging, to allow an analysis of which rules lead to the final\n        form of an expression.\n        \"\"\"\n        if recursive:\n            new_args = [_apply_rules(arg, rules) for arg in self.args]\n            new_kwargs = {\n                key: _apply_rules(val, rules)\n                for (key, val) in self.kwargs.items()}\n        else:\n            new_args = self.args\n            new_kwargs = self.kwargs\n        simplified = self.create(*new_args, **new_kwargs)\n        return _apply_rules_no_recurse(simplified, rules)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply a single rule to the expression containing one or more wildcards in .", "response": "def apply_rule(self, pattern, replacement, recursive=True):\n        \"\"\"Apply a single rules to the expression\n\n        This is equivalent to :meth:`apply_rules` with\n        ``rules=[(pattern, replacement)]``\n\n        Args:\n            pattern (.Pattern): A pattern containing one or more wildcards\n            replacement (callable): A callable that takes the wildcard names in\n                `pattern` as keyword arguments, and returns a replacement for\n                any expression that `pattern` matches.\n\n        Example:\n            Consider the following Heisenberg Hamiltonian::\n\n                >>> tls = SpinSpace(label='s', spin='1/2')\n                >>> i, j, n = symbols('i, j, n', cls=IdxSym)\n                >>> J = symbols('J', cls=sympy.IndexedBase)\n                >>> def Sig(i):\n                ...     return OperatorSymbol(\n                ...         StrLabel(sympy.Indexed('sigma', i)), hs=tls)\n                >>> H = - Sum(i, tls)(Sum(j, tls)(\n                ...     J[i, j] * Sig(i) * Sig(j)))\n                >>> unicode(H)\n                '- (\u2211_{i,j \u2208 \u210c\u209b} J_ij \u03c3\u0302_i^(s) \u03c3\u0302_j^(s))'\n\n            We can transform this into a classical Hamiltonian by replacing the\n            operators with scalars::\n\n                >>> H_classical = H.apply_rule(\n                ...     pattern(OperatorSymbol, wc('label', head=StrLabel)),\n                ...     lambda label: label.expr * IdentityOperator)\n                >>> unicode(H_classical)\n                '- (\u2211_{i,j \u2208 \u210c\u209b} J_ij \u03c3_i \u03c3_j)'\n        \"\"\"\n        return self.apply_rules([(pattern, replacement)], recursive=recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets of free SymPy symbols contained within the expression.", "response": "def free_symbols(self):\n        \"\"\"Set of free SymPy symbols contained within the expression.\"\"\"\n        if self._free_symbols is None:\n            res = set.union(\n                set([]), # dummy arg (union fails without arguments)\n                *[_free_symbols(val) for val in self.kwargs.values()])\n            res.update(\n                set([]), # dummy arg (update fails without arguments)\n                *[_free_symbols(arg) for arg in self.args])\n            self._free_symbols = res\n        return self._free_symbols"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets of bound SymPy symbols in the expression", "response": "def bound_symbols(self):\n        \"\"\"Set of bound SymPy symbols in the expression\"\"\"\n        if self._bound_symbols is None:\n            res = set.union(\n                set([]), # dummy arg (union fails without arguments)\n                *[_bound_symbols(val) for val in self.kwargs.values()])\n            res.update(\n                set([]), # dummy arg (update fails without arguments)\n                *[_bound_symbols(arg) for arg in self.args])\n            self._bound_symbols = res\n        return self._bound_symbols"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a single file from the remote server to a local file.", "response": "def download(url, dest):\n    \"\"\"\n    Platform-agnostic downloader.\n    \"\"\"\n    u = urllib.FancyURLopener()\n    logger.info(\"Downloading %s...\" % url)\n    u.retrieve(url, dest)\n    logger.info('Done, see %s' % dest)\n    return dest"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that all of the required programs are installed and download the data.", "response": "def requirements_check():\n    \"\"\"\n    Ensure we have programs needed to download/manipulate the data\n    \"\"\"\n    required_programs = [\n        ('samtools',\n         'http://samtools.sourceforge.net/'),\n        ('bedtools',\n         'http://bedtools.readthedocs.org/en/latest/'),\n        ('bigWigToBedGraph',\n         'http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/'),\n        ('bedGraphToBigWig',\n         'http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/'),\n    ]\n    for req, url in required_programs:\n        try:\n            p = subprocess.Popen(\n                [req], stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n            stdout, stderr = p.communicate()\n        except OSError:\n            raise ValueError(\"Please install %s (%s)\" % (req, url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _up_to_date(md5, fn):\n    if os.path.exists(fn):\n        if hashlib.md5(open(fn).read()).hexdigest() == md5:\n            logger.info('md5sum match for %s' % fn)\n            return True\n        else:\n            logger.info('wrong md5sum for %s' % fn)\n            os.unlink(fn)", "response": "Check if file is up to date and delete it if not"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logged_command(cmds):\n    \"helper function to log a command and then run it\"\n    logger.info(' '.join(cmds))\n    os.system(' '.join(cmds))", "response": "helper function to log a command and then run it"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads cufflinks GTF files", "response": "def get_cufflinks():\n    \"Download cufflinks GTF files\"\n    for size, md5, url in cufflinks:\n        cuff_gtf = os.path.join(args.data_dir, os.path.basename(url))\n        if not _up_to_date(md5, cuff_gtf):\n            download(url, cuff_gtf)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bams():\n    for size, md5, url in bams:\n        bam = os.path.join(\n            args.data_dir,\n            os.path.basename(url).replace('.bam', '_%s.bam' % CHROM))\n        if not _up_to_date(md5, bam):\n            logger.info(\n                'Downloading reads on chromosome %s from %s to %s'\n                % (CHROM, url, bam))\n            cmds = ['samtools', 'view', '-b', url, COORD, '>', bam]\n            logged_command(cmds)\n        bai = bam + '.bai'\n        if not os.path.exists(bai):\n            logger.info('indexing %s' % bam)\n            logger.info(' '.join(cmds))\n            cmds = [\n                'samtools',\n                'index',\n                bam]\n            logged_command(cmds)\n        if os.path.exists(os.path.basename(url) + '.bai'):\n            os.unlink(os.path.basename(url) + '.bai')\n\n    for size, md5, fn in bais:\n        if not _up_to_date(md5, fn):\n            cmds = [\n                'samtools', 'index', bai.replace('.bai', '')]\n            logged_command(cmds)", "response": "Download BAM files if needed extract only chr17 reads and regenerate. bai"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gtf():\n    size, md5, url = GTF\n    full_gtf = os.path.join(args.data_dir, os.path.basename(url))\n    subset_gtf = os.path.join(\n        args.data_dir,\n        os.path.basename(url).replace('.gtf.gz', '_%s.gtf' % CHROM))\n\n    if not _up_to_date(md5, subset_gtf):\n        download(url, full_gtf)\n        cmds = [\n            'zcat', '<',\n            full_gtf,\n            '|', 'awk -F \"\\\\t\" \\'{if ($1 == \"%s\") print $0}\\''\n            % CHROM.replace('chr', ''),\n            '|', 'awk \\'{print \"chr\"$0}\\'', '>', subset_gtf]\n\n        logged_command(cmds)", "response": "Download GTF file from Ensembl only keeping the chr17 entries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating gffutils database Create gffutils database", "response": "def make_db():\n    \"\"\"\n    Create gffutils database\n    \"\"\"\n    size, md5, fn = DB\n    if not _up_to_date(md5, fn):\n        gffutils.create_db(fn.replace('.db', ''), fn, verbose=True, force=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts Cufflinks output GTF files into tables of score and FPKM.", "response": "def cufflinks_conversion():\n    \"\"\"\n    convert Cufflinks output GTF files into tables of score and FPKM.\n    \"\"\"\n    for size, md5, fn in cufflinks_tables:\n        fn = os.path.join(args.data_dir, fn)\n        table = fn.replace('.gtf.gz', '.table')\n        if not _up_to_date(md5, table):\n            logger.info(\"Converting Cufflinks GTF %s to table\" % fn)\n            fout = open(table, 'w')\n            fout.write('id\\tscore\\tfpkm\\n')\n            x = pybedtools.BedTool(fn)\n            seen = set()\n            for i in x:\n                accession = i['transcript_id'].split('.')[0]\n                if accession not in seen:\n                    seen.update([accession])\n                    fout.write(\n                        '\\t'.join([accession, i.score, i['FPKM']]) + '\\n')\n            fout.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new figure showing data for the specified feature.", "response": "def plot(self, feature):\n        \"\"\"\n        Spawns a new figure showing data for `feature`.\n\n        :param feature: A `pybedtools.Interval` object\n\n        Using the pybedtools.Interval `feature`, creates figure specified in\n        :meth:`BaseMiniBrowser.make_fig` and plots data on panels according to\n        `self.panels()`.\n        \"\"\"\n        if isinstance(feature, gffutils.Feature):\n            feature = asinterval(feature)\n        self.make_fig()\n        axes = []\n        for ax, method in self.panels():\n            feature = method(ax, feature)\n            axes.append(ax)\n        return axes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_fig(self):\n        self.fig = plt.figure(figsize=(8, 4))\n        self._all_figures.append(self.fig)", "response": "Make a figure of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the gene models on an Axes object.", "response": "def genes_panel(self, ax, feature):\n        \"\"\"\n        Plots gene models on an Axes.\n\n        Queries the database\n\n        :param ax: matplotlib.Axes object\n        :param feature: pybedtools.Interval\n\n        \"\"\"\n        from gffutils.contrib.plotting import Gene\n        extent = [feature.start, feature.stop]\n        nearby_genes = self.db.region(\n            (feature.chrom, feature.start, feature.stop), featuretype='gene')\n        ybase = 0\n        ngenes = 0\n        for nearby_gene in nearby_genes:\n            # TODO: there should be a better way of identifying which gene is\n            # the same as the feature requested.  Might need to expose an \"ID\"\n            # kwarg.\n            try:\n                if nearby_gene['ID'][0] == feature['ID']:\n                    color = '0.2'\n                else:\n                    color = '0.5'\n            except KeyError:\n                color = '0.5'\n            ngenes += 1\n            extent.extend([nearby_gene.start, nearby_gene.stop])\n            gene_collection = Gene(\n                self.db,\n                nearby_gene,\n                transcripts=None,\n                cds=['CDS'],\n                utrs=['exon'],\n                ybase=ybase,\n                color=color)\n            gene_collection.name = nearby_gene.id\n            gene_collection.add_to_ax(ax)\n            ybase += gene_collection.max_y\n\n        xmin = min(extent)\n        xmax = max(extent)\n        ymax = ngenes\n\n        # 1% padding seems to work well\n        padding = (xmax - xmin) * 0.01\n        ax.axis('tight')\n\n        # add lines indicating extent of current feature\n        # vline_kwargs = dict(color='k', linestyle='--')\n        # ax.axvline(feature.start, **vline_kwargs)\n        # ax.axvline(feature.stop, **vline_kwargs)\n\n        # Make a new feature to represent the region plus surrounding genes\n        interval = pybedtools.create_interval_from_list(feature.fields)\n        interval.strand = '.'\n        for txt in ax.get_yticklabels():\n            txt.set_visible(False)\n        for tick in ax.get_yticklines():\n            tick.set_visible(False)\n        ax.set_ylabel('Genes')\n        ax.spines['right'].set_color('None')\n        ax.spines['left'].set_color('None')\n        ax.spines['top'].set_color('None')\n        ax.yaxis.set_ticks_position('none')\n        ax.xaxis.set_ticks_position('bottom')\n        ax.set_ylabel('Genes', rotation=0, horizontalalignment='right',\n                      verticalalignment='center')\n\n        return interval"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef signal_panel(self, ax, feature):\n        for gs, kwargs in zip(self.genomic_signal_objs, self.plotting_kwargs):\n            x, y = gs.local_coverage(feature, **self.local_coverage_kwargs)\n            ax.plot(x, y, **kwargs)\n        ax.axis('tight')\n        return feature", "response": "Plots each genomic signal as a line using the corresponding\n        plotting_kwargs\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef panels(self):\n        ax1 = self.fig.add_subplot(211)\n        ax2 = self.fig.add_subplot(212, sharex=ax1)\n        return (ax2, self.gene_panel), (ax1, self.signal_panel)", "response": "Add 2 panels to the figure top for gene models and bottom for signal models"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gene_panel(self, ax, feature):\n        from gffutils.contrib.plotting import Gene\n        extent = [feature.start, feature.stop]\n        nearby_genes = self.db.region(\n            (feature.chrom, feature.start, feature.stop), featuretype='gene')\n        ybase = 0\n        ngenes = 0\n        for nearby_gene in nearby_genes:\n            ngenes += 1\n            extent.extend([nearby_gene.start, nearby_gene.stop])\n            gene_collection = Gene(\n                self.db,\n                nearby_gene,\n                transcripts=['mRNA'],\n                cds=['CDS'],\n                utrs=['exon'],\n                ybase=ybase,\n                color=\"0.5\", picker=5)\n            gene_collection.name = nearby_gene.id\n            gene_collection.add_to_ax(ax)\n            ybase += gene_collection.max_y\n\n        xmin = min(extent)\n        xmax = max(extent)\n        ymax = ngenes\n\n        # 1% padding seems to work well\n        padding = (xmax - xmin) * 0.01\n        ax.axis('tight')\n\n        # add lines indicating extent of current feature\n        vline_kwargs = dict(color='k', linestyle='--')\n        ax.axvline(feature.start, **vline_kwargs)\n        ax.axvline(feature.stop, **vline_kwargs)\n\n        # Make a new feature to represent the region plus surrounding genes\n        interval = pybedtools.create_interval_from_list(feature.fields)\n        interval.start = xmin - padding\n        interval.stop = xmax + padding\n        interval.strand = '.'\n        return interval", "response": "Plot a gene model on an Axes object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simple():\n    MAX_VALUE = 100\n\n    # Create our test progress bar\n    bar = Bar(max_value=MAX_VALUE, fallback=True)\n\n    bar.cursor.clear_lines(2)\n     # Before beginning to draw our bars, we save the position\n    #   of our cursor so we can restore back to this position before writing\n    #   the next time.\n    bar.cursor.save()\n    for i in range(MAX_VALUE + 1):\n        sleep(0.1 * random.random())\n        # We restore the cursor to saved position before writing\n        bar.cursor.restore()\n        # Now we draw the bar\n        bar.draw(value=i)", "response": "Simple example using just the Bar class\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tree():\n\n    #############\n    # Test data #\n    #############\n\n    # For this example, we're obviously going to be feeding fictitious data\n    #   to ProgressTree, so here it is\n    leaf_values = [Value(0) for i in range(6)]\n    bd_defaults = dict(type=Bar, kwargs=dict(max_value=10))\n\n    test_d = {\n        \"Warp Jump\": {\n            \"1) Prepare fuel\": {\n                \"Load Tanks\": {\n                    \"Tank 1\": BarDescriptor(value=leaf_values[0], **bd_defaults),\n                    \"Tank 2\": BarDescriptor(value=leaf_values[1], **bd_defaults),\n                },\n                \"Refine tylium ore\": BarDescriptor(\n                    value=leaf_values[2], **bd_defaults\n                ),\n            },\n            \"2) Calculate jump co-ordinates\": {\n                \"Resolve common name to co-ordinates\": {\n                    \"Querying resolution from baseship\": BarDescriptor(\n                        value=leaf_values[3], **bd_defaults\n                    ),\n                },\n            },\n            \"3) Perform jump\": {\n                \"Check FTL drive readiness\": BarDescriptor(\n                    value=leaf_values[4], **bd_defaults\n                ),\n                \"Juuuuuump!\": BarDescriptor(value=leaf_values[5],\n                                            **bd_defaults)\n            }\n        }\n    }\n\n    # We'll use this function to bump up the leaf values\n    def incr_value(obj):\n        for val in leaf_values:\n            if val.value < 10:\n                val.value += 1\n                break\n\n    # And this to check if we're to stop drawing\n    def are_we_done(obj):\n        return all(val.value == 10 for val in leaf_values)\n\n    ###################\n    # The actual code #\n    ###################\n\n    # Create blessings.Terminal instance\n    t = Terminal()\n    # Initialize a ProgressTree instance\n    n = ProgressTree(term=t)\n    # We'll use the make_room method to make sure the terminal\n    #   is filled out with all the room we need\n    n.make_room(test_d)\n\n    while not are_we_done(test_d):\n        sleep(0.2 * random.random())\n        # After the cursor position is first saved (in the first draw call)\n        #   this will restore the cursor back to the top so we can draw again\n        n.cursor.restore()\n        # We use our incr_value method to bump the fake numbers\n        incr_value(test_d)\n        # Actually draw out the bars\n        n.draw(test_d, BarDescriptor(bd_defaults))", "response": "Example showing tree progress view"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the mean and 95% ci for the given array on the given axes", "response": "def ci_plot(x, arr, conf=0.95, ax=None, line_kwargs=None, fill_kwargs=None):\n    \"\"\"\n    Plots the mean and 95% ci for the given array on the given axes\n\n    Parameters\n    ----------\n    x : 1-D array-like\n        x values for the plot\n\n    arr : 2-D array-like\n        The array to calculate mean and std for\n\n    conf : float [.5 - 1]\n        Confidence interval to use\n\n    ax : matplotlib.Axes\n        The axes object on which to plot\n\n    line_kwargs : dict\n        Additional kwargs passed to Axes.plot\n\n    fill_kwargs : dict\n        Additiona kwargs passed to Axes.fill_between\n    \"\"\"\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n\n    line_kwargs = line_kwargs or {}\n    fill_kwargs = fill_kwargs or {}\n\n    m, lo, hi = ci(arr, conf)\n    ax.plot(x, m, **line_kwargs)\n    ax.fill_between(x, lo, hi, **fill_kwargs)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to help with plotting heatmaps of a single array.", "response": "def imshow(arr, x=None, ax=None, vmin=None, vmax=None, percentile=True,\n           strip=False, features=None, conf=0.95, sort_by=None,\n           line_kwargs=None, fill_kwargs=None, imshow_kwargs=None, figsize=(5, 12),\n           width_ratios=(4, 1), height_ratios=(4, 1),\n           subplot_params=dict(wspace=0.1, hspace=0.1),\n           subset_by=None, subset_order=None,):\n    \"\"\"\n    Do-it-all function to help with plotting heatmaps\n\n    Parameters\n    ----------\n    arr : array-like\n\n    x : 1D array\n        X values to use.  If None, use range(arr.shape[1])\n\n    ax : matplotlib.Axes\n        If not None, then only plot the array on the provided axes.  This will\n        ignore any additional arguments provided that apply to figure-level\n        configuration or to the average line plot.  For example, `figsize`,\n        `width_ratios`, `height_ratios`, `subplot_params`, `line_kwargs`, and\n        `fill_kwargs` will all be ignored.\n\n    vmin, vmax : float\n\n    percentile : bool\n        If True, then treat values for `vmin` and `vmax` as percentiles rather\n        than absolute values.\n\n    strip : bool\n        Include a strip plot alongside the array\n\n    features : pybedtools.BedTool or string filename\n        Features used to construct the array\n\n    conf : float\n        Confidence interval to use in line plot.\n\n    sort_by : array-like\n        Use the provided array to sort the array (e.g., an array of expression\n        values).  This array will be argsorted to get the proper order.\n\n    line_kwargs, fill_kwargs : dict\n        Passed directly to `ci_plot`.\n\n    figsize : tuple\n        (Width, height) of the figure to create.\n\n    imshow_kwargs : dict\n        Passed directly to matplotlib.pyplot.imshow.  By default, arguments\n        used are `origin='lower'`, `aspect=\"auto\"` and a colormap from\n        colormap_adjust.smart_colormap generated using the provided `vmin` and\n        `vmax`.\n\n    width_ratios, height_ratios: tuple\n        These tuples are passed to the `new_shell` function.  The default\n        values set up a 2x2 configuration of panels for heatmap, line plot,\n        colorbar axes, and optional strip plot.  However modifying\n        `width_ratios` or `height_ratios` can be used to create more or fewer panels.\n\n    subplot_params : dict\n        Passed to Figure.subplots_adjust\n\n    subset_by : array\n        An array of any type (but usually int or str) that contains a class\n        label for each row in the heatmap array.  For example, to subset by\n        expression, an array the values of \"up\", \"down\", or \"unchanged\" at each\n        of the positions could be provided.\n\n        Note that the heatmap array is first sorted by `sort_by` and then split\n        into groups according to `subset_by`, so each subset remains sorted by\n        `sort_by`.\n\n    subset_order : list-like\n        This provides the order in which the subsets are plotted.  Since the\n        default imshow arguments contain `origin=\"lower\"`, these will be\n        plotted in order starting at the bottom of the heatmap.\n\n    \"\"\"\n    if ax is None:\n        fig = new_shell(\n            figsize=figsize,\n            strip=strip,\n            subplot_params=subplot_params,\n            width_ratios=width_ratios,\n            height_ratios=height_ratios)\n\n    if x is None:\n        x = np.arange(arr.shape[1] + 1)\n\n    if percentile:\n        if vmin is None:\n            vmin = arr.min()\n        else:\n            vmin = mlab.prctile(arr.ravel(), vmin)\n        if vmax is None:\n            vmax = arr.max()\n        else:\n            vmax = mlab.prctile(arr.ravel(), vmax)\n    else:\n        if vmin is None:\n            vmin = arr.min()\n        if vmax is None:\n            vmax = arr.max()\n\n    cmap = colormap_adjust.smart_colormap(vmin, vmax)\n    _imshow_kwargs = dict(origin='lower', cmap=cmap, vmin=vmin, vmax=vmax,\n                          aspect='auto')\n    if imshow_kwargs is not None:\n        _imshow_kwargs.update(imshow_kwargs)\n\n    # previously we did an argsort first; with subsetting we don't want to do\n    # that yet....\n    #if sort_by is not None:\n    #    ind = np.argsort(sort_by)\n    #else:\n    #    ind = np.arange(arr.shape[0])\n\n    if sort_by is None:\n        sort_by = np.arange(arr.shape[0])\n\n    if ax is None:\n        array_ax = fig.array_axes\n    else:\n        array_ax = ax\n\n    # If not provided, assume all in the same subset.\n    if subset_by is None:\n        subset_by = np.zeros(arr.shape[0])\n\n    # Ensure always array, since we're doing indexing tricks\n    if not isinstance(subset_by, np.ndarray):\n        subset_by = np.array(subset_by)\n\n    # If not provided, use sorted order\n    if subset_order is None:\n        subset_order = sorted(np.unique(subset_by))\n\n    inds = []\n    for cls in subset_order:\n        subset_ind = np.nonzero(subset_by == cls)[0]\n        subset_sort_by = sort_by[subset_ind]\n        subset_argsort_by = np.argsort(subset_sort_by)\n        inds.append(subset_ind[subset_argsort_by])\n    ind = np.concatenate(inds)\n\n    mappable = array_ax.imshow(\n        arr[ind, :],\n        extent=(x.min(), x.max(), 0, arr.shape[0]),\n        **_imshow_kwargs\n    )\n\n    if line_kwargs is None:\n        line_kwargs = {}\n    if fill_kwargs is None:\n        fill_kwargs = {}\n\n    if isinstance(line_kwargs, dict):\n        line_kwargs = [line_kwargs]\n    if isinstance(fill_kwargs, dict):\n        fill_kwargs = [fill_kwargs]\n\n    _line_kwargs = itertools.cycle(line_kwargs)\n    _fill_kwargs = itertools.cycle(fill_kwargs)\n\n    if ax is None:\n        plt.colorbar(mappable, fig.cax)\n        for subset_ind, label, _lkw, _fkw in zip(inds, subset_order, _line_kwargs, _fill_kwargs):\n            ci_plot(\n                x,\n                arr[subset_ind],\n                ax=fig.line_axes,\n                line_kwargs=_lkw,\n                fill_kwargs=_fkw,\n            )\n        return fig\n    else:\n        return ax.figure"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the limits for a group of arrays in a flexible manner.", "response": "def calculate_limits(array_dict, method='global', percentiles=None, limit=()):\n    \"\"\"\n    Calculate limits for a group of arrays in a flexible manner.\n\n    Returns a dictionary of calculated (vmin, vmax), with the same keys as\n    `array_dict`.\n\n    Useful for plotting heatmaps of multiple datasets, and the vmin/vmax values\n    of the colormaps need to be matched across all (or a subset) of heatmaps.\n\n    Parameters\n    ----------\n    array_dict : dict of np.arrays\n\n    method : {'global', 'independent', callable}\n        If method=\"global\", then use the global min/max values across all\n        arrays in array_dict.  If method=\"independent\", then each array will\n        have its own min/max calcuated.  If a callable, then it will be used to\n        group the keys of `array_dict`, and each group will have its own\n        group-wise min/max calculated.\n\n\n    percentiles : None or list\n        If not None, a list of (lower, upper) percentiles in the range [0,100].\n    \"\"\"\n    if percentiles is not None:\n        for percentile in percentiles:\n            if not 0 <= percentile <= 100:\n                raise ValueError(\"percentile (%s) not between [0, 100]\")\n\n    if method == 'global':\n        all_arrays = np.concatenate(\n            [i.ravel() for i in array_dict.itervalues()]\n        )\n        if percentiles:\n            vmin = mlab.prctile(\n                all_arrays, percentiles[0])\n            vmax = mlab.prctile(\n                all_arrays, percentiles[1])\n\n        else:\n            vmin = all_arrays.min()\n            vmax = all_arrays.max()\n        d = dict([(i, (vmin, vmax)) for i in array_dict.keys()])\n\n    elif method == 'independent':\n        d = {}\n        for k, v in array_dict.iteritems():\n            d[k] = (v.min(), v.max())\n\n    elif hasattr(method, '__call__'):\n        d = {}\n        sorted_keys = sorted(array_dict.keys(), key=method)\n        for group, keys in itertools.groupby(sorted_keys, method):\n            keys = list(keys)\n            all_arrays = np.concatenate([array_dict[i] for i in keys])\n            if percentiles:\n                vmin = mlab.prctile(\n                    all_arrays, percentiles[0])\n                vmax = mlab.prctile(\n                    all_arrays, percentiles[1])\n            else:\n                vmin = all_arrays.min()\n                vmax = all_arrays.max()\n            for key in keys:\n                d[key] = (vmin, vmax)\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nice_log(x):\n    neg = x < 0\n    xi = np.log2(np.abs(x) + 1)\n    xi[neg] = -xi[neg]\n    return xi", "response": "Uses a log scale but with negative numbers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the target identification from profiles ( TIP ) zscores from Cheng et al. 2001 Bioinformatics 27 ( 23 ) 3221 - 3227.", "response": "def tip_zscores(a):\n    \"\"\"\n    Calculates the \"target identification from profiles\" (TIP) zscores\n    from Cheng et al. 2001, Bioinformatics 27(23):3221-3227.\n\n    :param a: NumPy array, where each row is the signal for a feature.\n    \"\"\"\n    weighted = a * a.mean(axis=0)\n    scores = weighted.sum(axis=1)\n    zscores = (scores - scores.mean()) / scores.std()\n    return zscores"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fdrcorrection(pvals, alpha=0.05, method='indep'):\n    '''\n    NOTE: This function was copied from\n    statsmodels.sandbox.stats.multicomp.fdrcorrection0, from statsmodels\n    version 0.5.0.\n\n    This is to avoid requiring all of statsmodels to be a dependency for\n    metaseq, just for this function.\n\n\n\n\n    pvalue correction for false discovery rate\n\n    This covers Benjamini/Hochberg for independent or positively correlated and\n    Benjamini/Yekutieli for general or negatively correlated tests. Both are\n    available in the function multipletests, as method=`fdr_bh`, resp.\n    `fdr_by`.\n\n    Parameters\n    ----------\n    pvals : array_like\n        set of p-values of the individual tests.\n    alpha : float\n        error rate\n    method : {'indep', 'negcorr')\n\n    Returns\n    -------\n    rejected : array, bool\n        True if a hypothesis is rejected, False if not\n    pvalue-corrected : array\n        pvalues adjusted for multiple hypothesis testing to limit FDR\n\n    Notes\n    -----\n\n    If there is prior information on the fraction of true hypothesis, then\n    alpha should be set to alpha * m/m_0 where m is the number of tests, given\n    by the p-values, and m_0 is an estimate of the true hypothesis.  (see\n    Benjamini, Krieger and Yekuteli)\n\n    The two-step method of Benjamini, Krieger and Yekutiel that estimates the\n    number of false hypotheses will be available (soon).\n\n    Method names can be abbreviated to first letter, 'i' or 'p' for fdr_bh and\n    'n' for fdr_by.\n\n    '''\n    pvals = np.asarray(pvals)\n\n    pvals_sortind = np.argsort(pvals)\n    pvals_sorted = pvals[pvals_sortind]\n    sortrevind = pvals_sortind.argsort()\n\n    if method in ['i', 'indep', 'p', 'poscorr']:\n        ecdffactor = _ecdf(pvals_sorted)\n    elif method in ['n', 'negcorr']:\n        cm = np.sum(1./np.arange(1, len(pvals_sorted)+1))  # corrected this\n        ecdffactor = _ecdf(pvals_sorted) / cm\n#    elif method in ['n', 'negcorr']:\n#        cm = np.sum(np.arange(len(pvals)))\n#        ecdffactor = ecdf(pvals_sorted)/cm\n    else:\n        raise ValueError('only indep and necorr implemented')\n    reject = pvals_sorted <= ecdffactor*alpha\n    if reject.any():\n        rejectmax = max(np.nonzero(reject)[0])\n        reject[:rejectmax] = True\n\n    pvals_corrected_raw = pvals_sorted / ecdffactor\n    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n    pvals_corrected[pvals_corrected > 1] = 1\n    return reject[sortrevind], pvals_corrected[sortrevind]", "response": "This function calculates the FDR for a set of p - values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tip_fdr(a, alpha=0.05):\n    zscores = tip_zscores(a)\n    pvals = stats.norm.pdf(zscores)\n    rejected, fdrs = fdrcorrection(pvals)\n    return fdrs", "response": "Returns adjusted TIP p - values for a particular alpha."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_logged(x, y):\n    xi = np.log2(x)\n    yi = np.log2(y)\n\n    xv = np.isfinite(xi)\n    yv = np.isfinite(yi)\n\n    global_min = min(xi[xv].min(), yi[yv].min())\n    global_max = max(xi[xv].max(), yi[yv].max())\n\n    xi[~xv] = global_min\n    yi[~yv] = global_min\n\n    return xi, yi", "response": "This function transform x and y to a log scale while dealing with zeros."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matrix_and_line_shell(figsize=(5, 12), strip=False):\n    fig = plt.figure(figsize=figsize)\n\n    # Constants to keep track\n    if strip:\n        STRIP_COLS = 1\n    else:\n        STRIP_COLS = 0\n    ROWS = 4\n    COLS = 8 + STRIP_COLS\n    MAT_COLS = 7\n    MAT_ROWS = 3\n    LINE_ROWS = ROWS - MAT_ROWS\n\n    mat_ax = plt.subplot2grid(\n        shape=(ROWS, COLS),\n        loc=(0, STRIP_COLS),\n        rowspan=MAT_ROWS,\n        colspan=MAT_COLS,\n    )\n\n    line_ax = plt.subplot2grid(\n        shape=(ROWS, COLS),\n        loc=(MAT_ROWS, STRIP_COLS),\n        rowspan=LINE_ROWS,\n        colspan=MAT_COLS,\n        sharex=mat_ax)\n\n    if strip:\n        strip_ax = plt.subplot2grid(\n            shape=(ROWS, COLS),\n            loc=(0, 0),\n            rowspan=MAT_ROWS,\n            colspan=STRIP_COLS,\n            sharey=mat_ax,\n        )\n    else:\n        strip_ax = None\n\n    cax = plt.subplot2grid(\n        shape=(ROWS, COLS),\n        loc=(ROWS - MAT_ROWS, MAT_COLS + STRIP_COLS),\n        rowspan=1,\n        colspan=1,\n    )\n\n    fig.subplots_adjust(hspace=0.1, wspace=0.2, right=0.88, left=0.23)\n    return fig, mat_ax, line_ax, strip_ax, cax", "response": "Helper function to construct an empty figure that has space for a matrix line and colorbar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clustered_sortind(x, k=10, scorefunc=None):\n    try:\n        from sklearn.cluster import MiniBatchKMeans\n    except ImportError:\n        raise ImportError('please install scikits.learn for '\n                          'clustering.')\n\n    # If integer, do it once and we're done\n    if isinstance(k, int):\n        best_k = k\n\n    else:\n        mean_dists = {}\n        for _k in k:\n            mbk = MiniBatchKMeans(init='k-means++', n_clusters=_k)\n            mbk.fit(x)\n            mean_dists[_k] = mbk.transform(x).mean()\n        best_k = sorted(mean_dists.items(), key=lambda x: x[1])[-1][0]\n\n    mbk = MiniBatchKMeans(init='k-means++', n_clusters=best_k)\n    mbk.fit(x)\n    k = best_k\n    labels = mbk.labels_\n    scores = np.zeros(labels.shape, dtype=float)\n\n    if not scorefunc:\n        def scorefunc(x):\n            return x.mean(axis=0).max()\n\n    for label in range(k):\n        ind = labels == label\n        score = scorefunc(x[ind, :])\n        scores[ind] = score\n\n    pos = 0\n    breaks = []\n    ind = np.argsort(scores)\n    for k, g in itertools.groupby(labels[ind]):\n        pos += len(list(g))\n        breaks.append(pos)\n\n    return ind, breaks", "response": "This function uses MiniBatch k - means clustering to cluster matrix into groups and returns the index of the rows of x that are sorted by scorefunc."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing MiniBatch k-means clustering to cluster matrix into groups. Each cluster of rows is then sorted by `scorefunc` -- by default, the max peak height when all rows in a cluster are averaged, or cluster.mean(axis=0).max(). Returns the index that will sort the rows of `x` and a list of \"breaks\". `breaks` is essentially a cumulative row count for each cluster boundary. In other words, after plotting the array you can use axhline on each \"break\" to plot the cluster boundary. If `k` is a list or tuple, iteratively try each one and select the best with the lowest mean distance from cluster centers. :param x: Matrix whose rows are to be clustered :param k: Number of clusters to create or a list of potential clusters; the optimum will be chosen from the list :param row_key: Optional function to act as a sort key for sorting rows within clusters. Signature should be `scorefunc(a)` where `a` is a 1-D NumPy array. :param cluster_key: Optional function for sorting clusters. Signature is `clusterfunc(a)` where `a` is a NumPy array containing all rows of `x` for cluster `i`. It must return a single value.", "response": "def new_clustered_sortind(x, k=10, row_key=None, cluster_key=None):\n    \"\"\"\n    Uses MiniBatch k-means clustering to cluster matrix into groups.\n\n    Each cluster of rows is then sorted by `scorefunc` -- by default, the max\n    peak height when all rows in a cluster are averaged, or\n    cluster.mean(axis=0).max().\n\n    Returns the index that will sort the rows of `x` and a list of \"breaks\".\n    `breaks` is essentially a cumulative row count for each cluster boundary.\n    In other words, after plotting the array you can use axhline on each\n    \"break\" to plot the cluster boundary.\n\n    If `k` is a list or tuple, iteratively try each one and select the best\n    with the lowest mean distance from cluster centers.\n\n    :param x: Matrix whose rows are to be clustered\n    :param k: Number of clusters to create or a list of potential clusters; the\n        optimum will be chosen from the list\n    :param row_key:\n        Optional function to act as a sort key for sorting rows within\n        clusters.  Signature should be `scorefunc(a)` where `a` is a 1-D NumPy\n        array.\n    :param cluster_key:\n        Optional function for sorting clusters.  Signature is `clusterfunc(a)`\n        where `a` is a NumPy array containing all rows of `x` for cluster `i`.\n        It must return a single value.\n    \"\"\"\n    try:\n        from sklearn.cluster import MiniBatchKMeans\n    except ImportError:\n        raise ImportError('please install scikits.learn for '\n                          'clustering.')\n\n    # If integer, do it once and we're done\n    if isinstance(k, int):\n        best_k = k\n\n    else:\n        mean_dists = {}\n        for _k in k:\n            mbk = MiniBatchKMeans(init='k-means++', n_clusters=_k)\n            mbk.fit(x)\n            mean_dists[_k] = mbk.transform(x).mean()\n        best_k = sorted(mean_dists.items(), key=lambda x: x[1])[-1][0]\n\n    mbk = MiniBatchKMeans(init='k-means++', n_clusters=best_k)\n    mbk.fit(x)\n    k = best_k\n    labels = mbk.labels_\n    scores = np.zeros(labels.shape, dtype=float)\n\n    if cluster_key:\n        # It's easier for calling code to provide something that operates on\n        # a cluster level, but here it's converted to work on a label level\n        # that looks in to the array `x`.\n        def _cluster_key(i):\n            return cluster_key(x[labels == i, :])\n        sorted_labels = sorted(range(k), key=_cluster_key)\n    else:\n        # Otherwise just use them as-is.\n        sorted_labels = range(k)\n\n    if row_key:\n        # Again, easier to provide a function to operate on a row.  But here we\n        # need it to accept an index\n        def _row_key(i):\n            return row_key(x[i, :])\n\n    final_ind = []\n    breaks = []\n    pos = 0\n    for label in sorted_labels:\n        # which rows in `x` have this label\n        label_inds = np.nonzero(labels == label)[0]\n        if row_key:\n            label_sort_ind = sorted(label_inds, key=_row_key)\n        else:\n            label_sort_ind = label_inds\n        for li in label_sort_ind:\n            final_ind.append(li)\n        pos += len(label_inds)\n        breaks.append(pos)\n\n    return np.array(final_ind), np.array(breaks)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a copy of a node with a new node.", "response": "def _updatecopy(orig, update_with, keys=None, override=False):\n    \"\"\"\n    Update a copy of dest with source.  If `keys` is a list, then only update\n    with those keys.\n    \"\"\"\n    d = orig.copy()\n    if keys is None:\n        keys = update_with.keys()\n    for k in keys:\n        if k in update_with:\n            if k in d and not override:\n                continue\n            d[k] = update_with[k]\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new scatter to the current scatterplot.", "response": "def append(self, x, y, scatter_kwargs, hist_kwargs=None, xhist_kwargs=None,\n               yhist_kwargs=None, num_ticks=3, labels=None, hist_share=False,\n               marginal_histograms=True):\n        \"\"\"\n        Adds a new scatter to self.scatter_ax as well as marginal histograms\n        for the same data, borrowing addtional room from the axes.\n\n        Parameters\n        ----------\n\n        x, y : array-like\n            Data to be plotted\n\n        scatter_kwargs : dict\n            Keyword arguments that are passed directly to scatter().\n\n        hist_kwargs : dict\n            Keyword arguments that are passed directly to hist(), for both the\n            top and side histograms.\n\n        xhist_kwargs, yhist_kwargs : dict\n            Additional, margin-specific kwargs for the x or y histograms\n            respectively.  These are used to update `hist_kwargs`\n\n        num_ticks : int\n            How many tick marks to use in each histogram's y-axis\n\n        labels : array-like\n            Optional NumPy array of labels that will be set on the collection\n            so that they can be accessed by a callback function.\n\n        hist_share : bool\n            If True, then all histograms will share the same frequency axes.\n            Useful for showing relative heights if you don't want to use the\n            hist_kwarg `normed=True`\n\n        marginal_histograms : bool\n            Set to False in order to disable marginal histograms and just use\n            as a normal scatterplot.\n        \"\"\"\n        scatter_kwargs = scatter_kwargs or {}\n        hist_kwargs = hist_kwargs or {}\n        xhist_kwargs = xhist_kwargs or {}\n        yhist_kwargs = yhist_kwargs or {}\n        yhist_kwargs.update(dict(orientation='horizontal'))\n\n        # Plot the scatter\n        coll = self.scatter_ax.scatter(x, y, **scatter_kwargs)\n        coll.labels = labels\n\n        if not marginal_histograms:\n            return\n\n        xhk = _updatecopy(hist_kwargs, xhist_kwargs)\n        yhk = _updatecopy(hist_kwargs, yhist_kwargs)\n\n        axhistx = self.divider.append_axes(\n            'top', size=self.hist_size,\n            pad=self.pad, sharex=self.scatter_ax, sharey=self.xfirst_ax)\n\n        axhisty = self.divider.append_axes(\n            'right', size=self.hist_size,\n            pad=self.pad, sharey=self.scatter_ax, sharex=self.yfirst_ax)\n\n        axhistx.yaxis.set_major_locator(\n            MaxNLocator(nbins=num_ticks, prune='both'))\n\n        axhisty.xaxis.set_major_locator(\n            MaxNLocator(nbins=num_ticks, prune='both'))\n\n        if not self.xfirst_ax and hist_share:\n            self.xfirst_ax = axhistx\n\n        if not self.yfirst_ax and hist_share:\n            self.yfirst_ax = axhisty\n\n        # Keep track of which axes are which, because looking into fig.axes\n        # list will get awkward....\n        self.top_hists.append(axhistx)\n        self.right_hists.append(axhisty)\n\n        # Scatter will deal with NaN, but hist will not.  So clean the data\n        # here.\n        hx = _clean(x)\n        hy = _clean(y)\n\n        self.hxs.append(hx)\n        self.hys.append(hy)\n\n        # Only plot hists if there's valid data\n        if len(hx) > 0:\n            if len(hx) == 1:\n                _xhk = _updatecopy(orig=xhk, update_with=dict(bins=[hx[0], hx[0]]), keys=['bins'])\n                axhistx.hist(hx, **_xhk)\n            else:\n                axhistx.hist(hx, **xhk)\n        if len(hy) > 0:\n            if len(hy) == 1:\n                _yhk = _updatecopy(orig=yhk, update_with=dict(bins=[hy[0], hy[0]]), keys=['bins'])\n                axhisty.hist(hy, **_yhk)\n            else:\n                axhisty.hist(hy, **yhk)\n\n        # Turn off unnecessary labels -- for these, use the scatter's axes\n        # labels\n        for txt in axhisty.get_yticklabels() + axhistx.get_xticklabels():\n            txt.set_visible(False)\n\n        for txt in axhisty.get_xticklabels():\n            txt.set_rotation(-90)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_legends(self, xhists=True, yhists=False, scatter=True, **kwargs):\n        axs = []\n        if xhists:\n            axs.extend(self.hxs)\n        if yhists:\n            axs.extend(self.hys)\n        if scatter:\n            axs.extend(self.ax)\n\n        for ax in axs:\n            ax.legend(**kwargs)", "response": "Add legends to axes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new genomic signal object for the given file format.", "response": "def genomic_signal(fn, kind):\n    \"\"\"\n    Factory function that makes the right class for the file format.\n\n    Typically you'll only need this function to create a new genomic signal\n    object.\n\n    :param fn: Filename\n    :param kind:\n        String.  Format of the file; see\n        metaseq.genomic_signal._registry.keys()\n    \"\"\"\n    try:\n        klass = _registry[kind.lower()]\n    except KeyError:\n        raise ValueError(\n            'No support for %s format, choices are %s'\n            % (kind, _registry.keys()))\n    m = klass(fn)\n    m.kind = kind\n    return m"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef array(self, features, processes=None, chunksize=1, ragged=False,\n              **kwargs):\n        \"\"\"\n        Creates an MxN NumPy array of genomic signal for the region defined by\n        each feature in `features`, where M=len(features) and N=(bins or\n        feature length)\n\n        Parameters\n        ----------\n        features : iterable of interval-like objects\n            An iterable of interval-like objects; see docstring for\n            `local_coverage` method for more details.\n\n        processes : int or None\n            If not None, then create the array in parallel, giving each process\n            chunks of length `chunksize` to work on.\n\n        chunksize : int\n            `features` will be split into `chunksize` pieces, and each piece\n            will be given to a different process. The optimum value is\n            dependent on the size of the features and the underlying data set,\n            but `chunksize=100` is a good place to start.\n\n        ragged : bool\n            If False (default), then return a 2-D NumPy array.  This requires\n            all rows to have the same number of columns, which you get when\n            supplying `bins` or if all features are of uniform length.  If\n            True, then return a list of 1-D NumPy arrays\n\n        Notes\n        -----\n        Additional keyword args are passed to local_coverage() which performs\n        the work for each feature; see that method for more details.\n        \"\"\"\n        if processes is not None:\n            arrays = _array_parallel(\n                self.fn, self.__class__, features, processes=processes,\n                chunksize=chunksize, **kwargs)\n        else:\n            arrays = _array(self.fn, self.__class__, features, **kwargs)\n        if not ragged:\n            stacked_arrays = np.row_stack(arrays)\n            del arrays\n            return stacked_arrays\n        else:\n            return arrays", "response": "Creates an array of genomic signal for the region defined by this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of the genome information for the current BAM file.", "response": "def genome(self):\n        \"\"\"\n        \"genome\" dictionary ready for pybedtools, based on the BAM header.\n        \"\"\"\n        # This gets the underlying pysam Samfile object\n        f = self.adapter.fileobj\n        d = {}\n        for ref, length in zip(f.references, f.lengths):\n            d[ref] = (0, length)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount the number of reads in a BAM file.", "response": "def mapped_read_count(self, force=False):\n        \"\"\"\n        Counts total reads in a BAM file.\n\n        If a file self.bam + '.scale' exists, then just read the first line of\n        that file that doesn't start with a \"#\".  If such a file doesn't exist,\n        then it will be created with the number of reads as the first and only\n        line in the file.\n\n        The result is also stored in self._readcount so that the time-consuming\n        part only runs once; use force=True to force re-count.\n\n        Parameters\n        ----------\n        force : bool\n            If True, then force a re-count; otherwise use cached data if\n            available.\n        \"\"\"\n        # Already run?\n        if self._readcount and not force:\n            return self._readcount\n\n        if os.path.exists(self.fn + '.mmr') and not force:\n            for line in open(self.fn + '.mmr'):\n                if line.startswith('#'):\n                    continue\n                self._readcount = float(line.strip())\n                return self._readcount\n\n        cmds = ['samtools',\n                'view',\n                '-c',\n                '-F', '0x4',\n                self.fn]\n        p = subprocess.Popen(\n            cmds, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if stderr:\n            sys.stderr.write('samtools says: %s' % stderr)\n            return None\n        mapped_reads = int(stdout)\n\n        # write to file so the next time you need the lib size you can access\n        # it quickly\n        if not os.path.exists(self.fn + '.mmr'):\n            fout = open(self.fn + '.mmr', 'w')\n            fout.write(str(mapped_reads) + '\\n')\n            fout.close()\n\n        self._readcount = mapped_reads\n        return self._readcount"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint a 2x2 table.", "response": "def print_2x2_table(table, row_labels, col_labels, fmt=\"%d\"):\n    \"\"\"\n    Prints a table used for Fisher's exact test. Adds row, column, and grand\n    totals.\n\n    :param table: The four cells of a 2x2 table: [r1c1, r1c2, r2c1, r2c2]\n    :param row_labels: A length-2 list of row names\n    :param col_labels: A length-2 list of column names\n\n    \"\"\"\n    grand = sum(table)\n\n    # Separate table into components and get row/col sums\n    t11, t12, t21, t22 = table\n\n    # Row sums, col sums, and grand total\n    r1 = t11 + t12\n    r2 = t21 + t22\n    c1 = t11 + t21\n    c2 = t12 + t22\n\n    # Re-cast everything as the appropriate format\n    t11, t12, t21, t22, c1, c2, r1, r2, grand = [\n        fmt % i for i in [t11, t12, t21, t22, c1, c2, r1, r2, grand]]\n\n    # Construct rows and columns the long way...\n    rows = [\n        [\"\"] + col_labels + ['total'],\n        [row_labels[0], t11, t12, r1],\n        [row_labels[1], t21, t22, r2],\n        ['total', c1, c2, grand],\n    ]\n\n    cols = [\n        [row[0] for row in rows],\n        [col_labels[0], t11, t21, c1],\n        [col_labels[1], t12, t22, c2],\n        ['total', r1, r2, grand],\n    ]\n\n    # Get max column width for each column; need this for nice justification\n    widths = []\n    for col in cols:\n        widths.append(max(len(i) for i in col))\n\n    # ReST-formatted header\n    sep = ['=' * i for i in widths]\n\n    # Construct the table one row at a time with nice justification\n    s = []\n    s.append(' '.join(sep))\n    s.append(' '.join(i.ljust(j) for i, j in zip(rows[0], widths)))\n    s.append(' '.join(sep))\n    for row in rows[1:]:\n        s.append(' '.join(i.ljust(j) for i, j in zip(row, widths)))\n    s.append(' '.join(sep) + '\\n')\n    return \"\\n\".join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints a table with row percentages rather than the totals", "response": "def print_row_perc_table(table, row_labels, col_labels):\n    \"\"\"\n    given a table, print the percentages rather than the totals\n    \"\"\"\n    r1c1, r1c2, r2c1, r2c2 = map(float, table)\n    row1 = r1c1 + r1c2\n    row2 = r2c1 + r2c2\n\n    blocks = [\n        (r1c1, row1),\n        (r1c2, row1),\n        (r2c1, row2),\n        (r2c2, row2)]\n\n    new_table = []\n\n    for cell, row in blocks:\n        try:\n            x = cell / row\n        except ZeroDivisionError:\n            x = 0\n        new_table.append(x)\n\n    s = print_2x2_table(new_table, row_labels, col_labels, fmt=\"%.2f\")\n    s = s.splitlines(True)\n    del s[5]\n    return ''.join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_col_perc_table(table, row_labels, col_labels):\n    r1c1, r1c2, r2c1, r2c2 = map(float, table)\n    col1 = r1c1 + r2c1\n    col2 = r1c2 + r2c2\n\n\n    blocks = [\n        (r1c1, col1),\n        (r1c2, col2),\n        (r2c1, col1),\n        (r2c2, col2)]\n\n    new_table = []\n\n    for cell, row in blocks:\n        try:\n            x = cell / row\n        except ZeroDivisionError:\n            x = 0\n        new_table.append(x)\n\n    s = print_2x2_table(new_table, row_labels, col_labels, fmt=\"%.2f\")\n    s = s.splitlines(False)\n    last_space = s[0].rindex(\" \")\n    new_s = [i[:last_space] for i in s]\n    return '\\n'.join(new_s)", "response": "print the col_perc_table given a table print the cols as percentages\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a table of items for a given subset.", "response": "def table_maker(subset, ind1, ind2, row_labels, col_labels, title):\n    \"\"\"\n    `subset` provides a subsetted boolean of items to consider.  If no subset,\n    you can use all with `np.ones_like(ind1) == 1`\n\n    `ind1` is used to subset rows, e.g., log2fc > 0.  This is used for rows, so\n    row_label might be ['upregulated', 'others']\n\n    `ind2` is used to subset cols.  For example, col_labels would be\n    ['bound', 'unbound']\n    \"\"\"\n    table = [\n        sum(subset & ind1 & ind2),\n        sum(subset & ind1 & ~ind2),\n        sum(subset & ~ind1 & ind2),\n        sum(subset & ~ind1 & ~ind2)\n    ]\n    print\n    print title\n    print '-' * len(title)\n    print print_2x2_table(table, row_labels=row_labels, col_labels=col_labels)\n    print print_row_perc_table(\n        table, row_labels=row_labels, col_labels=col_labels)\n    print print_col_perc_table(\n        table, row_labels=row_labels, col_labels=col_labels)\n    print fisher.pvalue(*table)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw(self, tree, bar_desc=None, save_cursor=True, flush=True):\n        if save_cursor:\n            self.cursor.save()\n\n        tree = deepcopy(tree)\n        # TODO: Automatically collapse hierarchy so something\n        #   will always be displayable (well, unless the top-level)\n        #   contains too many to display\n        lines_required = self.lines_required(tree)\n        ensure(lines_required <= self.cursor.term.height,\n               LengthOverflowError,\n               \"Terminal is not long ({} rows) enough to fit all bars \"\n               \"({} rows).\".format(self.cursor.term.height, lines_required))\n        bar_desc = BarDescriptor(type=Bar) if not bar_desc else bar_desc\n        self._calculate_values(tree, bar_desc)\n        self._draw(tree)\n        if flush:\n            self.cursor.flush()", "response": "Draw the tree to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_room(self, tree):\n        lines_req = self.lines_required(tree)\n        self.cursor.clear_lines(lines_req)", "response": "Clear lines in terminal below current cursor position as required\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate number of lines required to draw tree", "response": "def lines_required(self, tree, count=0):\n        \"\"\"Calculate number of lines required to draw ``tree``\"\"\"\n        if all([\n            isinstance(tree, dict),\n            type(tree) != BarDescriptor\n        ]):\n            return sum(self.lines_required(v, count=count)\n                       for v in tree.values()) + 2\n        elif isinstance(tree, BarDescriptor):\n            if tree.get(\"kwargs\", {}).get(\"title_pos\") in [\"left\", \"right\"]:\n                return 1\n            else:\n                return 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calculate_values(self, tree, bar_d):\n        if all([\n            isinstance(tree, dict),\n            type(tree) != BarDescriptor\n        ]):\n            # Calculate value and max_value\n            max_val = 0\n            value = 0\n            for k in tree:\n                # Get descriptor by recursing\n                bar_desc = self._calculate_values(tree[k], bar_d)\n                # Reassign to tuple of (new descriptor, tree below)\n                tree[k] = (bar_desc, tree[k])\n                value += bar_desc[\"value\"].value\n                max_val += bar_desc.get(\"kwargs\", {}).get(\"max_value\", 100)\n            # Merge in values from ``bar_d`` before returning descriptor\n            kwargs = merge_dicts(\n                [bar_d.get(\"kwargs\", {}),\n                 dict(max_value=max_val)],\n                deepcopy=True\n            )\n            ret_d = merge_dicts(\n                [bar_d,\n                 dict(value=Value(floor(value)), kwargs=kwargs)],\n                deepcopy=True\n            )\n            return BarDescriptor(ret_d)\n        elif isinstance(tree, BarDescriptor):\n            return tree\n        else:\n            raise TypeError(\"Unexpected type {}\".format(type(tree)))", "response": "Calculate values for drawing bars of non - leafs in tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_dicts(dicts, deepcopy=False):\n    assert isinstance(dicts, list) and all(isinstance(d, dict) for d in dicts)\n    return dict(chain(*[copy.deepcopy(d).items() if deepcopy else d.items()\n                        for d in dicts]))", "response": "Merges a list of dicts into a single dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the features and NumPy arrays that were saved with save_features_and_arrays.", "response": "def load_features_and_arrays(prefix, mmap_mode='r'):\n    \"\"\"\n    Returns the features and NumPy arrays that were saved with\n    save_features_and_arrays.\n\n    Parameters\n    ----------\n\n    prefix : str\n        Path to where data are saved\n\n    mmap_mode : {None, 'r+', 'r', 'w+', 'c'}\n        Mode in which to memory-map the file.  See np.load for details.\n    \"\"\"\n    features = pybedtools.BedTool(prefix + '.features')\n    arrays = np.load(prefix + '.npz', mmap_mode=mmap_mode)\n    return features, arrays"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the features and arrays to files for later use.", "response": "def save_features_and_arrays(features, arrays, prefix, compressed=False,\n                             link_features=False, overwrite=False):\n    \"\"\"\n    Saves NumPy arrays of processed data, along with the features that\n    correspond to each row, to files for later use.\n\n    Two files will be saved, both starting with `prefix`:\n\n        prefix.features : a file of features.  If GFF features were provided,\n        this will be in GFF format, if BED features were provided it will be in\n        BED format, and so on.\n\n        prefix.npz : A NumPy .npz file.\n\n    Parameters\n    ----------\n    arrays : dict of NumPy arrays\n        Rows in each array should correspond to `features`.  This dictionary is\n        passed to np.savez\n\n    features : iterable of Feature-like objects\n        This is usually the same features that were used to create the array in\n        the first place.\n\n    link_features : bool\n        If True, then assume that `features` is either a pybedtools.BedTool\n        pointing to a file, or a filename.  In this case, instead of making\n        a copy, a symlink will be created to the original features.  This helps\n        save disk space.\n\n    prefix : str\n        Path to where data will be saved.\n\n    compressed : bool\n        If True, saves arrays using np.savez_compressed rather than np.savez.\n        This will save disk space, but will be slower when accessing the data\n        later.\n    \"\"\"\n\n    if link_features:\n        if isinstance(features, pybedtools.BedTool):\n            assert isinstance(features.fn, basestring)\n            features_filename = features.fn\n        else:\n            assert isinstance(features, basestring)\n            features_filename = features\n\n        if overwrite:\n            force_flag = '-f'\n        else:\n            force_flag = ''\n\n        cmds = [\n            'ln', '-s', force_flag, os.path.abspath(features_filename), prefix + '.features']\n        os.system(' '.join(cmds))\n    else:\n        pybedtools.BedTool(features).saveas(prefix + '.features')\n\n    if compressed:\n        np.savez_compressed(\n            prefix,\n            **arrays)\n    else:\n        np.savez(prefix, **arrays)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_all(fritz, args):\n    devices = fritz.get_devices()\n\n    for device in devices:\n        print('#' * 30)\n        print('name=%s' % device.name)\n        print('  ain=%s' % device.ain)\n        print('  id=%s' % device.identifier)\n        print('  productname=%s' % device.productname)\n        print('  manufacturer=%s' % device.manufacturer)\n        print(\"  present=%s\" % device.present)\n        print(\"  lock=%s\" % device.lock)\n        print(\"  devicelock=%s\" % device.device_lock)\n\n        if device.present is False:\n            continue\n\n        if device.has_switch:\n            print(\" Switch:\")\n            print(\"  switch_state=%s\" % device.switch_state)\n        if device.has_switch:\n            print(\" Powermeter:\")\n            print(\"  power=%s\" % device.power)\n            print(\"  energy=%s\" % device.energy)\n            print(\"  voltage=%s\" % device.voltage)\n        if device.has_temperature_sensor:\n            print(\" Temperature:\")\n            print(\"  temperature=%s\" % device.temperature)\n            print(\"  offset=%s\" % device.offset)\n        if device.has_thermostat:\n            print(\" Thermostat:\")\n            print(\"  battery_low=%s\" % device.battery_low)\n            print(\"  battery_level=%s\" % device.battery_level)\n            print(\"  actual=%s\" % device.actual_temperature)\n            print(\"  target=%s\" % device.target_temperature)\n            print(\"  comfort=%s\" % device.comfort_temperature)\n            print(\"  eco=%s\" % device.eco_temperature)\n            print(\"  window=%s\" % device.window_open)\n            print(\"  summer=%s\" % device.summer_active)\n            print(\"  holiday=%s\" % device.holiday_active)\n        if device.has_alarm:\n            print(\" Alert:\")\n            print(\"  alert=%s\" % device.alert_state)", "response": "Command that prints all device information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef device_statistics(fritz, args):\n    stats = fritz.get_device_statistics(args.ain)\n    print(stats)", "response": "Command that prints the device statistics."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chunker(f, n):\n    f = iter(f)\n    x = []\n    while 1:\n        if len(x) < n:\n            try:\n                x.append(f.next())\n            except StopIteration:\n                if len(x) > 0:\n                    yield tuple(x)\n                break\n        else:\n            yield tuple(x)\n            x = []", "response": "Utility function to split iterable f into n chunks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a bed file from the pybedtools examples directory.", "response": "def example_filename(fn):\n    \"\"\"\n    Return a bed file from the pybedtools examples directory.  Use\n    :func:`list_example_files` to see a list of files that are included.\n    \"\"\"\n    fn = os.path.join(data_dir(), fn)\n    if not os.path.exists(fn):\n        raise ValueError(\"%s does not exist\" % fn)\n    return fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits an interval into n roughly equal portions", "response": "def split_feature(f, n):\n    \"\"\"\n    Split an interval into `n` roughly equal portions\n    \"\"\"\n    if not isinstance(n, int):\n        raise ValueError('n must be an integer')\n    orig_feature = copy(f)\n    step = (f.stop - f.start) / n\n    for i in range(f.start, f.stop, step):\n        f = copy(orig_feature)\n        start = i\n        stop = min(i + step, orig_feature.stop)\n        f.start = start\n        f.stop = stop\n        yield f\n        if stop == orig_feature.stop:\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a string to an interval.", "response": "def tointerval(s):\n    \"\"\"\n    If string, then convert to an interval; otherwise just return the input\n    \"\"\"\n    if isinstance(s, basestring):\n        m = coord_re.search(s)\n        if m.group('strand'):\n            return pybedtools.create_interval_from_list([\n                m.group('chrom'),\n                m.group('start'),\n                m.group('stop'),\n                '.',\n                '0',\n                m.group('strand')])\n        else:\n            return pybedtools.create_interval_from_list([\n                m.group('chrom'),\n                m.group('start'),\n                m.group('stop'),\n            ])\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the maximum width of the progress bar.", "response": "def max_width(self):\n        \"\"\"Get maximum width of progress bar\n\n        :rtype: int\n        :returns: Maximum column width of progress bar\n        \"\"\"\n        value, unit = float(self._width_str[:-1]), self._width_str[-1]\n\n        ensure(unit in [\"c\", \"%\"], ValueError,\n               \"Width unit must be either 'c' or '%'\")\n\n        if unit == \"c\":\n            ensure(value <= self.columns, ValueError,\n                   \"Terminal only has {} columns, cannot draw \"\n                   \"bar of size {}.\".format(self.columns, value))\n            retval = value\n        else:  # unit == \"%\"\n            ensure(0 < value <= 100, ValueError,\n                   \"value=={} does not satisfy 0 < value <= 100\".format(value))\n            dec = value / 100\n            retval = dec * self.columns\n\n        return floor(retval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef full_line_width(self):\n        bar_str_len = sum([\n            self._indent,\n            ((len(self.title) + 1) if self._title_pos in [\"left\", \"right\"]\n             else 0),  # Title if present\n            len(self.start_char),\n            self.max_width,  # Progress bar\n            len(self.end_char),\n            1,  # Space between end_char and amount_complete_str\n            len(str(self.max_value)) * 2 + 1  # 100/100\n        ])\n        return bar_str_len", "response": "Find actual length of bar_str e. g. Progress [    |     10 / 10"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the term supports colors.", "response": "def _supports_colors(term, raise_err, colors):\n        \"\"\"Check if ``term`` supports ``colors``\n\n        :raises ColorUnsupportedError: This is raised if ``raise_err``\n            is ``False`` and a color in ``colors`` is unsupported by ``term``\n        :type raise_err: bool\n        :param raise_err: Set to ``False`` to return a ``bool`` indicating\n            color support rather than raising ColorUnsupportedError\n        :type  colors: [str, ...]\n        \"\"\"\n        for color in colors:\n            try:\n                if isinstance(color, str):\n                    req_colors = 16 if \"bright\" in color else 8\n                    ensure(term.number_of_colors >= req_colors,\n                           ColorUnsupportedError,\n                           \"{} is unsupported by your terminal.\".format(color))\n                elif isinstance(color, int):\n                    ensure(term.number_of_colors >= color,\n                           ColorUnsupportedError,\n                           \"{} is unsupported by your terminal.\".format(color))\n            except ColorUnsupportedError as e:\n                if raise_err:\n                    raise e\n                else:\n                    return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting callable for string output using color on back_color on term", "response": "def _get_format_callable(term, color, back_color):\n        \"\"\"Get string-coloring callable\n\n        Get callable for string output using ``color`` on ``back_color``\n            on ``term``\n\n        :param term: blessings.Terminal instance\n        :param color: Color that callable will color the string it's passed\n        :param back_color: Back color for the string\n        :returns: callable(s: str) -> str\n        \"\"\"\n        if isinstance(color, str):\n            ensure(\n                any(isinstance(back_color, t) for t in [str, type(None)]),\n                TypeError,\n                \"back_color must be a str or NoneType\"\n            )\n            if back_color:\n                return getattr(term, \"_\".join(\n                    [color, \"on\", back_color]\n                ))\n            elif back_color is None:\n                return getattr(term, color)\n        elif isinstance(color, int):\n            return term.on_color(color)\n        else:\n            raise TypeError(\"Invalid type {} for color\".format(\n                type(color)\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a string to the terminal.", "response": "def _write(self, s, s_length=None, flush=False, ignore_overflow=False,\n               err_msg=None):\n        \"\"\"Write ``s``\n\n        :type  s: str|unicode\n        :param s: String to write\n        :param s_length: Custom length of ``s``\n        :param flush: Set this to flush the terminal stream after writing\n        :param ignore_overflow: Set this to ignore if s will exceed\n            the terminal's width\n        :param err_msg: The error message given to WidthOverflowError\n            if it is triggered\n        \"\"\"\n        if not ignore_overflow:\n            s_length = len(s) if s_length is None else s_length\n            if err_msg is None:\n                err_msg = (\n                    \"Terminal has {} columns; attempted to write \"\n                    \"a string {} of length {}.\".format(\n                        self.columns, repr(s), s_length)\n                )\n            ensure(s_length <= self.columns, WidthOverflowError, err_msg)\n        self.cursor.write(s)\n        if flush:\n            self.cursor.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the progress bar of the current entry.", "response": "def draw(self, value, newline=True, flush=True):\n        \"\"\"Draw the progress bar\n\n        :type  value: int\n        :param value: Progress value relative to ``self.max_value``\n        :type  newline: bool\n        :param newline: If this is set, a newline will be written after drawing\n        \"\"\"\n        # This is essentially winch-handling without having\n        #   to do winch-handling; cleanly redrawing on winch is difficult\n        #   and out of the intended scope of this class; we *can*\n        #   however, adjust the next draw to be proper by re-measuring\n        #   the terminal since the code is mostly written dynamically\n        #   and many attributes and dynamically calculated properties.\n        self._measure_terminal()\n        \n        # To avoid zero division, set amount_complete to 100% if max_value has been stupidly set to 0\n        amount_complete = 1.0 if self.max_value == 0 else value / self.max_value\n        fill_amount = int(floor(amount_complete * self.max_width))\n        empty_amount = self.max_width - fill_amount\n\n        # e.g., '10/20' if 'fraction' or '50%' if 'percentage'\n        amount_complete_str = (\n            u\"{}/{}\".format(value, self.max_value)\n            if self._num_rep == \"fraction\" else\n            u\"{}%\".format(int(floor(amount_complete * 100)))\n        )\n\n        # Write title if supposed to be above\n        if self._title_pos == \"above\":\n            title_str = u\"{}{}\\n\".format(\n                \" \" * self._indent,\n                self.title,\n            )\n            self._write(title_str, ignore_overflow=True)\n\n        # Construct just the progress bar\n        bar_str = u''.join([\n            u(self.filled(self._filled_char * fill_amount)),\n            u(self.empty(self._empty_char * empty_amount)),\n        ])\n        # Wrap with start and end character\n        bar_str = u\"{}{}{}\".format(self.start_char, bar_str, self.end_char)\n        # Add on title if supposed to be on left or right\n        if self._title_pos == \"left\":\n            bar_str = u\"{} {}\".format(self.title, bar_str)\n        elif self._title_pos == \"right\":\n            bar_str = u\"{} {}\".format(bar_str, self.title)\n        # Add indent\n        bar_str = u''.join([\" \" * self._indent, bar_str])\n        # Add complete percentage or fraction\n        bar_str = u\"{} {}\".format(bar_str, amount_complete_str)\n        # Set back to normal after printing\n        bar_str = u\"{}{}\".format(bar_str, self.term.normal)\n        # Finally, write the completed bar_str\n        self._write(bar_str, s_length=self.full_line_width)\n\n        # Write title if supposed to be below\n        if self._title_pos == \"below\":\n            title_str = u\"\\n{}{}\".format(\n                \" \" * self._indent,\n                self.title,\n            )\n            self._write(title_str, ignore_overflow=True)\n\n        # Newline to wrap up\n        if newline:\n            self.cursor.newline()\n        if flush:\n            self.cursor.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summarize(self, interval, bins=None, method='summarize',\n                  function='mean', zero_inf=True, zero_nan=True):\n        \"\"\"\n        Parameters\n        ----------\n\n        interval : object\n            Object with chrom (str), start (int) and stop (int) attributes.\n\n        bins : int or None\n            Number of bins; if None, bins will be the length of the interval\n\n        method : summarize | ucsc_summarize | get_as_array\n            \"summarize\" and \"get_as_array\" use bx-python; \"ucsc_summarize\" uses\n            bigWigSummarize. See other notes in docstring for\n            metaseq.array_helpers._local_coverage. If None, defaults to\n            \"summarize\".\n\n        function : mean | min | max | std | coverage\n            Determines the nature of the summarized values. Ignored if\n            `method=\"get_as_array\"`; \"coverage\" is only valid if method is\n            \"ucsc_summarize\".\n\n        zero_inf, zero_nan : bool\n            If `zero_inf` is True, set any inf or -inf to zero before\n            returning. If `zero_nan` is True, set any nan values to zero before\n            returning.\n        \"\"\"\n\n        if method is None:\n            method = 'summarize'\n\n        # We may be dividing by zero in some cases, which raises a warning in\n        # NumPy based on the IEEE 754 standard (see\n        # http://docs.scipy.org/doc/numpy/reference/generated/\n        #       numpy.seterr.html)\n        #\n        # That's OK -- we're expecting that to happen sometimes. So temporarily\n        # disable this error reporting for the duration of this method.\n        orig = np.geterr()['invalid']\n        np.seterr(invalid='ignore')\n\n        if (bins is None) or (method == 'get_as_array'):\n            bw = BigWigFile(open(self.fn))\n            s = bw.get_as_array(\n                interval.chrom,\n                interval.start,\n                interval.stop,)\n            if s is None:\n                s = np.zeros((interval.stop - interval.start,))\n            else:\n                if zero_nan:\n                    s[np.isnan(s)] = 0\n                if zero_inf:\n                    s[np.isinf(s)] = 0\n\n        elif method == 'ucsc_summarize':\n            if function in ['mean', 'min', 'max', 'std', 'coverage']:\n                return self.ucsc_summarize(interval, bins, function=function)\n            else:\n                raise ValueError('function \"%s\" not supported by UCSC\\'s'\n                                 'bigWigSummary')\n\n        elif method == 'summarize':\n            bw = BigWigFile(open(self.fn))\n            s = bw.summarize(\n                interval.chrom,\n                interval.start,\n                interval.stop, bins)\n            if s is None:\n                s = np.zeros((bins,))\n            else:\n                if function == 'sum':\n                    s = s.sum_data\n                elif function == 'mean':\n                    s = s.sum_data / s.valid_count\n                    if zero_nan:\n                        s[np.isnan(s)] = 0\n                elif function == 'min':\n                    s = s.min_val\n                    if zero_inf:\n                        s[np.isinf(s)] = 0\n                elif function == 'max':\n                    s = s.max_val\n                    if zero_inf:\n                        s[np.isinf(s)] = 0\n                elif function == 'std':\n                    s = (s.sum_squares / s.valid_count)\n                    if zero_nan:\n                        s[np.isnan(s)] = 0\n                else:\n                    raise ValueError(\n                            'function \"%s\" not supported by bx-python'\n                            % function\n                    )\n        else:\n            raise ValueError(\"method '%s' not in [summarize, ucsc_summarize, get_as_array]\" % method)\n\n        # Reset NumPy error reporting\n        np.seterr(divide=orig)\n        return s", "response": "Return a summary of the IEEE 754 data for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_text(nodelist):\n    value = []\n    for node in nodelist:\n        if node.nodeType == node.TEXT_NODE:\n            value.append(node.data)\n    return ''.join(value)", "response": "Get the value from a text node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _request(self, url, params=None, timeout=10):\n        rsp = self._session.get(url, params=params, timeout=timeout)\n        rsp.raise_for_status()\n        return rsp.text.strip()", "response": "Send a request with parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a login request with paramerters.", "response": "def _login_request(self, username=None, secret=None):\n        \"\"\"Send a login request with paramerters.\"\"\"\n        url = 'http://' + self._host + '/login_sid.lua'\n        params = {}\n        if username:\n            params['username'] = username\n        if secret:\n            params['response'] = secret\n\n        plain = self._request(url, params)\n        dom = xml.dom.minidom.parseString(plain)\n        sid = get_text(dom.getElementsByTagName('SID')[0].childNodes)\n        challenge = get_text(\n            dom.getElementsByTagName('Challenge')[0].childNodes)\n\n        return (sid, challenge)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _logout_request(self):\n        _LOGGER.debug('logout')\n        url = 'http://' + self._host + '/login_sid.lua'\n        params = {\n            'security:command/logout': '1',\n            'sid': self._sid\n        }\n\n        self._request(url, params)", "response": "Send a logout request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a login secret.", "response": "def _create_login_secret(challenge, password):\n        \"\"\"Create a login secret.\"\"\"\n        to_hash = (challenge + '-' + password).encode('UTF-16LE')\n        hashed = hashlib.md5(to_hash).hexdigest()\n        return '{0}-{1}'.format(challenge, hashed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an AHA request.", "response": "def _aha_request(self, cmd, ain=None, param=None, rf=str):\n        \"\"\"Send an AHA request.\"\"\"\n        url = 'http://' + self._host + '/webservices/homeautoswitch.lua'\n        params = {\n            'switchcmd': cmd,\n            'sid': self._sid\n        }\n        if param:\n            params['param'] = param\n        if ain:\n            params['ain'] = ain\n\n        plain = self._request(url, params)\n        if plain == 'inval':\n            raise InvalidError\n\n        if rf == bool:\n            return bool(int(plain))\n        return rf(plain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef login(self):\n        try:\n            (sid, challenge) = self._login_request()\n            if sid == '0000000000000000':\n                secret = self._create_login_secret(challenge, self._password)\n                (sid2, challenge) = self._login_request(username=self._user,\n                                                        secret=secret)\n                if sid2 == '0000000000000000':\n                    _LOGGER.warning(\"login failed %s\", sid2)\n                    raise LoginError(self._user)\n                self._sid = sid2\n        except xml.parsers.expat.ExpatError:\n            raise LoginError(self._user)", "response": "Login and get a valid session ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_device_elements(self):\n        plain = self._aha_request('getdevicelistinfos')\n        dom = xml.dom.minidom.parseString(plain)\n        _LOGGER.debug(dom)\n        return dom.getElementsByTagName(\"device\")", "response": "Get the DOM elements for the device list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the DOM element for the specified device.", "response": "def get_device_element(self, ain):\n        \"\"\"Get the DOM element for the specified device.\"\"\"\n        elements = self.get_device_elements()\n        for element in elements:\n            if element.getAttribute('identifier') == ain:\n                return element\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_devices(self):\n        devices = []\n        for element in self.get_device_elements():\n            device = FritzhomeDevice(self, node=element)\n            devices.append(device)\n        return devices", "response": "Get the list of all known devices."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a device specified by the AIN.", "response": "def get_device_by_ain(self, ain):\n        \"\"\"Returns a device specified by the AIN.\"\"\"\n        devices = self.get_devices()\n        for device in devices:\n            if device.ain == ain:\n                return device"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_target_temperature(self, ain, temperature):\n        param = 16 + ((float(temperature) - 8) * 2)\n\n        if param < min(range(16, 56)):\n            param = 253\n        elif param > max(range(16, 56)):\n            param = 254\n        self._aha_request('sethkrtsoll', ain=ain, param=int(param))", "response": "Set the thermostate target temperature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the device values.", "response": "def update(self):\n        \"\"\"Update the device values.\"\"\"\n        node = self._fritz.get_device_element(self.ain)\n        self._update_from_node(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the thermostate state.", "response": "def get_hkr_state(self):\n        \"\"\"Get the thermostate state.\"\"\"\n        self.update()\n        try:\n            return {\n                126.5: 'off',\n                127.0: 'on',\n                self.eco_temperature: 'eco',\n                self.comfort_temperature: 'comfort'\n            }[self.target_temperature]\n        except KeyError:\n            return 'manual'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the state of the thermostat.", "response": "def set_hkr_state(self, state):\n        \"\"\"Set the state of the thermostat.\n\n        Possible values for state are: 'on', 'off', 'comfort', 'eco'.\n        \"\"\"\n        try:\n            value = {\n                'off': 0,\n                'on': 100,\n                'eco': self.eco_temperature,\n                'comfort': self.comfort_temperature\n            }[state]\n        except KeyError:\n            return\n\n        self.set_target_temperature(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite s to the terminal output stream", "response": "def write(self, s):\n        \"\"\"Writes ``s`` to the terminal output stream\n\n        Writes can be disabled by setting the environment variable\n            `PROGRESSIVE_NOWRITE` to `'True'`\n        \"\"\"\n        should_write_s = os.getenv('PROGRESSIVE_NOWRITE') != \"True\"\n        if should_write_s:\n            self._stream.write(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self):\n        self.write(self.term.save)\n        self._saved = True", "response": "Saves current cursor position so that it can be restored later"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\neffecting a newline by moving the cursor down and clearing the Bol", "response": "def newline(self):\n        \"\"\"Effects a newline by moving the cursor down and clearing\"\"\"\n        self.write(self.term.move_down)\n        self.write(self.term.clear_bol)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef smart_colormap(vmin, vmax, color_high='#b11902', hue_low=0.6):\n    # first go from white to color_high\n    orig_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n        'test', ['#FFFFFF', color_high], N=2048)\n\n    # For example, say vmin=-3 and vmax=9.  If vmin were positive, what would\n    # its color be?\n    vmin = float(vmin)\n    vmax = float(vmax)\n    mx = max([vmin, vmax])\n    mn = min([vmin, vmax])\n    frac = abs(mn / mx)\n    rgb = orig_cmap(frac)[:-1]\n\n    # Convert to HSV and shift the hue\n    hsv = list(colorsys.rgb_to_hsv(*rgb))\n    hsv[0] = hue_low\n    new_rgb = colorsys.hsv_to_rgb(*hsv)\n    new_hex = matplotlib.colors.rgb2hex(new_rgb)\n\n    zeropoint = abs(-vmin / (vmax - vmin))\n\n    # Create a new colormap using the new hue-shifted color as the low end\n    new_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n        'test', [(0, new_rgb), (zeropoint, '#FFFFFF'), (1, color_high)],\n        N=2048)\n\n    return new_cmap", "response": "Creates a smart colormap that is centered on zero and accounts for asymmetrical vmin and vmax by matching saturation value of high and low\n    colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cmap_powerlaw_adjust(cmap, a):\n    if a < 0.:\n        return cmap\n    cdict = copy.copy(cmap._segmentdata)\n    fn = lambda x: (x[0] ** a, x[1], x[2])\n    for key in ('red', 'green', 'blue'):\n        cdict[key] = map(fn, cdict[key])\n        cdict[key].sort()\n        assert (cdict[key][0] < 0 or cdict[key][-1] > 1), \\\n            \"Resulting indices extend out of the [0, 1] segment.\"\n    return colors.LinearSegmentedColormap('colormap', cdict, 1024)", "response": "Returns a new colormap based on the one given\n    but adjusted via power - law."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new colormap based on the one given by cmap but adjusted so that the old center point higher than center_ratio is lower than center_ratio.", "response": "def cmap_center_adjust(cmap, center_ratio):\n    \"\"\"\n    Returns a new colormap based on the one given\n    but adjusted so that the old center point higher\n    (>0.5) or lower (<0.5)\n\n    :param cmap: colormap instance (e.g., cm.jet)\n    :param center_ratio:\n\n    \"\"\"\n    if not (0. < center_ratio) & (center_ratio < 1.):\n        return cmap\n    a = math.log(center_ratio) / math.log(0.5)\n    return cmap_powerlaw_adjust(cmap, a)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts center to a ratio between 0 and 1 of the range given and calls cmap_center_adjust.", "response": "def cmap_center_point_adjust(cmap, range, center):\n    \"\"\"\n    Converts center to a ratio between 0 and 1 of the\n    range given and calls cmap_center_adjust(). returns\n    a new adjusted colormap accordingly\n\n    :param cmap: colormap instance\n    :param range: Tuple of (min, max)\n    :param center: New cmap center\n    \"\"\"\n    if not ((range[0] < center) and (center < range[1])):\n        return cmap\n    return cmap_center_adjust(\n        cmap,\n        abs(center - range[0]) / abs(range[1] - range[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattaches a gffutils. FeatureDB to the current instance.", "response": "def attach_db(self, db):\n        \"\"\"\n        Attach a gffutils.FeatureDB for access to features.\n\n        Useful if you want to attach a db after this instance has already been\n        created.\n\n        Parameters\n        ----------\n        db : gffutils.FeatureDB\n        \"\"\"\n        if db is not None:\n            if isinstance(db, basestring):\n                db = gffutils.FeatureDB(db)\n            if not isinstance(db, gffutils.FeatureDB):\n                raise ValueError(\n                    \"`db` must be a filename or a gffutils.FeatureDB\")\n        self._kwargs['db'] = db\n        self.db = db"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef features(self, ignore_unknown=False):\n        if not self.db:\n            raise ValueError(\"Please attach a gffutils.FeatureDB\")\n        for i in self.data.index:\n            try:\n                yield gffutils.helpers.asinterval(self.db[i])\n            except gffutils.FeatureNotFoundError:\n                if ignore_unknown:\n                    continue\n                else:\n                    raise gffutils.FeatureNotFoundError('%s not found' % i)", "response": "Generator of features.\n\n        If a gffutils.FeatureDB is attached, returns a pybedtools.Interval for\n        every feature in the dataframe's index.\n\n        Parameters\n        ----------\n        ignore_unknown : bool\n            If True, silently ignores features that are not found in the db."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new table with only rows corresponding to feature names in x.", "response": "def reindex_to(self, x, attribute=\"Name\"):\n        \"\"\"\n        Returns a copy that only has rows corresponding to feature names in x.\n\n        Parameters\n        ----------\n        x : str or pybedtools.BedTool\n            BED, GFF, GTF, or VCF where the \"Name\" field (that is, the value\n            returned by feature['Name']) or any arbitrary attribute\n\n        attribute : str\n            Attribute containing the name of the feature to use as the index.\n        \"\"\"\n        names = [i[attribute] for i in x]\n        new = self.copy()\n        new.data = new.data.reindex(names)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a BED file of the 5 prime end of each feature represented in the specified basepairs up and downstream.", "response": "def five_prime(self, upstream=1, downstream=0):\n        \"\"\"\n        Creates a BED/GFF file of the 5' end of each feature represented in the\n        table and returns the resulting pybedtools.BedTool object. Needs an\n        attached database.\n\n        Parameters\n        ----------\n        upstream, downstream : int\n            Number of basepairs up and downstream to include\n        \"\"\"\n        return pybedtools.BedTool(self.features())\\\n            .each(featurefuncs.five_prime, upstream, downstream)\\\n            .saveas()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naligning the dataframe s index with another.", "response": "def align_with(self, other):\n        \"\"\"\n        Align the dataframe's index with another.\n        \"\"\"\n        return self.__class__(self.data.reindex_like(other), **self._kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_unknown_features(self):\n        if not self.db:\n            return self\n        ind = []\n        for i, gene_id in enumerate(self.data.index):\n            try:\n                self.db[gene_id]\n                ind.append(i)\n            except gffutils.FeatureNotFoundError:\n                pass\n        ind = np.array(ind)\n        return self.__class__(self.data.ix[ind], **self._kwargs)", "response": "Remove unknown features from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef genes_with_peak(self, peaks, transform_func=None, split=False,\n                        intersect_kwargs=None, id_attribute='ID', *args,\n                        **kwargs):\n        \"\"\"\n        Returns a boolean index of genes that have a peak nearby.\n\n        Parameters\n        ----------\n        peaks : string or pybedtools.BedTool\n            If string, then assume it's a filename to a BED/GFF/GTF file of\n            intervals; otherwise use the pybedtools.BedTool object directly.\n\n        transform_func : callable\n            This function will be applied to each gene object returned by\n            self.features().  Additional args and kwargs are passed to\n            `transform_func`. For example, if you're looking for peaks within\n            1kb upstream of TSSs, then pybedtools.featurefuncs.TSS would be\n            a useful `transform_func`, and you could supply additional kwargs\n            of `upstream=1000` and `downstream=0`.\n\n            This function can return iterables of features, too. For example,\n            you might want to look for peaks falling within the exons of\n            a gene.  In this case, `transform_func` should return an iterable\n            of pybedtools.Interval objects.  The only requirement is that the\n            `name` field of any feature matches the index of the dataframe.\n\n        intersect_kwargs : dict\n            kwargs passed to pybedtools.BedTool.intersect.\n\n        id_attribute : str\n            The attribute in the GTF or GFF file that contains the id of the\n            gene. For meaningful results to be returned, a gene's ID be also\n            found in the index of the dataframe.\n\n            For GFF files, typically you'd use `id_attribute=\"ID\"`.  For GTF\n            files, you'd typically use `id_attribute=\"gene_id\"`.\n        \"\"\"\n        def _transform_func(x):\n            \"\"\"\n            In order to support transform funcs that return a single feature or\n            an iterable of features, we need to wrap it\n            \"\"\"\n            result = transform_func(x)\n            if isinstance(result, pybedtools.Interval):\n                result = [result]\n            for i in result:\n                if i:\n                    yield result\n\n        intersect_kwargs = intersect_kwargs or {}\n        if not self._cached_features:\n            self._cached_features = pybedtools\\\n                .BedTool(self.features())\\\n                .saveas()\n\n        if transform_func:\n            if split:\n                features = self._cached_features\\\n                    .split(_transform_func, *args, **kwargs)\n            else:\n                features = self._cached_features\\\n                    .each(transform_func, *args, **kwargs)\n\n        else:\n            features = self._cached_features\n\n        hits = list(set([i[id_attribute] for i in features.intersect(\n            peaks, **intersect_kwargs)]))\n        return self.data.index.isin(hits)", "response": "Returns a boolean index of genes that have a peak nearby."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the changed entries in the sequence.", "response": "def changed(self, thresh=0.05, idx=True):\n        \"\"\"\n        Changed features.\n\n        {threshdoc}\n        \"\"\"\n        ind = self.data[self.pval_column] <= thresh\n        if idx:\n            return ind\n        return self[ind]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the unchanged entry.", "response": "def unchanged(self, thresh=0.05, idx=True):\n        \"\"\"\n        Changed features.\n\n        {threshdoc}\n        \"\"\"\n        ind = (\n            (self.data[self.pval_column] > thresh)\n            | np.isnan(self.data[self.pval_column])\n        )\n        if idx:\n            return ind\n        return self[ind]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the average read count across treatments across treatments across log2 fold change.", "response": "def ma_plot(self, thresh, up_kwargs=None, dn_kwargs=None,\n                zero_line=None, **kwargs):\n        \"\"\"\n        MA plot\n\n        Plots the average read count across treatments (x-axis) vs the log2\n        fold change (y-axis).\n\n        Additional kwargs are passed to self.scatter (useful ones might include\n        `genes_to_highlight`)\n\n        Parameters\n        ----------\n        thresh : float\n            Features with values <= `thresh` will be highlighted in the plot.\n\n        up_kwargs, dn_kwargs : None or dict\n            Kwargs passed to matplotlib's scatter(), used for styling up/down\n            regulated features (defined by `thresh` and `col`)\n\n        zero_line : None or dict\n            Kwargs passed to matplotlib.axhline(0).\n\n        \"\"\"\n        genes_to_highlight = kwargs.pop('genes_to_highlight', [])\n        genes_to_highlight.append(\n            (self.upregulated(thresh),\n             up_kwargs or dict(color='r')))\n        genes_to_highlight.append(\n            (self.downregulated(thresh),\n             dn_kwargs or dict(color='b')))\n        if zero_line is None:\n            zero_line = {}\n        x = self.mean_column\n        y = self.lfc_column\n\n        if 'xfunc' not in kwargs:\n            kwargs['xfunc'] = np.log\n        ax = self.scatter(\n            x=x,\n            y=y,\n            genes_to_highlight=genes_to_highlight,\n            **kwargs)\n        if zero_line:\n            ax.axhline(0, **zero_line)\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef colormapped_bedfile(self, genome, cmap=None):\n        if self.db is None:\n            raise ValueError(\"FeatureDB required\")\n        db = gffutils.FeatureDB(self.db)\n\n        def scored_feature_generator(d):\n            for i in range(len(d)):\n                try:\n                    feature = db[d.ix[i]]\n                except gffutils.FeatureNotFoundError:\n                    raise gffutils.FeatureNotFoundError(d.ix[i])\n                score = -10 * np.log10(d.padj[i])\n                lfc = d.log2FoldChange[i]\n                if np.isnan(lfc):\n                    score = 0\n                if lfc < 0:\n                    score *= -1\n                feature.score = str(score)\n                feature = extend_fields(\n                    gff2bed(gffutils.helpers.asinterval(feature)), 9)\n                fields = feature.fields[:]\n                fields[6] = fields[1]\n                fields[7] = fields[2]\n                fields.append(str(d.padj[i]))\n                fields.append(str(d.pval[i]))\n                fields.append('%.3f' % d.log2FoldChange[i])\n                fields.append('%.3f' % d.baseMeanB[i])\n                fields.append('%.3f' % d.baseMeanB[i])\n                yield pybedtools.create_interval_from_list(fields)\n\n        x = pybedtools.BedTool(scored_feature_generator(self)).saveas()\n        norm = x.colormap_normalize()\n        if cmap is None:\n            cmap = cm.RdBu_r\n        cmap = colormap_adjust.cmap_center_point_adjust(\n            cmap, [norm.vmin, norm.vmax], 0)\n\n        def score_zeroer(f):\n            f.score = '0'\n            return f\n        return x.each(add_color, cmap=cmap, norm=norm)\\\n                .sort()\\\n                .each(score_zeroer)\\\n                .truncate_to_chrom(genome)\\\n                .saveas()", "response": "Create a BED file with padj encoded as color\n 69."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef autosql_file(self):\n        fn = pybedtools.BedTool._tmp()\n\n        AUTOSQL = dedent(\n            \"\"\"\n            table example\n            \"output from DESeq\"\n            (\n            string  chrom;  \"chromosome\"\n            uint chromStart; \"start coord\"\n            uint chromEnd; \"stop coord\"\n            string name; \"name of feature\"\n            uint score; \"always zero\"\n            char[1] strand; \"+ or - for strand\"\n            uint    thickStart; \"Coding region start\"\n            uint    thickEnd;  \"Coding region end\"\n            uint reserved; \"color according to score\"\n            string padj; \"DESeq adjusted p value\"\n            string pval; \"DESeq raw p value\"\n            string logfoldchange; \"DESeq log2 fold change\"\n            string basemeana; \"DESeq baseMeanA\"\n            string basemeanb; \"DESeq baseMeanB\"\n        )\n        \"\"\")\n\n        fout = open(fn, 'w')\n        fout.write(AUTOSQL)\n        fout.close()\n        return fn", "response": "Generate the autosql for the DESeq results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _local_count(reader, feature, stranded=False):\n    if isinstance(feature, basestring):\n        feature = helpers.tointerval(feature)\n    if stranded:\n        strand = feature.strand\n    else:\n        strand = '.'\n\n    count = 0\n    for al in reader[feature]:\n        if stranded and al.strand != strand:\n            continue\n        count += 1\n    return count", "response": "Returns the number of genomic signals found within an interval."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _local_coverage(reader, features, read_strand=None, fragment_size=None,\n                    shift_width=0, bins=None, use_score=False, accumulate=True,\n                    preserve_total=False, method=None, function=\"mean\",\n                    zero_inf=True, zero_nan=True, processes=None,\n                    stranded=True, verbose=False):\n    \"\"\"\n    Returns a binned vector of coverage.\n\n    Computes a 1D vector of coverage at the coordinates for each feature in\n    `features`, extending each read by `fragmentsize` bp.\n\n    Some arguments cannot be used for bigWig files due to the structure of\n    these files.  The parameters docstring below indicates whether or not an\n    argument can be used with bigWig files.\n\n    Depending on the arguments provided, this method can return a vector\n    containing values from a single feature or from concatenated features.\n\n    An example of the flexibility afforded by the latter case:\n\n        `features` can be a 3-tuple of pybedtools.Intervals representing (TSS\n        + 1kb upstream, gene, TTS + 1kb downstream) and `bins` can be [100,\n        1000, 100].  This will return a vector of length 1200 containing the\n        three genomic intervals binned into 100, 1000, and 100 bins\n        respectively.  Note that is up to the caller to construct the right\n        axes labels in the final plot!\n\n    Parameters\n    ----------\n    features : str, interval-like object, or list\n\n        Can be a single interval or an iterable yielding intervals.\n\n        Interval-like objects must have chrom, start, and stop attributes, and\n        optionally a strand attribute.  One exception to this that if\n        `features` is a single string, it can be of the form \"chrom:start-stop\"\n        or \"chrom:start-stop[strand]\".\n\n        If `features` is a single interval, then return a 1-D array for that\n        interval.\n\n        If `features` is an iterable of intervals, then return a 1-D\n        array that is a concatenation of signal for these intervals.\n\n        Available for bigWig.\n\n    bins : None, int, list\n        If `bins` is None, then each value in the returned array will\n        correspond to one bp in the genome.\n\n        If `features` is a single Interval, then `bins` is an integer or None.\n\n        If `features` is an iterable of Intervals, `bins` is an iterable of\n        integers of the same length as `features`.\n\n        Available for bigWig.\n\n    fragment_size : None or int\n        If not None, then each item from the genomic signal (e.g., reads from\n        a BAM file) will be extended `fragment_size` bp in the 3' direction.\n        Higher fragment sizes will result in smoother signal.  Not available\n        for bigWig.\n\n    shift_width : int\n        Each item from the genomic signal (e.g., reads from a BAM\n        file) will be shifted `shift_width` bp in the 3' direction.  This can\n        be useful for reconstructing a ChIP-seq profile, using the shift width\n        determined from the peak-caller (e.g., modeled `d` in MACS). Not\n        available for bigWig.\n\n    read_strand : None or str\n        If `read_strand` is one of \"+\" or \"-\", then only items from the genomic\n        signal (e.g., reads from a BAM file) on that strand will be considered\n        and reads on the opposite strand ignored.  Useful for plotting genomic\n        signal for stranded libraries. Not available for bigWig.\n\n    stranded : bool\n        If True, then the profile will be reversed for features whose strand\n        attribute is \"-\".\n\n    use_score : bool\n        If True, then each bin will contain the sum of the *score* attribute of\n        genomic features in that bin instead of the *number* of genomic\n        features falling within each bin. Not available for bigWig.\n\n    accumulate : bool\n        If False, then only record *that* there was something there, rather\n        than acumulating reads.  This is useful for making matrices with called\n        peaks. Available for bigWig.\n\n    preserve_total : bool\n        If True, re-scales the returned value so that each binned row's total\n        is equal to the sum of the original, un-binned data.  The units of the\n        returned array will be in \"total per bin\".  This is useful for, e.g.,\n        counting reads in features.  If `preserve_total` is False, then the\n        returned array will have units of \"density\"; this is more generally\n        useful and is the default behavior.  Available for bigWig, but not when\n        using method=\"ucsc_summarize\".\n\n    method : str; one of [ \"summarize\" | \"get_as_array\" | \"ucsc_summarize\" ]\n        Only used for bigWig.  The method specifies how data are extracted from\n        the bigWig file.  \"summarize\" is the default.  It's quite fast, but may\n        yield slightly different results when compared to running this same\n        function on the BAM file from which the bigWig was created.\n\n        \"summarize\" uses bx-python.  The values returned will not be exactly\n        the same as the values returned when local_coverage is called on a BAM,\n        BED, or bigBed file, but they will be close.  This method is quite\n        fast, and is the default when bins is not None.\n\n        \"get_as_array\" uses bx-python, but does a separate binning step.  This\n        can be slower than the other two methods, but the results are exactly\n        the same as those from a BAM, BED, or bigBed file.  This method is\n        always used if bins=None.\n\n        \"ucsc_summarize\" is an alternative version of \"summarize\".  It uses the\n        UCSC program `bigWigSummary`, which must already installed and on your\n        path.\n\n    function : str; one of ['sum' | 'mean' | 'min' | 'max' | 'std']\n        Determine the nature of the values returned. Only valid if `method` is\n        \"summarize\" or \"ucsc_summarize\", which also implies bigWig. Default is\n        \"mean\". If `method=\"ucsc_summarize\", then there is an additional option\n        for function, \"coverage\", which returns the percent of region that is\n        covered.\n\n    zero_inf, zero_nan : bool\n        Only used for bigWig. If either are True, sets any missing or inf\n        values to zero before returning.\n\n        If `method=\"ucsc_summarize\"`, missinv values are always reported as\n        zero. If `method=\"get_as_array\"`, missing values always reported as\n        nan.\n\n        Values can be -inf, inf, or nan for missing values when\n        `method=\"summarize\"` according to the following table:\n\n        ========== ========================\n        `function` missing values appear as\n        ========== ========================\n        \"sum\"      0\n        \"mean\"     nan\n        \"min\"      inf\n        \"max\"      -inf\n        \"std\"      nan\n        ========== ========================\n\n    processes : int or None\n        The feature can be split across multiple processes.\n\n    Returns\n    -------\n\n    1-d NumPy array\n\n\n    Notes\n    -----\n    If a feature has a \"-\" strand attribute, then the resulting profile will be\n    *relative to a minus-strand feature*.  That is, the resulting profile will\n    be reversed.\n\n    Returns arrays `x` and `y`.  `x` is in genomic coordinates, and `y` is\n    the coverage at each of those coordinates after extending fragments.\n\n    The total number of reads is guaranteed to be the same no matter how it's\n    binned.\n\n    (with ideas from\n    http://www-huber.embl.de/users/anders/HTSeq/doc/tss.html)\n\n    \"\"\"\n    # bigWig files are handled differently, so we need to know if we're working\n    # with one; raise exeception if a kwarg was supplied that's not supported.\n    if isinstance(reader, filetype_adapters.BigWigAdapter):\n        is_bigwig = True\n        defaults = (\n            ('read_strand', read_strand, None),\n            ('fragment_size', fragment_size, None),\n            ('shift_width', shift_width, 0),\n            ('use_score', use_score, False),\n            ('preserve_total', preserve_total, False),\n        )\n        for name, check, default in defaults:\n            if (\n                ((default is None) and (check is not default))\n                or\n                (check != default)\n            ):\n                raise ArgumentError(\n                    \"Argument '%s' not supported for bigWig\" % name)\n\n        if method == 'ucsc_summarize':\n            if preserve_total:\n                raise ArgumentError(\n                    \"preserve_total=True not supported when using \"\n                    \"method='ucsc_summarize'\")\n    else:\n        is_bigwig = False\n\n    if isinstance(reader, filetype_adapters.BamAdapter):\n        if use_score:\n            raise ArgumentError(\"Argument 'use_score' not supported for \"\n                                \"bam\")\n\n    # e.g., features = \"chr1:1-1000\"\n    if isinstance(features, basestring):\n        features = helpers.tointerval(features)\n\n    if not ((isinstance(features, list) or isinstance(features, tuple))):\n        if bins is not None:\n            if not isinstance(bins, int):\n                raise ArgumentError(\n                    \"bins must be an int, got %s\" % type(bins))\n        features = [features]\n        bins = [bins]\n    else:\n        if bins is None:\n            bins = [None for i in features]\n        if not len(bins) == len(features):\n            raise ArgumentError(\n                \"bins must have same length as feature list\")\n\n    # nomenclature:\n    #   \"window\" is region we're getting data for\n    #   \"alignment\" is one item in that region\n    #\n    profiles = []\n    xs = []\n    for window, nbin in zip(features, bins):\n        window = helpers.tointerval(window)\n        chrom = window.chrom\n        start = window.start\n        stop = window.stop\n        strand = window.strand\n\n        if not is_bigwig:\n            # Extend the window to catch reads that would extend into the\n            # requested window\n            _fs = fragment_size or 0\n            padded_window = pybedtools.Interval(\n                chrom,\n                max(start - _fs - shift_width, 0),\n                stop + _fs + shift_width,\n            )\n            window_size = stop - start\n\n            # start off with an array of zeros to represent the window\n            profile = np.zeros(window_size, dtype=float)\n\n            for interval in reader[padded_window]:\n\n                if read_strand:\n                    if interval.strand != read_strand:\n                        continue\n\n                # Shift interval by modeled distance, if specified.\n                if shift_width:\n                    if interval.strand == '-':\n                        interval.start -= shift_width\n                        interval.stop -= shift_width\n                    else:\n                        interval.start += shift_width\n                        interval.stop += shift_width\n\n                # Extend fragment size from 3'\n                if fragment_size:\n                    if interval.strand == '-':\n                        interval.start = interval.stop - fragment_size\n                    else:\n                        interval.stop = interval.start + fragment_size\n\n                # Convert to 0-based coords that can be used as indices into\n                # array\n                start_ind = interval.start - start\n\n                # If the feature goes out of the window, then only include the\n                # part that's inside the window\n                start_ind = max(start_ind, 0)\n\n                # Same thing for stop\n                stop_ind = interval.stop - start\n                stop_ind = min(stop_ind, window_size)\n\n                # Skip if the feature is shifted outside the window. This can\n                # happen with large values of `shift_width`.\n                if start_ind >= window_size or stop_ind < 0:\n                    continue\n\n                # Finally, increment profile\n                if use_score:\n                    score = float(interval.score)\n                else:\n                    score = 1\n\n                if accumulate:\n                    if preserve_total:\n                        profile[start_ind:stop_ind] += (\n                            score / float((stop_ind - start_ind)))\n                    else:\n                        profile[start_ind:stop_ind] += score\n\n                else:\n                    profile[start_ind:stop_ind] = score\n\n        else:  # it's a bigWig\n            profile = reader.summarize(\n                window,\n                method=method,\n                function=function,\n                bins=(nbin or len(window)),\n                zero_inf=zero_inf,\n                zero_nan=zero_nan,\n                )\n\n        # If no bins, return genomic coords\n        if (nbin is None):\n            x = np.arange(start, stop)\n\n        # Otherwise do the downsampling; resulting x is stll in genomic\n        # coords\n        else:\n            if preserve_total:\n                total = float(profile.sum())\n            if not is_bigwig or method == 'get_as_array':\n                xi, profile = rebin(\n                    x=np.arange(start, stop), y=profile, nbin=nbin)\n                if not accumulate:\n                    nonzero = profile != 0\n                    profile[profile != 0] = 1\n                x = xi\n\n            else:\n                x = np.linspace(start, stop - 1, nbin)\n\n        # Minus-strand profiles should be flipped left-to-right.\n        if stranded and strand == '-':\n            profile = profile[::-1]\n        xs.append(x)\n        if preserve_total:\n            scale = profile.sum() / total\n            profile /= scale\n        profiles.append(profile)\n\n    stacked_xs = np.hstack(xs)\n    stacked_profiles = np.hstack(profiles)\n    del xs\n    del profiles\n    return stacked_xs, stacked_profiles", "response": "Internal function that returns a 1D vector of coverage for each local feature in the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _array_parallel(fn, cls, genelist, chunksize=250, processes=1, **kwargs):\n    pool = multiprocessing.Pool(processes)\n    chunks = list(chunker(genelist, chunksize))\n    # pool.map can only pass a single argument to the mapped function, so you\n    # need this trick for passing multiple arguments; idea from\n    # http://stackoverflow.com/questions/5442910/\n    #               python-multiprocessing-pool-map-for-multiple-arguments\n    #\n    results = pool.map(\n        func=_array_star,\n        iterable=itertools.izip(\n            itertools.repeat(fn),\n            itertools.repeat(cls),\n            chunks,\n            itertools.repeat(kwargs)))\n    pool.close()\n    pool.join()\n    return results", "response": "A parallel version of the array_star function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _array_star(args):\n    fn, cls, genelist, kwargs = args\n    return _array(fn, cls, genelist, **kwargs)", "response": "Unpacks the tuple args and calls _array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a meta - feature array with the number of reads falling in each bin of that row s modified feature.", "response": "def _array(fn, cls, genelist, **kwargs):\n    \"\"\"\n    Returns a \"meta-feature\" array, with len(genelist) rows and `bins`\n    cols.  Each row contains the number of reads falling in each bin of\n    that row's modified feature.\n    \"\"\"\n    reader = cls(fn)\n    _local_coverage_func = cls.local_coverage\n    biglist = []\n    if 'bins' in kwargs:\n        if isinstance(kwargs['bins'], int):\n            kwargs['bins'] = [kwargs['bins']]\n\n    for gene in genelist:\n        if not isinstance(gene, (list, tuple)):\n            gene = [gene]\n        coverage_x, coverage_y = _local_coverage_func(\n            reader, gene, **kwargs)\n        biglist.append(coverage_y)\n    return biglist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_spelling(n):\r\n    r = 0\r\n    d = n - 1\r\n    # divmod used for large numbers\r\n    quotient, remainder = divmod(d, 2)\r\n    # while we can still divide 2's into n-1...\r\n    while remainder != 1:\r\n        r += 1\r\n        d = quotient  # previous quotient before we overwrite it\r\n        quotient, remainder = divmod(d, 2)\r\n    return r, d", "response": "Finds d r s. t. n - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^r d - 1 = 2^n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef probably_prime(n, k=10):\r\n    if n == 2:\r\n        return True\r\n    if n % 2 == 0:\r\n        return False\r\n\r\n    r, d = find_spelling(n)\r\n    for check in range(k):\r\n        a = random.randint(2, n - 1)\r\n        x = pow(a, d, n)  # a^d % n\r\n        if x == 1 or x == n - 1:\r\n            continue\r\n        for i in range(r):\r\n            x = pow(x, 2, n)\r\n            if x == n - 1:\r\n                break\r\n        else:\r\n            return False\r\n    return True", "response": "Returns True if n is probably prime False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbins data for bigwig and save to disk. The .npz file will have the pattern {outdir}/{bigwig}.{chrom}.{windowsize}.{metric}.npz and will have two arrays, x (genomic coordinates of midpoints of each window) and y (metric for each window). It can be loaded like this:: d = np.load(filename, mmap_mode='r') bigwig : str or BigWigSignal object BigWig data that will be used to create the array metric : 'covered', 'sum', 'mean0', 'mean' Metric to store in array, as reported by bigWigAverageOverBed: * \"covered\": the number of bases covered by the bigWig. * \"sum\": sum of values over all bases covered * \"mean0\": average over bases with non-covered bases counted as zeros * mean: average over just the covered bases outdir : str or None Where to store output filenames. If None, store the file in the same directory as the bigwig file.", "response": "def to_npz(self, bigwig, metric='mean0', outdir=None):\n        \"\"\"\n        Bin data for bigwig and save to disk.\n\n        The .npz file will have the pattern\n        {outdir}/{bigwig}.{chrom}.{windowsize}.{metric}.npz and will have two\n        arrays, x (genomic coordinates of midpoints of each window) and\n        y (metric for each window).  It can be loaded like this::\n\n            d = np.load(filename, mmap_mode='r')\n\n        bigwig : str or BigWigSignal object\n            BigWig data that will be used to create the array\n\n        metric : 'covered', 'sum', 'mean0', 'mean'\n            Metric to store in array, as reported by bigWigAverageOverBed:\n                * \"covered\": the number of bases covered by the bigWig.\n                * \"sum\": sum of values over all bases covered\n                * \"mean0\": average over bases with non-covered bases counted as\n                  zeros\n                * mean: average over just the covered bases\n\n        outdir : str or None\n            Where to store output filenames.  If None, store the file in the\n            same directory as the bigwig file.\n\n        \"\"\"\n        if isinstance(bigwig, _genomic_signal.BigWigSignal):\n            bigwig = bigwig.fn\n\n        if outdir is None:\n            outdir = os.path.dirname(bigwig)\n\n        basename = os.path.basename(bigwig)\n        windowsize = self.windowsize\n\n        outfiles = []\n        for chrom in self.chroms:\n            tmp_output = pybedtools.BedTool._tmp()\n            windows = self.make_windows(chrom)\n\n            outfile = os.path.join(\n                outdir,\n                '{basename}.{chrom}.{windowsize}.{metric}'.format(**locals())\n                + '.npz')\n            cmds = [\n                'bigWigAverageOverBed',\n                bigwig,\n                windows,\n                tmp_output]\n            os.system(' '.join(cmds))\n            names = ['name', 'size', 'covered', 'sum', 'mean0', 'mean']\n            df = pandas.read_table(tmp_output, names=names)\n            x = df.size.cumsum() - df.size / 2\n            y = df[metric]\n            np.savez(outfile, x=x, y=y)\n            outfiles.append(outfile)\n            del x, y, df\n        return outfiles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare(signal1, signal2, features, outfn, comparefunc=np.subtract,\n        batchsize=5000, array_kwargs=None, verbose=False):\n    \"\"\"\n    Compares two genomic signal objects and outputs results as a bedGraph file.\n    Can be used for entire genome-wide comparisons due to its parallel nature.\n\n    Typical usage would be to create genome-wide windows of equal size to\n    provide as `features`::\n\n        windowsize = 10000\n        features = pybedtools.BedTool().window_maker(\n           genome='hg19', w=windowsize)\n\n    You will usually want to choose bins for the array based on the final\n    resolution you would like. Say you would like 10-bp bins in the final\n    bedGraph; using the example above you would use array_kwargs={'bins':\n    windowsize/10}.  Or, for single-bp resolution (beware: file will be large),\n    use {'bins': windowsize}.\n\n    Here's how it works.  This function:\n\n        * Takes `batchsize` features at a time from `features`\n\n        * Constructs normalized (RPMMR) arrays in parallel for each input\n          genomic signal object for those `batchsize` features\n\n        * Applies `comparefunc` (np.subtract by default) to the arrays to get\n          a \"compared\" (e.g., difference matrix by default) for the `batchsize`\n          features.\n\n        * For each row in this matrix, it outputs each nonzero column as\n          a bedGraph format line in `outfn`\n\n    `comparefunc` is a function with the signature::\n\n        def f(x, y):\n            return z\n\n    where `x` and `y` will be arrays for `signal1` and `signal2` (normalized to\n    RPMMR) and `z` is a new array.  By default this is np.subtract, but another\n    common `comparefunc` might be a log2-fold-change function::\n\n        def lfc(x, y):\n            return np.log2(x / y)\n\n    :param signal1: A genomic_signal object\n    :param signal2: Another genomic_signal object\n    :param features: An iterable of pybedtools.Interval objects. A list will be\n        created for every `batchsize` features, so you need enough memory for\n        this.\n    :param comparefunc: Function to use to compare arrays (default is\n        np.subtract)\n    :param outfn: String filename to write bedGraph file\n    :param batchsize: Number of features (each with length `windowsize` bp) to\n        process at a time\n    :param array_kwargs: Kwargs passed directly to genomic_signal.array.  Needs\n        `processes` and `chunksize` if you want parallel processing\n    :param verbose: Be noisy\n    \"\"\"\n    fout = open(outfn, 'w')\n    fout.write('track type=bedGraph\\n')\n\n    i = 0\n    this_batch = []\n    for feature in features:\n        if i <= batchsize:\n            this_batch.append(feature)\n            i += 1\n            continue\n\n        if verbose:\n            print 'working on batch of %s' % batchsize\n            sys.stdout.flush()\n\n        arr1 = signal1.array(this_batch, **array_kwargs).astype(float)\n        arr2 = signal2.array(this_batch, **array_kwargs).astype(float)\n        arr1 /= signal1.million_mapped_reads()\n        arr2 /= signal2.million_mapped_reads()\n        compared = comparefunc(arr1, arr2)\n\n        for feature, row in itertools.izip(this_batch, compared):\n            start = feature.start\n            bins = len(row)\n            binsize = len(feature) / len(row)\n\n            # Quickly move on if nothing here.  speed increase prob best for\n            # sparse data\n            if sum(row) == 0:\n                continue\n\n            for j in range(0, len(row)):\n                score = row[j]\n                stop = start + binsize\n                if score != 0:\n                    fout.write('\\t'.join([\n                        feature.chrom,\n                        str(start),\n                        str(stop),\n                        str(score)]) + '\\n')\n                start = start + binsize\n        this_batch = []\n        i = 0\n    fout.close()", "response": "Compares two genomic signal objects and outputs results as a bedGraph file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind next prime number between N and 3.", "response": "def _find_next_prime(N):\n    \"\"\"Find next prime >= N\"\"\"\n    def is_prime(n):\n        if n % 2 == 0:\n            return False\n        i = 3\n        while i * i <= n:\n            if n % i:\n                i += 2\n            else:\n                return False\n        return True\n    if N < 3:\n        return 2\n    if N % 2 == 0:\n        N += 1\n    for n in range(N, 2*N, 2):\n        if is_prime(n):\n            return n\n    raise AssertionError(\"Failed to find a prime number between {0} and {1}...\".format(N, 2*N))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new h5py. File object with cache specification.", "response": "def File(name, mode='a', chunk_cache_mem_size=1024**2, w0=0.75, n_cache_chunks=None, **kwds):\n    \"\"\"Create h5py File object with cache specification\n\n    This function is basically just a wrapper around the usual h5py.File constructor,\n    but accepts two additional keywords:\n\n    Parameters\n    ----------\n    name : str\n    mode : str\n    **kwds : dict (as keywords)\n        Standard h5py.File arguments, passed to its constructor\n    chunk_cache_mem_size : int\n        Number of bytes to use for the chunk cache.  Defaults to 1024**2 (1MB), which\n        is also the default for h5py.File -- though it cannot be changed through the\n        standard interface.\n    w0 : float between 0.0 and 1.0\n        Eviction parameter.  Defaults to 0.75.  \"If the application will access the\n        same data more than once, w0 should be set closer to 0, and if the application\n        does not, w0 should be set closer to 1.\"\n          --- <https://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/>\n    n_cache_chunks : int\n        Number of chunks to be kept in cache at a time.  Defaults to the (smallest\n        integer greater than) the square root of the number of elements that can fit\n        into memory.  This is just used for the number of slots (nslots) maintained\n        in the cache metadata, so it can be set larger than needed with little cost.\n\n    \"\"\"\n    import sys\n    import numpy as np\n    import h5py\n    name = name.encode(sys.getfilesystemencoding())\n    open(name, mode).close()  # Just make sure the file exists\n    if mode in [m+b for m in ['w', 'w+', 'r+', 'a', 'a+'] for b in ['', 'b']]:\n        mode = h5py.h5f.ACC_RDWR\n    else:\n        mode = h5py.h5f.ACC_RDONLY\n    if 'dtype' in kwds:\n        bytes_per_object = np.dtype(kwds['dtype']).itemsize\n    else:\n        bytes_per_object = np.dtype(np.float).itemsize  # assume float as most likely\n    if not n_cache_chunks:\n        n_cache_chunks = int(np.ceil(np.sqrt(chunk_cache_mem_size / bytes_per_object)))\n    nslots = _find_next_prime(100 * n_cache_chunks)\n    propfaid = h5py.h5p.create(h5py.h5p.FILE_ACCESS)\n    settings = list(propfaid.get_cache())\n    settings[1:] = (nslots, chunk_cache_mem_size, w0)\n    propfaid.set_cache(*settings)\n    return h5py.File(h5py.h5f.open(name, flags=mode, fapl=propfaid), **kwds)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(c, prefix, relative_paths=True):\n    dirname = os.path.dirname(prefix)\n\n    pybedtools.BedTool(c.features).saveas(prefix + '.intervals')\n\n    def usepath(f):\n        if relative_paths:\n            return os.path.relpath(f, start=dirname)\n        else:\n            return os.path.abspath(f)\n\n    with open(prefix + '.info', 'w') as fout:\n        info = {\n            'ip_bam': usepath(c.ip.fn),\n            'control_bam': usepath(c.control.fn),\n            'array_kwargs': c.array_kwargs,\n            'dbfn': usepath(c.dbfn),\n            'browser_local_coverage_kwargs': c.browser_local_coverage_kwargs,\n            'relative_paths': relative_paths,\n        }\n        fout.write(yaml.dump(info, default_flow_style=False))\n    np.savez(\n        prefix,\n        diffed_array=c.diffed_array,\n        ip_array=c.ip_array,\n        control_array=c.control_array\n    )", "response": "Save data from a Chipseq object to a new Chipseq object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the shift width of a ChIP - seq by cross - correlation on the signal.", "response": "def estimate_shift(signal, genome=None, windowsize=5000, thresh=None,\n                   nwindows=1000, maxlag=500, array_kwargs=None,\n                   verbose=False):\n    \"\"\"\n    Experimental: cross-correlation to estimate the shift width of ChIP-seq\n    data\n\n    This can be interpreted as the binding site footprint.\n\n    For ChIP-seq, the plus and minus strand reads tend to be shifted in the 5'\n    direction away from each other.  Various ChIP-seq peak-callers estimate\n    this distance; this function provides a quick, tunable way to do so using\n    cross-correlation.  The resulting shift can then be incorporated into\n    subsequent calls to `array` by adding the shift_width kwarg.\n\n\n    :param signal: genomic_signal object\n    :param genome: String assembly for constructing windows\n    :param nwindows: Number of windows to compute cross-correlation on\n    :param windowsize: Size of each window to compute cross-correlation on.\n    :param thresh: Threshold read coverage to run cross-correlation on.  This\n        is likely to be a function of the fragment size provided in\n        `array_kwargs` `windowsize`.  If `thresh` is small, then the cross\n        correlation can be noisy.\n    :param maxlag: Max shift to look for\n    :param array_kwargs: Kwargs passed directly to genomic_signal.array, with\n        the default of `bins=windowsize` for single-bp resolution, and\n        `read_strand` will be overwritten.\n    :param verbose: Be verbose.\n\n    Returns lags and a `maxlag*2+1` x `nwindows` matrix of cross-correlations.\n\n    You can then plot the average cross-correlation function with::\n\n        plt.plot(lags, shift.mean(axis=0))\n\n    and get the distance to shift with::\n\n        d = lags[np.argmax(shift.mean(axis=0))]\n\n    and then plot that with::\n\n        plt.axvline(d, color='k', linestyle='--')\n\n    The number of windows with at least `thresh` coverage is::\n\n        shift.shape[0]\n    \"\"\"\n    if thresh is None:\n        thresh = 0\n\n    if genome is None:\n        genome = signal.genome()\n\n    if array_kwargs is None:\n        array_kwargs = {}\n\n    array_kwargs.pop('read_strand', None)\n\n    if 'bins' not in array_kwargs:\n        array_kwargs['bins'] = windowsize\n\n    def add_strand(f, strand):\n        fields = f.fields[:]\n        while len(fields) < 5:\n            fields.append('.')\n        fields.append(strand)\n        return pybedtools.create_interval_from_list(fields)\n\n    windows = pybedtools.BedTool()\\\n        .window_maker(genome=genome, w=windowsize)\n\n    random_subset = pybedtools.BedTool(windows[:nwindows])\\\n        .shuffle(genome=genome).saveas()\n\n    if verbose:\n        sys.stderr.write(\"Getting plus-strand signal for %s regions...\\n\"\n                         % nwindows)\n        sys.stderr.flush()\n\n    plus = signal.array(\n        features=random_subset,\n        read_strand=\"+\",\n        **array_kwargs).astype(float)\n\n    if verbose:\n        sys.stderr.write(\"Getting minus-strand signal for %s regions...\\n\"\n                         % nwindows)\n        sys.stderr.flush()\n\n    minus = signal.array(\n        features=random_subset,\n        read_strand=\"-\",\n        **array_kwargs).astype(float)\n\n    # only do cross-correlation if you have enough reads to do so\n    enough = ((plus.sum(axis=1) / windowsize) > thresh) \\\n        & ((minus.sum(axis=1) / windowsize) > thresh)\n\n    if verbose:\n        sys.stderr.write(\n            \"Running cross-correlation on %s regions that passed \"\n            \"threshold\\n\" % sum(enough))\n    results = np.zeros((sum(enough), 2 * maxlag + 1))\n    for i, xy in enumerate(izip(plus[enough], minus[enough])):\n        x, y = xy\n        results[i] = xcorr(x, y, maxlag)\n\n    lags = np.arange(-maxlag, maxlag + 1)\n\n    return lags, results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstreamlining version of matplotlib s xcorr with the plots.", "response": "def xcorr(x, y, maxlags):\n    \"\"\"\n    Streamlined version of matplotlib's `xcorr`, without the plots.\n\n    :param x, y: NumPy arrays to cross-correlate\n    :param maxlags: Max number of lags; result will be `2*maxlags+1` in length\n    \"\"\"\n    xlen = len(x)\n    ylen = len(y)\n    assert xlen == ylen\n\n    c = np.correlate(x, y, mode=2)\n\n    # normalize\n    c /= np.sqrt(np.dot(x, x) * np.dot(y, y))\n\n    lags = np.arange(-maxlags, maxlags + 1)\n    c = c[xlen - 1 - maxlags:xlen + maxlags]\n\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscale the control and IP data to million mapped reads, then subtracts scaled control from scaled IP, applies `func(diffed)` to the diffed array, and finally sets `self.diffed_array` to be the result. Arrays `self.ip` and `self.control` are set as well, and if `force=False`, then previously-created arrays will be used instead of re-calculating new ones. This is useful if you want to easily try multiple `func` functions without having to re-calculate the data. Another side-effect is that `self.features` is set so that it can be accesed by other methods. :param features: a list of pybedtools.Interval objects :param array_kwargs: extra keyword args passed to genomic_signal.array; typically this will include `bins`, `processes`, and `chunksize` arguments. :param func: a function to apply to the diffed arrays. By default this is :func:`metaseq.plotutils.nice_log`; another option might be `lambda x: x`, or `lambda x: 1e6*x` :param force: Force a re-calculation of the arrays; otherwise uses cached values", "response": "def diff_array(self, features, force=True, func=None,\n                   array_kwargs=dict(), cache=None):\n        \"\"\"\n        Scales the control and IP data to million mapped reads, then subtracts\n        scaled control from scaled IP, applies `func(diffed)` to the diffed\n        array, and finally sets `self.diffed_array` to be the result.\n\n        Arrays `self.ip` and `self.control` are set as well, and if\n        `force=False`, then previously-created arrays will be used instead of\n        re-calculating new ones.  This is useful if you want to easily try\n        multiple `func` functions without having to re-calculate the data.\n\n        Another side-effect is that `self.features` is set so that it can be\n        accesed by other methods.\n\n        :param features: a list of pybedtools.Interval objects\n        :param array_kwargs: extra keyword args passed to genomic_signal.array;\n            typically this will include `bins`, `processes`, and `chunksize`\n            arguments.\n        :param func: a function to apply to the diffed arrays. By default\n            this is :func:`metaseq.plotutils.nice_log`; another option might be\n            `lambda x: x`, or `lambda x: 1e6*x`\n        :param force: Force a re-calculation of the arrays; otherwise uses\n            cached values\n        \"\"\"\n        self.features = list(features)\n        self.browser_local_coverage_kwargs = array_kwargs.copy()\n        self.browser_local_coverage_kwargs.pop('processes', None)\n        self.browser_local_coverage_kwargs.pop('chunksize', 1)\n\n        self.array_kwargs = array_kwargs.copy()\n\n        if (self.ip_array is None) or force:\n            self.ip_array = self.ip.array(features, **array_kwargs)\n            self.ip_array /= self.ip.mapped_read_count() / 1e6\n\n        if (self.control_array is None) or force:\n            self.control_array = self.control.array(features, **array_kwargs)\n            self.control_array /= self.control.mapped_read_count() / 1e6\n\n        if func is None:\n            #func = metaseq.plotutils.nice_log\n            self.diffed_array = self.ip_array - self.control_array\n        else:\n            self.diffed_array = func(self.ip_array - self.control_array)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, x, row_order=None, imshow_kwargs=None, strip=True):\n        nrows = self.diffed_array.shape[0]\n        if row_order is None:\n            row_order = np.arange(nrows)\n        extent = (min(x), max(x), 0, nrows)\n        axes_info = metaseq.plotutils.matrix_and_line_shell(strip=strip)\n        fig, matrix_ax, line_ax, strip_ax, cbar_ax = axes_info\n        _imshow_kwargs = dict(\n            aspect='auto', extent=extent, interpolation='nearest')\n        if imshow_kwargs:\n            _imshow_kwargs.update(imshow_kwargs)\n\n        if 'cmap' not in _imshow_kwargs:\n            _imshow_kwargs['cmap'] = metaseq.colormap_adjust.smart_colormap(\n                self.diffed_array.min(),\n                self.diffed_array.max()\n            )\n        mappable = matrix_ax.imshow(\n            self.diffed_array[row_order],\n            **_imshow_kwargs)\n        plt.colorbar(mappable, cbar_ax)\n        line_ax.plot(x, self.diffed_array.mean(axis=0))\n        if strip_ax:\n            line, = strip_ax.plot(np.zeros((nrows,)), np.arange(nrows) + 0.5,\n                                  **self._strip_kwargs)\n            line.features = self.features\n            line.ind = row_order\n\n        matrix_ax.axis('tight')\n        if strip_ax:\n            strip_ax.xaxis.set_visible(False)\n            matrix_ax.yaxis.set_visible(False)\n\n        matrix_ax.xaxis.set_visible(False)\n\n        if self.db:\n            self.minibrowser = GeneChipseqMiniBrowser(\n                [self.ip, self.control],\n                db=self.db,\n                plotting_kwargs=self.browser_plotting_kwargs,\n                local_coverage_kwargs=self.browser_local_coverage_kwargs)\n        else:\n            self.minibrowser = SignalChipseqMiniBrowser(\n                [self.ip, self.control],\n                plotting_kwargs=self.browser_plotting_kwargs,\n                local_coverage_kwargs=self.browser_local_coverage_kwargs)\n\n        fig.canvas.mpl_connect('pick_event', self.callback)\n\n        self.fig = fig\n        self.axes = {\n            'matrix_ax': matrix_ax,\n            'strip_ax': strip_ax,\n            'line_ax': line_ax,\n            'cbar_ax': cbar_ax\n        }", "response": "Plot the scaled ChIP - seq data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef observe(self, event, fn):\n        iscoroutine = asyncio.iscoroutinefunction(fn)\n        if not iscoroutine and not isfunction(fn):\n            raise TypeError('paco: fn param must be a callable '\n                            'object or coroutine function')\n\n        observers = self._pool.get(event)\n        if not observers:\n            observers = self._pool[event] = []\n\n        # Register the observer\n        observers.append(fn if iscoroutine else coroutine_wrapper(fn))", "response": "Subscribe to an event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, event=None):\n        observers = self._pool.get(event)\n        if observers:\n            self._pool[event] = []", "response": "Removes all the registered observers for the given event name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntriggering the given event and passes custom variadic arguments.", "response": "def trigger(self, event, *args, **kw):\n        \"\"\"\n        Triggers event observers for the given event name,\n        passing custom variadic arguments.\n        \"\"\"\n        observers = self._pool.get(event)\n\n        # If no observers registered for the event, do no-op\n        if not observers or len(observers) == 0:\n            return None\n\n        # Trigger observers coroutines in FIFO sequentially\n        for fn in observers:\n            # Review: perhaps this should not wait\n            yield from fn(*args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshows text and a caret under that.", "response": "def error(self, s, pos):\n        \"\"\"Show text and a caret under that. For example:\nx = 2y + z\n     ^\n\"\"\"\n        print(\"Lexical error:\")\n        print(\"%s\" % s[:pos+10])  # + 10 for trailing context\n        print(\"%s^\" % (\" \"*(pos-1)))\n        for t in self.rv: print(t)\n        raise SystemExit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a function that accepts one or more arguments of a function and either invokes func returning its result if at least arity number of arguments have been provided, or returns a function that accepts the remaining function arguments until the function arity is satisfied. This function is overloaded: you can pass a function or coroutine function as first argument or an `int` indicating the explicit function arity. Function arity can be inferred via function signature or explicitly passed via `arity_or_fn` param. You can optionally ignore keyword based arguments as well passsing the `ignore_kwargs` param with `True` value. This function can be used as decorator. Arguments: arity_or_fn (int|function|coroutinefunction): function arity to curry or function to curry. ignore_kwargs (bool): ignore keyword arguments as arity to satisfy during curry. evaluator (function): use a custom arity evaluator function. *args (mixed): mixed variadic arguments for partial function application. *kwargs (mixed): keyword variadic arguments for partial function application. Raises: TypeError: if function is not a function or a coroutine function. Returns: function or coroutinefunction: function will be returned until all the function arity is satisfied, where a coroutine function will be returned instead. Usage:: # Function signature inferred function arity @paco.curry async def task(x, y, z=0): return x * y + z await task(4)(4)(z=8) # => 24 # User defined function arity @paco.curry(4) async def task(x, y, *args, **kw): return x * y + args[0] * args[1] await task(4)(4)(8)(8) # => 80 # Ignore keyword arguments from arity @paco.curry(ignore_kwargs=True) async def task(x, y, z=0): return x * y await task(4)(4) # => 16", "response": "def curry(arity_or_fn=None, ignore_kwargs=False, evaluator=None, *args, **kw):\n    \"\"\"\n    Creates a function that accepts one or more arguments of a function and\n    either invokes func returning its result if at least arity number of\n    arguments have been provided, or returns a function that accepts the\n    remaining function arguments until the function arity is satisfied.\n\n    This function is overloaded: you can pass a function or coroutine function\n    as first argument or an `int` indicating the explicit function arity.\n\n    Function arity can be inferred via function signature or explicitly\n    passed via `arity_or_fn` param.\n\n    You can optionally ignore keyword based arguments as well passsing the\n    `ignore_kwargs` param with `True` value.\n\n    This function can be used as decorator.\n\n    Arguments:\n        arity_or_fn (int|function|coroutinefunction): function arity to curry\n            or function to curry.\n        ignore_kwargs (bool): ignore keyword arguments as arity to satisfy\n            during curry.\n        evaluator (function): use a custom arity evaluator function.\n        *args (mixed): mixed variadic arguments for partial function\n            application.\n        *kwargs (mixed): keyword variadic arguments for partial function\n            application.\n\n    Raises:\n        TypeError: if function is not a function or a coroutine function.\n\n    Returns:\n        function or coroutinefunction: function will be returned until all the\n            function arity is satisfied, where a coroutine function will be\n            returned instead.\n\n    Usage::\n\n        # Function signature inferred function arity\n        @paco.curry\n        async def task(x, y, z=0):\n            return x * y + z\n\n        await task(4)(4)(z=8)\n        # => 24\n\n        # User defined function arity\n        @paco.curry(4)\n        async def task(x, y, *args, **kw):\n            return x * y + args[0] * args[1]\n\n        await task(4)(4)(8)(8)\n        # => 80\n\n        # Ignore keyword arguments from arity\n        @paco.curry(ignore_kwargs=True)\n        async def task(x, y, z=0):\n            return x * y\n\n        await task(4)(4)\n        # => 16\n\n    \"\"\"\n    def isvalidarg(x):\n        return all([\n            x.kind != x.VAR_KEYWORD,\n            x.kind != x.VAR_POSITIONAL,\n            any([\n                not ignore_kwargs,\n                ignore_kwargs and x.default == x.empty\n            ])\n        ])\n\n    def params(fn):\n        return inspect.signature(fn).parameters.values()\n\n    def infer_arity(fn):\n        return len([x for x in params(fn) if isvalidarg(x)])\n\n    def merge_args(acc, args, kw):\n        _args, _kw = acc\n        _args = _args + args\n        _kw = _kw or {}\n        _kw.update(kw)\n        return _args, _kw\n\n    def currier(arity, acc, fn, *args, **kw):\n        \"\"\"\n        Function either continues curring of the arguments\n        or executes function if desired arguments have being collected.\n        If function curried is variadic then execution without arguments\n        will finish curring and trigger the function\n        \"\"\"\n        # Merge call arguments with accumulated ones\n        _args, _kw = merge_args(acc, args, kw)\n\n        # Get current function call accumulated arity\n        current_arity = len(args)\n\n        # Count keyword params as arity to satisfy, if required\n        if not ignore_kwargs:\n            current_arity += len(kw)\n\n        # Decrease function arity to satisfy\n        arity -= current_arity\n\n        # Use user-defined custom arity evaluator strategy, if present\n        currify = evaluator and evaluator(acc, fn)\n\n        # If arity is not satisfied, return recursive partial function\n        if currify is not False and arity > 0:\n            return functools.partial(currier, arity, (_args, _kw), fn)\n\n        # If arity is satisfied, instanciate coroutine and return it\n        return fn(*_args, **_kw)\n\n    def wrapper(fn, *args, **kw):\n        if not iscallable(fn):\n            raise TypeError('paco: first argument must a coroutine function, '\n                            'a function or a method.')\n\n        # Infer function arity, if required\n        arity = (arity_or_fn if isinstance(arity_or_fn, int)\n                 else infer_arity(fn))\n\n        # Wraps function as coroutine function, if needed.\n        fn = wraps(fn) if isfunc(fn) else fn\n\n        # Otherwise return recursive currier function\n        return currier(arity, (args, kw), fn, *args, **kw) if arity > 0 else fn\n\n    # Return currier function or decorator wrapper\n    return (wrapper(arity_or_fn, *args, **kw)\n            if iscallable(arity_or_fn)\n            else wrapper)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compose(*coros):\n    # Make list to inherit built-in type methods\n    coros = list(coros)\n\n    @asyncio.coroutine\n    def reducer(acc, coro):\n        return (yield from coro(acc))\n\n    @asyncio.coroutine\n    def wrapper(acc):\n        return (yield from reduce(reducer, coros,\n                                  initializer=acc, right=True))\n\n    return wrapper", "response": "A function that returns a coroutine function that can be used to compose a list of coroutine functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_config():\n\n    genius_key = input('Enter Genius key : ')\n    bing_key = input('Enter Bing key : ')\n\n    CONFIG['keys']['bing_key'] = bing_key\n    CONFIG['keys']['genius_key'] = genius_key\n\n    with open(config_path, 'w') as configfile:\n        CONFIG.write(configfile)", "response": "Prompts user for API keys adds them in an. ini file stored in the same location as the script\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tracks_from_album(album_name):\n    '''\n    Gets tracks from an album using Spotify's API\n    '''\n\n    spotify = spotipy.Spotify()\n\n    album = spotify.search(q='album:' + album_name, limit=1)\n    album_id = album['tracks']['items'][0]['album']['id']\n    results = spotify.album_tracks(album_id=str(album_id))\n    songs = []\n    for items in results['items']:\n        songs.append(items['name'])\n\n    return songs", "response": "Gets tracks from an album using Spotify s API\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides user with a list of songs to choose from returns the url of chosen song.", "response": "def get_url(song_input, auto):\n    '''\n    Provides user with a list of songs to choose from\n    returns the url of chosen song.\n    '''\n    youtube_list = OrderedDict()\n    num = 0  # List of songs index\n\n    html = requests.get(\"https://www.youtube.com/results\",\n                        params={'search_query': song_input})\n    soup = BeautifulSoup(html.text, 'html.parser')\n\n    # In all Youtube Search Results\n\n    for i in soup.findAll('a', {'rel': YOUTUBECLASS}):\n        song_url = 'https://www.youtube.com' + (i.get('href'))\n        song_title = (i.get('title'))\n        # Adds title and song url to dictionary\n        youtube_list.update({song_title: song_url})\n\n        if not auto:\n            print('(%s) %s' % (str(num + 1), song_title))  # Prints list\n            num = num + 1\n\n        elif auto:\n            print(song_title)\n            return list(youtube_list.values())[0], list(youtube_list.keys())[0]\n\n    # Checks if YouTube search return no results\n    if youtube_list == {}:\n        log.log_error('No match found!')\n        exit()\n\n    # Gets the demanded song title and url\n    song_url, song_title = prompt(youtube_list)\n\n    return song_url, song_title"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt for song number from list of songs", "response": "def prompt(youtube_list):\n    '''\n    Prompts for song number from list of songs\n    '''\n\n    option = int(input('\\nEnter song number > '))\n    try:\n        song_url = list(youtube_list.values())[option - 1]\n        song_title = list(youtube_list.keys())[option - 1]\n    except IndexError:\n        log.log_error('Invalid Input')\n        exit()\n\n    system('clear')\n    print('Download Song: ')\n    print(song_title)\n    print('Y/n?')\n    confirm = input('>')\n    if confirm == '' or confirm.lower() == 'y':\n        pass\n    elif confirm.lower() == 'n':\n        exit()\n    else:\n        log.log_error('Invalid Input')\n        exit()\n\n    return song_url, song_title"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts here, handles arguments", "response": "def main():\n    '''\n    Starts here, handles arguments\n    '''\n\n    system('clear') # Must be system('cls') for windows\n\n    setup()\n\n    parser = argparse.ArgumentParser(\n        description='Download songs with album art and metadata!')\n    parser.add_argument('-c', '--config', action='store_true',\n                        help='Set your API keys')\n    parser.add_argument('-m', '--multiple', action='store', dest='multiple_file',\n                        help='Download multiple songs from a text file list')\n    parser.add_argument('-a', '--auto', action='store_true',\n                        help='Automatically chooses top result')\n    parser.add_argument('--album', action='store_true',\n                        help='Downloads all songs from an album')\n    args = parser.parse_args()\n    arg_multiple = args.multiple_file or None\n    arg_auto = args.auto or None\n    arg_album = args.album or None\n    arg_config = args.config\n\n\n    if arg_config:\n        add_config()\n\n    elif arg_multiple and arg_album:\n        log.log_error(\"Can't do both!\")\n\n    elif arg_album:\n        album_name = input('Enter album name : ')\n        try:\n            tracks = get_tracks_from_album(album_name)\n            for songs in tracks:\n                print(songs)\n            confirm = input(\n                '\\nAre these the songs you want to download? (Y/n)\\n> ')\n\n        except IndexError:\n            log.log_error(\"Couldn't find album\")\n            exit()\n\n        if confirm == '' or confirm.lower() == ('y'):\n            for track_name in tracks:\n                track_name = track_name + ' song'\n                song_url, file_name = get_url(track_name, arg_auto)\n                download_song(song_url, file_name)\n                system('clear')\n                repair.fix_music(file_name + '.mp3')\n\n        elif confirm.lower() == 'n':\n            log.log_error(\"Sorry, if appropriate results weren't found\")\n            exit()\n        else:\n            log.log_error('Invalid Input')\n            exit()\n\n    elif arg_multiple:\n        with open(arg_multiple, \"r\") as f:\n            file_names = []\n            for line in f:\n                file_names.append(line.rstrip('\\n'))\n\n        for files in file_names:\n            files = files + ' song'\n            song_url, file_name = get_url(files, arg_auto)\n            download_song(song_url, file_name)\n            system('clear')\n            repair.fix_music(file_name + '.mp3')\n\n    else:\n        query = input('Enter Song Name : ')\n        song_url, file_name = get_url(query, arg_auto)  # Gets YT url\n        download_song(song_url, file_name)  # Saves as .mp3 file\n        system('clear')\n        repair.fix_music(file_name + '.mp3')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getRawReportDescriptor(self):\n        descriptor = _hidraw_report_descriptor()\n        size = ctypes.c_uint()\n        self._ioctl(_HIDIOCGRDESCSIZE, size, True)\n        descriptor.size = size\n        self._ioctl(_HIDIOCGRDESC, descriptor, True)\n        return ''.join(chr(x) for x in descriptor.value[:size.value])", "response": "Return the raw HID report descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a DevInfo instance with the most recent information about the current system.", "response": "def getInfo(self):\n        \"\"\"\n        Returns a DevInfo instance, a named tuple with the following items:\n        - bustype: one of BUS_USB, BUS_HIL, BUS_BLUETOOTH or BUS_VIRTUAL\n        - vendor: device's vendor number\n        - product: device's product number\n        \"\"\"\n        devinfo = _hidraw_devinfo()\n        self._ioctl(_HIDIOCGRAWINFO, devinfo, True)\n        return DevInfo(devinfo.bustype, devinfo.vendor, devinfo.product)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the name of the HID device.", "response": "def getName(self, length=512):\n        \"\"\"\n        Returns device name as an unicode object.\n        \"\"\"\n        name = ctypes.create_string_buffer(length)\n        self._ioctl(_HIDIOCGRAWNAME(length), name, True)\n        return name.value.decode('UTF-8')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the physical address of the current system as a string.", "response": "def getPhysicalAddress(self, length=512):\n        \"\"\"\n        Returns device physical address as a string.\n        See hidraw documentation for value signification, as it depends on\n        device's bus type.\n        \"\"\"\n        name = ctypes.create_string_buffer(length)\n        self._ioctl(_HIDIOCGRAWPHYS(length), name, True)\n        return name.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sendFeatureReport(self, report, report_num=0):\n        length = len(report) + 1\n        buf = bytearray(length)\n        buf[0] = report_num\n        buf[1:] = report\n        self._ioctl(\n            _HIDIOCSFEATURE(length),\n            (ctypes.c_char * length).from_buffer(buf),\n            True,\n        )", "response": "Send a feature report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getFeatureReport(self, report_num=0, length=63):\n        length += 1\n        buf = bytearray(length)\n        buf[0] = report_num\n        self._ioctl(\n            _HIDIOCGFEATURE(length),\n            (ctypes.c_char * length).from_buffer(buf),\n            True,\n        )\n        return buf", "response": "Receive a feature report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all the values in coll which pass an asynchronous truth test coroutine.", "response": "def filter(coro, iterable, assert_fn=None, limit=0, loop=None):\n    \"\"\"\n    Returns a list of all the values in coll which pass an asynchronous truth\n    test coroutine.\n\n    Operations are executed concurrently by default, but results\n    will be in order.\n\n    You can configure the concurrency via `limit` param.\n\n    This function is the asynchronous equivalent port Python built-in\n    `filter()` function.\n\n    This function is a coroutine.\n\n    This function can be composed in a pipeline chain with ``|`` operator.\n\n    Arguments:\n        coro (coroutine function): coroutine filter function to call accepting\n            iterable values.\n        iterable (iterable|asynchronousiterable): an iterable collection\n            yielding coroutines functions.\n        assert_fn (coroutinefunction): optional assertion function.\n        limit (int): max filtering concurrency limit. Use ``0`` for no limit.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n\n    Raises:\n        TypeError: if coro argument is not a coroutine function.\n\n    Returns:\n        list: ordered list containing values that passed\n            the filter.\n\n    Usage::\n\n        async def iseven(num):\n            return num % 2 == 0\n\n        async def assert_false(el):\n            return not el\n\n        await paco.filter(iseven, [1, 2, 3, 4, 5])\n        # => [2, 4]\n\n        await paco.filter(iseven, [1, 2, 3, 4, 5], assert_fn=assert_false)\n        # => [1, 3, 5]\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n    assert_iter(iterable=iterable)\n\n    # Check valid or empty iterable\n    if len(iterable) == 0:\n        return iterable\n\n    # Reduced accumulator value\n    results = [None] * len(iterable)\n\n    # Use a custom or default filter assertion function\n    assert_fn = assert_fn or assert_true\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop)\n\n    # Reducer partial function for deferred coroutine execution\n    def filterer(index, element):\n        @asyncio.coroutine\n        def wrapper():\n            result = yield from coro(element)\n            if (yield from assert_fn(result)):\n                results[index] = element\n        return wrapper\n\n    # Iterate and attach coroutine for defer scheduling\n    for index, element in enumerate(iterable):\n        pool.add(filterer(index, element))\n\n    # Wait until all coroutines finish\n    yield from pool.run(ignore_empty=True)\n\n    # Returns filtered elements\n    return [x for x in results if x is not None]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning `True` if every element in a given iterable satisfies the coroutine asynchronous test. If any iteratee coroutine call returns `False`, the process is inmediately stopped, and `False` will be returned. You can increase the concurrency limit for a fast race condition scenario. This function is a coroutine. This function can be composed in a pipeline chain with ``|`` operator. Arguments: coro (coroutine function): coroutine function to call with values to reduce. iterable (iterable): an iterable collection yielding coroutines functions. limit (int): max concurrency execution limit. Use ``0`` for no limit. loop (asyncio.BaseEventLoop): optional event loop to use. Raises: TypeError: if input arguments are not valid. Returns: bool: `True` if all the values passes the test, otherwise `False`. Usage:: async def gt_10(num): return num > 10 await paco.every(gt_10, [1, 2, 3, 11]) # => False await paco.every(gt_10, [11, 12, 13]) # => True", "response": "def every(coro, iterable, limit=1, loop=None):\n    \"\"\"\n    Returns `True` if every element in a given iterable satisfies the coroutine\n    asynchronous test.\n\n    If any iteratee coroutine call returns `False`, the process is inmediately\n    stopped, and `False` will be returned.\n\n    You can increase the concurrency limit for a fast race condition scenario.\n\n    This function is a coroutine.\n\n    This function can be composed in a pipeline chain with ``|`` operator.\n\n    Arguments:\n        coro (coroutine function): coroutine function to call with values\n            to reduce.\n        iterable (iterable): an iterable collection yielding\n            coroutines functions.\n        limit (int): max concurrency execution limit. Use ``0`` for no limit.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n\n    Raises:\n        TypeError: if input arguments are not valid.\n\n    Returns:\n        bool: `True` if all the values passes the test, otherwise `False`.\n\n    Usage::\n\n        async def gt_10(num):\n            return num > 10\n\n        await paco.every(gt_10, [1, 2, 3, 11])\n        # => False\n\n        await paco.every(gt_10, [11, 12, 13])\n        # => True\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n    assert_iter(iterable=iterable)\n\n    # Reduced accumulator value\n    passes = True\n\n    # Handle empty iterables\n    if len(iterable) == 0:\n        return passes\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop)\n\n    # Tester function to guarantee the file is canceled.\n    @asyncio.coroutine\n    def tester(element):\n        nonlocal passes\n        if not passes:\n            return None\n\n        if not (yield from coro(element)):\n            # Flag as not test passed\n            passes = False\n            # Force ignoring pending coroutines\n            pool.cancel()\n\n    # Iterate and attach coroutine for defer scheduling\n    for element in iterable:\n        pool.add(partial(tester, element))\n\n    # Wait until all coroutines finish\n    yield from pool.run()\n\n    return passes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining settings like being in the interface", "response": "def define_settings(ctx, model, values):\n    \"\"\" Define settings like being in the interface\n     Example :\n      - model = 'sale.config.settings' or ctx.env['sale.config.settings']\n      - values = {'default_invoice_policy': 'delivery'}\n     Be careful, settings onchange are not triggered with this function.\n    \"\"\"\n    if isinstance(model, basestring):\n        model = ctx.env[model]\n    model.create(values).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gather(*coros_or_futures, limit=0, loop=None, timeout=None,\n           preserve_order=False, return_exceptions=False):\n    \"\"\"\n    Return a future aggregating results from the given coroutine objects\n    with a concurrency execution limit.\n\n    If all the tasks are done successfully, the returned future\u2019s result is\n    the list of results (in the order of the original sequence,\n    not necessarily the order of results arrival).\n\n    If return_exceptions is `True`, exceptions in the tasks are treated the\n    same as successful results, and gathered in the result list; otherwise,\n    the first raised exception will be immediately propagated to the\n    returned future.\n\n    All futures must share the same event loop.\n\n    This functions is mostly compatible with Python standard\n    ``asyncio.gather``, but providing ordered results and concurrency control\n    flow.\n\n    This function is a coroutine.\n\n    Arguments:\n        *coros_or_futures (coroutines|list): an iterable collection yielding\n            coroutines functions or futures.\n        limit (int): max concurrency limit. Use ``0`` for no limit.\n        timeout can be used to control the maximum number\n            of seconds to wait before returning. timeout can be an int or\n            float. If timeout is not specified or None, there is no limit to\n            the wait time.\n        preserve_order (bool): preserves results order.\n        return_exceptions (bool): returns exceptions as valid results.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n\n    Returns:\n        list: coroutines returned results.\n\n    Usage::\n\n        async def sum(x, y):\n            return x + y\n\n        await paco.gather(\n            sum(1, 2),\n            sum(None, 'str'),\n            return_exceptions=True)\n        # => [3, TypeError(\"unsupported operand type(s) for +: 'NoneType' and 'str'\")]  # noqa\n\n    \"\"\"\n    # If no coroutines to schedule, return empty list (as Python stdlib)\n    if len(coros_or_futures) == 0:\n        return []\n\n    # Support iterable as first argument for better interoperability\n    if len(coros_or_futures) == 1 and isiter(coros_or_futures[0]):\n        coros_or_futures = coros_or_futures[0]\n\n    # Pre-initialize results\n    results = [None] * len(coros_or_futures) if preserve_order else []\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop)\n\n    # Iterate and attach coroutine for defer scheduling\n    for index, coro in enumerate(coros_or_futures):\n        # Validate coroutine object\n        if asyncio.iscoroutinefunction(coro):\n            coro = coro()\n        if not asyncio.iscoroutine(coro):\n            raise TypeError(\n                'paco: only coroutines or coroutine functions allowed')\n\n        # Add coroutine to the executor pool\n        pool.add(collect(coro, index, results,\n                         preserve_order=preserve_order,\n                         return_exceptions=return_exceptions))\n\n    # Wait until all the tasks finishes\n    yield from pool.run(timeout=timeout, return_exceptions=return_exceptions)\n\n    # Returns aggregated results\n    return results", "response": "Returns a list of objects from a coroutine or list of coroutine objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a given coroutine function, that when executed, if it takes more than the given timeout in seconds to execute, it will be canceled and raise an `asyncio.TimeoutError`. This function is equivalent to Python standard `asyncio.wait_for()` function. This function can be used as decorator. Arguments: coro (coroutinefunction|coroutine): coroutine to wrap. timeout (int|float): max wait timeout in seconds. loop (asyncio.BaseEventLoop): optional event loop to use. Raises: TypeError: if coro argument is not a coroutine function. Returns: coroutinefunction: wrapper coroutine function. Usage:: await paco.timeout(coro, timeout=10)", "response": "def timeout(coro, timeout=None, loop=None):\n    \"\"\"\n    Wraps a given coroutine function, that when executed, if it takes more\n    than the given timeout in seconds to execute, it will be canceled and\n    raise an `asyncio.TimeoutError`.\n\n    This function is equivalent to Python standard\n    `asyncio.wait_for()` function.\n\n    This function can be used as decorator.\n\n    Arguments:\n        coro (coroutinefunction|coroutine): coroutine to wrap.\n        timeout (int|float): max wait timeout in seconds.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n\n    Raises:\n        TypeError: if coro argument is not a coroutine function.\n\n    Returns:\n        coroutinefunction: wrapper coroutine function.\n\n    Usage::\n\n        await paco.timeout(coro, timeout=10)\n\n    \"\"\"\n    @asyncio.coroutine\n    def _timeout(coro):\n        return (yield from asyncio.wait_for(coro, timeout, loop=loop))\n\n    @asyncio.coroutine\n    def wrapper(*args, **kw):\n        return (yield from _timeout(coro(*args, **kw)))\n\n    return _timeout(coro) if asyncio.iscoroutine(coro) else wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef race(iterable, loop=None, timeout=None, *args, **kw):\n    assert_iter(iterable=iterable)\n\n    # Store coros and internal state\n    coros = []\n    resolved = False\n    result = None\n\n    # Resolve first yielded data from coroutine and stop pending ones\n    @asyncio.coroutine\n    def resolver(index, coro):\n        nonlocal result\n        nonlocal resolved\n\n        value = yield from coro\n        if not resolved:\n            resolved = True\n\n            # Flag as not test passed\n            result = value\n\n            # Force canceling pending coroutines\n            for _index, future in enumerate(coros):\n                if _index != index:\n                    future.cancel()\n\n    # Iterate and attach coroutine for defer scheduling\n    for index, coro in enumerate(iterable):\n        # Validate yielded object\n        isfunction = asyncio.iscoroutinefunction(coro)\n        if not isfunction and not asyncio.iscoroutine(coro):\n            raise TypeError(\n                'paco: coro must be a coroutine or coroutine function')\n\n        # Init coroutine function, if required\n        if isfunction:\n            coro = coro(*args, **kw)\n\n        # Store future tasks\n        coros.append(ensure_future(resolver(index, coro)))\n\n    # Run coroutines concurrently\n    yield from asyncio.wait(coros, timeout=timeout, loop=loop)\n\n    return result", "response": "A generator that yields all the coroutines in a given iterable concurrently with waiting until all of them complete."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef overload(fn):\n    if not isfunction(fn):\n        raise TypeError('paco: fn must be a callable object')\n\n    spec = getargspec(fn)\n    args = spec.args\n    if not spec.varargs and (len(args) < 2 or args[1] != 'iterable'):\n        raise ValueError('paco: invalid function signature or arity')\n\n    @functools.wraps(fn)\n    def decorator(*args, **kw):\n        # Check function arity\n        if len(args) < 2:\n            return PipeOverloader(fn, args, kw)\n        # Otherwise, behave like a normal wrapper\n        return fn(*args, **kw)\n\n    return decorator", "response": "Decorator for overloading a given callable object to be used with | operator\n    overloading."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug_reduce(self, rule, tokens, parent, i):\n        prefix = '           '\n        if parent and tokens:\n            p_token = tokens[parent]\n            if hasattr(p_token, 'line'):\n                prefix = 'L.%3d.%03d: ' % (p_token.line, p_token.column)\n                pass\n            pass\n        print(\"%s%s ::= %s\" % (prefix, rule[0], ' '.join(rule[1])))", "response": "Customized format and print for our kind of tokens\n            which gets called in debugging grammar reduce rules\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef consume(generator):  # pragma: no cover\n    # If synchronous generator, just consume and return as list\n    if hasattr(generator, '__next__'):\n        return list(generator)\n\n    if not PY_35:\n        raise RuntimeError(\n            'paco: asynchronous iterator protocol not supported')\n\n    # If asynchronous generator, consume it generator protocol manually\n    buf = []\n    while True:\n        try:\n            buf.append((yield from generator.__anext__()))\n        except StopAsyncIteration:  # noqa\n            break\n\n    return buf", "response": "Returns a list of items from generator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the given value is a function or method object.", "response": "def isfunc(x):\n    \"\"\"\n    Returns `True` if the given value is a function or method object.\n\n    Arguments:\n        x (mixed): value to check.\n\n    Returns:\n        bool\n    \"\"\"\n    return any([\n        inspect.isfunction(x) and not asyncio.iscoroutinefunction(x),\n        inspect.ismethod(x) and not asyncio.iscoroutinefunction(x)\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nasserts that a given values are a coroutine function.", "response": "def assert_corofunction(**kw):\n    \"\"\"\n    Asserts if a given values are a coroutine function.\n\n    Arguments:\n        **kw (mixed): value to check if it is an iterable.\n\n    Raises:\n        TypeError: if assertion fails.\n    \"\"\"\n    for name, value in kw.items():\n        if not asyncio.iscoroutinefunction(value):\n            raise TypeError(\n                'paco: {} must be a coroutine function'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassert that a given values implements a valid iterable interface.", "response": "def assert_iter(**kw):\n    \"\"\"\n    Asserts if a given values implements a valid iterable interface.\n\n    Arguments:\n        **kw (mixed): value to check if it is an iterable.\n\n    Raises:\n        TypeError: if assertion fails.\n    \"\"\"\n    for name, value in kw.items():\n        if not isiter(value):\n            raise TypeError(\n                'paco: {} must be an iterable object'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interval(coro, interval=1, times=None, loop=None):\n    assert_corofunction(coro=coro)\n\n    # Store maximum allowed number of calls\n    times = int(times or 0) or float('inf')\n\n    @asyncio.coroutine\n    def schedule(times, *args, **kw):\n        while times > 0:\n            # Decrement times counter\n            times -= 1\n\n            # Schedule coroutine\n            yield from coro(*args, **kw)\n            yield from asyncio.sleep(interval)\n\n    def wrapper(*args, **kw):\n        return ensure_future(schedule(times, *args, **kw), loop=loop)\n\n    return wrapper", "response": "Decorator for coroutine function execution every x amount of time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a grammar rule to _self. rules_ _self. rule2func_ _self. rule2name_ _self. rule2optional_nt and _self. rule2name_ .", "response": "def addRule(self, doc, func, _preprocess=True):\n        \"\"\"Add a grammar rules to _self.rules_, _self.rule2func_,\n            and _self.rule2name_\n\n        Comments, lines starting with # and blank lines are stripped from\n        doc. We also allow limited form of * and + when there it is of\n        the RHS has a single item, e.g.\n               stmts ::= stmt+\n        \"\"\"\n        fn = func\n\n        # remove blanks lines and comment lines, e.g. lines starting with \"#\"\n        doc = os.linesep.join([s for s in doc.splitlines() if s and not re.match(\"^\\s*#\", s)])\n\n        rules = doc.split()\n\n        index = []\n        for i in range(len(rules)):\n            if rules[i] == '::=':\n                index.append(i-1)\n        index.append(len(rules))\n\n        for i in range(len(index)-1):\n            lhs = rules[index[i]]\n            rhs = rules[index[i]+2:index[i+1]]\n            rule = (lhs, tuple(rhs))\n\n            if _preprocess:\n                rule, fn = self.preprocess(rule, func)\n\n            # Handle a stripped-down form of *, +, and ?:\n            #   allow only one nonterminal on the right-hand side\n            if len(rule[1]) == 1:\n\n                if rule[1][0] == rule[0]:\n                    raise TypeError(\"Complete recursive rule %s\" % rule2str(rule))\n\n                repeat = rule[1][-1][-1]\n                if repeat in ('*', '+', '?'):\n                    nt = rule[1][-1][:-1]\n\n                    if repeat == '?':\n                        new_rule_pair = [rule[0], list((nt,))]\n                        self.optional_nt.add(rule[0])\n                    else:\n                        self.list_like_nt.add(rule[0])\n                        new_rule_pair = [rule[0], [rule[0]] + list((nt,))]\n                    new_rule = rule2str(new_rule_pair)\n                    self.addRule(new_rule, func, _preprocess)\n                    if repeat == '+':\n                        second_rule_pair = (lhs, (nt,))\n                    else:\n                        second_rule_pair = (lhs, tuple())\n                    new_rule = rule2str(second_rule_pair)\n                    self.addRule(new_rule, func, _preprocess)\n                    continue\n\n            if lhs in self.rules:\n                if rule in self.rules[lhs]:\n                    if 'dups' in self.debug and self.debug['dups']:\n                        self.duplicate_rule(rule)\n                    continue\n                self.rules[lhs].append(rule)\n            else:\n                self.rules[lhs] = [ rule ]\n            self.rule2func[rule] = fn\n            self.rule2name[rule] = func.__name__[2:]\n            self.ruleschanged = True\n\n            # Note: In empty rules, i.e. len(rule[1] == 0, we don't\n            # call reductions on explicitly. Instead it is computed\n            # implicitly.\n            if self.profile_info is not None and len(rule[1]) > 0:\n                rule_str = self.reduce_string(rule)\n                if rule_str not in self.profile_info:\n                    self.profile_info[rule_str] = 0\n            pass\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_rules(self, doc):\n        # remove blanks lines and comment lines, e.g. lines starting with \"#\"\n        doc = os.linesep.join([s for s in doc.splitlines() if s and not re.match(\"^\\s*#\", s)])\n\n        rules = doc.split()\n        index = []\n        for i in range(len(rules)):\n            if rules[i] == '::=':\n                index.append(i-1)\n        index.append(len(rules))\n        for i in range(len(index)-1):\n            lhs = rules[index[i]]\n            rhs = rules[index[i]+2:index[i+1]]\n            rule = (lhs, tuple(rhs))\n\n            if lhs not in self.rules:\n                return\n\n            if rule in self.rules[lhs]:\n                self.rules[lhs].remove(rule)\n                del self.rule2func[rule]\n                del self.rule2name[rule]\n                self.ruleschanged = True\n\n                # If we are profiling, remove this rule from that as well\n                if self.profile_info is not None and len(rule[1]) > 0:\n                    rule_str = self.reduce_string(rule)\n                    if rule_str and rule_str in self.profile_info:\n                        del self.profile_info[rule_str]\n                        pass\n                    pass\n                pass\n\n        return", "response": "Remove a grammar rule from _self. rules_ _self. rule2func_ and _self. rule2name_ _self. rules_."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef errorstack(self, tokens, i, full=False):\n        print(\"\\n-- Stacks of completed symbols:\")\n        states = [s for s in self.edges.values() if s]\n        # States now has the set of states we are in\n        state_stack = set()\n        for state in states:\n            # Find rules which can follow, but keep only\n            # the part before the dot\n            for rule, dot in self.states[state].items:\n                lhs, rhs = rule\n                if dot > 0:\n                    if full:\n                        state_stack.add(\n                            \"%s ::= %s . %s\" %\n                            (lhs,\n                            ' '.join(rhs[:dot]),\n                            ' '.join(rhs[dot:])))\n                    else:\n                        state_stack.add(\n                            \"%s ::= %s\" %\n                            (lhs,\n                            ' '.join(rhs[:dot])))\n                    pass\n                pass\n            pass\n        for stack in sorted(state_stack):\n            print(stack)", "response": "Show the error stacks of the completed symbols."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, tokens, debug=None):\n\n        self.tokens = tokens\n\n        if debug:\n            self.debug = debug\n\n        sets = [ [(1, 0), (2, 0)] ]\n        self.links = {}\n\n        if self.ruleschanged:\n            self.computeNull()\n            self.newrules = {}\n            self.new2old = {}\n            self.makeNewRules()\n            self.ruleschanged = False\n            self.edges, self.cores = {}, {}\n            self.states = { 0: self.makeState0() }\n            self.makeState(0, self._BOF)\n\n        for i in range(len(tokens)):\n            sets.append([])\n\n            if sets[i] == []:\n                break\n            self.makeSet(tokens, sets, i)\n        else:\n            sets.append([])\n            self.makeSet(None, sets, len(tokens))\n\n        finalitem = (self.finalState(tokens), 0)\n        if finalitem not in sets[-2]:\n            if len(tokens) > 0:\n                if self.debug.get('errorstack', False):\n                    self.errorstack(tokens, i-1, str(self.debug['errorstack']) == 'full')\n                self.error(tokens, i-1)\n            else:\n                self.error(None, None)\n\n        if self.profile_info is not None:\n            self.dump_profile_info()\n\n        return self.buildTree(self._START, finalitem,\n                    tokens, len(sets)-2)", "response": "This method parses the tokens and returns a tree of the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_grammar(self, ok_start_symbols = set(),\n                      out=sys.stderr):\n        '''\n        Check grammar for:\n        -  unused left-hand side nonterminals that are neither start symbols\n           or listed in ok_start_symbols\n        -  unused right-hand side nonterminals, i.e. not tokens\n        -  right-recursive rules. These can slow down parsing.\n        '''\n        warnings = 0\n        (lhs, rhs, tokens, right_recursive,\n         dup_rhs) = self.check_sets()\n        if lhs - ok_start_symbols:\n            warnings += 1\n            out.write(\"LHS symbols not used on the RHS:\\n\")\n            out.write(\"  \" + (', '.join(sorted(lhs)) + \"\\n\"))\n        if rhs:\n            warnings += 1\n            out.write(\"RHS symbols not used on the LHS:\\n\")\n            out.write((', '.join(sorted(rhs))) + \"\\n\" )\n        if right_recursive:\n            warnings += 1\n            out.write(\"Right recursive rules:\\n\")\n            for rule in sorted(right_recursive):\n                out.write(\"  %s ::= %s\\n\" % (rule[0], ' '.join(rule[1])))\n                pass\n            pass\n        if dup_rhs:\n            warnings += 1\n            out.write(\"Nonterminals with the same RHS\\n\")\n            for rhs in sorted(dup_rhs.keys()):\n                out.write(\"  RHS: %s\\n\" % ' '.join(rhs))\n                out.write(\"  LHS: %s\\n\" % ', '.join(dup_rhs[rhs]))\n                out.write(\"  ---\\n\")\n                pass\n            pass\n        return warnings", "response": "Check the grammar for the current set of nonterminals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck grammar for sets of all possible set of keys.", "response": "def check_sets(self):\n        '''\n        Check grammar\n        '''\n        lhs_set         = set()\n        rhs_set         = set()\n        rhs_rules_set   = {}\n        token_set       = set()\n        right_recursive = set()\n        dup_rhs = {}\n        for lhs in self.rules:\n            rules_for_lhs = self.rules[lhs]\n            lhs_set.add(lhs)\n            for rule in rules_for_lhs:\n                rhs = rule[1]\n                if len(rhs) > 0 and rhs in rhs_rules_set:\n                    li = dup_rhs.get(rhs, [])\n                    li.append(lhs)\n                    dup_rhs[rhs] = li\n                else:\n                    rhs_rules_set[rhs] = lhs\n                for sym in rhs:\n                    # We assume any symbol starting with an uppercase letter is\n                    # terminal, and anything else is a nonterminal\n                    if re.match(\"^[A-Z]\", sym):\n                        token_set.add(sym)\n                    else:\n                        rhs_set.add(sym)\n                if len(rhs) > 0 and lhs == rhs[-1]:\n                    right_recursive.add((lhs, rhs))\n                pass\n            pass\n\n        lhs_set.remove(self._START)\n        rhs_set.remove(self._BOF)\n        missing_lhs = lhs_set - rhs_set\n        missing_rhs = rhs_set - lhs_set\n\n        # dup_rhs is missing first entry found, so add that\n        for rhs in dup_rhs:\n            dup_rhs[rhs].append(rhs_rules_set[rhs])\n            pass\n        return (missing_lhs, missing_rhs, token_set, right_recursive, dup_rhs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef profile_rule(self, rule):\n        rule_str = self.reduce_string(rule)\n        if rule_str not in self.profile_info:\n            self.profile_info[rule_str] = 1\n        else:\n            self.profile_info[rule_str] += 1", "response": "Increment the count of the number of times _rule_ was used"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows the accumulated results of how many times each rule was used", "response": "def get_profile_info(self):\n        \"\"\"Show the accumulated results of how many times each rule was used\"\"\"\n        return sorted(self.profile_info.items(),\n                      key=lambda kv: kv[1],\n                      reverse=False)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef partial(coro, *args, **kw):\n    assert_corofunction(coro=coro)\n\n    @asyncio.coroutine\n    def wrapper(*_args, **_kw):\n        call_args = args + _args\n        kw.update(_kw)\n        return (yield from coro(*call_args, **kw))\n\n    return wrapper", "response": "Decorator for coroutine functions that returns a new object that is used as a partial application."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates an expression string and return a source string.", "response": "def eval_expr(expr_str, show_tokens=False, showast=False,\n              showgrammar=False, compile_mode='exec'):\n    \"\"\"\n    evaluate simple expression\n    \"\"\"\n\n    parser_debug = {'rules': False, 'transition': False,\n                    'reduce': showgrammar,\n                    'errorstack': True, 'context': True }\n    parsed = parse_expr(expr_str, show_tokens=show_tokens,\n                        parser_debug=parser_debug)\n    if showast:\n        print(parsed)\n\n    assert parsed == 'expr', 'Should have parsed grammar start'\n\n    evaluator = ExprEvaluator()\n\n    # What we've been waiting for: Generate source from AST!\n    return evaluator.traverse(parsed)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef n_term(self, node):\n        if len(node) == 1:\n            self.preorder(node[0])\n            node.value = node[0].value\n            self.prune()\n        else:\n            self.preorder(node[0])\n            self.preorder(node[2])\n            if node[1].attr == '*':\n                node.value = node[0].value * node[2].value\n            elif node[1].attr == '/':\n                node.value = node[0].value / node[2].value\n            else:\n                assert False, \"Expecting operator to be '*' or '/'\"\n            self.prune()\n        assert False, \"Expecting atom to have length 1 or 3\"", "response": "n_term - Updates the node value according to the node s MULT_OP atom | atom"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up the global variables for the base class of the base class", "response": "def setup():\n    \"\"\"\n    Gathers all configs\n    \"\"\"\n\n    global CONFIG, BING_KEY, GENIUS_KEY, config_path, LOG_FILENAME, LOG_LINE_SEPERATOR \n\n    LOG_FILENAME = 'musicrepair_log.txt'\n    LOG_LINE_SEPERATOR = '........................\\n'\n\n    CONFIG = configparser.ConfigParser()\n    config_path = realpath(__file__).replace(basename(__file__),'')\n    config_path = config_path + 'config.ini'\n    CONFIG.read(config_path)\n\n    GENIUS_KEY = CONFIG['keys']['genius_key']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprovides a score out of 10 that determines the relevance of the search result", "response": "def matching_details(song_name, song_title, artist):\n    '''\n    Provides a score out of 10 that determines the\n    relevance of the search result\n    '''\n\n    match_name = difflib.SequenceMatcher(None, song_name, song_title).ratio()\n    match_title = difflib.SequenceMatcher(None, song_name, artist + song_title).ratio()\n\n    if max(match_name,match_title) >= 0.55:\n        return True, max(match_name,match_title)\n\n    else:\n        return False, (match_name + match_title) / 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_lyrics_letssingit(song_name):\n    '''\n    Scrapes the lyrics of a song since spotify does not provide lyrics\n    takes song title as arguement\n    '''\n\n    lyrics = \"\"\n    url = \"http://search.letssingit.com/cgi-exe/am.cgi?a=search&artist_id=&l=archive&s=\" + \\\n        quote(song_name.encode('utf-8'))\n    html = urlopen(url).read()\n    soup = BeautifulSoup(html, \"html.parser\")\n    link = soup.find('a', {'class': 'high_profile'})\n\n    try:\n        link = link.get('href')\n        link = urlopen(link).read()\n        soup = BeautifulSoup(link, \"html.parser\")\n\n        try:\n            lyrics = soup.find('div', {'id': 'lyrics'}).text\n            lyrics = lyrics[3:]\n\n        except AttributeError:\n            lyrics = \"\"\n\n    except:\n        lyrics = \"\"\n\n    return lyrics", "response": "Scrapes the lyrics of a song since spotify does not provide lyrics\n    takes song title as arguement\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_lyrics_genius(song_title):\n    '''\n    Scrapes the lyrics from Genius.com\n    '''\n    base_url = \"http://api.genius.com\"\n    headers = {'Authorization': 'Bearer %s' %(GENIUS_KEY)}\n    search_url = base_url + \"/search\"\n    data = {'q': song_title}\n\n    response = requests.get(search_url, data=data, headers=headers)\n    json = response.json()\n    song_api_path = json[\"response\"][\"hits\"][0][\"result\"][\"api_path\"]\n\n    song_url = base_url + song_api_path\n    response = requests.get(song_url, headers=headers)\n    json = response.json()\n    path = json[\"response\"][\"song\"][\"path\"]\n    page_url = \"http://genius.com\" + path\n\n    page = requests.get(page_url)\n    soup = BeautifulSoup(page.text, \"html.parser\")\n    div = soup.find('div',{'class': 'song_body-lyrics'})\n    lyrics = div.find('p').getText()\n  \n    return lyrics", "response": "Scrapes the lyrics from Genius. com\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_details_spotify(song_name):\n    '''\n    Tries finding metadata through Spotify\n    '''\n\n    song_name = improvename.songname(song_name)\n\n    spotify = spotipy.Spotify()\n    results = spotify.search(song_name, limit=1)  # Find top result\n\n    log.log_indented('* Finding metadata from Spotify.')\n\n    try:\n        album = (results['tracks']['items'][0]['album']\n                 ['name'])  # Parse json dictionary\n        artist = (results['tracks']['items'][0]['album']['artists'][0]['name'])\n        song_title = (results['tracks']['items'][0]['name'])\n        try:\n            log_indented(\"* Finding lyrics from Genius.com\")\n            lyrics = get_lyrics_genius(song_title)\n        except:\n            log_error(\"* Could not find lyrics from Genius.com, trying something else\")\n            lyrics = get_lyrics_letssingit(song_title)\n\n        match_bool, score = matching_details(song_name, song_title, artist)\n        if match_bool:\n            return artist, album, song_title, lyrics, match_bool, score\n        else:\n            return None\n\n    except IndexError:\n        log.log_error(\n            '* Could not find metadata from spotify, trying something else.', indented=True)\n        return None", "response": "Tries to find metadata through Spotify and returns it as a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_details_letssingit(song_name):\n    '''\n    Gets the song details if song details not found through spotify\n    '''\n\n    song_name = improvename.songname(song_name)\n\n    url = \"http://search.letssingit.com/cgi-exe/am.cgi?a=search&artist_id=&l=archive&s=\" + \\\n        quote(song_name.encode('utf-8'))\n    html = urlopen(url).read()\n    soup = BeautifulSoup(html, \"html.parser\")\n    link = soup.find('a', {'class': 'high_profile'})\n    try:\n        link = link.get('href')\n        link = urlopen(link).read()\n\n        soup = BeautifulSoup(link, \"html.parser\")\n\n        album_div = soup.find('div', {'id': 'albums'})\n        title_div = soup.find('div', {'id': 'content_artist'}).find('h1')\n\n        try:\n            lyrics = soup.find('div', {'id': 'lyrics'}).text\n            lyrics = lyrics[3:]\n        except AttributeError:\n            lyrics = \"\"\n            log.log_error(\"* Couldn't find lyrics\", indented=True)\n\n        try:\n            song_title = title_div.contents[0]\n            song_title = song_title[1:-8]\n        except AttributeError:\n            log.log_error(\"* Couldn't reset song title\", indented=True)\n            song_title = song_name\n\n        try:\n            artist = title_div.contents[1].getText()\n        except AttributeError:\n            log.log_error(\"* Couldn't find artist name\", indented=True)\n            artist = \"Unknown\"\n\n        try:\n            album = album_div.find('a').contents[0]\n            album = album[:-7]\n        except AttributeError:\n            log.log_error(\"* Couldn't find the album name\", indented=True)\n            album = artist\n\n    except AttributeError:\n        log.log_error(\"* Couldn't find song details\", indented=True)\n\n        album = song_name\n        song_title = song_name\n        artist = \"Unknown\"\n        lyrics = \"\"\n\n    match_bool, score = matching_details(song_name, song_title, artist)\n\n    return artist, album, song_title, lyrics, match_bool, score", "response": "Gets the song details if not found through spotify\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_albumart(albumart, song_title):\n    '''\n    Adds the album art to the song\n    '''\n\n    try:\n        img = urlopen(albumart)  # Gets album art from url\n\n    except Exception:\n        log.log_error(\"* Could not add album art\", indented=True)\n        return None\n\n    audio = EasyMP3(song_title, ID3=ID3)\n    try:\n        audio.add_tags()\n    except _util.error:\n        pass\n\n    audio.tags.add(\n        APIC(\n            encoding=3,  # UTF-8\n            mime='image/png',\n            type=3,  # 3 is for album art\n            desc='Cover',\n            data=img.read()  # Reads and adds album art\n        )\n    )\n    audio.save()\n    log.log(\"> Added album art\")", "response": "Adds the album art to the song\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the details to song", "response": "def add_details(file_name, title, artist, album, lyrics=\"\"):\n    '''\n    Adds the details to song\n    '''\n\n    tags = EasyMP3(file_name)\n    tags[\"title\"] = title\n    tags[\"artist\"] = artist\n    tags[\"album\"] = album\n    tags.save()\n\n    tags = ID3(file_name)\n    uslt_output = USLT(encoding=3, lang=u'eng', desc=u'desc', text=lyrics)\n    tags[\"USLT::'eng'\"] = uslt_output\n\n    tags.save(file_name)\n\n    log.log(\"> Adding properties\")\n    log.log_indented(\"[*] Title: %s\" % title)\n    log.log_indented(\"[*] Artist: %s\" % artist)\n    log.log_indented(\"[*] Album: %s \" % album)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfixing the music file name.", "response": "def fix_music(file_name):\n    '''\n    Searches for '.mp3' files in directory (optionally recursive)\n    and checks whether they already contain album art and album name tags or not.\n    '''\n\n    setup()\n\n    if not Py3:\n        file_name = file_name.encode('utf-8')\n\n    tags = File(file_name)\n\n    log.log(file_name)\n    log.log('> Adding metadata')\n\n    try:\n        artist, album, song_name, lyrics, match_bool, score = get_details_spotify(\n            file_name)  # Try finding details through spotify\n\n    except Exception:\n        artist, album, song_name, lyrics, match_bool, score = get_details_letssingit(\n            file_name)  # Use bad scraping method as last resort\n\n    try:\n        log.log_indented('* Trying to extract album art from Google.com')\n        albumart = albumsearch.img_search_google(artist+' '+album)\n    except Exception:\n        log.log_indented('* Trying to extract album art from Bing.com')\n        albumart = albumsearch.img_search_bing(artist+' '+album)\n\n    if match_bool:\n        add_albumart(albumart, file_name)\n        add_details(file_name, song_name, artist, album, lyrics)\n\n        try:\n            rename(file_name, artist+' - '+song_name+'.mp3')\n        except Exception:\n            log.log_error(\"Couldn't rename file\")\n            pass\n    else:\n        log.log_error(\n            \"* Couldn't find appropriate details of your song\", indented=True)\n\n    log.log(\"Match score: %s/10.0\" % round(score * 10, 1))\n    log.log(LOG_LINE_SEPERATOR)\n    log.log_success()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filterfalse(coro, iterable, limit=0, loop=None):\n    return (yield from filter(coro, iterable,\n                              assert_fn=assert_false,\n                              limit=limit, loop=loop))", "response": "A non - asynchronous version of filter that returns a list of all the values in coll which pass an asynchronous truth\n            test coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(coro, iterable, limit=0, loop=None, timeout=None,\n        return_exceptions=False, *args, **kw):\n    \"\"\"\n    Concurrently maps values yielded from an iterable, passing then\n    into an asynchronous coroutine function.\n\n    Mapped values will be returned as list.\n    Items order will be preserved based on origin iterable order.\n\n    Concurrency level can be configurable via ``limit`` param.\n\n    This function is the asynchronous equivalent port Python built-in\n    `map()` function.\n\n    This function is a coroutine.\n\n    This function can be composed in a pipeline chain with ``|`` operator.\n\n    Arguments:\n        coro (coroutinefunction): map coroutine function to use.\n        iterable (iterable|asynchronousiterable): an iterable collection\n            yielding coroutines functions.\n        limit (int): max concurrency limit. Use ``0`` for no limit.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n        timeout (int|float): timeout can be used to control the maximum number\n            of seconds to wait before returning. timeout can be an int or\n            float. If timeout is not specified or None, there is no limit to\n            the wait time.\n        return_exceptions (bool): returns exceptions as valid results.\n        *args (mixed): optional variadic arguments to be passed to the\n            coroutine map function.\n\n    Returns:\n        list: ordered list of values yielded by coroutines\n\n    Usage::\n\n        async def mul_2(num):\n            return num * 2\n\n        await paco.map(mul_2, [1, 2, 3, 4, 5])\n        # => [2, 4, 6, 8, 10]\n\n    \"\"\"\n    # Call each iterable but collecting yielded values\n    return (yield from each(coro, iterable,\n                            limit=limit, loop=loop,\n                            timeout=timeout, collect=True,\n                            return_exceptions=return_exceptions))", "response": "A generator function that maps values yielded from an iterable passing then\n            into an asynchronous coroutine function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uninstall(ctx, module_list):\n    if not module_list:\n        raise AnthemError(u\"You have to provide a list of \"\n                          \"module's name to uninstall\")\n\n    mods = ctx.env['ir.module.module'].search([('name', 'in', module_list)])\n    try:\n        mods.button_immediate_uninstall()\n    except Exception:\n        raise AnthemError(u'Cannot uninstall modules. See the logs')", "response": "Uninstalls modules from the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_translations(ctx, module_list):\n    if not module_list:\n        raise AnthemError(u\"You have to provide a list of \"\n                          \"module's name to update the translations\")\n\n    for module in module_list:\n        ctx.env['ir.module.module'].with_context(overwrite=True).search(\n            [('name', '=', module)]).update_translations()", "response": "Update translations from a list of modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flat_map(coro, iterable, limit=0, loop=None, timeout=None,\n             return_exceptions=False, initializer=None, *args, **kw):\n    \"\"\"\n    Concurrently iterates values yielded from an iterable, passing them to\n    an asynchronous coroutine.\n\n    This function is the flatten version of to ``paco.map()``.\n\n    Mapped values will be returned as an ordered list.\n    Items order is preserved based on origin iterable order.\n\n    Concurrency level can be configurable via ``limit`` param.\n\n    All coroutines will be executed in the same loop.\n\n    This function is a coroutine.\n\n    This function can be composed in a pipeline chain with ``|`` operator.\n\n    Arguments:\n        coro (coroutinefunction): coroutine iterator function that accepts\n            iterable values.\n        iterable (iterable|asynchronousiterable): an iterable collection\n            yielding coroutines functions.\n        limit (int): max iteration concurrency limit. Use ``0`` for no limit.\n        collect (bool): return yielded values from coroutines. Default False.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n        return_exceptions (bool): enable/disable returning exceptions in case\n            of error. `collect` param must be True.\n        timeout (int|float): timeout can be used to control the maximum number\n            of seconds to wait before returning. timeout can be an int or\n            float. If timeout is not specified or None, there is no limit to\n            the wait time.\n        *args (mixed): optional variadic arguments to pass to the\n            coroutine iterable function.\n\n    Returns:\n        results (list): ordered list of values yielded by coroutines\n\n    Raises:\n        TypeError: in case of invalid input arguments.\n\n    Usage::\n\n        async def mul_2(num):\n            return num * 2\n\n        await paco.flat_map(mul_2, [1, [2], [3, [4]], [(5,)]])\n        # => [2, 4, 6, 8, 10]\n\n        # Pipeline style\n        await [1, [2], [3, [4]], [(5,)]] | paco.flat_map(mul_2)\n        # => [2, 4, 6, 8, 10]\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n    assert_iter(iterable=iterable)\n\n    # By default do not collect yielded values from coroutines\n    results = initializer if isiter(initializer) else []\n\n    @asyncio.coroutine\n    def reducer(buf, value):\n        if isiter(value):\n            yield from _reduce(value)\n        else:\n            buf.append((yield from coro(value)))\n        return buf\n\n    def _reduce(iterable):\n        return reduce(reducer, iterable,\n                      initializer=results, limit=limit, loop=loop)\n\n    # Returns list of mapped reduced results\n    return (yield from _reduce(iterable))", "response": "A function that returns a list of all the items yielded from an iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generator_consumer(coro):  # pragma: no cover\n    if not asyncio.iscoroutinefunction(coro):\n        raise TypeError('paco: coro must be a coroutine function')\n\n    @functools.wraps(coro)\n    @asyncio.coroutine\n    def wrapper(*args, **kw):\n        if len(args) > 1 and isgenerator(args[1]):\n            args = list(args)\n            args[1] = (yield from consume(args[1])\n                       if hasattr(args[1], '__anext__')\n                       else list(args[1]))\n            args = tuple(args)\n        return (yield from coro(*args, **kw))\n    return wrapper", "response": "Decorator that returns a generator function that consumes sync or async generators provided as\n    interable input argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the role value from the matchedVars.", "response": "def _getRole(self, matchedVars):\n        \"\"\"\n        :param matchedVars:\n        :return: NULL or the role's integer value\n        \"\"\"\n        role = matchedVars.get(ROLE)\n        if role is not None and role.strip() == '':\n            role = NULL\n        else:\n            valid = Authoriser.isValidRoleName(role)\n            if valid:\n                role = Authoriser.getRoleFromName(role)\n            else:\n                self.print(\"Invalid role. Valid roles are: {}\".\n                           format(\", \".join(map(lambda r: r.name, Roles))),\n                           Token.Error)\n                return False\n        return role"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef throttle(coro, limit=1, timeframe=1,\n             return_value=None, raise_exception=False):\n    \"\"\"\n    Creates a throttled coroutine function that only invokes\n    ``coro`` at most once per every time frame of seconds or milliseconds.\n\n    Provide options to indicate whether func should be invoked on the\n    leading and/or trailing edge of the wait timeout.\n\n    Subsequent calls to the throttled coroutine\n    return the result of the last coroutine invocation.\n\n    This function can be used as decorator.\n\n    Arguments:\n        coro (coroutinefunction):\n            coroutine function to wrap with throttle strategy.\n        limit (int):\n            number of coroutine allowed execution in the given time frame.\n        timeframe (int|float):\n            throttle limit time frame in seconds.\n        return_value (mixed):\n            optional return if the throttle limit is reached.\n            Returns the latest returned value by default.\n        raise_exception (bool):\n            raise exception if throttle limit is reached.\n\n    Raises:\n        RuntimeError: if cannot throttle limit reached (optional).\n\n    Returns:\n        coroutinefunction\n\n    Usage::\n\n        async def mul_2(num):\n            return num * 2\n\n        # Use as simple wrapper\n        throttled = paco.throttle(mul_2, limit=1, timeframe=2)\n        await throttled(2)\n        # => 4\n        await throttled(3)  # ignored!\n        # => 4\n        await asyncio.sleep(2)\n        await throttled(3)  # executed!\n        # => 6\n\n        # Use as decorator\n        @paco.throttle(limit=1, timeframe=2)\n        async def mul_2(num):\n            return num * 2\n\n        await mul_2(2)\n        # => 4\n        await mul_2(3)  # ignored!\n        # => 4\n        await asyncio.sleep(2)\n        await mul_2(3)  # executed!\n        # => 6\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n\n    # Store execution limits\n    limit = max(int(limit), 1)\n    remaning = limit\n\n    # Turn seconds in milliseconds\n    timeframe = timeframe * 1000\n\n    # Keep call state\n    last_call = now()\n    # Cache latest retuned result\n    result = None\n\n    def stop():\n        if raise_exception:\n            raise RuntimeError('paco: coroutine throttle limit exceeded')\n        if return_value:\n            return return_value\n        return result\n\n    def elapsed():\n        return now() - last_call\n\n    @asyncio.coroutine\n    def wrapper(*args, **kw):\n        nonlocal result\n        nonlocal remaning\n        nonlocal last_call\n\n        if elapsed() > timeframe:\n            # Reset reamining calls counter\n            remaning = limit\n            # Update last call time\n            last_call = now()\n        elif elapsed() < timeframe and remaning <= 0:\n            return stop()\n\n        # Decrease remaining limit\n        remaning -= 1\n\n        # Schedule coroutine passing arguments and cache result\n        result = yield from coro(*args, **kw)\n        return result\n\n    return wrapper", "response": "Decorator that wraps a coroutine function with a throttled function that only invokes the given coroutine at most once per every time frame of seconds or milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a CSV file from a file path.", "response": "def load_csv(ctx, model, path, header=None, header_exclude=None, **fmtparams):\n    \"\"\"Load a CSV from a file path.\n\n    :param ctx: Anthem context\n    :param model: Odoo model name or model klass from env\n    :param path: absolute or relative path to CSV file.\n        If a relative path is given you must provide a value for\n        `ODOO_DATA_PATH` in your environment\n        or set `--odoo-data-path` option.\n    :param header: whitelist of CSV columns to load\n    :param header_exclude: blacklist of CSV columns to not load\n    :param fmtparams: keyword params for `csv_unireader`\n\n    Usage example::\n\n      from pkg_resources import Requirement, resource_string\n\n      req = Requirement.parse('my-project')\n      load_csv(ctx, ctx.env['res.users'],\n               resource_string(req, 'data/users.csv'),\n               delimiter=',')\n\n    \"\"\"\n    if not os.path.isabs(path):\n        if ctx.options.odoo_data_path:\n            path = os.path.join(ctx.options.odoo_data_path, path)\n        else:\n            raise AnthemError(\n                'Got a relative path. '\n                'Please, provide a value for `ODOO_DATA_PATH` '\n                'in your environment or set `--odoo-data-path` option.'\n            )\n\n    with open(path, 'rb') as data:\n        load_csv_stream(ctx, model, data,\n                        header=header, header_exclude=header_exclude,\n                        **fmtparams)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_csv_stream(ctx, model, data,\n                    header=None, header_exclude=None, **fmtparams):\n    \"\"\"Load a CSV from a stream.\n\n    :param ctx: current anthem context\n    :param model: model name as string or model klass\n    :param data: csv data to load\n    :param header: csv fieldnames whitelist\n    :param header_exclude: csv fieldnames blacklist\n\n    Usage example::\n\n      from pkg_resources import Requirement, resource_stream\n\n      req = Requirement.parse('my-project')\n      load_csv_stream(ctx, ctx.env['res.users'],\n                      resource_stream(req, 'data/users.csv'),\n                      delimiter=',')\n    \"\"\"\n    _header, _rows = read_csv(data, **fmtparams)\n    header = header if header else _header\n    if _rows:\n        # check if passed header contains all the fields\n        if header != _header and not header_exclude:\n            # if not, we exclude the rest of the fields\n            header_exclude = [x for x in _header if x not in header]\n        if header_exclude:\n            # exclude fields from header as well as respective values\n            header = [x for x in header if x not in header_exclude]\n            # we must loop trough all the rows too to pop values\n            # since odoo import works only w/ reader and not w/ dictreader\n            pop_idxs = [_header.index(x) for x in header_exclude]\n            rows = []\n            for i, row in enumerate(_rows):\n                rows.append(\n                    [x for j, x in enumerate(row) if j not in pop_idxs]\n                )\n        else:\n            rows = list(_rows)\n        if rows:\n            load_rows(ctx, model, header, rows)", "response": "Load a CSV file into a new anthem context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_translations(ctx, module_list):\n    modules.update_translations(ctx, module_list)\n    ctx.log_line(u'Deprecated: use anthem.lyrics.modules.update_translations'\n                 'instead of anthem.lyrics.loaders.update_translations')", "response": "Update translations from module list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef n_atom(self, node):\n        length = len(node)\n        if length == 1:\n            self.preorder(node[0])\n        elif length == 3:\n            self.preorder(node[0])\n            self.preorder(node[1])\n            self.preorder(node[2])\n        else:\n            assert False, \"Expecting atom to have length 1 or 3\"\n        self.prune()", "response": "n_atom - Anatomize the node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef n_exec_stmt(self, node):\n        self.write(self.indent, 'exec ')\n        self.preorder(node[1])\n\n        if len(node) > 2:\n            self.write(self.indent, ' in ')\n            self.preorder(node[3])\n            if len(node) > 5:\n                self.write(self.indent, ', ')\n                self.preorder(node[5])\n        self.println()\n        self.prune()", "response": "n_exec_stmt is the entry point for the EXEC expr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef songname(song_name):\n    '''\n    Improves file name by removing crap words\n    '''\n    try:\n        song_name = splitext(song_name)[0]\n    except IndexError:\n        pass\n\n    # Words to omit from song title for better results through spotify's API\n    chars_filter = \"()[]{}-:_/=+\\\"\\'\"\n    words_filter = ('official', 'lyrics', 'audio', 'remixed', 'remix', 'video',\n                    'full', 'version', 'music', 'mp3', 'hd', 'hq', 'uploaded')\n\n    # Replace characters to filter with spaces\n    song_name = ''.join(map(lambda c: \" \" if c in chars_filter else c, song_name))\n\n    # Remove crap words\n    song_name = re.sub('|'.join(re.escape(key) for key in words_filter),\n                       \"\", song_name, flags=re.IGNORECASE)\n\n    # Remove duplicate spaces\n    song_name = re.sub(' +', ' ', song_name)\n    return song_name.strip()", "response": "Returns a song name from a file name by removing crap words\n    Removes duplicate spaces\n    Removes duplicate characters with spaces with song name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating the swagger definition file for the given app", "response": "def swaggerify(app, parse_module_info=True, **kwargs):\n    \"\"\"\n    Parse information from given app, it's path handlers and callee module\n    itself to populate swagger definition file\n\n    Any part of swagger definition can be overriden with kwargs\n    \"\"\"\n\n    async def swagger_handler(request):\n        \"\"\"\n        Request handler to serve swagger json config for given app\n        \"\"\"\n        return web.json_response(app[\"_swagger_config\"])\n\n    # Populate configs for paths from app routes\n    paths_config = defaultdict(dict)\n    for r in app.router.routes():\n        # Title and description are retrieved from docstring of a handler\n        docstrings = parse_docstring(r.handler.__doc__ or \"\")\n        if len(docstrings[\"title\"]) > 120:\n            # TODO: logging\n            docstrings[\"title\"] = docstrings[\"title\"][:120]\n\n        # Some information is just hardcoded as we have strict requirements\n        # to input and output format of ws endpoints\n        desc = {\n            \"summary\": docstrings[\"title\"],\n            \"description\": docstrings[\"description\"],\n            \"consumes\": [\"application/json\"],\n            \"produces\": [\"application/json\"],\n            \"responses\": {\n                \"400\": {\"description\": \"Request is malformed or invalid\"},\n                \"500\": {\"description\": \"Response malformed or invalid\"},\n                \"405\": {\"description\": \"Method is not allowed\"},\n            }\n        }\n\n        if hasattr(r.handler, \"_request_schema\"):\n            desc[\"parameters\"] = [{\n                \"in\": \"body\",\n                \"name\": \"body\",\n                \"required\": True,\n                \"schema\": r.handler._request_schema\n            }]\n        else:\n            desc[\"parameters\"] = [{\n                \"in\": \"body\",\n                \"name\": \"body\"\n            }]\n\n        if hasattr(r.handler, \"_swg_input\"):\n            desc[\"parameters\"][0].update(r.handler._swg_input)\n\n        if hasattr(r.handler, \"_response_schema\"):\n            desc[\"responses\"][\"200\"] = {\n                # TODO: think how to override\n                \"description\": \"Results of the request\",\n                \"schema\": r.handler._response_schema\n            }\n        else:\n            desc[\"responses\"][\"200\"] = {\n                \"description\": \"Results of the request\"\n            }\n\n        if hasattr(r.handler, \"_swg_output\"):\n            desc[\"responses\"][\"200\"].update(r.handler._swg_output)\n\n        if hasattr(r.handler, \"_swg_info\"):\n            desc.update(r.handler._swg_info)\n\n        paths_config[r._resource._path][r.method.lower()] = desc\n\n    # Populate complete config\n\n    # Get optional information that overrides info part of the config\n    swagger_config_info = kwargs.pop(\"info\", {})\n    # Some sane defaults\n    swagger_config = {\n        \"swagger\": \"2.0\",\n        \"schemes\": [\"http\"],\n        \"paths\": paths_config\n    }\n\n    # That can be overriden with kwargs\n    swagger_config.update(kwargs)\n\n    # Some sane defaults for info part of the config\n    swagger_config_info_default = {\n        \"version\": \"1.0.0\",\n    }\n\n    # Which we now can try to extend using introspection, docstrings and\n    # global variables of the main module of the app\n    if parse_module_info:\n        try:\n            caller = inspect.currentframe().f_back\n            mod = sys.modules[caller.f_globals['__name__']]\n\n            # Try to pull information from docstrings\n            swagger_config_info_default.update(parse_docstring(mod.__doc__))\n\n            # As well as global variables\n            if hasattr(mod, \"__version__\"):\n                swagger_config_info_default[\"version\"] = mod.__version__\n\n            if hasattr(mod, \"__license__\"):\n                swagger_config_info_default[\"license\"] = {\n                    \"name\": mod.__license__\n                    # TODO: add urls for well-known licenses\n                }\n\n            # And map it to swagger definitions\n            contact_info = {}\n            if hasattr(mod, \"__author__\"):\n                contact_info[\"name\"] = mod.__author__\n            if hasattr(mod, \"__maintainer__\"):\n                contact_info[\"name\"] = mod.__maintainer__\n            if hasattr(mod, \"__email__\"):\n                contact_info[\"email\"] = mod.__email__\n\n            if contact_info:\n                swagger_config_info_default[\"contact\"] = contact_info\n        except:\n            # TODO: logging\n            pass\n\n    swagger_config[\"info\"] = swagger_config_info_default\n\n    # override it with data from kwargs[\"info\"]\n    swagger_config[\"info\"].update(swagger_config_info)\n\n    app[\"_swagger_config\"] = swagger_config\n\n    app.router.add_route(\"GET\", \"/swagger.json\", swagger_handler)\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef document(info=None, input=None, output=None):\n    def wrapper(func):\n        if info is not None:\n            setattr(func, \"_swg_info\", info)\n        if input is not None:\n            setattr(func, \"_swg_input\", input)\n        if output is not None:\n            setattr(func, \"_swg_output\", output)\n        return func\n\n    return wrapper", "response": "Decorator to add extra information about request handler and its params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef t_whitespace_or_comment(self, s):\n        r'([ \\t]*[#].*[^\\x04][\\n]?)|([ \\t]+)'\n        if '#' in s:\n            # We have a comment\n            matches = re.match('(\\s+)(.*[\\n]?)', s)\n            if matches and self.is_newline:\n                self.handle_indent_dedent(matches.group(1))\n                s = matches.group(2)\n            if s.endswith(\"\\n\"):\n                self.add_token('COMMENT', s[:-1])\n                self.add_token('NEWLINE', \"\\n\")\n            else:\n                self.add_token('COMMENT', s)\n        elif self.is_newline:\n            self.handle_indent_dedent(s)\n            pass\n        return", "response": "t_whitespace_or_comment - Handles whitespace or comment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reduce(coro, iterable, initializer=None, limit=1, right=False, loop=None):\n    assert_corofunction(coro=coro)\n    assert_iter(iterable=iterable)\n\n    # Reduced accumulator value\n    acc = initializer\n\n    # If interable is empty, just return the initializer value\n    if len(iterable) == 0:\n        return initializer\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop)\n\n    # Reducer partial function for deferred coroutine execution\n    def reducer(element):\n        @asyncio.coroutine\n        def wrapper():\n            nonlocal acc\n            acc = yield from coro(acc, element)\n        return wrapper\n\n    # Support right reduction\n    if right:\n        iterable.reverse()\n\n    # Iterate and attach coroutine for defer scheduling\n    for element in iterable:\n        pool.add(reducer(element))\n\n    # Wait until all coroutines finish\n    yield from pool.run(ignore_empty=True)\n\n    # Returns final reduced value\n    return acc", "response": "A function that returns a reduced value from the items of the iterable in a single thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef times(coro, limit=1, raise_exception=False, return_value=None):\n    assert_corofunction(coro=coro)\n\n    # Store call times\n    limit = max(limit, 1)\n    times = limit\n\n    # Store result from last execution\n    result = None\n\n    @asyncio.coroutine\n    def wrapper(*args, **kw):\n        nonlocal limit\n        nonlocal result\n\n        # Check execution limit\n        if limit == 0:\n            if raise_exception:\n                raise RuntimeError(ExceptionMessage.format(times))\n            if return_value:\n                return return_value\n            return result\n\n        # Decreases counter\n        limit -= 1\n\n        # If return_value is present, do not memoize result\n        if return_value:\n            return (yield from coro(*args, **kw))\n\n        # Schedule coroutine and memoize result\n        result = yield from coro(*args, **kw)\n        return result\n\n    return wrapper", "response": "A function decorator that returns a new object that is executed only a certain amount of time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef each(coro, iterable, limit=0, loop=None,\n         collect=False, timeout=None, return_exceptions=False, *args, **kw):\n    \"\"\"\n    Concurrently iterates values yielded from an iterable, passing them to\n    an asynchronous coroutine.\n\n    You can optionally collect yielded values passing collect=True param,\n    which would be equivalent to `paco.map()``.\n\n    Mapped values will be returned as an ordered list.\n    Items order is preserved based on origin iterable order.\n\n    Concurrency level can be configurable via `limit` param.\n\n    All coroutines will be executed in the same loop.\n\n    This function is a coroutine.\n\n    This function can be composed in a pipeline chain with ``|`` operator.\n\n    Arguments:\n        coro (coroutinefunction): coroutine iterator function that accepts\n            iterable values.\n        iterable (iterable|asynchronousiterable): an iterable collection\n            yielding coroutines functions.\n        limit (int): max iteration concurrency limit. Use ``0`` for no limit.\n        collect (bool): return yielded values from coroutines. Default False.\n        loop (asyncio.BaseEventLoop): optional event loop to use.\n        return_exceptions (bool): enable/disable returning exceptions in case\n            of error. `collect` param must be True.\n        timeout (int|float): timeout can be used to control the maximum number\n            of seconds to wait before returning. timeout can be an int or\n            float. If timeout is not specified or None, there is no limit to\n            the wait time.\n        *args (mixed): optional variadic arguments to pass to the\n            coroutine iterable function.\n\n    Returns:\n        results (list): ordered list of values yielded by coroutines\n\n    Raises:\n        TypeError: in case of invalid input arguments.\n\n    Usage::\n\n        async def mul_2(num):\n            return num * 2\n\n        await paco.each(mul_2, [1, 2, 3, 4, 5])\n        # => None\n\n        await paco.each(mul_2, [1, 2, 3, 4, 5], collect=True)\n        # => [2, 4, 6, 8, 10]\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n    assert_iter(iterable=iterable)\n\n    # By default do not collect yielded values from coroutines\n    results = None\n\n    if collect:\n        # Store ordered results\n        results = [None] * len(iterable)\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop)\n\n    @asyncio.coroutine\n    def collector(index, item):\n        result = yield from safe_run(coro(item, *args, **kw),\n                                     return_exceptions=return_exceptions)\n        if collect:\n            results[index] = result\n\n        return result\n\n    # Iterate and pass elements to coroutine\n    for index, value in enumerate(iterable):\n        pool.add(collector(index, value))\n\n    # Wait until all the coroutines finishes\n    yield from pool.run(return_exceptions=return_exceptions,\n                        ignore_empty=True,\n                        timeout=timeout)\n\n    # Returns list of mapped results in order\n    return results", "response": "A generator function that yields all the values yielded from an iterable yielding them and then returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log(func=None, name=None, timing=True, timestamp=False):\n    # support to be called as @log or as @log(name='')\n    if func is None:\n        return functools.partial(log, name=name, timing=timing,\n                                 timestamp=timestamp)\n\n    @functools.wraps(func)\n    def decorated(*args, **kwargs):\n        assert len(args) > 0 and hasattr(args[0], 'log'), \\\n            \"The first argument of the decorated function must be a Context\"\n        ctx = args[0]\n        message = name\n        if message is None:\n            if func.__doc__:\n                message = func.__doc__.splitlines()[0].strip()\n        if message is None:\n            message = func.__name__\n        with ctx.log(message, timing=timing, timestamp=timestamp):\n            return func(*args, **kwargs)\n    return decorated", "response": "Decorator to log a running function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef constant(value, delay=None):\n    @asyncio.coroutine\n    def coro():\n        if delay:\n            yield from asyncio.sleep(delay)\n        return value\n\n    return coro", "response": "Returns a coroutine function that always returns the provided value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dropwhile(coro, iterable, loop=None):\n    drop = False\n\n    @asyncio.coroutine\n    def assert_fn(element):\n        nonlocal drop\n\n        if element and not drop:\n            return False\n\n        if not element and not drop:\n            drop = True\n\n        return True if drop else element\n\n    @asyncio.coroutine\n    def filter_fn(element):\n        return (yield from coro(element))\n\n    return (yield from filter(filter_fn, iterable,\n                              assert_fn=assert_fn, limit=1, loop=loop))", "response": "Returns an iterator that drops elements from the iterable as long as the predicate is true ; afterwards returns every element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an XMLID to an existing record", "response": "def add_xmlid(ctx, record, xmlid, noupdate=False):\n    \"\"\" Add a XMLID on an existing record \"\"\"\n    try:\n        ref_id, __, __ = ctx.env['ir.model.data'].xmlid_lookup(xmlid)\n    except ValueError:\n        pass  # does not exist, we'll create a new one\n    else:\n        return ctx.env['ir.model.data'].browse(ref_id)\n    if '.' in xmlid:\n        module, name = xmlid.split('.')\n    else:\n        module = ''\n        name = xmlid\n    return ctx.env['ir.model.data'].create({\n        'name': name,\n        'module': module,\n        'model': record._name,\n        'res_id': record.id,\n        'noupdate': noupdate,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates or update a record matching xmlid with values", "response": "def create_or_update(ctx, model, xmlid, values):\n    \"\"\" Create or update a record matching xmlid with values \"\"\"\n    if isinstance(model, basestring):\n        model = ctx.env[model]\n\n    record = ctx.env.ref(xmlid, raise_if_not_found=False)\n    if record:\n        record.update(values)\n    else:\n        record = model.create(values)\n        add_xmlid(ctx, record, xmlid)\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_record(ctx, item):\n    if isinstance(item, basestring):\n        return ctx.env.ref(item)\n    return item", "response": "Make sure we get a record instance even if we pass an xmlid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef switch_company(ctx, company):\n    current_company = ctx.env.user.company_id\n    ctx.env.user.company_id = safe_record(ctx, company)\n    yield ctx\n    ctx.env.user.company_id = current_company", "response": "Context manager to switch company record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a continuation coroutine function with some arguments already applied. Useful as a shorthand when combined with other control flow functions. Any arguments passed to the returned function are added to the arguments originally passed to apply. This is similar to `paco.partial()`. This function can be used as decorator. arguments: coro (coroutinefunction): coroutine function to wrap. *args (mixed): mixed variadic arguments for partial application. *kwargs (mixed): mixed variadic keyword arguments for partial application. Raises: TypeError: if coro argument is not a coroutine function. Returns: coroutinefunction: wrapped coroutine function. Usage:: async def hello(name, mark='!'): print('Hello, {name}{mark}'.format(name=name, mark=mark)) hello_mike = paco.apply(hello, 'Mike') await hello_mike() # => Hello, Mike! hello_mike = paco.apply(hello, 'Mike', mark='?') await hello_mike() # => Hello, Mike?", "response": "def apply(coro, *args, **kw):\n    \"\"\"\n    Creates a continuation coroutine function with some arguments\n    already applied.\n\n    Useful as a shorthand when combined with other control flow functions.\n    Any arguments passed to the returned function are added to the arguments\n    originally passed to apply.\n\n    This is similar to `paco.partial()`.\n\n    This function can be used as decorator.\n\n    arguments:\n        coro (coroutinefunction): coroutine function to wrap.\n        *args (mixed): mixed variadic arguments for partial application.\n        *kwargs (mixed): mixed variadic keyword arguments for partial\n            application.\n\n    Raises:\n        TypeError: if coro argument is not a coroutine function.\n\n    Returns:\n        coroutinefunction: wrapped coroutine function.\n\n    Usage::\n\n        async def hello(name, mark='!'):\n            print('Hello, {name}{mark}'.format(name=name, mark=mark))\n\n        hello_mike = paco.apply(hello, 'Mike')\n        await hello_mike()\n        # => Hello, Mike!\n\n        hello_mike = paco.apply(hello, 'Mike', mark='?')\n        await hello_mike()\n        # => Hello, Mike?\n\n    \"\"\"\n    assert_corofunction(coro=coro)\n\n    @asyncio.coroutine\n    def wrapper(*_args, **_kw):\n        # Explicitely ignore wrapper arguments\n        return (yield from coro(*args, **kw))\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(coro, loop=None):\n    loop = loop or asyncio.get_event_loop()\n    return loop.run_until_complete(coro)", "response": "A coroutine function that runs the coroutine until it completes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef postorder(self, node=None):\n        if node is None:\n            node = self.ast\n\n        try:\n            first = iter(node)\n        except TypeError:\n            first = None\n\n        if first:\n            for kid in node:\n                self.postorder(kid)\n\n        try:\n            name = 'n_' + self.typestring(node)\n            if hasattr(self, name):\n                func = getattr(self, name)\n                func(node)\n            else:\n                self.default(node)\n        except GenericASTTraversalPruningException:\n            return\n\n        name = name + '_exit'\n        if hasattr(self, name):\n            func = getattr(self, name)\n            func(node)", "response": "Walk the tree in roughly postorder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_expr_add_term(self, args):\n        ' expr ::= expr ADD_OP term '\n        op = 'add' if args[1].attr == '+' else 'subtract'\n        return AST(op, [args[0], args[2]])", "response": "expr :: = expr ADD_OP term"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nterms :: = term MULT_OP factor", "response": "def p_term_mult_factor(self, args):\n        ' term ::= term MULT_OP factor '\n        op = 'multiply' if args[1].attr == '*' else 'divide'\n        return AST(op, [args[0], args[2]])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for the Futures and coroutine objects given by the sequence futures to complete, with optional concurrency limit. Coroutines will be wrapped in Tasks. ``timeout`` can be used to control the maximum number of seconds to wait before returning. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If ``return_exceptions`` is True, exceptions in the tasks are treated the same as successful results, and gathered in the result list; otherwise, the first raised exception will be immediately propagated to the returned future. ``return_when`` indicates when this function should return. It must be one of the following constants of the concurrent.futures module. All futures must share the same event loop. This functions is mostly compatible with Python standard ``asyncio.wait()``. Arguments: *coros_or_futures (iter|list): an iterable collection yielding coroutines functions. limit (int): optional concurrency execution limit. Use ``0`` for no limit. timeout (int/float): maximum number of seconds to wait before returning. return_exceptions (bool): exceptions in the tasks are treated the same as successful results, instead of raising them. return_when (str): indicates when this function should return. loop (asyncio.BaseEventLoop): optional event loop to use. *args (mixed): optional variadic argument to pass to the coroutines function. Returns: tuple: Returns two sets of Future: (done, pending). Raises: TypeError: in case of invalid coroutine object. ValueError: in case of empty set of coroutines or futures. TimeoutError: if execution takes more than expected. Usage:: async def sum(x, y): return x + y done, pending = await paco.wait( sum(1, 2), sum(3, 4)) [task.result() for task in done] # => [3, 7]", "response": "def wait(*coros_or_futures, limit=0, timeout=None, loop=None,\n         return_exceptions=False, return_when='ALL_COMPLETED'):\n    \"\"\"\n    Wait for the Futures and coroutine objects given by the sequence\n    futures to complete, with optional concurrency limit.\n    Coroutines will be wrapped in Tasks.\n\n    ``timeout`` can be used to control the maximum number of seconds to\n    wait before returning. timeout can be an int or float.\n    If timeout is not specified or None, there is no limit to the wait time.\n\n    If ``return_exceptions`` is True, exceptions in the tasks are treated the\n    same as successful results, and gathered in the result list; otherwise,\n    the first raised exception will be immediately propagated to the\n    returned future.\n\n    ``return_when`` indicates when this function should return.\n    It must be one of the following constants of the concurrent.futures module.\n\n    All futures must share the same event loop.\n\n    This functions is mostly compatible with Python standard\n    ``asyncio.wait()``.\n\n    Arguments:\n        *coros_or_futures (iter|list):\n            an iterable collection yielding coroutines functions.\n        limit (int):\n            optional concurrency execution limit. Use ``0`` for no limit.\n        timeout (int/float):\n            maximum number of seconds to wait before returning.\n        return_exceptions (bool):\n            exceptions in the tasks are treated the same as successful results,\n            instead of raising them.\n        return_when (str):\n            indicates when this function should return.\n        loop (asyncio.BaseEventLoop):\n            optional event loop to use.\n        *args (mixed):\n            optional variadic argument to pass to the coroutines function.\n\n    Returns:\n        tuple: Returns two sets of Future: (done, pending).\n\n    Raises:\n        TypeError: in case of invalid coroutine object.\n        ValueError: in case of empty set of coroutines or futures.\n        TimeoutError: if execution takes more than expected.\n\n    Usage::\n\n        async def sum(x, y):\n            return x + y\n\n        done, pending = await paco.wait(\n            sum(1, 2),\n            sum(3, 4))\n        [task.result() for task in done]\n        # => [3, 7]\n    \"\"\"\n    # Support iterable as first argument for better interoperability\n    if len(coros_or_futures) == 1 and isiter(coros_or_futures[0]):\n        coros_or_futures = coros_or_futures[0]\n\n    # If no coroutines to schedule, return empty list\n    # Mimics asyncio behaviour.\n    if len(coros_or_futures) == 0:\n        raise ValueError('paco: set of coroutines/futures is empty')\n\n    # Create concurrent executor\n    pool = ConcurrentExecutor(limit=limit, loop=loop,\n                              coros=coros_or_futures)\n\n    # Wait until all the tasks finishes\n    return (yield from pool.run(timeout=timeout,\n                                return_when=return_when,\n                                return_exceptions=return_exceptions))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an attribute to the set of pending attributes.", "response": "def addAttribute(self, attrib: Attribute):\n        \"\"\"\n        Used to create a new attribute on Sovrin\n        :param attrib: attribute to add\n        :return: number of pending txns\n        \"\"\"\n        self._attributes[attrib.key()] = attrib\n        req = attrib.ledgerRequest()\n        if req:\n            self.pendRequest(req, attrib.key())\n        return len(self._pending)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addNode(self, node: Node):\n        self._nodes[node.id] = node\n        req = node.ledgerRequest()\n        if req:\n            self.pendRequest(req, node.id)\n        return len(self._pending)", "response": "Used to add a new node on Sovrin\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef doPoolUpgrade(self, upgrade: Upgrade):\n        key = upgrade.key\n        self._upgrades[key] = upgrade\n        req = upgrade.ledgerRequest()\n        if req:\n            self.pendRequest(req, key)\n        return len(self._pending)", "response": "Used to send a new code to the pool"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handleIncomingReply(self, observer_name, reqId, frm, result,\n                            numReplies):\n        \"\"\"\n        Called by an external entity, like a Client, to notify of incoming\n        replies\n        :return:\n        \"\"\"\n        preparedReq = self._prepared.get((result[IDENTIFIER], reqId))\n        if not preparedReq:\n            raise RuntimeError('no matching prepared value for {},{}'.\n                               format(result[IDENTIFIER], reqId))\n        typ = result.get(TXN_TYPE)\n        if typ and typ in self.replyHandler:\n            self.replyHandler[typ](result, preparedReq)", "response": "Called by the client to notify about incoming reply"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting an attribute from the Sovrin .", "response": "def requestAttribute(self, attrib: Attribute, sender):\n        \"\"\"\n        Used to get a raw attribute from Sovrin\n        :param attrib: attribute to add\n        :return: number of pending txns\n        \"\"\"\n        self._attributes[attrib.key()] = attrib\n        req = attrib.getRequest(sender)\n        if req:\n            return self.prepReq(req, key=attrib.key())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting a schema from Sovrin s object store", "response": "def requestSchema(self, nym, name, version, sender):\n        \"\"\"\n        Used to get a schema from Sovrin\n        :param nym: nym that schema is attached to\n        :param name: name of schema\n        :param version: version of schema\n        :return: req object\n        \"\"\"\n        operation = { TARGET_NYM: nym,\n                      TXN_TYPE: GET_SCHEMA,\n                      DATA: {NAME : name,\n                             VERSION: version}\n        }\n\n        req = Request(sender, operation=operation)\n        return self.prepReq(req)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting a claim def from Sovrin", "response": "def requestClaimDef(self, seqNo, signature, sender):\n        \"\"\"\n        Used to get a claim def from Sovrin\n        :param seqNo: reference number of schema\n        :param signature: CL is only supported option currently\n        :return: req object\n        \"\"\"\n        operation = { TXN_TYPE: GET_CLAIM_DEF,\n                      ORIGIN: sender,\n                      REF : seqNo,\n                      SIGNATURE_TYPE : signature\n                    }\n\n        req = Request(sender, operation=operation)\n        return self.prepReq(req)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a file or function name.", "response": "def t_file_or_func(self, s):\n        r'(?:[^*-+,\\d\\'\"\\t \\n:][^\\'\"\\t \\n:,]*)|(?:^\"\"\".+\"\"\")|(?:\\'\\'\\'.+\\'\\'\\')'\n        maybe_funcname = True\n        if s == 'if':\n            self.add_token('IF', s)\n            return\n        if s[0] in frozenset(('\"', \"'\")):\n            # Pick out text inside of triple-quoted string\n            if ( (s.startswith(\"'''\") and s.endswith(\"'''\") ) or\n                 (s.startswith('\"\"\"') and s.endswith('\"\"\"') ) ):\n                base = s[3:-3]\n            else:\n                # Pick out text inside singly-quote string\n                base = s[1:-1]\n            maybe_funcname = False\n        else:\n            base = s\n        pos = self.pos\n        if maybe_funcname and re.match('[a-zA-Z_][[a-zA-Z_.0-9\\[\\]]+\\(\\)', s):\n            self.add_token('FUNCNAME', base)\n        else:\n            self.add_token('FILENAME', base)\n        self.pos = pos + len(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the given coroutine functions in series, each one running once the previous execution has completed. If any coroutines raises an exception, no more coroutines are executed. Otherwise, the coroutines returned values will be returned as `list`. ``timeout`` can be used to control the maximum number of seconds to wait before returning. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If ``return_exceptions`` is True, exceptions in the tasks are treated the same as successful results, and gathered in the result list; otherwise, the first raised exception will be immediately propagated to the returned future. All futures must share the same event loop. This functions is basically the sequential execution version of ``asyncio.gather()``. Interface compatible with ``asyncio.gather()``. This function is a coroutine. Arguments: *coros_or_futures (iter|list): an iterable collection yielding coroutines functions. timeout (int/float): maximum number of seconds to wait before returning. return_exceptions (bool): exceptions in the tasks are treated the same as successful results, instead of raising them. loop (asyncio.BaseEventLoop): optional event loop to use. *args (mixed): optional variadic argument to pass to the coroutines function. Returns: list: coroutines returned results. Raises: TypeError: in case of invalid coroutine object. ValueError: in case of empty set of coroutines or futures. TimeoutError: if execution takes more than expected. Usage:: async def sum(x, y): return x + y await paco.series( sum(1, 2), sum(2, 3), sum(3, 4)) # => [3, 5, 7]", "response": "def series(*coros_or_futures, timeout=None,\n           loop=None, return_exceptions=False):\n    \"\"\"\n    Run the given coroutine functions in series, each one\n    running once the previous execution has completed.\n\n    If any coroutines raises an exception, no more\n    coroutines are executed. Otherwise, the coroutines returned values\n    will be returned as `list`.\n\n    ``timeout`` can be used to control the maximum number of seconds to\n    wait before returning. timeout can be an int or float.\n    If timeout is not specified or None, there is no limit to the wait time.\n\n    If ``return_exceptions`` is True, exceptions in the tasks are treated the\n    same as successful results, and gathered in the result list; otherwise,\n    the first raised exception will be immediately propagated to the\n    returned future.\n\n    All futures must share the same event loop.\n\n    This functions is basically the sequential execution version of\n    ``asyncio.gather()``. Interface compatible with ``asyncio.gather()``.\n\n    This function is a coroutine.\n\n    Arguments:\n        *coros_or_futures (iter|list):\n            an iterable collection yielding coroutines functions.\n        timeout (int/float):\n            maximum number of seconds to wait before returning.\n        return_exceptions (bool):\n            exceptions in the tasks are treated the same as successful results,\n            instead of raising them.\n        loop (asyncio.BaseEventLoop):\n            optional event loop to use.\n        *args (mixed):\n            optional variadic argument to pass to the coroutines function.\n\n    Returns:\n        list: coroutines returned results.\n\n    Raises:\n        TypeError: in case of invalid coroutine object.\n        ValueError: in case of empty set of coroutines or futures.\n        TimeoutError: if execution takes more than expected.\n\n    Usage::\n\n        async def sum(x, y):\n            return x + y\n\n        await paco.series(\n            sum(1, 2),\n            sum(2, 3),\n            sum(3, 4))\n        # => [3, 5, 7]\n\n    \"\"\"\n    return (yield from gather(*coros_or_futures,\n                              loop=loop, limit=1, timeout=timeout,\n                              return_exceptions=return_exceptions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef repeat(coro, times=1, step=1, limit=1, loop=None):\n    assert_corofunction(coro=coro)\n\n    # Iterate and attach coroutine for defer scheduling\n    times = max(int(times), 1)\n    iterable = range(1, times + 1, step)\n\n    # Run iterable times\n    return (yield from map(coro, iterable, limit=limit, loop=loop))", "response": "Execute a coroutine function and return a list of yielded values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef once(coro, raise_exception=False, return_value=None):\n    return times(coro,\n                 limit=1,\n                 return_value=return_value,\n                 raise_exception=raise_exception)", "response": "A coroutine function that returns the value of the first entry in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef defer(coro, delay=1):\n    assert_corofunction(coro=coro)\n\n    @asyncio.coroutine\n    def wrapper(*args, **kw):\n        # Wait until we're done\n        yield from asyncio.sleep(delay)\n        return (yield from coro(*args, **kw))\n\n    return wrapper", "response": "Returns a coroutine function that will defer the given coroutine function execution for a certain amount of seconds in a non - blocking way."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef safe_run(coro, return_exceptions=False):\n    try:\n        result = yield from coro\n    except Exception as err:\n        if return_exceptions:\n            result = err\n        else:\n            raise err\n    return result", "response": "A context manager that returns a value from a given coroutine and optionally catches exceptions returning them as value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncollecting is used internally to execute coroutines and collect the returned value. This function is intended to be used internally.", "response": "def collect(coro, index, results,\n            preserve_order=False,\n            return_exceptions=False):\n    \"\"\"\n    Collect is used internally to execute coroutines and collect the returned\n    value. This function is intended to be used internally.\n    \"\"\"\n    result = yield from safe_run(coro, return_exceptions=return_exceptions)\n\n    if preserve_order:\n        results[index] = result\n    else:\n        results.append(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self):\n        if self.running:\n            raise RuntimeError('paco: executor is still running')\n\n        self.pool.clear()\n        self.observer.clear()\n        self.semaphore = asyncio.Semaphore(self.limit, loop=self.loop)", "response": "Resets the executer scheduler internal state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, coro, *args, **kw):\n        # Create coroutine object if a function is provided\n        if asyncio.iscoroutinefunction(coro):\n            coro = coro(*args, **kw)\n\n        # Verify coroutine\n        if not asyncio.iscoroutine(coro):\n            raise TypeError('paco: coro must be a coroutine object')\n\n        # Store coroutine with arguments for deferred execution\n        index = max(len(self.pool), 0)\n        task = Task(index, coro)\n\n        # Append the coroutine data to the pool\n        self.pool.append(task)\n\n        return coro", "response": "Adds a new coroutine function to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self,\n            timeout=None,\n            return_when=None,\n            return_exceptions=None,\n            ignore_empty=None):\n        \"\"\"\n        Executes the registered coroutines in the executor queue.\n\n        Arguments:\n            timeout (int/float): max execution timeout. No limit by default.\n            return_exceptions (bool): in case of coroutine exception.\n            return_when (str): sets when coroutine should be resolved.\n                See `asyncio.wait`_ for supported values.\n            ignore_empty (bool, optional): do not raise an exception if there are\n                no coroutines to schedule are empty.\n\n        Returns:\n            asyncio.Future (tuple): two sets of Futures: ``(done, pending)``\n\n        Raises:\n            ValueError: if there is no coroutines to schedule.\n            RuntimeError: if executor is still running.\n            TimeoutError: if execution takes more than expected.\n\n        .. _asyncio.wait: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait  # noqa\n        \"\"\"\n        # Only allow 1 concurrent execution\n        if self.running:\n            raise RuntimeError('paco: executor is already running')\n\n        # Overwrite ignore empty behaviour, if explicitly defined\n        ignore_empty = (self.ignore_empty if ignore_empty is None\n                        else ignore_empty)\n\n        # Check we have coroutines to schedule\n        if len(self.pool) == 0:\n            # If ignore empty mode enabled, just return an empty tuple\n            if ignore_empty:\n                return (tuple(), tuple())\n            # Othwerise raise an exception\n            raise ValueError('paco: pool of coroutines is empty')\n\n        # Set executor state to running\n        self.running = True\n\n        # Configure return exceptions\n        if return_exceptions is not None:\n            self.return_exceptions = return_exceptions\n\n        if return_exceptions is False and return_when is None:\n            return_when = 'FIRST_EXCEPTION'\n\n        if return_when is None:\n            return_when = 'ALL_COMPLETED'\n\n        # Trigger pre-execution event\n        yield from self.observer.trigger('start', self)\n\n        # Sequential coroutines execution\n        if self.limit == 1:\n            done, pending = yield from self._run_sequentially()\n\n        # Concurrent execution based on configured limit\n        if self.limit != 1:\n            done, pending = yield from self._run_concurrently(\n                timeout=timeout,\n                return_when=return_when)\n\n        # Reset internal state and queue\n        self.running = False\n\n        # Raise exception, if needed\n        if self.return_exceptions is False and self.errors:\n            err = self.errors[0]\n            err.errors = self.errors[1:]\n            raise err\n\n        # Trigger pre-execution event\n        yield from self.observer.trigger('finish', self)\n\n        # Reset executor state to defaults after each execution\n        self.reset()\n\n        # Return resultant futures in two tuples\n        return done, pending", "response": "Executes the registered coroutines in the executor queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a value of an interval, this function returns the next interval value", "response": "def next_interval(self, interval):\n        \"\"\"\n        Given a value of an interval, this function returns the \n        next interval value\n        \"\"\"\n        index = np.where(self.intervals == interval)\n        if index[0][0] + 1 < len(self.intervals):\n            return self.intervals[index[0][0] + 1]\n        else:\n            raise IndexError(\"Ran out of intervals!\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nearest_interval(self, interval):\n        thresh_range = 25  # in cents\n        if interval < self.intervals[0] - thresh_range or interval > self.intervals[-1] + thresh_range:\n            raise IndexError(\"The interval given is beyond \" + str(thresh_range)\n                             + \" cents over the range of intervals defined.\")\n\n        index = find_nearest_index(self.intervals, interval)\n        return self.intervals[index]", "response": "This function returns the nearest interval to any given interval."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noptimise ML distance between two partials. min and max set brackets", "response": "def brent_optimise(node1, node2, min_brlen=0.001, max_brlen=10, verbose=False):\n    \"\"\"\n    Optimise ML distance between two partials. min and max set brackets\n    \"\"\"\n    from scipy.optimize import minimize_scalar\n    wrapper = BranchLengthOptimiser(node1, node2, (min_brlen + max_brlen) / 2.)\n    n = minimize_scalar(lambda x: -wrapper(x)[0], method='brent', bracket=(min_brlen, max_brlen))['x']\n    if verbose:\n        logger.info(wrapper)\n    if n < min_brlen:\n        n = min_brlen\n        wrapper(n)\n    return n, -1 / wrapper.get_d2lnl(n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads an alignment and calculate all pairwise distances and variances of the same sequence.", "response": "def pairdists(alignment, subs_model, alpha=None, ncat=4, tolerance=1e-6, verbose=False):\n    \"\"\" Load an alignment, calculate all pairwise distances and variances\n        model parameter must be a Substitution model type from phylo_utils \"\"\"\n\n    # Check\n    if not isinstance(subs_model, phylo_utils.models.Model):\n        raise ValueError(\"Can't handle this model: {}\".format(model))\n\n    if alpha is None:\n        alpha = 1.0\n        ncat = 1\n\n    # Set up markov model\n    tm = TransitionMatrix(subs_model)\n\n    gamma_rates = discrete_gamma(alpha, ncat)\n    partials = alignment_to_partials(alignment)\n    seqnames = alignment.get_names()\n    nseq = len(seqnames)\n    distances = np.zeros((nseq, nseq))\n    variances = np.zeros((nseq, nseq))\n\n    # Check the model has the appropriate size\n    if not subs_model.size == partials[seqnames[0]].shape[1]:\n        raise ValueError(\"Model {} expects {} states, but the alignment has {}\".format(model.name,\n                                                                                       model.size,\n                                                                                       partials[seqnames[0]].shape[1]))\n\n    nodes = [phylo_utils.likelihood.LnlModel(tm) for seq in range(nseq)]\n    for node, header in zip(nodes, seqnames):\n        node.set_partials(partials[header])  # retrieve partial likelihoods from partials dictionary\n\n    for i, j in itertools.combinations(range(nseq), 2):\n        brlen, var = brent_optimise(nodes[i], nodes[j], verbose=verbose)\n        distances[i, j] = distances[j, i] = brlen\n        variances[i, j] = variances[j, i] = var\n    dm = DistanceMatrix.from_array(distances, names=seqnames)\n    vm = DistanceMatrix.from_array(variances, names=seqnames)\n    return dm, vm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the alignment to a file using Bio. AlignIO.", "response": "def write_alignment(self, filename, file_format, interleaved=None):\n        \"\"\"\n        Write the alignment to file using Bio.AlignIO\n        \"\"\"\n        if file_format == 'phylip':\n            file_format = 'phylip-relaxed'\n        AlignIO.write(self._msa, filename, file_format)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute pairwise distances between all sequences according to a given substitution model.", "response": "def compute_distances(self, model, alpha=None, ncat=4, tolerance=1e-6):\n        \"\"\"\n        Compute pairwise distances between all sequences according to\n        a given substitution model, `model`, of type phylo_utils.models.Model\n        (e.g. phylo_utils.models.WAG(freqs), phylo_utils.models.GTR(rates, freqs))\n        The number of gamma categories is controlled by `ncat`. Setting ncat=1\n        disable gamma rate variation. The gamma alpha parameter must be supplied\n        to enable gamma rate variation.\n        \"\"\"\n        return pairdists(self, model, alpha, ncat, tolerance)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsimulating the given set of sequences under the transition matrix s model .", "response": "def simulate(self, nsites, transition_matrix, tree, ncat=1, alpha=1):\n        \"\"\"\n        Return sequences simulated under the transition matrix's model \n        \"\"\"\n        sim = SequenceSimulator(transition_matrix, tree, ncat, alpha)\n        return list(sim.simulate(nsites).items())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bootstrap(self):\n        new_sites = sorted(sample_wr(self.get_sites()))\n        seqs = list(zip(self.get_names(), (''.join(seq) for seq in zip(*new_sites))))\n        return self.__class__(seqs)", "response": "Return a new Alignment that is a bootstrap replicate of self\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevolve multiple sites during one tree traversal", "response": "def simulate(self, n):\n        \"\"\"\n        Evolve multiple sites during one tree traversal\n        \"\"\"\n        self.tree._tree.seed_node.states = self.ancestral_states(n)\n        categories = np.random.randint(self.ncat, size=n).astype(np.intc)\n\n        for node in self.tree.preorder(skip_seed=True):\n            node.states = self.evolve_states(node.parent_node.states, categories, node.pmats)\n            if node.is_leaf():\n                self.sequences[node.taxon.label] = node.states\n        return self.sequences_to_string()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate ancestral sequence states from the equilibrium states.", "response": "def ancestral_states(self, n):\n        \"\"\"\n        Generate ancestral sequence states from the equilibrium frequencies\n        \"\"\"\n        anc = np.empty(n, dtype=np.intc)\n        _weighted_choices(self.state_indices, self.freqs, anc)\n        return anc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evolve_states(self, parent_states, categories, probs):\n        child_states = np.empty(parent_states.shape, dtype=np.intc)\n        _evolve_states(self.state_indices, parent_states, categories, probs, child_states)\n        return child_states", "response": "Evolve states from parent to child."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequences_to_string(self):\n        return {k: ''.join(self.states[v]) for (k, v) in self.sequences.items()}", "response": "Convert state indices to a string of characters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the CRC for given Motorola S - Record hexstring.", "response": "def crc_srec(hexstr):\n    \"\"\"Calculate the CRC for given Motorola S-Record hexstring.\n\n    \"\"\"\n\n    crc = sum(bytearray(binascii.unhexlify(hexstr)))\n    crc &= 0xff\n    crc ^= 0xff\n\n    return crc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crc_ihex(hexstr):\n\n    crc = sum(bytearray(binascii.unhexlify(hexstr)))\n    crc &= 0xff\n    crc = ((~crc + 1) & 0xff)\n\n    return crc", "response": "Calculate the CRC for given Intel HEX hexstring."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npacks a Motorola S - Record record into a string.", "response": "def pack_srec(type_, address, size, data):\n    \"\"\"Create a Motorola S-Record record of given data.\n\n    \"\"\"\n\n    if type_ in '0159':\n        line = '{:02X}{:04X}'.format(size + 2 + 1, address)\n    elif type_ in '268':\n        line = '{:02X}{:06X}'.format(size + 3 + 1, address)\n    elif type_ in '37':\n        line = '{:02X}{:08X}'.format(size + 4 + 1, address)\n    else:\n        raise Error(\n            \"expected record type 0..3 or 5..9, but got '{}'\".format(type_))\n\n    if data:\n        line += binascii.hexlify(data).decode('ascii').upper()\n\n    return 'S{}{}{:02X}'.format(type_, line, crc_srec(line))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunpacks given Motorola S - Record into variables.", "response": "def unpack_srec(record):\n    \"\"\"Unpack given Motorola S-Record record into variables.\n\n    \"\"\"\n\n    # Minimum STSSCC, where T is type, SS is size and CC is crc.\n    if len(record) < 6:\n        raise Error(\"record '{}' too short\".format(record))\n\n    if record[0] != 'S':\n        raise Error(\n            \"record '{}' not starting with an 'S'\".format(record))\n\n    size = int(record[2:4], 16)\n    type_ = record[1:2]\n\n    if type_ in '0159':\n        width = 4\n    elif type_ in '268':\n        width = 6\n    elif type_ in '37':\n        width = 8\n    else:\n        raise Error(\n            \"expected record type 0..3 or 5..9, but got '{}'\".format(type_))\n\n    data_offset = (4 + width)\n    crc_offset = (4 + 2 * size - 2)\n\n    address = int(record[4:data_offset], 16)\n    data = binascii.unhexlify(record[data_offset:crc_offset])\n    actual_crc = int(record[crc_offset:], 16)\n    expected_crc = crc_srec(record[2:crc_offset])\n\n    if actual_crc != expected_crc:\n        raise Error(\n            \"expected crc '{:02X}' in record {}, but got '{:02X}'\".format(\n                expected_crc,\n                record,\n                actual_crc))\n\n    return (type_, address, size - 1 - width // 2, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pack_ihex(type_, address, size, data):\n\n    line = '{:02X}{:04X}{:02X}'.format(size, address, type_)\n\n    if data:\n        line += binascii.hexlify(data).decode('ascii').upper()\n\n    return ':{}{:02X}'.format(line, crc_ihex(line))", "response": "Pack a Intel HEX record of given data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpack_ihex(record):\n\n    # Minimum :SSAAAATTCC, where SS is size, AAAA is address, TT is\n    # type and CC is crc.\n    if len(record) < 11:\n        raise Error(\"record '{}' too short\".format(record))\n\n    if record[0] != ':':\n        raise Error(\"record '{}' not starting with a ':'\".format(record))\n\n    size = int(record[1:3], 16)\n    address = int(record[3:7], 16)\n    type_ = int(record[7:9], 16)\n\n    if size > 0:\n        data = binascii.unhexlify(record[9:9 + 2 * size])\n    else:\n        data = b''\n\n    actual_crc = int(record[9 + 2 * size:], 16)\n    expected_crc = crc_ihex(record[1:9 + 2 * size])\n\n    if actual_crc != expected_crc:\n        raise Error(\n            \"expected crc '{:02X}' in record {}, but got '{:02X}'\".format(\n                expected_crc,\n                record,\n                actual_crc))\n\n    return (type_, address, size, data)", "response": "Unpack given Intel HEX record into variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chunks(self, size=32, alignment=1):\n\n        if (size % alignment) != 0:\n            raise Error(\n                'size {} is not a multiple of alignment {}'.format(\n                    size,\n                    alignment))\n\n        address = self.address\n        data = self.data\n\n        # First chunk may be shorter than `size` due to alignment.\n        chunk_offset = (address % alignment)\n\n        if chunk_offset != 0:\n            first_chunk_size = (alignment - chunk_offset)\n            yield self._Chunk(address, data[:first_chunk_size])\n            address += (first_chunk_size // self._word_size_bytes)\n            data = data[first_chunk_size:]\n        else:\n            first_chunk_size = 0\n\n        for offset in range(0, len(data), size):\n            yield self._Chunk(address + offset // self._word_size_bytes,\n                              data[offset:offset + size])", "response": "Yields size chunks of the data in the specified alignment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd given data to the current segment.", "response": "def add_data(self, minimum_address, maximum_address, data, overwrite):\n        \"\"\"Add given data to this segment. The added data must be adjacent to\n        the current segment data, otherwise an exception is thrown.\n\n        \"\"\"\n\n        if minimum_address == self.maximum_address:\n            self.maximum_address = maximum_address\n            self.data += data\n        elif maximum_address == self.minimum_address:\n            self.minimum_address = minimum_address\n            self.data = data + self.data\n        elif (overwrite\n              and minimum_address < self.maximum_address\n              and maximum_address > self.minimum_address):\n            self_data_offset = minimum_address - self.minimum_address\n\n            # Prepend data.\n            if self_data_offset < 0:\n                self_data_offset *= -1\n                self.data = data[:self_data_offset] + self.data\n                del data[:self_data_offset]\n                self.minimum_address = minimum_address\n\n            # Overwrite overlapping part.\n            self_data_left = len(self.data) - self_data_offset\n\n            if len(data) <= self_data_left:\n                self.data[self_data_offset:self_data_offset + len(data)] = data\n                data = bytearray()\n            else:\n                self.data[self_data_offset:] = data[:self_data_left]\n                data = data[self_data_left:]\n\n            # Append data.\n            if len(data) > 0:\n                self.data += data\n                self.maximum_address = maximum_address\n        else:\n            raise AddDataError(\n                'data added to a segment must be adjacent to or overlapping '\n                'with the original segment data')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_data(self, minimum_address, maximum_address):\n\n        if ((minimum_address >= self.maximum_address)\n            and (maximum_address <= self.minimum_address)):\n            raise Error('cannot remove data that is not part of the segment')\n\n        if minimum_address < self.minimum_address:\n            minimum_address = self.minimum_address\n\n        if maximum_address > self.maximum_address:\n            maximum_address = self.maximum_address\n\n        remove_size = maximum_address - minimum_address\n        part1_size = minimum_address - self.minimum_address\n        part1_data = self.data[0:part1_size]\n        part2_data = self.data[part1_size + remove_size:]\n\n        if len(part1_data) and len(part2_data):\n            # Update this segment and return the second segment.\n            self.maximum_address = self.minimum_address + part1_size\n            self.data = part1_data\n\n            return _Segment(maximum_address,\n                            maximum_address + len(part2_data),\n                            part2_data,\n                            self._word_size_bytes)\n        else:\n            # Update this segment.\n            if len(part1_data) > 0:\n                self.maximum_address = minimum_address\n                self.data = part1_data\n            elif len(part2_data) > 0:\n                self.minimum_address = maximum_address\n                self.data = part2_data\n            else:\n                self.maximum_address = self.minimum_address\n                self.data = bytearray()", "response": "Removes data from this segment. Returns the second segment if the removed data splits this segment in two."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a segment by ascending address.", "response": "def add(self, segment, overwrite=False):\n        \"\"\"Add segments by ascending address.\n\n        \"\"\"\n\n        if self._list:\n            if segment.minimum_address == self._current_segment.maximum_address:\n                # Fast insertion for adjacent segments.\n                self._current_segment.add_data(segment.minimum_address,\n                                              segment.maximum_address,\n                                              segment.data,\n                                              overwrite)\n            else:\n                # Linear insert.\n                for i, s in enumerate(self._list):\n                    if segment.minimum_address <= s.maximum_address:\n                        break\n\n                if segment.minimum_address > s.maximum_address:\n                    # Non-overlapping, non-adjacent after.\n                    self._list.append(segment)\n                elif segment.maximum_address < s.minimum_address:\n                    # Non-overlapping, non-adjacent before.\n                    self._list.insert(i, segment)\n                else:\n                    # Adjacent or overlapping.\n                    s.add_data(segment.minimum_address,\n                               segment.maximum_address,\n                               segment.data,\n                               overwrite)\n                    segment = s\n\n                self._current_segment = segment\n                self._current_segment_index = i\n\n            # Remove overwritten and merge adjacent segments.\n            while self._current_segment is not self._list[-1]:\n                s = self._list[self._current_segment_index + 1]\n\n                if self._current_segment.maximum_address >= s.maximum_address:\n                    # The whole segment is overwritten.\n                    del self._list[self._current_segment_index + 1]\n                elif self._current_segment.maximum_address >= s.minimum_address:\n                    # Adjacent or beginning of the segment overwritten.\n                    self._current_segment.add_data(\n                        self._current_segment.maximum_address,\n                        s.maximum_address,\n                        s.data[self._current_segment.maximum_address - s.minimum_address:],\n                        overwrite=False)\n                    del self._list[self._current_segment_index+1]\n                    break\n                else:\n                    # Segments are not overlapping, nor adjacent.\n                    break\n        else:\n            self._list.append(segment)\n            self._current_segment = segment\n            self._current_segment_index = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over all segments and return chunks of the data aligned as size and alignment.", "response": "def chunks(self, size=32, alignment=1):\n        \"\"\"Iterate over all segments and return chunks of the data aligned as\n        given by `alignment`. `size` must be a multiple of\n        `alignment`. Each chunk is returned as a named two-tuple of\n        its address and data.\n\n        \"\"\"\n\n        if (size % alignment) != 0:\n            raise Error(\n                'size {} is not a multiple of alignment {}'.format(\n                    size,\n                    alignment))\n\n        for segment in self:\n            for chunk in segment.chunks(size, alignment):\n                yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maximum_address(self):\n\n        maximum_address = self._segments.maximum_address\n\n        if maximum_address is not None:\n            maximum_address //= self.word_size_bytes\n\n        return maximum_address", "response": "The maximum address of the data in the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef header(self):\n\n        if self._header_encoding is None:\n            return self._header\n        else:\n            return self._header.decode(self._header_encoding)", "response": "The binary file header or None if missing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, data, overwrite=False):\n\n        if is_srec(data):\n            self.add_srec(data, overwrite)\n        elif is_ihex(data):\n            self.add_ihex(data, overwrite)\n        elif is_ti_txt(data):\n            self.add_ti_txt(data, overwrite)\n        else:\n            raise UnsupportedFileFormatError()", "response": "Add given data string by guessing its format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd given Motorola S - Records string. Set overwrite to True to allow already added data to be overwritten.", "response": "def add_srec(self, records, overwrite=False):\n        \"\"\"Add given Motorola S-Records string. Set `overwrite` to ``True`` to\n        allow already added data to be overwritten.\n\n        \"\"\"\n\n        for record in StringIO(records):\n            type_, address, size, data = unpack_srec(record.strip())\n\n            if type_ == '0':\n                self._header = data\n            elif type_ in '123':\n                address *= self.word_size_bytes\n                self._segments.add(_Segment(address,\n                                            address + size,\n                                            bytearray(data),\n                                            self.word_size_bytes),\n                                   overwrite)\n            elif type_ in '789':\n                self.execution_start_address = address"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_ihex(self, records, overwrite=False):\n\n        extended_segment_address = 0\n        extended_linear_address = 0\n\n        for record in StringIO(records):\n            type_, address, size, data = unpack_ihex(record.strip())\n\n            if type_ == IHEX_DATA:\n                address = (address\n                           + extended_segment_address\n                           + extended_linear_address)\n                address *= self.word_size_bytes\n                self._segments.add(_Segment(address,\n                                            address + size,\n                                            bytearray(data),\n                                            self.word_size_bytes),\n                                   overwrite)\n            elif type_ == IHEX_END_OF_FILE:\n                pass\n            elif type_ == IHEX_EXTENDED_SEGMENT_ADDRESS:\n                extended_segment_address = int(binascii.hexlify(data), 16)\n                extended_segment_address *= 16\n            elif type_ == IHEX_EXTENDED_LINEAR_ADDRESS:\n                extended_linear_address = int(binascii.hexlify(data), 16)\n                extended_linear_address <<= 16\n            elif type_ in [IHEX_START_SEGMENT_ADDRESS, IHEX_START_LINEAR_ADDRESS]:\n                self.execution_start_address = int(binascii.hexlify(data), 16)\n            else:\n                raise Error(\"expected type 1..5 in record {}, but got {}\".format(\n                    record,\n                    type_))", "response": "Add given Intel HEX records string. Set overwrite to True to add data to be overwritten."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd given TI - TXT string lines. Set overwrite to True to allow already added data to be overwritten.", "response": "def add_ti_txt(self, lines, overwrite=False):\n        \"\"\"Add given TI-TXT string `lines`. Set `overwrite` to ``True`` to\n        allow already added data to be overwritten.\n\n        \"\"\"\n\n        address = None\n        eof_found = False\n\n        for line in StringIO(lines):\n            # Abort if data is found after end of file.\n            if eof_found:\n                raise Error(\"bad file terminator\")\n\n            line = line.strip()\n\n            if len(line) < 1:\n                raise Error(\"bad line length\")\n\n            if line[0] == 'q':\n                eof_found = True\n            elif line[0] == '@':\n                try:\n                    address = int(line[1:], 16)\n                except ValueError:\n                    raise Error(\"bad section address\")\n            else:\n                # Try to decode the data.\n                try:\n                    data = bytearray(binascii.unhexlify(line.replace(' ', '')))\n                except (TypeError, binascii.Error):\n                    raise Error(\"bad data\")\n\n                size = len(data)\n\n                # Check that there are correct number of bytes per\n                # line. There should TI_TXT_BYTES_PER_LINE. Only\n                # exception is last line of section which may be\n                # shorter.\n                if size > TI_TXT_BYTES_PER_LINE:\n                    raise Error(\"bad line length\")\n\n                if address is None:\n                    raise Error(\"missing section address\")\n\n                self._segments.add(_Segment(address,\n                                            address + size,\n                                            data,\n                                            self.word_size_bytes),\n                                   overwrite)\n\n                if size == TI_TXT_BYTES_PER_LINE:\n                    address += size\n                else:\n                    address = None\n\n        if not eof_found:\n            raise Error(\"missing file terminator\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_binary(self, data, address=0, overwrite=False):\n\n        address *= self.word_size_bytes\n        self._segments.add(_Segment(address,\n                                    address + len(data),\n                                    bytearray(data),\n                                    self.word_size_bytes),\n                           overwrite)", "response": "Add given data at given address."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens given file and add its data to the set.", "response": "def add_file(self, filename, overwrite=False):\n        \"\"\"Open given file and add its data by guessing its format. The format\n        must be Motorola S-Records, Intel HEX or TI-TXT. Set `overwrite` to\n        ``True`` to allow already added data to be overwritten.\n\n        \"\"\"\n\n        with open(filename, 'r') as fin:\n            self.add(fin.read(), overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen given Motorola S - Records file and add its records.", "response": "def add_srec_file(self, filename, overwrite=False):\n        \"\"\"Open given Motorola S-Records file and add its records. Set\n        `overwrite` to ``True`` to allow already added data to be\n        overwritten.\n\n        \"\"\"\n\n        with open(filename, 'r') as fin:\n            self.add_srec(fin.read(), overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_ihex_file(self, filename, overwrite=False):\n\n        with open(filename, 'r') as fin:\n            self.add_ihex(fin.read(), overwrite)", "response": "Open given Intel HEX file and add its records. Set overwrite to True to allow already added data to be overwritten."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_ti_txt_file(self, filename, overwrite=False):\n\n        with open(filename, 'r') as fin:\n            self.add_ti_txt(fin.read(), overwrite)", "response": "Open given TI - TXT file and add its contents. Set overwrite to True to allow already added data to be overwritten."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_binary_file(self, filename, address=0, overwrite=False):\n\n        with open(filename, 'rb') as fin:\n            self.add_binary(fin.read(), address, overwrite)", "response": "Open given binary file and add its contents. Set overwrite to True to allow already added data to be overwritten."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the binary file as Motorola S - Records records and return them as a string.", "response": "def as_srec(self, number_of_data_bytes=32, address_length_bits=32):\n        \"\"\"Format the binary file as Motorola S-Records records and return\n        them as a string.\n\n        `number_of_data_bytes` is the number of data bytes in each\n        record.\n\n        `address_length_bits` is the number of address bits in each\n        record.\n\n        >>> print(binfile.as_srec())\n        S32500000100214601360121470136007EFE09D219012146017E17C20001FF5F16002148011973\n        S32500000120194E79234623965778239EDA3F01B2CA3F0156702B5E712B722B73214601342199\n        S5030002FA\n\n        \"\"\"\n\n        header = []\n\n        if self._header is not None:\n            record = pack_srec('0', 0, len(self._header), self._header)\n            header.append(record)\n\n        type_ = str((address_length_bits // 8) - 1)\n\n        if type_ not in '123':\n            raise Error(\"expected data record type 1..3, but got {}\".format(\n                type_))\n\n        data = [pack_srec(type_, address, len(data), data)\n                for address, data in self._segments.chunks(number_of_data_bytes)]\n        number_of_records = len(data)\n\n        if number_of_records <= 0xffff:\n            footer = [pack_srec('5', number_of_records, 0, None)]\n        elif number_of_records <= 0xffffff:\n            footer = [pack_srec('6', number_of_records, 0, None)]\n        else:\n            raise Error('too many records {}'.format(number_of_records))\n\n        # Add the execution start address.\n        if self.execution_start_address is not None:\n            if type_ == '1':\n                record = pack_srec('9', self.execution_start_address, 0, None)\n            elif type_ == '2':\n                record = pack_srec('8', self.execution_start_address, 0, None)\n            else:\n                record = pack_srec('7', self.execution_start_address, 0, None)\n\n            footer.append(record)\n\n        return '\\n'.join(header + data + footer) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the binary file as Intel HEX records and return them as a string.", "response": "def as_ihex(self, number_of_data_bytes=32, address_length_bits=32):\n        \"\"\"Format the binary file as Intel HEX records and return them as a\n        string.\n\n        `number_of_data_bytes` is the number of data bytes in each\n        record.\n\n        `address_length_bits` is the number of address bits in each\n        record.\n\n        >>> print(binfile.as_ihex())\n        :20010000214601360121470136007EFE09D219012146017E17C20001FF5F16002148011979\n        :20012000194E79234623965778239EDA3F01B2CA3F0156702B5E712B722B7321460134219F\n        :00000001FF\n\n        \"\"\"\n\n        def i32hex(address, extended_linear_address, data_address):\n            if address > 0xffffffff:\n                raise Error(\n                    'cannot address more than 4 GB in I32HEX files (32 '\n                    'bits addresses)')\n\n            address_upper_16_bits = (address >> 16)\n            address &= 0xffff\n\n            # All segments are sorted by address. Update the\n            # extended linear address when required.\n            if address_upper_16_bits > extended_linear_address:\n                extended_linear_address = address_upper_16_bits\n                packed = pack_ihex(IHEX_EXTENDED_LINEAR_ADDRESS,\n                                   0,\n                                   2,\n                                   binascii.unhexlify('{:04X}'.format(\n                                       extended_linear_address)))\n                data_address.append(packed)\n\n            return address, extended_linear_address\n\n        def i16hex(address, extended_segment_address, data_address):\n            if address > 16 * 0xffff + 0xffff:\n                raise Error(\n                    'cannot address more than 1 MB in I16HEX files (20 '\n                    'bits addresses)')\n\n            address_lower = (address - 16 * extended_segment_address)\n\n            # All segments are sorted by address. Update the\n            # extended segment address when required.\n            if address_lower > 0xffff:\n                extended_segment_address = (4096 * (address >> 16))\n\n                if extended_segment_address > 0xffff:\n                    extended_segment_address = 0xffff\n\n                address_lower = (address - 16 * extended_segment_address)\n                packed = pack_ihex(IHEX_EXTENDED_SEGMENT_ADDRESS,\n                                   0,\n                                   2,\n                                   binascii.unhexlify('{:04X}'.format(\n                                       extended_segment_address)))\n                data_address.append(packed)\n\n            return address_lower, extended_segment_address\n\n        def i8hex(address):\n            if address > 0xffff:\n                raise Error(\n                    'cannot address more than 64 kB in I8HEX files (16 '\n                    'bits addresses)')\n\n        data_address = []\n        extended_segment_address = 0\n        extended_linear_address = 0\n\n        for address, data in self._segments.chunks(number_of_data_bytes):\n            if address_length_bits == 32:\n                address, extended_linear_address = i32hex(address,\n                                                          extended_linear_address,\n                                                          data_address)\n            elif address_length_bits == 24:\n                address, extended_segment_address = i16hex(address,\n                                                           extended_segment_address,\n                                                           data_address)\n            elif address_length_bits == 16:\n                i8hex(address)\n            else:\n                raise Error(\n                    'expected address length 16, 24 or 32, but got {}'.format(\n                        address_length_bits))\n\n            data_address.append(pack_ihex(IHEX_DATA,\n                                          address,\n                                          len(data),\n                                          data))\n\n        footer = []\n\n        if self.execution_start_address is not None:\n            if address_length_bits == 24:\n                address = binascii.unhexlify(\n                    '{:08X}'.format(self.execution_start_address))\n                footer.append(pack_ihex(IHEX_START_SEGMENT_ADDRESS,\n                                        0,\n                                        4,\n                                        address))\n            elif address_length_bits == 32:\n                address = binascii.unhexlify(\n                    '{:08X}'.format(self.execution_start_address))\n                footer.append(pack_ihex(IHEX_START_LINEAR_ADDRESS,\n                                        0,\n                                        4,\n                                        address))\n\n        footer.append(pack_ihex(IHEX_END_OF_FILE, 0, 0, None))\n\n        return '\\n'.join(data_address + footer) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the binary file as a TI - TXT file and return it as a string.", "response": "def as_ti_txt(self):\n        \"\"\"Format the binary file as a TI-TXT file and return it as a string.\n\n        >>> print(binfile.as_ti_txt())\n        @0100\n        21 46 01 36 01 21 47 01 36 00 7E FE 09 D2 19 01\n        21 46 01 7E 17 C2 00 01 FF 5F 16 00 21 48 01 19\n        19 4E 79 23 46 23 96 57 78 23 9E DA 3F 01 B2 CA\n        3F 01 56 70 2B 5E 71 2B 72 2B 73 21 46 01 34 21\n        q\n\n        \"\"\"\n\n        lines = []\n\n        for segment in self._segments:\n            lines.append('@{:04X}'.format(segment.address))\n\n            for _, data in segment.chunks(TI_TXT_BYTES_PER_LINE):\n                lines.append(' '.join('{:02X}'.format(byte) for byte in data))\n\n        lines.append('q')\n\n        return '\\n'.join(lines) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_binary(self,\n                  minimum_address=None,\n                  maximum_address=None,\n                  padding=None):\n        \"\"\"Return a byte string of all data within given address range.\n\n        `minimum_address` is the absolute minimum address of the\n        resulting binary data.\n\n        `maximum_address` is the absolute maximum address of the\n        resulting binary data (non-inclusive).\n\n        `padding` is the word value of the padding between\n        non-adjacent segments. Give as a bytes object of length 1 when\n        the word size is 8 bits, length 2 when the word size is 16\n        bits, and so on.\n\n        >>> binfile.as_binary()\n        bytearray(b'!F\\\\x016\\\\x01!G\\\\x016\\\\x00~\\\\xfe\\\\t\\\\xd2\\\\x19\\\\x01!F\\\\x01~\\\\x17\\\\xc2\\\\x00\\\\x01\n        \\\\xff_\\\\x16\\\\x00!H\\\\x01\\\\x19\\\\x19Ny#F#\\\\x96Wx#\\\\x9e\\\\xda?\\\\x01\\\\xb2\\\\xca?\\\\x01Vp+^q+r+s!\n        F\\\\x014!')\n\n        \"\"\"\n\n        if len(self._segments) == 0:\n            return b''\n\n        if minimum_address is None:\n            current_maximum_address = self.minimum_address\n        else:\n            current_maximum_address = minimum_address\n\n        if maximum_address is None:\n            maximum_address = self.maximum_address\n\n        if current_maximum_address >= maximum_address:\n            return b''\n\n        if padding is None:\n            padding = b'\\xff' * self.word_size_bytes\n\n        binary = bytearray()\n\n        for address, data in self._segments:\n            length = len(data) // self.word_size_bytes\n\n            # Discard data below the minimum address.\n            if address < current_maximum_address:\n                if address + length <= current_maximum_address:\n                    continue\n\n                offset = (current_maximum_address - address) * self.word_size_bytes\n                data = data[offset:]\n                length = len(data) // self.word_size_bytes\n                address = current_maximum_address\n\n            # Discard data above the maximum address.\n            if address + length > maximum_address:\n                if address < maximum_address:\n                    size = (maximum_address - address) * self.word_size_bytes\n                    data = data[:size]\n                    length = len(data) // self.word_size_bytes\n                elif maximum_address >= current_maximum_address:\n                    binary += padding * (maximum_address - current_maximum_address)\n                    break\n\n            binary += padding * (address - current_maximum_address)\n            binary += data\n            current_maximum_address = address + length\n\n        return binary", "response": "Return a byte string of all data within given address range."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the binary file as a string values separated by given separator.", "response": "def as_array(self, minimum_address=None, padding=None, separator=', '):\n        \"\"\"Format the binary file as a string values separated by given\n        separator `separator`. This function can be used to generate\n        array initialization code for C and other languages.\n\n        `minimum_address` is the start address of the resulting binary\n        data.\n\n        `padding` is the value of the padding between not adjacent\n        segments.\n\n        >>> binfile.as_array()\n        '0x21, 0x46, 0x01, 0x36, 0x01, 0x21, 0x47, 0x01, 0x36, 0x00, 0x7e,\n         0xfe, 0x09, 0xd2, 0x19, 0x01, 0x21, 0x46, 0x01, 0x7e, 0x17, 0xc2,\n         0x00, 0x01, 0xff, 0x5f, 0x16, 0x00, 0x21, 0x48, 0x01, 0x19, 0x19,\n         0x4e, 0x79, 0x23, 0x46, 0x23, 0x96, 0x57, 0x78, 0x23, 0x9e, 0xda,\n         0x3f, 0x01, 0xb2, 0xca, 0x3f, 0x01, 0x56, 0x70, 0x2b, 0x5e, 0x71,\n         0x2b, 0x72, 0x2b, 0x73, 0x21, 0x46, 0x01, 0x34, 0x21'\n\n        \"\"\"\n\n        binary_data = self.as_binary(minimum_address,\n                                     padding=padding)\n        words = []\n\n        for offset in range(0, len(binary_data), self.word_size_bytes):\n            word = 0\n\n            for byte in binary_data[offset:offset + self.word_size_bytes]:\n                word <<= 8\n                word += byte\n\n            words.append('0x{:02x}'.format(word))\n\n        return separator.join(words)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the binary file as a hexdump and return it as a string.", "response": "def as_hexdump(self):\n        \"\"\"Format the binary file as a hexdump and return it as a string.\n\n        >>> print(binfile.as_hexdump())\n        00000100  21 46 01 36 01 21 47 01  36 00 7e fe 09 d2 19 01  |!F.6.!G.6.~.....|\n        00000110  21 46 01 7e 17 c2 00 01  ff 5f 16 00 21 48 01 19  |!F.~....._..!H..|\n        00000120  19 4e 79 23 46 23 96 57  78 23 9e da 3f 01 b2 ca  |.Ny#F#.Wx#..?...|\n        00000130  3f 01 56 70 2b 5e 71 2b  72 2b 73 21 46 01 34 21  |?.Vp+^q+r+s!F.4!|\n\n        \"\"\"\n\n        # Empty file?\n        if len(self) == 0:\n            return '\\n'\n\n        non_dot_characters = set(string.printable)\n        non_dot_characters -= set(string.whitespace)\n        non_dot_characters |= set(' ')\n\n        def align16(address):\n            return address - (address % 16)\n\n        def padding(length):\n            return [None] * length\n\n        def format_line(address, data):\n            \"\"\"`data` is a list of integers and None for unused elements.\n\n            \"\"\"\n\n            data += padding(16 - len(data))\n            hexdata = []\n\n            for byte in data:\n                if byte is not None:\n                    elem = '{:02x}'.format(byte)\n                else:\n                    elem = '  '\n\n                hexdata.append(elem)\n\n            first_half = ' '.join(hexdata[0:8])\n            second_half = ' '.join(hexdata[8:16])\n            text = ''\n\n            for byte in data:\n                if byte is None:\n                    text += ' '\n                elif chr(byte) in non_dot_characters:\n                    text += chr(byte)\n                else:\n                    text += '.'\n\n            return '{:08x}  {:23s}  {:23s}  |{:16s}|'.format(\n                address, first_half, second_half, text)\n\n        # Format one line at a time.\n        lines = []\n        line_address = align16(self.minimum_address)\n        line_data = []\n\n        for chunk in self._segments.chunks(size=16, alignment=16):\n            aligned_chunk_address = align16(chunk.address)\n\n            if aligned_chunk_address > line_address:\n                lines.append(format_line(line_address, line_data))\n\n                if aligned_chunk_address > line_address + 16:\n                    lines.append('...')\n\n                line_address = aligned_chunk_address\n                line_data = []\n\n            line_data += padding(chunk.address - line_address - len(line_data))\n            line_data += [byte for byte in chunk.data]\n\n        lines.append(format_line(line_address, line_data))\n\n        return '\\n'.join(lines) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfill all empty space between segments with given value.", "response": "def fill(self, value=b'\\xff'):\n        \"\"\"Fill all empty space between segments with given value `value`.\n\n        \"\"\"\n\n        previous_segment_maximum_address = None\n        fill_segments = []\n\n        for address, data in self._segments:\n            maximum_address = address + len(data)\n\n            if previous_segment_maximum_address is not None:\n                fill_size = address - previous_segment_maximum_address\n                fill_size_words = fill_size // self.word_size_bytes\n                fill_segments.append(_Segment(\n                    previous_segment_maximum_address,\n                    previous_segment_maximum_address + fill_size,\n                    value * fill_size_words,\n                    self.word_size_bytes))\n\n            previous_segment_maximum_address = maximum_address\n\n        for segment in fill_segments:\n            self._segments.add(segment)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exclude(self, minimum_address, maximum_address):\n\n        if maximum_address < minimum_address:\n            raise Error('bad address range')\n\n        minimum_address *= self.word_size_bytes\n        maximum_address *= self.word_size_bytes\n        self._segments.remove(minimum_address, maximum_address)", "response": "Exclude given range and keep the rest."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps given range and discard the rest.", "response": "def crop(self, minimum_address, maximum_address):\n        \"\"\"Keep given range and discard the rest.\n\n        `minimum_address` is the first word address to keep\n        (including).\n\n        `maximum_address` is the last word address to keep\n        (excluding).\n\n        \"\"\"\n\n        minimum_address *= self.word_size_bytes\n        maximum_address *= self.word_size_bytes\n        maximum_address_address = self._segments.maximum_address\n        self._segments.remove(0, minimum_address)\n        self._segments.remove(maximum_address, maximum_address_address)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self):\n\n        info = ''\n\n        if self._header is not None:\n            if self._header_encoding is None:\n                header = ''\n\n                for b in bytearray(self.header):\n                    if chr(b) in string.printable:\n                        header += chr(b)\n                    else:\n                        header += '\\\\x{:02x}'.format(b)\n            else:\n                header = self.header\n\n            info += 'Header:                  \"{}\"\\n'.format(header)\n\n        if self.execution_start_address is not None:\n            info += 'Execution start address: 0x{:08x}\\n'.format(\n                self.execution_start_address)\n\n        info += 'Data ranges:\\n\\n'\n\n        for address, data in self._segments:\n            minimum_address = address\n            size = len(data)\n            maximum_address = (minimum_address + size // self.word_size_bytes)\n            info += 4 * ' '\n            info += '0x{:08x} - 0x{:08x} ({})\\n'.format(\n                minimum_address,\n                maximum_address,\n                format_size(size, binary=True))\n\n        return info", "response": "Return a string of human readable information about the binary\n            file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects metric info in a single preorder traversal.", "response": "def _precompute(self, tree):\n        \"\"\"\n        Collect metric info in a single preorder traversal.\n        \"\"\"\n        d = {}\n        for n in tree.preorder_internal_node_iter():\n            d[n] = namedtuple('NodeDist', ['dist_from_root', 'edges_from_root'])\n            if n.parent_node:\n                d[n].dist_from_root = d[n.parent_node].dist_from_root + n.edge_length\n                d[n].edges_from_root = d[n.parent_node].edges_from_root + 1\n            else:\n                d[n].dist_from_root = 0.0\n                d[n].edges_from_root = 0\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_vectors(self, tree, precomputed_info):\n        little_m = []\n        big_m = []\n\n        leaf_nodes = sorted(tree.leaf_nodes(), key=lambda x: x.taxon.label)\n        # inner nodes, sorted order\n        for leaf_a, leaf_b in combinations(leaf_nodes, 2):\n            mrca = tree.mrca(taxa=[leaf_a.taxon, leaf_b.taxon])\n            little_m.append(precomputed_info[mrca].edges_from_root)\n            big_m.append(precomputed_info[mrca].dist_from_root)\n        \n        # leaf nodes, sorted order\n        for leaf in leaf_nodes:\n            little_m.append(1)\n            big_m.append(leaf.edge_length)\n\n        return np.array(little_m), np.array(big_m)", "response": "Populate the vectors m and M."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Euclidean distance between vectors v of two trees.", "response": "def get_distance(self, other, lbda=0.5, min_overlap=4):\n        \"\"\"\n        Return the Euclidean distance between vectors v of\n        two trees. Must have the same leaf set (too lazy to check).\n        \"\"\"\n        if self.tree ^ other.tree:\n            if len(self.tree & other.tree) < min_overlap:\n                return 0\n                # raise AttributeError('Can\\'t calculate tree distances when tree overlap is less than two leaves')\n            else:\n                t1, t2 = self._equalise_leaf_sets(other, False)\n                tmp_self = KendallColijn(t1)\n                tmp_other = KendallColijn(t2)\n\n                return np.sqrt(((tmp_self.get_vector(lbda) - tmp_other.get_vector(lbda)) ** 2).sum())\n        else:\n            return np.sqrt(((self.get_vector(lbda) - other.get_vector(lbda)) ** 2).sum())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ambiguate(sequence1, sequence2, delete_ambiguous=False):\n    delete = False\n    combination = list()\n    z = list(zip(sequence1, sequence2))\n    for (a, b) in z:\n        if a == b:\n            combination.append(a)\n        else:\n            if a == '-' or b == '-':\n                combination.append('-')\n            else:\n                if delete_ambiguous:\n                    delete = True\n                ambig = get_ambiguity(a, b)\n                combination.append(ambig)\n    if delete:\n        return 'X' * len(combination)\n    return ''.join(combination)", "response": "Returns the ambiguated sequence between two sequences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove empty sequences from the record.", "response": "def remove_empty(rec):\n    \"\"\" Deletes sequences that were marked for deletion by convert_to_IUPAC \"\"\"\n    for header, sequence in rec.mapping.items():\n        if all(char == 'X' for char in sequence):\n            rec.headers.remove(header)\n            rec.sequences.remove(sequence)\n    rec.update()\n    return rec"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef slugify(mapping, bind, values):\n    for value in values:\n        if isinstance(value, six.string_types):\n            value = transliterate(value)\n            value = normality.slugify(value)\n        yield value", "response": "Transform all values into URL - capable slugs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransliterate a given string into the latin alphabet.", "response": "def latinize(mapping, bind, values):\n    \"\"\" Transliterate a given string into the latin alphabet. \"\"\"\n    for v in values:\n        if isinstance(v, six.string_types):\n            v = transliterate(v)\n        yield v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join(mapping, bind, values):\n    return [' '.join([six.text_type(v) for v in values if v is not None])]", "response": "Merge all the strings. Put space between them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying functions like upper lower and strip.", "response": "def str_func(name):\n    \"\"\" Apply functions like upper(), lower() and strip(). \"\"\"\n    def func(mapping, bind, values):\n        for v in values:\n            if isinstance(v, six.string_types):\n                v = getattr(v, name)()\n            yield v\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a sha1 hash for each of the given values.", "response": "def hash(mapping, bind, values):\n    \"\"\" Generate a sha1 for each of the given values. \"\"\"\n    for v in values:\n        if v is None:\n            continue\n        if not isinstance(v, six.string_types):\n            v = six.text_type(v)\n        yield sha1(v.encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming several types of string cleaning for titles etc..", "response": "def clean(mapping, bind, values):\n    \"\"\" Perform several types of string cleaning for titles etc.. \"\"\"\n    categories = {'C': ' '}\n    for value in values:\n        if isinstance(value, six.string_types):\n            value = normality.normalize(value, lowercase=False, collapse=True,\n                                        decompose=False,\n                                        replace_categories=categories)\n        yield value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isconnected(mask):\n\n    nodes_to_check = list((np.where(mask[0, :])[0])[1:])\n    seen = [True] + [False] * (len(mask) - 1)\n    while nodes_to_check and not all(seen):\n        node = nodes_to_check.pop()\n        reachable = np.where(mask[node, :])[0]\n        for i in reachable:\n            if not seen[i]:\n                nodes_to_check.append(i)\n                seen[i] = True\n    return all(seen)", "response": "Checks that all nodes are reachable from the first node - i. e. that all nodes are fully connected."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the affinity matrix of a single resource class.", "response": "def affinity(\n        matrix,\n        mask=None,\n        scale=None,\n):\n    \"\"\"\n    Mask is a 2d boolean matrix. Scale is a 2d local scale matrix,\n    as output by kscale(). It's the outer product of the kdists column\n    vector produced by kdists.\n    \"\"\"\n\n    mask = (mask if mask is not None else np.ones(matrix.shape, dtype=bool))\n    assert isconnected(mask)\n    scale = (scale if scale is not None else np.ones(matrix.shape))\n\n    ix = np.where(np.logical_not(mask))\n    scaled_matrix = -matrix ** 2 / scale\n    # inputs where distance = 0 and scale = 0 result in NaN:\n    # the next line replaces NaNs with -1.0\n    scaled_matrix[np.where(np.isnan(scaled_matrix))] = -1.0\n    affinity_matrix = np.exp(scaled_matrix)\n    affinity_matrix[ix] = 0.  # mask\n    affinity_matrix.flat[::len(affinity_matrix) + 1] = 0.  # diagonal\n    return affinity_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef double_centre(matrix, square_input=True):\n\n    m = matrix.copy()\n    if square_input:\n        m **= 2\n\n    (rows, cols) = m.shape\n    cm = np.mean(m, axis=0)  # column means\n    rm = np.mean(m, axis=1).reshape((rows, 1))  # row means\n    gm = np.mean(cm)  # grand mean\n    m -= rm + cm - gm\n    m /= -2\n    return m", "response": "Double - centres the input matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _estimate_additive_constant(matrix):\n    topleft = np.zeros(matrix.shape)\n    topright = 2*double_centre(matrix)\n    bottomleft = -np.eye(matrix.shape[0])\n    bottomright = -4*double_centre(matrix, square_input=False)\n    Z = np.vstack([np.hstack([topleft,topright]),\n                   np.hstack([bottomleft,bottomright])])\n    return max(np.real(np.linalg.eigvals(Z)))", "response": "Estimate the additive constant of a 2x2 block matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_pd(matrix):\n    try:\n        np.linalg.cholesky(matrix)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "response": "Check if a symmetric matrix is PD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_psd(matrix, tolerance=1e-6):\n\n    hermitian = (matrix + matrix.T.conjugate()) / 2\n    eigenvalues = np.linalg.eigh(hermitian)[0]\n    return (eigenvalues > -tolerance).all()", "response": "Check if a square matrix is PSD"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscale all rows to length 1. Fails when row is 0 - length so that it leaves these unchanged", "response": "def normalise_rows(matrix):\n    \"\"\" Scales all rows to length 1. Fails when row is 0-length, so it\n    leaves these unchanged \"\"\"\n\n    lengths = np.apply_along_axis(np.linalg.norm, 1, matrix)\n    if not (lengths > 0).all():\n        # raise ValueError('Cannot normalise 0 length vector to length 1')\n        # print(matrix)\n        lengths[lengths == 0] = 1\n    return matrix / lengths[:, np.newaxis]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the k - th nearest distances row - wise as a column vector", "response": "def kdists(matrix, k=7, ix=None):\n    \"\"\" Returns the k-th nearest distances, row-wise, as a column vector \"\"\"\n\n    ix = ix or kindex(matrix, k)\n    return matrix[ix][np.newaxis].T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn indices to select the kth nearest neighbour", "response": "def kindex(matrix, k):\n    \"\"\" Returns indices to select the kth nearest neighbour\"\"\"\n\n    ix = (np.arange(len(matrix)), matrix.argsort(axis=0)[k])\n    return ix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kmask(matrix, k=7, dists=None, logic='or'):\n\n    dists = (kdists(matrix, k=k) if dists is None else dists)\n    mask = (matrix <= dists)\n    if logic == 'or' or logic == '|':\n        return mask | mask.T\n    elif logic == 'and' or logic == '&':\n        return mask & mask.T\n    return mask", "response": "Creates a boolean mask to include points within k nearest\n    neighbours and exclude the rest."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kscale(matrix, k=7, dists=None):\n    dists = (kdists(matrix, k=k) if dists is None else dists)\n    scale = dists.dot(dists.T)\n    return scale", "response": "Returns the local scale based on the k - th nearest neighbour"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef laplace(affinity_matrix, shi_malik_type=False):\n\n    diagonal = affinity_matrix.sum(axis=1) - affinity_matrix.diagonal()\n    zeros = diagonal <= 1e-10\n    diagonal[zeros] = 1\n    if (diagonal <= 1e-10).any():  # arbitrarily small value\n        raise ZeroDivisionError\n    if shi_malik_type:\n        inv_d = np.diag(1 / diagonal)\n        return inv_d.dot(affinity_matrix)\n    diagonal = np.sqrt(diagonal)\n    return affinity_matrix / diagonal / diagonal[:, np.newaxis]", "response": "Converts an affinity matrix into a Laplacian graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shift_and_scale(matrix, shift, scale):\n\n    zeroed = matrix - matrix.min()\n    scaled = (scale - shift) * (zeroed / zeroed.max())\n    return scaled + shift", "response": "Shift and scale a single node by shift and scale it by scale."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eigen(matrix):\n\n    (vals, vecs) = np.linalg.eigh(matrix)\n    ind = vals.argsort()[::-1]\n    vals = vals[ind]\n    vecs = vecs[:, ind]\n    vals_ = vals.copy()\n    vals_[vals_ < 0] = 0.\n    cum_var_exp = np.cumsum(vals_ / vals_.sum())\n    return Decomp(matrix.copy(), vals, vecs, cum_var_exp)", "response": "Calculates the eigenvalues and eigenvectors of the input matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _embedding_spectral(matrix, dimensions=3, unit_length=True,\n                        affinity_matrix=None, sigma=1):\n    \"\"\"\n    Private method to calculate Spectral embedding\n    :param dimensions: (int)\n    :return: coordinate matrix (np.array)\n    \"\"\"\n    if affinity_matrix is None:\n        aff = rbf(matrix, sigma=sigma)\n    else:\n        aff = affinity_matrix\n    coords = sklearn.manifold.spectral_embedding(aff, dimensions)\n    return normalise_rows(coords) if unit_length else coords", "response": "Private method to calculate Spectral embedding"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _embedding_tsne(matrix, dimensions=3, early_exaggeration=12.0,\n                    method='barnes_hut', perplexity=30, learning_rate=200,\n                    n_iter=1000):\n    \"\"\"\n    Private method to perform tSNE embedding\n    :param matrix: treeCl Distance Matrix\n    :param dimensions: Number of dimensions in which to embed points\n    :return: treeCl CoordinateMatrix\n    \"\"\"\n    tsne = sklearn.manifold.TSNE(n_components=dimensions,\n                                 metric=\"precomputed\",\n                                 early_exaggeration=early_exaggeration,\n                                 method=method,\n                                 perplexity=perplexity,\n                                 learning_rate=learning_rate,\n                                 n_iter=1000)\n    return tsne.fit_transform(matrix)", "response": "Private method to perform tSNE embedding on a treeCl CoordinateMatrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _embedding_metric_mds(matrix, dimensions=3):\n    mds = sklearn.manifold.MDS(n_components=dimensions,\n                               dissimilarity='precomputed',\n                               metric=True)\n    mds.fit(matrix)\n    return mds.embedding_", "response": "Private method to calculate MMDS embedding for a given matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn fitted coordinates in as many dimensions as needed to explain a given amount of variance", "response": "def coords_by_cutoff(self, cutoff=0.80):\n        \"\"\" Returns fitted coordinates in as many dimensions as are needed to\n        explain a given amount of variance (specified in the cutoff) \"\"\"\n\n        i = np.where(self.cve >= cutoff)[0][0]\n        coords_matrix = self.vecs[:, :i + 1]\n        return coords_matrix, self.cve[i]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coords_by_dimension(self, dimensions=3):\n\n        coords_matrix = self.vecs[:, :dimensions]\n        varexp = self.cve[dimensions - 1]\n        return coords_matrix, varexp", "response": "Returns fitted coordinates in specified number of dimensions and the varexp explained"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nembed the distance matrix in a coordinate space.", "response": "def embedding(self, dimensions, method, **kwargs):\n        \"\"\"\n        Embeds the distance matrix in a coordinate space. Implemented methods are:\n            cmds: Classical MultiDimensional Scaling\n            kpca: Kernel Principal Components Analysis\n            mmds: Metric MultiDimensional Scaling\n            nmmds: Non-Metric MultiDimensional Scaling\n            spectral: Spectral decomposition of Laplacian matrix\n            tsne: t-distributed Stochastic Neighbour Embedding\n\n        Valid kwargs:\n            kpca: affinity_matrix - a precomputed array of affinities\n                  sigma - the value of sigma to use when computing the affinity matrix via\n                          the Radial Basis Function\n            nmmds: initial_coords - a set of coordinates to refine. NMMDS works very badly\n                                    without this\n            spectral: affinity_matrix, sigma\n                      unit_length - scale the coordinates to unit length, so points sit\n                                    on the surface of the unit sphere\n        :param dimensions: (int) number of coordinate axes to use\n        :param method: (string) one of cmds, kpca, mmds, nmmds, spectral\n        :param kwargs: unit_length (bool), affinity_matrix (np.array), sigma (float), initial_coords (np.array)\n        :return: coordinate matrix (np.array)\n        \"\"\"\n        errors.optioncheck(method, ['cmds', 'kpca', 'mmds', 'nmmds', 'spectral', 'tsne'])\n        if method == 'cmds':\n            array =  _embedding_classical_mds(self.to_array(), dimensions, **kwargs)\n        elif method == 'kpca':\n            array = _embedding_kernel_pca(self.to_array(), dimensions, **kwargs)\n        elif method == 'mmds':\n            array = _embedding_metric_mds(self.to_array(), dimensions)\n        elif method == 'nmmds':\n            array = _embedding_nonmetric_mds(self.to_array(), dimensions, **kwargs)\n        elif method == 'spectral':\n            array = _embedding_spectral(self.to_array(), dimensions, **kwargs)\n        elif method == 'tsne':\n            array = _embedding_tsne(self.to_array(), dimensions, **kwargs)\n\n        return CoordinateMatrix(array, names=self.df.index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a mapping and JSON schema spec extract a value from data and apply certain transformations to normalize the value.", "response": "def extract_value(mapping, bind, data):\n    \"\"\" Given a mapping and JSON schema spec, extract a value from ``data``\n    and apply certain transformations to normalize the value. \"\"\"\n    columns = mapping.get('columns', [mapping.get('column')])\n    values = [data.get(c) for c in columns]\n\n    for transform in mapping.get('transforms', []):\n        # any added transforms must also be added to the schema.\n        values = list(TRANSFORMS[transform](mapping, bind, values))\n\n    format_str = mapping.get('format')\n    value = values[0] if len(values) else None\n    if not is_empty(format_str):\n        value = format_str % tuple('' if v is None else v for v in values)\n\n    empty = is_empty(value)\n    if empty:\n        value = mapping.get('default') or bind.schema.get('default')\n    return empty, convert_value(bind, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects the ideal type for the data.", "response": "def get_type(bind):\n    \"\"\" Detect the ideal type for the data, either using the explicit type\n    definition or the format (for date, date-time, not supported by JSON). \"\"\"\n    types = bind.types + [bind.schema.get('format')]\n    for type_name in ('date-time', 'date', 'decimal', 'integer', 'boolean',\n                      'number', 'string'):\n        if type_name in types:\n            return type_name\n    return 'string'"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_value(bind, value):\n    type_name = get_type(bind)\n    try:\n        return typecast.cast(type_name, value)\n    except typecast.ConverterError:\n        return value", "response": "Convert value to the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef peakdetect(y_axis, x_axis=None, lookahead=300, delta=0):\n    max_peaks = []\n    min_peaks = []\n    dump = []   # Used to pop the first hit which almost always is false\n    \n    # check input data\n    x_axis, y_axis = _datacheck_peakdetect(x_axis, y_axis)\n    # store data length for later use\n    length = len(y_axis)\n    \n    \n    #perform some checks\n    if lookahead < 1:\n        raise ValueError, \"Lookahead must be '1' or above in value\"\n    #NOTE: commented this to use the function with log(histogram)\n    #if not (np.isscalar(delta) and delta >= 0):\n    if not (np.isscalar(delta)):\n        raise ValueError, \"delta must be a positive number\"\n    \n    #maxima and minima candidates are temporarily stored in\n    #mx and mn respectively\n    mn, mx = np.Inf, -np.Inf\n    \n    #Only detect peak if there is 'lookahead' amount of points after it\n    for index, (x, y) in enumerate(zip(x_axis[:-lookahead], \n                                        y_axis[:-lookahead])):\n        if y > mx:\n            mx = y\n            mxpos = x\n        if y < mn:\n            mn = y\n            mnpos = x\n        \n        ####look for max####\n        if y < mx-delta and mx != np.Inf:\n            #Maxima peak candidate found\n            #look ahead in signal to ensure that this is a peak and not jitter\n            if y_axis[index:index+lookahead].max() < mx:\n                max_peaks.append([mxpos, mx])\n                dump.append(True)\n                #set algorithm to only find minima now\n                mx = np.Inf\n                mn = np.Inf\n                if index+lookahead >= length:\n                    #end is within lookahead no more peaks can be found\n                    break\n                continue\n            #else:  #slows shit down this does\n            #    mx = ahead\n            #    mxpos = x_axis[np.where(y_axis[index:index+lookahead]==mx)]\n        \n        ####look for min####\n        if y > mn+delta and mn != -np.Inf:\n            #Minima peak candidate found \n            #look ahead in signal to ensure that this is a peak and not jitter\n            if y_axis[index:index+lookahead].min() > mn:\n                min_peaks.append([mnpos, mn])\n                dump.append(False)\n                #set algorithm to only find maxima now\n                mn = -np.Inf\n                mx = -np.Inf\n                if index+lookahead >= length:\n                    #end is within lookahead no more peaks can be found\n                    break\n            #else:  #slows shit down this does\n            #    mn = ahead\n            #    mnpos = x_axis[np.where(y_axis[index:index+lookahead]==mn)]\n\n    #Remove the false hit on the first value of the y_axis\n    try:\n        if dump[0]:\n            max_peaks.pop(0)\n        else:\n            min_peaks.pop(0)\n        del dump\n    except IndexError:\n        #no peaks were found, should the function return empty lists?\n        pass\n        \n    return [max_peaks, min_peaks]", "response": "This function returns a list of lists where each element in the list contains the maximum and minimum peaks that are found in the signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef peaks(x, y, lookahead=20, delta=0.00003):\n    _max, _min = peakdetect(y, x, lookahead, delta)\n    x_peaks = [p[0] for p in _max]\n    y_peaks = [p[1] for p in _max]\n    x_valleys = [p[0] for p in _min]\n    y_valleys = [p[1] for p in _min]\n    \n    _peaks = [x_peaks, y_peaks]\n    _valleys = [x_valleys, y_valleys]\n    return {\"peaks\": _peaks, \"valleys\": _valleys}", "response": "A wrapper around peakdetect to pack the return values in a nicer format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef entropies(partition_1, partition_2):\n\n    if partition_1.num_elements() != partition_2.num_elements():\n        print('Partition lists are not the same length')\n        return 0\n    else:\n        total = partition_1.num_elements()\n\n    m1 = partition_1.get_membership()\n    m2 = partition_2.get_membership()\n    l1 = len(m1)\n    l2 = len(m2)\n    entropy_1 = 0\n    entropy_2 = 0\n    mut_inf = 0\n    for i in range(l1):\n        prob1 = len(m1[i]) / total  # float division ensured by __future__ import\n        entropy_1 -= prob1 * np.log2(prob1)\n        for j in range(l2):\n            if i == 0:  # only calculate these once\n                prob2 = len(m2[j]) / total\n                entropy_2 -= prob2 * np.log2(prob2)\n            intersect = len(set(m1[i]) & set(m2[j]))\n            if intersect == 0:\n                continue  # because 0 * log(0) = 0 (lim x->0: xlog(x)->0)\n            else:\n                mut_inf += intersect / total * np.log2(total * intersect\n                                                       / (len(m1[i]) * len(m2[j])))\n\n    return entropy_1, entropy_2, mut_inf", "response": "returns the number of entropies of a cluster in a dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random(cls, alpha, size):\n        props = np.concatenate([[0], (scipy.stats.dirichlet.rvs(alpha) * size).cumsum().round().astype(int)])\n        indices = np.array(list(range(size)))\n        random.shuffle(indices)\n        x = []\n        for i in range(len(props)-1):\n            ix = indices[props[i]:props[i+1]]\n            x.append(ix)\n        return cls.from_membership(x)", "response": "Generate a random entry from expected proportions alpha and size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the alternative representation of group membership", "response": "def get_membership(self):\n        \"\"\"\n        Alternative representation of group membership -\n        creates a list with one tuple per group; each tuple contains\n        the indices of its members\n\n        Example:\n        partition  = (0,0,0,1,0,1,2,2)\n        membership = [(0,1,2,4), (3,5), (6,7)]\n\n        :return: list of tuples giving group memberships by index\n        \"\"\"\n        result = defaultdict(list)\n        for (position, value) in enumerate(self.partition_vector):\n            result[value].append(position)\n        return sorted([tuple(x) for x in result.values()])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the variation of Information Metric between two clusterings .", "response": "def variation_of_information(self, other):\n        \"\"\" calculates Variation of Information Metric between two clusterings\n        of the same data - SEE Meila, M. (2007). Comparing clusterings: an\n        information based distance. Journal of Multivariate Analysis, 98(5),\n        873-895. doi:10.1016/j.jmva.2006.11.013\"\"\"\n        (entropy_1, entropy_2, mut_inf) = entropies(self, other)\n\n        return entropy_1 + entropy_2 - 2 * mut_inf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing the given data such that the area under the histogram is 1. Also applies smoothing once done.", "response": "def normalize(self):\n        \"\"\"\n        Normalizes the given data such that the area under the histogram/curve\n        comes to 1. Also re applies smoothing once done.\n        \"\"\"\n        median_diff = np.median(np.diff(self.x))\n        bin_edges = [self.x[0] - median_diff/2.0]\n        bin_edges.extend(median_diff/2.0 + self.x)\n        self.y_raw = self.y_raw/(self.y_raw.sum()*np.diff(bin_edges))\n        self.smooth()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the raw histogram data to the given path using pickle python module.", "response": "def serialize(self, path):\n        \"\"\"\n        Saves the raw (read unsmoothed) histogram data to the given path using\n        pickle python module.\n        \"\"\"\n        pickle.dump([self.x, self.y_raw], file(path, 'w'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(self, intervals=None, new_fig=True):\n\n        import pylab as p\n\n        if new_fig:\n            p.figure()\n\n        #step 1: plot histogram\n        p.plot(self.x, self.y, ls='-', c='b', lw='1.5')\n\n        #step 2: plot peaks\n        first_peak = None\n        last_peak = None\n        if self.peaks:\n            first_peak = min(self.peaks[\"peaks\"][0])\n            last_peak = max(self.peaks[\"peaks\"][0])\n            p.plot(self.peaks[\"peaks\"][0], self.peaks[\"peaks\"][1], 'rD', ms=10)\n            p.plot(self.peaks[\"valleys\"][0], self.peaks[\"valleys\"][1], 'yD', ms=5)\n\n        #Intervals\n        if intervals is not None:\n            #spacing = 0.02*max(self.y)\n            for interval in intervals:\n                if first_peak is not None:\n                    if interval <= first_peak or interval >= last_peak:\n                        continue\n                p.axvline(x=interval, ls='-.', c='g', lw='1.5')\n                if interval-1200 >= min(self.x):\n                    p.axvline(x=interval-1200, ls=':', c='b', lw='0.5')\n                if interval+1200 <= max(self.x):\n                    p.axvline(x=interval+1200, ls=':', c='b', lw='0.5')\n                if interval+2400 <= max(self.x):\n                    p.axvline(x=interval+2400, ls='-.', c='r', lw='0.5')\n                #spacing *= -1\n\n        #p.title(\"Tonic-aligned complete-range pitch histogram\")\n        #p.xlabel(\"Pitch value (Cents)\")\n        #p.ylabel(\"Normalized frequency of occurence\")\n        p.show()", "response": "This function plots the smoothed version and peak information for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parallel_map(client, task, args, message, batchsize=1, background=False, nargs=None):\n    show_progress = bool(message)\n    njobs = get_njobs(nargs, args)\n    nproc = len(client)\n    logger.debug('parallel_map: len(client) = {}'.format(len(client)))\n    view = client.load_balanced_view()\n    if show_progress:\n        message += ' (IP:{}w:{}b)'.format(nproc, batchsize)\n        pbar = setup_progressbar(message, njobs, simple_progress=True)\n        if not background:\n            pbar.start()\n    map_result = view.map(task, *list(zip(*args)), chunksize=batchsize)\n    if background:\n        return map_result, client\n    while not map_result.ready():\n        map_result.wait(1)\n        if show_progress:\n            pbar.update(min(njobs, map_result.progress * batchsize))\n    if show_progress:\n        pbar.finish()\n    return map_result", "response": "Helper function to map a function over a sequence of inputs in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequential_map(task, args, message, nargs=None):\n    njobs = get_njobs(nargs, args)\n    show_progress = bool(message)\n    if show_progress:\n        pbar = setup_progressbar(message, njobs, simple_progress=True)\n        pbar.start()\n    map_result = []\n    for (i, arglist) in enumerate(tupleise(args), start=1):\n        map_result.append(task(*arglist))\n        if show_progress:\n            pbar.update(i)\n    if show_progress:\n        pbar.finish()\n    return map_result", "response": "Helper function to map a function over a sequence of inputs sequentially with progress meter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef threadpool_map(task, args, message, concurrency, batchsize=1, nargs=None):\n    import concurrent.futures\n\n\n    njobs = get_njobs(nargs, args)\n    show_progress = bool(message)\n    batches = grouper(batchsize, tupleise(args))\n    batched_task = lambda batch: [task(*job) for job in batch]\n    if show_progress:\n        message += ' (TP:{}w:{}b)'.format(concurrency, batchsize)\n        pbar = setup_progressbar(message, njobs, simple_progress=True)\n        pbar.start()\n    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n        futures = []\n        completed_count = 0\n        for batch in batches:\n            futures.append(executor.submit(batched_task, batch))\n\n        if show_progress:\n            for i, fut in enumerate(concurrent.futures.as_completed(futures), start=1):\n                completed_count += len(fut.result())\n                pbar.update(completed_count)\n\n        else:\n            concurrent.futures.wait(futures)\n\n    if show_progress:\n        pbar.finish()\n\n    return flatten_list([fut.result() for fut in futures])", "response": "Helper to map a function over a range of inputs using a threadpool"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a list of tasks in a pool.", "response": "def processpool_map(task, args, message, concurrency, batchsize=1, nargs=None):\n    \"\"\"\n    See http://stackoverflow.com/a/16071616\n    \"\"\"\n    njobs = get_njobs(nargs, args)\n    show_progress = bool(message)\n    batches = grouper(batchsize, tupleise(args))\n    def batched_task(*batch):\n        return [task(*job) for job in batch]\n\n    if show_progress:\n        message += ' (PP:{}w:{}b)'.format(concurrency, batchsize)\n        pbar = setup_progressbar(message, njobs, simple_progress=True)\n        pbar.start()\n    \n    q_in   = multiprocessing.Queue()  # Should I limit either queue size? Limiting in-queue\n    q_out  = multiprocessing.Queue()  # increases time taken to send jobs, makes pbar less useful\n\n    proc = [multiprocessing.Process(target=fun, args=(batched_task, q_in, q_out)) for _ in range(concurrency)]\n    for p in proc:\n        p.daemon = True\n        p.start()\n    sent = [q_in.put((i, x)) for (i, x) in enumerate(batches)]\n    [q_in.put((None, None)) for _ in range(concurrency)]\n    res = []\n    completed_count = 0\n    for _ in range(len(sent)):\n        result = get_from_queue(q_out)\n        res.append(result)\n        completed_count += len(result[1])\n        if show_progress:\n            pbar.update(completed_count)\n\n    [p.join() for p in proc]\n    if show_progress:\n        pbar.finish()\n\n    return flatten_list([x for (i, x) in sorted(res)])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef concatenate(alignments):\n\n    # Get the full set of labels (i.e. sequence ids) for all the alignments\n    all_labels = set(seq.id for aln in alignments for seq in aln)\n\n    # Make a dictionary to store info as we go along\n    # (defaultdict is convenient -- asking for a missing key gives back an empty list)\n    tmp = defaultdict(list)\n\n    # Assume all alignments have same alphabet\n    alphabet = alignments[0]._alphabet\n\n    for aln in alignments:\n        length = aln.get_alignment_length()\n\n        # check if any labels are missing in the current alignment\n        these_labels = set(rec.id for rec in aln)\n        missing = all_labels - these_labels\n\n        # if any are missing, create unknown data of the right length,\n        # stuff the string representation into the tmp dict\n        for label in missing:\n            new_seq = UnknownSeq(length, alphabet=alphabet)\n            tmp[label].append(str(new_seq))\n\n        # else stuff the string representation into the tmp dict\n        for rec in aln:\n            tmp[rec.id].append(str(rec.seq))\n\n    # Stitch all the substrings together using join (most efficient way),\n    # and build the Biopython data structures Seq, SeqRecord and MultipleSeqAlignment\n    msa = MultipleSeqAlignment(SeqRecord(Seq(''.join(v), alphabet=alphabet), id=k, name=k, description=k)\n               for (k,v) in tmp.items())\n    return msa", "response": "Concatenates a list of Bio. Align. MultipleSeqAlignment objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup a sequence of strings into n - length groups.", "response": "def grouper(n, iterable):\n    \"\"\"\n    >>> list(grouper(3, 'ABCDEFG'))\n    [['A', 'B', 'C'], ['D', 'E', 'F'], ['G']]\n    \"\"\"\n    iterable = iter(iterable)\n    return iter(lambda: list(itertools.islice(iterable, n)), [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a partials dictionary from a treeCl. Alignment object", "response": "def alignment_to_partials(alignment, missing_data=None):\n    \"\"\" Generate a partials dictionary from a treeCl.Alignment \"\"\"\n    partials_dict = {}\n    for (name, sequence) in alignment.get_sequences():\n        datatype = 'dna' if alignment.is_dna() else 'protein'\n        partials_dict[name] = seq_to_partials(sequence, datatype)\n\n    if missing_data is not None:\n        l = len(alignment)\n        for name in missing_data:\n            if name not in partials_dict:\n                partials_dict[name] = seq_to_partials('-'*l, datatype)\n    return partials_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef biopython_to_partials(alignment, datatype):\n    partials_dict = {}\n    for seq in alignment:\n        partials_dict[seq.name] = seq_to_partials(seq, datatype)\n    return partials_dict", "response": "Generate a dictionary of partials from a treeCl. Alignment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a GammaMixture model for calculating a likelihood on a tree.", "response": "def create_gamma_model(alignment, missing_data=None, ncat=4):\n    \"\"\" Create a phylo_utils.likelihood.GammaMixture for calculating\n    likelihood on a tree, from a treeCl.Alignment and its matching \n    treeCl.Parameters \"\"\"\n    model = alignment.parameters.partitions.model\n    freqs = alignment.parameters.partitions.frequencies\n    alpha = alignment.parameters.partitions.alpha\n    if model == 'LG':\n        subs_model = LG(freqs)\n    elif model == 'WAG':\n        subs_model = WAG(freqs)\n    elif model == 'GTR':\n        rates = alignment.parameters.partitions.rates\n        subs_model = GTR(rates, freqs, True)\n    else:\n        raise ValueError(\"Can't handle this model: {}\".format(model))\n    tm = TransitionMatrix(subs_model)\n    gamma = GammaMixture(alpha, ncat)\n    gamma.init_models(tm, alignment_to_partials(alignment, missing_data))\n    return gamma"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling from lst with replacement", "response": "def sample_wr(lst):\n    \"\"\"\n    Sample from lst, with replacement\n    \"\"\"\n    arr = np.array(lst)\n    indices = np.random.randint(len(lst), size=len(lst))\n    sample = np.empty(arr.shape, dtype=arr.dtype)\n    for i, ix in enumerate(indices):\n        sample[i] = arr[ix]\n    return list(sample)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _preprocess_inputs(x, weights):\n    if weights is None:\n        w_arr = np.ones(len(x))\n    else:\n        w_arr = np.array(weights)\n    x_arr = np.array(x)\n    if x_arr.ndim == 2:\n        if w_arr.ndim == 1:\n            w_arr = w_arr[:, np.newaxis]\n    return x_arr, w_arr", "response": "Coerce inputs into compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef amean(x, weights=None):\n    w_arr, x_arr = _preprocess_inputs(x, weights)\n    return (w_arr*x_arr).sum(axis=0) / w_arr.sum(axis=0)", "response": "Return the weighted arithmetic mean of x"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the weighted geometric mean of x", "response": "def gmean(x, weights=None):\n    \"\"\"\n    Return the weighted geometric mean of x\n    \"\"\"\n    w_arr, x_arr = _preprocess_inputs(x, weights)\n    return np.exp((w_arr*np.log(x_arr)).sum(axis=0) / w_arr.sum(axis=0))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hmean(x, weights=None):\n    w_arr, x_arr = _preprocess_inputs(x, weights)\n    return w_arr.sum(axis=0) / (w_arr/x_arr).sum(axis=0)", "response": "Return the weighted harmonic mean of x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gapmask(simseqs, origseqs):\n    import numpy as np\n    simdict = dict(simseqs)\n    origdict = dict(origseqs)\n    for k in origdict:\n        origseq = np.array(list(origdict[k]))\n        gap_pos = np.where(origseq=='-')\n        simseq = np.array(list(simdict[k]))\n        simseq[gap_pos] = '-'\n        simdict[k] = ''.join(simseq)\n    return list(simdict.items())", "response": "returns a list of tuples of simulated sequences and original sequences"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of records in SORT_KEY order", "response": "def records(self):\n        \"\"\" Returns a list of records in SORT_KEY order \"\"\"\n        return [self._records[i] for i in range(len(self._records))]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the alignment files from an input directory and store them in self. files.", "response": "def read_alignments(self, input_dir, file_format, header_grep=None):\n        \"\"\" Get list of alignment files from an input directory *.fa, *.fas and\n        *.phy files only\n\n        Stores in self.files \"\"\"\n\n        compression = ['', 'gz', 'bz2']\n\n        if file_format == 'fasta':\n            extensions = ['fa', 'fas', 'fasta']\n\n        elif file_format == 'phylip':\n            extensions = ['phy']\n\n        else:\n            extensions = []\n\n        extensions = list('.'.join([x,y]) if y else x for x,y in itertools.product(extensions, compression))\n\n        files = fileIO.glob_by_extensions(input_dir, extensions)\n        files.sort(key=SORT_KEY)\n        self._input_files = files\n        records = []\n\n        if self.show_progress:\n            pbar = setup_progressbar(\"Loading files\", len(files), simple_progress=True)\n            pbar.start()\n\n        for i, f in enumerate(files):\n            if f.endswith('.gz') or f.endswith('.bz2'):\n                fd, tmpfile = tempfile.mkstemp()\n                with fileIO.freader(f, f.endswith('.gz'), f.endswith('.bz2')) as reader,\\\n                     fileIO.fwriter(tmpfile) as writer:\n                    for line in reader:\n                        if ISPY3:\n                            line = line.decode()\n                        writer.write(line)\n                try:\n                    record = Alignment(tmpfile, file_format, True)\n                except ValueError:\n                    record = Alignment(tmpfile, file_format, False)\n                finally:\n                    os.close(fd)\n                    os.unlink(tmpfile)\n\n            else:\n                try:\n                    record = Alignment(f, file_format, True)\n                except RuntimeError:\n                    record = Alignment(f, file_format, False)\n\n            if header_grep:\n                try:\n                    datatype = 'dna' if record.is_dna() else 'protein'\n\n                    record = Alignment([(header_grep(x), y) for (x, y) in record.get_sequences()], datatype)\n\n                except TypeError:\n                    raise TypeError(\"Couldn't apply header_grep to header\\n\"\n                                    \"alignment number={}, name={}\\n\"\n                                    \"header_grep={}\".format(i, fileIO.strip_extensions(f), header_grep))\n                except RuntimeError:\n                    print('RuntimeError occurred processing alignment number={}, name={}'\n                          .format(i, fileIO.strip_extensions(f)))\n                    raise\n\n            record.name = (fileIO.strip_extensions(f))\n            records.append(record)\n            if self.show_progress:\n                pbar.update(i)\n        if self.show_progress:\n            pbar.finish()\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a directory full of tree files matching them up to the current tree ID.", "response": "def read_trees(self, input_dir):\n        \"\"\" Read a directory full of tree files, matching them up to the\n        already loaded alignments \"\"\"\n\n        if self.show_progress:\n            pbar = setup_progressbar(\"Loading trees\", len(self.records))\n            pbar.start()\n\n        for i, rec in enumerate(self.records):\n            hook = os.path.join(input_dir, '{}.nwk*'.format(rec.name))\n            filename = glob.glob(hook)\n            try:\n                with fileIO.freader(filename[0]) as infile:\n                    tree = infile.read().decode('utf-8')\n\n                d = dict(ml_tree=tree)\n\n                rec.parameters.construct_from_dict(d)\n\n            except (IOError, IndexError):\n                continue\n\n            finally:\n                if self.show_progress:\n                    pbar.update(i)\n\n        if self.show_progress:\n            pbar.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a directory full of json parameter files and populate the parameters object with the values from those files.", "response": "def read_parameters(self, input_dir):\n        \"\"\" Read a directory full of json parameter files, matching them up to the\n        already loaded alignments \"\"\"\n\n        if self.show_progress:\n            pbar = setup_progressbar(\"Loading parameters\", len(self.records))\n            pbar.start()\n        for i, rec in enumerate(self.records):\n            hook = os.path.join(input_dir, '{}.json*'.format(rec.name))\n            filename = glob.glob(hook)\n            try:\n                with fileIO.freader(filename[0]) as infile:\n                    d = json.loads(infile.read().decode('utf-8'), parse_int=True)\n\n                rec.parameters.construct_from_dict(d)\n\n            except (IOError, IndexError):\n                continue\n\n            finally:\n                if self.show_progress:\n                    pbar.update(i)\n        if self.show_progress:\n            pbar.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate fast approximate intra - alignment pairwise distances and variances using a task.", "response": "def calc_distances(self, indices=None, task_interface=None, jobhandler=default_jobhandler, batchsize=1,\n                       show_progress=True):\n        \"\"\"\n        Calculate fast approximate intra-alignment pairwise distances and variances using\n        ML (requires ML models to have been set up using `calc_trees`).\n        :return: None (all side effects)\n        \"\"\"\n        if indices is None:\n            indices = list(range(len(self)))\n\n        if task_interface is None:\n            task_interface = tasks.MLDistanceTaskInterface()\n\n        records = [self[i] for i in indices]\n\n        # Assemble argument lists\n        args, to_delete = task_interface.scrape_args(records)\n\n        # Dispatch\n        msg = '{} estimation'.format(task_interface.name) if show_progress else ''\n        map_result = jobhandler(task_interface.get_task(), args, msg, batchsize)\n\n        # Process results\n        with fileIO.TempFileList(to_delete):\n            # pbar = setup_progressbar('Processing results', len(map_result))\n            # j = 0\n            # pbar.start()\n            for rec, result in zip(records, map_result):\n                rec.parameters.partitions.distances = result['partitions'][0]['distances']\n                rec.parameters.partitions.variances = result['partitions'][0]['variances']\n                rec.parameters.nj_tree = result['nj_tree']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate phylogenetic trees for the given alignments.", "response": "def calc_trees(self, indices=None, task_interface=None, jobhandler=default_jobhandler, batchsize=1,\n                   show_progress=True, **kwargs):\n        \"\"\"\n        Infer phylogenetic trees for the loaded Alignments\n\n        :param indices: Only run inference on the alignments at these given indices\n        :param task_interface: Inference tool specified via TaskInterface (default RaxmlTaskInterface)\n        :param jobhandler: Launch jobs via this JobHandler (default SequentialJobHandler; also available are\n            ThreadpoolJobHandler and ProcesspoolJobHandler for running inference in parallel)\n        :param batchsize: Batch size for Thread- or ProcesspoolJobHandlers)\n        :param kwargs: Remaining arguments to pass to the TaskInterface\n        :return: None\n        \"\"\"\n        if indices is None:\n            indices = list(range(len(self)))\n\n        if task_interface is None:\n            task_interface = tasks.RaxmlTaskInterface()\n\n        records = [self[i] for i in indices]\n\n        # Scrape args from records\n        args, to_delete = task_interface.scrape_args(records, **kwargs)\n\n        # Dispatch work\n        msg = '{} Tree estimation'.format(task_interface.name) if show_progress else ''\n        map_result = jobhandler(task_interface.get_task(), args, msg, batchsize)\n\n        # Process results\n        with fileIO.TempFileList(to_delete):\n            for rec, result in zip(records, map_result):\n                #logger.debug('Result - {}'.format(result))\n                rec.parameters.construct_from_dict(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_inter_tree_distances(self, metric, jobhandler=default_jobhandler,\n                                 normalise=False, min_overlap=4, overlap_fail_value=0,\n                                 batchsize=1, show_progress=True):\n        \"\"\" Generate a distance matrix from a fully-populated Collection.\n            Can silence progressbars with show_progress=False option\n        :param metric: str. Tree distance metric to use. Choice of 'euc', 'geo', 'rf', 'wrf'.\n        :param jobhandler: treeCl.Jobhandler. Choice of SequentialJobHandler, ThreadpoolJobHandler, or\n            ProcesspoolJobHandler.\n        :param normalise:  Bool. Whether to normalise the tree distance to the size of the leaf set.\n        :param min_overlap: int. Trees with fewer leaves in common than this threshold will not have their distance\n            calculated, but instead the distance returned will be the value in `overlap_fail_value`.\n        :param overlap_fail_value: Any. The distance between trees with fewer leaves in common than `min_overlap`\n            is set to this value.\n        :param batchsize: int. Number of jobs to process in a batch when using a ProcesspoolJobHandler or a\n            ThreadpoolJobHandler.\n        :return: treeCl.DistanceMatrix.\n        \"\"\"\n        metrics = {'euc': tasks.EuclideanTreeDistance,\n                   'geo': tasks.GeodesicTreeDistance,\n                   'rf': tasks.RobinsonFouldsTreeDistance,\n                   'wrf': tasks.WeightedRobinsonFouldsTreeDistance,\n                   'fasteuc': tasks.EqualLeafSetEuclideanTreeDistance,\n                   'fastgeo': tasks.EqualLeafSetGeodesicTreeDistance,\n                   'fastrf': tasks.EqualLeafSetRobinsonFouldsTreeDistance,\n                   'fastwrf': tasks.EqualLeafSetWeightedRobinsonFouldsTreeDistance}\n        optioncheck(metric, list(metrics.keys()))\n        task_interface = metrics[metric]()\n        if metric.startswith('fast'):\n            trees = (PhyloTree(newick, False) for newick in self.trees)\n        else:\n            trees = self.trees\n        args = task_interface.scrape_args(trees, normalise, min_overlap, overlap_fail_value)\n        logger.debug('{}'.format(args))\n        msg = task_interface.name if show_progress else ''\n        array = jobhandler(task_interface.get_task(), args, msg, batchsize, nargs=binom_coeff(len(trees)))\n        return DistanceMatrix.from_array(squareform(array), self.names)", "response": "Generates a distance matrix from a fully - populated collection of trees."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the number of species found over all records", "response": "def num_species(self):\n        \"\"\" Returns the number of species found over all records\n        \"\"\"\n        all_headers = reduce(lambda x, y: set(x) | set(y),\n                             (rec.get_names() for rec in self.records))\n        return len(all_headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a copy of the collection with all alignment columns permuted", "response": "def permuted_copy(self, partition=None):\n        \"\"\" Return a copy of the collection with all alignment columns permuted\n        \"\"\"\n        def take(n, iterable):\n            return [next(iterable) for _ in range(n)]\n\n        if partition is None:\n            partition = Partition([1] * len(self))\n\n        index_tuples = partition.get_membership()\n\n        alignments = []\n        for ix in index_tuples:\n            concat = Concatenation(self, ix)\n            sites = concat.alignment.get_sites()\n            random.shuffle(sites)\n            d = dict(zip(concat.alignment.get_names(), [iter(x) for x in zip(*sites)]))\n            new_seqs = [[(k, ''.join(take(l, d[k]))) for k in d] for l in concat.lengths]\n\n            for seqs, datatype, name in zip(new_seqs, concat.datatypes, concat.names):\n                alignment = Alignment(seqs, datatype)\n                alignment.name = name\n                alignments.append(alignment)\n\n        return self.__class__(records=sorted(alignments, key=lambda x: SORT_KEY(x.name)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the unique ID of the group", "response": "def get_id(self, grp):\n        \"\"\"\n        Return a hash of the tuple of indices that specify the group\n        \"\"\"\n        thehash = hex(hash(grp))\n        if ISPY3:  # use default encoding to get bytes\n            thehash = thehash.encode()\n        return self.cache.get(grp, hashlib.sha1(thehash).hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck for the existence of alignment and result files.", "response": "def check_work_done(self, grp):\n        \"\"\"\n        Check for the existence of alignment and result files.\n        \"\"\"\n        id_ = self.get_id(grp)\n        concat_file = os.path.join(self.cache_dir, '{}.phy'.format(id_))\n        result_file = os.path.join(self.cache_dir, '{}.{}.json'.format(id_, self.task_interface.name))\n        return os.path.exists(concat_file), os.path.exists(result_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_group(self, grp, overwrite=False, **kwargs):\n        id_ = self.get_id(grp)\n        alignment_done, result_done = self.check_work_done(grp)\n        self.cache[grp] = id_\n        al_filename = os.path.join(self.cache_dir, '{}.phy'.format(id_))\n        qfile_filename = os.path.join(self.cache_dir, '{}.partitions.txt'.format(id_))\n        if overwrite or not (alignment_done or result_done):\n            conc = self.collection.concatenate(grp)\n            al = conc.alignment\n            al.write_alignment(al_filename, 'phylip', True)\n            q = conc.qfile(**kwargs)\n            with open(qfile_filename, 'w') as fl:\n                fl.write(q + '\\n')", "response": "Write the concatenated alignment to disk in the location specified by grp."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_group_result(self, grp, **kwargs):\n        id_ = self.get_id(grp)\n        self.cache[grp] = id_\n\n        # Check if this file is already processed\n        alignment_written, results_written = self.check_work_done(grp)\n\n        if not results_written:\n            if not alignment_written:\n                self.write_group(grp, **kwargs)\n            logger.error('Alignment {} has not been analysed - run analyse_cache_dir'.format(id_))\n            raise ValueError('Missing result')\n        else:\n            with open(self.get_result_file(id_)) as fl:\n                return json.load(fl)", "response": "Retrieve the results for a group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscanning the cache directory and launch analysis for all unscored alignments and return the results.", "response": "def analyse_cache_dir(self, jobhandler=None, batchsize=1, **kwargs):\n        \"\"\"\n        Scan the cache directory and launch analysis for all unscored alignments\n        using associated task handler. KWargs are passed to the tree calculating\n        task managed by the TaskInterface in self.task_interface.\n        Example kwargs:\n          TreeCollectionTaskInterface: scale=1, guide_tree=None, \n                                       niters=10, keep_topology=False\n          RaxmlTaskInterface: -------- partition_files=None, model=None, threads=1\n          FastTreeTaskInterface: ----- No kwargs\n        \"\"\"\n        if jobhandler is None:\n            jobhandler = SequentialJobHandler()\n        files = glob.glob(os.path.join(self.cache_dir, '*.phy'))\n        #logger.debug('Files - {}'.format(files))\n        records = []\n        outfiles = []\n        dna = self.collection[0].is_dna() # THIS IS ONLY A GUESS AT SEQ TYPE!!\n        for infile in files:\n            id_ = fileIO.strip_extensions(infile)\n            outfile = self.get_result_file(id_)\n            #logger.debug('Looking for {}: {}'.format(outfile, os.path.exists(outfile)))\n            if not os.path.exists(outfile):\n                record = Alignment(infile, 'phylip', True)\n                records.append(record)\n                outfiles.append(outfile)\n\n        if len(records) == 0:\n            return []\n\n        args, to_delete = self.task_interface.scrape_args(records, outfiles=outfiles, **kwargs)\n        # logger.debug('Args - {}'.format(args))\n\n        with fileIO.TempFileList(to_delete):\n            result = jobhandler(self.task_interface.get_task(), args, 'Cache dir analysis', batchsize)\n            for (out, res) in zip(outfiles, result):\n                if not os.path.exists(out) and res:\n                    with open(out, 'w') as outfl:\n                        json.dump(res, outfl)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the score of a partition.", "response": "def get_partition_score(self, p):\n        \"\"\"\n        Assumes analysis is done and written to id.json!\n        \"\"\"\n        scores = []\n        for grp in p.get_membership():\n            try:\n                result = self.get_group_result(grp)\n                scores.append(result['likelihood'])\n            except ValueError:\n                scores.append(None)\n        return sum(scores)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the trees associated with a partition.", "response": "def get_partition_trees(self, p):\n        \"\"\"\n        Return the trees associated with a partition, p\n        \"\"\"\n        trees = []\n        for grp in p.get_membership():\n            try:\n                result = self.get_group_result(grp)\n                trees.append(result['ml_tree'])\n            except ValueError:\n                trees.append(None)\n                logger.error('No tree found for group {}'.format(grp))\n        return trees"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsimulates a set of alignments from the parameters inferred on a partition.", "response": "def simulate(self, partition, outdir, jobhandler=default_jobhandler, batchsize=1, **kwargs):\n        \"\"\"\n        Simulate a set of alignments from the parameters inferred on a partition\n        :param partition:\n        :return:\n        \"\"\"\n\n        results = self.get_partition_results(partition)\n        DEFAULT_DNA_MODEL = 'GTR'\n        DEFAULT_PROTEIN_MODEL = 'LG08'\n\n        # Collect argument list\n        args = [None] * len(self.collection)\n\n        for result in results:\n            if len(result['partitions']) > 1:\n                places = dict((j,i) for (i,j) in enumerate(rec.name for rec in self.collection.records))\n                for partition in result['partitions'].values():\n                    place = places[partition['name']]\n                    model = partition.get('model')\n                    freqs = partition.get('frequencies')\n                    rates = partition.get('rates')\n                    alpha = partition.get('alpha')\n                    tree  = str(result['ml_tree'])\n                    if model is None:\n                        model = DEFAULT_DNA_MODEL if self.collection[place].is_dna() else DEFAULT_PROTEIN_MODEL\n                    if freqs is not None:\n                        freqs = smooth_freqs(freqs)\n                    args[place] = (len(self.collection[place]),\n                                   model_translate(model),\n                                   freqs,\n                                   alpha,\n                                   tree,\n                                   rates)\n            else:\n                model = result['partitions']['0'].get('model')\n                freqs = result['partitions']['0'].get('frequencies')\n                rates = result['partitions']['0'].get('rates')\n                alpha = result['partitions']['0'].get('alpha')\n                tree  = str(result['ml_tree'])\n                if freqs is not None:\n                    freqs = smooth_freqs(freqs)\n                use_default_model = (model is None)\n                for i in range(len(self.collection)):\n                    if use_default_model:\n                        model = DEFAULT_DNA_MODEL if self.collection[i].is_dna() else DEFAULT_PROTEIN_MODEL\n                    args[i] = (len(self.collection[i]),\n                               model_translate(model),\n                               freqs,\n                               alpha,\n                               tree,\n                               rates)\n\n        # Distribute work\n        msg = 'Simulating'\n        map_result = jobhandler(tasks.simulate_task, args, msg, batchsize)\n\n        # Process results\n        for i, result in enumerate(map_result):\n            orig = self.collection[i]\n            simseqs = gapmask(result.items(), orig.get_sequences())\n            al = Alignment(simseqs, alphabet=('protein' if orig.is_protein() else 'dna'))\n            outfile = os.path.join(outdir, orig.name + '.phy')\n            al.write_alignment(outfile, 'phylip', True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expect(self, use_proportions=True):\n        changed = self.get_changed(self.partition, self.prev_partition)\n        lk_table = self.generate_lktable(self.partition, changed, use_proportions)\n        self.table = self.likelihood_table_to_probs(lk_table)", "response": "The Expectation step of the CEM algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef classify(self, table, weighted_choice=False, transform=None):\n        assert table.shape[1] == self.numgrp\n        if weighted_choice:\n            if transform is not None:\n                probs = transform_fn(table.copy(), transform)  #\n            else:\n                probs = table.copy()\n            cmprobs = probs.cumsum(1)\n            logger.info('Probabilities\\n{}'.format(probs))\n            r = np.random.random(cmprobs.shape[0])\n            search = np.apply_along_axis(np.searchsorted, 1, cmprobs, r) # Not very efficient\n            assignment = np.diag(search)\n        else:\n            probs = table\n            assignment = np.where(probs==probs.max(1)[:, np.newaxis])[1]\n        logger.info('Assignment\\n{}'.format(assignment))\n        assignment = self._fill_empty_groups(probs, assignment)  # don't want empty groups\n        new_partition = Partition(tuple(assignment))\n        self.set_partition(new_partition)", "response": "This method is used to classify the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maximise(self, **kwargs):\n        self.scorer.write_partition(self.partition)\n        self.scorer.analyse_cache_dir(**kwargs)\n        self.likelihood = self.scorer.get_partition_score(self.partition)\n        self.scorer.clean_cache()\n        changed = self.get_changed(self.partition, self.prev_partition)\n        self.update_perlocus_likelihood_objects(self.partition, changed)\n        return self.partition, self.likelihood, sum(inst.get_likelihood() for inst in self.insts)", "response": "Maximisation step of the CEM algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstore the partition in self. partition and move the old partition into self. prev_partition.", "response": "def set_partition(self, partition):\n        \"\"\"\n        Store the partition in self.partition, and\n        move the old self.partition into self.prev_partition\n        \"\"\"\n        assert len(partition) == self.numgrp\n        self.partition, self.prev_partition = partition, self.partition"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_changed(self, p1, p2):\n        if p1 is None or p2 is None:\n            return list(range(len(self.insts)))\n        return set(flatten_list(set(p1) - set(p2)))", "response": "Returns the loci that have changed between partitions p1 and p2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating likelihood model with new values from dictionary partition_parameters and tree.", "response": "def _update_likelihood_model(self, inst, partition_parameters, tree):\n        \"\"\" \n        Set parameters of likelihood model - inst -\n        using values in dictionary - partition_parameters -,\n        and - tree -\n        \"\"\"\n        # Build transition matrix from dict\n        model = partition_parameters['model']\n        freqs = partition_parameters.get('frequencies')\n        if model == 'LG':\n            subs_model = phylo_utils.models.LG(freqs)\n        elif model == 'WAG':\n            subs_model = phylo_utils.models.WAG(freqs)\n        elif model == 'GTR':\n            rates = partition_parameters.get('rates')\n            subs_model = phylo_utils.models.GTR(rates, freqs, True)\n        else:\n            raise ValueError(\"Can't handle this model: {}\".format(model))\n        tm = phylo_utils.markov.TransitionMatrix(subs_model)\n        \n        # Read alpha value\n        alpha = partition_parameters['alpha']\n        inst.set_tree(tree)\n        inst.update_alpha(alpha)\n        inst.update_transition_matrix(tm)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef likelihood_table_to_probs(self, lktable):\n        m = lktable.max(1)  # row max of lktable\n        shifted = lktable-m[:,np.newaxis]  # shift lktable of log-likelihoods to a non-underflowing range\n        expsum = np.exp(shifted).sum(1)  # convert logs to (scaled) normal space, and sum the rows\n        logexpsum = np.log(expsum)+m  # convert back to log space, and undo the scaling\n        return np.exp(lktable - logexpsum[:, np.newaxis])", "response": "Calculates the likelihood of a data point with parameters a_k and returns the probability of the probability of the prior probability of the data point with parameters a_k."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo the simple thing - if any group is empty then assign the highest probability of the data point with the highest probability of the data point", "response": "def _fill_empty_groups_old(self, probs, assignment):\n        \"\"\" Does the simple thing - if any group is empty, but needs to have at\n        least one member, assign the data point with highest probability of\n        membership \"\"\"\n        new_assignment = np.array(assignment.tolist())\n        for k in range(self.numgrp):\n            if np.count_nonzero(assignment==k) == 0:\n                logger.info('Group {} became empty'.format(k))\n                best = np.where(probs[:,k]==probs[:,k].max())[0][0]\n                new_assignment[best] = k\n                new_assignment = self._fill_empty_groups(probs, new_assignment)\n        return new_assignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes analysis result of partition.", "response": "def wipe_partition(self, partition):\n        \"\"\" Deletes analysis result of partition, e.g. so a repeat\n        optimisation of the same partition can be done with a\n        different model \"\"\"\n        for grp in partition.get_membership():\n            grpid = self.scorer.get_id(grp)\n            cache_dir = self.scorer.cache_dir\n            prog = self.scorer.task_interface.name\n            filename = os.path.join(cache_dir, '{}.{}.json'.format(grpid, prog))\n            if os.path.exists(filename):\n                os.unlink(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the G matrix for a given set of values.", "response": "def g(x,a,c):\n    \"\"\"\n    Christophe's suggestion for residuals,\n    G[i] = Sqrt(Sum_j (x[j] - a[i,j])^2) - C[i] \n    \"\"\"\n    return np.sqrt(((x-a)**2).sum(1)) - c"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jac(x,a):\n    return (x-a) / np.sqrt(((x-a)**2).sum(1))[:,np.newaxis]", "response": "Jacobian matrix given Christophe s suggestion of f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gradient(x, a, c):\n    return jac(x, a).T.dot(g(x, a, c))", "response": "Gradient of the J. G function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grad_desc_update(x, a, c, step=0.01):\n    return x - step * gradient(x,a,c)", "response": "Given a value of x return a better x \n    using gradient descent \n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a value of x return a better x using newton - gauss", "response": "def newton_update(x, a, c, step=1.0):\n    \"\"\" \n    Given a value of x, return a better x \n    using newton-gauss \n    \"\"\"\n    return x - step*np.linalg.inv(hessian(x, a)).dot(gradient(x, a, c))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef levenberg_marquardt_update(x, a, c, damping=0.001):\n    hess = hessian(x, a)\n    return x - np.linalg.inv(hess + damping*np.diag(hess)).dot(gradient(x, a, c))", "response": "Given a value of x return a better x \n    using newton - gauss \n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef golden_section_search(fn, a, b, tolerance=1e-5):\n    c = b - GOLDEN*(b-a)\n    d = a + GOLDEN*(b-a)\n    while abs(c-d) > tolerance:\n        fc, fd = fn(c), fn(d)\n        if fc < fd:\n            b = d\n            d = c  #fd=fc;fc=f(c)\n            c = b - GOLDEN*(b-a)\n        else:\n            a = c\n            c = d  #fc=fd;fd=f(d)\n            d = a + GOLDEN*(b-a)\n    return (b+a)/2", "response": "search for a golden section"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef optimise_newton(x, a, c, tolerance=0.001):\n    x_new = x\n    x_old = x-1 # dummy value\n    while np.abs(x_new - x_old).sum() > tolerance:\n        x_old = x_new\n        x_new = newton_update(x_old, a, c)\n    return x_new", "response": "Optimise value of x using Newton s algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noptimising value of x using levenberg - marquardt", "response": "def optimise_levenberg_marquardt(x, a, c, damping=0.001, tolerance=0.001):\n    \"\"\"\n    Optimise value of x using levenberg-marquardt\n    \"\"\"\n    x_new = x\n    x_old = x-1 # dummy value\n    f_old = f(x_new, a, c)\n    while np.abs(x_new - x_old).sum() > tolerance:\n        x_old = x_new\n        x_tmp = levenberg_marquardt_update(x_old, a, c, damping)\n        f_new = f(x_tmp, a, c)\n        if f_new < f_old:\n            damping = np.max(damping/10., 1e-20)\n            x_new = x_tmp\n            f_old = f_new\n        else:\n            damping *= 10.\n    return x_new"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optimise_gradient_descent(x, a, c, tolerance=0.001):\n    x_new = x\n    x_old = x-1 # dummy value\n    while np.abs(x_new - x_old).sum() > tolerance:\n        x_old = x_new\n        step_size = golden_section_search(lambda step: f(grad_desc_update(x_old, a, c, step), a, c), -1.0, 1.0)\n        x_new = grad_desc_update(x_old, a, c, step_size)\n    return x_new", "response": "Optimise value of x using gradient descent\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns out - of - sample MDS on the given index.", "response": "def run_out_of_sample_mds(boot_collection, ref_collection, ref_distance_matrix, index, dimensions, task=_fast_geo, rooted=False, **kwargs):\n    \"\"\"\n    index = index of the locus the bootstrap sample corresponds to - only important if\n            using recalc=True in kwargs\n    \"\"\"\n    fit = np.empty((len(boot_collection), dimensions))\n    if ISPY3:\n        query_trees = [PhyloTree(tree.encode(), rooted) for tree in boot_collection.trees]\n        ref_trees = [PhyloTree(tree.encode(), rooted) for tree in ref_collection.trees]\n    else:\n        query_trees = [PhyloTree(tree, rooted) for tree in boot_collection.trees]\n        ref_trees = [PhyloTree(tree, rooted) for tree in ref_collection.trees]\n    for i, tree in enumerate(query_trees):\n        distvec = np.array([task(tree, ref_tree, False) for ref_tree in ref_trees])\n        oos = OutOfSampleMDS(ref_distance_matrix)\n        fit[i] = oos.fit(index, distvec, dimensions=dimensions, **kwargs)\n    return fit"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stress(ref_cds, est_cds):\n    ref_dists = pdist(ref_cds)\n    est_dists = pdist(est_cds)\n    return np.sqrt(((ref_dists - est_dists)**2).sum() / (ref_dists**2).sum())", "response": "Compute the stress of a single node in a Kruskal dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rmsd(ref_cds, est_cds):\n    ref_dists = pdist(ref_cds)\n    est_dists = pdist(est_cds)\n    return np.sqrt(((ref_dists - est_dists)**2).mean())", "response": "Root - mean - squared - difference between two sets of CDSs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noptimises value of x using Newton s algorithm.", "response": "def newton(self, start_x=None, tolerance=1.0e-6):\n        \"\"\"\n        Optimise value of x using newton gauss\n        \"\"\"\n        if start_x is None:\n            start_x = self._analytical_fitter.fit(self._c)\n        return optimise_newton(start_x, self._a, self._c, tolerance)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noptimising value of x using gradient descent.", "response": "def gradient_descent(self, start_x=None, tolerance=1.0e-6):\n        \"\"\"\n        Optimise value of x using gradient descent\n        \"\"\"\n        if start_x is None:\n            start_x = self._analytical_fitter.fit(self._c)\n        return optimise_gradient_descent(start_x, self._a, self._c, tolerance)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noptimise value of x using levenberg marquardt", "response": "def levenberg_marquardt(self, start_x=None, damping=1.0e-3, tolerance=1.0e-6):\n        \"\"\"\n        Optimise value of x using levenberg marquardt\n        \"\"\"\n        if start_x is None:\n            start_x = self._analytical_fitter.fit(self._c)\n        return optimise_levenberg_marquardt(start_x, self._a, self._c, tolerance)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting a new set of coordinates to a new set of unique entries.", "response": "def fit(self, index, distvec, recalc=False, dimensions=3):\n        \"\"\"\n        Replace distance matrix values at row/column index with\n        distances in distvec, and compute new coordinates.\n        Optionally use distvec to update means and (potentially) \n        get a better estimate.\n        distvec values should be plain distances, not squared\n        distances.\n        \"\"\"\n        brow = self.new_B_row(index, distvec**2, recalc)\n        return self.new_coords(brow)[:dimensions]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_A_and_part_of_b_adjacent(self, ref_crds):\n        rot = self._rotate_rows(ref_crds)\n        A = 2*(rot - ref_crds)\n        partial_b = (rot**2 - ref_crds**2).sum(1)\n        return A, partial_b", "response": "Make A and part of b."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _analytical_fit_adjacent(self, ref_dists):\n        dists = ref_dists**2\n        rot_dists = self._rotate_rows(dists)\n        b = dists - rot_dists + self._partial_b\n        self._b = b\n        return self._pinvA.dot(b)", "response": "Fit coords x y z so that distances from reference coordinates match closest to reference distances\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating ES mapping for a given schema.", "response": "def generate_schema_mapping(resolver, schema_uri, depth=1):\n    \"\"\" Try and recursively iterate a JSON schema and to generate an ES mapping\n    that encasulates it. \"\"\"\n    visitor = SchemaVisitor({'$ref': schema_uri}, resolver)\n    return _generate_schema_mapping(visitor, set(), depth)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eucdist_task(newick_string_a, newick_string_b, normalise, min_overlap=4, overlap_fail_value=0):\n    tree_a = Tree(newick_string_a)\n    tree_b = Tree(newick_string_b)\n    return treedist.eucdist(tree_a, tree_b, normalise, min_overlap, overlap_fail_value)", "response": "Distributed version of tree_distance. eucdist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndistributing version of tree_distance. geodist", "response": "def geodist_task(newick_string_a, newick_string_b, normalise, min_overlap=4, overlap_fail_value=0):\n    \"\"\"\n    Distributed version of tree_distance.geodist\n    Parameters: two valid newick strings and a boolean\n    \"\"\"\n    tree_a = Tree(newick_string_a)\n    tree_b = Tree(newick_string_b)\n    return treedist.geodist(tree_a, tree_b, normalise, min_overlap, overlap_fail_value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndistribute version of tree_distance. rfdist", "response": "def rfdist_task(newick_string_a, newick_string_b, normalise, min_overlap=4, overlap_fail_value=0):\n    \"\"\"\n    Distributed version of tree_distance.rfdist\n    Parameters: two valid newick strings and a boolean\n    \"\"\"\n    tree_a = Tree(newick_string_a)\n    tree_b = Tree(newick_string_b)\n    return treedist.rfdist(tree_a, tree_b, normalise, min_overlap, overlap_fail_value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndistributing version of tree_distance. rfdist", "response": "def wrfdist_task(newick_string_a, newick_string_b, normalise, min_overlap=4, overlap_fail_value=0):\n    \"\"\"\n    Distributed version of tree_distance.rfdist\n    Parameters: two valid newick strings and a boolean\n    \"\"\"\n    tree_a = Tree(newick_string_a)\n    tree_b = Tree(newick_string_b)\n    return treedist.wrfdist(tree_a, tree_b, normalise, min_overlap, overlap_fail_value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscraping the command line arguments for the RAxML tree inference.", "response": "def scrape_args(self, records, executable='raxmlHPC-AVX', partition_files=None,\n                    model=None, outfiles=None, threads=1, parsimony=False, fast_tree=False,\n                    n_starts=1):\n        \"\"\"\n        Examine a list of records and generate RAxML command line arguments for tree inference.\n        :param records: list of `Alignment` records\n        :param executable: name of the RAxML executable on the system to use. Must be in the user's path.\n        :param partition_files: List of RAxML partition files used to describe any partitioning scheme\n            to be used (optional)\n        :param model: Choice of model to use. Defaults to GTRGAMMA for DNA, or PROTGAMMALGX for amino acid alignments.\n        :param outfiles: A list of output file locations to write results (required length = 1 per alignment)\n        :param threads: Number of threads for RAxML to use. This is independent of any threading used by the\n            `JobHandler`, and the user should be sure that their choice is appropriate for the number of threads\n            available to their system, and for the RAxML executable being used.\n        :param parsimony: Use RAxML's parsimony tree search only\n        :param fast_tree: Use RAxML's experimental fast tree search (-f E)\n        :return: (List of command line arguments, List of created temporary files)\n        \"\"\"\n        args = []\n        to_delete = []\n        if partition_files is None:\n            partition_files = [None for rec in records]\n        if outfiles is None:\n            outfiles = [None for rec in records]\n        for (rec, qfile, ofile) in zip(records, partition_files, outfiles):\n            if model is None:\n                model = 'GTRGAMMA' if rec.is_dna() else 'PROTGAMMALGX'\n            filename, delete = rec.get_alignment_file(as_phylip=True)\n            if delete:\n                to_delete.append(filename)\n                to_delete.append(filename + '.reduced')\n\n            if qfile is None:\n                # Attempt to find partition file on disk, using extension 'partitions.txt'\n                if filename.endswith('.phy'):\n                    likely_qfile = filename.replace('phy', 'partitions.txt')\n                else:\n                    likely_qfile = filename + '.partitions.txt'\n                if os.path.exists(likely_qfile):\n                    qfile = likely_qfile\n                else:\n                    with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmpfile:\n                        qfile = tmpfile.name\n                        to_delete.append(tmpfile.name)\n                        mymodel = 'DNAX' if rec.is_dna() else model.replace('PROT', '').replace('GAMMA', '').replace('CAT', '')\n                        partition_string = '{model}, {name} = 1-{seqlen}\\n'.format(\n                            model=mymodel,\n                            name=rec.name, seqlen=len(rec))\n                        tmpfile.write(partition_string)\n\n            args.append((executable, filename, model, qfile, ofile, threads, parsimony, fast_tree, n_starts))\n        return args, to_delete"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_alf_dirs_(self):\n        alf_dirs = {}\n        for k in range(self.num_classes):\n            dirname = fileIO.join_path(self.tmpdir, 'class{0:0>1}'.format(\n                k + 1))\n            alf_dirs[k + 1] = errors.directorymake(dirname)\n        self.alf_dirs = alf_dirs", "response": "DEPRECATED - Make all the alf directories for the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_alf_params_(self):\n        if not hasattr(self, 'alf_dirs'):\n            self.make_alf_dirs()\n\n        if not hasattr(self, 'class_trees'):\n            self.generate_class_trees()\n\n        alf_params = {}\n        for k in range(self.num_classes):\n            alfdir = self.alf_dirs[k + 1]\n            tree = self.class_trees[k + 1]\n            datatype = self.datatype\n            name = 'class{0}'.format(k + 1)\n            num_genes = self.class_list[k]\n            seqlength = self.gene_length_min\n            gene_length_kappa = self.gene_length_kappa\n            gene_length_theta = self.gene_length_theta\n            alf_obj = ALF(tree=tree,\n                          datatype=datatype, num_genes=num_genes,\n                          seqlength=seqlength, gene_length_kappa=gene_length_kappa,\n                          gene_length_theta=gene_length_theta, name=name, tmpdir=alfdir)\n            if datatype == 'protein':\n                alf_obj.params.one_word_model('WAG')\n            else:\n                alf_obj.params.jc_model()\n            alf_params[k + 1] = alf_obj\n\n        self.alf_params = alf_params", "response": "Write the alf parameters for each class in the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_mapping(mapping):\n    file_path = os.path.join(os.path.dirname(__file__),\n                             'schemas', 'mapping.json')\n    with open(file_path, 'r') as fh:\n        validator = Draft4Validator(json.load(fh))\n        validator.validate(mapping)\n    return mapping", "response": "Validate a mapping file against the relevant schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generic_matrix_calc(fn, trees, normalise, min_overlap=4, overlap_fail_value=0, show_progress=True):\n    jobs = itertools.combinations(trees, 2)\n    results = []\n    if show_progress:\n        pbar = setup_progressbar('Calculating tree distances', 0.5 * len(trees) * (len(trees) - 1))\n        pbar.start()\n    for i, (t1, t2) in enumerate(jobs):\n        results.append(_generic_distance_calc(fn, t1, t2, normalise, min_overlap, overlap_fail_value))\n        if show_progress:\n            pbar.update(i)\n    if show_progress:\n        pbar.finish()\n    return scipy.spatial.distance.squareform(results)", "response": "Calculates all pairwise distances between trees given in the parameter trees."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a 2D heatmap of the distance matrix dm.", "response": "def heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10):\n    \"\"\" heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10)\n    \n    Produce a 2D plot of the distance matrix, with values encoded by\n    coloured cells.\n\n    Args:\n        partition: treeCl.Partition object - if supplied, will reorder\n                   rows and columns of the distance matrix to reflect\n                   the groups defined by the partition\n        cmap: matplotlib colourmap object  - the colour palette to use\n        fontsize: int or None - sets the size of the locus lab\n\n    Returns:\n        matplotlib plottable object\n    \"\"\"\n    assert isinstance(dm, DistanceMatrix)\n    datamax = float(np.abs(dm.values).max())\n    length = dm.shape[0]\n\n    if partition:\n        sorting = np.array(flatten_list(partition.get_membership()))\n        new_dm = dm.reorder(dm.df.columns[sorting])\n    else:\n        new_dm = dm\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    \n    ax.xaxis.tick_top()\n    ax.grid(False)\n\n    tick_positions = np.array(list(range(length))) + 0.5\n    if fontsize is not None:\n        ax.set_yticks(tick_positions)\n        ax.set_xticks(tick_positions)\n        ax.set_xticklabels(new_dm.df.columns, rotation=90, fontsize=fontsize, ha='center')\n        ax.set_yticklabels(new_dm.df.index, fontsize=fontsize, va='center')\n\n    cbar_ticks_at = [0, 0.5 * datamax, datamax]\n    \n    cax = ax.imshow(\n        new_dm.values,\n        interpolation='nearest',\n        extent=[0., length, length, 0.],\n        vmin=0,\n        vmax=datamax,\n        cmap=cmap,\n    )\n    cbar = fig.colorbar(cax, ticks=cbar_ticks_at, format='%1.2g')\n    cbar.set_label('Distance')\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _plotly_3d_scatter(coords, partition=None):\n    from plotly.graph_objs import Scatter3d, Data, Figure, Layout, Line, Margin, Marker\n    # auto sign-in with credentials or use py.sign_in()\n\n    colourmap = {\n        'A':'#1f77b4', \n        'B':'#ff7f0e', \n        'C':'#2ca02c',\n        'D':'#d62728',\n        'E':'#9467bd',\n        1:'#1f77b4', \n        2:'#ff7f0e', \n        3:'#2ca02c',\n        4:'#d62728',\n        5:'#9467bd'\n    }\n\n    df = coords.df\n    if partition:\n        assert len(partition.partition_vector) == df.shape[0]\n        labels = [x+1 for x in partition.partition_vector]\n    else:\n        labels = [1 for _ in range(df.shape[0])]\n\n    x, y, z = df.columns[:3]\n    df['Label'] = labels\n    \n    colours = [colourmap[lab] for lab in df['Label']]\n    trace = Scatter3d(x=df[x], y=df[y], z=df[z], mode='markers',\n                      marker=Marker(size=9, color=colours, \n                                    line=Line(color=colours, width=0.5), opacity=0.8),\n                      text=[str(ix) for ix in df.index])\n\n    data = Data([trace])\n    layout = Layout(\n        margin=Margin(l=0, r=0, b=0, t=0 ),\n        hovermode='x',\n    )\n    fig = Figure(data=data, layout=layout)\n    return fig", "response": "Create a 3D scatterplot of the given treeCl. CoordinateMatrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_sphere(ax):\n    (u, v) = np.mgrid[0:2 * np.pi:20j, 0:np.pi:10j]\n    x = np.cos(u) * np.sin(v)\n    y = np.sin(u) * np.sin(v)\n    z = np.cos(v)\n    ax.plot_wireframe(x, y, z, color='grey', linewidth=0.2)\n    return ax", "response": "Add a wireframe unit sphere onto matplotlib 3D axes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot an embedding of a set of points on a unit sphere.", "response": "def plot_embedding(coordinates, partition=None, add_sphere=False, point_size=8,\n        colours=None, labels=None, legend=True, outfile=False, **kwargs):\n    \"\"\" plot_embedding(coordinates, partition=None, add_sphere=False, point_size=8,\n    colours=None, labels=None, legend=True, outfile=False, **kwargs):\n    \n    Plot a 2D / 3D scatterplot of coordinates, optionally\n    coloured by group membership.\n\n    Args:\n        coordinates: numpy array or treeCl.CoordinateMatrix -\n            The coordinates of the points to plot. The number\n            of columns determines the number of dimensions in\n            the plot.\n        add_sphere: bool -\n            Add a wireframe sphere to a 3D plot. Spectral clustering\n            places points on the surface of a unit sphere.\n        colours: list of rgb hexes, or 'auto', or None -\n            Colours to use to colour the points, as a list of\n            RGB hex values. If None, defaults\n            (colorbrewer set3). If 'auto', generates a set\n            of colours equally spaced from the colour wheel.\n        labels: Tuple(xlab, ylab, title, zlab) -\n            Plot labels. Must be given in the above order.\n            Missing options will be replaced by None. E.g.\n            to set the title: (None, None, \"Some points\")\n        outfile: str -\n            Save figure to this filename\n    \"\"\"\n    if isinstance(coordinates, CoordinateMatrix):\n        coordinates = coordinates.values\n    dimensions = min(3, coordinates.shape[1])\n    partition = (partition or\n                 Partition(tuple([0] * len(coordinates))))\n    ngrp = partition.num_groups()\n\n    if colours is None:\n        colours = SET2\n    elif colours == 'auto':\n        colours = ggColorSlice(ngrp)\n\n    colour_cycle = itertools.cycle(colours)\n    colours = np.array([hex2color(c) for c in itertools.islice(colour_cycle, ngrp)])\n\n    if labels is None:\n        xlab, ylab, zlab, title = None, None, None, None\n    else:\n        if isinstance(labels, (tuple, list)):\n            labels = list(labels[:4])\n            labels.extend([None]*(4-len(labels)))\n            xlab, ylab, title, zlab = labels\n\n    fig = plt.figure()\n\n    if dimensions == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        if add_sphere:\n            ax = _add_sphere(ax)\n    else:\n        ax = fig.add_subplot(111)\n    \n    members = partition.get_membership()\n    for grp in range(ngrp):\n        index = np.array(members[grp])\n        points = coordinates[index,:dimensions].T\n        ax.scatter(*points, s=point_size, c=colours[grp], edgecolor=None, label='Group {}'.format(grp+1), **kwargs)\n\n    if xlab:\n        ax.set_xlabel(xlab)\n    if ylab:\n        ax.set_ylabel(ylab)\n    if zlab:\n        ax.set_zlabel(zlab)\n    if title:\n        ax.set_title(title)\n    if legend:\n        plt.legend()\n    if outfile:\n        fig.savefig('{0}.pdf'.format(outfile))\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a visual representation of a distance matrix.", "response": "def heatmap(self, partition=None, cmap=CM.Blues):\n        \"\"\" Plots a visual representation of a distance matrix \"\"\"\n\n        if isinstance(self.dm, DistanceMatrix):\n            length = self.dm.values.shape[0]\n        else:\n            length = self.dm.shape[0]\n        datamax = float(np.abs(self.dm).max())\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ticks_at = [0, 0.5 * datamax, datamax]\n        if partition:\n            sorting = flatten_list(partition.get_membership())\n            self.dm = self.dm.reorder(sorting)\n        cax = ax.imshow(\n            self.dm.values,\n            interpolation='nearest',\n            origin='lower',\n            extent=[0., length, 0., length],\n            vmin=0,\n            vmax=datamax,\n            cmap=cmap,\n        )\n        cbar = fig.colorbar(cax, ticks=ticks_at, format='%1.2g')\n        cbar.set_label('Distance')\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tree_collection_strings(self, scale=1, guide_tree=None):\n        records = [self.collection[i] for i in self.indices]\n        return TreeCollectionTaskInterface().scrape_args(records)", "response": "Function to get input strings for tree_collection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(buffer, auto_flatten=True, raise_for_index=True):\n    buffer = to_bytes(buffer)\n\n    view_out = _ffi.new('lsm_view_t **')\n    index_out = _ffi.new('lsm_index_t **')\n\n    buffer = to_bytes(buffer)\n    rv = rustcall(\n        _lib.lsm_view_or_index_from_json,\n        buffer, len(buffer), view_out, index_out)\n    if rv == 1:\n        return View._from_ptr(view_out[0])\n    elif rv == 2:\n        index = Index._from_ptr(index_out[0])\n        if auto_flatten and index.can_flatten:\n            return index.into_view()\n        if raise_for_index:\n            raise IndexedSourceMap('Unexpected source map index',\n                                   index=index)\n        return index\n    else:\n        raise AssertionError('Unknown response from C ABI (%r)' % rv)", "response": "Parses a JSON string into either a view or an index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a sourcemap view from a JSON string.", "response": "def from_json(buffer):\n        \"\"\"Creates a sourcemap view from a JSON string.\"\"\"\n        buffer = to_bytes(buffer)\n        return View._from_ptr(rustcall(\n            _lib.lsm_view_from_json,\n            buffer, len(buffer)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_memdb(buffer):\n        buffer = to_bytes(buffer)\n        return View._from_ptr(rustcall(\n            _lib.lsm_view_from_memdb,\n            buffer, len(buffer)))", "response": "Creates a sourcemap view from MemDB bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_memdb_file(path):\n        path = to_bytes(path)\n        return View._from_ptr(rustcall(_lib.lsm_view_from_memdb_file, path))", "response": "Creates a sourcemap view from MemDB at a given file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump_memdb(self, with_source_contents=True, with_names=True):\n        len_out = _ffi.new('unsigned int *')\n        buf = rustcall(\n            _lib.lsm_view_dump_memdb,\n            self._get_ptr(), len_out,\n            with_source_contents, with_names)\n        try:\n            rv = _ffi.unpack(buf, len_out[0])\n        finally:\n            _lib.lsm_buffer_free(buf)\n        return rv", "response": "Dumps a sourcemap in MemDB format into bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_token(self, line, col):\n        # Silently ignore underflows\n        if line < 0 or col < 0:\n            return None\n        tok_out = _ffi.new('lsm_token_t *')\n        if rustcall(_lib.lsm_view_lookup_token, self._get_ptr(),\n                    line, col, tok_out):\n            return convert_token(tok_out[0])", "response": "Given a minified location this tries to locate the closest available token that is a match. Returns None if no match can be found."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a token location and a minified function name and the minified source file this returns the original function name if it can be found.", "response": "def get_original_function_name(self, line, col, minified_name,\n                                   minified_source):\n        \"\"\"Given a token location and a minified function name and the\n        minified source file this returns the original function name if it\n        can be found of the minified function in scope.\n        \"\"\"\n        # Silently ignore underflows\n        if line < 0 or col < 0:\n            return None\n        minified_name = minified_name.encode('utf-8')\n        sout = _ffi.new('const char **')\n        try:\n            slen = rustcall(_lib.lsm_view_get_original_function_name,\n                            self._get_ptr(), line, col, minified_name,\n                            minified_source, sout)\n            if slen > 0:\n                return _ffi.unpack(sout[0], slen).decode('utf-8', 'replace')\n        except SourceMapError:\n            # In some rare cases the library is/was known to panic.  We do\n            # not want to report this upwards  (this happens on slicing\n            # out of range on older rust versions in the rust-sourcemap\n            # library)\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_source_contents(self, src_id):\n        len_out = _ffi.new('unsigned int *')\n        must_free = _ffi.new('int *')\n        rv = rustcall(_lib.lsm_view_get_source_contents,\n                      self._get_ptr(), src_id, len_out, must_free)\n        if rv:\n            try:\n                return _ffi.unpack(rv, len_out[0])\n            finally:\n                if must_free[0]:\n                    _lib.lsm_buffer_free(rv)", "response": "Given a source ID this returns the embedded sourcecode if there\n        is."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_source_contents(self, src_id):\n        return bool(rustcall(_lib.lsm_view_has_source_contents,\n                             self._get_ptr(), src_id))", "response": "Checks if some sources exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name of the given source.", "response": "def get_source_name(self, src_id):\n        \"\"\"Returns the name of the given source.\"\"\"\n        len_out = _ffi.new('unsigned int *')\n        rv = rustcall(_lib.lsm_view_get_source_name,\n                      self._get_ptr(), src_id, len_out)\n        if rv:\n            return decode_rust_str(rv, len_out[0])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over all source names and IDs.", "response": "def iter_sources(self):\n        \"\"\"Iterates over all source names and IDs.\"\"\"\n        for src_id in xrange(self.get_source_count()):\n            yield src_id, self.get_source_name(src_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_json(buffer):\n        buffer = to_bytes(buffer)\n        return Index._from_ptr(rustcall(\n            _lib.lsm_index_from_json,\n            buffer, len(buffer)))", "response": "Creates an index from a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the index into a view", "response": "def into_view(self):\n        \"\"\"Converts the index into a view\"\"\"\n        try:\n            return View._from_ptr(rustcall(\n                _lib.lsm_index_into_view,\n                self._get_ptr()))\n        finally:\n            self._ptr = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a sourcemap view from a JSON string.", "response": "def from_bytes(buffer):\n        \"\"\"Creates a sourcemap view from a JSON string.\"\"\"\n        buffer = to_bytes(buffer)\n        return ProguardView._from_ptr(rustcall(\n            _lib.lsm_proguard_mapping_from_bytes,\n            buffer, len(buffer)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a sourcemap view from a file path.", "response": "def from_path(filename):\n        \"\"\"Creates a sourcemap view from a file path.\"\"\"\n        filename = to_bytes(filename)\n        if NULL_BYTE in filename:\n            raise ValueError('null byte in path')\n        return ProguardView._from_ptr(rustcall(\n            _lib.lsm_proguard_mapping_from_path,\n            filename + b'\\x00'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup(self, dotted_path, lineno=None):\n        rv = None\n        try:\n            rv = rustcall(\n                _lib.lsm_proguard_mapping_convert_dotted_path,\n                self._get_ptr(),\n                dotted_path.encode('utf-8'), lineno or 0)\n            return _ffi.string(rv).decode('utf-8', 'replace')\n        finally:\n            if rv is not None:\n                _lib.lsm_buffer_free(rv)", "response": "This function returns the value of the dotted path in the format class_name : method_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the given mapping to data recursively.", "response": "def apply(self, data):\n        \"\"\" Apply the given mapping to ``data``, recursively. The return type\n        is a tuple of a boolean and the resulting data element. The boolean\n        indicates whether any values were mapped in the child nodes of the\n        mapping. It is used to skip optional branches of the object graph. \"\"\"\n        if self.visitor.is_object:\n            obj = {}\n            if self.visitor.parent is None:\n                obj['$schema'] = self.visitor.path\n            obj_empty = True\n            for child in self.children:\n                empty, value = child.apply(data)\n                if empty and child.optional:\n                    continue\n                obj_empty = False if not empty else obj_empty\n\n                if child.visitor.name in obj and child.visitor.is_array:\n                    obj[child.visitor.name].extend(value)\n                else:\n                    obj[child.visitor.name] = value\n            return obj_empty, obj\n\n        elif self.visitor.is_array:\n            empty, value = self.children.apply(data)\n            return empty, [value]\n\n        elif self.visitor.is_value:\n            return extract_value(self.mapping, self.visitor, data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_iter(cls, rows, mapping, resolver, scope=None):\n        mapper = cls(mapping, resolver, scope=scope)\n        for row in rows:\n            _, data = mapper.apply(row)\n            yield data", "response": "Given an iterable rows that yield data records and a\n        mapping return a tuple of data records and an exception that is raised if the resulting data does not match the expected schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate(self, text):\n\n        # Reset substitution counter\n        self.count = 0\n\n        # Process text\n        return self._make_regex().sub(self, text)", "response": "Translate text returns the modified text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decompose(self,\n            noise=False,\n            verbosity=0,\n            logic='or',\n            **kwargs):\n        \"\"\" Use prune to remove links between distant points:\n\n        prune is None: no pruning\n        prune={int > 0}: prunes links beyond `prune` nearest neighbours\n        prune='estimate': searches for the smallest value that retains a fully\n        connected graph\n\n        \"\"\"\n        matrix = self.get_dm(noise)\n\n        # get local scale estimate\n        est_scale = None\n\n        # ADJUST MASK\n        if self._pruning_option == options.PRUNING_NONE:  \n            # Set kp to max value\n            kp = len(matrix) - 1\n            mask = np.ones(matrix.shape, dtype=bool)\n        elif self._pruning_option == options.PRUNING_MANUAL:\n            # Manually set value of kp\n            kp = self._manual_pruning\n            mask = kmask(matrix, self._manual_pruning, logic=logic)\n        elif self._pruning_option == options.PRUNING_ESTIMATE:\n            # Must estimate value of kp\n            kp, mask, est_scale = binsearch_mask(matrix, logic=logic) \n        else:\n            raise ValueError(\"Unexpected error: 'kp' not set\")\n\n        # ADJUST SCALE\n        if self._scale_option == options.LOCAL_SCALE_MEDIAN:\n            dist = np.median(matrix, axis=1)\n            scale = np.outer(dist, dist)\n        elif self._scale_option == options.LOCAL_SCALE_MANUAL:\n            scale = kscale(matrix, self._manual_scale)\n        elif self._scale_option == options.LOCAL_SCALE_ESTIMATE:\n            if est_scale is None:\n                _, _, scale = binsearch_mask(matrix, logic=logic) \n            else:\n                # Nothing to be done - est_scale was set during the PRUNING_ESTIMATE\n                scale = est_scale\n        else:\n            raise ValueError(\"Unexpected error: 'scale' not set\")\n\n        # ZeroDivisionError safety check\n        if not (scale > 1e-5).all():\n            if verbosity > 0:\n                print('Rescaling to avoid zero-div error')\n            _, _, scale = binsearch_mask(matrix, logic=logic)\n            assert (scale > 1e-5).all()\n\n        aff = affinity(matrix, mask, scale)\n        aff.flat[::len(aff)+1] = 1.0\n        return aff", "response": "Decomposes the distant point into a set of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cluster(self, n, embed_dim=None, algo=spectral.SPECTRAL, method=methods.KMEANS):\n        if n == 1:\n            return Partition([1] * len(self.get_dm(False)))\n\n        if embed_dim is None:\n            embed_dim = n\n\n        if algo == spectral.SPECTRAL:\n            self._coords = self.spectral_embedding(embed_dim)\n        elif algo == spectral.KPCA:\n            self._coords = self.kpca_embedding(embed_dim)\n        elif algo == spectral.ZELNIKMANOR:\n            self._coords = self.spectral_embedding_(embed_dim)\n        else:\n            raise OptionError(algo, list(spectral.reverse.values()))\n        if method == methods.KMEANS:\n            p = self.kmeans(n, self._coords.df.values)\n        elif method == methods.GMM:\n            p = self.gmm(n, self._coords.df.values)\n        elif method == methods.WARD:\n            linkmat = fastcluster.linkage(self._coords.values, 'ward')\n            p = _hclust(linkmat, n)\n        else:\n            raise OptionError(method, list(methods.reverse.values()))\n        if self._verbosity > 0:\n            print('Using clustering method: {}'.format(methods.reverse[method]))\n        return p", "response": "Cluster the embedded coordinates using spectral clustering"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nembeds the points using spectral decomposition of the laplacian of the affinity matrix.", "response": "def spectral_embedding(self, n):\n        \"\"\"\n        Embed the points using spectral decomposition of the laplacian of\n        the affinity matrix\n\n        Parameters\n        ----------\n        n:      int\n                The number of dimensions\n        \"\"\"\n        coords = spectral_embedding(self._affinity, n)\n        return CoordinateMatrix(normalise_rows(coords))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spectral_embedding_(self, n):\n        aff = self._affinity.copy()\n        aff.flat[::aff.shape[0]+1] = 0\n        laplacian = laplace(aff)\n        decomp = eigen(laplacian)\n        return CoordinateMatrix(normalise_rows(decomp.vecs[:,:n]))", "response": "Returns a spectral embedding matrix for the given number of tables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nembedding the points using kernel PCA of the affinity matrix", "response": "def kpca_embedding(self, n):\n        \"\"\"\n        Embed the points using kernel PCA of the affinity matrix\n\n        Parameters\n        ----------\n        n:      int\n                The number of dimensions\n        \"\"\"\n        return self.dm.embedding(n, 'kpca', affinity_matrix=self._affinity)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclustering the embedded coordinates using multidimensional scaling", "response": "def cluster(self, n, embed_dim=None, algo=mds.CLASSICAL, method=methods.KMEANS):\n        \"\"\"\n        Cluster the embedded coordinates using multidimensional scaling\n\n        Parameters\n        ----------\n        n:                 int\n                           The number of clusters to return\n        embed_dim          int\n                           The dimensionality of the underlying coordinates\n                           Defaults to same value as n\n        method:            enum value (methods.KMEANS | methods.GMM)\n                           The clustering method to use\n\n        Returns\n        -------\n        Partition: Partition object describing the data partition\n        \"\"\"\n        if n == 1:\n            return Partition([1] * len(self.get_dm(False)))\n\n        if embed_dim is None:\n            embed_dim = n\n\n        if algo == mds.CLASSICAL:\n            self._coords = self.dm.embedding(embed_dim, 'cmds')\n        elif algo == mds.METRIC:\n            self._coords = self.dm.embedding(embed_dim, 'mmds')\n        else:\n            raise OptionError(algo, list(mds.reverse.values()))\n\n        if method == methods.KMEANS:\n            p = self.kmeans(n, self._coords.values)\n        elif method == methods.GMM:\n            p = self.gmm(n, self._coords.values)\n        elif method == methods.WARD:\n            linkmat = fastcluster.linkage(self._coords.values, 'ward')\n            p = _hclust(linkmat, n)\n        else:\n            raise OptionError(method, list(methods.reverse.values()))\n        #if self._verbosity > 0:\n        #    print('Using clustering method: {}'.format(methods.reverse[method]))\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cluster(self, nclusters, linkage_method=linkage.WARD, **kwargs):\n\n        if linkage_method == linkage.SINGLE:\n            return self._hclust(nclusters, 'single', **kwargs)\n        elif linkage_method == linkage.COMPLETE:\n            return self._hclust(nclusters, 'complete', **kwargs)\n        elif linkage_method == linkage.AVERAGE:\n            return self._hclust(nclusters, 'average', **kwargs)\n        elif linkage_method == linkage.WARD:\n            return self._hclust(nclusters, 'ward', **kwargs)\n        elif linkage_method == linkage.WEIGHTED:\n            return self._hclust(nclusters, 'weighted', **kwargs)\n        elif linkage_method == linkage.CENTROID:\n            return self._hclust(nclusters, 'centroid', **kwargs)\n        elif linkage_method == linkage.MEDIAN:\n            return self._hclust(nclusters, 'median', **kwargs)\n        else:\n            raise ValueError('Unknown linkage_method: {}'.format(linkage_method))", "response": "Do hierarchical clustering on a distance matrix using the specified linkage method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _hclust(self, nclusters, method, noise=False):\n        matrix = self.get_dm(noise)\n\n        linkmat = fastcluster.linkage(squareform(matrix), method)\n        self.nclusters = nclusters  # Store these in case we want to plot\n        self.linkmat = linkmat      #\n        return _hclust(linkmat, nclusters)", "response": "Return a partition object describing a single cluster with a linkage method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_dendrogram(self, nclusters=None, leaf_font_size=8, leaf_rotation=90, names=None,\n                        title_font_size=16, ):\n        \"\"\"\n        Plots the dendrogram of the most recently generated partition\n        :param nclusters: Override the plot default number of clusters\n\n        :return: matplotlib.pyplot.figure\n        \"\"\"\n\n        if not hasattr(self, 'nclusters') and not hasattr(self, 'linkmat'):\n            raise ValueError(\"This instance has no plottable information.\")\n\n        if nclusters is None:\n            nclusters = self.nclusters\n\n        threshold = _get_threshold(self.linkmat, nclusters)\n\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(11.7, 8.3))\n\n        if names is not None:\n            labfn=lambda leaf: names[leaf]\n        else:\n            labfn=None\n            leaf_rotation=0\n\n        dendrogram(\n            self.linkmat,\n            color_threshold=threshold,\n            leaf_font_size=leaf_font_size,\n            leaf_rotation=leaf_rotation,\n            leaf_label_func=labfn,\n            count_sort=True,\n            )\n\n        plt.suptitle('Dendrogram', fontsize=title_font_size)\n        # plt.title('Distance metric: {0}    Linkage method: {1}    Number of classes: {2}'.format(compound_key[0],\n        #           compound_key[1], compound_key[2]), fontsize=12)\n        plt.axhline(threshold, color='grey', ls='dashed')\n        plt.xlabel('Gene')\n        plt.ylabel('Distance')\n        return fig", "response": "Plots the dendrogram of the most recently generated partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dbscan(self, eps=0.75, min_samples=3):\n        est = DBSCAN(metric='precomputed', eps=eps, min_samples=min_samples)\n        est.fit(self.get_dm(False))\n        return Partition(est.labels_)", "response": "Return a Partition object for each key - value pair in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _py2_and_3_joiner(sep, joinable):\n    if ISPY3:\n        sep = bytes(sep, DEFAULT_ENCODING)\n    joined = sep.join(joinable)\n    return joined.decode(DEFAULT_ENCODING) if ISPY3 else joined", "response": "A function that can be used to join a list of strings in a Py2 and Py3 fashion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _log_thread(self, pipe, queue):\n\n        # thread function to log subprocess output (LOG is a queue)\n        def enqueue_output(out, q):\n            for line in iter(out.readline, b''):\n                q.put(line.rstrip())\n            out.close()\n\n        # start thread\n        t = threading.Thread(target=enqueue_output,\n                                  args=(pipe, queue))\n        t.daemon = True  # thread dies with the program\n        t.start()\n        self.threads.append(t)", "response": "Start a thread logging output from a pipe and a queue"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _search_for_executable(self, executable):\n        if os.path.isfile(executable):\n            return os.path.abspath(executable)\n        else:\n            envpath = os.getenv('PATH')\n            if envpath is None:\n                return\n            for path in envpath.split(os.pathsep):\n                exe = os.path.join(path, executable)\n                if os.path.isfile(exe):\n                    return os.path.abspath(exe)", "response": "Search for the executable and return the absolute path to it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_stderr(self, tail=None):\n        if self.finished(): \n            self.join_threads()\n        while not self.stderr_q.empty():\n            self.stderr_l.append(self.stderr_q.get_nowait())\n        if tail is None:\n            tail = len(self.stderr_l)\n        return _py2_and_3_joiner('\\n', self.stderr_l[:tail])", "response": "Get the current total output written to standard error."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_stdout(self, tail=None):\n        if self.finished(): \n            self.join_threads()\n        while not self.stdout_q.empty():\n            self.stdout_l.append(self.stdout_q.get_nowait())\n        if tail is None:\n            tail = len(self.stdout_l)\n        return _py2_and_3_joiner('\\n', self.stdout_l[:tail])", "response": "Get the current total output written to standard output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kill(self):\n        if self.running():\n            if self.verbose:\n                print('Killing {} with PID {}'.format(self.exe, self.process.pid))\n            self.process.kill()\n\n            # Threads *should* tidy up after themselves, but we do it explicitly\n            self.join_threads()", "response": "Kill the running process if there is one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntemplating for Tika app commands", "response": "def _command_template(self, switches, objectInput=None):\n        \"\"\"Template for Tika app commands\n\n        Args:\n            switches (list): list of switches to Tika app Jar\n            objectInput (object): file object/standard input to analyze\n\n        Return:\n            Standard output data (unicode Python 2, str Python 3)\n        \"\"\"\n        command = [\"java\", \"-jar\", self.file_jar, \"-eUTF-8\"]\n        if self.memory_allocation:\n            command.append(\"-Xmx{}\".format(self.memory_allocation))\n        command.extend(switches)\n\n        if not objectInput:\n            objectInput = subprocess.PIPE\n\n        log.debug(\"Subprocess command: {}\".format(\", \".join(command)))\n\n        if six.PY2:\n            with open(os.devnull, \"w\") as devnull:\n                out = subprocess.Popen(\n                    command,\n                    stdin=objectInput,\n                    stdout=subprocess.PIPE,\n                    stderr=devnull)\n\n        elif six.PY3:\n            out = subprocess.Popen(\n                command,\n                stdin=objectInput,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.DEVNULL)\n\n        stdoutdata, _ = out.communicate()\n        return stdoutdata.decode(\"utf-8\").strip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detect_content_type(self, path=None, payload=None, objectInput=None):\n        # From Python detection content type from stdin doesn't work TO FIX\n        if objectInput:\n            message = \"Detection content type with file object is not stable.\"\n            log.exception(message)\n            raise TikaAppError(message)\n\n        f = file_path(path, payload, objectInput)\n        switches = [\"-d\", f]\n        result = self._command_template(switches).lower()\n        return result, path, f", "response": "Detect the content type of passed file or payload."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn only the text content of passed file.", "response": "def extract_only_content(self, path=None, payload=None, objectInput=None):\n        \"\"\"\n        Return only the text content of passed file.\n        These parameters are in OR. Only one of them can be analyzed.\n\n        Args:\n            path (string): Path of file to analyze\n            payload (string): Payload base64 to analyze\n            objectInput (object): file object/standard input to analyze\n\n        Returns:\n            text of file passed (string)\n        \"\"\"\n        if objectInput:\n            switches = [\"-t\"]\n            result = self._command_template(switches, objectInput)\n            return result, True, None\n        else:\n            f = file_path(path, payload)\n            switches = [\"-t\", f]\n            result = self._command_template(switches)\n            return result, path, f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_all_content(\n        self,\n        path=None,\n        payload=None,\n        objectInput=None,\n        pretty_print=False,\n        convert_to_obj=False,\n    ):\n        \"\"\"\n        This function returns a JSON of all contents and\n        metadata of passed file\n\n        Args:\n            path (string): Path of file to analyze\n            payload (string): Payload base64 to analyze\n            objectInput (object): file object/standard input to analyze\n            pretty_print (boolean): If True adds newlines and whitespace,\n                                    for better readability\n            convert_to_obj (boolean): If True convert JSON in object\n        \"\"\"\n        f = file_path(path, payload, objectInput)\n        switches = [\"-J\", \"-t\", \"-r\", f]\n        if not pretty_print:\n            switches.remove(\"-r\")\n        result = self._command_template(switches)\n\n        if result and convert_to_obj:\n            result = json.loads(result, encoding=\"utf-8\")\n\n        return result, path, f", "response": "This function returns a JSON of all contents and metadata of passed file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a file path, payload or file object, it writes file on disk and returns the temp path. Args: path (string): path of real file payload(string): payload in base64 of file objectInput (object): file object/standard input to analyze Returns: Path of file", "response": "def file_path(path=None, payload=None, objectInput=None):\n    \"\"\"\n    Given a file path, payload or file object, it writes file on disk and\n    returns the temp path.\n\n    Args:\n        path (string): path of real file\n        payload(string): payload in base64 of file\n        objectInput (object): file object/standard input to analyze\n\n    Returns:\n        Path of file\n    \"\"\"\n    f = path if path else write_payload(payload, objectInput)\n\n    if not os.path.exists(f):\n        msg = \"File {!r} does not exist\".format(f)\n        log.exception(msg)\n        raise TikaAppFilePathError(msg)\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_subject(self, data):\n        if not isinstance(data, Mapping):\n            return None\n        if data.get(self.subject):\n            return data.get(self.subject)\n        return uuid.uuid4().urn", "response": "Try to get a unique ID from the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the reverse of the resource.", "response": "def reverse(self):\n        \"\"\" Reverse links make sense for object to object links where we later\n        may want to also query the reverse of the relationship, e.g. when obj1\n        is a child of obj2, we want to infer that obj2 is a parent of obj1. \"\"\"\n        name = self.schema.get('rdfReverse')\n        if name is not None:\n            return name\n        if self.parent is not None and self.parent.is_array:\n            return self.parent.reverse"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _triplify_object(self, data, parent):\n        subject = self.get_subject(data)\n        if self.path:\n            yield (subject, TYPE_SCHEMA, self.path, TYPE_SCHEMA)\n\n        if parent is not None:\n            yield (parent, self.predicate, subject, TYPE_LINK)\n            if self.reverse is not None:\n                yield (subject, self.reverse, parent, TYPE_LINK)\n\n        for prop in self.properties:\n            for res in prop.triplify(data.get(prop.name), subject):\n                yield res", "response": "Yields a list of statements that represent the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a node ID return an object the information available about this node.", "response": "def objectify(self, load, node, depth=2, path=None):\n        \"\"\" Given a node ID, return an object the information available about\n        this node. This accepts a loader function as it's first argument, which\n        is expected to return all tuples of (predicate, object, source) for\n        the given subject. \"\"\"\n        if path is None:\n            path = set()\n\n        if self.is_object:\n            if depth < 1:\n                return\n            return self._objectify_object(load, node, depth, path)\n        elif self.is_array:\n            if depth < 1:\n                return\n            return [self.items.objectify(load, node, depth, path)]\n        else:\n            return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns matching results as JSON", "response": "def get_json(request, token):\n    \"\"\"Return matching results as JSON\"\"\"\n    result = []\n    searchtext = request.GET['q']\n    if len(searchtext) >= 3:\n        pickled = _simple_autocomplete_queryset_cache.get(token, None)\n        if pickled is not None:\n            app_label, model_name, query = pickle.loads(pickled)\n            model = apps.get_model(app_label, model_name)\n            queryset = QuerySet(model=model, query=query)\n            fieldname = get_search_fieldname(model)\n            di = {'%s__istartswith' % fieldname: searchtext}\n            app_label_model = '%s.%s' % (app_label, model_name)\n            max_items = get_setting(app_label_model, 'max_items', 10)\n            items = queryset.filter(**di).order_by(fieldname)[:max_items]\n\n            # Check for duplicate strings\n            counts = {}\n            for item in items:\n                if hasattr(item, \"__unicode__\"):\n                    key = item.__unicode__()\n                else:\n                    key = str(item)\n                #key = unicode(item)\n                counts.setdefault(key, 0)\n                counts[key] += 1\n\n            # Assemble result set\n            for item in items:\n                #key = value = unicode(item)\n                if hasattr(item, \"__unicode__\"):\n                    key = value = item.__unicode__()\n                else:\n                    key = value = str(item)\n                value = getattr(item, fieldname)\n                if counts[key] > 1:\n                    func = get_setting(\n                        app_label_model,\n                        'duplicate_format_function',\n                        lambda obj, model, content_type: content_type.name\n                    )\n                    content_type = ContentType.objects.get_for_model(model)\n                    value = '%s (%s)' % (value, func(item, model, content_type))\n                result.append((item.id, value))\n\n        else:\n            result = 'CACHE_MISS'\n\n    return HttpResponse(json.dumps(result))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a Dash FE file to a dictionary.", "response": "def _dash_f_e_to_dict(self, info_filename, tree_filename):\n        \"\"\"\n        Raxml provides an option to fit model params to a tree,\n        selected with -f e.\n        The output is different and needs a different parser.\n        \"\"\"\n        with open(info_filename) as fl:\n            models, likelihood, partition_params = self._dash_f_e_parser.parseFile(fl).asList()\n\n        with open(tree_filename) as fl:\n            tree = fl.read()\n\n        d = {'likelihood': likelihood, 'ml_tree': tree, 'partitions': {}}\n\n        for model, params in zip(models, partition_params):\n            subdict = {}\n            index, name, _, alpha, rates, freqs = params\n            subdict['alpha'] = alpha\n            subdict['name'] = name\n            subdict['rates'] = rates\n            subdict['frequencies'] = freqs\n            subdict['model'] = model\n            d['partitions'][index] = subdict\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses raxml output and return a dict containing the info and tree entries.", "response": "def to_dict(self, info_filename, tree_filename, dash_f_e=False):\n        \"\"\"\n        Parse raxml output and return a dict\n        Option dash_f_e=True will parse the output of a raxml -f e run,\n        which has different output\n        \"\"\"\n        logger.debug('info_filename: {} {}'\n                     .format(info_filename, '(FOUND)' if os.path.exists(info_filename) else '(NOT FOUND)'))\n        logger.debug('tree_filename: {} {}'\n                     .format(tree_filename, '(FOUND)' if os.path.exists(tree_filename) else '(NOT FOUND)'))\n        if dash_f_e:\n            return self._dash_f_e_to_dict(info_filename, tree_filename)\n        else:\n            return self._to_dict(info_filename, tree_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef freader(filename, gz=False, bz=False):\n\n    filecheck(filename)\n    if filename.endswith('.gz'):\n        gz = True\n    elif filename.endswith('.bz2'):\n        bz = True\n\n    if gz:\n        return gzip.open(filename, 'rb')\n    elif bz:\n        return bz2.BZ2File(filename, 'rb')\n    else:\n        return io.open(filename, 'rb')", "response": "Returns a filereader object that can handle gzipped input"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a filewriter object that can write plain or gzipped output.", "response": "def fwriter(filename, gz=False, bz=False):\n    \"\"\" Returns a filewriter object that can write plain or gzipped output.\n    If gzip or bzip2 compression is asked for then the usual filename extension will be added.\"\"\"\n\n    if filename.endswith('.gz'):\n        gz = True\n    elif filename.endswith('.bz2'):\n        bz = True\n\n    if gz:\n        if not filename.endswith('.gz'):\n            filename += '.gz'\n        return gzip.open(filename, 'wb')\n    elif bz:\n        if not filename.endswith('.bz2'):\n            filename += '.bz2'\n        return bz2.BZ2File(filename, 'w')\n    else:\n        return open(filename, 'w')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of files matched by all extensions in the list", "response": "def glob_by_extensions(directory, extensions):\n    \"\"\" Returns files matched by all extensions in the extensions list \"\"\"\n    directorycheck(directory)\n    files = []\n    xt = files.extend\n    for ex in extensions:\n        xt(glob.glob('{0}/*.{1}'.format(directory, ex)))\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the top n lines of a file", "response": "def head(filename, n=10):\n    \"\"\" prints the top `n` lines of a file \"\"\"\n    with freader(filename) as fr:\n        for _ in range(n):\n            print(fr.readline().strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef locate_file(filename, env_var='', directory=''):\n    f = locate_by_env(filename, env_var) or locate_by_dir(filename, directory)\n    return os.path.abspath(f) if can_locate(f) else None", "response": "Locates a file given an environment variable or directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef channels_for_role(role):\n    chans = []\n    chans_names = []\n    # default channels\n    if role == \"public\":\n        if ENABLE_PUBLIC_CHANNEL is True:\n            chan = dict(slug=PUBLIC_CHANNEL, path=None)\n            chans.append(chan)\n            chans_names.append(PUBLIC_CHANNEL)\n    elif role == \"users\":\n        if ENABLE_USERS_CHANNEL is True:\n            chan = _ensure_channel_is_private(DEFAULT_USERS_CHANNEL)\n            chan = dict(slug=chan, path=None)\n            chans.append(chan)\n            chans_names.append(DEFAULT_USERS_CHANNEL)\n    elif role == \"staff\":\n        if ENABLE_STAFF_CHANNEL is True:\n            chan = _ensure_channel_is_private(DEFAULT_STAFF_CHANNEL)\n            chan = dict(slug=chan, path=None)\n            chans.append(chan)\n            chans_names.append(DEFAULT_STAFF_CHANNEL)\n    elif role == \"superuser\":\n        if ENABLE_SUPERUSER_CHANNEL is True:\n            chan = _ensure_channel_is_private(DEFAULT_SUPERUSER_CHANNEL)\n            chan = dict(slug=chan, path=None)\n            chans.append(chan)\n            chans_names.append(DEFAULT_SUPERUSER_CHANNEL)\n    # declared channels\n    if role == \"public\":\n        if len(PUBLIC_CHANNELS) > 0:\n            for chanconf in PUBLIC_CHANNELS:\n                chan = _check_chanconf(chanconf, False)\n                chans.append(chan)\n                chans_names.append(chan[\"slug\"])\n    elif role == \"users\":\n        if len(USERS_CHANNELS) > 0:\n            for chanconf in USERS_CHANNELS:\n                chan = _check_chanconf(chanconf)\n                chans.append(chan)\n                chans_names.append(chan[\"slug\"])\n    elif role == \"staff\":\n        if len(STAFF_CHANNELS) > 0:\n            for chanconf in STAFF_CHANNELS:\n                chan = _check_chanconf(chanconf)\n                chans.append(chan)\n                chans_names.append(chan[\"slug\"])\n    elif role == \"superuser\":\n        if len(SUPERUSER_CHANNELS) > 0:\n            for chanconf in SUPERUSER_CHANNELS:\n                chan = _check_chanconf(chanconf)\n                chans.append(chan)\n                chans_names.append(chan[\"slug\"])\n    return chans, chans_names", "response": "Get the channels for a given role."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nraises error if length is not in interval [ 0 edge. length )", "response": "def edge_length_check(length, edge):\n    \"\"\" Raises error if length is not in interval [0, edge.length] \"\"\"\n    try:\n        assert 0 <= length <= edge.length\n    except AssertionError:\n        if length < 0:\n            raise TreeError('Negative edge-lengths are disallowed')\n        raise TreeError(\n            'This edge isn\\'t long enough to prune at length {0}\\n'\n            '(Edge length = {1})'.format(length, edge.length))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the log of the descendant rate of the parent rate with respect to the given autocorrelation parameter.", "response": "def logn_correlated_rate(parent_rate, branch_length, autocorrel_param, size=1):\n    \"\"\"\n    The log of the descendent rate, ln(Rd), is ~ N(mu, bl*ac), where\n    the variance = bl*ac = branch_length * autocorrel_param, and mu is set\n    so that E[Rd] = Rp:\n    E[X] where ln(X) ~ N(mu, sigma^2) = exp(mu+(1/2)*sigma_sq)\n    so Rp = exp(mu+(1/2)*bl*ac),\n    ln(Rp) = mu + (1/2)*bl*ac,\n    ln(Rp) - (1/2)*bl*ac = mu,\n    so ln(Rd) ~ N(ln(Rp) - (1/2)*bl*ac, bl*ac)\n    (NB: Var[Rd] = Rp^2 * (exp(bl*ac)-1),\n         Std[Rd] = Rp * sqrt(exp(bl*ac)-1)\n\n    See: H Kishino, J L Thorne, and W J Bruno (2001)\n    \"\"\"\n    if autocorrel_param <= 0:\n        raise Exception('Autocorrelation parameter must be greater than 0')\n\n    variance = branch_length * autocorrel_param\n    stdev = np.sqrt(variance)\n    ln_descendant_rate = np.random.normal(np.log(parent_rate) - 0.5 * variance,\n                                          scale=stdev, size=size)\n    descendant_rate = np.exp(ln_descendant_rate)\n    return float(descendant_rate) if size == 1 else descendant_rate"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_single_outgroup(self):\n        root_child_nodes = self.tree._tree.seed_node.child_nodes()\n        not_leaves = np.logical_not([n.is_leaf() for n in root_child_nodes])\n        if not_leaves[not_leaves].size <= 1:\n            return [root_child_nodes[np.where(not_leaves)[0]].edge]\n        return []", "response": "Check if there is only one outgroup node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npruning a subtree from the main Tree retaining an edge length", "response": "def prune(self, edge, length=None):\n        \"\"\" Prunes a subtree from the main Tree, retaining an edge length\n        specified by length (defaults to entire length). The length is sanity-\n        checked by edge_length_check, to ensure it is within the bounds\n        [0, edge.length].\n\n        Returns the basal node of the pruned subtree. \"\"\"\n\n        length = length or edge.length\n        edge_length_check(length, edge)\n\n        n = edge.head_node\n        self.tree._tree.prune_subtree(n, suppress_unifurcations=False)\n        n.edge_length = length\n        self.tree._dirty = True\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef regraft(self, edge, node, length=None):\n\n        rootcheck(edge, 'SPR regraft is not allowed on the root edge')\n        length = length or edge.length / 2.  # Length measured from head to tail\n        edge_length_check(length, edge)\n\n        t = edge.tail_node\n        h = edge.head_node\n        new = t.new_child(edge_length=edge.length - length)\n        t.remove_child(h)\n        new.add_child(h)\n        h.edge.length=length\n        new.add_child(node)\n        self.tree._dirty = True\n        self.tree._tree.encode_bipartitions(suppress_unifurcations=True)", "response": "Regrafts a node onto an edge of the Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rspr(self, disallow_sibling_sprs=False,\n             keep_entire_edge=False, rescale=False):\n        \"\"\" Random SPR, with prune and regraft edges chosen randomly, and\n        lengths drawn uniformly from the available edge lengths.\n\n        N1: disallow_sibling_sprs prevents sprs that don't alter the topology\n        of the tree \"\"\"\n\n        starting_length = self.tree._tree.length()\n\n        excl = [self.tree._tree.seed_node.edge]  # exclude r\n        if disallow_sibling_sprs:\n            excl.extend(self._check_single_outgroup())\n        prune_edge, l1 = self.tree.map_event_onto_tree(excl)\n        if keep_entire_edge:\n            l1 = prune_edge.length\n\n        prune_edge_child_nodes = prune_edge.head_node.preorder_iter()\n        excl.extend([node.edge for node in prune_edge_child_nodes])\n\n        if disallow_sibling_sprs:\n            sibs = [node.edge for node in prune_edge.head_node.sister_nodes()]\n            par = prune_edge.tail_node.edge\n            sibs.append(par)\n            for edge in sibs:\n                if edge not in excl:\n                    excl.append(edge)\n            if set(self.tree._tree.preorder_edge_iter()) - set(excl) == set([]):\n                print(repr(self.tree))\n                print(self.tree._tree.as_ascii_plot())\n                # print(edges[prune_edge])\n                raise Exception('No non-sibling sprs available')\n\n        regraft_edge, l2 = self.tree.map_event_onto_tree(excl)\n\n        # edges, nodes, redges, rnodes = self.tree._name_things()\n        # print(edges[prune_edge], l1, edges[regraft_edge], l2)\n        self.spr(prune_edge, l1, regraft_edge, l2)\n        if rescale:\n            self.tree.scale(starting_length / self.tree.length())\n            self.tree._dirty = True", "response": "Random SPR for this tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_exchangeable_nodes(self, n):\n        parent = n.parent_node\n        a, b = random.sample(n.child_nodes(), 2)\n        if parent.parent_node is None:\n            if self.tree.rooted:\n                c, d = random.sample(n.sister_nodes()[0].child_nodes(), 2)\n            else:\n                c, d = random.sample(n.sister_nodes(), 2)\n        else:\n            c = random.choice(n.sister_nodes())\n            d = random.choice(parent.sister_nodes())\n\n        return a, b, c, d", "response": "Returns a list of exchangeable nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ils(self, node, sorting_times=None, force_topology_change=True):\n        # node = '2', par = '1', gpar = '0' -- in above diagram\n        n_2 = node\n        n_1 = n_2.parent_node\n        if n_1 == self.tree._tree.seed_node:\n            logger.warn('Node 1 is the root - calling again on child')\n            self.ils(n_2.child_nodes())\n        n_0 = n_1.parent_node\n        a, b = node.child_nodes()\n        c, = node.sister_nodes()\n\n        ages = [a.age, b.age, c.age, n_2.age, n_1.age, n_0.age]\n\n        # Do topology changes\n        if force_topology_change:\n            swap_mode = random.choice([1, 2])\n        else:\n            swap_mode = random.choice([0, 1, 2])\n\n        if swap_mode == 1:\n            # Exchange 'a' and 'c'\n            n_2.remove_child(a)\n            n_1.remove_child(c)\n            n_2.add_child(c)\n            n_1.add_child(a)\n\n        elif swap_mode == 2:\n            # Exchange 'b' and 'c'\n            n_2.remove_child(b)\n            n_1.remove_child(c)\n            n_2.add_child(c)\n            n_1.add_child(b)\n\n        # Do branch length adjustments\n        # Bounds - between node 0 (upper) and node 1 (lower)\n        min_unsorted_age = n_1.age\n        max_unsorted_age = n_0.age\n        if sorting_times is None:\n            sorting_times = truncated_exponential(max_unsorted_age-min_unsorted_age,\n                                              scale=0.1*(max_unsorted_age-min_unsorted_age),\n                                              sample_size=2) # E(t) = n(n-1)/2, n = 3\n            sorting_times += min_unsorted_age\n            sorting_times = np.array([min_unsorted_age, ages[3]])\n\n        # Adjust node 1 edge length\n        new_n1_age = max(sorting_times)\n        prev_age = ages[4]\n        slide = (new_n1_age - prev_age)\n        if slide < 1e-6:\n            slide = 0\n            new_n1_age = prev_age\n        n_1.edge.length -= slide\n        n_2.edge.length += slide\n\n        # Adjust node 2 edge length\n        new_n2_age = min(sorting_times)\n        prev_age = ages[3]\n        slide = (new_n2_age - prev_age)\n        if slide < 1e-6:\n            slide = 0\n            new_n2_age = prev_age\n        n_2.edge.length -= slide\n\n        # Adjust a, b and c edge lengths\n        if swap_mode == 0:\n            a.edge.length = (new_n2_age - ages[0])\n            b.edge.length = (new_n2_age - ages[1])\n            c.edge.length = (new_n1_age - ages[2])\n\n        elif swap_mode == 1:\n            a.edge.length = (new_n1_age - ages[0])\n            b.edge.length = (new_n2_age - ages[1])\n            c.edge.length = (new_n2_age - ages[2])\n\n        else:\n            a.edge.length = (new_n2_age - ages[0])\n            b.edge.length = (new_n1_age - ages[1])\n            c.edge.length = (new_n2_age - ages[2])\n\n        # used to be .reindex_taxa() before dendropy 4.\n        # migrate_taxon_namespace is recommended migrated function,\n        # but not sure if its even needed anymore.\n        self.tree._tree.migrate_taxon_namespace(self.tree._tree.taxon_namespace)\n\n        self.tree._tree.encode_bipartitions()\n        self._validate()\n        logger.debug(self.tree)", "response": "A constrained and approximation of ILS using nearest - neighbour interchange."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_children(self, inner_edge):\n\n        h = inner_edge.head_node\n        t = inner_edge.tail_node\n        if not self.tree._tree.seed_node == t:\n            original_seed = self.tree._tree.seed_node\n            self.tree._tree.reseed_at(t)\n        else:\n            original_seed = None\n        head_children = h.child_nodes()\n        tail_children = list(set(t.child_nodes()) - {h})  # See N1\n        if original_seed:\n            self.tree._tree.reseed_at(original_seed)\n\n        return {'head': head_children, 'tail': tail_children}", "response": "Given an edge in the tree returns the child nodes of the edge and the tail nodes of the edge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nni(\n            self,\n            edge,\n            head_subtree,\n            tail_subtree,\n    ):\n        \"\"\" *Inplace* Nearest-neighbour interchange (NNI) operation.\n\n        An edge in the tree has two or more subtrees at each end (ends are\n        designated 'head' and 'tail'). The NNI operation exchanges one of the\n        head subtrees for one of the tail subtrees, as follows:\n\n            A      C                        C      A    | Subtree A is exchanged\n             \\    /        +NNI(A,C)         \\    /     | with subtree C.\n              --->        ==========>         --->      |\n             /    \\                          /    \\     |\n            B      D                        B      D\n\n\n        \"\"\"\n\n        # This implementation works on unrooted Trees. If the input Tree is\n        # rooted, the ReversibleDeroot decorator will temporarily unroot the\n        # tree while the NNI is carried out\n\n        original_seed = self.tree._tree.seed_node\n        head = edge.head_node\n        tail = edge.tail_node\n        self.tree._tree.reseed_at(tail)\n        try:\n            assert head_subtree.parent_node == head\n            assert tail_subtree.parent_node == tail\n        except:\n            print(head, tail, head_subtree, tail_subtree)\n            raise\n        head.remove_child(head_subtree)\n        tail.remove_child(tail_subtree)\n        head.add_child(tail_subtree)\n        tail.add_child(head_subtree)\n        self.tree._tree.reseed_at(original_seed)\n        self.tree._tree.encode_bipartitions()\n        self.tree._dirty = True", "response": "This function is used to make a Nearest - Neighbour interchange operation on a tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rnni(self, use_weighted_choice=False, invert_weights=False):\n        if use_weighted_choice:\n            leaves = list(self.tree._tree.leaf_edge_iter())\n            e, _ = self.tree.map_event_onto_tree(excluded_edges=leaves, invert_weights=invert_weights)\n        else:\n            e = random.choice(self.tree.get_inner_edges())\n        children = self.get_children(e)\n        h = random.choice(children['head'])\n        t = random.choice(children['tail'])\n        self.nni(e, h, t)", "response": "Apply a random NNI operation at a randomly selected edge."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef labels(self):\n        return set([n.taxon.label for n in self._tree.leaf_nodes()])", "response": "Returns the set of labels of the tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a set of n labels sampled from the labels of the tree", "response": "def sample_labels(self, n):\n        \"\"\" Returns a set of n labels sampled from the labels of the tree\n        :param n: Number of labels to sample\n        :return: set of randomly sampled labels\n        \"\"\"\n        if n >= len(self):\n            return self.labels\n        sample = random.sample(self.labels, n)\n        return set(sample)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representation of the current node s newick.", "response": "def newick(self):\n        \"\"\"\n        For more control the dendropy method self.as_string('newick', **kwargs)\n        can be used.\n        KWargs include:\n        suppress_internal_node_labels [True/False]\n            - turn on/off bootstrap labels\n        suppress_rooting [True/False]\n            - turn on/off [&U] or [&R] rooting\n              state labels\n        edge_label_compose_func\n            - function to convert edge lengths:\n              takes edge as arg, returns string\n        \"\"\"\n        n = self._tree.as_string('newick',\n                                 suppress_rooting=True,\n                                 suppress_internal_node_labels=True)\n        if n:\n            return n.strip(';\\n') + ';'\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef phylotree(self):\n        if not self._phylotree or self._dirty:\n            try:\n                if ISPY3:\n                    self._phylotree = PhyloTree(self.newick.encode(), self.rooted)\n                else:\n                    self._phylotree = PhyloTree(self.newick, self.rooted)\n            except ValueError:\n                logger.error('Couldn\\'t convert to C++ PhyloTree -- are there bootstrap values?')\n            self._dirty = False\n        return self._phylotree", "response": "Returns the PhyloTree object corresponding to this tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bifurcate_base(cls, newick):\n        t = cls(newick)\n        t._tree.resolve_polytomies()\n        return t.newick", "response": "Rewrites a newick string so that the base is a bifurcation tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trifurcate_base(cls, newick):\n        t = cls(newick)\n        t._tree.deroot()\n        return t.newick", "response": "Rewrites a newick string so that the base is a trifurcation tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of the internal edges of the tree.", "response": "def get_inner_edges(self):\n        \"\"\" Returns a list of the internal edges of the tree. \"\"\"\n        inner_edges = [e for e in self._tree.preorder_edge_iter() if e.is_internal()\n                       and e.head_node and e.tail_node]\n        return inner_edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the intersection of the taxon sets of two Trees", "response": "def intersection(self, other):\n        \"\"\" Returns the intersection of the taxon sets of two Trees \"\"\"\n        taxa1 = self.labels\n        taxa2 = other.labels\n        return taxa1 & taxa2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator that yields the nodes of the tree in postorder.", "response": "def postorder(self, skip_seed=False):\n        \"\"\"\n        Return a generator that yields the nodes of the tree in postorder.\n        If skip_seed=True then the root node is not included.\n        \"\"\"\n        for node in self._tree.postorder_node_iter():\n            if skip_seed and node is self._tree.seed_node:\n                continue\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a generator that yields the nodes of the tree in preorder.", "response": "def preorder(self, skip_seed=False):\n        \"\"\"\n        Return a generator that yields the nodes of the tree in preorder.\n        If skip_seed=True then the root node is not included.\n        \"\"\"\n        for node in self._tree.preorder_node_iter():\n            if skip_seed and node is self._tree.seed_node:\n                continue\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprune the Tree to just the taxon set given in subset.", "response": "def prune_to_subset(self, subset, inplace=False):\n        \"\"\" Prunes the Tree to just the taxon set given in `subset` \"\"\"\n        if not subset.issubset(self.labels):\n            print('\"subset\" is not a subset')\n            return\n        if not inplace:\n            t = self.copy()\n        else:\n            t = self\n        t._tree.retain_taxa_with_labels(subset)\n        t._tree.encode_bipartitions()\n        t._dirty = True\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef randomise_branch_lengths(\n            self,\n            i=(1, 1),\n            l=(1, 1),\n            distribution_func=random.gammavariate,\n            inplace=False,\n    ):\n        \"\"\" Replaces branch lengths with values drawn from the specified\n        distribution_func. Parameters of the distribution are given in the\n        tuples i and l, for interior and leaf nodes respectively. \"\"\"\n\n        if not inplace:\n            t = self.copy()\n        else:\n            t = self\n\n        for n in t._tree.preorder_node_iter():\n            if n.is_internal():\n                n.edge.length = max(0, distribution_func(*i))\n            else:\n                n.edge.length = max(0, distribution_func(*l))\n        t._dirty = True\n        return t", "response": "Returns a new tree with random branch lengths drawn from the specified distribution_func."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshuffles the leaf labels but doesn t alter the tree structure", "response": "def randomise_labels(\n            self,\n            inplace=False,\n    ):\n        \"\"\" Shuffles the leaf labels, but doesn't alter the tree structure \"\"\"\n\n        if not inplace:\n            t = self.copy()\n        else:\n            t = self\n\n        names = list(t.labels)\n        random.shuffle(names)\n        for l in t._tree.leaf_node_iter():\n            l.taxon._label = names.pop()\n        t._dirty = True\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reversible_deroot(self):\n        root_edge = self._tree.seed_node.edge\n        lengths = dict([(edge, edge.length) for edge\n                        in self._tree.seed_node.incident_edges() if edge is not root_edge])\n        self._tree.deroot()\n        reroot_edge = (set(self._tree.seed_node.incident_edges())\n                       & set(lengths.keys())).pop()\n        self._tree.encode_bipartitions()\n        self._dirty = True\n        return (reroot_edge, reroot_edge.length - lengths[reroot_edge],\n                lengths[reroot_edge])", "response": "Return the edge that was originally rooted and the length of e1 and length of e2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattaching rates to each node according to autocorrelated lognormal model from Kishino et al. 2001 or autocorrelated exponential model from Kishino et al. 2001.", "response": "def autocorrelated_relaxed_clock(self, root_rate, autocorrel,\n                                     distribution='lognormal'):\n        \"\"\"\n        Attaches rates to each node according to autocorrelated lognormal\n        model from Kishino et al.(2001), or autocorrelated exponential\n        \"\"\"\n        optioncheck(distribution, ['exponential', 'lognormal'])\n\n        if autocorrel == 0:\n            for node in self._tree.preorder_node_iter():\n                node.rate = root_rate\n            return\n\n        for node in self._tree.preorder_node_iter():\n            if node == self._tree.seed_node:\n                node.rate = root_rate\n            else:\n                parent_rate = node.parent_node.rate\n                bl = node.edge_length\n                if distribution == 'lognormal':\n                    node.rate = logn_correlated_rate(parent_rate, bl,\n                                                     autocorrel)\n                else:\n                    node.rate = np.random.exponential(parent_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rlgt(self, time=None, times=1,\n             disallow_sibling_lgts=False):\n        \"\"\" Uses class LGT to perform random lateral gene transfer on\n        ultrametric tree \"\"\"\n\n        lgt = LGT(self.copy())\n        for _ in range(times):\n            lgt.rlgt(time, disallow_sibling_lgts)\n        return lgt.tree", "response": "Perform random lateral gene transfer on the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply a NNI operation on a randomly chosen edge.", "response": "def rnni(self, times=1, **kwargs):\n        \"\"\" Applies a NNI operation on a randomly chosen edge.\n        keyword args: use_weighted_choice (True/False) weight the random edge selection by edge length\n                      transform (callable) transforms the edges using this function, prior to weighted selection\n        \"\"\"\n\n        nni = NNI(self.copy())\n        for _ in range(times):\n            nni.rnni(**kwargs)\n        # nni.reroot_tree()\n        return nni.tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmultiplies all branch lengths by factor.", "response": "def scale(self, factor, inplace=True):\n        \"\"\" Multiplies all branch lengths by factor. \"\"\"\n        if not inplace:\n            t = self.copy()\n        else:\n            t = self\n        t._tree.scale_edges(factor)\n        t._dirty = True\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a copy of the tree with all edge lengths set to None", "response": "def strip(self, inplace=False):\n        \"\"\" Sets all edge lengths to None \"\"\"\n        if not inplace:\n            t = self.copy()\n        else:\n            t = self\n        for e in t._tree.preorder_edge_iter():\n            e.length = None\n        t._dirty = True\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef translate(self, dct):\n        new_tree = self.copy()\n        for leaf in new_tree._tree.leaf_node_iter():\n            curr_name = leaf.taxon.label\n            leaf.taxon.label = dct.get(curr_name, curr_name)\n        return new_tree", "response": "Translate leaf names using a dictionary of current names -> updated names\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _name_things(self):\n        edges = {}\n        nodes = {None: 'root'}\n        for n in self._tree.postorder_node_iter():\n            nodes[n] = '.'.join([str(x.taxon) for x in n.leaf_nodes()])\n        for e in self._tree.preorder_edge_iter():\n            edges[e] = ' ---> '.join([nodes[e.tail_node], nodes[e.head_node]])\n\n        r_edges = {value: key for key, value in edges.items()}\n        r_nodes = {value: key for key, value in nodes.items()}\n        return edges, nodes, r_edges, r_nodes", "response": "Easy names for debugging"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gene_tree(\n            self,\n            scale_to=None,\n            population_size=1,\n            trim_names=True,\n    ):\n        \"\"\" Using the current tree object as a species tree, generate a gene\n        tree using the constrained Kingman coalescent process from dendropy. The\n        species tree should probably be a valid, ultrametric tree, generated by\n        some pure birth, birth-death or coalescent process, but no checks are\n        made. Optional kwargs are: -- scale_to, which is a floating point value\n        to scale the total tree tip-to-root length to, -- population_size, which\n        is a floating point value which all branch lengths will be divided by to\n        convert them to coalescent units, and -- trim_names, boolean, defaults\n        to true, trims off the number which dendropy appends to the sequence\n        name \"\"\"\n\n        tree = self.template or self.yule()\n\n        for leaf in tree._tree.leaf_node_iter():\n            leaf.num_genes = 1\n\n        dfr = tree._tree.seed_node.distance_from_root()\n        dft = tree._tree.seed_node.distance_from_tip()\n        tree_height = dfr + dft\n\n        if scale_to:\n            population_size = tree_height / scale_to\n\n        for edge in tree._tree.preorder_edge_iter():\n            edge.pop_size = population_size\n\n        gene_tree = dpy.simulate.treesim.constrained_kingman_tree(tree._tree)[0]\n\n        if trim_names:\n            for leaf in gene_tree.leaf_node_iter():\n                leaf.taxon.label = leaf.taxon.label.replace('\\'', '').split('_')[0]\n\n        # Dendropy changed its API\n        return {'gene_tree': tree.__class__(gene_tree.as_string('newick', suppress_rooting=True).strip(';\\n') + ';'),\n                'species_tree': tree}", "response": "Generate a gene tree from the current tree object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits a model with a particular estimation routine.", "response": "def fit(self, ini_betas=None, tol=1.0e-6, max_iter=200, solve='iwls'):\n        \"\"\"\n        Method that fits a model with a particular estimation routine.\n\n        Parameters\n        ----------\n\n        ini_betas     : array\n                        k*1, initial coefficient values, including constant.\n                        Default is None, which calculates initial values during\n                        estimation.\n        tol:            float\n                        Tolerence for estimation convergence.\n        max_iter       : integer\n                        Maximum number of iterations if convergence not\n                        achieved.\n        solve         :string\n                       Technique to solve MLE equations.\n                       'iwls' = iteratively (re)weighted least squares (default)\n        \"\"\"\n        self.fit_params['ini_betas'] = ini_betas\n        self.fit_params['tol'] = tol\n        self.fit_params['max_iter'] = max_iter\n        self.fit_params['solve'] = solve\n        if solve.lower() == 'iwls':\n            params, predy, w, n_iter = iwls(\n                self.y, self.X, self.family, self.offset, self.y_fix, ini_betas, tol, max_iter)\n            self.fit_params['n_iter'] = n_iter\n        return GLMResults(self, params.flatten(), predy, w)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deriv2(self, p):\n        from statsmodels.tools.numdiff import approx_fprime_cs\n        # TODO: workaround proplem with numdiff for 1d\n        return np.diag(approx_fprime_cs(p, self.deriv))", "response": "Second derivative of the link function g'' p implemented through numerical differentiation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inverse(self, z):\n        z = np.asarray(z)\n        t = np.exp(-z)\n        return 1. / (1. + t)", "response": "Inverse of the logit transform\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inverse(self, z):\n\n        p = np.power(z, 1. / self.power)\n        return p", "response": "Inverse of the power transform link function\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deriv(self, p):\n        return self.power * np.power(p, self.power - 1)", "response": "Derivative of the power transform of the array - like\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsecond derivative of the power transform Parameters ---------- p : array-like Mean parameters Returns -------- g''(p) : array Second derivative of the power transform of `p` Notes ----- g''(`p`) = `power` * (`power` - 1) * `p`**(`power` - 2)", "response": "def deriv2(self, p):\n        \"\"\"\n        Second derivative of the power transform\n\n        Parameters\n        ----------\n        p : array-like\n            Mean parameters\n\n        Returns\n        --------\n        g''(p) : array\n            Second derivative of the power transform of `p`\n\n        Notes\n        -----\n        g''(`p`) = `power` * (`power` - 1) * `p`**(`power` - 2)\n        \"\"\"\n        return self.power * (self.power - 1) * np.power(p, self.power - 2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the inverse derivative of the power transform on the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm of the logarithm.", "response": "def inverse_deriv(self, z):\n        \"\"\"\n        Derivative of the inverse of the power transform\n\n        Parameters\n        ----------\n        z : array-like\n            `z` is usually the linear predictor for a GLM or GEE model.\n\n        Returns\n        -------\n        g^(-1)'(z) : array\n            The value of the derivative of the inverse of the power transform\n        function\n        \"\"\"\n        return np.power(z, (1 - self.power)/self.power) / self.power"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deriv2(self, p):\n        from statsmodels.tools.numdiff import approx_fprime\n        p = np.atleast_1d(p)\n        # Note: special function for norm.ppf does not support complex\n        return np.diag(approx_fprime(p, self.deriv, centered=True))", "response": "Second derivative of the link function g'' p"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nseconding derivative of the Cauchy link function. Parameters ---------- p: array-like Probabilities Returns ------- g''(p) : array Value of the second derivative of Cauchy link function at `p`", "response": "def deriv2(self, p):\n        \"\"\"\n        Second derivative of the Cauchy link function.\n\n        Parameters\n        ----------\n        p: array-like\n            Probabilities\n\n        Returns\n        -------\n        g''(p) : array\n            Value of the second derivative of Cauchy link function at `p`\n        \"\"\"\n        a = np.pi * (p - 0.5)\n        d2 = 2 * np.pi**2 * np.sin(a) / np.cos(a)**3\n        return d2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nseconding derivative of the C-Log-Log ink function Parameters ---------- p : array-like Mean parameters Returns ------- g''(p) : array The second derivative of the CLogLog link function", "response": "def deriv2(self, p):\n        \"\"\"\n        Second derivative of the C-Log-Log ink function\n\n        Parameters\n        ----------\n        p : array-like\n            Mean parameters\n\n        Returns\n        -------\n        g''(p) : array\n            The second derivative of the CLogLog link function\n        \"\"\"\n        p = self._clean(p)\n        fl = np.log(1 - p)\n        d2 = -1 / ((1 - p)**2 * fl)\n        d2 *= 1 + 1 / fl\n        return d2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deriv2(self,p):\n        '''\n        Second derivative of the negative binomial link function.\n\n        Parameters\n        ----------\n        p : array-like\n            Mean parameters\n\n        Returns\n        -------\n        g''(p) : array\n            The second derivative of the negative binomial transform link\n            function\n\n        Notes\n        -----\n        g''(x) = -(1+2*alpha*x)/(x+alpha*x^2)^2\n        '''\n        numer = -(1 + 2 * self.alpha * p)\n        denom = (p + self.alpha * p**2)**2\n        return numer / denom", "response": "Derivative of the negative binomial transform linkon time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the derivative of the inverse of the negative binomial transform at the given time - domain z.", "response": "def inverse_deriv(self, z):\n        '''\n        Derivative of the inverse of the negative binomial transform\n\n        Parameters\n        -----------\n        z : array-like\n            Usually the linear predictor for a GLM or GEE model\n\n        Returns\n        -------\n        g^(-1)'(z) : array\n            The value of the derivative of the inverse of the negative\n            binomial link\n        '''\n        t = np.exp(z)\n        return t / (self.alpha * (1-t)**2)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef databases(self):\n        try:\n            return self._databases\n        except AttributeError:\n            self._databases = self.einfo().databases\n            return self._databases", "response": "get a list of databases available from eutils"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the einfo endpoint", "response": "def einfo(self, db=None):\n        \"\"\"query the einfo endpoint\n\n        :param db: string (optional)\n        :rtype: EInfo or EInfoDB object\n\n        If db is None, the reply is a list of databases, which is returned\n        in an EInfo object (which has a databases() method).\n\n        If db is not None, the reply is information about the specified\n        database, which is returned in an EInfoDB object.  (Version 2.0\n        data is automatically requested.)\n        \"\"\"\n\n        if db is None:\n            return EInfoResult(self._qs.einfo()).dblist\n        return EInfoResult(self._qs.einfo({'db': db, 'version': '2.0'})).dbinfo"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef esearch(self, db, term):\n        esr = ESearchResult(self._qs.esearch({'db': db, 'term': term}))\n        if esr.count > esr.retmax:\n            logger.warning(\"NCBI found {esr.count} results, but we truncated the reply at {esr.retmax}\"\n                        \" results; see https://github.com/biocommons/eutils/issues/124/\".format(esr=esr))\n        return esr", "response": "query the esearch endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef efetch(self, db, id):\n        db = db.lower()\n        xml = self._qs.efetch({'db': db, 'id': str(id)})\n        doc = le.XML(xml)\n        if db in ['gene']:\n            return EntrezgeneSet(doc)\n        if db in ['nuccore', 'nucest', 'protein']:\n            # TODO: GBSet is misnamed; it should be GBSeq and get the GBSeq XML node as root (see gbset.py)\n            return GBSet(doc)\n        if db in ['pubmed']:\n            return PubmedArticleSet(doc)\n        if db in ['snp']:\n            return ExchangeSet(xml)\n        if db in ['pmc']:\n            return PubmedCentralArticleSet(doc)\n        raise EutilsError('database {db} is not currently supported by eutils'.format(db=db))", "response": "query the efetch endpoint"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handleEvent(self, eventObj):\n\n        if eventObj.type not in (MOUSEMOTION, MOUSEBUTTONUP, MOUSEBUTTONDOWN) or not self._visible:\n            # The button only cares bout mouse-related events (or no events, if it is invisible)\n            return []\n\n        retVal = []\n\n        hasExited = False\n        if not self.mouseOverButton and self._rect.collidepoint(eventObj.pos):\n            # if mouse has entered the button:\n            self.mouseOverButton = True\n            self.mouseEnter(eventObj)\n            retVal.append('enter')\n        elif self.mouseOverButton and not self._rect.collidepoint(eventObj.pos):\n            # if mouse has exited the button:\n            self.mouseOverButton = False\n            hasExited = True # call mouseExit() later, since we want mouseMove() to be handled before mouseExit()\n\n        if self._rect.collidepoint(eventObj.pos):\n            # if mouse event happened over the button:\n            if eventObj.type == MOUSEMOTION:\n                self.mouseMove(eventObj)\n                retVal.append('move')\n            elif eventObj.type == MOUSEBUTTONDOWN:\n                self.buttonDown = True\n                self.lastMouseDownOverButton = True\n                self.mouseDown(eventObj)\n                retVal.append('down')\n        else:\n            if eventObj.type in (MOUSEBUTTONUP, MOUSEBUTTONDOWN):\n                # if an up/down happens off the button, then the next up won't cause mouseClick()\n                self.lastMouseDownOverButton = False\n\n        # mouse up is handled whether or not it was over the button\n        doMouseClick = False\n        if eventObj.type == MOUSEBUTTONUP:\n            if self.lastMouseDownOverButton:\n                doMouseClick = True\n            self.lastMouseDownOverButton = False\n\n            if self.buttonDown:\n                self.buttonDown = False\n                self.mouseUp(eventObj)\n                retVal.append('up')\n\n            if doMouseClick:\n                self.buttonDown = False\n                self.mouseClick(eventObj)\n                retVal.append('click')\n\n        if hasExited:\n            self.mouseExit(eventObj)\n            retVal.append('exit')\n\n        return retVal", "response": "This method handles mouse events and returns a list of the related items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef draw(self, surfaceObj):\n        if self._visible:\n            if self.buttonDown:\n                surfaceObj.blit(self.surfaceDown, self._rect)\n            elif self.mouseOverButton:\n                surfaceObj.blit(self.surfaceHighlight, self._rect)\n            else:\n                surfaceObj.blit(self.surfaceNormal, self._rect)", "response": "Blit the current button s appearance to the surface object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nredrawing the button s Surface object. Call this method when the button has changed appearance.", "response": "def _update(self):\n        \"\"\"Redraw the button's Surface object. Call this method when the button has changed appearance.\"\"\"\n        if self.customSurfaces:\n            self.surfaceNormal    = pygame.transform.smoothscale(self.origSurfaceNormal, self._rect.size)\n            self.surfaceDown      = pygame.transform.smoothscale(self.origSurfaceDown, self._rect.size)\n            self.surfaceHighlight = pygame.transform.smoothscale(self.origSurfaceHighlight, self._rect.size)\n            return\n\n        w = self._rect.width # syntactic sugar\n        h = self._rect.height # syntactic sugar\n\n        # fill background color for all buttons\n        self.surfaceNormal.fill(self.bgcolor)\n        self.surfaceDown.fill(self.bgcolor)\n        self.surfaceHighlight.fill(self.bgcolor)\n\n        # draw caption text for all buttons\n        captionSurf = self._font.render(self._caption, True, self.fgcolor, self.bgcolor)\n        captionRect = captionSurf.get_rect()\n        captionRect.center = int(w / 2), int(h / 2)\n        self.surfaceNormal.blit(captionSurf, captionRect)\n        self.surfaceDown.blit(captionSurf, captionRect)\n\n        # draw border for normal button\n        pygame.draw.rect(self.surfaceNormal, BLACK, pygame.Rect((0, 0, w, h)), 1) # black border around everything\n        pygame.draw.line(self.surfaceNormal, WHITE, (1, 1), (w - 2, 1))\n        pygame.draw.line(self.surfaceNormal, WHITE, (1, 1), (1, h - 2))\n        pygame.draw.line(self.surfaceNormal, DARKGRAY, (1, h - 1), (w - 1, h - 1))\n        pygame.draw.line(self.surfaceNormal, DARKGRAY, (w - 1, 1), (w - 1, h - 1))\n        pygame.draw.line(self.surfaceNormal, GRAY, (2, h - 2), (w - 2, h - 2))\n        pygame.draw.line(self.surfaceNormal, GRAY, (w - 2, 2), (w - 2, h - 2))\n\n        # draw border for down button\n        pygame.draw.rect(self.surfaceDown, BLACK, pygame.Rect((0, 0, w, h)), 1) # black border around everything\n        pygame.draw.line(self.surfaceDown, WHITE, (1, 1), (w - 2, 1))\n        pygame.draw.line(self.surfaceDown, WHITE, (1, 1), (1, h - 2))\n        pygame.draw.line(self.surfaceDown, DARKGRAY, (1, h - 2), (1, 1))\n        pygame.draw.line(self.surfaceDown, DARKGRAY, (1, 1), (w - 2, 1))\n        pygame.draw.line(self.surfaceDown, GRAY, (2, h - 3), (2, 2))\n        pygame.draw.line(self.surfaceDown, GRAY, (2, 2), (w - 3, 2))\n\n        # draw border for highlight button\n        self.surfaceHighlight = self.surfaceNormal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setSurfaces(self, normalSurface, downSurface=None, highlightSurface=None):\n        if downSurface is None:\n            downSurface = normalSurface\n        if highlightSurface is None:\n            highlightSurface = normalSurface\n\n        if type(normalSurface) == str:\n            self.origSurfaceNormal = pygame.image.load(normalSurface)\n        if type(downSurface) == str:\n            self.origSurfaceDown = pygame.image.load(downSurface)\n        if type(highlightSurface) == str:\n            self.origSurfaceHighlight = pygame.image.load(highlightSurface)\n\n        if self.origSurfaceNormal.get_size() != self.origSurfaceDown.get_size() != self.origSurfaceHighlight.get_size():\n            raise Exception('foo')\n\n        self.surfaceNormal = self.origSurfaceNormal\n        self.surfaceDown = self.origSurfaceDown\n        self.surfaceHighlight = self.origSurfaceHighlight\n        self.customSurfaces = True\n        self._rect = pygame.Rect((self._rect.left, self._rect.top, self.surfaceNormal.get_width(), self.surfaceNormal.get_height()))", "response": "Switch the button to a custom image type of button."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef weights(self, mu):\n        return 1. / (self.link.deriv(mu)**2 * self.variance(mu))", "response": "r Returns the weighted mean response variable w for the next set of steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resid_dev(self, endog, mu, scale=1.):\n\n        return (endog - mu) / np.sqrt(self.variance(mu)) / scale", "response": "Returns the Gaussian deviance residuals for a given endog and mu."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Gaussian deviance function at the specified endog and mu.", "response": "def deviance(self, endog, mu, freq_weights=1., scale=1.):\n        \"\"\"\n        Gaussian deviance function\n\n        Parameters\n        ----------\n        endog : array-like\n            Endogenous response variable\n        mu : array-like\n            Fitted mean response variable\n        freq_weights : array-like\n            1d array of frequency weights. The default is 1.\n        scale : float, optional\n            An optional scale argument. The default is 1.\n\n        Returns\n        -------\n        deviance : float\n            The deviance function at (endog,mu,freq_weights,scale)\n            as defined below.\n\n        \"\"\"\n        return np.sum((freq_weights * (endog - mu)**2)) / scale"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the log - likelihood of the fitted mean response at the specified endog and mu.", "response": "def loglike(self, endog, mu, freq_weights=1., scale=1.):\n        \"\"\"\n        The log-likelihood in terms of the fitted mean response.\n\n        Parameters\n        ----------\n        endog : array-like\n            Endogenous response variable\n        mu : array-like\n            Fitted mean response variable\n        freq_weights : array-like\n            1d array of frequency weights. The default is 1.\n        scale : float, optional\n            Scales the loglikelihood function. The default is 1.\n\n        Returns\n        -------\n        llf : float\n            The value of the loglikelihood function evaluated at\n            (endog,mu,freq_weights,scale) as defined below.\n\n        \"\"\"\n        if isinstance(self.link, L.Power) and self.link.power == 1:\n            # This is just the loglikelihood for classical OLS\n            nobs2 = endog.shape[0] / 2.\n            SSR = np.sum((endog-self.fitted(mu))**2, axis=0)\n            llf = -np.log(SSR) * nobs2\n            llf -= (1+np.log(np.pi/nobs2))*nobs2\n            return llf\n        else:\n            return np.sum(freq_weights * ((endog * mu - mu**2/2)/scale -\n                          endog**2/(2 * scale) - .5*np.log(2 * np.pi * scale)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resid_dev(self, endog, mu, scale=1.):\n        endog_mu = self._clean(endog / mu)\n        return np.sign(endog - mu) * np.sqrt(-2 * (-(endog - mu)/mu +\n                                                   np.log(endog_mu)))", "response": "r Returns the Gamma deviance residuals for a given endogenous and coded response variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the response variable.", "response": "def initialize(self, endog, freq_weights):\n        '''\n        Initialize the response variable.\n\n        Parameters\n        ----------\n        endog : array\n            Endogenous response variable\n\n        Returns\n        --------\n        If `endog` is binary, returns `endog`\n\n        If `endog` is a 2d array, then the input is assumed to be in the format\n        (successes, failures) and\n        successes/(success + failures) is returned.  And n is set to\n        successes + failures.\n        '''\n        # if not np.all(np.asarray(freq_weights) == 1):\n        #     self.variance = V.Binomial(n=freq_weights)\n        if (endog.ndim > 1 and endog.shape[1] > 1):\n            y = endog[:, 0]\n            # overwrite self.freq_weights for deviance below\n            self.n = endog.sum(1)\n            return y*1./self.n, self.n\n        else:\n            return endog, np.ones(endog.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_by_uuid(self, uuid):\n        for entry in self.entries:\n            if entry.uuid == uuid:\n                return entry\n        raise EntryNotFoundError(\"Entry not found for uuid: %s\" % uuid)", "response": "Find an entry by uuid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind an entry by exact title.", "response": "def find_by_title(self, title):\n        \"\"\"Find an entry by exact title.\n\n        :raise: EntryNotFoundError\n\n        \"\"\"\n        for entry in self.entries:\n            if entry.title == title:\n                return entry\n        raise EntryNotFoundError(\"Entry not found for title: %s\" % title)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding an entry by fuzzy match.", "response": "def fuzzy_search_by_title(self, title, ignore_groups=None):\n        \"\"\"Find an entry by by fuzzy match.\n\n        This will check things such as:\n\n            * case insensitive matching\n            * typo checks\n            * prefix matches\n\n        If the ``ignore_groups`` argument is provided, then any matching\n        entries in the ``ignore_groups`` list will not be returned.  This\n        argument can be used to filter out groups you are not interested in.\n\n        Returns a list of matches (an empty list is returned if no matches are\n        found).\n\n        \"\"\"\n        entries = []\n        # Exact matches trump\n        for entry in self.entries:\n            if entry.title == title:\n                entries.append(entry)\n        if entries:\n            return self._filter_entries(entries, ignore_groups)\n        # Case insensitive matches next.\n        title_lower = title.lower()\n        for entry in self.entries:\n            if entry.title.lower() == title.lower():\n                entries.append(entry)\n        if entries:\n            return self._filter_entries(entries, ignore_groups)\n        # Subsequence/prefix matches next.\n        for entry in self.entries:\n            if self._is_subsequence(title_lower, entry.title.lower()):\n                entries.append(entry)\n        if entries:\n            return self._filter_entries(entries, ignore_groups)\n        # Finally close matches that might have mispellings.\n        entry_map = {entry.title.lower(): entry for entry in self.entries}\n        matches = difflib.get_close_matches(\n            title.lower(), entry_map.keys(), cutoff=0.7)\n        if matches:\n            return self._filter_entries(\n                [entry_map[name] for name in matches], ignore_groups)\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_configuration(self):\n        configuration = config.Configuration()\n        pegtree = pegnode.parse(self.filestring)\n        for section_node in pegtree:\n            if isinstance(section_node, pegnode.GlobalSection):\n                configuration.globall = self.build_global(section_node)\n            elif isinstance(section_node, pegnode.FrontendSection):\n                configuration.frontends.append(\n                    self.build_frontend(section_node))\n            elif isinstance(section_node, pegnode.DefaultsSection):\n                configuration.defaults.append(\n                    self.build_defaults(section_node))\n            elif isinstance(section_node, pegnode.ListenSection):\n                configuration.listens.append(\n                    self.build_listen(section_node))\n            elif isinstance(section_node, pegnode.UserlistSection):\n                configuration.userlists.append(\n                    self.build_userlist(section_node))\n            elif isinstance(section_node, pegnode.BackendSection):\n                configuration.backends.append(\n                    self.build_backend(section_node))\n\n        return configuration", "response": "Parses the haproxy config file and returns a configuration object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the global section and return the config. Global object", "response": "def build_global(self, global_node):\n\n        \"\"\"parse `global` section, and return the config.Global\n\n        Args:\n            global_node (TreeNode):  `global` section treenode\n\n        Returns:\n            config.Global: an object\n        \"\"\"\n        config_block_lines = self.__build_config_block(\n            global_node.config_block)\n        return config.Global(config_block=config_block_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __build_config_block(self, config_block_node):\n        node_lists = []\n\n        for line_node in config_block_node:\n            if isinstance(line_node, pegnode.ConfigLine):\n                node_lists.append(self.__build_config(line_node))\n            elif isinstance(line_node, pegnode.OptionLine):\n                node_lists.append(self.__build_option(line_node))\n            elif isinstance(line_node, pegnode.ServerLine):\n                node_lists.append(\n                    self.__build_server(line_node))\n            elif isinstance(line_node, pegnode.BindLine):\n                node_lists.append(\n                    self.__build_bind(line_node))\n            elif isinstance(line_node, pegnode.AclLine):\n                node_lists.append(\n                    self.__build_acl(line_node))\n            elif isinstance(line_node, pegnode.BackendLine):\n                node_lists.append(\n                    self.__build_usebackend(line_node))\n            elif isinstance(line_node, pegnode.UserLine):\n                node_lists.append(\n                    self.__build_user(line_node))\n            elif isinstance(line_node, pegnode.GroupLine):\n                node_lists.append(\n                    self.__build_group(line_node))\n            else:\n                # may blank_line, comment_line\n                pass\n        return node_lists", "response": "parse config_block in each section\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_defaults(self, defaults_node):\n        proxy_name = defaults_node.defaults_header.proxy_name.text\n        config_block_lines = self.__build_config_block(\n            defaults_node.config_block)\n        return config.Defaults(\n            name=proxy_name,\n            config_block=config_block_lines)", "response": "parse the defaults section and return a config. Defaults object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses userlist sections and return a config. Userlist object", "response": "def build_userlist(self, userlist_node):\n        \"\"\"parse `userlist` sections, and return a config.Userlist\"\"\"\n        proxy_name = userlist_node.userlist_header.proxy_name.text\n        config_block_lines = self.__build_config_block(\n            userlist_node.config_block)\n        return config.Userlist(\n            name=proxy_name,\n            config_block=config_block_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing listen section and return a config. Listen object", "response": "def build_listen(self, listen_node):\n        \"\"\"parse `listen` sections, and return a config.Listen\n\n        Args:\n            listen_node (TreeNode): Description\n\n        Returns:\n            config.Listen: an object\n        \"\"\"\n        proxy_name = listen_node.listen_header.proxy_name.text\n        service_address_node = listen_node.listen_header.service_address\n\n        # parse the config block\n        config_block_lines = self.__build_config_block(\n            listen_node.config_block)\n\n        # parse host and port\n        host, port = '', ''\n        if isinstance(service_address_node, pegnode.ServiceAddress):\n            host = service_address_node.host.text\n            port = service_address_node.port.text\n        else:\n            # use `bind` in config lines to fill in host and port\n            # just use the first\n            for line in config_block_lines:\n                if isinstance(line, config.Bind):\n                    host, port = line.host, line.port\n                    break\n            else:\n                raise Exception(\n                    'Not specify host and port in `listen` definition')\n        return config.Listen(\n            name=proxy_name, host=host, port=port,\n            config_block=config_block_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the frontend section and return a config. Frontend object", "response": "def build_frontend(self, frontend_node):\n        \"\"\"parse `frontend` sections, and return a config.Frontend\n\n        Args:\n            frontend_node (TreeNode): Description\n\n        Raises:\n            Exception: Description\n\n        Returns:\n            config.Frontend: an object\n        \"\"\"\n        proxy_name = frontend_node.frontend_header.proxy_name.text\n        service_address_node = frontend_node.frontend_header.service_address\n\n        # parse the config block\n        config_block_lines = self.__build_config_block(\n            frontend_node.config_block)\n\n        # parse host and port\n        host, port = '', ''\n        if isinstance(service_address_node, pegnode.ServiceAddress):\n            host = service_address_node.host.text\n            port = service_address_node.port.text\n        else:\n            # use `bind` in config lines to fill in host and port\n            # just use the first\n            for line in config_block_lines:\n                if isinstance(line, config.Bind):\n                    host, port = line.host, line.port\n                    break\n            else:\n                raise Exception(\n                    'Not specify host and port in `frontend` definition')\n        return config.Frontend(\n            name=proxy_name, host=host, port=port,\n            config_block=config_block_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing `backend` sections Args: backend_node (TreeNode): Description Returns: config.Backend: an object", "response": "def build_backend(self, backend_node):\n        \"\"\"parse `backend` sections\n\n        Args:\n            backend_node (TreeNode): Description\n\n        Returns:\n            config.Backend: an object\n        \"\"\"\n        proxy_name = backend_node.backend_header.proxy_name.text\n        config_block_lines = self.__build_config_block(\n            backend_node.config_block)\n        return config.Backend(name=proxy_name, config_block=config_block_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to compile and return the compiled Mako template code.", "response": "def from_string(self, template_code):\n        \"\"\"\n        Trying to compile and return the compiled template code.\n\n        :raises: TemplateSyntaxError if there's a syntax error in\n        the template.\n        :param template_code: Textual template source.\n        :return: Returns a compiled Mako template.\n        \"\"\"\n        try:\n            return self.template_class(self.engine.from_string(template_code))\n        except mako_exceptions.SyntaxException as exc:\n            raise TemplateSyntaxError(exc.args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to get a compiled template given a template name.", "response": "def get_template(self, template_name):\n        \"\"\"\n        Trying to get a compiled template given a template name\n        :param template_name: The template name.\n        :raises: - TemplateDoesNotExist if no such template exists.\n                 - TemplateSyntaxError  if we couldn't compile the\n                    template using Mako syntax.\n        :return: Compiled Template.\n        \"\"\"\n        try:\n            return self.template_class(self.engine.get_template(template_name))\n        except mako_exceptions.TemplateLookupException as exc:\n            raise TemplateDoesNotExist(exc.args)\n        except mako_exceptions.CompileException as exc:\n            raise TemplateSyntaxError(exc.args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender the template with a given context.", "response": "def render(self, context=None, request=None):\n        \"\"\"\n        Render the template with a given context. Here we're adding\n        some context variables that are required for all templates in\n        the system like the statix url and the CSRF tokens, etc.\n\n        :param context: It must be a dict if provided\n        :param request: It must be a django.http.HttpRequest if provided\n        :return: A rendered template\n        \"\"\"\n        if context is None:\n            context = {}\n\n        context['static'] = static\n        context['url'] = self.get_reverse_url()\n\n        if request is not None:\n            # As Django doesn't have a global request object,\n            # it's useful to put it in the context.\n            context['request'] = request\n            # Passing the CSRF token is mandatory.\n            context['csrf_input'] = csrf_input_lazy(request)\n            context['csrf_token'] = csrf_token_lazy(request)\n\n        try:\n            return self.template.render(**context)\n        except Exception as e:\n            traceback = RichTraceback()\n\n            source = traceback.source\n            if not source:\n                # There's no template source lines then raise\n                raise e\n\n            source = source.split('\\n')\n            line = traceback.lineno\n            top = max(0, line - 4)\n            bottom = min(len(source), line + 5)\n            source_lines = [(i + 1, source[i]) for i in range(top, bottom)]\n\n            e.template_debug = {\n                'name': traceback.records[5][4],\n                'message': '{}: {}'.format(\n                    traceback.errorname, traceback.message),\n                'source_lines': source_lines,\n                'line': line,\n                'during': source_lines[line - top - 1][1],\n                'total': bottom - top,\n                'bottom': bottom,\n                'top': top + 1,\n                # mako's RichTraceback doesn't return column number\n                'before': '',\n                'after': '',\n            }\n\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the variance and covariance matrix of the parameters of the current state of the entry in the logarithmic model.", "response": "def cov_params(self, r_matrix=None, column=None, scale=None, cov_p=None,\n                   other=None):\n        \"\"\"\n        Returns the variance/covariance matrix.\n        The variance/covariance matrix can be of a linear contrast\n        of the estimates of params or all params multiplied by scale which\n        will usually be an estimate of sigma^2.  Scale is assumed to be\n        a scalar.\n        Parameters\n        ----------\n        r_matrix : array-like\n            Can be 1d, or 2d.  Can be used alone or with other.\n        column :  array-like, optional\n            Must be used on its own.  Can be 0d or 1d see below.\n        scale : float, optional\n            Can be specified or not.  Default is None, which means that\n            the scale argument is taken from the model.\n        other : array-like, optional\n            Can be used when r_matrix is specified.\n        Returns\n        -------\n        cov : ndarray\n            covariance matrix of the parameter estimates or of linear\n            combination of parameter estimates. See Notes.\n        Notes\n        -----\n        (The below are assumed to be in matrix notation.)\n        If no argument is specified returns the covariance matrix of a model\n        ``(scale)*(X.T X)^(-1)``\n        If contrast is specified it pre and post-multiplies as follows\n        ``(scale) * r_matrix (X.T X)^(-1) r_matrix.T``\n        If contrast and other are specified returns\n        ``(scale) * r_matrix (X.T X)^(-1) other.T``\n        If column is specified returns\n        ``(scale) * (X.T X)^(-1)[column,column]`` if column is 0d\n        OR\n        ``(scale) * (X.T X)^(-1)[column][:,column]`` if column is 1d\n        \"\"\"\n        if (hasattr(self, 'mle_settings') and\n                self.mle_settings['optimizer'] in ['l1', 'l1_cvxopt_cp']):\n            dot_fun = nan_dot\n        else:\n            dot_fun = np.dot\n\n        if (cov_p is None and self.normalized_cov_params is None and\n                not hasattr(self, 'cov_params_default')):\n            raise ValueError('need covariance of parameters for computing '\n                             '(unnormalized) covariances')\n        if column is not None and (r_matrix is not None or other is not None):\n            raise ValueError('Column should be specified without other '\n                             'arguments.')\n        if other is not None and r_matrix is None:\n            raise ValueError('other can only be specified with r_matrix')\n\n        if cov_p is None:\n            if hasattr(self, 'cov_params_default'):\n                cov_p = self.cov_params_default\n            else:\n                if scale is None:\n                    scale = self.scale\n                cov_p = self.normalized_cov_params * scale\n\n        if column is not None:\n            column = np.asarray(column)\n            if column.shape == ():\n                return cov_p[column, column]\n            else:\n                # return cov_p[column][:, column]\n                return cov_p[column[:, None], column]\n        elif r_matrix is not None:\n            r_matrix = np.asarray(r_matrix)\n            if r_matrix.shape == ():\n                raise ValueError(\"r_matrix should be 1d or 2d\")\n            if other is None:\n                other = r_matrix\n            else:\n                other = np.asarray(other)\n            tmp = dot_fun(r_matrix, dot_fun(cov_p, np.transpose(other)))\n            return tmp\n        else:  # if r_matrix is None and column is None:\n            return cov_p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the confidence interval of the fitted parameters. Parameters ---------- alpha : float, optional The significance level for the confidence interval. ie., The default `alpha` = .05 returns a 95% confidence interval. cols : array-like, optional `cols` specifies which confidence intervals to return method: string Not Implemented Yet Method to estimate the confidence_interval. \"Default\" : uses self.bse which is based on inverse Hessian for MLE. \"hjjh\" : \"jac\" : \"boot-bse\" \"boot_quant\" \"profile\" Returns -------- conf_int : array Each row contains [lower, upper] limits of the confidence interval for the corresponding parameter. The first column contains all lower, the second column contains all upper limits. Examples -------- >>> import libpysal as ps >>> from spglm.glm import GLM >>> import numpy as np >>> db = ps.io.open(ps.examples.get_path('columbus.dbf'),'r') >>> y = np.array(db.by_col(\"HOVAL\")).reshape((-1,1)) >>> X = [] >>> X.append(db.by_col(\"INC\")) >>> X.append(db.by_col(\"CRIME\")) >>> X = np.array(X).T >>> model = GLM(y, X) >>> results = model.fit() >>> results.conf_int() array([[ 20.57281401, 72.28355135], [ -0.42138121, 1.67934915], [ -0.84292086, -0.12685622]]) Notes ----- The confidence interval is based on the standard normal distribution. Models wish to use a different distribution should overwrite this method.", "response": "def conf_int(self, alpha=.05, cols=None, method='default'):\n        \"\"\"\n        Returns the confidence interval of the fitted parameters.\n\n        Parameters\n        ----------\n        alpha : float, optional\n                The significance level for the confidence interval.\n                ie., The default `alpha` = .05 returns a 95% confidence\n                interval.\n        cols  : array-like, optional\n               `cols` specifies which confidence intervals to return\n        method: string\n                 Not Implemented Yet\n                 Method to estimate the confidence_interval.\n                 \"Default\" : uses self.bse which is based on inverse Hessian\n                 for MLE.\n                 \"hjjh\" :\n                 \"jac\" :\n                 \"boot-bse\"\n                 \"boot_quant\"\n                 \"profile\"\n\n        Returns\n        --------\n        conf_int : array\n                   Each row contains [lower, upper] limits of the confidence\n                   interval for the corresponding parameter. The first column\n                   contains all lower, the second column contains all upper\n                   limits.\n\n        Examples\n        --------\n        >>> import libpysal as ps\n        >>> from spglm.glm import GLM\n        >>> import numpy as np\n        >>> db = ps.io.open(ps.examples.get_path('columbus.dbf'),'r')\n        >>> y = np.array(db.by_col(\"HOVAL\")).reshape((-1,1))\n        >>> X = []\n        >>> X.append(db.by_col(\"INC\"))\n        >>> X.append(db.by_col(\"CRIME\"))\n        >>> X = np.array(X).T\n        >>> model = GLM(y, X)\n        >>> results = model.fit()\n        >>> results.conf_int()\n        array([[ 20.57281401,  72.28355135],\n               [ -0.42138121,   1.67934915],\n               [ -0.84292086,  -0.12685622]])\n\n        Notes\n        -----\n        The confidence interval is based on the standard normal distribution.\n        Models wish to use a different distribution should overwrite this\n        method.\n        \"\"\"\n        bse = self.bse\n\n        if self.use_t:\n            dist = stats.t\n            df_resid = getattr(self, 'df_resid_inference', self.df_resid)\n            q = dist.ppf(1 - alpha / 2, df_resid)\n        else:\n            dist = stats.norm\n            q = dist.ppf(1 - alpha / 2)\n\n        if cols is None:\n            lower = self.params - q * bse\n            upper = self.params + q * bse\n        else:\n            cols = np.asarray(cols)\n            lower = self.params[cols] - q * bse[cols]\n            upper = self.params[cols] + q * bse[cols]\n        return np.asarray(lzip(lower, upper))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef other_seqids(self):\n        seqids = self._xml_root.xpath('GBSeq_other-seqids/GBSeqid/text()')\n        return {t: l.rstrip('|').split('|')\n                for t, _, l in [si.partition('|') for si in seqids]}", "response": "returns a dictionary of sequence ids like gi ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns md5 hash of a string.", "response": "def MD5Hash(password):\r\n    \"\"\"\r\n        Returns md5 hash of a string.\r\n        \r\n        @param password (string) - String to be hashed.\r\n        \r\n        @return (string) - Md5 hash of password.\r\n    \"\"\"\r\n    md5_password = md5.new(password)\r\n    password_md5 = md5_password.hexdigest()\r\n    return password_md5"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setVerbosity(self, verbose):\r\n        if not (verbose == True or verbose == False):\r\n            return False\r\n        else:\r\n            self.__verbose__ = verbose\r\n            return True", "response": "Set the verbosity of the SenseApi object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setServer(self, server):\r\n        if server == 'live':\r\n            self.__server__ = server\r\n            self.__server_url__ = 'api.sense-os.nl'\r\n            self.setUseHTTPS()\r\n            return True\r\n        elif server == 'dev':\r\n            self.__server__ = server\r\n            self.__server_url__ = 'api.dev.sense-os.nl'\r\n            # the dev server doesn't support https\r\n            self.setUseHTTPS(False)\r\n            return True\r\n        elif server == 'rc':\r\n            self.__server__ = server\r\n            self.__server_url__ = 'api.rc.dev.sense-os.nl'\r\n            self.setUseHTTPS(False)\r\n        else:\r\n            return False", "response": "Set the server to interact with. Returns True if successful False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving all the user s own sensors by iterating over the SensorsGet function", "response": "def getAllSensors(self):\r\n        \"\"\"\r\n            Retrieve all the user's own sensors by iterating over the SensorsGet function\r\n            \r\n            @return (list) - Array of sensors\r\n        \"\"\"\r\n        j = 0\r\n        sensors = []\r\n        parameters = {'page':0, 'per_page':1000, 'owned':1}\r\n        while True:\r\n            parameters['page'] = j\r\n            if self.SensorsGet(parameters):\r\n                s = json.loads(self.getResponse())['sensors']\r\n                sensors.extend(s)\r\n            else:\r\n                # if any of the calls fails, we cannot be cannot be sure about the sensors in CommonSense\r\n                return None\r\n\r\n            if len(s) < 1000:\r\n                break\r\n\r\n            j += 1\r\n\r\n        return sensors"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findSensor(self, sensors, sensor_name, device_type = None):\r\n\r\n        if device_type == None:\r\n            for sensor in sensors:\r\n                if sensor['name'] == sensor_name:\r\n                    return sensor['id']\r\n        else:\r\n            for sensor in sensors:\r\n                if sensor['name'] == sensor_name and sensor['device_type'] == device_type:\r\n                    return sensor['id']\r\n\r\n        return None", "response": "Find a sensor in the provided list of sensors and return the sensor_id of the sensor or None if not found"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AuthenticateSessionId(self, username, password):\r\n        self.__setAuthenticationMethod__('authenticating_session_id')\r\n\r\n        parameters = {'username':username, 'password':password}\r\n\r\n        if self.__SenseApiCall__(\"/login.json\", \"POST\", parameters = parameters):\r\n            try:\r\n                response = json.loads(self.__response__)\r\n            except:\r\n                self.__setAuthenticationMethod__('not_authenticated')\r\n                self.__error__ = \"notjson\"\r\n                return False\r\n            try:\r\n                self.__session_id__ = response['session_id']\r\n                self.__setAuthenticationMethod__('session_id')\r\n                return True\r\n            except:\r\n                self.__setAuthenticationMethod__('not_authenticated')\r\n                self.__error__ = \"no session_id\"\r\n                return False\r\n        else:\r\n            self.__setAuthenticationMethod__('not_authenticated')\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Authenticate using a username and password."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef LogoutSessionId(self):\r\n        if self.__SenseApiCall__('/logout.json', 'POST'):\r\n            self.__setAuthenticationMethod__('not_authenticated')\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Logout the current session_id from CommonSense\rical           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate using Oauth @param oauth_token_key (string) - A valid oauth token key obtained from CommonSense @param oauth_token_secret (string) - A valid oauth token secret obtained from CommonSense @param oauth_consumer_key (string) - A valid oauth consumer key obtained from CommonSense @param oauth_consumer_secret (string) - A valid oauth consumer secret obtained from CommonSense @return (boolean) - Boolean indicating whether the provided credentials were successfully authenticated", "response": "def AuthenticateOauth (self, oauth_token_key, oauth_token_secret, oauth_consumer_key, oauth_consumer_secret):\r\n        \"\"\"\r\n            Authenticate using Oauth\r\n            \r\n            @param oauth_token_key (string) - A valid oauth token key obtained from CommonSense\r\n            @param oauth_token_secret (string) - A valid oauth token secret obtained from CommonSense\r\n            @param oauth_consumer_key (string) - A valid oauth consumer key obtained from CommonSense\r\n            @param oauth_consumer_secret (string) - A valid oauth consumer secret obtained from CommonSense\r\n            \r\n            @return (boolean) - Boolean indicating whether the provided credentials were successfully authenticated\r\n        \"\"\"\r\n        self.__oauth_consumer__ = oauth.OAuthConsumer(str(oauth_consumer_key), str(oauth_consumer_secret))\r\n        self.__oauth_token__ = oauth.OAuthToken(str(oauth_token_key), str(oauth_token_secret))\r\n        self.__authentication__ = 'oauth'\r\n        if self.__SenseApiCall__('/users/current.json', 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the user has successfully retrieved an access token.", "response": "def OauthGetAccessToken(self):\r\n        \"\"\"\r\n            Use token_verifier to obtain an access token for the user. If this function returns True, the clients __oauth_token__ member\r\n            contains the access token. \r\n            \r\n            @return (boolean) - Boolean indicating whether OauthGetRequestToken was successful\r\n        \"\"\"\r\n\r\n        self.__setAuthenticationMethod__('authenticating_oauth')\r\n\r\n        # obtain access token\r\n        oauth_request = oauth.OAuthRequest.from_consumer_and_token(self.__oauth_consumer__, \\\r\n                                                                     token = self.__oauth_token__, \\\r\n                                                                     callback = '', \\\r\n                                                                     verifier = self.__oauth_token__.verifier, \\\r\n                                                                     http_url = 'http://api.sense-os.nl/oauth/access_token')\r\n        oauth_request.sign_request(oauth.OAuthSignatureMethod_HMAC_SHA1(), self.__oauth_consumer__, self.__oauth_token__)\r\n\r\n        parameters = []\r\n        for key in oauth_request.parameters.iterkeys():\r\n            parameters.append((key, oauth_request.parameters[key]))\r\n        parameters.sort()\r\n\r\n        if self.__SenseApiCall__('/oauth/access_token', 'GET', parameters = parameters):\r\n            response = urlparse.parse_qs(self.__response__)\r\n            self.__oauth_token__ = oauth.OAuthToken(response['oauth_token'][0], response['oauth_token_secret'][0])\r\n            self.__setAuthenticationMethod__('oauth')\r\n            return True\r\n        else:\r\n            self.__setAuthenticationMethod__('session_id')\r\n            self.__error__ = \"error getting access token\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nauthorizing an application using oauth.", "response": "def OauthAuthorizeApplication(self, oauth_duration = 'hour'):\r\n        \"\"\"\r\n            Authorize an application using oauth. If this function returns True, the obtained oauth token can be retrieved using getResponse and will be in url-parameters format.\r\n            TODO: allow the option to ask the user himself for permission, instead of doing this automatically. Especially important for web applications.\r\n            \r\n            @param oauth_duration (string) (optional) -'hour', 'day', 'week', 'year', 'forever'\r\n            \r\n            @return (boolean) - Boolean indicating whether OauthAuthorizeApplication was successful\r\n        \"\"\"\r\n        if self.__session_id__ == '':\r\n            self.__error__ = \"not logged in\"\r\n            return False\r\n\r\n    # automatically get authorization for the application\r\n        parameters = {'oauth_token':self.__oauth_token__.key, 'tok_expir':self.__OauthGetTokExpir__(oauth_duration), 'action':'ALLOW', 'session_id':self.__session_id__}\r\n\r\n        if self.__SenseApiCall__('/oauth/provider_authorize', 'POST', parameters = parameters):\r\n            if self.__status__ == 302:\r\n                response = urlparse.parse_qs(urlparse.urlparse(self.__headers__['location'])[4])\r\n                verifier = response['oauth_verifier'][0]\r\n                self.__oauth_token__.set_verifier(verifier)\r\n                return True\r\n            else:\r\n                self.__setAuthenticationMethod__('session_id')\r\n                self.__error__ = \"error authorizing application\"\r\n                return False\r\n        else:\r\n            self.__setAuthenticationMethod__('session_id')\r\n            self.__error__ = \"error authorizing application\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves sensors from CommonSense, according to parameters, or by sensor id. If successful, result can be obtained by a call to getResponse(), and should be a json string. @param parameters (dictionary) (optional) - Dictionary containing the parameters for the api-call. @note - http://www.sense-os.nl/45?nodeId=45&selectedId=11887 @param sensor_id (int) (optional) - Sensor id of sensor to retrieve details from. @return (boolean) - Boolean indicating whether SensorsGet was successful.", "response": "def SensorsGet(self, parameters = None, sensor_id = -1):\r\n        \"\"\"\r\n            Retrieve sensors from CommonSense, according to parameters, or by sensor id. \r\n            If successful, result can be obtained by a call to getResponse(), and should be a json string.\r\n            \r\n            @param parameters (dictionary) (optional) - Dictionary containing the parameters for the api-call.\r\n                    @note - http://www.sense-os.nl/45?nodeId=45&selectedId=11887\r\n            @param sensor_id (int) (optional) - Sensor id of sensor to retrieve details from.\r\n            \r\n            @return (boolean) - Boolean indicating whether SensorsGet was successful.\r\n        \"\"\"\r\n\r\n        url = ''\r\n        if parameters is None and sensor_id <> -1:\r\n            url = '/sensors/{0}.json'.format(sensor_id)\r\n        else:\r\n            url = '/sensors.json'\r\n\r\n        if self.__SenseApiCall__(url, 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a sensor from CommonSense.", "response": "def SensorsDelete(self, sensor_id):\r\n        \"\"\"\r\n            Delete a sensor from CommonSense.\r\n            \r\n            @param sensor_id (int) - Sensor id of sensor to delete from CommonSense.\r\n            \r\n            @return (bool) - Boolean indicating whether SensorsDelete was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/sensors/{0}.json'.format(sensor_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SensorsPost(self, parameters):\r\n        if self.__SenseApiCall__('/sensors.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to create a new sensor in CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SensorsMetatagsGet(self, parameters, namespace = None):\r\n        ns = \"default\" if namespace is None else namespace\r\n        parameters['namespace'] = ns\r\n        if self.__SenseApiCall__('/sensors/metatags.json', 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Retrieves sensors with their metatags."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GroupSensorsMetatagsGet(self, group_id, parameters, namespace = None):\r\n        ns = \"default\" if namespace is None else namespace\r\n        parameters['namespace'] = ns\r\n        if self.__SenseApiCall__('/groups/{0}/sensors/metatags.json'.format(group_id), 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Retrieve sensors in a group with their metatags."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SensorMetatagsGet(self, sensor_id, namespace = None):\r\n        ns = \"default\" if namespace is None else namespace\r\n        if self.__SenseApiCall__('/sensors/{0}/metatags.json'.format(sensor_id), 'GET', parameters = {'namespace': ns}):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Retrieve the metatags of a sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SensorMetatagsPost(self, sensor_id, metatags, namespace = None):\r\n        ns = \"default\" if namespace is None else namespace\r\n        if self.__SenseApiCall__(\"/sensors/{0}/metatags.json?namespace={1}\".format(sensor_id, ns), \"POST\", parameters = metatags):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Attach metatags to a sensor for a specific namespace"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding sensors in a group based on a number of filters on metatags", "response": "def GroupSensorsFind(self, group_id, parameters, filters, namespace = None):\r\n        \"\"\"\r\n            Find sensors in a group based on a number of filters on metatags\r\n            \r\n            @param group_id (int) - Id of the group in which to find sensors\r\n            @param namespace (string) - Namespace to use in filtering on metatags\r\n            @param parameters (dictionary) - Dictionary containing additional parameters\r\n            @param filters (dictionary) - Dictioanry containing the filters on metatags\r\n            \r\n            @return (bool) - Boolean indicating whether GroupSensorsFind was successful\r\n        \"\"\"\r\n        ns = \"default\" if namespace is None else namespace\r\n        parameters['namespace'] = ns\r\n        if self.__SenseApiCall__(\"/groups/{0}/sensors/find.json?{1}\".format(group_id, urllib.urlencode(parameters, True)), \"POST\", parameters = filters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MetatagDistinctValuesGet(self, metatag_name, namespace = None):\r\n        ns = \"default\" if namespace is None else namespace\r\n        if self.__SenseApiCall__(\"/metatag_name/{0}/distinct_values.json\", \"GET\", parameters = {'namespace': ns}):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Find the distinct values of a certain metatag name in a certain namespace\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SensorsDataGet(self, sensorIds, parameters):\r\n        if parameters is None:\r\n                parameters = {}\r\n        parameters[\"sensor_id[]\"] = sensorIds\r\n        if self.__SenseApiCall__('/sensors/data.json', 'GET', parameters = parameters):\r\n                return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to retrieve sensor data for the specified sensors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a sensor datum from a specific sensor in CommonSense.", "response": "def SensorDataDelete(self, sensor_id, data_id):\r\n        \"\"\"\r\n            Delete a sensor datum from a specific sensor in CommonSense.\r\n            \r\n            @param sensor_id (int) - Sensor id of the sensor to delete data from\r\n            @param data_id (int) - Id of the data point to delete\r\n            \r\n            @return (bool) - Boolean indicating whether SensorDataDelete was successful. \r\n        \"\"\"\r\n        if self.__SenseApiCall__('/sensors/{0}/data/{1}.json'.format(sensor_id, data_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error_ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SensorsDataPost(self, parameters):\r\n        if self.__SenseApiCall__('/sensors/data.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Post sensors data to multiple sensors in CommonSense simultaneously."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ServicesGet (self, sensor_id):\r\n        if self.__SenseApiCall__('/sensors/{0}/services.json'.format(sensor_id), 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to retrieve services connected to a sensor in CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ServicesPost (self, sensor_id, parameters):\r\n        if self.__SenseApiCall__('/sensors/{0}/services.json'.format(sensor_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Create a new service in CommonSense attached to a specific sensor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a service from CommonSense.", "response": "def ServicesDelete (self, sensor_id, service_id):\r\n        \"\"\"\r\n            Delete a service from CommonSense.\r\n            \r\n            @param sensor_id (int) - Sensor id of the sensor the service is connected to.\r\n            @param service_id (int) - Sensor id of the service to delete.\r\n            \r\n            @return (bool) - Boolean indicating whether ServicesDelete was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/sensors/{0}/services/{1}.json'.format(sensor_id, service_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ServicesSetMetod (self, sensor_id, service_id, method, parameters):\r\n        if self.__SenseApiCall__('/sensors/{0}/services/{1}/{2}.json'.format(sensor_id, service_id, method), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Set expression for the math service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ServicesGetMetod (self, sensor_id, service_id, method):\r\n        if self.__SenseApiCall__('/sensors/{0}/services/{1}/{2}.json'.format(sensor_id, service_id, method), 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Get the math expression for a service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ServicesSetUseDataTimestamp(self, sensor_id, service_id, parameters):\r\n        if self.__SenseApiCall__('/sensors/{0}/services/{1}/SetUseDataTimestamp.json'.format(sensor_id, service_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Set the expression of the math service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a user and returns the user object and session object", "response": "def CreateUser (self, parameters):\r\n        \"\"\"\r\n            Create a user\r\n            This method creates a user and returns the user object and session\r\n            \r\n            @param parameters (dictionary) - Parameters according to which to create the user.        \r\n        \"\"\"\r\n        print \"Creating user\"\r\n        print parameters\r\n        if self.__SenseApiCall__('/users.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the current user.", "response": "def UsersUpdate (self, user_id, parameters):\r\n        \"\"\"\r\n            Update the current user.\r\n            \r\n            @param user_id (int) - id of the user to be updated\r\n            @param parameters (dictionary) - user object to update the user with\r\n            \r\n            @return (bool) - Boolean indicating whether UserUpdate was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/users/{0}.json'.format(user_id), 'PUT', parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the password for the current user", "response": "def UsersChangePassword (self, current_password, new_password):\r\n        \"\"\"\r\n            Change the password for the current user\r\n            \r\n            @param current_password (string) - md5 hash of the current password of the user\r\n            @param new_password (string) - md5 hash of the new password of the user (make sure to doublecheck!)\r\n            \r\n            @return (bool) - Boolean indicating whether ChangePassword was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/change_password', \"POST\", {\"current_password\":current_password, \"new_password\":new_password}):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a user from CommonSense.", "response": "def UsersDelete (self, user_id):\r\n        \"\"\"\r\n            Delete user. \r\n            \r\n            @return (bool) - Boolean indicating whether UsersDelete was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/users/{user_id}.json'.format(user_id = user_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the details of a specific event - notification.", "response": "def EventsNotificationsGet(self, event_notification_id = -1):\r\n        \"\"\"\r\n            Retrieve either all notifications or the notifications attached to a specific event.\r\n            If successful, result can be obtained by a call to getResponse(), and should be a json string.\r\n            \r\n            @param event_notification_id (int) (optional) - Id of the event-notification to retrieve details from.\r\n            \r\n            @return (bool) - Boolean indicating whether EventsNotificationsGet was successful.\r\n        \"\"\"\r\n        if event_notification_id == -1:\r\n            url = '/events/notifications.json'\r\n        else:\r\n            url = '/events/notifications/{0}.json'.format(event_notification_id)\r\n\r\n        if self.__SenseApiCall__(url, 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef EventsNotificationsDelete(self, event_notification_id):\r\n        if self.__SenseApiCall__('/events/notifications/{0}.json'.format(event_notification_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Delete an event - notification from CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an event-notification in CommonSense. If EvensNotificationsPost was successful the result, including the event_notification_id can be obtained from getResponse(), and should be a json string. @param parameters (dictionary) - Parameters according to which to create the event notification. @note - @return (bool) - Boolean indicating whether EventsNotificationsPost was successful.", "response": "def EventsNotificationsPost(self, parameters):\r\n        \"\"\"\r\n            Create an event-notification in CommonSense.\r\n            If EvensNotificationsPost was successful the result, including the event_notification_id can be obtained from getResponse(), and should be a json string.\r\n            \r\n            @param parameters (dictionary) - Parameters according to which to create the event notification.\r\n                    @note - \r\n                    \r\n            @return (bool) - Boolean indicating whether EventsNotificationsPost was successful. \r\n        \"\"\"\r\n        if self.__SenseApiCall__('/events/notifications.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef TriggersGet(self, trigger_id = -1):\r\n        if trigger_id == -1:\r\n            url = '/triggers.json'\r\n        else:\r\n            url = '/triggers/{0}.json'.format(trigger_id)\r\n        if self.__SenseApiCall__(url, 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Retrieves details of a specific trigger."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef TriggersDelete(self, trigger_id):\r\n        if self.__SenseApiCall__('/triggers/{0}'.format(trigger_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to delete a specific trigger from CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef TriggersPost(self, parameters):\r\n        if self.__SenseApiCall__('/triggers.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to create a new trigger on CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SensorsTriggersGet(self, sensor_id, trigger_id = -1):\r\n        if trigger_id == -1:\r\n            url = '/sensors/{0}/triggers.json'.format(sensor_id)\r\n        else:\r\n            url = '/sensors/{0}/triggers/{1}.json'.format(sensor_id, trigger_id)\r\n\r\n        if self.__SenseApiCall__(url, 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Retrieves details of a specific sensor or a specific trigger."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SensorsTriggersNotificationsGet(self, sensor_id, trigger_id):\r\n        if self.__SenseApiCall__('/sensors/{0}/triggers/{1}/notifications.json'.format(sensor_id, trigger_id), 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Get all notifications connected to a sensor - trigger combination."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SensorsTriggersNotificationsDelete(self, sensor_id, trigger_id, notification_id):\r\n        if self.__SenseApiCall__('/sensors/{0}/triggers/{1}/notifications/{2}.json'.format(sensor_id, trigger_id, notification_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Disconnect a notification from a sensor - trigger combination."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting a notification to a sensor - trigger combination.", "response": "def SensorsTriggersNotificationsPost(self, sensor_id, trigger_id, parameters):\r\n        \"\"\"\r\n            Connect a notification to a sensor-trigger combination.\r\n            \r\n            @param sensor_id (int) - Sensor id if the sensor-trigger combination.\r\n            @param trigger_id (int) - Trigger id of the sensor-trigger combination.\r\n            @param parameters (dictionary) - Dictionary containing the notification to connect.\r\n                    @note - \r\n                    \r\n            @return (bool) - Boolean indicating whether SensorsTriggersNotificationsPost was successful.            \r\n        \"\"\"\r\n        if self.__SenseApiCall__('/sensors/{0}/triggers/{1}/notifications.json'.format(sensor_id, trigger_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef NotificationsGet(self, notification_id = -1):\r\n        if notification_id == -1:\r\n            url = '/notifications.json'\r\n        else:\r\n            url = '/notifications/{0}.json'.format(notification_id)\r\n\r\n        if self.__SenseApiCall__(url, 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Gets details of a specific notification from CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a notification from CommonSense.", "response": "def NotificationsDelete(self, notification_id):\r\n        \"\"\"\r\n            Delete a notification from CommonSense.\r\n            \r\n            @param notification_id (int) - Notification id of the notification to delete.\r\n            \r\n            @return (bool) - Boolean indicating whether NotificationsDelete was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/notifications/{0}.json'.format(notification_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a notification on CommonSense. If successful the result, including the notification_id, can be obtained from getResponse(), and should be a json string. @param parameters (dictionary) - Dictionary containing the notification to create. @note - @return (bool) - Boolean indicating whether NotificationsPost was successful.", "response": "def NotificationsPost(self, parameters):\r\n        \"\"\"\r\n            Create a notification on CommonSense.\r\n            If successful the result, including the notification_id, can be obtained from getResponse(), and should be a json string.\r\n            \r\n            @param parameters (dictionary) - Dictionary containing the notification to create.\r\n                    @note - \r\n                    \r\n            @return (bool) - Boolean indicating whether NotificationsPost was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/notifications.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DeviceGet(self, device_id):\r\n        if self.__SenseApiCall__('/devices/{0}'.format(device_id), 'GET'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Get details of a single device"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain a list of all sensors attached to a device. @param device_id (int) - Device for which to retrieve sensors @param parameters (dict) - Search parameters @return (bool) - Boolean indicating whether DeviceSensorsGet was succesful.", "response": "def DeviceSensorsGet(self, device_id, parameters):\r\n        \"\"\"\r\n            Obtain a list of all sensors attached to a device.\r\n\r\n            @param device_id (int) - Device for which to retrieve sensors\r\n            @param parameters (dict) - Search parameters\r\n\r\n            @return (bool) - Boolean indicating whether DeviceSensorsGet was succesful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/devices/{0}/sensors.json'.format(device_id), 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GroupsGet(self, parameters = None, group_id = -1):\r\n        if parameters is None and group_id == -1:\r\n            self.__error__ = \"no arguments\"\r\n            return False\r\n\r\n        url = ''\r\n        if group_id is -1:\r\n            url = '/groups.json'\r\n        else:\r\n            url = '/groups/{0}.json'.format(group_id)\r\n\r\n        if self.__SenseApiCall__(url, 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to retrieve groups from CommonSense according to parameters or by group id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a group from CommonSense. @param group_id (int) - group id of group to delete from CommonSense. @return (bool) - Boolean indicating whether GroupsDelete was successful.", "response": "def GroupsDelete(self, group_id):\r\n        \"\"\"\r\n            Delete a group from CommonSense.\r\n            \r\n            @param group_id (int) - group id of group to delete from CommonSense.\r\n            \r\n            @return (bool) - Boolean indicating whether GroupsDelete was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/groups/{0}.json'.format(group_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GroupsPost(self, parameters):\r\n        if self.__SenseApiCall__('/groups.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "CommonSense API to create a group in CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding users to a group in CommonSense. @param parameters (dictonary) - Dictionary containing the users to add. @return (bool) - Boolean indicating whether GroupsPost was successful.", "response": "def GroupsUsersPost(self, parameters, group_id):\r\n        \"\"\"\r\n            Add users to a group in CommonSense.\r\n            \r\n            @param parameters (dictonary) - Dictionary containing the users to add.\r\n                                    \r\n            @return (bool) - Boolean indicating whether GroupsPost was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/groups/{group_id}/users.json'.format(group_id = group_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GroupsUsersDelete(self, group_id, user_id):\r\n        if self.__SenseApiCall__('/groups/{group_id}/users/{user_id}.json'.format(group_id = group_id, user_id = user_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Delete a user from a group in CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SensorShare(self, sensor_id, parameters):\r\n\r\n        if not parameters['user']['id']:\r\n            parameters['user'].pop('id')\r\n\r\n        if not parameters['user']['username']:\r\n            parameters['user'].pop('username')\r\n\r\n        if self.__SenseApiCall__(\"/sensors/{0}/users\".format(sensor_id), \"POST\", parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Share a sensor with a user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsharing a number of sensors within a group.", "response": "def GroupsSensorsPost(self, group_id, sensors):\r\n        \"\"\"\r\n            Share a number of sensors within a group.\r\n            \r\n            @param group_id (int) - Id of the group to share sensors with\r\n            @param sensors (dictionary) - Dictionary containing the sensors to share within the groups\r\n            \r\n            @return (bool) - Boolean indicating whether the GroupsSensorsPost call was successful\r\n        \"\"\"\r\n        if self.__SenseApiCall__(\"/groups/{0}/sensors.json\".format(group_id), \"POST\", parameters = sensors):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the sensors shared within a group.", "response": "def GroupsSensorsGet(self, group_id, parameters):\r\n        \"\"\"\r\n            Retrieve sensors shared within the group.\r\n            \r\n            @param group_id (int) - Id of the group to retrieve sensors from\r\n            @param parameters (dictionary) - Additional parameters for the call\r\n            \r\n            @return (bool) - Boolean indicating whether GroupsSensorsGet was successful\r\n        \"\"\"\r\n        if self.__SenseApiCall(\"/groups/{0}/sensors.json\".format(group_id), \"GET\", parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GroupsSensorsDelete(self, group_id, sensor_id):\r\n        if self.__SenseApiCall__(\"/groups/{0}/sensors/{1}.json\".format(group_id, sensor_id), \"DELETE\"):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Stop sharing a sensor within a group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve users of the specified domain. @param domain_id (int) - Id of the domain to retrieve users from @param parameters (int) - parameters of the api call. @return (bool) - Boolean idicating whether DomainUsersGet was successful.", "response": "def DomainUsersGet(self, domain_id, parameters):\r\n        \"\"\"\r\n            Retrieve users of the specified domain.\r\n            \r\n            @param domain_id (int) - Id of the domain to retrieve users from\r\n            @param parameters (int) - parameters of the api call.\r\n            \r\n            @return (bool) - Boolean idicating whether DomainUsersGet was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/domains/{0}/users.json'.format(domain_id), 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new token for a domain.", "response": "def DomainTokensCreate(self, domain_id, amount):\r\n        \"\"\"\r\n            This method creates tokens that can be used by users who want to join the domain. \r\n            Tokens are automatically deleted after usage. \r\n            Only domain managers can create tokens.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/domains/{0}/tokens.json'.format(domain_id), 'POST', parameters = {\"amount\":amount}):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the users data processors.", "response": "def DataProcessorsGet(self, parameters):\r\n        \"\"\"\r\n            List the users data processors.\r\n\r\n            @param parameters (dictonary) - Dictionary containing the parameters of the request.\r\n                                    \r\n            @return (bool) - Boolean indicating whether this call was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/dataprocessors.json', 'GET', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Data processor in CommonSense. If DataProcessorsPost is successful, the data processor and sensor details, including its sensor_id, can be obtained by a call to getResponse(), and should be a json string. @param parameters (dictonary) - Dictionary containing the details of the data processor to be created. @note - http://www.sense-os.nl/46?nodeId=46&selectedId=11887 @return (bool) - Boolean indicating whether DataProcessorPost was successful.", "response": "def DataProcessorsPost(self, parameters):\r\n        \"\"\"\r\n            Create a Data processor  in CommonSense.\r\n            If DataProcessorsPost is successful, the data processor and sensor details, including its sensor_id, can be obtained by a call to getResponse(), and should be a json string.\r\n            \r\n            @param parameters (dictonary) - Dictionary containing the details of the data processor to be created. \r\n                    @note - http://www.sense-os.nl/46?nodeId=46&selectedId=11887            \r\n                                    \r\n            @return (bool) - Boolean indicating whether DataProcessorPost was successful.\r\n        \"\"\"\r\n        if self.__SenseApiCall__('/dataprocessors.json', 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef DataProcessorsDelete(self, dataProcessorId):\r\n        if self.__SenseApiCall__('/dataprocessors/{id}.json'.format(id = dataProcessorId), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "response": "Delete a data processor in CommonSense."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute MLE coefficients using iwls routine.", "response": "def _compute_betas(y, x):\n    \"\"\"\n    compute MLE coefficients using iwls routine\n\n    Methods: p189, Iteratively (Re)weighted Least Squares (IWLS),\n    Fotheringham, A. S., Brunsdon, C., & Charlton, M. (2002).\n    Geographically weighted regression: the analysis of spatially varying relationships.\n    \"\"\"\n    xT = x.T\n    xtx = spdot(xT, x)\n    xtx_inv = la.inv(xtx)\n    xtx_inv = sp.csr_matrix(xtx_inv)\n    xTy = spdot(xT, y, array_out=False)\n    betas = spdot(xtx_inv, xTy)\n    return betas"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes MLE coefficients using iwls routine.", "response": "def _compute_betas_gwr(y, x, wi):\n    \"\"\"\n    compute MLE coefficients using iwls routine\n\n    Methods: p189, Iteratively (Re)weighted Least Squares (IWLS),\n    Fotheringham, A. S., Brunsdon, C., & Charlton, M. (2002).\n    Geographically weighted regression: the analysis of spatially varying relationships.\n    \"\"\"\n    xT = (x * wi).T\n    xtx = np.dot(xT, x)\n    xtx_inv_xt = linalg.solve(xtx, xT)\n    betas = np.dot(xtx_inv_xt, y)\n    return betas, xtx_inv_xt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iwls(y, x, family, offset, y_fix,\n         ini_betas=None, tol=1.0e-8, max_iter=200, wi=None):\n    \"\"\"\n    Iteratively re-weighted least squares estimation routine\n\n    Parameters\n    ----------\n    y           : array\n                  n*1, dependent variable\n\n    x           : array\n                  n*k, designs matrix of k independent variables\n\n    family      : family object\n                  probability models: Gaussian, Poisson, or Binomial\n\n    offset      : array\n                  n*1, the offset variable for each observation.\n\n    y_fix       : array\n                  n*1, the fixed intercept value of y for each observation\n\n    ini_betas   : array\n                  1*k, starting values for the k betas within the iteratively\n                  weighted least squares routine\n\n    tol         : float\n                  tolerance for estimation convergence\n\n    max_iter    : integer maximum number of iterations if convergence not met\n\n    wi          : array\n                  n*1, weights to transform observations from location i in GWR\n\n\n\n    Returns\n    -------\n\n    betas       : array\n                  k*1, estimated coefficients\n\n    mu          : array\n                  n*1, predicted y values\n\n    wx          : array\n                  n*1, final weights used for iwls for GLM\n\n    n_iter      : integer\n                  number of iterations that when iwls algorithm terminates\n\n    w           : array\n                  n*1, final weights used for iwls for GWR\n\n    z           : array\n                  iwls throughput\n\n    v           : array\n                  iwls throughput\n\n    xtx_inv_xt  : array\n                  iwls throughout to compute GWR hat matrix\n                  [X'X]^-1 X'\n\n    \"\"\"\n    n_iter = 0\n    diff = 1.0e6\n\n    if ini_betas is None:\n        betas = np.zeros((x.shape[1], 1), np.float)\n    else:\n        betas = ini_betas\n\n    if isinstance(family, Binomial):\n        y = family.link._clean(y)\n    if isinstance(family, Poisson):\n        y_off = y / offset\n        y_off = family.starting_mu(y_off)\n        v = family.predict(y_off)\n        mu = family.starting_mu(y)\n    else:\n        mu = family.starting_mu(y)\n        v = family.predict(mu)\n\n    while diff > tol and n_iter < max_iter:\n        n_iter += 1\n        w = family.weights(mu)\n        z = v + (family.link.deriv(mu) * (y - mu))\n        w = np.sqrt(w)\n        if not isinstance(x, np.ndarray):\n            w = sp.csr_matrix(w)\n            z = sp.csr_matrix(z)\n        wx = spmultiply(x, w, array_out=False)\n        wz = spmultiply(z, w, array_out=False)\n        if wi is None:\n            n_betas = _compute_betas(wz, wx)\n        else:\n            n_betas, xtx_inv_xt = _compute_betas_gwr(wz, wx, wi)\n        v = spdot(x, n_betas)\n        mu = family.fitted(v)\n\n        if isinstance(family, Poisson):\n            mu = mu * offset\n\n        diff = min(abs(n_betas - betas))\n        betas = n_betas\n\n    if wi is None:\n        return betas, mu, wx, n_iter\n    else:\n        return betas, mu, v, w, z, xtx_inv_xt, n_iter", "response": "Iteratively re - weighted least squares estimation routine for GWRM model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _next_regular(target):\n    if target <= 6:\n        return target\n\n    # Quickly check if it's already a power of 2\n    if not (target & (target - 1)):\n        return target\n\n    match = float('inf')  # Anything found will be smaller\n    p5 = 1\n    while p5 < target:\n        p35 = p5\n        while p35 < target:\n            # Ceiling integer division, avoiding conversion to float\n            # (quotient = ceil(target / p35))\n            quotient = -(-target // p35)\n            # Quickly find next power of 2 >= quotient\n            try:\n                p2 = 2 ** ((quotient - 1).bit_length())\n            except AttributeError:\n                # Fallback for Python <2.7\n                p2 = 2 ** _bit_length_26(quotient - 1)\n\n            N = p2 * p35\n            if N == target:\n                return N\n            elif N < match:\n                match = N\n            p35 *= 3\n            if p35 == target:\n                return p35\n        if p35 < match:\n            match = p35\n        p5 *= 5\n        if p5 == target:\n            return p5\n    if p5 < match:\n        match = p5\n    return match", "response": "Find the next regular number greater than or equal to target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quantile_1D(data, weights, quantile):\n    # Check the data\n    if not isinstance(data, np.matrix):\n        data = np.asarray(data)\n    if not isinstance(weights, np.matrix):\n        weights = np.asarray(weights)\n    nd = data.ndim\n    if nd != 1:\n        raise TypeError(\"data must be a one dimensional array\")\n    ndw = weights.ndim\n    if ndw != 1:\n        raise TypeError(\"weights must be a one dimensional array\")\n    if data.shape != weights.shape:\n        raise TypeError(\"the length of data and weights must be the same\")\n    if ((quantile > 1.) or (quantile < 0.)):\n        raise ValueError(\"quantile must have a value between 0. and 1.\")\n    # Sort the data\n    ind_sorted = np.argsort(data)\n    sorted_data = data[ind_sorted]\n    sorted_weights = weights[ind_sorted]\n    # Compute the auxiliary arrays\n    Sn = np.cumsum(sorted_weights)\n    # TODO: Check that the weights do not sum zero\n    #assert Sn != 0, \"The sum of the weights must not be zero\"\n    Pn = (Sn-0.5*sorted_weights)/Sn[-1]\n    # Get the value of the weighted median\n    return np.interp(quantile, Pn, sorted_data)", "response": "Compute the weighted quantile of a 1D numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nweight quantile of an array with respect to the last axis of data.", "response": "def quantile(data, weights, quantile):\n    \"\"\"\n    Weighted quantile of an array with respect to the last axis.\n\n    Parameters\n    ----------\n    data : ndarray\n        Input array.\n    weights : ndarray\n        Array with the weights. It must have the same size of the last \n        axis of `data`.\n    quantile : float\n        Quantile to compute. It must have a value between 0 and 1.\n\n    Returns\n    -------\n    quantile : float\n        The output value.\n    \"\"\"\n    # TODO: Allow to specify the axis\n    nd = data.ndim\n    if nd == 0:\n        TypeError(\"data must have at least one dimension\")\n    elif nd == 1:\n        return quantile_1D(data, weights, quantile)\n    elif nd > 1:\n        n = data.shape\n        imr = data.reshape((np.prod(n[:-1]), n[-1]))\n        result = np.apply_along_axis(quantile_1D, -1, imr, weights, quantile)\n        return result.reshape(n[:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __render_config_block(self, config_block):\n        config_block_str = ''\n        for line in config_block:\n            if isinstance(line, config.Option):\n                line_str = self.__render_option(line)\n            elif isinstance(line, config.Config):\n                line_str = self.__render_config(line)\n            elif isinstance(line, config.Server):\n                line_str = self.__render_server(line)\n            elif isinstance(line, config.Bind):\n                line_str = self.__render_bind(line)\n            elif isinstance(line, config.Acl):\n                line_str = self.__render_acl(line)\n            elif isinstance(line, config.UseBackend):\n                line_str = self.__render_usebackend(line)\n            elif isinstance(line, config.User):\n                line_str = self.__render_user(line)\n            elif isinstance(line, config.Group):\n                line_str = self.__render_group(line)\n            # append line str\n            config_block_str = config_block_str + line_str\n\n        return config_block_str", "response": "Render the config block to a string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a NON - cached throttled einfo query einfo. fcgi?db = database", "response": "def einfo(self, args=None):\n        \"\"\"\n        execute a NON-cached, throttled einfo query\n\n        einfo.fcgi?db=<database>\n\n        Input: Entrez database (&db) or None (returns info on all Entrez databases)\n\n        Output: XML containing database statistics\n\n        Example: Find database statistics for Entrez Protein.\n\n            QueryService.einfo({'db': 'protein'})\n\n        Equivalent HTTP request:\n\n            https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi?db=protein\n\n        :param dict args: dict of query items (optional)\n        :returns: content of reply\n        :rtype: str\n        :raises EutilsRequestError: when NCBI replies, but the request failed (e.g., bogus database name)\n\n        \"\"\"\n        if args is None:\n            args = {}\n        return self._query('/einfo.fcgi', args, skip_cache=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _query(self, path, args=None, skip_cache=False, skip_sleep=False):\n        if args is None:\n            args = {}        \n        def _cacheable(r):\n            \"\"\"return False if r shouldn't be cached (contains a no-cache meta\n            line); True otherwise\"\"\"\n            return not (\"no-cache\" in r  # obviate parsing, maybe\n                        and lxml.etree.XML(r).xpath(\"//meta/@content='no-cache'\"))\n        \n        # cache key: the key associated with this endpoint and args The\n        # key intentionally excludes the identifying args (tool and email)\n        # and is independent of the request method (GET/POST) args are\n        # sorted for canonicalization\n\n        url = url_base + path\n\n        # next 3 lines converted by 2to3 -nm\n        defining_args = dict(list(self.default_args.items()) + list(args.items()))\n        full_args = dict(list(self._ident_args.items()) + list(defining_args.items()))\n        cache_key = hashlib.md5(pickle.dumps((url, sorted(defining_args.items())))).hexdigest()\n\n        sqas = ';'.join([k + '=' + str(v) for k, v in sorted(args.items())])\n        full_args_str = ';'.join([k + '=' + str(v) for k, v in sorted(full_args.items())])\n\n        logging.debug(\"CACHE:\" + str(skip_cache) + \"//\" + str(self._cache))\n        if not skip_cache and self._cache:\n            try:\n                v = self._cache[cache_key]\n                _logger.debug('cache hit for key {cache_key} ({url}, {sqas}) '.format(\n                    cache_key=cache_key,\n                    url=url,\n                    sqas=sqas))\n                return v\n            except KeyError:\n                _logger.debug('cache miss for key {cache_key} ({url}, {sqas}) '.format(\n                    cache_key=cache_key,\n                    url=url,\n                    sqas=sqas))\n                pass\n\n        if self.api_key:\n            url += '?api_key={self.api_key}'.format(self=self)\n\n        # --\n\n        if not skip_sleep:\n            req_int = self.request_interval\n            sleep_time = req_int - (time.clock() - self._last_request_clock)\n            if sleep_time > 0:\n                _logger.debug('sleeping {sleep_time:.3f}'.format(sleep_time=sleep_time))\n                time.sleep(sleep_time)\n\n        r = requests.post(url, full_args)\n        self._last_request_clock = time.clock()\n        _logger.debug('post({url}, {fas}): {r.status_code} {r.reason}, {len})'.format(\n            url=url,\n            fas=full_args_str,\n            r=r,\n            len=len(r.text)))\n\n        if not r.ok:\n            # TODO: discriminate between types of errors\n            if r.headers[\"Content-Type\"] == \"application/json\":\n                json = r.json()\n                raise EutilsRequestError('{r.reason} ({r.status_code}): {error}'.format(r=r, error=json[\"error\"]))\n            try:\n                xml = lxml.etree.fromstring(r.text.encode('utf-8'))\n                raise EutilsRequestError('{r.reason} ({r.status_code}): {error}'.format(r=r, error=xml.find('ERROR').text))\n            except Exception as ex:\n                raise EutilsNCBIError('Error parsing response object from NCBI: {}'.format(ex))\n\n        if any(bad_word in r.text for bad_word in ['<error>', '<ERROR>']):\n            if r.text is not None:\n                try:\n                    xml = lxml.etree.fromstring(r.text.encode('utf-8'))\n                    raise EutilsRequestError('{r.reason} ({r.status_code}): {error}'.format(r=r, error=xml.find('ERROR').text))\n                except Exception as ex:\n                    raise EutilsNCBIError('Error parsing response object from NCBI: {}'.format(ex))\n\n        if '<h1 class=\"error\">Access Denied</h1>' in r.text:\n            raise EutilsRequestError('Access Denied: {url}'.format(url=url))\n\n        if self._cache and _cacheable(r.text):\n            # N.B. we cache results even when skip_cache (read) is true\n            self._cache[cache_key] = r.content\n            _logger.info('cached results for key {cache_key} ({url}, {sqas}) '.format(\n                cache_key=cache_key,\n                url=url,\n                sqas=sqas))\n\n        return r.content", "response": "This function is used to make a query to the cache and cache the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_key(cls, result):\n        from boto.s3.key import Key\n\n        return isinstance(result, Key)", "response": "Return True if result is a key object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self, name, value):\n\n        if self.valid_set and value not in self.valid_set:\n            raise ImproperlyConfigured(\n                \"%s: \\\"%s\\\" is not a valid setting (choose between %s).\" %\n                (name, value, \", \".join(\"\\\"%s\\\"\" % x for x in self.valid_set)))\n\n        return value", "response": "Validate and return a value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts string or bool to bool.", "response": "def parse_bool(cls, value, default=None):\n        \"\"\"Convert ``string`` or ``bool`` to ``bool``.\"\"\"\n        if value is None:\n            return default\n\n        elif isinstance(value, bool):\n            return value\n\n        elif isinstance(value, str):\n            if value == 'True':\n                return True\n            elif value == 'False':\n                return False\n\n        raise Exception(\"Value %s is not boolean.\" % value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns whether or not a container is permitted.", "response": "def container_permitted(self, name):\n        \"\"\"Return whether or not a container is permitted.\n\n        :param name: Container name.\n        :return: ``True`` if container is permitted.\n        :rtype:  ``bool``\n        \"\"\"\n        white = self._container_whitelist\n        black = self._container_blacklist\n        return name not in black and (not white or name in white)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget application media root from real media root URL.", "response": "def app_media_url(self):\n        \"\"\"Get application media root from real media root URL.\"\"\"\n        url = None\n        media_dir = self.CLOUD_BROWSER_STATIC_MEDIA_DIR\n        if media_dir:\n            url = os.path.join(self.MEDIA_URL, media_dir).rstrip('/') + '/'\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets application media document root.", "response": "def app_media_doc_root(self):  # pylint: disable=R0201\n        \"\"\"Get application media document (file) root.\"\"\"\n        app_dir = os.path.abspath(os.path.dirname(__file__))\n        media_root = os.path.join(app_dir, 'media')\n\n        return media_root"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the mtime associated with a module.", "response": "def module_getmtime(filename):\n    \"\"\"\n    Get the mtime associated with a module.  If this is a .pyc or .pyo file and\n    a corresponding .py file exists, the time of the .py file is returned.\n\n    :param filename: filename of the module.\n    :returns: mtime or None if the file doesn\"t exist.\n    \"\"\"\n    if os.path.splitext(filename)[1].lower() in (\".pyc\", \".pyo\") and os.path.exists(filename[:-1]):\n        return os.path.getmtime(filename[:-1])\n    if os.path.exists(filename):\n        return os.path.getmtime(filename)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreloading a module if it has changed since we last imported it.", "response": "def module_reload_changed(key):\n    \"\"\"\n    Reload a module if it has changed since we last imported it.  This is\n    necessary if module a imports script b, script b is changed, and then\n    module c asks to import script b.\n\n    :param key: our key used in the WatchList.\n    :returns: True if reloaded.\n    \"\"\"\n    imp.acquire_lock()\n    try:\n        modkey = module_sys_modules_key(key)\n        if not modkey:\n            return False\n        found = None\n        if modkey:\n            for second in WatchList:\n                secmodkey = module_sys_modules_key(second)\n                if secmodkey and sys.modules[modkey] == sys.modules[secmodkey]:\n                    found = second\n                    foundmodkey = secmodkey\n                    break\n        if not found:\n            return\n        filemtime = module_getmtime(WatchList[found][\"file\"])\n        filemtime = latest_submodule_time(found, filemtime)\n        if filemtime > WatchList[found][\"time\"]:\n            tangelo.log(\"Reloaded %s\" % found)\n            reload_including_local(sys.modules[foundmodkey])\n            for second in WatchList:\n                if WatchList[second][\"file\"] == WatchList[found][\"file\"]:\n                    WatchList[second][\"time\"] = filemtime\n    finally:\n        imp.release_lock()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef module_sys_modules_key(key):\n    moduleparts = key.split(\".\")\n    for partnum, part in enumerate(moduleparts):\n        modkey = \".\".join(moduleparts[partnum:])\n        if modkey in sys.modules:\n            return modkey\n    return None", "response": "Check if a module is in sys. modules dictionary in some manner. If so return the key used in that dictionary. Otherwise return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload_including_local(module):\n    try:\n        reload(module)\n    except ImportError:\n        # This can happen if the module was loaded in the immediate script\n        # directory.  Add the service path and try again.\n        if not hasattr(cherrypy.thread_data, \"modulepath\"):\n            raise\n        path = os.path.abspath(cherrypy.thread_data.modulepath)\n        root = os.path.abspath(cherrypy.config.get(\"webroot\"))\n        if path not in sys.path and (path == root or path.startswith(root + os.path.sep)):\n            oldpath = sys.path\n            try:\n                sys.path = [path] + sys.path\n                reload(module)\n            finally:\n                sys.path = oldpath\n        else:\n            raise", "response": "Reload a module. If it isn t found try to include the local service\nTaxonomy directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reload_recent_submodules(module, mtime=0, processed=[]):\n    if module.endswith(\".py\"):\n        module = module[:-3]\n    if module in processed:\n        return False\n    any_reloaded = False\n    for key in WatchList:\n        if WatchList[key][\"parent\"] == module:\n            reloaded = reload_recent_submodules(key, mtime, processed)\n            filemtime = module_getmtime(WatchList[key][\"file\"])\n            filemtime = latest_submodule_time(key, filemtime)\n            any_reloaded = any_reloaded or reloaded\n            if reloaded or filemtime > WatchList[key][\"time\"]:\n                WatchList[key][\"time\"] = filemtime\n                for second in WatchList:\n                    if second != key and WatchList[second][\"file\"] == WatchList[key][\"file\"]:\n                        WatchList[second][\"time\"] = filemtime\n                modkey = module_sys_modules_key(key)\n                if modkey:\n                    try:\n                        reload_including_local(sys.modules[modkey])\n                        tangelo.log(\"Reloaded %s\" % modkey)\n                    except ImportError:\n                        del sys.modules[modkey]\n                        tangelo.log(\"Asking %s to reimport\" % modkey)\n                    any_reloaded = True\n    return any_reloaded", "response": "Recursively reloads the recent submodules of a module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef watch_import(name, globals=None, *args, **kwargs):\n    # Don\"t monitor builtin modules.  types seem special, so don\"t monitor it\n    # either.\n    monitor = not imp.is_builtin(name) and name not in (\"types\", )\n    # Don\"t monitor modules if we don\"t know where they came from\n    monitor = monitor and isinstance(globals, dict) and globals.get(\"__name__\")\n    if not monitor:\n        return builtin_import(name, globals, *args, **kwargs)\n    # This will be the dotted module name except for service modules where it\n    # will be the absolute file path.\n    parent = globals[\"__name__\"]\n    key = parent + \".\" + name\n    module_reload_changed(key)\n    try:\n        module = builtin_import(name, globals, *args, **kwargs)\n    except ImportError:\n        raise\n    if getattr(module, \"__file__\", None):\n        if key not in WatchList:\n            tangelo.log_info(\"WATCH\", \"Monitoring import %s from %s\" % (name, parent))\n        imp.acquire_lock()\n        try:\n            if key not in WatchList:\n                filemtime = module_getmtime(module.__file__) or 0\n                filemtime = latest_submodule_time(key, filemtime)\n                WatchList[key] = {\n                    \"time\": filemtime\n                }\n            WatchList[key].update({\n                \"parent\": parent,\n                \"name\": name,\n                \"file\": module.__file__\n            })\n        finally:\n            imp.release_lock()\n    return module", "response": "A function that checks if a module is imported and if so updates the watchlist with the latest time stamp of the module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef watch_module_cache_get(cache, module):\n    imp.acquire_lock()\n    try:\n        if not hasattr(cache, \"timestamps\"):\n            cache.timestamps = {}\n        mtime = os.path.getmtime(module)\n        mtime = latest_submodule_time(module, mtime)\n        if getattr(cache, \"config\", False):\n            config_file = module[:-2] + \"yaml\"\n            if os.path.exists(config_file):\n                # Our timestamp is the latest time of the config file or the\n                # module.\n                mtime = max(mtime, os.path.getmtime(config_file))\n            # If we have a config file and the timestamp is more recent than\n            # the recorded timestamp, remove the config file from the list of\n            # loaded files so that it will get loaded again.\n            if config_file in cache.config_files and mtime > cache.timestamps.get(module, 0):\n                del cache.config_files[config_file]\n                tangelo.log(\"WATCH\", \"Asking to reload config file %s\" % config_file)\n        # If the timestamp is more recent than the recorded value, remove the\n        # the module from our records so that it will be loaded again.\n        if module in cache.modules and mtime > cache.timestamps.get(module, 0):\n            del cache.modules[module]\n            tangelo.log(\"WATCH\", \"Asking to reload module %s\" % module)\n        if module not in cache.timestamps:\n            tangelo.log_info(\"WATCH\", \"Monitoring module %s\" % module)\n        reload_recent_submodules(module, mtime)\n        cache.timestamps[module] = mtime\n        service = tangelo_module_cache_get(cache, module)\n        # Update our time based on all the modules that we may have just\n        # imported.  The times can change from before because python files are\n        # compiled, for instance.\n        mtime = latest_submodule_time(module, mtime)\n        cache.timestamps[module] = mtime\n    finally:\n        imp.release_lock()\n    return service", "response": "Watch a module s cache for changes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_int(value, default, test_fn=None):\n    try:\n        converted = int(value)\n    except ValueError:\n        return default\n\n    test_fn = test_fn if test_fn else lambda x: True\n    return converted if test_fn(converted) else default", "response": "Convert value to integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequires minimum version of module using __version__ member.", "response": "def check_version(mod, required):\n    \"\"\"Require minimum version of module using ``__version__`` member.\"\"\"\n    vers = tuple(int(v) for v in mod.__version__.split('.')[:3])\n    if vers < required:\n        req = '.'.join(str(v) for v in required)\n        raise ImproperlyConfigured(\n            \"Module \\\"%s\\\" version (%s) must be >= %s.\" %\n            (mod.__name__, mod.__version__, req))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef requires(module, name=\"\"):\n    def wrapped(method):\n        \"\"\"Call and enforce method.\"\"\"\n        if module is None:\n            raise ImproperlyConfigured(\"Module '%s' is not installed.\" % name)\n        return method\n\n    return wrapped", "response": "Decorator that enforces module presence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string from RFC 3339 to datetime object.", "response": "def dt_from_rfc8601(date_str):\n    \"\"\"Convert 8601 (ISO) date string to datetime object.\n\n    Handles \"Z\" and milliseconds transparently.\n\n    :param date_str: Date string.\n    :type  date_str: ``string``\n    :return: Date time.\n    :rtype:  :class:`datetime.datetime`\n    \"\"\"\n    # Normalize string and adjust for milliseconds. Note that Python 2.6+ has\n    # \".%f\" format, but we're going for Python 2.5, so truncate the portion.\n    date_str = date_str.rstrip('Z').split('.')[0]\n\n    # Format string. (2010-04-13T14:02:48.000Z)\n    fmt = \"%Y-%m-%dT%H:%M:%S\"\n    # Python 2.6+: Could format and handle milliseconds.\n    # if date_str.find('.') >= 0:\n    #    fmt += \".%f\"\n\n    return datetime.strptime(date_str, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry various RFC conversions to datetime or return None.", "response": "def dt_from_header(date_str):\n    \"\"\"Try various RFC conversions to ``datetime`` or return ``None``.\n\n    :param date_str: Date string.\n    :type  date_str: ``string``\n    :return: Date time.\n    :rtype:  :class:`datetime.datetime` or ``None``\n    \"\"\"\n    convert_fns = (\n        dt_from_rfc8601,\n        dt_from_rfc1123,\n    )\n\n    for convert_fn in convert_fns:\n        try:\n            return convert_fn(date_str)\n        except ValueError:\n            pass\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basename(path):\n    base_path = path.strip(SEP)\n    sep_ind = base_path.rfind(SEP)\n    if sep_ind < 0:\n        return path\n\n    return base_path[sep_ind + 1:]", "response": "Rightmost part of path after separator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit path into container object tuple.", "response": "def path_parts(path):\n    \"\"\"Split path into container, object.\n\n    :param path: Path to resource (including container).\n    :type  path: `string`\n    :return: Container, storage object tuple.\n    :rtype:  `tuple` of `string`, `string`\n    \"\"\"\n    path = path if path is not None else ''\n    container_path = object_path = ''\n    parts = path_list(path)\n\n    if len(parts) >= 1:\n        container_path = parts[0]\n    if len(parts) > 1:\n        object_path = path_join(*parts[1:])\n\n    return container_path, object_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding all path parts.", "response": "def path_yield(path):\n    \"\"\"Yield on all path parts.\"\"\"\n    for part in (x for x in path.strip(SEP).split(SEP) if x not in (None, '')):\n        yield part"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining path parts to single path.", "response": "def path_join(*args):\n    \"\"\"Join path parts to single path.\"\"\"\n    return SEP.join((x for x in args if x not in (None, ''))).strip(SEP)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relpath(path, start):\n    path_items = path_list(path)\n    start_items = path_list(start)\n\n    # Find common parts of path.\n    common = []\n    for pth, stt in zip(path_items, start_items):\n        if pth != stt:\n            break\n        common.append(pth)\n\n    # Shared parts index in both lists.\n    common_ind = len(common)\n    parent_num = len(start_items) - common_ind\n\n    # Start with parent traversal and add relative parts.\n    rel_items = [PARENT] * parent_num + path_items[common_ind:]\n    return path_join(*rel_items)", "response": "Get relative path to start."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the collective total displacements squared for all the atoms in the set.", "response": "def collective_dr_squared( self ):\n        \"\"\"\n        Squared sum of total displacements for these atoms.\n\n        Args:\n            None\n\n        Returns:\n            (Float): The square of the summed total displacements for these atoms.\n        \"\"\"\n        return sum( np.square( sum( [ atom.dr for atom in self.atoms ] ) ) )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the number of atoms occupying a specific site type.", "response": "def occupations( self, site_label ):\n        \"\"\"\n        Number of these atoms occupying a specific site type.\n\n        Args:\n            site_label (Str): Label for the site type being considered.\n\n        Returns:\n            (Int): Number of atoms occupying sites of type `site_label`.\n        \"\"\"\n        return sum( atom.site.label == site_label for atom in self.atoms )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_class(class_string):\n    try:\n        mod_name, class_name = get_mod_func(class_string)\n        if class_name != '':\n            cls = getattr(__import__(mod_name, {}, {}, ['']), class_name)\n            return cls\n    except (ImportError, AttributeError):\n        pass\n    raise ImportError('Failed to import %s' % class_string)", "response": "Convert a string version of a function name to the callable object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the mod function for the given class string.", "response": "def get_mod_func(class_string):\n    \"\"\"\n    Converts 'django.views.news.stories.story_detail' to\n    ('django.views.news.stories', 'story_detail')\n\n    Taken from django.core.urlresolvers\n    \"\"\"\n    try:\n        dot = class_string.rindex('.')\n    except ValueError:\n        return class_string, ''\n    return class_string[:dot], class_string[dot + 1:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_gs_folder(cls, result):\n        return (cls.is_key(result) and\n                result.size == 0 and\n                result.name.endswith(cls._gs_folder_suffix))", "response": "Return True if GS standalone folder object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if result is a key object.", "response": "def is_key(cls, result):\n        \"\"\"Return ``True`` if result is a key object.\"\"\"\n        from boto.gs.key import Key\n\n        return isinstance(result, Key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_prefix(cls, result):\n        from boto.s3.prefix import Prefix\n\n        return isinstance(result, Prefix) or cls._is_gs_folder(result)", "response": "Return True if result is a prefix object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_prefix(cls, container, prefix):\n        if cls._is_gs_folder(prefix):\n            name, suffix, extra = prefix.name.partition(cls._gs_folder_suffix)\n            if (suffix, extra) == (cls._gs_folder_suffix, ''):\n                # Patch GS specific folder to remove suffix.\n                prefix.name = name\n\n        return super(GsObject, cls).from_prefix(container, prefix)", "response": "Create from prefix object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget objects from GCS.", "response": "def get_objects(self, path, marker=None,\n                    limit=settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT):\n        \"\"\"Get objects.\n\n        Certain upload clients may add a 0-byte object (e.g., ``FOLDER`` object\n        for path ``path/to/FOLDER`` - ``path/to/FOLDER/FOLDER``). We add an\n        extra +1 limit query and ignore any such file objects.\n        \"\"\"\n        # Get basename of implied folder.\n        folder = path.split(SEP)[-1]\n\n        # Query extra objects, then strip 0-byte dummy object if present.\n        objs = super(GsContainer, self).get_objects(path, marker, limit + 1)\n        objs = [o for o in objs if not (o.size == 0 and o.name == folder)]\n\n        return objs[:limit]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate a storage error into a resource class.", "response": "def translate(self, exc):\n        \"\"\"Return whether or not to do translation.\"\"\"\n        from boto.exception import StorageResponseError\n\n        if isinstance(exc, StorageResponseError):\n            if exc.status == 404:\n                return self.error_cls(str(exc))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates from ambiguous result.", "response": "def from_result(cls, container, result):\n        \"\"\"Create from ambiguous result.\"\"\"\n        if result is None:\n            raise errors.NoObjectException\n\n        elif cls.is_prefix(result):\n            return cls.from_prefix(container, result)\n\n        elif cls.is_key(result):\n            return cls.from_key(container, result)\n\n        raise errors.CloudException(\"Unknown boto result type: %s\" %\n                                    type(result))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates from prefix object.", "response": "def from_prefix(cls, container, prefix):\n        \"\"\"Create from prefix object.\"\"\"\n        if prefix is None:\n            raise errors.NoObjectException\n\n        return cls(container,\n                   name=prefix.name,\n                   obj_type=cls.type_cls.SUBDIR)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_key(cls, container, key):\n        if key is None:\n            raise errors.NoObjectException\n\n        # Get Key   (1123): Tue, 13 Apr 2010 14:02:48 GMT\n        # List Keys (8601): 2010-04-13T14:02:48.000Z\n        return cls(container,\n                   name=key.name,\n                   size=key.size,\n                   content_type=key.content_type,\n                   content_encoding=key.content_encoding,\n                   last_modified=dt_from_header(key.last_modified),\n                   obj_type=cls.type_cls.FILE)", "response": "Create from key object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_objects(self, path, marker=None,\n                    limit=settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT):\n        \"\"\"Get objects.\"\"\"\n        from itertools import islice\n\n        path = path.rstrip(SEP) + SEP if path else path\n        result_set = self.native_container.list(path, SEP, marker)\n\n        # Get +1 results because marker and first item can match as we strip\n        # the separator from results obscuring things. No real problem here\n        # because boto masks any real request limits.\n        results = list(islice(result_set, limit+1))\n        if results:\n            if marker and results[0].name.rstrip(SEP) == marker.rstrip(SEP):\n                results = results[1:]\n            else:\n                results = results[:limit]\n\n        return [self.obj_cls.from_result(self, r) for r in results]", "response": "Get objects from the native container."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating from bucket object.", "response": "def from_bucket(cls, connection, bucket):\n        \"\"\"Create from bucket object.\"\"\"\n        if bucket is None:\n            raise errors.NoContainerException\n\n        # It appears that Amazon does not have a single-shot REST query to\n        # determine the number of keys / overall byte size of a bucket.\n        return cls(connection, bucket.name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate configuration from Django settings or environment.", "response": "def from_settings(cls):\n        \"\"\"Create configuration from Django settings or environment.\"\"\"\n        from cloud_browser.app_settings import settings\n        from django.core.exceptions import ImproperlyConfigured\n\n        conn_cls = conn_fn = None\n        datastore = settings.CLOUD_BROWSER_DATASTORE\n        if datastore == 'AWS':\n            # Try AWS\n            from cloud_browser.cloud.aws import AwsConnection\n            account = settings.CLOUD_BROWSER_AWS_ACCOUNT\n            secret_key = settings.CLOUD_BROWSER_AWS_SECRET_KEY\n            if account and secret_key:\n                conn_cls = AwsConnection\n                conn_fn = lambda: AwsConnection(account, secret_key)\n\n        if datastore == 'Google':\n            # Try Google Storage\n            from cloud_browser.cloud.google import GsConnection\n            account = settings.CLOUD_BROWSER_GS_ACCOUNT\n            secret_key = settings.CLOUD_BROWSER_GS_SECRET_KEY\n            if account and secret_key:\n                conn_cls = GsConnection\n                conn_fn = lambda: GsConnection(account, secret_key)\n\n        elif datastore == 'Rackspace':\n            # Try Rackspace\n            account = settings.CLOUD_BROWSER_RACKSPACE_ACCOUNT\n            secret_key = settings.CLOUD_BROWSER_RACKSPACE_SECRET_KEY\n            servicenet = settings.CLOUD_BROWSER_RACKSPACE_SERVICENET\n            authurl = settings.CLOUD_BROWSER_RACKSPACE_AUTHURL\n            if account and secret_key:\n                from cloud_browser.cloud.rackspace import RackspaceConnection\n                conn_cls = RackspaceConnection\n                conn_fn = lambda: RackspaceConnection(\n                    account,\n                    secret_key,\n                    servicenet=servicenet,\n                    authurl=authurl)\n\n        elif datastore == 'Filesystem':\n            # Mock filesystem\n            root = settings.CLOUD_BROWSER_FILESYSTEM_ROOT\n            if root is not None:\n                from cloud_browser.cloud.fs import FilesystemConnection\n                conn_cls = FilesystemConnection\n                conn_fn = lambda: FilesystemConnection(root)\n\n        if conn_cls is None:\n            raise ImproperlyConfigured(\n                \"No suitable credentials found for datastore: %s.\" %\n                datastore)\n\n        # Adjust connection function.\n        conn_fn = staticmethod(conn_fn)\n\n        # Directly cache attributes.\n        cls.__connection_cls = conn_cls\n        cls.__connection_fn = conn_fn\n\n        return conn_cls, conn_fn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection_cls(cls):\n        if cls.__connection_cls is None:\n            cls.__connection_cls, _ = cls.from_settings()\n        return cls.__connection_cls", "response": "Return connection class.\n\n        :rtype: :class:`type`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_connection(cls):\n        if cls.__connection_obj is None:\n            if cls.__connection_fn is None:\n                _, cls.__connection_fn = cls.from_settings()\n            cls.__connection_obj = cls.__connection_fn()\n        return cls.__connection_obj", "response": "Return connection object.\n\n        :rtype: :class:`cloud_browser.cloud.base.CloudConnection`"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef base_path(self):\n        return os.path.join(self.container.base_path, self.name)", "response": "Base absolute path of container."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_path(cls, container, path):\n        from datetime import datetime\n\n        path = path.strip(SEP)\n        full_path = os.path.join(container.base_path, path)\n        last_modified = datetime.fromtimestamp(os.path.getmtime(full_path))\n        obj_type = cls.type_cls.SUBDIR if is_dir(full_path)\\\n            else cls.type_cls.FILE\n\n        return cls(container,\n                   name=path,\n                   size=os.path.getsize(full_path),\n                   content_type=None,\n                   last_modified=last_modified,\n                   obj_type=obj_type)", "response": "Create object from path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting objects from the specified path.", "response": "def get_objects(self, path, marker=None,\n                    limit=settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT):\n        \"\"\"Get objects.\"\"\"\n        def _filter(name):\n            \"\"\"Filter.\"\"\"\n            return (not_dot(name) and\n                    (marker is None or\n                     os.path.join(path, name).strip(SEP) > marker.strip(SEP)))\n\n        search_path = os.path.join(self.base_path, path)\n        objs = [self.obj_cls.from_path(self, os.path.join(path, o))\n                for o in os.listdir(search_path) if _filter(o)]\n        objs = sorted(objs, key=lambda x: x.base_path)\n        return objs[:limit]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef base_path(self):\n        return os.path.join(self.conn.abs_root, self.name)", "response": "Base absolute path of container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new object from a path.", "response": "def from_path(cls, conn, path):\n        \"\"\"Create container from path.\"\"\"\n        path = path.strip(SEP)\n        full_path = os.path.join(conn.abs_root, path)\n        return cls(conn, path, 0, os.path.getsize(full_path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a 3D lattice with the given number of times and a given number of times.", "response": "def honeycomb_lattice( a, b, spacing, alternating_sites=False ):\n    \"\"\"\n    Generate a honeycomb lattice.\n\n    Args:\n        a (Int):         Number of lattice repeat units along x.\n        b (Int):         Number of lattice repeat units along y.\n        spacing (Float): Distance between lattice sites.\n        alternating_sites (Bool, optional): Label alternating sites with 'A' and 'B'. Defaults to False.\n\n    Returns:\n        (Lattice): The new lattice\n\n    Notes:\n        The returned lattice is 3D periodic, but all sites and edges lie in the xy plane.\n    \"\"\"\n    if alternating_sites:\n        site_labels = [ 'A', 'B', 'A', 'B' ]\n    else:\n        site_labels = [ 'L', 'L', 'L', 'L' ]\n    unit_cell_lengths = np.array( [ sqrt(3), 3.0, 0.0 ] ) * spacing\n    cell_lengths = unit_cell_lengths * np.array( [ a, b, 1.0 ] )\n    grid = np.array( list( range( 1, int( a * b * 4 + 1 ) ) ) ).reshape( a, b, 4, order='C' )\n    sites = []\n    for i in range( a ):\n        for j in range( b ):\n            # site 1\n            r = np.array( [ i * sqrt(3) * spacing, j * 3 * spacing, 0.0 ] )\n            neighbours = [ grid[ i, j, 1 ],\n                           np.roll( grid, +1, axis=0 )[ i, j, 1 ],\n                           np.roll( grid, +1, axis=1 )[ i, j, 3 ] ]\n            sites.append( lattice_site.Site( grid[ i, j, 0 ], r, neighbours, 0.0, site_labels[0] ) )\n            # site 2\n            r = np.array( [ i * sqrt(3) * spacing + sqrt(3)/2 * spacing, ( j * 3 + 0.5 ) * spacing, 0.0 ] )\n            neighbours = [ grid[ i, j, 0 ], \n                           grid[ i, j, 2 ], \n                           np.roll( grid, -1, axis=0 )[ i, j, 0 ] ]\n            sites.append( lattice_site.Site( grid[ i, j, 1 ], r, neighbours, 0.0, site_labels[1] ) )\n            # site 3\n            r = np.array( [ i * sqrt(3) * spacing + sqrt(3)/2 * spacing, ( j * 3 + 1.5 ) * spacing, 0.0 ] )\n            neighbours = [ grid[ i, j, 1 ],\n                           grid[ i, j, 3 ],\n                           np.roll( grid, -1, axis=0 )[ i, j, 3 ] ]\n            sites.append( lattice_site.Site( grid[ i, j, 2 ], r, neighbours, 0.0, site_labels[2] ) )\n            # site 4\n            r = np.array( [ i * sqrt(3) * spacing, ( j * 3 + 2 ) * spacing, 0.0 ] )\n            neighbours = [ grid[ i, j, 2 ], \n                           np.roll( grid, +1, axis=0 )[ i, j, 2 ],\n                           np.roll( grid, -1, axis=1 )[ i, j, 0 ] ]\n            sites.append( lattice_site.Site( grid[ i, j, 3 ], r, neighbours, 0.0, site_labels[3] ) )\n    return lattice.Lattice( sites, cell_lengths=cell_lengths )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cubic_lattice( a, b, c, spacing ):\n    grid = np.array( list( range( 1, a * b * c + 1 ) ) ).reshape( a, b, c, order='F' )\n    it = np.nditer( grid, flags=[ 'multi_index' ] )\n    sites = []\n    while not it.finished:\n        x, y, z = it.multi_index\n        r = np.array( [ x, y, z ] ) * spacing\n        neighbours = [ np.roll( grid, +1, axis=0 )[x,y,z],\n                       np.roll( grid, -1, axis=0 )[x,y,z],\n                       np.roll( grid, +1, axis=1 )[x,y,z],\n                       np.roll( grid, -1, axis=1 )[x,y,z],\n                       np.roll( grid, +1, axis=2 )[x,y,z],\n                       np.roll( grid, -1, axis=2 )[x,y,z] ]\n        sites.append( lattice_site.Site( int( it[0] ), r, neighbours, 0.0, 'L' ) )\n        it.iternext()\n    return lattice.Lattice( sites, cell_lengths = np.array( [ a, b, c ] ) * spacing )", "response": "Generate a cubic lattice."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lattice_from_sites_file( site_file, cell_lengths ):\n    sites = []\n    site_re = re.compile( 'site:\\s+([-+]?\\d+)' )\n    r_re = re.compile( 'cent(?:er|re):\\s+([-\\d\\.e]+)\\s+([-\\d\\.e]+)\\s+([-\\d\\.e]+)' )\n    r_neighbours = re.compile( 'neighbou{0,1}rs:((\\s+[-+]?\\d+)+)' )\n    r_label = re.compile( 'label:\\s+(\\S+)' )\n    r_energy = re.compile( 'energy:\\s([-+\\d\\.]+)' )\n    with open( site_file ) as f:\n        filein = f.read().split(\"\\n\\n\")\n    number_of_sites = int( filein[0] )\n    for i in range( number_of_sites ):\n        block = filein[ i+1 ]\n        number = int( site_re.findall( block )[0] )\n        r = np.array( [ float(s) for s in r_re.findall( block )[0] ] )\n        neighbours = [ int( s ) for s in r_neighbours.findall( block )[0][0].split() ]\n        label = r_label.findall( block )[0]\n        energy = r_energy.findall( block )\n        if energy:\n            energy = float( energy[0] )\n        else:\n            energy = 0.0\n        sites.append( lattice_site.Site( number, r, neighbours, energy, label ) )\n    return lattice.Lattice( sites, cell_lengths = np.array( cell_lengths ) )", "response": "Generates a lattice from a site information file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nredirects to a given url while setting the chosen location in the the cookie.", "response": "def set_location(request):\n    \"\"\"\n    Redirect to a given url while setting the chosen location in the\n    cookie. The url and the location_id need to be\n    specified in the request parameters.\n\n    Since this view changes how the user will see the rest of the site, it must\n    only be accessed as a POST request. If called as a GET request, it will\n    redirect to the page in the request (the 'next' parameter) without changing\n    any state.\n    \"\"\"\n    next = request.GET.get('next', None) or request.POST.get('next', None)\n    if not next:\n        next = request.META.get('HTTP_REFERER', None)\n    if not next:\n        next = '/'\n    response = http.HttpResponseRedirect(next)\n    if request.method == 'POST':\n        location_id = request.POST.get('location_id', None) or request.POST.get('location', None)\n        if location_id:\n            try:\n                location = get_class(settings.GEOIP_LOCATION_MODEL).objects.get(pk=location_id)\n                storage_class(request=request, response=response).set(location=location, force=True)\n            except (ValueError, ObjectDoesNotExist):\n                pass\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef site_specific_nn_occupation( self ):\n        to_return = { l : 0 for l in set( ( site.label for site in self.p_neighbours ) ) }\n        for site in self.p_neighbours:\n            if site.is_occupied:\n             to_return[ site.label ] += 1\n        return to_return", "response": "Returns the number of occupied nearest - neighbour sites classified by site type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cn_occupation_energy( self, delta_occupation=None ):\n        nn_occupations = self.site_specific_nn_occupation()\n        if delta_occupation:\n            for site in delta_occupation:\n                assert( site in nn_occupations )\n                nn_occupations[ site ] += delta_occupation[ site ]\n        return sum( [ self.cn_occupation_energies[ s ][ n ] for s, n in nn_occupations.items() ] )", "response": "Calculates the coordination - number dependent energy for this site."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all geodata stored in database.", "response": "def clear_database(self):\n        \"\"\" Removes all geodata stored in database.\n            Useful for development, never use on production.\n        \"\"\"\n        self.logger.info('Removing obsolete geoip from database...')\n        IpRange.objects.all().delete()\n        City.objects.all().delete()\n        Region.objects.all().delete()\n        Country.objects.all().delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads and extracts the zip file and returns a dict with 2 extracted filenames", "response": "def _download_extract_archive(self, url):\n        \"\"\" Returns dict with 2 extracted filenames \"\"\"\n        self.logger.info('Downloading zipfile from ipgeobase.ru...')\n        temp_dir = tempfile.mkdtemp()\n        archive = zipfile.ZipFile(self._download_url_to_string(url))\n        self.logger.info('Extracting files...')\n        file_cities = archive.extract(settings.IPGEOBASE_CITIES_FILENAME, path=temp_dir)\n        file_cidr = archive.extract(settings.IPGEOBASE_CIDR_FILENAME, path=temp_dir)\n        return {'cities': file_cities, 'cidr': file_cidr}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _line_to_dict(self, file, field_names):\n        for line in file:\n            delimiter = settings.IPGEOBASE_FILE_FIELDS_DELIMITER\n            yield self._extract_data_from_line(line, field_names, delimiter)", "response": "Converts a line of a file into a dictonary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over ip info and extract useful data", "response": "def _process_cidr_file(self, file):\n        \"\"\" Iterate over ip info and extract useful data \"\"\"\n        data = {'cidr': list(), 'countries': set(), 'city_country_mapping': dict()}\n        allowed_countries = settings.IPGEOBASE_ALLOWED_COUNTRIES\n        for cidr_info in self._line_to_dict(file, field_names=settings.IPGEOBASE_CIDR_FIELDS):\n            city_id = cidr_info['city_id'] if cidr_info['city_id'] != '-' else None\n            if city_id is not None:\n                data['city_country_mapping'].update({cidr_info['city_id']: cidr_info['country_code']})\n\n            if allowed_countries and cidr_info['country_code'] not in allowed_countries:\n                continue\n            data['cidr'].append({'start_ip': cidr_info['start_ip'],\n                                 'end_ip': cidr_info['end_ip'],\n                                 'country_id': cidr_info['country_code'],\n                                 'city_id': city_id})\n            data['countries'].add(cidr_info['country_code'])\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating over cities info and extract useful data", "response": "def _process_cities_file(self, file, city_country_mapping):\n        \"\"\" Iterate over cities info and extract useful data \"\"\"\n        data = {'all_regions': list(), 'regions': list(), 'cities': list(), 'city_region_mapping': dict()}\n        allowed_countries = settings.IPGEOBASE_ALLOWED_COUNTRIES\n        for geo_info in self._line_to_dict(file, field_names=settings.IPGEOBASE_CITIES_FIELDS):\n            country_code = self._get_country_code_for_city(geo_info['city_id'], city_country_mapping, data['all_regions'])\n            new_region = {'name': geo_info['region_name'],\n                          'country__code': country_code}\n            if new_region not in data['all_regions']:\n                data['all_regions'].append(new_region)\n\n            if allowed_countries and country_code not in allowed_countries:\n                continue\n\n            if new_region not in data['regions']:\n                data['regions'].append(new_region)\n            data['cities'].append({'region__name': geo_info['region_name'],\n                                   'name': geo_info['city_name'],\n                                   'id': geo_info['city_id'],\n                                   'latitude': Decimal(geo_info['latitude']),\n                                   'longitude': Decimal(geo_info['longitude'])})\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating database with new countries regions and cities.", "response": "def _update_geography(self, countries, regions, cities, city_country_mapping):\n        \"\"\" Update database with new countries, regions and cities \"\"\"\n        existing = {\n            'cities': list(City.objects.values_list('id', flat=True)),\n            'regions': list(Region.objects.values('name', 'country__code')),\n            'countries': Country.objects.values_list('code', flat=True)\n        }\n        for country_code in countries:\n            if country_code not in existing['countries']:\n                Country.objects.create(code=country_code, name=ISO_CODES.get(country_code, country_code))\n        for entry in regions:\n            if entry not in existing['regions']:\n                Region.objects.create(name=entry['name'], country_id=entry['country__code'])\n        for entry in cities:\n            if int(entry['id']) not in existing['cities']:\n                code = city_country_mapping.get(entry['id'])\n                if code:\n                    region = Region.objects.get(name=entry['region__name'], country__code=code)\n                    City.objects.create(id=entry['id'], name=entry['name'], region=region,\n                                    latitude=entry.get('latitude'), longitude=entry.get('longitude'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the IPRegion table with new data.", "response": "def _update_cidr(self, cidr):\n        \"\"\" Rebuild IPRegion table with fresh data (old ip ranges are removed for simplicity)\"\"\"\n        new_ip_ranges = []\n        is_bulk_create_supported = hasattr(IpRange.objects, 'bulk_create')\n        IpRange.objects.all().delete()\n        city_region_mapping = self._build_city_region_mapping()\n\n        if self.logger.getEffectiveLevel() in [logging.INFO, logging.DEBUG]:\n            pbar = ProgressBar(widgets=[Percentage(), ' ', Bar()])\n        else:\n            pbar = iter\n        for entry in pbar(cidr['cidr']):\n            # skipping for country rows\n            if entry['city_id']:\n                entry.update({'region_id': city_region_mapping[int(entry['city_id'])]})\n            if is_bulk_create_supported:\n                new_ip_ranges.append(IpRange(**entry))\n\n                if len(new_ip_ranges) >= self.BULK_CHAIN:\n                    IpRange.objects.bulk_create(new_ip_ranges)\n                    new_ip_ranges = []\n            else:\n                IpRange.objects.create(**entry)\n\n        if is_bulk_create_supported and new_ip_ranges:\n            IpRange.objects.bulk_create(new_ip_ranges)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the relative probability of a jump between two sites with specific site types and coordination numbers.", "response": "def relative_probability( self, l1, l2, c1, c2 ):\n        \"\"\"\n        The relative probability for a jump between two sites with specific site types and coordination numbers.\n\n        Args:\n            l1 (Str): Site label for the initial site.\n            l2 (Str): Site label for the final site.\n            c1 (Int): Coordination number for the initial site.\n            c2 (Int): Coordination number for the final site.\n   \n        Returns:\n            (Float): The relative probability of this jump occurring.\n        \"\"\"\n        if self.site_energies:\n            site_delta_E = self.site_energies[ l2 ] - self.site_energies[ l1 ]\n        else:\n            site_delta_E = 0.0\n        if self.nn_energy:\n            delta_nn = c2 - c1 - 1 # -1 because the hopping ion is not counted in the final site occupation number\n            site_delta_E += delta_nn * self.nn_energy\n        return metropolis( site_delta_E )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a look - up table of relative jump probabilities for a nearest - neighbour interaction Hamiltonian.", "response": "def generate_nearest_neighbour_lookup_table( self ):\n        \"\"\"\n        Construct a look-up table of relative jump probabilities for a nearest-neighbour interaction Hamiltonian.\n\n        Args:\n            None.\n\n        Returns:\n            None.\n        \"\"\"\n        self.jump_probability = {}\n        for site_label_1 in self.connected_site_pairs:\n            self.jump_probability[ site_label_1 ] = {}\n            for site_label_2 in self.connected_site_pairs[ site_label_1 ]:\n                self.jump_probability[ site_label_1 ][ site_label_2 ] = {}\n                for coordination_1 in range( self.max_coordination_per_site[ site_label_1 ] ):\n                    self.jump_probability[ site_label_1 ][ site_label_2 ][ coordination_1 ] = {}\n                    for coordination_2 in range( 1, self.max_coordination_per_site[ site_label_2 ] + 1 ):\n                        self.jump_probability[ site_label_1 ][ site_label_2 ][ coordination_1 ][ coordination_2 ] = self.relative_probability( site_label_1, site_label_2, coordination_1, coordination_2 )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the internal state of the internal state of the object.", "response": "def reset( self ):\n        \"\"\"\n        Reinitialise the stored displacements, number of hops, and list of sites visited for this `Atom`.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.number_of_hops = 0\n        self.dr = np.array( [ 0.0, 0.0, 0.0 ] )\n        self.summed_dr2 = 0.0\n        self.sites_visited = [ self._site.number ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef by_ip(self, ip):\n        try:\n            number = inet_aton(ip)\n        except Exception:\n            raise IpRange.DoesNotExist\n\n        try:\n            return self.filter(start_ip__lte=number, end_ip__gte=number)\\\n                       .order_by('end_ip', '-start_ip')[0]\n        except IndexError:\n            raise IpRange.DoesNotExist", "response": "Return a new object containing the smallest IP range containing the given IP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting string or bool to bool.", "response": "def _parse_bool(value):\n    \"\"\"Convert ``string`` or ``bool`` to ``bool``.\"\"\"\n    if isinstance(value, bool):\n        return value\n\n    elif isinstance(value, str):\n        if value == 'True':\n            return True\n        elif value == 'False':\n            return False\n\n    raise Exception(\"Value %s is not boolean.\" % value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating API documentation using Sphinx.", "response": "def docs(output=DOC_OUTPUT, proj_settings=PROJ_SETTINGS, github=False):\n    \"\"\"Generate API documentation (using Sphinx).\n\n    :param output: Output directory.\n    :param proj_settings: Django project settings to use.\n    :param github: Convert to GitHub-friendly format?\n    \"\"\"\n\n    local(\"export PYTHONPATH='' && \"\n          \"export DJANGO_SETTINGS_MODULE=%s && \"\n          \"sphinx-build -b html %s %s\" % (proj_settings, DOC_INPUT, output),\n          capture=False)\n\n    if _parse_bool(github):\n        local(\"touch %s/.nojekyll\" % output, capture=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _manage(target, extra='', proj_settings=PROJ_SETTINGS):\n    local(\"export PYTHONPATH='' && \"\n          \"export DJANGO_SETTINGS_MODULE='%s' && \"\n          \"django-admin.py %s %s\" %\n          (proj_settings, target, extra),\n          capture=False)", "response": "Generic wrapper for django - admin. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_response(self, request, response):\n        if not hasattr(request, 'location'):\n            return response\n\n        storage = storage_class(request=request, response=response)\n        try:\n            storage.set(location=request.location)\n        except ValueError:\n            # bad location_id\n            pass\n        return response", "response": "Process the response and store it in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef types(**typefuncs):\n    def wrap(f):\n        @functools.wraps(f)\n        def typed_func(*pargs, **kwargs):\n            # Analyze the incoming arguments so we know how to apply the\n            # type-conversion functions in `typefuncs`.\n            argspec = inspect.getargspec(f)\n\n            # The `args` property contains the list of named arguments passed to\n            # f.  Construct a dict mapping from these names to the values that\n            # were passed.\n            #\n            # It is possible that `args` contains names that are not represented\n            # in `pargs`, if some of the arguments are passed as keyword\n            # arguments.  In this case, the relative shortness of `pargs` will\n            # cause the call to zip() to truncate the `args` list, and the\n            # keyword-style passed arguments will simply be present in `kwargs`.\n            pargs_dict = {name: value for (name, value) in zip(argspec.args, pargs)}\n\n            # Begin converting arguments according to the functions given in\n            # `typefuncs`.  If a given name does not appear in `typefuncs`,\n            # simply leave it unchanged.  If a name appears in `typefuncs` that\n            # does not appear in the argument list, this is considered an error.\n            try:\n                for name, func in typefuncs.iteritems():\n                    if name in pargs_dict:\n                        pargs_dict[name] = func(pargs_dict[name])\n                    elif name in kwargs:\n                        kwargs[name] = func(kwargs[name])\n                    else:\n                        http_status(400, \"Unknown Argument Name\")\n                        content_type(\"application/json\")\n                        return {\"error\": \"'%s' was registered for type conversion but did not appear in the arguments list\" % (name)}\n            except ValueError as e:\n                http_status(400, \"Input Value Conversion Failed\")\n                content_type(\"application/json\")\n                return {\"error\": str(e)}\n\n            # Unroll `pargs` into a list of arguments that are in the correct\n            # order.\n            pargs = []\n            for name in argspec.args:\n                try:\n                    pargs.append(pargs_dict[name])\n                except KeyError:\n                    break\n\n            # Call the wrapped function using the converted arguments.\n            return f(*pargs, **kwargs)\n\n        typed_func.typefuncs = typefuncs\n        return typed_func\n    return wrap", "response": "Decorator that returns a function that takes typed values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecorate a function to automatically convert its return type to a string using a custom function. Web-based service functions must return text to the client. Tangelo contains default logic to convert many kinds of values into string, but this decorator allows the service writer to specify custom behavior falling outside of the default. If the conversion fails, an appropriate server error will be raised.", "response": "def return_type(rettype):\n    \"\"\"\n    Decorate a function to automatically convert its return type to a string\n    using a custom function.\n\n    Web-based service functions must return text to the client.  Tangelo\n    contains default logic to convert many kinds of values into string, but this\n    decorator allows the service writer to specify custom behavior falling\n    outside of the default.  If the conversion fails, an appropriate server\n    error will be raised.\n    \"\"\"\n    def wrap(f):\n        @functools.wraps(f)\n        def converter(*pargs, **kwargs):\n            # Run the function to capture the output.\n            result = f(*pargs, **kwargs)\n\n            # Convert the result using the return type function.\n            try:\n                result = rettype(result)\n            except ValueError as e:\n                http_status(500, \"Return Value Conversion Failed\")\n                content_type(\"application/json\")\n                return {\"error\": str(e)}\n            return result\n\n        return converter\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning tuple of underlying exception classes to trap and wrap.", "response": "def excepts(cls):\n        \"\"\"Return tuple of underlying exception classes to trap and wrap.\n\n        :rtype: ``tuple`` of ``type``\n        \"\"\"\n        if cls._excepts is None:\n            cls._excepts = tuple(cls.translations.keys())\n        return cls._excepts"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns translation of exception to new class.", "response": "def translate(self, exc):\n        \"\"\"Return translation of exception to new class.\n\n        Calling code should only raise exception if exception class is passed\n        in, else ``None`` (which signifies no wrapping should be done).\n        \"\"\"\n        # Find actual class.\n        for key in self.translations.keys():\n            if isinstance(exc, key):\n                # pylint: disable=unsubscriptable-object\n                return self.translations[key](str(exc))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef settings_view_decorator(function):\n\n    dec = settings.CLOUD_BROWSER_VIEW_DECORATOR\n\n    # Trade-up string to real decorator.\n    if isinstance(dec, str):\n        # Split into module and decorator strings.\n        mod_str, _, dec_str = dec.rpartition('.')\n        if not (mod_str and dec_str):\n            raise ImportError(\"Unable to import module: %s\" % mod_str)\n\n        # Import and try to get decorator function.\n        mod = import_module(mod_str)\n        if not hasattr(mod, dec_str):\n            raise ImportError(\"Unable to import decorator: %s\" % dec)\n\n        dec = getattr(mod, dec_str)\n\n    if dec and callable(dec):\n        return dec(function)\n\n    return function", "response": "Insert decorator from settings if any. a\n   .. note :: Decorator in settings. CLOUD_BROWSER_VIEW_DECORATOR can be either a string path or a fully - qualified string path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _breadcrumbs(path):\n\n    full = None\n    crumbs = []\n    for part in path_yield(path):\n        full = path_join(full, part) if full else part\n        crumbs.append((full, part))\n\n    return crumbs", "response": "Return a dict of breadcrumb parts from a path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef browser(request, path='', template=\"cloud_browser/browser.html\"):\n\n    from itertools import islice\n\n    try:\n        # pylint: disable=redefined-builtin\n        from future_builtins import filter\n    except ImportError:\n        # pylint: disable=import-error\n        from builtins import filter\n\n    # Inputs.\n    container_path, object_path = path_parts(path)\n    incoming = request.POST or request.GET or {}\n\n    marker = incoming.get('marker', None)\n    marker_part = incoming.get('marker_part', None)\n    if marker_part:\n        marker = path_join(object_path, marker_part)\n\n    # Get and adjust listing limit.\n    limit_default = settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT\n\n    def limit_test(num):\n        return num > 0 and (MAX_LIMIT is None or num <= MAX_LIMIT - 1)\n\n    limit = get_int(incoming.get('limit', limit_default),\n                    limit_default,\n                    limit_test)\n\n    # Q1: Get all containers.\n    #     We optimize here by not individually looking up containers later,\n    #     instead going through this in-memory list.\n    # TODO: Should page listed containers with a ``limit`` and ``marker``.\n    conn = get_connection()\n    containers = conn.get_containers()\n\n    marker_part = None\n    container = None\n    objects = None\n    if container_path != '':\n        # Find marked container from list.\n        def cont_eq(container):\n            return container.name == container_path\n        filtered_conts = filter(cont_eq, containers)\n        cont_list = list(islice(filtered_conts, 1))\n        if not cont_list:\n            raise Http404(\"No container at: %s\" % container_path)\n\n        # Q2: Get objects for instant list, plus one to check \"next\".\n        container = cont_list[0]\n        objects = container.get_objects(object_path, marker, limit + 1)\n        marker = None\n\n        # If over limit, strip last item and set marker.\n        if len(objects) == limit + 1:\n            objects = objects[:limit]\n            marker = objects[-1].name\n            marker_part = relpath(marker, object_path)\n\n    return render(request, template,\n                  {'path': path,\n                   'marker': marker,\n                   'marker_part': marker_part,\n                   'limit': limit,\n                   'breadcrumbs': _breadcrumbs(path),\n                   'container_path': container_path,\n                   'containers': containers,\n                   'container': container,\n                   'object_path': object_path,\n                   'objects': objects})", "response": "View a file in a file path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nviews single document from path.", "response": "def document(_, path=''):\n    \"\"\"View single document from path.\n\n    :param path: Path to resource, including container as first part of path.\n    \"\"\"\n    container_path, object_path = path_parts(path)\n    conn = get_connection()\n    try:\n        container = conn.get_container(container_path)\n    except errors.NoContainerException:\n        raise Http404(\"No container at: %s\" % container_path)\n    except errors.NotPermittedException:\n        raise Http404(\"Access denied for container at: %s\" % container_path)\n\n    try:\n        storage_obj = container.get_object(object_path)\n    except errors.NoObjectException:\n        raise Http404(\"No object at: %s\" % object_path)\n\n    # Get content-type and encoding.\n    content_type = storage_obj.smart_content_type\n    encoding = storage_obj.smart_content_encoding\n    response = HttpResponse(content=storage_obj.read(),\n                            content_type=content_type)\n    if encoding not in (None, ''):\n        response['Content-Encoding'] = encoding\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntruncates string to a specified number of characters.", "response": "def truncatechars(value, num, end_text=\"...\"):\n    \"\"\"Truncate string on character boundary.\n\n    .. note::\n        Django ticket `5025 <http://code.djangoproject.com/ticket/5025>`_ has a\n        patch for a more extensible and robust truncate characters tag filter.\n\n    Example::\n\n        {{ my_variable|truncatechars:22 }}\n\n    :param value: Value to truncate.\n    :type  value: ``string``\n    :param num: Number of characters to trim to.\n    :type  num: ``int``\n    \"\"\"\n    length = None\n    try:\n        length = int(num)\n    except ValueError:\n        pass\n\n    if length is not None and len(value) > length:\n        return value[:length - len(end_text)] + end_text\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cloud_browser_media_url(_, token):\n    bits = token.split_contents()\n    if len(bits) != 2:\n        raise TemplateSyntaxError(\"'%s' takes one argument\" % bits[0])\n    rel_path = bits[1]\n\n    return MediaUrlNode(rel_path)", "response": "Returns the base media URL for application static media."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset( self ):\n        self.lattice.reset()\n        for atom in self.atoms.atoms:\n            atom.reset()", "response": "Reset all counters for this simulation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_number_of_atoms( self, n, selected_sites=None ):\n        self.number_of_atoms = n\n        self.atoms = species.Species( self.lattice.populate_sites( self.number_of_atoms, selected_sites=selected_sites ) )", "response": "Set the number of atoms for the simulation and populate the simulation lattice."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef define_lattice_from_file( self, filename, cell_lengths ):\n        self.lattice = init_lattice.lattice_from_sites_file( filename, cell_lengths = cell_lengths )", "response": "Define the simulation lattice from a file containing site data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_initialised( self ):\n        if not self.lattice:\n            raise AttributeError('Running a simulation needs the lattice to be initialised')\n        if not self.atoms:\n            raise AttributeError('Running a simulation needs the atoms to be initialised')\n        if not self.number_of_jumps and not self.for_time:\n            raise AttributeError('Running a simulation needs number_of_jumps or for_time to be set')", "response": "Check whether the simulation has been initialised."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run( self, for_time=None ):\n        self.for_time = for_time\n        try:\n            self.is_initialised()\n        except AttributeError:\n            raise\n        if self.number_of_equilibration_jumps > 0:\n            for step in range( self.number_of_equilibration_jumps ):\n                self.lattice.jump()\n            self.reset()\n        if self.for_time:\n            self.number_of_jumps = 0\n            while self.lattice.time < self.for_time:\n                self.lattice.jump()\n                self.number_of_jumps += 1\n        else: \n            for step in range( self.number_of_jumps ):\n                self.lattice.jump()\n        self.has_run = True", "response": "Run the simulation.\n\n        Args:\n            for_time (:obj:Float, optional): If `for_time` is set, then run the simulation until a set amount of time has passed. Otherwise, run the simulation for a set number of jumps. Defaults to None.\n\n        Returns:\n            None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeprecating tracer correlation factor for this simulation.", "response": "def old_tracer_correlation( self ):\n        \"\"\"\n        Deprecated tracer correlation factor for this simulation.\n       \n        Args:\n            None\n\n        Returns:\n            (Float): The tracer correlation factor, f.\n\n        Notes:\n            This function assumes that the jump distance between sites has\n            been normalised to a=1. If the jump distance is not equal to 1\n            then the value returned by this function should be divided by a^2.\n            Even better, use `self.tracer_correlation`.\n        \"\"\"\n        if self.has_run:\n            return self.atoms.sum_dr_squared() / float( self.number_of_jumps )\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn tracer diffusion coefficient", "response": "def tracer_diffusion_coefficient( self ):\n        \"\"\"\n        Tracer diffusion coefficient, D*.\n\n        Args:\n            None\n\n        Returns:\n            (Float): The tracer diffusion coefficient, D*.\n        \"\"\"\n        if self.has_run:\n            return self.atoms.sum_dr_squared() / ( 6.0 * float( self.number_of_atoms ) * self.lattice.time )\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the collective correlation factor of the current has", "response": "def old_collective_correlation( self ):\n        \"\"\"\n        Returns the collective correlation factor, f_I\n\n        Args:\n            None\n\n        Returns:\n            (Float): The collective correlation factor, f_I.\n\n        Notes:\n            This function assumes that the jump distance between sites has\n            been normalised to a=1. If the jumps distance is not equal to 1\n            then the value returned by this function should be divided by a^2.\n            Even better, use self.collective_correlation\n        \"\"\"\n        if self.has_run:\n            return self.atoms.collective_dr_squared() / float( self.number_of_jumps )\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the collective or jump diffusion coefficient of the current object.", "response": "def collective_diffusion_coefficient( self ):\n        \"\"\"\n        Returns the collective or \"jump\" diffusion coefficient, D_J.\n\n        Args:\n            None\n\n        Returns:\n            (Float): The collective diffusion coefficient, D_J.\n        \"\"\"\n        if self.has_run:\n            return self.atoms.collective_dr_squared() / ( 6.0 * self.lattice.time )\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_lookup_table( self, hamiltonian='nearest-neighbour' ):\n        expected_hamiltonian_values = [ 'nearest-neighbour', 'coordination_number' ]\n        if hamiltonian not in expected_hamiltonian_values:\n            raise ValueError\n        self.lattice.jump_lookup_table = lookup_table.LookupTable( self.lattice, hamiltonian )", "response": "Create a jump - probability look - up table corresponding to the appropriate Hamiltonian."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenforce periodic boundary conditions for the central simulation cell.", "response": "def enforce_periodic_boundary_conditions( self ):\n        \"\"\"\n        Ensure that all lattice sites are within the central periodic image of the simulation cell.\n        Sites that are outside the central simulation cell are mapped back into this cell.\n        \n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        for s in self.sites:\n            for i in range(3):\n                if s.r[i] < 0.0:\n                    s.r[i] += self.cell_lengths[i]\n                if s.r[i] > self.cell_lengths[i]:\n                    s.r[i] -= self.cell_lengths[i]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitialises the site lookup table for this object.", "response": "def initialise_site_lookup_table( self ):\n        \"\"\"\n        Create a lookup table allowing sites in this lattice to be queried using `self.site_lookup[n]` where `n` is the identifying site numbe.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.site_lookup = {}\n        for site in self.sites:\n            self.site_lookup[ site.number ] = site"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef potential_jumps( self ):\n        jumps = []\n        if self.number_of_occupied_sites <= self.number_of_sites / 2:\n            for occupied_site in self.occupied_sites():\n                unoccupied_neighbours = [ site for site in [ self.site_with_id( n ) for n in occupied_site.neighbours ] if not site.is_occupied ]\n                for vacant_site in unoccupied_neighbours:\n                    jumps.append( jump.Jump( occupied_site, vacant_site, self.nn_energy, self.cn_energies, self.jump_lookup_table ) )\n        else:\n            for vacant_site in self.vacant_sites():\n                occupied_neighbours = [ site for site in [ self.site_with_id( n ) for n in vacant_site.neighbours ] if site.is_occupied ]\n                for occupied_site in occupied_neighbours:\n                    jumps.append( jump.Jump( occupied_site, vacant_site, self.nn_energy, self.cn_energies, self.jump_lookup_table ) )\n        return jumps", "response": "Returns a list of possible jumps."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the lattice state by accepting a specific jump.", "response": "def update( self, jump ):\n        \"\"\"\n        Update the lattice state by accepting a specific jump\n\n        Args:\n            jump (Jump): The jump that has been accepted.\n\n        Returns:\n            None.\n        \"\"\"\n        atom = jump.initial_site.atom\n        dr = jump.dr( self.cell_lengths )\n        #print( \"atom {} jumped from site {} to site {}\".format( atom.number, jump.initial_site.number, jump.final_site.number ) )\n        jump.final_site.occupation = atom.number\n        jump.final_site.atom = atom\n        jump.final_site.is_occupied = True\n        jump.initial_site.occupation = 0\n        jump.initial_site.atom = None\n        jump.initial_site.is_occupied = False\n        # TODO: updating atom counters could be contained in an atom.move_to( site ) method\n        atom.site = jump.final_site\n        atom.number_of_hops += 1\n        atom.dr += dr\n        atom.summed_dr2 += np.dot( dr, dr )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the lattice sites with a specific number of atoms.", "response": "def populate_sites( self, number_of_atoms, selected_sites=None ):\n        \"\"\"\n        Populate the lattice sites with a specific number of atoms.\n\n        Args:\n            number_of_atoms (Int): The number of atoms to populate the lattice sites with.\n            selected_sites (:obj:List, optional): List of site labels if only some sites are to be occupied. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n        if number_of_atoms > self.number_of_sites:\n            raise ValueError\n        if selected_sites:\n            atoms = [ atom.Atom( initial_site = site ) for site in random.sample( [ s for s in self.sites if s.label in selected_sites ], number_of_atoms ) ]\n        else:\n            atoms = [ atom.Atom( initial_site = site ) for site in random.sample( self.sites, number_of_atoms ) ]\n        self.number_of_occupied_sites = number_of_atoms\n        return atoms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jump( self ):\n        potential_jumps = self.potential_jumps()\n        if not potential_jumps:\n            raise BlockedLatticeError('No moves are possible in this lattice')\n        all_transitions = transitions.Transitions( self.potential_jumps() )\n        random_jump = all_transitions.random()\n        delta_t = all_transitions.time_to_jump()\n        self.time += delta_t\n        self.update_site_occupation_times( delta_t )\n        self.update( random_jump )\n        return( all_transitions.time_to_jump() )", "response": "Select a jump at random from all potential jumps then update the lattice state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of site occupation statistics for each site type.", "response": "def site_occupation_statistics( self ):\n        \"\"\"\n        Average site occupation for each site type\n\n        Args:\n            None\n\n        Returns:\n            (Dict(Str:Float)): Dictionary of occupation statistics, e.g.::\n\n                { 'A' : 2.5, 'B' : 25.3 } \n        \"\"\"\n        if self.time == 0.0:\n            return None\n        occupation_stats = { label : 0.0 for label in self.site_labels }\n        for site in self.sites:\n            occupation_stats[ site.label ] += site.time_occupied\n        for label in self.site_labels:\n            occupation_stats[ label ] /= self.time\n        return occupation_stats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the energies for every site in the lattice according to the site labels.", "response": "def set_site_energies( self, energies ):\n        \"\"\"\n        Set the energies for every site in the lattice according to the site labels.\n\n        Args:\n            energies (Dict(Str:Float): Dictionary of energies for each site label, e.g.::\n\n                { 'A' : 1.0, 'B', 0.0 }\n\n        Returns:\n            None\n        \"\"\"\n        self.site_energies = energies\n        for site_label in energies:\n            for site in self.sites:\n                if site.label == site_label:\n                    site.energy = energies[ site_label ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the coordination number dependent energies for each site type.", "response": "def set_cn_energies( self, cn_energies ):\n        \"\"\"\n        Set the coordination number dependent energies for this lattice.\n\n        Args:\n            cn_energies (Dict(Str:Dict(Int:Float))): Dictionary of dictionaries specifying the coordination number dependent energies for each site type. e.g.::\n\n                { 'A' : { 0 : 0.0, 1 : 1.0, 2 : 2.0 }, 'B' : { 0 : 0.0, 1 : 2.0 } }\n\n        Returns:\n            None\n        \"\"\"\n        for site in self.sites:\n            site.set_cn_occupation_energies( cn_energies[ site.label ] )\n        self.cn_energies = cn_energies"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of the coordination numbers for each site label. e. g. A 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 B 2 4 4.", "response": "def site_coordination_numbers( self ):\n        \"\"\"\n        Returns a dictionary of the coordination numbers for each site label. e.g.::\n        \n            { 'A' : { 4 }, 'B' : { 2, 4 } }\n \n        Args:\n            none\n\n        Returns:\n            coordination_numbers (Dict(Str:Set(Int))): dictionary of coordination\n                                                       numbers for each site label.\n        \"\"\"\n        coordination_numbers = {}\n        for l in self.site_labels:\n            coordination_numbers[ l ] = set( [ len( site.neighbours ) for site in self.sites if site.label is l ] ) \n        return coordination_numbers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef max_site_coordination_numbers( self ):\n        return { l : max( c ) for l, c in self.site_coordination_numbers().items() }", "response": "Returns a dictionary of the maximum coordination number for each site label."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of coordination numbers for each site type.", "response": "def site_specific_coordination_numbers( self ):\n        \"\"\"\n        Returns a dictionary of coordination numbers for each site type.\n\n        Args:\n            None\n\n        Returns:\n            (Dict(Str:List(Int))) : Dictionary of coordination numbers for each site type, e.g.::\n\n                { 'A' : [ 2, 4 ], 'B' : [ 2 ] }\n        \"\"\"\n        specific_coordination_numbers = {}\n        for site in self.sites:\n            specific_coordination_numbers[ site.label ] = site.site_specific_neighbours()\n        return specific_coordination_numbers"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of all connected site pairs in the lattice.", "response": "def connected_site_pairs( self ):\n        \"\"\"\n        Returns a dictionary of all connections between pair of sites (by site label).\n        e.g. for a linear lattice A-B-C will return::\n        \n            { 'A' : [ 'B' ], 'B' : [ 'A', 'C' ], 'C' : [ 'B' ] }\n\n        Args:\n            None\n\n        Returns:\n            site_connections (Dict{Str List[Str]}): A dictionary of neighbouring site types in the lattice.\n        \"\"\"\n        site_connections = {}\n        for initial_site in self.sites:\n            if not initial_site.label in site_connections:\n                site_connections[ initial_site.label ] = []\n            for final_site in initial_site.p_neighbours:\n                if final_site.label not in site_connections[ initial_site.label ]:\n                    site_connections[ initial_site.label ].append( final_site.label )\n        return site_connections"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transmute_sites( self, old_site_label, new_site_label, n_sites_to_change ):\n        selected_sites = self.select_sites( old_site_label )\n        for site in random.sample( selected_sites, n_sites_to_change ):\n            site.label = new_site_label\n        self.site_labels = set( [ site.label for site in self.sites ] )", "response": "Selects a random subset of sites with a specific label and gives them a different label."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of sets of cluster objects for all contiguous sites in the lattice.", "response": "def connected_sites( self, site_labels=None ):\n        \"\"\"\n        Searches the lattice to find sets of sites that are contiguously neighbouring.\n        Mutually exclusive sets of contiguous sites are returned as Cluster objects.\n\n        Args:\n            site_labels (:obj:(List(Str)|Set(Str)|Str), optional): Labels for sites to be considered in the search.\n                This can be a list::\n\n                    [ 'A', 'B' ]\n\n                a set::\n\n                    ( 'A', 'B' )\n\n                or a string::\n\n                    'A'.\n\n        Returns:\n            (List(Cluster)): List of Cluster objects for groups of contiguous sites.\n        \"\"\"\n        if site_labels:\n           selected_sites = self.select_sites( site_labels )\n        else:\n           selected_sites = self.sites\n        initial_clusters = [ cluster.Cluster( [ site ] ) for site in selected_sites ]\n        if site_labels:\n            blocking_sites = self.site_labels - set( site_labels )\n            for c in initial_clusters:\n                c.remove_sites_from_neighbours( blocking_sites )\n        final_clusters = []\n        while initial_clusters: # loop until initial_clusters is empty\n            this_cluster = initial_clusters.pop(0)\n            while this_cluster.neighbours:\n                neighbouring_clusters = [ c for c in initial_clusters if this_cluster.is_neighbouring( c ) ] \n                for nc in neighbouring_clusters:\n                    initial_clusters.remove( nc )\n                    this_cluster = this_cluster.merge( nc ) \n            final_clusters.append( this_cluster )\n        return final_clusters"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_sites( self, site_labels ):\n        if type( site_labels ) in ( list, set ):\n            selected_sites = [ s for s in self.sites if s.label in site_labels ]\n        elif type( site_labels ) is str:\n            selected_sites = [ s for s in self.sites if s.label is site_labels ]\n        else:\n            raise ValueError( str( site_labels ) )\n        return selected_sites", "response": "Selects sites in the lattice with specified labels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all sites in the lattice that are not percolating.", "response": "def detached_sites( self, site_labels=None ):\n        \"\"\"\n        Returns all sites in the lattice (optionally from the set of sites with specific labels)\n        that are not part of a percolating network.\n        This is determined from clusters of connected sites that do not wrap round to\n        themselves through a periodic boundary.\n\n        Args:\n            site_labels (String or List(String)): Lables of sites to be considered.\n\n        Returns:\n            (List(Site)): List of sites not in a periodic percolating network.\n        \"\"\"\n        clusters = self.connected_sites( site_labels=site_labels )\n        island_clusters = [ c for c in clusters if not any( c.is_periodically_contiguous() ) ]\n        return list( itertools.chain.from_iterable( ( c.sites for c in island_clusters ) ) )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge( self, other_cluster ):\n        new_cluster = Cluster( self.sites | other_cluster.sites )\n        new_cluster.neighbours = ( self.neighbours | other_cluster.neighbours ).difference( new_cluster.sites )\n        return new_cluster", "response": "Combine two clusters into a single cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sites_at_edges( self ):\n        min_x = min( [ s.r[0] for s in self.sites ] )\n        max_x = max( [ s.r[0] for s in self.sites ] )\n        min_y = min( [ s.r[1] for s in self.sites ] )\n        max_y = max( [ s.r[1] for s in self.sites ] )\n        min_z = min( [ s.r[2] for s in self.sites ] )\n        max_z = max( [ s.r[2] for s in self.sites ] )\n        x_max = [ s for s in self.sites if s.r[0] == min_x ]\n        x_min = [ s for s in self.sites if s.r[0] == max_x ]\n        y_max = [ s for s in self.sites if s.r[1] == min_y ]\n        y_min = [ s for s in self.sites if s.r[1] == max_y ]\n        z_max = [ s for s in self.sites if s.r[2] == min_z ]\n        z_min = [ s for s in self.sites if s.r[2] == max_z ]\n        return ( x_max, x_min, y_max, y_min, z_max, z_min )", "response": "Finds the six sites with the maximum and minimum coordinates along x y and z."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a boolean indicating whether a cluster is strictly contiguous.", "response": "def is_periodically_contiguous( self ):\n        \"\"\"\n        logical check whether a cluster connects with itself across the\n        simulation periodic boundary conditions.\n\n        Args:\n            none\n\n        Returns\n            ( Bool, Bool, Bool ): Contiguity along the x, y, and z coordinate axes\n        \"\"\"\n        edges = self.sites_at_edges()\n        is_contiguous = [ False, False, False ]\n        along_x = any( [ s2 in s1.p_neighbours for s1 in edges[0] for s2 in edges[1] ] )\n        along_y = any( [ s2 in s1.p_neighbours for s1 in edges[2] for s2 in edges[3] ] )\n        along_z = any( [ s2 in s1.p_neighbours for s1 in edges[4] for s2 in edges[5] ] )\n        return ( along_x, along_y, along_z )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_sites_from_neighbours( self, remove_labels ):\n        if type( remove_labels ) is str:\n            remove_labels = [ remove_labels ]\n        self.neighbours = set( n for n in self.neighbours if n.label not in remove_labels )", "response": "Removes sites from the set of neighbouring sites if these have labels in remove_labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cumulative_probabilities( self ):\n        partition_function = np.sum( self.p )\n        return np.cumsum( self.p ) / partition_function", "response": "Returns the cumulative sum of the relative probabilities for all possible jumps."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects a random jump at random with appropriate relative probabilities.", "response": "def random( self ):\n        \"\"\"\n        Select a jump at random with appropriate relative probabilities.\n\n        Args:\n            None\n\n        Returns:\n            (Jump): The randomly selected Jump.\n        \"\"\"\n        j = np.searchsorted( self.cumulative_probabilities(), random.random() )\n        return self.jumps[ j ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the timestep until the next jump.", "response": "def time_to_jump( self ):\n        \"\"\"\n        The timestep until the next jump.\n\n        Args:\n            None\n\n        Returns:\n            (Float): The timestep until the next jump.\n        \"\"\" \n        k_tot = rate_prefactor * np.sum( self.p )\n        return -( 1.0 / k_tot ) * math.log( random.random() )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef locate(self):\n        stored_location = self._get_stored_location()\n        if not stored_location:\n            ip_range = self._get_ip_range()\n            stored_location = self._get_corresponding_location(ip_range)\n        return stored_location", "response": "Find out what is user location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_real_ip(self):\n        try:\n            # Trying to work with most common proxy headers\n            real_ip = self.request.META['HTTP_X_FORWARDED_FOR']\n            return real_ip.split(',')[0]\n        except KeyError:\n            return self.request.META['REMOTE_ADDR']\n        except Exception:\n            # Unknown IP\n            return None", "response": "Get IP from request.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching IpRange instance if request IP is found in database.", "response": "def _get_ip_range(self):\n        \"\"\"\n        Fetches IpRange instance if request IP is found in database.\n\n        :param request: A ususal request object\n        :type request: HttpRequest\n        :return: IpRange object or None\n        \"\"\"\n        ip = self._get_real_ip()\n        try:\n            geobase_entry = IpRange.objects.by_ip(ip)\n        except IpRange.DoesNotExist:\n            geobase_entry = None\n        return geobase_entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the location from cookie.", "response": "def _get_stored_location(self):\n        \"\"\" Get location from cookie.\n\n        :param request: A ususal request object\n        :type request: HttpRequest\n        :return: Custom location model\n        \"\"\"\n        location_storage = storage_class(request=self.request, response=None)\n        return location_storage.get()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_info(cls, container, info_obj):\n        create_fn = cls.from_subdir if 'subdir' in info_obj \\\n            else cls.from_file_info\n        return create_fn(container, info_obj)", "response": "Create from subdirectory or file info object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate from subdirectory info object.", "response": "def from_subdir(cls, container, info_obj):\n        \"\"\"Create from subdirectory info object.\"\"\"\n        return cls(container,\n                   info_obj['subdir'],\n                   obj_type=cls.type_cls.SUBDIR)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchoose object type from content type.", "response": "def choose_type(cls, content_type):\n        \"\"\"Choose object type from content type.\"\"\"\n        return cls.type_cls.SUBDIR if content_type in cls.subdir_types \\\n            else cls.type_cls.FILE"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating from regular info object.", "response": "def from_file_info(cls, container, info_obj):\n        \"\"\"Create from regular info object.\"\"\"\n        # RFC 8601: 2010-04-15T01:52:13.919070\n        return cls(container,\n                   name=info_obj['name'],\n                   size=info_obj['bytes'],\n                   content_type=info_obj['content_type'],\n                   last_modified=dt_from_header(info_obj['last_modified']),\n                   obj_type=cls.choose_type(info_obj['content_type']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating from regular info object.", "response": "def from_obj(cls, container, file_obj):\n        \"\"\"Create from regular info object.\"\"\"\n        # RFC 1123: Thu, 07 Jun 2007 18:57:07 GMT\n        return cls(container,\n                   name=file_obj.name,\n                   size=file_obj.size,\n                   content_type=file_obj.content_type,\n                   last_modified=dt_from_header(file_obj.last_modified),\n                   obj_type=cls.choose_type(file_obj.content_type))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_objects(self, path, marker=None,\n                    limit=settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT):\n        \"\"\"Get objects.\n\n        **Pseudo-directory Notes**: Rackspace has two approaches to pseudo-\n        directories within the (really) flat storage object namespace:\n\n          1. Dummy directory storage objects. These are real storage objects\n             of type \"application/directory\" and must be manually uploaded\n             by the client.\n          2. Implied subdirectories using the `path` API query parameter.\n\n        Both serve the same purpose, but the latter is much preferred because\n        there is no independent maintenance of extra dummy objects, and the\n        `path` approach is always correct (for the existing storage objects).\n\n        This package uses the latter `path` approach, but gets into an\n        ambiguous situation where there is both a dummy directory storage\n        object and an implied subdirectory. To remedy this situation, we only\n        show information for the dummy directory object in results if present,\n        and ignore the implied subdirectory. But, under the hood this means\n        that our `limit` parameter may end up with less than the desired\n        number of objects. So, we use the heuristic that if we **do** have\n        \"application/directory\" objects, we end up doing an extra query of\n        double the limit size to ensure we can get up to the limit amount\n        of objects. This double query approach is inefficient, but as\n        using dummy objects should now be deprecated, the second query should\n        only rarely occur.\n\n        \"\"\"\n        object_infos, full_query = self._get_object_infos(path, marker, limit)\n        if full_query and len(object_infos) < limit:\n            # The underlying query returned a full result set, but we\n            # truncated it to under limit. Re-run at twice the limit and then\n            # slice back.\n            object_infos, _ = self._get_object_infos(path, marker, 2 * limit)\n            object_infos = object_infos[:limit]\n\n        return [self.obj_cls.from_info(self, x) for x in object_infos]", "response": "Get objects from Rackspace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget raw object infos.", "response": "def _get_object_infos(self, path, marker=None,\n                          limit=settings.CLOUD_BROWSER_DEFAULT_LIST_LIMIT):\n        \"\"\"Get raw object infos (single-shot).\"\"\"\n        # Adjust limit to +1 to handle marker object as first result.\n        # We can get in to this situation for a marker of \"foo\", that will\n        # still return a 'subdir' object of \"foo/\" because of the extra\n        # slash.\n        orig_limit = limit\n        limit += 1\n\n        # Enforce maximum object size.\n        if limit > RS_MAX_LIST_OBJECTS_LIMIT:\n            raise errors.CloudException(\"Object limit must be less than %s\" %\n                                        RS_MAX_LIST_OBJECTS_LIMIT)\n\n        def _collapse(infos):\n            \"\"\"Remove duplicate dummy / implied objects.\"\"\"\n            name = None\n            for info in infos:\n                name = info.get('name', name)\n                subdir = info.get('subdir', '').strip(SEP)\n                if not name or subdir != name:\n                    yield info\n\n        path = path + SEP if path else ''\n        object_infos = self.native_container.list_objects_info(\n            limit=limit, delimiter=SEP, prefix=path, marker=marker)\n\n        full_query = len(object_infos) == limit\n        if object_infos:\n            # Check first object for marker match and truncate if so.\n            if (marker and\n                    object_infos[0].get('subdir', '').strip(SEP) == marker):\n                object_infos = object_infos[1:]\n\n            # Collapse subdirs and dummy objects.\n            object_infos = list(_collapse(object_infos))\n\n            # Adjust to original limit.\n            if len(object_infos) > orig_limit:\n                object_infos = object_infos[:orig_limit]\n\n        return object_infos, full_query"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_connection(self):\n        kwargs = {\n            'username': self.account,\n            'api_key': self.secret_key,\n        }\n\n        # Only add kwarg for servicenet if True because user could set\n        # environment variable 'RACKSPACE_SERVICENET' separately.\n        if self.servicenet:\n            kwargs['servicenet'] = True\n\n        if self.authurl:\n            kwargs['authurl'] = self.authurl\n\n        return cloudfiles.get_connection(**kwargs)", "response": "Return native connection object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delta_E( self ):\n        site_delta_E = self.final_site.energy - self.initial_site.energy\n        if self.nearest_neighbour_energy:\n            site_delta_E += self.nearest_neighbour_delta_E()\n        if self.coordination_number_energy:\n            site_delta_E += self.coordination_number_delta_E()\n        return site_delta_E", "response": "The change in system energy if this jump were accepted."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the nearest - neighbour interaction contribution to the change in system energy if this jump were accepted.", "response": "def nearest_neighbour_delta_E( self ):\n        \"\"\"\n        Nearest-neighbour interaction contribution to the change in system energy if this jump were accepted.\n\n        Args:\n            None\n\n        Returns:\n            (Float): delta E (nearest-neighbour)\n        \"\"\"\n        delta_nn = self.final_site.nn_occupation() - self.initial_site.nn_occupation() - 1 # -1 because the hopping ion is not counted in the final site occupation number\n        return ( delta_nn * self.nearest_neighbour_energy )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coordination_number_delta_E( self ):\n        initial_site_neighbours = [ s for s in self.initial_site.p_neighbours if s.is_occupied ] # excludes final site, since this is always unoccupied\n        final_site_neighbours = [ s for s in self.final_site.p_neighbours if s.is_occupied and s is not self.initial_site ] # excludes initial site\n        initial_cn_occupation_energy = ( self.initial_site.cn_occupation_energy() + \n            sum( [ site.cn_occupation_energy() for site in initial_site_neighbours ] ) +\n            sum( [ site.cn_occupation_energy() for site in final_site_neighbours ] ) )\n        final_cn_occupation_energy = ( self.final_site.cn_occupation_energy( delta_occupation = { self.initial_site.label : -1 } ) +\n            sum( [ site.cn_occupation_energy( delta_occupation = { self.initial_site.label : -1 } ) for site in initial_site_neighbours ] ) +\n            sum( [ site.cn_occupation_energy( delta_occupation = { self.final_site.label : +1 } ) for site in final_site_neighbours ] ) )\n        return ( final_cn_occupation_energy - initial_cn_occupation_energy )", "response": "Returns the delta E of the coordination - number dependent energy conrtibution to the change in system energy if this jump were accepted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the particle displacement vector for this jump.", "response": "def dr( self, cell_lengths ):\n        \"\"\"\n        Particle displacement vector for this jump\n\n        Args:\n            cell_lengths (np.array(x,y,z)): Cell lengths for the orthogonal simulation cell.\n\n        Returns\n            (np.array(x,y,z)): dr\n        \"\"\"\n        half_cell_lengths = cell_lengths / 2.0\n        this_dr = self.final_site.r - self.initial_site.r\n        for i in range( 3 ):\n            if this_dr[ i ] > half_cell_lengths[ i ]:\n                this_dr[ i ] -= cell_lengths[ i ]\n            if this_dr[ i ] < -half_cell_lengths[ i ]:\n                this_dr[ i ] += cell_lengths[ i ]\n        return this_dr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the relative probability of accepting this jump from a lookup table.", "response": "def relative_probability_from_lookup_table( self, jump_lookup_table ):\n        \"\"\"\n        Relative probability of accepting this jump from a lookup-table.\n\n        Args:\n            jump_lookup_table (LookupTable): the lookup table to be used for this jump.\n\n        Returns:\n            (Float): relative probability of accepting this jump.\n        \"\"\"\n        l1 = self.initial_site.label\n        l2 = self.final_site.label\n        c1 = self.initial_site.nn_occupation()\n        c2 = self.final_site.nn_occupation()\n        return jump_lookup_table.jump_probability[ l1 ][ l2 ][ c1 ][ c2 ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a module from the cache.", "response": "def module_cache_get(cache, module):\n    \"\"\"\n    Import a module with an optional yaml config file, but only if we haven't\n    imported it already.\n\n    :param cache: object which holds information on which modules and config\n                  files have been loaded and whether config files should be\n                  loaded.\n    :param module: the path of the module to load.\n    :returns: the loaded module.\n    \"\"\"\n    if getattr(cache, \"config\", False):\n        config_file = module[:-2] + \"yaml\"\n        if config_file not in cache.config_files and os.path.exists(config_file):\n            try:\n                config = yaml_safe_load(config_file, type=dict)\n            except TypeError as e:\n                tangelo.log_warning(\"TANGELO\", \"Bad configuration in file %s: %s\" % (config_file, e))\n                raise\n            except IOError:\n                tangelo.log_warning(\"TANGELO\", \"Could not open config file %s\" % (config_file))\n                raise\n            except ValueError as e:\n                tangelo.log_warning(\"TANGELO\", \"Error reading config file %s: %s\" % (config_file, e))\n                raise\n            cache.config_files[config_file] = True\n        else:\n            config = {}\n        cherrypy.config[\"module-config\"][module] = config\n        cherrypy.config[\"module-store\"].setdefault(module, {})\n    # If two threads are importing the same module nearly concurrently, we\n    # could load it twice unless we use the import lock.\n    imp.acquire_lock()\n    try:\n        if module not in cache.modules:\n            name = module[:-3]\n\n            # load the module.\n            service = imp.load_source(name, module)\n            cache.modules[module] = service\n        else:\n            service = cache.modules[module]\n    finally:\n        imp.release_lock()\n    return service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_virtual_offset(block_start_offset, within_block_offset):\n    if within_block_offset < 0 or within_block_offset >= 65536:\n        raise ValueError(\"Require 0 <= within_block_offset < 2**16, got %i\" % within_block_offset)\n    if block_start_offset < 0 or block_start_offset >= 281474976710656:\n        raise ValueError(\"Require 0 <= block_start_offset < 2**48, got %i\" % block_start_offset)\n    return (block_start_offset << 16) | within_block_offset", "response": "Compute a virtual offset from a block start and within block offsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        if self._buffer:\n            self.flush()\n        self._handle.write(_bgzf_eof)\n        self._handle.flush()\n        self._handle.close()", "response": "Flush data write 28 bytes BGZF EOF marker and close BGZF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if secret key to encryot sessions exists generate it otherwise.", "response": "def ensure_secret():\n    \"\"\"Check if secret key to encryot sessions exists,\n    generate it otherwise.\"\"\"\n    home_dir = os.environ['HOME']\n    file_name = home_dir + \"/.ipcamweb\"\n    if os.path.exists(file_name):\n        with open(file_name, \"r\") as s_file:\n            secret = s_file.readline()\n    else:\n        secret = os.urandom(24)\n        with open(file_name, \"w\") as s_file:\n            secret = s_file.write(secret+\"\\n\")\n    return secret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of ( date dir ) in which snapshopts are present", "response": "def list_snapshots_days(path, cam_id):\n    \"\"\"Returns a list of (date, dir) in which snapshopts are present\"\"\"\n    screenshoots_path = path + \"/\" + str(cam_id)\n    if os.path.exists(screenshoots_path):\n        days = []\n        for day_dir in os.listdir(screenshoots_path):\n            date = datetime.datetime.strptime(day_dir, \"%d%m%Y\").strftime('%d/%m/%y')\n            days.append((date, day_dir))\n        return days\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of hour / min in which snapshopts are present", "response": "def list_snapshots_hours(path, cam_id, day):\n    \"\"\"Returns a list of hour/min in which snapshopts are present\"\"\"\n    screenshoots_path = path+\"/\"+str(cam_id)+\"/\"+day\n    if os.path.exists(screenshoots_path):\n        hours = []\n        for hour_dir in sorted(os.listdir(screenshoots_path)):\n            hrm = datetime.datetime.strptime(hour_dir, \"%H%M\").strftime('%H:%M')\n            hours.append((hrm, hour_dir))\n        return hours\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_snapshots_for_a_minute(path, cam_id, day, hourm):\n    screenshoots_path = path+\"/\"+str(cam_id)+\"/\"+day+\"/\"+hourm\n    if os.path.exists(screenshoots_path):\n        screenshots = [scr for scr in sorted(os.listdir(screenshoots_path))]\n        return screenshots\n    else:\n        return []", "response": "Returns a list of screenshots for a given day and hourm"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if it is a SNV", "response": "def is_snv(self):\n        \"\"\"Return ``True`` if it is a SNV\"\"\"\n        return len(self.REF) == 1 and all(a.type == \"SNV\" for a in self.ALT)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef affected_start(self):\n        types = {alt.type for alt in self.ALT}  # set!\n        BAD_MIX = {INS, SV, BND, SYMBOLIC}  # don't mix well with others\n        if (BAD_MIX & types) and len(types) == 1 and list(types)[0] == INS:\n            # Only insertions, return 0-based position right of first base\n            return self.POS  # right of first base\n        else:  # Return 0-based start position of first REF base\n            return self.POS - 1", "response": "Return the start position of the affected record in 0 - based coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd label to FILTER if not set yet remove PASS entry if present", "response": "def add_filter(self, label):\n        \"\"\"Add label to FILTER if not set yet, removing ``PASS`` entry if\n        present\n        \"\"\"\n        if label not in self.FILTER:\n            if \"PASS\" in self.FILTER:\n                self.FILTER = [f for f in self.FILTER if f != \"PASS\"]\n            self.FILTER.append(label)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_format(self, key, value=None):\n        if key in self.FORMAT:\n            return\n        self.FORMAT.append(key)\n        if value is not None:\n            for call in self:\n                call.data.setdefault(key, value)", "response": "Add an entry to the format list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the actual genotype bases", "response": "def gt_bases(self):\n        \"\"\"Return the actual genotype bases, e.g. if VCF genotype is 0/1,\n        could return ('A', 'T')\n        \"\"\"\n        result = []\n        for a in self.gt_alleles:\n            if a is None:\n                result.append(None)\n            elif a == 0:\n                result.append(self.site.REF)\n            else:\n                result.append(self.site.ALT[a - 1].value)\n        return tuple(result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gt_type(self):\n        if not self.called:\n            return None  # not called\n        elif all(a == 0 for a in self.gt_alleles):\n            return HOM_REF\n        elif len(set(self.gt_alleles)) == 1:\n            return HOM_ALT\n        else:\n            return HET", "response": "The type of genotype returns one of HOM_REF HOM_ALT and HOM_HET"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the call to is filtered", "response": "def is_filtered(self, require=None, ignore=None):\n        \"\"\"Return ``True`` for filtered calls\n\n        :param iterable ignore: if set, the filters to ignore, make sure to\n            include 'PASS', when setting, default is ``['PASS']``\n        :param iterable require: if set, the filters to require for returning\n            ``True``\n        \"\"\"\n        ignore = ignore or [\"PASS\"]\n        if \"FT\" not in self.data or not self.data[\"FT\"]:\n            return False\n        for ft in self.data[\"FT\"]:\n            if ft in ignore:\n                continue  # skip\n            if not require:\n                return True\n            elif ft in require:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning string representation for VCF", "response": "def serialize(self):\n        \"\"\"Return string representation for VCF\"\"\"\n        if self.mate_chrom is None:\n            remote_tag = \".\"\n        else:\n            if self.within_main_assembly:\n                mate_chrom = self.mate_chrom\n            else:\n                mate_chrom = \"<{}>\".format(self.mate_chrom)\n            tpl = {FORWARD: \"[{}:{}[\", REVERSE: \"]{}:{}]\"}[self.mate_orientation]\n            remote_tag = tpl.format(mate_chrom, self.mate_pos)\n        if self.orientation == FORWARD:\n            return remote_tag + self.sequence\n        else:\n            return self.sequence + remote_tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides Series. trend to return a TimeSeries instance.", "response": "def trend(self, order=LINEAR):\n        '''Override Series.trend() to return a TimeSeries instance.'''\n        coefficients = self.trend_coefficients(order)\n        x = self.timestamps\n        trend_y = LazyImport.numpy().polyval(coefficients, x)\n        return TimeSeries(zip(x, trend_y))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trend_coefficients(self, order=LINEAR):\n        '''Calculate trend coefficients for the specified order.'''\n        if not len(self.points):\n            raise ArithmeticError('Cannot calculate the trend of an empty series')\n        return LazyImport.numpy().polyfit(self.timestamps, self.values, order)", "response": "Calculate trend coefficients for the specified order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate a moving average using the specified method and window", "response": "def moving_average(self, window, method=SIMPLE):\n        '''Calculate a moving average using the specified method and window'''\n        if len(self.points) < window:\n            raise ArithmeticError('Not enough points for moving average')\n        numpy = LazyImport.numpy()\n        if method == TimeSeries.SIMPLE:\n            weights = numpy.ones(window) / float(window)\n        ma_x = self.timestamps[window-1:]\n        ma_y = numpy.convolve(self.values, weights)[window-1:-(window-1)].tolist()\n        return TimeSeries(zip(ma_x, ma_y))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nforecast points beyond the time series range using the specified forecasting method. horizon is the number of points to forecast.", "response": "def forecast(self, horizon, method=ARIMA, frequency=None):\n        '''Forecast points beyond the time series range using the specified\n        forecasting method. `horizon` is the number of points to forecast.'''\n        if len(self.points) <= 1:\n            raise ArithmeticError('Cannot run forecast when len(series) <= 1')\n        R = LazyImport.rpy2()\n        series = LazyImport.numpy().array(self.values)\n        if frequency is not None:\n            series = R.ts(series, frequency=frequency)\n        if method == TimeSeries.ARIMA:\n            fit = R.forecast.auto_arima(series)\n        elif method == TimeSeries.ETS:\n            fit = R.forecast.ets(series)\n        else:\n            raise ValueError('Unknown forecast() method')\n        forecasted = R.forecast.forecast(fit, h=horizon)\n        forecast_y = list(forecasted.rx2('mean'))\n        interval = self.interval\n        last_x = self.points[-1][0]\n        forecast_x = [ last_x + x * interval for x in xrange(1, horizon+1) ]\n        return TimeSeries(zip(forecast_x, forecast_y))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse STL to decompose the time series into seasonal trend and and residual components.", "response": "def decompose(self, frequency, window=None, periodic=False):\n        '''Use STL to decompose the time series into seasonal, trend, and\n        residual components.'''\n        R = LazyImport.rpy2()\n        if periodic:\n            window = 'periodic'\n        elif window is None:\n            window = frequency\n        timestamps = self.timestamps\n        series = LazyImport.numpy().array(self.values)\n        length = len(series)\n        series = R.ts(series, frequency=frequency)\n        kwargs = { 's.window': window }\n        decomposed = R.robjects.r['stl'](series, **kwargs).rx2('time.series')\n        decomposed = [ row for row in decomposed ]\n        seasonal = decomposed[0:length]\n        trend = decomposed[length:2*length]\n        residual = decomposed[2*length:3*length]\n        seasonal = TimeSeries(zip(timestamps, seasonal))\n        trend = TimeSeries(zip(timestamps, trend))\n        residual = TimeSeries(zip(timestamps, residual))\n        return DataFrame(seasonal=seasonal, trend=trend, residual=residual)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, label=None, colour='g', style='-'): # pragma: no cover\n        '''Plot the time series.'''\n        pylab = LazyImport.pylab()\n        pylab.plot(self.dates, self.values, '%s%s' % (colour, style), label=label)\n        if label is not None:\n            pylab.legend()\n        pylab.show()", "response": "Plot the time series."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef table_output(data):\n    '''Get a table representation of a dictionary.'''\n    if type(data) == DictType:\n        data = data.items()\n    headings = [ item[0] for item in data ]\n    rows = [ item[1] for item in data ]\n    columns = zip(*rows)\n    if len(columns):\n        widths = [ max([ len(str(y)) for y in row ]) for row in rows ]\n    else:\n        widths = [ 0 for c in headings ]\n    for c, heading in enumerate(headings):\n        widths[c] = max(widths[c], len(heading))\n    column_count = range(len(rows))\n    table = [ ' '.join([ headings[c].ljust(widths[c]) for c in column_count ]) ]\n    table.append(' '.join([ '=' * widths[c] for c in column_count ]))\n    for column in columns:\n        table.append(' '.join([ str(column[c]).ljust(widths[c]) for c in column_count ]))\n    return '\\n'.join(table)", "response": "Get a table representation of a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_datetime(time):\n    '''Convert `time` to a datetime.'''\n    if type(time) == IntType or type(time) == LongType:\n        time = datetime.fromtimestamp(time // 1000)\n    return time", "response": "Convert time to a datetime."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nspelling check textgrids by using the praatio spellcheck function.", "response": "def spellCheckTextgrid(tg, targetTierName, newTierName, isleDict,\n                       printEntries=False):\n    '''\n    Spell check words by using the praatio spellcheck function\n    \n    Incorrect items are noted in a new tier and optionally\n        printed to the screen\n    '''\n    \n    def checkFunc(word):\n        try:\n            isleDict.lookup(word)\n        except isletool.WordNotInISLE:\n            returnVal = False\n        else:\n            returnVal = True\n        \n        return returnVal\n    \n    tg = praatio_scripts.spellCheckEntries(tg, targetTierName, newTierName,\n                                           checkFunc, printEntries)\n    \n    return tg"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms naive alignment for utterances in a textgrid Naive alignment gives each segment equal duration. Word duration is determined by the duration of an utterance and the number of phones in the word. By 'utterance' I mean a string of words separated by a space bounded in time eg (0.5, 1.5, \"he said he likes ketchup\"). phoneHelperTierName - creates a tier that is parallel to the word tier. However, the labels are the phones for the word, rather than the word removeOverlappingSegments - remove any labeled words or phones that fall under labeled utterances", "response": "def naiveWordAlignment(tg, utteranceTierName, wordTierName, isleDict,\n                       phoneHelperTierName=None,\n                       removeOverlappingSegments=False):\n    '''\n    Performs naive alignment for utterances in a textgrid\n    \n    Naive alignment gives each segment equal duration.  Word duration is\n    determined by the duration of an utterance and the number of phones in\n    the word.\n    \n    By 'utterance' I mean a string of words separated by a space bounded\n    in time eg (0.5, 1.5, \"he said he likes ketchup\").\n    \n    phoneHelperTierName - creates a tier that is parallel to the word tier.\n                          However, the labels are the phones for the word,\n                          rather than the word\n    removeOverlappingSegments - remove any labeled words or phones that\n                                fall under labeled utterances\n    '''\n    utteranceTier = tg.tierDict[utteranceTierName]\n    \n    wordTier = None\n    if wordTierName in tg.tierNameList:\n        wordTier = tg.tierDict[wordTierName]\n    \n    # Load in the word tier, if it exists:\n    wordEntryList = []\n    phoneEntryList = []\n    if wordTier is not None:\n        if removeOverlappingSegments:\n            for startT, stopT, _ in utteranceTier.entryList:\n                wordTier = wordTier.eraseRegion(startT, stopT,\n                                                'truncate', False)\n        wordEntryList = wordTier.entryList\n\n    # Do the naive alignment\n    for startT, stopT, label in utteranceTier.entryList:\n        wordList = label.split()\n\n        # Get the list of phones in each word\n        superPhoneList = []\n        numPhones = 0\n        i = 0\n        while i < len(wordList):\n            word = wordList[i]\n            try:\n                firstSyllableList = isleDict.lookup(word)[0][0][0]\n            except isletool.WordNotInISLE:\n                wordList.pop(i)\n                continue\n            phoneList = [phone for syllable in firstSyllableList\n                         for phone in syllable]\n            superPhoneList.append(phoneList)\n            numPhones += len(phoneList)\n            i += 1\n        \n        # Get the naive alignment for words, if alignment doesn't\n        # already exist for words\n        subWordEntryList = []\n        subPhoneEntryList = []\n        if wordTier is not None:\n            subWordEntryList = wordTier.crop(startT, stopT,\n                                             \"truncated\", False).entryList\n        \n        if len(subWordEntryList) == 0:\n            wordStartT = startT\n            phoneDur = (stopT - startT) / float(numPhones)\n            for i, word in enumerate(wordList):\n                phoneListTxt = \" \".join(superPhoneList[i])\n                wordStartT = wordStartT\n                wordEndT = wordStartT + (phoneDur * len(superPhoneList[i]))\n                subWordEntryList.append((wordStartT, wordEndT, word))\n                subPhoneEntryList.append((wordStartT, wordEndT, phoneListTxt))\n                wordStartT = wordEndT\n        \n        wordEntryList.extend(subWordEntryList)\n        phoneEntryList.extend(subPhoneEntryList)\n    \n    # Replace or add the word tier\n    newWordTier = tgio.IntervalTier(wordTierName,\n                                    wordEntryList,\n                                    tg.minTimestamp,\n                                    tg.maxTimestamp)\n    if wordTier is not None:\n        tg.replaceTier(wordTierName, newWordTier)\n    else:\n\n        tg.addTier(newWordTier)\n        \n    # Add the phone tier\n    # This is mainly used as an annotation tier\n    if phoneHelperTierName is not None and len(phoneEntryList) > 0:\n        newPhoneTier = tgio.IntervalTier(phoneHelperTierName,\n                                         phoneEntryList,\n                                         tg.minTimestamp,\n                                         tg.minTimestamp)\n        if phoneHelperTierName in tg.tierNameList:\n            tg.replaceTier(phoneHelperTierName, newPhoneTier)\n        else:\n            tg.addTier(newPhoneTier)\n    \n    return tg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef naivePhoneAlignment(tg, wordTierName, phoneTierName, isleDict,\n                        removeOverlappingSegments=False):\n    '''\n    Performs naive alignment for words in a textgrid\n    \n    Naive alignment gives each segment equal duration.\n    Phone duration is determined by the duration of the word\n    and the number of phones.\n    \n    removeOverlappingSegments - remove any labeled words or phones that\n                                fall under labeled utterances\n    '''\n    wordTier = tg.tierDict[wordTierName]\n    \n    phoneTier = None\n    if phoneTierName in tg.tierNameList:\n        phoneTier = tg.tierDict[phoneTierName]\n    \n    # Load in the phone tier, if it exists:\n    phoneEntryList = []\n    if phoneTier is not None:\n        if removeOverlappingSegments:\n            for startT, stopT, _ in wordTier.entryList:\n                phoneTier = phoneTier.eraseRegion(startT, stopT,\n                                                  'truncate', False)\n        phoneEntryList = phoneTier.entryList\n\n    # Do the naive alignment\n    for wordStartT, wordEndT, word in wordTier.entryList:\n        \n        # Get the list of phones in this word\n        try:\n            firstSyllableList = isleDict.lookup(word)[0][0][0]\n        except isletool.WordNotInISLE:\n            continue\n\n        phoneList = [phone for syllable in firstSyllableList\n                     for phone in syllable]\n        for char in [u'\u02c8', u'\u02cc']:\n            phoneList = [phone.replace(char, '') for phone in phoneList]\n        \n        # Get the naive alignment for phones, if alignment doesn't\n        # already exist for phones\n        subPhoneEntryList = []\n        if phoneTier is not None:\n            subPhoneEntryList = phoneTier.crop(wordStartT, wordEndT,\n                                               \"truncated\", False).entryList\n        \n        if len(subPhoneEntryList) == 0:\n            phoneDur = (wordEndT - wordStartT) / len(phoneList)\n            \n            phoneStartT = wordStartT\n            for phone in phoneList:\n                phoneEndT = phoneStartT + phoneDur\n                subPhoneEntryList.append((phoneStartT, phoneEndT, phone))\n                phoneStartT = phoneEndT\n\n        phoneEntryList.extend(subPhoneEntryList)\n    \n    # Replace or add the phone tier\n    newPhoneTier = tgio.IntervalTier(phoneTierName,\n                                     phoneEntryList,\n                                     tg.minTimestamp,\n                                     tg.maxTimestamp)\n    if phoneTier is not None:\n        tg.replaceTier(phoneTierName, newPhoneTier)\n    else:\n\n        tg.addTier(newPhoneTier)\n    \n    return tg", "response": "Performs a naive alignment for words in a textgrid and phonenumberal texts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a textgrid and a word tier and phone tier syllabifies the phones in the textgrid.", "response": "def syllabifyTextgrid(isleDict, tg, wordTierName, phoneTierName,\n                      skipLabelList=None, startT=None, stopT=None):\n    '''\n    Given a textgrid, syllabifies the phones in the textgrid\n    \n    skipLabelList allows you to skip labels without generating warnings\n    (e.g. '', 'sp', etc.)\n    \n    The textgrid must have a word tier and a phone tier.\n    \n    Returns a textgrid with only two tiers containing syllable information\n    (syllabification of the phone tier and a tier marking word-stress).\n    '''\n    minT = tg.minTimestamp\n    maxT = tg.maxTimestamp\n    \n    wordTier = tg.tierDict[wordTierName]\n    phoneTier = tg.tierDict[phoneTierName]\n    \n    if skipLabelList is None:\n        skipLabelList = []\n    \n    syllableEntryList = []\n    tonicSEntryList = []\n    tonicPEntryList = []\n    \n    if startT is not None or stopT is not None:\n        if startT is None:\n            startT = minT\n        if stopT is None:\n            stopT = maxT\n    \n        wordTier = wordTier.crop(startT, stopT, \"truncated\", False)\n    \n    for start, stop, word in wordTier.entryList:\n        \n        if word in skipLabelList:\n            continue\n        \n        subPhoneTier = phoneTier.crop(start, stop, \"strict\", False)\n        \n        # entry = (start, stop, phone)\n        phoneList = [entry[2] for entry in subPhoneTier.entryList\n                     if entry[2] != '']\n        phoneList = [phoneList, ]\n        \n        try:\n            sylTmp = pronunciationtools.findBestSyllabification(isleDict,\n                                                                word,\n                                                                phoneList)\n        except isletool.WordNotInISLE:\n            print(\"Word ('%s') not is isle -- skipping syllabification\" % word)\n            continue\n        except (pronunciationtools.NullPronunciationError):\n            print(\"Word ('%s') has no provided pronunciation\" % word)\n            continue\n        except AssertionError:\n            print(\"Unable to syllabify '%s'\" % word)\n            continue\n        \n        for syllabificationResultList in sylTmp:\n            stressI = syllabificationResultList[0]\n            stressJ = syllabificationResultList[1]\n            syllableList = syllabificationResultList[2]\n                \n            stressedPhone = None\n            if stressI is not None and stressJ is not None:\n                stressedPhone = syllableList[stressI][stressJ]\n                syllableList[stressI][stressJ] += u\"\u02c8\"\n    \n            i = 0\n    #         print(syllableList)\n            for k, syllable in enumerate(syllableList):\n                \n                # Create the syllable tier entry\n                j = len(syllable)\n                stubEntryList = subPhoneTier.entryList[i:i + j]\n                i += j\n                \n                # The whole syllable was deleted\n                if len(stubEntryList) == 0:\n                    continue\n                \n                syllableStart = stubEntryList[0][0]\n                syllableEnd = stubEntryList[-1][1]\n                label = \"-\".join([entry[2] for entry in stubEntryList])\n            \n                syllableEntryList.append((syllableStart, syllableEnd, label))\n                \n                # Create the tonic syllable tier entry\n                if k == stressI:\n                    tonicSEntryList.append((syllableStart, syllableEnd, 'T'))\n                \n                # Create the tonic phone tier entry\n                if k == stressI:\n                    syllablePhoneTier = phoneTier.crop(syllableStart,\n                                                       syllableEnd,\n                                                       \"strict\", False)\n                \n                    phoneList = [entry for entry in syllablePhoneTier.entryList\n                                 if entry[2] != '']\n                    justPhones = [phone for _, _, phone in phoneList]\n                    cvList = pronunciationtools._prepPronunciation(justPhones)\n                    \n                    try:\n                        tmpStressJ = cvList.index('V')\n                    except ValueError:\n                        for char in [u'r', u'n', u'l']:\n                            if char in cvList:\n                                tmpStressJ = cvList.index(char)\n                                break\n                            \n                    phoneStart, phoneEnd = phoneList[tmpStressJ][:2]\n                    tonicPEntryList.append((phoneStart, phoneEnd, 'T'))\n    \n    # Create a textgrid with the two syllable-level tiers\n    syllableTier = tgio.IntervalTier('syllable', syllableEntryList,\n                                     minT, maxT)\n    tonicSTier = tgio.IntervalTier('tonicSyllable', tonicSEntryList,\n                                   minT, maxT)\n    tonicPTier = tgio.IntervalTier('tonicVowel', tonicPEntryList,\n                                   minT, maxT)\n    \n    syllableTG = tgio.Textgrid()\n    syllableTG.addTier(syllableTier)\n    syllableTG.addTier(tonicSTier)\n    syllableTG.addTier(tonicPTier)\n\n    return syllableTG"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_pyvcf(args):\n    # open VCF reader\n    reader = vcf.Reader(filename=args.input_vcf)\n    # optionally, open VCF writer\n    writer = None\n    # read through input VCF file, optionally also writing out\n    start = time.clock()\n    num = 0\n    for num, r in enumerate(reader):\n        if num % 10000 == 0:\n            print(num, \"\".join(map(str, [r.CHROM, \":\", r.POS])), sep=\"\\t\", file=sys.stderr)\n        if writer:\n            writer.write_record(r)\n        if args.max_records and num >= args.max_records:\n            break\n    end = time.clock()\n    print(\"Read {} records in {} seconds\".format(num, (end - start)), file=sys.stderr)", "response": "Main entry point after parsing arguments"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(argv=None):\n    parser = argparse.ArgumentParser(description=\"Benchmark driver\")\n\n    parser.add_argument(\"--max-records\", type=int, default=100 * 1000)\n    parser.add_argument(\"--engine\", type=str, choices=(\"vcfpy\", \"pyvcf\"), default=\"vcfpy\")\n    parser.add_argument(\"--input-vcf\", type=str, required=True, help=\"Path to VCF file to read\")\n    parser.add_argument(\n        \"--output-vcf\", type=str, required=False, help=\"Path to VCF file to write if given\"\n    )\n\n    args = parser.parse_args(argv)\n    if args.engine == \"vcfpy\":\n        VCFPyRunner(args).run()\n    else:\n        PyVCFRunner(args).run()", "response": "Entry point for parsing command line arguments and running the benchmark driver."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _crc8(self, buffer):\n\n        polynomial = 0x31;\n        crc = 0xFF;\n  \n        index = 0\n        for index in range(0, len(buffer)):\n            crc ^= buffer[index]\n            for i in range(8, 0, -1):\n                if crc & 0x80:\n                    crc = (crc << 1) ^ polynomial\n                else:\n                    crc = (crc << 1)\n        return crc & 0xFF", "response": "Calculate the CRC of a memory - mapped array of memory - mapped entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsplit the str in pair_str at =", "response": "def split_mapping(pair_str):\n    \"\"\"Split the ``str`` in ``pair_str`` at ``'='``\n\n    Warn if key needs to be stripped\n    \"\"\"\n    orig_key, value = pair_str.split(\"=\", 1)\n    key = orig_key.strip()\n    if key != orig_key:\n        warnings.warn(\n            \"Mapping key {} has leading or trailing space\".format(repr(orig_key)),\n            LeadingTrailingSpaceInKey,\n        )\n    return key, value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a VCF header line mapping into a dictionary of key = value pairs.", "response": "def parse_mapping(value):\n    \"\"\"Parse the given VCF header line mapping\n\n    Such a mapping consists of \"key=value\" pairs, separated by commas and\n    wrapped into angular brackets (\"<...>\").  Strings are usually quoted,\n    for certain known keys, exceptions are made, depending on the tag key.\n    this, however, only gets important when serializing.\n\n    :raises: :py:class:`vcfpy.exceptions.InvalidHeaderException` if\n        there was a problem parsing the file\n    \"\"\"\n    if not value.startswith(\"<\") or not value.endswith(\">\"):\n        raise exceptions.InvalidHeaderException(\n            \"Header mapping value was not wrapped in angular brackets\"\n        )\n    # split the comma-separated list into pairs, ignoring commas in quotes\n    pairs = split_quoted_string(value[1:-1], delim=\",\", quote='\"')\n    # split these pairs into key/value pairs, converting flags to mappings\n    # to True\n    key_values = []\n    for pair in pairs:\n        if \"=\" in pair:\n            key, value = split_mapping(pair)\n            if value.startswith('\"') and value.endswith('\"'):\n                value = ast.literal_eval(value)\n            elif value.startswith(\"[\") and value.endswith(\"]\"):\n                value = [v.strip() for v in value[1:-1].split(\",\")]\n        else:\n            key, value = pair, True\n        key_values.append((key, value))\n    # return completely parsed mapping as OrderedDict\n    return OrderedDict(key_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a mapping for parsers to use for each VCF header type.", "response": "def build_header_parsers():\n    \"\"\"Return mapping for parsers to use for each VCF header type\n\n    Inject the WarningHelper into the parsers.\n    \"\"\"\n    result = {\n        \"ALT\": MappingHeaderLineParser(header.AltAlleleHeaderLine),\n        \"contig\": MappingHeaderLineParser(header.ContigHeaderLine),\n        \"FILTER\": MappingHeaderLineParser(header.FilterHeaderLine),\n        \"FORMAT\": MappingHeaderLineParser(header.FormatHeaderLine),\n        \"INFO\": MappingHeaderLineParser(header.InfoHeaderLine),\n        \"META\": MappingHeaderLineParser(header.MetaHeaderLine),\n        \"PEDIGREE\": MappingHeaderLineParser(header.PedigreeHeaderLine),\n        \"SAMPLE\": MappingHeaderLineParser(header.SampleHeaderLine),\n        \"__default__\": StupidHeaderLineParser(),  # fallback\n    }\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting atomic field value according to the type", "response": "def convert_field_value(type_, value):\n    \"\"\"Convert atomic field value according to the type\"\"\"\n    if value == \".\":\n        return None\n    elif type_ in (\"Character\", \"String\"):\n        if \"%\" in value:\n            for k, v in record.UNESCAPE_MAPPING:\n                value = value.replace(k, v)\n        return value\n    else:\n        try:\n            return _CONVERTERS[type_](value)\n        except ValueError:\n            warnings.warn(\n                (\"{} cannot be converted to {}, keeping as \" \"string.\").format(value, type_),\n                CannotConvertValue,\n            )\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses value according to field_info", "response": "def parse_field_value(field_info, value):\n    \"\"\"Parse ``value`` according to ``field_info``\n    \"\"\"\n    if field_info.id == \"FT\":\n        return [x for x in value.split(\";\") if x != \".\"]\n    elif field_info.type == \"Flag\":\n        return True\n    elif field_info.number == 1:\n        return convert_field_value(field_info.type, value)\n    else:\n        if value == \".\":\n            return []\n        else:\n            return [convert_field_value(field_info.type, x) for x in value.split(\",\")]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a BreakEnd into a tuple with results parameters for BreakEnd constructor", "response": "def parse_breakend(alt_str):\n    \"\"\"Parse breakend and return tuple with results, parameters for BreakEnd\n    constructor\n    \"\"\"\n    arr = BREAKEND_PATTERN.split(alt_str)\n    mate_chrom, mate_pos = arr[1].split(\":\", 1)\n    mate_pos = int(mate_pos)\n    if mate_chrom[0] == \"<\":\n        mate_chrom = mate_chrom[1:-1]\n        within_main_assembly = False\n    else:\n        within_main_assembly = True\n    FWD_REV = {True: record.FORWARD, False: record.REVERSE}\n    orientation = FWD_REV[alt_str[0] == \"[\" or alt_str[0] == \"]\"]\n    mate_orientation = FWD_REV[\"[\" in alt_str]\n    if orientation == record.FORWARD:\n        sequence = arr[2]\n    else:\n        sequence = arr[0]\n    return (mate_chrom, mate_pos, orientation, mate_orientation, sequence, within_main_assembly)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses substution where the string grows", "response": "def process_sub_grow(ref, alt_str):\n    \"\"\"Process substution where the string grows\"\"\"\n    if len(alt_str) == 0:\n        raise exceptions.InvalidRecordException(\"Invalid VCF, empty ALT\")\n    elif len(alt_str) == 1:\n        if ref[0] == alt_str[0]:\n            return record.Substitution(record.DEL, alt_str)\n        else:\n            return record.Substitution(record.INDEL, alt_str)\n    else:\n        return record.Substitution(record.INDEL, alt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_sub_shrink(ref, alt_str):\n    if len(ref) == 0:\n        raise exceptions.InvalidRecordException(\"Invalid VCF, empty REF\")\n    elif len(ref) == 1:\n        if ref[0] == alt_str[0]:\n            return record.Substitution(record.INS, alt_str)\n        else:\n            return record.Substitution(record.INDEL, alt_str)\n    else:\n        return record.Substitution(record.INDEL, alt_str)", "response": "Process substution where the string shrink"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess substitution of a single element in a sequence of elements.", "response": "def process_sub(ref, alt_str):\n    \"\"\"Process substitution\"\"\"\n    if len(ref) == len(alt_str):\n        if len(ref) == 1:\n            return record.Substitution(record.SNV, alt_str)\n        else:\n            return record.Substitution(record.MNV, alt_str)\n    elif len(ref) > len(alt_str):\n        return process_sub_grow(ref, alt_str)\n    else:  # len(ref) < len(alt_str):\n        return process_sub_shrink(ref, alt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_alt(header, ref, alt_str):  # pylint: disable=W0613\n    # By its nature, this function contains a large number of case distinctions\n    if \"]\" in alt_str or \"[\" in alt_str:\n        return record.BreakEnd(*parse_breakend(alt_str))\n    elif alt_str[0] == \".\" and len(alt_str) > 0:\n        return record.SingleBreakEnd(record.FORWARD, alt_str[1:])\n    elif alt_str[-1] == \".\" and len(alt_str) > 0:\n        return record.SingleBreakEnd(record.REVERSE, alt_str[:-1])\n    elif alt_str[0] == \"<\" and alt_str[-1] == \">\":\n        inner = alt_str[1:-1]\n        return record.SymbolicAllele(inner)\n    else:  # substitution\n        return process_sub(ref, alt_str)", "response": "Process alternative value using Header in header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits string s at delimiter correctly interpreting quotes Further interprets arrays wrapped in one level of [ ].", "response": "def run(self, s):\n        \"\"\"Split string ``s`` at delimiter, correctly interpreting quotes\n\n        Further, interprets arrays wrapped in one level of ``[]``.  No\n        recursive brackets are interpreted (as this would make the grammar\n        non-regular and currently this complexity is not needed).  Currently,\n        quoting inside of braces is not supported either.  This is just to\n        support the example from VCF v4.3.\n        \"\"\"\n        begins, ends = [0], []\n        # transition table\n        DISPATCH = {\n            self.NORMAL: self._handle_normal,\n            self.QUOTED: self._handle_quoted,\n            self.ARRAY: self._handle_array,\n            self.DELIM: self._handle_delim,\n            self.ESCAPED: self._handle_escaped,\n        }\n        # run state automaton\n        state = self.NORMAL\n        for pos, c in enumerate(s):\n            state = DISPATCH[state](c, pos, begins, ends)\n        ends.append(len(s))\n        assert len(begins) == len(ends)\n        # Build resulting list\n        return [s[start:end] for start, end in zip(begins, ends)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a line of VCF header into a header line object.", "response": "def parse_line(self, line):\n        \"\"\"Parse VCF header ``line`` (trailing '\\r\\n' or '\\n' is ignored)\n\n        :param str line: ``str`` with line to parse\n        :param dict sub_parsers: ``dict`` mapping header line types to\n            appropriate parser objects\n        :returns: appropriate :py:class:`HeaderLine` parsed from ``line``\n        :raises: :py:class:`vcfpy.exceptions.InvalidHeaderException` if\n            there was a problem parsing the file\n        \"\"\"\n        if not line or not line.startswith(\"##\"):\n            raise exceptions.InvalidHeaderException(\n                'Invalid VCF header line (must start with \"##\") {}'.format(line)\n            )\n        if \"=\" not in line:\n            raise exceptions.InvalidHeaderException(\n                'Invalid VCF header line (must contain \"=\") {}'.format(line)\n            )\n        line = line[len(\"##\") :].rstrip()  # trim '^##' and trailing whitespace\n        # split key/value pair at \"=\"\n        key, value = split_mapping(line)\n        sub_parser = self.sub_parsers.get(key, self.sub_parsers[\"__default__\"])\n        return sub_parser.parse_key_value(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a line from a file and return a Record object.", "response": "def parse_line(self, line_str):\n        \"\"\"Parse line from file (including trailing line break) and return\n        resulting Record\n        \"\"\"\n        line_str = line_str.rstrip()\n        if not line_str:\n            return None  # empty line, EOF\n        arr = self._split_line(line_str)\n        # CHROM\n        chrom = arr[0]\n        # POS\n        pos = int(arr[1])\n        # IDS\n        if arr[2] == \".\":\n            ids = []\n        else:\n            ids = arr[2].split(\";\")\n        # REF\n        ref = arr[3]\n        # ALT\n        alts = []\n        if arr[4] != \".\":\n            for alt in arr[4].split(\",\"):\n                alts.append(process_alt(self.header, ref, alt))\n        # QUAL\n        if arr[5] == \".\":\n            qual = None\n        else:\n            try:\n                qual = int(arr[5])\n            except ValueError:  # try as float\n                qual = float(arr[5])\n        # FILTER\n        if arr[6] == \".\":\n            filt = []\n        else:\n            filt = arr[6].split(\";\")\n        self._check_filters(filt, \"FILTER\")\n        # INFO\n        info = self._parse_info(arr[7], len(alts))\n        if len(arr) == 9:\n            raise exceptions.IncorrectVCFFormat(\"Expected 8 or 10+ columns, got 9!\")\n        elif len(arr) == 8:\n            format_ = None\n            calls = None\n        else:\n            # FORMAT\n            format_ = arr[8].split(\":\")\n            # sample/call columns\n            calls = self._handle_calls(alts, format_, arr[8], arr)\n        return record.Record(chrom, pos, ids, ref, alts, qual, filt, info, format_, calls)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_calls(self, alts, format_, format_str, arr):\n        if format_str not in self._format_cache:\n            self._format_cache[format_str] = list(map(self.header.get_format_field_info, format_))\n        # per-sample calls\n        calls = []\n        for sample, raw_data in zip(self.samples.names, arr[9:]):\n            if self.samples.is_parsed(sample):\n                data = self._parse_calls_data(format_, self._format_cache[format_str], raw_data)\n                call = record.Call(sample, data)\n                self._format_checker.run(call, len(alts))\n                self._check_filters(call.data.get(\"FT\"), \"FORMAT/FT\", call.sample)\n                calls.append(call)\n            else:\n                calls.append(record.UnparsedCall(sample, raw_data))\n        return calls", "response": "Handle FORMAT and calls columns factored out of parse_line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _split_line(self, line_str):\n        arr = line_str.rstrip().split(\"\\t\")\n        if len(arr) != self.expected_fields:\n            raise exceptions.InvalidRecordException(\n                (\n                    \"The line contains an invalid number of fields. Was \"\n                    \"{} but expected {}\\n{}\".format(len(arr), 9 + len(self.samples.names), line_str)\n                )\n            )\n        return arr", "response": "Split line and check number of columns"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses INFO column from string", "response": "def _parse_info(self, info_str, num_alts):\n        \"\"\"Parse INFO column from string\"\"\"\n        result = OrderedDict()\n        if info_str == \".\":\n            return result\n        # The standard is very nice to parsers, we can simply split at\n        # semicolon characters, although I (Manuel) don't know how strict\n        # programs follow this\n        for entry in info_str.split(\";\"):\n            if \"=\" not in entry:  # flag\n                key = entry\n                result[key] = parse_field_value(self.header.get_info_field_info(key), True)\n            else:\n                key, value = split_mapping(entry)\n                result[key] = parse_field_value(self.header.get_info_field_info(key), value)\n            self._info_checker.run(key, result[key], num_alts)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_calls_data(klass, format_, infos, gt_str):\n        data = OrderedDict()\n        # The standard is very nice to parsers, we can simply split at\n        # colon characters, although I (Manuel) don't know how strict\n        # programs follow this\n        for key, info, value in zip(format_, infos, gt_str.split(\":\")):\n            data[key] = parse_field_value(info, value)\n        return data", "response": "Parse genotype call information from arrays using format array\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck header lines for valid VCF version.", "response": "def _check_header_lines(self, header_lines):\n        \"\"\"Check header lines, in particular for starting file \"##fileformat\"\n        \"\"\"\n        if not header_lines:\n            raise exceptions.InvalidHeaderException(\n                \"The VCF file did not contain any header lines!\"\n            )\n        first = header_lines[0]\n        if first.key != \"fileformat\":\n            raise exceptions.InvalidHeaderException(\"The VCF file did not start with ##fileformat\")\n        if first.value not in SUPPORTED_VCF_VERSIONS:\n            warnings.warn(\"Unknown VCF version {}\".format(first.value), UnknownVCFVersion)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck value in INFO [ key ] of record", "response": "def run(self, key, value, num_alts):\n        \"\"\"Check value in INFO[key] of record\n\n        Currently, only checks for consistent counts are implemented\n\n        :param str key: key of INFO entry to check\n        :param value: value to check\n        :param int alts: list of alternative alleles, for length\n        \"\"\"\n        field_info = self.header.get_info_field_info(key)\n        if not isinstance(value, list):\n            return\n        TABLE = {\n            \".\": len(value),\n            \"A\": num_alts,\n            \"R\": num_alts + 1,\n            \"G\": binomial(num_alts + 1, 2),  # diploid only at the moment\n        }\n        expected = TABLE.get(field_info.number, field_info.number)\n        if len(value) != expected:\n            tpl = \"Number of elements for INFO field {} is {} instead of {}\"\n            warnings.warn(\n                tpl.format(key, len(value), field_info.number), exceptions.IncorrectListLength\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_next_line(self):\n        prev_line = self._line\n        self._line = self.stream.readline()\n        return prev_line", "response": "Read next line store in self. _line and return old one"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread and parse a VCF header from file set into self. header and return it.", "response": "def parse_header(self, parsed_samples=None):\n        \"\"\"Read and parse :py:class:`vcfpy.header.Header` from file, set\n        into ``self.header`` and return it\n\n        :param list parsed_samples: ``list`` of ``str`` for subsetting the\n            samples to parse\n        :returns: ``vcfpy.header.Header``\n        :raises: ``vcfpy.exceptions.InvalidHeaderException`` in the case of\n            problems reading the header\n        \"\"\"\n        # parse header lines\n        sub_parser = HeaderParser()\n        header_lines = []\n        while self._line and self._line.startswith(\"##\"):\n            header_lines.append(sub_parser.parse_line(self._line))\n            self._read_next_line()\n        # parse sample info line\n        self.samples = self._handle_sample_line(parsed_samples)\n        # construct Header object\n        self.header = header.Header(header_lines, self.samples)\n        # check header for consistency\n        self._header_checker.run(self.header)\n        # construct record parser\n        self._record_parser = RecordParser(self.header, self.samples, self.record_checks)\n        # read next line, must not be header\n        self._read_next_line()\n        if self._line and self._line.startswith(\"#\"):\n            raise exceptions.IncorrectVCFFormat(\n                'Expecting non-header line or EOF after \"#CHROM\" line'\n            )\n        return self.header"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_sample_line(self, parsed_samples=None):\n        if not self._line or not self._line.startswith(\"#CHROM\"):\n            raise exceptions.IncorrectVCFFormat('Missing line starting with \"#CHROM\"')\n        # check for space before INFO\n        line = self._line.rstrip()\n        pos = line.find(\"FORMAT\") if (\"FORMAT\" in line) else line.find(\"INFO\")\n        if pos == -1:\n            raise exceptions.IncorrectVCFFormat('Ill-formatted line starting with \"#CHROM\"')\n        if \" \" in line[:pos]:\n            warnings.warn(\n                (\n                    \"Found space in #CHROM line, splitting at whitespace \"\n                    \"instead of tab; this VCF file is ill-formatted\"\n                ),\n                SpaceInChromLine,\n            )\n            arr = self._line.rstrip().split()\n        else:\n            arr = self._line.rstrip().split(\"\\t\")\n\n        self._check_samples_line(arr)\n        return header.SamplesInfos(arr[len(REQUIRE_SAMPLE_HEADER) :], parsed_samples)", "response": "Check and interpret the ##CHROM line and return samples"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numpy():\n        '''Lazily import the numpy module'''\n        if LazyImport.numpy_module is None:\n            try:\n                LazyImport.numpy_module = __import__('numpypy')\n            except ImportError:\n                try:\n                    LazyImport.numpy_module = __import__('numpy')\n                except ImportError:\n                    raise ImportError('The numpy module is required')\n        return LazyImport.numpy_module", "response": "Lazily import the numpy module"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_position(pos):\n\n    posiction_dict = dict(zip(range(1, 17), [i for i in range(30, 62) if i % 2]))\n    return posiction_dict[pos]", "response": "Map natural position to machine code postion"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef snap(self, path=None):\n        if path is None:\n            path = \"/tmp\"\n        else:\n            path = path.rstrip(\"/\")\n        day_dir = datetime.datetime.now().strftime(\"%d%m%Y\")\n        hour_dir = datetime.datetime.now().strftime(\"%H%M\")\n        ensure_snapshot_dir(path+\"/\"+self.cam_id+\"/\"+day_dir+\"/\"+hour_dir)\n        f_path = \"{0}/{1}/{2}/{3}/{4}.jpg\".format(\n                path,\n                self.cam_id,\n                day_dir,\n                hour_dir,\n                datetime.datetime.now().strftime(\"%S\"),\n        )\n\n        urllib.urlretrieve(\n            'http://{0}/snapshot.cgi?user={1}&pwd={2}'.format(\n                                    self.address, \n                                    self.user, \n                                    self.pswd,\n                                    ),\n            f_path,\n        )", "response": "Get a snapshot and save it to disk."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving cam to given preset position.", "response": "def move(self, pos):\n        \"\"\"Move cam to given preset position.\n        pos - must be within 1 to 16.\n        Returns: CamException in case of errors, \"ok\" otherwise.\"\"\"\n\n        try:\n            payload = {\"address\":self.address, \"user\": self.user, \"pwd\": self.pswd, \"pos\": map_position(pos)}\n            resp = requests.get(\n                    \"http://{address}/decoder_control.cgi?command={pos}&user={user}&pwd={pwd}\".format(**payload)\n            )\n        except KeyError:\n            raise CamException(\"Position must be within 1 to 16.\")\n        if resp.status_code != 200:\n            raise CamException(\"Unauthorized. Wrong user or password.\")\n        return \"ok\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving some configuration params.", "response": "def status(self):\n        \"\"\"Retrieve some configuration params.\n        Note: info are returned even without password\"\"\"\n\n        resp = requests.get(\"http://{0}/get_status.cgi\".format(self.address))\n        data = resp.text.replace(\";\", \"\")\n        data = data.replace(\"var\", \"\")\n        data_s = data.split(\"\\n\")\n        # Last is an empty line \n        data_s.pop()\n        data_array = [s.split(\"=\") for s in data_s]\n        return dict(data_array)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepRESearchStr(matchStr, wordInitial='ok', wordFinal='ok',\n                     spanSyllable='ok', stressedSyllable='ok'):\n    '''\n    Prepares a user's RE string for a search\n    '''\n    \n    # Protect sounds that are two characters\n    # After this we can assume that each character represents a sound\n    # (We'll revert back when we're done processing the RE)\n    replList = [(u'ei', u'9'), (u't\u0283', u'='), (u'o\u028a', u'~'),\n                (u'd\u0292', u'@'), (u'a\u028a', u'%'), (u'\u0251\u026a', u'&'),\n                (u'\u0254i', u'$')]\n\n    # Add to the replList\n    currentReplNum = 0\n    startI = 0\n    for left, right in (('(', ')'), ('[', ']')):\n        while True:\n            try:\n                i = matchStr.index(left, startI)\n            except ValueError:\n                break\n            j = matchStr.index(right, i) + 1\n            replList.append((matchStr[i:j], str(currentReplNum)))\n            currentReplNum += 1\n            startI = j\n    \n    for charA, charB in replList:\n        matchStr = matchStr.replace(charA, charB)\n    \n    # Characters to check between all other characters\n    # Don't check between all other characters if the character is already\n    # in the search string or\n    interleaveStr = None\n    stressOpt = (stressedSyllable == 'ok' or stressedSyllable == 'only')\n    spanOpt = (spanSyllable == 'ok' or spanSyllable == 'only')\n    if stressOpt and spanOpt:\n        interleaveStr = u\"\\.?\u02c8?\"\n    elif stressOpt:\n        interleaveStr = u\"\u02c8?\"\n    elif spanOpt:\n        interleaveStr = u\"\\.?\"\n    \n    if interleaveStr is not None:\n        matchStr = interleaveStr.join(matchStr)\n    \n    # Setting search boundaries\n    # We search on '[^\\.#]' and not '.' so that the search doesn't span\n    # multiple syllables or words\n    if wordInitial == 'only':\n        matchStr = u'#' + matchStr\n    elif wordInitial == 'no':\n        # Match the closest preceeding syllable.  If there is none, look\n        # for word boundary plus at least one other character\n        matchStr = u'(?:\\.[^\\.#]*?|#[^\\.#]+?)' + matchStr\n    else:\n        matchStr = u'[#\\.][^\\.#]*?' + matchStr\n    \n    if wordFinal == 'only':\n        matchStr = matchStr + u'#'\n    elif wordFinal == 'no':\n        matchStr = matchStr + u\"(?:[^\\.#]*?\\.|[^\\.#]+?#)\"\n    else:\n        matchStr = matchStr + u'[^\\.#]*?[#\\.]'\n    \n    # For sounds that are designated two characters, prevent\n    # detecting those sounds if the user wanted a sound\n    # designated by one of the contained characters\n    \n    # Forward search ('a' and not 'ab')\n    insertList = []\n    for charA, charB in [(u'e', u'i'), (u't', u'\u0283'), (u'd', u'\u0292'),\n                         (u'o', u'\u028a'), (u'a', u'\u028a|\u026a'), (u'\u0254', u'i'), ]:\n        startI = 0\n        while True:\n            try:\n                i = matchStr.index(charA, startI)\n            except ValueError:\n                break\n            if matchStr[i + 1] != charB:\n                forwardStr = u'(?!%s)' % charB\n#                 matchStr = matchStr[:i + 1] + forwardStr + matchStr[i + 1:]\n                startI = i + 1 + len(forwardStr)\n                insertList.append((i + 1, forwardStr))\n        \n    # Backward search ('b' and not 'ab')\n    for charA, charB in [(u't', u'\u0283'), (u'd', u'\u0292'),\n                         (u'a|o', u'\u028a'), (u'e|\u0254', u'i'), (u'\u0251' u'\u026a'), ]:\n        startI = 0\n        while True:\n            try:\n                i = matchStr.index(charB, startI)\n            except ValueError:\n                break\n            if matchStr[i - 1] != charA:\n                backStr = u'(?<!%s)' % charA\n#                 matchStr = matchStr[:i] + backStr + matchStr[i:]\n                startI = i + 1 + len(backStr)\n                insertList.append((i, backStr))\n                \n    insertList.sort()\n    for i, insertStr in insertList[::-1]:\n        matchStr = matchStr[:i] + insertStr + matchStr[i:]\n    \n    # Revert the special sounds back from 1 character to 2 characters\n    for charA, charB in replList:\n        matchStr = matchStr.replace(charB, charA)\n\n    # Replace special characters\n    replDict = {\"D\": u\"(?:t(?!\u0283)|d(?!\u0292)|[sz])\",  # dentals\n                \"F\": u\"[\u0283\u0292fvsz\u0275\u00f0h]\",  # fricatives\n                \"S\": u\"(?:t(?!\u0283)|d(?!\u0292)|[pbkg])\",  # stops\n                \"N\": u\"[nm\u014b]\",  # nasals\n                \"R\": u\"[r\u025d\u025a]\",  # rhotics\n                \"V\": u\"(?:a\u028a|ei|o\u028a|\u0251\u026a|\u0254i|[iu\u00e6\u0251\u0254\u0259\u025b\u026a\u028a\u028c]):?\",  # vowels\n                \"B\": u\"\\.\",  # syllable boundary\n                }\n\n    for char, replStr in replDict.items():\n        matchStr = matchStr.replace(char, replStr)\n\n    return matchStr", "response": "Prepares a user s RE string for a search for a search."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for matching words in the dictionary with regular expressions.", "response": "def search(searchList, matchStr, numSyllables=None, wordInitial='ok',\n           wordFinal='ok', spanSyllable='ok', stressedSyllable='ok',\n           multiword='ok', pos=None):\n    '''\n    Searches for matching words in the dictionary with regular expressions\n    \n    wordInitial, wordFinal, spanSyllable, stressSyllable, and multiword\n    can take three different values: 'ok', 'only', or 'no'.\n    \n    pos: a tag in the Penn Part of Speech tagset\n        see isletool.posList for the full list of possible tags\n    \n    Special search characters:\n    'D' - any dental; 'F' - any fricative; 'S' - any stop\n    'V' - any vowel; 'N' - any nasal; 'R' - any rhotic\n    '#' - word boundary\n    'B' - syllable boundary\n    '.' - anything\n    \n    For advanced queries:\n    Regular expression syntax applies, so if you wanted to search for any\n    word ending with a vowel or rhotic, matchStr = '(?:VR)#', '[VR]#', etc.\n    '''\n    # Run search for words\n    \n    matchStr = _prepRESearchStr(matchStr, wordInitial, wordFinal,\n                                spanSyllable, stressedSyllable)\n    \n    compiledRE = re.compile(matchStr)\n    retList = []\n    for word, pronList in searchList:\n        newPronList = []\n        for pron, posList in pronList:\n            searchPron = pron.replace(\",\", \"\").replace(\" \", \"\")\n            \n            # Search for pos\n            if pos is not None:\n                if pos not in posList:\n                    continue\n            \n            # Ignore diacritics for now:\n            for diacritic in diacriticList:\n                if diacritic not in matchStr:\n                    searchPron = searchPron.replace(diacritic, \"\")\n            \n            if numSyllables is not None:\n                if numSyllables != searchPron.count('.') + 1:\n                    continue\n            \n            # Is this a compound word?\n            if multiword == 'only':\n                if searchPron.count('#') == 2:\n                    continue\n            elif multiword == 'no':\n                if searchPron.count('#') > 2:\n                    continue\n            \n            matchList = compiledRE.findall(searchPron)\n            if len(matchList) > 0:\n                if stressedSyllable == 'only':\n                    if all([u\"\u02c8\" not in match for match in matchList]):\n                        continue\n                if stressedSyllable == 'no':\n                    if all([u\"\u02c8\" in match for match in matchList]):\n                        continue\n                \n                # For syllable spanning, we check if there is a syllable\n                # marker inside (not at the border) of the match.\n                if spanSyllable == 'only':\n                    if all([\".\" not in txt[1:-1] for txt in matchList]):\n                        continue\n                if spanSyllable == 'no':\n                    if all([\".\" in txt[1:-1] for txt in matchList]):\n                        continue\n                newPronList.append((pron, posList))\n        \n        if len(newPronList) > 0:\n            retList.append((word, newPronList))\n    \n    retList.sort()\n    return retList"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parsePronunciation(pronunciationStr):\n    '''\n    Parses the pronunciation string\n    \n    Returns the list of syllables and a list of primary and\n    secondary stress locations\n    '''\n    retList = []\n    for syllableTxt in pronunciationStr.split(\"#\"):\n        if syllableTxt == \"\":\n            continue\n        syllableList = [x.split() for x in syllableTxt.split(' . ')]\n        \n        # Find stress\n        stressedSyllableList = []\n        stressedPhoneList = []\n        for i, syllable in enumerate(syllableList):\n            for j, phone in enumerate(syllable):\n                if u\"\u02c8\" in phone:\n                    stressedSyllableList.insert(0, i)\n                    stressedPhoneList.insert(0, j)\n                    break\n                elif u'\u02cc' in phone:\n                    stressedSyllableList.append(i)\n                    stressedPhoneList.append(j)\n        \n        retList.append((syllableList, stressedSyllableList, stressedPhoneList))\n    \n    return retList", "response": "Parses the pronunciation string and returns the list of syllables and a list of primary and secondary stress locations\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getNumPhones(isleDict, word, maxFlag):\n    '''\n    Get the number of syllables and phones in this word\n    \n    If maxFlag=True, use the longest pronunciation.  Otherwise, take the\n    average length.\n    '''\n    phoneCount = 0\n    syllableCount = 0\n    \n    syllableCountList = []\n    phoneCountList = []\n    \n    wordList = isleDict.lookup(word)\n    entryList = zip(*wordList)\n    \n    for lookupResultList in entryList:\n        syllableList = []\n        for wordSyllableList in lookupResultList:\n            syllableList.extend(wordSyllableList)\n        \n        syllableCountList.append(len(syllableList))\n        phoneCountList.append(len([phon for phoneList in syllableList for\n                                   phon in phoneList]))\n    \n    # The average number of phones for all possible pronunciations\n    #    of this word\n    if maxFlag is True:\n        syllableCount += max(syllableCountList)\n        phoneCount += max(phoneCountList)\n    else:\n        syllableCount += (sum(syllableCountList) /\n                          float(len(syllableCountList)))\n        phoneCount += sum(phoneCountList) / float(len(phoneCountList))\n    \n    return syllableCount, phoneCount", "response": "Get the number of syllables and phones in this word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all of the out - of - dictionary words found in a list of utterances", "response": "def findOODWords(isleDict, wordList):\n    '''\n    Returns all of the out-of-dictionary words found in a list of utterances\n    '''\n    oodList = []\n    for word in wordList:\n        try:\n            isleDict.lookup(word)\n        except WordNotInISLE:\n            oodList.append(word)\n                \n    oodList = list(set(oodList))\n    oodList.sort()\n    \n    return oodList"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef autopair(isleDict, wordList):\n    '''\n    Tests whether adjacent words are OOD or not\n    \n    It returns complete wordLists with the matching words replaced.\n    Each match yields one sentence.\n    \n    e.g.\n    red ball chaser\n    would return\n    [[red_ball chaser], [red ball_chaser]], [0, 1]\n    \n    if 'red_ball' and 'ball_chaser' were both in the dictionary\n    '''\n    \n    newWordList = [(\"%s_%s\" % (wordList[i], wordList[i + 1]), i)\n                   for i in range(0, len(wordList) - 1)]\n    \n    sentenceList = []\n    indexList = []\n    for word, i in newWordList:\n        if word in isleDict.data:\n            sentenceList.append(wordList[:i] + [word, ] + wordList[i + 1:])\n            indexList.append(i)\n    \n    return sentenceList, indexList", "response": "Autopair is used to generate a list of words that are adjacent to a word in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the isle textfile into a dictionary for fast searching", "response": "def _buildDict(self):\n        '''\n        Builds the isle textfile into a dictionary for fast searching\n        '''\n        lexDict = {}\n        with io.open(self.islePath, \"r\", encoding='utf-8') as fd:\n            wordList = [line.rstrip('\\n') for line in fd]\n        \n        for row in wordList:\n            word, pronunciation = row.split(\" \", 1)\n            word, extraInfo = word.split(\"(\", 1)\n            \n            extraInfo = extraInfo.replace(\")\", \"\")\n            extraInfoList = [segment for segment in extraInfo.split(\",\")\n                             if (\"_\" not in segment and \"+\" not in segment and\n                                 ':' not in segment and segment != '')]\n            \n            lexDict.setdefault(word, [])\n            lexDict[word].append((pronunciation, extraInfoList))\n        \n        return lexDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup(self, word):\n        '''\n        Lookup a word and receive a list of syllables and stressInfo\n        \n        Output example for the word 'another' which has two pronunciations\n        [(([[u'\u0259'], [u'n', u'\u02c8\u028c'], [u'\u00f0', u'\u025a']], [1], [1]),\n          ([[u'\u0259'], [u'n', u'\u02c8\u028c'], [u'\u00f0', u'\u0259', u'\u0279']], [1], [1]))]\n        '''\n        \n        # All words must be lowercase with no extraneous whitespace\n        word = word.lower()\n        word = word.strip()\n        \n        pronList = self.data.get(word, None)\n        \n        if pronList is None:\n            raise WordNotInISLE(word)\n        else:\n            pronList = [_parsePronunciation(pronunciationStr)\n                        for pronunciationStr, _ in pronList]\n            pronList = list(zip(*pronList))\n        \n        return pronList", "response": "Lookup a word and receive a list of syllables and stressInfo\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, matchStr, numSyllables=None, wordInitial='ok',\n               wordFinal='ok', spanSyllable='ok', stressedSyllable='ok',\n               multiword='ok', pos=None):\n        '''\n        for help on isletool.LexicalTool.search(), see see isletool.search()\n        '''\n        return search(self.data.items(), matchStr, numSyllables=numSyllables,\n                      wordInitial=wordInitial, wordFinal=wordFinal,\n                      spanSyllable=spanSyllable,\n                      stressedSyllable=stressedSyllable,\n                      multiword=multiword, pos=pos)", "response": "Search the data for a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timestamps(self):\n        '''Get all timestamps from all series in the group.'''\n        timestamps = set()\n        for series in self.groups.itervalues():\n            timestamps |= set(series.timestamps)\n        return sorted(list(timestamps))", "response": "Get all timestamps from all series in the group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trend(self, **kwargs):\n        '''Calculate a trend for all series in the group. See the\n        `TimeSeries.trend()` method for more information.'''\n        return DataFrame({ name: series.trend(**kwargs) \\\n            for name, series in self.groups.iteritems() })", "response": "Calculate a trend for all series in the group. See the\n       . trend method for more information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forecast(self, horizon, **kwargs):\n        '''Forecast all time series in the group. See the\n        `TimeSeries.forecast()` method for more information.'''\n        return DataFrame({ name: series.forecast(horizon, **kwargs) \\\n            for name, series in self.groups.iteritems() })", "response": "Forecast all time series in the group. See the\n       . forecast method for more information."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot all time series in the group.", "response": "def plot(self, overlay=True, **labels): # pragma: no cover\n        '''Plot all time series in the group.'''\n        pylab = LazyImport.pylab()\n        colours = list('rgbymc')\n        colours_len = len(colours)\n        colours_pos = 0\n        plots = len(self.groups)\n        for name, series in self.groups.iteritems():\n            colour = colours[colours_pos % colours_len]\n            colours_pos += 1\n            if not overlay:\n                pylab.subplot(plots, 1, colours_pos)\n            kwargs = {}\n            if name in labels:\n                name = labels[name]\n            if name is not None:\n                kwargs['label'] = name\n            pylab.plot(series.dates, series.values, '%s-' % colour, **kwargs)\n            if name is not None:\n                pylab.legend()\n        pylab.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename(self, **kwargs):\n        '''Rename series in the group.'''\n        for old, new in kwargs.iteritems():\n            if old in self.groups:\n                self.groups[new] = self.groups[old]\n                del self.groups[old]", "response": "Rename series in the group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_stream(\n        klass, stream, path=None, tabix_path=None, record_checks=None, parsed_samples=None\n    ):\n        \"\"\"Create new :py:class:`Reader` from file\n\n        .. note::\n            If you use the ``parsed_samples`` feature and you write out\n            records then you must not change the ``FORMAT`` of the record.\n\n        :param stream: ``file``-like object to read from\n        :param path: optional string with path to store (for display only)\n        :param list record_checks: record checks to perform, can contain\n            'INFO' and 'FORMAT'\n        :param list parsed_samples: ``list`` of ``str`` values with names of\n            samples to parse call information for (for speedup); leave to\n            ``None`` for ignoring\n        \"\"\"\n        record_checks = record_checks or []\n        if tabix_path and not path:\n            raise ValueError(\"Must give path if tabix_path is given\")\n        return Reader(\n            stream=stream,\n            path=path,\n            tabix_path=tabix_path,\n            record_checks=record_checks,\n            parsed_samples=parsed_samples,\n        )", "response": "Create a new reader from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new reader from a TBI file.", "response": "def from_path(klass, path, tabix_path=None, record_checks=None, parsed_samples=None):\n        \"\"\"Create new :py:class:`Reader` from path\n\n        .. note::\n            If you use the ``parsed_samples`` feature and you write out\n            records then you must not change the ``FORMAT`` of the record.\n\n        :param path: the path to load from (converted to ``str`` for\n            compatibility with ``path.py``)\n        :param tabix_path: optional string with path to TBI index,\n            automatic inferral from ``path`` will be tried on the fly\n            if not given\n        :param list record_checks: record checks to perform, can contain\n            'INFO' and 'FORMAT'\n        \"\"\"\n        record_checks = record_checks or []\n        path = str(path)\n        if path.endswith(\".gz\"):\n            f = gzip.open(path, \"rt\")\n            if not tabix_path:\n                tabix_path = path + \".tbi\"\n                if not os.path.exists(tabix_path):\n                    tabix_path = None  # guessing path failed\n        else:\n            f = open(path, \"rt\")\n        return klass.from_stream(\n            stream=f,\n            path=path,\n            tabix_path=tabix_path,\n            record_checks=record_checks,\n            parsed_samples=parsed_samples,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch(self, chrom_or_region, begin=None, end=None):\n        if begin is not None and end is None:\n            raise ValueError(\"begin and end must both be None or neither\")\n        # close tabix file if any and is open\n        if self.tabix_file and not self.tabix_file.closed:\n            self.tabix_file.close()\n        # open tabix file if not yet open\n        if not self.tabix_file or self.tabix_file.closed:\n            self.tabix_file = pysam.TabixFile(filename=self.path, index=self.tabix_path)\n        # jump to the next position\n        if begin is None:\n            self.tabix_iter = self.tabix_file.fetch(region=chrom_or_region)\n        else:\n            self.tabix_iter = self.tabix_file.fetch(reference=chrom_or_region, start=begin, end=end)\n        return self", "response": "Fetch the next set of ifoncite entries from the given chromosomal position."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if self.tabix_file and not self.tabix_file.closed:\n            self.tabix_file.close()\n        if self.stream:\n            self.stream.close()", "response": "Close underlying tabix file and underlying stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serialize_for_header(key, value):\n    if key in QUOTE_FIELDS:\n        return json.dumps(value)\n    elif isinstance(value, str):\n        if \" \" in value or \"\\t\" in value:\n            return json.dumps(value)\n        else:\n            return value\n    elif isinstance(value, list):\n        return \"[{}]\".format(\", \".join(value))\n    else:\n        return str(value)", "response": "Serialize value for the given mapping key for a VCF header line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef header_without_lines(header, remove):\n    remove = set(remove)\n    # Copy over lines that are not removed\n    lines = []\n    for line in header.lines:\n        if hasattr(line, \"mapping\"):\n            if (line.key, line.mapping.get(\"ID\", None)) in remove:\n                continue  # filter out\n        else:\n            if (line.key, line.value) in remove:\n                continue  # filter out\n        lines.append(line)\n    return Header(lines, header.samples)", "response": "Return a new header with lines given in remove."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a dictionary to a string", "response": "def mapping_to_str(mapping):\n    \"\"\"Convert mapping to string\"\"\"\n    result = [\"<\"]\n    for i, (key, value) in enumerate(mapping.items()):\n        if i > 0:\n            result.append(\",\")\n        result += [key, \"=\", serialize_for_header(key, value)]\n    result += [\">\"]\n    return \"\".join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_indices(self):\n        result = {key: OrderedDict() for key in LINES_WITH_ID}\n        for line in self.lines:\n            if line.key in LINES_WITH_ID:\n                result.setdefault(line.key, OrderedDict())\n                if line.mapping[\"ID\"] in result[line.key]:\n                    warnings.warn(\n                        (\"Seen {} header more than once: {}, using first\" \"occurence\").format(\n                            line.key, line.mapping[\"ID\"]\n                        ),\n                        DuplicateHeaderLineWarning,\n                    )\n                else:\n                    result[line.key][line.mapping[\"ID\"]] = line\n            else:\n                result.setdefault(line.key, [])\n                result[line.key].append(line)\n        return result", "response": "Build the indices for the different field types"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self):\n        return Header([line.copy() for line in self.lines], self.samples.copy())", "response": "Return a copy of this header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of header lines having the given key as their type", "response": "def get_lines(self, key):\n        \"\"\"Return header lines having the given ``key`` as their type\"\"\"\n        if key in self._indices:\n            return self._indices[key].values()\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if there is a header line with the given ID of the the type given by key.", "response": "def has_header_line(self, key, id_):\n        \"\"\"Return whether there is a header line with the given ID of the\n        type given by ``key``\n\n        :param key: The VCF header key/line type.\n        :param id_: The ID value to compare fore\n\n        :return: ``True`` if there is a header line starting with ``##${key}=``\n            in the VCF file having the mapping entry ``ID`` set to ``id_``.\n        \"\"\"\n        if key not in self._indices:\n            return False\n        else:\n            return id_ in self._indices[key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a header line to the list of available header lines and updates any necessary support indices", "response": "def add_line(self, header_line):\n        \"\"\"Add header line, updating any necessary support indices\n\n        :return: ``False`` on conflicting line and ``True`` otherwise\n        \"\"\"\n        self.lines.append(header_line)\n        self._indices.setdefault(header_line.key, OrderedDict())\n        if not hasattr(header_line, \"mapping\"):\n            return False  # no registration required\n        if self.has_header_line(header_line.key, header_line.mapping[\"ID\"]):\n            warnings.warn(\n                (\n                    \"Detected duplicate header line with type {} and ID {}. \"\n                    \"Ignoring this and subsequent one\"\n                ).format(header_line.key, header_line.mapping[\"ID\"]),\n                DuplicateHeaderLineWarning,\n            )\n            return False\n        else:\n            self._indices[header_line.key][header_line.mapping[\"ID\"]] = header_line\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self):\n        mapping = OrderedDict(self.mapping.items())\n        return self.__class__(self.key, self.value, mapping)", "response": "Return a copy of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_number(klass, number):\n        try:\n            return int(number)\n        except ValueError as e:\n            if number in VALID_NUMBERS:\n                return number\n            else:\n                raise e", "response": "Parse a number into an int or return number if a valid\n        expression for a INFO / FORMAT Number."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _adjustSyllabification(adjustedPhoneList, syllableList):\n    '''\n    Inserts spaces into a syllable if needed\n    \n    Originally the phone list and syllable list contained the same number\n    of phones.  But the adjustedPhoneList may have some insertions which are\n    not accounted for in the syllableList.\n    '''\n    i = 0\n    retSyllableList = []\n    for syllableNum, syllable in enumerate(syllableList):\n        j = len(syllable)\n        if syllableNum == len(syllableList) - 1:\n            j = len(adjustedPhoneList) - i\n        tmpPhoneList = adjustedPhoneList[i:i + j]\n        numBlanks = -1\n        phoneList = tmpPhoneList[:]\n        while numBlanks != 0:\n            \n            numBlanks = tmpPhoneList.count(u\"''\")\n            if numBlanks > 0:\n                tmpPhoneList = adjustedPhoneList[i + j:i + j + numBlanks]\n                phoneList.extend(tmpPhoneList)\n                j += numBlanks\n        \n        for k, phone in enumerate(phoneList):\n            if phone == u\"''\":\n                syllable.insert(k, u\"''\")\n        \n        i += j\n        \n        retSyllableList.append(syllable)\n    \n    return retSyllableList", "response": "Adjusts a syllable list to be a subset of the phone list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the pronunciation that is best to use for a given pronunciation.", "response": "def _findBestPronunciation(isleWordList, aPron):\n    '''\n    Words may have multiple candidates in ISLE; returns the 'optimal' one.\n    '''\n    \n    aP = _prepPronunciation(aPron)  # Mapping to simplified phone inventory\n    \n    numDiffList = []\n    withStress = []\n    i = 0\n    alignedSyllabificationList = []\n    alignedActualPronunciationList = []\n    for wordTuple in isleWordList:\n        aPronMap = copy.deepcopy(aPron)\n        syllableList = wordTuple[0]  # syllableList, stressList\n        \n        iP = [phone for phoneList in syllableList for phone in phoneList]\n        iP = _prepPronunciation(iP)\n\n        alignedIP, alignedAP = alignPronunciations(iP, aP)\n        \n        # Remapping to actual phones\n#         alignedAP = [origPronDict.get(phon, u\"''\") for phon in alignedAP]\n        alignedAP = [aPronMap.pop(0) if phon != u\"''\" else u\"''\"\n                     for phon in alignedAP]\n        alignedActualPronunciationList.append(alignedAP)\n        \n        # Adjusting the syllabification for differences between the dictionary\n        # pronunciation and the actual pronunciation\n        alignedSyllabification = _adjustSyllabification(alignedIP,\n                                                        syllableList)\n        alignedSyllabificationList.append(alignedSyllabification)\n        \n        # Count the number of misalignments between the two\n        numDiff = alignedIP.count(u\"''\") + alignedAP.count(u\"''\")\n        numDiffList.append(numDiff)\n        \n        # Is there stress in this word\n        hasStress = False\n        for syllable in syllableList:\n            for phone in syllable:\n                hasStress = u\"\u02c8\" in phone or hasStress\n        \n        if hasStress:\n            withStress.append(i)\n        i += 1\n    \n    # Return the pronunciation that had the fewest differences\n    #     to the actual pronunciation\n    minDiff = min(numDiffList)\n    \n    # When there are multiple candidates that have the minimum number\n    #     of differences, prefer one that has stress in it\n    bestIndex = None\n    bestIsStressed = None\n    for i, numDiff in enumerate(numDiffList):\n        if numDiff != minDiff:\n            continue\n        if bestIndex is None:\n            bestIndex = i\n            bestIsStressed = i in withStress\n        else:\n            if not bestIsStressed and i in withStress:\n                bestIndex = i\n                bestIsStressed = True\n    \n    return (isleWordList, alignedActualPronunciationList,\n            alignedSyllabificationList, bestIndex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a phone list and a syllable list syllabify the phones.", "response": "def _syllabifyPhones(phoneList, syllableList):\n    '''\n    Given a phone list and a syllable list, syllabify the phones\n    \n    Typically used by findBestSyllabification which first aligns the phoneList\n    with a dictionary phoneList and then uses the dictionary syllabification\n    to syllabify the input phoneList.\n    '''\n    \n    numPhoneList = [len(syllable) for syllable in syllableList]\n    \n    start = 0\n    syllabifiedList = []\n    for end in numPhoneList:\n        \n        syllable = phoneList[start:start + end]\n        syllabifiedList.append(syllable)\n        \n        start += end\n    \n    return syllabifiedList"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naligning the phones in two pronunciations in two pronunciations in two pronunciations in two pronunciations in order.", "response": "def alignPronunciations(pronI, pronA):\n    '''\n    Align the phones in two pronunciations\n    '''\n    \n    # First prep the two pronunctions\n    pronI = [char for char in pronI]\n    pronA = [char for char in pronA]\n    \n    # Remove any elements not in the other list (but maintain order)\n    pronITmp = pronI\n    pronATmp = pronA\n\n    # Find the longest sequence\n    sequence = _lcs(pronITmp, pronATmp)\n\n    # Find the index of the sequence\n    # TODO: investigate ambiguous cases\n    startA = 0\n    startI = 0\n    sequenceIndexListA = []\n    sequenceIndexListI = []\n    for phone in sequence:\n        startA = pronA.index(phone, startA)\n        startI = pronI.index(phone, startI)\n        \n        sequenceIndexListA.append(startA)\n        sequenceIndexListI.append(startI)\n    \n    # An index on the tail of both will be used to create output strings\n    # of the same length\n    sequenceIndexListA.append(len(pronA))\n    sequenceIndexListI.append(len(pronI))\n    \n    # Fill in any blanks such that the sequential items have the same\n    # index and the two strings are the same length\n    for x in range(len(sequenceIndexListA)):\n        indexA = sequenceIndexListA[x]\n        indexI = sequenceIndexListI[x]\n        if indexA < indexI:\n            for x in range(indexI - indexA):\n                pronA.insert(indexA, \"''\")\n            sequenceIndexListA = [val + indexI - indexA\n                                  for val in sequenceIndexListA]\n        elif indexA > indexI:\n            for x in range(indexA - indexI):\n                pronI.insert(indexI, \"''\")\n            sequenceIndexListI = [val + indexA - indexI\n                                  for val in sequenceIndexListI]\n    \n    return pronI, pronA"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _findBestSyllabification(inputIsleWordList, actualPronunciationList):\n    '''\n    Find the best syllabification for a word\n    \n    First find the closest pronunciation to a given pronunciation. Then take\n    the syllabification for that pronunciation and map it onto the\n    input pronunciation.\n    '''\n    retList = _findBestPronunciation(inputIsleWordList,\n                                     actualPronunciationList)\n    isleWordList, alignedAPronList, alignedSyllableList, bestIndex = retList\n    \n    alignedPhoneList = alignedAPronList[bestIndex]\n    alignedSyllables = alignedSyllableList[bestIndex]\n    syllabification = isleWordList[bestIndex][0]\n    stressedSyllableIndexList = isleWordList[bestIndex][1]\n    stressedPhoneIndexList = isleWordList[bestIndex][2]\n    \n    syllableList = _syllabifyPhones(alignedPhoneList, alignedSyllables)\n    \n    # Get the location of stress in the generated file\n    try:\n        stressedSyllableI = stressedSyllableIndexList[0]\n    except IndexError:\n        stressedSyllableI = None\n        stressedVowelI = None\n    else:\n        stressedVowelI = _getSyllableNucleus(syllableList[stressedSyllableI])\n    \n    # Count the index of the stressed phones, if the stress list has\n    # become flattened (no syllable information)\n    flattenedStressIndexList = []\n    for i, j in zip(stressedSyllableIndexList, stressedPhoneIndexList):\n        k = j\n        for l in range(i):\n            k += len(syllableList[l])\n        flattenedStressIndexList.append(k)\n    \n    return (stressedSyllableI, stressedVowelI, syllableList, syllabification,\n            stressedSyllableIndexList, stressedPhoneIndexList,\n            flattenedStressIndexList)", "response": "Find the best syllabification for a word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive the phones in a syllable retrieves the vowel index", "response": "def _getSyllableNucleus(phoneList):\n    '''\n    Given the phones in a syllable, retrieves the vowel index\n    '''\n    cvList = ['V' if isletool.isVowel(phone) else 'C' for phone in phoneList]\n    \n    vowelCount = cvList.count('V')\n    if vowelCount > 1:\n        raise TooManyVowelsInSyllable(phoneList, cvList)\n    \n    if vowelCount == 1:\n        stressI = cvList.index('V')\n    else:\n        stressI = None\n        \n    return stressI"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findClosestPronunciation(inputIsleWordList, aPron):\n    '''\n    Find the closest dictionary pronunciation to a provided pronunciation\n    '''\n    \n    retList = _findBestPronunciation(inputIsleWordList, aPron)\n    isleWordList = retList[0]\n    bestIndex = retList[3]\n    \n    return isleWordList[bestIndex]", "response": "Find the closest pronunciation to a provided pronunciation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_switch(type, settings, pin):\n\n\tswitch = None\n\tif type == \"A\":\n\t\tgroup, device = settings.split(\",\")\n\t\tswitch = pi_switch.RCSwitchA(group, device)\n\n\telif type == \"B\":\n\t\taddr, channel = settings.split(\",\")\n\t\taddr = int(addr)\n\t\tchannel = int(channel)\n\t\tswitch = pi_switch.RCSwitchB(addr, channel)\n\n\telif type == \"C\":\n\t\tfamily, group, device = settings.split(\",\")\n\t\tgroup = int(group)\n\t\tdevice = int(device)\n\t\tswitch = pi_switch.RCSwitchC(family, group, device)\n\n\telif type == \"D\":\n\t\tgroup, device = settings.split(\",\")\n\t\tdevice = int(device)\n\t\tswitch = pi_switch.RCSwitchD(group, device)\n\n\telse:\n\t\tprint \"Type %s is not supported!\" % type\n\t\tsys.exit()\n\n\tswitch.enableTransmit(pin)\n\treturn switch", "response": "Create a switch.\n\n    Args:\n        type: (str): type of the switch [A,B,C,D]\n        settings (str): a comma separted list\n        pin (int): wiringPi pin\n\n    Returns:\n        switch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting atomic value Taxonomy", "response": "def format_atomic(value):\n    \"\"\"Format atomic value\n\n    This function also takes care of escaping the value in case one of the\n    reserved characters occurs in the value.\n    \"\"\"\n    # Perform escaping\n    if isinstance(value, str):\n        if any(r in value for r in record.RESERVED_CHARS):\n            for k, v in record.ESCAPE_MAPPING:\n                value = value.replace(k, v)\n    # String-format the given value\n    if value is None:\n        return \".\"\n    else:\n        return str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat possibly compound value given the FieldInfo", "response": "def format_value(field_info, value, section):\n    \"\"\"Format possibly compound value given the FieldInfo\"\"\"\n    if section == \"FORMAT\" and field_info.id == \"FT\":\n        if not value:\n            return \".\"\n        elif isinstance(value, list):\n            return \";\".join(map(format_atomic, value))\n    elif field_info.number == 1:\n        if value is None:\n            return \".\"\n        else:\n            return format_atomic(value)\n    else:\n        if not value:\n            return \".\"\n        else:\n            return \",\".join(map(format_atomic, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_stream(klass, stream, header, path=None, use_bgzf=None):\n        if use_bgzf or (use_bgzf is None and path and path.endswith(\".gz\")):\n            stream = bgzf.BgzfWriter(fileobj=stream)\n        return Writer(stream, header, path)", "response": "Create a new instance of the given class from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_path(klass, path, header):\n        path = str(path)\n        use_bgzf = False  # we already interpret path\n        if path.endswith(\".gz\"):\n            f = bgzf.BgzfWriter(filename=path)\n        else:\n            f = open(path, \"wt\")\n        return klass.from_stream(f, header, path, use_bgzf=use_bgzf)", "response": "Create a new : py : class : Writer from a path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_header(self):\n        for line in self.header.lines:\n            print(line.serialize(), file=self.stream)\n        if self.header.samples.names:\n            print(\n                \"\\t\".join(list(parser.REQUIRE_SAMPLE_HEADER) + self.header.samples.names),\n                file=self.stream,\n            )\n        else:\n            print(\"\\t\".join(parser.REQUIRE_NO_SAMPLE_HEADER), file=self.stream)", "response": "Write out the header"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _serialize_record(self, record):\n        f = self._empty_to_dot\n        row = [record.CHROM, record.POS]\n        row.append(f(\";\".join(record.ID)))\n        row.append(f(record.REF))\n        if not record.ALT:\n            row.append(\".\")\n        else:\n            row.append(\",\".join([f(a.serialize()) for a in record.ALT]))\n        row.append(f(record.QUAL))\n        row.append(f(\";\".join(record.FILTER)))\n        row.append(f(self._serialize_info(record)))\n        if record.FORMAT:\n            row.append(\":\".join(record.FORMAT))\n        row += [\n            self._serialize_call(record.FORMAT, record.call_for_sample[s])\n            for s in self.header.samples.names\n        ]\n        print(*row, sep=\"\\t\", file=self.stream)", "response": "Serialize a single record into a table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns serialized version of record. INFO", "response": "def _serialize_info(self, record):\n        \"\"\"Return serialized version of record.INFO\"\"\"\n        result = []\n        for key, value in record.INFO.items():\n            info = self.header.get_info_field_info(key)\n            if info.type == \"Flag\":\n                result.append(key)\n            else:\n                result.append(\"{}={}\".format(key, format_value(info, value, \"INFO\")))\n        return \";\".join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _serialize_call(self, format_, call):\n        if isinstance(call, record.UnparsedCall):\n            return call.unparsed_data\n        else:\n            result = [\n                format_value(self.header.get_format_field_info(key), call.data.get(key), \"FORMAT\")\n                for key in format_\n            ]\n            return \":\".join(result)", "response": "Return serialized version of the Call using the record s FORMAT"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_extended_jinja_tags(self, nodes):\n\n        jinja_a = None\n        jinja_b = None\n        ext_node = None\n        ext_nodes = []\n\n        for node in nodes:\n\n            if isinstance(node, EmptyLine):\n                continue\n\n\n            if node.has_children():\n                node.children = self._create_extended_jinja_tags(node.children)\n\n            if not isinstance(node, JinjaTag):\n                jinja_a = None\n                continue\n\n            if jinja_a is None or (\n                node.tag_name in self._extended_tags and jinja_a.tag_name not in self._extended_tags[node.tag_name]):\n                jinja_a = node\n                continue\n\n\n            if node.tag_name in self._extended_tags and \\\n                jinja_a.tag_name in self._extended_tags[node.tag_name]:\n\n                if ext_node is None:\n                    ext_node = ExtendedJinjaTag()\n                    ext_node.add(jinja_a)\n                    ext_nodes.append(ext_node)\n                ext_node.add(node)\n\n            else:\n                ext_node = None\n                jinja_a = node\n\n        #replace the nodes with the new extended node\n        for node in ext_nodes:\n            nodes.insert(nodes.index(node.children[0]), node)\n\n            index = nodes.index(node.children[0])\n            del nodes[index:index+len(node.children)]\n\n        return nodes", "response": "Loops through the nodes and creates the extended jinja tags that are not part of the extended jinja tags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn False if children is empty or contains only empty lines else True.", "response": "def has_children(self):\n        \"returns False if children is empty or contains only empty lines else True.\"\n        return bool([x for x in self.children if not isinstance(x, EmptyLine)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the requirements. txt at path.", "response": "def parse_requirements(path):\n    \"\"\"Parse ``requirements.txt`` at ``path``.\"\"\"\n    requirements = []\n    with open(path, \"rt\") as reqs_f:\n        for line in reqs_f:\n            line = line.strip()\n            if line.startswith(\"-r\"):\n                fname = line.split()[1]\n                inner_path = os.path.join(os.path.dirname(path), fname)\n                requirements += parse_requirements(inner_path)\n            elif line != \"\" and not line.startswith(\"#\"):\n                requirements.append(line)\n    return requirements"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\narranging class init params from conf file. Returns a dict with values.", "response": "def __parse_args(cam_c):\n    \"\"\"Arrange class init params from conf file.\n    Returns: a dict with values.\"\"\"\n    user = None\n    pswd = None\n    name = None\n    address = \"{address}:{port}\".format(**cam_c)\n    if \"user\" in cam_c:\n        user = cam_c[\"user\"]\n        if \"pswd\" in cam_c:\n            pswd = cam_c[\"pswd\"]\n    if \"name\" in cam_c:\n        name = cam_c[\"name\"]\n    return {\"user\": user, \"pswd\": pswd, \"name\": name, \"address\": address}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread cams conf from file and intantiate appropiate classes. Returns an array of IpCam classes.", "response": "def load_cams(conf_file):\n    \"\"\"Reads cams conf from file and intantiate appropiate classes.\n    Returns: an array of IpCam classes.\"\"\"\n    with open(conf_file, \"r\") as c_file:\n        lines = c_file.readlines()\n    cams_conf = [json.loads(j) for j in lines]\n    cams = []\n    for cam_c in cams_conf:\n        init_params = __parse_args(cam_c)\n        cams.append(cam_types[cam_c[\"type\"]](**init_params))\n    return cams"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch(cams, path=None, delay=10):\n    while True:\n        for c in cams:\n            c.snap(path)\n        time.sleep(delay)", "response": "Get screenshots from all cams at defined intervall."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscores the current state of a specific phone number.", "response": "def score(self, phone_number, account_lifecycle_event, **params):\n        \"\"\"\n        Score is an API that delivers reputation scoring based on phone number intelligence, traffic patterns, machine\n        learning, and a global data consortium.\n\n        See https://developer.telesign.com/docs/score-api for detailed API documentation.\n        \"\"\"\n        return self.post(SCORE_RESOURCE.format(phone_number=phone_number),\n                         account_lifecycle_event=account_lifecycle_event,\n                         **params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_version(filename='fuzzyhashlib/version.py'):\n    with open(filename) as source:\n        text = source.read()\n        match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", text)\n        if not match:\n            msg = \"Unable to find version number in {}\".format(filename)\n            raise RuntimeError(msg)\n        version = match.group(1)\n        return version", "response": "Parse a __version__ number from a source file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_telesign_headers(customer_id,\n                                  api_key,\n                                  method_name,\n                                  resource,\n                                  url_encoded_fields,\n                                  date_rfc2616=None,\n                                  nonce=None,\n                                  user_agent=None,\n                                  content_type=None):\n        \"\"\"\n        Generates the TeleSign REST API headers used to authenticate requests.\n\n        Creates the canonicalized string_to_sign and generates the HMAC signature. This is used to authenticate requests\n        against the TeleSign REST API.\n\n        See https://developer.telesign.com/docs/authentication for detailed API documentation.\n\n        :param customer_id: Your account customer_id.\n        :param api_key: Your account api_key.\n        :param method_name: The HTTP method name of the request as a upper case string, should be one of 'POST', 'GET',\n            'PUT' or 'DELETE'.\n        :param resource: The partial resource URI to perform the request against, as a string.\n        :param url_encoded_fields: HTTP body parameters to perform the HTTP request with, must be a urlencoded string.\n        :param date_rfc2616: The date and time of the request formatted in rfc 2616, as a string.\n        :param nonce: A unique cryptographic nonce for the request, as a string.\n        :param user_agent: (optional) User Agent associated with the request, as a string.\n        :param content_type: (optional) ContentType of the request, as a string.\n        :return: The TeleSign authentication headers.\n        \"\"\"\n        if date_rfc2616 is None:\n            date_rfc2616 = formatdate(usegmt=True)\n\n        if nonce is None:\n            nonce = str(uuid.uuid4())\n        \n        if not content_type:\n            content_type = \"application/x-www-form-urlencoded\" if method_name in (\"POST\", \"PUT\") else \"\"\n\n        auth_method = \"HMAC-SHA256\"\n\n        string_to_sign_builder = [\"{method}\".format(method=method_name)]\n\n        string_to_sign_builder.append(\"\\n{content_type}\".format(content_type=content_type))\n\n        string_to_sign_builder.append(\"\\n{date}\".format(date=date_rfc2616))\n\n        string_to_sign_builder.append(\"\\nx-ts-auth-method:{auth_method}\".format(auth_method=auth_method))\n\n        string_to_sign_builder.append(\"\\nx-ts-nonce:{nonce}\".format(nonce=nonce))\n\n        if content_type and url_encoded_fields:\n            string_to_sign_builder.append(\"\\n{fields}\".format(fields=url_encoded_fields))\n\n        string_to_sign_builder.append(\"\\n{resource}\".format(resource=resource))\n\n        string_to_sign = \"\".join(string_to_sign_builder)\n\n        signer = hmac.new(b64decode(api_key), string_to_sign.encode(\"utf-8\"), sha256)\n        signature = b64encode(signer.digest()).decode(\"utf-8\")\n\n        authorization = \"TSA {customer_id}:{signature}\".format(\n            customer_id=customer_id,\n            signature=signature)\n\n        headers = {\n            \"Authorization\": authorization,\n            \"Date\": date_rfc2616,\n            \"Content-Type\": content_type,\n            \"x-ts-auth-method\": auth_method,\n            \"x-ts-nonce\": nonce\n        }\n\n        if user_agent:\n            headers['User-Agent'] = user_agent\n\n        return headers", "response": "Generates the TeleSign authentication headers for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(self, resource, **params):\n        return self._execute(self.session.post, 'POST', resource, **params)", "response": "Generic TeleSign REST API POST handler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _execute(self, method_function, method_name, resource, **params):\n        resource_uri = \"{api_host}{resource}\".format(api_host=self.api_host, resource=resource)\n\n        url_encoded_fields = self._encode_params(params)\n\n        headers = RestClient.generate_telesign_headers(self.customer_id,\n                                                       self.api_key,\n                                                       method_name,\n                                                       resource,\n                                                       url_encoded_fields,\n                                                       user_agent=self.user_agent)\n\n        if method_name in ['POST', 'PUT']:\n            payload = {'data': url_encoded_fields}\n        else:\n            payload = {'params': url_encoded_fields}\n\n        response = self.Response(method_function(resource_uri,\n                                                 headers=headers,\n                                                 timeout=self.timeout,\n                                                 **payload))\n\n        return response", "response": "Generic TeleSign REST API request handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef message(self, phone_number, message, message_type, **params):\n        return self.post(MESSAGING_RESOURCE,\n                         phone_number=phone_number,\n                         message=message,\n                         message_type=message_type,\n                         **params)", "response": "Send a message to the target phone_number."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the current status of a message.", "response": "def status(self, reference_id, **params):\n        \"\"\"\n        Retrieves the current status of the message.\n\n        See https://developer.telesign.com/docs/messaging-api for detailed API documentation.\n        \"\"\"\n        return self.get(MESSAGING_STATUS_RESOURCE.format(reference_id=reference_id),\n                        **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hexdigest(self):\n        if self._pre_computed_hash is None:\n            return libssdeep_wrapper.fuzzy_digest(self._state, 0)\n        else:\n            return self._pre_computed_hash", "response": "Return the digest value as a string of hexadecimal digits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, buf):\n        if self._updatable:\n            return libssdeep_wrapper.fuzzy_update(self._state, buf)\n        else:\n            raise InvalidOperation(\"Cannot update sdeep created from hash\")", "response": "Update this hash object s state with the provided string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self):\n        if self._pre_computed_hash is None:\n            temp = ssdeep(buf=\"\")\n        else:\n            temp = ssdeep(hash=hash)\n        libssdeep_wrapper.fuzzy_free(temp._state)\n        temp._state = libssdeep_wrapper.fuzzy_clone(self._state)\n        temp._updatable = self._updatable\n        temp._pre_computed_hash = self._pre_computed_hash\n        return temp", "response": "Returns a new instance which identical to this instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hexdigest(self):\n        if not self._final:\n            if self._buf_len >= self._MIN_LEN:\n                self._tlsh.final()\n                self._final = True\n            else:\n                raise ValueError(\"tlsh requires buffer with length >= %d \"\n                                 \"for mode where force = %s\" % \\\n                                 (self._MIN_LEN, False))\n        return self._tlsh.hexdigest()", "response": "Return the digest value as a string of hexadecimal digits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, buf):\n        if self._final:\n            raise InvalidOperation(\"Cannot update finalised tlsh\")\n        else:\n\t    self._buf_len += len(buf)\n            return self._tlsh.update(buf)", "response": "Update this hash object s state with the provided string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call(self, phone_number, message, message_type, **params):\n        return self.post(VOICE_RESOURCE,\n                         phone_number=phone_number,\n                         message=message,\n                         message_type=message_type,\n                         **params)", "response": "Send a voice call to the target phone_number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the current status of the voice call.", "response": "def status(self, reference_id, **params):\n        \"\"\"\n        Retrieves the current status of the voice call.\n\n        See https://developer.telesign.com/docs/voice-api for detailed API documentation.\n        \"\"\"\n        return self.get(VOICE_STATUS_RESOURCE.format(reference_id=reference_id),\n                        **params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the status of an App Verify transaction by external_id.", "response": "def status(self, external_id, **params):\n        \"\"\"\n        Retrieves the verification result for an App Verify transaction by external_id. To ensure a secure verification\n        flow you must check the status using TeleSign's servers on your backend. Do not rely on the SDK alone to\n        indicate a successful verification.\n\n        See https://developer.telesign.com/docs/app-verify-android-sdk-self#section-get-status-service or\n        https://developer.telesign.com/docs/app-verify-ios-sdk-self#section-get-status-service for detailed\n        API documentation.\n        \"\"\"\n        return self.get(APPVERIFY_STATUS_RESOURCE.format(external_id=external_id),\n                        **params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Asset Location. Remove leading slash e.g '/static/images.jpg' ==> static/images.jpg Also, if the url is also prefixed with static, it would be removed. e.g static/image.jpg ==> image.jpg", "response": "def get_asset_location(element, attr):\n    \"\"\"\n    Get Asset Location.\n\n    Remove leading slash e.g '/static/images.jpg' ==> static/images.jpg\n    Also, if the url is also prefixed with static, it would be removed.\n        e.g static/image.jpg ==> image.jpg\n    \"\"\"\n    asset_location = re.match(r'^/?(static)?/?(.*)', element[attr],\n                              re.IGNORECASE)\n\n    # replace relative links i.e (../../static)\n    asset_location = asset_location.group(2).replace('../', '')\n\n    return asset_location"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_elements(html_file, tags):\n    with open(html_file) as f:\n        document = BeautifulSoup(f, 'html.parser')\n\n        def condition(tag, attr):\n            # Don't include external links\n            return lambda x: x.name == tag \\\n                and not x.get(attr, 'http').startswith(('http', '//'))\n\n        all_tags = [(attr, document.find_all(condition(tag, attr)))\n                    for tag, attr in tags]\n\n        return all_tags", "response": "Extract all the elements we re interested in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace lines in the old file with the transformed lines.", "response": "def replace_lines(html_file, transformed):\n    \"\"\"Replace lines in the old file with the transformed lines.\"\"\"\n    result = []\n    with codecs.open(html_file, 'r', 'utf-8') as input_file:\n        for line in input_file:\n            # replace all single quotes with double quotes\n            line = re.sub(r'\\'', '\"', line)\n\n            for attr, value, new_link in transformed:\n                if attr in line and value in line:\n\n                    # replace old link with new staticfied link\n                    new_line = line.replace(value, new_link)\n\n                    result.append(new_line)\n                    break\n            else:\n                result.append(line)\n\n        return ''.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite to stdout or a file", "response": "def file_ops(staticfied, args):\n    \"\"\"Write to stdout or a file\"\"\"\n    destination = args.o or args.output\n\n    if destination:\n        with open(destination, 'w') as file:\n            file.write(staticfied)\n    else:\n        print(staticfied)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse command line arguments.", "response": "def parse_cmd_arguments():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('file', type=str,\n                        help='Filename to be staticfied')\n    parser.add_argument('--static-endpoint',\n                        help='Static endpoint which is \"static\" by default')\n    parser.add_argument('--add-tags', type=str,\n                        help='Additional tags to staticfy')\n    parser.add_argument('--exc-tags', type=str, help='tags to exclude')\n    parser.add_argument('--framework', type=str,\n                        help='Web Framework: Defaults to Flask')\n    parser.add_argument('--namespace', type=str,\n                        help='String to prefix url with')\n    parser.add_argument('-o', type=str, help='Specify output file')\n    parser.add_argument('--output', type=str, help='Specify output file')\n    args = parser.parse_args()\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_keys(args):\n    key = args['--key']\n    if key:\n        return [key]\n\n    keyfile = args['--apikeys']\n    if keyfile:\n        return read_keyfile(keyfile)\n\n    envkey = os.environ.get('TINYPNG_API_KEY', None)\n    if envkey:\n        return [envkey]\n\n    local_keys = join(abspath(\".\"), \"tinypng.keys\")\n\n    if isfile(local_keys):\n        return read_keyfile(local_keys)\n\n    home_keys = join(expanduser(\"~/.tinypng.keys\"))\n    if isfile(home_keys):\n        return read_keyfile(home_keys)\n\n    return []", "response": "Get keys specified in arguments\n    returns list of keys or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshrink binary data of a png returns api_info", "response": "def get_shrink_data_info(in_data, api_key=None):\n    \"\"\"Shrink binary data of a png\n\n    returns api_info\n    \"\"\"\n    if api_key:\n        return _shrink_info(in_data, api_key)\n\n    api_keys = find_keys()\n    for key in api_keys:\n        try:\n            return _shrink_info(in_data, key)\n        except ValueError:\n            pass\n\n    raise ValueError('No valid api key found')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_shrunk_data(shrink_info):\n    out_url = shrink_info['output']['url']\n    try:\n        return requests.get(out_url).content\n    except HTTPError as err:\n        if err.code != 404:\n            raise\n\n        exc = ValueError(\"Unable to read png file \\\"{0}\\\"\".format(out_url))\n        exc.__cause__ = err\n        raise exc", "response": "Read shrunk file from tinypng. org api."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nshrinking binary data of a png returns api_info shrunk_data", "response": "def shrink_data(in_data, api_key=None):\n    \"\"\"Shrink binary data of a png\n\n    returns (api_info, shrunk_data)\n    \"\"\"\n    info = get_shrink_data_info(in_data, api_key)\n    return info, get_shrunk_data(info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshrink png file and write it back to a new file", "response": "def shrink_file(in_filepath, api_key=None, out_filepath=None):\n    \"\"\"Shrink png file and write it back to a new file\n\n    The default file path replaces \".png\" with \".tiny.png\".\n    returns api_info (including info['ouput']['filepath'])\n    \"\"\"\n    info = get_shrink_file_info(in_filepath, api_key, out_filepath)\n    write_shrunk_file(info)\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_telesign_callback_signature(api_key, signature, json_str):\n    your_signature = b64encode(HMAC(b64decode(api_key), json_str.encode(\"utf-8\"), sha256).digest()).decode(\"utf-8\")\n\n    if len(signature) != len(your_signature):\n        return False\n\n    # avoid timing attack with constant time equality check\n    signatures_equal = True\n    for x, y in zip(signature, your_signature):\n        if not x == y:\n            signatures_equal = False\n\n    return signatures_equal", "response": "Verify that a callback was made by TeleSign and was not sent by a malicious client by verifying the signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setposition(self, position):\n        try:\n            if isinstance(position, list):\n                self.send('position startpos moves {}'.format(\n                    self.__listtostring(position)))\n                self.isready()\n            elif re.match('\\s*^(((?:[rnbqkpRNBQKP1-8]+\\/){7})[rnbqkpRNBQKP1-8]+)\\s([b|w])\\s([K|Q|k|q|-]{1,4})\\s(-|[a-h][1-8])\\s(\\d+\\s\\d+)$', position):\n                regexList = re.match('\\s*^(((?:[rnbqkpRNBQKP1-8]+\\/){7})[rnbqkpRNBQKP1-8]+)\\s([b|w])\\s([K|Q|k|q|-]{1,4})\\s(-|[a-h][1-8])\\s(\\d+\\s\\d+)$', position).groups()\n                fen = regexList[0].split(\"/\")\n                if len(fen) != 8:\n                    raise ValueError(\"expected 8 rows in position part of fen: {0}\".format(repr(fen)))\n\n                for fenPart in fen:\n                    field_sum = 0\n                    previous_was_digit, previous_was_piece = False, False\n\n                    for c in fenPart:\n                        if c in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]:\n                            if previous_was_digit:\n                                raise ValueError(\"two subsequent digits in position part of fen: {0}\".format(repr(fen)))\n                            field_sum += int(c)\n                            previous_was_digit = True\n                            previous_was_piece = False\n                        elif c == \"~\":\n                            if not previous_was_piece:\n                                raise ValueError(\"~ not after piece in position part of fen: {0}\".format(repr(fen)))\n                            previous_was_digit, previous_was_piece = False, False\n                        elif c.lower() in [\"p\", \"n\", \"b\", \"r\", \"q\", \"k\"]:\n                            field_sum += 1\n                            previous_was_digit = False\n                            previous_was_piece = True\n                        else:\n                            raise ValueError(\"invalid character in position part of fen: {0}\".format(repr(fen)))\n\n                    if field_sum != 8:\n                        raise ValueError(\"expected 8 columns per row in position part of fen: {0}\".format(repr(fen)))  \n                self.send('position fen {}'.format(position))\n                self.isready()\n            else: raise ValueError(\"fen doesn`t match follow this example: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1 \")  \n\n        except ValueError as e:\n            print('\\nCheck position correctness\\n')\n            sys.exit(e.message)", "response": "Sets the position of the current object in long algebraic notation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_item(self, host, key, value, clock=None, state=0):\n        if clock is None:\n            clock = self.clock\n        if self._config.data_type == \"items\":\n            item = {\"host\": host, \"key\": key,\n                    \"value\": value, \"clock\": clock, \"state\": state}\n        elif self._config.data_type == \"lld\":\n            item = {\"host\": host, \"key\": key, \"clock\": clock, \"state\": state,\n                    \"value\": json.dumps({\"data\": value})}\n        else:\n            if self.logger: # pragma: no cover\n                self.logger.error(\"Setup data_type before adding data\")\n            raise ValueError('Setup data_type before adding data')\n        self._items_list.append(item)", "response": "Add a single item into the data container"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, data):\n        for host in data:\n            for key in data[host]:\n                if not data[host][key] == []:\n                    self.add_item(host, key, data[host][key])", "response": "Add a list of items into the container\nCOOKIE"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend items to a specific entrypoint in Zabbix server.", "response": "def send(self):\n        \"\"\"\n        Entrypoint to send data to Zabbix\n        If debug is enabled, items are sent one by one\n        If debug isn't enable, we send items in bulk\n        Returns a list of results (1 if no debug, as many as items in other case)\n        \"\"\"\n        if self.logger: # pragma: no cover\n            self.logger.info(\"Starting to send %d items\" % len(self._items_list))\n        try:\n            # Zabbix trapper send a maximum of 250 items in bulk\n            # We have to respect that, in case of enforcement on zabbix server side\n            # Special case if debug is enabled: we need to send items one by one\n            max_value = ZBX_TRAPPER_MAX_VALUE\n            if self.debug_level >= 4:\n                max_value = 1\n                if self.logger: # pragma: no cover\n                    self.logger.debug(\"Bulk limit is %d items\" % max_value)\n            else:\n                if self.logger: # pragma: no cover\n                    self.logger.info(\"Bulk limit is %d items\" % max_value)\n            # Initialize offsets & counters\n            max_offset = len(self._items_list)\n            run = 0\n            start_offset = 0\n            stop_offset = min(start_offset + max_value, max_offset)\n            server_success = server_failure = processed = failed = total = time = 0\n            while start_offset < stop_offset:\n                run += 1\n                if self.logger: # pragma: no cover\n                    self.logger.debug(\n                        'run %d: start_offset is %d, stop_offset is %d' %\n                        (run, start_offset, stop_offset)\n                    )\n\n                # Extract items to be send from global item's list'\n                _items_to_send = self.items_list[start_offset:stop_offset]\n\n                # Send extracted items\n                run_response, run_processed, run_failed, run_total, run_time = self._send_common(_items_to_send)\n\n                # Update counters\n                if run_response == 'success':\n                    server_success += 1\n                elif run_response == 'failed':\n                    server_failure += 1\n                processed += run_processed\n                failed += run_failed\n                total += run_total\n                time += run_time\n                if self.logger: # pragma: no cover\n                    self.logger.info(\"%d items sent during run %d\" % (run_total, run))\n                    self.logger.debug(\n                        'run %d: processed is %d, failed is %d, total is %d' %\n                        (run, run_processed, run_failed, run_total)\n                    )\n\n                # Compute next run's offsets\n                start_offset = stop_offset\n                stop_offset = min(start_offset + max_value, max_offset)\n\n                # Reset socket, which is likely to be closed by server\n                self._socket_reset()\n        except:\n            self._reset()\n            self._socket_reset()\n            raise\n        if self.logger: # pragma: no cover\n            self.logger.info('All %d items have been sent in %d runs' % (total, run))\n            self.logger.debug(\n                'Total run is %d; item processed: %d, failed: %d, total: %d, during %f seconds' %\n                (run, processed, failed, total, time)\n            )\n        # Everything has been sent.\n        # Reset DataContainer & return results_list\n        self._reset()\n        return server_success, server_failure, processed, failed, total, time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reset(self):\n        # Reset DataContainer to default values\n        # So that it can be reused\n        if self.logger: # pragma: no cover\n            self.logger.info(\"Reset DataContainer\")\n        self._items_list = []\n        self._config.data_type = None", "response": "Reset main data container properties to default values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the logger instance for the class.", "response": "def logger(self, value):\n        \"\"\"\n        Set logger instance for the class\n        \"\"\"\n        if isinstance(value, logging.Logger):\n            self._logger = value\n        else:\n            if self._logger: # pragma: no cover\n                self._logger.error(\"logger requires a logging instance\")\n            raise ValueError('logger requires a logging instance')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind(self, event_name, callback):\n\n        if event_name not in self.event_callbacks.keys():\n            self.event_callbacks[event_name] = []\n\n        self.event_callbacks[event_name].append(callback)", "response": "Bind an event to a callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a full representation of the issue for the given issue key.", "response": "def get_issue(self, issue_id, params=None):\n        \"\"\"Returns a full representation of the issue for the given issue key.\n\n        The issue JSON consists of the issue key and a collection of fields. Additional information like links to\n        workflow transition sub-resources, or HTML rendered values of the fields supporting HTML rendering can be\n        retrieved with expand request parameter specified.\n\n        The fields request parameter accepts a comma-separated list of fields to include in the response. It can be used\n        to retrieve a subset of fields. By default all fields are returned in the response. A particular field can be\n        excluded from the response if prefixed with a \"-\" (minus) sign. Parameter can be provided multiple times on a\n        single request.\n\n        By default, all fields are returned in the response. Note: this is different from a JQL search - only navigable\n        fields are returned by default (*navigable).\n\n\n        Args:\n            issue_id:\n            params:\n\n        Returns:\n\n        \"\"\"\n        return self._get(self.API_URL + 'issue/{}'.format(issue_id), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an issue or a sub - task from a JSON representation.", "response": "def create_issue(self, data, params=None):\n        \"\"\"Creates an issue or a sub-task from a JSON representation.\n\n        You can provide two parameters in request's body: update or fields. The fields, that can be set on an issue\n        create operation, can be determined using the /rest/api/2/issue/createmeta resource. If a particular field is\n        not configured to appear on the issue's Create screen, then it will not be returned in the createmeta response.\n        A field validation error will occur if such field is submitted in request.\n\n        Creating a sub-task is similar to creating an issue with the following differences:\n        issueType field must be set to a sub-task issue type (use /issue/createmeta to find sub-task issue types), and\n\n        You must provide a parent field with the ID or key of the parent issue.\n\n        Args:\n            data:\n            params:\n\n        Returns:\n\n        \"\"\"\n        return self._post(self.API_URL + 'issue', data=data, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an individual issue.", "response": "def delete_issue(self, issue_id, params=None):\n        \"\"\"Deletes an individual issue.\n\n        If the issue has sub-tasks you must set the deleteSubtasks=true parameter to delete the issue. You cannot delete\n        an issue without deleting its sub-tasks.\n\n        Args:\n            issue_id:\n            params:\n\n        Returns:\n\n        \"\"\"\n        return self._delete(self.API_URL + 'issue/{}'.format(issue_id), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subscribe(self, channel_name):\n        data = {'channel': channel_name}\n\n        if channel_name.startswith('presence-'):\n            data['auth'] = self._generate_presence_key(\n                self.connection.socket_id,\n                self.key,\n                channel_name,\n                self.secret,\n                self.user_data\n            )\n            data['channel_data'] = json.dumps(self.user_data)\n        elif channel_name.startswith('private-'):\n            data['auth'] = self._generate_private_key(\n                self.connection.socket_id,\n                self.key,\n                channel_name,\n                self.secret\n            )\n\n        self.connection.send_event('pusher:subscribe', data)\n\n        self.channels[channel_name] = Channel(channel_name, self.connection)\n\n        return self.channels[channel_name]", "response": "Subscribe to a channel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nanalyzing Zabbix server response and return a list of items with the correct size", "response": "def _handle_response(self, zbx_answer):\n        \"\"\"\n        Analyze Zabbix Server response\n        Returns a list with number of:\n        * processed items\n        * failed items\n        * total items\n        * time spent\n\n        :zbx_answer: Zabbix server response as string\n        \"\"\"\n        zbx_answer = json.loads(zbx_answer)\n        if self._logger: # pragma: no cover\n            self._logger.info(\n                \"Anaylizing Zabbix Server's answer\"\n            )\n            if zbx_answer:\n                self._logger.debug(\"Zabbix Server response is: [%s]\" % zbx_answer)\n\n        # Default items number in length of th storage list\n        nb_item = len(self._items_list)\n        if self._config.debug_level >= 4:\n            # If debug enabled, force it to 1\n            nb_item = 1\n\n        # If dryrun is disabled, we can process answer\n        response = zbx_answer.get('response')\n        result = re.findall(ZBX_RESP_REGEX, zbx_answer.get('info'))\n        processed, failed, total, time = result[0]\n\n        return response, int(processed), int(failed), int(total), float(time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, pagination=True, page_size=None, page=None, **queryparams):\n        if page_size and pagination:\n            try:\n                page_size = int(page_size)\n            except (ValueError, TypeError):\n                page_size = 100\n            queryparams['page_size'] = page_size\n        result = self.requester.get(\n            self.instance.endpoint, query=queryparams, paginate=pagination\n        )\n        objects = SearchableList()\n        objects.extend(self.parse_list(result.json()))\n        if result.headers.get('X-Pagination-Next', False) and not page:\n            next_page = 2\n        else:\n            next_page = None\n        while next_page:\n            pageparams = queryparams.copy()\n            pageparams['page'] = next_page\n            result = self.requester.get(\n                self.instance.endpoint, query=pageparams,\n            )\n            objects.extend(self.parse_list(result.json()))\n            if result.headers.get('X-Pagination-Next', False):\n                next_page += 1\n            else:\n                next_page = None\n        return objects", "response": "Retrieves a list of objects from the remote API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a JSON array into a list of model instances.", "response": "def parse(cls, requester, entries):\n        \"\"\"Parse a JSON array into a list of model instances.\"\"\"\n        result_entries = SearchableList()\n        for entry in entries:\n            result_entries.append(cls.instance.parse(requester, entry))\n        return result_entries"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a JSON array into a list of model instances.", "response": "def parse_list(self, entries):\n        \"\"\"Parse a JSON array into a list of model instances.\"\"\"\n        result_entries = SearchableList()\n        for entry in entries:\n            result_entries.append(self.instance.parse(self.requester, entry))\n        return result_entries"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the current instance resource with the given attributes.", "response": "def update(self, **args):\n        \"\"\"\n        Update the current :class:`InstanceResource`\n        \"\"\"\n        self_dict = self.to_dict()\n        if args:\n            self_dict = dict(list(self_dict.items()) + list(args.items()))\n        response = self.requester.put(\n            '/{endpoint}/{id}', endpoint=self.endpoint,\n            id=self.id, payload=self_dict\n        )\n        obj_json = response.json()\n        if 'version' in obj_json:\n            self.__dict__['version'] = obj_json['version']\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch(self, fields, **args):\n        self_dict = dict([(key, value) for (key, value) in\n                          self.to_dict().items()\n                          if key in fields])\n        if args:\n            self_dict = dict(list(self_dict.items()) + list(args.items()))\n        response = self.requester.patch(\n            '/{endpoint}/{id}', endpoint=self.endpoint,\n            id=self.id, payload=self_dict\n        )\n        obj_json = response.json()\n        if 'version' in obj_json:\n            self.__dict__['version'] = obj_json['version']\n        return self", "response": "Patch the current instance resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self):\n        self.requester.delete(\n            '/{endpoint}/{id}', endpoint=self.endpoint,\n            id=self.id\n        )\n        return self", "response": "Delete the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a dictionary representation of the instance resource.", "response": "def to_dict(self):\n        \"\"\"\n        Get a dictionary representation of :class:`InstanceResource`\n        \"\"\"\n        self_dict = {}\n        for key, value in six.iteritems(self.__dict__):\n            if self.allowed_params and key in self.allowed_params:\n                self_dict[key] = value\n        return self_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(cls, requester, entry):\n        if not type(entry) is dict:\n            return entry\n        for key_to_parse, cls_to_parse in six.iteritems(cls.parser):\n            if key_to_parse in entry:\n                entry[key_to_parse] = cls_to_parse.parse(\n                    requester, entry[key_to_parse]\n                )\n        return cls(requester, **entry)", "response": "Parses a JSON object into a model instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_attribute(self, id, value, version=1):\n        attributes = self._get_attributes(cache=True)\n        formatted_id = '{0}'.format(id)\n        attributes['attributes_values'][formatted_id] = value\n        response = self.requester.patch(\n            '/{endpoint}/custom-attributes-values/{id}',\n            endpoint=self.endpoint, id=self.id,\n            payload={\n                'attributes_values': attributes['attributes_values'],\n                'version': version\n            }\n        )\n        cache_key = self.requester.get_full_url(\n            '/{endpoint}/custom-attributes-values/{id}',\n            endpoint=self.endpoint, id=self.id\n        )\n        self.requester.cache.put(cache_key, response)\n        return response.json()", "response": "Set the value of a specific attribute in the resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, project, name, **attrs):\n        attrs.update(\n            {\n                'project': project, 'name': name\n            }\n        )\n        return self._new_resource(payload=attrs)", "response": "Create a new custom attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of starred projects.", "response": "def starred_projects(self):\n        \"\"\"\n        Get a list of starred :class:`Project`.\n        \"\"\"\n        response = self.requester.get(\n            '/{endpoint}/{id}/starred', endpoint=self.endpoint,\n            id=self.id\n        )\n        return Projects.parse(self.requester, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new : class:`Membership.", "response": "def create(self, project, email, role, **attrs):\n        \"\"\"\n        Create a new :class:`Membership`.\n\n        :param project: :class:`Project` id\n        :param email: email of the :class:`Membership`\n        :param role: role of the :class:`Membership`\n        :param attrs: optional attributes of the :class:`Membership`\n        \"\"\"\n        attrs.update({'project': project, 'email': email, 'role': role})\n        return self._new_resource(payload=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new : class:`Attachment.", "response": "def create(self, project, object_id, attached_file, **attrs):\n        \"\"\"\n        Create a new :class:`Attachment`.\n\n        :param project: :class:`Project` id\n        :param object_id: id of the current object\n        :param ref: :class:`Task` reference\n        :param attached_file: file path that you want to upload\n        :param attrs: optional attributes for the :class:`Attachment`\n        \"\"\"\n        attrs.update({'project': project, 'object_id': object_id})\n\n        if isinstance(attached_file, file):\n            attachment = attached_file\n        elif isinstance(attached_file, six.string_types):\n            try:\n                attachment = open(attached_file, 'rb')\n            except IOError:\n                raise exceptions.TaigaException(\n                    \"Attachment must be a IOBase or a path to an existing file\"\n                )\n        else:\n            raise exceptions.TaigaException(\n                \"Attachment must be a IOBase or a path to an existing file\"\n            )\n\n        return self._new_resource(\n            files={'attached_file': attachment},\n            payload=attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_task(self, subject, status, **attrs):\n        return Tasks(self.requester).create(\n            self.project, subject, status,\n            user_story=self.id, **attrs\n        )", "response": "Add a Task to the current UserStory and return it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, project, subject, **attrs):\n        attrs.update({'project': project, 'subject': subject})\n        return self._new_resource(payload=attrs)", "response": "Create a new user story."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stats(self):\n        response = self.requester.get(\n            '/{endpoint}/{id}/stats',\n            endpoint=self.endpoint, id=self.id\n        )\n        return response.json()", "response": "Get the stats for the current : class : Milestone"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new entry in the cluster.", "response": "def create(self, project, name, estimated_start,\n               estimated_finish, **attrs):\n        \"\"\"\n        Create a new :class:`Milestone`.\n\n        :param project: :class:`Project` id\n        :param name: name of the :class:`Milestone`\n        :param estimated_start: est. start time of the :class:`Milestone`\n        :param estimated_finish: est. finish time of the :class:`Milestone`\n        :param attrs: optional attributes of the :class:`Milestone`\n        \"\"\"\n        if isinstance(estimated_start, datetime.datetime):\n            estimated_start = estimated_start.strftime('%Y-%m-%d')\n        if isinstance(estimated_finish, datetime.datetime):\n            estimated_finish = estimated_finish.strftime('%Y-%m-%d')\n        attrs.update({\n            'project': project,\n            'name': name,\n            'estimated_start': estimated_start,\n            'estimated_finish': estimated_finish\n        })\n        return self._new_resource(payload=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach(self, attached_file, **attrs):\n        return TaskAttachments(self.requester).create(\n            self.project, self.id,\n            attached_file, **attrs\n        )", "response": "Attach a file to the Task."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new task.", "response": "def create(self, project, subject, priority, status,\n               issue_type, severity, **attrs):\n        \"\"\"\n        Create a new :class:`Task`.\n\n        :param project: :class:`Project` id\n        :param subject: subject of the :class:`Issue`\n        :param priority: priority of the :class:`Issue`\n        :param status: status of the :class:`Issue`\n        :param issue_type: Issue type of the :class:`Issue`\n        :param severity: severity of the :class:`Issue`\n        :param attrs: optional attributes of the :class:`Task`\n        \"\"\"\n        attrs.update(\n            {\n                'project': project, 'subject': subject,\n                'priority': priority, 'status': status,\n                'type': issue_type, 'severity': severity\n            }\n        )\n        return self._new_resource(payload=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a : class : Task by ref.", "response": "def get_task_by_ref(self, ref):\n        \"\"\"\n        Get a :class:`Task` by ref.\n\n        :param ref: :class:`Task` reference\n        \"\"\"\n        response = self.requester.get(\n            '/{endpoint}/by_ref?ref={task_ref}&project={project_id}',\n            endpoint=Task.endpoint,\n            task_ref=ref,\n            project_id=self.id\n        )\n        return Task.parse(self.requester, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_userstory_by_ref(self, ref):\n        response = self.requester.get(\n            '/{endpoint}/by_ref?ref={us_ref}&project={project_id}',\n            endpoint=UserStory.endpoint,\n            us_ref=ref,\n            project_id=self.id\n        )\n        return UserStory.parse(self.requester, response.json())", "response": "Get a UserStory by ref."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a : class : Issue object by ref.", "response": "def get_issue_by_ref(self, ref):\n        \"\"\"\n        Get a :class:`Issue` by ref.\n\n        :param ref: :class:`Issue` reference\n        \"\"\"\n        response = self.requester.get(\n            '/{endpoint}/by_ref?ref={us_ref}&project={project_id}',\n            endpoint=Issue.endpoint,\n            us_ref=ref,\n            project_id=self.id\n        )\n        return Issue.parse(self.requester, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting stats for issues of the project", "response": "def issues_stats(self):\n        \"\"\"\n        Get stats for issues of the project\n        \"\"\"\n        response = self.requester.get(\n            '/{endpoint}/{id}/issues_stats',\n            endpoint=self.endpoint, id=self.id\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unlike(self):\n        self.requester.post(\n            '/{endpoint}/{id}/unlike',\n            endpoint=self.endpoint, id=self.id\n        )\n        return self", "response": "Unlike the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef star(self):\n        warnings.warn(\n            \"Deprecated! Update Taiga and use .like() instead\",\n            DeprecationWarning\n        )\n        self.requester.post(\n            '/{endpoint}/{id}/star',\n            endpoint=self.endpoint, id=self.id\n        )\n        return self", "response": "This method star the project."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_membership(self, email, role, **attrs):\n        return Memberships(self.requester).create(\n            self.id, email, role, **attrs\n        )", "response": "Adds a Membership to the project and returns a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_user_story(self, subject, **attrs):\n        return UserStories(self.requester).create(\n            self.id, subject, **attrs\n        )", "response": "Adds a UserStory to the user s list of attributes and returns a new UserStory resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports a user story and returns a UserStory resource.", "response": "def import_user_story(self, subject, status, **attrs):\n        \"\"\"\n        Import an user story and returns a :class:`UserStory` resource.\n\n        :param subject: subject of the :class:`UserStory`\n        :param status: status of the :class:`UserStory`\n        :param attrs: optional :class:`UserStory` attributes\n        \"\"\"\n        return UserStories(self.requester).import_(\n            self.id, subject, status, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a Issue and returns a resource.", "response": "def add_issue(self, subject, priority, status,\n                  issue_type, severity, **attrs):\n        \"\"\"\n        Adds a Issue and returns a :class:`Issue` resource.\n\n        :param subject: subject of the :class:`Issue`\n        :param priority: priority of the :class:`Issue`\n        :param priority: status of the :class:`Issue`\n        :param issue_type: type of the :class:`Issue`\n        :param severity: severity of the :class:`Issue`\n        :param attrs: other :class:`Issue` attributes\n        \"\"\"\n        return Issues(self.requester).create(\n            self.id, subject, priority, status,\n            issue_type, severity, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_issue(self, subject, priority, status,\n                     issue_type, severity, **attrs):\n        \"\"\"\n        Import and issue and returns a :class:`Issue` resource.\n\n        :param subject: subject of :class:`Issue`\n        :param priority: priority of :class:`Issue`\n        :param status: status of :class:`Issue`\n        :param issue_type: issue type of :class:`Issue`\n        :param severity: severity of :class:`Issue`\n        :param attrs: optional :class:`Issue` attributes\n        \"\"\"\n        return Issues(self.requester).import_(\n            self.id, subject, priority, status,\n            issue_type, severity, **attrs\n        )", "response": "Imports and issue and returns a resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_milestone(self, name, estimated_start, estimated_finish, **attrs):\n        return Milestones(self.requester).create(\n            self.id, name, estimated_start,\n            estimated_finish, **attrs\n        )", "response": "Adds a Milestone to the project and returns a Milestone object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_milestone(self, name, estimated_start, estimated_finish,\n                         **attrs):\n        \"\"\"\n        Import a Milestone and returns a :class:`Milestone` object.\n\n        :param name: name of the :class:`Milestone`\n        :param estimated_start: estimated start time of the\n                                :class:`Milestone`\n        :param estimated_finish: estimated finish time of the\n                                 :class:`Milestone`\n        :param attrs: optional attributes for :class:`Milestone`\n        \"\"\"\n        return Milestones(self.requester).import_(\n            self.id, name, estimated_start,\n            estimated_finish, **attrs\n        )", "response": "Imports a Milestone and returns a Milestone object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of Milestones for the project.", "response": "def list_milestones(self, **queryparams):\n        \"\"\"\n        Get the list of :class:`Milestone` resources for the project.\n        \"\"\"\n        return Milestones(self.requester).list(project=self.id, **queryparams)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_point(self, name, value, **attrs):\n        return Points(self.requester).create(self.id, name, value, **attrs)", "response": "Adds a Point to the project and returns a : class:`Point` object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_task_status(self, name, **attrs):\n        return TaskStatuses(self.requester).create(self.id, name, **attrs)", "response": "Adds a TaskStatus to the project and returns a new TaskStatus object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting a Task and return a Task object.", "response": "def import_task(self, subject, status, **attrs):\n        \"\"\"\n        Import a Task and return a :class:`Task` object.\n\n        :param subject: subject of the :class:`Task`\n        :param status: status of the :class:`Task`\n        :param attrs: optional attributes for :class:`Task`\n        \"\"\"\n        return Tasks(self.requester).import_(\n            self.id, subject, status, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a UserStory status to the project and returns a new UserStoryStatus object.", "response": "def add_user_story_status(self, name, **attrs):\n        \"\"\"\n        Add a UserStory status to the project and returns a\n        :class:`UserStoryStatus` object.\n\n        :param name: name of the :class:`UserStoryStatus`\n        :param attrs: optional attributes for :class:`UserStoryStatus`\n        \"\"\"\n        return UserStoryStatuses(self.requester).create(self.id, name, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new issue type to the project and returns a new issue type object.", "response": "def add_issue_type(self, name, **attrs):\n        \"\"\"\n        Add a Issue type to the project and returns a\n        :class:`IssueType` object.\n\n        :param name: name of the :class:`IssueType`\n        :param attrs: optional attributes for :class:`IssueType`\n        \"\"\"\n        return IssueTypes(self.requester).create(self.id, name, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a Severity to the project and returns a : class:`Severity` object.", "response": "def add_severity(self, name, **attrs):\n        \"\"\"\n        Add a Severity to the project and returns a :class:`Severity` object.\n\n        :param name: name of the :class:`Severity`\n        :param attrs: optional attributes for :class:`Severity`\n        \"\"\"\n        return Severities(self.requester).create(self.id, name, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_role(self, name, **attrs):\n        return Roles(self.requester).create(self.id, name, **attrs)", "response": "Adds a role to the project and returns a new role object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a Priority to the project and returns a : class:`Priority` object.", "response": "def add_priority(self, name, **attrs):\n        \"\"\"\n        Add a Priority to the project and returns a :class:`Priority` object.\n\n        :param name: name of the :class:`Priority`\n        :param attrs: optional attributes for :class:`Priority`\n        \"\"\"\n        return Priorities(self.requester).create(self.id, name, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_issue_status(self, name, **attrs):\n        return IssueStatuses(self.requester).create(self.id, name, **attrs)", "response": "Adds a new issue status to the project and returns a new issue status object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_wikipage(self, slug, content, **attrs):\n        return WikiPages(self.requester).create(\n            self.id, slug, content, **attrs\n        )", "response": "Adds a Wiki page to the project and returns a new WikiPage object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports a Wiki page and returns a WikiPage object.", "response": "def import_wikipage(self, slug, content, **attrs):\n        \"\"\"\n        Import a Wiki page and return a :class:`WikiPage` object.\n\n        :param slug: slug of the :class:`WikiPage`\n        :param content: content of the :class:`WikiPage`\n        :param attrs: optional attributes for :class:`Task`\n        \"\"\"\n        return WikiPages(self.requester).import_(\n            self.id, slug, content, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a Wiki link to the project and returns a new WikiLink object.", "response": "def add_wikilink(self, title, href, **attrs):\n        \"\"\"\n        Add a Wiki link to the project and returns a :class:`WikiLink` object.\n\n        :param title: title of the :class:`WikiLink`\n        :param href: href of the :class:`WikiLink`\n        :param attrs: optional attributes for :class:`WikiLink`\n        \"\"\"\n        return WikiLinks(self.requester).create(self.id, title, href, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports a Wiki link and returns a WikiLink object.", "response": "def import_wikilink(self, title, href, **attrs):\n        \"\"\"\n        Import a Wiki link and return a :class:`WikiLink` object.\n\n        :param title: title of the :class:`WikiLink`\n        :param href: href of the :class:`WikiLink`\n        :param attrs: optional attributes for :class:`WikiLink`\n        \"\"\"\n        return WikiLinks(self.requester).import_(self.id, title, href, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_issue_attribute(self, name, **attrs):\n        return IssueAttributes(self.requester).create(\n            self.id, name, **attrs\n        )", "response": "Add a new Issue attribute and return a : class:`IssueAttribute object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new Task attribute and return a TaskAttribute object.", "response": "def add_task_attribute(self, name, **attrs):\n        \"\"\"\n        Add a new Task attribute and return a :class:`TaskAttribute` object.\n\n        :param name: name of the :class:`TaskAttribute`\n        :param attrs: optional attributes for :class:`TaskAttribute`\n        \"\"\"\n        return TaskAttributes(self.requester).create(\n            self.id, name, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_user_story_attribute(self, name, **attrs):\n        return UserStoryAttributes(self.requester).create(\n            self.id, name, **attrs\n        )", "response": "Adds a new User Story attribute and returns a new User Story Attribute object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new Webhook and return a new Webhook object.", "response": "def add_webhook(self, name, url, key, **attrs):\n        \"\"\"\n        Add a new Webhook and return a :class:`Webhook` object.\n\n        :param name: name of the :class:`Webhook`\n        :param url: payload url of the :class:`Webhook`\n        :param key: secret key of the :class:`Webhook`\n        :param attrs: optional attributes for :class:`Webhook`\n        \"\"\"\n        return Webhooks(self.requester).create(\n            self.id, name, url, key, **attrs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new resource in the cluster.", "response": "def create(self, name, description, **attrs):\n        \"\"\"\n        Create a new :class:`Project`\n\n        :param name: name of the :class:`Project`\n        :param description: description of the :class:`Project`\n        :param attrs: optional attributes for :class:`Project`\n        \"\"\"\n        attrs.update({'name': name, 'description': description})\n        return self._new_resource(payload=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a : class:`Project` by slug", "response": "def get_by_slug(self, slug):\n        \"\"\"\n        Get a :class:`Project` by slug\n\n        :param slug: the slug of the :class:`Project`\n        \"\"\"\n        response = self.requester.get(\n            '/{endpoint}/by_slug?slug={slug}',\n            endpoint=self.instance.endpoint,\n            slug=slug\n        )\n        return self.instance.parse(self.requester, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, project, slug, content, **attrs):\n        attrs.update({'project': project, 'slug': slug, 'content': content})\n        return self._new_resource(payload=attrs)", "response": "create a new wiki page"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, project, title, href, **attrs):\n        attrs.update({'project': project, 'title': title, 'href': href})\n        return self._new_resource(payload=attrs)", "response": "Create a new wiki link."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, resource_id):\n        response = self.requester.get(\n            '/{endpoint}/{entity}/{id}',\n            endpoint=self.endpoint, entity=self.entity, id=resource_id,\n            paginate=False\n        )\n        return response.json()", "response": "Get a history element from a resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a comment from a resource.", "response": "def delete_comment(self, resource_id, ent_id):\n        \"\"\"\n        Delete a comment\n\n        :param resource_id: ...\n        :param ent_id: ...\n        \"\"\"\n        self.requester.post(\n            '/{endpoint}/{entity}/{id}/delete_comment?id={ent_id}',\n            endpoint=self.endpoint, entity=self.entity,\n            id=resource_id, ent_id=ent_id\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a class and a dict and attempts to build an instance of the class Taxonomy class", "response": "def parse(cls, value):\n    \"\"\"Takes a class and a dict and try to build an instance of the class\n\n    :param cls: The class to parse\n    :param value: either a dict, a list or a scalar value\n    \"\"\"\n    if is_list_annotation(cls):\n        if not isinstance(value, list):\n            raise TypeError('Could not parse {} because value is not a list'.format(cls))\n        return [parse(cls.__args__[0], o) for o in value]\n    else:\n        return GenericParser(cls, ModelProviderImpl()).parse(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(obj, fp, **kwargs):\n    kwargs['default'] = serialize\n    return json.dump(obj, fp, **kwargs)", "response": "wrapper for json. dump"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(cls, fp, **kwargs):\n    json_obj = json.load(fp, **kwargs)\n    return parse(cls, json_obj)", "response": "wrapper for json. load"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loads(cls, s, **kwargs):\n    json_obj = json.loads(s, **kwargs)\n    return parse(cls, json_obj)", "response": "wrapper for json. loads"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert Ensembl Id to Entrez Gene Id", "response": "def convert_ensembl_to_entrez(self, ensembl):\n        \"\"\"Convert Ensembl Id to Entrez Gene Id\"\"\"\n        if 'ENST' in ensembl:\n            pass\n        else:\n            raise (IndexError)\n        # Submit resquest to NCBI eutils/Gene database\n        server = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?\" + self.options + \"&db=gene&term={0}\".format(\n            ensembl)\n        r = requests.get(server, headers={\"Content-Type\": \"text/xml\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        # Process Request\n        response = r.text\n        info = xmltodict.parse(response)\n        try:\n            geneId = info['eSearchResult']['IdList']['Id']\n        except TypeError:\n            raise (TypeError)\n        return geneId"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts Entrez Id to Uniprot Id", "response": "def convert_entrez_to_uniprot(self, entrez):\n        \"\"\"Convert Entrez Id to Uniprot Id\"\"\"\n        server = \"http://www.uniprot.org/uniprot/?query=%22GENEID+{0}%22&format=xml\".format(entrez)\n        r = requests.get(server, headers={\"Content-Type\": \"text/xml\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        response = r.text\n        info = xmltodict.parse(response)\n        try:\n            data = info['uniprot']['entry']['accession'][0]\n            return data\n        except TypeError:\n            data = info['uniprot']['entry'][0]['accession'][0]\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_uniprot_to_entrez(self, uniprot):\n        # Submit request to NCBI eutils/Gene Database\n        server = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?\" + self.options + \"&db=gene&term={0}\".format(\n            uniprot)\n        r = requests.get(server, headers={\"Content-Type\": \"text/xml\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        # Process Request\n        response = r.text\n        info = xmltodict.parse(response)\n        geneId = info['eSearchResult']['IdList']['Id']\n        # check to see if more than one result is returned\n        # if you have more than more result then check which Entrez Id returns the same uniprot Id entered.\n        if len(geneId) > 1:\n            for x in geneId:\n                c = self.convert_entrez_to_uniprot(x)\n                c = c.lower()\n                u = uniprot.lower()\n                if c == u:\n                    return x\n        else:\n            return geneId", "response": "Convert Uniprot Id to Entrez Id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_accession_to_taxid(self, accessionid):\n        # Submit request to NCBI eutils/Taxonomy Database\n        server = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?\" + self.options + \"&db=nuccore&id={0}&retmode=xml\".format(\n            accessionid)\n        r = requests.get(server, headers={\"Content-Type\": \"text/xml\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        # Process Request\n        response = r.text\n        records = xmltodict.parse(response)\n        try:\n            for i in records['GBSet']['GBSeq']['GBSeq_feature-table']['GBFeature']['GBFeature_quals']['GBQualifier']:\n                for key, value in i.items():\n                    if value == 'db_xref':\n                        taxid = i['GBQualifier_value']\n                        taxid = taxid.split(':')[1]\n                        return taxid\n        except:\n            for i in records['GBSet']['GBSeq']['GBSeq_feature-table']['GBFeature'][0]['GBFeature_quals']['GBQualifier']:\n                for key, value in i.items():\n                    if value == 'db_xref':\n                        taxid = i['GBQualifier_value']\n                        taxid = taxid.split(':')[1]\n                        return taxid\n        return", "response": "Convert an Accession Id to a Tax Id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a symbol to Entrez Gene Id", "response": "def convert_symbol_to_entrezid(self, symbol):\n        \"\"\"Convert Symbol to Entrez Gene Id\"\"\"\n        entrezdict = {}\n        server = \"http://rest.genenames.org/fetch/symbol/{0}\".format(symbol)\n        r = requests.get(server, headers={\"Content-Type\": \"application/json\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        response = r.text\n        info = xmltodict.parse(response)\n        for data in info['response']['result']['doc']['str']:\n            if data['@name'] == 'entrez_id':\n                entrezdict[data['@name']] = data['#text']\n            if data['@name'] == 'symbol':\n                entrezdict[data['@name']] = data['#text']\n        return entrezdict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_local_message(message_format, *args):\n    prefix = '{} {}'.format(color('INFO', fg=248), color('request', fg=5))\n    message = message_format % args\n\n    sys.stderr.write('{} {}\\n'.format(prefix, message))", "response": "Log a message to stderr with a local log format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a object and produces a dict - like representation of the object", "response": "def serialize(obj):\n    \"\"\"Takes a object and produces a dict-like representation\n\n    :param obj: the object to serialize\n    \"\"\"\n    if isinstance(obj, list):\n        return [serialize(o) for o in obj]\n    return GenericSerializer(ModelProviderImpl()).serialize(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef of(self, *indented_blocks) -> \"CodeBlock\":\n\n        if self.closed_by is None:\n            self.expects_body_or_pass = True\n\n        for block in indented_blocks:\n            if block is not None:\n                self._blocks.append((1, block))\n\n        # Finalise it so that we cannot add more sub-blocks to this block.\n        self.finalise()\n\n        return self", "response": "Adds indented blocks to the end of the code block."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds sub - blocks at the specified indentation level.", "response": "def add(self, *blocks, indentation=0) -> \"CodeBlock\":\n        \"\"\"\n        Adds sub-blocks at the specified indentation level, which defaults to 0.\n\n        Nones are skipped.\n\n        Returns the parent block itself, useful for chaining.\n        \"\"\"\n        for block in blocks:\n            if block is not None:\n                self._blocks.append((indentation, block))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_blocks(self):\n\n        self.process_triple_quoted_doc_string()\n\n        num_indented_blocks = 0\n        for indentation, block in self._blocks:\n            if indentation > 0:\n                num_indented_blocks += 1\n            yield indentation, block\n\n        if self.expects_body_or_pass and num_indented_blocks == 0:\n            yield 1, \"pass\"\n            return", "response": "Return a generator of all the blocks that are in the current context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_code(self, context: Context =None):\n        # Do not override this method!\n\n        context = context or Context()\n        for imp in self.imports:\n            if imp not in context.imports:\n                context.imports.append(imp)\n\n        counter = Counter()\n        lines = list(self.to_lines(context=context, counter=counter))\n\n        if counter.num_indented_non_doc_blocks == 0:\n            if self.expects_body_or_pass:\n                lines.append(\"    pass\")\n            elif self.closed_by:\n                lines[-1] += self.closed_by\n        else:\n            if self.closed_by:\n                lines.append(self.closed_by)\n\n        return join_lines(*lines) + self._suffix", "response": "Generate the code and return it as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exec(self, globals=None, locals=None):\n        if locals is None:\n            locals = {}\n        builtins.exec(self.to_code(), globals, locals)\n        return locals", "response": "Execute the code in a simple code block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a basic code block.", "response": "def block(self, *blocks, **kwargs) -> \"CodeBlock\":\n        \"\"\"\n        Build a basic code block.\n        Positional arguments should be instances of CodeBlock or strings.\n        All code blocks passed as positional arguments are added at indentation level 0.\n        None blocks are skipped.\n        \"\"\"\n        assert \"name\" not in kwargs\n        kwargs.setdefault(\"code\", self)\n        code = CodeBlock(**kwargs)\n        for block in blocks:\n            if block is not None:\n                code._blocks.append((0, block))\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_from_locals(self, name, params: List[Parameter], not_specified_literal=Constants.VALUE_NOT_SET):\n        code = self.block(f\"{name} = {{}}\")\n        for p in params:\n            code.add(\n                self.block(f\"if {p.name} is not {not_specified_literal}:\").of(\n                    f\"{name}[{p.name!r}] = {p.name}\"\n                ),\n            )\n        return code", "response": "Generate code for a dictionary of locals whose value is not the specified literal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting the autoboto package generated in the build directory.", "response": "def import_generated_autoboto(self):\n        \"\"\"\n        Imports the autoboto package generated in the build directory (not target_dir).\n\n        For example:\n            autoboto = botogen.import_generated_autoboto()\n\n        \"\"\"\n        if str(self.config.build_dir) not in sys.path:\n            sys.path.append(str(self.config.build_dir))\n        return importlib.import_module(self.config.target_package)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_generated_autoboto_module(self, name):\n        if str(self.config.build_dir) not in sys.path:\n            sys.path.append(str(self.config.build_dir))\n        return importlib.import_module(f\"{self.config.target_package}.{name}\")", "response": "Imports a module from the generated autoboto package in the build directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching in your Taiga. io instance", "response": "def search(self, project, text=''):\n        \"\"\"\n        Search in your Taiga.io instance\n\n        :param project: the project id\n        :param text: the query of your search\n        \"\"\"\n        result = self.raw_request.get(\n            'search', query={'project': project, 'text': text}\n        )\n        result = result.json()\n        search_result = SearchResult()\n        search_result.tasks = self.tasks.parse_list(result['tasks'])\n        search_result.issues = self.issues.parse_list(result['issues'])\n        search_result.user_stories = self.user_stories.parse_list(\n            result['userstories']\n        )\n        search_result.wikipages = self.wikipages.parse_list(\n            result['wikipages']\n        )\n        return search_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef auth(self, username, password):\n        headers = {\n            'Content-type': 'application/json'\n        }\n        payload = {\n            'type': self.auth_type,\n            'username': username,\n            'password': password\n        }\n        try:\n            full_url = utils.urljoin(self.host, '/api/v1/auth')\n            response = requests.post(\n                full_url,\n                data=json.dumps(payload),\n                headers=headers,\n                verify=self.tls_verify\n            )\n        except RequestException:\n            raise exceptions.TaigaRestException(\n                full_url, 400,\n                'NETWORK ERROR', 'POST'\n            )\n        if response.status_code != 200:\n            raise exceptions.TaigaRestException(\n                full_url,\n                response.status_code,\n                response.text,\n                'POST'\n            )\n        self.token = response.json()['auth_token']\n        self.raw_request = RequestMaker('/api/v1', self.host, self.token,\n                                        'Bearer', self.tls_verify)\n        self._init_resources()", "response": "Authenticate you\n\n        :param username: your username\n        :param password: your password"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate an app with the specified auth code", "response": "def auth_app(self, app_id, app_secret, auth_code, state=''):\n        \"\"\"\n        Authenticate an app\n\n        :param app_id: the app id\n        :param app_secret: the app secret\n        :param auth_code: the app auth code\n        \"\"\"\n        headers = {\n            'Content-type': 'application/json'\n        }\n        payload = {\n            'application': app_id,\n            'auth_code': auth_code,\n            'state': state\n        }\n        try:\n            full_url = utils.urljoin(\n                self.host,\n                '/api/v1/application-tokens/validate'\n            )\n            response = requests.post(\n                full_url,\n                data=json.dumps(payload),\n                headers=headers,\n                verify=self.tls_verify\n            )\n        except RequestException:\n            raise exceptions.TaigaRestException(\n                full_url, 400,\n                'NETWORK ERROR', 'POST'\n            )\n        if response.status_code != 200:\n            raise exceptions.TaigaRestException(\n                full_url,\n                response.status_code,\n                response.text,\n                'POST'\n            )\n        cyphered_token = response.json().get('cyphered_token', '')\n        if cyphered_token:\n            from jwkest.jwk import SYMKey\n            from jwkest.jwe import JWE\n\n            sym_key = SYMKey(key=app_secret, alg='A128KW')\n            data, success = JWE().decrypt(cyphered_token, keys=[sym_key]), True\n            if isinstance(data, tuple):\n                data, success = data\n            try:\n                self.token = json.loads(data.decode('utf-8')).get('token', None)\n            except ValueError:  # pragma: no cover\n                self.token = None\n            if not success:\n                self.token = None\n        else:\n            self.token = None\n\n        if self.token is None:\n            raise exceptions.TaigaRestException(\n                full_url, 400,\n                'INVALID TOKEN', 'POST'\n            )\n\n        self.raw_request = RequestMaker('/api/v1', self.host, self.token,\n                                        'Application', self.tls_verify)\n        self._init_resources()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse geonames. org data from geonames. org and updates or adds records as needed.", "response": "def parse_geonames_data(lines_iterator):\n    \"\"\"\n    Parses countries table data from geonames.org, updating or adding records as needed.\n    currency_symbol is not part of the countries table and is supplemented using the data\n    obtained from the link provided in the countries table.\n    :type lines_iterator: collections.iterable\n    :return: num_updated: int, num_created: int\n    :raise GeonamesParseError:\n    \"\"\"\n    data_headers = []\n    num_created = 0\n    num_updated = 0\n    for line in lines_iterator:\n        line = line.decode()\n        if line[0] == \"#\":\n            if line[0:4] == \"#ISO\":\n                data_headers = line.strip('# ').split('\\t')\n                if data_headers != DATA_HEADERS_ORDERED:\n                    raise GeonamesParseError(\n                        \"The table headers do not match the expected headers.\")\n            continue\n        if not data_headers:\n            raise GeonamesParseError(\"No table headers found.\")\n        bits = line.split('\\t')\n\n        data = {DATA_HEADERS_MAP[DATA_HEADERS_ORDERED[x]]: bits[x] for x in range(0, len(bits))}\n        if 'currency_code' in data and data['currency_code']:\n            data['currency_symbol'] = CURRENCY_SYMBOLS.get(data['currency_code'])\n\n        # Remove empty items\n        clean_data = {x: y for x, y in data.items() if y}\n\n        # Puerto Rico and the Dominican Republic have two phone prefixes in the format \"123 and\n        # 456\"\n        if 'phone' in clean_data:\n            if 'and' in clean_data['phone']:\n                clean_data['phone'] = \",\".join(re.split('\\s*and\\s*', clean_data['phone']))\n\n        # Avoiding update_or_create to maintain compatibility with Django 1.5\n        try:\n            country = Country.objects.get(iso=clean_data['iso'])\n            created = False\n        except Country.DoesNotExist:\n            try:\n                country = Country.objects.create(**clean_data)\n            except ValidationError as e:\n                raise GeonamesParseError(\"Unexpected field length: %s\" % e.message_dict)\n            created = True\n\n        for k, v in six.iteritems(clean_data):\n            setattr(country, k, v)\n\n        try:\n            country.save()\n        except ValidationError as e:\n            raise GeonamesParseError(\"Unexpected field length: %s\" % e.message_dict)\n\n        if created:\n            num_created += 1\n        else:\n            num_updated += 1\n    return num_updated, num_created"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sorted_members(self):\n        members = collections.OrderedDict()\n        required_names = self.metadata.get(\"required\", ())\n        for name, shape in self.members.items():\n            members[name] = AbShapeMember(name=name, shape=shape, is_required=name in required_names)\n\n        if self.is_output_shape:\n            # ResponseMetadata is the first member for all output shapes.\n            yield AbShapeMember(\n                name=\"ResponseMetadata\",\n                shape=self._shape_resolver.get_shape_by_name(\"ResponseMetadata\"),\n                is_required=True,\n            )\n\n        yield from sorted(members.values(), key=lambda m: not m.is_required)", "response": "Iterate over sorted members of shape in the same order in which\n        the members are declared except yielding the required members before which\n        any optional members."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self):\n        self.frontends = []\n        self.backends = []\n        self.listeners = []\n\n        csv = [ l for l in self._fetch().strip(' #').split('\\n') if l ]\n        if self.failed:\n            return\n\n        #read fields header to create keys\n        self.fields = [ f for f in csv.pop(0).split(',') if f ]\n    \n        #add frontends and backends first\n        for line in csv:\n            service = HAProxyService(self.fields, line.split(','), self.name)\n\n            if service.svname == 'FRONTEND':\n                self.frontends.append(service)\n            elif service.svname == 'BACKEND':\n                service.listeners = []\n                self.backends.append(service)\n            else:\n                self.listeners.append(service)\n    \n        #now add listener  names to corresponding backends\n        for listener in self.listeners:\n            for backend in self.backends:\n                if backend.iid == listener.iid:\n                    backend.listeners.append(listener)\n\n        self.last_update = datetime.utcnow()", "response": "Fetch and parse stats and update internal state of the internal state of the internal state of the internal state of the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding byte strings and convert to int where needed is a string", "response": "def _decode(value):\n        \"\"\"\n        decode byte strings and convert to int where needed\n        \"\"\"\n        if value.isdigit():\n            return int(value)\n        if isinstance(value, bytes):\n            return value.decode('utf-8')\n        else:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef markdown(text, renderer=None, **options):\n    ext, rndr = make_flags(**options)\n    if renderer:\n        md = misaka.Markdown(renderer,ext)\n        result = md(text)\n    else:\n        result = misaka.html(text, extensions=ext, render_flags=rndr)\n    if options.get(\"smartypants\"):\n        result = misaka.smartypants(result)\n    return Markup(result)", "response": "Parses the provided Markdown - formatted text into valid HTML and returns the rendered text as a : class:`flask. Markup instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render(self, text, **overrides):\n        options = self.defaults\n        if overrides:\n            options = copy(options)\n            options.update(overrides)\n        return markdown(text, self.renderer, **options)", "response": "Renders the given text using the specified renderer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef caseinsensitive(cls):\n    if not issubclass(cls, Enum):\n        raise TypeError('caseinsensitive decorator can only be applied to subclasses of enum.Enum')\n    enum_options = getattr(cls, PYCKSON_ENUM_OPTIONS, {})\n    enum_options[ENUM_CASE_INSENSITIVE] = True\n    setattr(cls, PYCKSON_ENUM_OPTIONS, enum_options)\n    return cls", "response": "Annotation function to set an Enum to be case insensitive on parsing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing Windows s GetCommandLineArgvW to get sys. argv as a list of UTF - 8 strings.", "response": "def win32_utf8_argv():\n    \"\"\"Uses shell32.GetCommandLineArgvW to get sys.argv as a list of UTF-8\n    strings.\n\n    Versions 2.5 and older of Python don't support Unicode in sys.argv on\n    Windows, with the underlying Windows API instead replacing multi-byte\n    characters with '?'.\n\n    Returns None on failure.\n\n    Example usage:\n\n    >>> def main(argv=None):\n    ...    if argv is None:\n    ...        argv = win32_utf8_argv() or sys.argv\n    ...\n    \"\"\"\n\n    try:\n        from ctypes import POINTER, byref, cdll, c_int, windll\n        from ctypes.wintypes import LPCWSTR, LPWSTR\n\n        GetCommandLineW = cdll.kernel32.GetCommandLineW\n        GetCommandLineW.argtypes = []\n        GetCommandLineW.restype = LPCWSTR\n\n        CommandLineToArgvW = windll.shell32.CommandLineToArgvW\n        CommandLineToArgvW.argtypes = [LPCWSTR, POINTER(c_int)]\n        CommandLineToArgvW.restype = POINTER(LPWSTR)\n\n        cmd = GetCommandLineW()\n        argc = c_int(0)\n        argv = CommandLineToArgvW(cmd, byref(argc))\n        if argc.value > 0:\n            # Remove Python executable if present\n            if argc.value - len(sys.argv) == 1:\n                start = 1\n            else:\n                start = 0\n            return [argv[i].encode('utf-8') for i in\n                    range(start, argc.value)]\n    except Exception:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encrypt_ascii(self, data, key=None, v=None, extra_bytes=0,\n                      digest=\"hex\"):\n        \"\"\"\n        Encrypt data and return as ascii string. Hexadecimal digest as default.\n\n        Avaiable digests:\n            hex: Hexadecimal\n            base64: Base 64\n            hqx: hexbin4\n        \"\"\"\n        digests = {\"hex\": binascii.b2a_hex,\n                   \"base64\": binascii.b2a_base64,\n                   \"hqx\": binascii.b2a_hqx}\n        digestor = digests.get(digest)\n        if not digestor:\n            TripleSecError(u\"Digestor not supported.\")\n\n        binary_result = self.encrypt(data, key, v, extra_bytes)\n        result = digestor(binary_result)\n        return result", "response": "Encrypt data and return as ascii string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt_ascii(self, ascii_string, key=None, digest=\"hex\"):\n        digests = {\"hex\": binascii.a2b_hex,\n                   \"base64\": binascii.a2b_base64,\n                   \"hqx\": binascii.a2b_hqx}\n        digestor = digests.get(digest)\n        if not digestor:\n            TripleSecError(u\"Digestor not supported.\")\n\n        binary_string = digestor(ascii_string)\n        result = self.decrypt(binary_string, key)\n        return result", "response": "Decrypt an ASCII string and return decrypted data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to find a movie with a given title and year and returns a list of movie objects.", "response": "def resolve_movie(self, title, year=None):\n        \"\"\"Tries to find a movie with a given title and year\"\"\"\n        r = self.search_movie(title)\n\n        return self._match_results(r, title, year)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to find a movie with a given title and year", "response": "def resolve_tv_show(self, title, year=None):\n        \"\"\"Tries to find a movie with a given title and year\"\"\"\n        r = self.search_tv_show(title)\n\n        return self._match_results(r, title, year)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the Gelman - Rubin R value for a set of simulations.", "response": "def r_value(means, variances, n, approx=False):\n    '''Calculate the Gelman-Rubin R value (Chapter 2.2 in [GR92]_).\n\n    The R value can be used to quantify mixing of \"multiple iterative\n    simulations\" (e.g. Markov Chains) in parameter space.  An R value\n    \"close to one\" indicates that all chains explored the same region\n    of the parameter.\n\n    .. note::\n\n        The R value is defined only in *one* dimension.\n\n    :param means:\n\n        Vector-like array; the sample mean of each chain.\n\n    :param variances:\n\n        Vector-like array; the sample variance of each chain.\n\n    '''\n    # use same variable names as in [GR92]\n    # means is \\bar{x}_i\n    # variances is s_i^2\n\n    means     = _np.asarray(means)\n    variances = _np.asarray(variances)\n\n    assert means.ndim == 1, '``means`` must be vector-like'\n    assert variances.ndim == 1, '``variances`` must be vector-like'\n    assert len(means) == len(variances), \\\n    'Number of ``means`` (%i) does not match number of ``variances`` (%i)' %( len(means), len(variances) )\n\n    m = len(means)\n\n    x_bar    = _np.average(means)\n    B_over_n = ((means - x_bar)**2).sum() / (m - 1)\n\n    W = _np.average(variances)\n\n    # var_estimate is \\hat{\\sigma}^2\n    var_estimate = (n - 1) / n  *  W + B_over_n\n\n    if approx:\n        return var_estimate / W\n\n    V = var_estimate + B_over_n / m\n\n    # calculate the three terms of \\hat{var}(\\hat{V}) (equation (4) in [GR92]\n    # var_V is \\hat{var}(\\hat{V})\n    tmp_cov_matrix = _np.cov(variances, means)\n    # third term\n    var_V = _np.cov(variances, means**2)[1,0] - 2. * x_bar * tmp_cov_matrix[1,0]\n    var_V *= 2. * (m + 1) * (n - 1) / (m * m * n)\n    # second term (merged n in denominator into ``B_over_n``)\n    var_V += ((m + 1) / m)**2 * 2. / (m - 1) * B_over_n * B_over_n\n    # first term\n    var_V += ((n - 1) / n)**2 / m * tmp_cov_matrix[0,0]\n\n    df = 2. * V**2 / var_V\n\n    if df <= 2.:\n        return _np.inf\n\n    return V / W * df / (df - 2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngroups m markov chains whose common r_value is less than critical_r.", "response": "def r_group(means, variances, n, critical_r=2., approx=False):\n    '''Group ``m`` (Markov) chains whose common :py:func:`.r_value` is\n    less than ``critical_r`` in each of the D dimensions.\n\n    :param means:\n\n        (m x D) Matrix-like array; the mean value estimates.\n\n    :param variances:\n\n        (m x D) Matrix-like array; the variance estimates.\n\n    '''\n    assert len(means) == len(variances), \\\n    'Number of ``means`` (%i) does not match number of ``variances`` (%i)' % (len(means), len(variances))\n    means = _np.asarray(means)\n    variances  = _np.asarray(variances)\n    assert means.ndim == 2, '``means`` must be matrix-like'\n    assert variances.ndim == 2, '``variances`` must be 2-dimensional'\n    assert means.shape[1] == variances.shape[1], \\\n    'Dimensionality of ``means`` (%i) and ``variances`` (%i) does not match' % (means.shape[1], variances.shape[1])\n\n    groups = []\n\n    for i in range(len(means)):\n        assigned = False\n        # try to assign component i to an existing group\n        for group in groups:\n            rows = group + [i]\n            # R values for each parameter\n            r_values = _np.array([r_value(means[rows, j], variances[rows, j], n, approx) for j in range(means.shape[1])])\n            if _np.all(r_values < critical_r):\n                # add to group if R value small enough\n                group.append(i)\n                assigned = True\n                break\n        # if component i has not been added to an existing group case create a new group\n        if not assigned:\n            groups.append([i])\n\n    return groups"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_r_gaussmix(data, K_g=15, critical_r=2., indices=None, approx=False):\n    '''Use ``data`` from multiple \"Iterative Simulations\" (e.g. Markov\n    Chains) to form a Gaussian Mixture. This approach refers to the\n    \"long patches\" in [BC13]_.\n\n    The idea is to group chains according to their R-value as in\n    :py:func:`.r_group` and form ``K_g`` Gaussian Components per chain\n    group. Once the groups are found by :py:func:`.r_group`, the ``data``\n    from each chain group is partitioned into ``K_g`` parts (using\n    :py:func:`pypmc.tools.partition`). For each of these parts a Gaussian\n    with its empirical mean and covariance is created.\n\n    Return a :py:class:`pypmc.density.mixture.MixtureDensity` with\n    :py:class:`pypmc.density.gauss.Gauss` components.\n\n    .. seealso::\n\n        :py:func:`.make_r_tmix`\n\n    :param data:\n\n        Iterable of matrix-like arrays; the individual items are interpreted\n        as points from an individual chain.\n\n        .. important::\n            Every chain must bring the same number of points.\n\n    :param K_g:\n\n        Integer; the number of components per chain group.\n\n    :param critical_r:\n\n        Float; the maximum R value a chain group may have.\n\n    :param indices:\n\n        Integer; Iterable of Integers; use R value in these dimensions\n        only. Default is all.\n\n    .. note::\n\n        If ``K_g`` is too large, some covariance matrices may not be positive definite.\n        Reduce ``K_g`` or increase ``len(data)``!\n\n    '''\n    return _mkgauss(*_make_r_patches(data, K_g, critical_r, indices, approx))", "response": "Use data from multiple Iterative Simulations in [ BC13 ] _. r_group and _mkgauss to form a Gaussian Mixture."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse data from multiple Iterative Simulations to form a Student t Mixture.", "response": "def make_r_tmix(data, K_g=15, critical_r=2., dof=5., indices=None, approx=False):\n    '''Use ``data`` from multiple \"Iterative Simulations\" (e.g. Markov\n    Chains) to form a Student t Mixture. This approach refers to the\n    \"long patches\" in [BC13]_.\n\n    The idea is to group chains according to their R-value as in\n    :py:func:`.r_group` and form ``K_g`` Student t Components per chain\n    group. Once the groups are found by :py:func:`.r_group`, the ``data``\n    from each chain group is partitioned into ``K_g`` parts (using\n    :py:func:`pypmc.tools.partition`). For each of these parts a Student t\n    component with its empirical mean, covariance and degree of freedom\n    is created.\n\n    Return a :py:class:`pypmc.density.mixture.MixtureDensity` with\n    :py:class:`pypmc.density.student_t.StudentT` components.\n\n    .. seealso::\n\n        :py:func:`.make_r_gaussmix`\n\n    :param data:\n\n        Iterable of matrix-like arrays; the individual items are interpreted\n        as points from an individual chain.\n\n        .. important::\n            Every chain must bring the same number of points.\n\n    :param K_g:\n\n        Integer; the number of components per chain group.\n\n    :param critical_r:\n\n        Float; the maximum R value a chain group may have.\n\n    :param dof:\n\n        Float; the degree of freedom the components will have.\n\n    :param indices:\n\n        Integer; Iterable of Integers; use R value in these dimensions\n        only. Default is all.\n\n    '''\n    assert dof > 2., \"``dof`` must be larger than 2. (got %g)\" %dof\n\n    means, covs = _make_r_patches(data, K_g, critical_r, indices, approx)\n\n    sigmas  = _np.asarray(covs)\n    # cov = nu / (nu-2) * sigma\n    sigmas *= (dof - 2.) / dof\n\n    return _mkt(means, sigmas, [dof] * len(means))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads hero details from JSON file into memoy", "response": "def load_heroes():\n    \"\"\"\n    Load hero details from JSON file into memoy\n    \"\"\"\n\n    filename = os.path.join(os.path.dirname(__file__), \"data\", \"heroes.json\")\n\n    with open(filename) as f:\n        heroes = json.loads(f.read())[\"result\"][\"heroes\"]\n        for hero in heroes:\n            HEROES_CACHE[hero[\"id\"]] = hero"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload item details fom JSON file into memory", "response": "def load_items():\n    \"\"\"\n    Load item details fom JSON file into memory\n    \"\"\"\n\n    filename = os.path.join(os.path.dirname(__file__), \"data\", \"items.json\")\n\n    with open(filename) as f:\n        items = json.loads(f.read())[\"result\"][\"items\"]\n        for item in items:\n            ITEMS_CACHE[item[\"id\"]] = item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a tree of commands.", "response": "def _build_cmd_tree(self, cmd_cls, cmd_name=None):\n        \"\"\"\n        Build a tree of commands.\n\n        :param cmd_cls:\n            The Command class or object to start with.\n        :param cmd_name:\n            Hard-coded name of the command (can be None for auto-detection)\n        :returns:\n            A tree structure represented as tuple\n                ``(cmd_obj, cmd_name, children)``\n            Where ``cmd_obj`` is a Command instance, cmd_name is its name, if\n            any (it might be None) and ``children`` is a tuple of identical\n            tuples.\n\n        Note that command name auto-detection relies on\n        :meth:`guacamole.recipes.cmd.Command.get_cmd_name()`.\n\n        Let's look at a simple git-like example::\n\n            >>> from guacamole import Command\n\n            >>> class git_log(Command):\n            >>>     pass\n\n            >>> class git_stash_list(Command):\n            >>>     pass\n\n            >>> class git_stash(Command):\n            >>>     sub_commands = (('list', git_stash_list),)\n\n            >>> class git(Command):\n            >>>     sub_commands = (('log', git_log),\n            >>>                     ('stash', git_stash))\n\n            >>> build_cmd_tree(git)\n            (None, '<git>', (\n                ('log', <git_log>, ()),\n                ('stash', <git_stash>, (\n                    ('list', <git_stash_list>, ()),),),),)\n        \"\"\"\n        if isinstance(cmd_cls, type):\n            cmd_obj = cmd_cls()\n        else:\n            cmd_obj = cmd_cls\n        if cmd_name is None:\n            cmd_name = cmd_obj.get_cmd_name()\n        return cmd_tree_node(cmd_name, cmd_obj, tuple([\n            self._build_cmd_tree(subcmd_cls, subcmd_name)\n            for subcmd_name, subcmd_cls in cmd_obj.get_sub_commands()]))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef debug_dump(message, file_prefix=\"dump\"):\n\n    global index\n    index += 1\n\n    with open(\"%s_%s.dump\" % (file_prefix, index), 'w') as f:\n        f.write(message.SerializeToString())\n        f.close()", "response": "Dump a message to a file in the\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a player attribute that depends on which side the player is on.", "response": "def get_side_attr(attr, invert, player):\n    \"\"\"\n    Get a player attribute that depends on which side the player is on.\n    A creep kill for a radiant hero is a badguy_kill, while a creep kill\n    for a dire hero is a goodguy_kill.\n    \"\"\"\n    t = player.team\n    if invert:\n        t = not player.team\n\n    return getattr(player, \"%s_%s\" % (\"goodguy\" if t else \"badguy\", attr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nkills a creep from the database.", "response": "def creep_kill(self, target, timestamp):\n        \"\"\"\n        A creep was tragically killed. Need to split this into radiant/dire\n        and neutrals\n        \"\"\"\n        self.creep_kill_types[target] += 1\n\n        matched = False\n        for k, v in self.creep_types.iteritems():\n            if target.startswith(k):\n                matched = True\n                setattr(self, v, getattr(self, v) + 1)\n                break\n\n        if not matched:\n            print('> unhandled creep type'.format(target))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_kills(self):\n\n        aegises = deque(self.aegis)\n\n        next_aegis = aegises.popleft() if aegises else None\n        aegis_expires = None\n        active_aegis = None\n\n        real_kills = []\n\n        for kill in self.kills:\n            if next_aegis and next_aegis[0] < kill[\"tick\"]:\n                active_aegis = next_aegis[1]\n                aegis_expires = next_aegis[0] + 1800 * 10  # 10 minutes\n\n                self.indexed_players[active_aegis].aegises += 1\n\n                next_aegis = aegises.popleft() if aegises else None\n            elif aegis_expires and kill[\"tick\"] > aegis_expires:\n                active_aegis = None\n                aegis_expires = None\n\n            source = kill[\"source\"]\n            target = kill[\"target\"]\n            timestamp = kill[\"timestamp\"]\n\n            if active_aegis == self.heroes[target].index:\n                #This was an aegis kill\n                active_aegis = None\n                aegis_expires = None\n\n                self.heroes[target].aegis_deaths += 1\n            else:\n                real_kills.append(kill)\n                self.heroes[target].add_death(source, timestamp)\n                if target != source:\n                    #Don't count a deny as a kill\n                    self.heroes[source].add_kill(target, timestamp)\n\n        self.kills = real_kills", "response": "Calculate the kills and deaths for the active aegis and the active aegis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a say event and add it to the list of chatlog entries.", "response": "def parse_say_text(self, event):\n        \"\"\"\n        All chat\n        \"\"\"\n        if event.chat and event.format == \"DOTA_Chat_All\":\n            self.chatlog.append((event.prefix, event.text))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_dota_um(self, event):\n        if event.type == dota_usermessages_pb2.CHAT_MESSAGE_AEGIS:\n            self.aegis.append((self.tick, event.playerid_1))", "response": "Parse a chat message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_player_info(self, player):\n        if not player.ishltv:\n            self.player_info[player.name] = {\n                \"user_id\": player.userID,\n                \"guid\": player.guid,\n                \"bot\": player.fakeplayer,\n            }", "response": "Parse a PlayerInfo struct. This arrives before a FileInfo message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the file info into the internal dictionary.", "response": "def parse_file_info(self, file_info):\n        \"\"\"\n        The CDemoFileInfo contains our winners as well as the length of the\n        demo\n        \"\"\"\n\n        self.info[\"playback_time\"] = file_info.playback_time\n        self.info[\"match_id\"] = file_info.game_info.dota.match_id\n        self.info[\"game_mode\"] = file_info.game_info.dota.game_mode\n        self.info[\"game_winner\"] = file_info.game_info.dota.game_winner\n\n        for index, player in enumerate(file_info.game_info.dota.player_info):\n            p = self.heroes[player.hero_name]\n            p.name = player.player_name\n            p.index = index\n            p.team = 0 if index < 5 else 1\n\n            self.indexed_players[index] = p\n            self.info[\"players\"][player.player_name] = p"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_game_event(self, ge):\n\n        if ge.name == \"dota_combatlog\":\n            if ge.keys[\"type\"] == 4:\n                #Something died\n                try:\n                    source = self.dp.combat_log_names.get(ge.keys[\"sourcename\"],\n                                                          \"unknown\")\n                    target = self.dp.combat_log_names.get(ge.keys[\"targetname\"],\n                                                          \"unknown\")\n                    target_illusion = ge.keys[\"targetillusion\"]\n                    timestamp = ge.keys[\"timestamp\"]\n\n                    if (target.startswith(\"npc_dota_hero\") and not\n                        target_illusion):\n\n                        self.kills.append({\n                            \"target\": target,\n                            \"source\": source,\n                            \"timestamp\": timestamp,\n                            \"tick\": self.tick,\n                            })\n\n                    elif source.startswith(\"npc_dota_hero\"):\n                        self.heroes[source].creep_kill(target, timestamp)\n                except KeyError:\n                    \"\"\"\n                    Sometimes we get combat logs for things we dont have in\n                    combat_log_names. My theory is that the server sends\n                    us incremental updates to the string table using\n                    CSVCMsg_UpdateStringTable but I'm not sure how to parse\n                    that\n                    \"\"\"\n                    pass", "response": "Parse a game event and add it to the list of kills and update the string table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling fix_code on the source code from the passed in file over the given line_ranges.", "response": "def fix_file(file_name, line_ranges, options=None, in_place=False,\n             diff=False, verbose=0, cwd=None):\n    \"\"\"Calls fix_code on the source code from the passed in file over the given\n    line_ranges.\n\n    - If diff then this returns the udiff for the changes, otherwise\n    returns the fixed code.\n    - If in_place the changes are written to the file.\n\n    \"\"\"\n    import codecs\n    from os import getcwd\n    from pep8radius.diff import get_diff\n    from pep8radius.shell import from_dir\n\n    if cwd is None:\n        cwd = getcwd()\n\n    with from_dir(cwd):\n        try:\n            with codecs.open(file_name, 'r', encoding='utf-8') as f:\n                original = f.read()\n        except IOError:\n            # Most likely the file has been removed.\n            # Note: it would be nice if we could raise here, specifically\n            # for the case of passing in a diff when in the wrong directory.\n            return ''\n\n    fixed = fix_code(original, line_ranges, options, verbose=verbose)\n\n    if in_place:\n        with from_dir(cwd):\n            with codecs.open(file_name, 'w', encoding='utf-8') as f:\n                f.write(fixed)\n\n    return get_diff(original, fixed, file_name) if diff else fixed"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply autopep8 over the line_ranges returns the corrected code.", "response": "def fix_code(source_code, line_ranges, options=None, verbose=0):\n    '''Apply autopep8 over the line_ranges, returns the corrected code.\n\n    Note: though this is not checked for line_ranges should not overlap.\n\n    Example\n    -------\n    >>> code = \"def f( x ):\\\\n  if  True:\\\\n    return 2*x\"\n    >>> print(fix_code(code, [(1, 1), (3, 3)]))\n    def f(x):\n      if  True:\n          return 2 * x\n\n    '''\n    if options is None:\n        from pep8radius.main import parse_args\n        options = parse_args()\n\n    if getattr(options, \"yapf\", False):\n        from yapf.yapflib.yapf_api import FormatCode\n        result = FormatCode(source_code, style_config=options.style, lines=line_ranges)\n        # yapf<0.3 returns diff as str, >=0.3 returns a tuple of (diff, changed)\n        return result[0] if isinstance(result, tuple) else result\n\n    line_ranges = reversed(line_ranges)\n    # Apply line fixes \"up\" the file (i.e. in reverse) so that\n    # fixes do not affect changes we're yet to make.\n    partial = source_code\n    for start, end in line_ranges:\n        partial = fix_line_range(partial, start, end, options)\n        _maybe_print('.', end='', max_=1, verbose=verbose)\n    _maybe_print('', max_=1, verbose=verbose)\n    fixed = partial\n    return fixed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fix_line_range(source_code, start, end, options):\n    # TODO confirm behaviour outside range (indexing starts at 1)\n    start = max(start, 1)\n\n    options.line_range = [start, end]\n    from autopep8 import fix_code\n    fixed = fix_code(source_code, options)\n\n    try:\n        if options.docformatter:\n            from docformatter import format_code\n            fixed = format_code(\n                fixed,\n                summary_wrap_length=options.max_line_length - 1,\n                description_wrap_length=(options.max_line_length\n                                         - 2 * options.indent_size),\n                pre_summary_newline=options.pre_summary_newline,\n                post_description_blank=options.post_description_blank,\n                force_wrap=options.force_wrap,\n                line_range=[start, end])\n    except AttributeError:  # e.g. using autopep8.parse_args, pragma: no cover\n        pass\n\n    return fixed", "response": "Apply autopep8 to the lines start and end of\n    source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _maybe_print(something_to_print, end=None, min_=1, max_=99, verbose=0):\n    if min_ <= verbose <= max_:\n        import sys\n        print(something_to_print, end=end)\n        sys.stdout.flush()", "response": "Print something_to_print if verbose is within min_ and max_."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_diff(diff, options=None, cwd=None):\n        return RadiusFromDiff(diff=diff, options=options, cwd=cwd)", "response": "Create a Radius object from a diff."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix(self):\n        from pep8radius.diff import print_diff, udiff_lines_fixed\n\n        n = len(self.filenames_diff)\n        _maybe_print('Applying autopep8 to touched lines in %s file(s).' % n)\n\n        any_changes = False\n        total_lines_changed = 0\n        pep8_diffs = []\n        for i, file_name in enumerate(self.filenames_diff, start=1):\n            _maybe_print('%s/%s: %s: ' % (i, n, file_name), end='')\n            _maybe_print('', min_=2)\n\n            p_diff = self.fix_file(file_name)\n            lines_changed = udiff_lines_fixed(p_diff) if p_diff else 0\n            total_lines_changed += lines_changed\n\n            if p_diff:\n                any_changes = True\n                if self.diff:\n                    pep8_diffs.append(p_diff)\n\n        if self.in_place:\n            _maybe_print('pep8radius fixed %s lines in %s files.'\n                         % (total_lines_changed, n),\n                         verbose=self.verbose)\n        else:\n            _maybe_print('pep8radius would fix %s lines in %s files.'\n                         % (total_lines_changed, n),\n                         verbose=self.verbose)\n\n        if self.diff:\n            for diff in pep8_diffs:\n                print_diff(diff, color=self.color)\n\n        return any_changes", "response": "Runs fix_file on each modified file. Returns True if there were any changes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_file(self, file_name):\n        # We hope that a CalledProcessError would have already raised\n        # during the init if it were going to raise here.\n        modified_lines = self.modified_lines(file_name)\n\n        return fix_file(file_name, modified_lines, self.options,\n                        in_place=self.in_place, diff=True,\n                        verbose=self.verbose, cwd=self.cwd)", "response": "Apply autopep8 to the diff lines of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multi_evaluate(self, x, out=None):\n        if out is None:\n            out = _np.empty(len(x))\n        else:\n            assert len(out) == len(x)\n\n        for i, point in enumerate(x):\n            out[i] = self.evaluate(point)\n\n        return out", "response": "Evaluate log of the density to propose x namely log ( q x )."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef url_map(base, params):\n\n    url = base\n\n    if not params:\n        url.rstrip(\"?&\")\n    elif '?' not in url:\n        url += \"?\"\n\n    entries = []\n    for key, value in params.items():\n         if value is not None:\n            value = str(value)\n            entries.append(\"%s=%s\" % (quote_plus(key.encode(\"utf-8\")),\n                                      quote_plus(value.encode(\"utf-8\"))))\n\n    url += \"&\".join(entries)\n    return str(url)", "response": "Returns a URL with get parameters based on the params passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes an API request", "response": "def make_request(name, params=None, version=\"V001\", key=None, api_type=\"web\",\n                 fetcher=get_page, base=None, language=\"en_us\"):\n    \"\"\"\n    Make an API request\n    \"\"\"\n\n    params = params or {}\n    params[\"key\"] = key or API_KEY\n    params[\"language\"] = language\n\n    if not params[\"key\"]:\n        raise ValueError(\"API key not set, please set DOTA2_API_KEY\")\n\n    url = url_map(\"%s%s/%s/\" % (base or BASE_URL, name, version), params)\n    return fetcher(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_request_response(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        response = f(*args, **kwargs)\n        response.raise_for_status()\n        return json.loads(response.content.decode('utf-8'))\n\n    API_FUNCTIONS[f.__name__] = f\n    return wrapper", "response": "Decorator that parses the JSON from an API response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the most recent 25 matches before start_at_match_id", "response": "def get_match_history(start_at_match_id=None, player_name=None, hero_id=None,\n                      skill=0, date_min=None, date_max=None, account_id=None,\n                      league_id=None, matches_requested=None, game_mode=None,\n                      min_players=None, tournament_games_only=None,\n                      **kwargs):\n    \"\"\"\n    List of most recent 25 matches before start_at_match_id\n    \"\"\"\n\n    params = {\n        \"start_at_match_id\": start_at_match_id,\n        \"player_name\": player_name,\n        \"hero_id\": hero_id,\n        \"skill\": skill,\n        \"date_min\": date_min,\n        \"date_max\": date_max,\n        \"account_id\": account_id,\n        \"league_id\": league_id,\n        \"matches_requested\": matches_requested,\n        \"game_mode\": game_mode,\n        \"min_players\": min_players,\n        \"tournament_games_only\": tournament_games_only\n    }\n\n    return make_request(\"GetMatchHistory\", params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the most recent matches ordered by sequence number", "response": "def get_match_history_by_sequence_num(start_at_match_seq_num,\n                                      matches_requested=None, **kwargs):\n    \"\"\"\n    Most recent matches ordered by sequence number\n    \"\"\"\n    params = {\n        \"start_at_match_seq_num\": start_at_match_seq_num,\n        \"matches_requested\": matches_requested\n    }\n\n    return make_request(\"GetMatchHistoryBySequenceNum\", params,\n        **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_steam_id(vanityurl, **kwargs):\n    params = {\"vanityurl\": vanityurl}\n    return make_request(\"ResolveVanityURL\", params, version=\"v0001\",\n        base=\"http://api.steampowered.com/ISteamUser/\", **kwargs)", "response": "Get a players steam id from their vanity url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_player_summaries(players, **kwargs):\n    if (isinstance(players, list)):\n        params = {'steamids': ','.join(str(p) for p in players)}\n    elif (isinstance(players, int)):\n        params = {'steamids': players}\n    else:\n        raise ValueError(\"The players input needs to be a list or int\")\n    return make_request(\"GetPlayerSummaries\", params, version=\"v0002\",\n        base=\"http://api.steampowered.com/ISteamUser/\", **kwargs)", "response": "Get players steam profile from their steam ids\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_hero_image_url(hero_name, image_size=\"lg\"):\n\n    if hero_name.startswith(\"npc_dota_hero_\"):\n        hero_name = hero_name[len(\"npc_dota_hero_\"):]\n\n    valid_sizes = ['eg', 'sb', 'lg', 'full', 'vert']\n    if image_size not in valid_sizes:\n        raise ValueError(\"Not a valid hero image size\")\n\n    return \"http://media.steampowered.com/apps/dota2/images/heroes/{}_{}.png\".format(\n        hero_name, image_size)", "response": "Get a hero image url based on name and image size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a ProxyClass based view that uses the passed base_url.", "response": "def generate_proxy(\n        prefix, base_url='', verify_ssl=True, middleware=None,\n        append_middleware=None, cert=None, timeout=None):\n    \"\"\"Generate a ProxyClass based view that uses the passed base_url.\"\"\"\n    middleware = list(middleware or HttpProxy.proxy_middleware)\n    middleware += list(append_middleware or [])\n\n    return type('ProxyClass', (HttpProxy,), {\n        'base_url': base_url,\n        'reverse_urls': [(prefix, base_url)],\n        'verify_ssl': verify_ssl,\n        'proxy_middleware': middleware,\n        'cert': cert,\n        'timeout': timeout\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a list of urls that map to generated proxy views.", "response": "def generate_routes(config):\n    \"\"\"Generate a list of urls that map to generated proxy views.\n\n    generate_routes({\n        'test_proxy': {\n            'base_url': 'https://google.com/',\n            'prefix': '/test_prefix/',\n            'verify_ssl': False,\n            'csrf_exempt: False',\n            'middleware': ['djproxy.proxy_middleware.AddXFF'],\n            'append_middleware': ['djproxy.proxy_middleware.AddXFF'],\n            'timeout': 3.0,\n            'cert': None\n        }\n    })\n\n    Required configuration keys:\n\n    * `base_url`\n    * `prefix`\n\n    Optional configuration keys:\n\n    * `verify_ssl`: defaults to `True`.\n    * `csrf_exempt`: defaults to `True`.\n    * `cert`: defaults to `None`.\n    * `timeout`: defaults to `None`.\n    * `middleware`: Defaults to `None`. Specifying `None` causes djproxy to use\n      the default middleware set. If a list is passed, the default middleware\n      list specified by the HttpProxy definition will be replaced with the\n      provided list.\n    * `append_middleware`: Defaults to `None`. `None` results in no changes to\n      the default middleware set. If a list is specified, the list will be\n      appended to the default middleware list specified in the HttpProxy\n      definition or, if provided, the middleware key specificed in the config\n      dict.\n\n    Returns:\n\n    [\n        url(r'^test_prefix/', GeneratedProxy.as_view(), name='test_proxy')),\n    ]\n\n    \"\"\"\n    routes = []\n\n    for name, config in iteritems(config):\n        pattern = r'^%s(?P<url>.*)$' % re.escape(config['prefix'].lstrip('/'))\n        proxy = generate_proxy(\n            prefix=config['prefix'], base_url=config['base_url'],\n            verify_ssl=config.get('verify_ssl', True),\n            middleware=config.get('middleware'),\n            append_middleware=config.get('append_middleware'),\n            cert=config.get('cert'),\n            timeout=config.get('timeout'))\n        proxy_view_function = proxy.as_view()\n\n        proxy_view_function.csrf_exempt = config.get('csrf_exempt', True)\n\n        routes.append(url(pattern, proxy_view_function, name=name))\n\n    return routes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the Brier score for each forecast and observation.", "response": "def brier_score(observations, forecasts):\n    \"\"\"\n    Calculate the Brier score (BS)\n\n    The Brier score (BS) scores binary forecasts $k \\in \\{0, 1\\}$,\n\n    ..math:\n        BS(p, k) = (p_1 - k)^2,\n\n    where $p_1$ is the forecast probability of $k=1$.\n\n    Parameters\n    ----------\n    observations, forecasts : array_like\n        Broadcast compatible arrays of forecasts (probabilities between 0 and\n        1) and observations (0, 1 or NaN).\n\n    Returns\n    -------\n    out : np.ndarray\n        Brier score for each forecast/observation.\n\n    References\n    ----------\n    Jochen Broecker. Chapter 7 in Forecast Verification: A Practitioner's Guide\n        in Atmospheric Science. John Wiley & Sons, Ltd, Chichester, UK, 2nd\n        edition, 2012.\n        https://drive.google.com/a/climate.com/file/d/0B8AfRcot4nsIYmc3alpTeTZpLWc\n    Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules,\n        prediction, and estimation, 2005. University of Washington Department of\n        Statistics Technical Report no. 463R.\n        https://www.stat.washington.edu/research/reports/2004/tr463R.pdf\n    \"\"\"\n    machine_eps = np.finfo(float).eps\n    forecasts = np.asarray(forecasts)\n    if (forecasts < 0.0).any() or (forecasts > (1.0 + machine_eps)).any():\n        raise ValueError('forecasts must not be outside of the unit interval '\n                         '[0, 1]')\n    observations = np.asarray(observations)\n    if observations.ndim > 0:\n        valid_obs = observations[~np.isnan(observations)]\n    else:\n        valid_obs = observations if not np.isnan(observations) else []\n    if not set(np.unique(valid_obs)) <= {0, 1}:\n        raise ValueError('observations can only contain 0, 1, or NaN')\n    return (forecasts - observations) ** 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef threshold_brier_score(observations, forecasts, threshold, issorted=False,\n                          axis=-1):\n    \"\"\"\n    Calculate the Brier scores of an ensemble for exceeding given thresholds.\n\n    According to the threshold decomposition of CRPS, the resulting Brier\n    scores can thus be summed along the last axis to calculate CRPS, as\n\n    .. math::\n        CRPS(F, x) = \\int_z BS(F(z), H(z - x)) dz\n\n    where $F(x) = \\int_{z \\leq x} p(z) dz$ is the cumulative distribution\n    function (CDF) of the forecast distribution $F$, $x$ is a point estimate of\n    the true observation (observational error is neglected), $BS$ denotes the\n    Brier score and $H(x)$ denotes the Heaviside step function, which we define\n    here as equal to 1 for x >= 0 and 0 otherwise.\n\n    It is more efficient to calculate CRPS directly, but this threshold\n    decomposition itself provides a useful summary of model quality as a\n    function of measurement values.\n\n    The Numba accelerated version of this function is much faster for\n    calculating many thresholds simultaneously: it runs in time\n    O(N * (E * log(E) + T)), where N is the number of observations, E is the\n    ensemble size and T is the number of thresholds.\n\n    The non-Numba accelerated version requires time and space O(N * E * T).\n\n    Parameters\n    ----------\n    observations : float or array_like\n        Observations float or array. Missing values (NaN) are given scores of\n        NaN.\n    forecasts : float or array_like\n        Array of forecasts ensemble members, of the same shape as observations\n        except for the extra axis corresponding to the ensemble. If forecasts\n        has the same shape as observations, the forecasts are treated as\n        deterministic. Missing values (NaN) are ignored.\n    threshold : scalar or 1d array_like\n        Threshold value(s) at which to calculate exceedence Brier scores.\n    issorted : bool, optional\n        Optimization flag to indicate that the elements of `ensemble` are\n        already sorted along `axis`.\n    axis : int, optional\n        Axis in forecasts which corresponds to different ensemble members,\n        along which to calculate the threshold decomposition.\n\n    Returns\n    -------\n    out : np.ndarray\n        Brier scores at each thresold for each ensemble forecast against the\n        observations. If ``threshold`` is a scalar, the result will have the\n        same shape as observations. Otherwise, it will have an additional final\n        dimension corresponding to the threshold levels.\n\n    References\n    ----------\n    Gneiting, T. and Ranjan, R. Comparing density forecasts using threshold-\n       and quantile-weighted scoring rules. J. Bus. Econ. Stat. 29, 411-422\n       (2011). http://www.stat.washington.edu/research/reports/2008/tr533.pdf\n\n    See also\n    --------\n    crps_ensemble, brier_score\n    \"\"\"\n    observations = np.asarray(observations)\n    threshold = np.asarray(threshold)\n    forecasts = np.asarray(forecasts)\n\n    if axis != -1:\n        forecasts = move_axis_to_end(forecasts, axis)\n\n    if forecasts.shape == observations.shape:\n        forecasts = forecasts[..., np.newaxis]\n\n    if observations.shape != forecasts.shape[:-1]:\n        raise ValueError('observations and forecasts must have matching '\n                         'shapes or matching shapes except along `axis=%s`'\n                         % axis)\n\n    scalar_threshold = threshold.ndim == 0\n\n    if threshold.ndim > 1:\n        raise ValueError('threshold must be scalar or 1-dimensional')\n    if threshold.ndim == 1 and not (np.sort(threshold) == threshold).all():\n        raise ValueError('1D thresholds must be sorted')\n\n    threshold = threshold.reshape((1,) * observations.ndim + (-1,))\n\n    if not issorted:\n        forecasts = np.sort(forecasts, axis=-1)\n\n    result = _threshold_brier_score_core(observations, forecasts, threshold)\n\n    if scalar_threshold:\n        result = result.squeeze(axis=-1)\n\n    return result", "response": "This function calculates the Brier scores of an ensemble for exceeding given thresholds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap for json. dumps that uses the JSONArgonautsEncoder.", "response": "def dumps(*args, **kwargs):\n    \"\"\"\n    Wrapper for json.dumps that uses the JSONArgonautsEncoder.\n    \"\"\"\n    import json\n\n    from django.conf import settings\n    from argonauts.serializers import JSONArgonautsEncoder\n\n    kwargs.setdefault('cls', JSONArgonautsEncoder)\n    # pretty print in DEBUG mode.\n    if settings.DEBUG:\n        kwargs.setdefault('indent', 4)\n        kwargs.setdefault('separators', (',', ': '))\n    else:\n        kwargs.setdefault('separators', (',', ':'))\n\n    return json.dumps(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding method that applies SGR codes to log messages.", "response": "def format(self, record):\n        \"\"\"Overridden method that applies SGR codes to log messages.\"\"\"\n        # XXX: idea, colorize message arguments\n        s = super(ANSIFormatter, self).format(record)\n        if hasattr(self.context, 'ansi'):\n            s = self.context.ansi(s, **self.get_sgr(record))\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef added(self, context):\n        self._expose_argparse = context.bowl.has_spice(\"log:arguments\")\n        self.configure_logging(context)", "response": "Configure generic application logging."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure logging for the application.", "response": "def configure_logging(self, context):\n        \"\"\"\n        Configure logging for the application.\n\n        :param context:\n            The guacamole context object.\n\n        This method attaches a :py:class:logging.StreamHandler` with a\n        subclass of :py:class:`logging.Formatter` to the root logger. The\n        specific subclass is :class:`ANSIFormatter` and it adds basic ANSI\n        formatting (colors and some styles) to logging messages so that they\n        stand out from normal output.\n        \"\"\"\n        fmt = \"%(name)-12s: %(levelname)-8s %(message)s\"\n        formatter = ANSIFormatter(context, fmt)\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        logging.root.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadjusting the logging configuration of the guacamole subsystem.", "response": "def adjust_logging(self, context):\n        \"\"\"\n        Adjust logging configuration.\n\n        :param context:\n            The guacamole context object.\n\n        This method uses the context and the results of early argument parsing\n        to adjust the configuration of the logging subsystem. In practice the\n        values passed to ``--log-level`` and ``--trace`` are applied.\n        \"\"\"\n        if context.early_args.log_level:\n            log_level = context.early_args.log_level\n            logging.getLogger(\"\").setLevel(log_level)\n        for name in context.early_args.trace:\n            logging.getLogger(name).setLevel(logging.DEBUG)\n            _logger.info(\"Enabled tracing on logger %r\", name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a json encoding of the argument.", "response": "def json(a):\n    \"\"\"\n    Output the json encoding of its argument.\n\n    This will escape all the HTML/XML special characters with their unicode\n    escapes, so it is safe to be output anywhere except for inside a tag\n    attribute.\n\n    If the output needs to be put in an attribute, entitize the output of this\n    filter.\n    \"\"\"\n    json_str = json_dumps(a)\n\n    # Escape all the XML/HTML special characters.\n    escapes = ['<', '>', '&']\n    for c in escapes:\n        json_str = json_str.replace(c, r'\\u%04x' % ord(c))\n\n    # now it's safe to use mark_safe\n    return mark_safe(json_str)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef json_twisted_response(f):\n\n    def wrapper(*args, **kwargs):\n        response = f(*args, **kwargs)\n        response.addCallback(lambda x: json.loads(x))\n        return response\n\n    wrapper.func = f\n    wrapper = util.mergeFunctionMetadata(f.func, wrapper)\n    return wrapper", "response": "A decorator that parses the JSON from an API response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dispatch_failed(self, context):\n        traceback.print_exception(\n            context.exc_type, context.exc_value, context.traceback)\n        raise SystemExit(1)", "response": "Print the unhandled exception and exit the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the set of keywords in a uri template", "response": "def variables(template):\n    '''Returns the set of keywords in a uri template'''\n    vars = set()\n    for varlist in TEMPLATE.findall(template):\n        if varlist[0] in OPERATOR:\n            varlist = varlist[1:]\n        varspecs = varlist.split(',')\n        for var in varspecs:\n            # handle prefix values\n            var = var.split(':')[0]\n            # handle composite values\n            if var.endswith('*'):\n                var = var[:-1]\n            vars.add(var)\n    return vars"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expand(template, variables):\n    def _sub(match):\n        expression = match.group(1)\n        operator = \"\"\n        if expression[0] in OPERATOR:\n            operator = expression[0]\n            varlist = expression[1:]\n        else:\n            varlist = expression\n\n        safe = \"\"\n        if operator in [\"+\", \"#\"]:\n            safe = RESERVED\n        varspecs = varlist.split(\",\")\n        varnames = []\n        defaults = {}\n        for varspec in varspecs:\n            default = None\n            explode = False\n            prefix = None\n            if \"=\" in varspec:\n                varname, default = tuple(varspec.split(\"=\", 1))\n            else:\n                varname = varspec\n            if varname[-1] == \"*\":\n                explode = True\n                varname = varname[:-1]\n            elif \":\" in varname:\n                try:\n                    prefix = int(varname[varname.index(\":\")+1:])\n                except ValueError:\n                    raise ValueError(\"non-integer prefix '{0}'\".format(\n                       varname[varname.index(\":\")+1:]))\n                varname = varname[:varname.index(\":\")]\n            if default:\n                defaults[varname] = default\n            varnames.append((varname, explode, prefix))\n\n        retval = []\n        joiner = operator\n        start = operator\n        if operator == \"+\":\n            start = \"\"\n            joiner = \",\"\n        if operator == \"#\":\n            joiner = \",\"\n        if operator == \"?\":\n            joiner = \"&\"\n        if operator == \"&\":\n            start = \"&\"\n        if operator == \"\":\n            joiner = \",\"\n        for varname, explode, prefix in varnames:\n            if varname in variables:\n                value = variables[varname]\n                if not value and value != \"\" and varname in defaults:\n                    value = defaults[varname]\n            elif varname in defaults:\n                value = defaults[varname]\n            else:\n                continue\n            expanded = TOSTRING[operator](\n              varname, value, explode, prefix, operator, safe=safe)\n            if expanded is not None:\n                retval.append(expanded)\n        if len(retval) > 0:\n            return start + joiner.join(retval)\n        else:\n            return \"\"\n\n    return TEMPLATE.sub(_sub, template)", "response": "Expand a template as a URI Template using a set of variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_expectation(samples, weights, f):\n    r'''Calculate the expectation value of function ``f`` using weighted\n    samples (like the output of an importance-sampling run).\n\n    Denoting :math:`x_n` as the sample n and :math:`w_n` as its (normalized)\n    weight, the following is returned:\n\n    .. math::\n\n        \\sum_{n=1}^{N} w_n f(x_n)\n        \\mathrm{\\ \\ where\\ \\ } \\sum_{n=1}^{N}w_n \\overset{!}{=} 1\n\n    :param samples:\n\n        Matrix-like numpy array; the samples to be used.\n\n    :param weights:\n\n        Vector-like numpy array; the (unnormalized) importance weights.\n\n    :param f:\n\n        Callable, the function to be evaluated.\n\n    '''\n    assert len(samples) == len(weights), \"The number of samples (got %i) must equal the number of weights (got %i).\" % (len(samples),len(weights))\n    normalization = 0.\n    out           = 0.\n    for weight, sample in zip(weights, samples):\n        normalization += weight\n        out += weight * f(sample)\n    return out/normalization", "response": "r Calculates the expectation value of function f using weighted samples and weights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the deterministic mixture weights according to a base - set of samples and weights.", "response": "def combine_weights(samples, weights, proposals):\n    \"\"\"Calculate the `deterministic mixture weights` according to\n    [Cor+12]_ given ``samples``, standard ``weights`` and their ``proposals`` for a\n    number of steps in which importance samples are computed for the same target\n    density but different proposals.\n\n    Return the weights as a :class:`pypmc.tools.History` such that the\n    weights for each proposal are easily accessible.\n\n    :param samples:\n\n        Iterable of matrix-like arrays; the weighted samples whose importance\n        weights shall be combined. One sample per row in each array, one array\n        for each step, or different proposal.\n\n    :param weights:\n\n        Iterable of 1D arrays; the standard importance weights\n        :math:`P(x_i^t)/q_t(x_i^t)`. Each array in the iterable contains all\n        weights of the samples of step ``t``, they array's size has to match the\n        ``t``-th entry in samples.\n\n    :param proposals:\n\n        Iterable of :py:class:`pypmc.density.base.ProbabilityDensity` instances;\n        the proposal densities from which the ``samples`` have been\n        drawn.\n\n    \"\"\"\n    # shallow copy --> can safely modify (need numpy arrays --> can overwrite with np.asarray)\n    samples = list(samples)\n    weights = list(weights)\n\n    assert len(samples) == len(weights), \\\n    \"Got %i importance-sampling runs but %i weights\" % (len(samples), len(weights))\n\n    assert len(samples) == len(proposals), \\\n    \"Got %i importance-sampling runs but %i proposal densities\" % (len(samples), len(proposals))\n\n    # number of samples from each proposal\n    N = _np.empty(len(proposals))\n    N_total = 0\n\n    # basic consistency checks, conversion to numpy array and counting of the total number of samples\n    for i in range(len(N)):\n        samples[i] = _np.asarray(samples[i])\n        assert len(samples[i].shape) == 2, '``samples[%i]`` is not matrix like.' % i\n        dim = samples[0].shape[-1]\n        assert samples[i].shape[-1] == dim, \\\n            \"Dimension of samples[0] (%i) does not match the dimension of samples[%i] (%i)\" \\\n            % (dim, i, samples[i].shape[-1])\n        N[i] = len(samples[i])\n        N_total += N[i]\n\n        weights[i] = _np.asarray(weights[i])\n        assert N[i] == len(weights[i]), \\\n            'Length of weights[%i] (%i) does not match length of samples[%i] (%i)' \\\n            % (i, N[i], i, len(weights[i]))\n\n    combined_weights_history = _History(1, N_total)\n\n    # if all weights positive => prefer log scale\n    all_positive = True\n    for w in weights:\n        all_positive &= (w[:] > 0.0).all()\n        if not all_positive:\n            break\n    if all_positive:\n        combined_weights_history = _combine_weights_log(samples, weights, proposals, combined_weights_history, N_total, N)\n    else:\n        combined_weights_history = _combine_weights_linear(samples, weights, proposals, combined_weights_history, N_total, N)\n\n    assert _np.isfinite(combined_weights_history[:][:,0]).all(), 'Encountered inf or nan mixture weights'\n    return combined_weights_history"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing the history of samples and other internal variables to free memory.", "response": "def clear(self):\n        '''Clear history of samples and other internal variables to free memory.\n\n        .. note::\n            The proposal is untouched.\n\n        '''\n        self.samples.clear()\n        self.weights.clear()\n        if self.target_values is not None:\n            self.target_values.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, N=1, trace_sort=False):\n        '''Run the sampler, store the history of visited points into\n        the member variable ``self.samples`` and the importance weights\n        into ``self.weights``.\n\n        .. seealso::\n            :py:class:`pypmc.tools.History`\n\n        :param N:\n\n            Integer; the number of samples to be drawn.\n\n        :param trace_sort:\n\n            Bool; if True, return an array containing the responsible\n            component of ``self.proposal`` for each sample generated\n            during this run.\n\n            .. note::\n\n                This option only works for proposals of type\n                :py:class:`pypmc.density.mixture.MixtureDensity`\n\n            .. note::\n\n                If True, the samples will be ordered by the components.\n\n        '''\n        if N == 0:\n            return 0\n\n        if trace_sort:\n            this_samples, origin = self._get_samples(N, trace_sort=True)\n            self._calculate_weights(this_samples, N)\n            return origin\n        else:\n            this_samples = self._get_samples(N, trace_sort=False)\n            self._calculate_weights(this_samples, N)", "response": "Run the sampler and store the history of visited points into self. samples and self. weights."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating and save the weights of a run.", "response": "def _calculate_weights(self, this_samples, N):\n        \"\"\"Calculate and save the weights of a run.\"\"\"\n\n        this_weights = self.weights.append(N)[:,0]\n\n        if self.target_values is None:\n            for i in range(N):\n                tmp = self.target(this_samples[i]) - self.proposal.evaluate(this_samples[i])\n                this_weights[i] = _exp(tmp)\n        else:\n            this_target_values = self.target_values.append(N)\n            for i in range(N):\n                this_target_values[i] = self.target(this_samples[i])\n                tmp = this_target_values[i] - self.proposal.evaluate(this_samples[i])\n                this_weights[i] = _exp(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave N samples from self. proposal to self. samples.", "response": "def _get_samples(self, N, trace_sort):\n        \"\"\"Save N samples from ``self.proposal`` to ``self.samples``\n        This function does NOT calculate the weights.\n\n        Return a reference to this run's samples in ``self.samples``.\n        If ``trace_sort`` is True, additionally return an array\n        indicating the responsible component. (MixtureDensity only)\n\n        \"\"\"\n        # allocate an empty numpy array to store the run and append accept count\n        # (importance sampling accepts all points)\n        this_run = self.samples.append(N)\n\n        # store the proposed points (weights are still to be calculated)\n        if trace_sort:\n            this_run[:], origin = self.proposal.propose(N, self.rng, trace=True, shuffle=False)\n            return this_run, origin\n        else:\n            this_run[:] = self.proposal.propose(N, self.rng)\n            return this_run"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef x_forwarded_for(self):\n        ip = self._request.META.get('REMOTE_ADDR')\n        current_xff = self.headers.get('X-Forwarded-For')\n\n        return '%s, %s' % (current_xff, ip) if current_xff else ip", "response": "The X - Forwarded - For header value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize Django header names modified by Django.", "response": "def _normalize_django_header_name(header):\n        \"\"\"Unmunge header names modified by Django.\"\"\"\n        # Remove HTTP_ prefix.\n        new_header = header.rpartition('HTTP_')[2]\n        # Camel case and replace _ with -\n        new_header = '-'.join(\n            x.capitalize() for x in new_header.split('_'))\n\n        return new_header"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a HeaderDict based on django request object meta data.", "response": "def from_request(cls, request):\n        \"\"\"Generate a HeaderDict based on django request object meta data.\"\"\"\n        request_headers = HeaderDict()\n        other_headers = ['CONTENT_TYPE', 'CONTENT_LENGTH']\n\n        for header, value in iteritems(request.META):\n            is_header = header.startswith('HTTP_') or header in other_headers\n            normalized_header = cls._normalize_django_header_name(header)\n\n            if is_header and value:\n                request_headers[normalized_header] = value\n\n        return request_headers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a HeaderSet excluding the headers in the exclude list.", "response": "def filter(self, exclude):\n        \"\"\"Return a HeaderSet excluding the headers in the exclude list.\"\"\"\n        filtered_headers = HeaderDict()\n        lowercased_ignore_list = [x.lower() for x in exclude]\n\n        for header, value in iteritems(self):\n            if header.lower() not in lowercased_ignore_list:\n                filtered_headers[header] = value\n\n        return filtered_headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the crps of observations x relative to normally distributed forecasts with mean mu and standard deviation sig.", "response": "def crps_gaussian(x, mu, sig, grad=False):\n    \"\"\"\n    Computes the CRPS of observations x relative to normally distributed\n    forecasts with mean, mu, and standard deviation, sig.\n\n    CRPS(N(mu, sig^2); x)\n\n    Formula taken from Equation (5):\n\n    Calibrated Probablistic Forecasting Using Ensemble Model Output\n    Statistics and Minimum CRPS Estimation. Gneiting, Raftery,\n    Westveld, Goldman. Monthly Weather Review 2004\n\n    http://journals.ametsoc.org/doi/pdf/10.1175/MWR2904.1\n\n    Parameters\n    ----------\n    x : scalar or np.ndarray\n        The observation or set of observations.\n    mu : scalar or np.ndarray\n        The mean of the forecast normal distribution\n    sig : scalar or np.ndarray\n        The standard deviation of the forecast distribution\n    grad : boolean\n        If True the gradient of the CRPS w.r.t. mu and sig\n        is returned along with the CRPS.\n\n    Returns\n    -------\n    crps : scalar or np.ndarray or tuple of\n        The CRPS of each observation x relative to mu and sig.\n        The shape of the output array is determined by numpy\n        broadcasting rules.\n    crps_grad : np.ndarray (optional)\n        If grad=True the gradient of the crps is returned as\n        a numpy array [grad_wrt_mu, grad_wrt_sig].  The\n        same broadcasting rules apply.\n    \"\"\"\n    x = np.asarray(x)\n    mu = np.asarray(mu)\n    sig = np.asarray(sig)\n    # standadized x\n    sx = (x - mu) / sig\n    # some precomputations to speed up the gradient\n    pdf = _normpdf(sx)\n    cdf = _normcdf(sx)\n    pi_inv = 1. / np.sqrt(np.pi)\n    # the actual crps\n    crps = sig * (sx * (2 * cdf - 1) + 2 * pdf - pi_inv)\n    if grad:\n        dmu = 1 - 2 * cdf\n        dsig = 2 * pdf - pi_inv\n        return crps, np.array([dmu, dsig])\n    else:\n        return crps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _discover_bounds(cdf, tol=1e-7):\n    class DistFromCDF(stats.distributions.rv_continuous):\n        def cdf(self, x):\n            return cdf(x)\n    dist = DistFromCDF()\n    # the ppf is the inverse cdf\n    lower = dist.ppf(tol)\n    upper = dist.ppf(1. - tol)\n    return lower, upper", "response": "Compute the lower and upper limits of a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _crps_cdf_single(x, cdf_or_dist, xmin=None, xmax=None, tol=1e-6):\n    # TODO: this function is pretty slow.  Look for clever ways to speed it up.\n\n    # allow for directly passing in scipy.stats distribution objects.\n    cdf = getattr(cdf_or_dist, 'cdf', cdf_or_dist)\n    assert callable(cdf)\n\n    # if bounds aren't given, discover them\n    if xmin is None or xmax is None:\n        # Note that infinite values for xmin and xmax are valid, but\n        # it slows down the resulting quadrature significantly.\n        xmin, xmax = _discover_bounds(cdf)\n\n    # make sure the bounds haven't clipped the cdf.\n    if (tol is not None) and (cdf(xmin) >= tol) or (cdf(xmax) <= (1. - tol)):\n        raise ValueError('CDF does not meet tolerance requirements at %s '\n                         'extreme(s)! Consider using function defaults '\n                         'or using infinities at the bounds. '\n                         % ('lower' if cdf(xmin) >= tol else 'upper'))\n\n    # CRPS = int_-inf^inf (F(y) - H(x))**2 dy\n    #      = int_-inf^x F(y)**2 dy + int_x^inf (1 - F(y))**2 dy\n    def lhs(y):\n        # left hand side of CRPS integral\n        return np.square(cdf(y))\n    # use quadrature to integrate the lhs\n    lhs_int, lhs_tol = integrate.quad(lhs, xmin, x)\n    # make sure the resulting CRPS will be with tolerance\n    if (tol is not None) and (lhs_tol >= 0.5 * tol):\n        raise ValueError('Lower integral did not evaluate to within tolerance! '\n                         'Tolerance achieved: %f , Value of integral: %f \\n'\n                         'Consider setting the lower bound to -np.inf.' %\n                         (lhs_tol, lhs_int))\n\n    def rhs(y):\n        # right hand side of CRPS integral\n        return np.square(1. - cdf(y))\n    rhs_int, rhs_tol = integrate.quad(rhs, x, xmax)\n    # make sure the resulting CRPS will be with tolerance\n    if (tol is not None) and (rhs_tol >= 0.5 * tol):\n        raise ValueError('Upper integral did not evaluate to within tolerance! \\n'\n                         'Tolerance achieved: %f , Value of integral: %f \\n'\n                         'Consider setting the upper bound to np.inf or if '\n                         'you already have, set warn_level to `ignore`.' %\n                         (rhs_tol, rhs_int))\n\n    return lhs_int + rhs_int", "response": "Compute the crps of a single distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the continuously ranked probability score for a given forecast distribution x and observation x using numerical quadrature.", "response": "def crps_quadrature(x, cdf_or_dist, xmin=None, xmax=None, tol=1e-6):\n    \"\"\"\n    Compute the continuously ranked probability score (CPRS) for a given\n    forecast distribution (cdf) and observation (x) using numerical quadrature.\n\n    This implementation allows the computation of CRPS for arbitrary forecast\n    distributions. If gaussianity can be assumed ``crps_gaussian`` is faster.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Observations associated with the forecast distribution cdf_or_dist\n    cdf_or_dist : callable or scipy.stats.distribution\n        Function which returns the the cumulative density of the\n        forecast distribution at value x.  This can also be an object with\n        a callable cdf() method such as a scipy.stats.distribution object.\n    xmin : np.ndarray or scalar\n        The lower bounds for integration, this is required to perform\n        quadrature.\n    xmax : np.ndarray or scalar\n        The upper bounds for integration, this is required to perform\n        quadrature.\n    tol : float , optional\n        The desired accuracy of the CRPS, larger values will speed\n        up integration. If tol is set to None, bounds errors or integration\n        tolerance errors will be ignored.\n\n    Returns\n    -------\n    crps : np.ndarray\n        The continuously ranked probability score of an observation x\n        given forecast distribution.\n    \"\"\"\n    return _crps_cdf(x, cdf_or_dist, xmin, xmax, tol)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crps_ensemble(observations, forecasts, weights=None, issorted=False,\n                  axis=-1):\n    \"\"\"\n    Calculate the continuous ranked probability score (CRPS) for a set of\n    explicit forecast realizations.\n\n    The CRPS compares the empirical distribution of an ensemble forecast\n    to a scalar observation. Smaller scores indicate better skill.\n\n    CRPS is defined for one-dimensional random variables with a probability\n    density $p(x)$,\n\n    .. math::\n        CRPS(F, x) = \\int_z (F(z) - H(z - x))^2 dz\n\n    where $F(x) = \\int_{z \\leq x} p(z) dz$ is the cumulative distribution\n    function (CDF) of the forecast distribution $F$ and $H(x)$ denotes the\n    Heaviside step function, where $x$ is a point estimate of the true\n    observation (observational error is neglected).\n\n    This function calculates CRPS efficiently using the empirical CDF:\n    http://en.wikipedia.org/wiki/Empirical_distribution_function\n\n    The Numba accelerated version of this function requires time\n    O(N * E * log(E)) and space O(N * E) where N is the number of observations\n    and E is the size of the forecast ensemble.\n\n    The non-Numba accelerated version much slower for large ensembles: it\n    requires both time and space O(N * E ** 2).\n\n    Parameters\n    ----------\n    observations : float or array_like\n        Observations float or array. Missing values (NaN) are given scores of\n        NaN.\n    forecasts : float or array_like\n        Array of forecasts ensemble members, of the same shape as observations\n        except for the axis along which CRPS is calculated (which should be the\n        axis corresponding to the ensemble). If forecasts has the same shape as\n        observations, the forecasts are treated as deterministic. Missing\n        values (NaN) are ignored.\n    weights : array_like, optional\n        If provided, the CRPS is calculated exactly with the assigned\n        probability weights to each forecast. Weights should be positive, but\n        do not need to be normalized. By default, each forecast is weighted\n        equally.\n    issorted : bool, optional\n        Optimization flag to indicate that the elements of `ensemble` are\n        already sorted along `axis`.\n    axis : int, optional\n        Axis in forecasts and weights which corresponds to different ensemble\n        members, along which to calculate CRPS.\n\n    Returns\n    -------\n    out : np.ndarray\n        CRPS for each ensemble forecast against the observations.\n\n    References\n    ----------\n    Jochen Broecker. Chapter 7 in Forecast Verification: A Practitioner's Guide\n        in Atmospheric Science. John Wiley & Sons, Ltd, Chichester, UK, 2nd\n        edition, 2012.\n    Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules,\n        prediction, and estimation, 2005. University of Washington Department of\n        Statistics Technical Report no. 463R.\n        https://www.stat.washington.edu/research/reports/2004/tr463R.pdf\n    Wilks D.S. (1995) Chapter 8 in _Statistical Methods in the\n        Atmospheric Sciences_. Academic Press.\n    \"\"\"\n    observations = np.asarray(observations)\n    forecasts = np.asarray(forecasts)\n    if axis != -1:\n        forecasts = move_axis_to_end(forecasts, axis)\n\n    if weights is not None:\n        weights = move_axis_to_end(weights, axis)\n        if weights.shape != forecasts.shape:\n            raise ValueError('forecasts and weights must have the same shape')\n\n    if observations.shape not in [forecasts.shape, forecasts.shape[:-1]]:\n        raise ValueError('observations and forecasts must have matching '\n                         'shapes or matching shapes except along `axis=%s`'\n                         % axis)\n\n    if observations.shape == forecasts.shape:\n        if weights is not None:\n            raise ValueError('cannot supply weights unless you also supply '\n                             'an ensemble forecast')\n        return abs(observations - forecasts)\n\n    if not issorted:\n        if weights is None:\n            forecasts = np.sort(forecasts, axis=-1)\n        else:\n            idx = argsort_indices(forecasts, axis=-1)\n            forecasts = forecasts[idx]\n            weights = weights[idx]\n\n    if weights is None:\n        weights = np.ones_like(forecasts)\n\n    return _crps_ensemble_core(observations, forecasts, weights)", "response": "This function calculates the continuous ranked probability score of a set of forecast realizations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, new_points_len):\n        '''Allocate memory for a new run and return a reference to that memory\n        wrapped in an array of size ``(new_points_len, self.dim)``.\n\n        :param new_points_len:\n\n            Integer; the number of points to be stored in the target memory.\n\n        '''\n        new_points_len = int(new_points_len)\n        assert new_points_len >= 1, \"Must at least append one point!\"\n\n        # find out start and stop index of the new memory\n        try:\n            new_points_start = self._slice_for_run_nr[-1][-1]\n        except IndexError:\n            new_points_start = 0\n        new_points_stop  = new_points_start + new_points_len\n\n        # store slice for new_points\n        self._slice_for_run_nr.append( (new_points_start , new_points_stop) )\n\n        if self.memleft < new_points_len: #need to allocate new memory\n            self.memleft = 0\n            #careful: do not use self._points because this may include unused memory\n            self._points  = _np.vstack(  (self[:],_np.empty((new_points_len, self.dim)))  )\n        else: #have enough memory\n            self.memleft -= new_points_len\n\n        # return reference to the new points\n        return self._points[new_points_start:new_points_stop]", "response": "Allocate memory for a new run and return a reference to that memory\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch_data(data, L=100, try_diag=True, verbose=False):\n    '''Patch ``data`` (for example Markov chain output) into parts of\n    length ``L``. Return a Gaussian mixture where each component gets\n    the empirical mean and covariance of one patch.\n\n    :param data:\n\n        Matrix-like array; the points to be patched. Expect ``data[i]``\n        as the d-dimensional i-th point.\n\n    :param L:\n\n        Integer; the length of one patch. The last patch will be shorter\n        if ``L`` is not a divisor of ``len(data)``.\n\n    :param try_diag:\n\n        Bool; If some patch does not define a proper covariance matrix,\n        it cannot define a Gaussian component. ``try_diag`` defines how\n        to handle that case:\n        If ``True`` (default), the off-diagonal elements are set to zero\n        and it is tried to form a Gaussian with that matrix again. If\n        that fails as well, the patch is skipped.\n        If ``False`` the patch is skipped directly.\n\n    :param verbose:\n\n        Bool; If ``True`` print all status information.\n\n    '''\n    # patch data into length L patches\n    patches = _np.array([data[patch_start:patch_start + L] for patch_start in range(0, len(data), L)])\n\n    # calculate means and covs\n    means   = _np.array([_np.mean(patch,   axis=0) for patch in patches])\n    covs    = _np.array([_np.cov (patch, rowvar=0) for patch in patches])\n\n    # form gaussian components\n    components = []\n    skipped = []\n    for i, (mean, cov) in enumerate(zip(means, covs)):\n        try:\n            this_comp = Gauss(mean, cov)\n            components.append(this_comp)\n        except _np.linalg.LinAlgError as error1:\n            if verbose:\n                print(\"Could not form Gauss from patch %i. Reason: %s\" % (i, repr(error1)))\n            if try_diag:\n                cov = _np.diag(_np.diag(cov))\n                try:\n                    this_comp = Gauss(mean, cov)\n                    components.append(this_comp)\n                    if verbose:\n                        print('Diagonal covariance attempt succeeded.')\n                except _np.linalg.LinAlgError as error2:\n                    skipped.append(i)\n                    if verbose:\n                        print(\"Diagonal covariance attempt failed. Reason: %s\" % repr(error2))\n            else: # if not try_diag\n                skipped.append(i)\n\n    # print skipped components if any\n    if skipped:\n        print(\"WARNING: Could not form Gaussians from: %s\" % skipped)\n\n    # create and return mixture\n    return MixtureDensity(components)", "response": "Patch data into parts of a Markov chain output."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndispatching all HTTP methods to the proxy.", "response": "def dispatch(self, request, *args, **kwargs):\n        \"\"\"Dispatch all HTTP methods to the proxy.\"\"\"\n        self.request = DownstreamRequest(request)\n        self.args = args\n        self.kwargs = kwargs\n\n        self._verify_config()\n\n        self.middleware = MiddlewareSet(self.proxy_middleware)\n\n        return self.proxy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the upstream content and build an HttpResponse.", "response": "def proxy(self):\n        \"\"\"Retrieve the upstream content and build an HttpResponse.\"\"\"\n        headers = self.request.headers.filter(self.ignored_request_headers)\n        qs = self.request.query_string if self.pass_query_string else ''\n\n        # Fix for django 1.10.0 bug https://code.djangoproject.com/ticket/27005\n        if (self.request.META.get('CONTENT_LENGTH', None) == '' and\n                get_django_version() == '1.10'):\n            del self.request.META['CONTENT_LENGTH']\n\n        request_kwargs = self.middleware.process_request(\n            self, self.request, method=self.request.method, url=self.proxy_url,\n            headers=headers, data=self.request.body, params=qs,\n            allow_redirects=False, verify=self.verify_ssl, cert=self.cert,\n            timeout=self.timeout)\n\n        result = request(**request_kwargs)\n\n        response = HttpResponse(result.content, status=result.status_code)\n\n        # Attach forwardable headers to response\n        forwardable_headers = HeaderDict(result.headers).filter(\n            self.ignored_upstream_headers)\n        for header, value in iteritems(forwardable_headers):\n            response[header] = value\n\n        return self.middleware.process_response(\n            self, self.request, result, response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shell_out(cmd, stderr=STDOUT, cwd=None):\n    if cwd is None:\n        from os import getcwd\n        cwd = getcwd()  # TODO do I need to normalize this on Windows\n\n    out = check_output(cmd, cwd=cwd, stderr=stderr, universal_newlines=True)\n    return _clean_output(out)", "response": "Friendlier version of check_output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shell_out_ignore_exitcode(cmd, stderr=STDOUT, cwd=None):\n    try:\n        return shell_out(cmd, stderr=stderr, cwd=cwd)\n    except CalledProcessError as c:\n        return _clean_output(c.output)", "response": "Same as shell_out but doesn t raise if the cmd exits badly."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_dir(cwd):\n    \"Context manager to ensure in the cwd directory.\"\n    import os\n    curdir = os.getcwd()\n    try:\n        os.chdir(cwd)\n        yield\n    finally:\n        os.chdir(curdir)", "response": "Context manager to ensure in the cwd directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_function_with_indicator(function, indicator, alternative):\n    '''Returns a function such that a call to it is equivalent to:\n\n    if indicator(x):\n        return function(x)\n    else:\n        return alternative\n\n    Note that ``function`` is not called if indicator evaluates to False.\n\n\n    :param function:\n\n        The function to be called when indicator returns True.\n\n    :param indicator:\n\n        Bool-returning function; the indicator\n\n    :param alternative:\n\n        The object to be returned when indicator returns False\n\n    '''\n    if indicator is None:\n        return function\n    else:\n        def merged_function(x):\n            if indicator(x):\n                return function(x)\n            else:\n                return alternative\n        return merged_function", "response": "Returns a function that merges a function with an indicator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef text_filter(regex_base, value):\n    from thumbnails import get_thumbnail\n    regex = regex_base % {\n        'caption': '[a-zA-Z0-9\\.\\,:;/_ \\(\\)\\-\\!\\?\\\"]+',\n        'image': '[a-zA-Z0-9\\.:/_\\-\\% ]+'\n    }\n    images = re.findall(regex, value)\n\n    for i in images:\n        image_url = i[1]\n        image = get_thumbnail(\n            image_url,\n            **settings.THUMBNAIL_FILTER_OPTIONS\n        )\n        value = value.replace(i[1], image.url)\n\n    return value", "response": "A text - filter helper that uses the regex base to find the source URLs of the text in which the source URLs should be found."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eat(self, argv=None):\n        # The setup phase, here KeyboardInterrupt is a silent sign to exit the\n        # application. Any error that happens here will result in a raw\n        # backtrace being printed to the user.\n        try:\n            self.context.argv = argv\n            self._added()\n            self._build_early_parser()\n            self._preparse()\n            self._early_init()\n            self._build_parser()\n            self._parse()\n            self._late_init()\n        except KeyboardInterrupt:\n            self._shutdown()\n            return\n        # The execution phase. Here we differentiate SystemExit from all other\n        # exceptions. SystemExit is just re-raised as that's what any piece of\n        # code can raise to ask to exit the currently running application.  All\n        # other exceptions are recorded in the context and the failure-path of\n        # the dispatch is followed. In other case, when there are no\n        # exceptions, the success-path is followed. In both cases, ingredients\n        # are shut down.\n        try:\n            return self._dispatch()\n        except SystemExit:\n            raise\n        except BaseException:\n            (self.context.exc_type, self.context.exc_value,\n             self.context.traceback) = sys.exc_info()\n            self._dispatch_failed()\n        else:\n            self._dispatch_succeeded()\n        finally:\n            self._shutdown()", "response": "Eat the guacamole.\n\n        :param argv:\n            Command line arguments or None. None means that sys.argv is used\n        :return:\n            Whatever is returned by the first ingredient that agrees to perform\n            the command dispatch.\n\n        The eat method is called to run the application, as if it was invoked\n        from command line directly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the dispatch method on all ingredients.", "response": "def _dispatch(self):\n        \"\"\"Run the dispatch() method on all ingredients.\"\"\"\n        for ingredient in self.ingredients:\n            result = ingredient.dispatch(self.context)\n            if result is not None:\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, N=1, *args, **kwargs):\n        '''Call the parallelized sampler's ``run`` method. Each process\n        will run for ``N`` iterations. Then, the master process (process with\n        ``rank = 0``) collects the samples and weights from all processes and\n        stores it into ``self.samples_list`` and ``self.weights_list``.\n        Master process:   Return a list of the return values from the workers.\n        Other  processes: Return the same as the sequential sampler.\n\n        .. seealso::\n\n            :py:class:`pypmc.tools.History`\n\n        :param N:\n\n            Integer; the number of steps to be passed to the ``run`` method.\n\n        :param args, kwargs:\n\n            Additional arguments which are passed to the ``sampler_type``'s\n            run method.\n\n\n        '''\n        individual_return = self.sampler.run(N, *args, **kwargs)\n\n        # all workers send samples and weights to master\n        self.samples_list = self._comm.gather(self.sampler.samples, root=0)\n        if hasattr(self.sampler, 'weights'):\n            self.weights_list = self._comm.gather(self.sampler.weights, root=0)\n\n        # master returns list of worker return values\n        master_return = self._comm.gather(individual_return, root=0)\n\n        if self._rank == 0:\n            return master_return\n        else:\n            return individual_return", "response": "Calls the parallelized sampler s run method. Each process with N iterations collects samples and weights from all processes and stores them into self. samples_list and self. weights_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a path based on the location attribute of the backend and the path argument.", "response": "def path(self, path):\n        \"\"\"\n        Creates a path based on the location attribute of the backend and the path argument\n        of the function. If the path argument is an absolute path the path is returned.\n\n        :param path: The path that should be joined with the backends location.\n        \"\"\"\n        if os.path.isabs(path):\n            return path\n        return os.path.join(self.location, path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kullback_leibler(c1, c2):\n    d = c2.log_det_sigma - c1.log_det_sigma\n    d += np.trace(c2.inv_sigma.dot(c1.sigma))\n    mean_diff = c1.mu - c2.mu\n    d += mean_diff.transpose().dot(c2.inv_sigma).dot(mean_diff)\n    d -= len(c1.mu)\n\n    return 0.5 * d", "response": "Kullback Leibler divergence of two Gaussians."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cleanup(self, kill, verbose):\n        if kill:\n            removed_indices = self.g.prune()\n\n            self.nout -= len(removed_indices)\n\n            if verbose and removed_indices:\n                print('Removing %s' % removed_indices)\n\n            for j in removed_indices:\n                self.inv_map.pop(j[0])", "response": "Remove dead components from inv_map."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the distance function d f g \\ pi Eq. ( 3 )", "response": "def _distance(self):\n        \"\"\"Compute the distance function d(f,g,\\pi), Eq. (3)\"\"\"\n        return np.average(self.min_kl, weights=self.f.weights)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _refit(self):\n        # temporary variables for manipulation\n        mu_diff = np.empty_like(self.f.components[0].mu)\n        sigma   = np.empty_like(self.f.components[0].sigma)\n        mean    = np.empty_like(mu_diff)\n        cov     = np.empty_like(sigma)\n\n        for j, c in enumerate(self.g.components):\n            # stop if inv_map is empty for j-th comp.\n            if not self.inv_map[j]:\n                self.g.weights[j] = 0.\n                continue\n\n            # (re-)initialize new mean/cov to zero\n            mean[:] = 0.0\n            cov[:] = 0.0\n\n            # compute total weight and mean\n            self.g.weights[j] = self.f.weights[self.inv_map[j]].sum()\n            for i in self.inv_map[j]:\n                mean += self.f.weights[i] * self.f.components[i].mu\n\n            # rescale by total weight\n            mean /= self.g.weights[j]\n\n            # update covariance\n            for i in self.inv_map[j]:\n                # mu_diff = mu'_j - mu_i\n                mu_diff[:] = mean\n                mu_diff -= self.f.components[i].mu\n\n                # sigma = (mu'_j - mu_i) (mu'_j - mu_i)^T\n                sigma[:] = np.outer(mu_diff, mu_diff)\n\n                # sigma += sigma_i\n                sigma += self.f.components[i].sigma\n\n                # multiply with alpha_i\n                sigma *= self.f.weights[i]\n\n                # sigma_j += alpha_i * (sigma_i + (mu'_j - mu_i) (mu'_j - mu_i)^T\n                cov += sigma\n\n            # 1 / beta_j\n            cov /= self.g.weights[j]\n\n            # update the Mixture\n            c.update(mean, cov)", "response": "Update the map with the new values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _regroup(self):\n        # clean up old maps\n        for j in range(self.nout):\n            self.inv_map[j] = []\n\n        # find smallest divergence between input component i\n        # and output component j of the cluster mixture density\n        for i in range(self.nin):\n            self.min_kl[i] = np.inf\n            j_min = None\n            for j in range(self.nout):\n                kl = kullback_leibler(self.f.components[i], self.g.components[j])\n                if kl < self.min_kl[i]:\n                    self.min_kl[i] = kl\n                    j_min = j\n            assert j_min is not None\n            self.inv_map[j_min].append(i)", "response": "Update the output g keeping the map fixed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eventdata(payload):\n\n    headerinfo, data = payload.split('\\n', 1)\n    headers = get_headers(headerinfo)\n    return headers, data", "response": "Parse a Supervisor event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef formatTime(self, record, datefmt=None):\n\n        formatted = super(PalletFormatter, self).formatTime(\n            record, datefmt=datefmt)\n        return formatted + '.%03dZ' % record.msecs", "response": "Format time including milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract from a udiff an iterator of tuples of start end line numbers.", "response": "def modified_lines_from_udiff(udiff):\n    \"\"\"Extract from a udiff an iterator of tuples of (start, end) line\n    numbers.\"\"\"\n    chunks = re.split('\\n@@ [^\\n]+\\n', udiff)[1:]\n\n    line_numbers = re.findall('@@\\s[+-]\\d+,\\d+ \\+(\\d+)', udiff)\n    line_numbers = list(map(int, line_numbers))\n\n    for c, start in zip(chunks, line_numbers):\n        ilines = enumerate((line for line in c.splitlines()\n                            if not line.startswith('-')),\n                           start=start)\n        added_lines = [i for i, line in ilines if line.startswith('+')]\n        if added_lines:\n            yield (added_lines[0], added_lines[-1])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns text of unified diff between original and fixed.", "response": "def get_diff(original, fixed, file_name,\n             original_label='original', fixed_label='fixed'):\n    \"\"\"Return text of unified diff between original and fixed.\"\"\"\n    original, fixed = original.splitlines(True), fixed.splitlines(True)\n    newline = '\\n'\n\n    from difflib import unified_diff\n    diff = unified_diff(original, fixed,\n                        os.path.join(original_label, file_name),\n                        os.path.join(fixed_label, file_name),\n                        lineterm=newline)\n    text = ''\n    for line in diff:\n        text += line\n        # Work around missing newline (http://bugs.python.org/issue2142).\n        if not line.endswith(newline):\n            text += newline + r'\\ No newline at end of file' + newline\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_diff(diff, color=True):\n    import colorama\n\n    if not diff:\n        return\n\n    if not color:\n        colorama.init = lambda autoreset: None\n        colorama.Fore.RED = ''\n        colorama.Back.RED = ''\n        colorama.Fore.GREEN = ''\n        colorama.deinit = lambda: None\n\n    colorama.init(autoreset=True)  # TODO use context_manager\n    for line in diff.splitlines():\n        if line.startswith('+') and not line.startswith('+++ '):\n            # Note there shouldn't be trailing whitespace\n            # but may be nice to generalise this\n            print(colorama.Fore.GREEN + line)\n        elif line.startswith('-') and not line.startswith('--- '):\n            split_whitespace = re.split('(\\s+)$', line)\n            if len(split_whitespace) > 1:  # claim it must be 3\n                line, trailing, _ = split_whitespace\n            else:\n                line, trailing = split_whitespace[0], ''\n            print(colorama.Fore.RED + line, end='')\n            # give trailing whitespace a RED background\n            print(colorama.Back.RED + trailing)\n        elif line == '\\ No newline at end of file':\n            # The assumption here is that there is now a new line...\n            print(colorama.Fore.RED + line)\n        else:\n            print(line)\n    colorama.deinit()", "response": "Pretty printing for a diff."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, N=1):\n        '''Run the chain and store the history of visited points into\n        the member variable ``self.samples``. Returns the number of\n        accepted points during the run.\n\n        .. seealso::\n            :py:class:`pypmc.tools.History`\n\n        :param N:\n\n            An int which defines the number of steps to run the chain.\n\n        '''\n        if N == 0:\n            return 0\n\n        # set the accept function\n        if self.proposal.symmetric:\n            get_log_rho = self._get_log_rho_metropolis\n        else:\n            get_log_rho = self._get_log_rho_metropolis_hastings\n\n        # allocate an empty numpy array to store the run\n        if self.target_values is not None:\n            this_target_values = self.target_values.append(N)\n        this_run     = self.samples.append(N)\n        accept_count = 0\n\n        for i_N in range(N):\n            # propose new point\n            proposed_point = self.proposal.propose(self.current_point, self.rng)\n            proposed_eval  = self.target(proposed_point)\n\n            # log_rho := log(probability to accept point), where log_rho > 0 is meant to imply rho = 1\n            log_rho = get_log_rho(proposed_point, proposed_eval)\n\n            # check for NaN\n            if _np.isnan(log_rho): raise ValueError('encountered NaN')\n\n            # accept if rho = 1\n            if log_rho >=0:\n                accept_count += 1\n                this_run[i_N]            = proposed_point\n                self.current_point       = proposed_point\n                self.current_target_eval = proposed_eval\n\n            # accept with probability rho\n            elif log_rho >= _np.log(self.rng.rand()):\n                accept_count += 1\n                this_run[i_N]            = proposed_point\n                self.current_point       = proposed_point\n                self.current_target_eval = proposed_eval\n\n            # reject if not accepted\n            else:\n                this_run[i_N] = self.current_point\n                #do not need to update self.current\n                #self.current = self.current\n\n            # save target value if desired\n            if self.target_values is not None:\n                this_target_values[i_N] = self.current_target_eval\n\n        # ---------------------- end for --------------------------------\n\n        return accept_count", "response": "Run the chain and store the history of visited points into\n            the member variable self. samples. Returns the number of accepted points during the run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate log ( rho metropolis ratio times hastings factor", "response": "def _get_log_rho_metropolis_hastings(self, proposed_point, proposed_eval):\n        \"\"\"calculate log(metropolis ratio times hastings factor)\"\"\"\n        return self._get_log_rho_metropolis(proposed_point, proposed_eval)\\\n             - self.proposal.evaluate      (proposed_point, self.current) \\\n             + self.proposal.evaluate      (self.current, proposed_point)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for create method with cleanup.", "response": "def get_thumbnail(self, original, size, crop, options):\n        \"\"\"\n        Wrapper for .create() with cleanup.\n\n        :param original:\n        :param size:\n        :param crop:\n        :param options:\n        :return: An image object\n        \"\"\"\n        try:\n            image = self.create(original, size, crop, options)\n        except ThumbnailError:\n            image = None\n        finally:\n            self.cleanup(original)\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a thumbnail. It loads the image scales it and crops it.", "response": "def create(self, original, size, crop, options=None):\n        \"\"\"\n        Creates a thumbnail. It loads the image, scales it and crops it.\n\n        :param original:\n        :param size:\n        :param crop:\n        :param options:\n        :return:\n        \"\"\"\n        if options is None:\n            options = self.evaluate_options()\n        image = self.engine_load_image(original)\n        image = self.scale(image, size, crop, options)\n        crop = self.parse_crop(crop, self.get_image_size(image), size)\n        image = self.crop(image, size, crop, options)\n        image = self.colormode(image, options)\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for engine_scale that checks if the scaling factor is below one and calls engine_scale with the image size and crop options.", "response": "def scale(self, image, size, crop, options):\n        \"\"\"\n        Wrapper for ``engine_scale``, checks if the scaling factor is below one or that scale_up\n        option is set to True before calling ``engine_scale``.\n\n        :param image:\n        :param size:\n        :param crop:\n        :param options:\n        :return:\n        \"\"\"\n        original_size = self.get_image_size(image)\n        factor = self._calculate_scaling_factor(original_size, size, crop is not None)\n\n        if factor < 1 or options['scale_up']:\n            width = int(original_size[0] * factor)\n            height = int(original_size[1] * factor)\n            image = self.engine_scale(image, width, height)\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crop(self, image, size, crop, options):\n        if not crop:\n            return image\n        return self.engine_crop(image, size, crop, options)", "response": "Wrapper for engine_crop which will return without calling engine_crop"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps for ``engine_colormode``. :param image: :param options: :return:", "response": "def colormode(self, image, options):\n        \"\"\"\n        Wrapper for ``engine_colormode``.\n\n        :param image:\n        :param options:\n        :return:\n        \"\"\"\n        mode = options['colormode']\n        return self.engine_colormode(image, mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a size string into a tuple of two integers for width and height.", "response": "def parse_size(size):\n        \"\"\"\n        Parses size string into a tuple\n\n        :param size: String on the form '100', 'x100 or '100x200'\n        :return: Tuple of two integers for width and height\n        :rtype: tuple\n        \"\"\"\n        if size.startswith('x'):\n            return None, int(size.replace('x', ''))\n        if 'x' in size:\n            return int(size.split('x')[0]), int(size.split('x')[1])\n        return int(size), None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the crop string into a tuple usable by the crop function.", "response": "def parse_crop(self, crop, original_size, size):\n        \"\"\"\n        Parses crop into a tuple usable by the crop function.\n\n        :param crop: String with the crop settings.\n        :param original_size: A tuple of size of the image that should be cropped.\n        :param size: A tuple of the wanted size.\n        :return: Tuple of two integers with crop settings\n        :rtype: tuple\n        \"\"\"\n        if crop is None:\n            return None\n\n        crop = crop.split(' ')\n        if len(crop) == 1:\n            crop = crop[0]\n            x_crop = 50\n            y_crop = 50\n            if crop in CROP_ALIASES['x']:\n                x_crop = CROP_ALIASES['x'][crop]\n            elif crop in CROP_ALIASES['y']:\n                y_crop = CROP_ALIASES['y'][crop]\n\n        x_offset = self.calculate_offset(x_crop, original_size[0], size[0])\n        y_offset = self.calculate_offset(y_crop, original_size[1], size[1])\n        return int(x_offset), int(y_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the crop offset based on percentage.", "response": "def calculate_offset(percent, original_length, length):\n        \"\"\"\n        Calculates crop offset based on percentage.\n\n        :param percent: A percentage representing the size of the offset.\n        :param original_length: The length the distance that should be cropped.\n        :param length: The desired length.\n        :return: The offset in pixels\n        :rtype: int\n        \"\"\"\n        return int(\n            max(\n                0,\n                min(percent * original_length / 100.0, original_length - length / 2) - length / 2)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_app_template_dir(app_name):\n    if app_name in _cache:\n        return _cache[app_name]\n    template_dir = None\n    for app in settings.INSTALLED_APPS:\n        if app.split('.')[-1] == app_name:\n            # Do not hide import errors; these should never happen at this\n            # point anyway\n            mod = import_module(app)\n            template_dir = join(abspath(dirname(mod.__file__)), 'templates')\n            break\n    _cache[app_name] = template_dir\n    return template_dir", "response": "Get the template directory for an application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_template_sources(self, template_name, template_dirs=None):\n        if ':' not in template_name:\n            return []\n        app_name, template_name = template_name.split(\":\", 1)\n        template_dir = get_app_template_dir(app_name)\n        if template_dir:\n            return [get_template_path(template_dir, template_name, self)]\n        return []", "response": "Return the absolute paths to template_name in the specified app."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_arguments():\n\n    parser = argparse.ArgumentParser(prog=sys.argv[0],\n                                     description='Send Webhooks Channel events to IFTTT',\n                                     epilog='Visit https://ifttt.com/channels/maker_webhooks for more information')\n    parser.add_argument('--version', action='version', version=pyfttt.__version__)\n\n    sgroup = parser.add_argument_group(title='sending events')\n    sgroup.add_argument('-k', '--key', metavar='K',\n                        default=os.environ.get('IFTTT_API_KEY'),\n                        help='IFTTT secret key')\n    sgroup.add_argument('-e', '--event', metavar='E', required=True,\n                        help='The name of the event to trigger')\n    sgroup.add_argument('value1', nargs='?',\n                        help='Extra data sent with the event (optional)')\n    sgroup.add_argument('value2', nargs='?',\n                        help='Extra data sent with the event (optional)')\n    sgroup.add_argument('value3', nargs='?',\n                        help='Extra data sent with the event (optional)')\n\n    return parser.parse_args()", "response": "Parse command line arguments and return a list of arguments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the components of the mixture density as a single - sigma ellipse.", "response": "def plot_mixture(mixture, i=0, j=1, center_style=dict(s=0.15),\n                 cmap='nipy_spectral', cutoff=0.0, ellipse_style=dict(alpha=0.3),\n                 solid_edge=True, visualize_weights=False):\n    '''Plot the (Gaussian) components of the ``mixture`` density as\n    one-sigma ellipses in the ``(i,j)`` plane.\n\n    :param center_style:\n        If a non-empty ``dict``, plot mean value with the style passed to ``scatter``.\n\n    :param cmap:\n\n        The color map to which components are mapped in order to\n        choose their face color. It is unaffected by the\n        ``cutoff``. The meaning depends on ``visualize_weights``.\n\n    :param cutoff:\n        Ignore components whose weight is below the ``cut off``.\n\n    :param ellipse_style:\n        Passed on to define the properties of the ``Ellipse``.\n\n    :param solid_edge:\n        Draw the edge of the ellipse as solid opaque line.\n\n    :param visualize_weights:\n        Colorize the components according to their weights if ``True``.\n        One can do `plt.colorbar()` after this function and the bar allows to read off the weights.\n        If ``False``, coloring is based on the component index and the total number of components.\n        This option makes it easier to track components by assigning them the same color in subsequent calls to this function.\n\n    '''\n    # imports inside the function because then \"ImportError\" is raised on\n    # systems without 'matplotlib' only when 'plot_mixture' is called\n    import numpy as np\n    from matplotlib import pyplot as plt\n    from matplotlib.patches import Ellipse\n    from matplotlib.cm import get_cmap\n\n    assert i >= 0 and j >= 0, 'Invalid submatrix specification (%d, %d)' % (i, j)\n    assert i != j, 'Identical dimension given: i=j=%d' % i\n    assert mixture.dim >= 2, '1D plot not supported'\n\n    cmap = get_cmap(name=cmap)\n\n    if visualize_weights:\n        # colors according to weight\n        renormalized_component_weights  = np.array(mixture.weights)\n        colors = [cmap(k) for k in renormalized_component_weights]\n    else:\n        # colors according to index\n        colors = [cmap(k) for k in np.linspace(0, _max_color, len(mixture.components))]\n\n    mask = mixture.weights >= cutoff\n\n    # plot component means\n    means = np.array([c.mu for c in mixture.components])\n    x_values = means.T[i]\n    y_values = means.T[j]\n\n    for k, w in enumerate(mixture.weights):\n        # skip components by hand to retain consistent coloring\n        if w < cutoff:\n            continue\n\n        cov = mixture.components[k].sigma\n        submatrix = np.array([[cov[i,i], cov[i,j]], \\\n                              [cov[j,i], cov[j,j]]])\n\n        # for idea, check\n        # 'Combining error ellipses' by John E. Davis\n        correlation = np.array([[1.0, cov[i,j] / np.sqrt(cov[i,i] * cov[j,j])], [0.0, 1.0]])\n        correlation[1,0] = correlation[0,1]\n\n        assert abs(correlation[0,1]) <= 1, 'Invalid component %d with correlation %g' % (k, correlation[0, 1])\n\n        ew, ev = np.linalg.eigh(submatrix)\n        assert ew.min() > 0, 'Nonpositive eigenvalue in component %d: %s' % (k, ew)\n\n        # rotation angle of major axis with x-axis\n        if submatrix[0,0] == submatrix[1,1]:\n            theta = np.sign(submatrix[0,1]) * np.pi / 4.\n        else:\n            theta = 0.5 * np.arctan( 2 * submatrix[0,1] / (submatrix[1,1] - submatrix[0,0]))\n\n        # put larger eigen value on y'-axis\n        height = np.sqrt(ew.max())\n        width = np.sqrt(ew.min())\n\n        # but change orientation of coordinates if the other is larger\n        if submatrix[0,0] > submatrix[1,1]:\n            height = np.sqrt(ew.min())\n            width = np.sqrt(ew.max())\n\n        # change sign to rotate in right direction\n        angle = -theta * 180 / np.pi\n\n        # copy keywords but override some\n        ellipse_style_clone = dict(ellipse_style)\n\n        # overwrite facecolor\n        ellipse_style_clone['facecolor'] = colors[k]\n\n        ax = plt.gca()\n\n        # need full width/height\n        e = Ellipse(xy=(x_values[k], y_values[k]),\n                                   width=2*width, height=2*height, angle=angle,\n                                   **ellipse_style_clone)\n        ax.add_patch(e)\n\n        if solid_edge:\n            ellipse_style_clone['facecolor'] = 'none'\n            ellipse_style_clone['edgecolor'] = colors[k]\n            ellipse_style_clone['alpha'] = 1\n            ax.add_patch(Ellipse(xy=(x_values[k], y_values[k]),\n                                       width=2*width, height=2*height, angle=angle,\n                                       **ellipse_style_clone))\n\n    if center_style:\n        plt.scatter(x_values[mask], y_values[mask], **center_style)\n\n    if visualize_weights:\n        # to enable plt.colorbar()\n        mappable = plt.gci()\n        mappable.set_array(mixture.weights)\n        mappable.set_cmap(cmap)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclassifying the 2D data according to the responsibility and plot the most likely component of the data.", "response": "def plot_responsibility(data, responsibility,\n                        cmap='nipy_spectral'):\n    '''Classify the 2D ``data`` according to the ``responsibility`` and\n    make a scatter plot of each data point with the color of the\n    component it is most likely from. The ``responsibility`` is\n    normalized internally such that each row sums to unity.\n\n    :param data:\n\n        matrix-like; one row = one 2D sample\n\n    :param responsibility:\n\n        matrix-like; one row = probabilities that sample n is from\n        1st, 2nd, ... component. The number of rows has to agree with ``data``\n\n    :param cmap:\n\n        colormap; defines how component indices are mapped to the\n        color of the data points\n\n    '''\n    import numpy as np\n    from matplotlib import pyplot as plt\n    from matplotlib.cm import get_cmap\n\n    data = np.asarray(data)\n    responsibility = np.asarray(responsibility)\n\n    assert data.ndim == 2\n    assert responsibility.ndim == 2\n\n    D = data.shape[1]\n    N = data.shape[0]\n    K = responsibility.shape[1]\n\n    assert D == 2\n    assert N == responsibility.shape[0]\n\n    # normalize responsibility so each row sums to one\n    inv_row_sum = 1.0 / np.einsum('nk->n', responsibility)\n    responsibility = np.einsum('n,nk->nk', inv_row_sum, responsibility)\n\n    # index of the most likely component for each sample\n    indicators = np.argmax(responsibility, axis=1)\n\n    # same color range as in plot_mixture\n    if K > 1:\n        point_colors = indicators / (K - 1) * _max_color\n    else:\n        point_colors = np.zeros(N)\n    plt.scatter(data.T[0], data.T[1], c=point_colors, cmap=cmap)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting a dotted module path.", "response": "def import_string(dotted_path):\n    \"\"\"\n    Import a dotted module path.\n\n    Returns the attribute/class designated by the last name in the path.\n\n    Raises ImportError if the import fails.\n\n    \"\"\"\n    try:\n        module_path, class_name = dotted_path.rsplit('.', 1)\n    except ValueError:\n        raise ImportError('%s doesn\\'t look like a valid path' % dotted_path)\n\n    module = __import__(module_path, fromlist=[class_name])\n\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        msg = 'Module \"%s\" does not define a \"%s\" attribute/class' % (\n            dotted_path, class_name)\n        raise ImportError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the indicator function of a ball.", "response": "def ball(center, radius=1., bdy=True):\n    '''Returns the indicator function of a ball.\n\n    :param center:\n\n        A vector-like numpy array, defining the center of the ball.\\n\n        len(center) fixes the dimension.\n\n    :param radius:\n\n        Float or int, the radius of the ball\n\n    :param bdy:\n\n        Bool, When ``x`` is at the ball's boundary then\n        ``ball_indicator(x)`` returns ``True`` if and only if\n        ``bdy=True``.\n\n    '''\n    center = _np.array(center) # copy input parameter\n    dim = len(center)\n\n    if bdy:\n        def ball_indicator(x):\n            if len(x) != dim:\n                raise ValueError('input has wrong dimension (%i instead of %i)' % (len(x), dim))\n            if _np.linalg.norm(x - center) <= radius:\n                return True\n            return False\n    else:\n        def ball_indicator(x):\n            if len(x) != dim:\n                raise ValueError('input has wrong dimension (%i instead of %i)' % (len(x), dim))\n            if _np.linalg.norm(x - center) < radius:\n                return True\n            return False\n\n    # write docstring for ball_indicator\n    ball_indicator.__doc__  = 'automatically generated ball indicator function:'\n    ball_indicator.__doc__ += '\\ncenter = ' + repr(center)[6:-1]\n    ball_indicator.__doc__ += '\\nradius = ' + str(radius)\n    ball_indicator.__doc__ += '\\nbdy    = ' + str(bdy)\n\n    return ball_indicator"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the indicator function of a hyperrectangle.", "response": "def hyperrectangle(lower, upper, bdy=True):\n    '''Returns the indicator function of a hyperrectangle.\n\n    :param lower:\n\n        Vector-like numpy array, defining the lower boundary of the hyperrectangle.\\n\n        len(lower) fixes the dimension.\n\n    :param upper:\n\n        Vector-like numpy array, defining the upper boundary of the hyperrectangle.\\n\n\n    :param bdy:\n\n        Bool. When ``x`` is at the hyperrectangles's boundary then\n        ``hr_indicator(x)`` returns ``True`` if and only if ``bdy=True``.\n\n    '''\n    # copy input\n    lower = _np.array(lower)\n    upper = _np.array(upper)\n    dim = len(lower)\n    if (upper <= lower).any():\n        raise ValueError('invalid input; found upper <= lower')\n\n    if bdy:\n        def hr_indicator(x):\n            if len(x) != dim:\n                raise ValueError('input has wrong dimension (%i instead of %i)' % (len(x), dim))\n            if (lower <= x).all() and (x <= upper).all():\n                return True\n            return False\n    else:\n        def hr_indicator(x):\n            if len(x) != dim:\n                raise ValueError('input has wrong dimension (%i instead of %i)' % (len(x), dim))\n            if (lower < x).all() and (x < upper).all():\n                return True\n            return False\n\n    # write docstring for ball_indicator\n    hr_indicator.__doc__  = 'automatically generated hyperrectangle indicator function:'\n    hr_indicator.__doc__ += '\\nlower = ' + repr(lower)[6:-1]\n    hr_indicator.__doc__ += '\\nupper = ' + repr(upper)[6:-1]\n    hr_indicator.__doc__ += '\\nbdy   = ' + str(bdy)\n\n    return hr_indicator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates or gets a Thumbnail object for the given image with the given size and options.", "response": "def get_thumbnail(original, size, **options):\n    \"\"\"\n    Creates or gets an already created thumbnail for the given image with the given size and\n    options.\n\n    :param original: File-path, url or base64-encoded string of the image that you want an\n                     thumbnail.\n    :param size: String with the wanted thumbnail size. On the form: ``200x200``, ``200`` or\n                 ``x200``.\n\n    :param crop: Crop settings, should be ``center``, ``top``, ``right``, ``bottom``, ``left``.\n    :param force: If set to ``True`` the thumbnail will be created even if it exists before.\n    :param quality: Overrides ``THUMBNAIL_QUALITY``, will set the quality used by the backend while\n                    saving the thumbnail.\n    :param scale_up: Overrides ``THUMBNAIL_SCALE_UP``, if set to ``True`` the image will be scaled\n                     up if necessary.\n    :param colormode: Overrides ``THUMBNAIL_COLORMODE``, The default colormode for thumbnails.\n                      Supports all values supported by pillow. In other engines there is a best\n                      effort translation from pillow modes to the modes supported by the current\n                      engine.\n    :param format: Overrides the format the thumbnail will be saved in. This will override both the\n                   detected file type as well as the one specified in ``THUMBNAIL_FALLBACK_FORMAT``.\n    :return: A Thumbnail object\n    \"\"\"\n\n    engine = get_engine()\n    cache = get_cache_backend()\n    original = SourceFile(original)\n    crop = options.get('crop', None)\n    options = engine.evaluate_options(options)\n    thumbnail_name = generate_filename(original, size, crop)\n\n    if settings.THUMBNAIL_DUMMY:\n        engine = DummyEngine()\n        return engine.get_thumbnail(thumbnail_name, engine.parse_size(size), crop, options)\n\n    cached = cache.get(thumbnail_name)\n\n    force = options is not None and 'force' in options and options['force']\n    if not force and cached:\n        return cached\n\n    thumbnail = Thumbnail(thumbnail_name, engine.get_format(original, options))\n    if force or not thumbnail.exists:\n        size = engine.parse_size(size)\n        thumbnail.image = engine.get_thumbnail(original, size, crop, options)\n        thumbnail.save(options)\n\n        for resolution in settings.THUMBNAIL_ALTERNATIVE_RESOLUTIONS:\n            resolution_size = engine.calculate_alternative_resolution_size(resolution, size)\n            image = engine.get_thumbnail(original, resolution_size, crop, options)\n            thumbnail.save_alternative_resolution(resolution, image, options)\n\n    cache.set(thumbnail)\n    return thumbnail"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes argsort but returns an index suitable for sorting the original array even if that array is multidimensional", "response": "def argsort_indices(a, axis=-1):\n    \"\"\"Like argsort, but returns an index suitable for sorting the\n    the original array even if that array is multidimensional\n    \"\"\"\n    a = np.asarray(a)\n    ind = list(np.ix_(*[np.arange(d) for d in a.shape]))\n    ind[axis] = a.argsort(axis)\n    return tuple(ind)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending an event to the IFTTT maker channel", "response": "def send_event(api_key, event, value1=None, value2=None, value3=None):\n    \"\"\"Send an event to the IFTTT maker channel\n\n    Parameters:\n    -----------\n    api_key : string\n        Your IFTTT API key\n    event : string\n        The name of the IFTTT event to trigger\n    value1 :\n        Optional: Extra data sent with the event (default: None)\n    value2 :\n        Optional: Extra data sent with the event (default: None)\n    value3 :\n        Optional: Extra data sent with the event (default: None)\n\n    \"\"\"\n\n    url = 'https://maker.ifttt.com/trigger/{e}/with/key/{k}/'.format(e=event,\n                                                                     k=api_key)\n    payload = {'value1': value1, 'value2': value2, 'value3': value3}\n    return requests.post(url, data=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a cleaned - up localized copy of docstring of this class.", "response": "def get_localized_docstring(obj, domain):\n    \"\"\"Get a cleaned-up, localized copy of docstring of this class.\"\"\"\n    if obj.__class__.__doc__ is not None:\n        return inspect.cleandoc(\n            gettext.dgettext(domain, obj.__class__.__doc__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cmd_help(self):\n        try:\n            return self.help\n        except AttributeError:\n            pass\n        try:\n            return get_localized_docstring(\n                self, self.get_gettext_domain()\n            ).splitlines()[0].rstrip('.').lower()\n        except (AttributeError, IndexError, ValueError):\n            pass", "response": "Returns the help string of this command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cmd_description(self):\n        try:\n            return self.description\n        except AttributeError:\n            pass\n        try:\n            return '\\n'.join(\n                get_localized_docstring(\n                    self, self.get_gettext_domain()\n                ).splitlines()[1:]\n            ).split('@EPILOG@', 1)[0].strip()\n        except (AttributeError, IndexError, ValueError):\n            pass", "response": "Get the leading multi - line description of this command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cmd_epilog(self):\n        try:\n            return self.source.epilog\n        except AttributeError:\n            pass\n        try:\n            return '\\n'.join(\n                get_localized_docstring(\n                    self, self.get_gettext_domain()\n                ).splitlines()[1:]\n            ).split('@EPILOG@', 1)[1].strip()\n        except (AttributeError, IndexError, ValueError):\n            pass", "response": "Returns the epilog of the command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ingredients(self):\n        return [\n            cmdtree.CommandTreeBuilder(self.command),\n            cmdtree.CommandTreeDispatcher(),\n            argparse.AutocompleteIngredient(),\n            argparse.ParserIngredient(),\n            crash.VerboseCrashHandler(),\n            ansi.ANSIIngredient(),\n            log.Logging(),\n        ]", "response": "Get a list of ingredients for guacamole."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_arguments(self, parser):\n        parser.add_argument('x', type=int, help='the first value')\n        parser.add_argument('y', type=int, help='the second value')", "response": "Register the arguments for the\nTaxonomy command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invoked(self, ctx):\n        print(\"{} + {} = {}\".format(\n            ctx.args.x,\n            ctx.args.y,\n            ctx.args.x + ctx.args.y))", "response": "This method is called by the command line interface when all the command line arguments are parsed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hsv(h, s, v):\n    if 360 < h < 0:\n        raise ValueError(\"h out of range: {}\".format(h))\n    if 1 < s < 0:\n        raise ValueError(\"s out of range: {}\".format(h))\n    if 1 < v < 0:\n        raise ValueError(\"v out of range: {}\".format(h))\n    c = v * s  # chroma\n    h1 = h / 60\n    x = c * (1 - abs(h1 % 2 - 1))\n    if 0 <= h1 < 1:\n        r1, g1, b1 = (c, x, 0)\n    elif 1 <= h1 < 2:\n        r1, g1, b1 = (x, c, 0)\n    elif 2 <= h1 < 3:\n        r1, g1, b1 = (0, c, x)\n    elif 3 <= h1 < 4:\n        r1, g1, b1 = (0, x, c)\n    elif 4 <= h1 < 5:\n        r1, g1, b1 = (x, 0, c)\n    elif 5 <= h1 < 6:\n        r1, g1, b1 = (c, 0, x)\n    m = v - c\n    r, g, b = r1 + m, g1 + m, b1 + m\n    return int(r * 255), int(g * 255), int(b * 255)", "response": "Convert HSV to RGB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap for _get which converts the thumbnail_name to String before calling _get", "response": "def get(self, thumbnail_name):\n        \"\"\"\n        Wrapper for ``_get``, which converts the thumbnail_name to String if necessary before\n        calling ``_get``\n\n        :rtype: Thumbnail\n        \"\"\"\n        if isinstance(thumbnail_name, list):\n            thumbnail_name = '/'.join(thumbnail_name)\n        return self._get(thumbnail_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_settings(self):\n        host = '127.0.0.1'\n        port = 6379\n        db = 0\n        if self.connection_uri is not None:\n            re_connection_uri = r'redis://(?:([\\w]+)@)?([\\w\\d\\.]+):(\\d+)(?:/(\\d+))?'\n            match = re.match(re_connection_uri, self.connection_uri)\n            if match:\n                if match.group(2):\n                    host = match.group(2)\n                if match.group(3):\n                    port = int(match.group(3))\n                if match.group(4):\n                    db = int(match.group(4))\n\n        return {\n            'host': host,\n            'port': port,\n            'db': db\n        }", "response": "This function returns the settings dictionary for the redis client constructor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_environ(name, parse_class=ParseResult, **defaults):\n    return parse(os.environ[name], parse_class, **defaults)", "response": "same as parse but you pass in an environment variable name that will be used\n    to fetch the dsn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a dsn to parts similar to parseurl", "response": "def parse(dsn, parse_class=ParseResult, **defaults):\n    \"\"\"\n    parse a dsn to parts similar to parseurl\n\n    :param dsn: string, the dsn to parse\n    :param parse_class: ParseResult, the class that will be used to hold parsed values\n    :param **defaults: dict, any values you want to have defaults for if they aren't in the dsn\n    :returns: ParseResult() tuple-like instance\n    \"\"\"\n    r = parse_class(dsn, **defaults)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning username password hostloc port", "response": "def netloc(self):\n        \"\"\"return username:password@hostname:port\"\"\"\n        s = ''\n        prefix = ''\n        if self.username:\n            s += self.username\n            prefix = '@'\n\n        if self.password:\n            s += \":{password}\".format(password=self.password)\n            prefix = '@'\n\n        s += \"{prefix}{hostloc}\".format(prefix=prefix, hostloc=self.hostloc)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a default value for key", "response": "def setdefault(self, key, val):\n        \"\"\"\n        set a default value for key\n\n        this is different than dict's setdefault because it will set default either\n        if the key doesn't exist, or if the value at the key evaluates to False, so\n        an empty string or a None value will also be updated\n\n        :param key: string, the attribute to update\n        :param val: mixed, the attributes new value if key has a current value\n            that evaluates to False\n        \"\"\"\n        if not getattr(self, key, None):\n            setattr(self, key, val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geturl(self):\n        return urlparse.urlunparse((\n            self.scheme,\n            self.netloc,\n            self.path,\n            self.params,\n            self.query_str,\n            self.fragment,\n        ))", "response": "return the dsn back into url form"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing command line arguments with the early parser.", "response": "def preparse(self, context):\n        \"\"\"\n        Parse a portion of command line arguments with the early parser.\n\n        This method relies on ``context.argv`` and ``context.early_parser``\n        and produces ``context.early_args``.\n\n        The ``context.early_args`` object is the return value from argparse.\n        It is the dict/object like namespace object.\n        \"\"\"\n        context.early_args, unused = (\n            context.early_parser.parse_known_args(context.argv))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the argument parser for the available entry - point.", "response": "def build_parser(self, context):\n        \"\"\"\n        Create the final argument parser.\n\n        This method creates the non-early (full) argparse argument parser.\n        Unlike the early counterpart it is expected to have knowledge of\n        the full command tree.\n\n        This method relies on ``context.cmd_tree`` and produces\n        ``context.parser``. Other ingredients can interact with the parser\n        up until :meth:`parse()` is called.\n        \"\"\"\n        context.parser, context.max_level = self._create_parser(context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses command line arguments.", "response": "def parse(self, context):\n        \"\"\"\n        Parse command line arguments.\n\n        This method relies on ``context.argv`` and ``context.early_parser``\n        and produces ``context.args``. Note that ``.argv`` is modified by\n        :meth:`preparse()` so it actually has _less_ things in it.\n\n        The ``context.args`` object is the return value from argparse.\n        It is the dict/object like namespace object.\n        \"\"\"\n        context.args = context.parser.parse_args(context.argv)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the current entry point into a set of items.", "response": "def parse(self, context):\n        \"\"\"\n        Optionally trigger argument completion in the invoking shell.\n\n        This method is called to see if bash argument completion is requested\n        and to honor the request, if needed. This causes the process to exit\n        (early) without giving other ingredients a chance to initialize or shut\n        down.\n\n        Due to the way argcomple works, no other ingredient can print()\n        anything to stdout prior to this point.\n        \"\"\"\n        try:\n            import argcomplete\n        except ImportError:\n            return\n        try:\n            parser = context.parser\n        except AttributeError:\n            raise RecipeError(\n                \"\"\"\n                The context doesn't have the parser attribute.\n\n                The auto-complete ingredient depends on having a parser object\n                to generate completion data for she shell.  In a typical\n                application this requires that the AutocompleteIngredient and\n                ParserIngredient are present and that the auto-complete\n                ingredient precedes the parser.\n                \"\"\")\n        else:\n            argcomplete.autocomplete(parser)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting ANSI command code by name.", "response": "def ansi_cmd(cmd, *args):\n    \"\"\"Get ANSI command code by name.\"\"\"\n    try:\n        obj = getattr(ANSI, str('cmd_{}'.format(cmd)))\n    except AttributeError:\n        raise ValueError(\n            \"incorrect command: {!r}\".format(cmd))\n    if isinstance(obj, type(\"\")):\n        return obj\n    else:\n        return obj(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the visible counter - color.", "response": "def get_visible_color(color):\n    \"\"\"Get the visible counter-color.\"\"\"\n    if isinstance(color, (str, type(\"\"))):\n        try:\n            return getattr(_Visible, str('{}'.format(color)))\n        except AttributeError:\n            raise ValueError(\"incorrect color: {!r}\".format(color))\n    elif isinstance(color, tuple):\n        return (0x80 ^ color[0], 0x80 ^ color[1], 0x80 ^ color[2])\n    elif isinstance(color, int):\n        if 0 <= color <= 0x07:\n            index = color\n            return 0xFF if index == 0 else 0xE8\n        elif 0x08 <= color <= 0x0F:\n            index = color - 0x08\n            return 0xFF if index == 0 else 0xE8\n        elif 0x10 <= color <= 0xE7:\n            index = color - 0x10\n            if 0 <= index % 36 < 18:\n                return 0xFF\n            else:\n                return 0x10\n        elif 0xE8 <= color <= 0xFF:\n            index = color - 0x0E8\n            return 0xFF if 0 <= index < 12 else 0xE8\n    else:\n        raise ValueError(\"incorrect color: {!r}\".format(color))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ansi_sgr(text, fg=None, bg=None, style=None, reset=True, **sgr):\n    # Ensure that text is really a string\n    text = type(\"\")(text)\n    # NOTE: SGR stands for \"set graphics rendition\"\n    sgr_list = []  # List of SGR codes\n    # Load SGR code associated with desired foreground color\n    if isinstance(fg, (str, type(\"\"))):\n        try:\n            sgr_code = getattr(ANSI, str('sgr_fg_{}'.format(fg)))\n        except AttributeError:\n            raise ValueError(\"incorrect foreground color: {!r}\".format(fg))\n        else:\n            sgr_list.append(sgr_code)\n    elif isinstance(fg, tuple):\n        sgr_code = ANSI.sgr_fg_rgb(*fg)\n        sgr_list.append(sgr_code)\n    elif isinstance(fg, int):\n        if fg < -8:\n            assert fg in range(-16, -8), fg\n            # -16 to -9: bright colors\n            sgr_code = ANSI.sgr_fg_bright(-fg - 8 - 1)\n        elif fg < 0:\n            # -8 to -1 bright colors\n            # Negative numbers represent the classic colors\n            assert fg in range(-8, 0), fg\n            sgr_code = ANSI.sgr_fg_classic(-fg - 1)\n        else:\n            assert fg in range(256)\n            sgr_code = ANSI.sgr_fg_indexed(fg)\n        sgr_list.append(sgr_code)\n    elif fg is None:\n        pass\n    else:\n        raise ValueError(\"incorrect foreground color: {!r}\".format(fg))\n    # Load SGR code associated with desired background color\n    if isinstance(bg, (str, type(\"\"))):\n        try:\n            sgr_code = getattr(ANSI, str('sgr_bg_{}'.format(bg)))\n        except AttributeError:\n            raise ValueError(\"incorrect background color: {!r}\".format(bg))\n        else:\n            sgr_list.append(sgr_code)\n    elif isinstance(bg, tuple):\n        sgr_code = ANSI.sgr_bg_rgb(*bg)\n        sgr_list.append(sgr_code)\n    elif isinstance(bg, int):\n        if bg < -8:\n            assert bg in range(-16, -8), bg\n            # -16 to -9: bright colors\n            sgr_code = ANSI.sgr_bg_bright(-bg - 8 - 1)\n        elif bg < 0:\n            # -8 to -1 bright colors\n            # Negative numbers represent the classic colors\n            assert bg in range(-8, -1), bg\n            sgr_code = ANSI.sgr_bg_classic(-bg - 1)\n        else:\n            assert bg in range(256)\n            sgr_code = ANSI.sgr_bg_indexed(bg)\n        sgr_list.append(sgr_code)\n    elif bg is None:\n        pass\n    else:\n        raise ValueError(\"incorrect background color: {!r}\".format(bg))\n    # Load single SGR code for \"style\"\n    if style is not None:\n        try:\n            sgr_code = getattr(ANSI, str('sgr_{}'.format(style)))\n        except AttributeError:\n            raise ValueError(\"incorrect text style: {!r}\".format(style))\n        else:\n            sgr_list.append(sgr_code)\n    # Load additional SGR codes (custom)\n    for name, active in sgr.items():\n        try:\n            sgr_code = getattr(ANSI, str('sgr_{}'.format(name)))\n        except AttributeError:\n            raise ValueError(\"incorrect custom SGR code: {!r}\".format(name))\n        else:\n            if active:\n                sgr_list.append(sgr_code)\n    # Combine everything into one sequence\n    if reset:\n        return ANSI.cmd_sgr(sgr_list) + text + ANSI.cmd_sgr_reset_all\n    else:\n        return ANSI.cmd_sgr(sgr_list) + text", "response": "Apply SGR commands to text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget SGR foreground RGB color.", "response": "def sgr_fg_rgb(r, g, b):\n        \"\"\"Get SGR (Set Graphics Rendition) foreground RGB color.\"\"\"\n        assert r in range(256)\n        assert g in range(256)\n        assert b in range(256)\n        return '38;2;{};{};{}'.format(r, g, b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sgr_bg_rgb(r, g, b):\n        assert r in range(256)\n        assert g in range(256)\n        assert b in range(256)\n        return '48;2;{};{};{}'.format(r, g, b)", "response": "Get SGR background RGB color."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint ANSI control parameters to stdout.", "response": "def _aprint2(self, *values, **kwargs):\n        \"\"\"\n        ANSI formatting-aware print().\n\n        This method is a version of print() (function) that understands\n        additional ansi control parameters.\n\n        :param value:\n            The values to print, same as with ``print()``\n        :param sep:\n            Separator between values, same as with ``print()``\n        :param end:\n            Terminator of the line, same as with ``print()``\n        :param file:\n            File to print to, same as with ``print()``\n        :param flush:\n            Flag that controls stream flush behavior, same as with ``print()``\n        :param fg:\n            Foreground color, same as with :meth:`__call__()`.\n        :param bg:\n            Background color, same as with :meth:`__call__()`.\n        :param style:\n            Text style, same as with :meth:`__call__()`.\n        :param reset:\n            Flag that controls if ANSI attributes are reset at the end, same as\n            with :meth:`__call__()`.\n        :param sgr:\n            Additonal (custom) Set Graphics Rendition directives, same as with\n            :meth:`__call__()`.\n\n        .. note::\n            This implementation is intended for Python 2\n        \"\"\"\n        sep = kwargs.pop(str('sep'), ' ')\n        end = kwargs.pop(str('end'), '\\n')\n        file = kwargs.pop(str('file'), None) or sys.stdout\n        flush = kwargs.pop(str('flush'), False)\n        fg = kwargs.pop(str('fg'), None)\n        bg = kwargs.pop(str('bg'), None)\n        style = kwargs.pop(str('style'), None)\n        reset = kwargs.pop(str('reset'), True)\n        sgr = kwargs\n        text = sep.join(str(value) for value in values)\n        text = self(text, fg, bg, style, reset, **sgr)\n        print(text, end=end, file=file)\n        if flush:\n            file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef added(self, context):\n        context.ansi = ANSIFormatter(self._enable)\n        context.aprint = context.ansi.aprint", "response": "Ingredient method called before anything else."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest whether the directory cwd is contained in a git repository.", "response": "def using_git(cwd):\n    \"\"\"Test whether the directory cwd is contained in a git repository.\"\"\"\n    try:\n        git_log = shell_out([\"git\", \"log\"], cwd=cwd)\n        return True\n    except (CalledProcessError, OSError):  # pragma: no cover\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef using_hg(cwd):\n    try:\n        hg_log = shell_out([\"hg\",   \"log\"], cwd=cwd)\n        return True\n    except (CalledProcessError, OSError):\n        return False", "response": "Test whether the directory cwd is contained in a mercurial\n    repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests whether the directory cwd is contained in a bazaar repository.", "response": "def using_bzr(cwd):\n    \"\"\"Test whether the directory cwd is contained in a bazaar repository.\"\"\"\n    try:\n        bzr_log = shell_out([\"bzr\", \"log\"], cwd=cwd)\n        return True\n    except (CalledProcessError, OSError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_string(vc):\n        try:\n            # Note: this means all version controls must have\n            # a title naming convention (!)\n            vc = globals()[vc.title()]\n            assert(issubclass(vc, VersionControl))\n            return vc\n        except (KeyError, AssertionError):\n            raise NotImplementedError(\"Unknown version control system.\")", "response": "Return the VersionControl superclass from a string. For exampleVersionCtrl. from_string will return CJK."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef which(cwd=None):  # pragma: no cover\n        if cwd is None:\n            cwd = os.getcwd()\n        for (k, using_vc) in globals().items():\n            if k.startswith('using_') and using_vc(cwd=cwd):\n                return VersionControl.from_string(k[6:])\n\n        # Not supported (yet)\n        raise NotImplementedError(\"Unknown version control system, \"\n                                  \"or you're not in the project directory.\")", "response": "Try to find which version control system contains the cwd directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef modified_lines(self, r, file_name):\n        cmd = self.file_diff_cmd(r, file_name)\n        diff = shell_out_ignore_exitcode(cmd, cwd=self.root)\n        return list(self.modified_lines_from_diff(diff))", "response": "Returns the line numbers of a file which have been changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the lines that have been modified in a diff.", "response": "def modified_lines_from_diff(self, diff):\n        \"\"\"Returns the changed lines in a diff.\n\n        - Potentially this is vc specific (if not using udiff).\n\n        Note: this returns the line numbers in descending order.\n\n        \"\"\"\n        from pep8radius.diff import modified_lines_from_udiff\n        for start, end in modified_lines_from_udiff(diff):\n            yield start, end"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_filenames_diff(self, r):\n        cmd = self.filenames_diff_cmd(r)\n\n        diff_files = shell_out_ignore_exitcode(cmd, cwd=self.root)\n        diff_files = self.parse_diff_filenames(diff_files)\n\n        return set(f for f in diff_files if f.endswith('.py'))", "response": "Get the py files which have been changed since rev"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the output of filenames_diff_cmd.", "response": "def parse_diff_filenames(diff_files):\n        \"\"\"Parse the output of filenames_diff_cmd.\"\"\"\n        # ?   .gitignore\n        # M  0.txt\n        files = []\n        for line in diff_files.splitlines():\n            line = line.strip()\n            fn = re.findall('[^ ]+\\s+(.*.py)', line)\n            if fn and not line.startswith('?'):\n                files.append(fn[0])\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an HttpResponse object instance with Content - Type : application / json.", "response": "def render_to_response(self, obj, **response_kwargs):\n        \"\"\"\n        Returns an ``HttpResponse`` object instance with Content-Type:\n        application/json.\n\n        The response body will be the return value of ``self.serialize(obj)``\n        \"\"\"\n        return HttpResponse(self.serialize(obj), content_type='application/json', **response_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef http_method_not_allowed(self, *args, **kwargs):\n        resp = super(JsonResponseMixin, self).http_method_not_allowed(*args, **kwargs)\n        resp['Content-Type'] = 'application/json'\n\n        return resp", "response": "Returns super after setting the Content - Type header to\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Python object containing the data from the request body.", "response": "def data(self):\n        \"\"\"\n        Helper class for parsing JSON POST data into a Python object.\n        \"\"\"\n        if self.request.method == 'GET':\n            return self.request.GET\n        else:\n            assert self.request.META['CONTENT_TYPE'].startswith('application/json')\n            charset = self.request.encoding or settings.DEFAULT_CHARSET\n            return json.loads(self.request.body.decode(charset))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndispatch the request to the correct HTTP method.", "response": "def dispatch(self, *args, **kwargs):\n        \"\"\"\n        Authenticates the request and dispatches to the correct HTTP method\n        function (GET, POST, PUT,...).\n\n        Translates exceptions into proper JSON serialized HTTP responses:\n            - ValidationError: HTTP 409\n            - Http404: HTTP 404\n            - PermissionDenied: HTTP 403\n            - ValueError: HTTP 400\n        \"\"\"\n        try:\n            self.auth(*args, **kwargs)\n            return super(RestView, self).dispatch(*args, **kwargs)\n        except ValidationError as e:\n            return self.render_to_response(e.message_dict, status=409)\n        except Http404 as e:\n            return self.render_to_response(str(e), status=404)\n        except PermissionDenied as e:\n            return self.render_to_response(str(e), status=403)\n        except ValueError as e:\n            return self.render_to_response(str(e), status=400)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements a OPTIONS HTTP method function returning all allowed HTTP methods.", "response": "def options(self, request, *args, **kwargs):\n        \"\"\"\n        Implements a OPTIONS HTTP method function returning all allowed HTTP\n        methods.\n        \"\"\"\n        allow = []\n        for method in self.http_method_names:\n            if hasattr(self, method):\n                allow.append(method.upper())\n        r = self.render_to_response(None)\n        r['Allow'] = ','.join(allow)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(args=None, vc=None, cwd=None, apply_config=False):\n    import signal\n\n    try:  # pragma: no cover\n        # Exit on broken pipe.\n        signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n    except AttributeError:  # pragma: no cover\n        # SIGPIPE is not available on Windows.\n        pass\n\n    try:\n        if args is None:\n            args = []\n\n        try:\n            # Note: argparse on py 2.6 you can't pass a set\n            # TODO neater solution for this!\n            args_set = set(args)\n        except TypeError:\n            args_set = args  # args is a Namespace\n        if '--version' in args_set or getattr(args_set, 'version', 0):\n            print(version)\n            return 0\n        if '--list-fixes' in args_set or getattr(args_set, 'list_fixes', 0):\n            from autopep8 import supported_fixes\n            for code, description in sorted(supported_fixes()):\n                print('{code} - {description}'.format(\n                    code=code, description=description))\n            return 0\n\n        try:\n            try:\n                args = parse_args(args, apply_config=apply_config)\n            except TypeError:\n                pass  # args is already a Namespace (testing)\n            if args.from_diff:  # pragma: no cover\n                r = Radius.from_diff(args.from_diff.read(),\n                                     options=args, cwd=cwd)\n            else:\n                r = Radius(rev=args.rev, options=args, vc=vc, cwd=cwd)\n        except NotImplementedError as e:  # pragma: no cover\n            print(e)\n            return 1\n        except CalledProcessError as c:  # pragma: no cover\n            # cut off usage and exit\n            output = c.output.splitlines()[0]\n            print(output)\n            return c.returncode\n\n        any_changes = r.fix()\n        if any_changes and args.error_status:\n            return 1\n        return 0\n\n    except KeyboardInterrupt:  # pragma: no cover\n        return 1", "response": "PEP8 clean only the parts of the files touched since the last commit or branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_parser():\n    from argparse import ArgumentParser, FileType\n\n    description = (\"PEP8 clean only the parts of the files which you have \"\n                   \"touched since the last commit, a previous commit or \"\n                   \"(the merge-base of) a branch.\")\n    epilog = (\"Run before you commit, against a previous commit or \"\n              \"branch before merging.\")\n    parser = ArgumentParser(description=description,\n                            epilog=epilog,\n                            prog='pep8radius')\n\n    parser.add_argument('rev',\n                        help='commit or name of branch to compare against',\n                        nargs='?')\n\n    parser.add_argument('--version',\n                        help='print version number and exit',\n                        action='store_true')\n\n    parser.add_argument('-d', '--diff', action='store_true', dest='diff',\n                        help='print the diff of fixed source vs original')\n    parser.add_argument('--error-status', action='store_true',\n                        dest='error_status',\n                        help=\"return a shell status code of 1 if there are\"\n                             \" any fixes\")\n    parser.add_argument('-i', '--in-place', action='store_true',\n                        help=\"make the fixes in place; modify the files\")\n    parser.add_argument('--no-color', action='store_true',\n                        help='do not print diffs in color '\n                             '(default is to use color)')\n    parser.add_argument('-v', '--verbose', action='count', dest='verbose',\n                        default=0,\n                        help='print verbose messages; '\n                        'multiple -v result in more verbose messages '\n                        '(one less -v is passed to autopep8)')\n    parser.add_argument('--from-diff', type=FileType('r'), metavar='DIFF',\n                        help=\"Experimental: rather than calling out to version\"\n                             \" control, just pass in a diff; \"\n                             \"the modified lines will be fixed\")\n\n    ap = parser.add_argument_group('pep8', 'Pep8 options to pass to autopep8.')\n    ap.add_argument('-p', '--pep8-passes', metavar='n',\n                    default=-1, type=int,\n                    help='maximum number of additional pep8 passes '\n                    '(default: infinite)')\n    ap.add_argument('-a', '--aggressive', action='count', default=0,\n                    help='enable non-whitespace changes; '\n                    'multiple -a result in more aggressive changes')\n    ap.add_argument('--experimental', action='store_true',\n                    help='enable experimental fixes')\n    ap.add_argument('--exclude', metavar='globs',\n                    help='exclude file/directory names that match these '\n                    'comma-separated globs')\n    ap.add_argument('--list-fixes', action='store_true',\n                    help='list codes for fixes and exit; '\n                    'used by --ignore and --select')\n    ap.add_argument('--ignore', metavar='errors', default='',\n                    help='do not fix these errors/warnings '\n                    '(default: {0})'.format(DEFAULT_IGNORE))\n    ap.add_argument('--select', metavar='errors', default='',\n                    help='fix only these errors/warnings (e.g. E4,W)')\n    ap.add_argument('--max-line-length', metavar='n', default=79, type=int,\n                    help='set maximum allowed line length '\n                    '(default: %(default)s)')\n    ap.add_argument('--indent-size', default=DEFAULT_INDENT_SIZE,\n                    type=int, metavar='n',\n                    help='number of spaces per indent level '\n                    '(default %(default)s)')\n\n    df = parser.add_argument_group('docformatter',\n                                   'Fix docstrings for PEP257.')\n    df.add_argument('-f', '--docformatter', action='store_true',\n                    help='Use docformatter')\n    df.add_argument('--no-blank', dest='post_description_blank',\n                    action='store_false',\n                    help='Do not add blank line after description')\n    df.add_argument('--pre-summary-newline',\n                    action='store_true',\n                    help='add a newline before the summary of a '\n                    'multi-line docstring')\n    df.add_argument('--force-wrap', action='store_true',\n                    help='force descriptions to be wrapped even if it may '\n                    'result in a mess')\n\n    cg = parser.add_argument_group('config',\n                                   'Change default options based on global '\n                                   'or local (project) config files.')\n    cg.add_argument('--global-config',\n                    default=DEFAULT_CONFIG,\n                    metavar='filename',\n                    help='path to global pep8 config file; ' +\n                    \" if this file does not exist then this is ignored\" +\n                    '(default: %s)' % DEFAULT_CONFIG)\n    cg.add_argument('--ignore-local-config', action='store_true',\n                    help=\"don't look for and apply local config files; \"\n                    'if not passed, defaults are updated with any '\n                    \"config files in the project's root directory\")\n\n    yp = parser.add_argument_group('yapf',\n                                   'Options for yapf, alternative to autopep8. '\n                                   'Currently any other options are ignored.')\n    yp.add_argument('-y', '--yapf', action='store_true',\n                    help='Use yapf rather than autopep8. '\n                    'This ignores other arguments outside of this group.')\n    yp.add_argument('--style', metavar='', default='pep8',\n                    help='style either pep8, google, name of file with style'\n                    'settings, or a dict')\n\n    return parser", "response": "Create the parser for the pep8radius CLI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the arguments from the CLI.", "response": "def parse_args(arguments=None, root=None, apply_config=False):\n    \"\"\"Parse the arguments from the CLI.\n\n    If apply_config then we first look up and apply configs using\n    apply_config_defaults.\n\n    \"\"\"\n    if arguments is None:\n        arguments = []\n\n    parser = create_parser()\n    args = parser.parse_args(arguments)\n    if apply_config:\n        parser = apply_config_defaults(parser, args, root=root)\n        args = parser.parse_args(arguments)\n\n    # sanity check args (from autopep8)\n    if args.max_line_length <= 0:  # pragma: no cover\n        parser.error('--max-line-length must be greater than 0')\n\n    if args.select:\n        args.select = _split_comma_separated(args.select)\n\n    if args.ignore:\n        args.ignore = _split_comma_separated(args.ignore)\n    elif not args.select and args.aggressive:\n        # Enable everything by default if aggressive.\n        args.select = ['E', 'W']\n    else:\n        args.ignore = _split_comma_separated(DEFAULT_IGNORE)\n\n    if args.exclude:\n        args.exclude = _split_comma_separated(args.exclude)\n    else:\n        args.exclude = []\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_config_defaults(parser, args, root):\n    if root is None:\n        try:\n            from pep8radius.vcs import VersionControl\n            root = VersionControl.which().root_dir()\n        except NotImplementedError:\n            pass  # don't update local, could be using as module\n\n    config = SafeConfigParser()\n    config.read(args.global_config)\n    if root and not args.ignore_local_config:\n        config.read(local_config_files(root))\n\n    try:\n        defaults = dict((k.lstrip('-').replace('-', '_'), v)\n                        for k, v in config.items(\"pep8\"))\n        parser.set_defaults(**defaults)\n    except NoSectionError:\n        pass  # just do nothing, potentially this could raise ?\n    return parser", "response": "Update the parser s defaults from either the arguments. config_arg or\n    the config files given in config_files ( root )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a VarInt32 from the stream.", "response": "def read_vint32(self):\n        \"\"\"\n        This seems to be a variable length integer ala utf-8 style\n        \"\"\"\n        result = 0\n        count = 0\n        while True:\n            if count > 4:\n                raise ValueError(\"Corrupt VarInt32\")\n\n            b = self.read_byte()\n            result = result | (b & 0x7F) << (7 * count)\n            count += 1\n\n            if not b & 0x80:\n                return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a protobuf message from the stream.", "response": "def read_message(self, message_type, compressed=False, read_size=True):\n        \"\"\"\n        Read a protobuf message\n        \"\"\"\n        if read_size:\n            size = self.read_vint32()\n            b = self.read(size)\n        else:\n            b = self.read()\n\n        if compressed:\n            b = snappy.decompress(b)\n\n        m = message_type()\n        m.ParseFromString(b)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning any additional functions that want to process this type of packet.", "response": "def run_hooks(self, packet):\n        \"\"\"\n        Run any additional functions that want to process this type of packet.\n        These can be internal parser hooks, or external hooks that process\n        information\n        \"\"\"\n\n        if packet.__class__ in self.internal_hooks:\n            self.internal_hooks[packet.__class__](packet)\n\n        if packet.__class__ in self.hooks:\n            self.hooks[packet.__class__](packet)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_string_table(self, tables):\n        self.info(\"String table: %s\" % (tables.tables, ))\n\n        for table in tables.tables:\n            if table.table_name == \"userinfo\":\n                for item in table.items:\n                    if len(item.data) > 0:\n                        if len(item.data) == 140:\n                            p = PlayerInfo()\n                            ctypes.memmove(ctypes.addressof(p), item.data, 140)\n                            p.str = item.str\n                            self.run_hooks(p)\n            if table.table_name == \"CombatLogNames\":\n                self.combat_log_names = dict(enumerate(\n                    (item.str for item in table.items)))", "response": "Parse a string table and populate self. player_info with player information."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a game event and return a GameEvent object", "response": "def parse_game_event(self, event):\n        \"\"\"\n        So CSVCMsg_GameEventList is a list of all events that can happen.\n        A game event has an eventid which maps to a type of event that happened\n        \"\"\"\n\n        if event.eventid in self.event_lookup:\n            #Bash this into a nicer data format to work with\n            event_type = self.event_lookup[event.eventid]\n            ge = GameEvent(event_type.name)\n\n            for i, key in enumerate(event.keys):\n                key_type = event_type.keys[i]\n                ge.keys[key_type.name] = getattr(key,\n                                                    KEY_DATA_TYPES[key.type])\n\n            self.debug(\"|==========> %s\" % (ge, ))\n\n            self.run_hooks(ge)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self):\n\n        self.important(\"Parsing demo file '%s'\" % (self.filename, ))\n\n        with open(self.filename, 'rb') as f:\n            reader = Reader(StringIO(f.read()))\n\n            filestamp = reader.read(8)\n            offset = reader.read_int32()\n\n            if filestamp != \"PBUFDEM\\x00\":\n                raise ValueError(\"Invalid replay - incorrect filestamp\")\n\n            buff = StringIO(f.read())\n\n            frame = 0\n            more = True\n            while more and reader.remaining > 0:\n                cmd = reader.read_vint32()\n                tick = reader.read_vint32()\n                compressed = False\n\n                if cmd & demo_pb2.DEM_IsCompressed:\n                    compressed = True\n                    cmd = cmd & ~demo_pb2.DEM_IsCompressed\n\n                if cmd not in messages.MESSAGE_TYPES:\n                    raise KeyError(\"Unknown message type found\")\n\n                message_type = messages.MESSAGE_TYPES[cmd]\n                message = reader.read_message(message_type, compressed)\n\n                self.info('%s: %s' % (frame, message_type))\n                self.worthless(message)\n\n                self.run_hooks(message)\n\n                self.info('|%s' % ('-' * 79, ))\n\n                frame += 1\n\n                if self.frames and frame > self.frames:\n                    break", "response": "Parse a replay file and return a tuple of the last two elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(data):\n    try:\n        st = basestring\n    except NameError:\n        st = str\n    if isinstance(data, st):\n        return str(data)\n    elif isinstance(data, Mapping):\n        return dict(map(convert, data.iteritems()))\n    elif isinstance(data, Iterable):\n        return type(data)(map(convert, data))\n    else:\n        return data", "response": "Convert from unicode to native ascii\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget internal properties of a type property.", "response": "def get_type_properties(self, property_obj, name, additional_prop=False):\n        \"\"\" Get internal properties of property (extended in schema)\n\n        :param dict property_obj: raw property object\n        :param str name: name of property\n        :param bool additional_prop: recursion's param\n        :return: Type, format and internal properties of property\n        :rtype: tuple(str, str, dict)\n        \"\"\"\n        property_type = property_obj.get('type', 'object')\n        property_format = property_obj.get('format')\n        property_dict = {}\n\n        if property_type in ['object', 'array']:\n            schema_type = SchemaTypes.MAPPED if additional_prop else SchemaTypes.INLINE\n            schema_id = self._get_object_schema_id(property_obj, schema_type)\n            if not ('$ref' in property_obj or self.storage.get(schema_id)):\n                _schema = self.storage.create_schema(\n                    property_obj, name, schema_type, root=self.root)\n                self._after_create_schema(_schema)\n            property_type = schema_id\n\n        property_dict['default'] = property_obj.get('default')\n        property_dict['maximum'] = property_obj.get('maximum')\n        property_dict['exclusive_maximum'] = property_obj.get('exclusiveMaximum')\n        property_dict['minimum'] = property_obj.get('minimum')\n        property_dict['exclusive_minimum'] = property_obj.get('exclusiveMinimum')\n        property_dict['max_length'] = property_obj.get('maxLength')\n        property_dict['min_length'] = property_obj.get('minLength')\n\n        #TODO: fixme. remove ugly convert. add property template renderer instead\n        property_dict['enum'] = convert(property_obj.get('enum'))\n\n        #TODO: fixme. cleanup empty properties. add configurable filter for properties instead\n        property_dict = {k: v for k, v in property_dict.items() if v}\n\n        return property_type, property_format, property_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_type_by_schema(self, schema_obj, schema_type):\n        schema_id = self._get_object_schema_id(schema_obj, schema_type)\n\n        if not self.storage.contains(schema_id):\n            schema = self.storage.create_schema(\n                schema_obj, self.name, schema_type, root=self.root)\n            assert schema.schema_id == schema_id\n        self._type = schema_id", "response": "Set property type by schema object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a response that can be sent to the user to download the current filtered list of items", "response": "def tablib_export_action(modeladmin, request, queryset, file_type=\"xls\"):\n    \"\"\"\n    Allow the user to download the current filtered list of items\n\n    :param file_type:\n        One of the formats supported by tablib (e.g. \"xls\", \"csv\", \"html\",\n        etc.)\n    \"\"\"\n\n    dataset = SimpleDataset(queryset, headers=None)\n    filename = '{0}.{1}'.format(\n        smart_str(modeladmin.model._meta.verbose_name_plural), file_type)\n\n    response_kwargs = {\n        'content_type': get_content_type(file_type)\n    }\n\n    response = HttpResponse(getattr(dataset, file_type), **response_kwargs)\n    response['Content-Disposition'] = 'attachment; filename={0}'.format(\n        filename)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_type_properties(self, property_obj, name, additional_prop=False):\n        property_type, property_format, property_dict = \\\n            super(Schema, self).get_type_properties(property_obj, name, additional_prop=additional_prop)\n        _schema = self.storage.get(property_type)\n        if _schema and ('additionalProperties' in property_obj):\n            _property_type, _property_format, _property_dict = super(Schema, self).get_type_properties(\n                property_obj['additionalProperties'], '{}-mapped'.format(name), additional_prop=True)\n            if _property_type not in PRIMITIVE_TYPES:\n                SchemaMapWrapper.wrap(self.storage.get(_property_type))\n                _schema.nested_schemas.add(_property_type)\n            else:\n                _schema.type_format = _property_type\n\n        return property_type, property_format, property_dict", "response": "Get internal properties of property - method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sorted(collection):\n        if len(collection) < 1:\n            return collection\n\n        if isinstance(collection, dict):\n            return sorted(collection.items(), key=lambda x: x[0])\n\n        if isinstance(list(collection)[0], Operation):\n            key = lambda x: x.operation_id\n        elif isinstance(list(collection)[0], str):\n            key = lambda x: SchemaObjects.get(x).name\n        else:\n            raise TypeError(type(collection[0]))\n        return sorted(collection, key=key)", "response": "sort dict by key schema - collection by schema - name\n        operations by id\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes table with properties by schema_id", "response": "def get_regular_properties(self, _type, *args, **kwargs):\n        \"\"\"Make table with properties by schema_id\n        :param str _type:\n        :rtype: str\n        \"\"\"\n        if not SchemaObjects.contains(_type):\n            return _type\n        schema = SchemaObjects.get(_type)\n        if schema.schema_type == SchemaTypes.DEFINITION and not kwargs.get('definition'):\n            return ''\n        head = \"\"\".. csv-table::\n    :delim: |\n    :header: \"Name\", \"Required\", \"Type\", \"Format\", \"Properties\", \"Description\"\n    :widths: 20, 10, 15, 15, 30, 25\n\n\"\"\"\n        body = []\n        if schema.properties:\n            for p in schema.properties:\n                body.append('        {} | {} | {} | {} | {} | {}'.format(\n                    p.get('name') or '',\n                    'Yes' if p.get('required') else 'No',\n                    self.get_type_description(p['type'], *args, **kwargs),\n                    p.get('type_format') or '',\n                    '{}'.format(p.get('type_properties') or ''),\n                    p.get('description') or '')\n                )\n            body.sort()\n        return (head + '\\n'.join(body))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_type_description(self, _type, suffix='', *args, **kwargs):\n        if not SchemaObjects.contains(_type):\n            return _type\n        schema = SchemaObjects.get(_type)\n        if schema.all_of:\n            models = ','.join(\n                (self.get_type_description(_type, *args, **kwargs) for _type in schema.all_of)\n            )\n            result = '{}'.format(models.split(',')[0])\n            for r in models.split(',')[1:]:\n                result += ' extended {}'.format(r)\n        elif schema.is_array:\n            result = 'array of {}'.format(\n                self.get_type_description(schema.item['type'], *args, **kwargs))\n        else:\n            result = ':ref:`{} <{}{}>`'.format(schema.name, schema.schema_id, suffix)\n        return result", "response": "Get description of type\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking head and table with additional properties by schema_id", "response": "def get_additional_properties(self, _type, *args, **kwargs):\n        \"\"\"Make head and table with additional properties by schema_id\n        :param str _type:\n        :rtype: str\n        \"\"\"\n        if not SchemaObjects.contains(_type):\n            return _type\n        schema = SchemaObjects.get(_type)\n        body = []\n        for sch in schema.nested_schemas:  # complex types\n            nested_schema = SchemaObjects.get(sch)\n            if not (nested_schema or isinstance(nested_schema, SchemaMapWrapper)):\n                continue\n\n            body.append('Map of {{\"key\":\"{}\"}}\\n\\n'.format(self.get_type_description(\n                nested_schema.schema_id, *args, **kwargs))  # head\n            )\n            if nested_schema.is_array:  # table\n                _schema = SchemaObjects.get(nested_schema.item.get('type'))\n                if _schema and _schema.schema_type == SchemaTypes.INLINE:\n                    body.append(self.get_regular_properties(_schema.schema_id, *args, **kwargs))\n            else:\n                body.append(self.get_regular_properties(nested_schema.schema_id, *args, **kwargs))\n        if schema.type_format:  # basic types, only head\n            body.append(\n                'Map of {{\"key\":\"{}\"}}'.format(self.get_type_description(schema.type_format, *args, **kwargs)))\n        return ''.join(body)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the value of the display_field and the date values of the derived EDTF field.", "response": "def pre_save(self, instance, add):\n        \"\"\"\n        Updates the edtf value from the value of the display_field.\n        If there's a valid edtf, then set the date values.\n        \"\"\"\n        if not self.natural_text_field or self.attname not in instance.__dict__:\n            return\n\n        edtf = getattr(instance, self.attname)\n\n        # Update EDTF field based on latest natural text value, if any\n        natural_text = getattr(instance, self.natural_text_field)\n        if natural_text:\n            edtf = text_to_edtf(natural_text)\n        else:\n            edtf = None\n\n        # TODO If `natural_text_field` becomes cleared the derived EDTF field\n        # value should also be cleared, rather than left at original value?\n\n        # TODO Handle case where EDTF field is set to a string directly, not\n        # via `natural_text_field` (this is a slightly unexpected use-case, but\n        # is a very efficient way to set EDTF values in situations like for API\n        # imports so we probably want to continue to support it?)\n        if edtf and not isinstance(edtf, EDTFObject):\n            edtf = parse_edtf(edtf, fail_silently=True)\n\n        setattr(instance, self.attname, edtf)\n        # set or clear related date fields on the instance\n        for attr in DATE_ATTRS:\n            field_attr = \"%s_field\" % attr\n            g = getattr(self, field_attr, None)\n            if g:\n                if edtf:\n                    try:\n                        target_field = instance._meta.get_field(g)\n                    except FieldDoesNotExist:\n                        continue\n                    value = getattr(edtf, attr)()  # struct_time\n                    if isinstance(target_field, models.FloatField):\n                        value = struct_time_to_jd(value)\n                    elif isinstance(target_field, models.DateField):\n                        value = struct_time_to_date(value)\n                    else:\n                        raise NotImplementedError(\n                            u\"EDTFField does not support %s as a derived data\"\n                            u\" field, only FloatField or DateField\"\n                            % type(target_field))\n                    setattr(instance, g, value)\n                else:\n                    setattr(instance, g, None)\n        return edtf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_delta(op, time_struct, delta):\n    if not delta:\n        return time_struct  # No work to do\n\n    try:\n        dt_result = op(datetime(*time_struct[:6]), delta)\n        return dt_to_struct_time(dt_result)\n    except (OverflowError, ValueError):\n        # Year is not within supported 1 to 9999 AD range\n        pass\n\n    # Here we fake the year to one in the acceptable range to avoid having to\n    # write our own date rolling logic\n\n    # Adjust the year to be close to the 2000 millenium in 1,000 year\n    # increments to try and retain accurate relative leap years\n    actual_year = time_struct.tm_year\n    millenium = int(float(actual_year) / 1000)\n    millenium_diff = (2 - millenium) * 1000\n    adjusted_year = actual_year + millenium_diff\n    # Apply delta to the date/time with adjusted year\n    dt = datetime(*(adjusted_year,) + time_struct[1:6])\n    dt_result = op(dt, delta)\n    # Convert result year back to its original millenium\n    final_year = dt_result.year - millenium_diff\n    return struct_time(\n        (final_year,) + dt_result.timetuple()[1:6] + tuple(TIME_EMPTY_EXTRAS))", "response": "Apply a relative delta to a struct_time data structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _strict_date(self, lean):\n        return struct_time(\n            (\n                self._precise_year(lean),\n                self._precise_month(lean),\n                self._precise_day(lean),\n            ) + tuple(TIME_EMPTY_TIME) + tuple(TIME_EMPTY_EXTRAS)\n        )", "response": "Return a struct_time representation of the date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_fuzzy_padding(self, lean):\n        result = relativedelta(0)\n\n        if self.year_ua:\n            result += appsettings.PADDING_YEAR_PRECISION * self.year_ua._get_multiplier()\n        if self.month_ua:\n            result += appsettings.PADDING_MONTH_PRECISION * self.month_ua._get_multiplier()\n        if self.day_ua:\n            result += appsettings.PADDING_DAY_PRECISION * self.day_ua._get_multiplier()\n\n        if self.year_month_ua:\n            result += appsettings.PADDING_YEAR_PRECISION * self.year_month_ua._get_multiplier()\n            result += appsettings.PADDING_MONTH_PRECISION * self.year_month_ua._get_multiplier()\n        if self.month_day_ua:\n            result += appsettings.PADDING_DAY_PRECISION * self.month_day_ua._get_multiplier()\n            result += appsettings.PADDING_MONTH_PRECISION * self.month_day_ua._get_multiplier()\n\n        if self.season_ua:\n            result += appsettings.PADDING_SEASON_PRECISION * self.season_ua._get_multiplier()\n\n        if self.all_ua:\n            multiplier = self.all_ua._get_multiplier()\n\n            if self.precision == PRECISION_DAY:\n                result += multiplier * appsettings.PADDING_DAY_PRECISION\n                result += multiplier * appsettings.PADDING_MONTH_PRECISION\n                result += multiplier * appsettings.PADDING_YEAR_PRECISION\n            elif self.precision == PRECISION_MONTH:\n                result += multiplier * appsettings.PADDING_MONTH_PRECISION\n                result += multiplier * appsettings.PADDING_YEAR_PRECISION\n            elif self.precision == PRECISION_YEAR:\n                result += multiplier * appsettings.PADDING_YEAR_PRECISION\n\n        return result", "response": "Returns the fuzziness of the entry in the application."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npackage lambda data into a zip", "response": "def package(self):\n        \"\"\"Packages lambda data for deployment into a zip\"\"\"\n        logger.info('Packaging lambda {}'.format(self.lambda_name))\n        zfh = io.BytesIO()\n\n        if os.path.exists(os.path.join(self.lambda_dir, '.env')):\n            logger.warn(\n                'A .env file exists in your Lambda directory - be '\n                'careful that it does not contain any secrets you '\n                'don\\'t want uploaded to AWS!'\n            )\n\n        with zipfile.ZipFile(zfh, 'w') as zf:\n            self.add_directory_to_zip(self.lambda_dir, zf)\n\n            # Construct a .env file in the archive with our\n            # needed envrionment variables.\n            envinfo = zipfile.ZipInfo('.env')\n            envinfo.external_attr = 0644 << 16L\n            zf.writestr(\n                envinfo,\n                '\\n'.join(\n                    '{} = {}'.format(key, yaep.env(key))\n                    for key in self.env_vars\n                )\n            )\n\n            if 'requirements.txt' in os.listdir(self.lambda_dir):\n                with TemporaryDirectory() as temp_dir:\n                    pip_args = [\n                        'install',\n                        '-r',\n                        os.path.join(self.lambda_dir, 'requirements.txt'),\n                        '-t',\n                        temp_dir\n                    ]\n\n                    # Do pip install to temporary dir\n                    if pip.main(pip_args) == 0:\n                        self.add_directory_to_zip(temp_dir, zf)\n                    else:\n                        if sys.platform == 'darwin':\n                            logger.error(\n                                'A DistutilsOptionError about the prefix '\n                                'can occur when you are on OS X and '\n                                'installed Python via Homebrew.\\nIf this '\n                                'is you, please look at https://github.com'\n                                '/Homebrew/brew/blob/master/share/doc/'\n                                'homebrew/Homebrew-and-Python.md'\n                                '#note-on-pip-install---user\\n'\n                                'If this is not you, please contact us '\n                                ' for support.'\n                            )\n                        raise DependencyInstallationError(\n                            'Failed to install dependencies of {}'.format(\n                                self.lambda_name\n                            )\n                        )\n\n        zfh.seek(0)\n\n        return zfh"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deploy(self, *lambdas):\n\n        if not self.role:\n            logger.error('Missing AWS Role')\n            raise ArgumentsError('Role required')\n\n        logger.debug('Deploying lambda {}'.format(self.lambda_name))\n        zfh = self.package()\n\n        if self.lambda_name in self.get_function_names():\n            logger.info('Updating {} lambda'.format(self.lambda_name))\n\n            response = self.client.update_function_code(\n                FunctionName=self.lambda_name,\n                ZipFile=zfh.getvalue(),\n                Publish=True\n            )\n        else:\n            logger.info('Adding new {} lambda'.format(self.lambda_name))\n\n            response = self.client.create_function(\n                FunctionName=self.lambda_name,\n                Runtime=yaep.env(\n                    'LAMBDA_RUNTIME',\n                    'python2.7'\n                ),\n                Role=self.role,\n                Handler=yaep.env(\n                    'LAMBDA_HANDLER',\n                    'lambda_function.lambda_handler'\n                ),\n                Code={\n                    'ZipFile': zfh.getvalue(),\n                },\n                Description=yaep.env(\n                    'LAMBDA_DESCRIPTION',\n                    'Lambda code for {}'.format(self.lambda_name)\n                ),\n                Timeout=yaep.env(\n                    'LAMBDA_TIMEOUT',\n                    3,\n                    convert_booleans=False,\n                    type_class=int\n                ),\n                MemorySize=yaep.env(\n                    'LAMBDA_MEMORY_SIZE',\n                    128,\n                    convert_booleans=False,\n                    type_class=int\n                ),\n                Publish=True\n            )\n\n        status_code = response.get(\n            'ResponseMetadata', {}\n        ).get('HTTPStatusCode')\n\n        if status_code in [200, 201]:\n            logger.info('Successfully deployed {} version {}'.format(\n                self.lambda_name,\n                response.get('Version', 'Unkown')\n            ))\n        else:\n            logger.error('Error deploying {}: {}'.format(\n                self.lambda_name,\n                response\n            ))", "response": "Deploys lambdas to AWS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self):\n        for function in self.client.list_functions().get('Functions', []):\n            lines = json.dumps(function, indent=4, sort_keys=True).split('\\n')\n            for line in lines:\n                logger.info(line)", "response": "Lists already deployed lambdas"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_info(self):\n        if LooseVersion(django.get_version()) < LooseVersion('1.7.0'):\n            info = self.model._meta.app_label, self.model._meta.module_name\n        else:\n            info = self.model._meta.app_label, self.model._meta.model_name\n        return info", "response": "Get the model info in a form of app_label model_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a date to Julian Day.", "response": "def date_to_jd(year,month,day):\n    \"\"\"\n    Convert a date to Julian Day.\n\n    Algorithm from 'Practical Astronomy with your Calculator or Spreadsheet', \n        4th ed., Duffet-Smith and Zwart, 2011.\n\n    Parameters\n    ----------\n    year : int\n        Year as integer. Years preceding 1 A.D. should be 0 or negative.\n        The year before 1 A.D. is 0, 10 B.C. is year -9.\n\n    month : int\n        Month as integer, Jan = 1, Feb. = 2, etc.\n\n    day : float\n        Day, may contain fractional part.\n\n    Returns\n    -------\n    jd : float\n        Julian Day\n\n    Examples\n    --------\n    Convert 6 a.m., February 17, 1985 to Julian Day\n\n    >>> date_to_jd(1985,2,17.25)\n    2446113.75\n\n    \"\"\"\n    if month == 1 or month == 2:\n        yearp = year - 1\n        monthp = month + 12\n    else:\n        yearp = year\n        monthp = month\n\n    # this checks where we are in relation to October 15, 1582, the beginning\n    # of the Gregorian calendar.\n    if ((year < 1582) or\n        (year == 1582 and month < 10) or\n        (year == 1582 and month == 10 and day < 15)):\n        # before start of Gregorian calendar\n        B = 0\n    else:\n        # after start of Gregorian calendar\n        A = math.trunc(yearp / 100.)\n        B = 2 - A + math.trunc(A / 4.)\n\n    if yearp < 0:\n        C = math.trunc((365.25 * yearp) - 0.75)\n    else:\n        C = math.trunc(365.25 * yearp)\n\n    D = math.trunc(30.6001 * (monthp + 1))\n\n    jd = B + C + D + day + 1720994.5\n\n    return jd"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting Julian Day to date.", "response": "def jd_to_date(jd):\n    \"\"\"\n    Convert Julian Day to date.\n\n    Algorithm from 'Practical Astronomy with your Calculator or Spreadsheet', \n        4th ed., Duffet-Smith and Zwart, 2011.\n\n    Parameters\n    ----------\n    jd : float\n        Julian Day\n\n    Returns\n    -------\n    year : int\n        Year as integer. Years preceding 1 A.D. should be 0 or negative.\n        The year before 1 A.D. is 0, 10 B.C. is year -9.\n\n    month : int\n        Month as integer, Jan = 1, Feb. = 2, etc.\n\n    day : float\n        Day, may contain fractional part.\n\n    Examples\n    --------\n    Convert Julian Day 2446113.75 to year, month, and day.\n\n    >>> jd_to_date(2446113.75)\n    (1985, 2, 17.25)\n\n    \"\"\"\n    jd = jd + 0.5\n\n    F, I = math.modf(jd)\n    I = int(I)\n\n    A = math.trunc((I - 1867216.25)/36524.25)\n\n    if I > 2299160:\n        B = I + 1 + A - math.trunc(A / 4.)\n    else:\n        B = I\n\n    C = B + 1524\n\n    D = math.trunc((C - 122.1) / 365.25)\n\n    E = math.trunc(365.25 * D)\n\n    G = math.trunc((C - E) / 30.6001)\n\n    day = C - E + F - math.trunc(30.6001 * G)\n\n    if G < 13.5:\n        month = G - 1\n    else:\n        month = G - 13\n\n    if month > 2.5:\n        year = D - 4716\n    else:\n        year = D - 4715\n\n    return year, month, day"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hmsm_to_days(hour=0,min=0,sec=0,micro=0):\n    days = sec + (micro / 1.e6)\n\n    days = min + (days / 60.)\n\n    days = hour + (days / 60.)\n\n    return days / 24.", "response": "Convert hours minutes seconds and microseconds to fractional days."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef days_to_hmsm(days):\n    hours = days * 24.\n    hours, hour = math.modf(hours)\n\n    mins = hours * 60.\n    mins, min = math.modf(mins)\n\n    secs = mins * 60.\n    secs, sec = math.modf(secs)\n\n    micro = round(secs * 1.e6)\n\n    return int(hour), int(min), int(sec), int(micro)", "response": "Converts fractional days to hours minutes seconds and microseconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datetime_to_jd(date):\n    days = date.day + hmsm_to_days(date.hour,date.minute,date.second,date.microsecond)\n\n    return date_to_jd(date.year,date.month,days)", "response": "Convert a datetime. datetime object to Julian Day."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jd_to_datetime(jd):\n    year, month, day = jd_to_date(jd)\n\n    frac_days,day = math.modf(day)\n    day = int(day)\n\n    hour,min,sec,micro = days_to_hmsm(frac_days)\n\n    return datetime(year,month,day,hour,min,sec,micro)", "response": "Convert a Julian Day to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef timedelta_to_days(td):\n    seconds_in_day = 24. * 3600.\n\n    days = td.days + (td.seconds + (td.microseconds * 10.e6)) / seconds_in_day\n\n    return days", "response": "Convert a datetime. timedelta object to a total number of days."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_schema(cls, obj, name, schema_type, root):\n        if schema_type == SchemaTypes.MAPPED:\n            schema = SchemaMapWrapper(obj, storage=cls, name=name, root=root)\n        else:\n            schema = Schema(obj, schema_type, storage=cls, name=name, root=root)\n        cls.add_schema(schema)\n        return schema", "response": "Create a new schema object with the given name and schema type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets schemas by type.", "response": "def get_schemas(cls, schema_types=None, sort=True):\n        \"\"\"\n        Get schemas by type. If ``schema_type`` is None, return all schemas\n\n        :param schema_types: list of schema types\n        :type schema_types: list or None\n        :param bool sort: sort by name\n        :return: list of schemas\n        :rtype: list\n        \"\"\"\n        result = filter(lambda x: not x.is_inline_array, cls._schemas.values())\n        if schema_types:\n            result = filter(lambda x: x.schema_type in schema_types, result)\n        if sort:\n            result = sorted(result, key=attrgetter('name'))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_schemas(cls, schema, _schema):\n        tmp = schema.properties[:]  # copy\n        prop = {}\n        to_dict = lambda e: prop.update({e.pop('name'): e})\n        [to_dict(i) for i in tmp]  # map(to_dict, tmp)\n        for _prop in _schema.properties:\n            if prop.get(_prop['name']):\n                prop.pop(_prop['name'])\n        if prop:\n            for k, v in prop.items():\n                v['name'] = k\n                _schema.properties.append(v)\n        return _schema", "response": "Return second Schema, which is extended by first Schema\n        https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#composition-and-inheritance-polymorphism"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a datetime. date or datetime. datetime to a struct_time.", "response": "def dt_to_struct_time(dt):\n    \"\"\"\n    Convert a `datetime.date` or `datetime.datetime` to a `struct_time`\n    representation *with zero values* for data fields that we cannot always\n    rely on for ancient or far-future dates: tm_wday, tm_yday, tm_isdst\n\n    NOTE: If it wasn't for the requirement that the extra fields are unset\n    we could use the `timetuple()` method instead of this function.\n    \"\"\"\n    if isinstance(dt, datetime):\n        return struct_time(\n            [dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second] +\n            TIME_EMPTY_EXTRAS\n        )\n    elif isinstance(dt, date):\n        return struct_time(\n            [dt.year, dt.month, dt.day] + TIME_EMPTY_TIME + TIME_EMPTY_EXTRAS\n        )\n    else:\n        raise NotImplementedError(\n            \"Cannot convert %s to `struct_time`\" % type(dt))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a struct_time based on the one provided but with the extra fields tm_wday tm_yday tm_isdst and tm_hour tm_min tm_sec.", "response": "def trim_struct_time(st, strip_time=False):\n    \"\"\"\n    Return a `struct_time` based on the one provided but with the extra fields\n    `tm_wday`, `tm_yday`, and `tm_isdst` reset to default values.\n\n    If `strip_time` is set to true the time value are also set to zero:\n    `tm_hour`, `tm_min`, and `tm_sec`.\n    \"\"\"\n    if strip_time:\n        return struct_time(list(st[:3]) + TIME_EMPTY_TIME + TIME_EMPTY_EXTRAS)\n    else:\n        return struct_time(list(st[:6]) + TIME_EMPTY_EXTRAS)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef struct_time_to_jd(st):\n    year, month, day = st[:3]\n    hours, minutes, seconds = st[3:6]\n\n    # Convert time of day to fraction of day\n    day += jdutil.hmsm_to_days(hours, minutes, seconds)\n\n    return jdutil.date_to_jd(year, month, day)", "response": "Convert a struct_time to Julian Date."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a Julian Date float number to struct_time.", "response": "def jd_to_struct_time(jd):\n    \"\"\"\n    Return a `struct_time` converted from a Julian Date float number.\n\n    WARNING: Conversion to then from Julian Date value to `struct_time` can be\n    inaccurate and lose or gain time, especially for BC (negative) years.\n\n    NOTE: extra fields `tm_wday`, `tm_yday`, and `tm_isdst` are set to default\n    values, not real ones.\n    \"\"\"\n    year, month, day = jdutil.jd_to_date(jd)\n\n    # Convert time of day from fraction of day\n    day_fraction = day - int(day)\n    hour, minute, second, ms = jdutil.days_to_hmsm(day_fraction)\n    day = int(day)\n\n    # This conversion can return negative values for items we do not want to be\n    # negative: month, day, hour, minute, second.\n    year, month, day, hour, minute, second = _roll_negative_time_fields(\n        year, month, day, hour, minute, second)\n\n    return struct_time(\n        [year, month, day, hour, minute, second] + TIME_EMPTY_EXTRAS\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _roll_negative_time_fields(year, month, day, hour, minute, second):\n    if second < 0:\n        minute += int(second / 60.0)  # Adjust by whole minute in secs\n        minute -= 1  # Subtract 1 for negative second\n        second %= 60  # Convert negative second to positive remainder\n    if minute < 0:\n        hour += int(minute / 60.0)  # Adjust by whole hour in minutes\n        hour -= 1  # Subtract 1 for negative minutes\n        minute %= 60  # Convert negative minute to positive remainder\n    if hour < 0:\n        day += int(hour / 24.0)  # Adjust by whole day in hours\n        day -= 1  # Subtract 1 for negative minutes\n        hour %= 24  # Convert negative hour to positive remainder\n    if day < 0:\n        month += int(day / 30.0)  # Adjust by whole month in days (assume 30)\n        month -= 1  # Subtract 1 for negative minutes\n        day %= 30  # Convert negative day to positive remainder\n    if month < 0:\n        year += int(month / 12.0)  # Adjust by whole year in months\n        year -= 1  # Subtract 1 for negative minutes\n        month %= 12  # Convert negative month to positive remainder\n    return (year, month, day, hour, minute, second)", "response": "Roll negative time fields in the base object to positive values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_example_by_schema(cls, schema, ignored_schemas=None, paths=None, name=''):\n        if schema.schema_example:\n            return schema.schema_example\n\n        if ignored_schemas is None:\n            ignored_schemas = []\n\n        if paths is None:\n            paths = []\n\n        if name:\n            paths = list(map(lambda path: '.'.join((path, name)), paths))\n\n        if schema.ref_path:\n            paths.append(schema.ref_path)\n\n        if schema.schema_id in ignored_schemas:\n            result = [] if schema.is_array else {}\n        else:\n            schemas = ignored_schemas + [schema.schema_id]\n            kwargs = dict(\n                ignored_schemas=schemas,\n                paths=paths\n            )\n            if schema.is_array:\n                result = cls.get_example_for_array(\n                    schema.item, **kwargs)\n            elif schema.type in PRIMITIVE_TYPES:\n                result = cls.get_example_value_for_primitive_type(\n                    schema.type, schema.raw, schema.type_format, paths=paths\n                )\n            elif schema.all_of:\n                result = {}\n                for _schema_id in schema.all_of:\n                    schema = SchemaObjects.get(_schema_id)\n                    result.update(cls.get_example_by_schema(schema, **kwargs))\n            else:\n                result = cls.get_example_for_object(\n                    schema.properties, nested=schema.nested_schemas, **kwargs)\n        return result", "response": "Get example for a given schema object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_body_example(cls, operation):\n        path = \"#/paths/'{0.path}'/{0.method}/parameters/{name}\".format(\n            operation, name=operation.body.name or 'body')\n        return cls.get_example_by_schema(operation.body, paths=[path])", "response": "Get example for body parameter example by operation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting example for response object by operation object", "response": "def get_response_example(cls, operation, response):\n        \"\"\" Get example for response object by operation object\n\n        :param Operation operation: operation object\n        :param Response response: response object\n        \"\"\"\n        path = \"#/paths/'{}'/{}/responses/{}\".format(\n            operation.path, operation.method, response.name)\n        kwargs = dict(paths=[path])\n\n        if response.type in PRIMITIVE_TYPES:\n            result = cls.get_example_value_for_primitive_type(\n                response.type, response.properties, response.type_format, **kwargs)\n        else:\n            schema = SchemaObjects.get(response.type)\n            result = cls.get_example_by_schema(schema, **kwargs)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting example for header object", "response": "def get_header_example(cls, header):\n        \"\"\" Get example for header object\n\n        :param Header header: Header object\n        :return: example\n        :rtype: dict\n        \"\"\"\n        if header.is_array:\n            result = cls.get_example_for_array(header.item)\n        else:\n            example_method = getattr(cls, '{}_example'.format(header.type))\n            result = example_method(header.properties, header.type_format)\n        return {header.name: result}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_property_example(cls, property_, nested=None, **kw):\n        paths = kw.get('paths', [])\n        name = kw.get('name', '')\n        result = None\n        if name and paths:\n            paths = list(map(lambda path: '.'.join((path, name)), paths))\n            result, path = cls._get_custom_example(paths)\n            if result is not None and property_['type'] in PRIMITIVE_TYPES:\n                cls._example_validate(\n                    path, result, property_['type'], property_['type_format'])\n                return result\n\n        if SchemaObjects.contains(property_['type']):\n            schema = SchemaObjects.get(property_['type'])\n            if result is not None:\n                if schema.is_array:\n                    if not isinstance(result, list):\n                        result = [result] * cls.EXAMPLE_ARRAY_ITEMS_COUNT\n                else:\n                    if isinstance(result, list):\n                        cls.logger.warning(\n                            'Example type mismatch in path {}'.format(schema.ref_path))\n            else:\n                result = cls.get_example_by_schema(schema, **kw)\n\n            if (not result) and schema.nested_schemas:\n                for _schema_id in schema.nested_schemas:\n                    _schema = SchemaObjects.get(_schema_id)\n                    if _schema:\n                        if isinstance(_schema, SchemaMapWrapper):\n                            result[_schema.name] = cls.get_example_by_schema(_schema, **kw)\n                        elif _schema.nested_schemas:\n                            for _schema__id in _schema.nested_schemas:\n                                _schema_ = SchemaObjects.get(_schema__id)\n                                if isinstance(_schema_, SchemaMapWrapper):\n                                    result[_schema.name] = cls.get_example_by_schema(_schema_, **kw)\n        else:\n            result = cls.get_example_value_for_primitive_type(\n                property_['type'],\n                property_['type_properties'],\n                property_['type_format'],\n                **kw\n            )\n        return result", "response": "Get example for property\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a directory tree for the resized assets", "response": "def mkres(self):\n        \"\"\"\n        Create a directory tree for the resized assets\n        \"\"\"\n        for d in DENSITY_TYPES:\n            if d == 'ldpi' and not self.ldpi:\n                continue  # skip ldpi\n            if d == 'xxxhdpi' and not self.xxxhdpi:\n                continue  # skip xxxhdpi\n\n            try:\n                path = os.path.join(self.out, 'res/drawable-%s' % d)\n                os.makedirs(path, 0o755)\n            except OSError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the new image size for the target density", "response": "def get_size_for_density(self, size, target_density):\n        \"\"\"\n        Return the new image size for the target density\n        \"\"\"\n        current_size = size\n        current_density = DENSITY_MAP[self.source_density]\n        target_density = DENSITY_MAP[target_density]\n\n        return int(current_size * (target_density / current_density))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates assets from the given image and path in case you have already called Image. open called Image. close", "response": "def resize_image(self, path, im):\n        \"\"\"\n        Generate assets from the given image and path in case you've already\n        called Image.open\n        \"\"\"\n        # Get the original filename\n        _, filename = os.path.split(path)\n\n        # Generate the new filename\n        filename = self.get_safe_filename(filename)\n        filename = '%s%s' % (self.prefix if self.prefix else '', filename)\n\n        # Get the original image size\n        w, h = im.size\n\n        # Generate assets from the source image\n        for d in DENSITY_TYPES:\n            if d == 'ldpi' and not self.ldpi:\n                continue  # skip ldpi\n            if d == 'xxxhdpi' and not self.xxxhdpi:\n                continue  # skip xxxhdpi\n\n            out_file = os.path.join(self.out,\n                    self.get_out_for_density(d), filename)\n\n            if d == self.source_density:\n                im.save(out_file, quality=self.image_quality)\n            else:\n                size = (self.get_size_for_density(w, d),\n                        self.get_size_for_density(h, d))\n                im.resize(size, self.image_filter).save(out_file,\n                        quality=self.image_quality)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the records serialized as Transit / json in utf8", "response": "def encode_transit(records):\n    '''Returns the records serialized as Transit/json in utf8'''\n    with StringIO() as buf:\n        writer = Writer(buf, \"json\")\n        writer.write(records)\n        return buf.getvalue().encode('utf8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes a message to the table.", "response": "def push(self, message, callback_arg=None):\n        \"\"\"message should be a dict recognized by the Stitch Import API.\n\n        See https://www.stitchdata.com/docs/integrations/import-api.\n        \"\"\"\n\n        if message['action'] == 'upsert':\n            message.setdefault('key_names', self.key_names)\n\n        message['client_id'] = self.client_id\n        message.setdefault('table_name', self.table_name)\n\n        self._add_message(message, callback_arg)\n\n        batch = self._take_batch(self.target_messages_per_batch)\n        if batch:\n            self._send_batch(batch)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all the records in the buffer and then clears the buffer.", "response": "def _take_batch(self, min_records):\n        '''If we have enough data to build a batch, returns all the data in the\n        buffer and then clears the buffer.'''\n\n        if not self._buffer:\n            return []\n\n        enough_messages = len(self._buffer) >= min_records\n        enough_time = time.time() - self.time_last_batch_sent >= self.batch_delay_seconds\n        ready = enough_messages or enough_time\n\n        if not ready:\n            return []\n\n        result = list(self._buffer)\n        self._buffer.clear()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget parameters list by location.", "response": "def get_parameters_by_location(self, locations=None, excludes=None):\n        \"\"\" Get parameters list by location\n\n        :param locations: list of locations\n        :type locations: list or None\n        :param excludes: list of excludes locations\n        :type excludes: list or None\n        :return: list of Parameter\n        :rtype: list\n        \"\"\"\n        result = self.parameters\n        if locations:\n            result = filter(lambda x: x.location_in in locations, result)\n        if excludes:\n            result = filter(lambda x: x.location_in not in excludes, result)\n        return list(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the body request parameter", "response": "def body(self):\n        \"\"\" Return body request parameter\n\n        :return: Body parameter\n        :rtype: Parameter or None\n        \"\"\"\n        body = self.get_parameters_by_location(['body'])\n        return self.root.schemas.get(body[0].type) if body else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate EDTF string equivalent of a given natural language date string.", "response": "def text_to_edtf(text):\n    \"\"\"\n    Generate EDTF string equivalent of a given natural language date string.\n    \"\"\"\n    if not text:\n        return\n\n    t = text.lower()\n\n    # try parsing the whole thing\n    result = text_to_edtf_date(t)\n\n    if not result:\n        # split by list delims and move fwd with the first thing that returns a non-empty string.\n        # TODO: assemble multiple dates into a {} or [] structure.\n        for split in [\",\", \";\", \"or\"]:\n            for list_item in t.split(split):\n\n                # try parsing as an interval - split by '-'\n                toks = list_item.split(\"-\")\n                if len(toks) == 2:\n                    d1 = toks[0].strip()\n                    d2 = toks[1].strip()\n\n                    # match looks from the beginning of the string, search\n                    # looks anywhere.\n\n                    if re.match(r'\\d\\D\\b', d2):  # 1-digit year partial e.g. 1868-9\n                        if re.search(r'\\b\\d\\d\\d\\d$', d1):  # TODO: evaluate it and see if it's a year\n                            d2 = d1[-4:-1] + d2\n                    elif re.match(r'\\d\\d\\b', d2):  # 2-digit year partial e.g. 1809-10\n                        if re.search(r'\\b\\d\\d\\d\\d$', d1):\n                            d2 = d1[-4:-2] + d2\n                    else:\n                        century_range_match = re.search(r'\\b(\\d\\d)(th|st|nd|rd|)-(\\d\\d)(th|st|nd|rd) [cC]', \"%s-%s\" % (d1,d2))\n                        if century_range_match:\n                            g = century_range_match.groups()\n                            d1 = \"%sC\" % g[0]\n                            d2 = \"%sC\" % g[2]\n\n                    r1 = text_to_edtf_date(d1)\n                    r2 = text_to_edtf_date(d2)\n\n                    if r1 and r2:\n                        result = r1 + \"/\" + r2\n                        return result\n\n                # is it an either/or year \"1838/1862\" - that has a different\n                # representation in EDTF. If it's 'both', then we use {}. If\n                # it's 'or' then we use []. Assuming the latter for now.\n                # This whole section could be more friendly.\n\n                else:\n                    int_match = re.search(r\"(\\d\\d\\d\\d)\\/(\\d\\d\\d\\d)\", list_item)\n                    if int_match:\n                        return \"[%s, %s]\" % (int_match.group(1), int_match.group(2))\n\n                result = text_to_edtf_date(list_item)\n                if result:\n                    break\n            if result:\n                break\n\n    is_before = re.findall(r'\\bbefore\\b', t)\n    is_before = is_before or re.findall(r'\\bearlier\\b', t)\n\n    is_after = re.findall(r'\\bafter\\b', t)\n    is_after = is_after or re.findall(r'\\bsince\\b', t)\n    is_after = is_after or re.findall(r'\\blater\\b', t)\n\n    if is_before:\n        result = u\"unknown/%s\" % result\n    elif is_after:\n        result = u\"%s/unknown\" % result\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef text_to_edtf_date(text):\n    if not text:\n        return\n\n    t = text.lower()\n    result = ''\n\n    for reject_re in REJECT_RULES:\n        if re.match(reject_re, t):\n            return\n\n    # matches on '1800s'. Needs to happen before is_decade.\n    could_be_century = re.findall(r'(\\d{2}00)s', t)\n    # matches on '1800s' and '1910s'. Removes the 's'.\n    # Needs to happen before is_uncertain because e.g. \"1860s?\"\n    t, is_decade = re.subn(r'(\\d{3}0)s', r'\\1', t)\n\n    # detect approximation signifiers\n    # a few 'circa' abbreviations just before the year\n    is_approximate = re.findall(r'\\b(ca?\\.?) ?\\d{4}', t)\n    # the word 'circa' anywhere\n    is_approximate = is_approximate or re.findall(r'\\bcirca\\b', t)\n    # the word 'approx'/'around'/'about' anywhere\n    is_approximate = is_approximate or \\\n                     re.findall(r'\\b(approx|around|about)', t)\n    # a ~ before a year-ish number\n    is_approximate = is_approximate or re.findall(r'\\b~\\d{4}', t)\n    # a ~ at the beginning\n    is_approximate = is_approximate or re.findall(r'^~', t)\n\n    # detect uncertainty signifiers\n    t, is_uncertain = re.subn(r'(\\d{4})\\?', r'\\1', t)\n    # the words uncertain/maybe/guess anywhere\n    is_uncertain = is_uncertain or re.findall(\n        r'\\b(uncertain|possibly|maybe|guess)', t)\n\n    # detect century forms\n    is_century = re.findall(CENTURY_RE, t)\n\n    # detect CE/BCE year form\n    is_ce = re.findall(CE_RE, t)\n    if is_century:\n        result = \"%02dxx\" % (int(is_century[0][0]) - 1,)\n        is_approximate = is_approximate or \\\n                         re.findall(r'\\b(ca?\\.?) ?' + CENTURY_RE, t)\n        is_uncertain = is_uncertain or re.findall(CENTURY_RE + r'\\?', t)\n\n        try:\n            is_bc = is_century[0][-1] in (\"bc\", \"bce\")\n            if is_bc:\n                result = \"-%s\" % result\n        except IndexError:\n            pass\n\n    elif is_ce:\n        result = \"%04d\" % (int(is_ce[0][0]))\n        is_approximate = is_approximate or \\\n                         re.findall(r'\\b(ca?\\.?) ?' + CE_RE, t)\n        is_uncertain = is_uncertain or re.findall(CE_RE + r'\\?', t)\n\n        try:\n            is_bc = is_ce[0][-1] in (\"bc\", \"bce\")\n            if is_bc:\n                result = \"-%s\" % result\n        except IndexError:\n            pass\n\n    else:\n        # try dateutil.parse\n\n        try:\n            # parse twice, using different defaults to see what was\n            # parsed and what was guessed.\n            dt1 = parse(\n                t,\n                dayfirst=appsettings.DAY_FIRST,\n                yearfirst=False,\n                fuzzy=True,  # force a match, even if it's default date\n                default=DEFAULT_DATE_1\n            )\n\n            dt2 = parse(\n                t,\n                dayfirst=appsettings.DAY_FIRST,\n                yearfirst=False,\n                fuzzy=True,  # force a match, even if it's default date\n                default=DEFAULT_DATE_2\n            )\n\n        except ValueError:\n            return\n\n        if dt1.date() == DEFAULT_DATE_1.date() and \\\n                dt2.date() == DEFAULT_DATE_2.date():\n            # couldn't parse anything - defaults are untouched.\n            return\n\n        date1 = dt1.isoformat()[:10]\n        date2 = dt2.isoformat()[:10]\n\n        # guess precision of 'unspecified' characters to use\n        mentions_year = re.findall(r'\\byear\\b.+(in|during)\\b', t)\n        mentions_month = re.findall(r'\\bmonth\\b.+(in|during)\\b', t)\n        mentions_day = re.findall(r'\\bday\\b.+(in|during)\\b', t)\n\n        for i in xrange(len(date1)):\n            # if the given year could be a century (e.g. '1800s') then use\n            # approximate/uncertain markers to decide whether we treat it as\n            # a century or a decade.\n            if i == 2 and could_be_century and \\\n                not (is_approximate or is_uncertain):\n                result += 'x'\n            elif i == 3 and is_decade > 0:\n                if mentions_year:\n                    result += 'u'  # year precision\n                else:\n                    result += 'x'  # decade precision\n            elif date1[i] == date2[i]:\n                # since both attempts at parsing produced the same result\n                # it must be parsed value, not a default\n                result += date1[i]\n            else:\n                # different values were produced, meaning that it's likely\n                # a default. Use 'unspecified'\n                result += \"u\"\n\n        # strip off unknown chars from end of string - except the first 4\n\n        for i in reversed(xrange(len(result))):\n            if result[i] not in ('u', 'x', '-'):\n                smallest_length = 4\n\n                if mentions_month:\n                    smallest_length = 7\n                if mentions_day:\n                    smallest_length = 10\n\n                limit = max(smallest_length, i + 1)\n                result = result[:limit]\n                break\n\n        # check for seasons\n        if \"spring\" in t:\n            result = result[:4] + \"-21\" + result[7:]\n        elif \"summer\" in t:\n            result = result[:4] + \"-22\" + result[7:]\n        elif \"autumn\" in t or \"fall\" in t:\n            result = result[:4] + \"-23\" + result[7:]\n        elif \"winter\" in t:\n            result = result[:4] + \"-24\" + result[7:]\n\n            # end dateutil post-parsing\n\n    if is_uncertain:\n        result += \"?\"\n\n    if is_approximate:\n        result += \"~\"\n\n    # weed out bad parses\n    if result.startswith(\"uu-uu\"):\n        return None\n\n    return result", "response": "Converts a natural language date string to an EDTF date string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the canonical representative equivalent to node.", "response": "def find(node):\n    \"\"\"Find current canonical representative equivalent to node.\n\n    Adjust the parent pointer of each node along the way to the root\n    to point directly at the root for inverse-Ackerman-fast access.\n    \"\"\"\n    if node.parent is None:\n        return node\n    root = node\n    while root.parent is not None:\n        root = root.parent\n    parent = node\n    while parent.parent is not root:\n        grandparent = parent.parent\n        parent.parent = root\n        parent = grandparent\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasserting equality of two nodes a and b so find ( a ) is find ( b so find ( a ) is find ( b so find ( a ) is find ( b so find ( a ) is find ( b", "response": "def union(a, b):\n    \"\"\"Assert equality of two nodes a and b so find(a) is find(b).\"\"\"\n    a = find(a)\n    b = find(b)\n    if a is not b:\n        if a.rank < b.rank:\n            a.parent = b\n        elif b.rank < a.rank:\n            b.parent = a\n        else:\n            b.parent = a\n            a.rank += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef classes(equivalences):\n    node = OrderedDict()\n    def N(x):\n        if x in node:\n            return node[x]\n        n = node[x] = Node(x)\n        return n\n    for x, y in equivalences:\n        union(N(x), N(y))\n    eqclass = OrderedDict()\n    for x, n in node.iteritems():\n        x_ = find(n).element\n        if x_ not in eqclass:\n            eqclass[x_] = []\n        eqclass[x_].append(x)\n        eqclass[x] = eqclass[x_]\n    return eqclass", "response": "Compute mapping from element to list of equivalent elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ascii2h5(dat_fname, h5_fname):\n    table = np.loadtxt(dat_fname, skiprows=1, dtype='f4')\n\n    filter_kwargs = dict(\n        chunks=True,\n        compression='gzip',\n        compression_opts=3)\n\n    # Filter out pixels with all zeros\n    idx = ~np.all(table[:,2:32] < 1.e-5, axis=1)\n\n    with h5py.File(h5_fname, 'w') as f:\n        d = np.arange(0., 4.351, 0.15).astype('f4')\n\n        dset = f.create_dataset('dists', data=d, **filter_kwargs)\n        dset.attrs['description'] = 'Distances at which extinction is measured'\n        dset.attrs['units'] = 'kpc'\n\n        dset = f.create_dataset('pix_lb', data=table[idx,0:2], **filter_kwargs)\n        dset.attrs['description'] = 'Galactic (l, b) of each pixel'\n        dset.attrs['units'] = 'deg'\n\n        dset = f.create_dataset('A_r', data=table[idx,2:32], **filter_kwargs)\n        dset.attrs['description'] = 'Extinction'\n        dset.attrs['shape'] = '(pixel, distance)'\n        dset.attrs['band'] = 'r'\n        dset.attrs['units'] = 'mag'\n\n        dset = f.create_dataset('A_r_err', data=table[idx,32:], **filter_kwargs)\n        dset.attrs['description'] = 'Gaussian uncertainty in extinction'\n        dset.attrs['shape'] = '(pixel, distance)'\n        dset.attrs['band'] = 'r'\n        dset.attrs['units'] = 'mag'", "response": "Convert an ASCII. dat file to a HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch(clobber=False):\n\n    dest_dir = fname_pattern = os.path.join(data_dir(), 'chen2014')\n    url = 'http://lamost973.pku.edu.cn/site/Photometric-Extinctions-and-Distances/table2.dat'\n    dat_fname = os.path.join(dest_dir, 'chen2014.dat')\n    h5_fname = os.path.join(dest_dir, 'chen2014.h5')\n    md5 = 'f8a2bc46d411c57ca4c76dc344e291f1'\n\n    # Check if file already exists\n    if not clobber:\n        h5_size = 52768768 # Guess, in Bytes\n        h5_dsets = {\n            'dists': (30,),\n            'pix_lb': (557398, 2),\n            'A_r': (557398, 30),\n            'A_r_err': (557398, 30)\n        }\n        if fetch_utils.h5_file_exists(h5_fname, h5_size, dsets=h5_dsets):\n            print('File appears to exist already. Call `fetch(clobber=True)` '\n                  'to force overwriting of existing file.')\n            return\n\n    # Download the table\n    print('Downloading {}'.format(url))\n    fetch_utils.download_and_verify(url, md5, fname=dat_fname)\n\n    # Convert from ASCII to HDF5 format\n    print('Repacking files...')\n    ascii2h5(dat_fname, h5_fname)\n\n    # Cleanup\n    print('Removing original file...')\n    os.remove(dat_fname)", "response": "Downloads the Chen et al. ( 2017 ) dust map."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the internal data structure of the internal data structure of the internal data structure of the internal data structure of the internal data structure of the internal data structure.", "response": "def query(self, coords, return_sigma=False):\n        \"\"\"\n        Returns r-band extinction, A_r, at the given coordinates. Can also\n        return uncertainties.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): The coordinates to query.\n            return_sigma (Optional[:obj:`bool`]): If ``True``, returns the uncertainty in\n                extinction as well. Defaults to ``False``.\n\n        Returns:\n            Extinction in the r-band at the specified coordinates, in mags.\n            The shape of the output depends on whether :obj:`coords` contains\n            distances.\n\n            If :obj:`coords` does not specify distance(s), then the shape of the\n            output begins with :obj:`coords.shape`. If :obj:`coords` does specify\n            distance(s), then the shape of the output begins with\n            ``coords.shape + ([number of distance bins],)``.\n        \"\"\"\n        n_coords_ret = coords.shape[0]\n\n        # Determine if distance has been requested\n        has_dist = hasattr(coords.distance, 'kpc')\n        d = coords.distance.kpc if has_dist else None\n\n        # Convert coordinates to pixel indices\n        pix_idx = self._coords2idx(coords)\n\n        # Determine which coordinates are out of bounds\n        mask_idx = (pix_idx == self._n_pix)\n        if np.any(mask_idx):\n            pix_idx[mask_idx] = 0\n\n        # Which distances to extract\n        if has_dist:\n            d = coords.distance.kpc\n            dist_idx_ceil = np.searchsorted(self._dists, d)\n\n            ret = np.empty((n_coords_ret,), dtype='f8')\n            if return_sigma:\n                sigma_ret = np.empty((n_coords_ret,), dtype='f8')\n\n            # d < d(nearest distance slice)\n            idx_near = (dist_idx_ceil == 0) & ~mask_idx\n            print('d < d(nearest): {:d}'.format(np.sum(idx_near)))\n            if np.any(idx_near):\n                a = d[idx_near] / self._dists[0]\n                ret[idx_near] = a[:] * self._A[pix_idx[idx_near], 0]\n                if return_sigma:\n                    sigma_ret[idx_near] = a[:] * self._sigma_A[pix_idx[idx_near], 0]\n\n            # d > d(farthest distance slice)\n            idx_far = (dist_idx_ceil == self._n_dists) & ~mask_idx\n            print('d > d(farthest): {:d}'.format(np.sum(idx_far)))\n            if np.any(idx_far):\n                ret[idx_far] = self._A[pix_idx[idx_far], -1]\n                if return_sigma:\n                    sigma_ret[idx_far] = self._sigma_A[pix_idx[idx_far], -1]\n\n            # d(nearest distance slice) < d < d(farthest distance slice)\n            idx_btw = ~idx_near & ~idx_far & ~mask_idx\n            print('d(nearest) < d < d(farthest): {:d}'.format(np.sum(idx_btw)))\n            if np.any(idx_btw):\n                d_ceil = self._dists[dist_idx_ceil[idx_btw]]\n                d_floor = self._dists[dist_idx_ceil[idx_btw]-1]\n                a = (d_ceil - d[idx_btw]) / (d_ceil - d_floor)\n                ret[idx_btw] = (\n                    (1.-a[:]) * self._A[pix_idx[idx_btw], dist_idx_ceil[idx_btw]]\n                    +    a[:] * self._A[pix_idx[idx_btw], dist_idx_ceil[idx_btw]-1])\n                if return_sigma:\n                    w0 = (1.-a)**2\n                    w1 = a**2\n                    norm = 1. / (w0 + w1)\n                    w0 *= norm\n                    w1 *= norm\n                    sigma_ret[idx_btw] = np.sqrt(\n                        w0 * self._sigma_A[pix_idx[idx_btw], dist_idx_ceil[idx_btw]]**2\n                        + w1 * self._sigma_A[pix_idx[idx_btw], dist_idx_ceil[idx_btw]-1]**2\n                    )\n        else:\n            # TODO: Harmonize order of distances & samples with Bayestar.\n            ret = self._A[pix_idx, :]\n            if return_sigma:\n                sigma_ret = self._sigma_A[pix_idx, :]\n\n        if np.any(mask_idx):\n            ret[mask_idx] = np.nan\n            if return_sigma:\n                sigma_ret[mask_idx] = np.nan\n\n        if return_sigma:\n            return ret, sigma_ret\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lpad(msg, symbol, length):\n    if len(msg) >= length:\n        return msg\n    return symbol * (length - len(msg)) + msg", "response": "Left - pad a given string with a given character and length. Returns the padded string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange a string s characters from one base to another.", "response": "def changebase(string, frm, to, minlen=0):\n    \"\"\"\n    Change a string's characters from one base to another.\n    Return the re-encoded string\n    \"\"\"\n    if frm == to:\n        return lpad(string, get_code_string(frm)[0], minlen)\n\n    return encode(decode(string, frm), to, minlen)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef num_to_var_int(x):\n    x = int(x)\n    if x < 253:\n        return from_int_to_byte(x)\n\n    elif x < 65536:\n        return from_int_to_byte(253) + encode(x, 256, 2)[::-1]\n\n    elif x < 4294967296:\n        return from_int_to_byte(254) + encode(x, 256, 4)[::-1]\n\n    else:\n        return from_int_to_byte(255) + encode(x, 256, 8)[::-1]", "response": "convert an integer into a variable - length integer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(val, base, minlen=0):\n\n    base, minlen = int(base), int(minlen)\n    code_string = get_code_string(base)\n    result = \"\"\n    while val > 0:\n        result = code_string[val % base] + result\n        val //= base\n    return code_string[0] * max(minlen - len(result), 0) + result", "response": "Encode an integer value into a string of symbols with the given base."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a string and a numeric base decode it into an integer.", "response": "def decode(string, base):\n    \"\"\"\n    Given a string (string) and a numeric base (base),\n    decode the string into an integer.\n\n    Returns the integer\n    \"\"\"\n\n    base = int(base)\n    code_string = get_code_string(base)\n    result = 0\n    if base == 16:\n        string = string.lower()\n    while len(string) > 0:\n        result *= base\n        result += code_string.find(string[0])\n        string = string[1:]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json_is_base(obj, base):\n\n    alpha = get_code_string(base)\n    if isinstance(obj, (str, unicode)):\n        for i in range(len(obj)):\n            if alpha.find(obj[i]) == -1:\n                return False\n\n        return True\n\n    elif isinstance(obj, (int, long, float)) or obj is None:\n        return True\n\n    elif isinstance(obj, list):\n        for i in range(len(obj)):\n            if not json_is_base(obj[i], base):\n                return False\n\n        return True\n\n    else:\n        for x in obj:\n            if not json_is_base(obj[x], base):\n                return False\n\n        return True", "response": "Given a JSON object and a numeric base verify whether or not the object is a base."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a primitive compound Python object and a changer function that takes a dict int or list and applies the changer function to the object and each sub - component of the object.", "response": "def json_changebase(obj, changer):\n    \"\"\"\n    Given a primitive compound Python object (i.e. a dict,\n    string, int, or list) and a changer function that takes\n    a primitive Python object as an argument, apply the\n    changer function to the object and each sub-component.\n\n    Return the newly-reencoded object.\n    \"\"\"\n\n    if isinstance(obj, (str, unicode)):\n        return changer(obj)\n\n    elif isinstance(obj, (int, long)) or obj is None:\n        return obj\n\n    elif isinstance(obj, list):\n        return [json_changebase(x, changer) for x in obj]\n\n    elif isinstance(obj, dict):\n        return dict((x, json_changebase(obj[x], changer)) for x in obj)\n\n    else:\n        raise ValueError(\"Invalid object\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_CrossCatClient(client_type, **kwargs):\n    client = None\n\n    if client_type == 'local':\n        import crosscat.LocalEngine as LocalEngine\n        le = LocalEngine.LocalEngine(**kwargs)\n        client = CrossCatClient(le)\n\n    elif client_type == 'multiprocessing':\n        import crosscat.MultiprocessingEngine as MultiprocessingEngine\n        me =  MultiprocessingEngine.MultiprocessingEngine(**kwargs)\n        client = CrossCatClient(me)\n\n    else:\n        raise Exception('unknown client_type: %s' % client_type)\n\n    return client", "response": "Helper which instantiates the appropriate Engine and returns a Client"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries the value of the set at the given coordinates.", "response": "def query(self, coords):\n        \"\"\"\n        Args:\n            coords (`astropy.coordinates.SkyCoord`): The coordinates to query.\n\n        Returns:\n            A float array of the value of the map at the given coordinates. The\n            shape of the output is the same as the shape of the coordinates\n            stored by `coords`.\n        \"\"\"\n        pix_idx = coord2healpix(coords, self._frame,\n                                self._nside, nest=self._nest)\n        return self._pix_val[pix_idx]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serialize(xp_ast):\n    '''Generate token strings which, when joined together, form a valid\n    XPath serialization of the AST.'''\n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield tok\n    elif isinstance(xp_ast, string_types):\n        # strings in serialized xpath needed to be quoted\n        # (e.g. for use in paths, comparisons, etc)\n        # using repr to quote them; for unicode, the leading\n        # u (u'') needs to be removed.\n        yield repr(xp_ast).lstrip('u')\n    else:\n        yield str(xp_ast)", "response": "Generate token strings which when joined together form a valid\n    XPath serialization of the AST."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(python=PYTHON):\n    clean()\n    local(\n        \"LIBRARY_PATH={library_path} CPATH={include_path} {python} \"\n        \"setup.py build_ext --inplace\".format(\n            library_path=LIBRARY_PATH,\n            include_path=INCLUDE_PATH,\n            python=python,\n        ))", "response": "Build the bigfloat library for in - place testing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall into site - packages", "response": "def install(python=PYTHON):\n    \"\"\"Install into site-packages\"\"\"\n    local(\n        \"LIBRARY_PATH={library_path} CPATH={include_path} {python} \"\n        \"setup.py build\".format(\n            library_path=LIBRARY_PATH,\n            include_path=INCLUDE_PATH,\n            python=python,\n        ))\n    local(\"sudo {python} setup.py install\".format(python=python))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uninstall(python=PYTHON):\n    site_packages = local(\n        \"{python} -c 'from distutils.sysconfig import \"\n        \"get_python_lib; print(get_python_lib())'\".format(python=python),\n        capture=True,\n    )\n    with lcd(site_packages):\n        local(\"sudo rm mpfr.so\")\n        local(\"sudo rm -fr bigfloat\")\n        local(\"sudo rm bigfloat*.egg-info\")", "response": "Uninstall from site - packages"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sync_virtualchain(blockchain_opts, last_block, state_engine, expected_snapshots={}, tx_filter=None ):\n\n    rc = False\n    start = datetime.datetime.now()\n    while True:\n        try:\n\n            # advance state\n            rc = indexer.StateEngine.build(blockchain_opts, last_block + 1, state_engine, expected_snapshots=expected_snapshots, tx_filter=tx_filter )\n            break\n        \n        except Exception, e:\n            log.exception(e)\n            log.error(\"Failed to synchronize chain; exiting to safety\")\n            os.abort()\n\n    time_taken = \"%s seconds\" % (datetime.datetime.now() - start).seconds\n    log.info(time_taken)\n\n    return rc", "response": "Synchronize the virtual blockchain state up to a given block."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef virtualchain_set_opfields( op, **fields ):\n\n    # warn about unsupported fields\n    for f in fields.keys():\n        if f not in indexer.RESERVED_KEYS:\n            log.warning(\"Unsupported virtualchain field '%s'\" % f)\n\n    # propagate reserved fields\n    for f in fields.keys():\n        if f in indexer.RESERVED_KEYS:\n            op[f] = fields[f]\n\n    return op", "response": "Pass along reserved fields to a virtualchain operation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload the IPHAS 3D dust map of Sale et al.", "response": "def fetch(clobber=False):\n    \"\"\"\n    Downloads the IPHAS 3D dust map of Sale et al. (2014).\n\n    Args:\n        clobber (Optional[bool]): If ``True``, any existing file will be\n            overwritten, even if it appears to match. If ``False`` (the\n            default), ``fetch()`` will attempt to determine if the dataset\n            already exists. This determination is not 100\\% robust against data\n            corruption.\n    \"\"\"\n\n    dest_dir = fname_pattern = os.path.join(data_dir(), 'iphas')\n    url_pattern = 'http://www.iphas.org/data/extinction/A_samp_{:03d}.tar.gz'\n    fname_pattern = os.path.join(dest_dir, 'A_samp_') + '{:03d}.tar.gz'\n\n    # Check if file already exists\n    if not clobber:\n        h5_fname = os.path.join(dest_dir, 'iphas.h5')\n        h5_size = 227817543 # Guess, in Bytes\n        h5_dsets = {\n            'samples': (61130,)\n        }\n        if fetch_utils.h5_file_exists(h5_fname, h5_size, dsets=h5_dsets):\n            print('File appears to exist already. Call `fetch(clobber=True)` '\n                  'to force overwriting of existing file.')\n            return\n\n    # Expected MD5 sums of .samp files\n    file_md5sum = {\n        30:  'dd531e397622bc97d4ff92b6c7863ade',\n        40:  'b0f925eb3e46b77876e4054a26ad5b52',\n        50:  'ea3b9500f0419d66dd92d9f9c127c2b5',\n        60:  'cccf136f4e2306a6038e8093499216fd',\n        70:  'a05fe2f815086686056c18087cc5410b',\n        80:  '799bf618c8827b3d7250c884ec66ec49',\n        90:  'd2a302d917da768bacf6ea74cb9dcfad',\n        100: '2c75e31ad9320818556c4c9964b6af65',\n        110: '742ea8de6f5f8a7e549f6c56b0088789',\n        120: '9beabfa2c9634f953adadb5016eab072',\n        130: '7cd7313f466eb60e8318d0f1bd32e035',\n        140: 'fb6d09e4d939081b891e245c30b791f1',\n        150: '8e9b6dc1561183aeadc64f41c85a64a8',\n        160: '8a35828457b7b1d53d06998114553674',\n        170: '7ffb29ec23e2f625dcfaaa84c293821d',\n        180: 'c737da479d132b88483d6ddab5b25fc8',\n        190: '9bc5fc7f7ba55f36a167473bb3679601',\n        200: '7d8ffc4aa2f7c7026d8aa3ffb670d48e',\n        210: 'e31b04964b7970b81fc90c120b4ebc24'\n    }\n\n    # Download the .samp files\n    for key in file_md5sum:\n        url = url_pattern.format(key)\n        print('Downloading {}'.format(url))\n\n        fetch_utils.download_and_verify(\n            url,\n            file_md5sum[key],\n            fname_pattern.format(key))\n\n    # Convert from ASCII to HDF5 format\n    print('Repacking files...')\n    ascii2h5(dest_dir, os.path.join(dest_dir, 'iphas.h5'))\n\n    # Cleanup\n    print('Removing original files...')\n    for key in file_md5sum:\n        os.remove(fname_pattern.format(key))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the internal map for the given coordinates.", "response": "def query(self, coords, mode='random_sample'):\n        \"\"\"\n        Returns A0 at the given coordinates. There are several different query\n        modes, which handle the probabilistic nature of the map differently.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): The coordinates to query.\n            mode (Optional[:obj:`str`]): Five different query modes are available:\n                ``'random_sample'``, ``'random_sample_per_pix'``, ``'samples'``,\n                ``'median'`` and ``'mean'``. The ``mode`` determines how the output\n                will reflect the probabilistic nature of the IPHAS dust map.\n\n        Returns:\n            Monochromatic extinction, A0, at the specified coordinates, in mags.\n            The shape of the output depends on the ``mode``, and on whether\n            ``coords`` contains distances.\n\n            If ``coords`` does not specify distance(s), then the shape of the\n            output begins with `coords.shape`. If `coords` does specify\n            distance(s), then the shape of the output begins with\n            ``coords.shape + ([number of distance bins],)``.\n\n            If ``mode`` is ``'random_sample'``, then at each coordinate/distance, a\n            random sample of reddening is given.\n\n            If ``mode`` is ``'random_sample_per_pix'``, then the sample chosen for\n            each angular pixel of the map will be consistent. For example, if\n            two query coordinates lie in the same map pixel, then the same\n            random sample will be chosen from the map for both query\n            coordinates.\n\n            If ``mode`` is ``'median'``, then at each coordinate/distance, the\n            median reddening is returned.\n\n            If ``mode`` is ``'mean'``, then at each coordinate/distance, the mean\n            reddening is returned.\n\n            Finally, if ``mode`` is ``'samples'``, then all at each\n            coordinate/distance, all samples are returned.\n        \"\"\"\n\n        # Check that the query mode is supported\n        valid_modes = [\n            'random_sample',\n            'random_sample_per_pix',\n            'samples',\n            'median',\n            'mean']\n\n        if mode not in valid_modes:\n            raise ValueError(\n                '\"{}\" is not a valid `mode`. Valid modes are:\\n'\n                '  {}'.format(mode, valid_modes))\n\n        n_coords_ret = coords.shape[0]\n\n        # Determine if distance has been requested\n        has_dist = hasattr(coords.distance, 'kpc')\n        d = coords.distance.kpc if has_dist else None\n\n        # Convert coordinates to pixel indices\n        pix_idx = self._coords2idx(coords)\n\n        # Determine which coordinates are out of bounds\n        mask_idx = (pix_idx == self._n_pix)\n        if np.any(mask_idx):\n            pix_idx[mask_idx] = 0\n\n        # Which samples to extract\n        if mode == 'random_sample':\n            samp_idx = np.random.randint(0, self._n_samples, pix_idx.size)\n            n_samp_ret = 1\n        elif mode == 'random_sample_per_pix':\n            samp_idx = np.random.randint(0, self._n_samples, self._n_pix)[pix_idx]\n            n_sample_ret = 1\n        else:\n            samp_idx = slice(None)\n            n_samp_ret = self._n_samples\n\n        # Which distances to extract\n        if has_dist:\n            d = coords.distance.pc\n            dist_idx_ceil = np.searchsorted(self._dists, d)\n\n            if isinstance(samp_idx, slice):\n                ret = np.empty((n_coords_ret, n_samp_ret), dtype='f4')\n            else:\n                ret = np.empty((n_coords_ret,), dtype='f4')\n\n            # d < d(nearest distance slice)\n            idx_near = (dist_idx_ceil == 0)\n            if np.any(idx_near):\n                a = d[idx_near] / self._dists[0]\n                if isinstance(samp_idx, slice):\n                    ret[idx_near] = a[:,None] * self._data['A0'][pix_idx[idx_near], 0, samp_idx]\n                else:\n                    ret[idx_near] = a[:] * self._data['A0'][pix_idx[idx_near], 0, samp_idx[idx_near]]\n\n            # d > d(farthest distance slice)\n            idx_far = (dist_idx_ceil == self._n_dists)\n            if np.any(idx_far):\n                if isinstance(samp_idx, slice):\n                    ret[idx_far] = self._data['A0'][pix_idx[idx_far], -1, samp_idx]\n                else:\n                    ret[idx_far] = self._data['A0'][pix_idx[idx_far], -1, samp_idx[idx_far]]\n\n            # d(nearest distance slice) < d < d(farthest distance slice)\n            idx_btw = ~idx_near & ~idx_far\n\n            if np.any(idx_btw):\n                d_ceil = self._dists[dist_idx_ceil[idx_btw]]\n                d_floor = self._dists[dist_idx_ceil[idx_btw]-1]\n                a = (d_ceil - d[idx_btw]) / (d_ceil - d_floor)\n                if isinstance(samp_idx, slice):\n                    ret[idx_btw] = (\n                        (1.-a[:,None]) * self._data['A0'][pix_idx[idx_btw], dist_idx_ceil[idx_btw], samp_idx]\n                        +    a[:,None] * self._data['A0'][pix_idx[idx_btw], dist_idx_ceil[idx_btw]-1, samp_idx])\n                else:\n                    ret[idx_btw] = (\n                        (1.-a[:]) * self._data['A0'][pix_idx[idx_btw], dist_idx_ceil[idx_btw], samp_idx[idx_btw]]\n                        +    a[:] * self._data['A0'][pix_idx[idx_btw], dist_idx_ceil[idx_btw]-1, samp_idx[idx_btw]])\n        else:\n            # TODO: Harmonize order of distances & samples with Bayestar.\n            ret = self._data['A0'][pix_idx, :, samp_idx]\n\n        # Reduce the samples in the requested manner\n        samp_axis = 1 if has_dist else 2\n\n        if mode == 'median':\n            ret = np.median(ret, axis=samp_axis)\n        elif mode == 'mean':\n            ret = np.mean(ret, axis=samp_axis)\n\n        if np.any(mask_idx):\n            ret[mask_idx] = np.nan\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading the Peek & Graves dust map and placing it in the data directory for the dust map.", "response": "def fetch():\n    \"\"\"\n    Downloads the Peek & Graves (2010) dust map, placing it in\n    the data directory for :obj:`dustmap`.\n    \"\"\"\n    doi = '10.7910/DVN/VBSI4A'\n    \n    for component in ['dust', 'err']:\n        requirements = {'filename': 'PG_{}_4096_ngp.fits'.format(component)}\n        local_fname = os.path.join(\n            data_dir(),\n            'pg2010', 'PG_{}_4096_ngp.fits'.format(component))\n        print('Downloading P&G (2010) {} data file to {}'.format(\n            component, local_fname))\n        fetch_utils.dataverse_download_doi(\n            doi,\n            local_fname,\n            file_requirements=requirements)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries the SFD - 9E E ( B - V ) corrections at the specified coordinates.", "response": "def query(self, coords, order=1):\n        \"\"\"\n        Returns the P&G (2010) correction to the SFD'98 E(B-V) at the specified\n        location(s) on the sky. If component is 'err', then return the\n        uncertainty in the correction.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): The coordinates to query.\n            order (Optional[:obj:`int`]): Interpolation order to use. Defaults to ``1``,\n                for linear interpolation.\n\n        Returns:\n            A float array containing the P&G (2010) correction (or its\n            uncertainty) to SFD'98 at every input coordinate. The shape\n            of the output will be the same as the shape of the coordinates\n            stored by :obj:`coords`.\n        \"\"\"\n        return super(PG2010Query, self).query(coords, order=order)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a construction table uses valid references. Raises an exception if not.", "response": "def _check_construction_table(construction_table):\n        \"\"\"Checks if a construction table uses valid references.\n        Raises an exception (UndefinedCoordinateSystem) otherwise.\n        \"\"\"\n        c_table = construction_table\n        for row, i in enumerate(c_table.index):\n            give_message = (\"Not a valid construction table. \"\n                            \"The index {i} uses an invalid reference\").format\n            if row == 0:\n                pass\n            elif row == 1:\n                if c_table.loc[i, 'b'] not in c_table.index[:row]:\n                    raise UndefinedCoordinateSystem(give_message(i=i))\n            elif row == 2:\n                reference = c_table.loc[i, ['b', 'a']]\n                if not reference.isin(c_table.index[:row]).all():\n                    raise UndefinedCoordinateSystem(give_message(i=i))\n            else:\n                reference = c_table.loc[i, ['b', 'a', 'd']]\n                if not reference.isin(c_table.index[:row]).all():\n                    raise UndefinedCoordinateSystem(give_message(i=i))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a construction table for a fragment of the molecule.", "response": "def _get_frag_constr_table(self, start_atom=None, predefined_table=None,\n                               use_lookup=None, bond_dict=None):\n        \"\"\"Create a construction table for a Zmatrix.\n\n        A construction table is basically a Zmatrix without the values\n        for the bond lenghts, angles and dihedrals.\n        It contains the whole information about which reference atoms\n        are used by each atom in the Zmatrix.\n\n        This method creates a so called \"chemical\" construction table,\n        which makes use of the connectivity table in this molecule.\n\n        By default the first atom is the one nearest to the centroid.\n        (Compare with :meth:`~Cartesian.get_centroid()`)\n\n        Args:\n            start_atom: An index for the first atom may be provided.\n            predefined_table (pd.DataFrame): An uncomplete construction table\n                may be provided. The rest is created automatically.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            pd.DataFrame: Construction table\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        def modify_priority(bond_dict, user_defined):\n            def move_to_start(dct, key):\n                \"Due to PY27 compatibility\"\n                keys = dct.keys()\n                if key in keys and key != keys[0]:\n                    root = dct._OrderedDict__root\n                    first = root[1]\n                    link = dct._OrderedDict__map[key]\n                    link_prev, link_next, _ = link\n                    link_prev[1] = link_next\n                    link_next[0] = link_prev\n                    link[0] = root\n                    link[1] = first\n                    root[1] = first[0] = link\n                else:\n                    raise KeyError\n\n            for j in reversed(user_defined):\n                try:\n                    try:\n                        bond_dict.move_to_end(j, last=False)\n                    except AttributeError:\n                        # No move_to_end method in python 2.x\n                        move_to_start(bond_dict, j)\n                except KeyError:\n                    pass\n\n        if start_atom is not None and predefined_table is not None:\n            raise IllegalArgumentCombination('Either start_atom or '\n                                             'predefined_table has to be None')\n        if bond_dict is None:\n            bond_dict = self._give_val_sorted_bond_dict(use_lookup=use_lookup)\n        if predefined_table is not None:\n            self._check_construction_table(predefined_table)\n            construction_table = predefined_table.copy()\n\n        if predefined_table is None:\n            if start_atom is None:\n                molecule = self.get_distance_to(self.get_centroid())\n                i = molecule['distance'].idxmin()\n            else:\n                i = start_atom\n            order_of_def = [i]\n            user_defined = []\n            construction_table = {i: {'b': 'origin',\n                                      'a': 'e_z',\n                                      'd': 'e_x'}}\n        else:\n            i = construction_table.index[0]\n            order_of_def = list(construction_table.index)\n            user_defined = list(construction_table.index)\n            construction_table = construction_table.to_dict(orient='index')\n\n        visited = {i}\n        if len(self) > 1:\n            parent = {j: i for j in bond_dict[i]}\n            work_bond_dict = OrderedDict(\n                [(j, bond_dict[j] - visited) for j in bond_dict[i]])\n            modify_priority(work_bond_dict, user_defined)\n        else:\n            parent, work_bond_dict = {}, {}\n\n        while work_bond_dict:\n            new_work_bond_dict = OrderedDict()\n            for i in work_bond_dict:\n                if i in visited:\n                    continue\n                if i not in user_defined:\n                    b = parent[i]\n                    if b in order_of_def[:3]:\n                        if len(order_of_def) == 1:\n                            construction_table[i] = {'b': b,\n                                                     'a': 'e_z',\n                                                     'd': 'e_x'}\n                        elif len(order_of_def) == 2:\n                            a = (bond_dict[b] & set(order_of_def))[0]\n                            construction_table[i] = {'b': b, 'a': a,\n                                                     'd': 'e_x'}\n                        else:\n                            try:\n                                a = parent[b]\n                            except KeyError:\n                                a = (bond_dict[b] & set(order_of_def))[0]\n                            try:\n                                d = parent[a]\n                                if d in set([b, a]):\n                                    message = \"Don't make self references\"\n                                    raise UndefinedCoordinateSystem(message)\n                            except (KeyError, UndefinedCoordinateSystem):\n                                try:\n                                    d = ((bond_dict[a] & set(order_of_def))\n                                         - set([b, a]))[0]\n                                except IndexError:\n                                    d = ((bond_dict[b] & set(order_of_def))\n                                         - set([b, a]))[0]\n                            construction_table[i] = {'b': b, 'a': a, 'd': d}\n                    else:\n                        a, d = [construction_table[b][k] for k in ['b', 'a']]\n                        construction_table[i] = {'b': b, 'a': a, 'd': d}\n                    order_of_def.append(i)\n\n                visited.add(i)\n                for j in work_bond_dict[i]:\n                    new_work_bond_dict[j] = bond_dict[j] - visited\n                    parent[j] = i\n\n            work_bond_dict = new_work_bond_dict\n            modify_priority(work_bond_dict, user_defined)\n        output = pd.DataFrame.from_dict(construction_table, orient='index')\n        output = output.loc[order_of_def, ['b', 'a', 'd']]\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a construction table for a Zmatrix. A construction table is basically a Zmatrix without the values for the bond lengths, angles and dihedrals. It contains the whole information about which reference atoms are used by each atom in the Zmatrix. The absolute references in cartesian space are one of the following magic strings:: ['origin', 'e_x', 'e_y', 'e_z'] This method creates a so called \"chemical\" construction table, which makes use of the connectivity table in this molecule. Args: fragment_list (sequence): There are four possibilities to specify the sequence of fragments: 1. A list of tuples is given. Each tuple contains the fragment with its corresponding construction table in the form of:: [(frag1, c_table1), (frag2, c_table2)...] If the construction table of a fragment is not complete, the rest of each fragment's construction table is calculated automatically. 2. It is possible to omit the construction tables for some or all fragments as in the following example:: [(frag1, c_table1), frag2, (frag3, c_table3)...] 3. If ``self`` contains more atoms than the union over all fragments, the rest of the molecule without the fragments is automatically prepended using :meth:`~Cartesian.get_without`:: self.get_without(fragments) + fragment_list 4. If fragment_list is ``None`` then fragmentation, etc. is done automatically. The fragments are then sorted by their number of atoms, in order to use the largest fragment as reference for the other ones. use_lookup (bool): Use a lookup variable for :meth:`~chemcoord.Cartesian.get_bonds`. The default is specified in ``settings['defaults']['use_lookup']`` perform_checks (bool): The checks for invalid references are performed using :meth:`~chemcoord.Cartesian.correct_dihedral` and :meth:`~chemcoord.Cartesian.correct_absolute_refs`. Returns: :class:`pandas.DataFrame`: Construction table", "response": "def get_construction_table(self, fragment_list=None,\n                               use_lookup=None,\n                               perform_checks=True):\n        \"\"\"Create a construction table for a Zmatrix.\n\n        A construction table is basically a Zmatrix without the values\n        for the bond lengths, angles and dihedrals.\n        It contains the whole information about which reference atoms\n        are used by each atom in the Zmatrix.\n\n        The absolute references in cartesian space are one of the following\n        magic strings::\n\n            ['origin', 'e_x', 'e_y', 'e_z']\n\n        This method creates a so called \"chemical\" construction table,\n        which makes use of the connectivity table in this molecule.\n\n        Args:\n            fragment_list (sequence): There are four possibilities to specify\n                the sequence of fragments:\n\n                1. A list of tuples is given. Each tuple contains the fragment\n                with its corresponding construction table in the form of::\n\n                    [(frag1, c_table1), (frag2, c_table2)...]\n\n                If the construction table of a fragment is not complete,\n                the rest of each fragment's\n                construction table is calculated automatically.\n\n                2. It is possible to omit the construction tables for some\n                or all fragments as in the following example::\n\n                    [(frag1, c_table1), frag2, (frag3, c_table3)...]\n\n                3. If ``self`` contains more atoms than the union over all\n                fragments, the rest of the molecule without the fragments\n                is automatically prepended using\n                :meth:`~Cartesian.get_without`::\n\n                    self.get_without(fragments) + fragment_list\n\n                4. If fragment_list is ``None`` then fragmentation, etc.\n                is done automatically. The fragments are then sorted by\n                their number of atoms, in order to use the largest fragment\n                as reference for the other ones.\n\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n            perform_checks (bool): The checks for invalid references are\n                performed using :meth:`~chemcoord.Cartesian.correct_dihedral`\n                and :meth:`~chemcoord.Cartesian.correct_absolute_refs`.\n\n        Returns:\n            :class:`pandas.DataFrame`: Construction table\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        if fragment_list is None:\n            self.get_bonds(use_lookup=use_lookup)\n            self._give_val_sorted_bond_dict(use_lookup=use_lookup)\n            fragments = sorted(self.fragmentate(use_lookup=use_lookup),\n                               key=len, reverse=True)\n            # During function execution the bonding situation does not change,\n            # so the lookup may be used now.\n            use_lookup = True\n        else:\n            fragments = fragment_list\n\n        def prepend_missing_parts_of_molecule(fragment_list):\n            for fragment in fragment_list:\n                if pd.api.types.is_list_like(fragment):\n                    try:\n                        full_index |= fragment[0].index\n                    except NameError:\n                        full_index = fragment[0].index\n                else:\n                    try:\n                        full_index |= fragment.index\n                    except NameError:\n                        full_index = fragment.index\n\n            if not self.index.difference(full_index).empty:\n                missing_part = self.get_without(self.loc[full_index],\n                                                use_lookup=use_lookup)\n                fragment_list = missing_part + fragment_list\n            return fragment_list\n\n        fragments = prepend_missing_parts_of_molecule(fragments)\n\n        if isinstance(fragments[0], tuple):\n            fragment, references = fragments[0]\n            full_table = fragment._get_frag_constr_table(\n                use_lookup=use_lookup, predefined_table=references)\n        else:\n            fragment = fragments[0]\n            full_table = fragment._get_frag_constr_table(use_lookup=use_lookup)\n\n        for fragment in fragments[1:]:\n            finished_part = self.loc[full_table.index]\n            if pd.api.types.is_list_like(fragment):\n                fragment, references = fragment\n                if len(references) < min(3, len(fragment)):\n                    raise ValueError('If you specify references for a '\n                                     'fragment, it has to consist of at least'\n                                     'min(3, len(fragment)) rows.')\n                constr_table = fragment._get_frag_constr_table(\n                    predefined_table=references, use_lookup=use_lookup)\n            else:\n                i, b = fragment.get_shortest_distance(finished_part)[:2]\n                constr_table = fragment._get_frag_constr_table(\n                    start_atom=i, use_lookup=use_lookup)\n                if len(full_table) == 1:\n                    a, d = 'e_z', 'e_x'\n                elif len(full_table) == 2:\n                    if b == full_table.index[0]:\n                        a = full_table.index[1]\n                    else:\n                        a = full_table.index[0]\n                    d = 'e_x'\n                else:\n                    if b in full_table.index[:2]:\n                        if b == full_table.index[0]:\n                            a = full_table.index[2]\n                            d = full_table.index[1]\n                        else:\n                            a = full_table.loc[b, 'b']\n                            d = full_table.index[2]\n                    else:\n                        a, d = full_table.loc[b, ['b', 'a']]\n\n                if len(constr_table) >= 1:\n                    constr_table.iloc[0, :] = b, a, d\n                if len(constr_table) >= 2:\n                    constr_table.iloc[1, [1, 2]] = b, a\n                if len(constr_table) >= 3:\n                    constr_table.iloc[2, 2] = b\n\n            full_table = pd.concat([full_table, constr_table])\n\n        c_table = full_table\n        if perform_checks:\n            c_table = self.correct_dihedral(c_table)\n            c_table = self.correct_dihedral(c_table, use_lookup=use_lookup)\n            c_table = self.correct_absolute_refs(c_table)\n        return c_table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_dihedral(self, construction_table):\n        c_table = construction_table\n        angles = self.get_angle_degrees(c_table.iloc[3:, :].values)\n        problem_index = np.nonzero((175 < angles) | (angles < 5))[0]\n        rename = dict(enumerate(c_table.index[3:]))\n        problem_index = [rename[i] for i in problem_index]\n        return problem_index", "response": "Checks if the dihedral defining atom is colinear. Returns a list of indices starting from the third row of the construction_table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef correct_dihedral(self, construction_table,\n                         use_lookup=None):\n        \"\"\"Reindexe the dihedral defining atom if linear reference is used.\n\n        Uses :meth:`~Cartesian.check_dihedral` to obtain the problematic\n        indices.\n\n        Args:\n            construction_table (pd.DataFrame):\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            pd.DataFrame: Appropiately renamed construction table.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        problem_index = self.check_dihedral(construction_table)\n        bond_dict = self._give_val_sorted_bond_dict(use_lookup=use_lookup)\n        c_table = construction_table.copy()\n        for i in problem_index:\n            loc_i = c_table.index.get_loc(i)\n            b, a, problem_d = c_table.loc[i, ['b', 'a', 'd']]\n            try:\n                c_table.loc[i, 'd'] = (bond_dict[a] - {b, a, problem_d}\n                                       - set(c_table.index[loc_i:]))[0]\n            except IndexError:\n                visited = set(c_table.index[loc_i:]) | {b, a, problem_d}\n                tmp_bond_dict = OrderedDict([(j, bond_dict[j] - visited)\n                                             for j in bond_dict[problem_d]])\n                found = False\n                while tmp_bond_dict and not found:\n                    new_tmp_bond_dict = OrderedDict()\n                    for new_d in tmp_bond_dict:\n                        if new_d in visited:\n                            continue\n                        angle = self.get_angle_degrees([b, a, new_d])[0]\n                        if 5 < angle < 175:\n                            found = True\n                            c_table.loc[i, 'd'] = new_d\n                        else:\n                            visited.add(new_d)\n                            for j in tmp_bond_dict[new_d]:\n                                new_tmp_bond_dict[j] = bond_dict[j] - visited\n                    tmp_bond_dict = new_tmp_bond_dict\n                if not found:\n                    other_atoms = c_table.index[:loc_i].difference({b, a})\n                    molecule = self.get_distance_to(origin=i, sort=True,\n                                                    other_atoms=other_atoms)\n                    k = 0\n                    while not found and k < len(molecule):\n                        new_d = molecule.index[k]\n                        angle = self.get_angle_degrees([b, a, new_d])[0]\n                        if 5 < angle < 175:\n                            found = True\n                            c_table.loc[i, 'd'] = new_d\n                        k = k + 1\n                    if not found:\n                        message = ('The atom with index {} has no possibility '\n                                   'to get nonlinear reference atoms'.format)\n                        raise UndefinedCoordinateSystem(message(i))\n        return c_table", "response": "Reindexe the dihedral defining atom if linear reference is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if i uses valid absolute references.", "response": "def _has_valid_abs_ref(self, i, construction_table):\n        \"\"\"Checks, if ``i`` uses valid absolute references.\n\n        Checks for each index from first to third row of the\n        ``construction_table``, if the references are colinear.\n        This case has to be specially treated, because the references\n        are not only atoms (to fix internal degrees of freedom) but also points\n        in cartesian space called absolute references.\n        (to fix translational and rotational degrees of freedom)\n\n        Args:\n            i (label): The label has to be in the first three rows.\n            construction_table (pd.DataFrame):\n\n        Returns:\n            bool:\n        \"\"\"\n        c_table = construction_table\n        abs_refs = constants.absolute_refs\n        A = np.empty((3, 3))\n        row = c_table.index.get_loc(i)\n        if row > 2:\n            message = 'The index {i} is not from the first three, rows'.format\n            raise ValueError(message(i=i))\n        for k in range(3):\n            if k < row:\n                A[k] = self.loc[c_table.iloc[row, k], ['x', 'y', 'z']]\n            else:\n                A[k] = abs_refs[c_table.iloc[row, k]]\n        v1, v2 = A[2] - A[1], A[1] - A[0]\n        K = np.cross(v1, v2)\n        zero = np.full(3, 0.)\n        return not (np.allclose(K, zero) or np.allclose(v1, zero)\n                    or np.allclose(v2, zero))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_absolute_refs(self, construction_table):\n        c_table = construction_table\n        problem_index = [i for i in c_table.index[:3]\n                         if not self._has_valid_abs_ref(i, c_table)]\n        return problem_index", "response": "Checks for absolute references in the construction table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef correct_absolute_refs(self, construction_table):\n        c_table = construction_table.copy()\n        abs_refs = constants.absolute_refs\n        problem_index = self.check_absolute_refs(c_table)\n        for i in problem_index:\n            order_of_refs = iter(permutations(abs_refs.keys()))\n            finished = False\n            while not finished:\n                if self._has_valid_abs_ref(i, c_table):\n                    finished = True\n                else:\n                    row = c_table.index.get_loc(i)\n                    c_table.iloc[row, row:] = next(order_of_refs)[row:3]\n        return c_table", "response": "Reindexe construction_table if linear reference in first three rows\n        present."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the Zmat from a construction table.", "response": "def _build_zmat(self, construction_table):\n        \"\"\"Create the Zmatrix from a construction table.\n\n        Args:\n            Construction table (pd.DataFrame):\n\n        Returns:\n            Zmat: A new instance of :class:`Zmat`.\n        \"\"\"\n        c_table = construction_table\n        default_cols = ['atom', 'b', 'bond', 'a', 'angle', 'd', 'dihedral']\n        optional_cols = list(set(self.columns) - {'atom', 'x', 'y', 'z'})\n\n        zmat_frame = pd.DataFrame(columns=default_cols + optional_cols,\n                                  dtype='float', index=c_table.index)\n\n        zmat_frame.loc[:, optional_cols] = self.loc[c_table.index,\n                                                    optional_cols]\n\n        zmat_frame.loc[:, 'atom'] = self.loc[c_table.index, 'atom']\n        zmat_frame.loc[:, ['b', 'a', 'd']] = c_table\n\n        zmat_values = self._calculate_zmat_values(c_table)\n        zmat_frame.loc[:, ['bond', 'angle', 'dihedral']] = zmat_values\n\n        zmatrix = Zmat(zmat_frame, metadata=self.metadata,\n                       _metadata={'last_valid_cartesian': self.copy()})\n        return zmatrix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform to internal coordinates. Transforming to internal coordinates involves basically three steps: 1. Define an order of how to build and define for each atom the used reference atoms. 2. Check for problematic local linearity. In this algorithm an angle with ``170 < angle < 10`` is assumed to be linear. This is not the mathematical definition, but makes it safer against \"floating point noise\" 3. Calculate the bond lengths, angles and dihedrals using the references defined in step 1 and 2. In the first two steps a so called ``construction_table`` is created. This is basically a Zmatrix without the values for the bonds, angles and dihedrals hence containing only the information about the used references. ChemCoord uses a :class:`pandas.DataFrame` with the columns ``['b', 'a', 'd']``. Look into :meth:`~chemcoord.Cartesian.get_construction_table` for more information. It is important to know, that calculating the construction table is a very costly step since the algoritym tries to make some guesses based on connectivity to create a \"chemical\" zmatrix. If you create several zmatrices based on the same references you can obtain the construction table of a zmatrix with ``Zmat_instance.loc[:, ['b', 'a', 'd']]`` If you then pass the buildlist as argument to ``give_zmat``, the algorithm directly starts with step 3 (which is much faster). If a ``construction_table`` is passed into :meth:`~Cartesian.get_zmat` the check for pathological linearity is not performed! So if a ``construction_table`` is either manually created, or obtained from :meth:`~Cartesian.get_construction_table` under the option ``perform_checks = False``, it is recommended to use the following methods: * :meth:`~Cartesian.correct_dihedral` * :meth:`~Cartesian.correct_absolute_refs` If you want to check for problematic indices in order to solve the invalid references yourself, use the following methods: * :meth:`~Cartesian.check_dihedral` * :meth:`~Cartesian.check_absolute_refs` Args: construction_table (pandas.DataFrame): use_lookup (bool): Use a lookup variable for :meth:`~chemcoord.Cartesian.get_bonds`. The default is specified in ``settings['defaults']['use_lookup']`` Returns: Zmat: A new instance of :class:`~Zmat`.", "response": "def get_zmat(self, construction_table=None,\n                 use_lookup=None):\n        \"\"\"Transform to internal coordinates.\n\n        Transforming to internal coordinates involves basically three\n        steps:\n\n        1. Define an order of how to build and define for each atom\n        the used reference atoms.\n\n        2. Check for problematic local linearity. In this algorithm an\n        angle with ``170 < angle < 10`` is assumed to be linear.\n        This is not the mathematical definition, but makes it safer\n        against \"floating point noise\"\n\n        3. Calculate the bond lengths, angles and dihedrals using the\n        references defined in step 1 and 2.\n\n        In the first two steps a so called ``construction_table`` is created.\n        This is basically a Zmatrix without the values for the bonds, angles\n        and dihedrals hence containing only the information about the used\n        references. ChemCoord uses a :class:`pandas.DataFrame` with the columns\n        ``['b', 'a', 'd']``. Look into\n        :meth:`~chemcoord.Cartesian.get_construction_table` for more\n        information.\n\n        It is important to know, that calculating the construction table\n        is a very costly step since the algoritym tries to make some guesses\n        based on connectivity to create a \"chemical\" zmatrix.\n\n        If you create several zmatrices based on the same references\n        you can obtain the construction table of a zmatrix with\n        ``Zmat_instance.loc[:, ['b', 'a', 'd']]``\n        If you then pass the buildlist as argument to ``give_zmat``,\n        the algorithm directly starts with step 3 (which is much faster).\n\n        If a ``construction_table`` is passed into :meth:`~Cartesian.get_zmat`\n        the check for pathological linearity is not performed!\n        So if a ``construction_table`` is either manually created,\n        or obtained from :meth:`~Cartesian.get_construction_table`\n        under the option ``perform_checks = False``, it is recommended to use\n        the following methods:\n\n            * :meth:`~Cartesian.correct_dihedral`\n            * :meth:`~Cartesian.correct_absolute_refs`\n\n        If you want to check for problematic indices in order to solve the\n        invalid references yourself, use the following methods:\n\n            * :meth:`~Cartesian.check_dihedral`\n            * :meth:`~Cartesian.check_absolute_refs`\n\n        Args:\n            construction_table (pandas.DataFrame):\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            Zmat: A new instance of :class:`~Zmat`.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        self.get_bonds(use_lookup=use_lookup)\n        self._give_val_sorted_bond_dict(use_lookup=use_lookup)\n        use_lookup = True\n        # During function execution the connectivity situation won't change\n        # So use_look=True will be used\n        if construction_table is None:\n            c_table = self.get_construction_table(use_lookup=use_lookup)\n            c_table = self.correct_dihedral(c_table, use_lookup=use_lookup)\n            c_table = self.correct_absolute_refs(c_table)\n        else:\n            c_table = construction_table\n        return self._build_zmat(c_table)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_grad_zmat(self, construction_table, as_function=True):\n        if (construction_table.index != self.index).any():\n            message = \"construction_table and self must use the same index\"\n            raise ValueError(message)\n        c_table = construction_table.loc[:, ['b', 'a', 'd']]\n        c_table = c_table.replace(constants.int_label)\n        c_table = c_table.replace({k: v for v, k in enumerate(c_table.index)})\n        c_table = c_table.values.T\n        X = self.loc[:, ['x', 'y', 'z']].values.T\n        if X.dtype == np.dtype('i8'):\n            X = X.astype('f8')\n\n        err, row, grad_C = transformation.get_grad_C(X, c_table)\n        if err == ERR_CODE_InvalidReference:\n            rename = dict(enumerate(self.index))\n            i = rename[row]\n            b, a, d = construction_table.loc[i, ['b', 'a', 'd']]\n            raise InvalidReference(i=i, b=b, a=a, d=d)\n\n        if as_function:\n            return partial(xyz_functions.apply_grad_zmat_tensor,\n                           grad_C, construction_table)\n        else:\n            return grad_C", "response": "r Returns the gradient for the transformation to a Zmatrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_data(self, new_cols=None):\n        atoms = self['atom']\n        data = constants.elements\n        if pd.api.types.is_list_like(new_cols):\n            new_cols = set(new_cols)\n        elif new_cols is None:\n            new_cols = set(data.columns)\n        else:\n            new_cols = [new_cols]\n        new_frame = data.loc[atoms, set(new_cols) - set(self.columns)]\n        new_frame.index = self.index\n        return self.__class__(pd.concat([self._frame, new_frame], axis=1))", "response": "Adds a column with the requested data to the MOLCAS data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the total mass in g / mol.", "response": "def get_total_mass(self):\n        \"\"\"Returns the total mass in g/mol.\n\n        Args:\n            None\n\n        Returns:\n            float:\n        \"\"\"\n        try:\n            mass = self.loc[:, 'mass'].sum()\n        except KeyError:\n            mass_molecule = self.add_data('mass')\n            mass = mass_molecule.loc[:, 'mass'].sum()\n        return mass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_same_sumformula(self, other):\n        same_atoms = True\n        for atom in set(self['atom']):\n            own_atom_number = len(self[self['atom'] == atom])\n            other_atom_number = len(other[other['atom'] == atom])\n            same_atoms = (own_atom_number == other_atom_number)\n            if not same_atoms:\n                break\n        return same_atoms", "response": "Determines if self and other have the same sumformula as other."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the number of electrons in the molecule.", "response": "def get_electron_number(self, charge=0):\n        \"\"\"Return the number of electrons.\n\n        Args:\n            charge (int): Charge of the molecule.\n\n        Returns:\n            int:\n        \"\"\"\n        atomic_number = constants.elements['atomic_number'].to_dict()\n        return sum([atomic_number[atom] for atom in self['atom']]) - charge"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenhance the JSON encoding of the API instance.", "response": "def enhance_json_encode(api_instance, extra_settings=None):\n    \"\"\"use `JSONEncodeManager` replace default `output_json` function of Flask-RESTful\n    for the advantage of use `JSONEncodeManager`, please see https://github.com/anjianshi/json_encode_manager\"\"\"\n    api_instance.json_encoder = JSONEncodeManager()\n\n    dumps_settings = {} if extra_settings is None else extra_settings\n    dumps_settings['default'] = api_instance.json_encoder\n    dumps_settings.setdefault('ensure_ascii', False)\n\n    @api_instance.representation('application/json')\n    def output_json(data, code, headers=None):\n        if current_app.debug:\n            dumps_settings.setdefault('indent', 4)\n            dumps_settings.setdefault('sort_keys', True)\n\n        dumped = json.dumps(data, **dumps_settings)\n        if 'indent' in dumps_settings:\n            dumped += '\\n'\n\n        resp = make_response(dumped, code)\n        resp.headers.extend(headers or {})\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlets API instance can respond jsonp request automatically. `callback_name_source` can be a string or a callback. If it is a string, the system will find the argument that named by this string in `query string`. If found, determine this request to be a jsonp request, and use the argument's value as the js callback name. If `callback_name_source` is a callback, this callback should return js callback name when request is a jsonp request, and return False when request is not jsonp request. And system will handle request according to its return value. default support format\uff1aurl?callback=js_callback_name", "response": "def support_jsonp(api_instance, callback_name_source='callback'):\n    \"\"\"Let API instance can respond jsonp request automatically.\n\n    `callback_name_source` can be a string or a callback.\n        If it is a string, the system will find the argument that named by this string in `query string`.\n         If found, determine this request to be a jsonp request, and use the argument's value as the js callback name.\n\n        If `callback_name_source` is a callback, this callback should return js callback name when request\n         is a jsonp request, and return False when request is not jsonp request.\n         And system will handle request according to its return value.\n\n    default support format\uff1aurl?callback=js_callback_name\n    \"\"\"\n    output_json = api_instance.representations['application/json']\n\n    @api_instance.representation('application/json')\n    def handle_jsonp(data, code, headers=None):\n        resp = output_json(data, code, headers)\n\n        if code == 200:\n            callback = request.args.get(callback_name_source, False) if not callable(callback_name_source) \\\n                else callback_name_source()\n            if callback:\n                resp.set_data(str(callback) + '(' + resp.get_data().decode(\"utf-8\") + ')')\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort_values(self, by, axis=0, ascending=True,\n                    kind='quicksort', na_position='last'):\n        \"\"\"Sort by the values along either axis\n\n        Wrapper around the :meth:`pandas.DataFrame.sort_values` method.\n        \"\"\"\n        return self._frame.sort_values(by, axis=axis, ascending=ascending,\n                                       inplace=False, kind=kind,\n                                       na_position=na_position)", "response": "Sort the dataframes by the values along the specified axis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting the DataFrame by labels along an axis.", "response": "def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\n                   kind='quicksort', na_position='last',\n                   sort_remaining=True, by=None):\n        \"\"\"Sort object by labels (along an axis)\n\n        Wrapper around the :meth:`pandas.DataFrame.sort_index` method.\n        \"\"\"\n        return self._frame.sort_index(axis=axis, level=level,\n                                      ascending=ascending, inplace=inplace,\n                                      kind=kind, na_position=na_position,\n                                      sort_remaining=sort_remaining, by=by)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting column into molecule at specified location.", "response": "def insert(self, loc, column, value, allow_duplicates=False,\n               inplace=False):\n        \"\"\"Insert column into molecule at specified location.\n\n        Wrapper around the :meth:`pandas.DataFrame.insert` method.\n        \"\"\"\n        out = self if inplace else self.copy()\n        out._frame.insert(loc, column, value,\n                          allow_duplicates=allow_duplicates)\n        if not inplace:\n            return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_multisig_segwit_address_from_witness_script(script):\n    script_hash = hashing.bin_sha256(script.decode('hex')).encode('hex')\n    scriptsig_script = '0020' + script_hash\n    addr = btc_make_p2sh_address(scriptsig_script)\n    return addr", "response": "Make a multisig witness address from a p2sh - p2wsh script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a multisig address and redeem script.", "response": "def make_multisig_info( m, pks, compressed=None ):\n    \"\"\"\n    Make a multisig address and redeem script.\n    @m of the given @pks must sign.\n\n    Return {'address': p2sh address, 'redeem_script': redeem script, 'private_keys': private keys, 'segwit': False}\n    * privkeys will be hex-encoded\n    * redeem_script will be hex-encoded\n    \"\"\"\n\n    pubs = []\n    privkeys = []\n    for pk in pks:\n        priv = None\n        if compressed in [True, False]:\n            priv = BitcoinPrivateKey(pk, compressed=compressed)\n        else:\n            priv = BitcoinPrivateKey(pk)\n\n        priv_hex = priv.to_hex()\n        pub_hex = priv.public_key().to_hex()\n\n        privkeys.append(priv_hex)\n        pubs.append(pub_hex)\n\n    script = make_multisig_script(pubs, m)\n    addr = btc_make_p2sh_address(script)\n\n    return {\n        'address': addr,\n        'redeem_script': script,\n        'private_keys': privkeys,\n        'segwit': False,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_multisig_wallet( m, n ):\n\n    if m <= 1 and n <= 1:\n        raise ValueError(\"Invalid multisig parameters\")\n\n    pks = []\n    for i in xrange(0, n):\n        pk = BitcoinPrivateKey(compressed=True).to_wif()\n        pks.append(pk)\n\n    return make_multisig_info( m, pks )", "response": "Create a bundle of information that can be used to generate a multisig script."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a bundle of information that can be used to generate segwit transactions.", "response": "def make_segwit_info(privkey=None):\n    \"\"\"\n    Create a bundle of information\n    that can be used to generate\n    a p2sh-p2wpkh transaction\n    \"\"\"\n\n    if privkey is None:\n        privkey = BitcoinPrivateKey(compressed=True).to_wif()\n\n    return make_multisig_segwit_info(1, [privkey])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_multisig_segwit_wallet( m, n ):\n    pks = []\n    for i in xrange(0, n):\n        pk = BitcoinPrivateKey(compressed=True).to_wif()\n        pks.append(pk)\n\n    return make_multisig_segwit_info(m, pks)", "response": "Create a bundle of information that can be used to generate a multisig witness script."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_multisig_redeemscript( redeem_script_hex ):\n    script_parts = []\n    redeem_script_hex = str(redeem_script_hex)\n\n    try:\n        script_parts = btc_script_deserialize(redeem_script_hex)\n    except:\n        if os.environ.get(\"BLOCKSTACK_TEST\") == \"1\":\n            traceback.print_exc()\n\n        log.error(\"Invalid redeem script %s\" % redeem_script_hex)\n        return None, None\n\n    try:\n        assert len(script_parts) > 2\n        assert script_parts[-1] == OPCODE_VALUES['OP_CHECKMULTISIG']\n        script_parts.pop(-1)\n\n        # get n\n        n = script_parts.pop(-1)\n        pubkeys = []\n\n        # get m\n        m = script_parts.pop(0)\n\n        for i in xrange(0, n):\n            pubk = script_parts.pop(0)\n            \n            # must be a public key\n            BitcoinPublicKey(pubk)\n            pubkeys.append(pubk)\n\n        assert len(script_parts) == 0, \"script_parts = %s\" % script_parts\n        return (m, pubkeys)\n    except Exception, e:\n        if os.environ.get(\"BLOCKSTACK_TEST\") == \"1\":\n            traceback.print_exc()\n\n        log.error(\"Invalid redeem script %s (parses to %s)\" % (redeem_script_hex, script_parts))\n        return (None, None)", "response": "Given a hex string extract the multisig information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_multisig_scriptsig( scriptsig_hex ):\n    try:\n        script_parts = btc_script_deserialize(scriptsig_hex)\n    except:\n        if os.environ.get(\"BLOCKSTACK_TEST\") == \"1\":\n            traceback.print_exc()\n\n        return None\n\n    # sanity check \n    return script_parts", "response": "Given a scriptsig as hex extract the signatures."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the fee for a given transaction.", "response": "def calculate_tx_fee( tx_hex, fee_per_byte ):\n    \"\"\"\n    High-level API call (meant to be blockchain-agnostic)\n    What is the fee for the transaction?\n    \"\"\"\n    txobj = btc_tx_deserialize(tx_hex)\n    tx_num_bytes = len(tx_hex) / 2\n    num_virtual_bytes = None\n\n    if btc_tx_is_segwit(tx_hex):\n        # segwit--discount witness data \n        witness_len = 0\n        for inp in txobj['ins']:\n            witness_len += len(inp['witness_script']) / 2 \n\n        # see https://bitcoincore.org/en/segwit_wallet_dev/#transaction-fee-estimation\n        tx_num_bytes_original = tx_num_bytes - witness_len\n        num_virtual_bytes = 3 * tx_num_bytes_original + tx_num_bytes\n\n    else:\n        # non-segwit \n        num_virtual_bytes = tx_num_bytes * 4\n\n    return (fee_per_byte * num_virtual_bytes) / 4"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the tx fee per byte from the underlying blockchain", "response": "def get_tx_fee_per_byte(bitcoind_opts=None, config_path=None, bitcoind_client=None):\n    \"\"\"\n    Get the tx fee per byte from the underlying blockchain\n    Return the fee on success\n    Return None on error\n    \"\"\"\n    if bitcoind_client is None:\n        bitcoind_client = get_bitcoind_client(bitcoind_opts=bitcoind_opts, config_path=config_path)\n\n    try:\n        # try to confirm in 2-3 blocks\n        try:\n            fee_info = bitcoind_client.estimatesmartfee(2)\n            if 'errors' in fee_info and len(fee_info['errors']) > 0:\n                fee = -1\n            else:\n                fee = fee_info['feerate']\n\n        except JSONRPCException as je:\n            fee = bitcoind_client.estimatefee(2)\n\n        if fee < 0:\n            # if we're testing, then use our own fee\n            if os.environ.get(\"BLOCKSTACK_TEST\") == '1' or os.environ.get(\"BLOCKSTACK_TESTNET\", None) == \"1\":\n                fee = 5500.0 / 10**8\n\n            else:\n                log.error(\"Failed to estimate tx fee\")\n                return None\n        else:\n            log.debug(\"Bitcoin estimatefee(2) is {}\".format(fee))\n\n        fee = float(fee)\n\n        # fee is BTC/kb.  Return satoshis/byte\n        ret = int(round(fee * 10**8 / 1024.0))\n        log.debug(\"Bitcoin estimatefee(2) is {} ({} satoshi/byte)\".format(fee, ret))\n        return ret\n\n    except Exception as e:\n        if os.environ.get(\"BLOCKSTACK_DEBUG\") == '1':\n            log.exception(e)\n\n        log.error(\"Failed to estimate tx fee per byte\")\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the tx fee for a tx", "response": "def get_tx_fee(tx_hex, config_path=None, bitcoind_opts=None, bitcoind_client=None):\n    \"\"\"\n    Get the tx fee for a tx\n    Return the fee on success\n    Return None on error\n    \"\"\"\n    tx_fee_per_byte = get_tx_fee_per_byte(config_path=config_path, bitcoind_opts=bitcoind_opts, bitcoind_client=bitcoind_client)\n    if tx_fee_per_byte is None:\n        return None\n\n    return calculate_tx_fee(tx_hex, tx_fee_per_byte)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tx_estimate_signature_len(privkey_info):\n    if btc_is_singlesig(privkey_info):\n        # one signature produces a scriptsig of ~71 bytes (signature) + pubkey + encoding (4)\n        log.debug(\"Single private key makes a ~73 byte signature\")\n        pubkey = ecdsa_private_key(privkey_info).public_key().to_hex().decode('hex')\n        return 71 + len(pubkey) + 4\n\n    elif btc_is_multisig(privkey_info):\n        # one signature produces a scriptsig of redeem_script + (num_pubkeys * ~74 bytes) + encoding (~6)\n        m, _ = parse_multisig_redeemscript( privkey_info['redeem_script'] )\n        siglengths = 74 * m\n        scriptlen = len(privkey_info['redeem_script']) / 2\n        siglen = 6 + scriptlen + siglengths\n\n        log.debug(\"Multisig private key makes ~{} byte signature\".format(siglen))\n        return siglen\n\n    elif btc_is_singlesig_segwit(privkey_info):\n        # bitcoin p2sh-p2wpkh script\n        # one signature produces (pubkey + signature (~74 bytes)) + scriptsig len\n        privkey = privkey_info['private_keys'][0]\n        pubkey_hex = keylib.key_formatting.compress(ecdsa_private_key(privkey).public_key().to_hex())\n        redeem_script = btc_make_p2sh_p2wpkh_redeem_script(pubkey_hex)\n        witness_script = btc_witness_script_serialize(['00' * 74, pubkey_hex])\n\n        scriptsig_len = 6 + len(redeem_script) / 2\n        witness_len = len(witness_script) / 2\n        siglen = int(round(float(3 * scriptsig_len + (scriptsig_len + witness_len)) / 4))\n        \n        log.debug(\"Segwit p2sh-p2wpkh private key makes ~{} byte signature\".format(siglen))\n        return siglen\n\n    elif btc_is_multisig_segwit(privkey_info):\n        # bitcoin p2sh-p2wsh script\n        # one signature produces (witness script len + num_pubkeys * ~74) + scriptsig len \n        witness_script = privkey_info['redeem_script']\n        m, _ = parse_multisig_redeemscript(witness_script)    \n        redeem_script = btc_make_p2sh_p2wsh_redeem_script(witness_script)\n        \n        siglengths = 74 * m\n        scriptsig_len = 6 + len(redeem_script) / 2\n        witness_len = len(witness_script) / 2 + siglengths\n        siglen = int(round(float(3 * scriptsig_len + (scriptsig_len + witness_len)) / 4))\n\n        log.debug(\"Segwit p2sh-p2wsh private keys make ~{} byte signature\".format(siglen))\n        return siglen\n\n    raise ValueError(\"Unrecognized private key foramt\")", "response": "Estimate the number of bytes required to sign a private key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the problem about sometimes error message specified by programmer won t output to user.", "response": "def handle_error(self, e):\n        \"\"\"\n        Resolve the problem about sometimes error message specified by programmer won't output to user.\n\n        Flask-RESTFul's error handler handling format different exceptions has different behavior.\n        If we raise an normal Exception, it will raise it again.\n\n        If we report error by `restful.abort()`,\n         likes `restful.abort(400, message=\"my_msg\", custom_data=\"value\")`,\n         it will make a response like this:\n\n            Status     400\n            Content    {\"message\": \"my_msg\", \"custom_data\": \"value\"}\n\n        The error message we specified was outputted.\n\n        And if we raise an HTTPException,\n            likes `from werkzeug.exceptions import BadRequest; raise BadRequest('my_msg')`,\n            if will make a response too, but the error message specified by ourselves was lost:\n\n            Status     400\n            Content    {\"status\": 400, \"message\": \"Bad Request\"}\n\n        The reason is, flask-restful always use the `data` attribute of HTTPException to generate response content.\n        But, standard HTTPException object didn't has this attribute.\n        So, we use this method to add it manually.\n\n\n        Some reference material:\n\n        Structure of exceptions raised by restful.abort():\n            code: status code\n            description: predefined error message for this status code\n            data: \uff5b\n            \u3000\u3000\u3000\u3000message: error message\n            \uff5d\n\n        Structure of python2's standard Exception:\n            message: error message\n        Exceptions in python3 didn't has hte `message` attribute, but use `str(exception)` can get it's message.\n\n        Structure of standard `werkzeug.exceptions.HTTPException` (same as BadRequest):\n            code: status code\n            name: the name correspondence to status code\n            description: error message\n        \"\"\"\n        if isinstance(e, HTTPException) and not hasattr(e, 'data'):\n            e.data = dict(message=e.description)\n        return super(ErrorHandledApi, self).handle_error(e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading a schema from a specified URI and save it locally.", "response": "def download_schema(uri, path, comment=None):\n    \"\"\"Download a schema from a specified URI and save it locally.\n\n    :param uri: url where the schema should be downloaded\n    :param path: local file path where the schema should be saved\n    :param comment: optional comment; if specified, will be added to\n        the downloaded schema\n    :returns: true on success, false if there was an error and the\n        schema failed to download\n    \"\"\"\n    # if requests isn't available, warn and bail out\n    if requests is None:\n        sys.stderr.write(req_requests_msg)\n        return\n\n    # short-hand name of the schema, based on uri\n    schema = os.path.basename(uri)\n    try:\n\n        req = requests.get(uri, stream=True)\n        req.raise_for_status()\n        with open(path, 'wb') as schema_download:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk: # filter out keep-alive new chunks\n                    schema_download.write(chunk)\n        # if a comment is specified, add it to the locally saved schema\n        if comment is not None:\n            tree = etree.parse(path)\n            tree.getroot().append(etree.Comment(comment))\n            with open(path, 'wb') as xml_catalog:\n                xml_catalog.write(etree.tostring(tree, pretty_print=True,\n                    xml_declaration=True, encoding=\"UTF-8\"))\n            logger.debug('Downloaded schema %s', schema)\n\n        return True\n\n    except requests.exceptions.HTTPError as err:\n        msg = 'Failed to download schema %s' % schema\n        msg += '(error codes %s)' % err.response.status_code\n        logger.warn(msg)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_catalog(xsd_schemas=None, xmlcatalog_dir=None, xmlcatalog_file=None):\n    # if requests isn't available, warn and bail out\n    if requests is None:\n        sys.stderr.write(req_requests_msg)\n        return\n\n    logger.debug(\"Generating a new XML catalog\")\n    if xsd_schemas is None:\n        xsd_schemas = XSD_SCHEMAS\n\n    if xmlcatalog_file is None:\n        xmlcatalog_file = XMLCATALOG_FILE\n\n    if xmlcatalog_dir is None:\n        xmlcatalog_dir = XMLCATALOG_DIR\n    # if the catalog dir doesn't exist, create it\n    if not os.path.isdir(xmlcatalog_dir):\n        os.mkdir(xmlcatalog_dir)\n\n    # new xml catalog to be populated with saved schemas\n    catalog = Catalog()\n\n    # comment string to be added to locally-saved schemas\n    comment = 'Downloaded by eulxml %s on %s' % \\\n        (__version__, date.today().isoformat())\n\n    for schema_uri in xsd_schemas:\n        filename = os.path.basename(schema_uri)\n        schema_path = os.path.join(xmlcatalog_dir, filename)\n        saved = download_schema(schema_uri, schema_path, comment)\n        if saved:\n            # if download succeeded, add to our catalog.\n            # - name is the schema identifier (uri)\n            # - uri is the local path to load\n            # NOTE: using path relative to catalog file\n            catalog.uri_list.append(Uri(name=schema_uri, uri=filename))\n\n    # if we have any uris in our catalog, write it out\n    if catalog.uri_list:\n        with open(xmlcatalog_file, 'wb') as xml_catalog:\n            catalog.serializeDocument(xml_catalog, pretty=True)\n    return catalog", "response": "Generate an XML catalog for use in resolving schemas"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_index_range(blockchain_name, blockchain_client, impl, working_dir):\n\n    start_block = config.get_first_block_id(impl)\n    try:\n        current_block = get_blockchain_height(blockchain_name, blockchain_client)\n    except Exception, e:\n        log.exception(e)\n        return None, None\n\n    saved_block = StateEngine.get_lastblock(impl, working_dir)\n\n    if saved_block is None:\n        saved_block = 0\n    elif saved_block == current_block:\n        start_block = saved_block\n    elif saved_block < current_block:\n        start_block = saved_block + 1\n\n    return start_block, current_block", "response": "Get the range of block numbers that we need to fetch from the blockchain"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the sqlite3 binary returning the path to the binary on success Return None on error", "response": "def sqlite3_find_tool():\n    \"\"\"\n    Find the sqlite3 binary\n    Return the path to the binary on success\n    Return None on error\n    \"\"\"\n\n    # find sqlite3\n    path = os.environ.get(\"PATH\", None)\n    if path is None:\n        path = \"/usr/local/bin:/usr/bin:/bin\"\n\n    sqlite3_path = None\n    dirs = path.split(\":\")\n    for pathdir in dirs:\n        if len(pathdir) == 0:\n            continue\n\n        sqlite3_path = os.path.join(pathdir, 'sqlite3')\n        if not os.path.exists(sqlite3_path):\n            continue\n\n        if not os.path.isfile(sqlite3_path):\n            continue\n\n        if not os.access(sqlite3_path, os.X_OK):\n            continue\n\n        break\n\n    if sqlite3_path is None:\n        log.error(\"Could not find sqlite3 binary\")\n        return None\n\n    return sqlite3_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbacking up a sqlite3 database, while ensuring that no ongoing queries are being executed. Return True on success Return False on error.", "response": "def sqlite3_backup(src_path, dest_path):\n    \"\"\"\n    Back up a sqlite3 database, while ensuring\n    that no ongoing queries are being executed.\n\n    Return True on success\n    Return False on error.\n    \"\"\"\n\n    # find sqlite3\n    sqlite3_path = sqlite3_find_tool()\n    if sqlite3_path is None:\n        log.error(\"Failed to find sqlite3 tool\")\n        return False\n\n    sqlite3_cmd = [sqlite3_path, '{}'.format(src_path), '.backup \"{}\"'.format(dest_path)]\n    rc = None\n    backoff = 1.0\n\n    out = None\n    err = None\n\n    try:\n        while True:\n            log.debug(\"{}\".format(\" \".join(sqlite3_cmd)))\n            p = subprocess.Popen(sqlite3_cmd, shell=False, close_fds=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            out, err = p.communicate()\n            rc = p.wait()\n\n            if rc != 0:\n                if \"database is locked\" in out.lower() or \"database is locked\" in err.lower():\n                    # try again\n                    log.error(\"Database {} is locked; trying again in {} seconds\".format(src_path, backoff))\n                    time.sleep(backoff)\n                    backoff += 2 * backoff + random.random() * random.randint(0, int(backoff))\n                    continue\n\n                elif 'is not a database' in out.lower() or 'is not a database' in err.lower():\n                    # not a valid sqlite3 file\n                    log.error(\"File {} is not a SQLite database\".format(src_path))\n                    return False\n\n                else:\n                    # some other failure.  Try again\n                    log.error('Failed to back up with \"{}\".  Error log follows.\\n{}'.format(\" \".join(sqlite3_cmd), err))\n                    continue\n            else:\n                break\n\n    except Exception, e:\n        log.exception(e)\n        return False\n\n    if not os.WIFEXITED(rc):\n        # bad exit \n        # failed for some other reason\n        log.error(\"Backup failed: out='{}', err='{}', rc={}\".format(out, err, rc))\n        return False\n    \n    if os.WEXITSTATUS(rc) != 0:\n        # bad exit\n        log.error(\"Backup failed: out='{}', err='{}', exit={}\".format(out, err, os.WEXITSTATUS(rc)))\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef state_engine_replay_block(existing_state_engine, new_state_engine, block_height, expected_snapshots={}):\n    \n    assert new_state_engine.lastblock + 1 == block_height, 'Block height mismatch: {} + 1 != {}'.format(new_state_engine.lastblock, block_height)\n\n    db_con = StateEngine.db_open(existing_state_engine.impl, existing_state_engine.working_dir)\n    chainstate_block = existing_state_engine.db_chainstate_get_block(db_con, block_height)\n    db_con.close()\n\n    log.debug(\"{} transactions accepted at block {} in chainstate {}; replaying in {}\".format(len(chainstate_block), block_height, existing_state_engine.working_dir, new_state_engine.working_dir))\n\n    parsed_txs = dict([(txdata['txid'], transactions.tx_parse(txdata['tx_hex'], blockchain=existing_state_engine.impl.get_blockchain())) for txdata in chainstate_block])\n    txs = [\n        {\n            'txid': txdata['txid'],\n            'txindex': txdata['txindex'],\n            'nulldata': '{}{}{}'.format(existing_state_engine.impl.get_magic_bytes().encode('hex'), txdata['opcode'].encode('hex'), txdata['data_hex']),\n            'ins': parsed_txs[txdata['txid']]['ins'],\n            'outs': parsed_txs[txdata['txid']]['outs'],\n            'senders': txdata['senders'],\n            'fee': txdata['fee'],\n            'hex': txdata['tx_hex'],\n            'tx_merkle_path': txdata['tx_merkle_path'],\n        }\n        for txdata in chainstate_block]\n\n    new_state_engine.db_set_indexing(True, new_state_engine.impl, new_state_engine.working_dir)\n\n    ops = new_state_engine.parse_block(block_height, txs)\n    consensus_hash = new_state_engine.process_block(block_height, ops, expected_snapshots=expected_snapshots)\n\n    new_state_engine.db_set_indexing(False, new_state_engine.impl, new_state_engine.working_dir)\n\n    return consensus_hash", "response": "Replay the existing chain state at a particular block height."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef state_engine_replay(consensus_impl, existing_working_dir, new_state_engine, target_block_height, start_block=None, initial_snapshots={}, expected_snapshots={}):\n    \n    assert hasattr(consensus_impl, 'get_opcodes')\n    assert hasattr(consensus_impl, 'get_magic_bytes')\n    assert hasattr(consensus_impl, 'get_opfields')\n    assert hasattr(consensus_impl, 'get_first_block_id')\n\n    consensus_opcodes = consensus_impl.get_opcodes()\n    consensus_magic_bytes = consensus_impl.get_magic_bytes()\n    consensus_opfields = consensus_impl.get_opfields()\n\n    existing_state_engine = StateEngine(consensus_impl, existing_working_dir)\n    \n    # set up existing state engine \n    rc = existing_state_engine.db_setup()\n    if not rc:\n        # do not touch the existing db\n        raise Exception(\"Existing state in {} is unusable or corrupt\".format(os.path.dirname(existing_working_dir)))\n\n    if start_block is None:\n        # maybe we're resuming?\n        start_block = new_state_engine.get_lastblock(new_state_engine.impl, new_state_engine.working_dir)\n        if start_block is None:\n            # starting from scratch\n            start_block = consensus_impl.get_first_block_id()\n\n    log.debug(\"Rebuilding database from {} to {}\".format(start_block, target_block_height))\n\n    consensus_hashes = {}\n    for block_height in range(start_block, target_block_height+1):\n        \n        # recover virtualchain transactions from the existing db and feed them into the new db\n        consensus_hash = state_engine_replay_block(existing_state_engine, new_state_engine, block_height, expected_snapshots=expected_snapshots)\n\n        log.debug(\"VERIFY CONSENSUS({}): {}\".format(block_height, consensus_hash))\n        consensus_hashes[block_height] = consensus_hash\n\n        if block_height in expected_snapshots:\n            if expected_snapshots[block_height] != consensus_hash:\n                log.error(\"DATABASE IS NOT CONSISTENT AT {}: {} != {}\".format(block_height, expected_snapshots[block_height], consensus_hash))\n                return None\n\n    # final consensus hash\n    return consensus_hashes[target_block_height]", "response": "Rebuilds the virtual transactions of the given consensus rules into a given directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies that a database is consistent with the given consensus hash.", "response": "def state_engine_verify(trusted_consensus_hash, consensus_block_height, consensus_impl, untrusted_working_dir, new_state_engine, start_block=None, expected_snapshots={}):\n    \"\"\"\n    Verify that a database is consistent with a\n    known-good consensus hash.\n\n    This algorithm works by creating a new database,\n    parsing the untrusted database, and feeding the untrusted\n    operations into the new database block-by-block.  If we\n    derive the same consensus hash, then we can trust the\n    database.\n\n    Return True if consistent with the given consensus hash at the given consensus block height\n    Return False if not\n    \"\"\"\n    \n    assert hasattr(consensus_impl, 'get_initial_snapshots')\n\n    final_consensus_hash = state_engine_replay(consensus_impl, untrusted_working_dir, new_state_engine, consensus_block_height, \\\n                                               start_block=start_block, initial_snapshots=consensus_impl.get_initial_snapshots(), expected_snapshots=expected_snapshots)\n\n    # did we reach the consensus hash we expected?\n    if final_consensus_hash is not None and final_consensus_hash == trusted_consensus_hash:\n        return True\n\n    else:\n        log.error(\"Unverifiable database state stored in '{}': {} != {}\".format(untrusted_working_dir, final_consensus_hash, trusted_consensus_hash))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef db_setup(self):\n        if self.db_exists(impl=self.impl, working_dir=self.working_dir):\n            # resuming from previous indexing\n            # read/write and unclean shutdown?\n            if not self.read_only and self.db_is_indexing(self.impl, self.working_dir):\n                log.error(\"Unclean shutdown detected on read/write open\")\n                return False\n        \n        else:\n            # setting up for the first time\n            assert not self.read_only, 'Cannot instantiate database if read_only is True'\n            db_con = self.db_create(self.impl, self.working_dir)\n            initial_snapshots = self.impl.get_initial_snapshots()\n            for block_id in sorted(initial_snapshots.keys()):\n                self.db_snapshot_append(db_con, int(block_id), str(initial_snapshots[block_id]), None, int(time.time()))\n\n        self.chainstate_path = config.get_snapshots_filename(self.impl, self.working_dir)\n        self.lastblock = self.get_lastblock(self.impl, self.working_dir)\n        self.setup = True\n        return True", "response": "Setup the state engine database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef db_restore(self, block_number=None):\n        restored = False\n        if block_number is not None:\n            # restore a specific backup\n            try:\n                self.backup_restore(block_number, self.impl, self.working_dir)\n                restored = True\n            except AssertionError:\n                log.error(\"Failed to restore state from {}\".format(block_number))\n                return False\n\n        else:\n            # find the latest block\n            backup_blocks = self.get_backup_blocks(self.impl, self.working_dir)\n            for block_number in reversed(sorted(backup_blocks)):\n                try:\n                    self.backup_restore(block_number, self.impl, self.working_dir)\n                    restored = True\n                    log.debug(\"Restored state from {}\".format(block_number))\n                    break\n                except AssertionError:\n                    log.debug(\"Failed to restore state from {}\".format(block_number))\n                    continue\n\n            if not restored:\n                # failed to restore\n                log.error(\"Failed to restore state from {}\".format(','.join(backup_blocks)))\n                return False\n\n        # woo!\n        self.db_set_indexing(False, self.impl, self.working_dir)\n        return self.db_setup()", "response": "Restore the database and clear the indexing lockfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef db_exists(cls, impl, working_dir):\n        path = config.get_snapshots_filename(impl, working_dir)\n        return os.path.exists(path)", "response": "Returns True if the chainstate db exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a sqlite3 db at the given path.", "response": "def db_create(cls, impl, working_dir):\n        \"\"\"\n        Create a sqlite3 db at the given path.\n        Create all the tables and indexes we need.\n        Returns a db connection on success\n        Raises an exception on error\n        \"\"\"\n\n        global VIRTUALCHAIN_DB_SCRIPT\n       \n        log.debug(\"Setup chain state in {}\".format(working_dir))\n\n        path = config.get_snapshots_filename(impl, working_dir)\n        if os.path.exists( path ):\n            raise Exception(\"Database {} already exists\")\n\n        lines = [l + \";\" for l in VIRTUALCHAIN_DB_SCRIPT.split(\";\")]\n        con = sqlite3.connect(path, isolation_level=None, timeout=2**30)\n\n        for line in lines:\n            con.execute(line)\n\n        con.row_factory = StateEngine.db_row_factory\n        return con"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef db_connect(cls, path):\n        con = sqlite3.connect(path, isolation_level=None, timeout=2**30)\n        con.row_factory = StateEngine.db_row_factory\n        return con", "response": "Connect to our chainstate db"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef db_open(cls, impl, working_dir):\n        path = config.get_snapshots_filename(impl, working_dir)\n        return cls.db_connect(path)", "response": "Open a connection to our chainstate db"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting a query and return the number of rows affected.", "response": "def db_query_execute(cls, cur, query, values, verbose=True):\n        \"\"\"\n        Execute a query.\n        Handle db timeouts.\n        Abort on failure.\n        \"\"\"\n        timeout = 1.0\n\n        if verbose:\n            log.debug(cls.db_format_query(query, values))\n\n        while True:\n            try:\n                ret = cur.execute(query, values)\n                return ret\n            except sqlite3.OperationalError as oe:\n                if oe.message == \"database is locked\":\n                    timeout = timeout * 2 + timeout * random.random()\n                    log.error(\"Query timed out due to lock; retrying in %s: %s\" % (timeout, cls.db_format_query( query, values )))\n                    time.sleep(timeout)\n                \n                else:\n                    log.exception(oe)\n                    log.error(\"FATAL: failed to execute query (%s, %s)\" % (query, values))\n                    log.error(\"\\n\".join(traceback.format_stack()))\n                    os.abort()\n\n            except Exception, e:\n                log.exception(e)\n                log.error(\"FATAL: failed to execute query (%s, %s)\" % (query, values))\n                log.error(\"\\n\".join(traceback.format_stack()))\n                os.abort()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends a new entry into the chain state table.", "response": "def db_chainstate_append(cls, cur, **fields):\n        \"\"\"\n        Insert a row into the chain state.\n        Meant to be executed as part of a transaction.\n\n        Return True on success\n        Raise an exception if the fields are invalid\n        Abort on db error.\n        \"\"\"\n        missing = []\n        extra = []\n        for reqfield in CHAINSTATE_FIELDS:\n            if reqfield not in fields:\n                missing.append(reqfield)\n\n        for fieldname in fields:\n            if fieldname not in CHAINSTATE_FIELDS:\n                extra.append(fieldname)\n\n        if len(missing) > 0 or len(extra) > 0:\n            raise ValueError(\"Invalid fields: missing: {}, extra: {}\".format(','.join(missing), ','.join(extra)))\n\n        query = 'INSERT INTO chainstate ({}) VALUES ({});'.format(\n                ','.join( CHAINSTATE_FIELDS ),\n                ','.join( ['?'] * len(CHAINSTATE_FIELDS)))\n\n        args = tuple([fields[fieldname] for fieldname in CHAINSTATE_FIELDS])\n        cls.db_query_execute(cur, query, args)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend hash info for the last processed block and the time at which it was processed.", "response": "def db_snapshot_append(cls, cur, block_id, consensus_hash, ops_hash, timestamp):\n        \"\"\"\n        Append hash info for the last block processed, and the time at which it was done.\n        Meant to be executed as part of a transaction.\n\n        Return True on success\n        Raise an exception on invalid block number\n        Abort on db error\n        \"\"\"\n        \n        query = 'INSERT INTO snapshots (block_id,consensus_hash,ops_hash,timestamp) VALUES (?,?,?,?);'\n        args = (block_id,consensus_hash,ops_hash,timestamp)\n        \n        cls.db_query_execute(cur, query, args)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the list of virtualchain transactions accepted at a given block.", "response": "def db_chainstate_get_block(cls, cur, block_height):\n        \"\"\"\n        Get the list of virtualchain transactions accepted at a given block.\n        Returns the list of rows, where each row is a dict.\n        \"\"\"\n        query = 'SELECT * FROM chainstate WHERE block_id = ? ORDER BY vtxindex;'\n        args = (block_height,)\n\n        rows = cls.db_query_execute(cur, query, args, verbose=False)\n        ret = []\n\n        for r in rows:\n            rowdata = {\n                'txid': str(r['txid']),\n                'block_id': r['block_id'],\n                'txindex': r['txindex'],\n                'vtxindex': r['vtxindex'],\n                'opcode': str(r['opcode']),\n                'data_hex': str(r['data_hex']),\n                'senders': simplejson.loads(r['senders']),\n                'tx_hex': str(r['tx_hex']),\n                'tx_merkle_path': str(r['tx_merkle_path']),\n                'fee': r['fee']\n            }\n\n            ret.append(rowdata)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef db_set_indexing(cls, is_indexing, impl, working_dir):\n        indexing_lockfile_path = config.get_lockfile_filename(impl, working_dir)\n        if is_indexing:\n            # make sure this exists\n            with open(indexing_lockfile_path, 'w') as f:\n                pass\n\n        else:\n            # make sure it does not exist \n            try:\n                os.unlink(indexing_lockfile_path)\n            except:\n                pass", "response": "Set the lockfile path as to whether or not the system is indexing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef db_is_indexing(cls, impl, working_dir):\n        indexing_lockfile_path = config.get_lockfile_filename(impl, working_dir)\n        return os.path.exists(indexing_lockfile_path)", "response": "Return True if the system indexing?"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_lastblock(cls, impl, working_dir):\n        if not cls.db_exists(impl, working_dir):\n            return None\n\n        con = cls.db_open(impl, working_dir)\n        query = 'SELECT MAX(block_id) FROM snapshots;'\n        \n        rows = cls.db_query_execute(con, query, (), verbose=False)\n        ret = None\n        for r in rows:\n            ret = r['MAX(block_id)']\n\n        con.close()\n        return ret", "response": "Get the last block number of a user - specified entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_consensus_hashes(cls, impl, working_dir, start_block_height=None, end_block_height=None, db_con=None, completeness_check=True):\n        if (start_block_height is None and end_block_height is not None) or (start_block_height is not None and end_block_height is None):\n            raise ValueError(\"Need either both or neither start/end block height\")\n\n        we_opened = False\n        if db_con is not None:\n            con = db_con\n\n        else:\n            assert impl and working_dir, 'Need impl and working_dir if db_con is not given'\n            con = cls.db_open(impl, working_dir)\n            we_opened = True\n\n        range_query = ''\n        range_args = ()\n\n        if start_block_height and end_block_height:\n            range_query += ' WHERE block_id >= ? AND block_id < ?'\n            range_args += (start_block_height,end_block_height)\n\n        query = 'SELECT block_id,consensus_hash FROM snapshots' + range_query + ';'\n        args = range_args\n\n        rows = cls.db_query_execute(con, query, range_args, verbose=False)\n        ret = {}\n        block_min = None\n        block_max = None\n\n        for r in rows:\n            block_id = int(r['block_id'])\n            ret[block_id] = r['consensus_hash']\n\n            if block_min is None or block_min > block_id:\n                block_min = block_id\n\n            if block_max is None or block_max < block_id:\n                block_max = block_id\n\n        if we_opened:\n            con.close()\n       \n        if completeness_check:\n            # sanity check\n            for i in range(block_min,block_max+1):\n                ch = ret.get(i, None)\n\n                assert ch is not None, 'Missing consensus hash for {}'.format(i)\n                assert isinstance(ret[i], (str,unicode)), 'consensus hash for {} is type {}'.format(i, type(ret[i]))\n\n        return ret", "response": "Get all consensus hashes for a given block height."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ops_hashes(cls, impl, working_dir, start_block_height=None, end_block_height=None):\n        if (start_block_height is None and end_block_height is not None) or (start_block_height is not None and end_block_height is None):\n            raise ValueError(\"Need either both or neither start/end block height\")\n\n        con = cls.db_open(impl, working_dir)\n        range_query = ' WHERE ops_hash IS NOT NULL'\n        range_args = ()\n\n        if start_block_height and end_block_height:\n            range_query += ' AND block_id >= ? AND block_id < ?'\n            range_args += (start_block_height,end_block_height)\n\n        query = 'SELECT block_id,ops_hash FROM snapshots' + range_query + ';'\n        args = range_args\n\n        rows = cls.db_query_execute(con, query, range_args, verbose=False)\n        ret = {}\n        block_min = None\n        block_max = None\n\n        for r in rows:\n            ret[r['block_id']] = r['ops_hash']\n\n            if block_min is None or block_min > r['block_id']:\n                block_min = r['block_id']\n\n            if block_max is None or block_max < r['block_id']:\n                block_max = r['block_id']\n\n        con.close()\n        \n        # sanity check\n        if block_min is not None and block_max is not None:\n            for i in range(block_min,block_max+1):\n                oh = ret.get(i, None)\n\n                assert oh is not None, 'Missing ops hash for {}'.format(i)\n                assert isinstance(ret[i], (str,unicode)), 'ops hash for {} is type {}'.format(i, type(ret[i]))\n\n        return ret", "response": "Get all consensus hashes for the specified block height."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the set of state paths that point to the current chain and state info.", "response": "def get_state_paths(cls, impl, working_dir):\n        \"\"\"\n        Get the set of state paths that point to the current chain and state info.\n        Returns a list of paths.\n        \"\"\"\n        return [config.get_db_filename(impl, working_dir), config.get_snapshots_filename(impl, working_dir)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_backup_blocks(cls, impl, working_dir):\n        ret = []\n        backup_dir = config.get_backups_directory(impl, working_dir)\n        if not os.path.exists(backup_dir):\n            return []\n\n        for name in os.listdir( backup_dir ):\n            if \".bak.\" not in name:\n                continue \n\n            suffix = name.split(\".bak.\")[-1]\n            try:\n                block_id = int(suffix)\n            except:\n                continue \n\n            # must exist...\n            backup_paths = cls.get_backup_paths(block_id, impl, working_dir)\n            for p in backup_paths:\n                if not os.path.exists(p):\n                    # doesn't exist\n                    block_id = None\n                    continue\n\n            if block_id is not None:\n                # have backup at this block \n                ret.append(block_id)\n\n        return ret", "response": "Get the set of block IDs that were backed up by this module"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the set of backup paths given the virtualchain implementation module and block number", "response": "def get_backup_paths(cls, block_id, impl, working_dir):\n        \"\"\"\n        Get the set of backup paths, given the virtualchain implementation module and block number\n        \"\"\"\n        backup_dir = config.get_backups_directory(impl, working_dir)\n        backup_paths = []\n        for p in cls.get_state_paths(impl, working_dir):\n            pbase = os.path.basename(p)\n            backup_path = os.path.join( backup_dir, pbase + (\".bak.%s\" % block_id))\n            backup_paths.append( backup_path )\n\n        return backup_paths"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backup_restore(cls, block_id, impl, working_dir):\n        backup_dir = config.get_backups_directory(impl, working_dir)\n        backup_paths = cls.get_backup_paths(block_id, impl, working_dir)\n        for p in backup_paths:\n            assert os.path.exists(p), \"No such backup file: {}\".format(p)\n\n        for p in cls.get_state_paths(impl, working_dir):\n            pbase = os.path.basename(p)\n            backup_path = os.path.join(backup_dir, pbase + (\".bak.{}\".format(block_id)))\n            log.debug(\"Restoring '{}' to '{}'\".format(backup_path, p))\n            shutil.copy(backup_path, p)\n    \n        return True", "response": "Restore from a backup file given the virutalchain implementation module and block number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_backups(self, block_id):\n        assert self.setup, \"Not set up yet.  Call .db_setup() first!\"\n\n        # make a backup?\n        if self.backup_frequency is not None:\n            if (block_id % self.backup_frequency) == 0:\n\n                backup_dir = config.get_backups_directory(self.impl, self.working_dir)\n                if not os.path.exists(backup_dir):\n                    try:\n                        os.makedirs(backup_dir)\n                    except Exception, e:\n                        log.exception(e)\n                        log.error(\"FATAL: failed to make backup directory '%s'\" % backup_dir)\n                        traceback.print_stack()\n                        os.abort()\n\n                for p in self.get_state_paths(self.impl, self.working_dir):\n                    if os.path.exists(p):\n                        try:\n                            pbase = os.path.basename(p)\n                            backup_path = os.path.join(backup_dir, pbase + (\".bak.{}\".format(block_id - 1)))\n\n                            if not os.path.exists(backup_path):\n                                rc = sqlite3_backup(p, backup_path)\n                                if not rc:\n                                    log.warning(\"Failed to back up as an SQLite db.  Falling back to /bin/cp\")\n                                    shutil.copy(p, backup_path)\n                            else:\n                                log.error(\"Will not overwrite '%s'\" % backup_path)\n\n                        except Exception, e:\n                            log.exception(e)\n                            log.error(\"FATAL: failed to back up '%s'\" % p)\n                            traceback.print_stack()\n                            os.abort()\n\n        return", "response": "This method creates a backup of the state files for the given block_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear_old_backups(self, block_id):\n        assert self.setup, \"Not set up yet.  Call .db_setup() first!\"\n\n        if self.backup_max_age is None:\n            # never delete backups\n            return \n\n        # find old backups \n        backup_dir = config.get_backups_directory(self.impl, self.working_dir)\n        if not os.path.exists(backup_dir):\n            return \n\n        backups = os.listdir( backup_dir )\n        for backup_name in backups:\n            if backup_name in [\".\", \"..\"]:\n                continue \n\n            backup_path = os.path.join(backup_dir, backup_name)\n            backup_block = None \n\n            try:\n                backup_block = int(backup_path.split(\".\")[-1])\n            except:\n                # not a backup file\n                log.info(\"Skipping non-backup '%s'\" % backup_path)\n\n            if not backup_path.endswith( \".bak.%s\" % backup_block ):\n                # not a backup file \n                log.info(\"Skipping non-backup '%s'\" % backup_path)\n                continue\n        \n            if backup_block + self.backup_max_age < block_id:\n                # dead \n                log.info(\"Removing old backup '%s'\" % backup_path)\n                try:\n                    os.unlink(backup_path)\n                except:\n                    pass", "response": "Delete old backups that are older than the given block_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the state of a virtualchain state at a specific block.", "response": "def save(self, block_id, consensus_hash, ops_hash, accepted_ops, virtualchain_ops_hints, backup=False):\n        \"\"\"\n        Write out all state to the working directory.\n        Calls the implementation's 'db_save' method to store any state for this block.\n        Calls the implementation's 'db_continue' method at the very end, to signal\n        to the implementation that all virtualchain state has been saved.  This method\n        can return False, in which case, indexing stops\n        \n        Return True on success \n        Return False if the implementation wants to exit.\n        Aborts on fatal error\n        \"\"\"\n        assert self.setup, \"Not set up yet.  Call .db_setup() first!\"\n        assert len(accepted_ops) == len(virtualchain_ops_hints)\n\n        if self.read_only:\n            log.error(\"FATAL: StateEngine is read only\")\n            traceback.print_stack()\n            os.abort()\n\n        if block_id < self.lastblock:\n            log.error(\"FATAL: Already processed up to block {} (got {})\".format(self.lastblock, block_id))\n            traceback.print_stack()\n            os.abort()\n        \n        # ask the implementation to save \n        if hasattr(self.impl, 'db_save'):\n            rc = False\n            try:\n                rc = self.impl.db_save(block_id, consensus_hash, ops_hash, accepted_ops, virtualchain_ops_hints, db_state=self.state)\n            except Exception as e:\n                log.exception(e)\n                rc = False\n\n            if not rc:\n                log.error(\"FATAL: Implementation failed to save state at block {}\".format(block_id))\n                traceback.print_stack()\n                os.abort()\n\n        # save new chainstate\n        self.lastblock = block_id\n\n        # start a transaction to store the new data\n        db_con = self.db_open(self.impl, self.working_dir)\n        cur = db_con.cursor()\n\n        self.db_query_execute(cur, \"BEGIN\", (), verbose=False)\n        \n        # add chainstate\n        for i, (accepted_op, virtualchain_op_hints) in enumerate(zip(accepted_ops, virtualchain_ops_hints)):\n\n            # unpack virtualchain hints\n            senders = virtualchain_op_hints['virtualchain_senders']\n            data_hex = virtualchain_op_hints['virtualchain_data_hex']\n            tx_hex = virtualchain_op_hints['virtualchain_txhex']\n            txid = virtualchain_op_hints['virtualchain_txid']\n            fee = virtualchain_op_hints['virtualchain_fee']\n            opcode = virtualchain_op_hints['virtualchain_opcode']\n            txindex = virtualchain_op_hints['virtualchain_txindex']\n            vtxindex = i\n            merkle_path = virtualchain_op_hints['virtualchain_tx_merkle_path']\n\n            vtx_data = {\n                'txid': txid,\n                'senders': simplejson.dumps(senders),\n                'data_hex': data_hex,\n                'tx_hex': tx_hex,\n                'tx_merkle_path': merkle_path,\n                'fee': fee,\n                'opcode': opcode,\n                'txindex': txindex,\n                'vtxindex': vtxindex,\n                'block_id': block_id\n            }\n            \n            self.db_chainstate_append(cur, **vtx_data)\n            \n        # update snapshot info\n        self.db_snapshot_append(cur, block_id, consensus_hash, ops_hash, int(time.time()))\n        self.db_query_execute(cur, \"END\", (), verbose=False)\n        db_con.close()\n\n        # make new backups and clear old ones\n        self.make_backups(block_id)\n        self.clear_old_backups(block_id)\n        \n        # ask the implementation if we should continue\n        continue_indexing = True\n        if hasattr(self.impl, \"db_continue\"):\n            try:\n                continue_indexing = self.impl.db_continue( block_id, consensus_hash )\n            except Exception, e:\n                log.exception(e)\n                traceback.print_stack()\n                log.error(\"FATAL: implementation failed db_continue\")\n                os.abort()\n\n        return continue_indexing"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_ops_snapshot( cls, serialized_ops ):\n        record_hashes = []\n        for serialized_op in serialized_ops:\n            record_hash = bin_double_sha256( serialized_op ).encode('hex')\n            record_hashes.append(record_hash)\n\n        if len(record_hashes) == 0:\n            record_hashes.append(bin_double_sha256(\"\").encode('hex'))\n\n        # put records into their own Merkle tree, and mix the root with the consensus hashes.\n        record_hashes.sort()\n        record_merkle_tree = MerkleTree( record_hashes )\n        record_root_hash = record_merkle_tree.root()\n\n        return record_root_hash", "response": "Generate a deterministic hash over the sequence of serialized operations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the consensus hash from the hash over the current ops and all previous required consensus hashes.", "response": "def make_snapshot_from_ops_hash( cls, record_root_hash, prev_consensus_hashes ):\n        \"\"\"\n        Generate the consensus hash from the hash over the current ops, and \n        all previous required consensus hashes.\n        \"\"\"\n        # mix into previous consensus hashes...\n        all_hashes = prev_consensus_hashes[:] + [record_root_hash]\n        all_hashes.sort()\n        all_hashes_merkle_tree = MerkleTree( all_hashes )\n        root_hash = all_hashes_merkle_tree.root()\n\n        consensus_hash = StateEngine.calculate_consensus_hash( root_hash )\n        return consensus_hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a snapshot of a set of serialized name operations and a list of previous consensus hashes.", "response": "def make_snapshot( cls, serialized_ops, prev_consensus_hashes ):\n        \"\"\"\n        Generate a consensus hash, using the tx-ordered list of serialized name \n        operations, and a list of previous consensus hashes that contains\n        the (k-1)th, (k-2)th; (k-3)th; ...; (k - (2**i - 1))th consensus hashes, \n        all the way back to the beginning of time (prev_consensus_hashes[i] is the \n        (k - (2**(i+1) - 1))th consensus hash)\n\n        Returns (consensus_hash, ops_hash)\n        \"\"\"\n        record_root_hash = StateEngine.make_ops_snapshot( serialized_ops )\n        log.debug(\"Snapshot('{}', {})\".format(record_root_hash, prev_consensus_hashes))\n        return (cls.make_snapshot_from_ops_hash( record_root_hash, prev_consensus_hashes ), record_root_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef serialize_op( cls, opcode, opdata, opfields, verbose=True ):\n        fields = opfields.get( opcode, None )\n        if fields is None:\n            log.error(\"BUG: unrecongnized opcode '%s'\" % opcode )\n            return None \n\n        all_values = []\n        debug_all_values = []\n        missing = []\n        for field in fields:\n           if not opdata.has_key(field):\n              missing.append( field )\n\n           field_value = opdata.get(field, None)\n           if field_value is None:\n              field_value = \"\"\n          \n           # netstring format\n           debug_all_values.append( str(field) + \"=\" + str(len(str(field_value))) + \":\" + str(field_value) )\n           all_values.append( str(len(str(field_value))) + \":\" + str(field_value) )\n\n        if len(missing) > 0:\n           log.error(\"Missing fields; dump follows:\\n{}\".format(simplejson.dumps( opdata, indent=4, sort_keys=True )))\n           raise Exception(\"BUG: missing fields '{}'\".format(\",\".join(missing)))\n\n        if verbose:\n            log.debug(\"SERIALIZE: {}:{}\".format(opcode, \",\".join(debug_all_values) ))\n\n        field_values = \",\".join( all_values )\n\n        return opcode + \":\" + field_values", "response": "Given an opcode associated data and opfields to serialize return a canonical serialized form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef snapshot(self, block_id, oplist):\n        \n        assert self.setup, \"Not set up yet.  Call .db_setup() first!\"\n        log.debug(\"Snapshotting block {}\".format(block_id))\n        \n        serialized_ops = []\n        for opdata in oplist:\n            serialized_record = StateEngine.serialize_op(opdata['virtualchain_opcode'], opdata, self.opfields)\n            serialized_ops.append( serialized_record )\n\n        previous_consensus_hashes = []\n        k = block_id\n        i = 1\n        while k - (2**i - 1) >= self.impl.get_first_block_id():\n            prev_block = k - (2**i - 1)\n            prev_ch = self.get_consensus_at(prev_block)\n            log.debug(\"Snapshotting block %s: consensus hash of %s is %s\" % (block_id, prev_block, prev_ch))\n\n            if prev_ch is None:\n                log.error(\"BUG: None consensus for %s\" % prev_block )\n                traceback.print_stack()\n                os.abort()\n\n            previous_consensus_hashes.append(prev_ch)\n            i += 1\n\n        consensus_hash, ops_hash = StateEngine.make_snapshot(serialized_ops, previous_consensus_hashes) \n        return consensus_hash, ops_hash", "response": "This function takes a block ID and a set of operations committed and returns the consensus hash that represents the state of the virtual chain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_virtualchain_field(cls, opdata, virtualchain_field, value):\n        assert virtualchain_field in RESERVED_KEYS, 'Invalid field name {} (choose from {})'.format(virtualchain_field, ','.join(RESERVED_KEYS))\n        opdata[virtualchain_field] = value", "response": "Set a virtualchain field value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a block ID and a data -bearing transaction try to parse it into a virtual chain operation.", "response": "def parse_transaction(self, block_id, tx):\n        \"\"\"\n        Given a block ID and an data-bearing transaction, \n        try to parse it into a virtual chain operation.\n        \n        Use the implementation's 'db_parse' method to do so.\n\n        Data transactions that do not have the magic bytes or a valid opcode\n        will be skipped automatically.  The db_parse method does not need\n        to know how to handle them.\n\n        @tx is a dict with\n        `txid`: the transaction ID\n        `txindex`: the offset in the block where this tx occurs\n        `nulldata`: the hex-encoded scratch data from the transaction\n        `ins`: the list of transaction inputs\n        `outs`: the list of transaction outputs\n        `senders`: the list of transaction senders\n        `fee`: the transaction fee\n        `txhex`: the hex-encoded raw transaction\n        \n        Return a dict representing the data on success.\n        Return None on error\n        \"\"\"\n        \n        data_hex = tx['nulldata']\n        inputs = tx['ins']\n        outputs = tx['outs']\n        senders = tx['senders']\n        fee = tx['fee']\n        txhex = tx['hex']\n        merkle_path = tx['tx_merkle_path']\n        \n        if not is_hex(data_hex):\n            # should always work; the tx downloader converts the binary string to hex\n            # not a valid hex string \n            raise ValueError(\"Invalid nulldata: not hex-encoded\")\n        \n        if len(data_hex) % 2 != 0:\n            # should always work; the tx downloader converts the binary string to hex\n            # not valid hex string \n            raise ValueError(\"Invalid nulldata: not hex-encoded\")\n        \n        data_bin = None\n        try:\n            # should always work; the tx downloader converts the binary string to hex\n            data_bin = data_hex.decode('hex')\n        except Exception, e:\n            log.error(\"Failed to parse transaction: %s (data_hex = %s)\" % (tx, data_hex))\n            raise ValueError(\"Invalid nulldata: not hex-encoded\")\n        \n        if not data_bin.startswith(self.magic_bytes):\n            # not for us\n            return None\n        \n        if len(data_bin) < len(self.magic_bytes) + 1:\n            # invalid operation--no opcode\n            return None\n\n        # 3rd byte is always the operation code\n        op_code = data_bin[len(self.magic_bytes)]\n        if op_code not in self.opcodes:\n            return None \n        \n        # looks like an op.  Try to parse it.\n        op_payload = data_bin[len(self.magic_bytes)+1:]\n        \n        op = self.impl.db_parse(block_id, tx['txid'], tx['txindex'], op_code, op_payload, senders, inputs, outputs, fee, db_state=self.state, raw_tx=txhex)\n        if op is None:\n            # not valid \n            return None \n        \n        # store it\n        op['virtualchain_opcode'] = op_code\n        op['virtualchain_txid'] = tx['txid']\n        op['virtualchain_txindex'] = tx['txindex']\n        op['virtualchain_txhex'] = txhex\n        op['virtualchain_tx_merkle_path'] = merkle_path\n        op['virtualchain_senders'] = senders\n        op['virtualchain_fee'] = fee\n        op['virtualchain_data_hex'] = op_payload.encode('hex')\n        \n        return op"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_block(self, block_id, txs):\n        ops = []\n        for i in range(0,len(txs)):\n            tx = txs[i]\n            op = self.parse_transaction(block_id, tx)\n            if op is not None:\n                ops.append( op )\n            \n        return ops", "response": "Given the sequence of transactions in a block turn them into a\n        sequence of virtual chain operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves reserved keywords from an op dict and returns a new op dict and the reserved fields", "response": "def remove_reserved_keys(self, op):\n        \"\"\"\n        Remove reserved keywords from an op dict,\n        which can then safely be passed into the db.\n        \n        Returns a new op dict, and the reserved fields\n        \"\"\"\n        sanitized = {}\n        reserved = {}\n        \n        for k in op.keys():\n            if str(k) not in RESERVED_KEYS:\n                sanitized[str(k)] = copy.deepcopy(op[k])\n            else:\n                reserved[str(k)] = copy.deepcopy(op[k])\n                \n        return sanitized, reserved"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog an accepted operation", "response": "def log_accept(self, block_id, vtxindex, opcode, op_data):\n        \"\"\"\n        Log an accepted operation\n        \"\"\"\n        log.debug(\"ACCEPT op {} at ({}, {}) ({})\".format(opcode, block_id, vtxindex, json.dumps(op_data, sort_keys=True)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_ops(self, block_id, ops):\n\n        new_ops = defaultdict(list)\n\n        for op in self.opcodes:\n            new_ops[op] = []\n\n        # transaction-ordered listing of accepted operations\n        new_ops['virtualchain_ordered'] = []\n        new_ops['virtualchain_all_ops'] = ops\n\n        to_commit_sanitized = []\n        to_commit_reserved = []\n\n        # let the implementation do an initial scan over the blocks\n        # NOTE: these will be different objects in memory from the objects passed into db_check\n        initial_scan = []\n        for i in xrange(0, len(ops)):\n\n            op_data = ops[i]\n            op_sanitized, _ = self.remove_reserved_keys( op_data )\n            initial_scan.append( copy.deepcopy( op_sanitized ) )\n\n        # allow the implementation to do a pre-scan of the set of ops \n        # (e.g. in Blockstack, this gets used to find name registration collisions)\n        if hasattr(self.impl, \"db_scan_block\"):\n            self.impl.db_scan_block( block_id, initial_scan, db_state=self.state )\n        else:\n            log.debug(\"Compat: no db_scan_block\")\n\n        # check each operation \n        for i in range(0, len(ops)):\n            op_data = ops[i]\n            op_sanitized, reserved = self.remove_reserved_keys( op_data )\n            opcode = reserved['virtualchain_opcode']\n\n            # check this op\n            rc = self.impl.db_check(block_id, new_ops, opcode, op_sanitized, reserved['virtualchain_txid'], reserved['virtualchain_txindex'], to_commit_sanitized, db_state=self.state)\n            if rc:\n\n                # commit this op\n                new_op_list = self.impl.db_commit(block_id, opcode, op_sanitized, reserved['virtualchain_txid'], reserved['virtualchain_txindex'], db_state=self.state)\n                if type(new_op_list) != list:\n                    new_op_list = [new_op_list]\n\n                for new_op in new_op_list:\n                    if new_op is not None:\n                        if type(new_op) == dict:\n\n                            # externally-visible state transition \n                            to_commit_sanitized_op = copy.deepcopy( new_op )\n                            to_commit_sanitized.append( to_commit_sanitized_op )\n\n                            new_op.update( reserved )\n                            new_ops[opcode].append( new_op )\n                            new_ops['virtualchain_ordered'].append( new_op )\n\n                        else:\n                            # internal state transition \n                            continue\n\n            else:\n                self.log_reject( block_id, reserved['virtualchain_txindex'], opcode, copy.deepcopy(op_sanitized))\n\n        \n        # final commit hint.\n        # the implementation has a chance here to feed any extra data into the consensus hash with this call\n        # (e.g. to affect internal state transitions that occur as seconary, holistic consequences to the sequence\n        # of prior operations for this block).\n        final_ops = self.impl.db_commit( block_id, 'virtualchain_final', {'virtualchain_ordered': new_ops['virtualchain_ordered']}, None, None, db_state=self.state )\n        if final_ops is not None:\n            # make sure each one has all the virtualchain reserved fields\n            for i in range(0, len(final_ops)):\n                for fieldname in RESERVED_FIELDS:\n                    assert fieldname in final_ops[i], 'Extra consensus operation at offset {} is missing {}'.format(i, fieldname)\n\n            new_ops['virtualchain_final'] = final_ops\n            new_ops['virtualchain_ordered'] += final_ops\n            new_ops['virtualchain_all_ops'] += final_ops\n\n        return new_ops", "response": "Given a transaction - ordered sequence of parsed operations and a block ID check their validity and give them to the state engine to \n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a block and return the corresponding virtualchain related attributes.", "response": "def process_block(self, block_id, ops, backup=False, expected_snapshots=None):\n        \"\"\"\n        Top-level block processing method.\n        Feed the block and its data transactions \n        through the implementation, to build up the \n        implementation's state.  Cache the \n        resulting data to disk.\n       \n        Return the (consensus hash, ops hash) for this block on success.\n        Exit on failure.\n        \"\"\"\n        \n        log.debug(\"Process block {} ({} virtual transactions)\".format(block_id, len(ops)))\n\n        if expected_snapshots is None:\n            expected_snapshots = self.expected_snapshots\n        if expected_snapshots is None:\n            expected_snapshots = {}\n        \n        new_ops = self.process_ops(block_id, ops)\n        consensus_hash, ops_hash = self.snapshot(block_id, new_ops['virtualchain_ordered'])\n\n        # sanity check against a known sequence of consensus hashes\n        if block_id in expected_snapshots:\n            log.debug(\"Expecting CONSENSUS({}) == {}\".format(block_id, expected_snapshots[block_id]))\n            if expected_snapshots[block_id] != consensus_hash:\n                log.error(\"FATAL: consensus hash mismatch at height {}: {} != {}\".format(block_id, expected_snapshots[block_id], consensus_hash))\n                traceback.print_stack()\n                os.abort()\n        \n        # remove virtualchain-reserved keys\n        sanitized_ops = []\n        virtualchain_ops_hints = []\n        for opdata in new_ops['virtualchain_ordered']:\n            op_sanitized, op_reserved = self.remove_reserved_keys(opdata)\n            sanitized_ops.append(op_sanitized)\n            virtualchain_ops_hints.append(op_reserved)\n\n        # save state for this block\n        rc = self.save(block_id, consensus_hash, ops_hash, sanitized_ops, virtualchain_ops_hints, backup=backup)\n        if not rc:\n            # implementation requests early termination \n            log.debug(\"Early indexing termination at {}\".format(block_id))\n            return None\n\n        # store statistics if we're in test mode \n        if os.environ.get(\"BLOCKSTACK_TEST\"):\n            global STATISTICS\n            STATISTICS[block_id] = {\n                'consensus_hash': consensus_hash,\n                'num_parsed_ops': len(ops),\n                'num_processed_ops': len(new_ops['virtualchain_ordered']),\n                'ops_hash': ops_hash,\n                'backup': backup,\n            }\n\n        return consensus_hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_block_statistics(cls, block_id):\n        if not os.environ.get(\"BLOCKSTACK_TEST\"):\n            raise Exception(\"This method is only available in the test framework\")\n\n        global STATISTICS\n        return STATISTICS.get(block_id)", "response": "Get the block statistics for a given block id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a virtualchain state from the given block ids and transactions.", "response": "def build( cls, blockchain_opts, end_block_id, state_engine, expected_snapshots={}, tx_filter=None ):\n        \"\"\"\n        Top-level call to process all blocks in the blockchain.\n        Goes and fetches all data-bearing transactions in order,\n        and feeds them into the state engine implementation.\n        \n        Note that this method can take some time (hours, days) to complete \n        when called from the first block.\n        \n        Return True on success \n        Return False on error\n        Raise an exception on recoverable error--the caller should simply try again.\n        Exit on irrecoverable error--do not try to make forward progress\n        \"\"\"\n       \n        first_block_id = state_engine.lastblock + 1\n        if first_block_id >= end_block_id:\n            # built \n            log.debug(\"Up-to-date ({} >= {})\".format(first_block_id, end_block_id))\n            return True \n\n        rc = True\n        batch_size = config.BLOCK_BATCH_SIZE\n        log.debug(\"Sync virtualchain state from {} to {}\".format(first_block_id, end_block_id))\n        \n        for block_id in range( first_block_id, end_block_id+1, batch_size ):\n            \n            if not rc:\n                break \n           \n            last_block_id = min(block_id + batch_size, end_block_id)\n\n            # get the blocks and transactions from the underlying blockchain\n            block_ids_and_txs = transactions.get_virtual_transactions(state_engine.impl.get_blockchain(), blockchain_opts, block_id, last_block_id, tx_filter=tx_filter, spv_last_block=end_block_id - 1)\n            if block_ids_and_txs is None:\n                raise Exception(\"Failed to get virtual transactions {} to {}\".format(block_id, last_block_id))\n\n            # process in order by block ID\n            block_ids_and_txs.sort()\n            for processed_block_id, txs in block_ids_and_txs:\n\n                if state_engine.get_consensus_at(processed_block_id) is not None:\n                    raise Exception(\"Already processed block %s (%s)\" % (processed_block_id, state_engine.get_consensus_at( processed_block_id )) )\n                \n                cls.db_set_indexing(True, state_engine.impl, state_engine.working_dir)\n\n                ops = state_engine.parse_block(processed_block_id, txs)\n                consensus_hash = state_engine.process_block(processed_block_id, ops, expected_snapshots=expected_snapshots)\n\n                cls.db_set_indexing(False, state_engine.impl, state_engine.working_dir)\n                \n                if consensus_hash is None:\n                    # request to stop\n                    rc = False\n                    log.debug(\"Stopped processing at block %s\" % processed_block_id)\n                    break\n\n                log.debug(\"CONSENSUS({}): {}\".format(processed_block_id, state_engine.get_consensus_at(processed_block_id)))\n\n                # sanity check, if given \n                expected_consensus_hash = state_engine.get_expected_consensus_at( processed_block_id )\n                if expected_consensus_hash is not None:\n                    if str(consensus_hash) != str(expected_consensus_hash):\n                        rc = False\n                        log.error(\"FATAL: DIVERGENCE DETECTED AT {}: {} != {}\".format(processed_block_id, consensus_hash, expected_consensus_hash))\n                        traceback.print_stack()\n                        os.abort()\n       \n            if not rc:\n                break\n        \n        log.debug(\"Last block is %s\" % state_engine.lastblock )\n        return rc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_consensus_at(self, block_id):\n        query = 'SELECT consensus_hash FROM snapshots WHERE block_id = ?;'\n        args = (block_id,)\n\n        con = self.db_open(self.impl, self.working_dir)\n        rows = self.db_query_execute(con, query, args, verbose=False)\n        res = None\n\n        for r in rows:\n            res = r['consensus_hash']\n\n        con.close()\n        return res", "response": "Get the consensus hash at a given block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the block number with the given consensus hash. Return None if there is no such block.", "response": "def get_block_from_consensus( self, consensus_hash ):\n        \"\"\"\n        Get the block number with the given consensus hash.\n        Return None if there is no such block.\n        \"\"\"\n        query = 'SELECT block_id FROM snapshots WHERE consensus_hash = ?;'\n        args = (consensus_hash,)\n\n        con = self.db_open(self.impl, self.working_dir)\n        rows = self.db_query_execute(con, query, args, verbose=False)\n        res = None\n\n        for r in rows:\n            res = r['block_id']\n\n        con.close()\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the list of valid consensus hashes for a given block.", "response": "def get_valid_consensus_hashes( self, block_id ):\n        \"\"\"\n        Get the list of valid consensus hashes for a given block.\n        \"\"\"\n        first_block_to_check = block_id - self.impl.get_valid_transaction_window()\n\n        query = 'SELECT consensus_hash FROM snapshots WHERE block_id >= ? AND block_id <= ?;'\n        args = (first_block_to_check,block_id)\n\n        valid_consensus_hashes = []\n        \n        con = self.db_open(self.impl, self.working_dir)\n        rows = self.db_query_execute(con, query, args, verbose=False)\n\n        for r in rows:\n            assert r['consensus_hash'] is not None\n            assert isinstance(r['consensus_hash'], (str,unicode))\n\n            valid_consensus_hashes.append(str(r['consensus_hash']))\n\n        con.close()\n        return valid_consensus_hashes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lb2pix(nside, l, b, nest=True):\n\n    theta = np.radians(90. - b)\n    phi = np.radians(l)\n\n    if not hasattr(l, '__len__'):\n        if (b < -90.) or (b > 90.):\n            return -1\n\n        pix_idx = hp.pixelfunc.ang2pix(nside, theta, phi, nest=nest)\n\n        return pix_idx\n\n    idx = (b >= -90.) & (b <= 90.)\n\n    pix_idx = np.empty(l.shape, dtype='i8')\n    pix_idx[idx] = hp.pixelfunc.ang2pix(nside, theta[idx], phi[idx], nest=nest)\n    pix_idx[~idx] = -1\n\n    return pix_idx", "response": "Converts Galactic longitude and latitude to HEALPix pixel index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch(version='bayestar2017'):\n\n    doi = {\n        'bayestar2015': '10.7910/DVN/40C44C',\n        'bayestar2017': '10.7910/DVN/LCYHJG'\n    }\n\n    # Raise an error if the specified version of the map does not exist\n    try:\n        doi = doi[version]\n    except KeyError as err:\n        raise ValueError('Version \"{}\" does not exist. Valid versions are: {}'.format(\n            version,\n            ', '.join(['\"{}\"'.format(k) for k in doi.keys()])\n        ))\n\n    requirements = {\n        'bayestar2015': {'contentType': 'application/x-hdf'},\n        'bayestar2017': {'filename': 'bayestar2017.h5'}\n    }[version]\n\n    local_fname = os.path.join(data_dir(), 'bayestar', '{}.h5'.format(version))\n\n    # Download the data\n    fetch_utils.dataverse_download_doi(\n        doi,\n        local_fname,\n        file_requirements=requirements)", "response": "Downloads the specified version of the Bayestar dust map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nraising a ValueError if the mode is not one of the accepted values.", "response": "def _raise_on_mode(self, mode):\n        \"\"\"\n        Checks that the provided query mode is one of the accepted values. If\n        not, raises a :obj:`ValueError`.\n        \"\"\"\n        valid_modes = [\n            'random_sample',\n            'random_sample_per_pix',\n            'samples',\n            'median',\n            'mean',\n            'best',\n            'percentile']\n\n        if mode not in valid_modes:\n            raise ValueError(\n                '\"{}\" is not a valid `mode`. Valid modes are:\\n'\n                '  {}'.format(mode, valid_modes)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the object containing the specified coordinates.", "response": "def query(self, coords, mode='random_sample', return_flags=False, pct=None):\n        \"\"\"\n        Returns reddening at the requested coordinates. There are several\n        different query modes, which handle the probabilistic nature of the map\n        differently.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): The coordinates to query.\n            mode (Optional[:obj:`str`]): Seven different query modes are available:\n                'random_sample', 'random_sample_per_pix' 'samples', 'median',\n                'mean', 'best' and 'percentile'. The :obj:`mode` determines how the\n                output will reflect the probabilistic nature of the Bayestar\n                dust maps.\n            return_flags (Optional[:obj:`bool`]): If :obj:`True`, then QA flags will be\n                returned in a second numpy structured array. That is, the query\n                will return :obj:`ret`, :obj:'flags`, where :obj:`ret` is the normal return\n                value, containing reddening. Defaults to :obj:`False`.\n            pct (Optional[:obj:`float` or list/array of :obj:`float`]): If the mode is\n                :obj:`percentile`, then :obj:`pct` specifies which percentile(s) is\n                (are) returned.\n\n        Returns:\n            Reddening at the specified coordinates, in magnitudes of reddening.\n\n            The conversion to E(B-V) (or other reddening units) depends on\n            whether :obj:`version='bayestar2017'` (the default) or\n            :obj:`'bayestar2015'` was selected when the :obj:`BayestarQuery` object\n            was created. To convert Bayestar2017 to Pan-STARRS 1 extinctions,\n            multiply by the coefficients given in Table 1 of Green et al.\n            (2018). Conversion to extinction in non-PS1 passbands depends on the\n            choice of extinction law. To convert Bayestar2015 to extinction in\n            various passbands, multiply by the coefficients in Table 6 of\n            Schlafly & Finkbeiner (2011). See Green et al. (2015, 2018) for more\n            detailed discussion of how to convert the Bayestar dust maps into\n            reddenings or extinctions in different passbands.\n\n            The shape of the output depends on the :obj:`mode`, and on whether\n            :obj:`coords` contains distances.\n\n            If :obj:`coords` does not specify distance(s), then the shape of the\n            output begins with :obj:`coords.shape`. If :obj:`coords` does specify\n            distance(s), then the shape of the output begins with\n            :obj:`coords.shape + ([number of distance bins],)`.\n\n            If :obj:`mode` is :obj:`'random_sample'`, then at each\n            coordinate/distance, a random sample of reddening is given.\n\n            If :obj:`mode` is :obj:`'random_sample_per_pix'`, then the sample chosen\n            for each angular pixel of the map will be consistent. For example,\n            if two query coordinates lie in the same map pixel, then the same\n            random sample will be chosen from the map for both query\n            coordinates.\n\n            If :obj:`mode` is :obj:`'median'`, then at each coordinate/distance, the\n            median reddening is returned.\n\n            If :obj:`mode` is :obj:`'mean'`, then at each coordinate/distance, the\n            mean reddening is returned.\n\n            If :obj:`mode` is :obj:`'best'`, then at each coordinate/distance, the\n            maximum posterior density reddening is returned (the \"best fit\").\n\n            If :obj:`mode` is :obj:`'percentile'`, then an additional keyword\n            argument, :obj:`pct`, must be specified. At each coordinate/distance,\n            the requested percentiles (in :obj:`pct`) will be returned. If :obj:`pct`\n            is a list/array, then the last axis of the output will correspond to\n            different percentiles.\n\n            Finally, if :obj:`mode` is :obj:`'samples'`, then at each\n            coordinate/distance, all samples are returned. The last axis of the\n            output will correspond to different samples.\n\n            If :obj:`return_flags` is :obj:`True`, then in addition to reddening, a\n            structured array containing QA flags will be returned. If the input\n            coordinates include distances, the QA flags will be :obj:`\"converged\"`\n            (whether or not the line-of-sight fit converged in a given pixel)\n            and :obj:`\"reliable_dist\"` (whether or not the requested distance is\n            within the range considered reliable, based on the inferred\n            stellar distances). If the input coordinates do not include\n            distances, then instead of :obj:`\"reliable_dist\"`, the flags will\n            include :obj:`\"min_reliable_distmod\"` and :obj:`\"max_reliable_distmod\"`,\n            the minimum and maximum reliable distance moduli in the given pixel.\n        \"\"\"\n\n        # Check that the query mode is supported\n        self._raise_on_mode(mode)\n\n        # Validate percentile specification\n        pct, scalar_pct = self._interpret_percentile(mode, pct)\n\n        # Get number of coordinates requested\n        n_coords_ret = coords.shape[0]\n\n        # Determine if distance has been requested\n        has_dist = hasattr(coords.distance, 'kpc')\n        d = coords.distance.kpc if has_dist else None\n\n        # Extract the correct angular pixel(s)\n        # t0 = time.time()\n        pix_idx = self._find_data_idx(coords.l.deg, coords.b.deg)\n        in_bounds_idx = (pix_idx != -1)\n\n        # t1 = time.time()\n\n        # Extract the correct samples\n        if mode == 'random_sample':\n            # A different sample in each queried coordinate\n            samp_idx = np.random.randint(0, self._n_samples, pix_idx.size)\n            n_samp_ret = 1\n        elif mode == 'random_sample_per_pix':\n            # Choose same sample in all coordinates that fall in same angular\n            # HEALPix pixel\n            samp_idx = np.random.randint(0, self._n_samples, self._n_pix)[pix_idx]\n            n_samp_ret = 1\n        elif mode == 'best':\n            samp_idx = slice(None)\n            n_samp_ret = 1\n        else:\n            # Return all samples in each queried coordinate\n            samp_idx = slice(None)\n            n_samp_ret = self._n_samples\n\n        # t2 = time.time()\n\n        if mode == 'best':\n            val = self._best_fit\n        else:\n            val = self._samples\n\n        # Create empty array to store flags\n        if return_flags:\n            if has_dist:\n                # If distances are provided in query, return only covergence and\n                # whether or not this distance is reliable\n                dtype = [('converged', 'bool'),\n                         ('reliable_dist', 'bool')]\n                # shape = (n_coords_ret)\n            else:\n                # Return convergence and reliable distance ranges\n                dtype = [('converged', 'bool'),\n                         ('min_reliable_distmod', 'f4'),\n                         ('max_reliable_distmod', 'f4')]\n            flags = np.empty(n_coords_ret, dtype=dtype)\n        # samples = self._samples[pix_idx, samp_idx]\n        # samples[pix_idx == -1] = np.nan\n\n        # t3 = time.time()\n\n        # Extract the correct distance bin (possibly using linear interpolation)\n        if has_dist: # Distance has been provided\n            # Determine ceiling bin index for each coordinate\n            dm = 5. * (np.log10(d) + 2.)\n            bin_idx_ceil = np.searchsorted(self._DM_bin_edges, dm)\n\n            # Create NaN-filled return arrays\n            if isinstance(samp_idx, slice):\n                ret = np.full((n_coords_ret, n_samp_ret), np.nan, dtype='f4')\n            else:\n                ret = np.full((n_coords_ret,), np.nan, dtype='f4')\n\n            # d < d(nearest distance slice)\n            idx_near = (bin_idx_ceil == 0) & in_bounds_idx\n            if np.any(idx_near):\n                a = 10.**(0.2 * (dm[idx_near] - self._DM_bin_edges[0]))\n                if isinstance(samp_idx, slice):\n                    ret[idx_near] = (\n                        a[:,None]\n                        * val[pix_idx[idx_near], samp_idx, 0])\n                else:\n                    # print('idx_near: {} true'.format(np.sum(idx_near)))\n                    # print('ret[idx_near].shape = {}'.format(ret[idx_near].shape))\n                    # print('val.shape = {}'.format(val.shape))\n                    # print('pix_idx[idx_near].shape = {}'.format(pix_idx[idx_near].shape))\n\n                    ret[idx_near] = (\n                        a * val[pix_idx[idx_near], samp_idx[idx_near], 0])\n\n            # d > d(farthest distance slice)\n            idx_far = (bin_idx_ceil == self._n_distances) & in_bounds_idx\n            if np.any(idx_far):\n                # print('idx_far: {} true'.format(np.sum(idx_far)))\n                # print('pix_idx[idx_far].shape = {}'.format(pix_idx[idx_far].shape))\n                # print('ret[idx_far].shape = {}'.format(ret[idx_far].shape))\n                # print('val.shape = {}'.format(val.shape))\n                if isinstance(samp_idx, slice):\n                    ret[idx_far] = val[pix_idx[idx_far], samp_idx, -1]\n                else:\n                    ret[idx_far] = val[pix_idx[idx_far], samp_idx[idx_far], -1]\n\n            # d(nearest distance slice) < d < d(farthest distance slice)\n            idx_btw = ~idx_near & ~idx_far & in_bounds_idx\n            if np.any(idx_btw):\n                DM_ceil = self._DM_bin_edges[bin_idx_ceil[idx_btw]]\n                DM_floor = self._DM_bin_edges[bin_idx_ceil[idx_btw]-1]\n                a = (DM_ceil - dm[idx_btw]) / (DM_ceil - DM_floor)\n                if isinstance(samp_idx, slice):\n                    ret[idx_btw] = (\n                        (1.-a[:,None])\n                        * val[pix_idx[idx_btw], samp_idx, bin_idx_ceil[idx_btw]]\n                        + a[:,None]\n                        * val[pix_idx[idx_btw], samp_idx, bin_idx_ceil[idx_btw]-1]\n                    )\n                else:\n                    ret[idx_btw] = (\n                        (1.-a) * val[pix_idx[idx_btw], samp_idx[idx_btw], bin_idx_ceil[idx_btw]]\n                        +    a * val[pix_idx[idx_btw], samp_idx[idx_btw], bin_idx_ceil[idx_btw]-1]\n                    )\n\n            # Flag: distance in reliable range?\n            if return_flags:\n                dm_min = self._pixel_info['DM_reliable_min'][pix_idx]\n                dm_max = self._pixel_info['DM_reliable_max'][pix_idx]\n                flags['reliable_dist'] = (\n                    (dm >= dm_min) &\n                    (dm <= dm_max) &\n                    np.isfinite(dm_min) &\n                    np.isfinite(dm_max))\n                flags['reliable_dist'][~in_bounds_idx] = False\n        else:   # No distances provided\n            ret = val[pix_idx, samp_idx, :]   # Return all distances\n            ret[~in_bounds_idx] = np.nan\n\n            # Flag: reliable distance bounds\n            if return_flags:\n                dm_min = self._pixel_info['DM_reliable_min'][pix_idx]\n                dm_max = self._pixel_info['DM_reliable_max'][pix_idx]\n\n                flags['min_reliable_distmod'] = dm_min\n                flags['max_reliable_distmod'] = dm_max\n                flags['min_reliable_distmod'][~in_bounds_idx] = np.nan\n                flags['max_reliable_distmod'][~in_bounds_idx] = np.nan\n\n        # t4 = time.time()\n\n        # Flag: convergence\n        if return_flags:\n            flags['converged'] = (\n                self._pixel_info['converged'][pix_idx].astype(np.bool))\n            flags['converged'][~in_bounds_idx] = False\n\n        # t5 = time.time()\n\n        # Reduce the samples in the requested manner\n        if mode == 'median':\n            ret = np.median(ret, axis=1)\n        elif mode == 'mean':\n            ret = np.mean(ret, axis=1)\n        elif mode == 'percentile':\n            ret = np.nanpercentile(ret, pct, axis=1)\n            if not scalar_pct:\n                # (percentile, pixel) -> (pixel, percentile)\n                # (pctile, pixel, distance) -> (pixel, distance, pctile)\n                ret = np.moveaxis(ret, 0, -1)\n        elif mode == 'best':\n            # Remove \"samples\" axis\n            s = ret.shape\n            ret.shape = s[:1] + s[2:]\n        elif mode == 'samples':\n            # Swap sample and distance axes to be consistent with other 3D dust\n            # maps. The output shape will be (pixel, distance, sample).\n            if not has_dist:\n                np.swapaxes(ret, 1, 2)\n\n        # t6 = time.time()\n        #\n        # print('')\n        # print('time inside bayestar.query: {:.4f} s'.format(t6-t0))\n        # print('{: >7.4f} s : {: >6.4f} s : _find_data_idx'.format(t1-t0, t1-t0))\n        # print('{: >7.4f} s : {: >6.4f} s : sample slice spec'.format(t2-t0, t2-t1))\n        # print('{: >7.4f} s : {: >6.4f} s : create empty return flag array'.format(t3-t0, t3-t2))\n        # print('{: >7.4f} s : {: >6.4f} s : extract results'.format(t4-t0, t4-t3))\n        # print('{: >7.4f} s : {: >6.4f} s : convergence flag'.format(t5-t0, t5-t4))\n        # print('{: >7.4f} s : {: >6.4f} s : reduce'.format(t6-t0, t6-t5))\n        # print('')\n\n        if return_flags:\n            return ret, flags\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the Jinja2 silent_none flag to True.", "response": "def set_jinja2_silent_none(config):  # pragma: no cover\n    \"\"\" if variable is None print '' instead of 'None'\n    \"\"\"\n    config.commit()\n    jinja2_env = config.get_jinja2_environment()\n    jinja2_env.finalize = _silent_none"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _quotient_exponent(x, y):\n    assert mpfr.mpfr_regular_p(x)\n    assert mpfr.mpfr_regular_p(y)\n\n    # Make copy of x with the exponent of y.\n    x2 = mpfr.Mpfr_t()\n    mpfr.mpfr_init2(x2, mpfr.mpfr_get_prec(x))\n    mpfr.mpfr_set(x2, x, mpfr.MPFR_RNDN)\n    mpfr.mpfr_set_exp(x2, mpfr.mpfr_get_exp(y))\n\n    # Compare x2 and y, disregarding the sign.\n    extra = mpfr.mpfr_cmpabs(x2, y) >= 0\n    return extra + mpfr.mpfr_get_exp(x) - mpfr.mpfr_get_exp(y)", "response": "Return the unique exponent of x and y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mpfr_floordiv(rop, x, y, rnd):\n    # Algorithm notes\n    # ---------------\n    # A simple and obvious approach is to compute floor(x / y) exactly, and\n    # then round to the nearest representable value using the given rounding\n    # mode.  This requires computing x / y to a precision sufficient to ensure\n    # that floor(x / y) is exactly representable.  If abs(x / y) < 2**r, then\n    # abs(floor(x / y)) <= 2**r, and so r bits of precision is enough.\n    # However, for large quotients this is impractical, and we need some other\n    # method.  For x / y sufficiently large, it's possible to show that x / y\n    # and floor(x / y) are indistinguishable, in the sense that both quantities\n    # round to the same value.  More precisely, we have the following theorem:\n    #\n    # Theorem.  Suppose that x and y are nonzero finite binary floats\n    # representable with p and q bits of precision, respectively.  Let R be any\n    # of the IEEE 754 standard rounding modes, and choose a target precision r.\n    # Write rnd for the rounding operation from Q to precision-r binary floats\n    # with rounding mode R.  Write bin(x) for the binade of a nonzero float x.\n    #\n    # If R is a round-to-nearest rounding mode, and either\n    #\n    # (1) p <= q + r and |x / y| >= 2^(q + r), or\n    # (2) p > q + r and bin(x) - bin(y) >= p\n    #\n    # then\n    #\n    #    rnd(floor(x / y)) == rnd(x / y)\n    #\n    # Conversely, if R is a directed rounding mode, and either\n    #\n    # (1) p < q + r and |x / y| >= 2^(q + r - 1), or\n    # (2) p >= q + r and bin(x) - bin(y) >= p\n    #\n    # then again\n    #\n    #    rnd(floor(x / y)) == rnd(x / y).\n    #\n    # Proof.  See separate notes and Coq proof in the float-proofs\n    # repository.\n    #\n    # Rather than distinguish between the various cases (R directed\n    # or not, p large versus p small) above, we use a weaker but\n    # simpler amalgamation of the above result:\n    #\n    # Corollary 1. With x, y, p, q, R, r and rnd as above, if\n    #\n    #     |x / y| >= 2^max(q + r, p)\n    #\n    # then\n    #\n    #     rnd(floor(x / y)) == rnd(x / y).\n    #\n    # Proof. Note that |x / y| >= 2^p implies bin(x) - bin(y) >= p,\n    # so it's enough that |x / y| >= 2^max(p, q + r) in the case of\n    # a round-to-nearest mode, and that |x / y| >= 2^max(p, q + r - 1)\n    # in the case of a directed rounding mode.\n\n    # In special cases, it's safe to defer to mpfr_div: the result in\n    # these cases is always 0, infinity, or nan.\n    if not mpfr.mpfr_regular_p(x) or not mpfr.mpfr_regular_p(y):\n        return mpfr.mpfr_div(rop, x, y, rnd)\n\n    e = _quotient_exponent(x, y)\n\n    p = mpfr.mpfr_get_prec(x)\n    q = mpfr.mpfr_get_prec(y)\n    r = mpfr.mpfr_get_prec(rop)\n\n    # If e - 1 >= max(p, q+r) then |x / y| >= 2^(e-1) >= 2^max(p, q+r),\n    # so by the above theorem, round(floordiv(x, y)) == round(div(x, y)).\n    if e - 1 >= max(p, q + r):\n        return mpfr.mpfr_div(rop, x, y, rnd)\n\n    # Slow version: compute to sufficient bits to get integer precision.  Given\n    # that 2**(e-1) <= x / y < 2**e, need >= e bits of precision.\n    z_prec = max(e, 2)\n    z = mpfr.Mpfr_t()\n    mpfr.mpfr_init2(z, z_prec)\n\n    # Compute the floor exactly. The division may set the\n    # inexact flag, so we save its state first.\n    old_inexact = mpfr.mpfr_inexflag_p()\n    mpfr.mpfr_div(z, x, y, mpfr.MPFR_RNDD)\n    if not old_inexact:\n        mpfr.mpfr_clear_inexflag()\n\n    # Floor result should be exactly representable, so any rounding mode will\n    # do.\n    ternary = mpfr.mpfr_rint_floor(z, z, rnd)\n    assert ternary == 0\n\n    # ... and round to the given rounding mode.\n    return mpfr.mpfr_set(rop, z, rnd)", "response": "Compute the MPFR floor and round to the nearest value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute two MPRF numbers x and y and return the result of fmod with appropriate precision.", "response": "def mpfr_mod(rop, x, y, rnd):\n    \"\"\"\n    Given two MPRF numbers x and y, compute\n    x - floor(x / y) * y, rounded if necessary using the given\n    rounding mode.  The result is placed in 'rop'.\n\n    This is the 'remainder' operation, with sign convention\n    compatible with Python's % operator (where x % y has\n    the same sign as y).\n    \"\"\"\n    # There are various cases:\n    #\n    # 0. If either argument is a NaN, the result is NaN.\n    #\n    # 1. If x is infinite or y is zero, the result is NaN.\n    #\n    # 2. If y is infinite, return 0 with the sign of y if x is zero, x if x has\n    #    the same sign as y, and infinity with the sign of y if it has the\n    #    opposite sign.\n    #\n    # 3. If none of the above cases apply then both x and y are finite,\n    #    and y is nonzero.  If x and y have the same sign, simply\n    #    return the result of fmod(x, y).\n    #\n    # 4. Now both x and y are finite, y is nonzero, and x and y have\n    #    differing signs.  Compute r = fmod(x, y) with sufficient precision\n    #    to get an exact result.  If r == 0, return 0 with the sign of y\n    #    (which will be the opposite of the sign of x).  If r != 0,\n    #    return r + y, rounded appropriately.\n\n    if not mpfr.mpfr_number_p(x) or mpfr.mpfr_nan_p(y) or mpfr.mpfr_zero_p(y):\n        return mpfr.mpfr_fmod(rop, x, y, rnd)\n    elif mpfr.mpfr_inf_p(y):\n        x_negative = mpfr.mpfr_signbit(x)\n        y_negative = mpfr.mpfr_signbit(y)\n        if mpfr.mpfr_zero_p(x):\n            mpfr.mpfr_set_zero(rop, -y_negative)\n            return 0\n        elif x_negative == y_negative:\n            return mpfr.mpfr_set(rop, x, rnd)\n        else:\n            mpfr.mpfr_set_inf(rop, -y_negative)\n            return 0\n\n    x_negative = mpfr.mpfr_signbit(x)\n    y_negative = mpfr.mpfr_signbit(y)\n    if x_negative == y_negative:\n        return mpfr.mpfr_fmod(rop, x, y, rnd)\n    else:\n        p = max(mpfr.mpfr_get_prec(x), mpfr.mpfr_get_prec(y))\n        z = mpfr.Mpfr_t()\n        mpfr.mpfr_init2(z, p)\n        # Doesn't matter what rounding mode we use here; the result\n        # should be exact.\n        ternary = mpfr.mpfr_fmod(z, x, y, rnd)\n        assert ternary == 0\n        if mpfr.mpfr_zero_p(z):\n            mpfr.mpfr_set_zero(rop, -y_negative)\n            return 0\n        else:\n            return mpfr.mpfr_add(rop, y, z, rnd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef script_hex_to_address( script_hex, blockchain='bitcoin', **blockchain_opts):\n    if blockchain == 'bitcoin':\n        return btc_script_hex_to_address(script_hex, **blockchain_opts)\n    else:\n        raise ValueError(\"Unknown blockchain '{}'\".format(blockchain))", "response": "Given a hex - encoded script extract an address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a pay - to - address script.", "response": "def make_payment_script(address, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    High-level API call (meant to be blockchain agnostic)\n    Make a pay-to-address script.\n    \"\"\"\n    \n    if blockchain == 'bitcoin':\n        return btc_make_payment_script(address, **blockchain_opts)\n    else:\n        raise ValueError(\"Unknown blockchain '{}'\".format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a data - script from a hex string.", "response": "def make_data_script( data, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    High-level API call (meant to be blockchain agnostic)\n    Make a data-bearing transaction output.\n    Data must be a hex string\n    Returns a hex string.\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_make_data_script(data, **blockchain_opts)\n    else:\n        raise ValueError(\"Unknown blockchain '{}'\".format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complex_validates(validate_rule):\n\n    ref_dict = {\n        # column_name: (\n        #   (predicate, arg1, ... argN),\n        #   ...\n        # )\n    }\n\n    for column_names, predicate_refs in validate_rule.items():\n        for column_name in _to_tuple(column_names):\n            ref_dict[column_name] = \\\n                ref_dict.get(column_name, tuple()) + _normalize_predicate_refs(predicate_refs)\n\n    return validates(*ref_dict.keys())(\n        lambda self, name, value: _validate_handler(name, value, ref_dict[name]))", "response": "Quickly setup attributes validation by one - time based on sqlalchemy. orm. validates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_handler(column_name, value, predicate_refs):\n\n    # only does validate when attribute value is not None\n    # else, just return it, let sqlalchemy decide if the value was legal according to `nullable` argument's value\n    if value is not None:\n        for predicate_ref in predicate_refs:\n            predicate, predicate_name, predicate_args = _decode_predicate_ref(predicate_ref)\n            validate_result = predicate(value, *predicate_args)\n\n            if isinstance(validate_result, dict) and 'value' in validate_result:\n                value = validate_result['value']\n            elif type(validate_result) != bool:\n                raise Exception(\n                    'predicate (name={}) can only return bool or dict(value=new_value) value'.format(predicate_name))\n            elif not validate_result:\n                raise ModelInvalid(u'db model validate failed: column={}, value={}, predicate={}, arguments={}'.format(\n                    column_name, value, predicate_name, ','.join(map(str, predicate_args))\n                ))\n    return value", "response": "handle predicate s return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the typical the row is ( opposite of how anomalous ).", "response": "def row_structural_typicality(X_L_list, X_D_list, row_id):\n    \"\"\"Returns how typical the row is (opposite of how anomalous).\"\"\"\n    count = 0\n    assert len(X_L_list) == len(X_D_list)\n    for X_L, X_D in zip(X_L_list, X_D_list):\n        for r in range(len(X_D[0])):\n            for c in range(\n                    len(X_L['column_partition']['assignments'])):\n                if X_D[X_L['column_partition']['assignments'][c]][r] == \\\n                        X_D[X_L['column_partition']['assignments'][c]][row_id]:\n                    count += 1\n    return float(count) / \\\n        (len(X_D_list) * len(X_D[0]) *\n            len(X_L_list[0]['column_partition']['assignments']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef column_structural_typicality(X_L_list, col_id):\n    count = 0\n    for X_L in X_L_list:\n        for c in range(len(X_L['column_partition']['assignments'])):\n            if X_L['column_partition']['assignments'][col_id] ==\\\n                    X_L['column_partition']['assignments'][c]:\n                count += 1\n    return float(count) / \\\n        (len(X_L_list) * len(X_L_list[0]['column_partition']['assignments']))", "response": "Returns how typical column is ( opposite of how anomalous."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the simple predictive probability averaged over each sample.", "response": "def simple_predictive_probability_multistate(M_c, X_L_list, X_D_list, Y, Q):\n    \"\"\"Returns the simple predictive probability, averaged over each sample.\"\"\"\n    logprobs = [float(simple_predictive_probability(M_c, X_L, X_D, Y, Q))\n        for X_L, X_D in zip(X_L_list, X_D_list)]\n    return logmeanexp(logprobs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the predictive probability, averaged over each sample.", "response": "def predictive_probability_multistate(M_c, X_L_list, X_D_list, Y, Q):\n    \"\"\"\n    Returns the predictive probability, averaged over each sample.\n    \"\"\"\n    logprobs = [float(predictive_probability(M_c, X_L, X_D, Y, Q))\n        for X_L, X_D in zip(X_L_list, X_D_list)]\n    return logmeanexp(logprobs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef similarity(\n        M_c, X_L_list, X_D_list, given_row_id, target_row_id,\n        target_column=None):\n    \"\"\"Returns the similarity of the given row to the target row, averaged over\n    all the column indexes given by col_idxs.\n\n    Similarity is defined as the proportion of times that two cells are in the same\n    view and category.\n    \"\"\"\n    score = 0.0\n\n    # Set col_idxs: defaults to all columns.\n    if target_column:\n        if type(target_column) == str:\n            col_idxs = [M_c['name_to_idx'][target_column]]\n        elif type(target_column) == list:\n            col_idxs = target_column\n        else:\n            col_idxs = [target_column]\n    else:\n        col_idxs = M_c['idx_to_name'].keys()\n    col_idxs = [int(col_idx) for col_idx in col_idxs]\n\n    ## Iterate over all latent states.\n    for X_L, X_D in zip(X_L_list, X_D_list):\n        for col_idx in col_idxs:\n            view_idx = X_L['column_partition']['assignments'][col_idx]\n            if X_D[view_idx][given_row_id] == X_D[view_idx][target_row_id]:\n                score += 1.0\n\n    return score / (len(X_L_list)*len(col_idxs))", "response": "Returns the similarity of the given row to the target row."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ascii2h5(bh_dir=None):\n\n    if bh_dir is None:\n        bh_dir = os.path.join(data_dir_default, 'bh')\n\n    fname = os.path.join(bh_dir, '{}.ascii')\n\n    f = h5py.File('bh.h5', 'w')\n\n    for region in ('hinorth', 'hisouth'):\n        data = np.loadtxt(fname.format(region), dtype='f4')\n\n        # Reshape and clip\n        data.shape = (210, 201) # (R, N)\n        data = data[:201]   # Last 9 records are empty\n\n        # Use NaNs where no data\n        data[data < -9000] = np.nan\n\n        dset = f.create_dataset(\n            region,\n            data=data,\n            chunks=True,\n            compression='gzip',\n            compression_opts=3\n        )\n\n        dset.attrs['axes'] = ('R', 'N')\n        dset.attrs['description'] = (\n            'HI 21cm column densities, in units of 10*NHYD. '\n            'R = 100 + [(90^o-|b|) sin(l)]/[0.3 degrees]. '\n            'N = 100 + [(90^o-|b|) cos (l)]/[0.3 degrees].'\n        )\n\n    for region in ('rednorth', 'redsouth'):\n        data = np.loadtxt(fname.format(region), dtype='f4')\n\n        # Reshape and clip\n        data.shape = (94, 1200) # (R, N)\n        data = data[:93]   # Last record is empty\n\n        # Use NaNs where no data\n        data[data < -9000] = np.nan\n\n        dset = f.create_dataset(\n            region,\n            data=data,\n            chunks=True,\n            compression='gzip',\n            compression_opts=3\n        )\n\n        dset.attrs['axes'] = ('R', 'N')\n        dset.attrs['description'] = (\n            'E(B-V), in units of 0.001 mag. '\n            'R = (|b| - 10) / (0.6 degrees). '\n            'N = (l + 0.15) / 0.3 - 1.'\n        )\n\n    f.attrs['description'] = (\n        'The Burstein & Heiles (1982) dust map.'\n    )\n\n    f.close()", "response": "Convert the Burstein & Heiles dust map from ASCII to HDF5."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the reddening E ( B - V ) at the specified location.", "response": "def query(self, coords):\n        \"\"\"\n        Returns E(B-V) at the specified location(s) on the sky.\n        \n        Args:\n            coords (`astropy.coordinates.SkyCoord`): The coordinates to query.\n\n        Returns:\n            A float array of reddening, in units of E(B-V), at the given\n            coordinates. The shape of the output is the same as the shape of the\n            coordinates stored by `coords`.\n        \"\"\"\n        # gal = coords.transform_to('galactic')\n        gal = coords\n        l = gal.l.deg\n        b = gal.b.deg\n\n        # Detect scalar input\n        scalar_input = not hasattr(l, '__len__')\n        if scalar_input:\n            l = np.array([l])\n            b = np.array([b])\n\n        # Fill return array with NaNs\n        ebv = np.empty(l.shape, dtype='f8')\n        ebv[:] = np.nan\n\n        # Fill northern cap\n        idx = (b >= 65.) & (b <= 90.)\n        ebv[idx] = self._lb2ebv_northcap(l[idx], b[idx])\n\n        # Fill southern cap\n        idx = (b <= -65.) & (b >= -90.)\n        ebv[idx] = self._lb2ebv_southcap(l[idx], b[idx])\n\n        # Fill northern midplane\n        idx = (b < 65.) & (b >= 10.)\n        ebv[idx] = self._lb2ebv_midnorth(l[idx], b[idx])\n\n        # Fill southern midplane\n        idx = (b > -65.) & (b <= -10.)\n        ebv[idx] = self._lb2ebv_midsouth(l[idx], b[idx])\n\n        if scalar_input:\n            ebv = ebv[0]\n\n        return ebv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef db_parse( block_id, opcode, op_payload, senders, inputs, outputs, fee, db_state=None ):\n   print \"\\nreference implementation of db_parse\\n\"\n   return None", "response": "This function is a helper function that parses the OP_RETURN nulldata into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the operation is valid for this virtual chain s database.", "response": "def db_check( block_id, opcode, op, txid, vtxindex, checked, db_state=None ):\n   \"\"\"\n   Given the block ID and a parsed operation, check to see if this is a *valid* operation\n   for the purposes of this virtual chain's database.\n   \n   Return True if so; False if not.\n   \"\"\"\n   print \"\\nreference implementation of db_check\\n\"\n   return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows list of docker images.", "response": "def admin_docker_list_view(context, request):\n    \"\"\"Show list of docker images.\"\"\"\n    return {\n        'paginator': Page(\n            context.all,\n            url_maker=lambda p: request.path_url + \"?page=%s\" % p,\n            page=int(request.params.get('page', 1)),\n            items_per_page=6\n        )\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef coord2healpix(coords, frame, nside, nest=True):\n    if coords.frame.name != frame:\n        c = coords.transform_to(frame)\n    else:\n        c = coords\n\n    if hasattr(c, 'ra'):\n        phi = c.ra.rad\n        theta = 0.5*np.pi - c.dec.rad\n        return hp.pixelfunc.ang2pix(nside, theta, phi, nest=nest)\n    elif hasattr(c, 'l'):\n        phi = c.l.rad\n        theta = 0.5*np.pi - c.b.rad\n        return hp.pixelfunc.ang2pix(nside, theta, phi, nest=nest)\n    elif hasattr(c, 'x'):\n        return hp.pixelfunc.vec2pix(nside, c.x.kpc, c.y.kpc, c.z.kpc, nest=nest)\n    elif hasattr(c, 'w'):\n        return hp.pixelfunc.vec2pix(nside, c.w.kpc, c.u.kpc, c.v.kpc, nest=nest)\n    else:\n        raise dustexceptions.CoordFrameError(\n            'No method to transform from coordinate frame \"{}\" to HEALPix.'.format(\n                frame))", "response": "Returns an array of HEALPix indices from an astropy SkyCoord."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_flat_galactic(f):\n\n    @wraps(f)\n    def _wrapper_func(self, coords, **kwargs):\n        # t0 = time.time()\n\n        if coords.frame.name != 'galactic':\n            gal = coords.transform_to('galactic')\n        else:\n            gal = coords\n\n        # t1 = time.time()\n\n        is_array = not coords.isscalar\n        if is_array:\n            orig_shape = coords.shape\n            shape_flat = (np.prod(orig_shape),)\n            # print 'Original shape: {}'.format(orig_shape)\n            # print 'Flattened shape: {}'.format(shape_flat)\n            gal = gal_to_shape(gal, shape_flat)\n        else:\n            gal = gal_to_shape(gal, (1,))\n\n        # t2 = time.time()\n\n        out = f(self, gal, **kwargs)\n\n        # t3 = time.time()\n\n        if is_array:\n            if isinstance(out, list) or isinstance(out, tuple):\n                # Apply to each array in output list\n                for o in out:\n                    o.shape = orig_shape + o.shape[1:]\n            else:   # Only one array in output\n                out.shape = orig_shape + out.shape[1:]\n        else:\n            if isinstance(out, list) or isinstance(out, tuple):\n                out = list(out)\n\n                # Apply to each array in output list\n                for k,o in enumerate(out):\n                    out[k] = o[0]\n            else:   # Only one array in output\n                out = out[0]\n\n        # t4 = time.time()\n\n        # print('')\n        # print('time inside ensure_flat_galactic: {:.4f} s'.format(t4-t0))\n        # print('{: >7.4f} s : {: >6.4f} s : transform_to(\"galactic\")'.format(t1-t0, t1-t0))\n        # print('{: >7.4f} s : {: >6.4f} s : reshape coordinates'.format(t2-t0, t2-t1))\n        # print('{: >7.4f} s : {: >6.4f} s : execute query'.format(t3-t0, t3-t2))\n        # print('{: >7.4f} s : {: >6.4f} s : reshape output'.format(t4-t0, t4-t3))\n        # print('')\n\n        return out\n\n    return _wrapper_func", "response": "A function that ensures that the input coordinates of the class method is a flat array of Galactic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_gal(self, l, b, d=None, **kwargs):\n\n        if not isinstance(l, units.Quantity):\n            l = l * units.deg\n        if not isinstance(b, units.Quantity):\n            b = b * units.deg\n\n        if d is None:\n            coords = coordinates.SkyCoord(l, b, frame='galactic')\n        else:\n            if not isinstance(d, units.Quantity):\n                d = d * units.kpc\n            coords = coordinates.SkyCoord(\n                l, b,\n                distance=d,\n                frame='galactic')\n\n        return self.query(coords, **kwargs)", "response": "Query using Galactic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_equ(self, ra, dec, d=None, frame='icrs', **kwargs):\n\n        valid_frames = ['icrs', 'fk4', 'fk5', 'fk4noeterms']\n\n        if frame not in valid_frames:\n            raise ValueError(\n                '`frame` not understood. Must be one of {}.'.format(valid_frames))\n\n        if not isinstance(ra, units.Quantity):\n            ra = ra * units.deg\n        if not isinstance(dec, units.Quantity):\n            dec = dec * units.deg\n\n        if d is None:\n            coords = coordinates.SkyCoord(ra, dec, frame='icrs')\n        else:\n            if not isinstance(d, units.Quantity):\n                d = d * units.kpc\n            coords = coordinates.SkyCoord(\n                ra, dec,\n                distance=d,\n                frame='icrs')\n\n        return self.query(coords, **kwargs)", "response": "Query the Solar System for a set of equatorial coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_request_parser(model_or_inst, excludes=None, only=None, for_populate=False):\n    is_inst = _is_inst(model_or_inst)\n\n    if isinstance(excludes, six.string_types):\n        excludes = [excludes]\n    if excludes and only:\n        only = None\n    elif isinstance(only, six.string_types):\n        only = [only]\n\n    parser = RequestPopulator() if for_populate else reqparse.RequestParser()\n    for col in model_or_inst.__table__.columns:\n        if only:\n            if col.name not in only:\n                continue\n        elif (excludes and col.name in excludes) or col.primary_key:\n                continue\n\n        col_type = col.type.python_type\n        kwargs = {\n            \"type\": _type_dict.get(col_type.__name__, col_type) if hasattr(col_type, '__name__') else col_type\n        }\n        # When the context was to creating a new model instance, if a field has no default value, and is not nullable,\n        #  mark it's corresponding argument as `required`.\n        # \u521b\u5efa\u65b0\u6570\u636e\u5e93\u5b9e\u4f8b\u65f6\uff0c\u82e5\u4e00\u4e2a\u5b57\u6bb5\u65e2\u6ca1\u6709\u9ed8\u8ba4\u503c\uff0c\u53c8\u4e0d\u5141\u8bb8 NULL\uff0c\u5219\u628a\u5b83\u5bf9\u5e94 arg \u8bbe\u4e3a required\n        if not is_inst and col.default is None and col.server_default is None and not col.nullable:\n            kwargs[\"required\"] = True\n        parser.add_argument(col.name, **kwargs)\n    return parser", "response": "This function creates a RequestParser that extracts user request data from the request. json file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_model(model_or_inst, excludes=None, only=None):\n    inst = model_or_inst if _is_inst(model_or_inst) else model_or_inst()\n\n    parser = make_request_parser(model_or_inst, excludes, only, for_populate=True)\n    req_args = parser.parse_args()\n\n    for key, value in req_args.items():\n        setattr(inst, key, value)\n\n    return inst", "response": "Populate a new model instance with user - supplied data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mpfr_get_str2(base, ndigits, op, rounding_mode):\n    digits, exp = mpfr.mpfr_get_str(base, ndigits, op, rounding_mode)\n    negative = digits.startswith('-')\n    if negative:\n        digits = digits[1:]\n    return negative, digits, exp", "response": "This function is a private function that is used by the mpfr_get_str function. It is used by the mpfr module to convert the byte - string produced by mpfr_get_str to Unicode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a string of digits and an integer based on the decimal point relative to the start of that string.", "response": "def _format_finite(negative, digits, dot_pos):\n    \"\"\"Given a (possibly empty) string of digits and an integer\n    dot_pos indicating the position of the decimal point relative to\n    the start of that string, output a formatted numeric string with\n    the same value and same implicit exponent.\"\"\"\n\n    # strip leading zeros\n    olddigits = digits\n    digits = digits.lstrip('0')\n    dot_pos -= len(olddigits) - len(digits)\n\n    # value is 0.digits * 10**dot_pos\n    use_exponent = dot_pos <= -4 or dot_pos > len(digits)\n    if use_exponent:\n        exp = dot_pos - 1 if digits else dot_pos\n        dot_pos -= exp\n\n    # left pad with zeros, insert decimal point, and add exponent\n    if dot_pos <= 0:\n        digits = '0' * (1 - dot_pos) + digits\n        dot_pos += 1 - dot_pos\n    assert 1 <= dot_pos <= len(digits)\n    if dot_pos < len(digits):\n        digits = digits[:dot_pos] + '.' + digits[dot_pos:]\n    if use_exponent:\n        digits += \"e{0:+03d}\".format(exp)\n    return '-' + digits if negative else digits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_up(x, context=None):\n    x = BigFloat._implicit_convert(x)\n    # make sure we don't alter any flags\n    with _saved_flags():\n        with (context if context is not None else EmptyContext):\n            with RoundTowardPositive:\n                # nan maps to itself\n                if is_nan(x):\n                    return +x\n\n                # round to current context; if value changes, we're done\n                y = +x\n                if y != x:\n                    return y\n\n                # otherwise apply mpfr_nextabove\n                bf = y.copy()\n                mpfr.mpfr_nextabove(bf)\n                # apply + one more time to deal with subnormals\n                return +bf", "response": "next_up - return the least representable float that s strictly greater than x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_flagstate(flagset):\n    if not flagset <= _all_flags:\n        raise ValueError(\"unrecognized flags in flagset\")\n\n    for f in flagset:\n        set_flag(f)\n    for f in _all_flags - flagset:\n        clear_flag(f)", "response": "Set all flags in flagset and clear all other flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_str2(s, base, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        _set_from_whole_string,\n        (s, base),\n        context,\n    )", "response": "Convert the string s in base to a BigFloat instance rounding it according to the current context."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pos(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_set,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return x rounded to the current context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns x + y.", "response": "def add(x, y, context=None):\n    \"\"\"\n    Return ``x`` + ``y``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_add,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sub(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sub,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return x - y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mul(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_mul,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return x times y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sqr(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sqr,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the square of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns x divided by y.", "response": "def div(x, y, context=None):\n    \"\"\"\n    Return ``x`` divided by ``y``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_div,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the floor of x divided by y.", "response": "def floordiv(x, y, context=None):\n    \"\"\"\n    Return the floor of ``x`` divided by ``y``.\n\n    The result is a ``BigFloat`` instance, rounded to the\n    context if necessary.  Special cases match those of the\n    ``div`` function.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr_floordiv,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the remainder of x divided by y with sign matching that of y.", "response": "def mod(x, y, context=None):\n    \"\"\"\n    Return the remainder of x divided by y, with sign matching that of y.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr_mod,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef divmod(x, y, context=None):\n    return floordiv(x, y, context=context), mod(x, y, context=context)", "response": "Divide x and y into two parts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sqrt(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sqrt,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the square root of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the reciprocal square root of x.", "response": "def rec_sqrt(x, context=None):\n    \"\"\"\n    Return the reciprocal square root of x.\n\n    Return +Inf if x is \u00b10, +0 if x is +Inf, and NaN if x is negative.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_rec_sqrt,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the cube root of x.", "response": "def cbrt(x, context=None):\n    \"\"\"\n    Return the cube root of x.\n\n    For x negative, return a negative number.  The cube root of -0 is defined\n    to be -0.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_cbrt,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the kth root of x.", "response": "def root(x, k, context=None):\n    \"\"\"\n    Return the kth root of x.\n\n    For k odd and x negative (including -Inf), return a negative number.\n    For k even and x negative (including -Inf), return NaN.\n\n    The kth root of -0 is defined to be -0, whatever the parity of k.\n\n    This function is only implemented for nonnegative k.\n\n    \"\"\"\n    if k < 0:\n        raise ValueError(\"root function not implemented for negative k\")\n\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_root,\n        (BigFloat._implicit_convert(x), k),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pow(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_pow,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return x raised to the power y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef abs(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_abs,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the absolute value of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the dimension of x and y.", "response": "def dim(x, y, context=None):\n    \"\"\"\n    Return max(x - y, 0).\n\n    Return x - y if x > y, +0 if x <= y, and NaN if either x or y is NaN.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_dim,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmp(op1, op2):\n    op1 = BigFloat._implicit_convert(op1)\n    op2 = BigFloat._implicit_convert(op2)\n    if is_nan(op1) or is_nan(op2):\n        raise ValueError(\"Cannot perform comparison with NaN.\")\n    return mpfr.mpfr_cmp(op1, op2)", "response": "Perform a three - way comparison of op1 and op2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares the absolute values of op1 and op2.", "response": "def cmpabs(op1, op2):\n    \"\"\"\n    Compare the absolute values of op1 and op2.\n\n    Return a positive value if op1 > op2, zero if op1 = op2, and a negative\n    value if op1 < op2. Both op1 and op2 are considered to their full own\n    precision, which may differ. If one of the operands is NaN, raise\n    ValueError.\n\n    Note: This function may be useful to distinguish the three possible\n    cases. If you need to distinguish two cases only, it is recommended to use\n    the predicate functions like 'greaterequal'; they behave like the IEEE 754\n    comparisons, in particular when one or both arguments are NaN.\n\n    \"\"\"\n    op1 = BigFloat._implicit_convert(op1)\n    op2 = BigFloat._implicit_convert(op2)\n    if is_nan(op1) or is_nan(op2):\n        raise ValueError(\"Cannot perform comparison with NaN.\")\n    return mpfr.mpfr_cmpabs(op1, op2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sgn(x):\n    x = BigFloat._implicit_convert(x)\n    if is_nan(x):\n        raise ValueError(\"Cannot take sign of a NaN.\")\n    return mpfr.mpfr_sgn(x)", "response": "Return the sign of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if x > y and False otherwise.", "response": "def greater(x, y):\n    \"\"\"\n    Return True if x > y and False otherwise.\n\n    This function returns False whenever x and/or y is a NaN.\n\n    \"\"\"\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_greater_p(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef greaterequal(x, y):\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_greaterequal_p(x, y)", "response": "Return True if x > y and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef less(x, y):\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_less_p(x, y)", "response": "Return True if x < y and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if x < y and False otherwise.", "response": "def lessequal(x, y):\n    \"\"\"\n    Return True if x <= y and False otherwise.\n\n    This function returns False whenever x and/or y is a NaN.\n\n    \"\"\"\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_lessequal_p(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef equal(x, y):\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_equal_p(x, y)", "response": "Return True if x == y and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notequal(x, y):\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return not mpfr.mpfr_equal_p(x, y)", "response": "Return True if x!= y and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if x < y and False otherwise.", "response": "def lessgreater(x, y):\n    \"\"\"\n    Return True if x < y or x > y and False otherwise.\n\n    This function returns False whenever x and/or y is a NaN.\n\n    \"\"\"\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_lessgreater_p(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if x or y is a NaN and False otherwise.", "response": "def unordered(x, y):\n    \"\"\"\n    Return True if x or y is a NaN and False otherwise.\n\n    \"\"\"\n    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_unordered_p(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_log,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the natural logarithm of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log2(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_log2,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the base - two logarithm of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log10(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_log10,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the base - ten logarithm of x."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exp(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_exp,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the exponential of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exp2(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_exp2,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return two raised to the power x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning ten raised to the power x.", "response": "def exp10(x, context=None):\n    \"\"\"\n    Return ten raised to the power x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_exp10,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cos(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_cos,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the cosine of x."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sin(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sin,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the sine of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the tangent of x.", "response": "def tan(x, context=None):\n    \"\"\"\n    Return the tangent of ``x``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_tan,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sec(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sec,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the secant of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the cosecant of x.", "response": "def csc(x, context=None):\n    \"\"\"\n    Return the cosecant of ``x``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_csc,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the cotangent of x.", "response": "def cot(x, context=None):\n    \"\"\"\n    Return the cotangent of ``x``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_cot,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef acos(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_acos,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the inverse cosine of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asin(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_asin,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the inverse sine of x."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the inverse tangent of x.", "response": "def atan(x, context=None):\n    \"\"\"\n    Return the inverse tangent of ``x``.\n\n    The mathematically exact result lies in the range [-\u03c0/2, \u03c0/2].  However,\n    note that as a result of rounding to the current context, it's possible\n    for the actual value to lie just outside this range.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_atan,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the angle of the two given values.", "response": "def atan2(y, x, context=None):\n    \"\"\"\n    Return ``atan(y / x)`` with the appropriate choice of function branch.\n\n    If ``x > 0``, then ``atan2(y, x)`` is mathematically equivalent to ``atan(y\n    / x)``.  If ``x < 0`` and ``y > 0``, ``atan(y, x)`` is equivalent to ``\u03c0 +\n    atan(y, x)``.  If ``x < 0`` and ``y < 0``, the result is ``-\u03c0 + atan(y,\n    x)``.\n\n    Geometrically, ``atan2(y, x)`` is the angle (measured counterclockwise, in\n    radians) from the positive x-axis to the line segment joining (0, 0) to (x,\n    y), in the usual representation of the x-y plane.\n\n    Special values are handled as described in the ISO C99 and IEEE 754-2008\n    standards for the atan2 function.  The following examples illustrate the\n    rules for positive y; for negative y, apply the symmetry ``atan(-y, x) ==\n    -atan(y, x)``.\n\n        >>> finite = positive = 2.3\n        >>> negative = -2.3\n        >>> inf = BigFloat('inf')\n\n        >>> print(atan2(+0.0, -0.0))     # pi\n        3.1415926535897931\n        >>> print(atan2(+0.0, +0.0))     # 0\n        0\n        >>> print(atan2(+0.0, negative)) # pi\n        3.1415926535897931\n        >>> print(atan2(+0.0, positive)) # 0\n        0\n        >>> print(atan2(positive, 0.0))  # pi / 2\n        1.5707963267948966\n        >>> print(atan2(inf, -inf))      # 3*pi / 4\n        2.3561944901923448\n        >>> print(atan2(inf, inf))       # pi / 4\n        0.78539816339744828\n        >>> print(atan2(inf, finite))    # pi / 2\n        1.5707963267948966\n        >>> print(atan2(positive, -inf)) # pi\n        3.1415926535897931\n        >>> print(atan2(positive, +inf)) # 0\n        0\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_atan2,\n        (\n            BigFloat._implicit_convert(y),\n            BigFloat._implicit_convert(x),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cosh(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_cosh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the hyperbolic cosine of x."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sinh(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sinh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the hyperbolic sine of x."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tanh(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_tanh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the hyperbolic tangent of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the hyperbolic secant of x.", "response": "def sech(x, context=None):\n    \"\"\"\n    Return the hyperbolic secant of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_sech,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef csch(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_csch,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the hyperbolic cosecant of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the hyperbolic cotangent of x.", "response": "def coth(x, context=None):\n    \"\"\"\n    Return the hyperbolic cotangent of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_coth,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the inverse hyperbolic cosine of x.", "response": "def acosh(x, context=None):\n    \"\"\"\n    Return the inverse hyperbolic cosine of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_acosh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the inverse hyperbolic sine of x.", "response": "def asinh(x, context=None):\n    \"\"\"\n    Return the inverse hyperbolic sine of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_asinh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the inverse hyperbolic tangent of x.", "response": "def atanh(x, context=None):\n    \"\"\"\n    Return the inverse hyperbolic tangent of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_atanh,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log1p(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_log1p,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the logarithm of one plus x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expm1(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_expm1,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return one less than the exponential of x."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eint(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_eint,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the exponential integral of x."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the real part of the dilogarithm of x.", "response": "def li2(x, context=None):\n    \"\"\"\n    Return the real part of the dilogarithm of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_li2,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the Gamma function of x.", "response": "def gamma(x, context=None):\n    \"\"\"\n    Return the value of the Gamma function of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_gamma,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lngamma(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_lngamma,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the logarithm of the Gamma function of x."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lgamma(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        lambda rop, op, rnd: mpfr.mpfr_lgamma(rop, op, rnd)[0],\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the logarithm of the absolute value of the Gamma function at x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef digamma(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_digamma,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the digamma function on the current context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zeta(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_zeta,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the Riemann zeta function on x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef erf(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_erf,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the error function at x."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef erfc(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_erfc,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the complementary error function at x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the first kind Bessel function of order 0 at x.", "response": "def j0(x, context=None):\n    \"\"\"\n    Return the value of the first kind Bessel function of order 0 at x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_j0,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef j1(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_j1,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the first kind Bessel function of order 1 at x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the value of the first kind Bessel function of order n at x.", "response": "def jn(n, x, context=None):\n    \"\"\"\n    Return the value of the first kind Bessel function of order ``n`` at ``x``.\n\n    ``n`` should be a Python integer.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_jn,\n        (n, BigFloat._implicit_convert(x)),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the second kind Bessel function of order 0 at x.", "response": "def y0(x, context=None):\n    \"\"\"\n    Return the value of the second kind Bessel function of order 0 at x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_y0,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef y1(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_y1,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the value of the second kind Bessel function of order 1 at x."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef yn(n, x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_yn,\n        (n, BigFloat._implicit_convert(x)),\n        context,\n    )", "response": "Return the value of the second kind Bessel function of order n at\n    x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn x * y + z with a single rounding according to the current context.", "response": "def fma(x, y, z, context=None):\n    \"\"\"\n    Return (x * y) + z, with a single rounding according to the current\n    context.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_fma,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n            BigFloat._implicit_convert(z),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning x * y - z with a single rounding according to the current context.", "response": "def fms(x, y, z, context=None):\n    \"\"\"\n    Return (x * y) - z, with a single rounding according to the current\n    context.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_fms,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n            BigFloat._implicit_convert(z),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the arithmetic geometric mean of x and y.", "response": "def agm(x, y, context=None):\n    \"\"\"\n    Return the arithmetic geometric mean of x and y.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_agm,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hypot(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_hypot,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return the Euclidean norm of x and y i. e. the square root of the sum of x and y."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ai(x, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_ai,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "response": "Return the Airy function of x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the next higher or equal integer to x.", "response": "def ceil(x, context=None):\n    \"\"\"\n    Return the next higher or equal integer to x.\n\n    If the result is not exactly representable, it will be rounded according to\n    the current context.  Note that the rounding step means that it's possible\n    for the result to be smaller than ``x``.  For example::\n\n        >>> x = 2**100 + 1\n        >>> ceil(2**100 + 1) >= x\n        False\n\n    One way to be sure of getting a result that's greater than or equal to\n    ``x`` is to use the ``RoundTowardPositive`` rounding mode::\n\n        >>> with RoundTowardPositive:\n        ...     x = 2**100 + 1\n        ...     ceil(x) >= x\n        ...\n        True\n\n    Similar comments apply to the :func:`floor`, :func:`round` and\n    :func:`trunc` functions.\n\n    .. note::\n\n       This function corresponds to the MPFR function ``mpfr_rint_ceil``,\n       not to ``mpfr_ceil``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_rint_ceil,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the next lower or equal integer to x.", "response": "def floor(x, context=None):\n    \"\"\"\n    Return the next lower or equal integer to x.\n\n    If the result is not exactly representable, it will be rounded according to\n    the current context.\n\n    Note that it's possible for the result to be larger than ``x``.  See the\n    documentation of the :func:`ceil` function for more information.\n\n    .. note::\n\n       This function corresponds to the MPFR function ``mpfr_rint_floor``,\n       not to ``mpfr_floor``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_rint_floor,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the nearest integer to x rounded halfway cases away from zero.", "response": "def round(x, context=None):\n    \"\"\"\n    Return the nearest integer to x, rounding halfway cases *away from zero*.\n\n    If the result is not exactly representable, it will be rounded according to\n    the current context.\n\n    .. note::\n\n       This function corresponds to the MPFR function ``mpfr_rint_round``, not\n       to ``mpfr_round``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_rint_round,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the next integer towards zero.", "response": "def trunc(x, context=None):\n    \"\"\"\n    Return the next integer towards zero.\n\n    If the result is not exactly representable, it will be rounded according to\n    the current context.\n\n    .. note::\n\n       This function corresponds to the MPFR function ``mpfr_rint_trunc``,\n       not to ``mpfr_trunc``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_rint_trunc,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the fractional part of x.", "response": "def frac(x, context=None):\n    \"\"\"\n    Return the fractional part of ``x``.\n\n    The result has the same sign as ``x``.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_frac,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn x reduced modulo y.", "response": "def fmod(x, y, context=None):\n    \"\"\"\n    Return ``x`` reduced modulo ``y``.\n\n    Returns the value of x - n * y, where n is the integer quotient of x\n    divided by y, rounded toward zero.\n\n    Special values are handled as described in Section F.9.7.1 of the ISO C99\n    standard: If x is infinite or y is zero, the result is NaN. If y is\n    infinite and x is finite, the result is x rounded to the current context.\n    If the result is zero, it has the sign of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_fmod,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns x reduced modulo y.", "response": "def remainder(x, y, context=None):\n    \"\"\"\n    Return x reduced modulo y.\n\n    Returns the value of x - n * y, where n is the integer quotient of x\n    divided by y, rounded to the nearest integer (ties rounded to even).\n\n    Special values are handled as described in Section F.9.7.1 of the ISO C99\n    standard: If x is infinite or y is zero, the result is NaN. If y is\n    infinite and x is finite, the result is x (rounded to the current\n    context). If the result is zero, it has the sign of x.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_remainder,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef min(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_min,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return the minimum of x and y."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max(x, y, context=None):\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_max,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "response": "Return the maximum of x and y."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new BigFloat object with the magnitude of x but the sign of y.", "response": "def copysign(x, y, context=None):\n    \"\"\"\n    Return a new BigFloat object with the magnitude of x but the sign of y.\n\n    \"\"\"\n    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_copysign,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an integer float or BigFloat with no loss of precision.", "response": "def exact(cls, value, precision=None):\n        \"\"\"Convert an integer, float or BigFloat with no loss of precision.\n        Also convert a string with given precision.\n\n        This constructor makes no use of the current context.\n        \"\"\"\n        # figure out precision to use\n        if isinstance(value, six.string_types):\n            if precision is None:\n                raise TypeError(\"precision must be supplied when \"\n                                \"converting from a string\")\n        else:\n            if precision is not None:\n                raise TypeError(\"precision argument should not be \"\n                                \"specified except when converting \"\n                                \"from a string\")\n            if isinstance(value, float):\n                precision = _builtin_max(DBL_PRECISION, PRECISION_MIN)\n            elif isinstance(value, six.integer_types):\n                precision = _builtin_max(_bit_length(value), PRECISION_MIN)\n            elif isinstance(value, BigFloat):\n                precision = value.precision\n            else:\n                raise TypeError(\"Can't convert argument %s of type %s \"\n                                \"to BigFloat\" % (value, type(value)))\n\n        # Use unlimited exponents, with given precision.\n        with _saved_flags():\n            set_flagstate(set())  # clear all flags\n            context = (\n                WideExponentContext +\n                Context(precision=precision) +\n                RoundTiesToEven\n            )\n            with context:\n                result = BigFloat(value)\n            if test_flag(Overflow):\n                raise ValueError(\"value too large to represent as a BigFloat\")\n            if test_flag(Underflow):\n                raise ValueError(\"value too small to represent as a BigFloat\")\n            if test_flag(Inexact) and not isinstance(value, six.string_types):\n                # since this is supposed to be an exact conversion, the\n                # inexact flag should never be set except when converting\n                # from a string.\n                assert False, (\"Inexact conversion in BigFloat.exact. \"\n                               \"This shouldn't ever happen.  Please report.\")\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the significand of self as a BigFloat.", "response": "def _significand(self):\n        \"\"\"Return the significand of self, as a BigFloat.\n\n        If self is a nonzero finite number, return a BigFloat m\n        with the same precision as self, such that\n\n          0.5 <= m < 1. and\n          self = +/-m * 2**e\n\n        for some exponent e.\n\n        If self is zero, infinity or nan, return a copy of self with\n        the sign set to 0.\n\n        \"\"\"\n        m = self.copy()\n        if self and is_finite(self):\n            mpfr.mpfr_set_exp(m, 0)\n        mpfr.mpfr_setsign(m, m, False, ROUND_TIES_TO_EVEN)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _exponent(self):\n        if self and is_finite(self):\n            return mpfr.mpfr_get_exp(self)\n\n        if not self:\n            return '0'\n        elif is_inf(self):\n            return 'inf'\n        elif is_nan(self):\n            return 'nan'\n        else:\n            assert False, \"shouldn't ever get here\"", "response": "Return the exponent of self as an integer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of the current context.", "response": "def copy(self):\n        \"\"\" Return a copy of self.\n\n        This function does not make use of the context.  The result has the\n        same precision as the original.\n\n        \"\"\"\n        result = mpfr.Mpfr_t.__new__(BigFloat)\n        mpfr.mpfr_init2(result, self.precision)\n        mpfr.mpfr_set(result, self, ROUND_TIES_TO_EVEN)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a copy of self with the opposite sign bit set.", "response": "def copy_neg(self):\n        \"\"\" Return a copy of self with the opposite sign bit.\n\n        Unlike -self, this does not make use of the context:  the result\n        has the same precision as the original.\n\n        \"\"\"\n        result = mpfr.Mpfr_t.__new__(BigFloat)\n        mpfr.mpfr_init2(result, self.precision)\n        new_sign = not self._sign()\n        mpfr.mpfr_setsign(result, self, new_sign, ROUND_TIES_TO_EVEN)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_abs(self):\n        result = mpfr.Mpfr_t.__new__(BigFloat)\n        mpfr.mpfr_init2(result, self.precision)\n        mpfr.mpfr_setsign(result, self, False, ROUND_TIES_TO_EVEN)\n        return result", "response": "Return a copy of self with the sign bit unset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hex(self):\n\n        sign = '-' if self._sign() else ''\n        e = self._exponent()\n        if isinstance(e, six.string_types):\n            return sign + e\n\n        m = self._significand()\n        _, digits, _ = _mpfr_get_str2(\n            16,\n            0,\n            m,\n            ROUND_TIES_TO_EVEN,\n        )\n        # only print the number of digits that are actually necessary\n        n = 1 + (self.precision - 1) // 4\n        assert all(c == '0' for c in digits[n:])\n        result = '%s0x0.%sp%+d' % (sign, digits[:n], e)\n        return result", "response": "Return a hexadecimal representation of a BigFloat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning pair n d such that the value of self is relatively prime and n and d are relatively prime and n and d are relatively prime and n and d are relatively prime and d < 1.", "response": "def as_integer_ratio(self):\n        \"\"\"Return pair n, d of integers such that the value of self is\n        exactly equal to n/d, n and d are relatively prime, and d >= 1.\n\n        \"\"\"\n        if not is_finite(self):\n            raise ValueError(\"Can't express infinity or nan as \"\n                             \"an integer ratio\")\n        elif not self:\n            return 0, 1\n\n        # convert to a hex string, and from there to a fraction\n        negative, digits, e = _mpfr_get_str2(\n            16,\n            0,\n            self,\n            ROUND_TIES_TO_EVEN,\n        )\n        digits = digits.rstrip('0')\n\n        # find number of trailing 0 bits in last hex digit\n        v = int(digits[-1], 16)\n        v &= -v\n        n, d = int(digits, 16) // v, 1\n        e = (e - len(digits) << 2) + {1: 0, 2: 1, 4: 2, 8: 3}[v]\n\n        # abs(number) now has value n * 2**e, and n is odd\n        if e >= 0:\n            n <<= e\n        else:\n            d <<= -e\n\n        return (-n if negative else n), d"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a nonzero finite BigFloat instance to a given number of significant digits and return a triple of the format that is a floating precision.", "response": "def _format_to_floating_precision(self, precision):\n        \"\"\" Format a nonzero finite BigFloat instance to a given number of\n        significant digits.\n\n        Returns a triple (negative, digits, exp) where:\n\n          - negative is a boolean, True for a negative number, else False\n          - digits is a string giving the digits of the output\n          - exp represents the exponent of the output,\n\n        The normalization of the exponent is such that <digits>E<exp>\n        represents the decimal approximation to self.\n\n        Rounding is always round-to-nearest.\n        \"\"\"\n        if precision <= 0:\n            raise ValueError(\"precision argument should be at least 1\")\n\n        sign, digits, exp = _mpfr_get_str2(\n            10,\n            precision,\n            self,\n            ROUND_TIES_TO_EVEN,\n        )\n\n        return sign, digits, exp - len(digits)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_to_fixed_precision(self, precision):\n        # MPFR only provides functions to format to a given number of\n        # significant digits.  So we must:\n        #\n        #   (1) Identify an e such that 10**(e-1) <= abs(x) < 10**e.\n        #\n        #   (2) Determine the number of significant digits required, and format\n        #       to that number of significant digits.\n        #\n        #   (3) Adjust output if necessary if it's been rounded up to 10**e.\n\n        # Zeros\n        if is_zero(self):\n            return is_negative(self), '0', -precision\n\n        # Specials\n        if is_inf(self):\n            return is_negative(self), 'inf', None\n\n        if is_nan(self):\n            return is_negative(self), 'nan', None\n\n        # Figure out the exponent by making a call to get_str2.  exp satisfies\n        # 10**(exp-1) <= self < 10**exp\n        _, _, exp = _mpfr_get_str2(\n            10,\n            2,\n            self,\n            ROUND_TOWARD_ZERO,\n        )\n\n        sig_figs = exp + precision\n\n        if sig_figs < 0:\n            sign = self._sign()\n            return sign, '0', -precision\n\n        elif sig_figs == 0:\n            # Ex: 0.1 <= x < 1.0, rounding x to nearest multiple of 1.0.\n            # Or: 100.0 <= x < 1000.0, rounding x to nearest multiple of 1000.0\n            sign, digits, new_exp = _mpfr_get_str2(\n                10,\n                2,\n                self,\n                ROUND_TOWARD_NEGATIVE,\n            )\n            if int(digits) == 50:\n                # Halfway case\n                sign, digits, new_exp = _mpfr_get_str2(\n                    10,\n                    2,\n                    self,\n                    ROUND_TOWARD_POSITIVE,\n                )\n\n            digits = '1' if int(digits) > 50 or new_exp == exp + 1 else '0'\n            return sign, digits, -precision\n\n        negative, digits, new_exp = self._format_to_floating_precision(\n            sig_figs\n        )\n\n        # It's possible that the rounding up involved changes the exponent;\n        # in that case we have to adjust the digits accordingly.  The only\n        # possibility should be that new_exp == exp + 1.\n        if new_exp + len(digits) != exp:\n            assert new_exp + len(digits) == exp + 1\n            digits += '0'\n\n        return negative, digits, -precision", "response": "Format self to a given number of digits after the decimal point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _implicit_convert(cls, arg):\n\n        # ints, long and floats mix freely with BigFloats, and are\n        # converted exactly.\n        if isinstance(arg, six.integer_types) or isinstance(arg, float):\n            return cls.exact(arg)\n        elif isinstance(arg, BigFloat):\n            return arg\n        else:\n            raise TypeError(\"Unable to convert argument %s of type %s \"\n                            \"to BigFloat\" % (arg, type(arg)))", "response": "Implicit conversion used for binary operations comparisons and comparisons etc."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the merkle pairs of a row of a merkle tree.", "response": "def calculate_merkle_pairs(bin_hashes, hash_function=bin_double_sha256):\n    \"\"\"\n    Calculate the parents of a row of a merkle tree.\n    Takes in a list of binary hashes, returns a binary hash.\n\n    The returned parents list is such that parents[i] == hash(bin_hashes[2*i] + bin_hashes[2*i+1]).\n    \"\"\"\n    hashes = list(bin_hashes)\n\n    # if there are an odd number of hashes, double up the last one\n    if len(hashes) % 2 == 1:\n        hashes.append(hashes[-1])\n\n    new_hashes = []\n    for i in range(0, len(hashes), 2):\n        new_hashes.append(hash_function(hashes[i] + hashes[i+1]))\n\n    return new_hashes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies a given merkle path.", "response": "def verify_merkle_path(merkle_root_hex, serialized_path, leaf_hash_hex, hash_function=bin_double_sha256):\n    \"\"\"\n    Verify a merkle path.  The given path is the path from two leaf nodes to the root itself.\n\n    merkle_root_hex is a little-endian, hex-encoded hash.\n    serialized_path is the serialized merkle path\n    path_hex is a list of little-endian, hex-encoded hashes.\n\n    Return True if the path is consistent with the merkle root.\n    Return False if not.\n    \"\"\"\n    \n    merkle_root = hex_to_bin_reversed(merkle_root_hex)\n    leaf_hash = hex_to_bin_reversed(leaf_hash_hex)\n\n    path = MerkleTree.path_deserialize(serialized_path)\n    path = [{'order': p['order'], 'hash': hex_to_bin_reversed(p['hash'])} for p in path]\n\n    if len(path) == 0:\n        raise ValueError(\"Empty path\")\n\n    cur_hash = leaf_hash\n    for i in range(0, len(path)):\n        if path[i]['order'] == 'l':\n            # left sibling\n            cur_hash = hash_function(path[i]['hash'] + cur_hash)\n        elif path[i]['order'] == 'r':\n            # right sibling\n            cur_hash = hash_function(cur_hash + path[i]['hash'])\n        elif path[i]['order'] == 'm':\n            # merkle root\n            assert len(path) == 1\n            return cur_hash == path[i]['hash']\n\n    return cur_hash == merkle_root"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of dicts return a string.", "response": "def path_serialize(cls, path):\n        \"\"\"\n        Given a list of [{'hash': ..., 'order': ...}], serialize it to a string.\n        \"\"\"\n        # make it into a netstring\n        path_parts = ['{}-{}'.format(p['order'], p['hash']) for p in path]\n        path_ns_parts = ['{}:{},'.format(len(pp), pp) for pp in path_parts]\n        path_str = ''.join(path_ns_parts)\n        return '{}:{},'.format(len(path_str), path_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path_deserialize(cls, serialized_path):\n        def _chomp_netstring_payload(s):\n            try:\n                ns_len_str, ns_body = s.split(':', 1)\n                ns_len = int(ns_len_str)\n                assert ns_body[ns_len] == ','\n                ns_payload = ns_body[:ns_len]\n                return ns_payload, ns_body[ns_len+1:]\n            except:\n                raise ValueError(\"Invalid netstring '{}'\".format(s))\n\n        path_str, extra = _chomp_netstring_payload(serialized_path)\n        if len(extra) > 0:\n            raise ValueError(\"Danlging data in '{}'\".format(serialized_path))\n\n        path = []\n        while True:\n            path_part, path_str = _chomp_netstring_payload(path_str)\n            try:\n                order, hash_hex = path_part.split('-', 1)\n                assert order in ['l', 'r', 'm']\n                path.append({'order': order, 'hash': hash_hex})\n            except:\n                raise ValueError(\"Invalid path entry {}\".format(path_part))\n\n            if len(path_str) == 0:\n                break\n\n        return path", "response": "Given a netstring of path parts go and parse it back into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the path from the leaf hash to the root.", "response": "def path(self, leaf_hash_hex, serialize=True):\n        \"\"\"\n        Get the path (as a list of hashes) from the leaf hash to the root.\n        leaf_hash_hex is hex-encoded, little-endian.\n\n        The returned path will be a list of {'hash': hex-encoded little-endian hash, 'order': 'l' or 'r'}\n\n        Raise ValueError if leaf_hash is not present in the tree.\n        \"\"\"\n        \n        leaf_hash = hex_to_bin_reversed(leaf_hash_hex)\n\n        # sanity check\n        found = False\n        ri = None       # index into self.rows where leaf_hash occurs.  Note that self.rows[0] is the bottom (leaves) of the Merkle tree.\n        for ri, row in enumerate(self.rows):\n            found = found or (leaf_hash in row)\n            if found:\n                break\n\n        if not found:\n            raise ValueError(\"Hash {} is not present in Merkle tree {}\".format(leaf_hash, self.root()))\n\n        path = []\n        cur_hash = leaf_hash\n        for i in range(ri, len(self.rows)-1):\n            # find sibling\n            sibling = {}\n            leaf_index = self.rows[i].index(cur_hash)\n            if leaf_index % 2 == 0:\n                # append left sibling\n                sibling_hash = None\n                if leaf_index+1 >= len(self.rows[i]):\n                    # double-up the last hash\n                    assert leaf_index+1 == len(self.rows[i]), 'leaf_index = {}, i = {}, len(rows[i]) = {}, rows[0] = {}'.format(leaf_index, i, len(self.rows[i]), ','.join(r.encode('hex') for r in self.rows[0]))\n                    sibling_hash = self.rows[i][-1]\n                else:\n                    sibling_hash = self.rows[i][leaf_index+1]\n\n                sibling['hash'] = bin_to_hex_reversed(sibling_hash)\n                sibling['order'] = 'r'\n            else:\n                # append right sibling\n                sibling['hash'] = bin_to_hex_reversed(self.rows[i][leaf_index-1])\n                sibling['order'] = 'l'\n\n            path.append(sibling)\n\n            # find parent\n            cur_hash = self.rows[i+1][leaf_index/2]\n        \n        if len(path) == 0:\n            # single-node merkle tree\n            path = [{'hash': bin_to_hex_reversed(self.rows[-1][0]), 'order': 'm'}]\n\n        if not serialize:\n            return path\n        else:\n            return self.path_serialize(path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _coords2vec(self, coords):\n\n        # c = coords.transform_to(self._frame)\n        # vec = np.empty((c.shape[0], 2), dtype='f8')\n        # vec[:,0] = coordinates.Longitude(coords.l, wrap_angle=360.*units.deg).deg[:]\n        # vec[:,1] = coords.b.deg[:]\n        # return np.radians(vec)\n\n        c = coords.transform_to(self._frame).represent_as('cartesian')\n        vec_norm = np.sqrt(c.x**2 + c.y**2 + c.z**2)\n\n        vec = np.empty((c.shape[0], 3), dtype=c.x.dtype)\n        vec[:,0] = (c.x / vec_norm).value[:]\n        vec[:,1] = (c.y / vec_norm).value[:]\n        vec[:,2] = (c.z / vec_norm).value[:]\n\n        return vec", "response": "Converts from sky coordinates to unit vectors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert from sky coordinates to pixel indices.", "response": "def _coords2idx(self, coords):\n        \"\"\"\n        Converts from sky coordinates to pixel indices.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): Sky coordinates.\n\n        Returns:\n            Pixel indices of the coordinates, with the same shape as the input\n            coordinates. Pixels which are outside the map are given an index\n            equal to the number of pixels in the map.\n        \"\"\"\n\n        x = self._coords2vec(coords)\n        idx = self._kd.query(x, p=self._metric_p,\n                             distance_upper_bound=self._max_pix_scale)\n        return idx[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the Marshall et al. 2006 map from \\. dat. gz to \\. hdf5.", "response": "def dat2hdf5(table_dir):\n    \"\"\"\n    Convert the Marshall et al. (2006) map from \\*.dat.gz to \\*.hdf5.\n    \"\"\"\n\n    import astropy.io.ascii as ascii\n    import gzip\n    from contextlib import closing\n\n    readme_fname = os.path.join(table_dir, 'ReadMe')\n    table_fname = os.path.join(table_dir, 'table1.dat.gz')\n    h5_fname = os.path.join(table_dir, 'marshall.h5')\n\n    # Extract the gzipped table\n    with gzip.open(table_fname, 'rb') as f:\n        # Read in the table using astropy's CDS table reader\n        r = ascii.get_reader(ascii.Cds, readme=readme_fname)\n        r.data.table_name = 'table1.dat' # Hack to deal with bug in CDS reader.\n        table = r.read(f)\n        print(table)\n\n    # Reorder table entries according to Galactic (l, b)\n    l = coordinates.Longitude(\n        table['GLON'][:],\n        wrap_angle=180.*units.deg)\n    b = table['GLAT'][:]\n\n    sort_idx = np.lexsort((b, l))\n\n    l = l[sort_idx].astype('f4')\n    b = b[sort_idx].astype('f4')\n    l.shape = (801, 81)\n    b.shape = (801, 81)\n\n    # Extract arrays from the table\n    chi2_all = np.reshape((table['x2all'][sort_idx]).astype('f4'), (801,81))\n    chi2_giants = np.reshape((table['x2gts'][sort_idx]).astype('f4'), (801,81))\n\n    A = np.empty((801*81,33), dtype='f4')\n    sigma_A = np.empty((801*81,33), dtype='f4')\n    dist = np.empty((801*81,33), dtype='f4')\n    sigma_dist = np.empty((801*81,33), dtype='f4')\n\n    for k in range(33):\n        A[:,k] = table['ext{:d}'.format(k+1)][sort_idx]\n        sigma_A[:,k] = table['e_ext{:d}'.format(k+1)][sort_idx]\n        dist[:,k] = table['r{:d}'.format(k+1)][sort_idx]\n        sigma_dist[:,k] = table['e_r{:d}'.format(k+1)][sort_idx]\n\n    A.shape = (801,81,33)\n    sigma_A.shape = (801,81,33)\n    dist.shape = (801,81,33)\n    sigma_dist.shape = (801,81,33)\n\n    # Construct the HDF5 file\n    h5_fname = os.path.join(table_dir, 'marshall.h5')\n    filter_kwargs = dict(\n        chunks=True,\n        compression='gzip',\n        compression_opts=3,\n        # scaleoffset=4\n    )\n\n    with h5py.File(h5_fname, 'w') as f:\n        dset = f.create_dataset('A', data=A, **filter_kwargs)\n        dset.attrs['description'] = 'Extinction of each bin'\n        dset.attrs['band'] = 'Ks (2MASS)'\n        dset.attrs['units'] = 'mag'\n\n        dset = f.create_dataset('sigma_A', data=sigma_A, **filter_kwargs)\n        dset.attrs['description'] = 'Extinction uncertainty of each bin'\n        dset.attrs['band'] = 'Ks (2MASS)'\n        dset.attrs['units'] = 'mag'\n\n        dset = f.create_dataset('dist', data=dist, **filter_kwargs)\n        dset.attrs['description'] = 'Distance of each bin'\n        dset.attrs['units'] = 'kpc'\n\n        dset = f.create_dataset('sigma_dist', data=sigma_dist, **filter_kwargs)\n        dset.attrs['description'] = 'Distance uncertainty of each bin'\n        dset.attrs['units'] = 'kpc'\n\n        dset = f.create_dataset('chi2_all', data=chi2_all, **filter_kwargs)\n        dset.attrs['description'] = 'Chi^2, based on all the stars'\n        dset.attrs['units'] = 'unitless'\n\n        dset = f.create_dataset('chi2_giants', data=chi2_giants, **filter_kwargs)\n        dset.attrs['description'] = 'Chi^2, based on giants only'\n        dset.attrs['units'] = 'unitless'\n\n        # filter_kwargs.pop('scaleoffset')\n\n        dset = f.create_dataset('l', data=l, **filter_kwargs)\n        dset.attrs['description'] = 'Galactic longitude'\n        dset.attrs['units'] = 'deg'\n\n        dset = f.create_dataset('b', data=b, **filter_kwargs)\n        dset.attrs['description'] = 'Galactic latitude'\n        dset.attrs['units'] = 'deg'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading the ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO ISO.", "response": "def fetch(clobber=False):\n    \"\"\"\n    Downloads the Marshall et al. (2006) dust map, which is based on 2MASS\n    stellar photometry.\n\n    Args:\n        clobber (Optional[:obj:`bool`]): If ``True``, any existing file will be\n            overwritten, even if it appears to match. If ``False`` (the\n            default), :obj:`fetch()` will attempt to determine if the dataset\n            already exists. This determination is not 100\\% robust against data\n            corruption.\n    \"\"\"\n\n    table_dir = os.path.join(data_dir(), 'marshall')\n\n    # Check if file already exists\n    if not clobber:\n        h5_fname = os.path.join(table_dir, 'marshall.h5')\n        h5_size = 5033290 # Guess, in Bytes\n        h5_dsets = {\n            'l': (801, 81),\n            'b': (801, 81),\n            'chi2_all': (801, 81),\n            'chi2_giants': (801, 81),\n            'A': (801, 81, 33),\n            'sigma_A': (801, 81, 33),\n            'dist': (801, 81, 33),\n            'sigma_dist': (801, 81, 33)\n        }\n        if fetch_utils.h5_file_exists(h5_fname, h5_size, dsets=h5_dsets):\n            print('File appears to exist already. Call ``fetch(clobber=True)`` '\n                  'to force overwriting of existing file.')\n            return\n\n    # Download the ASCII table\n    url = 'ftp://cdsarc.u-strasbg.fr/pub/cats/J/A%2BA/453/635/table1.dat.gz'\n    md5 = '637b95b025517a8b9757b6465b632285'\n    table_fname = os.path.join(table_dir, 'table1.dat.gz')\n    fetch_utils.download_and_verify(url, md5, fname=table_fname)\n\n    # Download the README\n    url = 'ftp://cdsarc.u-strasbg.fr/pub/cats/J/A%2BA/453/635/ReadMe'\n    md5 = '3b7c1296b181b3d77106ab50193dc7ee'\n    readme_fname = os.path.join(table_dir, 'ReadMe')\n    fetch_utils.download_and_verify(url, md5, fname=readme_fname)\n\n    # Convert from ASCII table to HDF5\n    dat2hdf5(table_dir)\n\n    # Cleanup\n    print('Cleaning up ...')\n    os.remove(table_fname)\n    os.remove(readme_fname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _gal2idx(self, gal):\n\n        # Make sure that l is in domain [-180 deg, 180 deg)\n        l = coordinates.Longitude(gal.l, wrap_angle=180.*units.deg)\n\n        j = (self._inv_pix_scale * (l.deg - self._l_bounds[0])).astype('i4')\n        k = (self._inv_pix_scale * (gal.b.deg - self._b_bounds[0])).astype('i4')\n\n        idx = (j < 0) | (j >= self._shape[0]) | (k < 0) | (k >= self._shape[1])\n\n        if np.any(idx):\n            j[idx] = -1\n            k[idx] = -1\n\n        return j, k, ~idx", "response": "Convert from Galactic coordinates to pixel indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self, coords, return_sigma=False):\n\n        # Ensure that distance has been requested\n        has_dist = hasattr(coords.distance, 'kpc')\n        if not has_dist:\n            raise ValueError('Input `coords` must specify distance.')\n\n        # Convert coordinates to pixel indices\n        j, k, mask_idx = self._gal2idx(coords)\n\n        # Which distances to extract\n        d = coords.distance.kpc\n        dist_idx_ceil = np.sum(d[:,None] > self._dist[j, k, :], axis=1)\n\n        # Initialize return arrays\n        A_ret = np.full(coords.shape, np.nan, dtype='f4')\n        if return_sigma:\n            sigma_ret = np.full(coords.shape, np.nan, dtype='f4')\n\n        # d < d(nearest distance slice)\n        idx_near = (dist_idx_ceil == 0) & mask_idx\n\n        if np.any(idx_near):\n            a = d[idx_near] / self._dist[j[idx_near], k[idx_near], 0]\n            A_ret[idx_near] = a * self._A[j[idx_near], k[idx_near], 0]\n\n            if return_sigma:\n                sigma_ret[idx_near] = (\n                    self._sigma_A[\n                        j[idx_near],\n                        k[idx_near],\n                        0\n                    ])\n\n        # d > d(farthest distance slice)\n        idx_far = (dist_idx_ceil == self._n_dists[j,k]) & mask_idx\n\n        if np.any(idx_far):\n            A_ret[idx_far] = (\n                self._A[\n                    j[idx_far],\n                    k[idx_far],\n                    self._n_dists[j[idx_far],k[idx_far]]-1\n                ])\n\n            if return_sigma:\n                sigma_ret[idx_far] = (\n                    self._sigma_A[\n                        j[idx_far],\n                        k[idx_far],\n                        self._n_dists[j[idx_far],k[idx_far]]-1\n                    ])\n\n        # d(nearest distance slice) < d < d(farthest distance slice)\n        idx_btw = (~idx_near & ~idx_far) & mask_idx\n\n        if np.any(idx_btw):\n            d_ceil = (\n                self._dist[\n                    j[idx_btw],\n                    k[idx_btw],\n                    dist_idx_ceil[idx_btw]\n                ])\n            d_floor = (\n                self._dist[\n                    j[idx_btw],\n                    k[idx_btw],\n                    dist_idx_ceil[idx_btw]-1\n                ])\n            a = (d_ceil - d[idx_btw]) / (d_ceil - d_floor)\n\n            A_ret[idx_btw] = (\n                (1.-a) * self._A[j[idx_btw], k[idx_btw], dist_idx_ceil[idx_btw]]\n                + a * self._A[j[idx_btw], k[idx_btw], dist_idx_ceil[idx_btw]-1]\n            )\n\n            if return_sigma:\n                w0 = (1.-a)**2\n                w1 = a**2\n                norm = 1. / (w0 + w1)\n                w0 *= norm\n                w1 *= norm\n                sigma_ret[idx_btw] = np.sqrt(\n                    w0 * self._sigma_A[j[idx_btw], k[idx_btw], dist_idx_ceil[idx_btw]]**2\n                    + w1 * self._sigma_A[j[idx_btw], k[idx_btw], dist_idx_ceil[idx_btw]-1]**2\n                )\n\n        if return_sigma:\n            return A_ret, sigma_ret\n\n        return A_ret", "response": "Query the 2MASS Ks - band extinction at the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a block hash to the list of block hashes for which to get headers.", "response": "def add_block_hash( self, block_hash ):\n        \"\"\"\n        Append up to 2000 block hashes for which to get headers.\n        \"\"\"\n        if len(self.block_hashes) > 2000:\n            raise Exception(\"A getheaders request cannot have over 2000 block hashes\")\n\n        hash_num = int(\"0x\" + block_hash, 16)\n        \n        bh = BlockHash()\n        bh.block_hash = hash_num\n\n        self.block_hashes.append( bh )\n        self.hash_stop = hash_num"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninteract with the blockchain peer and loop until we get a socket error.", "response": "def run( self ):\n        \"\"\"\n        Interact with the blockchain peer,\n        until we get a socket error or we\n        exit the loop explicitly.\n        Return True on success\n        Raise on error\n        \"\"\"\n\n        self.handshake()\n\n        try:\n            self.loop()\n        except socket.error, se:\n            if self.finished:\n                return True\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_headers( self, message_header, block_headers_message ):\n        log.debug(\"handle headers (%s)\" % len(block_headers_message.headers))\n\n        block_headers = block_headers_message.headers\n\n        serializer = BlockHeaderSerializer()\n\n        # verify that the local header chain connects to this sequence\n        current_height = SPVClient.height( self.path )\n        if current_height is None:\n            assert USE_TESTNET\n            current_height = -1\n\n        assert (current_height >= 0 and USE_MAINNET) or USE_TESTNET, \"Invalid height %s\" % current_height\n    \n        last_header = None\n\n        if current_height >= 0:\n            last_header = SPVClient.read_header( self.path, current_height )\n            log.debug(\"Receive %s headers (%s to %s)\" % (len(block_headers), current_height, current_height + len(block_headers)))\n\n        else:\n            # first testnet header\n            log.debug(\"Receive %s testnet headers (%s to %s)\" % (len(block_headers), current_height + 1, current_height + len(block_headers)))\n            last_header = {\n                \"version\": block_headers[0].version,\n                \"prev_block_hash\": \"%064x\" % block_headers[0].prev_block,\n                \"merkle_root\": \"%064x\" % block_headers[0].merkle_root,\n                \"timestamp\": block_headers[0].timestamp,\n                \"bits\": block_headers[0].bits,\n                \"nonce\": block_headers[0].nonce,\n                \"hash\": block_headers[0].calculate_hash()\n            }\n\n        if (USE_MAINNET or (USE_TESTNET and current_height >= 0)) and last_header['hash'] != self.hash_to_string(block_headers[0].prev_block):\n            raise Exception(\"Received discontinuous block header at height %s: hash '%s' (expected '%s')\" % \\\n                    (current_height,\n                    self.hash_to_string(block_headers[0].prev_block),\n                    last_header['hash'] ))\n\n        header_start = 1\n        if USE_TESTNET and current_height < 0:\n            # save initial header\n            header_start = 0\n\n        # verify that this sequence of headers constitutes a hash chain \n        for i in xrange(header_start, len(block_headers)):\n            prev_block_hash = self.hash_to_string(block_headers[i].prev_block)\n            if i > 0 and prev_block_hash != block_headers[i-1].calculate_hash():\n                raise Exception(\"Block '%s' is not continuous with block '%s'\" % \\\n                        prev_block_hash,\n                        block_headers[i-1].calculate_hash())\n\n        if current_height < 0:\n            # save the first header \n            if not os.path.exists(self.path):\n                with open(self.path, \"wb\") as f:\n                    block_header_serializer = BlockHeaderSerializer()\n                    bin_data = block_header_serializer.serialize( block_headers[0] )\n                    f.write( bin_data )\n\n            # got all headers, including the first\n            current_height = 0\n\n        # insert into to local headers database\n        next_block_id = current_height + 1\n        for block_header in block_headers:\n            with open(self.path, \"rb+\") as f:\n\n                # omit tx count \n                block_header.txns_count = 0\n                bin_data = serializer.serialize( block_header )\n\n                if len(bin_data) != BLOCK_HEADER_SIZE:\n                    raise Exception(\"Block %s (%s) has %s-byte header\" % (next_block_id, block_header.calculate_hash(), len(bin_data)))\n\n                # NOTE: the fact that we use seek + write ensures that we can:\n                # * restart synchronizing at any point\n                # * allow multiple processes to work on the chain safely (even if they're duplicating effort)\n                f.seek( BLOCK_HEADER_SIZE * next_block_id, os.SEEK_SET )\n                f.write( bin_data )\n\n                if SPVClient.height( self.path ) >= self.last_block_id - 1:\n                    break\n\n                next_block_id += 1\n            \n        current_block_id = SPVClient.height( self.path )\n        if current_block_id >= self.last_block_id - 1:\n            # got all headers\n            self.loop_exit()\n            return\n\n        prev_block_header = SPVClient.read_header( self.path, current_block_id )\n        prev_block_hash = prev_block_header['hash']\n        self.send_getheaders( prev_block_hash )", "response": "Handle a headers message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a getheaders message to the server.", "response": "def send_getheaders( self, prev_block_hash ):\n        \"\"\"\n        Request block headers from a particular block hash.\n        Will receive up to 2000 blocks, starting with the block *after*\n        the given block hash (prev_block_hash)\n        \"\"\"\n\n        getheaders = GetHeaders()\n\n        getheaders.add_block_hash( prev_block_hash )\n\n        log.debug(\"send getheaders\")\n        self.send_message( getheaders )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init(cls, path):\n        if not os.path.exists( path ):\n\n            block_header_serializer = BlockHeaderSerializer()\n            genesis_block_header = BlockHeader()\n\n            if USE_MAINNET:\n                # we know the mainnet block header\n                # but we don't know the testnet/regtest block header\n                genesis_block_header.version = 1\n                genesis_block_header.prev_block = 0\n                genesis_block_header.merkle_root = int(GENESIS_BLOCK_MERKLE_ROOT, 16 )\n                genesis_block_header.timestamp = 1231006505\n                genesis_block_header.bits = int( \"1d00ffff\", 16 )\n                genesis_block_header.nonce = 2083236893\n                genesis_block_header.txns_count = 0\n\n                with open(path, \"wb\") as f:\n                    bin_data = block_header_serializer.serialize( genesis_block_header )\n                    f.write( bin_data )", "response": "Initialize an SPV client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the locally - stored block height of a file.", "response": "def height(cls, path):\n        \"\"\"\n        Get the locally-stored block height\n        \"\"\"\n        if os.path.exists( path ):\n            sb = os.stat( path )\n            h = (sb.st_size / BLOCK_HEADER_SIZE) - 1\n            return h\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_header_at( cls, f):\n        header_parser = BlockHeaderSerializer()\n        hdr = header_parser.deserialize( f )\n        h = {}\n        h['version'] = hdr.version\n        h['prev_block_hash'] = \"%064x\" % hdr.prev_block\n        h['merkle_root'] = \"%064x\" % hdr.merkle_root\n        h['timestamp'] = hdr.timestamp\n        h['bits'] = hdr.bits\n        h['nonce'] = hdr.nonce\n        h['hash'] = hdr.calculate_hash()\n        return h", "response": "Reads a block header at the given file - like object and returns it as a dict containing the keys version merkle_root timestamp bits nonce and hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the header chain from disk.", "response": "def load_header_chain( cls, chain_path ):\n        \"\"\"\n        Load the header chain from disk.\n        Each chain element will be a dictionary with:\n        * \n        \"\"\"\n\n        header_parser = BlockHeaderSerializer()\n        chain = []\n        height = 0\n        with open(chain_path, \"rb\") as f:\n\n            h = SPVClient.read_header_at( f )\n            h['block_height'] = height \n\n            height += 1\n            chain.append(h)\n\n        return chain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a block header at a particular height from disk.", "response": "def read_header(cls, headers_path, block_height, allow_none=False):\n        \"\"\"\n        Get a block header at a particular height from disk.\n        Return the header if found\n        Return None if not.\n        \"\"\"\n        if os.path.exists(headers_path):\n    \n            header_parser = BlockHeaderSerializer()\n            sb = os.stat( headers_path )\n            if sb.st_size < BLOCK_HEADER_SIZE * block_height:\n                # beyond EOF \n                if allow_none:\n                    return None \n                else:\n                    raise Exception('EOF on block headers')\n\n            with open( headers_path, \"rb\" ) as f:\n                f.seek( block_height * BLOCK_HEADER_SIZE, os.SEEK_SET )\n                hdr = SPVClient.read_header_at( f )\n\n            return hdr\n        else:\n            if allow_none:\n                return None\n            else:\n                raise Exception('No such file or directory: {}'.format(headers_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_target(cls, path, index, chain=None):\n        if chain is None:\n            chain = []  # Do not use mutables as default values!\n\n        max_target = 0x00000000FFFF0000000000000000000000000000000000000000000000000000\n        if index == 0:\n            return 0x1d00ffff, max_target\n\n        first = SPVClient.read_header( path, (index-1)*BLOCK_DIFFICULTY_CHUNK_SIZE)\n        last = SPVClient.read_header( path, index*BLOCK_DIFFICULTY_CHUNK_SIZE - 1, allow_none=True)\n        if last is None:\n            for h in chain:\n                if h.get('block_height') == index*BLOCK_DIFFICULTY_CHUNK_SIZE - 1:\n                    last = h\n\n        nActualTimespan = last.get('timestamp') - first.get('timestamp')\n        nTargetTimespan = BLOCK_DIFFICULTY_INTERVAL\n        nActualTimespan = max(nActualTimespan, nTargetTimespan/4)\n        nActualTimespan = min(nActualTimespan, nTargetTimespan*4)\n\n        bits = last.get('bits')\n        # convert to bignum\n        MM = 256*256*256\n        a = bits%MM\n        if a < 0x8000:\n            a *= 256\n        target = (a) * pow(2, 8 * (bits/MM - 3))\n\n        # new target\n        new_target = min( max_target, (target * nActualTimespan)/nTargetTimespan )\n\n        # convert it to bits\n        c = (\"%064X\"%new_target)[2:]\n        i = 31\n        while c[0:2]==\"00\":\n            c = c[2:]\n            i -= 1\n\n        c = int('0x'+c[0:6],16)\n        if c >= 0x800000:\n            c /= 256\n            i += 1\n\n        new_bits = c + MM * i\n        return new_bits, new_target", "response": "Calculate the target difficulty at a particular difficulty interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the block s hash and block_data are valid.", "response": "def block_header_verify( cls, headers_path, block_id, block_hash, block_header ):\n        \"\"\"\n        Given the block's numeric ID, its hash, and the bitcoind-returned block_data,\n        use the SPV header chain to verify the block's integrity.\n\n        block_header must be a dict with the following structure:\n        * version: protocol version (int)\n        * prevhash: previous block hash (hex str)\n        * merkleroot: block Merkle root (hex str)\n        * timestamp: UNIX time stamp (int)\n        * bits: difficulty bits (hex str)\n        * nonce: PoW nonce (int)\n        * hash: block hash (hex str)\n        (i.e. the format that the reference bitcoind returns via JSON RPC)\n\n        Return True on success\n        Return False on error\n        \"\"\"\n        prev_header = cls.read_header( headers_path, block_id - 1 )\n        prev_hash = prev_header['hash']\n        return bits.block_header_verify( block_header, prev_hash, block_hash )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the block s verification header and transaction IDs are legit.", "response": "def block_verify( cls, verified_block_header, block_txids ):\n        \"\"\"\n        Given the block's verified header structure (see block_header_verify) and\n        its list of transaction IDs (as hex strings), verify that the transaction IDs are legit.\n\n        Return True on success\n        Return False on error.\n        \"\"\"\n\n        block_data = {\n            'merkleroot': verified_block_header['merkleroot'],\n            'tx': block_txids\n        }\n\n        return bits.block_verify( block_data )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tx_hash( cls, tx ):\n        tx_hex = bits.btc_bitcoind_tx_serialize( tx )\n        tx_hash = hashing.bin_double_sha256(tx_hex.decode('hex'))[::-1].encode('hex')\n        return tx_hash", "response": "Calculate the hash of a transction structure given by bitcoind\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tx_verify( cls, verified_block_txids, tx ):\n        tx_hash = cls.tx_hash( tx )\n        return tx_hash in verified_block_txids", "response": "Verify that a transaction is legit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tx_index( cls, verified_block_txids, verified_tx ):\n        tx_hash = cls.tx_hash( verified_tx )\n        return verified_block_txids.index( tx_hash )", "response": "Given a list of verified block txids and a verified transaction find out where it is in the list of txids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef block_header_index( cls, path, block_header ):\n        with open( path, \"rb\" ) as f:\n            chain_raw = f.read()\n\n        for blk in xrange(0, len(chain_raw) / (BLOCK_HEADER_SIZE)):\n            if chain_raw[blk * BLOCK_HEADER_SIZE : blk * BLOCK_HEADER_SIZE + BLOCK_HEADER_SIZE] == block_header:\n                return blk\n\n        return -1", "response": "Given a path and a block s serialized header return the index of the block header in the chain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_header_chain(cls, path, chain=None):\n        if chain is None:\n            chain = SPVClient.load_header_chain( path )\n\n        prev_header = chain[0]\n        \n        for i in xrange(1, len(chain)):\n            header = chain[i]\n            height = header.get('block_height')\n            prev_hash = prev_header.get('hash')\n            if prev_hash != header.get('prev_block_hash'):\n                log.error(\"prev hash mismatch: %s vs %s\" % (prev_hash, header.get('prev_block_hash')))\n                return False\n\n            bits, target = SPVClient.get_target( path, height/BLOCK_DIFFICULTY_CHUNK_SIZE, chain)\n            if bits != header.get('bits'):\n                log.error(\"bits mismatch: %s vs %s\" % (bits, header.get('bits')))\n                return False\n\n            _hash = header.get('hash')\n            if int('0x'+_hash, 16) > target:\n                log.error(\"insufficient proof of work: %s vs target %s\" % (int('0x'+_hash, 16), target))\n                return False\n\n            prev_header = header\n\n        return True", "response": "Verify that a given chain of block headers has sufficient proof of work."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsynchronizing our local block headers up to the last block ID given.", "response": "def sync_header_chain(cls, path, bitcoind_server, last_block_id ):\n        \"\"\"\n        Synchronize our local block headers up to the last block ID given.\n        @last_block_id is *inclusive*\n        @bitcoind_server is host:port or just host\n        \"\"\"\n        current_block_id = SPVClient.height( path )\n        if current_block_id is None:\n            assert USE_TESTNET\n            current_block_id = -1\n\n        assert (current_block_id >= 0 and USE_MAINNET) or USE_TESTNET\n\n        if current_block_id < last_block_id:\n          \n            if USE_MAINNET:\n                log.debug(\"Synchronize %s to %s\" % (current_block_id, last_block_id))\n            else:\n                log.debug(\"Synchronize testnet %s to %s\" % (current_block_id + 1, last_block_id ))\n\n            # need to sync\n            if current_block_id >= 0:\n                prev_block_header = SPVClient.read_header( path, current_block_id )\n                prev_block_hash = prev_block_header['hash']\n\n            else:\n                # can only happen when in testnet\n                prev_block_hash = GENESIS_BLOCK_HASH_TESTNET\n\n            # connect \n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n            # timeout (10 min)\n            sock.settimeout(600)\n\n            bitcoind_port = 8333\n            if \":\" in bitcoind_server:\n                p = bitcoind_server.split(\":\")\n                bitcoind_server = p[0]\n                bitcoind_port = int(p[1])\n\n            log.debug(\"connect to %s:%s\" % (bitcoind_server, bitcoind_port))\n            sock.connect( (bitcoind_server, bitcoind_port) )\n\n            client = BlockHeaderClient( sock, path, prev_block_hash, last_block_id )\n\n            # get headers\n            client.run()\n\n            # verify headers\n            if SPVClient.height(path) < last_block_id:\n                raise Exception(\"Did not receive all headers up to %s (only got %s)\" % (last_block_id, SPVClient.height(path)))\n\n            # defensive: make sure it's *exactly* that many blocks \n\n            rc = SPVClient.verify_header_chain( path )\n            if not rc:\n                raise Exception(\"Failed to verify headers (stored in '%s')\" % path)\n\n        log.debug(\"synced headers from %s to %s in %s\" % (current_block_id, last_block_id, path))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_email_message(cls, message, local_id=None):\n        '''\n        Convert an :class:`email.message.Message` or compatible message\n        object into a CERP XML :class:`eulxml.xmlmap.cerp.Message`. If an\n        id is specified, it will be stored in the Message <LocalId>.\n\n        :param message: `email.message.Message` object\n        :param id: optional message id to be set as `local_id`\n\n        :returns: :class:`eulxml.xmlmap.cerp.Message` instance populated\n    \t    with message information\n\n        '''\n        result = cls()\n        if local_id is not None:\n            result.local_id = id\n\n        message_id = message.get('Message-Id')\n        if message_id:\n            result.message_id_supplied = True\n            result.message_id = message_id\n\n        result.mime_version = message.get('MIME-Version')\n\n        dates = message.get_all('Date', [])\n        result.orig_date_list.extend([parse_mail_date(d) for d in dates])\n\n        result.from_list.extend(message.get_all('From', []))\n        result.sender_list.extend(message.get_all('From', []))\n        try:\n            result.to_list.extend(message.get_all('To', []))\n        except UnicodeError:\n            print(repr(message['To']))\n            raise\n        result.cc_list.extend(message.get_all('Cc', []))\n        result.bcc_list.extend(message.get_all('Bcc', []))\n        result.in_reply_to_list.extend(message.get_all('In-Reply-To', []))\n        result.references_list.extend(message.get_all('References', []))\n        result.subject_list.extend(message.get_all('Subject', []))\n        result.comments_list.extend(message.get_all('Comments', []))\n        result.keywords_list.extend(message.get_all('Keywords', []))\n\n        headers = [ Header(name=key, value=val) for key, val in message.items() ]\n        result.headers.extend(headers)\n\n        # FIXME: skip multipart messages for now\n        if not message.is_multipart():\n            result.create_single_body()\n\n            # FIXME: this is a small subset of the actual elements CERP allows.\n            # we should add the rest of them, too.\n\n            # message.get_content_type() always returns something. only\n            # put it in the CERP if a Content-Type was explicitly specified.\n            if message['Content-Type']:\n                result.single_body.content_type_list.append(message.get_content_type())\n            if message.get_content_charset():\n                result.single_body.charset_list.append(message.get_content_charset())\n            if message.get_filename():\n                result.single_body.content_name_list.append(message.get_filename())\n\n            # FIXME: attaching the body_content only makes sense for text\n            # content types. we'll eventually need a better solution for\n            # non-text messages\n            result.single_body.create_body_content()\n            payload = message.get_payload(decode=False)\n\n            # if not unicode, attempt to convert\n            if isinstance(payload, six.binary_type):\n                charset = message.get_charset()\n                # decode according to the specified character set, if any\n                if charset is not None:\n                    charset_decoder = codecs.getdecoder(str(charset))\n                    payload, length = charset_decoder(payload)\n\n                # otherwise, just try to convert\n                else:\n                    payload = u(payload)\n\n            # remove any control characters not allowed in XML\n            control_char_map = dict.fromkeys(range(32))\n            for i in [9, 10, 13]: # preserve horizontal tab, line feed, carriage return\n                del control_char_map[i]\n\n            payload = u(payload).translate(control_char_map)\n\n            result.single_body.body_content.content = payload\n\n        else:\n            # TODO: handle multipart\n            logger.warn('CERP conversion does not yet handle multipart')\n\n        # assume we've normalized newlines:\n        result.eol = EOLMAP[os.linesep]\n\n        return result", "response": "Convert an email. message. Message object into a CERP XML Message object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the IEEE 754 - 2008 context for a given bit width.", "response": "def IEEEContext(bitwidth):\n    \"\"\"\n    Return IEEE 754-2008 context for a given bit width.\n\n    The IEEE 754 standard specifies binary interchange formats with bitwidths\n    16, 32, 64, 128, and all multiples of 32 greater than 128.  This function\n    returns the context corresponding to the interchange format for the given\n    bitwidth.\n\n    See section 3.6 of IEEE 754-2008 or the bigfloat source for more details.\n\n    \"\"\"\n    try:\n        precision = {16: 11, 32: 24, 64: 53, 128: 113}[bitwidth]\n    except KeyError:\n        if not (bitwidth >= 128 and bitwidth % 32 == 0):\n            raise ValueError(\"nonstandard bitwidth: bitwidth should be \"\n                             \"16, 32, 64, 128, or k*32 for some k >= 4\")\n        # The formula for the precision involves rounding 4*log2(width) to the\n        # nearest integer. We have:\n        #\n        #   round(4*log2(width)) == round(log2(width**8)/2)\n        #                        == floor((log2(width**8) + 1)/2)\n        #                        == (width**8).bit_length() // 2\n        #\n        # (Note that 8*log2(width) can never be an odd integer, so we\n        # don't care which way half-way cases round in the 'round'\n        # operation.)\n        precision = bitwidth - _bit_length(bitwidth ** 8) // 2 + 13\n\n    emax = 1 << bitwidth - precision - 1\n    return Context(\n        precision=precision,\n        emin=4 - emax - precision,\n        emax=emax,\n        subnormalize=True,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef view(molecule, viewer=settings['defaults']['viewer'], use_curr_dir=False):\n    try:\n        molecule.view(viewer=viewer, use_curr_dir=use_curr_dir)\n    except AttributeError:\n        if pd.api.types.is_list_like(molecule):\n            cartesian_list = molecule\n        else:\n            raise ValueError('Argument is neither list nor Cartesian.')\n        if use_curr_dir:\n            TEMP_DIR = os.path.curdir\n        else:\n            TEMP_DIR = tempfile.gettempdir()\n\n        def give_filename(i):\n            filename = 'ChemCoord_list_' + str(i) + '.molden'\n            return os.path.join(TEMP_DIR, filename)\n\n        i = 1\n        while os.path.exists(give_filename(i)):\n            i = i + 1\n\n        to_molden(cartesian_list, buf=give_filename(i))\n\n        def open_file(i):\n            \"\"\"Open file and close after being finished.\"\"\"\n            try:\n                subprocess.check_call([viewer, give_filename(i)])\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                raise\n            finally:\n                if use_curr_dir:\n                    pass\n                else:\n                    os.remove(give_filename(i))\n        Thread(target=open_file, args=(i,)).start()", "response": "View a single molecule or list of molecules."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_molden(cartesian_list, buf=None, sort_index=True,\n              overwrite=True, float_format='{:.6f}'.format):\n    \"\"\"Write a list of Cartesians into a molden file.\n\n    .. note:: Since it permamently writes a file, this function\n        is strictly speaking **not sideeffect free**.\n        The list to be written is of course not changed.\n\n    Args:\n        cartesian_list (list):\n        buf (str): StringIO-like, optional buffer to write to\n        sort_index (bool): If sort_index is true, the Cartesian\n            is sorted by the index before writing.\n        overwrite (bool): May overwrite existing files.\n        float_format (one-parameter function): Formatter function\n            to apply to column\u2019s elements if they are floats.\n            The result of this function must be a unicode string.\n\n    Returns:\n        formatted : string (or unicode, depending on data and options)\n    \"\"\"\n    if sort_index:\n        cartesian_list = [molecule.sort_index() for molecule in cartesian_list]\n\n    give_header = (\"[MOLDEN FORMAT]\\n\"\n                   + \"[N_GEO]\\n\"\n                   + str(len(cartesian_list)) + \"\\n\"\n                   + '[GEOCONV]\\n'\n                   + 'energy\\n{energy}'\n                   + 'max-force\\n{max_force}'\n                   + 'rms-force\\n{rms_force}'\n                   + '[GEOMETRIES] (XYZ)\\n').format\n\n    values = len(cartesian_list) * '1\\n'\n    energy = [str(m.metadata.get('energy', 1)) for m in cartesian_list]\n    energy = '\\n'.join(energy) + '\\n'\n\n    header = give_header(energy=energy, max_force=values, rms_force=values)\n\n    coordinates = [x.to_xyz(sort_index=sort_index, float_format=float_format)\n                   for x in cartesian_list]\n    output = header + '\\n'.join(coordinates)\n\n    if buf is not None:\n        if overwrite:\n            with open(buf, mode='w') as f:\n                f.write(output)\n        else:\n            with open(buf, mode='x') as f:\n                f.write(output)\n    else:\n        return output", "response": "Write a list of Cartesians into a molden file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a molden file.", "response": "def read_molden(inputfile, start_index=0, get_bonds=True):\n    \"\"\"Read a molden file.\n\n    Args:\n        inputfile (str):\n        start_index (int):\n\n    Returns:\n        list: A list containing :class:`~chemcoord.Cartesian` is returned.\n    \"\"\"\n    from chemcoord.cartesian_coordinates.cartesian_class_main import Cartesian\n    with open(inputfile, 'r') as f:\n        found = False\n        while not found:\n            line = f.readline()\n            if '[N_GEO]' in line:\n                found = True\n                number_of_molecules = int(f.readline().strip())\n\n        energies = []\n        found = False\n        while not found:\n            line = f.readline()\n            if 'energy' in line:\n                found = True\n                for _ in range(number_of_molecules):\n                    energies.append(float(f.readline().strip()))\n\n        found = False\n        while not found:\n            line = f.readline()\n            if '[GEOMETRIES] (XYZ)' in line:\n                found = True\n                current_line = f.tell()\n                number_of_atoms = int(f.readline().strip())\n                f.seek(current_line)\n\n        cartesians = []\n        for energy in energies:\n            cartesian = Cartesian.read_xyz(\n                f, start_index=start_index, get_bonds=get_bonds,\n                nrows=number_of_atoms, engine='python')\n            cartesian.metadata['energy'] = energy\n            cartesians.append(cartesian)\n    return cartesians"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isclose(a, b, align=False, rtol=1.e-5, atol=1.e-8):\n    coords = ['x', 'y', 'z']\n    if not (set(a.index) == set(b.index)\n            and np.alltrue(a.loc[:, 'atom'] == b.loc[a.index, 'atom'])):\n        message = 'Can only compare molecules with the same atoms and labels'\n        raise ValueError(message)\n\n    if align:\n        a = a.get_inertia()['transformed_Cartesian']\n        b = b.get_inertia()['transformed_Cartesian']\n    A, B = a.loc[:, coords], b.loc[a.index, coords]\n    out = a._frame.copy()\n    out['atom'] = True\n    out.loc[:, coords] = np.isclose(A, B, rtol=rtol, atol=atol)\n    return out", "response": "Compare two set of molecules for numerical equality."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare two molecules for numerical equality.", "response": "def allclose(a, b, align=False, rtol=1.e-5, atol=1.e-8):\n    \"\"\"Compare two molecules for numerical equality.\n\n    Args:\n        a (Cartesian):\n        b (Cartesian):\n        align (bool): a and b are\n            prealigned along their principal axes of inertia and moved to their\n            barycenters before comparing.\n        rtol (float): Relative tolerance for the numerical equality comparison\n            look into :func:`numpy.allclose` for further explanation.\n        atol (float): Relative tolerance for the numerical equality comparison\n            look into :func:`numpy.allclose` for further explanation.\n\n    Returns:\n        bool:\n    \"\"\"\n    return np.alltrue(isclose(a, b, align=align, rtol=rtol, atol=atol))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new list of cartesians with the same order as the passed in cartesians.", "response": "def concat(cartesians, ignore_index=False, keys=None):\n    \"\"\"Join list of cartesians into one molecule.\n\n    Wrapper around the :func:`pandas.concat` function.\n    Default values are the same as in the pandas function except for\n    ``verify_integrity`` which is set to true in case of this library.\n\n    Args:\n        ignore_index (sequence, bool, int): If it is a boolean, it\n            behaves like in the description of\n            :meth:`pandas.DataFrame.append`.\n            If it is a sequence, it becomes the new index.\n            If it is an integer,\n            ``range(ignore_index, ignore_index + len(new))``\n            becomes the new index.\n        keys (sequence): If multiple levels passed, should contain tuples.\n            Construct hierarchical index using the passed keys as\n            the outermost level\n\n    Returns:\n        Cartesian:\n    \"\"\"\n    frames = [molecule._frame for molecule in cartesians]\n    new = pd.concat(frames, ignore_index=ignore_index, keys=keys,\n                    verify_integrity=True)\n\n    if type(ignore_index) is bool:\n        new = pd.concat(frames, ignore_index=ignore_index, keys=keys,\n                        verify_integrity=True)\n    else:\n        new = pd.concat(frames, ignore_index=True, keys=keys,\n                        verify_integrity=True)\n        if type(ignore_index) is int:\n            new.index = range(ignore_index,\n                              ignore_index + len(new))\n        else:\n            new.index = ignore_index\n    return cartesians[0].__class__(new)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rotation_matrix(axis, angle):\n    axis = normalize(np.array(axis))\n    if not (np.array([1, 1, 1]).shape) == (3, ):\n        raise ValueError('axis.shape has to be 3')\n    angle = float(angle)\n    return _jit_get_rotation_matrix(axis, angle)", "response": "Returns the rotation matrix around the given axis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _jit_get_rotation_matrix(axis, angle):\n    axis = _jit_normalize(axis)\n    a = m.cos(angle / 2)\n    b, c, d = axis * m.sin(angle / 2)\n    rot_matrix = np.empty((3, 3))\n    rot_matrix[0, 0] = a**2 + b**2 - c**2 - d**2\n    rot_matrix[0, 1] = 2. * (b * c - a * d)\n    rot_matrix[0, 2] = 2. * (b * d + a * c)\n    rot_matrix[1, 0] = 2. * (b * c + a * d)\n    rot_matrix[1, 1] = a**2 + c**2 - b**2 - d**2\n    rot_matrix[1, 2] = 2. * (c * d - a * b)\n    rot_matrix[2, 0] = 2. * (b * d - a * c)\n    rot_matrix[2, 1] = 2. * (c * d + a * b)\n    rot_matrix[2, 2] = a**2 + d**2 - b**2 - c**2\n    return rot_matrix", "response": "Returns the rotation matrix for the counterclockwise rotation\n    around the given axis and angle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef orthonormalize_righthanded(basis):\n    v1, v2 = basis[:, 0], basis[:, 1]\n    e1 = normalize(v1)\n    e3 = normalize(np.cross(e1, v2))\n    e2 = normalize(np.cross(e3, e1))\n    return np.array([e1, e2, e3]).T", "response": "Orthonormalizes righthandedly a given 3D basis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the optimal rotation matrix from P unto Q.", "response": "def get_kabsch_rotation(Q, P):\n    \"\"\"Calculate the optimal rotation from ``P`` unto ``Q``.\n\n    Using the Kabsch algorithm the optimal rotation matrix\n    for the rotation of ``other`` unto ``self`` is calculated.\n    The algorithm is described very well in\n    `wikipedia <http://en.wikipedia.org/wiki/Kabsch_algorithm>`_.\n\n    Args:\n        other (Cartesian):\n\n    Returns:\n        :class:`~numpy.array`: Rotation matrix\n    \"\"\"\n    # Naming of variables follows the wikipedia article:\n    # http://en.wikipedia.org/wiki/Kabsch_algorithm\n    A = np.dot(np.transpose(P), Q)\n    # One can't initialize an array over its transposed\n    V, S, W = np.linalg.svd(A)  # pylint:disable=unused-variable\n    W = W.T\n    d = np.linalg.det(np.dot(W, V.T))\n    return np.linalg.multi_dot((W, np.diag([1., 1., d]), V.T))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_grad_zmat_tensor(grad_C, construction_table, cart_dist):\n    if (construction_table.index != cart_dist.index).any():\n        message = \"construction_table and cart_dist must use the same index\"\n        raise ValueError(message)\n    X_dist = cart_dist.loc[:, ['x', 'y', 'z']].values.T\n    C_dist = np.tensordot(grad_C, X_dist, axes=([3, 2], [0, 1])).T\n    if C_dist.dtype == np.dtype('i8'):\n        C_dist = C_dist.astype('f8')\n    try:\n        C_dist[:, [1, 2]] = np.rad2deg(C_dist[:, [1, 2]])\n    except AttributeError:\n        C_dist[:, [1, 2]] = sympy.deg(C_dist[:, [1, 2]])\n\n    from chemcoord.internal_coordinates.zmat_class_main import Zmat\n    cols = ['atom', 'b', 'bond', 'a', 'angle', 'd', 'dihedral']\n    dtypes = ['O', 'i8', 'f8', 'i8', 'f8', 'i8', 'f8']\n    new = pd.DataFrame(data=np.zeros((len(construction_table), 7)),\n                       index=cart_dist.index, columns=cols, dtype='f8')\n    new = new.astype(dict(zip(cols, dtypes)))\n    new.loc[:, ['b', 'a', 'd']] = construction_table\n    new.loc[:, 'atom'] = cart_dist.loc[:, 'atom']\n    new.loc[:, ['bond', 'angle', 'dihedral']] = C_dist\n    return Zmat(new, _metadata={'last_valid_cartesian': cart_dist})", "response": "Applies the gradient for transformation to Zmatrix space onto cart_dist."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _remove_xml(xast, node, context, if_empty=False):\n    '''Remove a node or attribute.  For multipart XPaths that are\n    constructible by :mod:`eulxml.xmlmap`, the corresponding nodes\n    will be removed if they are empty (other than predicates specified\n    in the XPath).\n\n    :param xast: parsed xpath (xpath abstract syntax tree) from\n\t:mod:`eulxml.xpath`\n    :param node: lxml node relative to which xast should be removed\n    :param context: any context required for the xpath (e.g.,\n    \tnamespace definitions)\n    :param if_empty: optional boolean; only remove a node if it is\n\tempty (no attributes and no child nodes); defaults to False\n\n    :returns: True if something was deleted\n    '''\n    if isinstance(xast, ast.Step):\n        if isinstance(xast.node_test, ast.NameTest):\n            if xast.axis in (None, 'child'):\n                return _remove_child_node(node, context, xast, if_empty=if_empty)\n            elif xast.axis in ('@', 'attribute'):\n                return _remove_attribute_node(node, context, xast)\n        # special case for text()\n        # since it can't be removed, at least clear out any value in the text node\n        elif _is_text_nodetest(xast):\n            node.text = ''\n            return True\n\n    # If the xpath is a multi-step path (e.g., foo[@id=\"a\"]/bar[@id=\"b\"]/baz),\n    # remove the leaf node.  If the remaining portions of that path\n    # could have been constructed when setting the node and are empty\n    # (other than any predicates defined in the xpath), remove them as well.\n    elif isinstance(xast, ast.BinaryExpression):\n        if xast.op == '/':\n            left_xpath = serialize(xast.left)\n            left_node = _find_xml_node(left_xpath, node, context)\n            if left_node is not None:\n                # remove the last element in the xpath\n                removed = _remove_xml(xast.right, left_node, context,\n                                      if_empty=if_empty) # honor current if_empty flag\n\n                # If the left portion of the xpath is something we\n                # could have constructed, remove it if it is empty.\n                if removed and _predicate_is_constructible(left_xpath):\n                    _remove_xml(xast.left, node, context, if_empty=True)\n\n                # report on whether the leaf node was removed or not,\n                # regardless of what was done with left portion of the path\n                return removed\n\n    return False", "response": "Remove a node or attribute from the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remove_child_node(node, context, xast, if_empty=False):\n    '''Remove a child node based on the specified xpath.\n\n    :param node: lxml element relative to which the xpath will be\n    \tinterpreted\n    :param context: any context required for the xpath (e.g.,\n    \tnamespace definitions)\n    :param xast: parsed xpath (xpath abstract syntax tree) from\n\t:mod:`eulxml.xpath`\n    :param if_empty: optional boolean; only remove a node if it is\n\tempty (no attributes and no child nodes); defaults to False\n\n    :returns: True if a node was deleted\n    '''\n    xpath = serialize(xast)\n    child = _find_xml_node(xpath, node, context)\n    if child is not None:\n        # if if_empty was specified and node has children or attributes\n        # other than any predicates defined in the xpath, don't remove\n        if if_empty is True and \\\n               not _empty_except_predicates(xast, child, context):\n            return False\n        node.remove(child)\n        return True", "response": "Remove a child node based on the xpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remove_predicates(xast, node, context):\n    '''Remove any constructible predicates specified in the xpath\n    relative to the specified node.\n\n    :param xast: parsed xpath (xpath abstract syntax tree) from\n\t:mod:`eulxml.xpath`\n    :param node: lxml element which predicates will be removed from\n    :param context: any context required for the xpath (e.g.,\n    \tnamespace definitions)\n\n    :returns: updated a copy of the xast without the predicates that\n\twere successfully removed\n    '''\n    # work from a copy since it may be modified\n    xast_c = deepcopy(xast)\n    # check if predicates are constructable\n    for pred in list(xast_c.predicates):\n        # ignore predicates that we can't construct\n        if not _predicate_is_constructible(pred):\n            continue\n\n        if isinstance(pred, ast.BinaryExpression):\n            # TODO: support any other predicate operators?\n            # predicate construction supports op /\n\n            # If the xml still matches the constructed value, remove it.\n            # e.g., @type='text' or level='leaf'\n            if pred.op == '=' and \\\n                   node.xpath(serialize(pred), **context) is True:\n                # predicate xpath returns True if node=value\n\n                if isinstance(pred.left, ast.Step):\n                    if pred.left.axis in ('@', 'attribute'):\n                        if _remove_attribute_node(node, context, pred.left):\n                            # remove from the xast\n                            xast_c.predicates.remove(pred)\n                    elif pred.left.axis in (None, 'child'):\n                        if _remove_child_node(node, context, pred.left, if_empty=True):\n                            xast_c.predicates.remove(pred)\n\n                elif isinstance(pred.left, ast.BinaryExpression):\n                    # e.g., level/@id='b' or level/deep='deeper'\n                    # - value has already been checked by xpath above,\n                    # so just remove the multipart path\n                    _remove_xml(pred.left, node, context, if_empty=True)\n\n    return xast_c", "response": "Remove any constructible predicates specified in the xpath\n    relative to the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a node is empty except any predicates defined in the specified xpath.", "response": "def _empty_except_predicates(xast, node, context):\n    '''Check if a node is empty (no child nodes or attributes) except\n    for any predicates defined in the specified xpath.\n\n    :param xast: parsed xpath (xpath abstract syntax tree) from\n\t:mod:`eulxml.xpath`\n    :param node: lxml element to check\n    :param context: any context required for the xpath (e.g.,\n    \tnamespace definitions)\n\n    :returns: boolean indicating if the element is empty or not\n    '''\n    # copy the node, remove predicates, and check for any remaining\n    # child nodes or attributes\n    node_c = deepcopy(node)\n    _remove_predicates(xast, node_c, context)\n    return bool(len(node_c) == 0 and len(node_c.attrib) == 0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the item at the given position in the list and returns it. If no index is specified removes and returns the last item in the list. If no index is specified removes and returns the last item in the list. If no index is specified removes and returns the last item in the list.", "response": "def pop(self, i=None):\n        \"\"\"Remove the item at the given position in the list, and return it.\n        If no index is specified, removes and returns the last item in the list.\"\"\"\n        if i is None:\n            i = len(self) - 1\n        val = self[i]\n        del(self[i])\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting an item at a given position.", "response": "def insert(self, i, x):\n        \"\"\"Insert an item (x) at a given position (i).\"\"\"\n        if i == len(self):  # end of list or empty list: append\n            self.append(x)\n        elif len(self.matches) > i:\n            # create a new xml node at the requested position\n            insert_index = self.matches[i].getparent().index(self.matches[i])\n            _create_xml_node(self.xast, self.node, self.context, insert_index)\n            # then use default set logic\n            self[i] = x\n        else:\n            raise IndexError(\"Can't insert '%s' at index %d - list length is only %d\" \\\n                            % (x, i, len(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the requested type definition from the schema and return the corresponding Field object.", "response": "def get_field(self, schema):\n        \"\"\"Get the requested type definition from the schema and return the\n        appropriate :class:`~eulxml.xmlmap.fields.Field`.\n\n        :param schema: instance of :class:`eulxml.xmlmap.core.XsdSchema`\n        :rtype: :class:`eulxml.xmlmap.fields.Field`\n        \"\"\"\n        type = schema.get_type(self.schema_type)\n        logger.debug('Found schema type %s; base type %s, restricted values %s' % \\\n                     (self.schema_type, type.base_type(), type.restricted_values))\n        kwargs = {}\n        if type.restricted_values:\n            # field has a restriction with enumerated values - pass as choices to field\n            # - empty value at beginning of list for unset value; for required fields,\n            #   will force user to select a value, rather than first item being default\n            choices = []\n            choices.extend(type.restricted_values)\n            # restricted values could include a blank\n            # if it's there, remove it so we don't get two\n            if '' in choices:\n                choices.remove('')\n            choices.insert(0, '')   # add blank choice at the beginning of the list\n            kwargs['choices'] = choices\n\n        # TODO: possibly also useful to look for pattern restrictions\n\n        basetype = type.base_type()\n        if basetype == 'string':\n            newfield = StringField(self.xpath, required=self.required, **kwargs)\n            # copy original creation counter to newly created field\n            # to preserve declaration order\n            newfield.creation_counter = self.creation_counter\n            return newfield\n        else:\n            raise Exception(\"basetype %s is not yet supported by SchemaField\" % basetype)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef roundrobin(*iterables):\n    \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\"\n    # Recipe credited to George Sakkis\n    pending = len(iterables)\n    nexts = itertools.cycle(iter(it).next for it in iterables)\n    while pending:\n        try:\n            for next in nexts:\n                yield next()\n        except StopIteration:\n            pending -= 1\n            nexts = itertools.cycle(itertools.islice(nexts, pending))", "response": "A generator that yields all the iterables in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving set of equivalences return a map of transitive equivalence classes.", "response": "def get_scc_from_tuples(constraints):\n    \"\"\"Given set of equivalences, return map of transitive equivalence classes.\n\n    >> constraints = [(1,2), (2,3)]\n    >> get_scc_from_tuples(constraints)\n    {\n        1: (1, 2, 3),\n        2: (1, 2, 3),\n        3: (1, 2, 3),\n    }\n    \"\"\"\n    classes = unionfind.classes(constraints)\n    return dict((x, tuple(c)) for x, c in classes.iteritems())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_field_list(fieldnames, include_parents=False):\n    field_parts = (name.split('.') for name in fieldnames)\n    return _collect_fields(field_parts, include_parents)", "response": "Parse a list of field names possibly including dot - separated subform\n fields into an internal ParsedFieldList object representing the base order subform fields and subform listed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _collect_fields(field_parts_list, include_parents):\n    fields = []\n    subpart_lists = defaultdict(list)\n\n    for parts in field_parts_list:\n        field, subparts = parts[0], parts[1:]\n        if subparts:\n            if include_parents and field not in fields:\n                fields.append(field)\n            subpart_lists[field].append(subparts)\n        else:\n            fields.append(field)\n\n    subfields = dict((field, _collect_fields(subparts, include_parents))\n                     for field, subparts in six.iteritems(subpart_lists))\n\n    return ParsedFieldList(fields, subfields)", "response": "utility function to enable recursion in _parse_field_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef formfields_for_xmlobject(model, fields=None, exclude=None, widgets=None, options=None,\n        declared_subforms=None, max_num=None, extra=None):\n    \"\"\"\n    Returns three sorted dictionaries (:class:`django.utils.datastructures.SortedDict`).\n     * The first is a dictionary of form fields based on the\n       :class:`~eulxml.xmlmap.XmlObject` class fields and their types.\n     * The second is a sorted dictionary of subform classes for any fields of type\n       :class:`~eulxml.xmlmap.fields.NodeField` on the model.\n     * The third is a sorted dictionary of formsets for any fields of type\n       :class:`~eulxml.xmlmap.fields.NodeListField` on the model.\n\n    Default sorting (within each dictionary) is by XmlObject field creation order.\n\n    Used by :class:`XmlObjectFormType` to set up a new :class:`XmlObjectForm`\n    class.\n\n    :param fields: optional list of field names; if specified, only the named fields\n                will be returned, in the specified order\n    :param exclude: optional list of field names that should not be included on\n                the form; if a field is listed in both ``fields`` and ``exclude``,\n                it will be excluded\n    :param widgets: optional dictionary of widget options to be passed to form\n                field constructor, keyed on field name\n    :param options: optional :class:`~django.forms.models.ModelFormOptions`.\n                if specified then fields, exclude, and widgets will default\n                to its values.\n    :param declared_subforms: optional dictionary of field names and form classes;\n                if specified, the specified form class will be used to initialize\n                the corresponding subform (for a :class:`~eulxml.xmlmap.fields.NodeField`)\n                or a formset (for a :class:`~eulxml.xmlmap.fields.NodeListField`)\n    :param max_num: optional value for the maximum number of times a fieldset should repeat.\n    :param max_num: optional value for the number of extra forms to provide.\n    \"\"\"\n\n    # first collect fields and excludes for the form and all subforms. base\n    # these on the specified options object unless overridden in args.\n    fieldlist = getattr(options, 'parsed_fields', None)\n    if isinstance(fields, ParsedFieldList):\n        fieldlist = fields\n    elif fields is not None:\n        fieldlist = _parse_field_list(fields, include_parents=True)\n\n    excludelist = getattr(options, 'parsed_exclude', None)\n    if isinstance(fields, ParsedFieldList):\n        fieldlist = fields\n    elif exclude is not None:\n        excludelist = _parse_field_list(exclude, include_parents=False)\n\n    if widgets is None and options is not None:\n        widgets = options.widgets\n\n    if max_num is None and options is not None:\n        max_num = options.max_num\n\n    # collect the fields (unordered for now) that we're going to be returning\n    formfields = {}\n    subforms = {}\n    formsets = {}\n    field_order = {}\n    subform_labels = {}\n\n    for name, field in six.iteritems(model._fields):\n        if fieldlist and not name in fieldlist.fields:\n            # if specific fields have been requested and this is not one of them, skip it\n            continue\n        if excludelist and name in excludelist.fields:\n            # if exclude has been specified and this field is listed, skip it\n            continue\n        if widgets and name in widgets:\n            # if a widget has been specified for this field, pass as option to form field init\n            kwargs = {'widget': widgets[name] }\n        else:\n            kwargs = {}\n        # get apppropriate form widget based on xmlmap field type\n        field_type = None\n\n        # if the xmlmap field knows whether or not it is required, use for form\n        if field.required is not None:\n            kwargs['required'] = field.required\n        if field.verbose_name is not None:\n            kwargs['label'] = field.verbose_name\n        if field.help_text is not None:\n            kwargs['help_text'] = field.help_text\n\n        if hasattr(field, 'choices') and field.choices:\n            # if a field has choices defined, use a choice field (no matter what base type)\n            field_type = ChoiceField\n            kwargs['choices'] = [(val, val) for val in field.choices]\n            # FIXME: how to properly do non-required choice field?\n            # if field is optional, add a blank choice at the beginning of the list\n            if field.required == False and '' not in field.choices:\n                # TODO: add an empty_label option (like django ModelChoiceField)\n                # to xmlobjectform and pass it in to make this easier to customize\n                kwargs['choices'].insert(0, ('', ''))\n        elif isinstance(field, xmlmap.fields.StringField):\n            field_type = CharField\n        elif isinstance(field, xmlmap.fields.IntegerField):\n            field_type = IntegerField\n        elif isinstance(field, xmlmap.fields.DateField):\n            field_type = DateField\n        elif isinstance(field, xmlmap.fields.SimpleBooleanField):\n            # by default, fields are required - for a boolean, required means it must be checked\n            # since that seems nonsensical and not useful for a boolean,\n            # setting required to False to allow True or False values\n            kwargs['required'] = False\n            field_type = BooleanField\n\n        # datefield ? - not yet well-supported; leaving out for now\n        # ... should probably distinguish between date and datetime field\n\n        elif isinstance(field, xmlmap.fields.NodeField) or \\\n            isinstance(field, xmlmap.fields.NodeListField):\n            form_label = kwargs['label'] if 'label' in kwargs else fieldname_to_label(name)\n            # store subform label in case we can't set on subform/formset\n            subform_labels[name] = form_label\n\n             # if a subform class was declared, use that class exactly as is\n            if name in declared_subforms:\n            \tsubform = declared_subforms[name]\n\n            # otherwise, define a new xmlobject form for the nodefield or\n            # nodelistfield class, using any options passed in for fields under this one\n            else:\n                subform_opts = {\n                    'fields': fieldlist.subfields[name] if fieldlist and name in fieldlist.subfields else None,\n                    'exclude': excludelist.subfields[name] if excludelist and name in excludelist.subfields else None,\n                    'widgets': widgets[name] if widgets and name in widgets else None,\n                    'label': form_label,\n                }\n\n                # create the subform class\n                subform = xmlobjectform_factory(field.node_class, **subform_opts)\n\n            # store subform or generate and store formset, depending on field type\n            if isinstance(field, xmlmap.fields.NodeField):\n                subforms[name] = subform\n            elif isinstance(field, xmlmap.fields.NodeListField):\n                # formset_factory is from django core and we link into it here.\n                formsets[name] = formset_factory(subform, formset=BaseXmlObjectFormSet,\n                    max_num=subform._meta.max_num, can_delete=subform._meta.can_delete,\n                    extra=subform._meta.extra, can_order=subform._meta.can_order)\n\n                formsets[name].form_label = form_label\n\n        elif isinstance(field, xmlmap.fields.StringListField) or \\\n\t    isinstance(field, xmlmap.fields.IntegerListField):\n            form_label = kwargs['label'] if 'label' in kwargs else fieldname_to_label(name)\n\n            if isinstance(field, xmlmap.fields.IntegerListField):\n            \tlistform = IntegerListFieldForm\n            else:\n            \tlistform = ListFieldForm\n\n            # generate a listfield formset\n            formsets[name] = formset_factory(listform, formset=BaseXmlObjectListFieldFormSet)\n            # don't need can_delete: since each form is a single field, empty implies delete\n            # todo: extra, max_num ? widget?\n            formsets[name].form_label = form_label\n\n        # TODO: other list variants\n\n\n        else:\n            # raise exception for unsupported fields\n            # currently doesn't handle list fields\n            raise Exception('Error on field \"%s\": XmlObjectForm does not yet support auto form field generation for %s.' \\\n            \t% (name, field.__class__))\n\n        if field_type is not None:\n            if 'label' not in kwargs:\n                kwargs['label'] = fieldname_to_label(name)\n            formfields[name] = field_type(**kwargs)\n\n        # create a dictionary indexed by field creation order, for default field ordering\n        field_order[field.creation_counter] = name\n\n    # if fields were explicitly specified, return them in that order\n    if fieldlist:\n        ordered_fields = SortedDict((name, formfields[name])\n                                    for name in fieldlist.fields\n                                    if name in formfields)\n        ordered_subforms = SortedDict((name, subforms[name])\n                                      for name in fieldlist.fields\n                                      if name in subforms)\n        ordered_formsets = SortedDict((name, formsets[name])\n                                      for name in fieldlist.fields\n                                      if name in formsets)\n    else:\n        # sort on field creation counter and generate a django sorted dictionary\n        ordered_fields = SortedDict(\n            [(field_order[key], formfields[field_order[key]]) for key in sorted(field_order.keys())\n                                                if field_order[key] in formfields ]\n        )\n        ordered_subforms = SortedDict(\n            [(field_order[key], subforms[field_order[key]]) for key in sorted(field_order.keys())\n                                                if field_order[key] in subforms ]\n        )\n        ordered_formsets = SortedDict(\n            [(field_order[key], formsets[field_order[key]]) for key in sorted(field_order.keys())\n                                                if field_order[key] in formsets ]\n        )\n    return ordered_fields, ordered_subforms, ordered_formsets, subform_labels", "response": "Returns a list of form fields for the given XmlObject."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a dictionary based on the data in an XmlObject instance.", "response": "def xmlobject_to_dict(instance, fields=None, exclude=None, prefix=''):\n    \"\"\"\n    Generate a dictionary based on the data in an XmlObject instance to pass as\n    a Form's ``initial`` keyword argument.\n\n    :param instance: instance of :class:`~eulxml.xmlmap.XmlObject`\n    :param fields: optional list of fields - if specified, only the named fields\n            will be included in the data returned\n    :param exclude: optional list of fields to exclude from the data\n    \"\"\"\n    data = {}\n    # convert prefix to combining form for convenience\n    if prefix:\n        prefix = '%s-' % prefix\n    else:\n        prefix = ''\n\n    for name, field in six.iteritems(instance._fields):\n        # not editable?\n        if fields and not name in fields:\n            continue\n        if exclude and name in exclude:\n            continue\n        if isinstance(field, xmlmap.fields.NodeField):\n            nodefield = getattr(instance, name)\n            if nodefield is not None:\n                subprefix = '%s%s' % (prefix, name)\n                node_data = xmlobject_to_dict(nodefield, prefix=subprefix)\n                data.update(node_data)   # FIXME: fields/exclude\n        if isinstance(field, xmlmap.fields.NodeListField):\n            for i, child in enumerate(getattr(instance, name)):\n                subprefix = '%s%s-%d' % (prefix, name, i)\n                node_data = xmlobject_to_dict(child, prefix=subprefix)\n                data.update(node_data)   # FIXME: fields/exclude\n        else:\n            data[prefix + name] = getattr(instance, name)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xmlobjectform_factory(model, form=XmlObjectForm, fields=None, exclude=None,\n                          widgets=None, max_num=None, label=None, can_delete=True,\n                          extra=None, can_order=False):\n    \"\"\"Dynamically generate a new :class:`XmlObjectForm` class using the\n    specified :class:`eulxml.xmlmap.XmlObject` class.\n\n    Based on django's modelform_factory.\n    \"\"\"\n\n    attrs = {'model': model}\n    if fields is not None:\n        attrs['fields'] = fields\n    if exclude is not None:\n        attrs['exclude'] = exclude\n    if widgets is not None:\n        attrs['widgets'] = widgets\n    if max_num is not None:\n        attrs['max_num'] = max_num\n    if extra is not None:\n        attrs['extra'] = extra\n    if can_delete is not None:\n        attrs['can_delete'] = can_delete\n    if can_order is not None:\n        attrs['can_order'] = can_order\n\n    # If parent form class already has an inner Meta, the Meta we're\n    # creating needs to inherit from the parent's inner meta.\n    parent = (object,)\n    if hasattr(form, 'Meta'):\n        parent = (form.Meta, object)\n    Meta = type(str('Meta'), parent, attrs)\n\n    # Give this new form class a reasonable name.\n    class_name = model.__name__ + str('XmlObjectForm')\n\n    # Class attributes for the new form class.\n    form_class_attrs = {\n        'Meta': Meta,\n        # django has a callback formfield here; do we need that?\n        # label for a subform/formset\n        'form_label': label,\n    }\n\n    return XmlObjectFormType(class_name, (form,), form_class_attrs)", "response": "Dynamically generate a new XmlObjectForm class using the specified model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave bound form data into the XmlObject model instance and return the updated instance.", "response": "def update_instance(self):\n        \"\"\"Save bound form data into the XmlObject model instance and return the\n        updated instance.\"\"\"\n\n        # NOTE: django model form has a save method - not applicable here,\n        # since an XmlObject by itself is not expected to have a save method\n        # (only likely to be saved in context of a fedora or exist object)\n\n        if hasattr(self, 'cleaned_data'):   # possible to have an empty object/no data\n\n            opts = self._meta\n\n            # NOTE: _fields doesn't seem to order, which is\n            # problematic for some xml (e.g., where order matters for validity)\n\n            # use field order as declared in the form for update order\n            # when possible.\n            # (NOTE: this could be problematic also, since display order may\n            # not always be the same as schema order)\n            fields_in_order = []\n            if hasattr(self.Meta, 'fields'):\n                fields_in_order.extend(self.Meta.fields)\n                fields_in_order.extend([name for name in six.iterkeys(self.instance._fields)\n                                        if name in self.Meta.fields])\n            else:\n                fields_in_order = self.instance._fields.keys()\n\n            for name in fields_in_order:\n            # for name in self.instance._fields.iterkeys():\n            # for name in self.declared_fields.iterkeys():\n                if opts.fields and name not in opts.parsed_fields.fields:\n                    continue\n                if opts.exclude and name in opts.parsed_exclude.fields:\n                    continue\n                if name in self.cleaned_data:\n                    # special case: we don't want empty attributes and elements\n                    # for fields which returned no data from the form\n                    # converting '' to None and letting XmlObject handle\n                    if self.cleaned_data[name] == '':\n                        self.cleaned_data[name] = None\n                    setattr(self.instance, name, self.cleaned_data[name])\n\n            # update sub-model portions via any subforms\n            for name, subform in six.iteritems(self.subforms):\n                self._update_subinstance(name, subform)\n            for formset in six.itervalues(self.formsets):\n                formset.update_instance()\n        return self.instance"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_subinstance(self, name, subform):\n        old_subinstance = getattr(self.instance, name)\n        new_subinstance = subform.update_instance()\n\n        # if our instance previously had no node for the subform AND the\n        # updated one has data, then attach the new node.\n        if old_subinstance is None and not new_subinstance.is_empty():\n            setattr(self.instance, name, new_subinstance)\n\n        # on the other hand, if the instance previously had a node for the\n        # subform AND the updated one is empty, then remove the node.\n        if old_subinstance is not None and new_subinstance.is_empty():\n            delattr(self.instance, name)", "response": "Update the bound data for a single subform into the XmlObject model\n        instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if this form and all subforms are valid.", "response": "def is_valid(self):\n        \"\"\"Returns True if this form and all subforms (if any) are valid.\n\n        If all standard form-validation tests pass, uses :class:`~eulxml.xmlmap.XmlObject`\n        validation methods to check for schema-validity (if a schema is associated)\n        and reporting errors.  Additonal notes:\n\n         * schema validation requires that the :class:`~eulxml.xmlmap.XmlObject`\n           be initialized with the cleaned form data, so if normal validation\n           checks pass, the associated :class:`~eulxml.xmlmap.XmlObject` instance\n           will be updated with data via :meth:`update_instance`\n         * schema validation errors SHOULD NOT happen in a production system\n\n        :rtype: boolean\n        \"\"\"\n        valid = super(XmlObjectForm, self).is_valid() and \\\n                all(s.is_valid() for s in six.itervalues(self.subforms)) and \\\n                all(s.is_valid() for s in six.itervalues(self.formsets))\n        # schema validation can only be done after regular validation passes,\n        # because xmlobject must be updated with cleaned_data\n        if valid and self.instance is not None:\n            # update instance required to check schema-validity\n            instance = self.update_instance()\n            if instance.is_valid():\n                return True\n            else:\n                # if not schema-valid, add validation errors to error dictionary\n                # NOTE: not overriding _get_errors because that is used by the built-in validation\n                # append to any existing non-field errors\n                if NON_FIELD_ERRORS not in self._errors:\n                    self._errors[NON_FIELD_ERRORS] = self.error_class()\n                self._errors[NON_FIELD_ERRORS].append(\"There was an unexpected schema validation error.  \" +\n                    \"This should not happen!  Please report the following errors:\")\n                for err in instance.validation_errors():\n                    self._errors[NON_FIELD_ERRORS].append('VALIDATION ERROR: %s' % err.message)\n                return False\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextend BaseForm s helper function for outputting HTML.", "response": "def _html_output(self, normal_row, error_row, row_ender,  help_text_html, errors_on_separate_row):\n        \"\"\"Extend BaseForm's helper function for outputting HTML. Used by as_table(), as_ul(), as_p().\n\n        Combines the HTML version of the main form's fields with the HTML content\n        for any subforms.\n        \"\"\"\n        parts = []\n        parts.append(super(XmlObjectForm, self)._html_output(normal_row, error_row, row_ender,\n                help_text_html, errors_on_separate_row))\n\n        def _subform_output(subform):\n            return subform._html_output(normal_row, error_row, row_ender,\n                                        help_text_html, errors_on_separate_row)\n\n        for name, subform in six.iteritems(self.subforms):\n            # use form label if one was set\n            if hasattr(subform, 'form_label'):\n                name = subform.form_label\n            parts.append(self._html_subform_output(subform, name, _subform_output))\n\n        for name, formset in six.iteritems(self.formsets):\n            parts.append(u(formset.management_form))\n            # use form label if one was set\n            # - use declared subform label if any\n            if hasattr(formset.forms[0], 'form_label') and \\\n                    formset.forms[0].form_label is not None:\n                 name = formset.forms[0].form_label\n            # fallback to generated label from field name\n            elif hasattr(formset, 'form_label'):\n                name = formset.form_label\n\n            # collect the html output for all the forms in the formset\n            subform_parts = list()\n\n            for subform in formset.forms:\n                subform_parts.append(self._html_subform_output(subform,\n                                      gen_html=_subform_output, suppress_section=True))\n            # then wrap all forms in the section container, so formset label appears once\n            parts.append(self._html_subform_output(name=name, content=u'\\n'.join(subform_parts)))\n\n        return mark_safe(u'\\n'.join(parts))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_bitcoind_connection( rpc_username, rpc_password, server, port, use_https, timeout ):\n    \n    from .bitcoin_blockchain import AuthServiceProxy\n\n    global do_wrap_socket, create_ssl_authproxy\n        \n    log.debug(\"[%s] Connect to bitcoind at %s://%s@%s:%s, timeout=%s\" % (os.getpid(), 'https' if use_https else 'http', rpc_username, server, port, timeout) )\n    \n    protocol = 'https' if use_https else 'http'\n    if not server or len(server) < 1:\n        raise Exception('Invalid bitcoind host address.')\n    if not port or not is_valid_int(port):\n        raise Exception('Invalid bitcoind port number.')\n    \n    authproxy_config_uri = '%s://%s:%s@%s:%s' % (protocol, rpc_username, rpc_password, server, port)\n    \n    if use_https:\n        # TODO: ship with a cert\n        if do_wrap_socket:\n           # ssl._create_unverified_context and ssl.create_default_context are not supported.\n           # wrap the socket directly \n           connection = BitcoindConnection( server, int(port), timeout=timeout )\n           ret = AuthServiceProxy(authproxy_config_uri, connection=connection)\n           \n        elif create_ssl_authproxy:\n           # ssl has _create_unverified_context, so we're good to go \n           ret = AuthServiceProxy(authproxy_config_uri, timeout=timeout)\n        \n        else:\n           # have to set up an unverified context ourselves \n           ssl_ctx = ssl.create_default_context()\n           ssl_ctx.check_hostname = False\n           ssl_ctx.verify_mode = ssl.CERT_NONE\n           connection = httplib.HTTPSConnection( server, int(port), context=ssl_ctx, timeout=timeout )\n           ret = AuthServiceProxy(authproxy_config_uri, connection=connection)\n          \n    else:\n        ret = AuthServiceProxy(authproxy_config_uri)\n\n    # remember the options \n    bitcoind_opts = {\n       \"bitcoind_user\": rpc_username,\n       \"bitcoind_passwd\": rpc_password,\n       \"bitcoind_server\": server,\n       \"bitcoind_port\": port,\n       \"bitcoind_use_https\": use_https,\n       \"bitcoind_timeout\": timeout\n    }\n    \n    setattr( ret, \"opts\", bitcoind_opts )\n    return ret", "response": "Creates an RPC client to a bitcoind instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_bitcoind_impl( bitcoind_opts ):\n\n    if 'bitcoind_port' not in bitcoind_opts.keys() or bitcoind_opts['bitcoind_port'] is None:\n        log.error(\"No port given\")\n        raise ValueError(\"No RPC port given (bitcoind_port)\")\n\n    if 'bitcoind_timeout' not in bitcoind_opts.keys() or bitcoind_opts['bitcoind_timeout'] is None:\n        # default\n        bitcoind_opts['bitcoind_timeout'] = 300\n\n    try:\n        int(bitcoind_opts['bitcoind_port'])\n    except:\n        log.error(\"Not an int: '%s'\" % bitcoind_opts.get('bitcoind_port'))\n        raise\n\n    try:\n        float(bitcoind_opts.get('bitcoind_timeout', 300))\n    except:\n        log.error(\"Not a float: '%s'\" % bitcoind_opts.get('bitcoind_timeout', 300))\n        raise\n\n    return create_bitcoind_connection( bitcoind_opts['bitcoind_user'], bitcoind_opts['bitcoind_passwd'], \\\n                                       bitcoind_opts['bitcoind_server'], int(bitcoind_opts['bitcoind_port']), \\\n                                       bitcoind_opts.get('bitcoind_use_https', False), float(bitcoind_opts.get('bitcoind_timeout', 300)) )", "response": "Create a connection to bitcoind using a dict of config options."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to bitcoind and return a client object", "response": "def get_bitcoind_client(config_path=None, bitcoind_opts=None):\n    \"\"\"\n    Connect to bitcoind\n    \"\"\"\n    if bitcoind_opts is None and config_path is None:\n        raise ValueError(\"Need bitcoind opts or config path\")\n\n    bitcoind_opts = get_bitcoind_config(config_file=config_path)\n    log.debug(\"Connect to bitcoind at %s:%s (%s)\" % (bitcoind_opts['bitcoind_server'], bitcoind_opts['bitcoind_port'], config_path))\n    client = connect_bitcoind_impl( bitcoind_opts )\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new ECPrivateKey object from a string.", "response": "def ecdsa_private_key(privkey_str=None, compressed=None):\n    \"\"\"\n    Make a private key, but enforce the following rule:\n    * unless the key's hex encoding specifically ends in '01', treat it as uncompressed.\n    \"\"\"\n    if compressed is None:\n        compressed = False\n        if privkey_str is not None:\n            if len(privkey_str) == 66 and privkey_str[-2:] == '01':\n                compressed = True\n\n    return _ECPrivateKey(privkey_str, compressed=compressed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a public key object from a string.", "response": "def ecdsa_public_key(pubkey_str, compressed=None):\n    \"\"\"\n    Make a public key object, but enforce the following rule:\n    * if compressed is True or False, make the key compressed/uncompressed.\n    * otherwise, return whatever the hex encoding is\n    \"\"\"\n    if compressed == True:\n        pubkey_str = keylib.key_formatting.compress(pubkey_str)\n    elif compressed == False:\n        pubkey_str = keylib.key_formatting.decompress(pubkey_str)\n\n    return _ECPublicKey(pubkey_str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_privkey_compressed(privkey, compressed=True):\n    if len(privkey) != 64 and len(privkey) != 66:\n        raise ValueError(\"expected 32-byte private key as a hex string\")\n\n    # compressed?\n    if compressed and len(privkey) == 64:\n        privkey += '01'\n\n    if not compressed and len(privkey) == 66:\n        if privkey[-2:] != '01':\n            raise ValueError(\"private key does not end in '01'\")\n\n        privkey = privkey[:-2]\n\n    return privkey", "response": "Set the private key to be compressed or not"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the uncompressed hex form of a private key", "response": "def get_pubkey_hex( privatekey_hex ):\n    \"\"\"\n    Get the uncompressed hex form of a private key\n    \"\"\"\n    if not isinstance(privatekey_hex, (str, unicode)):\n        raise ValueError(\"private key is not a hex string but {}\".format(str(type(privatekey_hex))))\n\n    # remove 'compressed' hint\n    if len(privatekey_hex) > 64:\n        if privatekey_hex[-2:] != '01':\n            raise ValueError(\"private key does not end in 01\")\n\n        privatekey_hex = privatekey_hex[:64]\n\n    # get hex public key\n    privatekey_int = int(privatekey_hex, 16)\n    privk = ec.derive_private_key(privatekey_int, ec.SECP256K1(), default_backend())\n    pubk = privk.public_key()\n    x = pubk.public_numbers().x\n    y = pubk.public_numbers().y\n\n    pubkey_hex = \"04{:064x}{:064x}\".format(x, y)\n    return pubkey_hex"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the private and public keys from a private key string.", "response": "def get_uncompressed_private_and_public_keys( privkey_str ):\n    \"\"\"\n    Get the private and public keys from a private key string.\n    Make sure the both are *uncompressed*\n    \"\"\"\n    if not isinstance(privkey_str, (str, unicode)):\n        raise ValueError(\"private key given is not a string\")\n\n    pk = ecdsa_private_key(str(privkey_str))\n    pk_hex = pk.to_hex()\n\n    # force uncompressed\n    if len(pk_hex) > 64:\n        if pk_hex[-2:] != '01':\n            raise ValueError(\"private key does not end in '01'\")\n\n        pk_hex = pk_hex[:64]\n\n    pubk_hex = ecdsa_private_key(pk_hex).public_key().to_hex()\n    return pk_hex, pubk_hex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a private key for ecdsa signature", "response": "def decode_privkey_hex(privkey_hex):\n    \"\"\"\n    Decode a private key for ecdsa signature\n    \"\"\"\n    if not isinstance(privkey_hex, (str, unicode)):\n        raise ValueError(\"private key is not a string\")\n\n    # force uncompressed\n    priv = str(privkey_hex)\n    if len(priv) > 64:\n        if priv[-2:] != '01':\n            raise ValueError(\"private key does not end in '01'\")\n\n        priv = priv[:64]\n\n    pk_i = int(priv, 16)\n    return pk_i"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_pubkey_hex(pubkey_hex):\n    if not isinstance(pubkey_hex, (str, unicode)):\n        raise ValueError(\"public key is not a string\")\n\n    pubk = keylib.key_formatting.decompress(str(pubkey_hex))\n    assert len(pubk) == 130\n\n    pubk_raw = pubk[2:]\n    pubk_i = (int(pubk_raw[:64], 16), int(pubk_raw[64:], 16))\n    return pubk_i", "response": "Decode a public key for ecdsa verification\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes an ECDSA signature with low - s", "response": "def encode_signature(sig_r, sig_s):\n    \"\"\"\n    Encode an ECDSA signature, with low-s\n    \"\"\"\n    # enforce low-s \n    if sig_s * 2 >= SECP256k1_order:\n        log.debug(\"High-S to low-S\")\n        sig_s = SECP256k1_order - sig_s\n\n    sig_bin = '{:064x}{:064x}'.format(sig_r, sig_s).decode('hex')\n    assert len(sig_bin) == 64\n\n    sig_b64 = base64.b64encode(sig_bin)\n    return sig_b64"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_signature(sigb64):\n    sig_bin = base64.b64decode(sigb64)\n    if len(sig_bin) != 64:\n        raise ValueError(\"Invalid base64 signature\")\n\n    sig_hex = sig_bin.encode('hex')\n    sig_r = int(sig_hex[:64], 16)\n    sig_s = int(sig_hex[64:], 16)\n    return sig_r, sig_s", "response": "Decode a signature into r s"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsigns a string of data.", "response": "def sign_raw_data(raw_data, privatekey_hex):\n    \"\"\"\n    Sign a string of data.\n    Returns signature as a base64 string\n    \"\"\"\n    if not isinstance(raw_data, (str, unicode)):\n        raise ValueError(\"Data is not a string\")\n\n    raw_data = str(raw_data)\n\n    si = ECSigner(privatekey_hex)\n    si.update(raw_data)\n    return si.finalize()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_raw_data(raw_data, pubkey_hex, sigb64):\n    if not isinstance(raw_data, (str, unicode)):\n        raise ValueError(\"data is not a string\")\n\n    raw_data = str(raw_data)\n\n    vi = ECVerifier(pubkey_hex, sigb64)\n    vi.update(raw_data)\n    return vi.verify()", "response": "Verify the signature over a string given the public key\n    and base64 - encode signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign_digest(hash_hex, privkey_hex, hashfunc=hashlib.sha256):\n    if not isinstance(hash_hex, (str, unicode)):\n        raise ValueError(\"hash hex is not a string\")\n\n    hash_hex = str(hash_hex)\n\n    pk_i = decode_privkey_hex(privkey_hex)\n    privk = ec.derive_private_key(pk_i, ec.SECP256K1(), default_backend())\n\n    sig = privk.sign(hash_hex.decode('hex'), ec.ECDSA(utils.Prehashed(hashes.SHA256())))\n\n    sig_r, sig_s = decode_dss_signature(sig)\n    sigb64 = encode_signature(sig_r, sig_s)\n    return sigb64", "response": "Given a digest and a private key sign it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify_digest(hash_hex, pubkey_hex, sigb64, hashfunc=hashlib.sha256):\n    if not isinstance(hash_hex, (str, unicode)):\n        raise ValueError(\"hash hex is not a string\")\n\n    hash_hex = str(hash_hex)\n    pubk_uncompressed_hex = keylib.key_formatting.decompress(pubkey_hex)\n    sig_r, sig_s = decode_signature(sigb64)\n\n    pubk = ec.EllipticCurvePublicNumbers.from_encoded_point(ec.SECP256K1(), pubk_uncompressed_hex.decode('hex')).public_key(default_backend())\n    signature = encode_dss_signature(sig_r, sig_s)\n\n    try:\n        pubk.verify(signature, hash_hex.decode('hex'), ec.ECDSA(utils.Prehashed(hashes.SHA256())))\n        return True\n    except InvalidSignature:\n        return False", "response": "Given a digest public key and base64 signature verify that the public key signed the digest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef finalize(self):\n        signature = self.signer.finalize()\n        sig_r, sig_s = decode_dss_signature(signature)\n        sig_b64 = encode_signature(sig_r, sig_s)\n        return sig_b64", "response": "Get the base64 - encoded signature itself."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, data):\n        try:\n            self.verifier.update(data)\n        except TypeError:\n            log.error(\"Invalid data: {} ({})\".format(type(data), data))\n            raise", "response": "Update the hash used to generate the signature"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef semiconvergents(x):\n\n    (q, n), d = divmod(x.numerator, x.denominator), x.denominator\n    yield Fraction(q)\n    p0, q0, p1, q1 = 1, 0, q, 1\n    while n:\n        (q, n), d = divmod(d, n), n\n        for _ in range(q):\n            p0, q0 = p0+p1, q0+q1\n            yield Fraction(p0, q0)\n        p0, q0, p1, q1 = p1, q1, p0, q0", "response": "Semiconvergents of continued fraction expansion of a Fraction x."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the best p - bit lower and upper bounds for log2 n.", "response": "def logn2(n, p):\n    \"\"\"Best p-bit lower and upper bounds for log(2)/log(n), as Fractions.\"\"\"\n    with precision(p):\n        extra = 10\n        while True:\n            with precision(p+extra):\n                # use extra precision for intermediate step\n                log2upper = log2(n, RoundTowardPositive)\n                log2lower = log2(n, RoundTowardNegative)\n\n            lower = div(1, log2upper, RoundTowardNegative)\n            upper = div(1, log2lower, RoundTowardPositive)\n\n            # if lower and upper are adjacent (or equal) we're done\n            if next_up(lower) == upper:\n                return (Fraction(*lower.as_integer_ratio()),\n                        Fraction(*upper.as_integer_ratio()))\n\n            # otherwise, increase the precision and try again\n            extra += 10"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_values(self, by, axis=0, ascending=True, inplace=False,\n                    kind='quicksort', na_position='last'):\n        \"\"\"Sort by the values along either axis\n\n        Wrapper around the :meth:`pandas.DataFrame.sort_values` method.\n        \"\"\"\n        if inplace:\n            self._frame.sort_values(\n                by, axis=axis, ascending=ascending,\n                inplace=inplace, kind=kind, na_position=na_position)\n        else:\n            new = self.__class__(self._frame.sort_values(\n                by, axis=axis, ascending=ascending, inplace=inplace,\n                kind=kind, na_position=na_position))\n            new.metadata = self.metadata.copy()\n            new._metadata = copy.deepcopy(self._metadata)\n            return new", "response": "Sort by the values along either axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace(self, to_replace=None, value=None, inplace=False,\n                limit=None, regex=False, method='pad', axis=None):\n        \"\"\"Replace values given in 'to_replace' with 'value'.\n\n        Wrapper around the :meth:`pandas.DataFrame.replace` method.\n        \"\"\"\n        if inplace:\n            self._frame.replace(to_replace=to_replace, value=value,\n                                inplace=inplace, limit=limit, regex=regex,\n                                method=method, axis=axis)\n        else:\n            new = self.__class__(self._frame.replace(\n                to_replace=to_replace, value=value, inplace=inplace,\n                limit=limit, regex=regex, method=method, axis=axis))\n            new.metadata = self.metadata.copy()\n            new._metadata = copy.deepcopy(self._metadata)\n            return new", "response": "Returns a new instance with the new values replaced."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_index(self, keys, drop=True, append=False,\n                  inplace=False, verify_integrity=False):\n        \"\"\"Set the DataFrame index (row labels) using one or more existing\n        columns.\n\n        Wrapper around the :meth:`pandas.DataFrame.set_index` method.\n        \"\"\"\n\n        if drop is True:\n            try:\n                assert type(keys) is not str\n                dropped_cols = set(keys)\n            except (TypeError, AssertionError):\n                dropped_cols = set([keys])\n\n        if not self._required_cols <= (set(self.columns) - set(dropped_cols)):\n            raise PhysicalMeaning('You drop a column that is needed to '\n                                  'be a physical meaningful description '\n                                  'of a molecule.')\n\n        if inplace:\n            self._frame.set_index(keys, drop=drop, append=append,\n                                  inplace=inplace,\n                                  verify_integrity=verify_integrity)\n        else:\n            new = self._frame.set_index(keys, drop=drop, append=append,\n                                        inplace=inplace,\n                                        verify_integrity=verify_integrity)\n            return self.__class__(new, _metadata=self._metadata,\n                                  metadata=self.metadata)", "response": "Set the DataFrame index using one or more existing\n        columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append(self, other, ignore_index=False):\n        if not isinstance(other, self.__class__):\n            raise ValueError('May only append instances of same type.')\n        if type(ignore_index) is bool:\n            new_frame = self._frame.append(other._frame,\n                                           ignore_index=ignore_index,\n                                           verify_integrity=True)\n        else:\n            new_frame = self._frame.append(other._frame,\n                                           ignore_index=True,\n                                           verify_integrity=True)\n            if type(ignore_index) is int:\n                new_frame.index = range(ignore_index,\n                                        ignore_index + len(new_frame))\n            else:\n                new_frame.index = ignore_index\n        return self.__class__(new_frame)", "response": "Append rows of other to the end of this DataFrame returning a new object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies function along input axis of DataFrame. Wrapper around the pandas. DataFrame. apply method.", "response": "def apply(self, *args, **kwargs):\n        \"\"\"Applies function along input axis of DataFrame.\n\n        Wrapper around the :meth:`pandas.DataFrame.apply` method.\n        \"\"\"\n        return self.__class__(self._frame.apply(*args, **kwargs),\n                              metadata=self.metadata,\n                              _metadata=self._metadata)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef applymap(self, *args, **kwargs):\n        return self.__class__(self._frame.applymap(*args, **kwargs),\n                              metadata=self.metadata,\n                              _metadata=self._metadata)", "response": "Applies function elementwise\n        Wrapper around the pandas. DataFrame. applymap method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef article_views(\n            self, project, articles,\n            access='all-access', agent='all-agents', granularity='daily',\n            start=None, end=None):\n        \"\"\"\n        Get pageview counts for one or more articles\n        See `<https://wikimedia.org/api/rest_v1/metrics/pageviews/?doc\\\\\n                #!/Pageviews_data/get_metrics_pageviews_per_article_project\\\\\n                _access_agent_article_granularity_start_end>`_\n\n        :Parameters:\n            project : str\n                a wikimedia project such as en.wikipedia or commons.wikimedia\n            articles : list(str) or a simple str if asking for a single article\n            access : str\n                access method (desktop, mobile-web, mobile-app, or by default, all-access)\n            agent : str\n                user agent type (spider, user, bot, or by default, all-agents)\n            end : str|date\n                can be a datetime.date object or string in YYYYMMDD format\n                default: today\n            start : str|date\n                can be a datetime.date object or string in YYYYMMDD format\n                default: 30 days before end date\n            granularity : str\n                can be daily or monthly\n                default: daily\n\n        :Returns:\n            a nested dictionary that looks like: {\n                start_date: {\n                    article_1: view_count,\n                    article_2: view_count,\n                    ...\n                    article_n: view_count,\n                },\n                ...\n                end_date: {\n                    article_1: view_count,\n                    article_2: view_count,\n                    ...\n                    article_n: view_count,\n                }\n            }\n            The view_count will be None where no data is available, to distinguish from 0\n\n        TODO: probably doesn't handle unicode perfectly, look into it\n        \"\"\"\n        endDate = end or date.today()\n        if type(endDate) is not date:\n            endDate = parse_date(end)\n\n        startDate = start or endDate - timedelta(30)\n        if type(startDate) is not date:\n            startDate = parse_date(start)\n\n        # If the user passes in a string as \"articles\", convert to a list\n        if type(articles) is str:\n            articles = [articles]\n\n        articles = [a.replace(' ', '_') for a in articles]\n        articlesSafe = [quote(a, safe='') for a in articles]\n\n        urls = [\n            '/'.join([\n                endpoints['article'], project, access, agent, a, granularity,\n                format_date(startDate), format_date(endDate),\n            ])\n            for a in articlesSafe\n        ]\n\n        outputDays = timestamps_between(startDate, endDate, timedelta(days=1))\n        if granularity == 'monthly':\n            outputDays = list(set([month_from_day(day) for day in outputDays]))\n        output = defaultdict(dict, {\n            day: {a: None for a in articles} for day in outputDays\n        })\n\n        try:\n            results = self.get_concurrent(urls)\n            some_data_returned = False\n            for result in results:\n                if 'items' in result:\n                    some_data_returned = True\n                else:\n                    continue\n                for item in result['items']:\n                    output[parse_date(item['timestamp'])][item['article']] = item['views']\n            if not some_data_returned:\n                raise Exception(\n                    'The pageview API returned nothing useful at: {}'.format(urls)\n                )\n\n            return output\n        except:\n            print('ERROR while fetching and parsing ' + str(urls))\n            traceback.print_exc()\n            raise", "response": "Return a dictionary of pageviews for one or more articles."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef project_views(\n            self, projects,\n            access='all-access', agent='all-agents', granularity='daily',\n            start=None, end=None):\n        \"\"\"\n        Get pageview counts for one or more wikimedia projects\n        See `<https://wikimedia.org/api/rest_v1/metrics/pageviews/?doc\\\\\n                #!/Pageviews_data/get_metrics_pageviews_aggregate_project\\\\\n                _access_agent_granularity_start_end>`_\n\n        :Parameters:\n            project : list(str)\n                a list of wikimedia projects such as en.wikipedia or commons.wikimedia\n            access : str\n                access method (desktop, mobile-web, mobile-app, or by default, all-access)\n            agent : str\n                user agent type (spider, user, bot, or by default, all-agents)\n            granularity : str\n                the granularity of the timeseries to return (hourly, daily, or monthly)\n            end : str|date\n                can be a datetime.date object or string in YYYYMMDDHH format\n                default: today\n            start : str|date\n                can be a datetime.date object or string in YYYYMMDDHH format\n                default: 30 days before end date\n\n        :Returns:\n            a nested dictionary that looks like: {\n                start_date: {\n                    project_1: view_count,\n                    project_2: view_count,\n                    ...\n                    project_n: view_count,\n                },\n                ...\n                end_date: {\n                    project_1: view_count,\n                    project_2: view_count,\n                    ...\n                    project_n: view_count,\n                }\n            }\n            The view_count will be None where no data is available, to distinguish from 0\n        \"\"\"\n        endDate = end or date.today()\n        if type(endDate) is not date:\n            endDate = parse_date(end)\n\n        startDate = start or endDate - timedelta(30)\n        if type(startDate) is not date:\n            startDate = parse_date(start)\n\n        urls = [\n            '/'.join([\n                endpoints['project'], p, access, agent, granularity,\n                format_date(startDate), format_date(endDate),\n            ])\n            for p in projects\n        ]\n\n        if granularity == 'hourly':\n            increment = timedelta(hours=1)\n        elif granularity == 'daily':\n            increment = timedelta(days=1)\n        elif granularity == 'monthly':\n            increment = timedelta(months=1)\n\n        outputDays = timestamps_between(startDate, endDate, increment)\n        output = defaultdict(dict, {\n            day: {p: None for p in projects} for day in outputDays\n        })\n\n        try:\n            results = self.get_concurrent(urls)\n            some_data_returned = False\n            for result in results:\n                if 'items' in result:\n                    some_data_returned = True\n                else:\n                    continue\n                for item in result['items']:\n                    output[parse_date(item['timestamp'])][item['project']] = item['views']\n\n            if not some_data_returned:\n                raise Exception(\n                    'The pageview API returned nothing useful at: {}'.format(urls)\n                )\n            return output\n        except:\n            print('ERROR while fetching and parsing ' + str(urls))\n            traceback.print_exc()\n            raise", "response": "Get the pageviews for one or more wikimedia projects"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget pageview counts for one or more articles", "response": "def top_articles(\n            self, project, access='all-access',\n            year=None, month=None, day=None, limit=1000):\n        \"\"\"\n        Get pageview counts for one or more articles\n        See `<https://wikimedia.org/api/rest_v1/metrics/pageviews/?doc\\\\\n                #!/Pageviews_data/get_metrics_pageviews_top_project\\\\\n                _access_year_month_day>`_\n\n        :Parameters:\n            project : str\n                a wikimedia project such as en.wikipedia or commons.wikimedia\n            access : str\n                access method (desktop, mobile-web, mobile-app, or by default, all-access)\n            year : int\n                default : yesterday's year\n            month : int\n                default : yesterday's month\n            day : int\n                default : yesterday's day\n            limit : int\n                limit the number of articles returned to only the top <limit>\n                default : 1000\n\n        :Returns:\n            a sorted list of articles that looks like: [\n                {\n                    rank: <int>,\n                    article: <str>,\n                    views: <int>\n                }\n                ...\n            ]\n        \"\"\"\n        yesterday = date.today() - timedelta(days=1)\n        year = str(year or yesterday.year)\n        month = str(month or yesterday.month).rjust(2, '0')\n        day = str(day or yesterday.day).rjust(2, '0')\n\n        url = '/'.join([endpoints['top'], project, access, year, month, day])\n\n        try:\n            result = requests.get(url, headers=self.headers).json()\n\n            if 'items' in result and len(result['items']) == 1:\n                r = result['items'][0]['articles']\n                r.sort(key=lambda x: x['rank'])\n                return r[0:(limit)]\n        except:\n            print('ERROR while fetching or parsing ' + url)\n            traceback.print_exc()\n            raise\n\n        raise Exception(\n            'The pageview API returned nothing useful at: {}'.format(url)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quick_marshal(*args, **kwargs):\n    @marshal_with_model(*args, **kwargs)\n    def fn(value):\n        return value\n    return fn", "response": "A function that can be used to marshal a single record in a tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wrap_field(field):\n    class WrappedField(field):\n        def output(self, key, obj):\n            value = _fields.get_value(key if self.attribute is None else self.attribute, obj)\n\n            # For all fields, when its value was null (None), return null directly,\n            #  instead of return its default value (eg. int type's default value was 0)\n            # Because sometimes the client **needs** to know, was a field of the model empty, to decide its behavior.\n            return None if value is None else self.format(value)\n    return WrappedField", "response": "Improve Flask - RESTFul s original field type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the Lenz Hensley & Dor\u00e9 2017 dust map and placing it in the default : obj : dustmaps.", "response": "def fetch():\n    \"\"\"\n    Downloads the Lenz, Hensley & Dor\u00e9 (2017) dust map, placing it in the\n    default :obj:`dustmaps` data directory.\n    \"\"\"\n    doi = '10.7910/DVN/AFJNWJ'\n    fname = os.path.join(\n        data_dir(),\n        'lenz2017',\n        'ebv_lhd.hpx.fits')\n    fetch_utils.dataverse_download_doi(\n        doi, fname,\n        file_requirements={'filename': 'ebv_lhd.hpx.fits'})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn E ( B - V ) at the specified location.", "response": "def query(self, coords, **kwargs):\n        \"\"\"\n        Returns E(B-V), in mags, at the specified location(s) on the sky.\n\n        Args:\n            coords (:obj:`astropy.coordinates.SkyCoord`): The coordinates to query.\n\n        Returns:\n            A float array of the reddening, in magnitudes of E(B-V), at the\n            selected coordinates.\n        \"\"\"\n        return super(Lenz2017Query, self).query(coords, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a configuration file.", "response": "def write_configuration_file(filepath=_give_default_file_path(),\n                             overwrite=False):\n    \"\"\"Create a configuration file.\n\n    Writes the current state of settings into a configuration file.\n\n    .. note:: Since a file is permamently written, this function\n        is strictly speaking not sideeffect free.\n\n    Args:\n        filepath (str): Where to write the file.\n            The default is under both UNIX and Windows ``~/.chemcoordrc``.\n        overwrite (bool):\n\n    Returns:\n        None:\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read_dict(settings)\n\n    if os.path.isfile(filepath) and not overwrite:\n        try:\n            raise FileExistsError\n        except NameError:  # because of python2\n            warn('File exists already and overwrite is False (default).')\n    else:\n        with open(filepath, 'w') as configfile:\n            config.write(configfile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the configuration file.", "response": "def read_configuration_file(filepath=_give_default_file_path()):\n    \"\"\"Read the configuration file.\n\n    .. note:: This function changes ``cc.settings`` inplace and is\n        therefore not sideeffect free.\n\n    Args:\n        filepath (str): Where to read the file.\n            The default is under both UNIX and Windows ``~/.chemcoordrc``.\n\n    Returns:\n        None:\n    \"\"\"\n    config = configparser.ConfigParser()\n    config.read(filepath)\n\n    def get_correct_type(section, key, config):\n        \"\"\"Gives e.g. the boolean True for the string 'True'\"\"\"\n        def getstring(section, key, config):\n            return config[section][key]\n\n        def getinteger(section, key, config):  # pylint:disable=unused-variable\n            return config[section].getint(key)\n\n        def getboolean(section, key, config):\n            return config[section].getboolean(key)\n\n        def getfloat(section, key, config):  # pylint:disable=unused-variable\n            return config[section].getfloat(key)\n        special_actions = {}  # Something different than a string is expected\n        special_actions['defaults'] = {}\n        special_actions['defaults']['use_lookup'] = getboolean\n        try:\n            return special_actions[section][key](section, key, config)\n        except KeyError:\n            return getstring(section, key, config)\n\n    for section in config.sections():\n        for key in config[section]:\n            settings[section][key] = get_correct_type(section, key, config)\n    return settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef short(self):\n        '''Short-form of the unit title, excluding any unit date, as an instance\n        of :class:`~eulxml.xmlmap.eadmap.UnitTitle` . Can be used with formatting\n        anywhere the full form of the unittitle can be used.'''\n        # if there is no unitdate to remove, just return the current object\n        if not self.unitdate:\n            return self\n\n        # preserve any child elements (e.g., title or emph)\n        # initialize a unittitle with a *copy* of the current node\n        ut = UnitTitle(node=deepcopy(self.node))\n        # remove the unitdate node and return\n        ut.node.remove(ut.unitdate.node)\n        return ut", "response": "Short form of the unit title excluding any unit date as an instance\n        of ~eulxml. xmlmap. eclmap. UnitTitle. Can be used with formatting\n        anywhere."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hasSubseries(self):\n        if self.c and self.c[0] and ((self.c[0].level in ('series', 'subseries')) or\n                                     (self.c[0].c and self.c[0].c[0])):\n            return True\n        else:\n            return False", "response": "Check if this component has subseries or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if this finding aid has series or subseries.", "response": "def hasSeries(self):\n        \"\"\"Check if this finding aid has series/subseries.\n\n           Determined based on level of first component (series) or if first\n           component has subcomponents present.\n\n           :rtype: boolean\n        \"\"\"\n        if len(self.c) and (self.c[0].level == 'series' or (self.c[0].c and self.c[0].c[0])):\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize(\n            self, M_c, M_r, T, seed, initialization=b'from_the_prior',\n            row_initialization=-1, n_chains=1,\n            ROW_CRP_ALPHA_GRID=(), COLUMN_CRP_ALPHA_GRID=(),\n            S_GRID=(), MU_GRID=(), N_GRID=31,):\n        \"\"\"Sample a latent state from prior.\n\n        T, list of lists:\n            The data table in mapped representation (all floats, generated\n            by data_utils.read_data_objects)\n\n        :returns: X_L, X_D -- the latent state\n        \"\"\"\n        # FIXME: why is M_r passed?\n        arg_tuples = self.get_initialize_arg_tuples(\n            M_c, M_r, T, initialization, row_initialization, n_chains,\n            ROW_CRP_ALPHA_GRID, COLUMN_CRP_ALPHA_GRID, S_GRID, MU_GRID, N_GRID,\n            make_get_next_seed(seed),)\n\n        chain_tuples = self.mapper(self.do_initialize, arg_tuples)\n        X_L_list, X_D_list = zip(*chain_tuples)\n        if n_chains == 1:\n            X_L_list, X_D_list = X_L_list[0], X_D_list[0]\n\n        return X_L_list, X_D_list", "response": "Sample a latent state from prior."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts mutates the data T.", "response": "def insert(\n            self, M_c, T, X_L_list, X_D_list, new_rows=None, N_GRID=31,\n            CT_KERNEL=0):\n        \"\"\"Insert mutates the data T.\"\"\"\n        if new_rows is None:\n            raise ValueError(\"new_row must exist\")\n\n        if not isinstance(new_rows, list):\n            raise TypeError('new_rows must be list of lists')\n            if not isinstance(new_rows[0], list):\n                raise TypeError('new_rows must be list of lists')\n\n        X_L_list, X_D_list, was_multistate = su.ensure_multistate(\n            X_L_list, X_D_list)\n\n        # get insert arg tuples\n        arg_tuples = self.get_insert_arg_tuples(\n            M_c, T, X_L_list, X_D_list, new_rows, N_GRID, CT_KERNEL)\n\n        chain_tuples = self.mapper(self.do_insert, arg_tuples)\n        X_L_list, X_D_list = zip(*chain_tuples)\n\n        if not was_multistate:\n            X_L_list, X_D_list = X_L_list[0], X_D_list[0]\n\n        T.extend(new_rows)\n        ret_tuple = X_L_list, X_D_list, T\n        return ret_tuple"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef analyze(self, M_c, T, X_L, X_D, seed, kernel_list=(), n_steps=1, c=(),\n                r=(),\n                max_iterations=-1, max_time=-1, do_diagnostics=False,\n                diagnostics_every_N=1,\n                ROW_CRP_ALPHA_GRID=(),\n                COLUMN_CRP_ALPHA_GRID=(),\n                S_GRID=(), MU_GRID=(),\n                N_GRID=31,\n                do_timing=False,\n                CT_KERNEL=0,\n                progress=None,\n                ):\n        \"\"\"Evolve the latent state by running MCMC transition kernels.\n\n        :param seed: The random seed\n        :type seed: int\n        :param M_c: The column metadata\n        :type M_c: dict\n        :param T: The data table in mapped representation (all floats, generated\n            by data_utils.read_data_objects)\n        :param X_L: the latent variables associated with the latent state\n        :type X_L: dict\n        :param X_D: the particular cluster assignments of each row in each view\n        :type X_D: list of lists\n        :param kernel_list: names of the MCMC transition kernels to run\n        :type kernel_list: list of strings\n        :param n_steps: the number of times to run each MCMC transition kernel\n        :type n_steps: int\n        :param c: the (global) column indices to run MCMC transition kernels on\n        :type c: list of ints\n        :param r: the (global) row indices to run MCMC transition kernels on\n        :type r: list of ints\n        :param max_iterations: the maximum number of times ot run each MCMC\n            transition kernel. Applicable only if max_time != -1.\n        :type max_iterations: int\n        :param max_time: the maximum amount of time (seconds) to run MCMC\n            transition kernels for before stopping to return progress\n        :type max_time: float\n        :param progress: a function accepting\n            (n_steps, max_time, step_idx, elapsed_secs, end=None) where\n            `n_steps` is the total number of transition steps, `max_time` is the\n            timeout in secods, `step_idx` is number of transitions so far,\n            `elapsed_secs` is the amount of time so far, and `end=None` is an\n            optional kwarg for indicating the analysis has been completed.\n            For example, `progress` may be used to print a progress bar\n            to standard out.\n        :type progress: function pointer.\n        :returns: X_L, X_D -- the evolved latent state\n        \"\"\"\n        if n_steps <= 0:\n            raise ValueError(\"You must do at least one analyze step.\")\n\n        if CT_KERNEL not in [0, 1]:\n            raise ValueError(\"CT_KERNEL must be 0 (Gibbs) or 1 (MH)\")\n\n        if do_timing:\n            # Diagnostics and timing are exclusive.\n            do_diagnostics = False\n\n        diagnostic_func_dict, reprocess_diagnostics_func = \\\n            do_diagnostics_to_func_dict(do_diagnostics)\n\n        X_L_list, X_D_list, was_multistate = su.ensure_multistate(X_L, X_D)\n\n        arg_tuples = self.get_analyze_arg_tuples(\n            M_c,\n            T,\n            X_L_list,\n            X_D_list,\n            kernel_list,\n            n_steps,\n            c,\n            r,\n            max_iterations,\n            max_time,\n            diagnostic_func_dict,\n            diagnostics_every_N,\n            ROW_CRP_ALPHA_GRID,\n            COLUMN_CRP_ALPHA_GRID,\n            S_GRID,\n            MU_GRID,\n            N_GRID,\n            do_timing,\n            CT_KERNEL,\n            progress,\n            make_get_next_seed(seed))\n\n        chain_tuples = self.mapper(self.do_analyze, arg_tuples)\n\n        X_L_list, X_D_list, diagnostics_dict_list = zip(*chain_tuples)\n\n        if do_timing:\n            timing_list = diagnostics_dict_list\n\n        if not was_multistate:\n            X_L_list, X_D_list = X_L_list[0], X_D_list[0]\n        ret_tuple = X_L_list, X_D_list\n\n        if diagnostic_func_dict is not None:\n            diagnostics_dict = munge_diagnostics(diagnostics_dict_list)\n            if reprocess_diagnostics_func is not None:\n                diagnostics_dict = reprocess_diagnostics_func(diagnostics_dict)\n            ret_tuple = ret_tuple + (diagnostics_dict, )\n\n        if do_timing:\n            ret_tuple = ret_tuple + (timing_list, )\n\n        return ret_tuple", "response": "Evolve the latent state by running MCMC transition kernels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsamples values from predictive distribution of the given latent state. :param Y: A list of constraints to apply when sampling. Each constraint is a triplet of (r, d, v): r is the row index, d is the column index and v is the value of the constraint :type Y: list of lists :param Q: A list of values to sample. Each value is doublet of (r, d): r is the row index, d is the column index :type Q: list of lists :param n: the number of samples to draw :type n: int :returns: list of floats. Samples in the same order specified by Q", "response": "def simple_predictive_sample(self, M_c, X_L, X_D, Y, Q, seed, n=1):\n        \"\"\"Sample values from predictive distribution of the given latent state.\n\n        :param Y: A list of constraints to apply when sampling.  Each constraint\n            is a triplet of (r, d, v): r is the row index, d is the column\n            index and v is the value of the constraint\n        :type Y: list of lists\n        :param Q: A list of values to sample.  Each value is doublet of (r, d):\n            r is the row index, d is the column index\n        :type Q: list of lists\n        :param n: the number of samples to draw\n        :type n: int\n\n        :returns: list of floats.  Samples in the same order specified by Q\n        \"\"\"\n        get_next_seed = make_get_next_seed(seed)\n        samples = _do_simple_predictive_sample(\n            M_c, X_L, X_D, Y, Q, n, get_next_seed)\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simple_predictive_probability(self, M_c, X_L, X_D, Y, Q):\n        return su.simple_predictive_probability(M_c, X_L, X_D, Y, Q)", "response": "Simple predictive probability of a cell taking a value given a latent state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simple_predictive_probability_multistate(\n            self, M_c, X_L_list, X_D_list, Y, Q):\n        \"\"\"Calculate probability of a cell taking a value given a latent state.\n\n        :param Y: A list of constraints to apply when querying.  Each constraint\n            is a triplet of (r,d,v): r is the row index, d is the column\n            index and v is the value of the constraint\n        :type Y: list of lists\n\n        :param Q: A list of values to query.  Each value is triplet of (r,d,v):\n            r is the row index, d is the column index, and v is the value at\n            which the density is evaluated.\n        :type Q: list of lists\n\n        :returns: list of floats -- probabilities of the values specified by Q\n        \"\"\"\n        return su.simple_predictive_probability_multistate(\n            M_c, X_L_list, X_D_list, Y, Q)", "response": "Simple predictive probability of a cell taking a value given a latent state."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the jointly taking probability of cells jointly taking values given a latent state.", "response": "def predictive_probability(self, M_c, X_L, X_D, Y, Q):\n        \"\"\"Calculate probability of cells jointly taking values given a\n        latent state.\n\n        :param Y: A list of constraints to apply when querying.  Each constraint\n            is a triplet of (r, d, v): r is the row index, d is the column\n            index and v is the value of the constraint\n        :type Y: list of lists\n        :param Q: A list of values to query.  Each value is triplet of (r, d, v):\n            r is the row index, d is the column index, and v is the value at\n            which the density is evaluated.\n        :type Q: list of lists\n\n        :returns: float -- joint log probability of the values specified by Q\n        \"\"\"\n        return su.predictive_probability(M_c, X_L, X_D, Y, Q)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predictive_probability_multistate(self, M_c, X_L_list, X_D_list, Y, Q):\n        return su.predictive_probability_multistate(\n            M_c, X_L_list, X_D_list, Y, Q)", "response": "Calculate the predictive probability of cells jointly taking values given a latent state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nestimating the mutual information for each pair of columns on Q given the set of samples.", "response": "def mutual_information(\n            self, M_c, X_L_list, X_D_list, Q, seed, n_samples=1000):\n        \"\"\"Estimate mutual information for each pair of columns on Q given\n        the set of samples.\n\n        :param Q: List of tuples where each tuple contains the two column\n            indexes to compare\n        :type Q: list of two-tuples of ints\n        :param n_samples: the number of simple predictive samples to use\n        :type n_samples: int\n\n        :returns: list of list -- where each sublist is a set of MIs and\n            Linfoots from each crosscat sample.\n        \"\"\"\n        get_next_seed = make_get_next_seed(seed)\n        return iu.mutual_information(\n            M_c, X_L_list, X_D_list, Q, get_next_seed, n_samples)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef row_structural_typicality(self, X_L_list, X_D_list, row_id):\n        return su.row_structural_typicality(X_L_list, X_D_list, row_id)", "response": "Returns the typicality of given row."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef similarity(\n            self, M_c, X_L_list, X_D_list, given_row_id, target_row_id,\n            target_columns=None):\n        \"\"\"Computes the similarity of the given row to the target row,\n        averaged over all the column indexes given by target_columns.\n\n        :param given_row_id: the id of one of the rows to measure similarity\n            between\n        :type given_row_id: int\n        :param target_row_id: the id of the other row to measure similarity\n            between\n        :type target_row_id: int\n        :param target_columns: the columns to average the similarity over.\n            Defaults to all columns.\n        :type target_columns: int, string, or list of ints\n\n        :returns: float\n        \"\"\"\n        return su.similarity(\n            M_c, X_L_list, X_D_list, given_row_id,\n            target_row_id, target_columns)", "response": "Computes the similarity of the given row to the target row."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef impute(self, M_c, X_L, X_D, Y, Q, seed, n):\n        get_next_seed = make_get_next_seed(seed)\n        e = su.impute(M_c, X_L, X_D, Y, Q, n, get_next_seed)\n        return e", "response": "Impute values from predictive distribution of the given latent state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef impute_and_confidence(self, M_c, X_L, X_D, Y, Q, seed, n):\n        get_next_seed = make_get_next_seed(seed)\n        if isinstance(X_L, (list, tuple)):\n            assert isinstance(X_D, (list, tuple))\n            # TODO: multistate impute doesn't exist yet\n            # e,confidence = su.impute_and_confidence_multistate(\n            #   M_c, X_L, X_D, Y, Q, n, self.get_next_seed)\n            e, confidence = su.impute_and_confidence(\n                M_c, X_L, X_D, Y, Q, n, get_next_seed)\n        else:\n            e, confidence = su.impute_and_confidence(\n                M_c, X_L, X_D, Y, Q, n, get_next_seed)\n        return (e, confidence)", "response": "Impute values and confidence of the value from the predictive cluster distribution of the given latent state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_col_dep_constraints(\n            self, M_c, M_r, T, X_L, X_D, dep_constraints,\n            seed, max_rejections=100):\n        \"\"\"Ensures dependencey or indepdendency between columns.\n\n        `dep_constraints` is a list of where each entry is an (int, int, bool)\n        tuple where the first two entries are column indices and the third entry\n        describes whether the columns are to be dependent (True) or independent\n        (False).\n\n        Behavior Notes:\n        `ensure_col_dep_constraints` will add `col_ensure` enforcement to the\n        metadata (top level of `X_L`); unensure_col will remove it. Calling\n        ensure_col_dep_constraints twice will replace the first ensure.\n\n        This operation destroys the existing `X_L` and `X_D` metadata; the user\n        should be aware that it will clobber any existing analyses.\n\n        Implementation Notes:\n        Initialization is implemented via rejection (by repeatedly initalizing\n        states and throwing ones out that do not adhear to dep_constraints).\n        This means that in the event the contraints in dep_constraints are\n        complex, or impossible, that the rejection alogrithm may fail.\n\n        The returned metadata looks like this:\n        >>> dep_constraints\n        [(1, 2, True), (2, 5, True), (1, 3, False)]\n        >>> X_L['col_ensure']\n        {\n            \"dependent\" : {\n                1 : (1, 2, 5),\n                2 : (1, 2, 5),\n                5 : (1, 5, 2),\n            },\n            \"independent\" : {\n                1 : [3],\n                3 : [1],\n            }\n        }\n        \"\"\"\n        X_L_list, X_D_list, was_multistate = su.ensure_multistate(X_L, X_D)\n\n        if was_multistate:\n            num_states = len(X_L_list)\n        else:\n            num_states = 1\n\n        dependencies = [(c[0], c[1]) for c in dep_constraints if c[2]]\n        independencies = [(c[0], c[1]) for c in dep_constraints if not c[2]]\n\n        col_ensure_md = dict()\n        col_ensure_md[True] = {\n            str(key) : list(val) for\n            key, val in gu.get_scc_from_tuples(dependencies).iteritems()\n        }\n        col_ensure_md[False] = {\n            str(key) : list(val) for\n            key, val in gu.get_scc_from_tuples(independencies).iteritems()\n        }\n\n        def assert_dep_constraints(X_L, X_D, dep_constraints):\n            for col1, col2, dep in dep_constraints:\n                if not self.assert_col_dep_constraints(\n                        X_L, X_D, col1, col2, dep, True):\n                    return False\n            return True\n\n        X_L_out = []\n        X_D_out = []\n        get_next_seed = make_get_next_seed(seed)\n\n        for _ in range(num_states):\n            counter = 0\n            X_L_i, X_D_i = self.initialize(M_c, M_r, T, get_next_seed())\n            while not assert_dep_constraints(X_L_i, X_D_i, dep_constraints):\n                if counter > max_rejections:\n                    raise RuntimeError(\n                        'Could not ranomly generate a partition '\n                        'that satisfies the constraints in dep_constraints.')\n                counter += 1\n                X_L_i, X_D_i = self.initialize(M_c, M_r, T, get_next_seed())\n\n            X_L_i['col_ensure'] = dict()\n            X_L_i['col_ensure']['dependent'] = col_ensure_md[True]\n            X_L_i['col_ensure']['independent'] = col_ensure_md[False]\n\n            X_D_out.append(X_D_i)\n            X_L_out.append(X_L_i)\n\n        if was_multistate:\n            return X_L_out, X_D_out\n        else:\n            return X_L_out[0], X_D_out[0]", "response": "Ensures that the columns of the user have dependencey or indepdendency between columns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_row_dep_constraint(\n            self, M_c, T, X_L, X_D, row1, row2, dependent=True, wrt=None,\n            max_iter=100, force=False):\n        \"\"\"Ensures dependencey or indepdendency between rows with respect to\n        columns.\"\"\"\n        X_L_list, X_D_list, was_multistate = su.ensure_multistate(X_L, X_D)\n\n        if force:\n            raise NotImplementedError\n        else:\n            kernel_list = ('row_partition_assignements',)\n            for i, (X_L_i, X_D_i) in enumerate(zip(X_L_list, X_D_list)):\n\n                iters = 0\n                X_L_tmp = copy.deepcopy(X_L_i)\n                X_D_tmp = copy.deepcopy(X_D_i)\n\n                while not self.assert_row(\n                        X_L_tmp, X_D_tmp, row1, row2,\n                        dependent=dependent, wrt=wrt):\n                    if iters >= max_iter:\n                        raise RuntimeError(\n                            'Maximum ensure iterations reached.')\n                    # XXX No seed?\n                    res = self.analyze(\n                        M_c, T, X_L_i, X_D_i, kernel_list=kernel_list,\n                        n_steps=1, r=(row1,))\n                    X_L_tmp = res[0]\n                    X_D_tmp = res[1]\n                    iters += 1\n\n                X_L_list[i] = X_L_tmp\n                X_D_list[i] = X_D_tmp\n\n        if was_multistate:\n            return X_L_list, X_D_list\n        else:\n            return X_L_list[0], X_D_list[0]", "response": "Ensures that the row with respect to the columns of the specified row are indepdendency."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_format_specifier(specification):\n    m = _parse_format_specifier_regex.match(specification)\n    if m is None:\n        raise ValueError(\n            \"Invalid format specifier: {!r}\".format(specification))\n    format_dict = m.groupdict('')\n\n    # Convert zero-padding into fill and alignment.\n    zeropad = format_dict.pop('zeropad')\n    if zeropad:\n        # If zero padding is requested, fill and align fields should be absent.\n        if format_dict['align']:\n            raise ValueError(\n                \"Invalid format specifier: {!r}\".format(specification))\n        # Impossible to have 'fill' without 'align'.\n        assert not format_dict['fill']\n        format_dict['align'] = '='\n        format_dict['fill'] = '0'\n\n    # Default alignment is right-aligned.\n    if not format_dict['align']:\n        format_dict['align'] = '>'\n\n    # Default fill character is space.\n    if not format_dict['fill']:\n        format_dict['fill'] = ' '\n\n    # Default sign is '-'.\n    if not format_dict['sign']:\n        format_dict['sign'] = '-'\n\n    # Convert minimum width to an int; default is zero.\n    format_dict['minimumwidth'] = int(format_dict['minimumwidth'] or '0')\n\n    # Convert precision to an int, or `None` if no precision given.\n    if format_dict['precision']:\n        format_dict['precision'] = int(format_dict['precision'][1:])\n    else:\n        format_dict['precision'] = None\n\n    # If no rounding mode is given, assume 'N'.\n    if not format_dict['rounding']:\n        format_dict['rounding'] = 'N'\n\n    return format_dict", "response": "Parse the given format specification and return a dictionary containing relevant values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_align(sign, body, spec):\n    padding = spec['fill'] * (spec['minimumwidth'] - len(sign) - len(body))\n    align = spec['align']\n    if align == '<':\n        result = sign + body + padding\n    elif align == '>':\n        result = padding + sign + body\n    elif align == '=':\n        result = sign + padding + body\n    elif align == '^':\n        half = len(padding)//2\n        result = padding[:half] + sign + body + padding[half:]\n    else:\n        raise ValueError(\"Unrecognised alignment field: {!r}\".format(align))\n\n    return result", "response": "Given an unpadded non - aligned numeric string body and sign\n string sign add padding and alignment conforming to the given\n format specifier dictionary spec."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subs(self, *args):\n        cols = ['x', 'y', 'z']\n        out = self.copy()\n\n        def get_subs_f(*args):\n            def subs_function(x):\n                if hasattr(x, 'subs'):\n                    x = x.subs(*args)\n                    try:\n                        x = float(x)\n                    except TypeError:\n                        pass\n                return x\n            return subs_function\n\n        for col in cols:\n            if out.loc[:, col].dtype is np.dtype('O'):\n                out.loc[:, col] = out.loc[:, col].map(get_subs_f(*args))\n                try:\n                    out.loc[:, col] = out.loc[:, col].astype('f8')\n                except (SystemError, TypeError):\n                    pass\n        return out", "response": "Substitute a symbolic expression in the column x y z of self with a value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _jit_give_bond_array(pos, bond_radii, self_bonding_allowed=False):\n        n = pos.shape[0]\n        bond_array = np.empty((n, n), dtype=nb.boolean)\n\n        for i in range(n):\n            for j in range(i, n):\n                D = 0\n                for h in range(3):\n                    D += (pos[i, h] - pos[j, h])**2\n                B = (bond_radii[i] + bond_radii[j])**2\n                bond_array[i, j] = (B - D) >= 0\n                bond_array[j, i] = bond_array[i, j]\n        if not self_bonding_allowed:\n            for i in range(n):\n                bond_array[i, i] = False\n        return bond_array", "response": "Calculate a boolean array where A [ i j ] is True indicates a\n        bond between the i - th atom and j - th atom."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_bond_dict(self, fragment_indices,\n                          positions,\n                          bond_radii,\n                          bond_dict=None,\n                          self_bonding_allowed=False,\n                          convert_index=None):\n        \"\"\"If bond_dict is provided, this function is not side effect free\n        bond_dict has to be a collections.defaultdict(set)\n        \"\"\"\n        assert (isinstance(bond_dict, collections.defaultdict)\n                or bond_dict is None)\n        fragment_indices = list(fragment_indices)\n        if convert_index is None:\n            convert_index = dict(enumerate(fragment_indices))\n        if bond_dict is None:\n            bond_dict = collections.defaultdict(set)\n\n        frag_pos = positions[fragment_indices, :]\n        frag_bond_radii = bond_radii[fragment_indices]\n\n        bond_array = self._jit_give_bond_array(\n            frag_pos, frag_bond_radii,\n            self_bonding_allowed=self_bonding_allowed)\n        a, b = bond_array.nonzero()\n        a, b = [convert_index[i] for i in a], [convert_index[i] for i in b]\n        for row, index in enumerate(a):\n            # bond_dict is a collections.defaultdict(set)\n            bond_dict[index].add(b[row])\n        return bond_dict", "response": "This function is not side effect free\n       . It is not side effect free\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bonds(self,\n                  self_bonding_allowed=False,\n                  offset=3,\n                  modified_properties=None,\n                  use_lookup=False,\n                  set_lookup=True,\n                  atomic_radius_data=None\n                  ):\n        \"\"\"Return a dictionary representing the bonds.\n\n        .. warning:: This function is **not sideeffect free**, since it\n            assigns the output to a variable ``self._metadata['bond_dict']`` if\n            ``set_lookup`` is ``True`` (which is the default). This is\n            necessary for performance reasons.\n\n        ``.get_bonds()`` will use or not use a lookup\n        depending on ``use_lookup``. Greatly increases performance if\n        True, but could introduce bugs in certain situations.\n\n        Just imagine a situation where the :class:`~Cartesian` is\n        changed manually. If you apply lateron a method e.g.\n        :meth:`~get_zmat()` that makes use of :meth:`~get_bonds()`\n        the dictionary of the bonds\n        may not represent the actual situation anymore.\n\n        You have two possibilities to cope with this problem.\n        Either you just re-execute ``get_bonds`` on your specific instance,\n        or you change the ``internally_use_lookup`` option in the settings.\n        Please note that the internal use of the lookup variable\n        greatly improves performance.\n\n        Args:\n            modified_properties (dic): If you want to change the van der\n                Vaals radius of one or more specific atoms, pass a\n                dictionary that looks like::\n\n                    modified_properties = {index1: 1.5}\n\n                For global changes use the constants module.\n            offset (float):\n            use_lookup (bool):\n            set_lookup (bool):\n            self_bonding_allowed (bool):\n            atomic_radius_data (str): Defines which column of\n                :attr:`constants.elements` is used. The default is\n                ``atomic_radius_cc`` and can be changed with\n                :attr:`settings['defaults']['atomic_radius_data']`.\n                Compare with :func:`add_data`.\n\n        Returns:\n            dict: Dictionary mapping from an atom index to the set of\n            indices of atoms bonded to.\n        \"\"\"\n        if atomic_radius_data is None:\n            atomic_radius_data = settings['defaults']['atomic_radius_data']\n\n        def complete_calculation():\n            old_index = self.index\n            self.index = range(len(self))\n            fragments = self._divide_et_impera(offset=offset)\n            positions = np.array(self.loc[:, ['x', 'y', 'z']], order='F')\n            data = self.add_data([atomic_radius_data, 'valency'])\n            bond_radii = data[atomic_radius_data]\n            if modified_properties is not None:\n                bond_radii.update(pd.Series(modified_properties))\n            bond_radii = bond_radii.values\n            bond_dict = collections.defaultdict(set)\n            for i, j, k in product(*[range(x) for x in fragments.shape]):\n                # The following call is not side effect free and changes\n                # bond_dict\n                self._update_bond_dict(\n                    fragments[i, j, k], positions, bond_radii,\n                    bond_dict=bond_dict,\n                    self_bonding_allowed=self_bonding_allowed)\n\n            for i in set(self.index) - set(bond_dict.keys()):\n                bond_dict[i] = {}\n\n            self.index = old_index\n            rename = dict(enumerate(self.index))\n            bond_dict = {rename[key]: {rename[i] for i in bond_dict[key]}\n                         for key in bond_dict}\n            return bond_dict\n\n        if use_lookup:\n            try:\n                bond_dict = self._metadata['bond_dict']\n            except KeyError:\n                bond_dict = complete_calculation()\n        else:\n            bond_dict = complete_calculation()\n\n        if set_lookup:\n            self._metadata['bond_dict'] = bond_dict\n        return bond_dict", "response": "Returns a dictionary representing the bonds of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_coordination_sphere(\n            self, index_of_atom, n_sphere=1, give_only_index=False,\n            only_surface=True, exclude=None,\n            use_lookup=None):\n        \"\"\"Return a Cartesian of atoms in the n-th coordination sphere.\n\n        Connected means that a path along covalent bonds exists.\n\n        Args:\n            index_of_atom (int):\n            give_only_index (bool): If ``True`` a set of indices is\n                returned. Otherwise a new Cartesian instance.\n            n_sphere (int): Determines the number of the coordination sphere.\n            only_surface (bool): Return only the surface of the coordination\n                sphere.\n            exclude (set): A set of indices that should be ignored\n                for the path finding.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            A set of indices or a new Cartesian instance.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n        exclude = set() if exclude is None else exclude\n        bond_dict = self.get_bonds(use_lookup=use_lookup)\n        i = index_of_atom\n        if n_sphere != 0:\n            visited = set([i]) | exclude\n            try:\n                tmp_bond_dict = {j: (bond_dict[j] - visited)\n                                 for j in bond_dict[i]}\n            except KeyError:\n                tmp_bond_dict = {}\n            n = 0\n            while tmp_bond_dict and (n + 1) < n_sphere:\n                new_tmp_bond_dict = {}\n                for i in tmp_bond_dict:\n                    if i in visited:\n                        continue\n                    visited.add(i)\n                    for j in tmp_bond_dict[i]:\n                        new_tmp_bond_dict[j] = bond_dict[j] - visited\n                tmp_bond_dict = new_tmp_bond_dict\n                n += 1\n            if only_surface:\n                index_out = set(tmp_bond_dict.keys())\n            else:\n                index_out = visited | set(tmp_bond_dict.keys())\n        else:\n            index_out = {i}\n\n        if give_only_index:\n            return index_out - exclude\n        else:\n            return self.loc[index_out - exclude]", "response": "Returns a set of indices that are connected to the n - th coordination sphere."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nis called after cutting geometric shapes. If you want to change the rules how bonds are preserved, when applying e.g. :meth:`Cartesian.cut_sphere` this is the function you have to modify. It is recommended to inherit from the Cartesian class to tailor it for your project, instead of modifying the source code of ChemCoord. Args: sliced_frame (Cartesian): use_lookup (bool): Use a lookup variable for :meth:`~chemcoord.Cartesian.get_bonds`. The default is specified in ``settings['defaults']['use_lookup']`` Returns: Cartesian:", "response": "def _preserve_bonds(self, sliced_cartesian,\n                        use_lookup=None):\n        \"\"\"Is called after cutting geometric shapes.\n\n        If you want to change the rules how bonds are preserved, when\n            applying e.g. :meth:`Cartesian.cut_sphere` this is the\n            function you have to modify.\n        It is recommended to inherit from the Cartesian class to\n            tailor it for your project, instead of modifying the\n            source code of ChemCoord.\n\n        Args:\n            sliced_frame (Cartesian):\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        included_atoms_set = set(sliced_cartesian.index)\n        assert included_atoms_set.issubset(set(self.index)), \\\n            'The sliced Cartesian has to be a subset of the bigger frame'\n        bond_dic = self.get_bonds(use_lookup=use_lookup)\n        new_atoms = set([])\n        for atom in included_atoms_set:\n            new_atoms = new_atoms | bond_dic[atom]\n        new_atoms = new_atoms - included_atoms_set\n        while not new_atoms == set([]):\n            index_of_interest = new_atoms.pop()\n            included_atoms_set = (\n                included_atoms_set |\n                self.get_coordination_sphere(\n                    index_of_interest,\n                    n_sphere=float('inf'),\n                    only_surface=False,\n                    exclude=included_atoms_set,\n                    give_only_index=True,\n                    use_lookup=use_lookup))\n            new_atoms = new_atoms - included_atoms_set\n        molecule = self.loc[included_atoms_set, :]\n        return molecule"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncut a covalent sphere.", "response": "def cut_sphere(\n            self,\n            radius=15.,\n            origin=None,\n            outside_sliced=True,\n            preserve_bonds=False):\n        \"\"\"Cut a sphere specified by origin and radius.\n\n        Args:\n            radius (float):\n            origin (list): Please note that you can also pass an\n                integer. In this case it is interpreted as the\n                index of the atom which is taken as origin.\n            outside_sliced (bool): Atoms outside/inside the sphere\n                are cut out.\n            preserve_bonds (bool): Do not cut covalent bonds.\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        if origin is None:\n            origin = np.zeros(3)\n        elif pd.api.types.is_list_like(origin):\n            origin = np.array(origin, dtype='f8')\n        else:\n            origin = self.loc[origin, ['x', 'y', 'z']]\n\n        molecule = self.get_distance_to(origin)\n        if outside_sliced:\n            molecule = molecule[molecule['distance'] < radius]\n        else:\n            molecule = molecule[molecule['distance'] > radius]\n\n        if preserve_bonds:\n            molecule = self._preserve_bonds(molecule)\n\n        return molecule"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new molecule with the cuboid specified by edge and radius.", "response": "def cut_cuboid(\n            self,\n            a=20,\n            b=None,\n            c=None,\n            origin=None,\n            outside_sliced=True,\n            preserve_bonds=False):\n        \"\"\"Cut a cuboid specified by edge and radius.\n\n        Args:\n            a (float): Value of the a edge.\n            b (float): Value of the b edge. Takes value of a if None.\n            c (float): Value of the c edge. Takes value of a if None.\n            origin (list): Please note that you can also pass an\n                integer. In this case it is interpreted as the index\n                of the atom which is taken as origin.\n            outside_sliced (bool): Atoms outside/inside the sphere are\n                cut away.\n            preserve_bonds (bool): Do not cut covalent bonds.\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        if origin is None:\n            origin = np.zeros(3)\n        elif pd.api.types.is_list_like(origin):\n            origin = np.array(origin, dtype='f8')\n        else:\n            origin = self.loc[origin, ['x', 'y', 'z']]\n        b = a if b is None else b\n        c = a if c is None else c\n\n        sides = np.array([a, b, c])\n        pos = self.loc[:, ['x', 'y', 'z']]\n        if outside_sliced:\n            molecule = self[((pos - origin) / (sides / 2)).max(axis=1) < 1.]\n        else:\n            molecule = self[((pos - origin) / (sides / 2)).max(axis=1) > 1.]\n\n        if preserve_bonds:\n            molecule = self._preserve_bonds(molecule)\n        return molecule"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_barycenter(self):\n        try:\n            mass = self['mass'].values\n        except KeyError:\n            mass = self.add_data('mass')['mass'].values\n        pos = self.loc[:, ['x', 'y', 'z']].values\n        return (pos * mass[:, None]).sum(axis=0) / self.get_total_mass()", "response": "Return the mass weighted average location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the distances between the atoms with the given indices i and b.", "response": "def get_bond_lengths(self, indices):\n        \"\"\"Return the distances between given atoms.\n\n        Calculates the distance between the atoms with\n        indices ``i`` and ``b``.\n        The indices can be given in three ways:\n\n        * As simple list ``[i, b]``\n        * As list of lists: ``[[i1, b1], [i2, b2]...]``\n        * As :class:`pd.DataFrame` where ``i`` is taken from the index and\n          ``b`` from the respective column ``'b'``.\n\n        Args:\n            indices (list):\n\n        Returns:\n            :class:`numpy.ndarray`: Vector of angles in degrees.\n        \"\"\"\n        coords = ['x', 'y', 'z']\n        if isinstance(indices, pd.DataFrame):\n            i_pos = self.loc[indices.index, coords].values\n            b_pos = self.loc[indices.loc[:, 'b'], coords].values\n        else:\n            indices = np.array(indices)\n            if len(indices.shape) == 1:\n                indices = indices[None, :]\n            i_pos = self.loc[indices[:, 0], coords].values\n            b_pos = self.loc[indices[:, 1], coords].values\n        return np.linalg.norm(i_pos - b_pos, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_angle_degrees(self, indices):\n        coords = ['x', 'y', 'z']\n        if isinstance(indices, pd.DataFrame):\n            i_pos = self.loc[indices.index, coords].values\n            b_pos = self.loc[indices.loc[:, 'b'], coords].values\n            a_pos = self.loc[indices.loc[:, 'a'], coords].values\n        else:\n            indices = np.array(indices)\n            if len(indices.shape) == 1:\n                indices = indices[None, :]\n            i_pos = self.loc[indices[:, 0], coords].values\n            b_pos = self.loc[indices[:, 1], coords].values\n            a_pos = self.loc[indices[:, 2], coords].values\n\n        BI, BA = i_pos - b_pos, a_pos - b_pos\n        bi, ba = [v / np.linalg.norm(v, axis=1)[:, None] for v in (BI, BA)]\n        dot_product = np.sum(bi * ba, axis=1)\n        dot_product[dot_product > 1] = 1\n        dot_product[dot_product < -1] = -1\n        angles = np.degrees(np.arccos(dot_product))\n        return angles", "response": "Calculates the angle in degrees between the atoms with the given indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the dihedrals between the atoms with the given indices.", "response": "def get_dihedral_degrees(self, indices, start_row=0):\n        \"\"\"Return the dihedrals between given atoms.\n\n        Calculates the dihedral angle in degrees between the atoms with\n        indices ``i, b, a, d``.\n        The indices can be given in three ways:\n\n        * As simple list ``[i, b, a, d]``\n        * As list of lists: ``[[i1, b1, a1, d1], [i2, b2, a2, d2]...]``\n        * As :class:`pandas.DataFrame` where ``i`` is taken from the index and\n          ``b``, ``a`` and ``d``from the respective columns\n          ``'b'``, ``'a'`` and ``'d'``.\n\n        Args:\n            indices (list):\n\n        Returns:\n            :class:`numpy.ndarray`: Vector of angles in degrees.\n        \"\"\"\n        coords = ['x', 'y', 'z']\n        if isinstance(indices, pd.DataFrame):\n            i_pos = self.loc[indices.index, coords].values\n            b_pos = self.loc[indices.loc[:, 'b'], coords].values\n            a_pos = self.loc[indices.loc[:, 'a'], coords].values\n            d_pos = self.loc[indices.loc[:, 'd'], coords].values\n        else:\n            indices = np.array(indices)\n            if len(indices.shape) == 1:\n                indices = indices[None, :]\n            i_pos = self.loc[indices[:, 0], coords].values\n            b_pos = self.loc[indices[:, 1], coords].values\n            a_pos = self.loc[indices[:, 2], coords].values\n            d_pos = self.loc[indices[:, 3], coords].values\n\n        IB = b_pos - i_pos\n        BA = a_pos - b_pos\n        AD = d_pos - a_pos\n\n        N1 = np.cross(IB, BA, axis=1)\n        N2 = np.cross(BA, AD, axis=1)\n        n1, n2 = [v / np.linalg.norm(v, axis=1)[:, None] for v in (N1, N2)]\n\n        dot_product = np.sum(n1 * n2, axis=1)\n        dot_product[dot_product > 1] = 1\n        dot_product[dot_product < -1] = -1\n        dihedrals = np.degrees(np.arccos(dot_product))\n\n        # the next lines are to test the direction of rotation.\n        # is a dihedral really 90 or 270 degrees?\n        # Equivalent to direction of rotation of dihedral\n        where_to_modify = np.sum(BA * np.cross(n1, n2, axis=1), axis=1) > 0\n        where_to_modify = np.nonzero(where_to_modify)[0]\n\n        length = indices.shape[0] - start_row\n        sign = np.full(length, 1, dtype='float64')\n        to_add = np.full(length, 0, dtype='float64')\n        sign[where_to_modify] = -1\n        to_add[where_to_modify] = 360\n        dihedrals = to_add + sign * dihedrals\n        return dihedrals"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fragmentate(self, give_only_index=False,\n                    use_lookup=None):\n        \"\"\"Get the indices of non bonded parts in the molecule.\n\n        Args:\n            give_only_index (bool): If ``True`` a set of indices is returned.\n                Otherwise a new Cartesian instance.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            list: A list of sets of indices or new Cartesian instances.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        fragments = []\n        pending = set(self.index)\n        self.get_bonds(use_lookup=use_lookup)\n\n        while pending:\n            index = self.get_coordination_sphere(\n                pending.pop(), use_lookup=True, n_sphere=float('inf'),\n                only_surface=False, give_only_index=True)\n            pending = pending - index\n            if give_only_index:\n                fragments.append(index)\n            else:\n                fragment = self.loc[index]\n                fragment._metadata['bond_dict'] = fragment.restrict_bond_dict(\n                    self._metadata['bond_dict'])\n                try:\n                    fragment._metadata['val_bond_dict'] = (\n                        fragment.restrict_bond_dict(\n                            self._metadata['val_bond_dict']))\n                except KeyError:\n                    pass\n                fragments.append(fragment)\n        return fragments", "response": "Get the indices of non bonded parts in the molecule."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestrict a bond dictionary to self.", "response": "def restrict_bond_dict(self, bond_dict):\n        \"\"\"Restrict a bond dictionary to self.\n\n        Args:\n            bond_dict (dict): Look into :meth:`~chemcoord.Cartesian.get_bonds`,\n                to see examples for a bond_dict.\n\n        Returns:\n            bond dictionary\n        \"\"\"\n        return {j: bond_dict[j] & set(self.index) for j in self.index}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fragment(self, list_of_indextuples, give_only_index=False,\n                     use_lookup=None):\n        \"\"\"Get the indices of the atoms in a fragment.\n\n        The list_of_indextuples contains all bondings from the\n        molecule to the fragment. ``[(1,3), (2,4)]`` means for example that the\n        fragment is connected over two bonds. The first bond is from atom 1 in\n        the molecule to atom 3 in the fragment. The second bond is from atom\n        2 in the molecule to atom 4 in the fragment.\n\n        Args:\n            list_of_indextuples (list):\n            give_only_index (bool): If ``True`` a set of indices\n                is returned. Otherwise a new Cartesian instance.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            A set of indices or a new Cartesian instance.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        exclude = [tuple[0] for tuple in list_of_indextuples]\n        index_of_atom = list_of_indextuples[0][1]\n        fragment_index = self.get_coordination_sphere(\n            index_of_atom, exclude=set(exclude), n_sphere=float('inf'),\n            only_surface=False, give_only_index=True, use_lookup=use_lookup)\n        if give_only_index:\n            return fragment_index\n        else:\n            return self.loc[fragment_index, :]", "response": "Get the indices of the atoms in a fragment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning self without the specified fragments.", "response": "def get_without(self, fragments,\n                    use_lookup=None):\n        \"\"\"Return self without the specified fragments.\n\n        Args:\n            fragments: Either a list of :class:`~chemcoord.Cartesian` or a\n                :class:`~chemcoord.Cartesian`.\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            list: List containing :class:`~chemcoord.Cartesian`.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        if pd.api.types.is_list_like(fragments):\n            for fragment in fragments:\n                try:\n                    index_of_all_fragments |= fragment.index\n                except NameError:\n                    index_of_all_fragments = fragment.index\n        else:\n            index_of_all_fragments = fragments.index\n        missing_part = self.loc[self.index.difference(index_of_all_fragments)]\n        missing_part = missing_part.fragmentate(use_lookup=use_lookup)\n        return sorted(missing_part, key=len, reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _jit_pairwise_distances(pos1, pos2):\n        n1 = pos1.shape[0]\n        n2 = pos2.shape[0]\n        D = np.empty((n1, n2))\n\n        for i in range(n1):\n            for j in range(n2):\n                D[i, j] = np.sqrt(((pos1[i] - pos2[j])**2).sum())\n        return D", "response": "Optimized function for calculating the distance between each pair\n        of points in positions1 and positions2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the inertia tensor and transforms along the rotation axes and returns the inertia tensor and returns the inertia tensor a 4 - tuple.", "response": "def get_inertia(self):\n        \"\"\"Calculate the inertia tensor and transforms along\n        rotation axes.\n\n        This function calculates the inertia tensor and returns\n        a 4-tuple.\n\n        The unit is ``amu * length-unit-of-xyz-file**2``\n\n        Args:\n            None\n\n        Returns:\n            dict: The returned dictionary has four possible keys:\n\n            ``transformed_Cartesian``:\n            A :class:`~chemcoord.Cartesian`\n            that is transformed to the basis spanned by\n            the eigenvectors of the inertia tensor. The x-axis\n            is the axis with the lowest inertia moment, the\n            z-axis the one with the highest. Contains also a\n            column for the mass\n\n            ``diag_inertia_tensor``:\n            A vector containing the ascendingly sorted inertia moments after\n            diagonalization.\n\n            ``inertia_tensor``:\n            The inertia tensor in the old basis.\n\n            ``eigenvectors``:\n            The eigenvectors of the inertia tensor in the old basis.\n            Since the inertia_tensor is hermitian, they are orthogonal and\n            are returned as an orthonormal righthanded basis.\n            The i-th eigenvector corresponds to the i-th eigenvalue in\n            ``diag_inertia_tensor``.\n        \"\"\"\n        def calculate_inertia_tensor(molecule):\n            masses = molecule.loc[:, 'mass'].values\n            pos = molecule.loc[:, ['x', 'y', 'z']].values\n            inertia = np.sum(\n                masses[:, None, None]\n                * ((pos**2).sum(axis=1)[:, None, None]\n                   * np.identity(3)[None, :, :]\n                   - pos[:, :, None] * pos[:, None, :]),\n                axis=0)\n            diag_inertia, eig_v = np.linalg.eig(inertia)\n            sorted_index = np.argsort(diag_inertia)\n            diag_inertia = diag_inertia[sorted_index]\n            eig_v = eig_v[:, sorted_index]\n            return inertia, eig_v, diag_inertia\n\n        molecule = self.add_data('mass')\n        molecule = molecule - molecule.get_barycenter()\n        inertia, eig_v, diag_inertia = calculate_inertia_tensor(molecule)\n        eig_v = xyz_functions.orthonormalize_righthanded(eig_v)\n        molecule = molecule.basistransform(eig_v)\n        return {'transformed_Cartesian': molecule, 'eigenvectors': eig_v,\n                'diag_inertia_tensor': diag_inertia, 'inertia_tensor': inertia}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef basistransform(self, new_basis, old_basis=None,\n                       orthonormalize=True):\n        \"\"\"Transform the frame to a new basis.\n\n        This function transforms the cartesian coordinates from an\n        old basis to a new one. Please note that old_basis and\n        new_basis are supposed to have full Rank and consist of\n        three linear independent vectors. If rotate_only is True,\n        it is asserted, that both bases are orthonormal and right\n        handed. Besides all involved matrices are transposed\n        instead of inverted.\n        In some applications this may require the function\n        :func:`xyz_functions.orthonormalize` as a previous step.\n\n        Args:\n            old_basis (np.array):\n            new_basis (np.array):\n            rotate_only (bool):\n\n        Returns:\n            Cartesian: The transformed molecule.\n        \"\"\"\n        if old_basis is None:\n            old_basis = np.identity(3)\n\n        is_rotation_matrix = np.isclose(np.linalg.det(new_basis), 1)\n        if not is_rotation_matrix and orthonormalize:\n            new_basis = xyz_functions.orthonormalize_righthanded(new_basis)\n            is_rotation_matrix = True\n\n        if is_rotation_matrix:\n            return dot(np.dot(new_basis.T, old_basis), self)\n        else:\n            return dot(np.dot(np.linalg.inv(new_basis), old_basis), self)", "response": "This function transforms the frame from an old basis to a new basis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Cartesian with a column for the distance from origin.", "response": "def get_distance_to(self, origin=None, other_atoms=None, sort=False):\n        \"\"\"Return a Cartesian with a column for the distance from origin.\n        \"\"\"\n        if origin is None:\n            origin = np.zeros(3)\n        elif pd.api.types.is_list_like(origin):\n            origin = np.array(origin, dtype='f8')\n        else:\n            origin = self.loc[origin, ['x', 'y', 'z']]\n\n        if other_atoms is None:\n            other_atoms = self.index\n\n        new = self.loc[other_atoms, :].copy()\n        norm = np.linalg.norm\n        try:\n            new['distance'] = norm((new - origin).loc[:, ['x', 'y', 'z']],\n                                   axis=1)\n        except AttributeError:\n            # Happens if molecule consists of only one atom\n            new['distance'] = norm((new - origin).loc[:, ['x', 'y', 'z']])\n        if sort:\n            new.sort_values(by='distance', inplace=True)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_numbering(self, rename_dict, inplace=False):\n        output = self if inplace else self.copy()\n        new_index = [rename_dict.get(key, key) for key in self.index]\n        output.index = new_index\n        if not inplace:\n            return output", "response": "Returns a new object with the numbering changed according to the passed dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partition_chem_env(self, n_sphere=4,\n                           use_lookup=None):\n        \"\"\"This function partitions the molecule into subsets of the\n        same chemical environment.\n\n        A chemical environment is specified by the number of\n        surrounding atoms of a certain kind around an atom with a\n        certain atomic number represented by a tuple of a string\n        and a frozenset of tuples.\n        The ``n_sphere`` option determines how many branches the\n        algorithm follows to determine the chemical environment.\n\n        Example:\n        A carbon atom in ethane has bonds with three hydrogen (atomic\n        number 1) and one carbon atom (atomic number 6).\n        If ``n_sphere=1`` these are the only atoms we are\n        interested in and the chemical environment is::\n\n        ('C', frozenset([('H', 3), ('C', 1)]))\n\n        If ``n_sphere=2`` we follow every atom in the chemical\n        enviromment of ``n_sphere=1`` to their direct neighbours.\n        In the case of ethane this gives::\n\n        ('C', frozenset([('H', 6), ('C', 1)]))\n\n        In the special case of ethane this is the whole molecule;\n        in other cases you can apply this operation recursively and\n        stop after ``n_sphere`` or after reaching the end of\n        branches.\n\n\n        Args:\n            n_sphere (int):\n            use_lookup (bool): Use a lookup variable for\n                :meth:`~chemcoord.Cartesian.get_bonds`. The default is\n                specified in ``settings['defaults']['use_lookup']``\n\n        Returns:\n            dict: The output will look like this::\n\n                { (element_symbol, frozenset([tuples])) : set([indices]) }\n\n                A dictionary mapping from a chemical environment to\n                the set of indices of atoms in this environment.\n        \"\"\"\n        if use_lookup is None:\n            use_lookup = settings['defaults']['use_lookup']\n\n        def get_chem_env(self, i, n_sphere):\n            env_index = self.get_coordination_sphere(\n                i, n_sphere=n_sphere, only_surface=False,\n                give_only_index=True, use_lookup=use_lookup)\n            env_index.remove(i)\n            atoms = self.loc[env_index, 'atom']\n            environment = frozenset(collections.Counter(atoms).most_common())\n            return (self.loc[i, 'atom'], environment)\n\n        chemical_environments = collections.defaultdict(set)\n        for i in self.index:\n            chemical_environments[get_chem_env(self, i, n_sphere)].add(i)\n        return dict(chemical_environments)", "response": "This function partitions the molecule into subsets of the chemical environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nalign two Cartesians. Minimize the RMSD (root mean squared deviation) between ``self`` and ``other``. Returns a tuple of copies of ``self`` and ``other`` where both are centered around their centroid and ``other`` is rotated unto ``self``. The rotation minimises the distances between the atom pairs of same label. Uses the Kabsch algorithm implemented within :func:`~.xyz_functions.get_kabsch_rotation` .. note:: If ``indices is None``, then ``len(self) == len(other)`` must be true and the elements in each index have to be the same. Args: other (Cartesian): indices (sequence): It is possible to specify a subset of indices that is used for the determination of the best rotation matrix:: [[i1, i2,...], [j1, j2,...]] If ``indices`` is given in this form, the rotation matrix minimises the distance between ``i1`` and ``j1``, ``i2`` and ``j2`` and so on. ignore_hydrogens (bool): Returns: tuple:", "response": "def align(self, other, indices=None, ignore_hydrogens=False):\n        \"\"\"Align two Cartesians.\n\n\n\n        Minimize the RMSD (root mean squared deviation) between\n        ``self`` and ``other``.\n        Returns a tuple of copies of ``self`` and ``other`` where\n        both are centered around their centroid and\n        ``other`` is rotated unto ``self``.\n        The rotation minimises the distances between the\n        atom pairs of same label.\n        Uses the Kabsch algorithm implemented within\n        :func:`~.xyz_functions.get_kabsch_rotation`\n\n        .. note:: If ``indices is None``, then ``len(self) == len(other)``\n            must be true and the elements in each index have to be the same.\n\n\n        Args:\n            other (Cartesian):\n            indices (sequence): It is possible to specify a subset of indices\n                that is used for the determination of\n                the best rotation matrix::\n\n                    [[i1, i2,...], [j1, j2,...]]\n\n                If ``indices`` is given in this form, the rotation matrix\n                minimises the distance between ``i1`` and ``j1``,\n                ``i2`` and ``j2`` and so on.\n            ignore_hydrogens (bool):\n\n        Returns:\n            tuple:\n        \"\"\"\n        m1 = (self - self.get_centroid()).sort_index()\n        m2 = (other - other.get_centroid()).sort_index()\n        if indices is not None and ignore_hydrogens:\n            message = 'Indices != None and ignore_hydrogens == True is invalid'\n            raise IllegalArgumentCombination(message)\n        elif ignore_hydrogens:\n            m1 = m1[m1['atom'] != 'H']\n            m2 = m2[m2['atom'] != 'H']\n        elif indices is not None:\n            pos1 = m1.loc[indices[0], ['x', 'y', 'z']].values\n            pos2 = m2.loc[indices[1], ['x', 'y', 'z']].values\n        else:\n            pos1 = m1.loc[:, ['x', 'y', 'z']].values\n            pos2 = m2.loc[m1.index, ['x', 'y', 'z']].values\n        m2 = dot(xyz_functions.get_kabsch_rotation(pos1, pos2), m2)\n        return m1, m2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reindex_similar(self, other, n_sphere=4):\n        def make_subset_similar(m1, subset1, m2, subset2, index_dct):\n            \"\"\"Changes index_dct INPLACE\"\"\"\n            coords = ['x', 'y', 'z']\n            index1 = list(subset1)\n            for m1_i in index1:\n                dist_m2_to_m1_i = m2.get_distance_to(m1.loc[m1_i, coords],\n                                                     subset2, sort=True)\n\n                m2_i = dist_m2_to_m1_i.index[0]\n                dist_new = dist_m2_to_m1_i.loc[m2_i, 'distance']\n                m2_pos_i = dist_m2_to_m1_i.loc[m2_i, coords]\n\n                counter = itertools.count()\n                found = False\n                while not found:\n                    if m2_i in index_dct.keys():\n                        old_m1_pos = m1.loc[index_dct[m2_i], coords]\n                        if dist_new < np.linalg.norm(m2_pos_i - old_m1_pos):\n                            index1.append(index_dct[m2_i])\n                            index_dct[m2_i] = m1_i\n                            found = True\n                        else:\n                            m2_i = dist_m2_to_m1_i.index[next(counter)]\n                            dist_new = dist_m2_to_m1_i.loc[m2_i, 'distance']\n                            m2_pos_i = dist_m2_to_m1_i.loc[m2_i, coords]\n                    else:\n                        index_dct[m2_i] = m1_i\n                        found = True\n            return index_dct\n\n        molecule1 = self.copy()\n        molecule2 = other.copy()\n\n        partition1 = molecule1.partition_chem_env(n_sphere)\n        partition2 = molecule2.partition_chem_env(n_sphere)\n\n        index_dct = {}\n        for key in partition1:\n            message = ('You have chemically different molecules, regarding '\n                       'the topology of their connectivity.')\n            assert len(partition1[key]) == len(partition2[key]), message\n            index_dct = make_subset_similar(molecule1, partition1[key],\n                                            molecule2, partition2[key],\n                                            index_dct)\n        molecule2.index = [index_dct[i] for i in molecule2.index]\n        return molecule2.loc[molecule1.index]", "response": "Reindex self to be similarly indexed as other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_date(date_str):\n    if not date_str:\n        return None\n\n    try:\n        date = ciso8601.parse_datetime(date_str)\n        if not date:\n            date = arrow.get(date_str).datetime\n    except TypeError:\n        date = arrow.get(date_str[0]).datetime\n    return date", "response": "Parse elastic datetime string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dates(schema):\n    dates = [config.LAST_UPDATED, config.DATE_CREATED]\n    for field, field_schema in schema.items():\n        if field_schema['type'] == 'datetime':\n            dates.append(field)\n    return dates", "response": "Return list of datetime fields for given schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats given doc to match given schema.", "response": "def format_doc(hit, schema, dates):\n    \"\"\"Format given doc to match given schema.\"\"\"\n    doc = hit.get('_source', {})\n    doc.setdefault(config.ID_FIELD, hit.get('_id'))\n    doc.setdefault('_type', hit.get('_type'))\n    if hit.get('highlight'):\n        doc['es_highlight'] = hit.get('highlight')\n\n    if hit.get('inner_hits'):\n        doc['_inner_hits'] = {}\n        for key, value in hit.get('inner_hits').items():\n            doc['_inner_hits'][key] = []\n            for item in value.get('hits', {}).get('hits', []):\n                doc['_inner_hits'][key].append(item.get('_source', {}))\n\n    for key in dates:\n        if key in doc:\n            doc[key] = parse_date(doc[key])\n\n    return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_filters(query, base_filters):\n    filters = [f for f in base_filters if f is not None]\n    query_filter = query['query']['filtered'].get('filter', None)\n    if query_filter is not None:\n        if 'and' in query_filter:\n            filters.extend(query_filter['and'])\n        else:\n            filters.append(query_filter)\n    if filters:\n        query['query']['filtered']['filter'] = {'and': filters}", "response": "Put together all filters we have and set them as and filter\n    within filtered query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating elasticsearch client instance.", "response": "def get_es(url, **kwargs):\n    \"\"\"Create elasticsearch client instance.\n\n    :param url: elasticsearch url\n    \"\"\"\n    urls = [url] if isinstance(url, str) else url\n    kwargs.setdefault('serializer', ElasticJSONSerializer())\n    es = elasticsearch.Elasticsearch(urls, **kwargs)\n    return es"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild an elastic search query from a document object.", "response": "def build_elastic_query(doc):\n    \"\"\"\n    Build a query which follows ElasticSearch syntax from doc.\n\n    1. Converts {\"q\":\"cricket\"} to the below elastic query::\n\n        {\n            \"query\": {\n                \"filtered\": {\n                    \"query\": {\n                        \"query_string\": {\n                            \"query\": \"cricket\",\n                            \"lenient\": false,\n                            \"default_operator\": \"AND\"\n                        }\n                    }\n                }\n            }\n        }\n\n    2. Converts a faceted query::\n\n        {\"q\":\"cricket\", \"type\":['text'], \"source\": \"AAP\"}\n\n    to the below elastic query::\n\n        {\n            \"query\": {\n                \"filtered\": {\n                    \"filter\": {\n                        \"and\": [\n                            {\"terms\": {\"type\": [\"text\"]}},\n                            {\"term\": {\"source\": \"AAP\"}}\n                        ]\n                    },\n                    \"query\": {\n                        \"query_string\": {\n                            \"query\": \"cricket\",\n                            \"lenient\": false,\n                            \"default_operator\": \"AND\"\n                        }\n                    }\n                }\n            }\n        }\n\n    :param doc: A document object which is inline with the syntax specified in the examples.\n                It's the developer responsibility to pass right object.\n    :returns ElasticSearch query\n    \"\"\"\n    elastic_query, filters = {\"query\": {\"filtered\": {}}}, []\n\n    for key in doc.keys():\n        if key == 'q':\n            elastic_query['query']['filtered']['query'] = _build_query_string(doc['q'])\n        else:\n            _value = doc[key]\n            filters.append({\"terms\": {key: _value}} if isinstance(_value, list) else {\"term\": {key: _value}})\n\n    set_filters(elastic_query, filters)\n    return elastic_query"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_query_string(q, default_field=None, default_operator='AND'):\n    def _is_phrase_search(query_string):\n        clean_query = query_string.strip()\n        return clean_query and clean_query.startswith('\"') and clean_query.endswith('\"')\n\n    def _get_phrase(query_string):\n        return query_string.strip().strip('\"')\n\n    if _is_phrase_search(q):\n        query = {'match_phrase': {'_all': _get_phrase(q)}}\n    else:\n        query = {'query_string': {'query': q, 'default_operator': default_operator}}\n        query['query_string'].update({'lenient': False} if default_field else {'default_field': default_field})\n\n    return query", "response": "Build query_string object from q."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts mongo. ObjectId to string.", "response": "def default(self, value):\n        \"\"\"Convert mongo.ObjectId.\"\"\"\n        if isinstance(value, ObjectId):\n            return str(value)\n        return super(ElasticJSONSerializer, self).default(value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd extra info to response.", "response": "def extra(self, response):\n        \"\"\"Add extra info to response.\"\"\"\n        if 'facets' in self.hits:\n            response['_facets'] = self.hits['facets']\n        if 'aggregations' in self.hits:\n            response['_aggregations'] = self.hits['aggregations']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate indexes and put mapping.", "response": "def init_index(self, app=None):\n        \"\"\"Create indexes and put mapping.\"\"\"\n        elasticindexes = self._get_indexes()\n\n        for index, settings in elasticindexes.items():\n            es = settings['resource']\n            if not es.indices.exists(index):\n                self.create_index(index, settings.get('index_settings'), es)\n                continue\n            else:\n                self.put_settings(app, index, settings.get('index_settings').get('settings'), es)\n\n            for mapping_type, mappings in settings.get('index_settings', {}).get('mappings').items():\n                self._put_resource_mapping(mapping_type, es,\n                                           properties=mappings,\n                                           index=index, doc_type=mapping_type)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_indexes(self):\n        indexes = {}\n        for resource in self._get_elastic_resources():\n            try:\n                index = self._resource_index(resource)\n\n            except KeyError:  # ignore missing\n                continue\n\n            if index not in indexes:\n                indexes.update({\n                    index: {\n                        'resource': self.elastic(resource),\n                        'index_settings': {\n                            'mappings': {}\n                        }\n                    }\n                })\n\n                settings = self._resource_config(resource, 'SETTINGS')\n                if settings:\n                    indexes[index]['index_settings'].update(settings)\n\n            resource_config = self.app.config['DOMAIN'][resource]\n            properties = self._get_mapping_properties(resource_config, parent=self._get_parent_type(resource))\n            datasource = self.get_datasource(resource)\n            indexes[index]['index_settings']['mappings'][datasource[0]] = properties\n\n        return indexes", "response": "Returns the index definition for the elastic resources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting mapping for given resource or item schema.", "response": "def _get_mapping(self, schema):\n        \"\"\"Get mapping for given resource or item schema.\n\n        :param schema: resource or dict/list type item schema\n        \"\"\"\n        properties = {}\n        for field, field_schema in schema.items():\n            field_mapping = self._get_field_mapping(field_schema)\n            if field_mapping:\n                properties[field] = field_mapping\n        return {'properties': properties}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_field_mapping(self, schema):\n        if 'mapping' in schema:\n            return schema['mapping']\n        elif schema['type'] == 'dict' and 'schema' in schema:\n            return self._get_mapping(schema['schema'])\n        elif schema['type'] == 'list' and 'schema' in schema.get('schema', {}):\n            return self._get_mapping(schema['schema']['schema'])\n        elif schema['type'] == 'datetime':\n            return {'type': 'date'}\n        elif schema['type'] == 'string' and schema.get('unique'):\n            return {'type': 'string', 'index': 'not_analyzed'}", "response": "Get mapping for single field schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_index(self, index=None, settings=None, es=None):\n        if index is None:\n            index = self.index\n        if es is None:\n            es = self.es\n        try:\n            alias = index\n            index = generate_index_name(alias)\n\n            args = {'index': index}\n            if settings:\n                args['body'] = settings\n\n            es.indices.create(**args)\n            es.indices.put_alias(index, alias)\n            logger.info('created index alias=%s index=%s' % (alias, index))\n        except elasticsearch.TransportError:  # index exists\n            pass", "response": "Create new index and ignore if it exists already."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put_mapping(self, app, index=None):\n        for resource, resource_config in self._get_elastic_resources().items():\n            datasource = resource_config.get('datasource', {})\n\n            if not is_elastic(datasource):\n                continue\n\n            if datasource.get('source', resource) != resource:  # only put mapping for core types\n                continue\n\n            properties = self._get_mapping_properties(resource_config)\n\n            kwargs = {\n                'index': index or self._resource_index(resource),\n                'doc_type': resource,\n                'body': properties,\n            }\n\n            try:\n                self.elastic(resource).indices.put_mapping(**kwargs)\n            except elasticsearch.exceptions.RequestError:\n                logger.exception('mapping error, updating settings resource=%s' % resource)", "response": "Put mapping for elasticsearch for current schema."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the mapping for the current node s ID.", "response": "def get_mapping(self, index, doc_type=None):\n        \"\"\"Get mapping for index.\n\n        :param index: index name\n        \"\"\"\n        mapping = self.es.indices.get_mapping(index=index, doc_type=doc_type)\n        return next(iter(mapping.values()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the settings for the given index.", "response": "def get_settings(self, index):\n        \"\"\"Get settings for index.\n\n        :param index: index name\n        \"\"\"\n        settings = self.es.indices.get_settings(index=index)\n        return next(iter(settings.values()))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_index_by_alias(self, alias):\n        try:\n            info = self.es.indices.get_alias(name=alias)\n            return next(iter(info.keys()))\n        except elasticsearch.exceptions.NotFoundError:\n            return alias", "response": "Get index name for given alias."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding documents for resource.", "response": "def find(self, resource, req, sub_resource_lookup):\n        \"\"\"Find documents for resource.\"\"\"\n        args = getattr(req, 'args', request.args if request else {}) or {}\n        source_config = config.SOURCES[resource]\n\n        if args.get('source'):\n            query = json.loads(args.get('source'))\n            if 'filtered' not in query.get('query', {}):\n                _query = query.get('query')\n                query['query'] = {'filtered': {}}\n                if _query:\n                    query['query']['filtered']['query'] = _query\n        else:\n            query = {'query': {'filtered': {}}}\n\n        if args.get('q', None):\n            query['query']['filtered']['query'] = _build_query_string(args.get('q'),\n                                                                      default_field=args.get('df', '_all'),\n                                                                      default_operator=args.get('default_operator', 'OR'))\n\n        if 'sort' not in query:\n            if req.sort:\n                sort = ast.literal_eval(req.sort)\n                set_sort(query, sort)\n            elif self._default_sort(resource) and 'query' not in query['query']['filtered']:\n                set_sort(query, self._default_sort(resource))\n\n        if req.max_results:\n            query.setdefault('size', req.max_results)\n\n        if req.page > 1:\n            query.setdefault('from', (req.page - 1) * req.max_results)\n\n        filters = []\n        filters.append(source_config.get('elastic_filter'))\n        filters.append(source_config.get('elastic_filter_callback', noop)())\n        filters.append({'and': _build_lookup_filter(sub_resource_lookup)} if sub_resource_lookup else None)\n        filters.append(json.loads(args.get('filter')) if 'filter' in args else None)\n        filters.extend(args.get('filters') if 'filters' in args else [])\n\n        if req.where:\n            try:\n                filters.append({'term': json.loads(req.where)})\n            except ValueError:\n                try:\n                    filters.append({'term': parse(req.where)})\n                except ParseError:\n                    abort(400)\n\n        set_filters(query, filters)\n\n        if 'facets' in source_config:\n            query['facets'] = source_config['facets']\n\n        if 'aggregations' in source_config and self.should_aggregate(req):\n            query['aggs'] = source_config['aggregations']\n\n        if 'es_highlight' in source_config and self.should_highlight(req):\n            query_string = query['query'].get('filtered', {}).get('query', {}).get('query_string')\n            highlights = source_config.get('es_highlight', noop)(query_string)\n\n            if highlights:\n                query['highlight'] = highlights\n                query['highlight'].setdefault('require_field_match', False)\n\n        source_projections = None\n        if self.should_project(req):\n            source_projections = self.get_projected_fields(req)\n\n        args = self._es_args(resource, source_projections=source_projections)\n        try:\n            hits = self.elastic(resource).search(body=query, **args)\n        except elasticsearch.exceptions.RequestError as e:\n            if e.status_code == 400 and \"No mapping found for\" in e.error:\n                hits = {}\n            elif e.status_code == 400 and 'SearchParseException' in e.error:\n                raise InvalidSearchString\n            else:\n                raise\n\n        return self._parse_hits(hits, resource)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef should_aggregate(self, req):\n        try:\n            return self.app.config.get('ELASTICSEARCH_AUTO_AGGREGATIONS') or \\\n                   bool(req.args and int(req.args.get('aggregations')))\n        except (AttributeError, TypeError):\n            return False", "response": "Check the environment variable and the given argument parameter to decide if aggregations needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef should_highlight(self, req):\n        try:\n            return bool(req.args and int(req.args.get('es_highlight', 0)))\n        except (AttributeError, TypeError):\n            return False", "response": "Check the given argument parameter to decide if highlights needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the given argument parameter to decide if projections needed.", "response": "def should_project(self, req):\n        \"\"\"\n        Check the given argument parameter to decide if projections needed.\n\n        argument value is expected to be a list of strings\n        \"\"\"\n        try:\n            return req.args and json.loads(req.args.get('projections', []))\n        except (AttributeError, TypeError):\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_projected_fields(self, req):\n        try:\n            args = getattr(req, 'args', {})\n            return ','.join(json.loads(args.get('projections')))\n        except (AttributeError, TypeError):\n            return None", "response": "Returns the projected fields from request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_one(self, resource, req, **lookup):\n\n        if config.ID_FIELD in lookup:\n            return self._find_by_id(resource=resource, _id=lookup[config.ID_FIELD], parent=lookup.get('parent'))\n        else:\n            args = self._es_args(resource)\n            filters = [{'term': {key: val}} for key, val in lookup.items()]\n            query = {'query': {'constant_score': {'filter': {'and': filters}}}}\n\n            try:\n                args['size'] = 1\n                hits = self.elastic(resource).search(body=query, **args)\n                docs = self._parse_hits(hits, resource)\n                return docs.first()\n            except elasticsearch.NotFoundError:\n                return", "response": "Find single document in elasticsearch"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_by_id(self, resource, _id, parent=None):\n        def is_found(hit):\n            if 'exists' in hit:\n                hit['found'] = hit['exists']\n            return hit.get('found', False)\n\n        args = self._es_args(resource)\n        try:\n            # set the parent if available\n            if parent:\n                args['parent'] = parent\n\n            hit = self.elastic(resource).get(id=_id, **args)\n\n            if not is_found(hit):\n                return\n\n            docs = self._parse_hits({'hits': {'hits': [hit]}}, resource)\n            return docs.first()\n\n        except elasticsearch.NotFoundError:\n            return\n        except elasticsearch.TransportError as tex:\n            if tex.error == 'routing_missing_exception' or 'RoutingMissingException' in tex.error:\n                # search for the item\n                args = self._es_args(resource)\n                query = {'query': {'bool': {'must': [{'term': {'_id': _id}}]}}}\n                try:\n                    args['size'] = 1\n                    hits = self.elastic(resource).search(body=query, **args)\n                    docs = self._parse_hits(hits, resource)\n                    return docs.first()\n                except elasticsearch.NotFoundError:\n                    return", "response": "Find the document by Id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_one_raw(self, resource, _id):\n        return self._find_by_id(resource=resource, _id=_id)", "response": "Find a single document by id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_list_of_ids(self, resource, ids, client_projection=None):\n        args = self._es_args(resource)\n        return self._parse_hits(self.elastic(resource).mget(body={'ids': ids}, **args), resource)", "response": "Find documents by ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert(self, resource, doc_or_docs, **kwargs):\n        ids = []\n        kwargs.update(self._es_args(resource))\n        for doc in doc_or_docs:\n            self._update_parent_args(resource, kwargs, doc)\n            _id = doc.pop('_id', None)\n            res = self.elastic(resource).index(body=doc, id=_id, **kwargs)\n            doc.setdefault('_id', res.get('_id', _id))\n            ids.append(doc.get('_id'))\n        self._refresh_resource_index(resource)\n        return ids", "response": "Insert document into elasticsearch index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating document in index.", "response": "def update(self, resource, id_, updates):\n        \"\"\"Update document in index.\"\"\"\n        args = self._es_args(resource, refresh=True)\n        if self._get_retry_on_conflict():\n            args['retry_on_conflict'] = self._get_retry_on_conflict()\n\n        updates.pop('_id', None)\n        updates.pop('_type', None)\n        self._update_parent_args(resource, args, updates)\n        return self.elastic(resource).update(id=id_, body={'doc': updates}, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace(self, resource, id_, document):\n        args = self._es_args(resource, refresh=True)\n        document.pop('_id', None)\n        document.pop('_type', None)\n        self._update_parent_args(resource, args, document)\n        return self.elastic(resource).index(body=document, id=id_, **args)", "response": "Replace document in index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving docs for resource.", "response": "def remove(self, resource, lookup=None, parent=None, **kwargs):\n        \"\"\"Remove docs for resource.\n\n        :param resource: resource name\n        :param lookup: filter\n        :param parent: parent id\n        \"\"\"\n        kwargs.update(self._es_args(resource))\n        if parent:\n            kwargs['parent'] = parent\n\n        if lookup:\n            if lookup.get('_id'):\n                try:\n                    return self.elastic(resource).delete(id=lookup.get('_id'), refresh=True, **kwargs)\n                except elasticsearch.NotFoundError:\n                    return\n        return ValueError('there must be `lookup._id` specified')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_empty(self, resource):\n        args = self._es_args(resource)\n        res = self.elastic(resource).count(body={'query': {'match_all': {}}}, **args)\n        return res.get('count', 0) == 0", "response": "Test if there is no document for resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmodifying index settings. Index must exist already.", "response": "def put_settings(self, app=None, index=None, settings=None, es=None):\n        \"\"\"Modify index settings.\n\n        Index must exist already.\n        \"\"\"\n        if not index:\n            index = self.index\n\n        if not app:\n            app = self.app\n\n        if not es:\n            es = self.es\n\n        if not settings:\n            return\n\n        for alias, old_settings in self.es.indices.get_settings(index=index).items():\n            try:\n                if test_settings_contain(old_settings['settings']['index'], settings['settings']):\n                    return\n            except KeyError:\n                pass\n\n        es.indices.close(index=index)\n        es.indices.put_settings(index=index, body=settings)\n        es.indices.open(index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_hits(self, hits, resource):\n        datasource = self.get_datasource(resource)\n        schema = {}\n        schema.update(config.DOMAIN[datasource[0]].get('schema', {}))\n        schema.update(config.DOMAIN[resource].get('schema', {}))\n        dates = get_dates(schema)\n        docs = []\n        for hit in hits.get('hits', {}).get('hits', []):\n            docs.append(format_doc(hit, schema, dates))\n        return ElasticCursor(hits, docs)", "response": "Parse hits response into documents."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _es_args(self, resource, refresh=None, source_projections=None):\n        datasource = self.get_datasource(resource)\n        args = {\n            'index': self._resource_index(resource),\n            'doc_type': datasource[0],\n        }\n        if source_projections:\n            args['_source'] = source_projections\n        if refresh:\n            args['refresh'] = refresh\n\n        return args", "response": "Get index and doctype args."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the Parent Id of the document containing the parent id", "response": "def get_parent_id(self, resource, document):\n        \"\"\"Get the Parent Id of the document\n\n        :param resource: resource name\n        :param document: document containing the parent id\n        \"\"\"\n        parent_type = self._get_parent_type(resource)\n        if parent_type and document:\n            return document.get(parent_type.get('field'))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets projection fields for given resource.", "response": "def _fields(self, resource):\n        \"\"\"Get projection fields for given resource.\"\"\"\n        datasource = self.get_datasource(resource)\n        keys = datasource[2].keys()\n        return ','.join(keys) + ','.join([config.LAST_UPDATED, config.DATE_CREATED])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget index for given resource.", "response": "def _resource_index(self, resource):\n        \"\"\"Get index for given resource.\n\n        by default it will be `self.index`, but it can be overriden via app.config\n\n        :param resource: resource name\n        \"\"\"\n        datasource = self.get_datasource(resource)\n        indexes = self._resource_config(resource, 'INDEXES') or {}\n        default_index = self._resource_config(resource, 'INDEX')\n        return indexes.get(datasource[0], default_index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _refresh_resource_index(self, resource):\n        if self._resource_config(resource, 'FORCE_REFRESH', True):\n            self.elastic(resource).indices.refresh(self._resource_index(resource))", "response": "Refresh the index for given resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting elastic prefix for given resource.", "response": "def _resource_prefix(self, resource=None):\n        \"\"\"Get elastic prefix for given resource.\n\n        Resource can specify ``elastic_prefix`` which behaves same like ``mongo_prefix``.\n        \"\"\"\n        px = 'ELASTICSEARCH'\n        if resource and config.DOMAIN[resource].get('elastic_prefix'):\n            px = config.DOMAIN[resource].get('elastic_prefix')\n        return px"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting config using resource elastic prefix if any.", "response": "def _resource_config(self, resource=None, key=None, default=None):\n        \"\"\"Get config using resource elastic prefix (if any).\"\"\"\n        px = self._resource_prefix(resource)\n        return self.app.config.get('%s_%s' % (px, key), default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget ElasticSearch instance for given resource.", "response": "def elastic(self, resource=None):\n        \"\"\"Get ElasticSearch instance for given resource.\"\"\"\n        px = self._resource_prefix(resource)\n\n        if px not in self.elastics:\n            url = self._resource_config(resource, 'URL')\n            assert url, 'no url for %s' % px\n            self.elastics[px] = get_es(url, **self.kwargs)\n\n        return self.elastics[px]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the MD5 checksum of a file.", "response": "def get_md5sum(fname, chunk_size=1024):\n    \"\"\"\n    Returns the MD5 checksum of a file.\n\n    Args:\n        fname (str): Filename\n        chunk_size (Optional[int]): Size (in Bytes) of the chunks that should be\n            read in at once. Increasing chunk size reduces the number of reads\n            required, but increases the memory usage. Defaults to 1024.\n\n    Returns:\n        The MD5 checksum of the file, which is a string.\n    \"\"\"\n\n    def iter_chunks(f):\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk\n\n    sig = hashlib.md5()\n\n    with open(fname, 'rb') as f:\n        for chunk in iter_chunks(f):\n            sig.update(chunk)\n\n        # data = f.read()\n        # return hashlib.md5(data).hexdigest()\n\n    return sig.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef h5_file_exists(fname, size_guess=None, rtol=0.1, atol=1., dsets={}):\n    # Check if the file exists\n    if not os.path.isfile(fname):\n        # print('File does not exist.')\n        return False\n\n    # Check file size, withe the given tolerances\n    if size_guess is not None:\n        size = os.path.getsize(fname)\n        tol = atol + rtol * size_guess\n\n        if abs(size - size_guess) > tol:\n            # print('File size is wrong:')\n            # print('  expected: {: >16d}'.format(size_guess))\n            # print('     found: {: >16d}'.format(size))\n            return False\n\n    # Check the datasets in the file\n    if len(dsets):\n        import h5py\n        try:\n            with h5py.File(fname, 'r') as f:\n                for key in dsets:\n                    # Check that dataset is in file\n                    if key not in f:\n                        # print('Dataset \"{}\" not in file.'.format(key))\n                        return False\n                    # Check that the shape of the dataset is correct\n                    if dsets[key] is not None:\n                        if f[key].shape != dsets[key]:\n                            # print('Dataset \"{}\" has wrong shape:'.format(key))\n                            # print('  expected: {}'.format(dsets[key]))\n                            # print('     found: {}'.format(f[key].shape))\n                            return False\n        except IOError:\n            # print('Problem reading file.')\n            return False\n\n    return True", "response": "Checks if an HDF5 file exists and has the expected file size and the expected shape of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading a file and verify the MD5 sum.", "response": "def download_and_verify(url, md5sum, fname=None,\n                        chunk_size=1024, clobber=False,\n                        verbose=True):\n    \"\"\"\n    Download a file and verify the MD5 sum.\n\n    Args:\n        url (str): The URL to download.\n        md5sum (str): The expected MD5 sum.\n        fname (Optional[str]): The filename to store the downloaded file in.\n            If `None`, infer the filename from the URL. Defaults to `None`.\n        chunk_size (Optional[int]): Process in chunks of this size (in Bytes).\n            Defaults to 1024.\n        clobber (Optional[bool]): If `True`, any existing, identical file will\n            be overwritten. If `False`, the MD5 sum of any existing file with\n            the destination filename will be checked. If the MD5 sum does not\n            match, the existing file will be overwritten. Defaults to `False`.\n        verbose (Optional[bool]): If `True` (the default), then a progress bar\n            will be shownd during downloads.\n\n    Returns:\n        The filename the URL was downloaded to.\n\n    Raises:\n        DownloadError: The MD5 sum of the downloaded file does not match\n            `md5sum`.\n        requests.exceptions.HTTPError: There was a problem connecting to the\n            URL.\n    \"\"\"\n\n    # Determine the filename\n    if fname is None:\n        fname = url.split('/')[-1]\n\n    # Check if the file already exists on disk\n    if (not clobber) and os.path.isfile(fname):\n        print('Checking existing file to see if MD5 sum matches ...')\n        md5_existing = get_md5sum(fname, chunk_size=chunk_size)\n        if md5_existing == md5sum:\n            print('File exists. Not overwriting.')\n            return fname\n\n    # Make sure the directory it's going into exists\n    dir_name = os.path.dirname(fname)\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\n    sig = hashlib.md5()\n\n    if verbose:\n        print('Downloading {} ...'.format(url))\n\n    if url.startswith('http://') or url.startswith('https://'):\n        # Stream the URL as a file, copying to local disk\n        with contextlib.closing(requests.get(url, stream=True)) as r:\n            try:\n                r.raise_for_status()\n            except requests.exceptions.HTTPError as error:\n                print('Error connecting to URL: \"{}\"'.format(url))\n                print(r.text)\n                raise error\n\n            with open(fname, 'wb') as f:\n                content_length = r.headers.get('content-length')\n                if content_length is not None:\n                    content_length = int(content_length)\n                bar = FileTransferProgressBar(content_length)\n\n                for k,chunk in enumerate(r.iter_content(chunk_size=chunk_size)):\n                    f.write(chunk)\n                    sig.update(chunk)\n\n                    if verbose:\n                        bar_val = chunk_size*(k+1)\n                        if content_length is not None:\n                            bar_val = min(bar_val, content_length)\n                        bar.update(bar_val)\n    else: # e.g., ftp://\n        with contextlib.closing(urlopen(url)) as r:\n            content_length = r.headers.get('content-length')\n            if content_length is not None:\n                content_length = int(content_length)\n            bar = FileTransferProgressBar(content_length)\n\n            with open(fname, 'wb') as f:\n                k = 0\n                while True:\n                    chunk = r.read(chunk_size)\n\n                    if not chunk:\n                        break\n\n                    f.write(chunk)\n                    sig.update(chunk)\n\n                    if verbose:\n                        k += 1\n                        bar_val = chunk_size*k\n                        if content_length is not None:\n                            bar_val = min(bar_val, content_length)\n                        bar.update(bar_val)\n\n\n    if sig.hexdigest() != md5sum:\n        raise DownloadError('The MD5 sum of the downloaded file is incorrect.\\n'\n                            + '  download: {}\\n'.format(sig.hexdigest())\n                            + '  expected: {}\\n'.format(md5sum))\n\n    return fname"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download(url, fname=None):\n    # Determine the filename\n    if fname is None:\n        fname = url.split('/')[-1]\n\n    # Stream the URL as a file, copying to local disk\n    with contextlib.closing(requests.get(url, stream=True)) as r:\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as error:\n            print('Error connecting to URL: \"{}\"'.format(url))\n            print(r.text)\n            raise error\n\n        with open(fname, 'wb') as f:\n            shutil.copyfileobj(r.raw, f)\n\n    return fname", "response": "Downloads a file from the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching metadata pertaining to a Digital Object Identifier (DOI) in the Harvard Dataverse. Args: doi (str): The Digital Object Identifier (DOI) of the entry in the Dataverse. Raises: requests.exceptions.HTTPError: The given DOI does not exist, or there was a problem connecting to the Dataverse.", "response": "def dataverse_search_doi(doi):\n    \"\"\"\n    Fetches metadata pertaining to a Digital Object Identifier (DOI) in the\n    Harvard Dataverse.\n\n    Args:\n        doi (str): The Digital Object Identifier (DOI) of the entry in the\n            Dataverse.\n\n    Raises:\n        requests.exceptions.HTTPError: The given DOI does not exist, or there\n            was a problem connecting to the Dataverse.\n    \"\"\"\n\n    url = '{}/api/datasets/:persistentId?persistentId=doi:{}'.format(dataverse, doi)\n    r = requests.get(url)\n\n    try:\n        r.raise_for_status()\n    except requests.exceptions.HTTPError as error:\n        print('Error looking up DOI \"{}\" in the Harvard Dataverse.'.format(doi))\n        print(r.text)\n        raise error\n\n    return json.loads(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataverse_download_doi(doi,\n                           local_fname=None,\n                           file_requirements={},\n                           clobber=False):\n    \"\"\"\n    Downloads a file from the Dataverse, using a DOI and set of metadata\n    parameters to locate the file.\n\n    Args:\n        doi (str): Digital Object Identifier (DOI) containing the file.\n        local_fname (Optional[str]): Local filename to download the file to. If\n            `None`, then use the filename provided by the Dataverse. Defaults to\n            `None`.\n        file_requirements (Optional[dict]): Select the file containing the\n            given metadata entries. If multiple files meet these requirements,\n            only the first in downloaded. Defaults to `{}`, corresponding to no\n            requirements.\n\n    Raises:\n        DownloadError: Either no matching file was found under the given DOI, or\n            the MD5 sum of the file was not as expected.\n        requests.exceptions.HTTPError: The given DOI does not exist, or there\n            was a problem connecting to the Dataverse.\n\n    \"\"\"\n    metadata = dataverse_search_doi(doi)\n\n    def requirements_match(metadata):\n        for key in file_requirements.keys():\n            if metadata['dataFile'].get(key, None) != file_requirements[key]:\n                return False\n        return True\n\n    for file_metadata in metadata['data']['latestVersion']['files']:\n        if requirements_match(file_metadata):\n            file_id = file_metadata['dataFile']['id']\n            md5sum = file_metadata['dataFile']['md5']\n\n            if local_fname is None:\n                local_fname = file_metadata['dataFile']['filename']\n\n            # Check if the file already exists on disk\n            if (not clobber) and os.path.isfile(local_fname):\n                print('Checking existing file to see if MD5 sum matches ...')\n                md5_existing = get_md5sum(local_fname)\n                if md5_existing == md5sum:\n                    print('File exists. Not overwriting.')\n                    return\n\n            print(\"Downloading data to '{}' ...\".format(local_fname))\n\n            dataverse_download_id(file_id, md5sum,\n                                  fname=local_fname, clobber=False)\n\n            return\n\n    raise DownloadError(\n        'No file found under the given DOI matches the requirements.\\n'\n        'The metadata found for this DOI was:\\n'\n        + json.dumps(file_metadata, indent=2, sort_keys=True))", "response": "Downloads a file from the Dataverse using a DOI and returns the URL of the downloaded file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize a numpy. dtype object into a dictionary that can be passed to json. dumps.", "response": "def serialize_dtype(o):\n    \"\"\"\n    Serializes a :obj:`numpy.dtype`.\n\n    Args:\n        o (:obj:`numpy.dtype`): :obj:`dtype` to be serialized.\n\n    Returns:\n        A dictionary that can be passed to :obj:`json.dumps`.\n    \"\"\"\n    if len(o) == 0:\n        return dict(\n            _type='np.dtype',\n            descr=str(o))\n    return dict(\n        _type='np.dtype',\n        descr=o.descr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize a numpy. ndarray into a dictionary where the datatype and shape are base64 encoded.", "response": "def serialize_ndarray_b64(o):\n    \"\"\"\n    Serializes a :obj:`numpy.ndarray` in a format where the datatype and shape are\n    human-readable, but the array data itself is binary64 encoded.\n\n    Args:\n        o (:obj:`numpy.ndarray`): :obj:`ndarray` to be serialized.\n\n    Returns:\n        A dictionary that can be passed to :obj:`json.dumps`.\n    \"\"\"\n    if o.flags['C_CONTIGUOUS']:\n        o_data = o.data\n    else:\n        o_data = np.ascontiguousarray(o).data\n    data_b64 = base64.b64encode(o_data)\n    return dict(\n        _type='np.ndarray',\n        data=data_b64.decode('utf-8'),\n        dtype=o.dtype,\n        shape=o.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hint_tuples(o):\n    if isinstance(o, tuple):\n        return dict(_type='tuple', items=o)\n    elif isinstance(o, list):\n        return [hint_tuples(el) for el in o]\n    else:\n        return o", "response": "Takes a list of tuples and returns a dict of tuples that can be reconstructed during deserialization."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize_ndarray_readable(o):\n    return dict(\n        _type='np.ndarray',\n        dtype=o.dtype,\n        value=hint_tuples(o.tolist()))", "response": "Serializes a numpy. ndarray into a human - readable format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize_ndarray_npy(o):\n    with io.BytesIO() as f:\n        np.save(f, o)\n        f.seek(0)\n        serialized = json.dumps(f.read().decode('latin-1'))\n    return dict(\n        _type='np.ndarray',\n        npy=serialized)", "response": "Serializes a numpy. ndarray using numpy s built - in save function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deserialize_ndarray_npy(d):\n    with io.BytesIO() as f:\n        f.write(json.loads(d['npy']).encode('latin-1'))\n        f.seek(0)\n        return np.load(f)", "response": "Deserializes a JSONified numpy. ndarray that was created using numpy s save function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deserialize_ndarray(d):\n    if 'data' in d:\n        x = np.fromstring(\n            base64.b64decode(d['data']),\n            dtype=d['dtype'])\n        x.shape = d['shape']\n        return x\n    elif 'value' in d:\n        return np.array(d['value'], dtype=d['dtype'])\n    elif 'npy' in d:\n        return deserialize_ndarray_npy(d)\n    else:\n        raise ValueError('Malformed np.ndarray encoding.')", "response": "Deserializes a JSONified np. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize_quantity(o):\n    return dict(\n        _type='astropy.units.Quantity',\n        value=o.value,\n        unit=o.unit.to_string())", "response": "Serializes an astropy. units. Quantity to a dictionary for JSONification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize_skycoord(o):\n    representation = o.representation.get_name()\n    frame = o.frame.name\n\n    r = o.represent_as('spherical')\n\n    d = dict(\n        _type='astropy.coordinates.SkyCoord',\n        frame=frame,\n        representation=representation,\n        lon=r.lon,\n        lat=r.lat)\n\n    if len(o.distance.unit.to_string()):\n        d['distance'] = r.distance\n\n    return d", "response": "Serializes an astropy. coordinates. SkyCoord object to a dictionary that can be passed to json. dumps."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a JSON encoder that can serialize the contents of the array into a new version of the base class.", "response": "def get_encoder(ndarray_mode='b64'):\n    \"\"\"\n    Returns a JSON encoder that can handle:\n        * :obj:`numpy.ndarray`\n        * :obj:`numpy.floating` (converted to :obj:`float`)\n        * :obj:`numpy.integer` (converted to :obj:`int`)\n        * :obj:`numpy.dtype`\n        * :obj:`astropy.units.Quantity`\n        * :obj:`astropy.coordinates.SkyCoord`\n\n    Args:\n        ndarray_mode (Optional[:obj:`str`]): Which method to use to serialize\n            :obj:`numpy.ndarray` objects. Defaults to :obj:`'b64'`, which converts the\n            array data to binary64 encoding (non-human-readable), and stores the\n            datatype/shape in human-readable formats. Other options are\n            :obj:`'readable'`, which produces fully human-readable output, and\n            :obj:`'npy'`, which uses numpy's built-in :obj:`save` function and\n            produces completely unreadable output. Of all the methods :obj:`'npy'`\n            is the most reliable, but also least human-readable. :obj:`'readable'`\n            produces the most human-readable output, but is the least reliable\n            and loses precision.\n\n    Returns:\n        A subclass of :obj:`json.JSONEncoder`.\n    \"\"\"\n\n    # Use specified numpy.ndarray serialization mode\n    serialize_fns = {\n        'b64': serialize_ndarray_b64,\n        'readable': serialize_ndarray_readable,\n        'npy': serialize_ndarray_npy}\n\n    if ndarray_mode not in serialize_fns:\n        raise ValueError('\"ndarray_mode\" must be one of {}'.format(\n            serialize_fns.keys))\n\n    serialize_ndarray = serialize_fns[ndarray_mode]\n\n    class MultiJSONEncoder(json.JSONEncoder):\n        \"\"\"\n        A JSON encoder that can handle:\n            * :obj:`numpy.ndarray`\n            * :obj:`numpy.floating` (converted to :obj:`float`)\n            * :obj:`numpy.integer` (converted to :obj:`int`)\n            * :obj:`numpy.dtype`\n            * :obj:`astropy.units.Quantity`\n            * :obj:`astropy.coordinates.SkyCoord`\n        \"\"\"\n        def default(self, o):\n            if isinstance(o, coords.SkyCoord):\n                return serialize_skycoord(o)\n            if isinstance(o, units.Quantity):\n                return serialize_quantity(o)\n            elif isinstance(o, np.ndarray):\n                return serialize_ndarray(o)\n            elif isinstance(o, np.dtype):\n                return serialize_dtype(o)\n            elif isinstance(o, np.floating):\n                return float(o)\n            elif isinstance(o, np.integer):\n                return int(o)\n            elif isinstance(o, np.bool_):\n                return bool(o)\n            elif isinstance(o, np.void):\n                try:\n                    o = np.array(o)\n                except:\n                    pass\n                else:\n                    return o\n            return json.JSONEncoder.default(self, o)\n\n    return MultiJSONEncoder"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the given private key bundle is a multisig bundle.", "response": "def is_multisig(privkey_info, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Is the given private key bundle a multisig bundle?\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_is_multisig(privkey_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_multisig_address(addr, blockchain='bitcoin', **blockchain_opts):\n    if blockchain == 'bitcoin':\n        return btc_is_multisig_address(addr, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))", "response": "Is the given address a multisig address?"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a script is a multisig script.", "response": "def is_multisig_script(script, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Is the given script a multisig script?\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_is_multisig_script(script, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the given private key bundle is a single - sig key bundle.", "response": "def is_singlesig(privkey_info, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Is the given private key bundle a single-sig key bundle?\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_is_singlesig(privkey_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a private key bundle get the single private key", "response": "def get_singlesig_privkey(privkey_info, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Given a private key bundle, get the (single) private key\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_get_singlesig_privkey(privkey_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_singlesig_address(addr, blockchain='bitcoin', **blockchain_opts):\n    if blockchain == 'bitcoin':\n        return btc_is_singlesig_address(addr, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))", "response": "Is the given address a single - sig address?"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_privkey_address(privkey_info, blockchain='bitcoin', **blockchain_opts):\n    if blockchain == 'bitcoin':\n        return btc_get_privkey_address(privkey_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))", "response": "Get the address from a private key bundle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply the gradient for transformation to cartesian space onto zmat_dist.", "response": "def apply_grad_cartesian_tensor(grad_X, zmat_dist):\n    \"\"\"Apply the gradient for transformation to cartesian space onto zmat_dist.\n\n    Args:\n        grad_X (:class:`numpy.ndarray`): A ``(3, n, n, 3)`` array.\n            The mathematical details of the index layout is explained in\n            :meth:`~chemcoord.Cartesian.get_grad_zmat()`.\n        zmat_dist (:class:`~chemcoord.Zmat`):\n            Distortions in Zmatrix space.\n\n    Returns:\n        :class:`~chemcoord.Cartesian`: Distortions in cartesian space.\n    \"\"\"\n    columns = ['bond', 'angle', 'dihedral']\n    C_dist = zmat_dist.loc[:, columns].values.T\n    try:\n        C_dist = C_dist.astype('f8')\n        C_dist[[1, 2], :] = np.radians(C_dist[[1, 2], :])\n    except (TypeError, AttributeError):\n        C_dist[[1, 2], :] = sympy.rad(C_dist[[1, 2], :])\n    cart_dist = np.tensordot(grad_X, C_dist, axes=([3, 2], [0, 1])).T\n    from chemcoord.cartesian_coordinates.cartesian_class_main import Cartesian\n    return Cartesian(atoms=zmat_dist['atom'],\n                     coords=cart_dist, index=zmat_dist.index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_model_converter(model, app):\n    if hasattr(model, 'id'):\n        class Converter(_ModelConverter):\n            _model = model\n        app.url_map.converters[model.__name__] = Converter", "response": "Register url converter for model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iupacify(self):\n        def convert_d(d):\n            r = d % 360\n            return r - (r // 180) * 360\n\n        new = self.copy()\n\n        new.unsafe_loc[:, 'angle'] = new['angle'] % 360\n        select = new['angle'] > 180\n        new.unsafe_loc[select, 'angle'] = new.loc[select, 'angle'] - 180\n        new.unsafe_loc[select, 'dihedral'] = new.loc[select, 'dihedral'] + 180\n\n        new.unsafe_loc[:, 'dihedral'] = convert_d(new.loc[:, 'dihedral'])\n        return new", "response": "Returns a new Zmatrix with the equivalent angles and dihedrals."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subs(self, *args, **kwargs):\n        perform_checks = kwargs.pop('perform_checks', True)\n        cols = ['bond', 'angle', 'dihedral']\n        out = self.copy()\n\n        def get_subs_f(*args):\n            def subs_function(x):\n                if hasattr(x, 'subs'):\n                    x = x.subs(*args)\n                    try:\n                        x = float(x)\n                    except TypeError:\n                        pass\n                return x\n            return subs_function\n\n        for col in cols:\n            if out.loc[:, col].dtype is np.dtype('O'):\n                out.unsafe_loc[:, col] = out.loc[:, col].map(get_subs_f(*args))\n                try:\n                    out.unsafe_loc[:, col] = out.loc[:, col].astype('f8')\n                except (SystemError, TypeError):\n                    pass\n        if perform_checks:\n            try:\n                new_cartesian = out.get_cartesian()\n            except (AttributeError, TypeError):\n                # Unevaluated symbolic expressions are remaining.\n                pass\n            except InvalidReference as e:\n                if out.dummy_manipulation_allowed:\n                    out._manipulate_dummies(e, inplace=True)\n                else:\n                    raise e\n            else:\n                out._metadata['last_valid_cartesian'] = new_cartesian\n                self._metadata['last_valid_cartesian'] = new_cartesian\n        return out", "response": "Substitute a symbolic expression in the column bonds and angles and dihedral of the current Zmatrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the numbering of the numbering of the species.", "response": "def change_numbering(self, new_index=None):\n        \"\"\"Change numbering to a new index.\n\n        Changes the numbering of index and all dependent numbering\n            (bond_with...) to a new_index.\n        The user has to make sure that the new_index consists of distinct\n            elements.\n\n        Args:\n            new_index (list): If None the new_index is taken from 1 to the\n                number of atoms.\n\n        Returns:\n            Zmat: Reindexed version of the zmatrix.\n        \"\"\"\n        if (new_index is None):\n            new_index = range(len(self))\n        elif len(new_index) != len(self):\n            raise ValueError('len(new_index) has to be the same as len(self)')\n\n        c_table = self.loc[:, ['b', 'a', 'd']]\n        # Strange bug in pandas where .replace is transitive for object columns\n        # and non-transitive for all other types.\n        # (Remember that string columns are just object columns)\n        # Example:\n        # A = {1: 2, 2: 3}\n        # Transtitive [1].replace(A) gives [3]\n        # Non-Transtitive [1].replace(A) gives [2]\n        # https://github.com/pandas-dev/pandas/issues/5338\n        # https://github.com/pandas-dev/pandas/issues/16051\n        # https://github.com/pandas-dev/pandas/issues/5541\n        # For this reason convert to int and replace then.\n\n        c_table = c_table.replace(constants.int_label)\n        try:\n            c_table = c_table.astype('i8')\n        except ValueError:\n            raise ValueError('Due to a bug in pandas it is necessary to have '\n                             'integer columns')\n        c_table = c_table.replace(self.index, new_index)\n        c_table = c_table.replace(\n            {v: k for k, v in constants.int_label.items()})\n\n        out = self.copy()\n        out.unsafe_loc[:, ['b', 'a', 'd']] = c_table\n        out._frame.index = new_index\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _insert_dummy_cart(self, exception, last_valid_cartesian=None):\n        def get_normal_vec(cartesian, reference_labels):\n            b_pos, a_pos, d_pos = cartesian._get_positions(reference_labels)\n            BA = a_pos - b_pos\n            AD = d_pos - a_pos\n            N1 = np.cross(BA, AD)\n            n1 = N1 / np.linalg.norm(N1)\n            return n1\n\n        def insert_dummy(cartesian, reference_labels, n1):\n            cartesian = cartesian.copy()\n            b_pos, a_pos, d_pos = cartesian._get_positions(reference_labels)\n            BA = a_pos - b_pos\n            N2 = np.cross(n1, BA)\n            n2 = N2 / np.linalg.norm(N2)\n            i_dummy = max(self.index) + 1\n            cartesian.loc[i_dummy, 'atom'] = 'X'\n            cartesian.loc[i_dummy, ['x', 'y', 'z']] = a_pos + n2\n            return cartesian, i_dummy\n\n        if last_valid_cartesian is None:\n            last_valid_cartesian = self._metadata['last_valid_cartesian']\n        ref_labels = self.loc[exception.index, ['b', 'a', 'd']]\n        n1 = get_normal_vec(last_valid_cartesian, ref_labels)\n        return insert_dummy(exception.already_built_cartesian, ref_labels, n1)", "response": "Insert dummy atom into the already built cartesian of exception."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert dummy atom into zmat.", "response": "def _insert_dummy_zmat(self, exception, inplace=False):\n        \"\"\"Works INPLACE\"\"\"\n        def insert_row(df, pos, key):\n            if pos < len(df):\n                middle = df.iloc[pos:(pos + 1)]\n                middle.index = [key]\n                start, end = df.iloc[:pos], df.iloc[pos:]\n                return pd.concat([start, middle, end])\n            elif pos == len(df):\n                start = df.copy()\n                start.loc[key] = start.iloc[-1]\n                return start\n\n        def raise_warning(i, dummy_d):\n            give_message = ('For the dihedral reference of atom {i} the '\n                            'dummy atom {dummy_d} was inserted').format\n            warnings.warn(give_message(i=i, dummy_d=dummy_d), UserWarning)\n\n        def insert_dummy(zmat, i, dummy_cart, dummy_d):\n            \"\"\"Works INPLACE on self._frame\"\"\"\n            cols = ['b', 'a', 'd']\n            actual_d = zmat.loc[i, 'd']\n            zframe = insert_row(zmat, zmat.index.get_loc(i), dummy_d)\n            zframe.loc[i, 'd'] = dummy_d\n            zframe.loc[dummy_d, 'atom'] = 'X'\n            zframe.loc[dummy_d, cols] = zmat.loc[actual_d, cols]\n            zmat_values = dummy_cart._calculate_zmat_values(\n                [dummy_d] + list(zmat.loc[actual_d, cols]))[0]\n            zframe.loc[dummy_d, ['bond', 'angle', 'dihedral']] = zmat_values\n\n            zmat._frame = zframe\n            zmat._metadata['has_dummies'][i] = {'dummy_d': dummy_d,\n                                                'actual_d': actual_d}\n            raise_warning(i, dummy_d)\n\n        zmat = self if inplace else self.copy()\n\n        if exception.index in zmat._metadata['has_dummies']:\n            zmat._remove_dummies(to_remove=[exception.index], inplace=True)\n        else:\n            insert_dummy(zmat, exception.index,\n                         *zmat._insert_dummy_cart(exception))\n\n        try:\n            zmat._metadata['last_valid_cartesian'] = zmat.get_cartesian()\n        except InvalidReference as e:\n            zmat._insert_dummy_zmat(e, inplace=True)\n\n        if not inplace:\n            return zmat"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the dummy atoms from the zmat.", "response": "def _remove_dummies(self, to_remove=None, inplace=False):\n        \"\"\"Works INPLACE\"\"\"\n        zmat = self if inplace else self.copy()\n        if to_remove is None:\n            to_remove = zmat._has_removable_dummies()\n        if not to_remove:\n            if inplace:\n                return None\n            else:\n                return zmat\n        has_dummies = zmat._metadata['has_dummies']\n\n        c_table = zmat.loc[to_remove, ['b', 'a', 'd']]\n        c_table['d'] = [has_dummies[k]['actual_d'] for k in to_remove]\n        zmat.unsafe_loc[to_remove, 'd'] = c_table['d'].astype('i8')\n\n        zmat_values = zmat.get_cartesian()._calculate_zmat_values(c_table)\n        zmat.unsafe_loc[to_remove, ['bond', 'angle', 'dihedral']] = zmat_values\n        zmat._frame.drop([has_dummies[k]['dummy_d'] for k in to_remove],\n                         inplace=True)\n        warnings.warn('The dummy atoms {} were removed'.format(to_remove),\n                      UserWarning)\n        for k in to_remove:\n            zmat._metadata['has_dummies'].pop(k)\n        if not inplace:\n            return zmat"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cartesian(self):\n        def create_cartesian(positions, row):\n            xyz_frame = pd.DataFrame(columns=['atom', 'x', 'y', 'z'],\n                                     index=self.index[:row], dtype='f8')\n            xyz_frame['atom'] = self.loc[xyz_frame.index, 'atom']\n            xyz_frame.loc[:, ['x', 'y', 'z']] = positions[:row]\n            from chemcoord.cartesian_coordinates.cartesian_class_main \\\n                import Cartesian\n            cartesian = Cartesian(xyz_frame, metadata=self.metadata)\n            return cartesian\n\n        c_table = self.loc[:, ['b', 'a', 'd']]\n        c_table = c_table.replace(constants.int_label)\n        c_table = c_table.replace({k: v for v, k in enumerate(c_table.index)})\n        c_table = c_table.values.astype('i8').T\n\n        C = self.loc[:, ['bond', 'angle', 'dihedral']].values.T\n        C[[1, 2], :] = np.radians(C[[1, 2], :])\n\n        err, row, positions = transformation.get_X(C, c_table)\n        positions = positions.T\n\n        if err == ERR_CODE_InvalidReference:\n            rename = dict(enumerate(self.index))\n            i = rename[row]\n            b, a, d = self.loc[i, ['b', 'a', 'd']]\n            cartesian = create_cartesian(positions, row)\n            raise InvalidReference(i=i, b=b, a=a, d=d,\n                                   already_built_cartesian=cartesian)\n        elif err == ERR_CODE_OK:\n            return create_cartesian(positions, row + 1)", "response": "Return the molecule in cartesian coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the sequence of virtualchain transactions from a particular blockchain over a given range of block heights.", "response": "def get_virtual_transactions(blockchain_name, blockchain_opts, first_block_height, last_block_height, tx_filter=None, **hints):\n    \"\"\"\n    Get the sequence of virtualchain transactions from a particular blockchain over a given range of block heights.\n    Returns a list of tuples in the format of [(block height, [txs])], where\n    each tx in [txs] is the parsed transaction.  The parsed transaction will conform to... # TODO write a spec for this\n    \n    Each transaction has at least the following fields:\n    \n    `version`: the version of the transaction\n    `txindex`: the index into the block where this tx occurs\n    `ins`: a list of transaction inputs, where each member is a dict with:\n        `outpoint`: a dict of {'hash': txid of transaction that fed it in, 'index': the index into the feeder tx's outputs list}\n        `script`: the signature script for this input\n    `outs`: a list of transaction outputs, where each member is a dict with:\n        `value`: the amount of currency units spent (in the fundamental units of the chain)\n        `script`: the spending script for this input\n    `senders`: a list of information in 1-to-1 correspondence with each input regarding the transactions that funded it:\n        `value`: the amount of currency units sent (in fundamental units of the chain) \n        `script_pubkey`: the spending script for the sending transaction\n    \n    Returns [(block height, [txs])] on success\n    Returns None on error.\n    Raises ValueError on unknown blockchain\n    \"\"\"\n    if blockchain_name == 'bitcoin':\n        return get_bitcoin_virtual_transactions(blockchain_opts, first_block_height, last_block_height, tx_filter=tx_filter, **hints)\n\n    else:\n        raise ValueError(\"Unknown blockchain {}\".format(blockchain_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a raw transaction into a dict of virtual transactions", "response": "def tx_parse(raw_tx, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Parse a raw transaction, based on the type of blockchain it's from\n    Returns a tx dict on success (see get_virtual_transactions)\n    Raise ValueError for unknown blockchain\n    Raise some other exception for invalid raw_tx (implementation-specific)\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_tx_deserialize(raw_tx, **blockchain_opts)\n    else:\n        raise ValueError(\"Unknown blockchain {}\".format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining whether or not a tx output has data", "response": "def tx_output_has_data(output, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Give a blockchain name and a tx output, determine whether or not it is a\n    data-bearing script--i.e. one with data for the state engine.\n\n    Return True if so\n    Return False if not\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_tx_output_has_data(output, **blockchain_opts)\n    else:\n        return ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a blockchain name and an output script determine whether or not it is a data - script.", "response": "def tx_is_data_script(out_script, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Given a blockchain name and an output script (tx['outs'][x]['script']),\n    determine whether or not it is a data-bearing script---i.e. one with data for the state engine.\n\n    Return True if so\n    Reurn False if not\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_tx_output_script_has_data(out_script, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tx_extend(partial_tx_hex, new_inputs, new_outputs, blockchain='bitcoin', **blockchain_opts):\n    if blockchain == 'bitcoin':\n        return btc_tx_extend(partial_tx_hex, new_inputs, new_outputs, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))", "response": "Extend a set of inputs and outputs to a tx."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsigns a given input in a transaction given the previous output script and previous output amount.", "response": "def tx_sign_input(tx_hex, idx, prevout_script, prevout_amount, private_key_info, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Sign a given input in a transaction, given the previous output script and previous output amount.\n    Different blockchains can require additional fields; pass thse in **blockchain_opts.\n\n    Return the serialized tx with the given input signed on success\n    Raise on error\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_tx_sign_input(tx_hex, idx, prevout_script, prevout_amount, private_key_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsigns all unsigned inputs to a given transaction with the given private key.", "response": "def tx_sign_all_unsigned_inputs(privkey_info, prev_outputs, unsigned_tx_hex, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Sign all unsigned inputs to a given transaction with the given private key.  Also, pass in the list of previous outputs to the transaction so they\n    can be paired with the right input (i.e. prev_outputs is a list of tx outputs that are in 1-to-1 correspondance with the inputs in the serialized tx)\n    \n    Different blockchains can require additional fields; pass thse in **blockchain_opts.\n\n    Return the signed transaction on success\n    Raise on error\n    \"\"\"\n    if blockchain == 'bitcoin':\n        return btc_tx_sign_all_unsigned_inputs(privkey_info, prev_outputs, unsigned_tx_hex, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch():\n    url = 'http://pla.esac.esa.int/pla/aio/product-action?MAP.MAP_ID=HFI_CompMap_ThermalDustModel_2048_R1.20.fits'\n    md5 = '8d804f4e64e709f476a63f0dfed1fd11'\n    fname = os.path.join(\n        data_dir(),\n        'planck',\n        'HFI_CompMap_ThermalDustModel_2048_R1.20.fits')\n    fetch_utils.download_and_verify(url, md5, fname=fname)", "response": "Downloads the Planck Collaboration dust map and placing it in the dustmaps data directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, coords, **kwargs):\n        return self._scale * super(PlanckQuery, self).query(coords, **kwargs)", "response": "Query the Planck class for the given location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpanding the action definition passed in by the user and return the action and data lines needed for elasticsearch s bulk api.", "response": "def expand_action(data):\n    \"\"\"\n    From one document or action definition passed in by the user extract the\n    action/data lines needed for elasticsearch's\n    :meth:`~elasticsearch.Elasticsearch.bulk` api.\n    \"\"\"\n    # when given a string, assume user wants to index raw json\n    if isinstance(data, string_types):\n        return '{\"index\":{}}', data\n\n    # make sure we don't alter the action\n    data = data.copy()\n    op_type = data.pop('_op_type', 'index')\n    action = {op_type: {}}\n    for key in ('_index', '_parent', '_percolate', '_routing', '_timestamp',\n            '_ttl', '_type', '_version', '_version_type', '_id', '_retry_on_conflict'):\n        if key in data:\n            action[op_type][key] = data.pop(key)\n\n    # no data payload for delete\n    if op_type == 'delete':\n        return action, None\n\n    return action, data.get('_source', data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit the list of actions into chunks by number or size or start a new one.", "response": "def _chunk_actions(actions, chunk_size, max_chunk_bytes, serializer):\n    \"\"\"\n    Split actions into chunks by number or size, serialize them into strings in\n    the process.\n    \"\"\"\n    bulk_actions = []\n    size, action_count = 0, 0\n    for action, data in actions:\n        action = serializer.dumps(action)\n        cur_size = len(action) + 1\n\n        if data is not None:\n            data = serializer.dumps(data)\n            cur_size += len(data) + 1\n\n        # full chunk, send it and start a new one\n        if bulk_actions and (size + cur_size > max_chunk_bytes or action_count == chunk_size):\n            yield bulk_actions\n            bulk_actions = []\n            size, action_count = 0, 0\n\n        bulk_actions.append(action)\n        if data is not None:\n            bulk_actions.append(data)\n        size += cur_size\n        action_count += 1\n\n    if bulk_actions:\n        yield bulk_actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_bulk_chunk(client, bulk_actions, raise_on_exception=True, raise_on_error=True, **kwargs):\n    # if raise on error is set, we need to collect errors per chunk before raising them\n    errors = []\n\n    try:\n        # send the actual request\n        resp = client.bulk('\\n'.join(bulk_actions) + '\\n', **kwargs)\n    except TransportError as e:\n        # default behavior - just propagate exception\n        if raise_on_exception:\n            raise e\n\n        # if we are not propagating, mark all actions in current chunk as failed\n        err_message = str(e)\n        exc_errors = []\n\n        # deserialize the data back, thisis expensive but only run on\n        # errors if raise_on_exception is false, so shouldn't be a real\n        # issue\n        bulk_data = iter(map(client.transport.serializer.loads, bulk_actions))\n        while True:\n            try:\n                # collect all the information about failed actions\n                action = next(bulk_data)\n                op_type, action = action.popitem()\n                info = {\"error\": err_message, \"status\": e.status_code, \"exception\": e}\n                if op_type != 'delete':\n                    info['data'] = next(bulk_data)\n                info.update(action)\n                exc_errors.append({op_type: info})\n            except StopIteration:\n                break\n\n        # emulate standard behavior for failed actions\n        if raise_on_error:\n            raise BulkIndexError('%i document(s) failed to index.' % len(exc_errors), exc_errors)\n        else:\n            for err in exc_errors:\n                yield False, err\n            return\n\n    # go through request-reponse pairs and detect failures\n    for op_type, item in map(methodcaller('popitem'), resp['items']):\n        ok = 200 <= item.get('status', 500) < 300\n        if not ok and raise_on_error:\n            errors.append({op_type: item})\n\n        if ok or not errors:\n            # if we are not just recording all errors to be able to raise\n            # them all at once, yield items individually\n            yield ok, {op_type: item}\n\n    if errors:\n        raise BulkIndexError('%i document(s) failed to index.' % len(errors), errors)", "response": "Process a bulk request and return a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstreaming bulk returns a list of items from the client.", "response": "def streaming_bulk(client, actions, chunk_size=500, max_chunk_bytes=100 * 1014 * 1024,\n        raise_on_error=True, expand_action_callback=expand_action,\n        raise_on_exception=True, **kwargs):\n    \"\"\"\n    Streaming bulk consumes actions from the iterable passed in and yields\n    results per action. For non-streaming usecases use\n    :func:`~elasticsearch.helpers.bulk` which is a wrapper around streaming\n    bulk that returns summary information about the bulk operation once the\n    entire input is consumed and sent.\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg actions: iterable containing the actions to be executed\n    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n        from the execution of the last chunk when some occur. By default we raise.\n    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n        call to ``bulk`` and just report the items that failed as failed.\n    :arg expand_action_callback: callback executed on each action passed in,\n        should return a tuple containing the action line and the data line\n        (`None` if data line should be omitted).\n    \"\"\"\n    actions = map(expand_action_callback, actions)\n\n    for bulk_actions in _chunk_actions(actions, chunk_size, max_chunk_bytes, client.transport.serializer):\n        for result in _process_bulk_chunk(client, bulk_actions, raise_on_exception, raise_on_error, **kwargs):\n            yield result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bulk(client, actions, stats_only=False, **kwargs):\n    success, failed = 0, 0\n\n    # list of errors to be collected is not stats_only\n    errors = []\n\n    for ok, item in streaming_bulk(client, actions, **kwargs):\n        # go through request-reponse pairs and detect failures\n        if not ok:\n            if not stats_only:\n                errors.append(item)\n            failed += 1\n        else:\n            success += 1\n\n    return success, failed if stats_only else errors", "response": "This is a convenience method that provides a more human friendly interface for the Elasticsearch bulk API. It returns a tuple with summary of the number of successful and failed items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parallel_bulk(client, actions, thread_count=4, chunk_size=500,\n        max_chunk_bytes=100 * 1014 * 1024,\n        expand_action_callback=expand_action, **kwargs):\n    \"\"\"\n    Parallel version of the bulk helper run in multiple threads at once.\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg actions: iterator containing the actions\n    :arg thread_count: size of the threadpool to use for the bulk requests\n    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n        from the execution of the last chunk when some occur. By default we raise.\n    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n        call to ``bulk`` and just report the items that failed as failed.\n    :arg expand_action_callback: callback executed on each action passed in,\n        should return a tuple containing the action line and the data line\n        (`None` if data line should be omitted).\n    \"\"\"\n    # Avoid importing multiprocessing unless parallel_bulk is used\n    # to avoid exceptions on restricted environments like App Engine\n    from multiprocessing.dummy import Pool\n    actions = map(expand_action_callback, actions)\n\n    pool = Pool(thread_count)\n\n    for result in pool.imap(\n        lambda chunk: list(_process_bulk_chunk(client, chunk, **kwargs)),\n        _chunk_actions(actions, chunk_size, max_chunk_bytes, client.transport.serializer)\n        ):\n        for item in result:\n            yield item\n\n    pool.close()\n    pool.join()", "response": "This function is used to run bulk requests in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscanning the index for all items in the cluster.", "response": "def scan(client, query=None, scroll='5m', raise_on_error=True,\n         preserve_order=False, size=1000, **kwargs):\n    \"\"\"\n    Simple abstraction on top of the\n    :meth:`~elasticsearch.Elasticsearch.scroll` api - a simple iterator that\n    yields all hits as returned by underlining scroll requests.\n    By default scan does not return results in any pre-determined order. To\n    have a standard order in the returned documents (either by score or\n    explicit sort definition) when scrolling, use ``preserve_order=True``. This\n    may be an expensive operation and will negate the performance benefits of\n    using ``scan``.\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg query: body for the :meth:`~elasticsearch.Elasticsearch.search` api\n    :arg scroll: Specify how long a consistent view of the index should be\n        maintained for scrolled search\n    :arg raise_on_error: raises an exception (``ScanError``) if an error is\n        encountered (some shards fail to execute). By default we raise.\n    :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will\n        cause the scroll to paginate with preserving the order. Note that this\n        can be an extremely expensive operation and can easily lead to\n        unpredictable results, use with caution.\n    :arg size: size (per shard) of the batch send at each iteration.\n    Any additional keyword arguments will be passed to the initial\n    :meth:`~elasticsearch.Elasticsearch.search` call::\n        scan(es,\n            query={\"query\": {\"match\": {\"title\": \"python\"}}},\n            index=\"orders-*\",\n            doc_type=\"books\"\n        )\n    \"\"\"\n    if not preserve_order:\n        kwargs['search_type'] = 'scan'\n    # initial search\n    resp = client.search(body=query, scroll=scroll, size=size, **kwargs)\n\n    scroll_id = resp.get('_scroll_id')\n    if scroll_id is None:\n        return\n\n    first_run = True\n    while True:\n        # if we didn't set search_type to scan initial search contains data\n        if preserve_order and first_run:\n            first_run = False\n        else:\n            resp = client.scroll(scroll_id, scroll=scroll)\n\n        for hit in resp['hits']['hits']:\n            yield hit\n\n        # check if we have any errrors\n        if resp[\"_shards\"][\"failed\"]:\n            logger.warning(\n                'Scroll request has failed on %d shards out of %d.',\n                resp['_shards']['failed'], resp['_shards']['total']\n            )\n            if raise_on_error:\n                raise ScanError(\n                    'Scroll request has failed on %d shards out of %d.' %\n                    (resp['_shards']['failed'], resp['_shards']['total'])\n                )\n\n        scroll_id = resp.get('_scroll_id')\n        # end of scroll\n        if scroll_id is None or not resp['hits']['hits']:\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setcontext(context, _local=local):\n    oldcontext = getcontext()\n    _local.__bigfloat_context__ = oldcontext + context", "response": "Sets the current context to that given."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying an MPFR function f to the given arguments args rounding to the current context. Returns a new Mpfr object with precision taken from the current context.", "response": "def _apply_function_in_context(cls, f, args, context):\n    \"\"\" Apply an MPFR function 'f' to the given arguments 'args', rounding to\n    the given context.  Returns a new Mpfr object with precision taken from\n    the current context.\n\n    \"\"\"\n    rounding = context.rounding\n    bf = mpfr.Mpfr_t.__new__(cls)\n    mpfr.mpfr_init2(bf, context.precision)\n    args = (bf,) + args + (rounding,)\n    ternary = f(*args)\n    with _temporary_exponent_bounds(context.emin, context.emax):\n        ternary = mpfr.mpfr_check_range(bf, ternary, rounding)\n        if context.subnormalize:\n            # mpfr_subnormalize doesn't set underflow and\n            # subnormal flags, so we do that ourselves.  We choose\n            # to set the underflow flag for *all* cases where the\n            # 'after rounding' result is smaller than the smallest\n            # normal number, even if that result is exact.\n\n            # if bf is zero but ternary is nonzero, the underflow\n            # flag will already have been set by mpfr_check_range;\n            underflow = (\n                mpfr.mpfr_number_p(bf) and\n                not mpfr.mpfr_zero_p(bf) and\n                mpfr.mpfr_get_exp(bf) < context.precision - 1 + context.emin)\n            if underflow:\n                mpfr.mpfr_set_underflow()\n            ternary = mpfr.mpfr_subnormalize(bf, ternary, rounding)\n            if ternary:\n                mpfr.mpfr_set_inexflag()\n    return bf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_filter_expr_predicate(p):\n    if not hasattr(p[1], 'append_predicate'):\n        p[1] = ast.PredicatedExpression(p[1])\n    p[1].append_predicate(p[2])\n    p[0] = p[1]", "response": "FilterExpr predicate is a special case for filter expressions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logger(name=None):\n\n    level = logging.CRITICAL\n    if DEBUG:\n        logging.disable(logging.NOTSET)\n        level = logging.DEBUG\n\n    if name is None:\n        name = \"<unknown>\"\n\n    log = logging.getLogger(name=name)\n    log.setLevel( level )\n    console = logging.StreamHandler()\n    console.setLevel( level )\n    log_format = ('[%(asctime)s] [%(levelname)s] [%(module)s:%(lineno)d] (' + str(os.getpid()) + '.%(thread)d) %(message)s' if DEBUG else '%(message)s')\n    formatter = logging.Formatter( log_format )\n    console.setFormatter(formatter)\n    log.propagate = False\n\n    if len(log.handlers) > 0:\n        for i in xrange(0, len(log.handlers)):\n            log.handlers.pop(0)\n    \n    log.addHandler(console)\n    return log", "response": "Get virtualchain s logger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the absolute path to the config file.", "response": "def get_config_filename(impl, working_dir):\n    \"\"\"\n    Get the absolute path to the config file.\n    \"\"\"\n    config_filename = impl.get_virtual_chain_name() + \".ini\"\n    return os.path.join(working_dir, config_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_db_filename(impl, working_dir):\n    db_filename = impl.get_virtual_chain_name() + \".db\"\n    return os.path.join(working_dir, db_filename)", "response": "Get the absolute path to the last - block file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_snapshots_filename(impl, working_dir):\n    snapshots_filename = impl.get_virtual_chain_name() + \".snapshots\"\n    return os.path.join(working_dir, snapshots_filename)", "response": "Get the absolute path to the chain s consensus snapshots file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the absolute path to the chain s indexing lockfile", "response": "def get_lockfile_filename(impl, working_dir):\n    \"\"\"\n    Get the absolute path to the chain's indexing lockfile\n    \"\"\"\n    lockfile_name = impl.get_virtual_chain_name() + \".lock\"\n    return os.path.join(working_dir, lockfile_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget bitcoind config from config file.", "response": "def get_bitcoind_config(config_file=None, impl=None):\n    \"\"\"\n    Set bitcoind options globally.\n    Call this before trying to talk to bitcoind.\n    \"\"\"\n\n    loaded = False\n\n    bitcoind_server = None\n    bitcoind_port = None\n    bitcoind_user = None\n    bitcoind_passwd = None\n    bitcoind_timeout = None\n    bitcoind_regtest = None\n    bitcoind_p2p_port = None\n    bitcoind_spv_path = None\n\n    regtest = None\n\n    if config_file is not None:\n\n        parser = SafeConfigParser()\n        parser.read(config_file)\n\n        if parser.has_section('bitcoind'):\n\n            if parser.has_option('bitcoind', 'server'):\n                bitcoind_server = parser.get('bitcoind', 'server')\n\n            if parser.has_option('bitcoind', 'port'):\n                bitcoind_port = int(parser.get('bitcoind', 'port'))\n\n            if parser.has_option('bitcoind', 'p2p_port'):\n                bitcoind_p2p_port = int(parser.get('bitcoind', 'p2p_port'))\n\n            if parser.has_option('bitcoind', 'user'):\n                bitcoind_user = parser.get('bitcoind', 'user')\n\n            if parser.has_option('bitcoind', 'passwd'):\n                bitcoind_passwd = parser.get('bitcoind', 'passwd')\n\n            if parser.has_option('bitcoind', 'spv_path'):\n                bitcoind_spv_path = parser.get('bitcoind', 'spv_path')\n\n            if parser.has_option('bitcoind', 'regtest'):\n                regtest = parser.get('bitcoind', 'regtest')\n            else:\n                regtest = 'no'\n\n            if parser.has_option('bitcoind', 'timeout'):\n                bitcoind_timeout = float(parser.get('bitcoind', 'timeout'))\n\n            if regtest.lower() in [\"yes\", \"y\", \"true\", \"1\", \"on\"]:\n                bitcoind_regtest = True\n            else:\n                bitcoind_regtest = False\n            \n            loaded = True\n\n    if not loaded:\n\n        bitcoind_server = 'bitcoin.blockstack.com'\n        bitcoind_port = 8332\n        bitcoind_user = 'blockstack'\n        bitcoind_passwd = 'blockstacksystem'\n        bitcoind_regtest = False\n        bitcoind_timeout = 300\n        bitcoind_p2p_port = 8333\n        bitcoind_spv_path = os.path.expanduser(\"~/.virtualchain-spv-headers.dat\")\n\n    default_bitcoin_opts = {\n        \"bitcoind_user\": bitcoind_user,\n        \"bitcoind_passwd\": bitcoind_passwd,\n        \"bitcoind_server\": bitcoind_server,\n        \"bitcoind_port\": bitcoind_port,\n        \"bitcoind_timeout\": bitcoind_timeout,\n        \"bitcoind_regtest\": bitcoind_regtest,\n        \"bitcoind_p2p_port\": bitcoind_p2p_port,\n        \"bitcoind_spv_path\": bitcoind_spv_path\n    }\n\n    return default_bitcoin_opts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the map value at the specified location on the sky.", "response": "def query(self, coords, order=1):\n        \"\"\"\n        Returns the map value at the specified location(s) on the sky.\n\n        Args:\n            coords (`astropy.coordinates.SkyCoord`): The coordinates to query.\n            order (Optional[int]): Interpolation order to use. Defaults to `1`,\n                for linear interpolation.\n\n        Returns:\n            A float array containing the map value at every input coordinate.\n            The shape of the output will be the same as the shape of the\n            coordinates stored by `coords`.\n        \"\"\"\n        out = np.full(len(coords.l.deg), np.nan, dtype='f4')\n\n        for pole in self.poles:\n            m = (coords.b.deg >= 0) if pole == 'ngp' else (coords.b.deg < 0)\n\n            if np.any(m):\n                data, w = self._data[pole]\n                x, y = w.wcs_world2pix(coords.l.deg[m], coords.b.deg[m], 0)\n                out[m] = map_coordinates(data, [y, x], order=order, mode='nearest')\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the SFD E - V at the specified location.", "response": "def query(self, coords, order=1):\n        \"\"\"\n        Returns E(B-V) at the specified location(s) on the sky. See Table 6 of\n        Schlafly & Finkbeiner (2011) for instructions on how to convert this\n        quantity to extinction in various passbands.\n\n        Args:\n            coords (`astropy.coordinates.SkyCoord`): The coordinates to query.\n            order (Optional[int]): Interpolation order to use. Defaults to `1`,\n                for linear interpolation.\n\n        Returns:\n            A float array containing the SFD E(B-V) at every input coordinate.\n            The shape of the output will be the same as the shape of the\n            coordinates stored by `coords`.\n        \"\"\"\n        return super(SFDQuery, self).query(coords, order=order)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_empty(self):\n        return all(date.is_empty() for date in [self.created, self.issued]) \\\n               and not self.publisher", "response": "Returns True if all child date elements present are empty."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_empty(self):\n        non_type_attributes = [attr for attr in self.node.attrib.keys() if attr != 'type']\n        return len(self.node) == 0 and len(non_type_attributes) == 0 \\\n            and not self.node.text and not self.node.tail", "response": "Returns True if the root node contains no child elements text and no attributes other than type. Returns False if any of the child elements text and attributes other than type are present. Returns False if any of the child elements text and attributes other than type are present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if all titleInfo subfields are not set or empty.", "response": "def is_empty(self):\n        '''Returns True if all titleInfo subfields are not set or\n        empty; returns False if any of the fields are not empty.'''\n        return not bool(self.title or self.subtitle or self.part_number \\\n                        or self.part_name or self.non_sort or self.type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if details extent and type are not set or return False if any of the if fields are not set or return True for is_empty ; returns False for is_empty.", "response": "def is_empty(self):\n        '''Returns True if details, extent, and type are not set or\n        return True for ``is_empty``; returns False if any of the\n        fields are not empty.'''\n        return all(field.is_empty() for field in [self.details, self.extent]\n                   \t\t\tif field is not None) \\\n               and not self.type"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef batch_(self, rpc_calls):\n        batch_data = []\n        for rpc_call in rpc_calls:\n            AuthServiceProxy.__id_count += 1\n            m = rpc_call.pop(0)\n            batch_data.append({\"jsonrpc\":\"2.0\", \"method\":m, \"params\":rpc_call, \"id\":AuthServiceProxy.__id_count})\n\n        postdata = json.dumps(batch_data, default=EncodeDecimal)\n        log.debug(\"--> \"+postdata)\n        self.__conn.request('POST', self.__url.path, postdata,\n                            {'Host': self.__url.hostname,\n                             'User-Agent': USER_AGENT,\n                             'Authorization': self.__auth_header,\n                             'Content-type': 'application/json'})\n        results = []\n        responses = self._get_response()\n        for response in responses:\n            if response['error'] is not None:\n                raise JSONRPCException(response['error'])\n            elif 'result' not in response:\n                raise JSONRPCException({\n                    'code': -343, 'message': 'missing JSON-RPC result'})\n            else:\n                results.append(response['result'])\n        return results", "response": "Batch RPC call.\n           Pass array of arrays: [ [ \"method\", params... ], ... ]\n           Returns array of results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getinfo(self):\n        try:\n            old_getinfo = AuthServiceProxy(self.__service_url, 'getinfo', self.__timeout, self.__conn, True)\n            res = old_getinfo()\n            if 'error' not in res:\n                # 0.13 and earlier\n                return res\n\n        except JSONRPCException:\n            pass\n\n        network_info = self.getnetworkinfo()\n        blockchain_info = self.getblockchaininfo()\n        try:\n            wallet_info = self.getwalletinfo()\n        except:\n            wallet_info = {\n                'walletversion': None,\n                'balance': None,\n                'keypoololdest': None,\n                'keypoolsize': None,\n                'paytxfee': None,\n            }\n\n        res = {\n            'version': network_info['version'],\n            'protocolversion': network_info['protocolversion'],\n            'walletversion': wallet_info['walletversion'],\n            'balance': wallet_info['balance'],\n            'blocks': blockchain_info['blocks'],\n            'timeoffset': network_info['timeoffset'],\n            'connections': network_info['connections'],\n            'proxy': network_info['networks'],\n            'difficulty': blockchain_info['difficulty'],\n            'testnet': blockchain_info['chain'] == 'testnet',\n            'keypoololdest': wallet_info['keypoololdest'],\n            'keypoolsize': wallet_info['keypoolsize'],\n            'paytxfee': wallet_info['paytxfee'],\n            'errors': network_info['warnings'],\n        }\n        \n        for k in ['unlocked_until', 'relayfee', 'paytxfee']:\n            if wallet_info.has_key(k):\n                res[k] = wallet_info[k]\n\n        return res", "response": "Get the current user s information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cartesian(self):\n        coords = ['x', 'y', 'z']\n        eq_sets = self._metadata['eq']['eq_sets']\n        sym_ops = self._metadata['eq']['sym_ops']\n        frame = pd.DataFrame(index=[i for v in eq_sets.values() for i in v],\n                             columns=['atom', 'x', 'y', 'z'], dtype='f8')\n        frame['atom'] = pd.Series(\n            {i: self.loc[k, 'atom'] for k, v in eq_sets.items() for i in v})\n        frame.loc[self.index, coords] = self.loc[:, coords]\n        for i in eq_sets:\n            for j in eq_sets[i]:\n                frame.loc[j, coords] = np.dot(sym_ops[i][j],\n                                              frame.loc[i, coords])\n        return Cartesian(frame)", "response": "Return a : class : ~Cartesian instance where all the allUnusedEntries members of a symmetry equivalence class are inserted back in."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the string representation of a script and return the hex version.", "response": "def btc_script_to_hex(script):\n    \"\"\" Parse the string representation of a script and return the hex version.\n        Example: \"OP_DUP OP_HASH160 c629...a6db OP_EQUALVERIFY OP_CHECKSIG\"\n    \"\"\"\n\n    hex_script = ''\n    parts = script.split(' ')\n    for part in parts:\n        if part[0:3] == 'OP_':\n            value = OPCODE_VALUES.get(part)\n            if not value:\n                raise ValueError(\"Unrecognized opcode {}\".format(part))\n\n            hex_script += \"%0.2x\" % value\n\n        elif hashing.is_hex(part):\n            hex_script += '%0.2x' % hashing.count_bytes(part) + part\n\n        else:\n            raise Exception('Invalid script - only opcodes and hex characters allowed.')\n\n    return hex_script"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_script_deserialize(script):\n\n    if isinstance(script, str) and re.match('^[0-9a-fA-F]*$', script):\n       script = binascii.unhexlify(script)\n\n    # output buffer\n    out = []\n    pos = 0\n\n    while pos < len(script):\n        # next script op...\n        code = encoding.from_byte_to_int(script[pos])\n\n        if code == 0:\n            # empty (OP_0)\n            out.append(None)\n            pos += 1\n\n        elif code <= 75:\n            # literal numeric constant, followed by a slice of data.\n            # push the slice of data.\n            out.append(script[pos+1:pos+1+code])\n            pos += 1 + code\n\n        elif code <= 78:\n            # OP_PUSHDATA1, OP_PUSHDATA2, OP_PUSHDATA4, followed by length and data\n            # push the data itself\n            szsz = pow(2, code - 76)\n            sz = encoding.decode(script[pos+szsz: pos:-1], 256)\n            out.append(script[pos + 1 + szsz : pos + 1 + szsz + sz])\n            pos += 1 + szsz + sz\n\n        elif code <= 96:\n            # OP_1NEGATE, OP_RESERVED, OP_1 thru OP_16\n            # pass -1 for OP_1NEGATE\n            # pass 0 for OP_RESERVED (shouldn't be used anyway)\n            # pass 1 thru 16 for OP_1 thru OP_16\n            out.append(code - 80)\n            pos += 1\n\n        else:\n            # raw opcode\n            out.append(code)\n            pos += 1\n\n    # make sure each string is hex'ed\n    out = encoding.json_changebase(out, lambda x: encoding.safe_hexlify(x))\n    return out", "response": "Given a script returns a list of strings and ints."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing one item of a BTC script into a string.", "response": "def _btc_script_serialize_unit(unit):\n    \"\"\"\n    Encode one item of a BTC script\n    Return the encoded item (as a string)\n\n    Based on code from pybitcointools (https://github.com/vbuterin/pybitcointools)\n    by Vitalik Buterin\n    \"\"\"\n    \n    if isinstance(unit, int):\n        # cannot be less than -1, since btc_script_deserialize() never returns such numbers\n        if unit < -1:\n            raise ValueError('Invalid integer: {}'.format(unit))\n\n        if unit < 16:\n            if unit == 0:\n                # OP_RESERVED\n                return encoding.from_int_to_byte(OPCODE_VALUES['OP_RESERVED'])\n            else:\n                # OP_1 thru OP_16, or OP_1NEGATE\n                return encoding.from_int_to_byte(unit + 80)\n        else:\n            # pass as numeric literal or raw opcode\n            return encoding.from_int_to_byte(unit)\n\n    elif unit is None:\n        # None means OP_0\n        return b'\\x00'\n\n    else:\n        if len(unit) <= 75:\n            # length + payload\n            return encoding.from_int_to_byte(len(unit)) + unit\n\n        elif len(unit) < 256:\n            # OP_PUSHDATA1 + length (1 byte) + payload\n            return encoding.from_int_to_byte(OPCODE_VALUES['OP_PUSHDATA1']) + encoding.from_int_to_byte(len(unit)) + unit\n\n        elif len(unit) < 65536:\n            # OP_PUSHDATA2 + length (2 bytes, big-endian) + payload\n            return encoding.from_int_to_byte(OPCODE_VALUES['OP_PUSHDATA2']) + encoding.encode(len(unit), 256, 2)[::-1] + unit\n        else:\n            # OP_PUSHDATA4 + length (4 bytes, big-endian) + payload\n            return encoding.from_int_to_byte(OPCODE_VALUES['OP_PUSHDATA4']) + encoding.encode(len(unit), 256, 4)[::-1] + unit"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_script_serialize(_script):\n    script = _script\n    if encoding.json_is_base(_script, 16):\n        # hex-to-bin all hex strings in this script\n        script = encoding.json_changebase(_script, lambda x: binascii.unhexlify(x))\n\n    # encode each item and return the concatenated list\n    return encoding.safe_hexlify( ''.join(map(_btc_script_serialize_unit, script)) )", "response": "Serialize a serialized script into a hex script"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a pay - to - address script.", "response": "def btc_make_payment_script( address, segwit=None, **ignored ):\n    \"\"\"\n    Make a pay-to-address script.\n    \"\"\"\n    \n    if segwit is None:\n        segwit = get_features('segwit')\n\n    # is address bech32-encoded?\n    witver, withash = segwit_addr_decode(address)\n    if witver is not None and withash is not None:\n        # bech32 segwit address\n        if not segwit:\n            raise ValueError(\"Segwit is disabled\")\n\n        if len(withash) == 20:\n            # p2wpkh\n            script_hex = '0014' + withash.encode('hex')\n            return script_hex\n            \n        elif len(withash) == 32:\n            # p2wsh\n            script_hex = '0020' + withash.encode('hex')\n            return script_hex\n\n        else:\n            raise ValueError(\"Unrecognized address '%s'\" % address )\n\n    else:\n        # address is b58check-encoded\n        vb = keylib.b58check.b58check_version_byte(address)\n        if vb == version_byte:\n            # p2pkh\n            hash160 = binascii.hexlify( keylib.b58check.b58check_decode(address) )\n            script = 'OP_DUP OP_HASH160 {} OP_EQUALVERIFY OP_CHECKSIG'.format(hash160)\n            script_hex = btc_script_to_hex(script)\n            return script_hex\n\n        elif vb == multisig_version_byte:\n            # p2sh\n            hash160 = binascii.hexlify( keylib.b58check.b58check_decode(address) )\n            script = 'OP_HASH160 {} OP_EQUAL'.format(hash160)\n            script_hex = btc_script_to_hex(script)\n            return script_hex\n\n        else:\n            raise ValueError(\"Unrecognized address '%s'\" % address )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_make_data_script( data, **ignored ):\n    if len(data) >= MAX_DATA_LEN * 2:\n        raise ValueError(\"Data hex string is too long\")     # note: data is a hex string\n\n    if len(data) % 2 != 0:\n        raise ValueError(\"Data hex string is not even length\")\n\n    return \"6a{:02x}{}\".format(len(data)/2, data)", "response": "Make a data - behind transaction output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_script_hex_to_address( script_hex, segwit=None ):\n    # TODO: make this support more than bitcoin-like scripts\n    if script_hex.startswith(\"76a914\") and script_hex.endswith(\"88ac\") and len(script_hex) == 50:\n        # p2pkh script\n        hash160_bin = binascii.unhexlify(script_hex[6:-4])\n        return bin_hash160_to_address(hash160_bin, version_byte=version_byte)\n\n    elif script_hex.startswith(\"a914\") and script_hex.endswith(\"87\") and len(script_hex) == 46:\n        # p2sh script\n        hash160_bin = binascii.unhexlify(script_hex[4:-2])\n        return bin_hash160_to_address(hash160_bin, version_byte=multisig_version_byte)\n\n    elif script_hex.startswith('0014') and len(script_hex) == 44:\n        # p2wpkh script (bech32 address)\n        hash160_bin = binascii.unhexlify(script_hex[4:])\n        return segwit_addr_encode(hash160_bin) \n\n    elif script_hex.startswith('0020') and len(script_hex) == 68:\n        # p2wsh script (bech32 address)\n        sha256_bin = binascii.unhexlify(script_hex[4:])\n        return segwit_addr_encode(sha256_bin)\n\n    return None", "response": "Given a hex - encoded script and a segwit address extract an address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btc_make_p2sh_address( script_hex ):\n    h = hashing.bin_hash160(binascii.unhexlify(script_hex))\n    addr = bin_hash160_to_address(h, version_byte=multisig_version_byte)\n    return addr", "response": "Make a P2SH address from a hex script"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btc_make_p2wpkh_address( pubkey_hex ):\n    pubkey_hex = keylib.key_formatting.compress(pubkey_hex)\n    hash160_bin = hashing.bin_hash160(pubkey_hex.decode('hex'))\n    return segwit_addr_encode(hash160_bin)", "response": "Make a p2wpkh address from a hex pubkey"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef btc_make_p2sh_p2wpkh_redeem_script( pubkey_hex ):\n    pubkey_hash = hashing.bin_hash160(pubkey_hex.decode('hex')).encode('hex')\n    redeem_script = btc_script_serialize(['0014' + pubkey_hash])\n    return redeem_script", "response": "Make the redeem script for a p2sh - p2wpkh witness script."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes the redeem script for a p2sh - p2wsh witness script", "response": "def btc_make_p2sh_p2wsh_redeem_script( witness_script_hex ):\n    \"\"\"\n    Make the redeem script for a p2sh-p2wsh witness script\n    \"\"\"\n    witness_script_hash = hashing.bin_sha256(witness_script_hex.decode('hex')).encode('hex')\n    redeem_script = btc_script_serialize(['0020' + witness_script_hash])\n    return redeem_script"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef btc_is_p2sh_address( address ):\n    vb = keylib.b58check.b58check_version_byte( address )\n    if vb == multisig_version_byte:\n        return True\n    else:\n        return False", "response": "Is the given address a p2sh address?"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nis the given address a p2pkh address?", "response": "def btc_is_p2pkh_address( address ):\n    \"\"\"\n    Is the given address a p2pkh address?\n    \"\"\"\n    vb = keylib.b58check.b58check_version_byte( address )\n    if vb == version_byte:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nis the given address a p2wpkh address?", "response": "def btc_is_p2wpkh_address( address ):\n    \"\"\"\n    Is the given address a p2wpkh address?\n    \"\"\"\n    wver, whash = segwit_addr_decode(address)\n    if whash is None:\n        return False\n\n    if len(whash) != 20:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam the given address a p2wsh address?", "response": "def btc_is_p2wsh_address( address ):\n    \"\"\"\n    Is the given address a p2wsh address?\n    \"\"\"\n    wver, whash = segwit_addr_decode(address)\n    if whash is None:\n        return False\n    \n    if len(whash) != 32:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nis the given scriptpubkey a p2sh script?", "response": "def btc_is_p2sh_script( script_hex ):\n    \"\"\"\n    Is the given scriptpubkey a p2sh script?\n    \"\"\"\n    if script_hex.startswith(\"a914\") and script_hex.endswith(\"87\") and len(script_hex) == 46:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btc_address_reencode( address, **blockchain_opts ):\n    # re-encode bitcoin address\n    network = blockchain_opts.get('network', None)\n    opt_version_byte = blockchain_opts.get('version_byte', None)\n\n    if btc_is_segwit_address(address):\n        # bech32 address\n        hrp = None\n        if network == 'mainnet':\n            hrp = 'bc'\n\n        elif network == 'testnet':\n            hrp = 'tb'\n\n        else:\n            if os.environ.get('BLOCKSTACK_TESTNET') == '1' or os.environ.get('BLOCKSTACK_TESTNET3') == '1':\n                hrp = 'tb'\n\n            else:\n                hrp = 'bc'\n\n        wver, whash = segwit_addr_decode(address)\n        return segwit_addr_encode(whash, hrp=hrp, witver=wver)\n\n    else:\n        # base58 address\n        vb = keylib.b58check.b58check_version_byte( address )\n\n        if network == 'mainnet':\n            if vb == 0 or vb == 111:\n                vb = 0\n\n            elif vb == 5 or vb == 196:\n                vb = 5\n\n            else:\n                raise ValueError(\"Unrecognized address %s\" % address)\n        \n        elif network == 'testnet':\n            if vb == 0 or vb == 111:\n                vb = 111\n            \n            elif vb == 5 or vb == 196:\n                vb = 196\n\n            else:\n                raise ValueError(\"Unrecognized address %s\" % address)\n\n        else:\n            if opt_version_byte is not None:\n                vb = opt_version_byte\n\n            elif os.environ.get(\"BLOCKSTACK_TESTNET\") == \"1\" or os.environ.get(\"BLOCKSTACK_TESTNET3\") == \"1\":\n                if vb == 0 or vb == 111:\n                    # convert to testnet p2pkh\n                    vb = 111\n\n                elif vb == 5 or vb == 196:\n                    # convert to testnet p2sh\n                    vb = 196\n\n                else:\n                    raise ValueError(\"unrecognized address %s\" % address)\n\n            else:\n                if vb == 0 or vb == 111:\n                    # convert to mainnet p2pkh\n                    vb = 0\n\n                elif vb == 5 or vb == 196:\n                    # convert to mainnet p2sh\n                    vb = 5\n\n                else:\n                    raise ValueError(\"unrecognized address %s\" % address)\n\n        return keylib.b58check.b58check_encode( keylib.b58check.b58check_decode(address), vb )", "response": "Re - encode an address according to the blockchain options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef btc_is_multisig(privkey_info, **blockchain_opts):\n    try:\n        jsonschema.validate(privkey_info, PRIVKEY_MULTISIG_SCHEMA)\n        return not privkey_info.get('segwit', False)\n    except ValidationError as e:\n        return False", "response": "Returns True if the given private key info represent\n    a multisig bundle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef btc_is_multisig_segwit(privkey_info):\n    try:\n        jsonschema.validate(privkey_info, PRIVKEY_MULTISIG_SCHEMA)\n        if len(privkey_info['private_keys']) == 1:\n            return False\n\n        return privkey_info.get('segwit', False)\n    except ValidationError as e:\n        return False", "response": "Returns True if the given private key info represent\n    a multisig bundle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef btc_is_singlesig(privkey_info, **blockchain_opts):\n    try:\n        jsonschema.validate(privkey_info, PRIVKEY_SINGLESIG_SCHEMA)\n        return True\n    except ValidationError as e:\n        return False", "response": "Returns True if the given private key info represent\n    a single signature bundle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef btc_is_singlesig_segwit(privkey_info):\n    try:\n        jsonschema.validate(privkey_info, PRIVKEY_MULTISIG_SCHEMA)\n        if len(privkey_info['private_keys']) > 1:\n            return False\n\n        return privkey_info.get('segwit', False)\n    except ValidationError:\n        return False", "response": "Is the given key bundle a p2sh - p2wpkh key bundle?"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_get_privkey_address(privkey_info, **blockchain_opts):\n    \n    from .multisig import make_multisig_segwit_address_from_witness_script\n\n    if btc_is_singlesig(privkey_info):\n        return btc_address_reencode( ecdsalib.ecdsa_private_key(privkey_info).public_key().address() )\n    \n    if btc_is_multisig(privkey_info) or btc_is_singlesig_segwit(privkey_info):\n        redeem_script = str(privkey_info['redeem_script'])\n        return btc_make_p2sh_address(redeem_script)\n    \n    if btc_is_multisig_segwit(privkey_info):\n        return make_multisig_segwit_address_from_witness_script(str(privkey_info['redeem_script']))\n\n    raise ValueError(\"Invalid private key info\")", "response": "Get the address for a given private key info bundle"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef segwit_addr_decode(addr, hrp=bech32_prefix):\n    hrpgot, data = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    return (data[0], ''.join([chr(x) for x in decoded]))", "response": "Decode a segwit address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode a segwit script hash to a bech32 address.", "response": "def segwit_addr_encode(witprog_bin, hrp=bech32_prefix, witver=bech32_witver):\n    \"\"\"\n    Encode a segwit script hash to a bech32 address.\n    Returns the bech32-encoded string on success\n    \"\"\"\n    witprog_bytes = [ord(c) for c in witprog_bin]\n    ret = bech32_encode(hrp, [int(witver)] + convertbits(witprog_bytes, 8, 5))\n    assert segwit_addr_decode(hrp, ret) is not (None, None)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_git_describe(git_str, pep440=False):\n    if git_str is None:\n        return None\n    if \"-\" not in git_str:  # currently at a tag\n        return git_str\n    else:\n        # formatted as version-N-githash\n        # want to convert to version.postN-githash\n        git_str = git_str.replace(\"-\", \".post\", 1)\n        if pep440:  # does not allow git hash afterwards\n            return git_str.split(\"-\")[0]\n        else:\n            return git_str.replace(\"-g\", \"+git\")", "response": "format the result of calling git describe as a python version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading version information from VERSION file", "response": "def read_release_version():\n    \"\"\"Read version information from VERSION file\"\"\"\n    try:\n        with open(VERSION_FILE, \"r\") as infile:\n            version = str(infile.read().strip())\n        if len(version) == 0:\n            version = None\n        return version\n    except IOError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_version(pep440=False):\n\n    git_version = format_git_describe(call_git_describe(), pep440=pep440)\n    if git_version is None:  # not a git repository\n        return read_release_version()\n    return git_version", "response": "Returns the version number of the current release."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_git_branch():\n    try:\n        with open(devnull, \"w\") as fnull:\n            arguments = [GIT_COMMAND, 'rev-parse', '--abbrev-ref', 'HEAD']\n            return check_output(arguments, cwd=CURRENT_DIRECTORY,\n                                stderr=fnull).decode(\"ascii\").strip()\n    except (OSError, CalledProcessError):\n        return None", "response": "return the string output of git desribe"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread version information from VERSION file", "response": "def read_git_branch():\n    \"\"\"Read version information from VERSION file\"\"\"\n    try:\n        with open(CC_INIT, \"r\") as f:\n            found = False\n            while not found:\n                line = f.readline().strip().split()\n                try:\n                    found = True if line[0] == '_git_branch' else False\n                except IndexError:\n                    pass\n        branch = line[2].replace('\"', '')\n        if len(branch) == 0:\n            branch = None\n        return branch\n    except IOError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads an XML document from a URI and return a : mod : lxml. etree. Element object.", "response": "def parseUri(stream, uri=None):\n    \"\"\"Read an XML document from a URI, and return a :mod:`lxml.etree`\n    document.\"\"\"\n    return etree.parse(stream, parser=_get_xmlparser(), base_url=uri)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parseString(string, uri=None):\n    return etree.fromstring(string, parser=_get_xmlparser(), base_url=uri)", "response": "Read an XML document provided as a byte string and return a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loadSchema(uri, base_uri=None):\n\n    # uri to use for reporting errors - include base uri if any\n    if uri in _loaded_schemas:\n        return _loaded_schemas[uri]\n\n    error_uri = uri\n    if base_uri is not None:\n        error_uri += ' (base URI %s)' % base_uri\n\n\n    try:\n        logger.debug('Loading schema %s' % uri)\n        _loaded_schemas[uri] = etree.XMLSchema(etree.parse(uri,\n                                                           parser=_get_xmlparser(),\n                                                           base_url=base_uri))\n        return _loaded_schemas[uri]\n    except IOError as io_err:\n        # add a little more detail to the error message - but should still be an IO error\n        raise IOError('Failed to load schema %s : %s' % (error_uri, io_err))\n    except etree.XMLSchemaParseError as parse_err:\n        # re-raise as a schema parse error, but ensure includes details about schema being loaded\n        raise etree.XMLSchemaParseError('Failed to parse schema %s -- %s' % (error_uri, parse_err))", "response": "Load an XSD XML document and return a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading and compile an XSLT document specified by filename or string.", "response": "def load_xslt(filename=None, xsl=None):\n    '''Load and compile an XSLT document (specified by filename or string)\n    for repeated use in transforming XML.\n    '''\n    parser = _get_xmlparser()\n    if filename is not None:\n        xslt_doc = etree.parse(filename, parser=parser)\n    if xsl is not None:\n        xslt_doc = etree.fromstring(xsl, parser=parser)\n\n    return etree.XSLT(xslt_doc)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_xmlparser(xmlclass=XmlObject, validate=False, resolver=None):\n    if validate:\n        if hasattr(xmlclass, 'XSD_SCHEMA') and xmlclass.XSD_SCHEMA is not None:\n            # If the schema has already been loaded, use that.\n            # (since we accessing the *class*, accessing 'xmlschema' returns a property,\n            # not the initialized schema object we actually want).\n            xmlschema = getattr(xmlclass, '_xmlschema', None)\n            # otherwise, load the schema\n            if xmlschema is None:\n                xmlschema = loadSchema(xmlclass.XSD_SCHEMA)\n            opts = {'schema': xmlschema}\n        else:\n            # if configured XmlObject does not have a schema defined, assume DTD validation\n            opts = {'dtd_validation': True}\n    else:\n        # If validation is not requested, then the parsing should fail\n        # only for well-formedness issues.\n        #\n        # Therefore, we must turn off collect_ids, otherwise lxml will\n        # have a problem with duplicate IDs as it collects\n        # them. However, the XML spec declares ID uniqueness as a\n        # validation constraint, not a well-formedness\n        # constraint. (See https://www.w3.org/TR/xml/#id.)\n        opts = {\"collect_ids\": False}\n\n    parser = etree.XMLParser(**opts)\n\n    if resolver is not None:\n        parser.resolvers.add(resolver)\n\n    return parser", "response": "Initialize an instance of lxml. etree. XMLParser with appropriate settings for validation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_xmlobject_from_string(string, xmlclass=XmlObject, validate=False,\n        resolver=None):\n    \"\"\"Initialize an XmlObject from a string.\n\n    If an xmlclass is specified, construct an instance of that class instead\n    of :class:`~eulxml.xmlmap.XmlObject`. It should be a subclass of XmlObject.\n    The constructor will be passed a single node.\n\n    If validation is requested and the specified subclass of :class:`XmlObject`\n    has an XSD_SCHEMA defined, the parser will be configured to validate against\n    the specified schema.  Otherwise, the parser will be configured to use DTD\n    validation, and expect a Doctype declaration in the xml content.\n\n    :param string: xml content to be loaded, as a string\n    :param xmlclass: subclass of :class:`~eulxml.xmlmap.XmlObject` to initialize\n    :param validate: boolean, enable validation; defaults to false\n    :rtype: instance of :class:`~eulxml.xmlmap.XmlObject` requested\n    \"\"\"\n    parser = _get_xmlparser(xmlclass=xmlclass, validate=validate, resolver=resolver)\n    element = etree.fromstring(string, parser)\n    return xmlclass(element)", "response": "Initialize an XmlObject from a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize an XmlObject from a file.", "response": "def load_xmlobject_from_file(filename, xmlclass=XmlObject, validate=False,\n        resolver=None):\n    \"\"\"Initialize an XmlObject from a file.\n\n    See :meth:`load_xmlobject_from_string` for more details; behaves exactly the\n    same, and accepts the same parameters, except that it takes a filename\n    instead of a string.\n\n    :param filename: name of the file that should be loaded as an xmlobject.\n        :meth:`etree.lxml.parse` will accept a file name/path, a file object, a\n        file-like object, or an HTTP or FTP url, however file path and URL are\n        recommended, as they are generally faster for lxml to handle.\n    \"\"\"\n    parser = _get_xmlparser(xmlclass=xmlclass, validate=validate, resolver=resolver)\n\n    tree = etree.parse(filename, parser)\n    return xmlclass(tree.getroot())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_string(self, buf=None, format_abs_ref_as='string',\n                  upper_triangle=True, header=True, index=True, **kwargs):\n        \"\"\"Render a DataFrame to a console-friendly tabular output.\n\n        Wrapper around the :meth:`pandas.DataFrame.to_string` method.\n        \"\"\"\n        out = self._sympy_formatter()\n        out = out._abs_ref_formatter(format_as=format_abs_ref_as)\n        if not upper_triangle:\n            out = out._remove_upper_triangle()\n\n        content = out._frame.to_string(buf=buf, header=header, index=index,\n                                       **kwargs)\n        if not index and not header:\n            # NOTE(the following might be removed in the future\n            # introduced because of formatting bug in pandas\n            # See https://github.com/pandas-dev/pandas/issues/13032)\n            space = ' ' * (out.loc[:, 'atom'].str.len().max()\n                           - len(out.iloc[0, 0]))\n            content = space + content\n        return content", "response": "Render a DataFrame to a console - friendly tabular output."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders a DataFrame to a LaTeX document.", "response": "def to_latex(self, buf=None, upper_triangle=True, **kwargs):\n        \"\"\"Render a DataFrame to a tabular environment table.\n\n        You can splice this into a LaTeX document.\n        Requires ``\\\\usepackage{booktabs}``.\n        Wrapper around the :meth:`pandas.DataFrame.to_latex` method.\n        \"\"\"\n        out = self._sympy_formatter()\n        out = out._abs_ref_formatter(format_as='latex')\n        if not upper_triangle:\n            out = out._remove_upper_triangle()\n        return out._frame.to_latex(buf=buf, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_zmat(cls, inputfile, implicit_index=True):\n        cols = ['atom', 'b', 'bond', 'a', 'angle', 'd', 'dihedral']\n        if implicit_index:\n            zmat_frame = pd.read_table(inputfile, comment='#',\n                                       delim_whitespace=True,\n                                       names=cols)\n            zmat_frame.index = range(1, len(zmat_frame) + 1)\n        else:\n            zmat_frame = pd.read_table(inputfile, comment='#',\n                                       delim_whitespace=True,\n                                       names=['temp_index'] + cols)\n            zmat_frame.set_index('temp_index', drop=True, inplace=True)\n            zmat_frame.index.name = None\n        if pd.isnull(zmat_frame.iloc[0, 1]):\n            zmat_values = [1.27, 127., 127.]\n            zmat_refs = [constants.int_label[x] for x in\n                         ['origin', 'e_z', 'e_x']]\n            for row, i in enumerate(zmat_frame.index[:3]):\n                cols = ['b', 'a', 'd']\n                zmat_frame.loc[:, cols] = zmat_frame.loc[:, cols].astype('O')\n                if row < 2:\n                    zmat_frame.loc[i, cols[row:]] = zmat_refs[row:]\n                    zmat_frame.loc[i, ['bond', 'angle', 'dihedral'][row:]\n                                   ] = zmat_values[row:]\n                else:\n                    zmat_frame.loc[i, 'd'] = zmat_refs[2]\n                    zmat_frame.loc[i, 'dihedral'] = zmat_values[2]\n\n        elif zmat_frame.iloc[0, 1] in constants.int_label.keys():\n            zmat_frame = zmat_frame.replace(\n                {col: constants.int_label for col in ['b', 'a', 'd']})\n        zmat_frame = cls._cast_correct_types(zmat_frame)\n        try:\n            Zmat = cls(zmat_frame)\n        except InvalidReference:\n            raise UndefinedCoordinateSystem(\n                'Your zmatrix cannot be transformed to cartesian coordinates')\n        return Zmat", "response": "Reads a zmat file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_zmat(self, buf=None, upper_triangle=True, implicit_index=True,\n                float_format='{:.6f}'.format, overwrite=True,\n                header=False):\n        \"\"\"Write zmat-file\n\n        Args:\n            buf (str): StringIO-like, optional buffer to write to\n            implicit_index (bool): If implicit_index is set, the zmat indexing\n                is changed to ``range(1, len(self) + 1)``.\n                Using :meth:`~chemcoord.Zmat.change_numbering`\n                Besides the index is omitted while writing which means,\n                that the index is given\n                implicitly by the row number.\n            float_format (one-parameter function): Formatter function\n                to apply to column\u2019s elements if they are floats.\n                The result of this function must be a unicode string.\n            overwrite (bool): May overwrite existing files.\n\n        Returns:\n            formatted : string (or unicode, depending on data and options)\n        \"\"\"\n        out = self.copy()\n        if implicit_index:\n            out = out.change_numbering(new_index=range(1, len(self) + 1))\n        if not upper_triangle:\n            out = out._remove_upper_triangle()\n\n        output = out.to_string(index=(not implicit_index),\n                               float_format=float_format, header=header)\n\n        if buf is not None:\n            if overwrite:\n                with open(buf, mode='w') as f:\n                    f.write(output)\n            else:\n                with open(buf, mode='x') as f:\n                    f.write(output)\n        else:\n            return output", "response": "Write the current object to a zmat - file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_latex(self, buf=None, columns=None, col_space=None, header=True,\n                 index=True, na_rep='NaN', formatters=None, float_format=None,\n                 sparsify=None, index_names=True, bold_rows=True,\n                 column_format=None, longtable=None, escape=None,\n                 encoding=None, decimal='.', multicolumn=None,\n                 multicolumn_format=None, multirow=None):\n        \"\"\"Render a DataFrame to a tabular environment table.\n\n        You can splice this into a LaTeX document.\n        Requires ``\\\\usepackage{booktabs}``.\n        Wrapper around the :meth:`pandas.DataFrame.to_latex` method.\n        \"\"\"\n        return self._frame.to_latex(\n            buf=buf, columns=columns, col_space=col_space, header=header,\n            index=index, na_rep=na_rep, formatters=formatters,\n            float_format=float_format, sparsify=sparsify,\n            index_names=index_names, bold_rows=bold_rows,\n            column_format=column_format, longtable=longtable, escape=escape,\n            encoding=encoding, decimal=decimal, multicolumn=multicolumn,\n            multicolumn_format=multicolumn_format, multirow=multirow)", "response": "Render a DataFrame to a LaTeX table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_xyz(self, buf=None, sort_index=True,\n               index=False, header=False, float_format='{:.6f}'.format,\n               overwrite=True):\n        \"\"\"Write xyz-file\n\n        Args:\n            buf (str): StringIO-like, optional buffer to write to\n            sort_index (bool): If sort_index is true, the\n                :class:`~chemcoord.Cartesian`\n                is sorted by the index before writing.\n            float_format (one-parameter function): Formatter function\n                to apply to column\u2019s elements if they are floats.\n                The result of this function must be a unicode string.\n            overwrite (bool): May overwrite existing files.\n\n        Returns:\n            formatted : string (or unicode, depending on data and options)\n        \"\"\"\n        if sort_index:\n            molecule_string = self.sort_index().to_string(\n                header=header, index=index, float_format=float_format)\n        else:\n            molecule_string = self.to_string(header=header, index=index,\n                                             float_format=float_format)\n\n        # NOTE the following might be removed in the future\n        # introduced because of formatting bug in pandas\n        # See https://github.com/pandas-dev/pandas/issues/13032\n        space = ' ' * (self.loc[:, 'atom'].str.len().max()\n                       - len(self.iloc[0, 0]))\n\n        output = '{n}\\n{message}\\n{alignment}{frame_string}'.format(\n            n=len(self), alignment=space, frame_string=molecule_string,\n            message='Created by chemcoord http://chemcoord.readthedocs.io/')\n\n        if buf is not None:\n            if overwrite:\n                with open(buf, mode='w') as f:\n                    f.write(output)\n            else:\n                with open(buf, mode='x') as f:\n                    f.write(output)\n        else:\n            return output", "response": "Write the current object to a xyz - file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_xyz(cls, buf, start_index=0, get_bonds=True,\n                 nrows=None, engine=None):\n        \"\"\"Read a file of coordinate information.\n\n        Reads xyz-files.\n\n        Args:\n            inputfile (str):\n            start_index (int):\n            get_bonds (bool):\n            nrows (int): Number of rows of file to read.\n                Note that the first two rows are implicitly excluded.\n            engine (str): Wrapper for argument of :func:`pandas.read_csv`.\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        frame = pd.read_table(buf, skiprows=2, comment='#',\n                              nrows=nrows,\n                              delim_whitespace=True,\n                              names=['atom', 'x', 'y', 'z'], engine=engine)\n\n        remove_digits = partial(re.sub, r'[0-9]+', '')\n        frame['atom'] = frame['atom'].apply(remove_digits)\n\n        molecule = cls(frame)\n        molecule.index = range(start_index, start_index + len(molecule))\n\n        if get_bonds:\n            molecule.get_bonds(use_lookup=False, set_lookup=True)\n        return molecule", "response": "Reads a file of coordinate information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_cjson(self, buf=None, **kwargs):\n        cjson_dict = {'chemical json': 0}\n\n        cjson_dict['atoms'] = {}\n\n        atomic_number = constants.elements['atomic_number'].to_dict()\n        cjson_dict['atoms'] = {'elements': {}}\n        cjson_dict['atoms']['elements']['number'] = [\n            int(atomic_number[x]) for x in self['atom']]\n\n        cjson_dict['atoms']['coords'] = {}\n        coords = self.loc[:, ['x', 'y', 'z']].values.reshape(len(self) * 3)\n        cjson_dict['atoms']['coords']['3d'] = [float(x) for x in coords]\n\n        bonds = []\n        bond_dict = self.get_bonds()\n        for i in bond_dict:\n            for b in bond_dict[i]:\n                bonds += [int(i), int(b)]\n                bond_dict[b].remove(i)\n\n        cjson_dict['bonds'] = {'connections': {}}\n        cjson_dict['bonds']['connections']['index'] = bonds\n\n        if buf is not None:\n            with open(buf, mode='w') as f:\n                f.write(json.dumps(cjson_dict, **kwargs))\n        else:\n            return cjson_dict", "response": "Write a cjson file or return dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_cjson(cls, buf):\n        if isinstance(buf, dict):\n            data = buf.copy()\n        else:\n            with open(buf, 'r') as f:\n                data = json.load(f)\n            assert data['chemical json'] == 0\n\n        n_atoms = len(data['atoms']['coords']['3d'])\n        metadata = {}\n        _metadata = {}\n\n        coords = np.array(\n            data['atoms']['coords']['3d']).reshape((n_atoms // 3, 3))\n\n        atomic_number = constants.elements['atomic_number']\n        elements = [dict(zip(atomic_number, atomic_number.index))[x]\n                    for x in data['atoms']['elements']['number']]\n\n        try:\n            connections = data['bonds']['connections']['index']\n        except KeyError:\n            pass\n        else:\n            bond_dict = defaultdict(set)\n            for i, b in zip(connections[::2], connections[1::2]):\n                bond_dict[i].add(b)\n                bond_dict[b].add(i)\n            _metadata['bond_dict'] = dict(bond_dict)\n\n        try:\n            metadata.update(data['properties'])\n        except KeyError:\n            pass\n\n        out = cls(atoms=elements, coords=coords, _metadata=_metadata,\n                  metadata=metadata)\n        return out", "response": "Read a cjson file or a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nviews the molecule with another external viewer.", "response": "def view(self, viewer=None, use_curr_dir=False):\n        \"\"\"View your molecule.\n\n        .. note:: This function writes a temporary file and opens it with\n            an external viewer.\n            If you modify your molecule afterwards you have to recall view\n            in order to see the changes.\n\n        Args:\n            viewer (str): The external viewer to use. If it is None,\n                the default as specified in cc.settings['defaults']['viewer']\n                is used.\n            use_curr_dir (bool): If True, the temporary file is written to\n                the current diretory. Otherwise it gets written to the\n                OS dependendent temporary directory.\n\n        Returns:\n            None:\n        \"\"\"\n        if viewer is None:\n            viewer = settings['defaults']['viewer']\n        if use_curr_dir:\n            TEMP_DIR = os.path.curdir\n        else:\n            TEMP_DIR = tempfile.gettempdir()\n\n        def give_filename(i):\n            filename = 'ChemCoord_' + str(i) + '.xyz'\n            return os.path.join(TEMP_DIR, filename)\n\n        i = 1\n        while os.path.exists(give_filename(i)):\n            i = i + 1\n        self.to_xyz(give_filename(i))\n\n        def open_file(i):\n            \"\"\"Open file and close after being finished.\"\"\"\n            try:\n                subprocess.check_call([viewer, give_filename(i)])\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                raise\n            finally:\n                if use_curr_dir:\n                    pass\n                else:\n                    os.remove(give_filename(i))\n\n        Thread(target=open_file, args=(i,)).start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a Molecule instance of the pymatgen library .", "response": "def get_pymatgen_molecule(self):\n        \"\"\"Create a Molecule instance of the pymatgen library\n\n        .. warning:: The `pymatgen library <http://pymatgen.org>`_ is imported\n            locally in this function and will raise\n            an ``ImportError`` exception, if it is not installed.\n\n        Args:\n            None\n\n        Returns:\n            :class:`pymatgen.core.structure.Molecule`:\n        \"\"\"\n        from pymatgen import Molecule\n        return Molecule(self['atom'].values,\n                        self.loc[:, ['x', 'y', 'z']].values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of the own class from a pymatgen molecule", "response": "def from_pymatgen_molecule(cls, molecule):\n        \"\"\"Create an instance of the own class from a pymatgen molecule\n\n        Args:\n            molecule (:class:`pymatgen.core.structure.Molecule`):\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        new = cls(atoms=[el.value for el in molecule.species],\n                  coords=molecule.cart_coords)\n        return new._to_numeric()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an instance of the own class from an ase. atoms. Atoms object", "response": "def from_ase_atoms(cls, atoms):\n        \"\"\"Create an instance of the own class from an ase molecule\n\n        Args:\n            molecule (:class:`ase.atoms.Atoms`):\n\n        Returns:\n            Cartesian:\n        \"\"\"\n        return cls(atoms=atoms.get_chemical_symbols(), coords=atoms.positions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwork INPLACE on eq", "response": "def _convert_eq(self, eq):\n        \"\"\"WORKS INPLACE on eq\n        \"\"\"\n        rename = dict(enumerate(self.index))\n        eq['eq_sets'] = {rename[k]: {rename[x] for x in v}\n                         for k, v in eq['eq_sets'].items()}\n        eq['sym_ops'] = {rename[k]: {rename[x]: v[x] for x in v}\n                         for k, v in eq['sym_ops'].items()}\n        try:\n            sym_mol = self.from_pymatgen_molecule(eq['sym_mol'])\n            sym_mol.index = self.index\n            eq['sym_mol'] = sym_mol._to_numeric()\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a PointGroup object for the molecule.", "response": "def get_pointgroup(self, tolerance=0.3):\n        \"\"\"Returns a PointGroup object for the molecule.\n\n        Args:\n            tolerance (float): Tolerance to generate the full set of symmetry\n                operations.\n\n        Returns:\n            :class:`~PointGroupOperations`\n\n        \"\"\"\n        PA = self._get_point_group_analyzer(tolerance=tolerance)\n        return PointGroupOperations(PA.sch_symbol, PA.symmops)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_equivalent_atoms(self, tolerance=0.3):\n        PA = self._get_point_group_analyzer(tolerance=tolerance)\n        eq = PA.get_equivalent_atoms()\n        self._convert_eq(eq)\n        return eq", "response": "Returns a dictionary of sets of equivalent atoms with symmetry operations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a symmetrized version of the molecule.", "response": "def symmetrize(self, max_n=10, tolerance=0.3, epsilon=1e-3):\n        \"\"\"Returns a symmetrized molecule\n\n        The equivalent atoms obtained via\n        :meth:`~Cartesian.get_equivalent_atoms`\n        are rotated, mirrored... unto one position.\n        Then the average position is calculated.\n        The average position is rotated, mirrored... back with the inverse\n        of the previous symmetry operations, which gives the\n        symmetrized molecule.\n        This operation is repeated iteratively ``max_n`` times at maximum\n        until the difference between subsequently symmetrized structures is\n        smaller than ``epsilon``.\n\n        Args:\n            max_n (int): Maximum number of iterations.\n            tolerance (float): Tolerance for detecting symmetry.\n                Gets passed as Argument into\n                :class:`~pymatgen.analyzer.symmetry.PointGroupAnalyzer`.\n            epsilon (float): If the elementwise absolute difference of two\n                subsequently symmetrized structures is smaller epsilon,\n                the iteration stops before ``max_n`` is reached.\n\n        Returns:\n            dict: The returned dictionary has three possible keys:\n\n            ``sym_mol``:\n            A symmetrized molecule :class:`~Cartesian`\n\n            ``eq_sets``:\n            A dictionary of indices mapping to sets of indices,\n            each key maps to indices of all equivalent atoms.\n            The keys are guaranteed to be not symmetry-equivalent.\n\n            ``sym_ops``:\n            Twofold nested dictionary.\n            ``operations[i][j]`` gives the symmetry operation\n            that maps atom ``i`` unto ``j``.\n        \"\"\"\n        mg_mol = self.get_pymatgen_molecule()\n        eq = iterative_symmetrize(mg_mol, max_n=max_n, tolerance=tolerance,\n                                  epsilon=epsilon)\n        self._convert_eq(eq)\n        return eq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the entire transaction body into a dict.", "response": "def read_tx_body(ptr, tx):\n    \"\"\"\n    Returns {'ins': [...], 'outs': [...]}\n    \"\"\"\n    _obj = {\"ins\": [], \"outs\": [], 'locktime': None}\n\n    # number of inputs\n    ins = read_var_int(ptr, tx)\n\n    # all inputs\n    for i in range(ins):\n        _obj[\"ins\"].append({\n            \"outpoint\": {\n                \"hash\": read_bytes(ptr, tx, 32)[::-1],\n                \"index\": read_as_int(ptr, tx, 4)\n            },\n            \"script\": read_var_string(ptr, tx),\n            \"sequence\": read_as_int(ptr, tx, 4)\n        })\n\n    # number of outputs\n    outs = read_var_int(ptr, tx)\n\n    # all outputs\n    for i in range(outs):\n        _obj[\"outs\"].append({\n            \"value\": read_as_int(ptr, tx, 8),\n            \"script\": read_var_string(ptr, tx)\n        })\n\n    return _obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the witnesses from the given transaction.", "response": "def read_tx_witnesses(ptr, tx, num_witnesses):\n    \"\"\"\n    Returns an array of witness scripts.\n    Each witness will be a bytestring (i.e. encoding the witness script)\n    \"\"\"\n    witnesses = []\n    for i in xrange(0, num_witnesses):\n\n        witness_stack_len = read_var_int(ptr, tx)\n        witness_stack = []\n\n        for j in xrange(0, witness_stack_len):\n\n            stack_item = read_var_string(ptr, tx)\n            witness_stack.append(stack_item)\n             \n        witness_script = btc_witness_script_serialize(witness_stack).decode('hex')\n        witnesses.append(witness_script)\n\n    return witnesses"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a var - string from a string", "response": "def make_var_string(string):\n    \"\"\"\n    Make a var-string (a var-int with the length, concatenated with the data)\n    Return the hex-encoded string\n    \"\"\"\n    s = None\n    if isinstance(string, str) and re.match('^[0-9a-fA-F]*$', string):\n        # convert from hex to bin, safely\n        s = binascii.unhexlify(string)\n    else:\n        s = string[:]\n\n    buf = encoding.num_to_var_int(len(s)) + s\n    return buf.encode('hex')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _btc_witness_serialize_unit(unit):\n    \n    if isinstance(unit, int):\n        # pass literal\n        return encoding.from_int_to_byte(unit)\n\n    elif unit is None:\n        # None means OP_0\n        return b'\\x00'\n\n    else:\n        # return as a varint-prefixed string\n        return make_var_string(unit)", "response": "Serialize a single item of a BTC witness script into a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_witness_script_serialize(_stack):\n    stack = _stack\n    if encoding.json_is_base(_stack, 16):\n        # hex-to-bin all hex strings \n        stack = encoding.json_changebase(_stack, lambda x: binascii.unhexlify(x))\n\n    return encoding.safe_hexlify(_btc_witness_serialize_unit(len(stack)) + ''.join(map(lambda stack_unit: _btc_witness_serialize_unit(stack_unit), stack)))", "response": "Given a serialized witness script stack return a hex - encoded script that is compatible with the witness script."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a hex - encoded serialized witness script turn it into a witness stack", "response": "def btc_witness_script_deserialize(_script):\n    \"\"\"\n    Given a hex-encoded serialized witness script, turn it into a witness stack\n    (i.e. an array of Nones, ints, and strings)\n    \"\"\"\n\n    script = None\n    if isinstance(_script, str) and re.match('^[0-9a-fA-F]*$', _script):\n        # convert from hex to bin, safely\n        script = binascii.unhexlify(_script)\n    else:\n        script = _script[:]\n\n    # pointer to byte offset in _script (as an array due to Python scoping rules)\n    ptr = [0]\n\n    witness_stack_len = read_var_int(ptr, script)\n    witness_stack = []\n\n    for _ in xrange(0, witness_stack_len):\n\n        stack_item = read_var_string(ptr, script)\n        witness_stack.append(stack_item)\n\n    return witness_stack"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_tx_deserialize(_tx, **blockchain_opts):\n\n    tx = None\n    if isinstance(_tx, str) and re.match('^[0-9a-fA-F]*$', _tx):\n        # convert from hex to bin, safely\n        tx = binascii.unhexlify(_tx)\n    else:\n        tx = _tx[:]\n\n    # pointer to byte offset in _tx (as an array due to Python scoping rules)\n    ptr = [0]\n    \n    # top-level tx\n    obj = {\"ins\": [], \"outs\": [], 'version': None, 'locktime': None}\n\n    # get version\n    obj[\"version\"] = read_as_int(ptr, tx, 4)\n\n    # segwit? (bip143)\n    # 5th byte will be 0 and 6th byte will be flags (nonzero) if so\n    bip143 = peek_bytes(ptr, tx, 2)\n    if ord(bip143[0]) == 0 and ord(bip143[1]) != 0:\n        # segwit\n        # consume marker\n        read_bytes(ptr, tx, 2)\n\n        # get the rest of the body\n        body = read_tx_body(ptr, tx)\n        obj['ins'] = body['ins']\n        obj['outs'] = body['outs']\n     \n        # read witnesses for each input\n        witness_scripts = read_tx_witnesses(ptr, tx, len(obj['ins']))\n      \n        if len(witness_scripts) != len(obj['ins']):\n            raise ValueError('Invald number of witnesses in {}'.format(_tx))\n\n        for i in xrange(0, len(witness_scripts)):\n            obj['ins'][i]['witness_script'] = witness_scripts[i]\n        \n    else:\n        # non-segwit\n        body = read_tx_body(ptr, tx)\n        obj['ins'] = body['ins']\n        obj['outs'] = body['outs']\n\n    # locktime\n    obj[\"locktime\"] = read_as_int(ptr, tx, 4)\n\n    if not ptr[0] == len(tx):\n        # log.warning('Did not parse entire tx ({} bytes remaining)'.format(len(tx) - ptr[0]))\n        raise ValueError('Did not parse entire tx ({} bytes remaining)'.format(len(tx) - ptr[0]))\n\n    # hexlify each byte field \n    obj = encoding.json_changebase(obj, lambda x: encoding.safe_hexlify(x))\n    return obj", "response": "Given a hex - encoded transaction deserialize it into a dict containing the top - level object that is returned by btc_tx_serialize."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a transaction dict returned by btc_tx_deserialize convert it back into a base - 16 byte string.", "response": "def btc_tx_serialize(_txobj):\n    \"\"\"\n    Given a transaction dict returned by btc_tx_deserialize, convert it back into a\n    hex-encoded byte string.\n\n    Derived from code written by Vitalik Buterin in pybitcointools (https://github.com/vbuterin/pybitcointools)\n    \"\"\"\n    \n    # output buffer\n    o = []\n    txobj = None\n    if encoding.json_is_base(_txobj, 16):\n        # txobj is built from hex strings already.  deserialize them \n        txobj = encoding.json_changebase(_txobj, lambda x: binascii.unhexlify(x))\n    else:\n        txobj = copy.deepcopy(_txobj)\n\n    # version\n    o.append(encoding.encode(txobj[\"version\"], 256, 4)[::-1])\n\n    # do we have any witness scripts?\n    have_witness = False\n    for inp in txobj['ins']:\n        if inp.has_key('witness_script') and len(inp['witness_script']) > 0:\n            have_witness = True\n            break\n\n    if have_witness:\n        # add segwit marker \n        o.append('\\x00\\x01')\n    \n    # number of inputs\n    o.append(encoding.num_to_var_int(len(txobj[\"ins\"])))\n\n    # all inputs\n    for inp in txobj[\"ins\"]:\n        # input tx hash\n        o.append(inp[\"outpoint\"][\"hash\"][::-1])\n\n        # input tx outpoint\n        o.append(encoding.encode(inp[\"outpoint\"][\"index\"], 256, 4)[::-1])\n\n        # input scriptsig\n        script = inp.get('script')\n        if not script:\n            script = bytes()\n\n        scriptsig = encoding.num_to_var_int(len(script)) + script\n        o.append(scriptsig)\n\n        # sequence\n        o.append(encoding.encode(inp.get(\"sequence\", UINT_MAX - 1), 256, 4)[::-1])\n\n    # number of outputs\n    o.append(encoding.num_to_var_int(len(txobj[\"outs\"])))\n\n    # all outputs\n    for out in txobj[\"outs\"]:\n        # value\n        o.append(encoding.encode(out[\"value\"], 256, 8)[::-1])\n\n        # scriptPubKey\n        scriptpubkey = encoding.num_to_var_int(len(out['script'])) + out['script']\n        o.append(scriptpubkey)\n\n    # add witnesses \n    if have_witness:\n        for inp in txobj['ins']:\n            witness_script = inp.get('witness_script')\n            if not witness_script:\n                witness_script = '\\x00'\n\n            o.append(witness_script)\n\n    # locktime\n    o.append(encoding.encode(txobj[\"locktime\"], 256, 4)[::-1])\n\n    # full string\n    ret = ''.join( encoding.json_changebase(o, lambda x: encoding.safe_hexlify(x)) )\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_bitcoind_tx_serialize( tx ):\n     tx_ins = []\n     tx_outs = []\n\n     try:\n         for inp in tx['vin']:\n             next_inp = {\n                \"outpoint\": {\n                   \"index\": int(inp['vout']),\n                   \"hash\": str(inp['txid'])\n                }\n             }\n             if 'sequence' in inp:\n                 next_inp['sequence'] = int(inp['sequence'])\n             else:\n                 next_inp['sequence'] = UINT_MAX\n\n             if 'scriptSig' in inp:\n                 next_inp['script'] = str(inp['scriptSig']['hex'])\n             else:\n                 next_inp['script'] = \"\"\n\n             if 'txinwitness' in inp:\n                 next_inp['witness_script'] = btc_witness_script_serialize(inp['txinwitness'])\n\n             tx_ins.append(next_inp)\n\n         for out in tx['vout']:\n         \n             assert out['value'] < 1000, \"High transaction value\\n%s\" % simplejson.dumps(tx, indent=4, sort_keys=True)\n             next_out = {\n                'value': int(Decimal(out['value'] * 10**8)),\n                'script': str(out['scriptPubKey']['hex'])\n             }\n             tx_outs.append(next_out)\n\n         tx_fields = {\n            \"locktime\": int(tx['locktime']),\n            \"version\": int(tx['version']),\n            \"ins\": tx_ins,\n            \"outs\": tx_outs\n         }\n\n         tx_serialized = btc_tx_serialize( tx_fields )\n         return str(tx_serialized)\n\n     except KeyError, ke:\n         if btc_bitcoind_tx_is_coinbase(tx) and 'hex' in tx.keys():\n             tx_serialized = tx['hex']\n             return str(tx_serialized)\n\n         log.error(\"Key error in:\\n%s\" % simplejson.dumps(tx, indent=4, sort_keys=True))\n         traceback.print_exc()\n         raise ke", "response": "Convert a *Bitcoind* - given transaction into its hex string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nis this serialized transaction a segwit transaction?", "response": "def btc_tx_is_segwit( tx_serialized ):\n    \"\"\"\n    Is this serialized (hex-encoded) transaction a segwit transaction?\n    \"\"\"\n    marker_offset = 4       # 5th byte is the marker byte\n    flag_offset = 5         # 6th byte is the flag byte\n    \n    marker_byte_string = tx_serialized[2*marker_offset:2*(marker_offset+1)]\n    flag_byte_string = tx_serialized[2*flag_offset:2*(flag_offset+1)]\n\n    if marker_byte_string == '00' and flag_byte_string != '00':\n        # segwit (per BIP144)\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btc_tx_witness_strip( tx_serialized ):\n    if not btc_tx_is_segwit(tx_serialized):\n        # already strippped\n        return tx_serialized\n     \n    tx = btc_tx_deserialize(tx_serialized)\n    for inp in tx['ins']:\n        del inp['witness_script']\n\n    tx_stripped = btc_tx_serialize(tx)\n    return tx_stripped", "response": "Strip the witness information from a serialized transaction"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_tx_get_hash( tx_serialized, hashcode=None ):\n    if btc_tx_is_segwit(tx_serialized):\n        raise ValueError('Segwit transaction: {}'.format(tx_serialized))\n\n    tx_bin = binascii.unhexlify(tx_serialized)\n    if hashcode:\n        return binascii.hexlify( hashing.bin_double_sha256(tx_bin + encoding.encode(int(hashcode), 256, 4)[::-1]) )\n\n    else:\n        return binascii.hexlify( hashing.bin_double_sha256(tx_bin)[::-1] )", "response": "This function takes a hex tx and returns a hash of the hex tx."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_tx_script_to_asm( script_hex ):\n    if len(script_hex) == 0:\n        return \"\"\n\n    try:\n        script_array = btc_script_deserialize(script_hex)\n    except:\n        log.error(\"Failed to convert '%s' to assembler\" % script_hex)\n        raise\n\n    script_tokens = []\n    for token in script_array:\n        if token is None:\n            token = 0\n\n        token_name = None\n\n        if type(token) in [int,long]:\n            token_name = OPCODE_NAMES.get(token, None)\n            if token_name is None:\n                token_name = str(token)\n        \n        else:\n            token_name = token\n\n        script_tokens.append(token_name)\n\n    return \" \".join(script_tokens)", "response": "Convert a script into an assembler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_tx_extend(partial_tx_hex, new_inputs, new_outputs, **blockchain_opts):\n\n    # recover tx\n    tx = btc_tx_deserialize(partial_tx_hex)\n    tx_inputs, tx_outputs = tx['ins'], tx['outs']\n    locktime, version = tx['locktime'], tx['version']\n\n    tx_inputs += new_inputs\n    tx_outputs += new_outputs\n\n    new_tx = {\n        'ins': tx_inputs,\n        'outs': tx_outputs,\n        'locktime': locktime,\n        'version': version,\n    }\n\n    new_unsigned_tx = btc_tx_serialize(new_tx)\n    return new_unsigned_tx", "response": "Given an unsigned serialized transaction add more inputs and outputs to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a DER - encoded length field of a given length.", "response": "def btc_tx_der_encode_length(l):\n    \"\"\"\n    Return a DER-encoded length field\n\n    Based on code from python-ecdsa (https://github.com/warner/python-ecdsa)\n    by Brian Warner.  Subject to the MIT license.\n    \"\"\"\n    if l < 0:\n        raise ValueError(\"length cannot be negative\")\n\n    if l < 0x80:\n        return int2byte(l)\n    s = (\"%x\" % l).encode()\n    if len(s) % 2:\n        s = b(\"0\") + s\n    s = binascii.unhexlify(s)\n    llen = len(s)\n    return int2byte(0x80 | llen) + s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DER - encoded sequence of bytes.", "response": "def btc_tx_der_encode_sequence(*encoded_pieces):\n    \"\"\"\n    Return a DER-encoded sequence\n\n    Based on code from python-ecdsa (https://github.com/warner/python-ecdsa)\n    by Brian Warner.  Subject to the MIT license.\n    \"\"\"\n    # borrowed from python-ecdsa\n    total_len = sum([len(p) for p in encoded_pieces])\n    return b('\\x30') + btc_tx_der_encode_length(total_len) + b('').join(encoded_pieces)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the sighash of a non - segwit transaction.", "response": "def btc_tx_sighash( tx, idx, script, hashcode=SIGHASH_ALL):\n    \"\"\"\n    Calculate the sighash of a non-segwit transaction.\n\n    If it's SIGHASH_NONE, then digest the inputs but no outputs \n    If it's SIGHASH_SINGLE, then digest all inputs and all outputs up to i (excluding values and scripts), and fully digest the ith input and output\n    If it's (something) | SIGHASH_ANYONECANPAY, then only digest the ith input.\n    \n    Return the double-sha256 digest of the relevant fields.\n\n    THIS DOES NOT WORK WITH SEGWIT OUTPUTS\n\n    Adapted from https://github.com/vbuterin/pybitcointools, by Vitalik Buterin\n    \"\"\"\n\n    txobj = btc_tx_deserialize(tx)\n\n    idx = int(idx)\n    hashcode = int(hashcode)\n    newtx = copy.deepcopy(txobj)\n\n    # remove all scriptsigs in all inputs, except for the ith input's scriptsig.\n    # the other inputs will be 'partially signed', except for SIGHASH_ANYONECANPAY mode.\n    for i in xrange(0, len(newtx['ins'])):\n        newtx['ins'][i][\"script\"] = ''\n        if i == idx:\n            if newtx['ins'][i].has_key('witness_script') and newtx['ins'][i]['witness_script']:\n                raise ValueError('this method does not handle segwit inputs')\n\n        if newtx['ins'][i].has_key('witness_script'):\n            del newtx['ins'][i]['witness_script']\n\n    newtx[\"ins\"][idx][\"script\"] = script\n\n    if (hashcode & 0x1f) == SIGHASH_NONE:\n        # don't care about the outputs with this signature\n        newtx[\"outs\"] = []\n        for inp in newtx['ins']:\n            inp['sequence'] = 0\n\n    elif (hashcode & 0x1f) == SIGHASH_SINGLE:\n        # only signing for this input.\n        # all outputs after this input will not be signed.\n        # all outputs before this input will be partially signed (but not their values or scripts)\n        if len(newtx['ins']) > len(newtx['outs']):\n            raise ValueError('invalid hash code: {} inputs but {} outputs'.format(len(newtx['ins']), len(newtx['outs'])))\n\n        newtx[\"outs\"] = newtx[\"outs\"][:len(newtx[\"ins\"])]\n        for out in newtx[\"outs\"][:len(newtx[\"ins\"]) - 1]:\n            out['value'] = 2**64 - 1\n            out['script'] = \"\"\n\n    elif (hashcode & SIGHASH_ANYONECANPAY) != 0:\n        # only going to sign this specific input, and nothing else\n        newtx[\"ins\"] = [newtx[\"ins\"][idx]]\n   \n    signing_tx = btc_tx_serialize(newtx)\n    sighash = btc_tx_get_hash( signing_tx, hashcode )\n    return sighash"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the sighash for a segwit transaction according to bip143", "response": "def btc_tx_sighash_segwit(tx, i, prevout_amount, prevout_script, hashcode=SIGHASH_ALL):\n    \"\"\"\n    Calculate the sighash for a segwit transaction, according to bip143\n    \"\"\"\n    txobj = btc_tx_deserialize(tx)\n\n    hash_prevouts = encoding.encode(0, 256, 32)\n    hash_sequence = encoding.encode(0, 256, 32)\n    hash_outputs = encoding.encode(0, 256, 32)\n\n    if (hashcode & SIGHASH_ANYONECANPAY) == 0:\n        prevouts = ''\n        for inp in txobj['ins']:\n            prevouts += hashing.reverse_hash(inp['outpoint']['hash'])\n            prevouts += encoding.encode(inp['outpoint']['index'], 256, 4)[::-1].encode('hex')\n\n        hash_prevouts = hashing.bin_double_sha256(prevouts.decode('hex'))\n\n        # print 'prevouts: {}'.format(prevouts)\n\n    if (hashcode & SIGHASH_ANYONECANPAY) == 0 and (hashcode & 0x1f) != SIGHASH_SINGLE and (hashcode & 0x1f) != SIGHASH_NONE:\n        sequences = ''\n        for inp in txobj['ins']:\n            sequences += encoding.encode(inp['sequence'], 256, 4)[::-1].encode('hex')\n\n        hash_sequence = hashing.bin_double_sha256(sequences.decode('hex'))\n\n        # print 'sequences: {}'.format(sequences)\n\n    if (hashcode & 0x1f) != SIGHASH_SINGLE and (hashcode & 0x1f) != SIGHASH_NONE:\n        outputs = ''\n        for out in txobj['outs']:\n            outputs += encoding.encode(out['value'], 256, 8)[::-1].encode('hex')\n            outputs += make_var_string(out['script'])\n\n        hash_outputs = hashing.bin_double_sha256(outputs.decode('hex'))\n\n        # print 'outputs: {}'.format(outputs)\n\n    elif (hashcode & 0x1f) == SIGHASH_SINGLE and i < len(txobj['outs']):\n        outputs = ''\n        outputs += encoding.encode(txobj['outs'][i]['value'], 256, 8)[::-1].encode('hex')\n        outputs += make_var_string(txobj['outs'][i]['script'])\n\n        hash_outputs = hashing.bin_double_sha256(outputs.decode('hex'))\n\n        # print 'outputs: {}'.format(outputs)\n\n    # print 'hash_prevouts: {}'.format(hash_prevouts.encode('hex'))\n    # print 'hash_sequence: {}'.format(hash_sequence.encode('hex'))\n    # print 'hash_outputs: {}'.format(hash_outputs.encode('hex'))\n    # print 'prevout_script: {}'.format(prevout_script)\n    # print 'prevout_amount: {}'.format(prevout_amount)\n\n    sighash_preimage = ''\n    sighash_preimage += encoding.encode(txobj['version'], 256, 4)[::-1].encode('hex')\n    sighash_preimage += hash_prevouts.encode('hex')\n    sighash_preimage += hash_sequence.encode('hex')\n\n    # this input's prevout, script, amount, and sequence\n    sighash_preimage += hashing.reverse_hash(txobj['ins'][i]['outpoint']['hash'])\n    sighash_preimage += encoding.encode(txobj['ins'][i]['outpoint']['index'], 256, 4)[::-1].encode('hex')\n    sighash_preimage += make_var_string(prevout_script)\n    sighash_preimage += encoding.encode(prevout_amount, 256, 8)[::-1].encode('hex')\n    sighash_preimage += encoding.encode(txobj['ins'][i]['sequence'], 256, 4)[::-1].encode('hex')\n\n    sighash_preimage += hash_outputs.encode('hex')\n    sighash_preimage += encoding.encode(txobj['locktime'], 256, 4)[::-1].encode('hex')\n    sighash_preimage += encoding.encode(hashcode, 256, 4)[::-1].encode('hex')\n\n    sighash = hashing.bin_double_sha256(sighash_preimage.decode('hex')).encode('hex')\n\n    # print 'sighash_preimage: {}'.format(sighash_preimage)\n    # print 'sighash: {}'.format(sighash)\n\n    return sighash"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsign a single input of a transaction.", "response": "def btc_tx_make_input_signature(tx, idx, prevout_script, privkey_str, hashcode):\n    \"\"\"\n    Sign a single input of a transaction, given the serialized tx,\n    the input index, the output's scriptPubkey, and the hashcode.\n\n    tx must be a hex-encoded string\n    privkey_str must be a hex-encoded private key\n\n    Return the hex signature.\n\n    THIS DOES NOT WORK WITH SEGWIT TRANSACTIONS\n    \"\"\"\n    if btc_tx_is_segwit(tx):\n        raise ValueError('tried to use the standard sighash to sign a segwit transaction')\n\n    pk = ecdsalib.ecdsa_private_key(str(privkey_str))\n    priv = pk.to_hex()\n\n    # get the parts of the tx we actually need to sign\n    sighash = btc_tx_sighash(tx, idx, prevout_script, hashcode)\n    # print 'non-segwit sighash: {}'.format(sighash)\n\n    # sign using uncompressed private key\n    pk_uncompressed_hex, pubk_uncompressed_hex = ecdsalib.get_uncompressed_private_and_public_keys(priv)\n    sigb64 = ecdsalib.sign_digest( sighash, priv )\n\n    # sanity check \n    # assert ecdsalib.verify_digest( txhash, pubk_uncompressed_hex, sigb64 )\n\n    sig_r, sig_s = ecdsalib.decode_signature(sigb64)\n    sig_bin = btc_tx_der_encode_signature(sig_r, sig_s)\n    sig = sig_bin.encode('hex') + encoding.encode(hashcode, 16, 2)\n\n    return sig"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsign a single input of a transaction, given the serialized tx, the input index, the output's scriptPubkey, and the hashcode. tx must be a hex-encoded string privkey_str must be a hex-encoded private key Return the hex signature.", "response": "def btc_tx_make_input_signature_segwit(tx, idx, prevout_amount, prevout_script, privkey_str, hashcode):\n    \"\"\"\n    Sign a single input of a transaction, given the serialized tx,\n    the input index, the output's scriptPubkey, and the hashcode.\n\n    tx must be a hex-encoded string\n    privkey_str must be a hex-encoded private key\n\n    Return the hex signature.\n    \"\"\"\n    # always compressed\n    if len(privkey_str) == 64:\n        privkey_str += '01'\n\n    pk = ecdsalib.ecdsa_private_key(str(privkey_str))\n    pubk = pk.public_key()\n    \n    priv = pk.to_hex()\n\n    # must always be compressed\n    pub = keylib.key_formatting.compress(pubk.to_hex())\n    sighash = btc_tx_sighash_segwit(tx, idx, prevout_amount, prevout_script, hashcode=hashcode)\n    \n    # sign using uncompressed private key\n    # pk_uncompressed_hex, pubk_uncompressed_hex = ecdsalib.get_uncompressed_private_and_public_keys(priv)\n    sigb64 = ecdsalib.sign_digest( sighash, priv )\n\n    # sanity check \n    # assert ecdsalib.verify_digest( txhash, pubk_uncompressed_hex, sigb64 )\n\n    sig_r, sig_s = ecdsalib.decode_signature(sigb64)\n    sig_bin = btc_tx_der_encode_signature(sig_r, sig_s)\n    sig = sig_bin.encode('hex') + encoding.encode(hashcode, 16, 2)\n\n    # print 'segwit signature: {}'.format(sig)\n    return sig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef btc_tx_sign_multisig(tx, idx, redeem_script, private_keys, hashcode=SIGHASH_ALL):\n    \n    from .multisig import parse_multisig_redeemscript\n\n    # sign in the right order.  map all possible public keys to their private key\n    txobj = btc_tx_deserialize(str(tx))\n\n    privs = {}\n    for pk in private_keys:\n        pubk = ecdsalib.ecdsa_private_key(pk).public_key().to_hex()\n\n        compressed_pubkey = keylib.key_formatting.compress(pubk)\n        uncompressed_pubkey = keylib.key_formatting.decompress(pubk)\n\n        privs[compressed_pubkey] = pk\n        privs[uncompressed_pubkey] = pk\n\n    m, public_keys = parse_multisig_redeemscript(str(redeem_script))\n\n    used_keys, sigs = [], []\n    for public_key in public_keys:\n        if public_key not in privs:\n            continue\n\n        if len(used_keys) == m:\n            break\n\n        if public_key in used_keys:\n            raise ValueError('Tried to reuse key in redeem script: {}'.format(public_key))\n\n        pk_str = privs[public_key]\n        used_keys.append(public_key)\n\n        sig = btc_tx_make_input_signature(tx, idx, redeem_script, pk_str, hashcode)\n        sigs.append(sig)\n\n    if len(used_keys) != m:\n        raise ValueError('Missing private keys (used {}, required {})'.format(len(used_keys), m))\n\n    txobj[\"ins\"][idx][\"script\"] = btc_script_serialize([None] + sigs + [redeem_script])\n    return btc_tx_serialize(txobj)", "response": "Sign a p2sh multisig input."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef btc_tx_sign_multisig_segwit(tx, idx, prevout_amount, witness_script, private_keys, hashcode=SIGHASH_ALL, hashcodes=None, native=False):\n\n    from .multisig import parse_multisig_redeemscript\n\n    if hashcodes is None:\n        hashcodes = [hashcode] * len(private_keys)\n\n    txobj = btc_tx_deserialize(str(tx))\n    privs = {}\n    for pk in private_keys:\n        pubk = ecdsalib.ecdsa_private_key(pk).public_key().to_hex()\n\n        compressed_pubkey = keylib.key_formatting.compress(pubk)\n        privs[compressed_pubkey] = pk\n\n    m, public_keys = parse_multisig_redeemscript(witness_script)\n\n    used_keys, sigs = [], []\n    for i, public_key in enumerate(public_keys):\n        if public_key not in privs:\n            continue\n\n        if len(used_keys) == m:\n            break\n\n        if public_key in used_keys:\n            raise ValueError('Tried to reuse key in witness script: {}'.format(public_key))\n\n        pk_str = privs[public_key]\n        used_keys.append(public_key)\n\n        sig = btc_tx_make_input_signature_segwit(tx, idx, prevout_amount, witness_script, pk_str, hashcodes[i])\n        sigs.append(sig)\n\n        # print ''\n\n    if len(used_keys) != m:\n        raise ValueError('Missing private keys (used {}, required {})'.format(len(used_keys), m))\n   \n    if native: \n        # native p2wsh\n        txobj['ins'][idx]['witness_script'] = btc_witness_script_serialize([None] + sigs + [witness_script]) \n\n        # print 'segwit multisig: native p2wsh: witness script {}'.format(txobj['ins'][idx]['witness_script'])\n\n    else:\n        # p2sh-p2wsh\n        redeem_script = btc_make_p2sh_p2wsh_redeem_script(witness_script)\n        txobj['ins'][idx]['script'] = redeem_script\n        txobj['ins'][idx]['witness_script'] = btc_witness_script_serialize([None] + sigs + [witness_script]) \n        \n        # print 'segwit multisig: p2sh p2wsh: witness script {}'.format(txobj['ins'][idx]['witness_script'])\n        # print 'segwit multisig: p2sh p2wsh: redeem script {}'.format(txobj['ins'][idx]['script'])\n    \n    return btc_tx_serialize(txobj)", "response": "Sign a native p2wsh or p2sh - p2wsh multisig input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef btc_tx_sign(tx_hex, idx, prevout_script, prevout_amount, private_key_info, scriptsig_type, hashcode=SIGHASH_ALL, hashcodes=None, redeem_script=None, witness_script=None):\n    new_tx = None\n    \n    # print 'sign input {} as {}'.format(idx, scriptsig_type)\n\n    if scriptsig_type in ['p2pkh', 'p2pk']:\n        if not btc_is_singlesig(private_key_info):\n            raise ValueError('Need only one private key for {}'.format(scriptsig_type))\n\n        pk = ecdsalib.ecdsa_private_key(str(private_key_info))\n        pubk = pk.public_key()\n        pub = pubk.to_hex()\n\n        sig = btc_tx_make_input_signature(tx_hex, idx, prevout_script, private_key_info, hashcode)\n\n        # print 'non-segwit sig: {}'.format(sig)\n        # print 'non-segwit pubk: {}'.format(pub)\n\n        # NOTE: sig and pub need to be hex-encoded\n        txobj = btc_tx_deserialize(str(tx_hex))\n\n        if scriptsig_type == 'p2pkh':\n            # scriptSig + scriptPubkey is <signature> <pubkey> OP_DUP OP_HASH160 <pubkeyhash> OP_EQUALVERIFY OP_CHECKSIG\n            txobj['ins'][idx]['script'] = btc_script_serialize([sig, pub])\n\n        else:\n            # p2pk\n            # scriptSig + scriptPubkey is <signature> <pubkey> OP_CHECKSIG\n            txobj['ins'][idx]['script'] = btc_script_serialize([sig])\n\n        new_tx = btc_tx_serialize(txobj)\n\n    elif scriptsig_type == 'p2wpkh' or scriptsig_type == 'p2sh-p2wpkh':\n        # must be a segwit singlesig bundle\n        if not btc_is_singlesig_segwit(private_key_info):\n            raise ValueError('Keys are not for p2wpkh or p2sh-p2wpkh')\n\n        privkey_str = str(btc_get_singlesig_privkey(private_key_info))\n        pk = ecdsalib.ecdsa_private_key(privkey_str)\n        pubk = pk.public_key()\n        pub = keylib.key_formatting.compress(pubk.to_hex())\n\n        # special bip141 rule: this is always 0x1976a914{20-byte-pubkey-hash}88ac\n        prevout_script_sighash = '76a914' + hashing.bin_hash160(pub.decode('hex')).encode('hex') + '88ac'\n\n        sig = btc_tx_make_input_signature_segwit(tx_hex, idx, prevout_amount, prevout_script_sighash, privkey_str, hashcode)\n\n        txobj = btc_tx_deserialize(str(tx_hex))\n        txobj['ins'][idx]['witness_script'] = btc_witness_script_serialize([sig, pub])\n\n        if scriptsig_type == 'p2wpkh':\n            # native\n            # NOTE: sig and pub need to be hex-encoded\n            txobj['ins'][idx]['script'] = ''\n\n            if redeem_script:\n                # goes in scriptSig \n                txobj['ins'][idx]['script'] = btc_script_serialize([redeem_script])\n\n        else:\n            # p2sh-p2wpkh\n            redeem_script = btc_make_p2sh_p2wpkh_redeem_script(pub)\n            txobj['ins'][idx]['script'] = redeem_script\n\n            # print 'scriptsig {} from {} is {}'.format(pub, privkey_str, txobj['ins'][idx]['script'])\n\n        new_tx = btc_tx_serialize(txobj)\n\n        # print 'scriptsig type: {}'.format(scriptsig_type)\n        # print 'segwit scriptsig: {}'.format(txobj['ins'][idx]['script'])\n        # print 'segwit witness script: {}'.format(txobj['ins'][idx]['witness_script'])\n\n    elif scriptsig_type == 'p2wsh' or scriptsig_type == 'p2sh-p2wsh':\n        # only support p2wsh for multisig purposes at this time\n        if not btc_is_multisig_segwit(private_key_info):\n            raise ValueError('p2wsh requires a multisig key bundle')\n\n        native = None\n        if scriptsig_type == 'p2wsh':\n            native = True\n        else:\n            native = False\n\n        new_tx = btc_tx_sign_multisig_segwit(tx_hex, idx, prevout_amount, private_key_info['redeem_script'], private_key_info['private_keys'], hashcode=hashcode, hashcodes=hashcodes, native=native)\n       \n        txobj = btc_tx_deserialize(new_tx)\n\n        # print 'segwit scriptsig: {}'.format(txobj['ins'][idx]['script'])\n        # print 'segwit witness script: {}'.format(txobj['ins'][idx]['witness_script'])\n\n    elif scriptsig_type == 'p2sh':\n    \n        if not redeem_script:\n            # p2sh multisig\n            if not btc_is_multisig(private_key_info):\n                raise ValueError('No redeem script given, and not a multisig key bundle')\n\n            new_tx = btc_tx_sign_multisig(tx_hex, idx, private_key_info['redeem_script'], private_key_info['private_keys'], hashcode=hashcode)\n\n        else:\n            # NOTE: sig and pub need to be hex-encoded\n            txobj = btc_tx_deserialize(str(tx_hex))\n\n            # scriptSig + scriptPubkey is <redeem script> OP_HASH160 <script hash> OP_EQUAL\n            txobj['ins'][idx]['script'] = btc_script_serialize([redeem_script]) \n            new_tx = btc_tx_serialize(txobj)\n\n    else:\n        raise ValueError(\"Unknown script type {}\".format(scriptsig_type))\n\n    return new_tx", "response": "Insert a scriptsig for an input that will later be spent by a p2pkh, p2pk, or p2sh scriptPubkey.\n\n    @private_key_info is either a single private key, or a dict with 'redeem_script' and 'private_keys' defined.\n\n    @redeem_script, if given, must NOT start with the varint encoding its length.\n    However, it must otherwise be a hex string\n\n    Return the transaction with the @idx'th scriptSig filled in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclassify a scriptpubkey optionally using the private key info that will generate the corresponding scriptsig and witness tuples.", "response": "def btc_script_classify(scriptpubkey, private_key_info=None):\n    \"\"\"\n    Classify a scriptpubkey, optionally also using the private key info that will generate the corresponding scriptsig/witness\n    Return None if not known (nonstandard)\n    \"\"\"\n    if scriptpubkey.startswith(\"76a914\") and scriptpubkey.endswith(\"88ac\") and len(scriptpubkey) == 50:\n        return 'p2pkh'\n\n    elif scriptpubkey.startswith(\"a914\") and scriptpubkey.endswith(\"87\") and len(scriptpubkey) == 46:\n        # maybe p2sh-p2wpkh or p2sh-p2wsh?\n        if private_key_info:\n            if btc_is_singlesig_segwit(private_key_info):\n                return 'p2sh-p2wpkh'\n            elif btc_is_multisig_segwit(private_key_info):\n                return 'p2sh-p2wsh'\n\n        return 'p2sh'\n\n    elif scriptpubkey.startswith('0014') and len(scriptpubkey) == 44:\n        return 'p2wpkh'\n\n    elif scriptpubkey.startswith('0020') and len(scriptpubkey) == 68:\n        return 'p2wsh'\n\n    script_tokens = btc_script_deserialize(scriptpubkey)\n    if len(script_tokens) == 0:\n        return None\n\n    if script_tokens[0] == OPCODE_VALUES['OP_RETURN']:\n        return \"nulldata\"\n\n    elif script_tokens[-1] == OPCODE_VALUES['OP_CHECKMULTISIG']:\n        return \"multisig\"\n\n    elif len(script_tokens) == 2 and script_tokens[-1] == OPCODE_VALUES[\"OP_CHECKSIG\"]:\n        return \"p2pk\"\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef btc_privkey_scriptsig_classify(private_key_info):\n    if btc_is_singlesig(private_key_info):\n        return 'p2pkh'\n\n    if btc_is_multisig(private_key_info):\n        return 'p2sh'\n\n    if btc_is_singlesig_segwit(private_key_info):\n        return 'p2sh-p2wpkh'\n\n    if btc_is_multisig_segwit(private_key_info):\n        return 'p2sh-p2wsh'\n\n    return None", "response": "Return the scripting class that can be used for this private key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsigning a particular input in the given transaction.", "response": "def btc_tx_sign_input(tx, idx, prevout_script, prevout_amount, private_key_info, hashcode=SIGHASH_ALL, hashcodes=None, segwit=None, scriptsig_type=None, redeem_script=None, witness_script=None, **blockchain_opts):\n    \"\"\"\n    Sign a particular input in the given transaction.\n    @private_key_info can either be a private key, or it can be a dict with 'redeem_script' and 'private_keys' defined\n\n    Returns the tx with the signed input\n    \"\"\"\n    if segwit is None:\n        segwit = get_features('segwit')\n    \n    if scriptsig_type is None:\n        scriptsig_type = btc_privkey_scriptsig_classify(private_key_info)\n    \n    if scriptsig_type in ['p2wpkh', 'p2wsh', 'p2sh-p2wpkh', 'p2sh-p2wsh'] and not segwit:\n        raise ValueError(\"Segwit is not enabled, but {} is a segwit scriptsig type\".format(prevout_script))\n\n    return btc_tx_sign(tx, idx, prevout_script, prevout_amount, private_key_info, scriptsig_type, hashcode=hashcode, hashcodes=hashcodes, redeem_script=redeem_script, witness_script=witness_script)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef btc_tx_sign_all_unsigned_inputs(private_key_info, prev_outputs, unsigned_tx_hex, scriptsig_type=None, segwit=None, **blockchain_opts):\n    if segwit is None:\n        segwit = get_features('segwit')\n    \n    txobj = btc_tx_deserialize(unsigned_tx_hex)\n    inputs = txobj['ins']\n    \n    if scriptsig_type is None:\n        scriptsig_type = btc_privkey_scriptsig_classify(private_key_info)\n\n    tx_hex = unsigned_tx_hex\n    prevout_index = 0\n    \n    # import json\n    # print ''\n    # print 'transaction:\\n{}'.format(json.dumps(btc_tx_deserialize(unsigned_tx_hex), indent=4, sort_keys=True))\n    # print 'prevouts:\\n{}'.format(json.dumps(prev_outputs, indent=4, sort_keys=True))\n    # print ''\n\n    for i, inp in enumerate(inputs):\n        do_witness_script = segwit\n        if inp.has_key('witness_script'):\n            do_witness_script = True\n\n        elif segwit:\n            # all inputs must receive a witness script, even if it's empty \n            inp['witness_script'] = ''\n\n        if (inp['script'] and len(inp['script']) > 0) or (inp.has_key('witness_script') and len(inp['witness_script']) > 0):\n            continue\n\n        if prevout_index >= len(prev_outputs):\n            raise ValueError(\"Not enough prev_outputs ({} given, {} more prev-outputs needed)\".format(len(prev_outputs), len(inputs) - prevout_index))\n\n        # tx with index i signed with privkey\n        tx_hex = btc_tx_sign_input(str(unsigned_tx_hex), i, prev_outputs[prevout_index]['out_script'], prev_outputs[prevout_index]['value'], private_key_info, segwit=do_witness_script, scriptsig_type=scriptsig_type)\n        unsigned_tx_hex = tx_hex\n        prevout_index += 1\n\n    return tx_hex", "response": "Signs all unsigned inputs with a given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives block header information serialize it and return the hex hash.", "response": "def block_header_serialize( inp ):\n    \"\"\"\n    Given block header information, serialize it and return the hex hash.\n\n    inp has:\n    * version (int)\n    * prevhash (str)\n    * merkle_root (str)\n    * timestamp (int)\n    * bits (int)\n    * nonce (int)\n\n    Based on code from pybitcointools (https://github.com/vbuterin/pybitcointools)\n    by Vitalik Buterin\n    \"\"\"\n    \n    # concatenate to form header\n    o = encoding.encode(inp['version'], 256, 4)[::-1] + \\\n        inp['prevhash'].decode('hex')[::-1] + \\\n        inp['merkle_root'].decode('hex')[::-1] + \\\n        encoding.encode(inp['timestamp'], 256, 4)[::-1] + \\\n        encoding.encode(inp['bits'], 256, 4)[::-1] + \\\n        encoding.encode(inp['nonce'], 256, 4)[::-1]\n\n    # get (reversed) hash\n    h = hashing.bin_sha256(hashing.bin_sha256(o))[::-1].encode('hex')\n    assert h == inp['hash'], (hashing.bin_sha256(o).encode('hex'), inp['hash'])\n\n    return o.encode('hex')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the hex form of a block s header given its getblock information.", "response": "def block_header_to_hex( block_data, prev_hash ):\n    \"\"\"\n    Calculate the hex form of a block's header, given its getblock information from bitcoind.\n    \"\"\"\n    header_info = {\n       \"version\": block_data['version'],\n       \"prevhash\": prev_hash,\n       \"merkle_root\": block_data['merkleroot'],\n       \"timestamp\": block_data['time'],\n       \"bits\": int(block_data['bits'], 16),\n       \"nonce\": block_data['nonce'],\n       \"hash\": block_data['hash']\n    }\n\n    return block_header_serialize(header_info)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that or not bitcoind s block header matches the hash we expect.", "response": "def block_header_verify( block_data, prev_hash, block_hash ):\n    \"\"\"\n    Verify whether or not bitcoind's block header matches the hash we expect.\n    \"\"\"\n    serialized_header = block_header_to_hex( block_data, prev_hash )\n    candidate_hash_bin_reversed = hashing.bin_double_sha256(binascii.unhexlify(serialized_header))\n    candidate_hash = binascii.hexlify( candidate_hash_bin_reversed[::-1] )\n\n    return block_hash == candidate_hash"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify that the block data is consistent with bitcoind s getblock JSON RPC method.", "response": "def block_verify( block_data ):\n    \"\"\"\n    Given block data (a dict with 'merkleroot' hex string and 'tx' list of hex strings--i.e.\n    a block compatible with bitcoind's getblock JSON RPC method), verify that the\n    transactions are consistent.\n\n    Return True on success\n    Return False if not.\n    \"\"\"\n    \n    # verify block data txs \n    m = merkle.MerkleTree( block_data['tx'] )\n    root_hash = str(m.root())\n\n    return root_hash == str(block_data['merkleroot'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a hex representation of a script return a nice - to - read dict.", "response": "def btc_tx_output_parse_script( scriptpubkey ):\n    \"\"\"\n    Given the hex representation of a script,\n    turn it into a nice, easy-to-read dict.\n    The dict will have:\n    * asm: the disassembled script as a string\n    * hex: the raw hex (given as an argument)\n    * type: the type of script\n\n    Optionally, it will have:\n    * addresses: a list of addresses the script represents (if applicable)\n    * reqSigs: the number of required signatures (if applicable)\n    \"\"\"\n    \n    script_type = None\n    reqSigs = None\n    addresses = []\n   \n    script_type = btc_script_classify(scriptpubkey)\n    script_tokens = btc_script_deserialize(scriptpubkey)\n\n    if script_type in ['p2pkh']:\n        script_type = \"pubkeyhash\"\n        reqSigs = 1\n        addr = btc_script_hex_to_address(scriptpubkey)\n        if not addr:\n            raise ValueError(\"Failed to parse scriptpubkey address\")\n\n        addresses = [addr]\n\n    elif script_type in ['p2sh', 'p2sh-p2wpkh', 'p2sh-p2wsh']:\n        script_type = \"scripthash\"\n        reqSigs = 1\n        addr = btc_script_hex_to_address(scriptpubkey)\n        if not addr:\n            raise ValueError(\"Failed to parse scriptpubkey address\")\n\n        addresses = [addr]\n\n    elif script_type == 'p2pk':\n        script_type = \"pubkey\"\n        reqSigs = 1\n\n    elif script_type is None:\n        script_type = \"nonstandard\"\n\n    ret = {\n        \"asm\": btc_tx_script_to_asm(scriptpubkey),\n        \"hex\": scriptpubkey,\n        \"type\": script_type\n    }\n\n    if addresses is not None:\n        ret['addresses'] = addresses\n\n    if reqSigs is not None:\n        ret['reqSigs'] = reqSigs\n\n    # print 'parse script {}: {}'.format(scriptpubkey, ret)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_code_to_value(M_c, cidx, code):\n    if M_c['column_metadata'][cidx]['modeltype'] == 'normal_inverse_gamma':\n        return float(code)\n    else:\n        try:\n            return M_c['column_metadata'][cidx]['value_to_code'][int(code)]\n        except KeyError:\n            return M_c['column_metadata'][cidx]['value_to_code'][str(int(code))]", "response": "This function converts the code to a value in the column metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_value_to_code(M_c, cidx, value):\n    if M_c['column_metadata'][cidx]['modeltype'] == 'normal_inverse_gamma':\n        return float(value)\n    else:\n        return M_c['column_metadata'][cidx]['code_to_value'][str(value)]", "response": "Converts a value to the underlying representation of the column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, force=False):\n        if (not self._success) and (not force):\n            raise ConfigError((\n                'The config file appears to be corrupted:\\n\\n'\n                '    {fname}\\n\\n'\n                'Before attempting to save the configuration, please either '\n                'fix the config file manually, or overwrite it with a blank '\n                'configuration as follows:\\n\\n'\n                '    from dustmaps.config import config\\n'\n                '    config.reset()\\n\\n'\n                ).format(fname=self.fname))\n\n        with open(self.fname, 'w') as f:\n            json.dump(self._options, f, indent=2)", "response": "Saves the current configuration to a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the configuration and overwrites the existing configuration.", "response": "def reset(self):\n        \"\"\"\n        Resets the configuration, and overwrites the existing configuration\n        file.\n        \"\"\"\n        self._options = {}\n        self.save(force=True)\n        self._success = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sys_info():\n    \"Returns system information as a dict\"\n\n    blob = []\n\n    # commit = cc._git_hash\n    # blob.append(('commit', commit))\n\n    try:\n        (sysname, nodename, release, version,\n         machine, processor) = platform.uname()\n        blob.extend([\n            (\"python\", \"%d.%d.%d.%s.%s\" % sys.version_info[:]),\n            (\"python-bits\", struct.calcsize(\"P\") * 8),\n            (\"OS\", \"%s\" % (sysname)),\n            (\"OS-release\", \"%s\" % (release)),\n            # (\"Version\", \"%s\" % (version)),\n            (\"machine\", \"%s\" % (machine)),\n            (\"processor\", \"%s\" % (processor)),\n            # (\"byteorder\", \"%s\" % sys.byteorder),\n            (\"LC_ALL\", \"%s\" % os.environ.get('LC_ALL', \"None\")),\n            (\"LANG\", \"%s\" % os.environ.get('LANG', \"None\")),\n            (\"LOCALE\", \"%s.%s\" % locale.getlocale()),\n        ])\n    except Exception:\n        pass\n\n    return blob", "response": "Returns system information as a dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bitcoin_virtual_transactions(blockchain_opts, first_block_height, last_block_height, tx_filter=None, spv_last_block=None, first_block_hash=None, **hints):\n\n    headers_path = blockchain_opts['bitcoind_spv_path']\n    bitcoind_server = \"%s:%s\" % (blockchain_opts['bitcoind_server'], blockchain_opts['bitcoind_p2p_port'])\n    spv_last_block = spv_last_block if spv_last_block is not None else last_block_height - 1\n\n    if headers_path is None:\n        log.error(\"FATAL: bitcoind_spv_path not defined in blockchain options\")\n        os.abort()\n\n    if not os.path.exists(headers_path):\n        log.debug(\"Will download SPV headers to %s\" % headers_path)\n\n    # synchronize SPV headers\n    SPVClient.init( headers_path )\n\n    rc = None\n    for i in xrange(0, 65536, 1):\n        # basically try forever\n        try:\n            rc = SPVClient.sync_header_chain( headers_path, bitcoind_server, spv_last_block )\n            if not rc:\n                delay = min( 600, 2**i + ((2**i) * random.random()) )\n                log.error(\"Failed to synchronize SPV headers (%s) up to %s.  Try again in %s seconds\" % (headers_path, last_block_height, delay))\n                time.sleep( delay )\n                continue\n\n            else:\n                break\n\n        except SystemExit, s:\n            log.error(\"Aborting on SPV header sync\")\n            os.abort()\n\n        except Exception, e:\n            log.exception(e)\n            delay = min( 600, 2**i + ((2**i) * random.random()) )\n            log.debug(\"Try again in %s seconds\" % delay)\n            time.sleep( delay )\n            continue\n\n    downloader = None\n    for i in xrange(0, 65536, 1):\n        # basically try forever\n        try:\n            \n            # fetch all blocks\n            downloader = BlockchainDownloader( blockchain_opts, blockchain_opts['bitcoind_spv_path'], first_block_height, last_block_height - 1, \\\n                                       p2p_port=blockchain_opts['bitcoind_p2p_port'], tx_filter=tx_filter )\n\n            if first_block_height > last_block_height - 1:\n                downloader.loop_exit()\n                break\n\n            rc = downloader.run()\n            if not rc:\n                delay = min( 600, 2**i + ((2**i) * random.random()) )\n                log.error(\"Failed to fetch %s-%s; trying again in %s seconds\" % (first_block_height, last_block_height, delay))\n                time.sleep( delay )\n                continue\n            else:\n                break\n\n        except SystemExit, s:\n            log.error(\"Aborting on blockchain sync\")\n            os.abort()\n\n        except Exception, e:\n            log.exception(e)\n            delay = min( 600, 2**i + ((2**i) * random.random()) )\n            log.debug(\"Try again in %s seconds\" % delay)\n            time.sleep( delay )\n            continue            \n\n    if not rc or downloader is None:\n        log.error(\"Failed to fetch blocks %s-%s\" % (first_block_height, last_block_height))\n        return None\n\n    # extract\n    block_info = downloader.get_block_info()\n    return block_info", "response": "Get the virtualchain transactions from the blockchain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_block_info(self):\n        if not self.finished:\n            raise Exception(\"Not finished downloading\")\n\n        ret = []\n        for (block_hash, block_data) in self.block_info.items():\n            ret.append( (block_data['height'], block_data['txns']) )\n\n        return ret", "response": "Get the retrieved block information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninteracting with the blockchain peer and return the result of the loop.", "response": "def run( self ):\n        \"\"\"\n        Interact with the blockchain peer,\n        until we get a socket error or we\n        exit the loop explicitly.\n\n        The order of operations is:\n        * send version\n        * receive version\n        * send verack\n        * send getdata\n        * receive blocks\n        * for each block:\n          * for each transaction with nulldata:\n             * for each input:\n                * get the transaction that produced the consumed input\n\n        Return True on success\n        Return False on error\n        \"\"\"\n        \n        log.debug(\"Segwit support: {}\".format(get_features('segwit')))\n\n        self.begin()\n\n        try:\n            self.loop()\n        except socket.error, se:\n            if not self.finished:\n                # unexpected\n                log.exception(se)\n                return False\n\n        # fetch remaining sender transactions\n        try:\n            self.fetch_sender_txs()\n        except Exception, e:\n            log.exception(e)\n            return False\n\n        # should be done now\n        try:\n            self.block_data_sanity_checks()\n        except AssertionError, ae:\n            log.exception(ae)\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef have_all_block_data(self):\n        if not (self.num_blocks_received == self.num_blocks_requested):\n            log.debug(\"num blocks received = %s, num requested = %s\" % (self.num_blocks_received, self.num_blocks_requested))\n            return False\n\n        return True", "response": "Return True if we have all block data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch all sender txs via JSON - RPC and merge them into our block data.", "response": "def fetch_sender_txs(self):\n        \"\"\"\n        Fetch all sender txs via JSON-RPC,\n        and merge them into our block data.\n\n        Try backing off (up to 5 times) if we fail\n        to fetch transactions via JSONRPC\n\n        Return True on success\n        Raise on error\n        \"\"\"\n        \n        # fetch remaining sender transactions\n        if len(self.sender_info.keys()) > 0:\n\n            sender_txids = self.sender_info.keys()[:]\n            sender_txid_batches = []\n            batch_size = 20\n\n            for i in xrange(0, len(sender_txids), batch_size ):\n                sender_txid_batches.append( sender_txids[i:i+batch_size] )\n\n            for i in xrange(0, len(sender_txid_batches)):\n\n                sender_txid_batch = sender_txid_batches[i]\n                log.debug(\"Fetch %s TXs via JSON-RPC (%s-%s of %s)\" % (len(sender_txid_batch), i * batch_size, i * batch_size + len(sender_txid_batch), len(sender_txids)))\n\n                sender_txs = None\n\n                for j in xrange(0, 5):\n                    sender_txs = self.fetch_txs_rpc( self.bitcoind_opts, sender_txid_batch )\n                    if sender_txs is None:\n                        log.error(\"Failed to fetch transactions; trying again (%s of %s)\" % (j+1, 5))\n                        time.sleep(j+1)\n                        continue\n\n                    break\n\n                if sender_txs is None:\n                    raise Exception(\"Failed to fetch transactions\")\n                \n                # pair back up with nulldata transactions\n                for sender_txid, sender_tx in sender_txs.items():\n\n                    assert sender_txid in self.sender_info.keys(), \"Unsolicited sender tx %s\" % sender_txid\n\n                    # match sender outputs to the nulldata tx's inputs\n                    for nulldata_input_vout_index in self.sender_info[sender_txid].keys():\n                        if sender_txid != \"0000000000000000000000000000000000000000000000000000000000000000\":\n                            \n                            # regular tx, not coinbase \n                            assert nulldata_input_vout_index < len(sender_tx['outs']), 'Output index {} is out of bounds for {}'.format(nulldata_input_vout_index, sender_txid)\n\n                            # save sender info \n                            self.add_sender_info(sender_txid, nulldata_input_vout_index, sender_tx['outs'][nulldata_input_vout_index])\n                        \n                        else:\n\n                            # coinbase\n                            self.add_sender_info(sender_txid, nulldata_input_vout_index, sender_tx['outs'][0])\n\n                    # update accounting\n                    self.num_txs_received += 1\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef block_data_sanity_checks(self):\n        assert self.have_all_block_data(), \"Still missing block data\"\n        assert self.num_txs_received == len(self.sender_info.keys()), \"Num TXs received: %s; num TXs requested: %s\" % (self.num_txs_received, len(self.sender_info.keys()))\n\n        for (block_hash, block_info) in self.block_info.items():\n            for tx in block_info['txns']:\n                assert None not in tx['senders'], \"Missing one or more senders in %s; dump follows\\n%s\" % (tx['txid'], simplejson.dumps(tx, indent=4, sort_keys=True)) \n                for i in range(0, len(tx['ins'])):\n                    inp = tx['ins'][i]\n                    sinfo = tx['senders'][i]\n\n                    assert sinfo['txid'] in self.sender_info, 'Surreptitious sender tx {}'.format(sinfo['txid'])\n                    assert inp['outpoint']['index'] == sinfo['nulldata_vin_outpoint'], 'Mismatched sender/input index ({}: {} != {}); dump follows\\n{}'.format(\n                            sinfo['txid'], inp['outpoint']['index'], sinfo['nulldata_vin_outpoint'], simplejson.dumps(tx, indent=4, sort_keys=True))\n\n                    assert inp['outpoint']['hash'] == sinfo['txid'], 'Mismatched sender/input txid ({} != {}); dump follows\\n{}'.format(inp['txid'], sinfo['txid'], simplejson.dumps(tx, indent=4, sort_keys=True))\n\n        return True", "response": "Verify that the data we received makes sense."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef begin(self):\n        log.debug(\"handshake (version %s)\" % PROTOCOL_VERSION)\n        version = Version()\n        version.services = 0    # can't send blocks\n        log.debug(\"send Version\")\n        self.send_message(version)", "response": "This method is called by the server to send the version message and block until the verack is received."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_version(self, message_header, message):\n        log.debug(\"handle version\")\n        verack = VerAck()\n        log.debug(\"send VerAck\")\n        self.send_message(verack)\n        self.verack = True\n\n        start_block_height = sorted(self.blocks.keys())[0]\n        if start_block_height < 1:\n            start_block_height = 1\n\n        # ask for all blocks\n        block_hashes = []\n        for height in sorted(self.blocks.keys()):\n            block_hashes.append( int(self.blocks[height], 16) )\n\n        start_block_height = sorted(self.blocks.keys())[0]\n        end_block_height = sorted(self.blocks.keys())[-1]\n    \n        log.debug(\"send getdata for %s-%s (%064x-%064x)\" % (start_block_height, end_block_height, block_hashes[0], block_hashes[-1]))\n\n        # send off the getdata\n        getdata = GetData()\n        block_inv_vec = []\n        for block_hash in block_hashes:\n            block_inv = Inventory()\n            block_inv.inv_type = INVENTORY_TYPE[\"MSG_BLOCK\"]\n            block_inv.inv_hash = block_hash\n\n            block_inv_vec.append(block_inv)\n\n        getdata.inventory = block_inv_vec\n        self.send_message(getdata)", "response": "This method is called when the protocol is sending a VerAck message and then sends a GetData message to the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle an incoming INV packet.", "response": "def handle_inv(self, message_header, inv_packet ):\n        \"\"\"\n        Get the data we just requested.\n        Shouldn't happen with newer servers, since they use\n        getheaders/headers followed by getdata/blocks\n        (older peers use getblocks/inv/getdata/inv exchanges)\n        \"\"\"\n        log.debug(\"handle inv of %s item(s)\" % len(inv_packet.inventory))\n\n        reply_inv = []\n\n        for inv_info in inv_packet.inventory:\n            inv_hash = \"%064x\" % inv_info.inv_hash\n            if inv_info.inv_type == INVENTORY_TYPE[\"MSG_BLOCK\"]:\n                # only ask for the block if we need it\n                if inv_hash in self.block_info.keys() and not self.block_info[inv_hash]['handled']:\n                    log.debug(\"Will request block %s\" % inv_hash)\n                    reply_inv.append( inv_info )\n                    inv_hash = None\n\n\n        if len(reply_inv) > 0:\n            getdata = GetData()\n            getdata.inventory = reply_inv\n            log.debug(\"send GetData in reply to Inv for %s item(s)\" % len(reply_inv))\n            self.send_message(getdata)\n\n        else:\n            if self.have_all_block_data():\n                self.loop_exit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds sender info to our block info.", "response": "def add_sender_info( self, sender_txhash, nulldata_vin_outpoint, sender_out_data ):\n        \"\"\"\n        Record sender information in our block info.\n        @sender_txhash: txid of the sender\n        @nulldata_vin_outpoint: the 'vout' index from the nulldata tx input that this transaction funded\n        \"\"\"\n        assert sender_txhash in self.sender_info.keys(), \"Missing sender info for %s\" % sender_txhash\n        assert nulldata_vin_outpoint in self.sender_info[sender_txhash], \"Missing outpoint %s for sender %s\" % (nulldata_vin_outpoint, sender_txhash)\n\n        block_hash = self.sender_info[sender_txhash][nulldata_vin_outpoint]['block_hash']\n        relindex = self.sender_info[sender_txhash][nulldata_vin_outpoint]['relindex']\n        relinput_index = self.sender_info[sender_txhash][nulldata_vin_outpoint]['relinput']\n\n        value_in_satoshis = sender_out_data['value']\n        script_pubkey = sender_out_data['script']\n        script_info = bits.btc_tx_output_parse_script(script_pubkey)\n        script_type = script_info['type']\n        addresses = script_info.get('addresses', [])\n        \n        sender_info = {\n            \"value\": value_in_satoshis,\n            \"script_pubkey\": script_pubkey,\n            \"script_type\": script_type,\n            \"addresses\": addresses,\n            \"nulldata_vin_outpoint\": nulldata_vin_outpoint,\n            \"txid\": sender_txhash,\n        }\n        \n        # debit this tx's total value\n        self.block_info[block_hash]['txns'][relindex]['fee'] += value_in_satoshis\n\n        # remember this sender, but put it in the right place.\n        # senders[i] must correspond to tx['vin'][i]\n        self.block_info[block_hash]['txns'][relindex]['senders'][relinput_index] = sender_info\n        self.block_info[block_hash]['num_senders'] += 1\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a transaction message and its index in the block return a dict containing all the information in a nice - to - read transaction structure.", "response": "def parse_tx( self, txn, block_header, block_hash, txindex ):\n        \"\"\"\n        Given a transaction message and its index in the block,\n        go and create a \"verbose\" transaction structure\n        containing all the information in a nice, easy-to-read\n        dict (i.e. like what bitcoind would give us).\n\n        Does not work on coinbase transactions.\n        Does not include segwit witnesses\n        \"\"\"\n\n        txn_serializer = TxSerializer()\n        tx_bin = txn_serializer.serialize(txn)\n\n        txdata = {\n            \"version\": txn.version,\n            \"locktime\": txn.lock_time,\n            \"hex\": binascii.hexlify( tx_bin ),\n            \"txid\": txn.calculate_hash(),\n            \"size\": len( tx_bin ),\n            \"blockhash\": block_hash,\n            \"blocktime\": block_header.get('timestamp', 0),\n\n            # non-standard; added by us for virtualchain\n            \"txindex\": txindex,\n            \"relindex\": None,\n            \"senders\": None,\n            \"fee\": 0,\n            \"nulldata\": None,\n            \"ins\": None,     # library-specific field, to be passed to the state engine\n            \"outs\": None,    # library-specific field, to be passed to the state engine\n            \"tx_merkle_path\": None\n        }\n        \n        # keep these around too, since this is what gets fed into the virtualchain state engine implementation \n        virtualchain_btc_tx_data = bits.btc_tx_deserialize(txdata['hex'])\n        txdata['ins'] = virtualchain_btc_tx_data['ins']\n        txdata['outs'] = virtualchain_btc_tx_data['outs']\n\n        # we know how many senders there have to be \n        txdata['senders'] = [None] * len(txdata['ins'])\n\n        return txdata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking sender information bundle for a particular input of a nulldata transaction.", "response": "def make_sender_info( self, block_hash, txn, i, block_height ):\n        \"\"\"\n        Make sender information bundle for a particular input of\n        a nulldata transaction.\n\n        We'll use it to go find the transaction output that\n        funded the ith input of the given tx.\n        \"\"\"\n\n        inp = txn['ins'][i]\n        ret = {\n            # to be filled in...\n            'scriptPubKey': None,\n            'addresses': None,\n\n            # for matching the input and sender funded\n            \"txindex\": txn['txindex'],\n            \"relindex\": txn['relindex'],\n            \"output_index\": inp['outpoint']['index'],\n            \"block_hash\": block_hash,\n            \"relinput\": i,\n            \"block_height\": block_height,\n        }\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles a single block.", "response": "def handle_block( self, message_header, block ):\n        \"\"\"\n        Got a block.\n        * validate it\n        * load its transactions\n        * ask for each transaction's sender transaction\n        \"\"\"\n\n        if self.have_all_block_data():\n            self.loop_exit()\n            return\n\n        block_hash = block.calculate_hash()\n        \n        # is this a solicited block?\n        if block_hash not in self.block_info.keys():\n            log.error(\"Ignoring unsolicited block %s\" % block_hash)\n            return\n        \n        header = self.block_info[block_hash]['header']\n        height = self.block_info[block_hash]['height']\n        \n        log.debug(\"handle block %s (%s)\" % (height, block_hash))\n\n        # does this block's transaction hashes match the merkle root?\n        tx_hashes = [block.txns[i].calculate_hash() for i in range(0, len(block.txns))]\n        block_merkle_tree = merkle.MerkleTree(tx_hashes)\n        mr = block_merkle_tree.root()\n\n        if mr != header['merkle_root']:\n            log.error(\"Merkle root of %s (%s) mismatch: expected %s, got %s\" % (block_hash, height, header['merkle_root'], mr))\n            return\n\n        merkle_paths = {}       # map txid to merkle path\n\n        # make sure we have merkle paths for each tx\n        for i in range(0, len(block.txns)):\n            txid = block.txns[i].calculate_hash()\n            merkle_path = block_merkle_tree.path(txid)\n            if merkle_path is None:\n                log.error(\"No merkle path for {}\".format(txid))\n                return \n\n            merkle_paths[txid] = merkle_path\n \n        nulldata_txs = []\n        relindex = 0\n        for txindex in range(0, len(block.txns)):\n\n            txdata = self.parse_tx( block.txns[txindex], header, block_hash, txindex )\n\n            # if there is no nulldata output, then we don't care about this one.\n            has_nulldata = False\n            nulldata_payload = None\n\n            for outp in txdata['outs']:\n                script_type = bits.btc_script_classify(outp['script'])\n                if script_type == 'nulldata':\n                    # to be clear, we only care about outputs that take the form\n                    # OP_RETURN <string length> <string data>\n                    nulldata_script = bits.btc_script_deserialize(outp['script'])\n                    if len(nulldata_script) < 2:\n                        # malformed OP_RETURN; no data after '6a'\n                        nulldata_payload = None\n                        has_nulldata = False\n\n                    elif len(nulldata_script) == 2 and isinstance(nulldata_script[1], (str,unicode)):\n                        # well-formed OP_RETURN output\n                        nulldata_payload = nulldata_script[1]\n                        has_nulldata = True\n                   \n                    else:\n                        # this is something like OP_RETURN OP_2 (e.g. \"6a52\")\n                        # there's nothing for us here.\n                        nulldata_payload = None\n                        has_nulldata = False\n\n            # count all txs processed\n            self.num_txs_processed += 1\n\n            if not has_nulldata:\n                continue\n\n            # remember nulldata, even if it's empty\n            txdata['nulldata'] = nulldata_payload \n\n            # remember merkle path\n            txdata['tx_merkle_path'] = merkle_paths[txdata['txid']]\n            \n            # calculate total output (part of fee; will be debited when we discover the senders)\n            # NOTE: this works because we have out['value'] as type Decimal\n            # txdata['fee'] -= sum( int(out['value'] * 10**8) for out in txdata['vout'] )\n            txdata['fee'] -= sum(out['value'] for out in txdata['outs'])\n\n            # remember the relative tx index (i.e. the ith nulldata tx)\n            txdata['relindex'] = relindex\n            \n\t    # do we actually want this?\n            if self.tx_filter is not None:\n                if not self.tx_filter( txdata ):\n                    continue\n\n            # yup, we want it!\n            relindex += 1\n            nulldata_txs.append( txdata )\n\n\n        self.block_info[block_hash]['txns'] = nulldata_txs\n        self.block_info[block_hash]['num_txns'] = len(block.txns)\n        self.block_info[block_hash]['num_senders'] = 0\n\n        # get each input's transaction\n        sender_txhashes = []\n\n        for txn in self.block_info[block_hash]['txns']:\n            for i in range(0, len(txn['ins'])):\n\n                # record information about the transaction\n                # that created this input (so we can go find\n                # it later).\n                inp = txn['ins'][i]\n                sender_txid = inp['outpoint']['hash']\n                inp_sender_outp = inp['outpoint']['index']\n\n                if str(sender_txid) not in sender_txhashes:\n                    sender_txhashes.append( str(sender_txid) )\n\n                sinfo = self.make_sender_info( block_hash, txn, i, height )\n\n                if not self.sender_info.has_key(sender_txid):\n                    # map outpoint for this input to the tx info\n                    self.sender_info[sender_txid] = {}\n\n                # sinfo is the information from the output in \n                # the sender-tx that funded inp\n                self.sender_info[sender_txid][inp_sender_outp] = sinfo\n\n        # update accounting...\n        self.num_blocks_received += 1\n        self.block_info[block_hash]['handled'] = True\n\n        log.debug(\"Request %s nulldata sender TXs\" % len(sender_txhashes))\n\n        if self.have_all_block_data():\n            self.loop_exit()\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the given list of transactions via the JSON - RPC interface.", "response": "def fetch_txs_rpc( self, bitcoind_opts, txids ):\n        \"\"\"\n        Fetch the given list of transactions\n        via the JSON-RPC interface.\n\n        Return a dict of parsed transactions on success,\n        keyed by txid.\n\n        Return None on error\n        \"\"\"\n\n        headers = {'content-type': 'application/json'}\n        reqs = []\n        ret = {}\n        for i in xrange(0, len(txids)):\n            txid = txids[i]\n            if txid == \"0000000000000000000000000000000000000000000000000000000000000000\":\n                # coinbase; we never send these\n                ret[txid] = {\n                    'version': 1,\n                    'locktime': 0,\n                    'ins': [],\n                    'outs': [\n                        {\n                            'script': '',\n                            'value': 0      # not really 0, but we don't care about coinbases anyway\n                        }\n                    ],\n                }\n                continue\n\n            req = {'method': 'getrawtransaction', 'params': [txid, 0], 'jsonrpc': '2.0', 'id': i}\n            reqs.append( req )\n\n        proto = \"http\"\n        if bitcoind_opts.has_key('bitcoind_use_https') and bitcoind_opts['bitcoind_use_https']:\n            proto = \"https\"\n            \n        server_url = \"%s://%s:%s@%s:%s\" % (proto, bitcoind_opts['bitcoind_user'], bitcoind_opts['bitcoind_passwd'], bitcoind_opts['bitcoind_server'], bitcoind_opts['bitcoind_port'])\n        try:\n            resp = requests.post( server_url, headers=headers, data=simplejson.dumps(reqs), verify=False )\n        except Exception, e:\n            log.exception(e)\n            log.error(\"Failed to fetch %s transactions\" % len(txids))\n            return None\n\n        # get responses\n        try:\n            resp_json = resp.json()\n            assert type(resp_json) in [list]\n        except Exception, e:\n            log.exception(e)\n            log.error(\"Failed to parse transactions\")\n            return None\n\n        try:\n            for resp in resp_json:\n                assert 'result' in resp, \"Missing result\"\n\n                txhex = resp['result']\n                assert txhex is not None, \"Invalid RPC response '%s' (for %s)\" % (simplejson.dumps(resp), txids[resp['id']])\n               \n                if bits.btc_tx_is_segwit(txhex) and not get_features('segwit'):\n                    # no segwit support yet\n                    log.error(\"FATAL: SegWit transaction detected!  Support for SegWit-formatted transactions is not yet activated\")\n                    log.error(\"Please ensure your bitcoind node has `rpcserialversion=0` set.\")\n                    log.error(\"Aborting...\")\n                    os.abort()\n\n                try:\n\n                    tx_bin = txhex.decode('hex')\n                    assert tx_bin is not None\n\n                    tx_hash_bin = hashing.bin_double_sha256(tx_bin)[::-1]\n                    assert tx_hash_bin is not None\n\n                    tx_hash = tx_hash_bin.encode('hex')\n                    assert tx_hash is not None\n\n                except Exception, e:\n                    log.error(\"Failed to calculate txid of %s\" % txhex)\n                    raise\n\n                # solicited transaction?\n                assert tx_hash in txids, \"Unsolicited transaction %s\" % tx_hash\n                \n                # unique?\n                if tx_hash in ret.keys():\n                    continue\n\n                # parse from hex string\n                txn_serializer = TxSerializer()\n                txn = txn_serializer.deserialize( StringIO( binascii.unhexlify(txhex) ) )\n\n                ret[tx_hash] = self.parse_tx( txn, {}, \"\", -1 )\n\n\n        except Exception, e:\n            log.exception(e)\n            log.error(\"Failed to receive transactions\")\n            return None\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread configuration file perform sanity check and return configuration dictionary used by other functions.", "response": "def read_config(config_file=CONFIG_FILE_DEFAULT, override_url=None):\n    ''' Read configuration file, perform sanity check and return configuration\n        dictionary used by other functions.'''\n    config = ConfigParser()\n    config.read_dict(DEFAULT_SETTINGS)\n\n    try:\n        config.readfp(open(config_file))\n        logger.debug(\"Using config file at \" + config_file)\n    except:\n        logger.error(\n            \"Could not find {0}, running with defaults.\".format(config_file))\n\n    if not logger.handlers:\n        # Before doing anything else, configure logging\n        # Handlers might be already registered in repeated test suite runs\n        # In production, this should never happen\n        if config.getboolean(\"Logging\", \"to_file\"):\n            handler = logging.FileHandler(config.get(\"Logging\", \"file\"))\n        else:\n            handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter(\n            config.get(\"Logging\", \"format\")))\n        logger.addHandler(handler)\n    logger.setLevel(config.get(\"Logging\", \"level\"))\n\n    if override_url:\n        config['Server']['url'] = override_url\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the executor config file for consistency.", "response": "def check_config(config):\n    '''\n        Check the executor config file for consistency.\n    '''\n    # Check server URL\n    url = config.get(\"Server\", \"url\")\n    try:\n        urlopen(url)\n    except Exception as e:\n        logger.error(\n            \"The configured OpenSubmit server URL ({0}) seems to be invalid: {1}\".format(url, e))\n        return False\n    # Check directory specification\n    targetdir = config.get(\"Execution\", \"directory\")\n    if platform.system() is not \"Windows\" and not targetdir.startswith(\"/\"):\n        logger.error(\n            \"Please use absolute paths, starting with a /, in your Execution-directory setting.\")\n        return False\n    if not targetdir.endswith(os.sep):\n        logger.error(\n            \"Your Execution-directory setting must end with a \" + os.sep)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if the given config file exists.", "response": "def has_config(config_fname):\n    '''\n    Determine if the given config file exists.\n    '''\n    config = RawConfigParser()\n    try:\n        config.readfp(open(config_fname))\n        return True\n    except IOError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_config(config_fname, override_url=None):\n    '''\n    Create the config file from the defaults under the given name.\n    '''\n    config_path = os.path.dirname(config_fname)\n    os.makedirs(config_path, exist_ok=True)\n\n    # Consider override URL. Only used by test suite runs\n    settings = DEFAULT_SETTINGS_FLAT\n    if override_url:\n        settings['url'] = override_url\n\n    # Create fresh config file, including new UUID\n    with open(config_fname, 'wt') as config:\n        config.write(DEFAULT_FILE_CONTENT.format(**settings))\n    return True", "response": "Create the config file from the defaults under the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake the given user a student.", "response": "def make_student(user):\n    '''\n    Makes the given user a student.\n    '''\n    tutor_group, owner_group = _get_user_groups()\n    user.is_staff = False\n    user.is_superuser = False\n    user.save()\n    owner_group.user_set.remove(user)\n    owner_group.save()\n    tutor_group.user_set.remove(user)\n    tutor_group.save()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake the given user a tutor.", "response": "def make_tutor(user):\n    '''\n    Makes the given user a tutor.\n    '''\n    tutor_group, owner_group = _get_user_groups()\n    user.is_staff = True\n    user.is_superuser = False\n    user.save()\n    owner_group.user_set.remove(user)\n    owner_group.save()\n    tutor_group.user_set.add(user)\n    tutor_group.save()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the given user a owner and tutor.", "response": "def make_owner(user):\n    '''\n    Makes the given user a owner and tutor.\n    '''\n    tutor_group, owner_group = _get_user_groups()\n    user.is_staff = True\n    user.is_superuser = False\n    user.save()\n    owner_group.user_set.add(user)\n    owner_group.save()\n    tutor_group.user_set.add(user)\n    tutor_group.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the given user an admin.", "response": "def make_admin(user):\n    '''\n    Makes the given user an admin.\n    '''\n    tutor_group, owner_group = _get_user_groups()\n    user.is_staff = True\n    user.is_superuser = True\n    user.save()\n    owner_group.user_set.add(user)\n    owner_group.save()\n    tutor_group.user_set.add(user)\n    tutor_group.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbesting attempt to get username and hostname returns na if problem.", "response": "def _get_username_hostname():\n    '''Best attempt to get username and hostname, returns \"na\" if problem.'''\n    user = 'na'\n    host = 'na'\n    try:\n        user = getpass.getuser()\n    except Exception:\n        pass\n    try:\n        host = socket.gethostname()\n    except Exception:\n        pass\n    return user, host"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naccept a dict of key - value pairs returning list of key - value tuples ordered by desc values.", "response": "def sort_dict(key_counts, by_key=False):\n    '''Accept a dict of key:values (numerics) returning list of key-value tuples ordered by desc values.\n\n    If by_key=True, sorts by dict key.'''\n    sort_key = lambda x: (-1 * x[1], x[0])\n    if by_key:\n        sort_key = lambda x: x[0]\n    return sorted(key_counts.items(), key=sort_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield subset of base_collection / generator based on filters.", "response": "def filter(self, base_collection):\n        '''Yields subset of base_collection/generator based on filters.'''\n        for item in base_collection:\n            excluded = []\n            for (name, exclude) in self._filters:\n                if exclude(item):\n                    excluded.append(name)\n            if excluded:\n                filter_value = \"; \".join(excluded)\n            else:\n                filter_value = None\n            yield item, filter_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncompleting with additional information from original LTI POST data as available.", "response": "def get_user_details(self, response):\n        \"\"\" Complete with additional information from original LTI POST data, as available. \"\"\"\n        data = {}\n        # None of them is mandatory\n        data['id'] = response.get('user_id', None)\n        data['username'] = response.get('custom_username', None)\n        if not data['username']:\n            data['username'] = response.get('ext_user_username', None)\n        data['last_name'] = response.get('lis_person_name_family', None)\n        data['email'] = response.get(\n            'lis_person_contact_email_primary', None)\n        data['first_name'] = response.get('lis_person_name_given', None)\n        data['fullname'] = response.get('lis_person_name_full', None)\n        logger.debug(\"User details being used: \" + str(data))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_unicode(self):\n    '''\n        Monkey patch for getting better user name stringification,\n        user proxies did not make the job.\n        Django's custom user model feature would have needed to be introduced\n        before the first syncdb, which does not work for existing installations.\n'''\n    if self.email:\n        shortened = self.email.split('@')[0]\n        return '%s %s (%s@...)' % (self.first_name, self.last_name, shortened)\n    elif self.first_name or self.last_name:\n        return '%s %s' % (self.first_name, self.last_name)\n    elif self.username:\n        return '%s' % (self.username)\n    else:\n        return 'User %u' % (self.pk)", "response": "Return a unicode string of the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef move_user_data(primary, secondary):\n    '''\n        Moves all submissions and other data linked to the secondary user into the primary user.\n        Nothing is deleted here, we just modify foreign user keys.\n    '''\n    # Update all submission authorships of the secondary to the primary\n    submissions = Submission.objects.filter(authors__id=secondary.pk)\n    for subm in submissions:\n        if subm.submitter == secondary:\n            subm.submitter = primary\n        subm.authors.remove(secondary)\n        subm.authors.add(primary)\n        subm.save()\n    # Transfer course registrations\n    try:\n        for course in secondary.profile.courses.all():\n            primary.profile.courses.add(course)\n            primary.profile.save()\n    except UserProfile.DoesNotExist:\n        # That's a database consistency problem, but he will go away anyway\n        pass", "response": "Moves all submissions and other data linked to the primary user into the secondary user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a course for the user after conducting a set of sanity checks. Returns the title of the course or an exception.", "response": "def add_course_safe(self, id):\n        '''\n            Adds a course for the user after conducting a set of sanity checks.\n            Return the title of the course or an exception.\n        '''\n        course = get_object_or_404(Course, pk=int(id), active=True)\n        if course not in self.courses.all():\n            self.courses.add(course)\n        return course.title"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of courses this user is subscribed for and tutoring for.", "response": "def user_courses(self):\n        '''\n            Returns the list of courses this user is subscribed for,\n            or owning, or tutoring.\n            This leads to the fact that tutors and owners don't need\n            course membership.\n        '''\n        registered = self.courses.filter(active__exact=True).distinct()\n        return (self.tutor_courses() | registered).distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of open assignments for this user.", "response": "def open_assignments(self):\n        '''\n            Returns the list of open assignments from the\n            viewpoint of this user.\n        '''\n        # Include only assignments with future, or no, hard deadline\n        qs = Assignment.objects.filter(hard_deadline__gt=timezone.now(\n        )) | Assignment.objects.filter(hard_deadline__isnull=True)\n        # Include only assignments that are already published,\n        # as long as you are not a tutor / course owner\n        if not self.can_see_future():\n            qs = qs.filter(publish_at__lt=timezone.now())\n        # Include only assignments from courses that you are registered for\n        qs = qs.filter(course__in=self.user_courses())\n        # Ordering of resulting list\n        qs = qs.order_by('soft_deadline', '-gradingScheme', 'title')\n        waiting_for_action = [subm.assignment for subm in self.user.authored.all(\n        ).exclude(state=Submission.WITHDRAWN)]\n        # Emulate is_null sorting for soft_deadline\n        qs_without_soft_deadline = qs.filter(soft_deadline__isnull=True)\n        qs_with_soft_deadline = qs.filter(soft_deadline__isnull=False)\n        ass_list = [\n            ass for ass in qs_without_soft_deadline if ass not in waiting_for_action]\n        ass_list += [\n            ass for ass in qs_with_soft_deadline if ass not in waiting_for_action]\n        return ass_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gone_assignments(self):\n        '''\n            Returns the list of past assignments the user did not submit for\n            before the hard deadline.\n        '''\n        # Include only assignments with past hard deadline\n        qs = Assignment.objects.filter(hard_deadline__lt=timezone.now())\n        # Include only assignments from courses this user is registered for\n        qs = qs.filter(course__in=self.user_courses())\n        # Include only assignments this user has no submission for\n        return qs.order_by('-hard_deadline')", "response": "Returns the list of assignments that have not been submitted for\n            before the hard deadline."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_message(self, to, text, subject=None, mtype=None, unsubscribe_url=None):\n        # TODO Maybe file attachments handling through `files` message_model context var.\n\n        if subject is None:\n            subject = u'%s' % _('No Subject')\n\n        if mtype == 'html':\n            msg = self.mime_multipart()\n            text_part = self.mime_multipart('alternative')\n            text_part.attach(self.mime_text(strip_tags(text), _charset='utf-8'))\n            text_part.attach(self.mime_text(text, 'html', _charset='utf-8'))\n            msg.attach(text_part)\n\n        else:\n            msg = self.mime_text(text, _charset='utf-8')\n\n        msg['From'] = self.from_email\n        msg['To'] = to\n        msg['Subject'] = subject\n\n        if unsubscribe_url:\n            msg['List-Unsubscribe'] = '<%s>' % unsubscribe_url\n\n        return msg", "response": "Constructs a MIME message from a message and dispatch models."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nview function that redirects to social auth login in case the user is not logged in.", "response": "def _social_auth_login(self, request, **kwargs):\n    '''\n        View function that redirects to social auth login,\n        in case the user is not logged in.\n    '''\n    if request.user.is_authenticated():\n        if not request.user.is_active or not request.user.is_staff:\n            raise PermissionDenied()\n    else:\n        messages.add_message(request, messages.WARNING, 'Please authenticate first.')\n        return redirect_to_login(request.get_full_path())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestrict the listed courses for the current user.", "response": "def get_queryset(self, request):\n        ''' Restrict the listed courses for the current user.'''\n        qs = super(CourseAdmin, self).get_queryset(request)\n        if request.user.is_superuser:\n            return qs\n        else:\n            return qs.filter(Q(tutors__pk=request.user.pk) | Q(owner=request.user)).distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_tag_families(tagged_paired_aligns,\n                        ranked_tags,\n                        hamming_threshold,\n                        consensus_threshold,\n                        family_filter=lambda _: None):\n    '''Partition paired aligns into families.\n\n    Each read is considered against each ranked tag until all reads are\n    partitioned into families.'''\n    tag_aligns = defaultdict(set)\n    tag_inexact_match_count = defaultdict(int)\n\n    for paired_align in tagged_paired_aligns:\n        (left_umt, right_umt) =  paired_align.umt\n        for best_tag in ranked_tags:\n            if paired_align.umt == best_tag:\n                tag_aligns[best_tag].add(paired_align)\n                break\n            elif left_umt == best_tag[0] or right_umt == best_tag[1]:\n                tag_aligns[best_tag].add(paired_align)\n                tag_inexact_match_count[best_tag] += 1\n                break\n            elif (_hamming_dist(left_umt, best_tag[0]) <= hamming_threshold) \\\n                or (_hamming_dist(right_umt, best_tag[1]) <= hamming_threshold):\n                tag_aligns[best_tag].add(paired_align)\n                tag_inexact_match_count[best_tag] += 1\n                break\n    tag_families = []\n    for tag in sorted(tag_aligns):\n        tag_family = TagFamily(tag,\n                               tag_aligns[tag],\n                               tag_inexact_match_count[tag],\n                               consensus_threshold,\n                               family_filter)\n        tag_families.append(tag_family)\n    return tag_families", "response": "Partition paired aligns into families."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of tags ranked from most to least popular.", "response": "def _rank_tags(tagged_paired_aligns):\n    '''Return the list of tags ranked from most to least popular.'''\n    tag_count_dict = defaultdict(int)\n    for paired_align in tagged_paired_aligns:\n        tag_count_dict[paired_align.umt] += 1\n    tags_by_count = utils.sort_dict(tag_count_dict)\n    ranked_tags = [tag_count[0] for tag_count in tags_by_count]\n    return ranked_tags"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nschedules an email message for delivery.", "response": "def schedule_email(message, to, subject=None, sender=None, priority=None):\n    \"\"\"Schedules an email message for delivery.\n\n    :param dict, str message: str or dict: use str for simple text email;\n        dict - to compile email from a template (default: `sitemessage/messages/email_html__smtp.html`).\n    :param list|str|unicode to: recipients addresses or Django User model heir instances\n    :param str subject: email subject\n    :param User sender: User model heir instance\n    :param int priority: number describing message priority. If set overrides priority provided with message type.\n    \"\"\"\n\n    if SHORTCUT_EMAIL_MESSAGE_TYPE:\n        message_cls = get_registered_message_type(SHORTCUT_EMAIL_MESSAGE_TYPE)\n\n    else:\n\n        if isinstance(message, dict):\n            message_cls = EmailHtmlMessage\n        else:\n            message_cls = EmailTextMessage\n\n    schedule_messages(\n        message_cls(subject, message),\n        recipients(SHORTCUT_EMAIL_MESSENGER_TYPE, to),\n        sender=sender, priority=priority\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscheduling a Jabber XMPP message for delivery.", "response": "def schedule_jabber_message(message, to, sender=None, priority=None):\n    \"\"\"Schedules Jabber XMPP message for delivery.\n\n    :param str message: text to send.\n    :param list|str|unicode to: recipients addresses or Django User model heir instances with `email` attributes.\n    :param User sender: User model heir instance\n    :param int priority: number describing message priority. If set overrides priority provided with message type.\n    \"\"\"\n    schedule_messages(message, recipients('xmppsleek', to), sender=sender, priority=priority)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schedule_tweet(message, to='', sender=None, priority=None):\n    schedule_messages(message, recipients('twitter', to), sender=sender, priority=priority)", "response": "Schedules a Tweet for delivery."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nschedule Telegram message for delivery.", "response": "def schedule_telegram_message(message, to, sender=None, priority=None):\n    \"\"\"Schedules Telegram message for delivery.\n\n    :param str message: text to send.\n    :param list|str|unicode to: recipients addresses or Django User model heir instances with `telegram` attributes.\n    :param User sender: User model heir instance\n    :param int priority: number describing message priority. If set overrides priority provided with message type.\n    \"\"\"\n    schedule_messages(message, recipients('telegram', to), sender=sender, priority=priority)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef schedule_facebook_message(message, sender=None, priority=None):\n    schedule_messages(message, recipients('fb', ''), sender=sender, priority=priority)", "response": "Schedules Facebook wall message for delivery."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nschedule VKontakte message for delivery on the wall.", "response": "def schedule_vkontakte_message(message, to, sender=None, priority=None):\n    \"\"\"Schedules VKontakte message for delivery.\n\n    :param str message: text or URL to publish on wall.\n    :param list|str|unicode to: recipients addresses or Django User model heir instances with `vk` attributes.\n    :param User sender: User model heir instance\n    :param int priority: number describing message priority. If set overrides priority provided with message type.\n    \"\"\"\n    schedule_messages(message, recipients('vk', to), sender=sender, priority=priority)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn messenger alias. :return: str :rtype: str", "response": "def get_alias(cls):\n        \"\"\"Returns messenger alias.\n\n        :return: str\n        :rtype: str\n        \"\"\"\n        if cls.alias is None:\n            cls.alias = cls.__name__\n        return cls.alias"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _structure_recipients_data(cls, recipients):\n        try:  # That's all due Django 1.7 apps loading.\n            from django.contrib.auth import get_user_model\n            USER_MODEL = get_user_model()\n        except ImportError:\n            # Django 1.4 fallback.\n            from django.contrib.auth.models import User as USER_MODEL\n\n        if not is_iterable(recipients):\n            recipients = (recipients,)\n\n        objects = []\n        for r in recipients:\n            user = None\n            if isinstance(r, USER_MODEL):\n                user = r\n            address = cls.get_address(r)  # todo maybe raise an exception of not a string?\n\n            objects.append(Recipient(cls.get_alias(), user, address))\n\n        return objects", "response": "Converts recipients data into a list of Recipient objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mark_error(self, dispatch, error_log, message_cls):\n        if message_cls.send_retry_limit is not None and (dispatch.retry_count + 1) >= message_cls.send_retry_limit:\n            self.mark_failed(dispatch, error_log)\n        else:\n            dispatch.error_log = error_log\n            self._st['error'].append(dispatch)", "response": "Mark a dispatch as having error or consequently as failed\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mark_failed(self, dispatch, error_log):\n        dispatch.error_log = error_log\n        self._st['failed'].append(dispatch)", "response": "Mark a dispatch as failed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms message processing. :param dict messages: indexed by message id dict with messages data :param bool ignore_unknown_message_types: whether to silence exceptions :raises UnknownMessageTypeError:", "response": "def _process_messages(self, messages, ignore_unknown_message_types=False):\n        \"\"\"Performs message processing.\n\n        :param dict messages: indexed by message id dict with messages data\n        :param bool ignore_unknown_message_types: whether to silence exceptions\n        :raises UnknownMessageTypeError:\n        \"\"\"\n        with self.before_after_send_handling():\n            for message_id, message_data in messages.items():\n                message_model, dispatch_models = message_data\n                try:\n                    message_cls = get_registered_message_type(message_model.cls)\n                except UnknownMessageTypeError:\n                    if ignore_unknown_message_types:\n                        continue\n                    raise\n\n                message_type_cache = None\n                for dispatch in dispatch_models:\n                    if not dispatch.message_cache:  # Create actual message text for further usage.\n                        try:\n                            if message_type_cache is None and not message_cls.has_dynamic_context:\n                                # If a message class doesn't depend upon a dispatch data for message compilation,\n                                # we'd compile a message just once.\n                                message_type_cache = message_cls.compile(message_model, self, dispatch=dispatch)\n\n                            dispatch.message_cache = message_type_cache or message_cls.compile(\n                                message_model, self, dispatch=dispatch)\n\n                        except Exception as e:\n                            self.mark_error(dispatch, e, message_cls)\n\n                self.send(message_cls, message_model, dispatch_models)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_dispatches(self):\n        Dispatch.log_dispatches_errors(self._st['error'] + self._st['failed'])\n        Dispatch.set_dispatches_statuses(**self._st)\n        self._init_delivery_statuses_dict()", "response": "Updates dispatched data in DB according to information gather by mark_* methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self, app):\n        '''\n        Make sure that other installed plugins don't affect the same keyword argument.\n        '''\n        for other in app.plugins:\n            if not isinstance(other, MySQLPlugin):\n                continue\n            if other.keyword == self.keyword:\n                raise PluginError(\"Found another mysql plugin with conflicting settings (non-unique keyword).\")\n            elif other.name == self.name:\n                self.name += '_%s' % self.keyword", "response": "Make sure that other plugins don t affect the same keyword argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assign_role(backend, user, response, *args, **kwargs):\n    '''\n    Part of the Python Social Auth Pipeline.\n    Checks if the created demo user should be pushed into some group.\n    '''\n    if backend.name is 'passthrough' and settings.DEMO is True and 'role' in kwargs['request'].session[passthrough.SESSION_VAR]:\n        role = kwargs['request'].session[passthrough.SESSION_VAR]['role']\n        if role == 'tutor':\n            make_tutor(user)\n        if role == 'admin':\n            make_admin(user)\n        if role == 'owner':\n            make_owner(user)", "response": "Assign a user to a demo user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches data from an URL and save it under the given target name.", "response": "def fetch(url, fullpath):\n    '''\n    Fetch data from an URL and save it under the given target name.\n    '''\n    logger.debug(\"Fetching %s from %s\" % (fullpath, url))\n\n    try:\n        tmpfile, headers = urlretrieve(url)\n        if os.path.exists(fullpath):\n            os.remove(fullpath)\n        shutil.move(tmpfile, fullpath)\n    except Exception as e:\n        logger.error(\"Error during fetching: \" + str(e))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_post(config, urlpath, post_data):\n    '''\n    Send POST data to an OpenSubmit server url path,\n    according to the configuration.\n    '''\n    server = config.get(\"Server\", \"url\")\n    logger.debug(\"Sending executor payload to \" + server)\n    post_data = urlencode(post_data)\n    post_data = post_data.encode(\"utf-8\", errors=\"ignore\")\n    url = server + urlpath\n    try:\n        urlopen(url, post_data)\n    except Exception as e:\n        logger.error('Error while sending data to server: ' + str(e))", "response": "Send POST data to an OpenSubmit server url path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_hostinfo(config):\n    '''\n    Register this host on OpenSubmit test machine.\n    '''\n    info = all_host_infos()\n    logger.debug(\"Sending host information: \" + str(info))\n    post_data = [(\"Config\", json.dumps(info)),\n                 (\"Action\", \"get_config\"),\n                 (\"UUID\", config.get(\"Server\", \"uuid\")),\n                 (\"Address\", ipaddress()),\n                 (\"Secret\", config.get(\"Server\", \"secret\"))\n                 ]\n\n    send_post(config, \"/machines/\", post_data)", "response": "Send host info to OpenSubmit test machine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compatible_api_version(server_version):\n    '''\n    Check if this server API version is compatible to us.\n    '''\n    try:\n        semver = server_version.split('.')\n        if semver[0] != '1':\n            logger.error(\n                'Server API version (%s) is too new for us. Please update the executor installation.' % server_version)\n            return False\n        else:\n            return True\n    except Exception:\n        logger.error(\n            'Cannot understand the server API version (%s). Please update the executor installation.' % server_version)\n        return False", "response": "Check if the server API version is compatible with us."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch_job(config):\n    '''\n    Fetch any available work from the OpenSubmit server and\n    return an according job object.\n\n    Returns None if no work is available.\n\n    Errors are reported by this function directly.\n    '''\n    url = \"%s/jobs/?Secret=%s&UUID=%s\" % (config.get(\"Server\", \"url\"),\n                                          config.get(\"Server\", \"secret\"),\n                                          config.get(\"Server\", \"uuid\"))\n\n    try:\n        # Fetch information from server\n        result = urlopen(url)\n        headers = result.info()\n        logger.debug(\"Raw job data: \" + str(result.headers).replace('\\n', ', '))\n        if not compatible_api_version(headers[\"APIVersion\"]):\n            # No proper reporting possible, so only logging.\n            logger.error(\"Incompatible API version. Please update OpenSubmit.\")\n            return None\n\n        if headers[\"Action\"] == \"get_config\":\n            # The server does not know us,\n            # so it demands registration before hand.\n            logger.info(\"Machine unknown on server, sending registration ...\")\n            send_hostinfo(config)\n            return None\n\n        # Create job object with information we got\n        from .job import Job\n        job = Job(config)\n\n        job.submitter_name = headers['SubmitterName']\n        job.author_names = headers['AuthorNames']\n        job.submitter_studyprogram = headers['SubmitterStudyProgram']\n        job.course = headers['Course']\n        job.assignment = headers['Assignment']\n        job.action = headers[\"Action\"]\n        job.file_id = headers[\"SubmissionFileId\"]\n        job.sub_id = headers[\"SubmissionId\"]\n        job.file_name = headers[\"SubmissionOriginalFilename\"]\n        job.submitter_student_id = headers[\"SubmitterStudentId\"]\n        if \"Timeout\" in headers:\n            job.timeout = int(headers[\"Timeout\"])\n        if \"PostRunValidation\" in headers:\n            # Ignore server-given host + port and use the configured one instead\n            # This fixes problems with the arbitrary Django LiveServer port choice\n            # It would be better to return relative URLs only for this property,\n            # but this is a Bernhard-incompatible API change\n            from urllib.parse import urlparse\n            relative_path = urlparse(headers[\"PostRunValidation\"]).path\n            job.validator_url = config.get(\"Server\", \"url\") + relative_path\n        job.working_dir = create_working_dir(config, job.sub_id)\n\n        # Store submission in working directory\n        submission_fname = job.working_dir + job.file_name\n        with open(submission_fname, 'wb') as target:\n            target.write(result.read())\n        assert(os.path.exists(submission_fname))\n\n        # Store validator package in working directory\n        validator_fname = job.working_dir + 'download.validator'\n        fetch(job.validator_url, validator_fname)\n\n        try:\n            prepare_working_directory(job, submission_fname, validator_fname)\n        except JobException as e:\n            job.send_fail_result(e.info_student, e.info_tutor)\n            return None\n        logger.debug(\"Got job: \" + str(job))\n        return job\n    except HTTPError as e:\n        if e.code == 404:\n            logger.debug(\"Nothing to do.\")\n            return None\n    except URLError as e:\n        logger.error(\"Error while contacting {0}: {1}\".format(url, str(e)))\n        return None", "response": "Fetch any available work from the OpenSubmit server and return an according job object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fake_fetch_job(config, src_dir):\n    '''\n    Act like fetch_job, but take the validator file and the student\n    submission files directly from a directory.\n\n    Intended for testing purposes when developing test scripts.\n\n    Check also cmdline.py.\n    '''\n    logger.debug(\"Creating fake job from \" + src_dir)\n    from .job import Job\n    job = Job(config, online=False)\n    job.working_dir = create_working_dir(config, '42')\n    for fname in glob.glob(src_dir + os.sep + '*'):\n        logger.debug(\"Copying {0} to {1} ...\".format(fname, job.working_dir))\n        shutil.copy(fname, job.working_dir)\n    case_files = glob.glob(job.working_dir + os.sep + '*')\n    assert(len(case_files) == 2)\n    if os.path.basename(case_files[0]) in ['validator.py', 'validator.zip']:\n        validator = case_files[0]\n        submission = case_files[1]\n    else:\n        validator = case_files[1]\n        submission = case_files[0]\n    logger.debug('{0} is the validator.'.format(validator))\n    logger.debug('{0} the submission.'.format(submission))\n    try:\n        prepare_working_directory(job,\n                                  submission_path=submission,\n                                  validator_path=validator)\n    except JobException as e:\n        job.send_fail_result(e.info_student, e.info_tutor)\n        return None\n    logger.debug(\"Got fake job: \" + str(job))\n    return job", "response": "This function creates a fake job from a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the configure program in the working directory.", "response": "def run_configure(self, mandatory=True):\n        \"\"\"Runs the 'configure' program in the working directory.\n\n        Args:\n            mandatory (bool): Throw exception if 'configure' fails or a\n                              'configure' file is missing.\n\n        \"\"\"\n        if not has_file(self.working_dir, 'configure'):\n            if mandatory:\n                raise FileNotFoundError(\n                    \"Could not find a configure script for execution.\")\n            else:\n                return\n        try:\n            prog = RunningProgram(self, 'configure')\n            prog.expect_exit_status(0)\n        except Exception:\n            if mandatory:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a compiler in the working directory.", "response": "def run_compiler(self, compiler=GCC, inputs=None, output=None):\n        \"\"\"Runs a compiler in the working directory.\n\n        Args:\n            compiler (tuple): The compiler program and its command-line arguments,\n                              including placeholders for output and input files.\n            inputs (tuple):   The list of input files for the compiler.\n            output (str):     The name of the output file.\n\n        \"\"\"\n        # Let exceptions travel through\n        prog = RunningProgram(self, *compiler_cmdline(compiler=compiler,\n                                                      inputs=inputs,\n                                                      output=output))\n        prog.expect_exit_status(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the build steps for the current locale.", "response": "def run_build(self, compiler=GCC, inputs=None, output=None):\n        \"\"\"Combined call of 'configure', 'make' and the compiler.\n\n        The success of 'configure' and 'make' is optional.\n        The arguments are the same as for run_compiler.\n\n        \"\"\"\n        logger.info(\"Running build steps ...\")\n        self.run_configure(mandatory=False)\n        self.run_make(mandatory=False)\n        self.run_compiler(compiler=compiler,\n                          inputs=inputs,\n                          output=output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nspawns a program in the working directory.", "response": "def spawn_program(self, name, arguments=[], timeout=30, exclusive=False):\n        \"\"\"Spawns a program in the working directory.\n\n        This method allows the interaction with the running program,\n        based on the returned RunningProgram object.\n\n        Args:\n            name (str):        The name of the program to be executed.\n            arguments (tuple): Command-line arguments for the program.\n            timeout (int):     The timeout for execution.\n            exclusive (bool):  Prevent parallel validation runs on the\n                               test machines, e.g. when doing performance\n                               measurements for submitted code.\n\n        Returns:\n            RunningProgram: An object representing the running program.\n\n        \"\"\"\n        logger.debug(\"Spawning program for interaction ...\")\n        if exclusive:\n            kill_longrunning(self.config)\n\n        return RunningProgram(self, name, arguments, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a program in the working directory to completion.", "response": "def run_program(self, name, arguments=[], timeout=30, exclusive=False):\n        \"\"\"Runs a program in the working directory to completion.\n\n        Args:\n            name (str):        The name of the program to be executed.\n            arguments (tuple): Command-line arguments for the program.\n            timeout (int):     The timeout for execution.\n            exclusive (bool):  Prevent parallel validation runs on the\n                               test machines, e.g. when doing performance\n                               measurements for submitted code.\n\n        Returns:\n            tuple: A tuple of the exit code, as reported by the operating system,\n            and the output produced during the execution.\n        \"\"\"\n        logger.debug(\"Running program ...\")\n        if exclusive:\n            kill_longrunning(self.config)\n\n        prog = RunningProgram(self, name, arguments, timeout)\n        return prog.expect_end()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscanning the student files for text patterns and returns a list of names that match the regular expression.", "response": "def grep(self, regex):\n        \"\"\"Scans the student files for text patterns.\n\n        Args:\n            regex (str):       Regular expression used for scanning inside the files.\n\n        Returns:\n            tuple:     Names of the matching files in the working directory.\n        \"\"\"\n        matches = []\n        logger.debug(\"Searching student files for '{0}'\".format(regex))\n        for fname in self.student_files:\n            if os.path.isfile(self.working_dir + fname):\n                for line in open(self.working_dir + fname, 'br'):\n                    if re.search(regex.encode(), line):\n                        logger.debug(\"{0} contains '{1}'\".format(fname, regex))\n                        matches.append(fname)\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_files(self, filenames):\n        logger.debug(\"Testing {0} for the following files: {1}\".format(\n            self.working_dir, filenames))\n        dircontent = os.listdir(self.working_dir)\n        for fname in filenames:\n            if fname not in dircontent:\n                return False\n        return True", "response": "Checks the student submission for specific files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying any available live chats as advertisements.", "response": "def live_chat_banner(context):\n    \"\"\" Display any available live chats as advertisements.\n    \"\"\"\n    context = copy(context)\n\n    # Find any upcoming or current live chat. The Chat date must be less than 5\n    # days away, or currently in progress.\n\n    oldchat = LiveChat.chat_finder.get_last_live_chat()\n\n    if oldchat:\n        context['last_live_chat'] = {\n            'title': oldchat.title,\n            'chat_ends_at': oldchat.chat_ends_at,\n            'expert': oldchat.expert,\n            'url': reverse('livechat:show_archived_livechat')\n\n        }\n\n    chat = LiveChat.chat_finder.upcoming_live_chat()\n    if chat is not None:\n        context['live_chat_advert'] = {\n            'title': chat.title,\n            'description': chat.description,\n            'expert': chat.expert,\n            'commenting_closed': chat.comments_closed,\n            'cancelled': chat.is_cancelled,\n            'archived': chat.is_archived,\n            'in_progress': chat.is_in_progress(),\n            'url':  reverse(\n                'livechat:show_livechat',\n                kwargs={\n                    'slug': chat.slug}),\n            'archive_url':reverse('livechat:show_archived_livechat')\n\n        }\n        context['live_chat_advert']['datetime'] = {\n            'time': chat.chat_starts_at.time,\n            'date': chat.chat_starts_at.date\n            }\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kill_longrunning(config):\n    '''\n        Terminate everything under the current user account\n        that has run too long. This is a final safeguard if\n        the subprocess timeout stuff is not working.\n        You better have no production servers running also\n        under the current user account ...\n    '''\n    import psutil\n    ourpid = os.getpid()\n    username = psutil.Process(ourpid).username\n    # Check for other processes running under this account\n    # Take the timeout definition from the config file\n    timeout = config.getint(\"Execution\", \"timeout\")\n    for proc in psutil.process_iter():\n        if proc.username == username and proc.pid != ourpid:\n            runtime = time.time() - proc.create_time\n            logger.debug(\"This user already runs %u for %u seconds.\" %\n                         (proc.pid, runtime))\n            if runtime > timeout:\n                logger.debug(\"Killing %u due to exceeded runtime.\" % proc.pid)\n                try:\n                    proc.kill()\n                except Exception:\n                    logger.error(\"ERROR killing process %d.\" % proc.pid)", "response": "Kill processes that are running too long."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_exitstatus(self):\n        logger.debug(\"Exit status is {0}\".format(self._spawn.exitstatus))\n        return self._spawn.exitstatus", "response": "Get the exit status of the program execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expect_output(self, pattern, timeout=-1):\n        logger.debug(\"Expecting output '{0}' from '{1}'\".format(pattern, self.name))\n        try:\n            return self._spawn.expect(pattern, timeout)\n        except pexpect.exceptions.EOF as e:\n            logger.debug(\"Raising termination exception.\")\n            raise TerminationException(instance=self, real_exception=e, output=self.get_output())\n        except pexpect.exceptions.TIMEOUT as e:\n            logger.debug(\"Raising timeout exception.\")\n            raise TimeoutException(instance=self, real_exception=e, output=self.get_output())\n        except Exception as e:\n            logger.debug(\"Expecting output failed: \" + str(e))\n            raise NestedException(instance=self, real_exception=e, output=self.get_output())", "response": "Wait until the running program performs some given output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendline(self, text):\n        logger.debug(\"Sending input '{0}' to '{1}'\".format(text, self.name))\n        try:\n            return self._spawn.sendline(text)\n        except pexpect.exceptions.EOF as e:\n            logger.debug(\"Raising termination exception.\")\n            raise TerminationException(instance=self, real_exception=e, output=self.get_output())\n        except pexpect.exceptions.TIMEOUT as e:\n            logger.debug(\"Raising timeout exception.\")\n            raise TimeoutException(instance=self, real_exception=e, output=self.get_output())\n        except Exception as e:\n            logger.debug(\"Sending input failed: \" + str(e))\n            raise NestedException(instance=self, real_exception=e, output=self.get_output())", "response": "Sends a line to the running program including os. linesep. Returns the number of bytes sent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting for the running program to finish and return the exit code and the output produced by the operating system.", "response": "def expect_end(self):\n        \"\"\"Wait for the running program to finish.\n\n        Returns:\n            A tuple with the exit code, as reported by the operating system, and the output produced.\n        \"\"\"\n        logger.debug(\"Waiting for termination of '{0}'\".format(self.name))\n        try:\n            # Make sure we fetch the last output bytes.\n            # Recommendation from the pexpect docs.\n            self._spawn.expect(pexpect.EOF)\n            self._spawn.wait()\n            dircontent = str(os.listdir(self.job.working_dir))\n            logger.debug(\"Working directory after execution: \" + dircontent)\n            return self.get_exitstatus(), self.get_output()\n        except pexpect.exceptions.EOF as e:\n            logger.debug(\"Raising termination exception.\")\n            raise TerminationException(instance=self, real_exception=e, output=self.get_output())\n        except pexpect.exceptions.TIMEOUT as e:\n            logger.debug(\"Raising timeout exception.\")\n            raise TimeoutException(instance=self, real_exception=e, output=self.get_output())\n        except Exception as e:\n            logger.debug(\"Waiting for expected program end failed.\")\n            raise NestedException(instance=self, real_exception=e, output=self.get_output())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait for the running program to finish and expect some exit status.", "response": "def expect_exitstatus(self, exit_status):\n        \"\"\"Wait for the running program to finish and expect some exit status.\n\n        Args:\n            exit_status (int):  The expected exit status.\n\n        Raises:\n            WrongExitStatusException: The produced exit status is not the expected one.\n        \"\"\"\n        self.expect_end()\n        logger.debug(\"Checking exit status of '{0}', output so far: {1}\".format(\n            self.name, self.get_output()))\n        if self._spawn.exitstatus is None:\n            raise WrongExitStatusException(\n                instance=self, expected=exit_status, output=self.get_output())\n\n        if self._spawn.exitstatus is not exit_status:\n            raise WrongExitStatusException(\n                instance=self,\n                expected=exit_status,\n                got=self._spawn.exitstatus,\n                output=self.get_output())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling unsubscribe request. :param Request request: :param int message_id: :param int dispatch_id: :param str hashed: :param str redirect_to: :return:", "response": "def unsubscribe(request, message_id, dispatch_id, hashed, redirect_to=None):\n    \"\"\"Handles unsubscribe request.\n\n    :param Request request:\n    :param int message_id:\n    :param int dispatch_id:\n    :param str hashed:\n    :param str redirect_to:\n    :return:\n    \"\"\"\n    return _generic_view(\n        'handle_unsubscribe_request', sig_unsubscribe_failed,\n        request, message_id, dispatch_id, hashed, redirect_to=redirect_to\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mark_read(request, message_id, dispatch_id, hashed, redirect_to=None):\n    if redirect_to is None:\n        redirect_to = get_static_url('img/sitemessage/blank.png')\n\n    return _generic_view(\n        'handle_mark_read_request', sig_mark_read_failed,\n        request, message_id, dispatch_id, hashed, redirect_to=redirect_to\n    )", "response": "Handles mark message as read request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef schedule_messages(messages, recipients=None, sender=None, priority=None):\n    if not is_iterable(messages):\n        messages = (messages,)\n\n    results = []\n    for message in messages:\n        if isinstance(message, six.string_types):\n            message = PlainTextMessage(message)\n\n        resulting_priority = message.priority\n        if priority is not None:\n            resulting_priority = priority\n        results.append(message.schedule(sender=sender, recipients=recipients, priority=resulting_priority))\n\n    return results", "response": "Schedules a list of messages."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_scheduled_messages(priority=None, ignore_unknown_messengers=False, ignore_unknown_message_types=False):\n    dispatches_by_messengers = Dispatch.group_by_messengers(Dispatch.get_unsent(priority=priority))\n\n    for messenger_id, messages in dispatches_by_messengers.items():\n        try:\n            messenger_obj = get_registered_messenger_object(messenger_id)\n            messenger_obj._process_messages(messages, ignore_unknown_message_types=ignore_unknown_message_types)\n        except UnknownMessengerError:\n            if ignore_unknown_messengers:\n                continue\n            raise", "response": "Sends scheduled messages.\n\n    :param int, None priority: number to limit sending message by this priority.\n    :param bool ignore_unknown_messengers: to silence UnknownMessengerError\n    :param bool ignore_unknown_message_types: to silence UnknownMessageTypeError\n    :raises UnknownMessengerError:\n    :raises UnknownMessageTypeError:"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_undelivered(to=None):\n    failed_count = Dispatch.objects.filter(dispatch_status=Dispatch.DISPATCH_STATUS_FAILED).count()\n\n    if failed_count:\n        from sitemessage.shortcuts import schedule_email\n        from sitemessage.messages.email import EmailTextMessage\n\n        if to is None:\n            admins = settings.ADMINS\n\n            if admins:\n                to = list(dict(admins).values())\n\n        if to:\n            priority = 999\n\n            register_message_types(EmailTextMessage)\n\n            schedule_email(\n                'You have %s undelivered dispatch(es) at %s' % (failed_count, get_site_url()),\n                subject='[SITEMESSAGE] Undelivered dispatches',\n                to=to, priority=priority)\n\n            send_scheduled_messages(priority=priority)\n\n    return failed_count", "response": "Sends an email if any undelivered dispatches are found. Returns undelivered dispatches count."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cleanup_sent_messages(ago=None, dispatches_only=False):\n    filter_kwargs = {\n        'dispatch_status': Dispatch.DISPATCH_STATUS_SENT,\n    }\n\n    objects = Dispatch.objects\n\n    if ago:\n        filter_kwargs['time_dispatched__lte'] = timezone.now() - timedelta(days=int(ago))\n\n    dispatch_map = dict(objects.filter(**filter_kwargs).values_list('pk', 'message_id'))\n\n    # Remove dispatches\n    objects.filter(pk__in=list(dispatch_map.keys())).delete()\n\n    if not dispatches_only:\n        # Remove messages also.\n        messages_ids = set(dispatch_map.values())\n\n        if messages_ids:\n            messages_blocked = set(chain.from_iterable(\n                objects.filter(message_id__in=messages_ids).values_list('message_id')))\n\n            messages_stale = messages_ids.difference(messages_blocked)\n\n            if messages_stale:\n                Message.objects.filter(pk__in=messages_stale).delete()", "response": "Cleans up DB : removes messages and dispatches that were sent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_preferences_for_ui(user, message_filter=None, messenger_filter=None, new_messengers_titles=None):\n    if new_messengers_titles is None:\n        new_messengers_titles = {}\n\n    msgr_to_msg = defaultdict(set)\n    msg_titles = OrderedDict()\n    msgr_titles = OrderedDict()\n\n    for msgr in get_registered_messenger_objects().values():\n        if not (messenger_filter is None or messenger_filter(msgr)) or not msgr.allow_user_subscription:\n            continue\n\n        msgr_alias = msgr.alias\n        msgr_title = new_messengers_titles.get(msgr.alias) or msgr.title\n\n        for msg in get_registered_message_types().values():\n            if not (message_filter is None or message_filter(msg)) or not msg.allow_user_subscription:\n                continue\n\n            msgr_supported = msg.supported_messengers\n            is_supported = (not msgr_supported or msgr.alias in msgr_supported)\n\n            if not is_supported:\n                continue\n\n            msg_alias = msg.alias\n            msg_titles.setdefault('%s' % msg.title, []).append(msg_alias)\n\n            msgr_to_msg[msgr_alias].update((msg_alias,))\n            msgr_titles[msgr_title] = msgr_alias\n\n    def sort_titles(titles):\n        return OrderedDict(sorted([(k, v) for k, v in titles.items()], key=itemgetter(0)))\n\n    msgr_titles = sort_titles(msgr_titles)\n\n    user_prefs = OrderedDict()\n\n    user_subscriptions = ['%s%s%s' % (pref.message_cls, _ALIAS_SEP, pref.messenger_cls)\n                          for pref in Subscription.get_for_user(user)]\n\n    for msg_title, msg_aliases in sort_titles(msg_titles).items():\n\n        for __, msgr_alias in msgr_titles.items():\n            msg_candidates = msgr_to_msg[msgr_alias].intersection(msg_aliases)\n\n            alias = ''\n            msg_supported = False\n            subscribed = False\n\n            if msg_candidates:\n                alias = '%s%s%s' % (msg_candidates.pop(), _ALIAS_SEP, msgr_alias)\n                msg_supported = True\n                subscribed = alias in user_subscriptions\n\n            user_prefs.setdefault(msg_title, []).append((alias, msg_supported, subscribed))\n\n    return msgr_titles.keys(), user_prefs", "response": "Returns a two element tuple with user subscription preferences for the given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_user_preferences_from_request(request):\n    prefs = []\n\n    for pref in request.POST.getlist(_PREF_POST_KEY):\n        message_alias, messenger_alias = pref.split(_ALIAS_SEP)\n\n        try:\n            get_registered_message_type(message_alias)\n            get_registered_messenger_object(messenger_alias)\n\n        except (UnknownMessengerError, UnknownMessageTypeError):\n            pass\n\n        else:\n            prefs.append((message_alias, messenger_alias))\n\n    Subscription.replace_for_user(request.user, prefs)\n\n    return bool(prefs)", "response": "Sets user preferences using data sent by the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of urlpatterns that can be attached to a project", "response": "def get_sitemessage_urls():\n    \"\"\"Returns sitemessage urlpatterns, that can be attached to urlpatterns of a project:\n\n        # Example from urls.py.\n\n        from sitemessage.toolbox import get_sitemessage_urls\n\n        urlpatterns = patterns('',\n            # Your URL Patterns belongs here.\n\n        ) + get_sitemessage_urls()  # Now attaching additional URLs.\n\n    \"\"\"\n    url_unsubscribe = url(\n        r'^messages/unsubscribe/(?P<message_id>\\d+)/(?P<dispatch_id>\\d+)/(?P<hashed>[^/]+)/$',\n        unsubscribe,\n        name='sitemessage_unsubscribe'\n    )\n\n    url_mark_read = url(\n        r'^messages/ping/(?P<message_id>\\d+)/(?P<dispatch_id>\\d+)/(?P<hashed>[^/]+)/$',\n        mark_read,\n        name='sitemessage_mark_read'\n    )\n\n    if VERSION >= (1, 9):\n        return [url_unsubscribe, url_mark_read]\n\n    from django.conf.urls import patterns\n    return patterns('', url_unsubscribe, url_mark_read)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list of gradings in this scheme as rendered string.", "response": "def gradings(gradingScheme):\n    ''' Determine the list of gradings in this scheme as rendered string.\n        TODO: Use nice little icons instead of (p) / (f) marking.\n    '''\n    result = []\n    for grading in gradingScheme.gradings.all():\n        if grading.means_passed:\n            result.append(str(grading) + \" (pass)\")\n        else:\n            result.append(str(grading) + \" (fail)\")\n    return '  -  '.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef formfield_for_dbfield(self, db_field, **kwargs):\n        ''' Offer only gradings that are not used by other schemes, which means they are used by this scheme or not at all.'''\n        if db_field.name == \"gradings\":\n            request=kwargs['request']\n            try:\n                #TODO:  MockRequst object from unit test does not contain path information, so an exception occurs during test.\n                #       Find a test-suite compatible solution here.\n                obj=resolve(request.path).args[0]\n                filterexpr=Q(schemes=obj) | Q(schemes=None)\n                kwargs['queryset'] = Grading.objects.filter(filterexpr).distinct()\n            except:\n                pass\n        return super(GradingSchemeAdmin, self).formfield_for_dbfield(db_field, **kwargs)", "response": "Offer only gradings that are not used by other schemes which means they are not at all."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring the report link for the LTI result record.", "response": "def store_report_link(backend, user, response, *args, **kwargs):\n    '''\n    Part of the Python Social Auth Pipeline.\n    Stores the result service URL reported by the LMS / LTI tool consumer so that we can use it later.\n    '''\n    if backend.name is 'lti':\n        assignment_pk = response.get('assignment_pk', None)\n        assignment = get_object_or_404(Assignment, pk=assignment_pk)\n        lti_result, created = LtiResult.objects.get_or_create(assignment=assignment, user=user)\n        if created:\n            logger.debug(\"LTI result record not found, creating it.\")\n        else:\n            logger.debug(\"LTI result record found, updating it.\")   # Expected, check LTI standard\n        lti_result.lis_result_sourcedid = response.get('lis_result_sourcedid')\n        lti_result.lis_outcome_service_url = response.get('lis_outcome_service_url')\n        lti_result.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_bam_tags():\n    '''builds the list of BAM tags to be added to output BAMs'''\n    #pylint: disable=unused-argument\n    def _combine_filters(fam, paired_align, align):\n        filters = [x.filter_value for x in [fam, align] if x and x.filter_value]\n        if filters:\n            return \";\".join(filters).replace('; ', ';')\n        return None\n\n    boolean_tag_value = {True:1}\n    tags = [\n        BamTag(\"X0\", \"Z\",\n               (\"filter (why the alignment was excluded)\"),\n               _combine_filters),\n        BamTag(\"X1\", \"Z\",\n               (\"leftmost~rightmost matched pair positions\"),\n               lambda fam, pair, align: pair.positions('{left}~{right}')),\n        BamTag(\"X2\", \"Z\",\n               (\"L~R CIGARs\"),\n               lambda fam, pair, align: pair.cigars('{left}~{right}')),\n        BamTag(\"X3\", \"i\",\n               \"unique identifier for this alignment family\",\n               lambda fam, pair, align: fam.umi_sequence),\n        BamTag(\"X4\", \"Z\",\n               (\"L~R UMT barcodes for this alignment family; because \"\n                \"of fuzzy matching the family UMT may be distinct \"\n                \"from the UMT of the original alignment\"),\n               lambda fam, pair, align: fam.umt('{left}~{right}')),\n        BamTag(\"X5\", \"i\",\n               \"family size (number of align pairs in this family)\",\n               lambda fam, pair, align: fam.included_pair_count),\n        BamTag(\"X6\", \"i\",\n               (\"presence of this tag signals that this alignment \"\n                \"would be the template for the consensus alignment\"),\n               lambda fam, pair, align: boolean_tag_value.get(fam.is_consensus_template(align), None))]\n    return tags", "response": "builds the list of BAM tags to be added to output BAMs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_livechat(request):\n    result = {}\n    livechat = LiveChat.chat_finder.get_current_live_chat()\n    if livechat:\n        result['live_chat'] = {}\n        result['live_chat']['current_live_chat'] = livechat\n        can_comment, reason_code = livechat.can_comment(request)\n        result['live_chat']['can_render_comment_form'] = can_comment\n        result['live_chat']['can_comment_code'] = reason_code\n    return result", "response": "Returns a dict that represents the current live chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the path to the file that should be uploaded to the server.", "response": "def upload_path(instance, filename):\n    '''\n        Sanitize the user-provided file name, add timestamp for uniqness.\n    '''\n\n    filename = filename.replace(\" \", \"_\")\n    filename = unicodedata.normalize('NFKD', filename).lower()\n    return os.path.join(str(timezone.now().date().isoformat()), filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the MD5 of the attachment.", "response": "def attachment_md5(self):\n        '''\n            Calculate the checksum of the file upload.\n            For binary files (e.g. PDFs), the MD5 of the file itself is used.\n\n            Archives are unpacked and the MD5 is generated from the sanitized textfiles\n            in the archive. This is done with some smartness:\n            - Whitespace and tabs are removed before comparison.\n            - For MD5, ordering is important, so we compute it on the sorted list of\n              file hashes.\n        '''\n        MAX_MD5_FILE_SIZE = 10000\n        md5_set = []\n\n        def md5_add_text(text):\n            try:\n                text = str(text, errors='ignore')\n                text = text.replace(' ', '').replace(\n                    '\\n', '').replace('\\t', '')\n                hexvalues = hashlib.md5(text.encode('utf-8')).hexdigest()\n                md5_set.append(hexvalues)\n            except Exception as e:\n                # not unicode decodable\n                pass\n\n        def md5_add_file(f):\n            try:\n                md5 = hashlib.md5()\n                for chunk in f.chunks():\n                    md5.update(chunk)\n                md5_set.append(md5.hexdigest())\n            except Exception:\n                pass\n\n        try:\n            if zipfile.is_zipfile(self.attachment.path):\n                zf = zipfile.ZipFile(self.attachment.path, 'r')\n                for zipinfo in zf.infolist():\n                    if zipinfo.file_size < MAX_MD5_FILE_SIZE:\n                        md5_add_text(zf.read(zipinfo))\n            elif tarfile.is_tarfile(self.attachment.path):\n                tf = tarfile.open(self.attachment.path, 'r')\n                for tarinfo in tf.getmembers():\n                    if tarinfo.isfile():\n                        if tarinfo.size < MAX_MD5_FILE_SIZE:\n                            md5_add_text(tf.extractfile(tarinfo).read())\n            else:\n                md5_add_file(self.attachment)\n        except Exception as e:\n            logger.warning(\n                \"Exception on archive MD5 computation, using file checksum: \" + str(e))\n\n        result = hashlib.md5(\n            ''.join(sorted(md5_set)).encode('utf-8')).hexdigest()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the attachment is an archive.", "response": "def is_archive(self):\n        '''\n            Determines if the attachment is an archive.\n        '''\n        try:\n            if zipfile.is_zipfile(self.attachment.path) or tarfile.is_tarfile(self.attachment.path):\n                return True\n        except Exception:\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef previews(self):\n        '''\n            Return preview on archive file / single file content as dictionary.\n            In order to avoid browser and web server trashing by the students,\n            there is a size limit for the single files shown.\n        '''\n        MAX_PREVIEW_SIZE = 1000000\n\n        def sanitize(bytes):\n            return bytes.decode('utf-8', 'ignore')\n\n        def is_code(fname):\n            code_endings = ['.c', '.cpp', 'Makefile',\n                            '.java', '.py', '.rb', '.js']\n            for ending in code_endings:\n                if fname.endswith(ending):\n                    return True\n            return False\n\n        result = []\n        try:\n            assert(self.attachment.path is not None)\n        except ValueError:\n            # The file behind the given path does not exist\n            return None\n        if zipfile.is_zipfile(self.attachment.path):\n            zf = zipfile.ZipFile(self.attachment.path, 'r')\n            for zipinfo in zf.infolist():\n                if zipinfo.file_size < MAX_PREVIEW_SIZE:\n                    try:\n                        result.append({'name': zipinfo.filename, 'is_code': is_code(\n                            zipinfo.filename), 'preview': sanitize(zf.read(zipinfo))})\n                    except NotImplementedError as e:\n                        result.append(\n                                {'name': zipinfo.filename, 'is_code': False, 'preview': '(NotImplementedError: %s)' % (e)})\n\n                    result.append({'name': zipinfo.filename, 'is_code': is_code(\n                        zipinfo.filename), 'preview': sanitize(zf.read(zipinfo))})\n                else:\n                    result.append(\n                        {'name': zipinfo.filename, 'is_code': False, 'preview': '(maximum size exceeded)'})\n        elif tarfile.is_tarfile(self.attachment.path):\n            tf = tarfile.open(self.attachment.path, 'r')\n            for tarinfo in tf.getmembers():\n                if tarinfo.isfile():\n                    if tarinfo.size < MAX_PREVIEW_SIZE:\n                        result.append({'name': tarinfo.name, 'is_code': is_code(\n                            tarinfo.name), 'preview': sanitize(tf.extractfile(tarinfo).read())})\n                    else:\n                        result.append(\n                            {'name': tarinfo.name, 'is_code': False, 'preview': '(maximum size exceeded)'})\n        else:\n            # single file\n            f = open(self.attachment.path, 'rb')\n            fname = f.name[f.name.rfind(os.sep) + 1:]\n            result = [{'name': fname, 'is_code': is_code(\n                fname), 'preview': sanitize(f.read())}, ]\n        return result", "response": "Return preview on archive file or single file content as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompletes loging process must return user instance", "response": "def auth_complete(self, *args, **kwargs):\n        \"\"\"Completes loging process, must return user instance\"\"\"\n        if SESSION_VAR not in self.strategy.request.session:\n            # This is the only protection layer when people\n            # go directly to the passthrough login view.\n            logger.warn(\"Auth data for passthrough provider not found in session. Raising 404.\")\n            raise PermissionDenied\n        auth_data = self.strategy.request.session[SESSION_VAR]\n        kwargs.update({'response': auth_data, 'backend': self})\n        return self.strategy.authenticate(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_details(self, response):\n        result = {\n            'id': response['id'],\n            'username': response.get('username', None),\n            'email': response.get('email', None),\n            'first_name': response.get('first_name', None),\n            'last_name': response.get('last_name', None)\n        }\n        if result['first_name'] and result['last_name']:\n            result['fullname'] = result['first_name'] + \\\n                ' ' + result['last_name']\n        return result", "response": "Complete with additional information from session as available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paired_reader_from_bamfile(args,\n                               log,\n                               usage_logger,\n                               annotated_writer):\n    '''Given a BAM file, return a generator that yields filtered, paired reads'''\n    total_aligns = pysamwrapper.total_align_count(args.input_bam)\n    bamfile_generator  = _bamfile_generator(args.input_bam)\n    return _paired_reader(args.umt_length,\n                          bamfile_generator,\n                          total_aligns,\n                          log,\n                          usage_logger,\n                          annotated_writer)", "response": "Given a BAM file return a generator that yields filtered paired reads"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns count of all mapped alignments in input BAM ( based on index", "response": "def total_align_count(bam_filepath):\n    '''Returns count of all mapped alignments in input BAM (based on index)'''\n    count = 0\n    for line in idxstats(bam_filepath):\n        if line:\n            chrom, _, mapped, unmapped = line.strip().split('\\t')\n            if chrom != '*':\n                count += int(mapped) + int(unmapped)\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive collection of random aligns returns alternating forward and reverse strands up to total_aligns will return smaller of forward/reverse collection if less than total_aligns", "response": "def _balanced_strand_gen(base_aligns, limit):\n    '''given collection of random {aligns}, returns alternating forward/reverse\n    strands up to {total_aligns}; will return smaller of forward/reverse\n    collection if less than total_aligns'''\n    predicate=lambda align: align.is_reverse\n    gen1, gen2 = itertools.tee((predicate(align), align) for align in base_aligns)\n    for_strand_gen = (align for pred, align in gen1 if not pred)\n    rev_strand_gen = (align for pred, align in gen2 if pred)\n    combined_gen = iter_zip(for_strand_gen, rev_strand_gen)\n    interleaved_gen = (align for for_rev_aligns in combined_gen for align in for_rev_aligns)\n    return itertools.islice(interleaved_gen, limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ipaddress():\n    '''\n    Determine our own IP adress.\n    This seems to be far more complicated than you would think:\n    '''\n    try:\n        import socket\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((\"gmail.com\", 80))\n        result = s.getsockname()[0]\n        s.close()\n        return result\n    except Exception:\n        return \"\"", "response": "Determine our own IP adress."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine some system information about the OpenCL device.", "response": "def opencl():\n    '''\n        Determine some system information about the installed OpenCL device.\n    '''\n    result = []\n    try:\n        import pyopencl as ocl\n        for plt in ocl.get_platforms():\n            result.append(\"Platform: \" + platform.name)\n            for device in plt.get_devices():\n                result.append(\"    Device:\" + device.name.strip())\n                infoset = [key for key in dir(device) if not key.startswith(\n                    \"__\") and key not in [\"extensions\", \"name\"]]\n                for attr in infoset:\n                    try:\n                        result.append(\"        %s: %s\" % (\n                            attr.strip(), getattr(device, attr).strip()))\n                    except Exception:\n                        pass\n        return \"\\n\".join(result)\n    except Exception:\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all_host_infos():\n    '''\n        Summarize all host information.\n    '''\n    output = []\n    output.append([\"Operating system\", os()])\n    output.append([\"CPUID information\", cpu()])\n    output.append([\"CC information\", compiler()])\n    output.append([\"JDK information\", from_cmd(\"java -version\")])\n    output.append([\"MPI information\", from_cmd(\"mpirun -version\")])\n    output.append([\"Scala information\", from_cmd(\"scala -version\")])\n    output.append([\"OpenCL headers\", from_cmd(\n        \"find /usr/include|grep opencl.h\")])\n    output.append([\"OpenCL libraries\", from_cmd(\n        \"find /usr/lib/ -iname '*opencl*'\")])\n    output.append([\"NVidia SMI\", from_cmd(\"nvidia-smi -q\")])\n    output.append([\"OpenCL Details\", opencl()])\n    return output", "response": "Summarize all host information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn one or more families whose end < rightmost boundary", "response": "def _completed_families(self, reference_name, rightmost_boundary):\n        '''returns one or more families whose end < rightmost boundary'''\n        in_progress = self._right_coords_in_progress[reference_name]\n        while len(in_progress):\n            right_coord = in_progress[0]\n            if right_coord < rightmost_boundary:\n                in_progress.pop(0)\n                left_families = self._coordinate_family.pop((reference_name, right_coord), {})\n                for family in sorted(left_families.values(),\n                                     key=lambda x:x[0].left.reference_start):\n                    family.sort(key=lambda x: x.query_name)\n                    self.pending_pair_count -= len(family)\n                    yield family\n            else:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a stream of paired aligns return a list of pairs that share same coordinates ( coordinate family.", "response": "def build_coordinate_families(self, paired_aligns):\n        '''Given a stream of paired aligns, return a list of pairs that share\n        same coordinates (coordinate family).  Flushes families in progress\n        when any of:\n        a) incoming right start > family end\n        b) incoming chrom != current chrom\n        c) incoming align stream is exhausted'''\n        rightmost_start = None\n        current_chrom = None\n        def _new_coordinate(pair):\n            return pair.right.reference_start != rightmost_start\n        def _new_chrom(pair):\n            return current_chrom != pair.right.reference_name\n\n        for pair in paired_aligns:\n            if rightmost_start is None:\n                rightmost_start = pair.right.reference_start\n                current_chrom = pair.right.reference_name\n            if _new_chrom(pair):\n                self._right_coords_in_progress[current_chrom].clear()\n                rightmost_start = None\n                current_chrom = None\n                for family in self._remaining_families():\n                    yield family\n            elif _new_coordinate(pair):\n                right = pair.right\n                for family in self._completed_families(right.reference_name,\n                                                       right.reference_start):\n                    yield family\n            self._add(pair)\n\n        for family in self._remaining_families():\n            yield family"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_usage_logger(self, log):\n        '''Return a method that logs progress/mem usage.'''\n        def _log_usage():\n            log.debug(\"{}mb peak memory\", peak_memory())\n            log.debug(\"{} pending alignment pairs; {} peak pairs\",\n                      self.pending_pair_count,\n                      self.pending_pair_peak_count)\n        return _log_usage", "response": "Return a method that logs progress and mem usage."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inform_student(submission, request, state):\n    '''\n    Create an email message for the student,\n    based on the given submission state.\n\n    Sending eMails on validation completion does\n    not work, since this may have been triggered\n    by the admin.\n    '''\n    details_url = request.build_absolute_uri(reverse('details', args=(submission.pk,)))\n\n    if state == submission.TEST_VALIDITY_FAILED:\n        subject = STUDENT_FAILED_SUB\n        message = STUDENT_FAILED_MSG\n        message = message % (submission.assignment,\n                             submission.assignment.course,\n                             details_url)\n\n    elif state == submission.CLOSED:\n        if submission.assignment.is_graded():\n            subject = STUDENT_GRADED_SUB\n            message = STUDENT_GRADED_MSG\n        else:\n            subject = STUDENT_PASSED_SUB\n            message = STUDENT_PASSED_MSG\n        message = message % (submission.assignment,\n                             submission.assignment.course,\n                             details_url)\n    else:\n        return\n\n    subject = \"[%s] %s\" % (submission.assignment.course, subject)\n    from_email = submission.assignment.course.owner.email\n    recipients = submission.authors.values_list(\n        'email', flat=True).distinct().order_by('email')\n    # send student email with BCC to course owner.\n    # TODO: This might be configurable later\n    # email = EmailMessage(subject, message, from_email, recipients,\n    # [self.assignment.course.owner.email])\n    email = EmailMessage(subject, message, from_email, recipients)\n    email.send(fail_silently=True)", "response": "Create an email message for the student based on the given submission state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graded_submissions(self):\n        '''\n            Queryset for the graded submissions, which are worth closing.\n        '''\n        qs = self._valid_submissions().filter(state__in=[Submission.GRADED])\n        return qs", "response": "Returns a Queryset of the graded submissions which are worth closing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef authors(self):\n        '''\n            Queryset for all distinct authors this course had so far. Important for statistics.\n            Note that this may be different from the list of people being registered for the course,\n            f.e. when they submit something and the leave the course.\n        '''\n        qs = self._valid_submissions().values_list('authors',flat=True).distinct()\n        return qs", "response": "Returns a Queryset for all distinct authors in the course."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_header(self, string):\n\n        taf_header_pattern = \"\"\"\n            ^\n            (TAF)?    # TAF header (at times missing or duplicate)\n            \\s*\n            (?P<type> (COR|AMD|AMD\\sCOR|COR\\sAMD|RTD)){0,1}\n\n            \\s* # There may or may not be space as COR/AMD/RTD is optional\n            (?P<icao_code> [A-Z]{4}) # Station ICAO code\n\n            \\s* # at some aerodromes does not appear\n            (?P<origin_date> \\d{0,2}) # at some aerodromes does not appear\n            (?P<origin_hours> \\d{0,2}) # at some aerodromes does not appear\n            (?P<origin_minutes> \\d{0,2}) # at some aerodromes does not appear\n            Z? # Zulu time (UTC, that is) # at some aerodromes does not appear\n\n            \\s*\n            (?P<valid_from_date> \\d{0,2})\n            (?P<valid_from_hours> \\d{0,2})\n            /\n            (?P<valid_till_date> \\d{0,2})\n            (?P<valid_till_hours> \\d{0,2})\n        \"\"\"\n\n        metar_header_pattern = \"\"\"\n            ^\n            (METAR)?    # METAR header (at times missing or duplicate)\n            \\s*\n            (?P<icao_code> [A-Z]{4}) # Station ICAO code\n\n            \\s* # at some aerodromes does not appear\n            (?P<origin_date> \\d{0,2}) # at some aerodromes does not appear\n            (?P<origin_hours> \\d{0,2}) # at some aerodromes does not appear\n            (?P<origin_minutes> \\d{0,2}) # at some aerodromes does not appear\n            Z? # Zulu time (UTC, that is) # at some aerodromes does not appear\n            \\s+\n            (?P<type> (COR){0,1}) # Corrected # TODO: Any other values possible?\n        \"\"\"\n\n        header_taf = re.match(taf_header_pattern, string, re.VERBOSE)\n        header_metar = re.match(metar_header_pattern, string, re.VERBOSE)\n\n        # The difference between a METAR and TAF header isn't that big\n        # so it's likely to get both regex to match. TAF is a bit more specific so if\n        # both regex match then we're most likely dealing with a TAF string.\n        if header_taf:\n            header_dict = header_taf.groupdict()\n            header_dict['form'] = 'taf'\n        elif header_metar:\n            header_dict = header_metar.groupdict()\n            header_dict['form'] = 'metar'\n        else:\n            raise MalformedTAF(\"No valid TAF/METAR header found\")\n\n        return header_dict", "response": "Extracts header part from TAF and METAR string and populates header dictionary with the values extracted from the report string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_groups(self, string):\n\n        taf_group_pattern = \"\"\"\n            (?:FM|(?:PROB(?:\\d{1,2})\\s*(?:TEMPO)?)|TEMPO|BECMG|[\\S\\s])[A-Z0-9\\+\\-/\\s$]+?(?=FM|PROB|TEMPO|BECMG|$)\n        \"\"\"\n\n        group_list = []\n\n        groups = re.findall(taf_group_pattern, string, re.VERBOSE)\n        if not groups:\n            raise MalformedTAF(\"No valid groups found\")\n\n        for group in groups:\n            group_list.append(group)\n\n        return(group_list)", "response": "Extracts weather groups from a TAF report string and populates the list of groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a profile for the user when missing.", "response": "def post_user_login(sender, request, user, **kwargs):\n    \"\"\"\n        Create a profile for the user, when missing.\n        Make sure that all neccessary user groups exist and have the right permissions.\n        We need that automatism for people not calling the configure tool,\n        admin rights for admins after the first login, and similar cases.\n    \"\"\"\n    logger.debug(\"Running post-processing for user login.\")\n    # Users created by social login or admins have no profile.\n    # We fix that during their first login.\n    try:\n        with transaction.atomic():\n            profile, created = UserProfile.objects.get_or_create(user=user)\n            if created:\n                logger.info(\"Created missing profile for user \" + str(user.pk))\n    except Exception as e:\n        logger.error(\"Error while creating user profile: \" + str(e))\n    check_permission_system()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submissionfile_post_save(sender, instance, signal, created, **kwargs):\n    '''\n        Update MD5 field for newly uploaded files.\n    '''\n    if created:\n        logger.debug(\"Running post-processing for new submission file.\")\n        instance.md5 = instance.attachment_md5()\n        instance.save()", "response": "Called when a submission file is saved."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submission_post_save(sender, instance, **kwargs):\n    ''' Several sanity checks after we got a valid submission object.'''\n    logger.debug(\"Running post-processing for submission\")\n    # Make the submitter an author\n    if instance.submitter not in instance.authors.all():\n        instance.authors.add(instance.submitter)\n        instance.save()\n    # Mark all existing submissions for this assignment by these authors as invalid.\n    # This fixes a race condition with parallel new submissions in multiple browser windows by the same user.\n    # Solving this as pre_save security exception does not work, since we have no instance with valid foreign keys to check there.\n    # Considering that this runs also on tutor correction backend activities, it also serves as kind-of cleanup functionality\n    # for multiplse submissions by the same students for the same assignment - however they got in here.\n    if instance.state == instance.get_initial_state():\n        for author in instance.authors.all():\n            same_author_subm = User.objects.get(pk=author.pk).authored.all().exclude(\n                pk=instance.pk).filter(assignment=instance.assignment)\n            for subm in same_author_subm:\n                subm.state = Submission.WITHDRAWN\n                subm.save()", "response": "Several sanity checks after we got a valid submission object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscheduling a message for delivery.", "response": "def schedule(self, recipients=None, sender=None, priority=None):\n        \"\"\"Schedules message for a delivery.\n        Puts message (and dispatches if any) data into DB.\n\n        :param list|None recipients: recipient (or a list) or None.\n            If `None` Dispatches should be created before send using `prepare_dispatches()`.\n        :param User|None sender: Django User model heir instance\n        :param int|None priority: number describing message priority\n        :return: a tuple with message model and a list of dispatch models.\n        :rtype: tuple\n        \"\"\"\n        if priority is None:\n            priority = self.priority\n\n        self._message_model, self._dispatch_models = Message.create(\n            self.get_alias(), self.get_context(), recipients=recipients, sender=sender, priority=priority\n        )\n        return self._message_model, self._dispatch_models"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_subscribers(cls, active_only=True):\n        subscribers_raw = Subscription.get_for_message_cls(cls.alias)\n        subscribers = []\n\n        for subscriber in subscribers_raw:\n            messenger_cls = subscriber.messenger_cls\n            address = subscriber.address\n            recipient = subscriber.recipient\n\n            # Do not send messages to inactive users.\n            if active_only and recipient:\n                if not getattr(recipient, 'is_active', False):\n                    continue\n\n            if address is None:\n                try:\n                    address = get_registered_messenger_object(messenger_cls).get_address(recipient)\n                except UnknownMessengerError:\n                    pass\n\n            if address and isinstance(address, string_types):\n                subscribers.append(Recipient(messenger_cls, recipient, address))\n\n        return subscribers", "response": "Returns a list of Recipient objects subscribed for this message type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a common pattern sitemessage URL.", "response": "def _get_url(cls, name, message_model, dispatch_model):\n        \"\"\"Returns a common pattern sitemessage URL.\n\n        :param str name: URL name\n        :param Message message_model:\n        :param Dispatch|None dispatch_model:\n        :return:\n        \"\"\"\n        global APP_URLS_ATTACHED\n\n        url = ''\n\n        if dispatch_model is None:\n            return url\n\n        if APP_URLS_ATTACHED != False:  # sic!\n\n            hashed = cls.get_dispatch_hash(dispatch_model.id, message_model.id)\n\n            try:\n                url = reverse(name, args=[message_model.id, dispatch_model.id, hashed])\n                url = '%s%s' % (get_site_url(), url)\n            except NoReverseMatch:\n                if APP_URLS_ATTACHED is None:\n                    APP_URLS_ATTACHED = False\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_unsubscribe_request(cls, request, message, dispatch, hash_is_valid, redirect_to):\n\n        if hash_is_valid:\n            Subscription.cancel(\n                dispatch.recipient_id or dispatch.address, cls.alias, dispatch.messenger\n            )\n            signal = sig_unsubscribe_success\n        else:\n            signal = sig_unsubscribe_failed\n\n        signal.send(cls, request=request, message=message, dispatch=dispatch)\n        return redirect(redirect_to)", "response": "Handles user subscription cancelling request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_mark_read_request(cls, request, message, dispatch, hash_is_valid, redirect_to):\n\n        if hash_is_valid:\n            dispatch.mark_read()\n            dispatch.save()\n            signal = sig_mark_read_success\n        else:\n            signal = sig_mark_read_failed\n\n        signal.send(cls, request=request, message=message, dispatch=dispatch)\n        return redirect(redirect_to)", "response": "Handles a request to mark a message as read."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a template path to compile a message.", "response": "def get_template(cls, message, messenger):\n        \"\"\"Get a template path to compile a message.\n\n        1. `tpl` field of message context;\n        2. `template` field of message class;\n        3. deduced from message, messenger data and `template_ext` message type field\n           (e.g. `sitemessage/messages/plain__smtp.txt` for `plain` message type).\n\n        :param Message message: Message model\n        :param MessengerBase messenger: a MessengerBase heir\n        :return: str\n        :rtype: str\n        \"\"\"\n        template = message.context.get('tpl', None)\n\n        if template:  # Template name is taken from message context.\n            return template\n\n        if cls.template is None:\n            cls.template = 'sitemessage/messages/%s__%s.%s' % (\n                cls.get_alias(), messenger.get_alias(), cls.template_ext\n            )\n        return cls.template"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling and returns a message text.", "response": "def compile(cls, message, messenger, dispatch=None):\n        \"\"\"Compiles and returns a message text.\n\n        Considers `use_tpl` field from message context to decide whether\n        template compilation is used.\n\n        Otherwise a SIMPLE_TEXT_ID field from message context is used as message contents.\n\n        :param Message message: model instance\n        :param MessengerBase messenger: MessengerBase heir instance\n        :param Dispatch dispatch: model instance to consider context from\n        :return: str\n        :rtype: str\n        \"\"\"\n        if message.context.get('use_tpl', False):\n            context = message.context\n            context.update({\n                'SITE_URL': get_site_url(),\n                'directive_unsubscribe': cls.get_unsubscribe_directive(message, dispatch),\n                'directive_mark_read': cls.get_mark_read_directive(message, dispatch),\n                'message_model': message,\n                'dispatch_model': dispatch\n            })\n            context = cls.get_template_context(context)\n            return render_to_string(cls.get_template(message, messenger), context)\n        return message.context[cls.SIMPLE_TEXT_ID]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the base_context dictionary with the contents of the string or dict str_or_dict.", "response": "def update_context(cls, base_context, str_or_dict, template_path=None):\n        \"\"\"Helper method to structure initial message context data.\n\n        NOTE: updates `base_context` inplace.\n\n        :param dict base_context: context dict to update\n        :param dict, str str_or_dict: text representing a message, or a dict to be placed into message context.\n        :param str template_path: template path to be used for message rendering\n        \"\"\"\n        if isinstance(str_or_dict, dict):\n            base_context.update(str_or_dict)\n            base_context['use_tpl'] = True\n        else:\n            base_context[cls.SIMPLE_TEXT_ID] = str_or_dict\n\n        if cls.SIMPLE_TEXT_ID in str_or_dict:\n            base_context['use_tpl'] = False\n\n        base_context['tpl'] = template_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_dispatches(cls, message, recipients=None):\n        return Dispatch.create(message, recipients or cls.get_subscribers())", "response": "Creates Dispatch models for a given message and returns them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of never expired page tokens indexed by page names.", "response": "def get_page_access_token(self, app_id, app_secret, user_token):\n        \"\"\"Returns a dictionary of never expired page token indexed by page names.\n\n        :param str app_id: Application ID\n        :param str app_secret: Application secret\n        :param str user_token: User short-lived token\n        :rtype: dict\n\n        \"\"\"\n        url_extend = (\n            self._url_base + '/oauth/access_token?grant_type=fb_exchange_token&'\n                             'client_id=%(app_id)s&client_secret=%(app_secret)s&fb_exchange_token=%(user_token)s')\n\n        response = self.lib.get(url_extend % {'app_id': app_id, 'app_secret': app_secret, 'user_token': user_token})\n\n        user_token_long_lived = response.text.split('=')[-1]\n\n        response = self.lib.get(self._url_versioned + '/me/accounts?access_token=%s' % user_token_long_lived)\n        json = response.json()\n\n        tokens = {item['name']: item['access_token'] for item in json['data'] if item.get('access_token')}\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nunpacking a single file into a single directory.", "response": "def unpack_if_needed(destination_path, fpath):\n    '''\n    fpath is the fully qualified path to a single file that\n    might be a ZIP / TGZ archive.\n\n    The function moves the file, or the content if it is an\n    archive, to the directory given by destination_path.\n\n    The function returns two values. The first one is a \n    directory name if:\n\n    - fpath is an archive.\n    - The archive contains only one this single directory with\n      arbitrary content.\n\n    Otherwise, it is zero.\n\n    This is helpful in catching the typical \"right-click to compress\"\n    cases for single ZIP files in Explorer / Finder.\n\n    The second return value is a boolean indicating if \n    fpath was an archive.\n\n    '''\n    single_dir = None\n    did_unpack = False\n\n    dircontent = os.listdir(destination_path)\n    logger.debug(\"Content of %s before unarchiving: %s\" %\n                 (destination_path, str(dircontent)))\n\n    # Perform un-archiving, in case\n    if zipfile.is_zipfile(fpath):\n        logger.debug(\"Detected ZIP file at %s, unpacking it.\" % (fpath))\n        did_unpack = True\n        with zipfile.ZipFile(fpath, \"r\") as zip:\n            infolist = zip.infolist()\n            directories = [\n                entry.filename for entry in infolist if entry.filename.endswith('/')]\n            logger.debug(\"List of directory entries: \" + str(directories))\n\n            # Consider this case: ['subdir1/', 'subdir1/subdir2/']\n            if len(directories) > 1:\n                redundant = []\n                for current in directories:\n                    starts_with_this = [\n                        el for el in directories if el.startswith(current)]\n                    if len(starts_with_this) == len(directories):\n                        # current is a partial directory name that is contained\n                        # in all others\n                        redundant.append(current)\n                logger.debug(\"Redundant directory entries: \" + str(redundant))\n                directories = [\n                    entry for entry in directories if entry not in redundant]\n                logger.debug(\n                    \"Updated list of directory entries: \" + str(directories))\n\n            files = [\n                entry.filename for entry in infolist if not entry.filename.endswith('/')]\n            logger.debug(\"List of files: \" + str(files))\n            if len(directories) == 1:\n                d = directories[0]\n                in_this_dir = [entry for entry in files if entry.startswith(d)]\n                if len(files) == len(in_this_dir):\n                    logger.debug(\"ZIP archive contains only one subdirectory\")\n                    single_dir = d\n            zip.extractall(destination_path)\n    elif tarfile.is_tarfile(fpath):\n        logger.debug(\"Detected TAR file at %s, unpacking it.\" % (fpath))\n        did_unpack = True\n        with tarfile.open(fpath) as tar:\n            infolist = tar.getmembers()\n            # A TGZ file of one subdirectory with arbitrary files\n            # has one infolist entry per directory and file\n            directories = [entry.name for entry in infolist if entry.isdir()]\n            files = [entry.name for entry in infolist if entry.isfile()]\n            logger.debug(directories)\n            logger.debug(files)\n            if len(directories) == 1:\n                d = directories[0]\n                in_this_dir = [entry for entry in files if entry.startswith(d)]\n                if len(files) == len(in_this_dir):\n                    logger.debug(\"TGZ archive contains only one subdirectory\")\n                    single_dir = d\n            tar.extractall(destination_path)\n    else:\n        if not fpath.startswith(destination_path):\n            logger.debug(\n                \"File at %s is a single non-archive file, copying it to %s\" % (fpath, destination_path))\n            shutil.copy(fpath, destination_path)\n\n    dircontent = os.listdir(destination_path)\n    logger.debug(\"Content of %s after unarchiving: %s\" %\n                 (destination_path, str(dircontent)))\n    return single_dir, did_unpack"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_working_dir(config, prefix):\n    '''\n        Create a fresh temporary directory, based on the fiven prefix.\n        Returns the new path.\n    '''\n    # Fetch base directory from executor configuration\n    basepath = config.get(\"Execution\", \"directory\")\n\n    if not prefix:\n        prefix = 'opensubmit'\n\n    finalpath = tempfile.mkdtemp(prefix=prefix + '_', dir=basepath)\n    if not finalpath.endswith(os.sep):\n        finalpath += os.sep\n    logger.debug(\"Created fresh working directory at {0}.\".format(finalpath))\n\n    return finalpath", "response": "Create a fresh temporary directory based on the fiven prefix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare the student submission and validation package for submission.", "response": "def prepare_working_directory(job, submission_path, validator_path):\n    '''\n    Based on two downloaded files in the working directory,\n    the student submission and the validation package,\n    the working directory is prepared.\n\n    We unpack student submission first, so that teacher files overwrite\n    them in case.\n\n    When the student submission is a single directory, we change the\n    working directory and go directly into it, before dealing with the\n    validator stuff.\n\n    If unrecoverable errors happen, such as an empty student archive,\n    a JobException is raised.\n    '''\n    # Safeguard for fail-fast in disk full scenarios on the executor\n\n    dusage = shutil.disk_usage(job.working_dir)\n    if dusage.free < 1024 * 1024 * 50:   # 50 MB\n        info_student = \"Internal error with the validator. Please contact your course responsible.\"\n        info_tutor = \"Error: Execution cancelled, less then 50MB of disk space free on the executor.\"\n        logger.error(info_tutor)\n        raise JobException(info_student=info_student, info_tutor=info_tutor)\n\n    submission_fname = os.path.basename(submission_path)\n    validator_fname = os.path.basename(validator_path)\n\n    # Un-archive student submission\n    single_dir, did_unpack = unpack_if_needed(job.working_dir, submission_path)\n    job.student_files = os.listdir(job.working_dir)\n    if did_unpack:\n        job.student_files.remove(submission_fname)\n\n    # Fail automatically on empty student submissions\n    if len(job.student_files) is 0:\n        info_student = \"Your compressed upload is empty - no files in there.\"\n        info_tutor = \"Submission archive file has no content.\"\n        logger.error(info_tutor)\n        raise JobException(info_student=info_student, info_tutor=info_tutor)\n\n    # Handle student archives containing a single directory with all data\n    if single_dir:\n        logger.warning(\n            \"The submission archive contains only one directory. Changing working directory.\")\n        # Set new working directory\n        job.working_dir = job.working_dir + single_dir + os.sep\n        # Move validator package there\n        shutil.move(validator_path, job.working_dir)\n        validator_path = job.working_dir + validator_fname\n        # Re-scan for list of student files\n        job.student_files = os.listdir(job.working_dir)\n\n    # The working directory now only contains the student data and the downloaded\n    # validator package.\n    # Update the file list accordingly.\n    job.student_files.remove(validator_fname)\n    logger.debug(\"Student files: {0}\".format(job.student_files))\n\n    # Unpack validator package\n    single_dir, did_unpack = unpack_if_needed(job.working_dir, validator_path)\n    if single_dir:\n        info_student = \"Internal error with the validator. Please contact your course responsible.\"\n        info_tutor = \"Error: Directories are not allowed in the validator archive.\"\n        logger.error(info_tutor)\n        raise JobException(info_student=info_student, info_tutor=info_tutor)\n\n    if not os.path.exists(job.validator_script_name):\n        if did_unpack:\n            # The download was an archive, but the validator was not inside.\n            # This is a failure of the tutor.\n            info_student = \"Internal error with the validator. Please contact your course responsible.\"\n            info_tutor = \"Error: Missing validator.py in the validator archive.\"\n            logger.error(info_tutor)\n            raise JobException(info_student=info_student,\n                               info_tutor=info_tutor)\n        else:\n            # The download is already the script, but has the wrong name\n            logger.warning(\"Renaming {0} to {1}.\".format(\n                validator_path, job.validator_script_name))\n            shutil.move(validator_path, job.validator_script_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns something like it would be done through Django s manage. py.", "response": "def django_admin(args):\n    '''\n        Run something like it would be done through Django's manage.py.\n    '''\n    from django.core.management import execute_from_command_line\n    from django.core.exceptions import ImproperlyConfigured\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"opensubmit.settings\")\n    try:\n        execute_from_command_line([sys.argv[0]] + args)\n    except ImproperlyConfigured as e:\n        print(str(e))\n        exit(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a valid Apache configuration file based on the given settings.", "response": "def apache_config(config, outputfile):\n    '''\n        Generate a valid Apache configuration file, based on the given settings.\n    '''\n    if os.path.exists(outputfile):\n        os.rename(outputfile, outputfile + \".old\")\n        print(\"Renamed existing Apache config file to \" + outputfile + \".old\")\n\n    from django.conf import settings\n    f = open(outputfile, 'w')\n    print(\"Generating Apache configuration in \" + outputfile)\n    subdir = (len(settings.HOST_DIR) > 0)\n    text = \"\"\"\n    # OpenSubmit Configuration for Apache 2.4\n    # These directives are expected to live in some <VirtualHost> block\n    \"\"\"\n    if subdir:\n        text += \"Alias /%s/static/ %s\\n\" % (settings.HOST_DIR,\n                                            settings.STATIC_ROOT)\n        text += \"    WSGIScriptAlias /%s %s/wsgi.py\\n\" % (\n            settings.HOST_DIR, settings.SCRIPT_ROOT)\n    else:\n        text += \"Alias /static/ %s\\n\" % (settings.STATIC_ROOT)\n        text += \"    WSGIScriptAlias / %s/wsgi.py\" % (settings.SCRIPT_ROOT)\n    text += \"\"\"\n    WSGIPassAuthorization On\n    <Directory {static_path}>\n         Require all granted\n    </Directory>\n    <Directory {install_path}>\n         <Files wsgi.py>\n              Require all granted\n         </Files>\n    </Directory>\n    \"\"\".format(static_path=settings.STATIC_ROOT, install_path=settings.SCRIPT_ROOT)\n\n    f.write(text)\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the directories for this path exist and creates them in case.", "response": "def check_path(file_path):\n    '''\n        Checks if the directories for this path exist, and creates them in case.\n    '''\n    directory = os.path.dirname(file_path)\n    if directory != '':\n        if not os.path.exists(directory):\n            os.makedirs(directory, 0o775)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the file exists and if it does not create it.", "response": "def check_file(filepath):\n    '''\n        - Checks if the parent directories for this path exist.\n        - Checks that the file exists.\n        - Donates the file to the web server user.\n\n        TODO: This is Debian / Ubuntu specific.\n    '''\n    check_path(filepath)\n    if not os.path.exists(filepath):\n        print(\"WARNING: File does not exist. Creating it: %s\" % filepath)\n        open(filepath, 'a').close()\n    try:\n        print(\"Setting access rights for %s for www-data user\" % (filepath))\n        uid = pwd.getpwnam(\"www-data\").pw_uid\n        gid = grp.getgrnam(\"www-data\").gr_gid\n        os.chown(filepath, uid, gid)\n        os.chmod(filepath, 0o660)  # rw-rw---\n    except Exception:\n        print(\"WARNING: Could not adjust file system permissions for %s. Make sure your web server can write into it.\" % filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the web application config file for consistency.", "response": "def check_web_config_consistency(config):\n    '''\n        Check the web application config file for consistency.\n    '''\n    login_conf_deps = {\n        'LOGIN_TWITTER_OAUTH_KEY': ['LOGIN_TWITTER_OAUTH_SECRET'],\n        'LOGIN_GOOGLE_OAUTH_KEY': ['LOGIN_GOOGLE_OAUTH_SECRET'],\n        'LOGIN_GITHUB_OAUTH_KEY': ['LOGIN_GITHUB_OAUTH_SECRET'],\n        'LOGIN_GITLAB_OAUTH_KEY': ['LOGIN_GITLAB_OAUTH_SECRET'],\n        'LOGIN_TWITTER_OAUTH_SECRET': ['LOGIN_TWITTER_OAUTH_KEY'],\n        'LOGIN_GOOGLE_OAUTH_SECRET': ['LOGIN_GOOGLE_OAUTH_KEY'],\n        'LOGIN_GITHUB_OAUTH_SECRET': ['LOGIN_GITHUB_OAUTH_KEY'],\n        'LOGIN_GITLAB_OAUTH_SECRET': ['LOGIN_GITLAB_OAUTH_KEY'],\n        'LOGIN_OIDC_ENDPOINT': ['LOGIN_OIDC_CLIENT_ID', 'LOGIN_OIDC_CLIENT_SECRET', 'LOGIN_OIDC_DESCRIPTION'],\n        'LOGIN_OIDC_CLIENT_ID': ['LOGIN_OIDC_ENDPOINT', 'LOGIN_OIDC_CLIENT_SECRET', 'LOGIN_OIDC_DESCRIPTION'],\n        'LOGIN_OIDC_CLIENT_SECRET': ['LOGIN_OIDC_ENDPOINT', 'LOGIN_OIDC_CLIENT_ID', 'LOGIN_OIDC_DESCRIPTION'],\n    }\n\n    print(\"Checking configuration of the OpenSubmit web application...\")\n    # Let Django's manage.py load the settings file, to see if this works in general\n    django_admin([\"check\"])\n    # Check configuration dependencies\n    for k, v in list(login_conf_deps.items()):\n        if config.get('login', k):\n            for needed in v:\n                if len(config.get('login', needed)) < 1:\n                    print(\n                        \"ERROR: You have enabled %s in settings.ini, but %s is not set.\" % (k, needed))\n                    return False\n    # Check media path\n    check_path(config.get('server', 'MEDIA_ROOT'))\n    # Prepare empty log file, in case the web server has no creation rights\n    log_file = config.get('server', 'LOG_FILE')\n    print(\"Preparing log file at \" + log_file)\n    check_file(log_file)\n    # If SQLite database, adjust file system permissions for the web server\n    if config.get('database', 'DATABASE_ENGINE') == 'sqlite3':\n        name = config.get('database', 'DATABASE_NAME')\n        if not os.path.isabs(name):\n            print(\"ERROR: Your SQLite database name must be an absolute path. The web server must have directory access permissions for this path.\")\n            return False\n        check_file(config.get('database', 'DATABASE_NAME'))\n    # everything ok\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_web_config(config_fname):\n    '''\n        Try to load the Django settings.\n        If this does not work, than settings file does not exist.\n\n        Returns:\n            Loaded configuration, or None.\n    '''\n    print(\"Looking for config file at {0} ...\".format(config_fname))\n    config = RawConfigParser()\n    try:\n        config.readfp(open(config_fname))\n        return config\n    except IOError:\n        print(\"ERROR: Seems like the config file does not exist. Please call 'opensubmit-web configcreate' first, or specify a location with the '-c' option.\")\n        return None", "response": "Try to load the Django settings file and return the loaded configuration or None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_url(url: str) -> str:\n    if url.startswith('/'):\n        url = url[1:]\n\n    if url.endswith('/'):\n        url = url[:-1]\n\n    return url", "response": "Normalizes a URL to be a URL with no leading and trailing slashes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unwrap(variable_parts: VariablePartsType):\n    curr_parts = variable_parts\n    var_any = []\n\n    while curr_parts:\n        curr_parts, (var_type, part) = curr_parts\n\n        if var_type == Routes._VAR_ANY_NODE:\n            var_any.append(part)\n            continue\n\n        if var_type == Routes._VAR_ANY_BREAK:\n            if var_any:\n                yield tuple(reversed(var_any))\n                var_any.clear()\n\n            var_any.append(part)\n            continue\n\n        if var_any:\n            yield tuple(reversed(var_any))\n            var_any.clear()\n            yield part\n            continue\n\n        yield part\n\n    if var_any:\n        yield tuple(reversed(var_any))", "response": "Yields the given URL parts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a regular URL into parts", "response": "def _deconstruct_url(self, url: str) -> List[str]:\n        \"\"\"\n        Split a regular URL into parts\n\n        :param url: A normalized URL\n        :return: Parts of the URL\n        :raises kua.routes.RouteError: \\\n        If the depth of the URL exceeds\\\n        the max depth of the deepest\\\n        registered pattern\n\n        :private:\n        \"\"\"\n        parts = url.split('/', self._max_depth + 1)\n\n        if depth_of(parts) > self._max_depth:\n            raise RouteError('No match')\n\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch URL parts to a registered pattern.", "response": "def _match(self, parts: Sequence[str]) -> RouteResolved:\n        \"\"\"\n        Match URL parts to a registered pattern.\n\n        This function is basically where all\\\n        the CPU-heavy work is done.\n\n        :param parts: URL parts\n        :return: Matched route\n        :raises kua.routes.RouteError: If there is no match\n\n        :private:\n        \"\"\"\n        route_match = None  # type: RouteResolved\n        route_variable_parts = tuple()  # type: VariablePartsType\n        # (route_partial, variable_parts, depth)\n        to_visit = [(self._routes, tuple(), 0)]  # type: List[Tuple[dict, tuple, int]]\n\n        # Walk through the graph,\n        # keep track of all possible\n        # matching branches and do\n        # backtracking if needed\n        while to_visit:\n            curr, curr_variable_parts, depth = to_visit.pop()\n\n            try:\n                part = parts[depth]\n            except IndexError:\n                if self._ROUTE_NODE in curr:\n                    route_match = curr[self._ROUTE_NODE]\n                    route_variable_parts = curr_variable_parts\n                    break\n                else:\n                    continue\n\n            if self._VAR_ANY_NODE in curr:\n                to_visit.append((\n                    {self._VAR_ANY_NODE: curr[self._VAR_ANY_NODE]},\n                    (curr_variable_parts,\n                     (self._VAR_ANY_NODE, part)),\n                    depth + 1))\n                to_visit.append((\n                    curr[self._VAR_ANY_NODE],\n                    (curr_variable_parts,\n                     (self._VAR_ANY_BREAK, part)),\n                    depth + 1))\n\n            if self._VAR_NODE in curr:\n                to_visit.append((\n                    curr[self._VAR_NODE],\n                    (curr_variable_parts,\n                     (self._VAR_NODE, part)),\n                    depth + 1))\n\n            if part in curr:\n                to_visit.append((\n                    curr[part],\n                    curr_variable_parts,\n                    depth + 1))\n\n        if not route_match:\n            raise RouteError('No match')\n\n        return RouteResolved(\n            params=make_params(\n                key_parts=route_match.key_parts,\n                variable_parts=route_variable_parts),\n            anything=route_match.anything)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match(self, url: str) -> RouteResolved:\n        url = normalize_url(url)\n        parts = self._deconstruct_url(url)\n        return self._match(parts)", "response": "Match a URL to a registered pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a URL pattern into the routes for later matching.", "response": "def add(self, url: str, anything: Any) -> None:\n        \"\"\"\n        Register a URL pattern into\\\n        the routes for later matching.\n\n        It's possible to attach any kind of\\\n        object to the pattern for later\\\n        retrieving. A dict with methods and callbacks,\\\n        for example. Anything really.\n\n        Registration order does not matter.\\\n        Adding a URL first or last makes no difference.\n\n        :param url: URL\n        :param anything: Literally anything.\n        \"\"\"\n        url = normalize_url(url)\n        parts = url.split('/')\n        curr_partial_routes = self._routes\n        curr_key_parts = []\n\n        for part in parts:\n            if part.startswith(':*'):\n                curr_key_parts.append(part[2:])\n                part = self._VAR_ANY_NODE\n                self._max_depth = self._max_depth_custom\n\n            elif part.startswith(':'):\n                curr_key_parts.append(part[1:])\n                part = self._VAR_NODE\n\n            curr_partial_routes = (curr_partial_routes\n                                   .setdefault(part, {}))\n\n        curr_partial_routes[self._ROUTE_NODE] = _Route(\n            key_parts=curr_key_parts,\n            anything=anything)\n\n        self._max_depth = max(self._max_depth, depth_of(parts))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_site_url():\n    site_url = getattr(_THREAD_LOCAL, _THREAD_SITE_URL, None)\n    if site_url is None:\n        site_url = SITE_URL or get_site_url_()\n        setattr(_THREAD_LOCAL, _THREAD_SITE_URL, site_url)\n\n    return site_url", "response": "Returns a URL for current site."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_message_type_for_app(app_name, default_message_type_alias):\n    message_type = default_message_type_alias\n    try:\n        message_type = _MESSAGES_FOR_APPS[app_name][message_type]\n    except KeyError:\n        pass\n    return get_registered_message_type(message_type)", "response": "Returns a registered message type object for a given application."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recipients(messenger, addresses):\n    if isinstance(messenger, six.string_types):\n        messenger = get_registered_messenger_object(messenger)\n    return messenger._structure_recipients_data(addresses)", "response": "Structures recipients data.\n\n    :param str|unicode, MessageBase messenger: MessengerBase heir\n    :param list[str|unicode]|str|unicode addresses: recipients addresses or Django User\n        model heir instances (NOTE: if supported by a messenger)\n\n    :return: list of Recipient\n    :rtype: list[Recipient]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upcoming_live_chat(self):\n        chat = None\n        now = datetime.now()\n\n\n        lcqs = self.get_query_set()\n        lcqs = lcqs.filter(\n            chat_ends_at__gte=now).order_by('-chat_starts_at')\n        try:\n            if settings.LIVECHAT_PRIMARY_CATEGORY:\n                lcqs = lcqs.filter(\n                    primary_category__slug=settings.LIVECHAT_PRIMARY_CATEGORY)\n        except AttributeError:\n            pass\n        try:\n            if settings.LIVECHAT_CATEGORIES:\n                lcqs = lcqs.filter(\n                    categories__slug__in=settings.LIVECHAT_CATEGORIES)\n        except AttributeError:\n            pass\n        if lcqs.exists():\n            chat = lcqs.latest('chat_starts_at')\n\n        return chat", "response": "Find any upcoming live chat that is currently on the home page or the current live chat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_current_live_chat(self):\n        now = datetime.now()\n        chat = self.upcoming_live_chat()\n        if chat and chat.is_in_progress():\n            return chat\n        return None", "response": "Check if there is a live chat on the go and if so return the current chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if there is a live chat that ended in the last 3 days and if so return it.", "response": "def get_last_live_chat(self):\n        \"\"\" Check if there is a live chat that ended in the last 3 days, and\n            return it. We will display a link to it on the articles page.\n        \"\"\"\n        now = datetime.now()\n\n        lcqs = self.get_query_set()\n        lcqs = lcqs.filter(\n            chat_ends_at__lte=now,\n            ).order_by('-chat_ends_at')\n        for itm in lcqs:\n            if itm.chat_ends_at + timedelta(days=3) > now:\n                return itm\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the comments that have been submitted for the chat", "response": "def comment_set(self):\n        \"\"\" Get the comments that have been submitted for the chat\n        \"\"\"\n        ct = ContentType.objects.get_for_model(self.__class__)\n        qs = Comment.objects.filter(\n            content_type=ct,\n            object_pk=self.pk)\n        qs = qs.exclude(is_removed=True)\n        qs = qs.order_by('-submit_date')\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_dispatches(filter_kwargs):\n\n    dispatches = Dispatch.objects.prefetch_related('message').filter(\n        **filter_kwargs\n    ).order_by('-message__time_created')\n\n    return list(dispatches)", "response": "Simplified version. Not distributed friendly."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndistributing friendly version using select for update.", "response": "def _get_dispatches_for_update(filter_kwargs):\n    \"\"\"Distributed friendly version using ``select for update``.\"\"\"\n\n    dispatches = Dispatch.objects.prefetch_related('message').filter(\n        **filter_kwargs\n\n    ).select_for_update(\n        **GET_DISPATCHES_ARGS[1]\n\n    ).order_by('-message__time_created')\n\n    try:\n        dispatches = list(dispatches)\n\n    except NotSupportedError:\n        return None\n\n    except DatabaseError:  # Probably locked. That's fine.\n        return []\n\n    return dispatches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qs_valid(qs):\n        '''\n            A filtering of the given Submission queryset for all submissions that were successfully validated. This includes the following cases:\n\n            - The submission was submitted and there are no tests.\n            - The submission was successfully validity-tested, regardless of the full test status (not existent / failed / success).\n            - The submission was graded or the grading was already started.\n            - The submission was closed.\n\n            The idea is to get all submissions that were a valid solution, regardless of the point in time where you check the list.\n        '''\n        return qs.filter(state__in=[Submission.SUBMITTED, Submission.SUBMITTED_TESTED, Submission.TEST_FULL_FAILED, Submission.GRADING_IN_PROGRESS, Submission.GRADED, Submission.CLOSED, Submission.CLOSED_TEST_FULL_PENDING])", "response": "A filter of the given Submission queryset for all submissions that were successfully validated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grading_value_text(self):\n        '''\n        A rendering of the grading that is an answer to the question\n        \"What is the grade?\".\n        '''\n        if self.assignment.is_graded():\n            if self.is_grading_finished():\n                return str(self.grading)\n            else:\n                return str('pending')\n        else:\n            if self.is_grading_finished():\n                return str('done')\n            else:\n                return str('not done')", "response": "A rendering of the grading that is an answer to the question\n            \"What is the grade?"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grading_means_passed(self):\n        '''\n        Information if the given grading means passed.\n        Non-graded assignments are always passed.\n        '''\n        if self.assignment.is_graded():\n            if self.grading and self.grading.means_passed:\n                return True\n            else:\n                return False\n        else:\n            return True", "response": "Information if the given grading means passed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines whether the submission can be modified.", "response": "def can_modify(self, user=None):\n        \"\"\"Determines whether the submission can be modified.\n        Returns a boolean value.\n        The 'user' parameter is optional and additionally checks whether\n        the given user is authorized to perform these actions.\n\n        This function checks the submission states and assignment deadlines.\"\"\"\n\n        # The user must be authorized to commit these actions.\n        if user and not self.user_can_modify(user):\n            #self.log('DEBUG', \"Submission cannot be modified, user is not an authorized user ({!r} not in {!r})\", user, self.authorized_users)\n            return False\n\n        # Modification of submissions, that are withdrawn, graded or currently being graded, is prohibited.\n        if self.state in [self.WITHDRAWN, self.GRADED, self.GRADING_IN_PROGRESS, ]:\n            #self.log('DEBUG', \"Submission cannot be modified, is in state '{}'\", self.state)\n            return False\n\n        # Modification of closed submissions is prohibited.\n        if self.is_closed():\n            if self.assignment.is_graded():\n                # There is a grading procedure, so taking it back would invalidate the tutors work\n                #self.log('DEBUG', \"Submission cannot be modified, it is closed and graded\")\n                return False\n            else:\n                #self.log('DEBUG', \"Closed submission can be modified, since it has no grading scheme.\")\n                return True\n\n        # Submissions, that are executed right now, cannot be modified\n        if self.state in [self.TEST_VALIDITY_PENDING, self.TEST_FULL_PENDING]:\n            if not self.file_upload:\n                self.log(\n                    'CRITICAL', \"Submission is in invalid state! State is '{}', but there is no file uploaded!\", self.state)\n                raise AssertionError()\n                return False\n            if self.file_upload.is_executed():\n                # The above call informs that the uploaded file is being executed, or execution has been completed.\n                # Since the current state is 'PENDING', the execution cannot yet be completed.\n                # Thus, the submitted file is being executed right now.\n                return False\n\n        # Submissions must belong to an assignment.\n        if not self.assignment:\n            self.log('CRITICAL', \"Submission does not belong to an assignment!\")\n            raise AssertionError()\n\n        # Submissions, that belong to an assignment where the hard deadline has passed,\n        # cannot be modified.\n        if self.assignment.hard_deadline and timezone.now() > self.assignment.hard_deadline:\n            #self.log('DEBUG', \"Submission cannot be modified - assignment's hard deadline has passed (hard deadline is: {})\", self.assignment.hard_deadline)\n            return False\n\n        # The soft deadline has no effect (yet).\n        if self.assignment.soft_deadline:\n            if timezone.now() > self.assignment.soft_deadline:\n                # The soft deadline has passed\n                pass  # do nothing.\n\n        #self.log('DEBUG', \"Submission can be modified.\")\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines whether a submission can be re - uploaded.", "response": "def can_reupload(self, user=None):\n        \"\"\"Determines whether a submission can be re-uploaded.\n        Returns a boolean value.\n\n        Requires: can_modify.\n\n        Re-uploads are allowed only when test executions have failed.\"\"\"\n\n        # Re-uploads are allowed only when test executions have failed.\n        if self.state not in (self.TEST_VALIDITY_FAILED, self.TEST_FULL_FAILED):\n            return False\n\n        # It must be allowed to modify the submission.\n        if not self.can_modify(user=user):\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the initial state of the submission.", "response": "def get_initial_state(self):\n        '''\n            Return first state for this submission after upload,\n            which depends on the kind of assignment.\n        '''\n        if not self.assignment.attachment_is_tested():\n            return Submission.SUBMITTED\n        else:\n            if self.assignment.attachment_test_validity:\n                return Submission.TEST_VALIDITY_PENDING\n            elif self.assignment.attachment_test_full:\n                return Submission.TEST_FULL_PENDING"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info_file(self, delete=True):\n        '''\n            Prepares an open temporary file with information about the submission.\n            Closing it will delete it, which must be considered by the caller.\n            This file is not readable, since the tempfile library wants either readable or writable files.\n        '''\n        info = tempfile.NamedTemporaryFile(\n            mode='wt', encoding='utf-8', delete=delete)\n        info.write(\"Submission ID:\\t%u\\n\" % self.pk)\n        info.write(\"Submitter:\\t%s (%u)\\n\" %\n                   (self.submitter.get_full_name(), self.submitter.pk))\n        info.write(\"Authors:\\n\")\n        for auth in self.authors.all():\n            info.write(\"\\t%s (%u)\\n\" % (auth.get_full_name(), auth.pk))\n        info.write(\"\\n\")\n        info.write(\"Creation:\\t%s\\n\" % str(self.created))\n        info.write(\"Last modification:\\t%s\\n\" % str(self.modified))\n        info.write(\"Status:\\t%s\\n\" % self.state_for_students())\n        if self.grading:\n            info.write(\"Grading:\\t%s\\n\" % str(self.grading))\n        if self.notes:\n            notes = self.notes\n            info.write(\"Author notes:\\n-------------\\n%s\\n\\n\" % notes)\n        if self.grading_notes:\n            notes = self.grading_notes\n            info.write(\"Grading notes:\\n--------------\\n%s\\n\\n\" % notes)\n        info.flush()    # no closing here, because it disappears then\n        return info", "response": "Returns a tempfile. NamedTemporaryFile object with information about the submission."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy the currently valid file upload into the given directory.", "response": "def copy_file_upload(self, targetdir):\n        '''\n            Copies the currently valid file upload into the given directory.\n            If possible, the content is un-archived in the target directory.\n        '''\n        assert(self.file_upload)\n        # unpack student data to temporary directory\n        # os.chroot is not working with tarfile support\n        tempdir = tempfile.mkdtemp()\n        try:\n            if zipfile.is_zipfile(self.file_upload.absolute_path()):\n                f = zipfile.ZipFile(self.file_upload.absolute_path(), 'r')\n                f.extractall(targetdir)\n            elif tarfile.is_tarfile(self.file_upload.absolute_path()):\n                tar = tarfile.open(self.file_upload.absolute_path())\n                tar.extractall(targetdir)\n                tar.close()\n            else:\n                # unpacking not possible, just copy it\n                shutil.copyfile(self.file_upload.absolute_path(),\n                                targetdir + \"/\" + self.file_upload.basename())\n        except IOError:\n            logger.error(\"I/O exception while accessing %s.\" %\n                         (self.file_upload.absolute_path()))\n            pass\n        except (UnicodeEncodeError, NotImplementedError)  as e:\n            # unpacking not possible, just copy it\n            shutil.copyfile(self.file_upload.absolute_path(),\n                            targetdir + \"/\" + self.file_upload.basename())\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns unique chat IDs from start command messages sent to our bot by users.", "response": "def get_chat_ids(self):\n        \"\"\"Returns unique chat IDs from `/start` command messages sent to our bot by users.\n        Those chat IDs can be used to send messages to chats.\n\n        :rtype: list\n        \"\"\"\n        updates = self.get_updates()\n        chat_ids = []\n        if updates:\n            for update in updates:\n                message = update['message']\n                if message['text'] == '/start':\n                    chat_ids.append(message['chat']['id'])\n        return list(set(chat_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a command to API.", "response": "def _send_command(self, method_name, data=None):\n        \"\"\"Sends a command to API.\n\n        :param str method_name:\n        :param dict data:\n        :return:\n        \"\"\"\n        try:\n            response = self.lib.post(self._tpl_url % {'token': self.auth_token, 'method': method_name}, data=data)\n            json = response.json()\n\n            if not json['ok']:\n                raise TelegramMessengerException(json['description'])\n\n            return json['result']\n\n        except self.lib.exceptions.RequestException as e:\n            raise TelegramMessengerException(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_queryset(self, request):\n        ''' Restrict the listed submission files for the current user.'''\n        qs = super(SubmissionFileAdmin, self).get_queryset(request)\n        if request.user.is_superuser:\n            return qs\n        else:\n            return qs.filter(Q(submissions__assignment__course__tutors__pk=request.user.pk) | Q(submissions__assignment__course__owner=request.user)).distinct()", "response": "Restrict the listed submission files for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering the built - in message types.", "response": "def register_builtin_message_types():\n    \"\"\"Registers the built-in message types.\"\"\"\n    from .plain import PlainTextMessage\n    from .email import EmailTextMessage, EmailHtmlMessage\n    register_message_types(PlainTextMessage, EmailTextMessage, EmailHtmlMessage)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlinking to performance data and duplicate overview.", "response": "def view_links(obj):\n    ''' Link to performance data and duplicate overview.'''\n    result=format_html('')\n    result+=format_html('<a href=\"%s\" style=\"white-space: nowrap\">Show duplicates</a><br/>'%reverse('duplicates', args=(obj.pk,)))\n    result+=format_html('<a href=\"%s\" style=\"white-space: nowrap\">Show submissions</a><br/>'%obj.grading_url())\n    result+=format_html('<a href=\"%s\" style=\"white-space: nowrap\">Download submissions</a>'%reverse('assarchive', args=(obj.pk,)))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean(self):\n        '''\n            Check if such an assignment configuration makes sense, and reject it otherwise.\n            This mainly relates to interdependencies between the different fields, since\n            single field constraints are already clatified by the Django model configuration.\n        '''\n        super(AssignmentAdminForm, self).clean()\n        d = defaultdict(lambda: False)\n        d.update(self.cleaned_data)\n        # Having validation or full test enabled demands file upload\n        if d['attachment_test_validity'] and not d['has_attachment']:\n            raise ValidationError('You cannot have a validation script without allowing file upload.')\n        if d['attachment_test_full'] and not d['has_attachment']:\n            raise ValidationError('You cannot have a full test script without allowing file upload.')\n        # Having validation or full test enabled demands a test machine\n        if d['attachment_test_validity'] and 'test_machines' in d and not len(d['test_machines'])>0:\n            raise ValidationError('You cannot have a validation script without specifying test machines.')\n        if d['attachment_test_full'] and 'test_machines' in d and not len(d['test_machines'])>0:\n            raise ValidationError('You cannot have a full test script without specifying test machines.')\n        if d['download'] and d['description']:\n            raise ValidationError('You can only have a description link OR a description file.')\n        if not d['download'] and not d['description']:\n            raise ValidationError('You need a description link OR a description file.')\n        # Having test machines demands compilation or validation scripts\n        if 'test_machines' in d and len(d['test_machines'])>0               \\\n            and not 'attachment_test_validity' in d  \\\n            and not 'attachment_test_full' in d:\n            raise ValidationError('For using test machines, you need to enable validation or full test.')", "response": "Check if such an assignment configuration makes sense and reject it otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestricts the listed assignments for the current user.", "response": "def get_queryset(self, request):\n        ''' Restrict the listed assignments for the current user.'''\n        qs = super(AssignmentAdmin, self).get_queryset(request)\n        if not request.user.is_superuser:\n            qs = qs.filter(course__active=True).filter(Q(course__tutors__pk=request.user.pk) | Q(course__owner=request.user)).distinct()\n        return qs.order_by('title')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth_complete(self, *args, **kwargs):\n        if self.ENV_USERNAME in os.environ:\n            response = os.environ\n        elif type(self.strategy).__name__ == \"DjangoStrategy\" and self.ENV_USERNAME in self.strategy.request.META:\n            # Looks like the Django strategy. In this case, it might by mod_wsgi, which stores\n            # authentication environment variables in request.META\n            response = self.strategy.request.META\n        else:\n            raise AuthMissingParameter(self, \"%s, found only: %s\"%(self.ENV_USERNAME, str(os.environ)))\n        kwargs.update({'response': response, 'backend': self})\n        return self.strategy.authenticate(*args, **kwargs)", "response": "Completes loging process must return user instance"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_details(self, response):\n        result = {\n            'username': response[self.ENV_USERNAME],\n            'email': response.get(self.ENV_EMAIL, None),\n            'first_name': response.get(self.ENV_FIRST_NAME, None),\n            'last_name': response.get(self.ENV_LAST_NAME, None)\n        }\n        if result['first_name'] and result['last_name']:\n            result['fullname']=result['first_name']+' '+result['last_name']\n        logger.debug(\"Returning user details: \"+str(result))\n        return result", "response": "Complete with additional information from environment as available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef file_link(self, instance):\n        '''\n            Renders the link to the student upload file.\n        '''\n        sfile = instance.file_upload\n        if not sfile:\n            return mark_safe('No file submitted by student.')\n        else:\n            return mark_safe('<a href=\"%s\">%s</a><br/>(<a href=\"%s\" target=\"_new\">Preview</a>)' % (sfile.get_absolute_url(), sfile.basename(), sfile.get_preview_url()))", "response": "Renders the link to the file submitted by student."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestricts the listed submission for the current user.", "response": "def get_queryset(self, request):\n        ''' Restrict the listed submission for the current user.'''\n        qs = super(SubmissionAdmin, self).get_queryset(request)\n        if request.user.is_superuser:\n            return qs\n        else:\n            return qs.filter(Q(assignment__course__tutors__pk=request.user.pk) | Q(assignment__course__owner=request.user)).distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noffering grading choices from the assignment definition as potential form field values for 'grading'. When no object is given in the form, the this is a new manual submission", "response": "def formfield_for_dbfield(self, db_field, **kwargs):\n        ''' Offer grading choices from the assignment definition as potential form\n            field values for 'grading'.\n            When no object is given in the form, the this is a new manual submission\n        '''\n        if db_field.name == \"grading\":\n            submurl = kwargs['request'].path\n            try:\n                # Does not work on new submission action by admin or with a change of URLs. The former is expectable.\n                submid = [int(s) for s in submurl.split('/') if s.isdigit()][0]\n                kwargs[\"queryset\"] = Submission.objects.get(\n                    pk=submid).assignment.gradingScheme.gradings\n            except:\n                kwargs[\"queryset\"] = Grading.objects.none()\n        return super(SubmissionAdmin, self).formfield_for_dbfield(db_field, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_model(self, request, obj, form, change):\n        ''' Our custom addition to the view\n            adds an easy radio button choice for the new state. This is meant to be for tutors.\n            We need to peel this choice from the form data and set the state accordingly.\n            The radio buttons have no default, so that we can keep the existing state\n            if the user makes no explicit choice.\n            Everything else can be managed as prepared by the framework.\n        '''\n        if 'newstate' in request.POST:\n            if request.POST['newstate'] == 'finished':\n                obj.state = Submission.GRADED\n            elif request.POST['newstate'] == 'unfinished':\n                obj.state = Submission.GRADING_IN_PROGRESS\n        obj.save()", "response": "This method is called when a user changes the state of a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets all marked submissions to grading not finished.", "response": "def setGradingNotFinishedStateAction(self, request, queryset):\n        '''\n            Set all marked submissions to \"grading not finished\".\n            This is intended to support grading corrections on a larger scale.\n        '''\n        for subm in queryset:\n            subm.state = Submission.GRADING_IN_PROGRESS\n            subm.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets all marked submissions to grading finished.", "response": "def setGradingFinishedStateAction(self, request, queryset):\n        '''\n            Set all marked submissions to \"grading finished\".\n            This is intended to support grading corrections on a larger scale.\n        '''\n        for subm in queryset:\n            subm.state = Submission.GRADED\n            subm.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef closeAndNotifyAction(self, request, queryset):\n        ''' Close all submissions were the tutor sayed that the grading is finished,\n            and inform the student. CLosing only graded submissions is a safeguard,\n            since backend users tend to checkbox-mark all submissions without thinking.\n        '''\n        mails = []\n        qs = queryset.filter(Q(state=Submission.GRADED))\n        for subm in qs:\n            subm.inform_student(request, Submission.CLOSED)\n            mails.append(str(subm.pk))\n        # works in bulk because inform_student never fails\n        qs.update(state=Submission.CLOSED)\n        if len(mails) == 0:\n            self.message_user(request, \"Nothing closed, no mails sent.\")\n        else:\n            self.message_user(\n                request, \"Mail sent for submissions: \" + \",\".join(mails))", "response": "Close all submissions were the tutor sayed that the grading is finished and inform the student."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef downloadArchiveAction(self, request, queryset):\n        '''\n        Download selected submissions as archive, for targeted correction.\n        '''\n        output = io.BytesIO()\n        z = zipfile.ZipFile(output, 'w')\n\n        for sub in queryset:\n            sub.add_to_zipfile(z)\n\n        z.close()\n        # go back to start in ZIP file so that Django can deliver it\n        output.seek(0)\n        response = HttpResponse(\n            output, content_type=\"application/x-zip-compressed\")\n        response['Content-Disposition'] = 'attachment; filename=submissions.zip'\n        return response", "response": "Download selected submissions as archive for targeted correction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef directory_name_with_course(self):\n        ''' The assignment name in a format that is suitable for a directory name.  '''\n        coursename = self.course.directory_name()\n        assignmentname = self.title.replace(\" \", \"_\").replace(\"\\\\\", \"_\").replace(\",\",\"\").lower()\n        return coursename + os.sep + assignmentname", "response": "The assignment name in a format that is suitable for a directory name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the teacher backend link to the filtered list of gradable submissions for this assignment.", "response": "def grading_url(self):\n        '''\n            Determines the teacher backend link to the filtered list of gradable submissions for this assignment.\n        '''\n        grading_url=\"%s?coursefilter=%u&assignmentfilter=%u&statefilter=tobegraded\"%(\n                            reverse('teacher:opensubmit_submission_changelist'),\n                            self.course.pk, self.pk\n                        )\n        return grading_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_perf_results(self):\n        '''\n            Figure out if any submission for this assignment has performance data being available.\n        '''\n        num_results = SubmissionTestResult.objects.filter(perf_data__isnull=False).filter(submission_file__submissions__assignment=self).count()\n        return num_results != 0", "response": "Determines if any submission for this assignment has performance data being available."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url(self, request):\n        '''\n            Return absolute URL for assignment description.\n        '''\n        if self.pk:\n            if self.has_description():\n                return request.build_absolute_uri(reverse('assignment_description_file', args=[self.pk]))\n            else:\n                return self.download\n        else:\n            return None", "response": "Return absolute URL for assignment description."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the user can create a submission for this assignment.", "response": "def can_create_submission(self, user=None):\n        '''\n            Central access control for submitting things related to assignments.\n        '''\n        if user:\n            # Super users, course owners and tutors should be able to test their validations\n            # before the submission is officially possible.\n            # They should also be able to submit after the deadline.\n            if user.is_superuser or user is self.course.owner or self.course.tutors.filter(pk=user.pk).exists():\n                return True\n            if self.course not in user.profile.user_courses():\n                # The user is not enrolled in this assignment's course.\n                logger.debug('Submission not possible, user not enrolled in the course.')\n                return False\n\n            if user.authored.filter(assignment=self).exclude(state=Submission.WITHDRAWN).count() > 0:\n                # User already has a valid submission for this assignment.\n                logger.debug('Submission not possible, user already has one for this assignment.')\n                return False\n\n        if self.hard_deadline and self.hard_deadline < timezone.now():\n            # Hard deadline has been reached.\n            logger.debug('Submission not possible, hard deadline passed.')\n            return False\n\n        if self.publish_at > timezone.now() and not user.profile.can_see_future():\n            # The assignment has not yet been published.\n            logger.debug('Submission not possible, assignment has not yet been published.')\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for duplicate submissions for this assignment. This includes the search in other course whether inactive or not. This includes the search in other course whether inactive or not. This includes the search in other course whether inactive or not. This includes the search in other course whether inactive or not.", "response": "def duplicate_files(self):\n        '''\n        Search for duplicates of submission file uploads for this assignment.\n        This includes the search in other course, whether inactive or not.\n        Returns a list of lists, where each latter is a set of duplicate submissions\n        with at least on of them for this assignment\n        '''\n        result=list()\n        files = SubmissionFile.valid_ones.order_by('md5')\n\n        for key, dup_group in groupby(files, lambda f: f.md5):\n            file_list=[entry for entry in dup_group]\n            if len(file_list)>1:\n                for entry in file_list:\n                    if entry.submissions.filter(assignment=self).count()>0:\n                        result.append([key, file_list])\n                        break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes the validate method in the test script belonging to this job.", "response": "def _run_validate(self):\n        '''\n        Execute the validate() method in the test script belonging to this job.\n        '''\n        assert(os.path.exists(self.validator_script_name))\n        old_path = sys.path\n        sys.path = [self.working_dir] + old_path\n        # logger.debug('Python search path is now {0}.'.format(sys.path))\n\n        try:\n            module = importlib.import_module(self._validator_import_name)\n        except Exception as e:\n            text_student = \"Internal validation problem, please contact your course responsible.\"\n            text_tutor = \"Exception while loading the validator: \" + str(e)\n            self._send_result(text_student, text_tutor, UNSPECIFIC_ERROR)\n            return\n\n        # Looped validator loading in the test suite demands this\n        importlib.reload(module)\n\n        # make the call\n        try:\n            module.validate(self)\n        except Exception as e:\n            # get more info\n            text_student = None\n            text_tutor = None\n            if type(e) is TerminationException:\n                text_student = \"The execution of '{0}' terminated unexpectely.\".format(\n                    e.instance.name)\n                text_tutor = \"The execution of '{0}' terminated unexpectely.\".format(\n                    e.instance.name)\n                text_student += \"\\n\\nOutput so far:\\n\" + e.output\n                text_tutor += \"\\n\\nOutput so far:\\n\" + e.output\n            elif type(e) is TimeoutException:\n                text_student = \"The execution of '{0}' was cancelled, since it took too long.\".format(\n                    e.instance.name)\n                text_tutor = \"The execution of '{0}' was cancelled due to timeout.\".format(\n                    e.instance.name)\n                text_student += \"\\n\\nOutput so far:\\n\" + e.output\n                text_tutor += \"\\n\\nOutput so far:\\n\" + e.output\n            elif type(e) is NestedException:\n                text_student = \"Unexpected problem during the execution of '{0}'. {1}\".format(\n                    e.instance.name,\n                    str(e.real_exception))\n                text_tutor = \"Unkown exception during the execution of '{0}'. {1}\".format(\n                    e.instance.name,\n                    str(e.real_exception))\n                text_student += \"\\n\\nOutput so far:\\n\" + e.output\n                text_tutor += \"\\n\\nOutput so far:\\n\" + e.output\n            elif type(e) is WrongExitStatusException:\n                text_student = \"The execution of '{0}' resulted in the unexpected exit status {1}.\".format(\n                    e.instance.name,\n                    e.got)\n                text_tutor = \"The execution of '{0}' resulted in the unexpected exit status {1}.\".format(\n                    e.instance.name,\n                    e.got)\n                text_student += \"\\n\\nOutput so far:\\n\" + e.output\n                text_tutor += \"\\n\\nOutput so far:\\n\" + e.output\n            elif type(e) is JobException:\n                # Some problem with our own code\n                text_student = e.info_student\n                text_tutor = e.info_tutor\n            elif type(e) is FileNotFoundError:\n                text_student = \"A file is missing: {0}\".format(\n                    str(e))\n                text_tutor = \"Missing file: {0}\".format(\n                    str(e))\n            elif type(e) is AssertionError:\n                # Need this harsh approach to kill the\n                # test suite execution at this point\n                # Otherwise, the problem gets lost in\n                # the log storm\n                logger.error(\n                    \"Failed assertion in validation script. Should not happen in production.\")\n                exit(-1)\n            else:\n                # Something really unexpected\n                text_student = \"Internal problem while validating your submission. Please contact the course responsible.\"\n                text_tutor = \"Unknown exception while running the validator: {0}\".format(\n                    str(e))\n            # We got the text. Report the problem.\n            self._send_result(text_student, text_tutor, UNSPECIFIC_ERROR)\n            return\n        # no unhandled exception during the execution of the validator\n        if not self.result_sent:\n            logger.debug(\"Validation script forgot result sending, assuming success.\")\n            self.send_pass_result()\n        # roll back\n        sys.path = old_path\n        # Test script was executed, result was somehow sent\n        # Clean the file system, since we can't do anything else\n        remove_working_directory(self.working_dir, self._config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_and_run(config):\n    '''\n    Main operation of the executor.\n\n    Returns True when a job was downloaded and executed.\n    Returns False when no job could be downloaded.\n    '''\n    job = fetch_job(config)\n    if job:\n        job._run_validate()\n        return True\n    else:\n        return False", "response": "Download and run a job."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies a new node into src_dir and run the job.", "response": "def copy_and_run(config, src_dir):\n    '''\n    Local-only operation of the executor.\n    Intended for validation script developers,\n    and the test suite.\n\n    Please not that this function only works correctly\n    if the validator has one of the following names:\n        - validator.py\n        - validator.zip\n\n    Returns True when a job was prepared and executed.\n    Returns False when no job could be prepared.\n    '''\n    job = fake_fetch_job(config, src_dir)\n    if job:\n        job._run_validate()\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef console_script():\n    '''\n        The main entry point for the production\n        administration script 'opensubmit-exec',\n        installed by setuptools.\n    '''\n    if len(sys.argv) == 1:\n        print(\"opensubmit-exec [configcreate <server_url>|configtest|run|test <dir>|unlock|help] [-c config_file]\")\n        return 0\n\n    if \"help\" in sys.argv[1]:\n        print(\"configcreate <server_url>:  Create initial config file for the OpenSubmit executor.\")\n        print(\"configtest:                 Check config file for correct installation of the OpenSubmit executor.\")\n        print(\"run:                        Fetch and run code to be tested from the OpenSubmit web server. Suitable for crontab.\")\n        print(\"test <dir>:                 Run test script from a local folder for testing purposes.\")\n        print(\"unlock:                     Break the script lock, because of crashed script.\")\n        print(\"help:                       Print this help\")\n        print(\n            \"-c config_file    Configuration file to be used (default: {0})\".format(CONFIG_FILE_DEFAULT))\n        return 0\n\n    # Translate legacy commands\n    if sys.argv[1] == \"configure\":\n        sys.argv[1] = 'configtest'\n\n    config_fname = get_config_fname(sys.argv)\n\n    if \"configcreate\" in sys.argv[1]:\n        print(\"Creating config file at \" + config_fname)\n\n        server_url = sys.argv[2]\n\n        if create_config(config_fname, override_url=server_url):\n            print(\"Config file created, fetching jobs from \" + server_url)\n            return 0\n        else:\n            return 1\n\n    if \"configtest\" in sys.argv[1]:\n        print(\"Testing config file at \" + config_fname)\n\n        if has_config(config_fname):\n            config = read_config(config_fname)\n            if not check_config(config):\n                return 1\n        else:\n            print(\"ERROR: Seems like the config file %s does not exist. Call 'opensubmit-exec configcreate <server_url>' first.\" %\n                  config_fname)\n            return 1\n\n        print(\"Sending host information update to server ...\")\n        send_hostinfo(config)\n        return 0\n\n    if \"unlock\" in sys.argv[1]:\n        config = read_config(config_fname)\n        break_lock(config)\n        return 0\n\n    if \"run\" in sys.argv[1]:\n        config = read_config(config_fname)\n        # Perform additional precautions for unattended mode in cron\n        kill_longrunning(config)\n        with ScriptLock(config):\n            download_and_run(config)\n        return 0\n\n    if \"test\" in sys.argv[1]:\n        config = read_config(config_fname)\n        copy_and_run(config, sys.argv[2])\n        return 0", "response": "This is the main entry point for the administration script. It is used by the script s main entry point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hashdict(d):\n    k = 0\n    for key,val in d.items():\n        k ^= hash(key) ^ hash(val)\n    return k", "response": "Hash a dictionary of objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_df(cls, df_long, df_short):\n        pop = cls(1,1,1,1,1) #dummy population\n        pop.orbpop_long = OrbitPopulation.from_df(df_long)\n        pop.orbpop_short = OrbitPopulation.from_df(df_short)\n        return pop", "response": "Builds TripleOrbitPopulation from DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_hdf(cls, filename, path=''):\n        df_long = pd.read_hdf(filename,'{}/long/df'.format(path))\n        df_short = pd.read_hdf(filename,'{}/short/df'.format(path))\n        return cls.from_df(df_long, df_short)", "response": "Load TripleOrbitPopulation from saved. h5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dRV(self,dt,com=False):\n        if type(dt) != Quantity:\n            dt *= u.day\n\n        mean_motions = np.sqrt(G*(self.mred)*MSUN/(self.semimajor*AU)**3)\n        mean_motions = np.sqrt(const.G*(self.mred)/(self.semimajor)**3)\n\n        newM = self.M + mean_motions * dt\n        pos,vel = orbit_posvel(newM,self.ecc,self.semimajor.value,\n                               self.mred.value,\n                               self.obspos)\n\n        if com:\n            return (vel.z - self.RV) * (self.M2 / (self.M1 + self.M2))\n        else:\n            return vel.z-self.RV", "response": "Compute the change in RV of star 1 over time separation dt."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RV_timeseries(self,ts,recalc=False):\n        if type(ts) != Quantity:\n            ts *= u.day\n\n        if not recalc and hasattr(self,'RV_measurements'):\n            if (ts == self.ts).all():\n                return self._RV_measurements\n            else:\n                pass\n\n        RVs = Quantity(np.zeros((len(ts),self.N)),unit='km/s')\n        for i,t in enumerate(ts):\n            RVs[i,:] = self.dRV(t,com=True)\n        self._RV_measurements = RVs\n        self.ts = ts\n        return RVs", "response": "Returns the Radial Velocity time series for star 1 at given times ts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an OrbitPopulation from a DataFrame.", "response": "def from_df(cls, df):\n        \"\"\"Creates an OrbitPopulation from a DataFrame.\n\n        :param df:\n            :class:`pandas.DataFrame` object.  Must contain the following\n            columns: ``['M1','M2','P','ecc','mean_anomaly','obsx','obsy','obsz']``,\n            i.e., as what is accessed via :attr:`OrbitPopulation.dataframe`.\n\n        :return:\n            :class:`OrbitPopulation`.\n        \"\"\"\n        return cls(df['M1'], df['M2'], df['P'],\n                   ecc=df['ecc'], mean_anomaly=df['mean_anomaly'],\n                   obsx=df['obsx'], obsy=df['obsy'], obsz=df['obsz'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_hdf(cls, filename, path=''):\n        df = pd.read_hdf(filename,'{}/df'.format(path))\n        return cls.from_df(df)", "response": "Loads OrbitPopulation from HDF file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a grid of RMS values assuming observations at given times.", "response": "def RV_RMSgrid(self,ts,res=20,mres=None,Pres=None,\n                   conf=0.95,measured_rms=None,drv=0,\n                   plot=True,fig=None,contour=True,sigma=1):\n        \"\"\"Writes a grid of RV RMS values, assuming observations at given times.\n\n        Caveat Emptor: Written a long time ago, and\n        hasn't really been tested.\n\n        :param ts:\n            Times of observations\n\n        :param res, mres, Pres: (optional)\n            Resolution of grids.  ``res`` relates to both mass and period;\n            otherwise ``mres`` and ``Pres`` can be set separately.\n\n        :param conf: (optional)\n            Confidence at which to exclude regions.  Used if ``measured_rms``\n            is ``None``.\n\n        :param measured_rms: (optional)\n            Measured RV RMS, if applicable [not sure exactly how this is used]\n\n        :param drv: (optional)\n            Uncertainties in RV to simulate.\n\n        :param plot: (optional)\n            Whether to plot result.\n\n        :param fig: (optional)\n            Passed to :func:`plotutils.setfig`.\n\n        :param contour: (optional)\n            Whether to plot contours.\n\n        :param sigma: (optional)\n            Level at which to exclude, based on ``measured_rms``.\n\n        \"\"\"\n        RVs = self.RV_timeseries(ts)\n        RVs += rand.normal(size=np.size(RVs)).reshape(RVs.shape)*drv\n        rms = RVs.std(axis=0)\n\n        if mres is None:\n            mres = res\n        if Pres is None:\n            Pres = res\n\n        mbins = np.linspace(self.M2.min(),self.M2.max(),mres+1)\n        Pbins = np.logspace(np.log10(self.P.min()),np.log10(self.P.max()),Pres+1)\n        logPbins = np.log10(Pbins)\n\n        mbin_centers = (mbins[:-1] + mbins[1:])/2.\n        logPbin_centers = (logPbins[:-1] + logPbins[1:])/2.\n\n        minds = np.digitize(self.M2,mbins)\n        Pinds = np.digitize(self.P,Pbins)\n\n        pctiles = np.zeros((mres,Pres))\n        ns = np.zeros((mres,Pres))\n\n        for i in np.arange(mres):\n            for j in np.arange(Pres):\n                w = np.where((minds==i+1) & (Pinds==j+1))\n                these = rms[w]\n                n = size(these)\n                ns[i,j] = n\n                if measured_rms is not None:\n                    pctiles[i,j] = (these > sigma*measured_rms).sum()/float(n)\n                else:\n                    inds = np.argsort(these)\n                    pctiles[i,j] = these[inds][int((1-conf)*n)]\n\n        Ms,logPs = np.meshgrid(mbin_centers,logPbin_centers)\n\n        if plot:\n            setfig(fig)\n\n            if contour:\n                mbin_centers = (mbins[:-1] + mbins[1:])/2.\n                logPbins = np.log10(Pbins)\n                logPbin_centers = (logPbins[:-1] + logPbins[1:])/2.\n                if measured_rms is not None:\n                    levels = [0.68,0.95,0.99]\n                else:\n                    levels = np.arange(0,20,2)\n                c = plt.contour(logPbin_centers,mbin_centers,pctiles,levels=levels,colors='k')\n                plt.clabel(c, fontsize=10, inline=1)\n\n            else:\n                extent = [np.log10(self.P.min()),np.log10(self.P.max()),self.M2.min(),self.M2.max()]\n                im = plt.imshow(pctiles,cmap='Greys',extent=extent,aspect='auto')\n\n                fig = plt.gcf()\n                ax = plt.gca()\n\n\n                if measured_rms is None:\n                    cbarticks = np.arange(0,21,2)\n                else:\n                    cbarticks = np.arange(0,1.01,0.1)\n                cbar = fig.colorbar(im, ticks=cbarticks)\n\n            plt.xlabel('Log P')\n            plt.ylabel('M2')\n\n        return mbins,Pbins,pctiles,ns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndrawing random periods and eccentricities according to empirical survey data.", "response": "def draw_pers_eccs(n,**kwargs):\n    \"\"\"\n    Draw random periods and eccentricities according to empirical survey data.\n    \"\"\"\n    pers = draw_raghavan_periods(n)\n    eccs = draw_eccs(n,pers,**kwargs)\n    return pers,eccs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing eccentricities appropriate to given periods generated according to empirical data from Multiple Star Catalog", "response": "def draw_eccs(n,per=10,binsize=0.1,fuzz=0.05,maxecc=0.97):\n    \"\"\"draws eccentricities appropriate to given periods, generated according to empirical data from Multiple Star Catalog\n    \"\"\"\n    if np.size(per) == 1 or np.std(np.atleast_1d(per))==0:\n        if np.size(per)>1:\n            per = per[0]\n        if per==0:\n            es = np.zeros(n)\n        else:\n            ne=0\n            while ne<10:\n                mask = np.absolute(np.log10(MSC_TRIPLEPERS)-np.log10(per))<binsize/2.\n                es = MSC_TRIPDATA.e[mask]\n                ne = len(es)\n                if ne<10:\n                    binsize*=1.1\n            inds = rand.randint(ne,size=n)\n            es = es[inds] * (1 + rand.normal(size=n)*fuzz)\n    else:\n        longmask = (per > 25)\n        shortmask = (per <= 25)\n        es = np.zeros(np.size(per))\n\n        elongs = MSC_TRIPDATA.e[MSC_TRIPLEPERS > 25]\n        eshorts = MSC_TRIPDATA.e[MSC_TRIPLEPERS <= 25]\n\n        n = np.size(per)\n        nlong = longmask.sum()\n        nshort = shortmask.sum()\n        nelongs = np.size(elongs)\n        neshorts = np.size(eshorts)\n        ilongs = rand.randint(nelongs,size=nlong)\n        ishorts = rand.randint(neshorts,size=nshort)\n\n        es[longmask] = elongs[ilongs]\n        es[shortmask] = eshorts[ishorts]\n\n    es = es * (1 + rand.normal(size=n)*fuzz)\n    es[es>maxecc] = maxecc\n    return np.absolute(es)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn boolean array that is True where two stars are within Roche lobe", "response": "def withinroche(semimajors,M1,R1,M2,R2):\n    \"\"\"\n    Returns boolean array that is True where two stars are within Roche lobe\n    \"\"\"\n    q = M1/M2\n    return ((R1+R2)*RSUN) > (rochelobe(q)*semimajors*AU)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns semimajor axis in AU given P in days mstar in solar masses.", "response": "def semimajor(P,mstar=1):\n    \"\"\"Returns semimajor axis in AU given P in days, mstar in solar masses.\n    \"\"\"\n    return ((P*DAY/2/np.pi)**2*G*mstar*MSUN)**(1./3)/AU"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning fraction of total flux in first argument assuming all are magnitudes.", "response": "def fluxfrac(*mags):\n    \"\"\"Returns fraction of total flux in first argument, assuming all are magnitudes.\n    \"\"\"\n    Ftot = 0\n    for mag in mags:\n        Ftot += 10**(-0.4*mag)\n    F1 = 10**(-0.4*mags[0])\n    return F1/Ftot"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dfromdm(dm):\n    if np.size(dm)>1:\n        dm = np.atleast_1d(dm)\n    return 10**(1+dm/5)", "response": "Returns distance given distance modulus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the distance modulus given d in parsec.", "response": "def distancemodulus(d):\n    \"\"\"Returns distance modulus given d in parsec.\n    \"\"\"\n    if type(d)==Quantity:\n        x = d.to('pc').value\n    else:\n        x = d #assumed to be pc\n\n    if np.size(x)>1:\n        d = np.atleast_1d(x)\n    return 5*np.log10(x/10)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mult_masses(mA, f_binary=0.4, f_triple=0.12,\n                minmass=0.11, qmin=0.1, n=1e5):\n    \"\"\"Returns m1, m2, and m3 appropriate for TripleStarPopulation, given \"primary\" mass (most massive of system) and binary/triple fractions.\n\n\n    star with m1 orbits (m2 + m3).  This means that the primary mass mA will correspond\n    either to m1 or m2.  Any mass set to 0 means that component does not exist.\n    \"\"\"\n\n    if np.size(mA) > 1:\n        n = len(mA)\n    else:\n        mA = np.ones(n) * mA\n\n\n    r = rand.random(n)\n    is_single = r > (f_binary + f_triple)\n    is_double = (r > f_triple) & (r < (f_binary + f_triple))\n    is_triple = r <= f_triple\n\n    CwA = rand.random(n) < 0.5\n    CwB = ~CwA\n\n\n    #these for Triples:\n\n    minq2_A = minmass/mA\n    q2_A = rand.random(n)*(1-minq2_A) + minq2_A\n    minq1_A = (minmass/mA)/(1+q2_A)\n    maxq1_A = 1/(1+q2_A)\n    q1_A = rand.random(n)*(maxq1_A-minq1_A) + minq1_A\n\n    minq1_B = 2*minmass/mA\n    q1_B = rand.random(n)*(1-minq1_B) + minq1_B\n    minq2_B = np.maximum(((q1_B*mA)-minmass)/minmass,\n                         (q1_B*mA - minmass)/(q1_B*mA + minmass))\n    maxq2_B = 1.\n    q2_B = rand.random(n)*(maxq2_B-minq2_B) + minq2_B\n\n\n    mB_A = q1_A*(1 + q2_A) * mA\n    mC_A = q2_A * mA\n\n    mB_B = (q1_B/(1 + q2_B)) * mA\n    mC_B = (q1_B*q2_B)/(1 + q2_B) * mA\n\n    mB = CwA*mB_A + CwB*mB_B\n    mC = CwA*mC_A + CwB*mC_B\n\n    #for binaries-only\n    qmin = minmass/mA\n    q = rand.random(n)*(1-qmin) + qmin\n    mB[is_double] = q[is_double]*mA[is_double]\n\n\n    #now need to define the proper mapping from A,B,C to 1,2,3:\n    # If no B or C present, then A=1\n    # If B present but not C, then A=1, B=2\n    # If both B and C present then:\n    #     If C is with A, then A=2, C=3, B=1\n    #     If C is with B, then A=1, B=2, C=3\n    m1 = (mA)*(is_single + is_double) + (CwA*mB + CwB*mA)*is_triple\n    m2 = (mB)*is_double + (CwA*mA + CwB*mB)*is_triple\n    m3 = mC*is_triple\n\n    return m1, m2, m3", "response": "Returns m1 m2 and m3 appropriate for TripleStarPopulation given primary mass mA and binary fractions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_AV_infinity(ra,dec,frame='icrs'):\n    coords = SkyCoord(ra,dec,unit='deg',frame=frame).transform_to('icrs')\n\n    rah,ram,ras = coords.ra.hms\n    decd,decm,decs = coords.dec.dms\n    if decd > 0:\n        decsign = '%2B'\n    else:\n        decsign = '%2D'\n    url = 'http://ned.ipac.caltech.edu/cgi-bin/nph-calc?in_csys=Equatorial&in_equinox=J2000.0&obs_epoch=2010&lon='+'%i' % rah + \\\n        '%3A'+'%i' % ram + '%3A' + '%05.2f' % ras + '&lat=%s' % decsign + '%i' % abs(decd) + '%3A' + '%i' % abs(decm) + '%3A' + '%05.2f' % abs(decs) + \\\n        '&pa=0.0&out_csys=Equatorial&out_equinox=J2000.0'\n\n    tmpfile = '/tmp/nedsearch%s%s.html' % (ra,dec)\n    cmd = 'curl -s \\'%s\\' -o %s' % (url,tmpfile)\n    sp.Popen(cmd,shell=True).wait()\n    AV = None\n    with open(tmpfile, 'r') as f:\n        for line in f:\n            m = re.search('V \\(0.54\\)\\s+(\\S+)',line)\n            if m:\n                AV = float(m.group(1))\n    if AV is None:\n        logging.warning('Error accessing NED, url={}'.format(url))\n        with open(tmpfile) as f:\n            for line in f:\n                logging.warning(line)\n\n\n\n    os.remove(tmpfile)\n    return AV", "response": "Get the A_V exctinction at infinity for a given line of sight."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit trapezoidal shape to eclipses described by parameters in data.", "response": "def fitebs(data, MAfn=None, conv=True, cadence=0.020434028,\n           use_pbar=True, msg=''):\n    \"\"\"Fits trapezoidal shape to eclipses described by parameters in data\n\n    Takes a few minutes for 20,000 simulations.\n\n    :param data:\n        Input ``DataFrame`` holding data.  Must have following columns:\n        ``'P', 'mass_1', 'mass_2', 'radius_1', 'radius_2'``,\n        ``'inc', 'ecc', 'w', 'dpri', 'dsec', 'b_sec', 'b_pri'``,\n        ``'fluxfrac_1', 'fluxfrac_2', 'u1_1', 'u1_2', 'u2_1', 'u2_2'``.\n\n    :param MAfn:\n        :class:`MAInterpolationFunction` object; if not passed, it will be\n        created.\n\n    :param conv:\n        Whether to convolve theoretical transit shape\n        with box-car filter to simulate observation integration time.\n\n    :param cadence:\n        Integration time used if ``conv`` is ``True``.  Defaults to\n        Kepler mission cadence.\n\n    :param use_pbar:\n        Whether to use nifty visual progressbar when doing calculation.\n\n    :param msg:\n        Message to print for pbar.\n\n    :return df:\n        Returns dataframe with the following columns:\n        ``'depth', 'duration', 'slope'``,\n        ``'secdepth', 'secondary'``.\n\n    \"\"\"\n    n = len(data)\n\n\n    p0s, bs, aRs = eclipse_pars(data['P'], data['mass_1'], data['mass_2'],\n                                data['radius_1'], data['radius_2'],\n                                inc=data['inc'],\n                                ecc=data['ecc'], w=data['w'])\n\n    deps, durs, slopes = (np.zeros(n), np.zeros(n), np.zeros(n))\n    secs = np.zeros(n).astype(bool)\n    dsec = np.zeros(n)\n\n    if use_pbar and pbar_ok:\n        widgets = [msg+'fitting shape parameters for %i systems: ' % n,Percentage(),\n                   ' ',Bar(marker=RotatingMarker()),' ',ETA()]\n        pbar = ProgressBar(widgets=widgets,maxval=n)\n        pbar.start()\n\n    for i in range(n):\n        logging.debug('Fitting star {}'.format(i))\n        pri = (data['dpri'][i] > data['dsec'][i]) or np.isnan(data['dsec'][i])\n        sec = not pri\n        secs[i] = sec\n        p0, aR = (p0s[i], aRs[i])\n        if sec:\n            b = data['b_sec'][i]\n            frac = data['fluxfrac_2'][i]\n            dsec[i] = data['dpri'][i]\n            u1 = data['u1_2'][i]\n            u2 = data['u2_2'][i]\n        else:\n            b = data['b_pri'][i]\n            frac = data['fluxfrac_1'][i]\n            dsec[i] = data['dsec'][i]\n            u1 = data['u1_1'][i]\n            u2 = data['u2_1'][i]\n        try:\n            logging.debug('p0={}, b={}, aR={}, P={}, frac={}, ecc={}, w={}, sec={}, u1={}, u2={}'.format(p0,b,aR,data['P'][i],frac,data['ecc'][i],data['w'][i],sec,u1,u2))\n            logging.debug('dpri={}, dsec={}'.format(data['dpri'][i], data['dsec'][i]))\n            trap_pars = eclipse_tt(p0,b,aR,data['P'][i],conv=conv,\n                                   cadence=cadence, frac=frac,\n                                   ecc=data['ecc'][i],w=data['w'][i],\n                                   sec=sec,u1=u1,u2=u2)\n            #logging.debug('{}'.format(trap_pars))\n\n            durs[i], deps[i], slopes[i] = trap_pars\n            if use_pbar and pbar_ok:\n                pbar.update(i)\n\n        except NoEclipseError:\n            logging.error('No eclipse registered for index {}'.format(i))\n            continue\n        except NoFitError:\n            logging.error('Fit did not converge for index {}'.format(i))\n            continue\n        except KeyboardInterrupt:\n            raise\n        except:\n            logging.error('unknown error for index {}'.format(i))\n            raise\n            continue\n\n    return pd.DataFrame({'depth':deps, 'duration':durs,\n                         'slope':slopes, 'secdepth':dsec,\n                         'secondary':secs})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits a path into a list of directories.", "response": "def split_path(path):\n    \"\"\"\n        \"/tmp/test\"\n\n    Becomes:\n\n        (\"/\", \"tmp\", \"test\")\n    \"\"\"\n    parts = []\n    path, tail = os.path.split(path)\n    while path and tail:\n        parts.append(tail)\n        path, tail = os.path.split(path)\n    parts.append(os.path.join(path, tail))\n    return map(os.path.normpath, parts)[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_files(path):\n    return_files = []\n    for root, dirs, files in os.walk(path):\n\n        # Skip hidden files\n        files = [f for f in files if not f[0] == '.']\n        dirs[:] = [d for d in dirs if not d[0] == '.']\n\n        for filename in files:\n            return_files.append(os.path.join(root, filename))\n    return return_files", "response": "Returns a recursive list of all non - hidden files in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the current object to a pickle file.", "response": "def save_pkl(self, filename):\n        \"\"\"\n        Pickles TransitSignal.\n        \"\"\"\n        with open(filename, 'wb') as fout:\n            pickle.dump(self, fout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the signal with the most - sqine trapezoid fit.", "response": "def plot(self, fig=None, plot_trap=False, name=False, trap_color='g',\n             trap_kwargs=None, **kwargs):\n        \"\"\"\n        Makes a simple plot of signal\n\n        :param fig: (optional)\n            Argument for :func:`plotutils.setfig`.\n\n        :param plot_trap: (optional)\n            Whether to plot the (best-fit least-sq) trapezoid fit.\n\n        :param name: (optional)\n            Whether to annotate plot with the name of the signal;\n            can be ``True`` (in which case ``self.name`` will be\n            used), or any arbitrary string.\n\n        :param trap_color: (optional)\n            Color of trapezoid fit line.\n\n        :param trap_kwargs: (optional)\n            Keyword arguments to pass to trapezoid fit line.\n\n        :param **kwargs: (optional)\n            Additional keyword arguments passed to ``plt.plot``.\n\n\n        \"\"\"\n\n        setfig(fig)\n\n        plt.plot(self.ts,self.fs,'.',**kwargs)\n\n        if plot_trap and hasattr(self,'trapfit'):\n            if trap_kwargs is None:\n                trap_kwargs = {}\n            plt.plot(self.ts, traptransit(self.ts,self.trapfit),\n                     color=trap_color, **trap_kwargs)\n\n        if name is not None:\n            if type(name)==type(''):\n                text = name\n            else:\n                text = self.name\n            plt.annotate(text,xy=(0.1,0.1),xycoords='axes fraction',fontsize=22)\n\n        if hasattr(self,'depthfit') and not np.isnan(self.depthfit[0]):\n            lo = 1 - 3*self.depthfit[0]\n            hi = 1 + 2*self.depthfit[0]\n        else:\n            lo = 1\n            hi = 1\n\n        sig = qstd(self.fs,0.005)\n        hi = max(hi,self.fs.mean() + 7*sig)\n        lo = min(lo,self.fs.mean() - 7*sig)\n        logging.debug('lo={}, hi={}'.format(lo,hi))\n        plt.ylim((lo,hi))\n        plt.xlabel('time [days]')\n        plt.ylabel('Relative flux')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit transit signal to trapezoid model using MCMC.", "response": "def MCMC(self, niter=500, nburn=200, nwalkers=200, threads=1,\n             fit_partial=False, width=3, savedir=None, refit=False,\n             thin=10, conf=0.95, maxslope=MAXSLOPE, debug=False, p0=None):\n        \"\"\"\n        Fit transit signal to trapezoid model using MCMC\n\n        .. note:: As currently implemented, this method creates a\n            bunch of attributes relevant to the MCMC fit; I plan\n            to refactor this to define those attributes as properties\n            so as not to have their creation hidden away here.  I plan\n            to refactor how this works.\n        \"\"\"\n        if fit_partial:\n            wok = np.where((np.absolute(self.ts-self.center) < (width*self.dur)) &\n                           ~np.isnan(self.fs))\n        else:\n            wok = np.where(~np.isnan(self.fs))\n\n        if savedir is not None:\n            if not os.path.exists(savedir):\n                os.mkdir(savedir)\n\n        alreadydone = True\n        alreadydone &= savedir is not None\n        alreadydone &= os.path.exists('%s/ts.npy' % savedir)\n        alreadydone &= os.path.exists('%s/fs.npy' % savedir)\n\n        if savedir is not None and alreadydone:\n            ts_done = np.load('%s/ts.npy' % savedir)\n            fs_done = np.load('%s/fs.npy' % savedir)\n            alreadydone &= np.all(ts_done == self.ts[wok])\n            alreadydone &= np.all(fs_done == self.fs[wok])\n\n        if alreadydone and not refit:\n            logging.info('MCMC fit already done for %s.  Loading chains.' % self.name)\n            Ts = np.load('%s/duration_chain.npy' % savedir)\n            ds = np.load('%s/depth_chain.npy' % savedir)\n            slopes = np.load('%s/slope_chain.npy' % savedir)\n            tcs = np.load('%s/tc_chain.npy' % savedir)\n        else:\n            logging.info('Fitting data to trapezoid shape with MCMC for %s....' % self.name)\n            if p0 is None:\n                p0 = self.trapfit.copy()\n                p0[0] = np.absolute(p0[0])\n                if p0[2] < 2:\n                    p0[2] = 2.01\n                if p0[1] < 0:\n                    p0[1] = 1e-5\n            logging.debug('p0 for MCMC = {}'.format(p0))\n            sampler = traptransit_MCMC(self.ts[wok],self.fs[wok],self.dfs[wok],\n                                        niter=niter,nburn=nburn,nwalkers=nwalkers,\n                                        threads=threads,p0=p0,return_sampler=True,\n                                        maxslope=maxslope)\n\n            Ts,ds,slopes,tcs = (sampler.flatchain[:,0],sampler.flatchain[:,1],\n                                sampler.flatchain[:,2],sampler.flatchain[:,3])\n\n            self.sampler = sampler\n            if savedir is not None:\n                np.save('%s/duration_chain.npy' % savedir,Ts)\n                np.save('%s/depth_chain.npy' % savedir,ds)\n                np.save('%s/slope_chain.npy' % savedir,slopes)\n                np.save('%s/tc_chain.npy' % savedir,tcs)\n                np.save('%s/ts.npy' % savedir,self.ts[wok])\n                np.save('%s/fs.npy' % savedir,self.fs[wok])\n\n        if debug:\n            print(Ts)\n            print(ds)\n            print(slopes)\n            print(tcs)\n\n        N = len(Ts)\n        try:\n            self.Ts_acor = integrated_time(Ts)\n            self.ds_acor = integrated_time(ds)\n            self.slopes_acor = integrated_time(slopes)\n            self.tcs_acor = integrated_time(tcs)\n            self.fit_converged = True\n        except AutocorrError:\n            self.fit_converged = False\n\n\n        ok = (Ts > 0) & (ds > 0) & (slopes > 0) & (slopes < self.maxslope)\n        logging.debug('trapezoidal fit has {} good sample points'.format(ok.sum()))\n        if ok.sum()==0:\n            if (Ts > 0).sum()==0:\n                #logging.debug('{} points with Ts > 0'.format((Ts > 0).sum()))\n                logging.debug('{}'.format(Ts))\n                raise MCMCError('{}: 0 points with Ts > 0'.format(self.name))\n            if (ds > 0).sum()==0:\n                #logging.debug('{} points with ds > 0'.format((ds > 0).sum()))\n                logging.debug('{}'.format(ds))\n                raise MCMCError('{}: 0 points with ds > 0'.format(self.name))\n            if (slopes > 0).sum()==0:\n                #logging.debug('{} points with slopes > 0'.format((slopes > 0).sum()))\n                logging.debug('{}'.format(slopes))\n                raise MCMCError('{}: 0 points with slopes > 0'.format(self.name))\n            if (slopes < self.maxslope).sum()==0:\n                #logging.debug('{} points with slopes < maxslope ({})'.format((slopes < self.maxslope).sum(),self.maxslope))\n                logging.debug('{}'.format(slopes))\n                raise MCMCError('{} points with slopes < maxslope ({})'.format((slopes < self.maxslope).sum(),self.maxslope))\n\n\n        durs,deps,logdeps,slopes = (Ts[ok],ds[ok],np.log10(ds[ok]),\n                                              slopes[ok])\n\n\n        inds = (np.arange(len(durs)/thin)*thin).astype(int)\n        durs,deps,logdeps,slopes = (durs[inds],deps[inds],logdeps[inds],\n                                              slopes[inds])\n\n        self.durs,self.deps,self.logdeps,self.slopes = (durs,deps,logdeps,slopes)\n\n        self._make_kde(conf=conf)\n\n        self.hasMCMC = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning desired confidence interval for provided KDE object", "response": "def kdeconf(kde,conf=0.683,xmin=None,xmax=None,npts=500,\n            shortest=True,conftol=0.001,return_max=False):\n    \"\"\"\n    Returns desired confidence interval for provided KDE object\n    \"\"\"\n    if xmin is None:\n        xmin = kde.dataset.min()\n    if xmax is None:\n        xmax = kde.dataset.max()\n    x = np.linspace(xmin,xmax,npts)\n    return conf_interval(x,kde(x),shortest=shortest,conf=conf,\n                         conftol=conftol,return_max=return_max)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning std ignoring outer quant pctiles", "response": "def qstd(x,quant=0.05,top=False,bottom=False):\n    \"\"\"returns std, ignoring outer 'quant' pctiles\n    \"\"\"\n    s = np.sort(x)\n    n = np.size(x)\n    lo = s[int(n*quant)]\n    hi = s[int(n*(1-quant))]\n    if top:\n        w = np.where(x>=lo)\n    elif bottom:\n        w = np.where(x<=hi)\n    else:\n        w = np.where((x>=lo)&(x<=hi))\n    return np.std(x[w])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn desired 1 - d confidence interval for provided x L", "response": "def conf_interval(x,L,conf=0.683,shortest=True,\n                  conftol=0.001,return_max=False):\n    \"\"\"\n    Returns desired 1-d confidence interval for provided x, L[PDF]\n    \n    \"\"\"\n    cum = np.cumsum(L)\n    cdf = cum/cum.max()\n    if shortest:\n        maxind = L.argmax()\n        if maxind==0:   #hack alert\n            maxind = 1\n        if maxind==len(L)-1:\n            maxind = len(L)-2\n        Lval = L[maxind]\n\n        lox = x[0:maxind]\n        loL = L[0:maxind]\n        locdf = cdf[0:maxind]\n        hix = x[maxind:]\n        hiL = L[maxind:]\n        hicdf = cdf[maxind:]\n\n        dp = 0\n        s = -1\n        dL = Lval\n        switch = False\n        last = 0\n        while np.absolute(dp-conf) > conftol:\n            Lval += s*dL\n            if maxind==0:\n                loind = 0\n            else:\n                loind = (np.absolute(loL - Lval)).argmin()\n            if maxind==len(L)-1:\n                hiind = -1\n            else:\n                hiind = (np.absolute(hiL - Lval)).argmin()\n\n            dp = hicdf[hiind]-locdf[loind]\n            lo = lox[loind]\n            hi = hix[hiind]\n            if dp == last:\n                break\n            last = dp\n            cond = dp > conf\n            if cond ^ switch:\n                dL /= 2.\n                s *= -1\n                switch = not switch\n\n    else:\n        alpha = (1-conf)/2.\n        lo = x[np.absolute(cdf-alpha).argmin()]\n        hi = x[(np.absolute(cdf-(1-(alpha)))).argmin()]\n        \n    if return_max:\n        xmaxL = x[L.argmax()]\n        return xmaxL,lo,hi\n    else:\n        return (lo,hi)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_units(self, units, factor, latexrepr=None):\n        if units in self._units:\n            raise ValueError('%s already defined' % units)\n        if factor == 1:\n            raise ValueError('Factor cannot be equal to 1')\n        if latexrepr is None:\n            latexrepr = units\n\n        self._units[units] = factor\n        self._latexrepr[units] = latexrepr", "response": "Add new possible units."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a value expressed in certain units to a new units.", "response": "def convert(self, value, units, newunits):\n        \"\"\"\n        Converts a value expressed in certain *units* to a new units.\n        \"\"\"\n        return value * self._units[units] / self._units[newunits]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the figure to fig and clears it if fig is 0.", "response": "def setfig(fig=None,**kwargs):\n    \"\"\"\n    Sets figure to 'fig' and clears; if fig is 0, does nothing (e.g. for overplotting)\n\n    if fig is None (or anything else), creates new figure\n    \n    I use this for basically every function I write to make a plot.\n    I give the function\n    a \"fig=None\" kw argument, so that it will by default create a new figure.\n\n    .. note::\n\n      There's most certainly a better, more object-oriented\n      way of going about writing functions that make figures, but\n      this was put together before I knew how to think that way,\n      so this stays for now as a convenience.\n      \n    \"\"\"\n    if fig:\n        plt.figure(fig,**kwargs)\n        plt.clf()\n    elif fig==0:\n        pass\n    else:\n        plt.figure(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot2dhist(xdata,ydata,cmap='binary',interpolation='nearest',\n               fig=None,logscale=True,xbins=None,ybins=None,\n               nbins=50,pts_only=False,**kwargs):\n    \"\"\"Plots a 2d density histogram of provided data\n\n    :param xdata,ydata: (array-like)\n        Data to plot.\n\n    :param cmap: (optional)\n        Colormap to use for density plot.\n\n    :param interpolation: (optional)\n        Interpolation scheme for display (passed to ``plt.imshow``).\n\n    :param fig: (optional)\n        Argument passed to :func:`setfig`.\n\n    :param logscale: (optional)\n        If ``True`` then the colormap will be based on a logarithmic\n        scale, rather than linear.\n\n    :param xbins,ybins: (optional)\n        Bin edges to use (if ``None``, then use ``np.histogram2d`` to\n        find bins automatically).\n\n    :param nbins: (optional)\n        Number of bins to use (if ``None``, then use ``np.histogram2d`` to\n        find bins automatically).\n\n    :param pts_only: (optional)\n        If ``True``, then just a scatter plot of the points is made,\n        rather than the density plot.\n\n    :param **kwargs:\n        Keyword arguments passed either to ``plt.plot`` or ``plt.imshow``\n        depending upon whether ``pts_only`` is set to ``True`` or not.\n        \n    \"\"\"\n\n    setfig(fig)\n    if pts_only:\n        plt.plot(xdata,ydata,**kwargs)\n        return\n\n    ok = (~np.isnan(xdata) & ~np.isnan(ydata) & \n           ~np.isinf(xdata) & ~np.isinf(ydata))\n    if ~ok.sum() > 0:\n        logging.warning('{} x values and {} y values are nan'.format(np.isnan(xdata).sum(),\n                                                                     np.isnan(ydata).sum()))\n        logging.warning('{} x values and {} y values are inf'.format(np.isinf(xdata).sum(),\n                                                                     np.isinf(ydata).sum()))\n\n    if xbins is not None and ybins is not None:\n        H,xs,ys = np.histogram2d(xdata[ok],ydata[ok],bins=(xbins,ybins))\n    else:\n        H,xs,ys = np.histogram2d(xdata[ok],ydata[ok],bins=nbins)        \n    H = H.T\n\n    if logscale:\n        H = np.log(H)\n\n    extent = [xs[0],xs[-1],ys[0],ys[-1]]\n    plt.imshow(H,extent=extent,interpolation=interpolation,\n               aspect='auto',cmap=cmap,origin='lower',**kwargs)", "response": "Plots a 2d density histogram of data xdata ydata."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the limb - darkening coefficients in Kepler band.", "response": "def ldcoeffs(teff,logg=4.5,feh=0):\n    \"\"\"\n    Returns limb-darkening coefficients in Kepler band.\n    \"\"\"\n    teffs = np.atleast_1d(teff)\n    loggs = np.atleast_1d(logg)\n\n    Tmin,Tmax = (LDPOINTS[:,0].min(),LDPOINTS[:,0].max())\n    gmin,gmax = (LDPOINTS[:,1].min(),LDPOINTS[:,1].max())\n\n    teffs[(teffs < Tmin)] = Tmin + 1\n    teffs[(teffs > Tmax)] = Tmax - 1\n    loggs[(loggs < gmin)] = gmin + 0.01\n    loggs[(loggs > gmax)] = gmax - 0.01\n\n    u1,u2 = (U1FN(teffs,loggs),U2FN(teffs,loggs))\n    return u1,u2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the impact parameter of a resource in AU R sun inc and w.", "response": "def impact_parameter(a, R, inc, ecc=0, w=0, return_occ=False):\n    \"\"\"a in AU, R in Rsun, inc & w in radians\n    \"\"\"\n    b_tra = a*AU*np.cos(inc)/(R*RSUN) * (1-ecc**2)/(1 + ecc*np.sin(w))\n\n    if return_occ:\n        b_tra = a*AU*np.cos(inc)/(R*RSUN) * (1-ecc**2)/(1 - ecc*np.sin(w))\n        return b_tra, b_occ\n    else:\n        return b_tra"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef eclipse_depth(mafn,Rp,Rs,b,u1=0.394,u2=0.261,max_only=False,npts=100,force_1d=False):\n    k = Rp*REARTH/(Rs*RSUN)\n    \n    if max_only:\n        return 1 - mafn(k,b,u1,u2)\n\n    if np.size(b) == 1:\n        x = np.linspace(0,np.sqrt(1-b**2),npts)\n        y = b\n        zs = np.sqrt(x**2 + y**2)\n        fs = mafn(k,zs,u1,u2) # returns array of shape (nks,nzs)\n        depth = 1-fs\n    else:\n        xmax = np.sqrt(1-b**2)\n        x = np.linspace(0,1,npts)*xmax[:,Nones]\n        y = b[:,None]\n        zs = np.sqrt(x**2 + y**2)\n        fs = mafn(k,zs.ravel(),u1,u2)\n        if not force_1d:\n            fs = fs.reshape(size(k),*zs.shape)\n        depth = 1-fs\n        \n    meandepth = np.squeeze(depth.mean(axis=depth.ndim-1))\n    \n    return meandepth", "response": "Calculates the eclipse depth of a single resource in a single resource tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the minimum inclination at which two bodies from two given sets eclipse aClass = > A1 A2 A3 A4 A", "response": "def minimum_inclination(P,M1,M2,R1,R2):\n    \"\"\"\n    Returns the minimum inclination at which two bodies from two given sets eclipse\n\n    Only counts systems not within each other's Roche radius\n    \n    :param P:\n        Orbital periods.\n\n    :param M1,M2,R1,R2:\n        Masses and radii of primary and secondary stars.  \n    \"\"\"\n    P,M1,M2,R1,R2 = (np.atleast_1d(P),\n                     np.atleast_1d(M1),\n                     np.atleast_1d(M2),\n                     np.atleast_1d(R1),\n                     np.atleast_1d(R2))\n    semimajors = semimajor(P,M1+M2)\n    rads = ((R1+R2)*RSUN/(semimajors*AU))\n    ok = (~np.isnan(rads) & ~withinroche(semimajors,M1,R1,M2,R2))\n    if ok.sum() == 0:\n        logging.error('P: {}'.format(P))\n        logging.error('M1: {}'.format(M1))\n        logging.error('M2: {}'.format(M2))\n        logging.error('R1: {}'.format(R1))\n        logging.error('R2: {}'.format(R2))\n        if np.all(withinroche(semimajors,M1,R1,M2,R2)):\n            raise AllWithinRocheError('All simulated systems within Roche lobe')\n        else:\n            raise EmptyPopulationError('no valid systems! (see above)')\n    mininc = np.arccos(rads[ok].max())*180/np.pi\n    return mininc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef a_over_Rs(P,R2,M2,M1=1,R1=1,planet=True):\n    if planet:\n        M2 *= REARTH/RSUN\n        R2 *= MEARTH/MSUN\n    return semimajor(P,M1+M2)*AU/(R1*RSUN)", "response": "Returns a for given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eclipse_pars(P,M1,M2,R1,R2,ecc=0,inc=90,w=0,sec=False):\n    a = semimajor(P,M1+M2)\n    if sec:\n        b = a*AU*np.cos(inc*np.pi/180)/(R1*RSUN) * (1-ecc**2)/(1 - ecc*np.sin(w*np.pi/180))\n        #aR = a*AU/(R2*RSUN) #I feel like this was to correct a bug, but this should not be.\n        #p0 = R1/R2 #why this also?\n    else:\n        b = a*AU*np.cos(inc*np.pi/180)/(R1*RSUN) * (1-ecc**2)/(1 + ecc*np.sin(w*np.pi/180))\n        #aR = a*AU/(R1*RSUN)\n        #p0 = R2/R1\n    p0 = R2/R1\n    aR = a*AU/(R1*RSUN)\n    return p0,b,aR", "response": "parses the eclipse parameters from P M1 M2 R1 R2 ecc inc w"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eclipse_tt(p0,b,aR,P=1,ecc=0,w=0,npts=100,u1=0.394,u2=0.261,conv=True,\n               cadence=1626./86400,frac=1,sec=False,pars0=None,tol=1e-4,width=3):\n    \"\"\"\n    Trapezoidal parameters for simulated orbit.\n    \n    All arguments passed to :func:`eclipse` except the following:\n\n    :param pars0: (optional)\n        Initial guess for least-sq optimization for trapezoid parameters.\n\n    :return dur,dep,slope:\n        Best-fit duration, depth, and T/tau for eclipse shape.\n    \n    \"\"\"\n    ts,fs = eclipse(p0=p0,b=b,aR=aR,P=P,ecc=ecc,w=w,npts=npts,u1=u1,u2=u2,\n                    conv=conv,cadence=cadence,frac=frac,sec=sec,tol=tol,width=width)\n    \n    #logging.debug('{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}'.format(p0,b,aR,P,ecc,w,xmax,npts,u1,u2,leastsq,conv,cadence,frac,sec,new))\n    #logging.debug('ts: {} fs: {}'.format(ts,fs))\n\n    if pars0 is None:\n        depth = 1 - fs.min()\n        duration = (fs < (1-0.01*depth)).sum()/float(len(fs)) * (ts[-1] - ts[0])\n        tc0 = ts[fs.argmin()]\n        pars0 = np.array([duration,depth,5.,tc0])\n    \n    dur,dep,slope,epoch = fit_traptransit(ts,fs,pars0)\n    return dur,dep,slope", "response": "Compute the duration of a simulated orbit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfit trapezoid model to provided ts fs", "response": "def fit_traptransit(ts,fs,p0):\n    \"\"\"\n    Fits trapezoid model to provided ts,fs\n    \"\"\"\n    pfit,success = leastsq(traptransit_resid,p0,args=(ts,fs))\n    if success not in [1,2,3,4]:\n        raise NoFitError\n    #logging.debug('success = {}'.format(success))\n    return pfit"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits trapezoidal model to provided ts fs dfs using MCMC.", "response": "def traptransit_MCMC(ts,fs,dfs=1e-5,nwalkers=200,nburn=300,niter=1000,\n                     threads=1,p0=[0.1,0.1,3,0],return_sampler=False,\n                     maxslope=MAXSLOPE):\n    \"\"\"\n    Fit trapezoidal model to provided ts, fs, [dfs] using MCMC.\n\n    Standard emcee usage.\n    \"\"\"\n    model = TraptransitModel(ts,fs,dfs,maxslope=maxslope)\n    sampler = emcee.EnsembleSampler(nwalkers,4,model,threads=threads)\n    T0 = p0[0]*(1+rand.normal(size=nwalkers)*0.1)\n    d0 = p0[1]*(1+rand.normal(size=nwalkers)*0.1)\n    slope0 = p0[2]*(1+rand.normal(size=nwalkers)*0.1)\n    ep0 = p0[3]+rand.normal(size=nwalkers)*0.0001\n\n    p0 = np.array([T0,d0,slope0,ep0]).T\n\n    pos, prob, state = sampler.run_mcmc(p0, nburn)\n    sampler.reset()\n    sampler.run_mcmc(pos, niter, rstate0=state)\n    if return_sampler:\n        return sampler\n    else:\n        return sampler.flatchain[:,0],sampler.flatchain[:,1],sampler.flatchain[:,2],sampler.flatchain[:,3]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backup(path, password_file=None):\n    vault = VaultLib(get_vault_password(password_file))\n    with open(path, 'r') as f:\n        encrypted_data = f.read()\n\n        # Normally we'd just try and catch the exception, but the\n        # exception raised here is not very specific (just\n        # `AnsibleError`), so this feels safer to avoid suppressing\n        # other things that might go wrong.\n        if vault.is_encrypted(encrypted_data):\n            decrypted_data = vault.decrypt(encrypted_data)\n\n            # Create atk vault files\n            atk_path = os.path.join(ATK_VAULT, path)\n            mkdir_p(atk_path)\n            # ... encrypted\n            with open(os.path.join(atk_path, 'encrypted'), 'wb') as f:\n                f.write(encrypted_data)\n            # ... hash\n            with open(os.path.join(atk_path, 'hash'), 'wb') as f:\n                f.write(hashlib.sha1(decrypted_data).hexdigest())\n\n            # Replace encrypted file with decrypted one\n            with open(path, 'wb') as f:\n                f.write(decrypted_data)", "response": "Backup a file with its decrypted counterpart storing the encrypted version and a hash of the file contents for later\nFormula retrieval."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef restore(path, password_file=None):\n    vault = VaultLib(get_vault_password(password_file))\n    atk_path = os.path.join(ATK_VAULT, path)\n\n    # Load stored data\n    with open(os.path.join(atk_path, 'encrypted'), 'rb') as f:\n        old_data = f.read()\n    with open(os.path.join(atk_path, 'hash'), 'rb') as f:\n        old_hash = f.read()\n\n    # Load new data\n    with open(path, 'rb') as f:\n        new_data = f.read()\n        new_hash = hashlib.sha1(new_data).hexdigest()\n\n    # Determine whether to re-encrypt\n    if old_hash != new_hash:\n        new_data = vault.encrypt(new_data)\n    else:\n        new_data = old_data\n\n    # Update file\n    with open(path, 'wb') as f:\n        f.write(new_data)\n\n    # Clean atk vault\n    os.remove(os.path.join(atk_path, 'encrypted'))\n    os.remove(os.path.join(atk_path, 'hash'))", "response": "Restores a file from the atk vault to its original location"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Rsky(self):\n        r = (self.orbpop.Rsky/self.distance)\n        return r.to('arcsec',equivalencies=u.dimensionless_angles())", "response": "Returns the arcsec of the sky of the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends stars from another StarPopulation in place.", "response": "def append(self, other):\n        \"\"\"Appends stars from another StarPopulations, in place.\n\n        :param other:\n            Another :class:`StarPopulation`; must have same columns as ``self``.\n\n        \"\"\"\n        if not isinstance(other,StarPopulation):\n            raise TypeError('Only StarPopulation objects can be appended to a StarPopulation.')\n        if not np.all(self.stars.columns == other.stars.columns):\n            raise ValueError('Two populations must have same columns to combine them.')\n\n        if len(self.constraints) > 0:\n            logging.warning('All constraints are cleared when appending another population.')\n\n        self.stars = pd.concat((self.stars, other.stars))\n\n        if self.orbpop is not None and other.orbpop is not None:\n            self.orbpop = self.orbpop + other.orbpop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bands(self):\n        bands = []\n        for c in self.stars.columns:\n            if re.search('_mag',c):\n                bands.append(c)\n        return bands", "response": "Returns a list of all bands in the StarPopulation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef distance(self,value):\n        self.stars['distance'] = value.to('pc').value\n\n        old_distmod = self.stars['distmod'].copy()\n        new_distmod = distancemodulus(self.stars['distance'])\n\n        for m in self.bands:\n            self.stars[m] += new_distmod - old_distmod\n\n        self.stars['distmod'] = new_distmod\n\n        logging.warning('Setting the distance manually may have screwed up your constraints.  Re-apply constraints as necessary.')", "response": "Set the distance of the set of bands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distok(self):\n        ok = np.ones(len(self.stars)).astype(bool)\n        for name in self.constraints:\n            c = self.constraints[name]\n            if c.name not in self.distribution_skip:\n                ok &= c.ok\n        return ok", "response": "Boolean array showing which stars pass all distribution constraints."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef countok(self):\n        ok = np.ones(len(self.stars)).astype(bool)\n        for name in self.constraints:\n            c = self.constraints[name]\n            if c.name not in self.selectfrac_skip:\n                ok &= c.ok\n        return ok", "response": "A boolean array showing which stars pass all count constraints."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a 2d density histogram of two given properties.", "response": "def prophist2d(self,propx,propy, mask=None,\n                   logx=False,logy=False,\n                   fig=None,selected=False,**kwargs):\n        \"\"\"Makes a 2d density histogram of two given properties\n\n        :param propx,propy:\n            Names of properties to histogram.  Must be names of columns\n            in ``self.stars`` table.\n\n        :param mask: (optional)\n            Boolean mask (``True`` is good) to say which indices to plot.\n            Must be same length as ``self.stars``.\n\n        :param logx,logy: (optional)\n            Whether to plot the log10 of x and/or y properties.\n\n        :param fig: (optional)\n            Argument passed to :func:`plotutils.setfig`.\n\n        :param selected: (optional)\n            If ``True``, then only the \"selected\" stars (that is, stars\n            obeying all distribution constraints attached to this object)\n            will be plotted.  In this case, ``mask`` will be ignored.\n\n        :param kwargs:\n            Additional keyword arguments passed to :func:`plotutils.plot2dhist`.\n\n        \"\"\"\n\n        if mask is not None:\n            inds = np.where(mask)[0]\n        else:\n            if selected:\n                inds = self.selected.index\n            else:\n                inds = self.stars.index\n\n        if selected:\n            xvals = self.selected[propx].iloc[inds].values\n            yvals = self.selected[propy].iloc[inds].values\n        else:\n            if mask is None:\n                mask = np.ones_like(self.stars.index)\n            xvals = self.stars[mask][propx].values\n            yvals = self.stars[mask][propy].values\n\n        #forward-hack for EclipsePopulations...\n        #TODO: reorganize.\n        if propx=='depth' and hasattr(self,'depth'):\n            xvals = self.depth.iloc[inds].values\n        if propy=='depth' and hasattr(self,'depth'):\n            yvals = self.depth.iloc[inds].values\n\n        if logx:\n            xvals = np.log10(xvals)\n        if logy:\n            yvals = np.log10(yvals)\n\n        plot2dhist(xvals,yvals,fig=fig,**kwargs)\n        plt.xlabel(propx)\n        plt.ylabel(propy)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prophist(self,prop,fig=None,log=False, mask=None,\n                 selected=False,**kwargs):\n        \"\"\"Plots a 1-d histogram of desired property.\n\n        :param prop:\n            Name of property to plot.  Must be column of ``self.stars``.\n\n        :param fig: (optional)\n            Argument for :func:`plotutils.setfig`\n\n        :param log: (optional)\n            Whether to plot the histogram of log10 of the property.\n\n        :param mask: (optional)\n            Boolean array (length of ``self.stars``) to say\n            which indices to plot (``True`` is good).\n\n        :param selected: (optional)\n            If ``True``, then only the \"selected\" stars (that is, stars\n            obeying all distribution constraints attached to this object)\n            will be plotted.  In this case, ``mask`` will be ignored.\n\n        :param **kwargs:\n            Additional keyword arguments passed to :func:`plt.hist`.\n\n        \"\"\"\n\n        setfig(fig)\n\n        inds = None\n        if mask is not None:\n            inds = np.where(mask)[0]\n        elif inds is None:\n            if selected:\n                #inds = np.arange(len(self.selected))\n                inds = self.selected.index\n            else:\n                #inds = np.arange(len(self.stars))\n                inds = self.stars.index\n\n        if selected:\n            vals = self.selected[prop].values#.iloc[inds] #invalidates mask?\n        else:\n            vals = self.stars[prop].iloc[inds].values\n\n        if prop=='depth' and hasattr(self,'depth'):\n            vals *= self.dilution_factor[inds]\n\n        if log:\n            h = plt.hist(np.log10(vals),**kwargs)\n        else:\n            h = plt.hist(vals,**kwargs)\n\n        plt.xlabel(prop)", "response": "Plots a 1 - d histogram of the desired property."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning information about effect of constraints on the population.", "response": "def constraint_stats(self,primarylist=None):\n        \"\"\"Returns information about effect of constraints on population.\n\n        :param primarylist:\n           List of constraint names that you want specific information on\n           (i.e., not blended within \"multiple constraints\".)\n\n        :return:\n           ``dict`` of what percentage of population is ruled out by\n           each  constraint, including a \"multiple constraints\" entry.\n        \"\"\"\n        if primarylist is None:\n            primarylist = []\n        n = len(self.stars)\n        primaryOK = np.ones(n).astype(bool)\n        tot_reject = np.zeros(n)\n\n        for name in self.constraints:\n            if name in self.selectfrac_skip:\n                continue\n            c = self.constraints[name]\n            if name in primarylist:\n                primaryOK &= c.ok\n            tot_reject += ~c.ok\n        primary_rejected = ~primaryOK\n        secondary_rejected = tot_reject - primary_rejected\n        lone_reject = {}\n        for name in self.constraints:\n            if name in primarylist or name in self.selectfrac_skip:\n                continue\n            c = self.constraints[name]\n            lone_reject[name] = ((secondary_rejected==1) & (~primary_rejected) & (~c.ok)).sum()/float(n)\n        mult_rejected = (secondary_rejected > 1) & (~primary_rejected)\n        not_rejected = ~(tot_reject.astype(bool))\n        primary_reject_pct = primary_rejected.sum()/float(n)\n        mult_reject_pct = mult_rejected.sum()/float(n)\n        not_reject_pct = not_rejected.sum()/float(n)\n        tot = 0\n\n        results = {}\n        results['pri'] = primary_reject_pct\n        tot += primary_reject_pct\n        for name in lone_reject:\n            results[name] = lone_reject[name]\n            tot += lone_reject[name]\n        results['multiple constraints'] = mult_reject_pct\n        tot += mult_reject_pct\n        results['remaining'] = not_reject_pct\n        tot += not_reject_pct\n\n        if tot != 1:\n            logging.warning('total adds up to: %.2f (%s)' % (tot,self.model))\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef constraint_piechart(self,primarylist=None,\n                            fig=None,title='',colordict=None,\n                            legend=True,nolabels=False):\n        \"\"\"Makes piechart illustrating constraints on population\n\n        :param primarylist: (optional)\n            List of most import constraints to show (see\n            :func:`StarPopulation.constraint_stats`)\n\n        :param fig: (optional)\n            Passed to :func:`plotutils.setfig`.\n\n        :param title: (optional)\n            Title for pie chart\n\n        :param colordict: (optional)\n            Dictionary describing colors (keys are constraint names).\n\n        :param legend: (optional)\n            ``bool`` indicating whether to display a legend.\n\n        :param nolabels: (optional)\n            If ``True``, then leave out legend labels.\n\n        \"\"\"\n\n        setfig(fig,figsize=(6,6))\n        stats = self.constraint_stats(primarylist=primarylist)\n        if primarylist is None:\n            primarylist = []\n        if len(primarylist)==1:\n            primaryname = primarylist[0]\n        else:\n            primaryname = ''\n            for name in primarylist:\n                primaryname += '%s,' % name\n            primaryname = primaryname[:-1]\n        fracs = []\n        labels = []\n        explode = []\n        colors = []\n        fracs.append(stats['remaining']*100)\n        labels.append('remaining')\n        explode.append(0.05)\n        colors.append('b')\n        if 'pri' in stats and stats['pri']>=0.005:\n            fracs.append(stats['pri']*100)\n            labels.append(primaryname)\n            explode.append(0)\n            if colordict is not None:\n                colors.append(colordict[primaryname])\n        for name in stats:\n            if name == 'pri' or \\\n                    name == 'multiple constraints' or \\\n                    name == 'remaining':\n                continue\n\n            fracs.append(stats[name]*100)\n            labels.append(name)\n            explode.append(0)\n            if colordict is not None:\n                colors.append(colordict[name])\n\n        if stats['multiple constraints'] >= 0.005:\n            fracs.append(stats['multiple constraints']*100)\n            labels.append('multiple constraints')\n            explode.append(0)\n            colors.append('w')\n\n        autopct = '%1.1f%%'\n\n        if nolabels:\n            labels = None\n        if legend:\n            legendlabels = []\n            for i,l in enumerate(labels):\n                legendlabels.append('%s (%.1f%%)' % (l,fracs[i]))\n            labels = None\n            autopct = ''\n        if colordict is None:\n            plt.pie(fracs,labels=labels,autopct=autopct,explode=explode)\n        else:\n            plt.pie(fracs,labels=labels,autopct=autopct,explode=explode,\n                    colors=colors)\n        if legend:\n            plt.legend(legendlabels,bbox_to_anchor=(-0.05,0),\n                       loc='lower left',prop={'size':10})\n        plt.title(title)", "response": "Makes a piechart showing the constraints on the most import constraints."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a constraint dictionary that can be used to create the current population.", "response": "def constraints(self):\n        \"\"\"\n        Constraints applied to the population.\n        \"\"\"\n        try:\n            return self._constraints\n        except AttributeError:\n            self._constraints = ConstraintDict()\n            return self._constraints"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a constraint dictionary that can be used to filter the population.", "response": "def hidden_constraints(self):\n        \"\"\"\n        Constraints applied to the population, but temporarily removed.\n        \"\"\"\n        try:\n            return self._hidden_constraints\n        except AttributeError:\n            self._hidden_constraints = ConstraintDict()\n            return self._hidden_constraints"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply a constraint to the population.", "response": "def apply_constraint(self,constraint,selectfrac_skip=False,\n                         distribution_skip=False,overwrite=False):\n        \"\"\"Apply a constraint to the population\n\n        :param constraint:\n            Constraint to apply.\n        :type constraint:\n            :class:`Constraint`\n\n        :param selectfrac_skip: (optional)\n            If ``True``, then this constraint will not be considered\n            towards diminishing the\n        \"\"\"\n        #grab properties\n        constraints = self.constraints\n        my_selectfrac_skip = self.selectfrac_skip\n        my_distribution_skip = self.distribution_skip\n\n        if constraint.name in constraints and not overwrite:\n            logging.warning('constraint already applied: {}'.format(constraint.name))\n            return\n        constraints[constraint.name] = constraint\n        if selectfrac_skip:\n            my_selectfrac_skip.append(constraint.name)\n        if distribution_skip:\n            my_distribution_skip.append(constraint.name)\n\n        #forward-looking for EclipsePopulation\n        if hasattr(self, '_make_kde'):\n            self._make_kde()\n\n        self.constraints = constraints\n        self.selectfrac_skip = my_selectfrac_skip\n        self.distribution_skip = my_distribution_skip"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace a constraint that has been removed from the set of hidden constraints.", "response": "def replace_constraint(self,name,selectfrac_skip=False,distribution_skip=False):\n        \"\"\"\n        Re-apply constraint that had been removed\n\n        :param name:\n            Name of constraint to replace\n\n        :param selectfrac_skip,distribution_skip: (optional)\n            Same as :func:`StarPopulation.apply_constraint`\n\n        \"\"\"\n        hidden_constraints = self.hidden_constraints\n        if name in hidden_constraints:\n            c = hidden_constraints[name]\n            self.apply_constraint(c,selectfrac_skip=selectfrac_skip,\n                                  distribution_skip=distribution_skip)\n            del hidden_constraints[name]\n        else:\n            logging.warning('Constraint {} not available for replacement.'.format(name))\n\n        self.hidden_constraints = hidden_constraints"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_constraint(self,name):\n        constraints = self.constraints\n        hidden_constraints = self.hidden_constraints\n        my_distribution_skip = self.distribution_skip\n        my_selectfrac_skip = self.selectfrac_skip\n\n        if name in constraints:\n            hidden_constraints[name] = constraints[name]\n            del constraints[name]\n            if name in self.distribution_skip:\n                my_distribution_skip.remove(name)\n            if name in self.selectfrac_skip:\n                my_selectfrac_skip.remove(name)\n            #self._apply_all_constraints()\n        else:\n            logging.warning('Constraint {} does not exist.'.format(name))\n\n        self.constraints = constraints\n        self.hidden_constraints = hidden_constraints\n        self.selectfrac_skip = my_selectfrac_skip\n        self.distribution_skip = my_distribution_skip", "response": "Removes a constraint from the set of constraints and the set of hidden constraints."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying constraint that constrains a property.", "response": "def constrain_property(self,prop,lo=-np.inf,hi=np.inf,\n                           measurement=None,thresh=3,\n                           selectfrac_skip=False,distribution_skip=False):\n        \"\"\"Apply constraint that constrains property.\n\n        :param prop:\n            Name of property.  Must be column in ``self.stars``.\n        :type prop:\n            ``str``\n\n        :param lo,hi: (optional)\n            Low and high allowed values for ``prop``.  Defaults\n            to ``-np.inf`` and ``np.inf`` to allow for defining\n            only lower or upper limits if desired.\n\n        :param measurement: (optional)\n            Value and error of measurement in form ``(value, error)``.\n\n        :param thresh: (optional)\n            Number of \"sigma\" to allow for measurement constraint.\n\n        :param selectfrac_skip,distribution_skip:\n            Passed to :func:`StarPopulation.apply_constraint`.\n\n        \"\"\"\n        if prop in self.constraints:\n            logging.info('re-doing {} constraint'.format(prop))\n            self.remove_constraint(prop)\n        if measurement is not None:\n            val,dval = measurement\n            self.apply_constraint(MeasurementConstraint(getattr(self.stars,prop),\n                                                        val,dval,name=prop,\n                                                        thresh=thresh),\n                                  selectfrac_skip=selectfrac_skip,\n                                  distribution_skip=distribution_skip)\n        else:\n            self.apply_constraint(RangeConstraint(getattr(self.stars,prop),\n                                                  lo=lo,hi=hi,name=prop),\n                                  selectfrac_skip=selectfrac_skip,\n                                  distribution_skip=distribution_skip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_trend_constraint(self, limit, dt, distribution_skip=False,\n                               **kwargs):\n        \"\"\"\n        Constrains change in RV to be less than limit over time dt.\n\n        Only works if ``dRV`` and ``Plong`` attributes are defined\n        for population.\n\n        :param limit:\n            Radial velocity limit on trend.  Must be\n            :class:`astropy.units.Quantity` object, or\n            else interpreted as m/s.\n\n        :param dt:\n            Time baseline of RV observations.  Must be\n            :class:`astropy.units.Quantity` object; else\n            interpreted as days.\n\n        :param distribution_skip:\n            This is by default ``True``.  *To be honest, I'm not\n            exactly sure why.  Might be important, might not\n            (don't remember).*\n\n        :param **kwargs:\n            Additional keyword arguments passed to\n            :func:`StarPopulation.apply_constraint`.\n\n        \"\"\"\n        if type(limit) != Quantity:\n            limit = limit * u.m/u.s\n        if type(dt) != Quantity:\n            dt = dt * u.day\n\n        dRVs = np.absolute(self.dRV(dt))\n        c1 = UpperLimit(dRVs, limit)\n        c2 = LowerLimit(self.Plong, dt*4)\n\n        self.apply_constraint(JointConstraintOr(c1,c2,name='RV monitoring',\n                                                Ps=self.Plong,dRVs=dRVs),\n                              distribution_skip=distribution_skip, **kwargs)", "response": "Applies trend constraint to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_cc(self, cc, distribution_skip=False,\n                 **kwargs):\n        \"\"\"\n        Apply contrast-curve constraint to population.\n\n        Only works if object has ``Rsky``, ``dmag`` attributes\n\n        :param cc:\n            Contrast curve.\n        :type cc:\n            :class:`ContrastCurveConstraint`\n\n        :param distribution_skip:\n            This is by default ``True``.  *To be honest, I'm not\n            exactly sure why.  Might be important, might not\n            (don't remember).*\n\n        :param **kwargs:\n            Additional keyword arguments passed to\n            :func:`StarPopulation.apply_constraint`.\n\n        \"\"\"\n        rs = self.Rsky.to('arcsec').value\n        dmags = self.dmag(cc.band)\n        self.apply_constraint(ContrastCurveConstraint(rs,dmags,cc,name=cc.name),\n                              distribution_skip=distribution_skip, **kwargs)", "response": "Applies contrast - curve constraint to population."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_vcc(self, vcc, distribution_skip=False,\n                  **kwargs):\n        \"\"\"\n        Applies \"velocity contrast curve\" to population.\n\n        That is, the constraint that comes from not seeing two sets\n        of spectral lines in a high resolution spectrum.\n\n        Only works if population has ``dmag`` and ``RV`` attributes.\n\n        :param vcc:\n            Velocity contrast curve; dmag vs. delta-RV.\n        :type cc:\n            :class:`VelocityContrastCurveConstraint`\n\n        :param distribution_skip:\n            This is by default ``True``.  *To be honest, I'm not\n            exactly sure why.  Might be important, might not\n            (don't remember).*\n\n        :param **kwargs:\n            Additional keyword arguments passed to\n            :func:`StarPopulation.apply_constraint`.\n\n        \"\"\"\n        rvs = self.RV.value\n        dmags = self.dmag(vcc.band)\n        self.apply_constraint(VelocityContrastCurveConstraint(rvs,dmags,vcc,\n                                                              name='secondary spectrum'),\n                              distribution_skip=distribution_skip, **kwargs)", "response": "Applies velocity contrast curve to the current population."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a constraint that rejects everything with Rsky > maxrad.", "response": "def set_maxrad(self,maxrad, distribution_skip=True):\n        \"\"\"\n        Adds a constraint that rejects everything with Rsky > maxrad\n\n        Requires ``Rsky`` attribute, which should always have units.\n\n        :param maxrad:\n            The maximum angular value of Rsky.\n        :type maxrad:\n            :class:`astropy.units.Quantity`\n\n        :param distribution_skip:\n            This is by default ``True``.  *To be honest, I'm not\n            exactly sure why.  Might be important, might not\n            (don't remember).*\n\n        \"\"\"\n        self.maxrad = maxrad\n        self.apply_constraint(UpperLimit(self.Rsky,maxrad,\n                                         name='Max Rsky'),\n                              overwrite=True,\n                              distribution_skip=distribution_skip)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave a new object to HDF5 file.", "response": "def save_hdf(self,filename,path='',properties=None,\n                 overwrite=False, append=False):\n        \"\"\"Saves to HDF5 file.\n\n        Subclasses should be sure to define\n        ``_properties`` attribute to ensure that all\n        correct attributes get saved.  Load a saved population\n        with :func:`StarPopulation.load_hdf`.\n\n        Example usage::\n\n            >>> from vespa.stars import Raghavan_BinaryPopulation, StarPopulation\n            >>> pop = Raghavan_BinaryPopulation(1., n=1000)\n            >>> pop.save_hdf('test.h5')\n            >>> pop2 = StarPopulation.load_hdf('test.h5')\n            >>> pop == pop2\n                True\n            >>> pop3 = Ragahavan_BinaryPopulation.load_hdf('test.h5')\n            >>> pop3 == pop2\n                True\n\n\n        :param filename:\n            Name of HDF file.\n\n        :param path: (optional)\n            Path within HDF file to save object.\n\n        :param properties: (optional)\n            Names of any properties (in addition to\n            those defined in ``_properties`` attribute)\n            that you wish to save.  (This is an old\n            keyword, and should probably be removed.\n            Feel free to ignore it.)\n\n        :param overwrite: (optional)\n            Whether to overwrite file if it already\n            exists.  If ``True``, then any existing file\n            will be deleted before object is saved.  Use\n            ``append`` if you don't wish this to happen.\n\n        :param append: (optional)\n            If ``True``, then if the file exists,\n            then only the particular path in the file\n            will get written/overwritten.  If ``False`` and both\n            file and path exist, then an ``IOError`` will\n            be raised.  If ``False`` and file exists but not\n            path, then no error will be raised.\n\n        \"\"\"\n        if os.path.exists(filename):\n            with pd.HDFStore(filename) as store:\n                if path in store:\n                    if overwrite:\n                        os.remove(filename)\n                    elif not append:\n                        raise IOError('{} in {} exists.  '.format(path,filename) +\n                                     'Set either overwrite or append option.')\n\n        if properties is None:\n            properties = {}\n\n        for prop in self._properties:\n            properties[prop] = getattr(self, prop)\n\n        self.stars.to_hdf(filename,'{}/stars'.format(path))\n        self.constraint_df.to_hdf(filename,'{}/constraints'.format(path))\n\n        if self.orbpop is not None:\n            self.orbpop.save_hdf(filename, path=path+'/orbpop')\n\n        with pd.HDFStore(filename) as store:\n            attrs = store.get_storer('{}/stars'.format(path)).attrs\n            attrs.selectfrac_skip = self.selectfrac_skip\n            attrs.distribution_skip = self.distribution_skip\n            attrs.name = self.name\n            attrs.poptype = type(self)\n            attrs.properties = properties"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_hdf(cls, filename, path=''):\n        stars = pd.read_hdf(filename,path+'/stars')\n        constraint_df = pd.read_hdf(filename,path+'/constraints')\n\n        with pd.HDFStore(filename) as store:\n            has_orbpop = '{}/orbpop/df'.format(path) in store\n            has_triple_orbpop = '{}/orbpop/long/df'.format(path) in store\n            attrs = store.get_storer('{}/stars'.format(path)).attrs\n\n            poptype = attrs.poptype\n            new = poptype()\n\n            #if poptype != type(self):\n            #    raise TypeError('Saved population is {}.  Please instantiate proper class before loading.'.format(poptype))\n\n\n            distribution_skip = attrs.distribution_skip\n            selectfrac_skip = attrs.selectfrac_skip\n            name = attrs.name\n\n            for kw,val in attrs.properties.items():\n                setattr(new, kw, val)\n\n        #load orbpop if there\n        orbpop = None\n        if has_orbpop:\n            orbpop = OrbitPopulation.load_hdf(filename, path=path+'/orbpop')\n        elif has_triple_orbpop:\n            orbpop = TripleOrbitPopulation.load_hdf(filename, path=path+'/orbpop')\n\n        new.stars = stars\n        new.orbpop = orbpop\n\n\n        for n in constraint_df.columns:\n            mask = np.array(constraint_df[n])\n            c = Constraint(mask,name=n)\n            sel_skip = n in selectfrac_skip\n            dist_skip = n in distribution_skip\n            new.apply_constraint(c,selectfrac_skip=sel_skip,\n                                  distribution_skip=dist_skip)\n\n        return new", "response": "Loads StarPopulation from. h5 file containing stars constraint_df and orbpop."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef binary_fraction(self,query='mass_A >= 0'):\n        subdf = self.stars.query(query)\n        nbinaries = (subdf['mass_B'] > 0).sum()\n        frac = nbinaries/len(subdf)\n        return frac, frac/np.sqrt(nbinaries)", "response": "Returns the binary fraction of stars passing given query"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rsky_distribution(self,rmax=None,smooth=0.1,nbins=100):\n        if rmax is None:\n            if hasattr(self,'maxrad'):\n                rmax = self.maxrad\n            else:\n                rmax = np.percentile(self.Rsky,99)\n        dist = dists.Hist_Distribution(self.Rsky.value,bins=nbins,maxval=rmax,smooth=smooth)\n        return dist", "response": "Returns a : class : ~simpledists. Hist_Distribution describing the distribution of projected separations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rsky_lhood(self,rsky,**kwargs):\n        dist = self.rsky_distribution(**kwargs)\n        return dist(rsky)", "response": "Evaluates the Rsky likelihood at provided position ( s )."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that generates a new binary population.", "response": "def generate(self, M, age=9.6, feh=0.0,\n                 ichrone='mist', n=1e4, bands=None, **kwargs):\n        \"\"\"\n        Function that generates population.\n\n        Called by ``__init__`` if ``M`` is passed.\n        \"\"\"\n        ichrone = get_ichrone(ichrone, bands=bands)\n        if np.size(M) > 1:\n            n = np.size(M)\n        else:\n            n = int(n)\n        M2 = M * self.q_fn(n, qmin=np.maximum(self.qmin,self.minmass/M))\n\n        P = self.P_fn(n)\n        ecc = self.ecc_fn(n,P)\n\n        mass = np.ascontiguousarray(np.ones(n)*M)\n        mass2 = np.ascontiguousarray(M2)\n        age = np.ascontiguousarray(age)\n        feh = np.ascontiguousarray(feh)\n        pri = ichrone(mass, age, feh, return_df=True, bands=bands)\n        sec = ichrone(mass2, age, feh, return_df=True, bands=bands)\n\n        BinaryPopulation.__init__(self, primary=pri, secondary=sec,\n                                  period=P, ecc=ecc, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef A_brighter(self, band='g'):\n        mA = self.stars['{}_mag_A'.format(band)]\n        mBC = addmags(self.stars['{}_mag_B'.format(band)],\n                     self.stars['{}_mag_C'.format(band)])\n        return mA < mBC", "response": "Returns True if A is brighter than B + C."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the dRV of the given time period", "response": "def dRV(self, dt, band='g'):\n        \"\"\"Returns dRV of star A, if A is brighter than B+C, or of star B if B+C is brighter\n        \"\"\"\n        return (self.orbpop.dRV_1(dt)*self.A_brighter(band) +\n                self.orbpop.dRV_2(dt)*self.BC_brighter(band))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef triple_fraction(self,query='mass_A > 0', unc=False):\n        subdf = self.stars.query(query)\n        ntriples = ((subdf['mass_B'] > 0) & (subdf['mass_C'] > 0)).sum()\n        frac = ntriples/len(subdf)\n        if unc:\n            return frac, frac/np.sqrt(ntriples)\n        else:\n            return frac", "response": "Return the triple fraction of stars following given query"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of properties that are used to store the starmodel in the database.", "response": "def starmodel_props(self):\n        \"\"\"Default mag_err is 0.05, arbitrarily\n        \"\"\"\n        props = {}\n        mags = self.mags\n        mag_errs = self.mag_errs\n        for b in mags.keys():\n            if np.size(mags[b])==2:\n                props[b] = mags[b]\n            elif np.size(mags[b])==1:\n                mag = mags[b]\n                try:\n                    e_mag = mag_errs[b]\n                except:\n                    e_mag = 0.05\n                props[b] = (mag, e_mag)\n\n        if self.Teff is not None:\n            props['Teff'] = self.Teff\n        if self.logg is not None:\n            props['logg'] = self.logg\n        if self.feh is not None:\n            props['feh'] = self.feh\n\n        return props"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a new set of stellar properties for a single - mass population.", "response": "def generate(self, mA=1, age=9.6, feh=0.0, n=1e5, ichrone='mist',\n                 orbpop=None, bands=None, **kwargs):\n        \"\"\"\n        Generates population.\n\n        Called if :class:`MultipleStarPopulation` is initialized without\n        providing ``stars``, and if ``mA`` is provided.\n\n        \"\"\"\n        ichrone = get_ichrone(ichrone, bands=bands)\n\n        n = int(n)\n        #star with m1 orbits (m2+m3).  So mA (most massive)\n        # will correspond to either m1 or m2.\n        m1, m2, m3 = self.multmass_fn(mA, f_binary=self.f_binary,\n                                      f_triple=self.f_triple,\n                                      qmin=self.qmin, minmass=self.minmass,\n                                      n=n)\n        #reset n if need be\n        n = len(m1)\n\n        feh = np.ascontiguousarray(np.atleast_1d(feh))\n        age = np.ascontiguousarray(age)\n\n        #generate stellar properties\n        primary = ichrone(np.ascontiguousarray(m1), age, feh,\n                          bands=bands)\n        secondary = ichrone(np.ascontiguousarray(m2),age,feh,\n                            bands=bands)\n        tertiary = ichrone(np.ascontiguousarray(m3),age,feh,\n                           bands=bands)\n\n        #clean up columns that become nan when called with mass=0\n        # Remember, we want mass=0 and mags=inf when something doesn't exist\n        no_secondary = (m2==0)\n        no_tertiary = (m3==0)\n        for c in secondary.columns: #\n            if re.search('_mag',c):\n                secondary[c][no_secondary] = np.inf\n                tertiary[c][no_tertiary] = np.inf\n        secondary['mass'][no_secondary] = 0\n        tertiary['mass'][no_tertiary] = 0\n\n        if kwargs['period_short'] is None:\n            if kwargs['period_long'] is None:\n                period_1 = self.period_long_fn(n)\n                period_2 = self.period_short_fn(n)\n                kwargs['period_short'] = np.minimum(period_1, period_2)\n                kwargs['period_long'] = np.maximum(period_1, period_2)\n            else:\n                kwargs['period_short'] = self.period_short_fn(n)\n\n                #correct any short periods that are longer than period_long\n                bad = kwargs['period_short'] > kwargs['period_long']\n                n_bad = bad.sum()\n                good_inds = np.where(~bad)[0]\n                inds = np.random.randint(len(good_inds),size=n_bad)\n                kwargs['period_short'][bad] = \\\n                    kwargs['period_short'][good_inds[inds]]\n        else:\n            if kwargs['period_long'] is None:\n                kwargs['period_long'] = self.period_long_fn(n)\n\n                #correct any long periods that are shorter than period_short\n                bad = kwargs['period_long'] < kwargs['period_short']\n                n_bad = bad.sum()\n                good_inds = np.where(~bad)[0]\n                inds = np.random.randint(len(good_inds),size=n_bad)\n                kwargs['period_long'][bad] = \\\n                    kwargs['period_long'][good_inds[inds]]\n\n        if 'ecc_short' not in kwargs:\n            kwargs['ecc_short'] = self.ecc_fn(n, kwargs['period_short'])\n        if 'ecc_long' not in kwargs:\n            kwargs['ecc_long'] = self.ecc_fn(n, kwargs['period_long'])\n\n        TriplePopulation.__init__(self, primary=primary,\n                                  secondary=secondary, tertiary=tertiary,\n                                  orbpop=orbpop, **kwargs)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the magnitude difference between primary star and BG stars", "response": "def dmag(self,band):\n        \"\"\"\n        Magnitude difference between primary star and BG stars\n\n        \"\"\"\n        if self.mags is None:\n            raise ValueError('dmag is not defined because primary mags are not defined for this population.')\n        return self.stars['{}_mag'.format(band)] - self.mags[band]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validator(ch):\n    global screen_needs_update\n    try:\n        if screen_needs_update:\n            curses.doupdate()\n            screen_needs_update = False\n        return ch\n    finally:\n        winlock.release()\n        sleep(0.01) # let receiveThread in if necessary\n        winlock.acquire()", "response": "Validate a single character."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resample(self, inds):\n        new = copy.deepcopy(self)\n        for arr in self.arrays:\n            x = getattr(new, arr)\n            setattr(new, arr, x[inds])\n        return new", "response": "Returns a copy of constraint with mask rearranged according to indices\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes a FPPCalculation from a. ini file.", "response": "def from_ini(cls, folder, ini_file='fpp.ini', ichrone='mist', recalc=False,\n                refit_trap=False, **kwargs):\n        \"\"\"\n        To enable simple usage, initializes a FPPCalculation from a .ini file\n\n        By default, a file called ``fpp.ini`` will be looked for in the\n        current folder.  Also present must be a ``star.ini`` file that\n        contains the observed properties of the target star.\n\n        ``fpp.ini`` must be of the following form::\n\n            name = k2oi\n            ra = 11:30:14.510\n            dec = +07:35:18.21\n\n            period = 32.988 #days\n            rprs = 0.0534   #Rp/Rstar\n            photfile = lc_k2oi.csv\n\n            [constraints]\n            maxrad = 10 #exclusion radius [arcsec]\n            secthresh = 0.001 #maximum allowed secondary signal depth\n\n            #This variable defines contrast curves\n            #ccfiles = Keck_J.cc, Lick_J.cc\n\n        Photfile must be a text file with columns ``(days_from_midtransit,\n        flux, flux_err)``.  Both whitespace- and comma-delimited\n        will be tried, using ``np.loadtxt``.  Photfile need not be there\n        if there is a pickled :class:`TransitSignal` saved in the same\n        directory as ``ini_file``, named ``trsig.pkl`` (or another name\n        as defined by ``trsig`` keyword in ``.ini`` file).\n\n        ``star.ini`` should look something like the following::\n\n            B = 15.005, 0.06\n            V = 13.496, 0.05\n            g = 14.223, 0.05\n            r = 12.858, 0.04\n            i = 11.661, 0.08\n            J = 9.763, 0.03\n            H = 9.135, 0.03\n            K = 8.899, 0.02\n            W1 = 8.769, 0.023\n            W2 = 8.668, 0.02\n            W3 = 8.552, 0.025\n            Kepler = 12.473\n\n            #Teff = 3503, 80\n            #feh = 0.09, 0.09\n            #logg = 4.89, 0.1\n\n        Any star properties can be defined; if errors are included\n        then they will be used in the :class:`isochrones.StarModel`\n        MCMC fit.\n        Spectroscopic parameters (``Teff, feh, logg``) are optional.\n        If included, then they will also be included in\n        :class:`isochrones.StarModel` fit.  A magnitude for the\n        band in which the transit signal is observed (e.g., ``Kepler``)\n        is required, though need not have associated uncertainty.\n\n\n        :param folder:\n            Folder to find configuration files.\n\n        :param ini_file:\n            Input configuration file.\n\n        :param star_ini_file:\n            Input config file for :class:`isochrones.StarModel` fits.\n\n        :param recalc:\n            Whether to re-calculate :class:`PopulationSet`, if a\n            ``popset.h5`` file is already present\n\n        :param **kwargs:\n            Keyword arguments passed to :class:`PopulationSet`.\n\n        Creates:\n\n            * ``trsig.pkl``: the pickled :class:`vespa.TransitSignal` object.\n            * ``starfield.h5``: the TRILEGAL field star simulation\n            * ``starmodel.h5``: the :class:`isochrones.StarModel` fit\n            * ``popset.h5``: the :class:`vespa.PopulationSet` object\n              representing the model population simulations.\n\n        Raises\n        ------\n        RuntimeError :\n            If single, double, and triple starmodels are\n            not computed, then raises with admonition to run\n            `starfit --all`.\n\n        AttributeError :\n            If `trsig.pkl` not present in folder, and\n            `photfile` is not defined in config file.\n\n        \"\"\"\n\n        # Check if all starmodel fits are done.\n        # If not, tell user to run 'starfit --all'\n        config = ConfigObj(os.path.join(folder, ini_file))\n\n        # Load required entries from ini_file\n        try:\n            name = config['name']\n            ra, dec = config['ra'], config['dec']\n            period = float(config['period'])\n            rprs = float(config['rprs'])\n        except KeyError as err:\n            raise KeyError('Missing required element of ini file: {}'.format(err))\n\n        try:\n            cadence = float(config['cadence'])\n        except KeyError:\n            logging.warning('Cadence not provided in fpp.ini; defaulting to Kepler cadence.')\n            logging.warning('If this is not a Kepler target, please set cadence (in days).')\n            cadence = 1626./86400 # Default to Kepler cadence\n\n        def fullpath(filename):\n            if os.path.isabs(filename):\n                return filename\n            else:\n                return os.path.join(folder, filename)\n\n        # Non-required entries with default values\n        popset_file = fullpath(config.get('popset', 'popset.h5'))\n        starfield_file = fullpath(config.get('starfield', 'starfield.h5'))\n        trsig_file = fullpath(config.get('trsig', 'trsig.pkl'))\n\n        # Check for StarModel fits\n        starmodel_basename = config.get('starmodel_basename',\n                                        '{}_starmodel'.format(ichrone))\n        single_starmodel_file = os.path.join(folder,'{}_single.h5'.format(starmodel_basename))\n        binary_starmodel_file = os.path.join(folder,'{}_binary.h5'.format(starmodel_basename))\n        triple_starmodel_file = os.path.join(folder,'{}_triple.h5'.format(starmodel_basename))\n\n        try:\n            single_starmodel = StarModel.load_hdf(single_starmodel_file)\n            binary_starmodel = StarModel.load_hdf(binary_starmodel_file)\n            triple_starmodel = StarModel.load_hdf(triple_starmodel_file)\n        except Exception as e:\n            print(e)\n            raise RuntimeError('Cannot load StarModels.  ' +\n                               'Please run `starfit --all {}`.'.format(folder))\n\n        # Create (or load) TransitSignal\n        if os.path.exists(trsig_file):\n            logging.info('Loading transit signal from {}...'.format(trsig_file))\n            with open(trsig_file, 'rb') as f:\n                trsig = pickle.load(f)\n        else:\n            try:\n                photfile = fullpath(config['photfile'])\n            except KeyError:\n                raise AttributeError('If transit pickle file (trsig.pkl) ' +\n                                     'not present, \"photfile\" must be' +\n                                     'defined.')\n\n            trsig = TransitSignal.from_ascii(photfile, P=period, name=name)\n            if not trsig.hasMCMC or refit_trap:\n                logging.info('Fitting transitsignal with MCMC...')\n                trsig.MCMC()\n                trsig.save(trsig_file)\n\n        # Create (or load) PopulationSet\n        do_only = DEFAULT_MODELS\n        if os.path.exists(popset_file):\n            if recalc:\n                os.remove(popset_file)\n            else:\n                with pd.HDFStore(popset_file) as store:\n                    do_only = [m for m in DEFAULT_MODELS if m not in store]\n\n        # Check that properties of saved population match requested\n        try:\n            popset = PopulationSet.load_hdf(popset_file)\n            for pop in popset.poplist:\n                if pop.cadence != cadence:\n                    raise ValueError('Requested cadence ({}) '.format(cadence) +\n                                    'does not match stored {})! Set recalc=True.'.format(pop.cadence))\n        except:\n            raise\n\n        if do_only:\n            logging.info('Generating {} models for PopulationSet...'.format(do_only))\n        else:\n            logging.info('Populations ({}) already generated.'.format(DEFAULT_MODELS))\n\n        popset = PopulationSet(period=period, cadence=cadence,\n                               mags=single_starmodel.mags,\n                               ra=ra, dec=dec,\n                               trilegal_filename=starfield_file, # Maybe change parameter name?\n                               starmodel=single_starmodel,\n                               binary_starmodel=binary_starmodel,\n                               triple_starmodel=triple_starmodel,\n                               rprs=rprs, do_only=do_only,\n                               savefile=popset_file, **kwargs)\n\n\n        fpp = cls(trsig, popset, folder=folder)\n\n\n        #############\n        # Apply constraints\n\n        # Exclusion radius\n        maxrad = float(config['constraints']['maxrad'])\n        fpp.set_maxrad(maxrad)\n        if 'secthresh' in config['constraints']:\n            secthresh = float(config['constraints']['secthresh'])\n            if not np.isnan(secthresh):\n                fpp.apply_secthresh(secthresh)\n\n        # Odd-even constraint\n        diff = 3 * np.max(trsig.depthfit[1])\n        fpp.constrain_oddeven(diff)\n\n        #apply contrast curve constraints if present\n        if 'ccfiles' in config['constraints']:\n            ccfiles = config['constraints']['ccfiles']\n            if isinstance(ccfiles, string_types):\n                ccfiles = [ccfiles]\n            for ccfile in ccfiles:\n                if not os.path.isabs(ccfile):\n                    ccfile = os.path.join(folder, ccfile)\n                m = re.search('(\\w+)_(\\w+)\\.cc',os.path.basename(ccfile))\n                if not m:\n                    logging.warning('Invalid CC filename ({}); '.format(ccfile) +\n                                     'skipping.')\n                    continue\n                else:\n                    band = m.group(2)\n                    inst = m.group(1)\n                    name = '{} {}-band'.format(inst, band)\n                    cc = ContrastCurveFromFile(ccfile, band, name=name)\n                    fpp.apply_cc(cc)\n\n        #apply \"velocity contrast curve\" if present\n        if 'vcc' in config['constraints']:\n            dv = float(config['constraints']['vcc'][0])\n            dmag = float(config['constraints']['vcc'][1])\n            vcc = VelocityContrastCurve(dv, dmag)\n            fpp.apply_vcc(vcc)\n\n        return fpp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, overwrite=True):\n        self.save_popset(overwrite=overwrite)\n        self.save_signal()", "response": "Saves the current population set and transit signal to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading PopulationSet from folder . h5 and trsig. pkl must exist in folder . pkl must exist in folder . pkl must exist in folder.", "response": "def load(cls, folder):\n        \"\"\"\n        Loads PopulationSet from folder\n\n        ``popset.h5`` and ``trsig.pkl`` must exist in folder.\n\n        :param folder:\n            Folder from which to load.\n        \"\"\"\n        popset = PopulationSet.load_hdf(os.path.join(folder,'popset.h5'))\n        sigfile = os.path.join(folder,'trsig.pkl')\n        with open(sigfile, 'rb') as f:\n            trsig = pickle.load(f)\n        return cls(trsig, popset, folder=folder)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking likelihood fuzz plot for each model and writes a FPP summary figure and the TransitSignal figure.", "response": "def FPPplots(self, folder=None, format='png', tag=None, **kwargs):\n        \"\"\"\n        Make FPP diagnostic plots\n\n        Makes likelihood \"fuzz plot\" for each model, a FPP summary figure,\n        a plot of the :class:`TransitSignal`, and writes a ``results.txt``\n        file.\n\n        :param folder: (optional)\n            Destination folder for plots/``results.txt``.  Default\n            is ``self.folder``.\n\n        :param format: (optional)\n            Desired format of figures.  e.g. ``png``, ``pdf``...\n\n        :param tag: (optional)\n            If this is provided (string), then filenames will have\n            ``_[tag]`` appended to the filename, before the extension.\n\n        :param **kwargs:\n            Additional keyword arguments passed to :func:`PopulationSet.lhoodplots`.\n\n        \"\"\"\n        if folder is None:\n            folder = self.folder\n\n        self.write_results(folder=folder)\n        self.lhoodplots(folder=folder,figformat=format,tag=tag,**kwargs)\n        self.FPPsummary(folder=folder,saveplot=True,figformat=format,tag=tag)\n        self.plotsignal(folder=folder,saveplot=True,figformat=format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the current TransitSignal with the specified figure.", "response": "def plotsignal(self,fig=None,saveplot=True,folder=None,figformat='png',**kwargs):\n        \"\"\"\n        Plots TransitSignal\n\n        Calls :func:`TransitSignal.plot`, saves to provided folder.\n\n        :param fig: (optional)\n            Argument for :func:`plotutils.setfig`.\n\n        :param saveplot: (optional)\n            Whether to save figure.\n\n        :param folder: (optional)\n            Folder to which to save plot\n\n        :param figformat: (optional)\n            Desired format for figure.\n\n        :param **kwargs:\n            Additional keyword arguments passed to :func:`TransitSignal.plot`.\n        \"\"\"\n        if folder is None:\n            folder = self.folder\n\n        self.trsig.plot(plot_trap=True,fig=fig,**kwargs)\n        if saveplot:\n            plt.savefig('%s/signal.%s' % (folder,figformat))\n            plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites text file of calculation summary.", "response": "def write_results(self,folder=None, filename='results.txt', to_file=True):\n        \"\"\"\n        Writes text file of calculation summary.\n\n        :param folder: (optional)\n            Folder to which to write ``results.txt``.\n\n        :param filename:\n            Filename to write.  Default=``results.txt``.\n\n        :param to_file:\n            If True, then writes file.  Otherwise just return header, line.\n\n        :returns:\n            Header string, line\n        \"\"\"\n        if folder is None:\n            folder = self.folder\n        if to_file:\n            fout = open(os.path.join(folder,filename), 'w')\n        header = ''\n        for m in self.popset.shortmodelnames:\n            header += 'lhood_{0} L_{0} pr_{0} '.format(m)\n        header += 'fpV fp FPP'\n        if to_file:\n            fout.write('{} \\n'.format(header))\n        Ls = {}\n        Ltot = 0\n        lhoods = {}\n        for model in self.popset.modelnames:\n            lhoods[model] = self.lhood(model)\n            Ls[model] = self.prior(model)*lhoods[model]\n            Ltot += Ls[model]\n\n        line = ''\n        for model in self.popset.modelnames:\n            line += '%.2e %.2e %.2e ' % (lhoods[model], Ls[model], Ls[model]/Ltot)\n        line += '%.3g %.3f %.2e' % (self.fpV(),self.priorfactors['fp_specific'],self.FPP())\n\n        if to_file:\n            fout.write(line+'\\n')\n            fout.close()\n\n        return header, line"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the PopulationSet to a file.", "response": "def save_popset(self,filename='popset.h5',**kwargs):\n        \"\"\"Saves the PopulationSet\n\n        Calls :func:`PopulationSet.save_hdf`.\n        \"\"\"\n        self.popset.save_hdf(os.path.join(self.folder,filename))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_signal(self,filename=None):\n        if filename is None:\n            filename = os.path.join(self.folder,'trsig.pkl')\n        self.trsig.save(filename)", "response": "Saves the current TransitSignal to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning true if provenance of property is SPE or AST", "response": "def use_property(kepid, prop):\n    \"\"\"Returns true if provenance of property is SPE or AST\n    \"\"\"\n    try:\n        prov = kicu.DATA.ix[kepid, '{}_prov'.format(prop)]\n        return any([prov.startswith(s) for s in ['SPE', 'AST']])\n    except KeyError:\n        raise MissingStellarError('{} not in stellar table?'.format(kepid))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef star_config(koi, bands=['g','r','i','z','J','H','K'],\n                unc=dict(g=0.05, r=0.05, i=0.05, z=0.05,\n                         J=0.02, H=0.02, K=0.02), **kwargs):\n\n    \"\"\"returns star config object for given KOI\n    \"\"\"\n    folder = os.path.join(KOI_FPPDIR, ku.koiname(koi))\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    config = ConfigObj(os.path.join(folder,'star.ini'))\n\n    koi = ku.koiname(koi)\n\n    maxAV = koi_maxAV(koi)\n    config['maxAV'] = maxAV\n\n    mags = ku.KICmags(koi)\n    for band in bands:\n        if not np.isnan(mags[band]):\n            config[band] = (mags[band], unc[band])\n    config['Kepler'] = mags['Kepler']\n\n    kepid = KOIDATA.ix[koi,'kepid']\n\n    if use_property(kepid, 'teff'):\n        teff, e_teff = (kicu.DATA.ix[kepid, 'teff'],\n                          kicu.DATA.ix[kepid, 'teff_err1'])\n        if not any(np.isnan([teff, e_teff])):\n            config['Teff'] = (teff, e_teff)\n\n    if use_property(kepid, 'logg'):\n        logg, e_logg = (kicu.DATA.ix[kepid, 'logg'],\n                          kicu.DATA.ix[kepid, 'logg_err1'])\n        if not any(np.isnan([logg, e_logg])):\n            config['logg'] = (logg, e_logg)\n\n    if use_property(kepid, 'feh'):\n        feh, e_feh = (kicu.DATA.ix[kepid, 'feh'],\n                          kicu.DATA.ix[kepid, 'feh_err1'])\n        if not any(np.isnan([feh, e_feh])):\n            config['feh'] = (feh, e_feh)\n\n    for kw,val in kwargs.items():\n        config[kw] = val\n\n    return config", "response": "returns star config object for given KOI"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fpp_config(koi, **kwargs):\n    folder = os.path.join(KOI_FPPDIR, ku.koiname(koi))\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    config = ConfigObj(os.path.join(folder,'fpp.ini'))\n\n    koi = ku.koiname(koi)\n\n    rowefit = jrowe_fit(koi)\n\n    config['name'] = koi\n    ra,dec = ku.radec(koi)\n    config['ra'] = ra\n    config['dec'] = dec\n    config['rprs'] = rowefit.ix['RD1','val']\n    config['period'] = rowefit.ix['PE1', 'val']\n\n    config['starfield'] = kepler_starfield_file(koi)\n\n    for kw,val in kwargs.items():\n        config[kw] = val\n\n    config['constraints'] = {}\n    config['constraints']['maxrad'] = default_r_exclusion(koi)\n    try:\n        config['constraints']['secthresh'] = pipeline_weaksec(koi)\n    except NoWeakSecondaryError:\n        pass\n\n    return config", "response": "returns config object for given KOI\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies default secthresh & exclusion radius constraints", "response": "def apply_default_constraints(self):\n        \"\"\"Applies default secthresh & exclusion radius constraints\n        \"\"\"\n        try:\n            self.apply_secthresh(pipeline_weaksec(self.koi))\n        except NoWeakSecondaryError:\n            logging.warning('No secondary eclipse threshold set for {}'.format(self.koi))\n        self.set_maxrad(default_r_exclusion(self.koi))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the SHA for the original file that was changed in a diff part.", "response": "def get_old_sha(diff_part):\n    \"\"\"\n    Returns the SHA for the original file that was changed in a diff part.\n    \"\"\"\n    r = re.compile(r'index ([a-fA-F\\d]*)')\n    return r.search(diff_part).groups()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_old_filename(diff_part):\n    regexps = (\n        # e.g. \"+++ a/foo/bar\"\n        r'^--- a/(.*)',\n        # e.g. \"+++ /dev/null\"\n        r'^\\-\\-\\- (.*)',\n    )\n    for regexp in regexps:\n        r = re.compile(regexp, re.MULTILINE)\n        match = r.search(diff_part)\n        if match is not None:\n            return match.groups()[0]\n    raise MalformedGitDiff(\"No old filename in diff part found.  \"\n                           \"Examined diff part: {}\".format(diff_part))", "response": "Returns the filename for the original file that was changed in a diff part."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_new_filename(diff_part):\n    regexps = (\n        # e.g. \"+++ b/foo/bar\"\n        r'^\\+\\+\\+ b/(.*)',\n        # e.g. \"+++ /dev/null\"\n        r'^\\+\\+\\+ (.*)',\n    )\n    for regexp in regexps:\n        r = re.compile(regexp, re.MULTILINE)\n        match = r.search(diff_part)\n        if match is not None:\n            return match.groups()[0]\n    raise MalformedGitDiff(\"No new filename in diff part found.  \"\n                           \"Examined diff part: {}\".format(diff_part))", "response": "Returns the filename for the updated file in a diff part."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tuple of old content and new content.", "response": "def get_contents(diff_part):\n    \"\"\"\n    Returns a tuple of old content and new content.\n    \"\"\"\n    old_sha = get_old_sha(diff_part)\n    old_filename = get_old_filename(diff_part)\n    old_contents = get_old_contents(old_sha, old_filename)\n    new_filename = get_new_filename(diff_part)\n    new_contents = get_new_contents(new_filename)\n    return old_contents, new_contents"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the eclipses for the given set of masses and masses and masses.", "response": "def calculate_eclipses(M1s, M2s, R1s, R2s, mag1s, mag2s,\n                       u11s=0.394, u21s=0.296, u12s=0.394, u22s=0.296,\n                       Ps=None, period=None, logperkde=RAGHAVAN_LOGPERKDE,\n                       incs=None, eccs=None,\n                       mininc=None, calc_mininc=True,\n                       maxecc=0.97, ecc_fn=draw_eccs,\n                       band='Kepler',\n                       return_probability_only=False, return_indices=True,\n                       MAfn=None):\n    \"\"\"Returns random eclipse parameters for provided inputs\n\n\n    :param M1s, M2s, R1s, R2s, mag1s, mag2s: (array-like)\n        Primary and secondary properties (mass, radius, magnitude)\n\n    :param u11s, u21s, u12s, u22s: (optional)\n        Limb darkening parameters (u11 = u1 for star 1, u21 = u2 for star 1, etc.)\n\n    :param Ps: (array-like, optional)\n        Orbital periods; same size as ``M1s``, etc.\n        If only a single period is desired, use ``period``.\n\n    :param period: (optional)\n        Orbital period; use this keyword if only a single period is desired.\n\n    :param logperkde: (optional)\n        If neither ``Ps`` nor ``period`` is provided, then periods will be\n        randomly generated according to this log-period distribution.\n        Default is taken from the Raghavan (2010) period distribution.\n\n    :param incs, eccs: (optional)\n        Inclinations and eccentricities.  If not passed, they will be generated.\n        Eccentricities will be generated according to ``ecc_fn``; inclinations\n        will be randomly generated out to ``mininc``.\n\n    :param mininc: (optional)\n        Minimum inclination to generate.  Useful if you want to enhance\n        efficiency by only generating mostly eclipsing, instead of mostly\n        non-eclipsing systems.  If not provided and ``calc_mininc`` is\n        ``True``, then this will be calculated based on inputs.\n\n    :param calc_mininc: (optional)\n        Whether to calculate ``mininc`` based on inputs.  If truly isotropic\n        inclinations are desired, set this to ``False``.\n\n    :param maxecc: (optional)\n        Maximum eccentricity to generate.\n\n    :param ecc_fn: (callable, optional)\n        Orbital eccentricity generating function.  Must return ``n`` orbital\n        eccentricities generated according to provided period(s)::\n\n            eccs = ecc_fn(n,Ps)\n\n        Defaults to :func:`stars.utils.draw_eccs`.\n\n    :param band: (optional)\n        Photometric bandpass in which eclipse is observed.\n\n    :param return_probability_only: (optional)\n        If ``True``, then will return only the average eclipse probability\n        of population.\n\n    :param return_indices: (optional)\n        If ``True``, returns the indices of the original input arrays\n        that the output ``DataFrame`` corresponds to.  **This behavior\n        will/should be changed to just return a ``DataFrame`` of the same\n        length as inputs...**\n\n    :param MAfn: (optional)\n        :class:`transit_basic.MAInterpolationFunction` object.\n        If not passed, then one with default parameters will\n        be created.\n\n    :return:\n        * [``wany``: indices describing which of the original input\n          arrays the output ``DataFrame`` corresponds to.\n        * ``df``: ``DataFrame`` with the following columns:\n          ``[{band}_mag_tot, P, ecc, inc, w, dpri, dsec,\n             T14_pri, T23_pri, T14_sec, T23_sec, b_pri,\n             b_sec, {band}_mag_1, {band}_mag_2, fluxfrac_1,\n             fluxfrac_2, switched, u1_1, u2_1, u1_2, u2_2]``.\n             **N.B. that this will be shorter than your input arrays,\n             because not everything will eclipse; this behavior\n             will likely be changed in the future because it's confusing.**\n        * ``(prob, dprob)`` Eclipse probability with Poisson uncertainty\n\n    \"\"\"\n    if MAfn is None:\n        logging.warning('MAInterpolationFunction not passed, so generating one...')\n        MAfn = MAInterpolationFunction(nzs=200,nps=400,pmin=0.007,pmax=1/0.007)\n\n    M1s = np.atleast_1d(M1s)\n    M2s = np.atleast_1d(M2s)\n    R1s = np.atleast_1d(R1s)\n    R2s = np.atleast_1d(R2s)\n\n    nbad = (np.isnan(M1s) | np.isnan(M2s) | np.isnan(R1s) | np.isnan(R2s)).sum()\n    if nbad > 0:\n        logging.warning('{} M1s are nan'.format(np.isnan(M1s).sum()))\n        logging.warning('{} M2s are nan'.format(np.isnan(M2s).sum()))\n        logging.warning('{} R1s are nan'.format(np.isnan(R1s).sum()))\n        logging.warning('{} R2s are nan'.format(np.isnan(R2s).sum()))\n\n    mag1s = mag1s * np.ones_like(M1s)\n    mag2s = mag2s * np.ones_like(M1s)\n    u11s = u11s * np.ones_like(M1s)\n    u21s = u21s * np.ones_like(M1s)\n    u12s = u12s * np.ones_like(M1s)\n    u22s = u22s * np.ones_like(M1s)\n\n    n = np.size(M1s)\n\n    #a bit clunky here, but works.\n    simPs = False\n    if period:\n        Ps = np.ones(n)*period\n    else:\n        if Ps is None:\n            Ps = 10**(logperkde.rvs(n))\n            simPs = True\n    simeccs = False\n    if eccs is None:\n        if not simPs and period is not None:\n            eccs = ecc_fn(n,period,maxecc=maxecc)\n        else:\n            eccs = ecc_fn(n,Ps,maxecc=maxecc)\n        simeccs = True\n\n    bad_Ps = np.isnan(Ps)\n    if bad_Ps.sum()>0:\n        logging.warning('{} nan periods.  why?'.format(bad_Ps.sum()))\n    bad_eccs = np.isnan(eccs)\n    if bad_eccs.sum()>0:\n        logging.warning('{} nan eccentricities.  why?'.format(bad_eccs.sum()))\n\n    semimajors = semimajor(Ps, M1s+M2s)*AU #in AU\n\n    #check to see if there are simulated instances that are\n    # too close; i.e. periastron sends secondary within roche\n    # lobe of primary\n    tooclose = withinroche(semimajors*(1-eccs)/AU,M1s,R1s,M2s,R2s)\n    ntooclose = tooclose.sum()\n    tries = 0\n    maxtries=5\n    if simPs:\n        while ntooclose > 0:\n            lastntooclose=ntooclose\n            Ps[tooclose] = 10**(logperkde.rvs(ntooclose))\n            if simeccs:\n                eccs[tooclose] = draw_eccs(ntooclose,Ps[tooclose])\n            semimajors[tooclose] = semimajor(Ps[tooclose],M1s[tooclose]+M2s[tooclose])*AU\n            tooclose = withinroche(semimajors*(1-eccs)/AU,M1s,R1s,M2s,R2s)\n            ntooclose = tooclose.sum()\n            if ntooclose==lastntooclose:   #prevent infinite loop\n                tries += 1\n                if tries > maxtries:\n                    logging.info('{} binaries are \"too close\"; gave up trying to fix.'.format(ntooclose))\n                    break\n    else:\n        while ntooclose > 0:\n            lastntooclose=ntooclose\n            if simeccs:\n                eccs[tooclose] = draw_eccs(ntooclose,Ps[tooclose])\n            semimajors[tooclose] = semimajor(Ps[tooclose],M1s[tooclose]+M2s[tooclose])*AU\n            #wtooclose = where(semimajors*(1-eccs) < 2*(R1s+R2s)*RSUN)\n            tooclose = withinroche(semimajors*(1-eccs)/AU,M1s,R1s,M2s,R2s)\n            ntooclose = tooclose.sum()\n            if ntooclose==lastntooclose:   #prevent infinite loop\n                tries += 1\n                if tries > maxtries:\n                    logging.info('{} binaries are \"too close\"; gave up trying to fix.'.format(ntooclose))\n                    break\n\n    #randomize inclinations, either full range, or within restricted range\n    if mininc is None and calc_mininc:\n        mininc = minimum_inclination(Ps, M1s, M2s, R1s, R2s)\n\n    if incs is None:\n        if mininc is None:\n            incs = np.arccos(np.random.random(n)) #random inclinations in radians\n        else:\n            incs = np.arccos(np.random.random(n)*np.cos(mininc*np.pi/180))\n    if mininc:\n        prob = np.cos(mininc*np.pi/180)\n    else:\n        prob = 1\n\n    logging.debug('initial probability given mininc starting at {}'.format(prob))\n\n    ws = np.random.random(n)*2*np.pi\n\n    switched = (R2s > R1s)\n    R_large = switched*R2s + ~switched*R1s\n    R_small = switched*R1s + ~switched*R2s\n\n\n    b_tras = semimajors*np.cos(incs)/(R_large*RSUN) * (1-eccs**2)/(1 + eccs*np.sin(ws))\n    b_occs = semimajors*np.cos(incs)/(R_large*RSUN) * (1-eccs**2)/(1 - eccs*np.sin(ws))\n\n    b_tras[tooclose] = np.inf\n    b_occs[tooclose] = np.inf\n\n    ks = R_small/R_large\n    Rtots = (R_small + R_large)/R_large\n    tra = (b_tras < Rtots)\n    occ = (b_occs < Rtots)\n    nany = (tra | occ).sum()\n    peb = nany/float(n)\n    prob *= peb\n    if return_probability_only:\n        return prob,prob*np.sqrt(nany)/n\n\n\n    i = (tra | occ)\n    wany = np.where(i)\n    P,M1,M2,R1,R2,mag1,mag2,inc,ecc,w = Ps[i],M1s[i],M2s[i],R1s[i],R2s[i],\\\n        mag1s[i],mag2s[i],incs[i]*180/np.pi,eccs[i],ws[i]*180/np.pi\n    a = semimajors[i]  #in cm already\n    b_tra = b_tras[i]\n    b_occ = b_occs[i]\n    u11 = u11s[i]\n    u21 = u21s[i]\n    u12 = u12s[i]\n    u22 = u22s[i]\n\n\n    switched = (R2 > R1)\n    R_large = switched*R2 + ~switched*R1\n    R_small = switched*R1 + ~switched*R2\n    k = R_small/R_large\n\n    #calculate durations\n    T14_tra = P/np.pi*np.arcsin(R_large*RSUN/a * np.sqrt((1+k)**2 - b_tra**2)/np.sin(inc*np.pi/180)) *\\\n        np.sqrt(1-ecc**2)/(1+ecc*np.sin(w*np.pi/180)) #*24*60\n    T23_tra = P/np.pi*np.arcsin(R_large*RSUN/a * np.sqrt((1-k)**2 - b_tra**2)/np.sin(inc*np.pi/180)) *\\\n        np.sqrt(1-ecc**2)/(1+ecc*np.sin(w*np.pi/180)) #*24*60\n    T14_occ = P/np.pi*np.arcsin(R_large*RSUN/a * np.sqrt((1+k)**2 - b_occ**2)/np.sin(inc*np.pi/180)) *\\\n        np.sqrt(1-ecc**2)/(1-ecc*np.sin(w*np.pi/180)) #*24*60\n    T23_occ = P/np.pi*np.arcsin(R_large*RSUN/a * np.sqrt((1-k)**2 - b_occ**2)/np.sin(inc*np.pi/180)) *\\\n        np.sqrt(1-ecc**2)/(1-ecc*np.sin(w*np.pi/180)) #*24*60\n\n    bad = (np.isnan(T14_tra) & np.isnan(T14_occ))\n    if bad.sum() > 0:\n        logging.error('Something snuck through with no eclipses!')\n        logging.error('k: {}'.format(k[bad]))\n        logging.error('b_tra: {}'.format(b_tra[bad]))\n        logging.error('b_occ: {}'.format(b_occ[bad]))\n        logging.error('T14_tra: {}'.format(T14_tra[bad]))\n        logging.error('T14_occ: {}'.format(T14_occ[bad]))\n        logging.error('under sqrt (tra): {}'.format((1+k[bad])**2 - b_tra[bad]**2))\n        logging.error('under sqrt (occ): {}'.format((1+k[bad])**2 - b_occ[bad]**2))\n        logging.error('eccsq: {}'.format(ecc[bad]**2))\n        logging.error('a in Rsun: {}'.format(a[bad]/RSUN))\n        logging.error('R_large: {}'.format(R_large[bad]))\n        logging.error('R_small: {}'.format(R_small[bad]))\n        logging.error('P: {}'.format(P[bad]))\n        logging.error('total M: {}'.format(M1[bad]+M2[bad]))\n\n    T14_tra[(np.isnan(T14_tra))] = 0\n    T23_tra[(np.isnan(T23_tra))] = 0\n    T14_occ[(np.isnan(T14_occ))] = 0\n    T23_occ[(np.isnan(T23_occ))] = 0\n\n    #calling mandel-agol\n    ftra = MAfn(k,b_tra,u11,u21)\n    focc = MAfn(1/k,b_occ/k,u12,u22)\n\n    #fix those with k or 1/k out of range of MAFN....or do it in MAfn eventually?\n    wtrabad = np.where((k < MAfn.pmin) | (k > MAfn.pmax))\n    woccbad = np.where((1/k < MAfn.pmin) | (1/k > MAfn.pmax))\n    for ind in wtrabad[0]:\n        ftra[ind] = occultquad(b_tra[ind],u11[ind],u21[ind],k[ind])\n    for ind in woccbad[0]:\n        focc[ind] = occultquad(b_occ[ind]/k[ind],u12[ind],u22[ind],1/k[ind])\n\n    F1 = 10**(-0.4*mag1) + switched*10**(-0.4*mag2)\n    F2 = 10**(-0.4*mag2) + switched*10**(-0.4*mag1)\n\n    dtra = 1-(F2 + F1*ftra)/(F1+F2)\n    docc = 1-(F1 + F2*focc)/(F1+F2)\n\n    totmag = -2.5*np.log10(F1+F2)\n\n    #wswitched = where(switched)\n    dtra[switched],docc[switched] = (docc[switched],dtra[switched])\n    T14_tra[switched],T14_occ[switched] = (T14_occ[switched],T14_tra[switched])\n    T23_tra[switched],T23_occ[switched] = (T23_occ[switched],T23_tra[switched])\n    b_tra[switched],b_occ[switched] = (b_occ[switched],b_tra[switched])\n    #mag1[wswitched],mag2[wswitched] = (mag2[wswitched],mag1[wswitched])\n    F1[switched],F2[switched] = (F2[switched],F1[switched])\n    u11[switched],u12[switched] = (u12[switched],u11[switched])\n    u21[switched],u22[switched] = (u22[switched],u21[switched])\n\n    dtra[(np.isnan(dtra))] = 0\n    docc[(np.isnan(docc))] = 0\n\n    if np.any(np.isnan(ecc)):\n        logging.warning('{} nans in eccentricity.  why?'.format(np.isnan(ecc).sum()))\n\n    df =  pd.DataFrame({'{}_mag_tot'.format(band) : totmag,\n                        'P':P, 'ecc':ecc, 'inc':inc, 'w':w,\n                        'dpri':dtra, 'dsec':docc,\n                        'T14_pri':T14_tra, 'T23_pri':T23_tra,\n                        'T14_sec':T14_occ, 'T23_sec':T23_occ,\n                        'b_pri':b_tra, 'b_sec':b_occ,\n                        '{}_mag_1'.format(band) : mag1,\n                        '{}_mag_2'.format(band) : mag2,\n                        'fluxfrac_1':F1/(F1+F2),\n                        'fluxfrac_2':F2/(F1+F2),\n                        'switched':switched,\n                        'u1_1':u11, 'u2_1':u21, 'u1_2':u12, 'u2_2':u22})\n\n    df.reset_index(inplace=True)\n\n    logging.debug('final prob: {}'.format(prob))\n\n    if return_indices:\n        return wany, df, (prob, prob*np.sqrt(nany)/n)\n    else:\n        return df, (prob, prob*np.sqrt(nany)/n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _loadcache(cachefile):\n    cache = {}\n    if os.path.exists(cachefile):\n        with open(cachefile) as f:\n            for line in f:\n                line = line.split()\n                if len(line) == 2:\n                    try:\n                        cache[int(line[0])] = float(line[1])\n                    except:\n                        pass\n    return cache", "response": "Loads a likelihood cachefile into a dictionary resulting from reading a likelihood cachefile\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit_trapezoids(self, MAfn=None, msg=None, use_pbar=True, **kwargs):\n        logging.info('Fitting trapezoid models for {}...'.format(self.model))\n\n        if msg is None:\n            msg = '{}: '.format(self.model)\n\n        n = len(self.stars)\n        deps, durs, slopes = (np.zeros(n), np.zeros(n), np.zeros(n))\n        secs = np.zeros(n, dtype=bool)\n        dsec = np.zeros(n)\n\n        if use_pbar and pbar_ok:\n            widgets = [msg+'fitting shape parameters for %i systems: ' % n,Percentage(),\n                       ' ',Bar(marker=RotatingMarker()),' ',ETA()]\n            pbar = ProgressBar(widgets=widgets,maxval=n)\n            pbar.start()\n\n        for i in range(n):\n            logging.debug('Fitting star {}'.format(i))\n            pri = (self.stars['dpri'][i] > self.stars['dsec'][i] or\n                   np.isnan(self.stars['dsec'][i]))\n            sec = not pri\n            secs[i] = sec\n            if sec:\n                dsec[i] = self.stars['dpri'][i]\n            else:\n                dsec[i] = self.stars['dsec'][i]\n\n            try:\n                trap_pars = self.eclipse_trapfit(i, secondary=sec, **kwargs)\n\n            except NoEclipseError:\n                logging.error('No eclipse registered for star {}'.format(i))\n                trap_pars = (np.nan, np.nan, np.nan)\n            except NoFitError:\n                logging.error('Fit did not converge for star {}'.format(i))\n                trap_pars = (np.nan, np.nan, np.nan)\n            except KeyboardInterrupt:\n                raise\n            except:\n                logging.error('Unknown error for star {}'.format(i))\n                trap_pars = (np.nan, np.nan, np.nan)\n\n            if use_pbar and pbar_ok:\n                pbar.update(i)\n            durs[i], deps[i], slopes[i] = trap_pars\n\n        logging.info('Done.')\n\n        self.stars['depth'] = deps\n        self.stars['duration'] = durs\n        self.stars['slope'] = slopes\n        self.stars['secdepth'] = dsec\n        self.stars['secondary'] = secs\n\n        self._make_kde()", "response": "Fit the trapezoid shape to each instance in the population."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eclipseprob(self):\n        #TODO: incorporate eccentricity/omega for exact calculation?\n        s = self.stars\n        return ((s['radius_1'] + s['radius_2'])*RSUN /\n                (semimajor(s['P'],s['mass_1'] + s['mass_2'])*AU))", "response": "Returns the eclipse probability of the species in the species."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstrains the observed secondary depth to be less than a given value.", "response": "def constrain_secdepth(self, thresh):\n        \"\"\"\n        Constrain the observed secondary depth to be less than a given value\n\n        :param thresh:\n            Maximum allowed fractional depth for diluted secondary\n            eclipse depth\n\n        \"\"\"\n        self.apply_constraint(UpperLimit(self.secondary_depth, thresh, name='secondary depth'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the prior for this model.", "response": "def prior(self):\n        \"\"\"\n        Model prior for particular model.\n\n        Product of eclipse probability (``self.prob``),\n        the fraction of scenario that is allowed by the various\n        constraints (``self.selectfrac``), and all additional\n        factors in ``self.priorfactors``.\n\n        \"\"\"\n        prior = self.prob * self.selectfrac\n        for f in self.priorfactors:\n            prior *= self.priorfactors[f]\n        return prior"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_priorfactor(self,**kwargs):\n        for kw in kwargs:\n            if kw in self.priorfactors:\n                logging.error('%s already in prior factors for %s.  use change_prior function instead.' % (kw,self.model))\n                continue\n            else:\n                self.priorfactors[kw] = kwargs[kw]\n                logging.info('%s added to prior factors for %s' % (kw,self.model))", "response": "Adds given values to priorfactors\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_prior(self, **kwargs):\n        for kw in kwargs:\n            if kw in self.priorfactors:\n                self.priorfactors[kw] = kwargs[kw]\n                logging.info('{0} changed to {1} for {2} model'.format(kw,kwargs[kw],\n                                                                       self.model))", "response": "Changes existing priorfactors.\n\n        If given keyword isn't already in priorfactors,\n        then will be ignored."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_kde(self, use_sklearn=False, bandwidth=None, rtol=1e-6,\n                  sig_clip=50, no_sig_clip=False, cov_all=True,\n                  **kwargs):\n        \"\"\"Creates KDE objects for 3-d shape parameter distribution\n\n        KDE represents likelihood as function of trapezoidal\n        shape parameters (log(delta), T, T/tau).\n\n        Uses :class:`scipy.stats.gaussian_kde`` KDE by default;\n        Scikit-learn KDE implementation tested a bit, but not\n        fully implemented.\n\n        :param use_sklearn:\n            Whether to use scikit-learn implementation of KDE.\n            Not yet fully implemented, so this should stay ``False``.\n\n        :param bandwidth, rtol:\n            Parameters for sklearn KDE.\n\n        :param **kwargs:\n            Additional keyword arguments passed to\n            :class:`scipy.stats.gaussian_kde``.\n\n        \"\"\"\n\n        try:\n            #define points that are ok to use\n            first_ok = ((self.stars['slope'] > 0) &\n                        (self.stars['duration'] > 0) &\n                        (self.stars['duration'] < self.period) &\n                        (self.depth > 0))\n        except KeyError:\n            logging.warning('Must do trapezoid fits before making KDE.')\n            return\n\n        self.empty = False\n        if first_ok.sum() < 4:\n            logging.warning('Empty population ({}): < 4 valid systems! Cannot calculate lhood.'.format(self.model))\n            self.is_empty = True #will cause is_ruled_out to be true as well.\n            return\n            #raise EmptyPopulationError('< 4 valid systems in population')\n\n        logdeps = np.log10(self.depth)\n        durs = self.stars['duration']\n        slopes = self.stars['slope']\n\n        #Now sigma-clip those points that passed first cuts\n        ok = np.ones(len(logdeps), dtype=bool)\n        for x in [logdeps, durs, slopes]:\n            med = np.median(x[first_ok])\n            mad = np.median(np.absolute(x[first_ok] - med))\n            ok &= np.absolute(x - med) / mad < sig_clip\n\n        second_ok = first_ok & ok\n\n        # Before making KDE for real, first calculate\n        #  covariance and inv_cov of uncut data, to use\n        #  when it's cut, too.\n\n        points = np.array([durs[second_ok],\n                           logdeps[second_ok],\n                           slopes[second_ok]])\n        kde = gaussian_kde(np.vstack(points)) #backward compatibility?\n        cov_all = kde._data_covariance\n        icov_all = kde._data_inv_cov\n        factor = kde.factor\n\n        # OK, now cut the data for constraints & proceed\n\n        ok = second_ok & self.distok\n\n        logdeps = logdeps[ok]\n        durs = durs[ok]\n        slopes = slopes[ok]\n\n        if ok.sum() < 4 and not self.empty:\n            logging.warning('Empty population ({}): < 4 valid systems! Cannot calculate lhood.'.format(self.model))\n            self.is_empty = True\n            return\n            #raise EmptyPopulationError('< 4 valid systems in population')\n\n\n        if use_sklearn:\n            self.sklearn_kde = True\n            logdeps_normed = (logdeps - logdeps.mean())/logdeps.std()\n            durs_normed = (durs - durs.mean())/durs.std()\n            slopes_normed = (slopes - slopes.mean())/slopes.std()\n\n            #TODO: use sklearn preprocessing to replace below\n            self.mean_logdepth = logdeps.mean()\n            self.std_logdepth = logdeps.std()\n            self.mean_dur = durs.mean()\n            self.std_dur = durs.std()\n            self.mean_slope = slopes.mean()\n            self.std_slope = slopes.std()\n\n            points = np.array([logdeps_normed, durs_normed, slopes_normed])\n\n            #find best bandwidth.  For some reason this doesn't work?\n            if bandwidth is None:\n                grid = GridSearchCV(KernelDensity(rtol=rtol),\n                                    {'bandwidth':np.linspace(0.05,1,50)})\n                grid.fit(points)\n                self._best_bandwidth = grid.best_params_\n                self.kde = grid.best_estimator_\n            else:\n                self.kde = KernelDensity(rtol=rtol, bandwidth=bandwidth).fit(points)\n        else:\n            self.sklearn_kde = False\n            points = np.array([durs, logdeps, slopes])\n            self.kde = gaussian_kde(np.vstack(points), **kwargs) #backward compatibility?\n\n            # Reset covariance based on uncut data\n            self.kde._data_covariance = cov_all\n            self.kde._data_inv_cov = icov_all\n            self.kde._compute_covariance()", "response": "Creates a 3 - d shape parameter distribution of sklearn KDE."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _density(self, logd, dur, slope):\n        if self.sklearn_kde:\n            #TODO: fix preprocessing\n            pts = np.array([(logd - self.mean_logdepth)/self.std_logdepth,\n                            (dur - self.mean_dur)/self.std_dur,\n                            (slope - self.mean_slope)/self.std_slope])\n            return self.kde.score_samples(pts)\n        else:\n            return self.kde(np.array([logd, dur, slope]))", "response": "Evaluate KDE at given points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the likelihood of a transit signal at self. kde.", "response": "def lhood(self, trsig, recalc=False, cachefile=None):\n        \"\"\"Returns likelihood of transit signal\n\n        Returns sum of ``trsig`` MCMC samples evaluated\n        at ``self.kde``.\n\n        :param trsig:\n            :class:`vespa.TransitSignal` object.\n\n        :param recalc: (optional)\n            Whether to recalculate likelihood (if calculation\n            is cached).\n\n        :param cachefile: (optional)\n            File that holds likelihood calculation cache.\n\n        \"\"\"\n        if not hasattr(self,'kde'):\n            self._make_kde()\n\n        if cachefile is None:\n            cachefile = self.lhoodcachefile\n            if cachefile is None:\n                cachefile = 'lhoodcache.dat'\n\n        lhoodcache = _loadcache(cachefile)\n        key = hashcombine(self, trsig)\n        if key in lhoodcache and not recalc:\n            return lhoodcache[key]\n\n        if self.is_ruled_out:\n            return 0\n\n        N = trsig.kde.dataset.shape[1]\n        lh = self.kde(trsig.kde.dataset).sum() / N\n\n        with open(cachefile, 'a') as fout:\n            fout.write('%i %g\\n' % (key, lh))\n\n        return lh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a plot of likelihood density function for the given transit signal and returns the plotted version of the plotted version of the plot.", "response": "def lhoodplot(self, trsig=None, fig=None,\n                  piechart=True, figsize=None, logscale=True,\n                  constraints='all', suptitle=None, Ltot=None,\n                  maxdur=None, maxslope=None, inverse=False,\n                  colordict=None, cachefile=None, nbins=20,\n                  dur_range=None, slope_range=None, depth_range=None,\n                  recalc=False,**kwargs):\n        \"\"\"\n        Makes plot of likelihood density function, optionally with transit signal\n\n        If ``trsig`` not passed, then just density plot of the likelidhoo\n        will be made; if it is passed, then it will be plotted\n        over the density plot.\n\n        :param trsig: (optional)\n            :class:`vespa.TransitSignal` object.\n\n        :param fig: (optional)\n            Argument for :func:`plotutils.setfig`.\n\n        :param piechart: (optional)\n            Whether to include a plot of the piechart that describes\n            the effect of the constraints on the population.\n\n        :param figsize: (optional)\n            Passed to :func:`plotutils.setfig`.\n\n        :param logscale: (optional)\n            If ``True``, then shading will be based on the log-histogram\n            (thus showing more detail at low density).  Passed to\n            :func:`vespa.stars.StarPopulation.prophist2d`.\n\n        :param constraints: (``'all', 'none'`` or ``list``; optional)\n            Which constraints to apply in making plot.  Picking\n            specific constraints allows you to visualize in more\n            detail what the effect of a constraint is.\n\n        :param suptitle: (optional)\n            Title for the figure.\n\n        :param Ltot: (optional)\n            Total of ``prior * likelihood`` for all models.  If this is\n            passed, then \"Probability of scenario\" gets a text box\n            in the middle.\n\n        :param inverse: (optional)\n            Intended to allow showing only the instances that are\n            ruled out, rather than those that remain.  Not sure if this\n            works anymore.\n\n        :param colordict: (optional)\n            Dictionary to define colors of constraints to be used\n            in pie chart.  Intended to unify constraint colors among\n            different models.\n\n        :param cachefile: (optional)\n            Likelihood calculation cache file.\n\n        :param nbins: (optional)\n            Number of bins with which to make the 2D histogram plot;\n            passed to :func:`vespa.stars.StarPopulation.prophist2d`.\n\n        :param dur_range, slope_range, depth_range: (optional)\n            Define ranges of plots.\n\n        :param **kwargs:\n            Additional keyword arguments passed to\n            :func:`vespa.stars.StarPopulation.prophist2d`.\n\n        \"\"\"\n\n        setfig(fig, figsize=figsize)\n\n        if trsig is not None:\n            dep,ddep = trsig.logdepthfit\n            dur,ddur = trsig.durfit\n            slope,dslope = trsig.slopefit\n\n            ddep = ddep.reshape((2,1))\n            ddur = ddur.reshape((2,1))\n            dslope = dslope.reshape((2,1))\n\n            if dur_range is None:\n                dur_range = (0,dur*2)\n            if slope_range is None:\n                slope_range = (2,slope*2)\n\n        if constraints == 'all':\n            mask = self.distok\n        elif constraints == 'none':\n            mask = np.ones(len(self.stars)).astype(bool)\n        else:\n            mask = np.ones(len(self.stars)).astype(bool)\n            for c in constraints:\n                if c not in self.distribution_skip:\n                    mask &= self.constraints[c].ok\n\n        if inverse:\n            mask = ~mask\n\n        if dur_range is None:\n            dur_range = (self.stars[mask]['duration'].min(),\n                         self.stars[mask]['duration'].max())\n        if slope_range is None:\n            slope_range = (2,self.stars[mask]['slope'].max())\n        if depth_range is None:\n            depth_range = (-5,-0.1)\n\n        #This may mess with intended \"inverse\" behavior, probably?\n        mask &= ((self.stars['duration'] > dur_range[0]) &\n                 (self.stars['duration'] < dur_range[1]))\n        mask &= ((self.stars['duration'] > dur_range[0]) &\n                 (self.stars['duration'] < dur_range[1]))\n\n        mask &= ((self.stars['slope'] > slope_range[0]) &\n                 (self.stars['slope'] < slope_range[1]))\n        mask &= ((self.stars['slope'] > slope_range[0]) &\n                 (self.stars['slope'] < slope_range[1]))\n\n        mask &= ((np.log10(self.depth) > depth_range[0]) &\n                 (np.log10(self.depth) < depth_range[1]))\n        mask &= ((np.log10(self.depth) > depth_range[0]) &\n                 (np.log10(self.depth) < depth_range[1]))\n\n\n\n\n        if piechart:\n            a_pie = plt.axes([0.07, 0.5, 0.4, 0.5])\n            self.constraint_piechart(fig=0, colordict=colordict)\n\n        ax1 = plt.subplot(222)\n        if not self.is_ruled_out:\n            self.prophist2d('duration', 'depth', logy=True, fig=0,\n                            mask=mask, interpolation='bicubic',\n                            logscale=logscale, nbins=nbins, **kwargs)\n        if trsig is not None:\n            plt.errorbar(dur,dep,xerr=ddur,yerr=ddep,color='w',marker='x',\n                         ms=12,mew=3,lw=3,capsize=3,mec='w')\n            plt.errorbar(dur,dep,xerr=ddur,yerr=ddep,color='r',marker='x',\n                         ms=10,mew=1.5)\n        plt.ylabel(r'log($\\delta$)')\n        plt.xlabel('')\n        plt.xlim(dur_range)\n        plt.ylim(depth_range)\n        yt = ax1.get_yticks()\n        plt.yticks(yt[1:])\n        xt = ax1.get_xticks()\n        plt.xticks(xt[2:-1:2])\n\n        ax3 = plt.subplot(223)\n        if not self.is_ruled_out:\n            self.prophist2d('depth', 'slope', logx=True, fig=0,\n                            mask=mask, interpolation='bicubic',\n                            logscale=logscale, nbins=nbins, **kwargs)\n        if trsig is not None:\n            plt.errorbar(dep,slope,xerr=ddep,yerr=dslope,color='w',marker='x',\n                         ms=12,mew=3,lw=3,capsize=3,mec='w')\n            plt.errorbar(dep,slope,xerr=ddep,yerr=dslope,color='r',marker='x',\n                         ms=10,mew=1.5)\n        plt.ylabel(r'$T/\\tau$')\n        plt.xlabel(r'log($\\delta$)')\n        plt.ylim(slope_range)\n        plt.xlim(depth_range)\n        yt = ax3.get_yticks()\n        plt.yticks(yt[1:])\n\n        ax4 = plt.subplot(224)\n        if not self.is_ruled_out:\n            self.prophist2d('duration', 'slope', fig=0,\n                            mask=mask, interpolation='bicubic',\n                            logscale=logscale, nbins=nbins, **kwargs)\n        if trsig is not None:\n            plt.errorbar(dur,slope,xerr=ddur,yerr=dslope,color='w',marker='x',\n                         ms=12,mew=3,lw=3,capsize=3,mec='w')\n            plt.errorbar(dur,slope,xerr=ddur,yerr=dslope,color='r',marker='x',\n                         ms=10,mew=1.5)\n        plt.ylabel('')\n        plt.xlabel(r'$T$ [days]')\n        plt.ylim(slope_range)\n        plt.xlim(dur_range)\n        plt.xticks(xt[2:-1:2])\n        plt.yticks(ax3.get_yticks())\n\n        ticklabels = ax1.get_xticklabels() + ax4.get_yticklabels()\n        plt.setp(ticklabels,visible=False)\n\n        plt.subplots_adjust(hspace=0.001,wspace=0.001)\n\n        if suptitle is None:\n            suptitle = self.model\n        plt.suptitle(suptitle,fontsize=20)\n\n        if Ltot is not None:\n            lhood = self.lhood(trsig, recalc=recalc)\n            plt.annotate('%s:\\nProbability\\nof scenario: %.3f' % (trsig.name,\n                                                                  self.prior*lhood/Ltot),\n                         xy=(0.5,0.5),ha='center',va='center',\n                         bbox=dict(boxstyle='round',fc='w'),\n                         xycoords='figure fraction',fontsize=15)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn times and fluxes of an eclipse i.", "response": "def eclipse_new(self, i, secondary=False, npoints=200, width=3,\n                texp=None):\n        \"\"\"\n        Returns times and fluxes of eclipse i (centered at t=0)\n        \"\"\"\n        texp = self.cadence\n        s = self.stars.iloc[i]\n\n        e = s['ecc']\n        P = s['P']\n        if secondary:\n            mu1, mu2 = s[['u1_2', 'u2_2']]\n            w = np.mod(np.deg2rad(s['w']) + np.pi, 2*np.pi)\n            mass_central, radius_central = s[['mass_2','radius_2']]\n            mass_body, radius_body = s[['mass_1','radius_1']]\n            b = s['b_sec'] * s['radius_1']/s['radius_2']\n            frac = s['fluxfrac_2']\n        else:\n            mu1, mu2 = s[['u1_1', 'u2_1']]\n            w = np.deg2rad(s['w'])\n            mass_central, radius_central = s[['mass_1','radius_1']]\n            mass_body, radius_body = s[['mass_2','radius_2']]\n            b = s['b_pri']\n            frac = s['fluxfrac_1']\n\n\n        central_kwargs = dict(mass=mass_central, radius=radius_central,\n                              mu1=mu1, mu2=mu2)\n        central = Central(**central_kwargs)\n\n        body_kwargs = dict(radius=radius_body, mass=mass_body, b=b,\n                           period=P, e=e, omega=w)\n        body = Body(**body_kwargs)\n\n        logging.debug('central: {}'.format(central_kwargs))\n        logging.debug('body: {}'.format(body_kwargs))\n\n        s = System(central)\n        s.add_body(body)\n\n        # As of now, body.duration returns strictly circular duration\n        dur = body.duration\n\n        logging.debug('duration: {}'.format(dur))\n\n        ts = np.linspace(-width/2*dur, width/2*dur, npoints)\n        fs = s.light_curve(ts, texp=texp)\n        fs = 1 - frac*(1-fs)\n        return ts, fs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_hdf(cls, filename, path=''): #perhaps this doesn't need to be written?\n\n        new = StarPopulation.load_hdf(filename, path=path)\n\n        #setup lazy loading of starmodel if present\n        try:\n            with pd.HDFStore(filename) as store:\n                if '{}/starmodel'.format(path) in store:\n                    new._starmodel = None\n                    new._starmodel_file = filename\n                    new._starmodel_path = '{}/starmodel'.format(path)\n        except:\n            pass\n\n        try:\n            new._make_kde()\n        except NoTrapfitError:\n            logging.warning('Trapezoid fit not done.')\n        return new", "response": "Loads an EclipsePopulation from HDF file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resample(self):\n        new = copy.deepcopy(self)\n        N = len(new.stars)\n        inds = np.random.randint(N, size=N)\n\n        # Resample stars\n        new.stars = new.stars.iloc[inds].reset_index()\n\n        # Resample constraints\n        if hasattr(new, '_constraints'):\n            for c in new._constraints:\n                new._constraints[c] = new._constraints[c].resample(inds)\n\n        new._make_kde()\n        return new", "response": "Resample the population with stars resampled."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate Population All arguments defined in ``__init__``.", "response": "def generate(self,rprs=None, mass=None, radius=None,\n                n=2e4, fp_specific=0.01, u1=None, u2=None,\n                 starmodel=None,\n                Teff=None, logg=None, rbin_width=0.3,\n                MAfn=None, lhoodcachefile=None):\n        \"\"\"Generates Population\n\n        All arguments defined in ``__init__``.\n        \"\"\"\n\n        n = int(n)\n\n        if starmodel is None:\n            if type(mass) is type((1,)):\n                mass = dists.Gaussian_Distribution(*mass)\n            if isinstance(mass, dists.Distribution):\n                mdist = mass\n                mass = mdist.rvs(1e5)\n\n            if type(radius) is type((1,)):\n                radius = dists.Gaussian_Distribution(*radius)\n            if isinstance(radius, dists.Distribution):\n                rdist = radius\n                radius = rdist.rvs(1e5)\n        else:\n            samples = starmodel.random_samples(1e5)\n            mass = samples['mass_0_0'].values\n            radius = samples['radius_0_0'].values\n            Teff = samples['Teff_0_0'].mean()\n            logg = samples['logg_0_0'].mean()\n\n        logging.debug('star mass: {}'.format(mass))\n        logging.debug('star radius: {}'.format(radius))\n        logging.debug('Teff: {}'.format(Teff))\n        logging.debug('logg: {}'.format(logg))\n\n        if u1 is None or u2 is None:\n            if Teff is None or logg is None:\n                logging.warning('Teff, logg not provided; using solar limb darkening')\n                u1 = 0.394; u2=0.296\n            else:\n                u1,u2 = ldcoeffs(Teff, logg)\n\n        #use point estimate of rprs to construct planets in radius bin\n        #rp = self.rprs*np.median(radius)\n        #rbin_min = (1-rbin_width)*rp\n        #rbin_max = (1+rbin_width)*rp\n\n        rprs_bin_min = (1-rbin_width)*self.rprs\n        rprs_bin_max = (1+rbin_width)*self.rprs\n\n        radius_p = radius * (np.random.random(int(1e5))*(rprs_bin_max - rprs_bin_min) + rprs_bin_min)\n        mass_p = (radius_p*RSUN/REARTH)**2.06 * MEARTH/MSUN #hokey, but doesn't matter\n\n        logging.debug('planet radius: {}'.format(radius_p))\n\n        stars = pd.DataFrame()\n        #df_orbpop = pd.DataFrame() #for orbit population\n\n        tot_prob = None; tot_dprob = None; prob_norm = None\n        n_adapt = n\n        while len(stars) < n:\n            n_adapt = int(n_adapt)\n            inds = np.random.randint(len(mass), size=n_adapt)\n\n            #calculate eclipses.\n            ecl_inds, df, (prob,dprob) = calculate_eclipses(mass[inds], mass_p[inds],\n                                                        radius[inds], radius_p[inds],\n                                                        15, np.inf, #arbitrary\n                                                        u11s=u1, u21s=u2,\n                                                        band=self.band,\n                                                        period=self.period,\n                                                        calc_mininc=True,\n                                                        return_indices=True,\n                                                        MAfn=MAfn)\n\n            df['mass_A'] = mass[inds][ecl_inds]\n            df['mass_B'] = mass_p[inds][ecl_inds]\n            df['radius_A'] = radius[inds][ecl_inds]\n            df['radius_B'] = radius_p[inds][ecl_inds]\n            df['u1'] = u1 * np.ones_like(df['mass_A'])\n            df['u2'] = u2 * np.ones_like(df['mass_A'])\n            df['P'] = self.period * np.ones_like(df['mass_A'])\n\n            ok = (df['dpri']>0) & (df['T14_pri'] > 0)\n\n            stars = pd.concat((stars, df[ok]))\n\n            logging.info('{} Transiting planet systems generated (target {})'.format(len(stars),n))\n            logging.debug('{} nans in stars[dpri]'.format(np.isnan(stars['dpri']).sum()))\n\n            if tot_prob is None:\n                prob_norm = (1/dprob**2)\n                tot_prob = prob\n                tot_dprob = dprob\n            else:\n                prob_norm = (1/tot_dprob**2 + 1/dprob**2)\n                tot_prob = (tot_prob/tot_dprob**2 + prob/dprob**2)/prob_norm\n                tot_dprob = 1/np.sqrt(prob_norm)\n\n            n_adapt = min(int(1.2*(n-len(stars)) * n_adapt//len(df)), 5e4)\n            n_adapt = max(n_adapt, 100)\n\n        stars = stars.reset_index()\n        stars.drop('index', axis=1, inplace=True)\n        stars = stars.iloc[:n]\n\n        stars['mass_1'] = stars['mass_A']\n        stars['radius_1'] = stars['radius_A']\n        stars['mass_2'] = stars['mass_B']\n        stars['radius_2'] = stars['radius_B']\n\n        #make OrbitPopulation?\n\n        #finish below.\n\n        if fp_specific is None:\n            rp = stars['radius_2'].mean() * RSUN/REARTH\n            fp_specific = fp_fressin(rp)\n\n        priorfactors = {'fp_specific':fp_specific}\n\n        self._starmodel = starmodel\n\n        EclipsePopulation.__init__(self, stars=stars,\n                                   period=self.period, cadence=self.cadence,\n                                   model=self.model,\n                                   priorfactors=priorfactors, prob=tot_prob,\n                                   lhoodcachefile=lhoodcachefile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating stars and eclipsesunused objects from the specified parameters.", "response": "def generate(self, mags, n=2e4, mag_errs=None,\n                 Teff=None, logg=None, feh=None,\n                 MAfn=None, f_triple=0.12, starmodel=None,\n                 **kwargs):\n        \"\"\"Generates stars and eclipses\n\n        All arguments previously defined.\n        \"\"\"\n        n = int(n)\n\n\n        #create master population from which to create eclipses\n        pop = Observed_TriplePopulation(mags=mags, mag_errs=mag_errs,\n                                        Teff=Teff,\n                                        logg=logg, feh=feh,\n                                        starmodel=starmodel,\n                                        period=self.period,\n                                        n=2*n)\n\n        all_stars = pop.stars\n\n        #start with empty; will concatenate onto\n        stars = pd.DataFrame()\n        df_orbpop_short = pd.DataFrame()\n        df_orbpop_long = pd.DataFrame()\n\n\n        #calculate eclipses\n\n        if MAfn is None:\n            MAfn = MAInterpolationFunction(pmin=0.007, pmax=1/0.007, nzs=200, nps=400)\n\n        tot_prob = None; tot_dprob = None; prob_norm = None\n        n_adapt = n\n        while len(stars) < n:\n            n_adapt = int(n_adapt)\n            inds = np.random.randint(len(all_stars), size=n_adapt)\n\n            s = all_stars.iloc[inds]\n\n            #calculate limb-darkening coefficients\n            u1A, u2A = ldcoeffs(s['Teff_A'], s['logg_A'])\n            u1B, u2B = ldcoeffs(s['Teff_B'], s['logg_B'])\n            u1C, u2C = ldcoeffs(s['Teff_C'], s['logg_C'])\n\n            cur_orbpop_short_df = pop.orbpop.orbpop_short.dataframe.iloc[inds].copy()\n            cur_orbpop_long_df = pop.orbpop.orbpop_long.dataframe.iloc[inds].copy()\n\n            #calculate eclipses.\n            inds, df, (prob,dprob) = calculate_eclipses(s['mass_B'], s['mass_C'],\n                                                        s['radius_B'], s['radius_C'],\n                                                        s['{}_mag_B'.format(self.band)],\n                                                        s['{}_mag_C'.format(self.band)],\n                                                        u11s=u1A, u21s=u2A,\n                                                        u12s=u1B, u22s=u2B,\n                                                        band=self.band,\n                                                        period=self.period,\n                                                        calc_mininc=True,\n                                                        return_indices=True,\n                                                        MAfn=MAfn)\n\n            s = s.iloc[inds].copy()\n            s.reset_index(inplace=True)\n            for col in df.columns:\n                s[col] = df[col]\n            stars = pd.concat((stars, s))\n\n            new_df_orbpop_short = cur_orbpop_short_df.iloc[inds].copy()\n            new_df_orbpop_short.reset_index(inplace=True)\n\n            new_df_orbpop_long = cur_orbpop_long_df.iloc[inds].copy()\n            new_df_orbpop_long.reset_index(inplace=True)\n\n            df_orbpop_short = pd.concat((df_orbpop_short, new_df_orbpop_short))\n            df_orbpop_long = pd.concat((df_orbpop_long, new_df_orbpop_long))\n\n            logging.info('{} eclipsing HEB systems generated (target {})'.format(len(stars),n))\n            logging.debug('{} nans in stars[dpri]'.format(np.isnan(stars['dpri']).sum()))\n            logging.debug('{} nans in df[dpri]'.format(np.isnan(df['dpri']).sum()))\n\n            if tot_prob is None:\n                prob_norm = (1/dprob**2)\n                tot_prob = prob\n                tot_dprob = dprob\n            else:\n                prob_norm = (1/tot_dprob**2 + 1/dprob**2)\n                tot_prob = (tot_prob/tot_dprob**2 + prob/dprob**2)/prob_norm\n                tot_dprob = 1/np.sqrt(prob_norm)\n\n            n_adapt = min(int(1.2*(n-len(stars)) * n_adapt//len(s)), 5e4)\n            n_adapt = max(n_adapt, 100)\n\n        stars = stars.iloc[:n]\n        df_orbpop_short = df_orbpop_short.iloc[:n]\n        df_orbpop_long = df_orbpop_long.iloc[:n]\n        orbpop = TripleOrbitPopulation.from_df(df_orbpop_long, df_orbpop_short)\n\n        stars = stars.reset_index()\n        stars.drop('index', axis=1, inplace=True)\n\n        stars['mass_1'] = stars['mass_B']\n        stars['radius_1'] = stars['radius_B']\n        stars['mass_2'] = stars['mass_C']\n        stars['radius_2'] = stars['radius_C']\n\n        ## Why does this make it go on infinite loop??\n        #Observed_TriplePopulation.__init__(self, stars=stars, orbpop=orbpop,\n        #                                   mags=mags, mag_errs=mag_errs,\n        #                                   Teff=Teff, logg=logg, feh=feh,\n        #                                   starmodel=starmodel)\n        #############\n\n        self.mags = mags\n        self.mag_errs = mag_errs\n        self.Teff = Teff\n        self.logg = logg\n        self.feh = feh\n        self._starmodel = pop.starmodel\n\n        priorfactors = {'f_triple':f_triple}\n\n        EclipsePopulation.__init__(self, stars=stars, orbpop=orbpop,\n                                   period=self.period, cadence=self.cadence,\n                                   model=self.model,\n                                   priorfactors=priorfactors, prob=tot_prob,\n                                   lhoodcachefile=self.lhoodcachefile)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate(self, trilegal_filename, ra=None, dec=None,\n                 n=2e4, ichrone='mist', MAfn=None,\n                 mags=None, maxrad=None, f_binary=0.4, **kwargs):\n        \"\"\"\n        Generate population.\n        \"\"\"\n        n = int(n)\n\n        #generate/load BG primary stars from TRILEGAL simulation\n        bgpop = BGStarPopulation_TRILEGAL(trilegal_filename,\n                                        ra=ra, dec=dec, mags=mags,\n                                        maxrad=maxrad, **kwargs)\n\n        # Make sure that\n        # properties of stars are within allowable range for isochrone.\n        # This is a bit hacky, admitted.\n        mass = bgpop.stars['m_ini'].values\n        age = bgpop.stars['logAge'].values\n        feh = bgpop.stars['[M/H]'].values\n\n        ichrone = get_ichrone(ichrone)\n\n        pct = 0.05 #pct distance from \"edges\" of ichrone interpolation\n        mass[mass < ichrone.minmass*(1+pct)] = ichrone.minmass*(1+pct)\n        mass[mass > ichrone.maxmass*(1-pct)] = ichrone.maxmass*(1-pct)\n        age[age < ichrone.minage*(1+pct)] = ichrone.minage*(1+pct)\n        age[age > ichrone.maxage*(1-pct)] = ichrone.maxage*(1-pct)\n        feh[feh < ichrone.minfeh+0.05] = ichrone.minfeh+0.05\n        feh[feh > ichrone.maxfeh-0.05] = ichrone.maxfeh-0.05\n\n        distance = bgpop.stars['distance'].values\n\n        #Generate binary population to draw eclipses from\n        pop = MultipleStarPopulation(mA=mass, age=age, feh=feh,\n                                            f_triple=0, f_binary=1,\n                                            distance=distance,\n                                            ichrone=ichrone)\n\n        all_stars = pop.stars.dropna(subset=['mass_A'])\n        all_stars.reset_index(inplace=True)\n\n        #generate eclipses\n        stars = pd.DataFrame()\n        df_orbpop = pd.DataFrame()\n        tot_prob = None; tot_dprob=None; prob_norm=None\n\n        n_adapt = n\n        while len(stars) < n:\n            n_adapt = int(n_adapt)\n            inds = np.random.randint(len(all_stars), size=n_adapt)\n\n            s = all_stars.iloc[inds]\n\n            #calculate limb-darkening coefficients\n            u1A, u2A = ldcoeffs(s['Teff_A'], s['logg_A'])\n            u1B, u2B = ldcoeffs(s['Teff_B'], s['logg_B'])\n\n            inds, df, (prob,dprob) = calculate_eclipses(s['mass_A'], s['mass_B'],\n                                                        s['radius_A'], s['radius_B'],\n                                                        s['{}_mag_A'.format(self.band)],\n                                                        s['{}_mag_B'.format(self.band)],\n                                                        u11s=u1A, u21s=u2A,\n                                                        u12s=u1B, u22s=u2B,\n                                                        band=self.band,\n                                                        period=self.period,\n                                                        calc_mininc=True,\n                                                        return_indices=True,\n                                                        MAfn=MAfn)\n            s = s.iloc[inds].copy()\n            s.reset_index(inplace=True)\n            for col in df.columns:\n                s[col] = df[col]\n            stars = pd.concat((stars, s))\n\n            #new_df_orbpop = pop.orbpop.orbpop_long.dataframe.iloc[inds].copy()\n            #new_df_orbpop.reset_index(inplace=True)\n\n            #df_orbpop = pd.concat((df_orbpop, new_df_orbpop))\n\n            logging.info('{} BEB systems generated (target {})'.format(len(stars),n))\n            #logging.debug('{} nans in stars[dpri]'.format(np.isnan(stars['dpri']).sum()))\n            #logging.debug('{} nans in df[dpri]'.format(np.isnan(df['dpri']).sum()))\n\n            if tot_prob is None:\n                prob_norm = (1/dprob**2)\n                tot_prob = prob\n                tot_dprob = dprob\n            else:\n                prob_norm = (1/tot_dprob**2 + 1/dprob**2)\n                tot_prob = (tot_prob/tot_dprob**2 + prob/dprob**2)/prob_norm\n                tot_dprob = 1/np.sqrt(prob_norm)\n\n            n_adapt = min(int(1.2*(n-len(stars)) * n_adapt//len(s)), 5e5)\n            #logging.debug('n_adapt = {}'.format(n_adapt))\n            n_adapt = max(n_adapt, 100)\n            n_adapt = int(n_adapt)\n\n        stars = stars.iloc[:n]\n\n        if 'level_0' in stars:\n            stars.drop('level_0', axis=1, inplace=True) #dunno where this came from\n        stars = stars.reset_index()\n        stars.drop('index', axis=1, inplace=True)\n\n        stars['mass_1'] = stars['mass_A']\n        stars['radius_1'] = stars['radius_A']\n        stars['mass_2'] = stars['mass_B']\n        stars['radius_2'] = stars['radius_B']\n\n        MultipleStarPopulation.__init__(self, stars=stars,\n                                        #orbpop=orbpop,\n                                        f_triple=0, f_binary=f_binary,\n                                        period_long=self.period)\n\n        priorfactors = {'f_binary':f_binary}\n\n        #attributes needed for BGStarPopulation\n        self.density = bgpop.density\n        self.trilegal_args = bgpop.trilegal_args\n        self._maxrad = bgpop._maxrad\n\n        #create an OrbitPopulation here?\n\n        EclipsePopulation.__init__(self, stars=stars, #orbpop=orbpop,\n                                   period=self.period, cadence=self.cadence,\n                                   model=self.model,\n                                   lhoodcachefile=self.lhoodcachefile,\n                                   priorfactors=priorfactors, prob=tot_prob)\n\n        #add Rsky property\n        self.stars['Rsky'] = randpos_in_circle(len(self.stars),\n                                               self._maxrad, return_rad=True)", "response": "Generate a new population from a TRILEGAL file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate(self, ra, dec, period, cadence, mags,\n                 n=2e4, Teff=None, logg=None, feh=None,\n                 MAfn=None,\n                 rprs=None, trilegal_filename=None,\n                 starmodel=None,\n                 binary_starmodel=None, triple_starmodel=None,\n                 heb_kws=None, eb_kws=None,\n                 beb_kws=None, pl_kws=None, savefile=None,\n                 hide_exceptions=False, fit_trap=True,\n                 do_only=None):\n        \"\"\"\n        Generates PopulationSet.\n        \"\"\"\n        do_all = False\n        if do_only is None:\n            do_all = True\n            do_only = DEFAULT_MODELS\n\n        if MAfn is None:\n            MAfn = MAInterpolationFunction(pmin=0.007, pmax=1/0.007, nzs=200, nps=400)\n\n        if beb_kws is None:\n            beb_kws = {}\n        if heb_kws is None:\n            heb_kws = {}\n        if eb_kws is None:\n            eb_kws = {}\n        if pl_kws is None:\n            pl_kws = {}\n\n        if 'heb' in do_only:\n            try:\n                hebpop = HEBPopulation(mags=mags,\n                                       Teff=Teff, logg=logg, feh=feh,\n                                       period=period, cadence=cadence,\n                                       starmodel=triple_starmodel,\n                                       starfield=trilegal_filename,\n                                       MAfn=MAfn, n=n, **heb_kws)\n                if fit_trap:\n                    hebpop.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    if do_all:\n                        hebpop.save_hdf(savefile, 'heb', overwrite=True)\n                    else:\n                        hebpop.save_hdf(savefile, 'heb', append=True)\n            except:\n                logging.error('Error generating HEB population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'heb_Px2' in do_only:\n            try:\n                hebpop_Px2 = HEBPopulation_Px2(mags=mags,\n                                       Teff=Teff, logg=logg, feh=feh,\n                                       period=period, cadence=cadence,\n                                       starmodel=triple_starmodel,\n                                       starfield=trilegal_filename,\n                                       MAfn=MAfn, n=n, **heb_kws)\n                if fit_trap:\n                    hebpop_Px2.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    if do_all:\n                        hebpop_Px2.save_hdf(savefile, 'heb_Px2', overwrite=True)\n                    else:\n                        hebpop_Px2.save_hdf(savefile, 'heb_Px2', append=True)\n            except:\n                logging.error('Error generating HEB_Px2 population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'eb' in do_only:\n            try:\n                ebpop = EBPopulation(mags=mags,\n                                     Teff=Teff, logg=logg, feh=feh,\n                                     period=period, cadence=cadence,\n                                     starmodel=binary_starmodel,\n                                     starfield=trilegal_filename,\n                                     MAfn=MAfn, n=n, **eb_kws)\n                if fit_trap:\n                    ebpop.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    ebpop.save_hdf(savefile, 'eb', append=True)\n            except:\n                logging.error('Error generating EB population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'eb_Px2' in do_only:\n            try:\n                ebpop_Px2 = EBPopulation_Px2(mags=mags,\n                                     Teff=Teff, logg=logg, feh=feh,\n                                     period=period, cadence=cadence,\n                                     starmodel=binary_starmodel,\n                                     starfield=trilegal_filename,\n                                     MAfn=MAfn, n=n, **eb_kws)\n                if fit_trap:\n                    ebpop_Px2.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    ebpop_Px2.save_hdf(savefile, 'eb_Px2', append=True)\n            except:\n                logging.error('Error generating EB_Px2 population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'beb' in do_only:\n            try:\n                bebpop = BEBPopulation(trilegal_filename=trilegal_filename,\n                                       ra=ra, dec=dec, period=period, cadence=cadence,\n                                       mags=mags, MAfn=MAfn, n=n, **beb_kws)\n                if fit_trap:\n                    bebpop.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    bebpop.save_hdf(savefile, 'beb', append=True)\n            except:\n                logging.error('Error generating BEB population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'beb_Px2' in do_only:\n            try:\n                bebpop_Px2 = BEBPopulation_Px2(trilegal_filename=trilegal_filename,\n                                       ra=ra, dec=dec, period=period, cadence=cadence,\n                                       mags=mags, MAfn=MAfn, n=n, **beb_kws)\n                if fit_trap:\n                    bebpop_Px2.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    bebpop_Px2.save_hdf(savefile, 'beb_Px2', append=True)\n            except:\n                logging.error('Error generating BEB_Px2 population.')\n                if not hide_exceptions:\n                    raise\n\n        if 'pl' in do_only:\n            try:\n                plpop = PlanetPopulation(period=period, cadence=cadence,\n                                         rprs=rprs,\n                                         starmodel=starmodel,\n                                         MAfn=MAfn, n=n, **pl_kws)\n\n                if fit_trap:\n                    plpop.fit_trapezoids(MAfn=MAfn)\n                if savefile is not None:\n                    plpop.save_hdf(savefile, 'pl', append=True)\n            except:\n                logging.error('Error generating Planet population.')\n                if not hide_exceptions:\n                    raise\n\n        if not do_all and savefile is not None:\n            hebpop = HEBPopulation.load_hdf(savefile, 'heb')\n            hebpop_Px2 = HEBPopulation.load_hdf(savefile, 'heb_Px2')\n            ebpop = EBPopulation.load_hdf(savefile, 'eb')\n            ebpop_Px2 = EBPopulation.load_hdf(savefile, 'eb_Px2')\n            bebpop = BEBPopulation.load_hdf(savefile, 'beb')\n            bebpop_Px2 = BEBPopulation.load_hdf(savefile, 'beb_Px2')\n            plpop = PlanetPopulation.load_hdf(savefile, 'pl')\n\n\n        self.poplist = [hebpop, hebpop_Px2,\n                        ebpop, ebpop_Px2,\n                        bebpop, bebpop_Px2, plpop]", "response": "Generates a new population set for the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of unique constraints among all populations in set.", "response": "def constraints(self):\n        \"\"\"\n        Unique list of constraints among all populations in set.\n        \"\"\"\n        cs = []\n        for pop in self.poplist:\n            cs += [c for c in pop.constraints]\n        return list(set(cs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_hdf(self, filename, path='', overwrite=False):\n        if os.path.exists(filename) and overwrite:\n            os.remove(filename)\n\n        for pop in self.poplist:\n            name = pop.modelshort\n            pop.save_hdf(filename, path='{}/{}'.format(path,name), append=True)", "response": "Saves PopulationSet to HDF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_hdf(cls, filename, path=''):\n        with pd.HDFStore(filename) as store:\n            models = []\n            types = []\n            for k in store.keys():\n                m = re.search('/(\\S+)/stars', k)\n                if m:\n                    models.append(m.group(1))\n                    types.append(store.get_storer(m.group(0)).attrs.poptype)\n        poplist = []\n        for m,t in zip(models,types):\n            poplist.append(t().load_hdf(filename, path='{}/{}'.format(path,m)))\n\n        return cls(poplist)", "response": "Loads PopulationSet from file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a population to the PopulationSet.", "response": "def add_population(self,pop):\n        \"\"\"Adds population to PopulationSet\n        \"\"\"\n        if pop.model in self.modelnames:\n            raise ValueError('%s model already in PopulationSet.' % pop.model)\n        self.modelnames.append(pop.model)\n        self.shortmodelnames.append(pop.modelshort)\n        self.poplist.append(pop)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a population from the PopulationSet.", "response": "def remove_population(self,pop):\n        \"\"\"Removes population from PopulationSet\n        \"\"\"\n        iremove=None\n        for i in range(len(self.poplist)):\n            if self.modelnames[i]==self.poplist[i].model:\n                iremove=i\n        if iremove is not None:\n            self.modelnames.pop(i)\n            self.shortmodelnames.pop(i)\n            self.poplist.pop(i)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef colordict(self):\n        d = {}\n        i=0\n        n = len(self.constraints)\n        for c in self.constraints:\n            #self.colordict[c] = colors[i % 6]\n            d[c] = cm.jet(1.*i/n)\n            i+=1\n        return d", "response": "Returns a dictionary holding colors that correspond to constraints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef priorfactors(self):\n        priorfactors = {}\n        for pop in self.poplist:\n            for f in pop.priorfactors:\n                if f in priorfactors:\n                    if pop.priorfactors[f] != priorfactors[f]:\n                        raise ValueError('prior factor %s is inconsistent!' % f)\n                else:\n                    priorfactors[f] = pop.priorfactors[f]\n        return priorfactors", "response": "Combinartion of priorfactors from all populations\n othewise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange prior factors in all populations", "response": "def change_prior(self,**kwargs):\n        \"\"\"Changes prior factor(s) in all populations\n        \"\"\"\n        for kw,val in kwargs.items():\n            if kw=='area':\n                logging.warning('cannot change area in this way--use change_maxrad instead')\n                continue\n            for pop in self.poplist:\n                k = {kw:val}\n                pop.change_prior(**k)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies constraints corresponding to measuring transit in different band", "response": "def apply_multicolor_transit(self,band,depth):\n        \"\"\"\n        Applies constraint corresponding to measuring transit in different band\n\n        This is not implemented yet.\n        \"\"\"\n        if '{} band transit'.format(band) not in self.constraints:\n            self.constraints.append('{} band transit'.format(band))\n        for pop in self.poplist:\n            pop.apply_multicolor_transit(band,depth)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_maxrad(self,newrad):\n        if not isinstance(newrad, Quantity):\n            newrad = newrad * u.arcsec\n        #if 'Rsky' not in self.constraints:\n        #    self.constraints.append('Rsky')\n        for pop in self.poplist:\n            if not pop.is_specific:\n                try:\n                    pop.maxrad = newrad\n                except AttributeError:\n                    pass", "response": "Sets max allowed radius in populations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a constraint that sets the maximum brightness for non - target star to the maximum brightness for non - target star.", "response": "def apply_dmaglim(self,dmaglim=None):\n        \"\"\"\n        Applies a constraint that sets the maximum brightness for non-target star\n\n        :func:`stars.StarPopulation.set_dmaglim` not yet implemented.\n\n        \"\"\"\n        raise NotImplementedError\n        if 'bright blend limit' not in self.constraints:\n            self.constraints.append('bright blend limit')\n        for pop in self.poplist:\n            if not hasattr(pop,'dmaglim') or pop.is_specific:\n                continue\n            if dmaglim is None:\n                dmag = pop.dmaglim\n            else:\n                dmag = dmaglim\n            pop.set_dmaglim(dmag)\n        self.dmaglim = dmaglim"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies constraint corresponding to RV trend non - detection to each population.", "response": "def apply_trend_constraint(self, limit, dt, **kwargs):\n        \"\"\"\n        Applies constraint corresponding to RV trend non-detection to each population\n\n        See :func:`stars.StarPopulation.apply_trend_constraint`;\n        all arguments passed to that function for each population.\n\n        \"\"\"\n        if 'RV monitoring' not in self.constraints:\n            self.constraints.append('RV monitoring')\n        for pop in self.poplist:\n            if not hasattr(pop,'dRV'):\n                continue\n            pop.apply_trend_constraint(limit, dt, **kwargs)\n        self.trend_limit = limit\n        self.trend_dt = dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_secthresh(self, secthresh, **kwargs):\n\n        if 'secondary depth' not in self.constraints:\n            self.constraints.append('secondary depth')\n        for pop in self.poplist:\n            if not isinstance(pop, EclipsePopulation_Px2):\n                pop.apply_secthresh(secthresh, **kwargs)\n        self.secthresh = secthresh", "response": "Applies secondary depth constraint to each entry in the population."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstrain the difference b/w primary and secondary to be < diff", "response": "def constrain_oddeven(self, diff, **kwargs):\n        \"\"\"Constrains the difference b/w primary and secondary to be < diff\n        \"\"\"\n        if 'odd-even' not in self.constraints:\n            self.constraints.append('odd-even')\n        for pop in self.poplist:\n            if isinstance(pop, EclipsePopulation_Px2):\n                pop.constrain_oddeven(diff, **kwargs)\n        self.oddeven_diff = diff"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstrain property for each stars in the list.", "response": "def constrain_property(self,prop,**kwargs):\n        \"\"\"\n        Constrains property for each population\n\n        See :func:`vespa.stars.StarPopulation.constrain_property`;\n        all arguments passed to that function for each population.\n\n        \"\"\"\n        if prop not in self.constraints:\n            self.constraints.append(prop)\n        for pop in self.poplist:\n            try:\n                pop.constrain_property(prop,**kwargs)\n            except AttributeError:\n                logging.info('%s model does not have property stars.%s (constraint not applied)' % (pop.model,prop))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing removed constraint in each population.", "response": "def replace_constraint(self,name,**kwargs):\n        \"\"\"\n        Replaces removed constraint in each population.\n\n        See :func:`vespa.stars.StarPopulation.replace_constraint`\n\n        \"\"\"\n\n        for pop in self.poplist:\n            pop.replace_constraint(name,**kwargs)\n        if name not in self.constraints:\n            self.constraints.append(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_constraint(self,*names):\n        for name in names:\n            for pop in self.poplist:\n                if name in pop.constraints:\n                    pop.remove_constraint(name)\n                else:\n                    logging.info('%s model does not have %s constraint' % (pop.model,name))\n            if name in self.constraints:\n                self.constraints.remove(name)", "response": "Removes constraint from each population"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply contrast curve constraint to each population.", "response": "def apply_cc(self, cc, **kwargs):\n        \"\"\"\n        Applies contrast curve constraint to each population\n\n        See :func:`vespa.stars.StarPopulation.apply_cc`;\n        all arguments passed to that function for each population.\n\n        \"\"\"\n        if type(cc)==type(''):\n            pass\n        if cc.name not in self.constraints:\n            self.constraints.append(cc.name)\n        for pop in self.poplist:\n            if not pop.is_specific:\n                try:\n                    pop.apply_cc(cc, **kwargs)\n                except AttributeError:\n                    logging.info('%s cc not applied to %s model' % (cc.name,pop.model))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies velocity contrast curve constraint to each entry in the list of alternatives.", "response": "def apply_vcc(self,vcc):\n        \"\"\"\n        Applies velocity contrast curve constraint to each population\n\n        See :func:`vespa.stars.StarPopulation.apply_vcc`;\n        all arguments passed to that function for each population.\n\n        \"\"\"\n        if 'secondary spectrum' not in self.constraints:\n            self.constraints.append('secondary spectrum')\n        for pop in self.poplist:\n            if not pop.is_specific:\n                try:\n                    pop.apply_vcc(vcc)\n                except:\n                    logging.info('VCC constraint not applied to %s model' % (pop.model))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_trilegal(filename,ra,dec,folder='.', galactic=False,\n                 filterset='kepler_2mass',area=1,maglim=27,binaries=False,\n                 trilegal_version='1.6',sigma_AV=0.1,convert_h5=True):\n    \"\"\"Runs get_trilegal perl script; optionally saves output into .h5 file\n\n    Depends on a perl script provided by L. Girardi; calls the\n    web form simulation, downloads the file, and (optionally) converts\n    to HDF format.\n\n    Uses A_V at infinity from :func:`utils.get_AV_infinity`.\n\n    .. note::\n\n        Would be desirable to re-write the get_trilegal script\n        all in python.\n\n    :param filename:\n        Desired output filename.  If extension not provided, it will\n        be added.\n\n    :param ra,dec:\n        Coordinates (ecliptic) for line-of-sight simulation.\n\n    :param folder: (optional)\n        Folder to which to save file.  *Acknowledged, file control\n        in this function is a bit wonky.*\n\n    :param filterset: (optional)\n        Filter set for which to call TRILEGAL.\n\n    :param area: (optional)\n        Area of TRILEGAL simulation [sq. deg]\n\n    :param maglim: (optional)\n        Limiting magnitude in first mag (by default will be Kepler band)\n        If want to limit in different band, then you have to\n        got directly to the ``get_trilegal`` perl script.\n\n    :param binaries: (optional)\n        Whether to have TRILEGAL include binary stars.  Default ``False``.\n\n    :param trilegal_version: (optional)\n        Default ``'1.6'``.\n\n    :param sigma_AV: (optional)\n        Fractional spread in A_V along the line of sight.\n\n    :param convert_h5: (optional)\n        If true, text file downloaded from TRILEGAL will be converted\n        into a ``pandas.DataFrame`` stored in an HDF file, with ``'df'``\n        path.\n\n    \"\"\"\n    if galactic:\n        l, b = ra, dec\n    else:\n        try:\n            c = SkyCoord(ra,dec)\n        except UnitsError:\n            c = SkyCoord(ra,dec,unit='deg')\n        l,b = (c.galactic.l.value,c.galactic.b.value)\n\n    if os.path.isabs(filename):\n        folder = ''\n\n    if not re.search('\\.dat$',filename):\n        outfile = '{}/{}.dat'.format(folder,filename)\n    else:\n        outfile = '{}/{}'.format(folder,filename)\n    AV = get_AV_infinity(l,b,frame='galactic')\n    #cmd = 'get_trilegal %s %f %f %f %i %.3f %.2f %s 1 %.1f %s' % (trilegal_version,l,b,\n    #                                                              area,binaries,AV,sigma_AV,\n    #                                                              filterset,maglim,outfile)\n    #sp.Popen(cmd,shell=True).wait()\n    trilegal_webcall(trilegal_version,l,b,area,binaries,AV,sigma_AV,filterset,maglim,outfile)\n    if convert_h5:\n        df = pd.read_table(outfile, sep='\\s+', skipfooter=1, engine='python')\n        df = df.rename(columns={'#Gc':'Gc'})\n        for col in df.columns:\n            if col not in NONMAG_COLS:\n                df.rename(columns={col:'{}_mag'.format(col)},inplace=True)\n        if not re.search('\\.h5$', filename):\n            h5file = '{}/{}.h5'.format(folder,filename)\n        else:\n            h5file = '{}/{}'.format(folder,filename)\n        df.to_hdf(h5file,'df')\n        with pd.HDFStore(h5file) as store:\n            attrs = store.get_storer('df').attrs\n            attrs.trilegal_args = {'version':trilegal_version,\n                                   'ra':ra, 'dec':dec,\n                                   'l':l,'b':b,'area':area,\n                                   'AV':AV, 'sigma_AV':sigma_AV,\n                                   'filterset':filterset,\n                                   'maglim':maglim,\n                                   'binaries':binaries}\n        os.remove(outfile)", "response": "Runs get_trilegal perl script and saves output into HDF format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trilegal_webcall(trilegal_version,l,b,area,binaries,AV,sigma_AV,filterset,maglim,\n\t\t\t\t\t outfile):\n    \"\"\"Calls TRILEGAL webserver and downloads results file.\n    :param trilegal_version:\n        Version of trilegal (only tested on 1.6).\n    :param l,b:\n        Coordinates (galactic) for line-of-sight simulation.\n    :param area:\n        Area of TRILEGAL simulation [sq. deg]\n    :param binaries:\n        Whether to have TRILEGAL include binary stars.  Default ``False``.\n    :param AV:\n    \tExtinction along the line of sight.\n    :param sigma_AV:\n        Fractional spread in A_V along the line of sight.\n    :param filterset: (optional)\n        Filter set for which to call TRILEGAL.\n    :param maglim:\n        Limiting magnitude in mag (by default will be 1st band of filterset)\n        If want to limit in different band, then you have to\n        change function directly.\n    :param outfile:\n        Desired output filename.\n    \"\"\"\n    webserver = 'http://stev.oapd.inaf.it'\n    args = [l,b,area,AV,sigma_AV,filterset,maglim,1,binaries]\n    mainparams = ('imf_file=tab_imf%2Fimf_chabrier_lognormal.dat&binary_frac=0.3&'\n    \t\t\t  'binary_mrinf=0.7&binary_mrsup=1&extinction_h_r=100000&extinction_h_z='\n    \t\t\t  '110&extinction_kind=2&extinction_rho_sun=0.00015&extinction_infty={}&'\n    \t\t\t  'extinction_sigma={}&r_sun=8700&z_sun=24.2&thindisk_h_r=2800&'\n    \t\t\t  'thindisk_r_min=0&thindisk_r_max=15000&thindisk_kind=3&thindisk_h_z0='\n    \t\t\t  '95&thindisk_hz_tau0=4400000000&thindisk_hz_alpha=1.6666&'\n    \t\t\t  'thindisk_rho_sun=59&thindisk_file=tab_sfr%2Ffile_sfr_thindisk_mod.dat&'\n    \t\t\t  'thindisk_a=0.8&thindisk_b=0&thickdisk_kind=0&thickdisk_h_r=2800&'\n    \t\t\t  'thickdisk_r_min=0&thickdisk_r_max=15000&thickdisk_h_z=800&'\n    \t\t\t  'thickdisk_rho_sun=0.0015&thickdisk_file=tab_sfr%2Ffile_sfr_thickdisk.dat&'\n    \t\t\t  'thickdisk_a=1&thickdisk_b=0&halo_kind=2&halo_r_eff=2800&halo_q=0.65&'\n    \t\t\t  'halo_rho_sun=0.00015&halo_file=tab_sfr%2Ffile_sfr_halo.dat&halo_a=1&'\n    \t\t\t  'halo_b=0&bulge_kind=2&bulge_am=2500&bulge_a0=95&bulge_eta=0.68&'\n    \t\t\t  'bulge_csi=0.31&bulge_phi0=15&bulge_rho_central=406.0&'\n    \t\t\t  'bulge_cutoffmass=0.01&bulge_file=tab_sfr%2Ffile_sfr_bulge_zoccali_p03.dat&'\n    \t\t\t  'bulge_a=1&bulge_b=-2.0e9&object_kind=0&object_mass=1280&object_dist=1658&'\n    \t\t\t  'object_av=1.504&object_avkind=1&object_cutoffmass=0.8&'\n    \t\t\t  'object_file=tab_sfr%2Ffile_sfr_m4.dat&object_a=1&object_b=0&'\n    \t\t\t  'output_kind=1').format(AV,sigma_AV)\n    cmdargs = [trilegal_version,l,b,area,filterset,1,maglim,binaries,mainparams,\n    \t\t   webserver,trilegal_version]\n    cmd = (\"wget -o lixo -Otmpfile --post-data='submit_form=Submit&trilegal_version={}\"\n    \t   \"&gal_coord=1&gc_l={}&gc_b={}&eq_alpha=0&eq_delta=0&field={}&photsys_file=\"\n    \t   \"tab_mag_odfnew%2Ftab_mag_{}.dat&icm_lim={}&mag_lim={}&mag_res=0.1&\"\n    \t   \"binary_kind={}&{}' {}/cgi-bin/trilegal_{}\").format(*cmdargs)\n    complete = False\n    while not complete:\n        notconnected = True\n        busy = True\n        print(\"TRILEGAL is being called with \\n l={} deg, b={} deg, area={} sqrdeg\\n \"\n        \"Av={} with {} fractional r.m.s. spread \\n in the {} system, complete down to \"\n        \"mag={} in its {}th filter, use_binaries set to {}.\".format(*args))\n        sp.Popen(cmd,shell=True).wait()\n        if os.path.exists('tmpfile') and os.path.getsize('tmpfile')>0:\n            notconnected = False\n        else:\n            print(\"No communication with {}, will retry in 2 min\".format(webserver))\n            time.sleep(120)\n        if not notconnected:\n            with open('tmpfile','r') as f:\n                lines = f.readlines()\n            for line in lines:\n                if 'The results will be available after about 2 minutes' in line:\n                    busy = False\n                    break\n            sp.Popen('rm -f lixo tmpfile',shell=True)\n            if not busy:\n                filenameidx = line.find('<a href=../tmp/') +15\n                fileendidx = line[filenameidx:].find('.dat')\n                filename = line[filenameidx:filenameidx+fileendidx+4]\n                print(\"retrieving data from {} ...\".format(filename))\n                while not complete:\n                    time.sleep(40)\n                    modcmd = 'wget -o lixo -O{} {}/tmp/{}'.format(filename,webserver,filename)\n                    modcall = sp.Popen(modcmd,shell=True).wait()\n                    if os.path.getsize(filename)>0:\n                        with open(filename,'r') as f:\n                            lastline = f.readlines()[-1]\n                        if 'normally' in lastline:\n                            complete = True\n                            print('model downloaded!..')\n                    if not complete:\n                        print('still running...')        \n            else:\n                print('Server busy, trying again in 2 minutes')\n                time.sleep(120)\n    sp.Popen('mv {} {}'.format(filename,outfile),shell=True).wait()\n    print('results copied to {}'.format(outfile))", "response": "Calls TRILEGAL webserver and downloads results file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_wrapper(self):\n        log = logging.getLogger('client.py')\n\n        # Set the log format and log level\n        try:\n            debug = self.params[\"debug\"]\n            log.setLevel(logging.DEBUG)\n        except KeyError:\n            log.setLevel(logging.INFO)\n\n        # Set the log format.\n        stream = logging.StreamHandler()\n        logformat = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            datefmt='%b %d %H:%M:%S')\n        stream.setFormatter(logformat)\n\n        log.addHandler(stream)\n        return log", "response": "Wrapper to set the logging parameters for output\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode setid as uint128", "response": "def decode_setid(encoded):\n    \"\"\"Decode setid as uint128\"\"\"\n    try:\n        lo, hi = struct.unpack('<QQ', b32decode(encoded.upper() + '======'))\n    except struct.error:\n        raise ValueError('Cannot decode {!r}'.format(encoded))\n    return (hi << 64) + lo"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_setid(uint128):\n    hi, lo = divmod(uint128, 2**64)\n    return b32encode(struct.pack('<QQ', lo, hi))[:-6].lower()", "response": "Encode uint128 setid as stripped b32encoded string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _reduce_opacity(self, watermark, opacity):\n\n        if watermark.type() != ImageType.TrueColorMatteType:\n            watermark.type(ImageType.TrueColorMatteType)\n        depth = 255 - int(255 * opacity)\n        watermark.quantumOperator(ChannelType.OpacityChannel, QuOp.MaxQuantumOp, depth)", "response": "Reduces the opacity of the watermark image to RGBA."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns development webserver. ATTENTION: By default it is only served on localhost. To run it within a container and access it from the outside, you need to forward the port and tell it to listen on all IPs instead of only localhost.", "response": "def marvcli_develop_server(port, public):\n    \"\"\"Run development webserver.\n\n    ATTENTION: By default it is only served on localhost. To run it\n    within a container and access it from the outside, you need to\n    forward the port and tell it to listen on all IPs instead of only\n    localhost.\n    \"\"\"\n    from flask_cors import CORS\n    app = create_app(push=False)\n    app.site.load_for_web()\n    CORS(app)\n\n    class IPDBMiddleware(object):\n        def __init__(self, app):\n            self.app = app\n\n        def __call__(self, environ, start_response):\n            from ipdb import launch_ipdb_on_exception\n            with launch_ipdb_on_exception():\n                appiter = self.app(environ, start_response)\n                for item in appiter:\n                    yield item\n\n    app.debug = True\n    if IPDB:\n        app.wsgi_app = IPDBMiddleware(app.wsgi_app)\n        app.run(use_debugger=False,\n                use_reloader=False,\n                host=('0.0.0.0' if public else '127.0.0.1'),\n                port=port,\n                threaded=False)\n    else:\n        app.run(host=('0.0.0.0' if public else '127.0.0.1'),\n                port=port,\n                reloader_type='watchdog',\n                threaded=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef marvcli_discard(datasets, all_nodes, nodes, tags, comments, confirm):\n    mark_discarded = not any([all_nodes, nodes, tags, comments])\n\n    site = create_app().site\n    setids = parse_setids(datasets)\n\n    if tags or comments:\n        if confirm:\n            msg = ' and '.join(filter(None, ['tags' if tags else None,\n                                             'comments' if comments else None]))\n            click.echo('About to delete {}'.format(msg))\n            click.confirm('This cannot be undone. Do you want to continue?', abort=True)\n\n        ids = [x[0] for x in db.session.query(Dataset.id).filter(Dataset.setid.in_(setids))]\n        if tags:\n            where = dataset_tag.c.dataset_id.in_(ids)\n            stmt = dataset_tag.delete().where(where)\n            db.session.execute(stmt)\n\n        if comments:\n            comment_table = Comment.__table__\n            where = comment_table.c.dataset_id.in_(ids)\n            stmt = comment_table.delete().where(where)\n            db.session.execute(stmt)\n\n    if nodes or all_nodes:\n        storedir = site.config.marv.storedir\n        for setid in setids:\n            setdir = os.path.join(storedir, setid)\n        # TODO: see where we are getting with dep tree tables\n\n    if mark_discarded:\n        dataset = Dataset.__table__\n        stmt = dataset.update()\\\n                      .where(dataset.c.setid.in_(setids))\\\n                      .values(discarded=True)\n        db.session.execute(stmt)\n\n    db.session.commit()", "response": "Delete the specified datasets and optionally discard associated data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef marvcli_undiscard(datasets):\n    create_app()\n\n    setids = parse_setids(datasets, discarded=True)\n    dataset = Dataset.__table__\n    stmt = dataset.update()\\\n                  .where(dataset.c.setid.in_(setids))\\\n                  .values(discarded=False)\n    db.session.execute(stmt)\n    db.session.commit()", "response": "Undiscard DATASETS previously discarded."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrestoring previously dumped database", "response": "def marvcli_restore(file):\n    \"\"\"Restore previously dumped database\"\"\"\n    data = json.load(file)\n    site = create_app().site\n    site.restore_database(**data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying datasets. Use --collection=* to list all datasets across all collections.", "response": "def marvcli_query(ctx, list_tags, collections, discarded, outdated, path, tags, null):\n    \"\"\"Query datasets.\n\n    Use --collection=* to list all datasets across all collections.\n    \"\"\"\n    if not any([collections, discarded, list_tags, outdated, path, tags]):\n        click.echo(ctx.get_help())\n        ctx.exit(1)\n\n    sep = '\\x00' if null else '\\n'\n\n    site = create_app().site\n\n    if '*' in collections:\n        collections = None\n    else:\n        for col in collections:\n            if col not in site.collections:\n                ctx.fail('Unknown collection: {}'.format(col))\n\n    if list_tags:\n        tags = site.listtags(collections)\n        if tags:\n            click.echo(sep.join(tags), nl=not null)\n        else:\n            click.echo('no tags', err=True)\n        return\n\n    setids = site.query(collections, discarded, outdated, path, tags)\n    if setids:\n        sep = '\\x00' if null else '\\n'\n        click.echo(sep.join(setids), nl=not null)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef marvcli_run(ctx, datasets, deps, excluded_nodes, force, force_dependent,\n                force_deps, keep, keep_going, list_nodes,\n                list_dependent, selected_nodes, update_detail,\n                update_listing, cachesize, collections):\n    \"\"\"Run nodes for selected datasets.\n\n    Datasets are specified by a list of set ids, or --collection\n    <name>, use --collection=* to run for all collections. --node in\n    conjunction with --collection=* will pick those collections for\n    which the selected nodes are configured.\n\n    Set ids may be abbreviated to any uniquely identifying\n    prefix. Suffix a prefix by '+' to match multiple.\n\n    \"\"\"\n    if collections and datasets:\n        ctx.fail('--collection and DATASETS are mutually exclusive')\n\n    if list_dependent and not selected_nodes:\n        ctx.fail('--list-dependent needs at least one selected --node')\n\n    if not any([datasets, collections, list_nodes]):\n        click.echo(ctx.get_help())\n        ctx.exit(1)\n\n    deps = 'force' if force_deps else deps\n    force = force_deps or force\n\n    site = create_app().site\n\n    if '*' in collections:\n        if selected_nodes:\n            collections = [k for k, v in site.collections.items()\n                           if set(v.nodes).issuperset(selected_nodes)]\n            if not collections:\n                ctx.fail('No collections have all selected nodes')\n        else:\n            collections = None\n    else:\n        for col in collections:\n            if col not in site.collections:\n                ctx.fail('Unknown collection: {}'.format(col))\n\n    if list_nodes:\n        for col in (collections or sorted(site.collections.keys())):\n            click.echo('{}:'.format(col))\n            for name in sorted(site.collections[col].nodes):\n                if name == 'dataset':\n                    continue\n                click.echo('    {}'.format(name))\n        return\n\n    if list_dependent:\n        for col in (collections or sorted(site.collections.keys())):\n            click.echo('{}:'.format(col))\n            dependent = {x for name in selected_nodes\n                         for x in site.collections[col].nodes[name].dependent}\n            for name in sorted(x.name for x in dependent):\n                click.echo('    {}'.format(name))\n        return\n\n    errors = []\n\n    setids = [SetID(x) for x in parse_setids(datasets)]\n    if not setids:\n        query = db.session.query(Dataset.setid)\\\n                           .filter(Dataset.discarded.isnot(True))\\\n                           .filter(Dataset.status.op('&')(STATUS_MISSING) == 0)\n        if collections is not None:\n            query = query.filter(Dataset.collection.in_(collections))\n        setids = (SetID(x[0]) for x in query)\n\n    for setid in setids:\n        if IPDB:\n            site.run(setid, selected_nodes, deps, force, keep,\n                     force_dependent, update_detail, update_listing,\n                     excluded_nodes, cachesize=cachesize)\n        else:\n            try:\n                site.run(setid, selected_nodes, deps, force, keep,\n                         force_dependent, update_detail, update_listing,\n                         excluded_nodes, cachesize=cachesize)\n            except UnknownNode as e:\n                ctx.fail('Collection {} has no node {}'.format(*e.args))\n            except NoResultFound:\n                click.echo('ERROR: unknown {!r}'.format(setid), err=True)\n                if not keep_going:\n                    raise\n            except BaseException as e:\n                errors.append(setid)\n                if isinstance(e, KeyboardInterrupt):\n                    log.warn('KeyboardInterrupt: aborting')\n                    raise\n                elif isinstance(e, DirectoryAlreadyExists):\n                    click.echo(\"\"\"\nERROR: Directory for node run already exists:\n{!r}\nIn case no other node run is in progress, this is a bug which you are kindly\nasked to report, providing information regarding any previous, failed node runs.\n\"\"\".format(e.args[0]), err=True)\n                    if not keep_going:\n                        ctx.abort()\n                else:\n                    log.error('Exception occured for dataset %s:', setid, exc_info=True)\n                    log.error('Error occured for dataset %s: %s', setid, e)\n                    if not keep_going:\n                        ctx.exit(1)\n    if errors:\n        log.error('There were errors for %r', errors)", "response": "Run the tree for the selected datasets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding or remove tags to datasets", "response": "def marvcli_tag(ctx, add, remove, datasets):\n    \"\"\"Add or remove tags to datasets\"\"\"\n    if not any([add, remove]) or not datasets:\n        click.echo(ctx.get_help())\n        ctx.exit(1)\n\n    app = create_app()\n    setids = parse_setids(datasets)\n    app.site.tag(setids, add, remove)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef marvcli_comment_add(user, message, datasets):\n    app = create_app()\n    try:\n        db.session.query(User).filter(User.name==user).one()\n    except NoResultFound:\n        click.echo(\"ERROR: No such user '{}'\".format(user), err=True)\n        sys.exit(1)\n    ids = parse_setids(datasets, dbids=True)\n    app.site.comment(user, message, ids)", "response": "Add comment as user for one or more datasets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist comments for datasets. Output : setid comment_id date time author message", "response": "def marvcli_comment_list(datasets):\n    \"\"\"Lists comments for datasets.\n\n    Output: setid comment_id date time author message\n    \"\"\"\n    app = create_app()\n    ids = parse_setids(datasets, dbids=True)\n    comments = db.session.query(Comment)\\\n                         .options(db.joinedload(Comment.dataset))\\\n                         .filter(Comment.dataset_id.in_(ids))\n    for comment in sorted(comments, key=lambda x: (x.dataset._setid, x.id)):\n        print(comment.dataset.setid, comment.id,\n              datetime.datetime.fromtimestamp(int(comment.time_added / 1000)),\n              comment.author, repr(comment.text))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef marvcli_comment_rm(ids):\n    app = create_app()\n    db.session.query(Comment)\\\n              .filter(Comment.id.in_(ids))\\\n              .delete(synchronize_session=False)\n    db.session.commit()", "response": "Remove comments by id as given in second column of marv comment list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef marvcli_user_add(ctx, username, password):\n    if not re.match(r'[0-9a-zA-Z\\-_\\.@+]+$', username):\n        click.echo('Invalid username: {}'.format(username), err=True)\n        click.echo('Must only contain ASCII letters, numbers, dash, underscore and dot',\n                   err=True)\n        sys.exit(1)\n    if password is None:\n        password = click.prompt('Password', hide_input=True, confirmation_prompt=True)\n    app = create_app()\n    try:\n        app.um.user_add(username, password, 'marv', '')\n    except ValueError as e:\n        click.echo('Error: {}'.format(e.args[0], err=True))\n        sys.exit(1)", "response": "Add a user to Marvcli"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef marvcli_user_pw(ctx, username, password):\n    app = create_app()\n    try:\n        app.um.user_pw(username, password)\n    except ValueError as e:\n        ctx.fail(e.args[0])", "response": "Change password of user"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for _watermark that takes an image and options and returns a new image object.", "response": "def watermark(self, image, options):\n        \"\"\"\n        Wrapper for ``_watermark``\n\n        Takes care of all the options handling.\n        \"\"\"\n        watermark_img = options.get(\"watermark\", settings.THUMBNAIL_WATERMARK)\n        if not watermark_img:\n            raise AttributeError(\"No THUMBNAIL_WATERMARK defined or set on tag.\")\n        watermark_path = find(watermark_img)\n        if not watermark_path:\n            raise RuntimeError(\"Could not find the configured watermark file.\")\n        if not os.path.isfile(watermark_path):\n            raise RuntimeError(\"Set watermark does not point to a file.\")\n\n        if \"cropbox\" not in options:\n            options[\"cropbox\"] = None\n        if \"watermark_alpha\" not in options:\n            options[\"watermark_alpha\"] = settings.THUMBNAIL_WATERMARK_OPACITY\n\n        mark_sizes = options.get(\"watermark_size\", settings.THUMBNAIL_WATERMARK_SIZE)\n        if mark_sizes:\n            try:\n                options[\"watermark_size\"] = parse_geometry(\n                    mark_sizes, self.get_image_ratio(image, options)\n                )\n            except TypeError as e:\n                raise TypeError(\n                    \"Please, update sorl-thumbnail package version to  >= 11.12b. %s\"\n                    % e\n                )\n        else:\n            options[\"watermark_size\"] = False\n\n        if \"watermark_pos\" not in options:\n            options[\"watermark_pos\"] = settings.THUMBNAIL_WATERMARK_POSITION\n\n        return self._watermark(\n            image,\n            watermark_path,\n            options[\"watermark_alpha\"],\n            options[\"watermark_size\"],\n            options[\"watermark_pos\"],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the new watermark size.", "response": "def _get_new_watermark_size(self, size, mark_default_size):\n        \"\"\"\n        New size can be passed as a pair of valuer (tuple) or\n        a fsloat (persentage case)\n        \"\"\"\n        if hasattr(size, \"__getitem__\"):\n            # a tuple or any iterable already\n            mark_size = size\n        elif isinstance(size, float):\n            mark_size = map(lambda coord: int(coord * size), mark_default_size)\n        else:\n            raise ImproperlyConfigured(\n                \"Watermark sizes must be a pair \" \"of integers or a float number\"\n            )\n        return mark_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict of functions available for listing columns and filters.", "response": "def make_funcs(dataset, setdir, store):\n    \"\"\"Functions available for listing columns and filters.\"\"\"\n    return {\n        'cat': lambda *lists: [x for lst in lists for x in lst],\n        'comments': lambda: None,\n        'detail_route': detail_route,\n        'format': lambda fmt, *args: fmt.format(*args),\n        'get': partial(getnode, dataset, setdir, store),\n        'join': lambda sep, *args: sep.join([x for x in args if x]),\n        'len': len,\n        'link': (lambda href, title, target=None:\n                 {'href': href or \"\",\n                  'title': title or \"\",\n                  'target': '_blank' if target is None else target}),\n        'list': lambda *x: filter(None, list(x)),\n        'max': max,\n        'min': min,\n        'status': lambda: ['#STATUS#'],\n        'sum': sum,\n        'tags': lambda: ['#TAGS#'],\n        'trace': print_trace,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_summary_funcs(rows, ids):\n    return {\n        'len': len,\n        'list': lambda *x: filter(None, list(x)),\n        'max': max,\n        'min': min,\n        'rows': partial(summary_rows, rows, ids),\n        'sum': sum,\n        'trace': print_trace\n    }", "response": "Functions available for listing summary fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cached_property(func):\n    @functools.wraps(func)\n    def cached_func(self):\n        cacheattr = '_{}'.format(func.func_name)\n        try:\n            return getattr(self, cacheattr)\n        except AttributeError:\n            value = func(self)\n            setattr(self, cacheattr, value)\n            return value\n    return property(cached_func)", "response": "Create read - only property that caches its function s value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a stream for publishing messages.", "response": "def create_stream(name, **header):\n    \"\"\"Create a stream for publishing messages.\n\n    All keyword arguments will be used to form the header.\n    \"\"\"\n    assert isinstance(name, basestring), name\n    return CreateStream(parent=None, name=name, group=False, header=header)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npull next message for a given handle.", "response": "def pull(handle, enumerate=False):\n    \"\"\"Pulls next message for handle.\n\n    Args:\n        handle: A :class:`.stream.Handle` or GroupHandle.\n        enumerate (bool): boolean to indicate whether a tuple ``(idx, msg)``\n            should be returned, not unlike Python's enumerate().\n\n    Returns:\n        A :class:`Pull` task to be yielded. Marv will send the\n        corresponding message as soon as it is available. For groups\n        this message will be a handle to a member of the\n        group. Members of groups are either streams or groups.\n\n    Examples:\n        Pulling (enumerated) message from stream::\n\n            msg = yield marv.pull(stream)\n            idx, msg = yield marv.pull(stream, enumerate=True)\n\n        Pulling stream from group and message from stream::\n\n            stream = yield marv.pull(group)  # a group of streams\n            msg = yield marv.pull(stream)\n\n    \"\"\"\n    assert isinstance(handle, Handle), handle\n    return Pull(handle, enumerate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an endpoint to the set of endpoints.", "response": "def add_endpoint(self, ep):\n        \"\"\"endpoints and groups are all the same (for now)\"\"\"\n        assert ep.name not in self.endpoints, ep\n        self.endpoints[ep.name] = ep"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_geometry(geometry, ratio=None):\n    if \"%\" not in geometry:\n        # fall back to old parser\n        return xy_geometry_parser(geometry, ratio)\n    # parse with float so geometry strings like \"42.11%\" are possible\n    return float(geometry.strip(\"%\")) / 100.0", "response": "Enhanced parse_geometry parser with percentage support."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract first image of input stream to jpg file.", "response": "def image(cam):\n    \"\"\"Extract first image of input stream to jpg file.\n\n    Args:\n        cam: Input stream of raw rosbag messages.\n\n    Returns:\n        File instance for first image of input stream.\n    \"\"\"\n    # Set output stream title and pull first message\n    yield marv.set_header(title=cam.topic)\n    msg = yield marv.pull(cam)\n    if msg is None:\n        return\n\n    # Deserialize raw ros message\n    pytype = get_message_type(cam)\n    rosmsg = pytype()\n    rosmsg.deserialize(msg.data)\n\n    # Write image to jpeg and push it to output stream\n    name = '{}.jpg'.format(cam.topic.replace('/', ':')[1:])\n    imgfile = yield marv.make_file(name)\n    img = imgmsg_to_cv2(rosmsg, \"rgb8\")\n    cv2.imwrite(imgfile.path, img, (cv2.IMWRITE_JPEG_QUALITY, 60))\n    yield marv.push(imgfile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating detail section with one image.", "response": "def image_section(image, title):\n    \"\"\"Create detail section with one image.\n\n    Args:\n        title (str): Title to be displayed for detail section.\n        image: marv image file.\n\n    Returns\n        One detail section.\n    \"\"\"\n    # pull first image\n    img = yield marv.pull(image)\n    if img is None:\n        return\n\n    # create image widget and section containing it\n    widget = {'title': image.title, 'image': {'src': img.relpath}}\n    section = {'title': title, 'widgets': [widget]}\n    yield marv.push(section)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting images from input stream to jpg files.", "response": "def images(cam):\n    \"\"\"Extract images from input stream to jpg files.\n\n    Args:\n        cam: Input stream of raw rosbag messages.\n\n    Returns:\n        File instances for images of input stream.\n    \"\"\"\n    # Set output stream title and pull first message\n    yield marv.set_header(title=cam.topic)\n\n    # Fetch and process first 20 image messages\n    name_template = '%s-{}.jpg' % cam.topic.replace('/', ':')[1:]\n    while True:\n        idx, msg = yield marv.pull(cam, enumerate=True)\n        if msg is None or idx >= 20:\n            break\n\n        # Deserialize raw ros message\n        pytype = get_message_type(cam)\n        rosmsg = pytype()\n        rosmsg.deserialize(msg.data)\n\n        # Write image to jpeg and push it to output stream\n        img = imgmsg_to_cv2(rosmsg, \"rgb8\")\n        name = name_template.format(idx)\n        imgfile = yield marv.make_file(name)\n        cv2.imwrite(imgfile.path, img)\n        yield marv.push(imgfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates detail section with gallery.", "response": "def gallery_section(images, title):\n    \"\"\"Create detail section with gallery.\n\n    Args:\n        title (str): Title to be displayed for detail section.\n        images: stream of marv image files\n\n    Returns\n        One detail section.\n    \"\"\"\n    # pull all images\n    imgs = []\n    while True:\n        img = yield marv.pull(images)\n        if img is None:\n            break\n        imgs.append({'src': img.relpath})\n    if not imgs:\n        return\n\n    # create gallery widget and section containing it\n    widget = {'title': images.title, 'gallery': {'images': imgs}}\n    section = {'title': title, 'widgets': [widget]}\n    yield marv.push(section)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a decorator that can be used to create a node with given arguments.", "response": "def node(schema=None, header=None, group=None, version=None):\n    \"\"\"Turn function into node.\n\n    Args:\n        schema: capnproto schema describing the output messages format\n        header: This parameter is currently not supported and only for\n            internal usage.\n        group (bool): A boolean indicating whether the default stream\n            of the node is a group, meaning it will be used to\n            published handles for streams or further groups. In case\n            of :paramref:`marv.input.foreach` specifications this flag will\n            default to `True`. This parameter is currently only for\n            internal usage.\n        version (int): This parameter currently has no effect.\n\n    Returns:\n        A :class:`Node` instance according to the given\n        arguments and :func:`input` decorators.\n    \"\"\"\n    def deco(func):\n        \"\"\"Turn function into node with given arguments.\n\n        :func:`node`(schema={!r}, header={!r}, group={!r})\n        \"\"\".format(schema, header, group)\n\n        if isinstance(func, Node):\n            raise TypeError('Attempted to convert function into node twice.')\n        assert isgeneratorfunction(func), \\\n            \"Node {}:{} needs to be a generator function\".format(func.__module__,\n                                                                 func.__name__)\n\n        specs = getattr(func, '__marv_input_specs__', None)\n        if hasattr(func, '__marv_input_specs__'):\n            del func.__marv_input_specs__\n\n        node = Node(func, schema=schema, header_schema=header,\n                    group=group, specs=specs, version=version)\n        functools.update_wrapper(node, func)\n        return node\n    return deco"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef indent(rows, hasHeader=False, headerChar='-', delim=' | ', justify='left',\n           separateRows=False, prefix='', postfix='', wrapfunc=lambda x: x):\n        '''Indents a table by column.\n             - rows: A sequence of sequences of items, one sequence per row.\n             - hasHeader: True if the first row consists of the columns' names.\n             - headerChar: Character to be used for the row separator line\n                 (if hasHeader==True or separateRows==True).\n             - delim: The column delimiter.\n             - justify: Determines how are data justified in their column.\n                 Valid values are 'left','right' and 'center'.\n             - separateRows: True if rows are to be separated by a line\n                 of 'headerChar's.\n             - prefix: A string prepended to each printed row.\n             - postfix: A string appended to each printed row.\n             - wrapfunc: A function f(text) for wrapping text; each element in\n                 the table is first wrapped by this function.'''\n        # closure for breaking logical rows to physical, using wrapfunc\n        def rowWrapper(row):\n                newRows = [wrapfunc(item).split('\\n') for item in row]\n                return [[substr or '' for substr in item] for item in map(None, *newRows)]  # NOQA\n        # break each logical row into one or more physical ones\n        logicalRows = [rowWrapper(row) for row in rows]\n        # columns of physical rows\n        columns = map(None, *reduce(operator.add, logicalRows))\n        # get the maximum of each column by the string length of its items\n        maxWidths = [max([len(str(item)) for item in column])\n                     for column in columns]\n        rowSeparator = headerChar * (len(prefix) + len(postfix) +\n                                     sum(maxWidths) +\n                                     len(delim)*(len(maxWidths)-1))\n        # select the appropriate justify method\n        justify = {'center': str.center, 'right': str.rjust, 'left': str.ljust}[justify.lower()]  # NOQA\n        output = cStringIO.StringIO()\n        if separateRows:\n            print >> output, rowSeparator\n        for physicalRows in logicalRows:\n            for row in physicalRows:\n                print >> output, prefix \\\n                    + delim.join([justify(str(item), width) for (item, width) in zip(row, maxWidths)]) + postfix  # NOQA\n            if separateRows or hasHeader:\n                print >> output, rowSeparator\n                hasHeader = False\n        return output.getvalue()", "response": "Indents a table by column."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef message(message, name=None):\n    def decorator(func):\n        wf = update_wrapper(Msg(func, message), func)\n        if name:\n            wf.name = name\n        return wf\n    return decorator", "response": "A decorator that applies a message to a sequence of functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a name on a validator callable.", "response": "def name(name, validator=None):\n    \"\"\" Set a name on a validator callable.\n\n    Useful for user-friendly reporting when using lambdas to populate the [`Invalid.expected`](#invalid) field:\n\n    ```python\n    from good import Schema, name\n\n    Schema(lambda x: int(x))('a')\n    #-> Invalid: invalid literal for int(): expected <lambda>(), got\n    Schema(name('int()', lambda x: int(x))('a')\n    #-> Invalid: invalid literal for int(): expected int(), got a\n    ```\n\n    Note that it is only useful with lambdas, since function name is used if available:\n    see notes on [Schema Callables](#callables).\n\n    :param name: Name to assign on the validator callable\n    :type name: unicode\n    :param validator: Validator callable. If not provided -- a decorator is returned instead:\n\n        ```python\n        from good import name\n\n        @name(u'int()')\n        def int(v):\n            return int(v)\n        ```\n\n    :type validator: callable\n    :return: The same validator callable\n    :rtype: callable\n    \"\"\"\n    # Decorator mode\n    if validator is None:\n        def decorator(f):\n            f.name = name\n            return f\n        return decorator\n\n    # Direct mode\n    validator.name = name\n    return validator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_z(cls, offset):\n        assert len(offset) == 5, 'Invalid offset string format, must be \"+HHMM\"'\n        return timedelta(hours=int(offset[:3]), minutes=int(offset[0] + offset[3:]))", "response": "Parse a time offset into a timedelta object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_z(cls, offset):\n        sec = offset.total_seconds()\n        return '{s}{h:02d}{m:02d}'.format(s='-' if sec<0 else '+', h=abs(int(sec/3600)), m=int((sec%3600)/60))", "response": "Format timedelta into %z"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strptime(cls, value, format):\n        # Simplest case: direct parsing\n        if cls.python_supports_z or '%z' not in format:\n            return datetime.strptime(value, format)\n        else:\n            # %z emulation case\n            assert format[-2:] == '%z', 'For performance, %z is only supported at the end of the string'\n\n            # Parse\n            dt = datetime.strptime(value[:-5], format[:-2])  # cutoff '%z' and '+0000'\n            tz = FixedOffset(value[-5:])  # parse %z into tzinfo\n\n            # Localize\n            return dt.replace(tzinfo=tz)", "response": "Parse a datetime string using the provided format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_random_type(valid):\n    type = choice(['int', 'str'])\n\n    r = lambda: randrange(-1000000000, 1000000000)\n\n    if type == 'int':\n        return int,  (r() if valid else str(r()) for i in itertools.count())\n    elif type == 'str':\n        return str, (str(r()) if valid else r() for i in itertools.count())\n    else:\n        raise AssertionError('!')", "response": "Generate a random type and samples for it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a random plain schema and a sample generator.", "response": "def generate_random_schema(valid):\n    \"\"\" Generate a random plain schema, and a sample generation function.\n\n    :param valid: Generate valid samples?\n    :type valid: bool\n    :returns: schema, sample-generator\n    :rtype: *, generator\n    \"\"\"\n    schema_type = choice(['literal', 'type'])\n\n    if schema_type == 'literal':\n        type, gen = generate_random_type(valid)\n        value = next(gen)\n        return value, (value if valid else None for i in itertools.count())\n    elif schema_type == 'type':\n        return generate_random_type(valid)\n    else:\n        raise AssertionError('!')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_dict_schema(size, valid):\n\n    schema = {}\n    generator_items = []\n\n    # Generate schema\n    for i in range(0, size):\n        while True:\n            key_schema,   key_generator   = generate_random_schema(valid)\n            if key_schema not in schema:\n                break\n        value_schema, value_generator = generate_random_schema(valid)\n\n        schema[key_schema] = value_schema\n        generator_items.append((key_generator, value_generator))\n\n    # Samples\n    generator = ({next(k_gen): next(v_gen) for k_gen, v_gen in generator_items} for i in itertools.count())\n\n    # Finish\n    return schema, generator", "response": "Generates a schema dict of size using library lib."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning that can be used as copytree() ignore parameter.", "response": "def ignore_patterns(*patterns):\n    \"\"\"Function that can be used as copytree() ignore parameter.\n\n    Patterns is a sequence of glob-style patterns\n    that are used to exclude files\"\"\"\n    import fnmatch\n    def _ignore_patterns(path, names):\n        ignored_names = []\n        for pattern in patterns:\n            ignored_names.extend(fnmatch.filter(names, pattern))\n        return set(ignored_names)\n    return _ignore_patterns"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copytree(src, dst, symlinks=False, ignore=None):\n    from shutil import copy2, Error, copystat\n    names = os.listdir(src)\n    if ignore is not None:\n        ignored_names = ignore(src, names)\n    else:\n        ignored_names = set()\n\n    os.makedirs(dst)\n    errors = []\n    for name in names:\n        if name in ignored_names:\n            continue\n        srcname = os.path.join(src, name)\n        dstname = os.path.join(dst, name)\n        try:\n            if symlinks and os.path.islink(srcname):\n                linkto = os.readlink(srcname)\n                os.symlink(linkto, dstname)\n            elif os.path.isdir(srcname):\n                copytree(srcname, dstname, symlinks, ignore)\n            else:\n                # Will raise a SpecialFileError for unsupported file types\n                copy2(srcname, dstname)\n        # catch the Error from the recursive copytree so that we can\n        # continue with other files\n        except Error as err:\n            errors.extend(err.args[0])\n        except EnvironmentError as why:\n            errors.append((srcname, dstname, str(why)))\n    try:\n        copystat(src, dst)\n    except OSError as why:\n        if WindowsError is not None and isinstance(why, WindowsError):\n            # Copying file access times may fail on Windows\n            pass\n        else:\n            errors.extend((src, dst, str(why)))\n    if errors:\n        raise Error(errors)", "response": "Recursively copy a directory tree using copy2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_envconfig() -> bool:\n        if (os.getenv(\"VAULT_TOKEN\", None) or\n                (os.getenv(\"VAULT_APPID\", None) and os.getenv(\"VAULT_USERID\", None)) or\n                (os.getenv(\"VAULT_SSLCERT\", None) and os.getenv(\"VAULT_SSLKEY\", None)) or\n                (os.getenv(\"VAULT_ROLEID\", None) and os.getenv(\"VAULT_SECRETID\", None))):\n            return True\n\n        return False", "response": "Returns True if enough information is available in the environment for Vault to authenticate to Vault."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads configuration from the environment and return a configured instance of VaultAuth12Factor.", "response": "def fromenv() -> 'VaultAuth12Factor':\n        \"\"\"\n        :return: Load configuration from the environment and return a configured instance\n        \"\"\"\n        i = None  # type: VaultAuth12Factor\n        if os.getenv(\"VAULT_TOKEN\", None):\n            i = VaultAuth12Factor.token(os.getenv(\"VAULT_TOKEN\"))\n        elif os.getenv(\"VAULT_APPID\", None) and os.getenv(\"VAULT_USERID\", None):\n            i = VaultAuth12Factor.app_id(os.getenv(\"VAULT_APPID\"), os.getenv(\"VAULT_USERID\"))\n        elif os.getenv(\"VAULT_ROLEID\", None) and os.getenv(\"VAULT_SECRETID\", None):\n            i = VaultAuth12Factor.approle(os.getenv(\"VAULT_ROLEID\"), os.getenv(\"VAULT_SECRETID\"))\n        elif os.getenv(\"VAULT_SSLCERT\", None) and os.getenv(\"VAULT_SSLKEY\", None):\n            i = VaultAuth12Factor.ssl_client_cert(os.getenv(\"VAULT_SSLCERT\"), os.getenv(\"VAULT_SSLKEY\"))\n\n        if i:\n            e = os.getenv(\"VAULT_UNWRAP\", \"False\")\n            if e.lower() in [\"true\", \"1\", \"yes\"]:\n                i.unwrap_response = True\n            return i\n\n        raise VaultCredentialProviderException(\"Unable to configure Vault authentication from the environment\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef topoplot(values, locations, axes=None, offset=(0, 0), plot_locations=True,\n             plot_head=True, **kwargs):\n    \"\"\"Wrapper function for :class:`Topoplot.\n    \"\"\"\n    topo = Topoplot(**kwargs)\n    topo.set_locations(locations)\n    topo.set_values(values)\n    topo.create_map()\n    topo.plot_map(axes=axes, offset=offset)\n    if plot_locations:\n        topo.plot_locations(axes=axes, offset=offset)\n    if plot_head:\n        topo.plot_head(axes=axes, offset=offset)\n    return topo", "response": "Wrapper function for Topoplot. Topoplot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _construct_var_eqns(data, p, delta=None):\n        t, m, l = np.shape(data)\n        n = (l - p) * t  # number of linear relations\n        rows = n if delta is None else n + m * p\n\n        # Construct matrix x (predictor variables)\n        x = np.zeros((rows, m * p))\n        for i in range(m):\n            for k in range(1, p + 1):\n                x[:n, i * p + k - 1] = np.reshape(data[:, i, p - k:-k].T, n)\n        if delta is not None:\n            np.fill_diagonal(x[n:, :], delta)\n\n        # Construct vectors yi (response variables for each channel i)\n        y = np.zeros((rows, m))\n        for i in range(m):\n            y[:n, i] = np.reshape(data[:, i, p:].T, n)\n\n        return x, y", "response": "Construct VAR equation system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calc_q_statistic(x, h, nt):\n    t, m, n = x.shape\n\n    # covariance matrix of x\n    c0 = acm(x, 0)\n\n    # LU factorization of covariance matrix\n    c0f = sp.linalg.lu_factor(c0, overwrite_a=False, check_finite=True)\n\n    q = np.zeros((3, h + 1))\n    for l in range(1, h + 1):\n        cl = acm(x, l)\n\n        # calculate tr(cl' * c0^-1 * cl * c0^-1)\n        a = sp.linalg.lu_solve(c0f, cl)\n        b = sp.linalg.lu_solve(c0f, cl.T)\n        tmp = a.dot(b).trace()\n\n        # Box-Pierce\n        q[0, l] = tmp\n\n        # Ljung-Box\n        q[1, l] = tmp / (nt - l)\n\n        # Li-McLeod\n        q[2, l] = tmp\n\n    q *= nt\n    q[1, :] *= (nt + 2)\n\n    q = np.cumsum(q, axis=1)\n\n    for l in range(1, h+1):\n        q[2, l] = q[0, l] + m * m * l * (l + 1) / (2 * nt)\n\n    return q", "response": "Calculate Portmanteau statistics up to a lag of h."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate q under the null hypothesis of whiteness.", "response": "def _calc_q_h0(n, x, h, nt, n_jobs=1, verbose=0, random_state=None):\n    \"\"\"Calculate q under the null hypothesis of whiteness.\n    \"\"\"\n    rng = check_random_state(random_state)\n    par, func = parallel_loop(_calc_q_statistic, n_jobs, verbose)\n    q = par(func(rng.permutation(x.T).T, h, nt) for _ in range(n))\n    \n    return np.array(q)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a copy of the VAR model.", "response": "def copy(self):\n        \"\"\"Create a copy of the VAR model.\"\"\"\n        other = self.__class__(self.p)\n        other.coef = self.coef.copy()\n        other.residuals = self.residuals.copy()\n        other.rescov = self.rescov.copy()\n        return other"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining VAR model from autocorrelation matrices by solving the Yule - Walk equations.", "response": "def from_yw(self, acms):\n        \"\"\"Determine VAR model from autocorrelation matrices by solving the\n        Yule-Walker equations.\n\n        Parameters\n        ----------\n        acms : array, shape (n_lags, n_channels, n_channels)\n            acms[l] contains the autocorrelation matrix at lag l. The highest\n            lag must equal the model order.\n\n        Returns\n        -------\n        self : :class:`VAR`\n            The :class:`VAR` object to facilitate method chaining (see usage\n            example).\n        \"\"\"\n        if len(acms) != self.p + 1:\n            raise ValueError(\"Number of autocorrelation matrices ({}) does not\"\n                             \" match model order ({}) + 1.\".format(len(acms),\n                                                                   self.p))\n\n        n_channels = acms[0].shape[0]\n\n        acm = lambda l: acms[l] if l >= 0 else acms[-l].T\n\n        r = np.concatenate(acms[1:], 0)\n\n        rr = np.array([[acm(m-k) for k in range(self.p)]\n                      for m in range(self.p)])\n        rr = np.concatenate(np.concatenate(rr, -2), -1)\n\n        c = sp.linalg.solve(rr, r)\n\n        # calculate residual covariance\n        r = acm(0)\n        for k in range(self.p):\n            bs = k * n_channels\n            r -= np.dot(c[bs:bs + n_channels, :].T, acm(k + 1))\n\n        self.coef = np.concatenate([c[m::n_channels, :]\n                                    for m in range(n_channels)]).T\n        self.rescov = r\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simulate(self, l, noisefunc=None, random_state=None):\n        m, n = np.shape(self.coef)\n        p = n // m\n\n        try:\n            l, t = l\n        except TypeError:\n            t = 1\n\n        if noisefunc is None:\n            rng = check_random_state(random_state)\n            noisefunc = lambda: rng.normal(size=(1, m))\n\n        n = l + 10 * p\n\n        y = np.zeros((n, m, t))\n        res = np.zeros((n, m, t))\n\n        for s in range(t):\n            for i in range(p):\n                e = noisefunc()\n                res[i, :, s] = e\n                y[i, :, s] = e\n            for i in range(p, n):\n                e = noisefunc()\n                res[i, :, s] = e\n                y[i, :, s] = e\n                for k in range(1, p + 1):\n                    y[i, :, s] += self.coef[:, (k - 1)::p].dot(y[i - k, :, s])\n\n        self.residuals = res[10 * p:, :, :].T\n        self.rescov = sp.cov(cat_trials(self.residuals).T, rowvar=False)\n\n        return y[10 * p:, :, :].transpose([2, 1, 0])", "response": "Simulate vector autoregressive model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, data):\n        data = atleast_3d(data)\n        t, m, l = data.shape\n\n        p = int(np.shape(self.coef)[1] / m)\n\n        y = np.zeros(data.shape)\n        if t > l - p:  # which takes less loop iterations\n            for k in range(1, p + 1):\n                bp = self.coef[:, (k - 1)::p]\n                for n in range(p, l):\n                    y[:, :, n] += np.dot(data[:, :, n - k], bp.T)\n        else:\n            for k in range(1, p + 1):\n                bp = self.coef[:, (k - 1)::p]\n                for s in range(t):\n                    y[s, :, p:] += np.dot(bp, data[s, :, (p - k):(l - k)])\n\n        return y", "response": "Predict samples on actual data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest if the VAR model is stable.", "response": "def is_stable(self):\n        \"\"\"Test if VAR model is stable.\n\n        This function tests stability of the VAR model as described in [1]_.\n\n        Returns\n        -------\n        out : bool\n            True if the model is stable.\n\n        References\n        ----------\n        .. [1] H. L\u00fctkepohl, \"New Introduction to Multiple Time Series\n               Analysis\", 2005, Springer, Berlin, Germany.\n        \"\"\"\n        m, mp = self.coef.shape\n        p = mp // m\n        assert(mp == m * p)  # TODO: replace with raise?\n\n        top_block = []\n        for i in range(p):\n            top_block.append(self.coef[:, i::p])\n        top_block = np.hstack(top_block)\n\n        im = np.eye(m)\n        eye_block = im\n        for i in range(p - 2):\n            eye_block = sp.linalg.block_diag(im, eye_block)\n        eye_block = np.hstack([eye_block, np.zeros((m * (p - 1), m))])\n\n        tmp = np.vstack([top_block, eye_block])\n\n        return np.all(np.abs(np.linalg.eig(tmp)[0]) < 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch(dataset=\"mi\", datadir=datadir):\n    if dataset not in datasets:\n        raise ValueError(\"Example data '{}' not available.\".format(dataset))\n    else:\n        files = datasets[dataset][\"files\"]\n        url = datasets[dataset][\"url\"]\n        md5 = datasets[dataset][\"md5\"]\n    if not isdir(datadir):\n        makedirs(datadir)\n\n    data = []\n\n    for n, filename in enumerate(files):\n        fullfile = join(datadir, filename)\n        if not isfile(fullfile):\n            with open(fullfile, \"wb\") as f:\n                response = get(join(url, filename))\n                f.write(response.content)\n        with open(fullfile, \"rb\") as f:  # check if MD5 of downloaded file matches original hash\n            hash = hashlib.md5(f.read()).hexdigest()\n        if hash != md5[n]:\n            raise MD5MismatchError(\"MD5 hash of {} does not match {}.\".format(fullfile, md5[n]))\n        data.append(convert(dataset, loadmat(fullfile)))\n\n    return data", "response": "Fetch example dataset.\n\n    If the requested dataset is not found in the location specified by\n    `datadir`, the function attempts to download it.\n\n    Parameters\n    ----------\n    dataset : str\n        Which dataset to load. Currently only 'mi' is supported.\n    datadir : str\n        Path to the storage location of example datasets. Datasets are\n        downloaded to this location if they cannot be found. If the directory\n        does not exist it is created.\n\n    Returns\n    -------\n        data : list of dicts\n            The data set is stored in a list, where each list element\n            corresponds to data from one subject. Each list element is a\n            dictionary with the following keys:\n              \"eeg\" ... EEG signals\n              \"triggers\" ... Trigger latencies\n              \"labels\" ... Class labels\n              \"fs\" ... Sample rate\n              \"locations\" ... Channel locations"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef supports_undefined(self):\n        # Test\n        try:\n            yes = self(const.UNDEFINED) is not const.UNDEFINED\n        except (Invalid, SchemaError):\n            yes = False\n\n        # Remember (lame @cached_property)\n        self.__dict__['supports_undefined'] = yes\n        return yes", "response": "Test whether this schema supports Undefined."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the schema type for the argument .", "response": "def get_schema_type(cls, schema):\n        \"\"\" Get schema type for the argument\n\n        :param schema: Schema to analyze\n        :return: COMPILED_TYPE constant\n        :rtype: str|None\n        \"\"\"\n        schema_type = type(schema)\n\n        # Marker\n        if issubclass(schema_type, markers.Marker):\n            return const.COMPILED_TYPE.MARKER\n        # Marker Type\n        elif issubclass(schema_type, six.class_types) and issubclass(schema, markers.Marker):\n            return const.COMPILED_TYPE.MARKER\n        # CompiledSchema\n        elif isinstance(schema, CompiledSchema):\n            return const.COMPILED_TYPE.SCHEMA\n        else:\n            return primitive_type(schema)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef priority(self):\n        # Markers have priority set on the class\n        if self.compiled_type == const.COMPILED_TYPE.MARKER:\n            return self.compiled.priority\n\n        # Other types have static priority\n        return const.compiled_type_priorities[self.compiled_type]", "response": "Get priority for this Schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the provided list of compiled schemas according to their priority.", "response": "def sort_schemas(cls, schemas_list):\n        \"\"\" Sort the provided list of schemas according to their priority.\n\n        This also supports markers, and markers of a single type are also sorted according to the priority of the wrapped schema.\n\n        :type schemas_list: list[CompiledSchema]\n        :rtype: list[CompiledSchema]\n        \"\"\"\n        return sorted(schemas_list,\n                      key=lambda x: (\n                          # Top-level priority:\n                          # priority of the schema itself\n                          x.priority,\n                          # Second-level priority (for markers of the common type)\n                          # This ensures that Optional(1) always goes before Optional(int)\n                          x.compiled.key_schema.priority if x.compiled_type == const.COMPILED_TYPE.MARKER else 0\n                      ), reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sub_compile(self, schema, path=None, matcher=False):\n        return type(self)(\n            schema,\n            self.path + (path or []),\n            None,\n            None,\n            matcher\n        )", "response": "Compile a sub - schema with optional path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Invalid(self, message, expected):\n        def InvalidPartial(provided, path=None, **info):\n            \"\"\" Create an Invalid exception\n\n            :type provided: unicode\n            :type path: list|None\n            :rtype: Invalid\n            \"\"\"\n            return Invalid(\n                message,\n                expected, #six.text_type(expected),  # -- must be unicode\n                provided, #six.text_type(provided),  # -- must be unicode\n                self.path + (path or []),\n                self.schema,\n                **info\n            )\n        return InvalidPartial", "response": "Returns an Invalid exception."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the compiler method for the provided schema.", "response": "def get_schema_compiler(self, schema):\n        \"\"\" Get compiler method for the provided schema\n\n        :param schema: Schema to analyze\n        :return: Callable compiled\n        :rtype: callable|None\n        \"\"\"\n        # Schema type\n        schema_type = self.get_schema_type(schema)\n        if schema_type is None:\n            return None\n\n        # Compiler\n        compilers = {\n            const.COMPILED_TYPE.LITERAL: self._compile_literal,\n            const.COMPILED_TYPE.TYPE: self._compile_type,\n            const.COMPILED_TYPE.SCHEMA: self._compile_schema,\n            const.COMPILED_TYPE.ENUM: self._compile_enum,\n            const.COMPILED_TYPE.CALLABLE: self._compile_callable,\n            const.COMPILED_TYPE.ITERABLE: self._compile_iterable,\n            const.COMPILED_TYPE.MAPPING: self._compile_mapping,\n            const.COMPILED_TYPE.MARKER: self._compile_marker,\n        }\n\n        return compilers[schema_type]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compile_schema(self, schema):\n        compiler = self.get_schema_compiler(schema)\n\n        if compiler is None:\n            raise SchemaError(_(u'Unsupported schema data type {!r}').format(type(schema).__name__))\n\n        return compiler(schema)", "response": "Compile the current schema into a callable that returns the current schema s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compile_literal(self, schema):\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.LITERAL\n        self.name = get_literal_name(schema)\n\n        # Error partials\n        schema_type = type(schema)\n        err_type  = self.Invalid(_(u'Wrong value type'), get_type_name(schema_type))\n        err_value = self.Invalid(_(u'Invalid value'), self.name)\n\n        # Matcher\n        if self.matcher:\n            def match_literal(v):\n                return type(v) == schema_type and v == schema, v\n            return match_literal\n\n        # Validator\n        def validate_literal(v):\n            # Type check\n            if type(v) != schema_type:\n                # expected=<type>, provided=<type>\n                raise err_type(get_type_name(type(v)))\n            # Equality check\n            if v != schema:\n                # expected=<value>, provided=<value>\n                raise err_value(get_literal_name(v))\n            # Fine\n            return v\n        return validate_literal", "response": "Compile literal schema type and value matching"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompiling type schema for internal use.", "response": "def _compile_type(self, schema):\n        \"\"\" Compile type schema: plain type matching \"\"\"\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.TYPE\n        self.name = get_type_name(schema)\n\n        # Error partials\n        err_type = self.Invalid(_(u'Wrong type'), self.name)\n\n        # Type check function\n        if six.PY2 and schema is basestring:\n            # Relaxed rule for Python2 basestring\n            typecheck = lambda v: isinstance(v, schema)\n        else:\n            # Strict type check for everything else\n            typecheck = lambda v: type(v) == schema\n\n        # Matcher\n        if self.matcher:\n            def match_type(v):\n                return typecheck(v), v\n            return match_type\n\n        # Validator\n        def validate_type(v):\n            # Type check\n            if not typecheck(v):\n                # expected=<type>, provided=<type>\n                raise err_type(get_type_name(type(v)))\n            # Fine\n            return v\n\n        return validate_type"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compile_callable(self, schema):\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.CALLABLE\n        self.name = get_callable_name(schema)\n\n        # Error utils\n        enrich_exception = lambda e, value: e.enrich(\n            expected=self.name,\n            provided=get_literal_name(value),\n            path=self.path,\n            validator=schema)\n\n        # Validator\n        def validate_with_callable(v):\n            try:\n                # Try this callable\n                return schema(v)\n            except Invalid as e:\n                # Enrich & re-raise\n                enrich_exception(e, v)\n                raise\n            except const.transformed_exceptions as e:\n                message = _(u'{message}').format(\n                    Exception=type(e).__name__,\n                    message=six.text_type(e))\n                e = Invalid(message)\n                raise enrich_exception(e, v)\n\n        # Matcher\n        if self.matcher:\n            def match_with_callable(v):\n                try:\n                    return True, validate_with_callable(v)\n                except Invalid:\n                    return False, v\n            return match_with_callable\n\n        return validate_with_callable", "response": "Compile callable that checks if the schema passes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compile_iterable(self, schema):\n        # Compile each member as a schema\n        schema_type = type(schema)\n        schema_subs = tuple(map(self.sub_compile, schema))\n\n        # When the schema is an iterable with a single item (e.g. [dict(...)]),\n        # Invalid errors from schema members should be immediately used.\n        # This allows to report sane errors with `Schema([{'age': int}])`\n        error_passthrough = len(schema_subs) == 1\n\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.ITERABLE\n        self.name = _(u'{iterable_cls}[{iterable_options}]').format(\n            iterable_cls=get_type_name(schema_type),\n            iterable_options=_(u'|').join(x.name for x in schema_subs)\n        )\n\n        # Error partials\n        err_type = self.Invalid(_(u'Wrong value type'), get_type_name(schema_type))\n        err_value = self.Invalid(_(u'Invalid value'), self.name)\n\n        # Validator\n        def validate_iterable(l):\n            # Type check\n            if not isinstance(l, schema_type):\n                # expected=<type>, provided=<type>\n                raise err_type(provided=get_type_name(type(l)))\n\n            # Each `v` member should match to any `schema` member\n            errors = []  # Errors for every value\n            values = []  # Sanitized values\n            for value_index, value in list(enumerate(l)):\n                # Walk through schema members and test if any of them match\n                for value_schema in schema_subs:\n                    try:\n                        # Try to validate\n                        values.append(value_schema(value))\n                        break  # Success!\n                    except signals.RemoveValue:\n                        # `value_schema` commanded to drop this value\n                        break\n                    except Invalid as e:\n                        if error_passthrough:\n                            # Error-Passthrough enabled: add the original error\n                            errors.append(e.enrich(path=[value_index]))\n                            break\n                        else:\n                            # Error-Passthrough disabled: Ignore errors and hope other members will succeed better\n                            pass\n                else:\n                    errors.append(err_value(get_literal_name(value), path=[value_index]))\n\n            # Errors?\n            if errors:\n                raise MultipleInvalid.if_multiple(errors)\n\n            # Typecast and finish\n            return schema_type(values)\n\n        # Matcher\n        if self.matcher:\n            return self._compile_callable(validate_iterable)  # Stupidly use it as callable\n\n        return validate_iterable", "response": "Compile an iterable into a single item and store the result in self. name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompile a marker and return the schema.", "response": "def _compile_marker(self, schema):\n        \"\"\" Compile marker: sub-schema with special type \"\"\"\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.MARKER\n\n        # If this marker is not instantiated -- do it with an identity callable which is valid for everything\n        if issubclass(type(schema), six.class_types):\n            schema = schema(Identity)  \\\n                .on_compiled(name=Identity.name)  # Set a special name on it\n\n        # Compile Marker's schema\n        key_schema = self.sub_compile(schema.key, matcher=self.matcher)\n        schema.on_compiled(key_schema=key_schema, name=key_schema.name)\n\n        # Marker is a callable\n        self.name = schema.name\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile a mapping from a schema to a set of keys and values.", "response": "def _compile_mapping(self, schema):\n        \"\"\" Compile mapping: key-value matching \"\"\"\n        assert not self.matcher, 'Mappings cannot be matchers'\n\n        # This stuff is tricky, but thankfully, I like comments :)\n\n        # Set default Marker on all keys.\n        # This makes sure that all keys will still become markers, and hence have the correct default behavior\n        schema = {self.default_keys(k) if self.get_schema_type(k) != const.COMPILED_TYPE.MARKER else k: v\n                  for k, v in schema.items()}\n\n        # Add `Extra`\n        # Every Schema implicitly has an `Extra` that defaults to `extra_keys`.\n        # Note that this is the only place in the code where Marker behavior is hardcoded :)\n        schema.setdefault(markers.Extra, self.extra_keys)\n\n        # Compile both keys & values as schemas.\n        # Key schemas are compiled as \"Matchers\" for performance.\n        compiled = {self.sub_compile(key, matcher=True): self.sub_compile(value)\n                    for key, value in schema.items()}\n\n        # Notify Markers that they were compiled.\n        # _compile_marker() has already done part of the job: it only specified `key_schema`.\n        # Here we let the Marker know its `value_schema` as well.\n        for key_schema, value_schema in compiled.items():\n            if key_schema.compiled_type == const.COMPILED_TYPE.MARKER:\n                key_schema.compiled.on_compiled(value_schema=value_schema, as_mapping_key=True)\n\n        # Sort key schemas for matching.\n\n        # Since various schema types have different priority, we need to sort these accordingly.\n        # Then, literals match before types, and Markers can define execution order using `priority`.\n        # For instance, Remove() should be called first (before any validation takes place),\n        # while Extra() should be checked last so it catches all extra keys that did not match other key schemas.\n\n        # In addition, since mapping keys are mostly literals, we want direct matching instead of the costly function calls.\n        # Hence, remember which of them are literals or 'catch-all' markers.\n        is_literal  = lambda key_schema: key_schema.compiled.key_schema.compiled_type == const.COMPILED_TYPE.LITERAL\n        is_identity = lambda identity: key_schema.compiled.key_schema is Identity\n\n        compiled = [ (key_schema, compiled[key_schema], is_literal(key_schema), is_identity(key_schema))\n                     for key_schema in self.sort_schemas(compiled.keys())]\n        ''' :var  compiled: Sorted list of CompiledSchemas: (key-schema, value-schema, is-literal),\n            :type compiled: list[CompiledSchema, CompiledSchema, bool]\n        '''\n\n        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.MAPPING\n        self.name = _(u'{mapping_cls}[{mapping_keys}]').format(\n            mapping_cls=get_type_name(type(schema)),\n            mapping_keys=_(u',').join(key_schema.name for key_schema, value_schema, is_literal, is_identity in compiled)\n        )\n\n        # Error partials\n        schema_type = type(schema)\n        err_type = self.Invalid(_(u'Wrong value type'), get_type_name(schema_type))\n\n        # Validator\n        def validate_mapping(d):\n            # Type check\n            if not isinstance(d, schema_type):\n                # expected=<type>, provided=<type>\n                raise err_type(provided=get_type_name(type(d)))\n\n            # For each schema key, pick matching input key-value pairs.\n            # Since we always have Extra which is a catch-all -- this will always result into a full input coverage.\n            # Also, key schemas are sorted according to the priority, we're handling each set of matching keys in order.\n\n            errors = []  # Collect errors on the fly\n            d_keys = set(d.keys())  # Make a copy of dict keys for destructive iteration\n\n            for key_schema, value_schema, is_literal, is_identity in compiled:\n                # First, collect matching (key, value) pairs for the `key_schema`.\n                # Note that `key_schema` can change the value (e.g. `Coerce(int)`), so for every key\n                # we store both the initial value (`input-key`) and the sanitized value (`sanitized-key`).\n                # This results into a list of triples: [(input-key, sanitized-key, input-value), ...].\n\n                matches = []\n\n                if is_literal:  # (short-circuit for literals)\n                    # Since mapping keys are mostly literals --\n                    # save some iterations & function calls in favor of direct matching,\n                    # which introduces a HUGE performance improvement\n                    k = key_schema.schema.key  # get the literal from the marker\n                    if k in d_keys:\n                        # (See comments below)\n                        matches.append(( k, k, d[k] ))\n                        d_keys.remove(k)\n                elif is_identity:  # (short-circuit for Marker(Identity))\n                    # When this value is an identity function -- we plainly add all keys to it.\n                    # This is to short-circuit catch-all markers like `Extra`, which, being executed last,\n                    # just gets all remaining keys.\n                    matches.extend((k, k, d[k]) for k in d_keys)\n                    d_keys = None  # empty it since we've processed everything\n                elif d_keys:\n                    # For non-literal schemas we have to walk all input keys\n                    # and detect those that match the current `key_schema`.\n                    # In contrast to literals, such keys may have multiple matches (e.g. `{ int: 1 }`).\n\n                    # Note that this condition branch includes the logic from the short-circuited logic implemented above,\n                    # but is less performant.\n\n                    for k in tuple(d_keys):\n                        # Exec key schema on the input key.\n                        # Since all key schemas are compiled as matchers -- we get a tuple (key-matched, sanitized-key)\n\n                        okay, sanitized_k = key_schema(k)\n\n                        # If this key has matched -- append it to the list of matches for the current `key_schema`.\n                        # Also, remove the key from the original input so it does not match any other key schemas\n                        # with lower priorities.\n                        if okay:\n                            matches.append(( k, sanitized_k, d[k] ))\n                            d_keys.remove(k)\n\n                # Now, having a `key_schema` and a list of matches for it, do validation.\n                # If the key is a marker -- execute the marker first so it has a chance to modify the input,\n                # and then proceed with value validation.\n\n                # Execute Marker first.\n                if key_schema.compiled_type == const.COMPILED_TYPE.MARKER:\n                    # Note that Markers can raise errors as well.\n                    # Since they're compiled - all marker errors are raised as `Invalid`.\n                    try:\n                        matches = key_schema.compiled.execute(d, matches)\n                    except Invalid as e:\n                        # Add marker errors to the list of Invalid reports for this schema.\n                        # Using enrich(), we're also setting `path` prefix, and other info known at this step.\n                        errors.append(e.enrich(\n                            # Markers are responsible to set `expected`, `provided`, `validator`\n                            expected=key_schema.name,\n                            provided=None,  # Marker's required to set that\n                            path=self.path,\n                            validator=key_schema.compiled\n                        ))\n                        # If a marker raised an error -- the (key, value) pair is already Invalid, and no\n                        # further validation is required.\n                        continue\n\n                # Proceed with validation.\n                # Now, we validate values for every (key, value) pairs in the current list of matches,\n                # and rebuild the mapping.\n                for k, sanitized_k, v in matches:\n                    try:\n                        # Execute the value schema and store it into the rebuilt mapping\n                        # using the sanitized key, which might be different from the original key.\n                        d[sanitized_k] = value_schema(v)\n\n                        # Remove the original key in case `key_schema` has transformed it.\n                        if k != sanitized_k:\n                            del d[k]\n                    except signals.RemoveValue:\n                        # `value_schema` commanded to drop this value\n                        del d[k]\n                    except Invalid as e:\n                        # Any value validation errors are appended to the list of Invalid reports for the schema\n                        # enrich() adds more info on the collected errors.\n                        errors.append(e.enrich(\n                            expected=value_schema.name,\n                            provided=get_literal_name(v),\n                            path=self.path + [k],\n                            validator=value_schema\n                        ))\n\n            assert not d_keys, 'Keys must be empty after destructive iteration. Remainder: {!r}'.format(d_keys)\n\n            # Errors?\n            if errors:\n                # Note that we did not care about whether a sub-schema raised a single Invalid or MultipleInvalid,\n                # since MultipleInvalid will flatten the list for us.\n                raise MultipleInvalid.if_multiple(errors)\n\n            # Finish\n            return d\n\n        return validate_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pca_svd(x):\n    w, s, _ = np.linalg.svd(x, full_matrices=False)\n    return w, s ** 2", "response": "Calculate PCA using SVD."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pca_eig(x):\n    s, w = np.linalg.eigh(x.dot(x.T))\n    return w, s", "response": "Calculate PCA using eigenvalue decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pca(x, subtract_mean=False, normalize=False, sort_components=True,\n        reducedim=None, algorithm=pca_eig):\n    \"\"\"Calculate principal component analysis (PCA).\n    \n    Parameters\n    ----------\n    x : ndarray, shape (trials, channels, samples) or (channels, samples)\n        Input data.\n    subtract_mean : bool, optional\n        Subtract sample mean from x.\n    normalize : bool, optional\n        Normalize variances before applying PCA.\n    sort_components : bool, optional\n        Sort principal components in order of decreasing eigenvalues.\n    reducedim : float or int or None, optional\n        A value less than 1 is interpreted as the fraction of variance that\n        should be retained in the data. All components that account for less\n        than `1 - reducedim` of the variance are removed.\n        An integer value of 1 or greater is interpreted as the number of\n        (sorted) components to retain.\n        If None, do not reduce dimensionality (i.e. keep all components).\n    algorithm : func, optional\n        Function to use for eigenvalue decomposition\n        (:func:`pca_eig` or :func:`pca_svd`).\n        \n    Returns\n    -------\n    w : ndarray, shape (channels, components)\n        PCA transformation matrix.\n    v : ndarray, shape (components, channels)\n        Inverse PCA transformation matrix.\n    \"\"\"\n\n    x = np.asarray(x)\n    if x.ndim == 3:\n        x = cat_trials(x)\n\n    if reducedim:\n        sort_components = True\n\n    if subtract_mean:\n        x = x - np.mean(x, axis=1, keepdims=True)\n\n    k, l = None, None\n    if normalize:\n        l = np.std(x, axis=1, ddof=1)\n        k = np.diag(1.0 / l)\n        l = np.diag(l)\n        x = np.dot(k, x)\n\n    w, latent = algorithm(x)\n\n    # PCA is just a rotation, so inverse is equal to transpose\n    v = w.T\n\n    if normalize:\n        w = np.dot(k, w)\n        v = np.dot(v, l)\n\n    latent /= sum(latent)\n\n    if sort_components:\n        order = np.argsort(latent)[::-1]\n        w = w[:, order]\n        v = v[order, :]\n        latent = latent[order]\n\n    if reducedim is not None:\n        if reducedim < 1:\n            selected = np.nonzero(np.cumsum(latent) < reducedim)[0]\n            try:\n                selected = np.concatenate([selected, [selected[-1] + 1]])\n            except IndexError:\n                selected = [0]\n            if selected[-1] >= w.shape[1]:\n                selected = selected[0:-1]\n            w = w[:, selected]\n            v = v[selected, :]\n        else:\n            w = w[:, :reducedim]\n            v = v[:reducedim, :]\n\n    return w, v", "response": "Calculates the principal component analysis of a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infomax(data, weights=None, l_rate=None, block=None, w_change=1e-12,\n            anneal_deg=60., anneal_step=0.9, extended=False, n_subgauss=1,\n            kurt_size=6000, ext_blocks=1, max_iter=200,\n            random_state=None, verbose=None):\n    \"\"\"Run the (extended) Infomax ICA decomposition on raw data\n\n    based on the publications of Bell & Sejnowski 1995 (Infomax)\n    and Lee, Girolami & Sejnowski, 1999 (extended Infomax)\n\n    Parameters\n    ----------\n    data : np.ndarray, shape (n_samples, n_features)\n        The data to unmix.\n    w_init : np.ndarray, shape (n_features, n_features)\n        The initialized unmixing matrix. Defaults to None. If None, the\n        identity matrix is used.\n    l_rate : float\n        This quantity indicates the relative size of the change in weights.\n        Note. Smaller learining rates will slow down the procedure.\n        Defaults to 0.010d / alog(n_features ^ 2.0)\n    block : int\n        The block size of randomly chosen data segment.\n        Defaults to floor(sqrt(n_times / 3d))\n    w_change : float\n        The change at which to stop iteration. Defaults to 1e-12.\n    anneal_deg : float\n        The angle at which (in degree) the learning rate will be reduced.\n        Defaults to 60.0\n    anneal_step : float\n        The factor by which the learning rate will be reduced once\n        ``anneal_deg`` is exceeded:\n            l_rate *= anneal_step\n        Defaults to 0.9\n    extended : bool\n        Wheather to use the extended infomax algorithm or not. Defaults to\n        True.\n    n_subgauss : int\n        The number of subgaussian components. Only considered for extended\n        Infomax.\n    kurt_size : int\n        The window size for kurtosis estimation. Only considered for extended\n        Infomax.\n    ext_blocks : int\n        The number of blocks after which to recompute Kurtosis.\n        Only considered for extended Infomax.\n    max_iter : int\n        The maximum number of iterations. Defaults to 200.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    Returns\n    -------\n    unmixing_matrix : np.ndarray of float, shape (n_features, n_features)\n        The linear unmixing operator.\n    \"\"\"\n    rng = check_random_state(random_state)\n\n    # define some default parameter\n    max_weight = 1e8\n    restart_fac = 0.9\n    min_l_rate = 1e-10\n    blowup = 1e4\n    blowup_fac = 0.5\n    n_small_angle = 20\n    degconst = 180.0 / np.pi\n\n    # for extended Infomax\n    extmomentum = 0.5\n    signsbias = 0.02\n    signcount_threshold = 25\n    signcount_step = 2\n    if ext_blocks > 0:  # allow not to recompute kurtosis\n        n_subgauss = 1  # but initialize n_subgauss to 1 if you recompute\n\n    # check data shape\n    n_samples, n_features = data.shape\n    n_features_square = n_features ** 2\n\n    # check input parameter\n    # heuristic default - may need adjustment for\n    # large or tiny data sets\n    if l_rate is None:\n        l_rate = 0.01 / math.log(n_features ** 2.0)\n\n    if block is None:\n        block = int(math.floor(math.sqrt(n_samples / 3.0)))\n\n    logger.info('computing%sInfomax ICA' % ' Extended ' if extended is True\n                else ' ')\n\n    # collect parameter\n    nblock = n_samples // block\n    lastt = (nblock - 1) * block + 1\n\n    # initialize training\n    if weights is None:\n        # initialize weights as identity matrix\n        weights = np.identity(n_features, dtype=np.float64)\n\n    BI = block * np.identity(n_features, dtype=np.float64)\n    bias = np.zeros((n_features, 1), dtype=np.float64)\n    onesrow = np.ones((1, block), dtype=np.float64)\n    startweights = weights.copy()\n    oldweights = startweights.copy()\n    step = 0\n    count_small_angle = 0\n    wts_blowup = False\n    blockno = 0\n    signcount = 0\n\n    # for extended Infomax\n    if extended is True:\n        signs = np.identity(n_features)\n        signs.flat[slice(0, n_features * n_subgauss, n_features)]\n        kurt_size = min(kurt_size, n_samples)\n        old_kurt = np.zeros(n_features, dtype=np.float64)\n        oldsigns = np.zeros((n_features, n_features))\n\n    # trainings loop\n    olddelta, oldchange = 1., 0.\n    while step < max_iter:\n\n        # shuffle data at each step\n        permute = list(range(n_samples))\n        rng.shuffle(permute)\n\n        # ICA training block\n        # loop across block samples\n        for t in range(0, lastt, block):\n            u = np.dot(data[permute[t:t + block], :], weights)\n            u += np.dot(bias, onesrow).T\n\n            if extended is True:\n                # extended ICA update\n                y = np.tanh(u)\n                weights += l_rate * np.dot(weights,\n                                           BI - np.dot(np.dot(u.T, y), signs) -\n                                           np.dot(u.T, u))\n                bias += l_rate * np.reshape(np.sum(y, axis=0,\n                                            dtype=np.float64) * -2.0,\n                                            (n_features, 1))\n\n            else:\n                # logistic ICA weights update\n                y = 1.0 / (1.0 + np.exp(-u))\n                weights += l_rate * np.dot(weights,\n                                           BI + np.dot(u.T, (1.0 - 2.0 * y)))\n                bias += l_rate * np.reshape(np.sum((1.0 - 2.0 * y), axis=0,\n                                            dtype=np.float64), (n_features, 1))\n\n            # check change limit\n            max_weight_val = np.max(np.abs(weights))\n            if max_weight_val > max_weight:\n                wts_blowup = True\n\n            blockno += 1\n            if wts_blowup:\n                break\n\n            # ICA kurtosis estimation\n            if extended is True:\n\n                n = np.fix(blockno / ext_blocks)\n\n                if np.abs(n) * ext_blocks == blockno:\n                    if kurt_size < n_samples:\n                        rp = np.floor(rng.uniform(0, 1, kurt_size) *\n                                      (n_samples - 1))\n                        tpartact = np.dot(data[rp.astype(int), :], weights).T\n                    else:\n                        tpartact = np.dot(data, weights).T\n\n                    # estimate kurtosis\n                    kurt = kurtosis(tpartact, axis=1, fisher=True)\n\n                    if extmomentum != 0:\n                        kurt = (extmomentum * old_kurt +\n                                (1.0 - extmomentum) * kurt)\n                        old_kurt = kurt\n\n                    # estimate weighted signs\n                    signs.flat[::n_features + 1] = ((kurt + signsbias) /\n                                                    np.abs(kurt + signsbias))\n\n                    ndiff = ((signs.flat[::n_features + 1] -\n                              oldsigns.flat[::n_features + 1]) != 0).sum()\n                    if ndiff == 0:\n                        signcount += 1\n                    else:\n                        signcount = 0\n                    oldsigns = signs\n\n                    if signcount >= signcount_threshold:\n                        ext_blocks = np.fix(ext_blocks * signcount_step)\n                        signcount = 0\n\n        # here we continue after the for\n        # loop over the ICA training blocks\n        # if weights in bounds:\n        if not wts_blowup:\n            oldwtchange = weights - oldweights\n            step += 1\n            angledelta = 0.0\n            delta = oldwtchange.reshape(1, n_features_square)\n            change = np.sum(delta * delta, dtype=np.float64)\n            if step > 1:\n                angledelta = math.acos(np.sum(delta * olddelta) /\n                                       math.sqrt(change * oldchange))\n                angledelta *= degconst\n\n            # anneal learning rate\n            oldweights = weights.copy()\n            if angledelta > anneal_deg:\n                l_rate *= anneal_step    # anneal learning rate\n                # accumulate angledelta until anneal_deg reached l_rates\n                olddelta = delta\n                oldchange = change\n                count_small_angle = 0  # reset count when angle delta is large\n            else:\n                if step == 1:  # on first step only\n                    olddelta = delta  # initialize\n                    oldchange = change\n                count_small_angle += 1\n                if count_small_angle > n_small_angle:\n                    max_iter = step\n\n            # apply stopping rule\n            if step > 2 and change < w_change:\n                step = max_iter\n            elif change > blowup:\n                l_rate *= blowup_fac\n\n        # restart if weights blow up\n        # (for lowering l_rate)\n        else:\n            step = 0  # start again\n            wts_blowup = 0  # re-initialize variables\n            blockno = 1\n            l_rate *= restart_fac  # with lower learning rate\n            weights = startweights.copy()\n            oldweights = startweights.copy()\n            olddelta = np.zeros((1, n_features_square), dtype=np.float64)\n            bias = np.zeros((n_features, 1), dtype=np.float64)\n\n            # for extended Infomax\n            if extended:\n                signs = np.identity(n_features)\n                signs.flat[slice(0, n_features * n_subgauss, n_features)]\n                oldsigns = np.zeros((n_features, n_features))\n\n            if l_rate > min_l_rate:\n                if verbose:\n                    logger.info('... lowering learning rate to %g'\n                                '\\n... re-starting...' % l_rate)\n            else:\n                raise ValueError('Error in Infomax ICA: unmixing_matrix matrix'\n                                 'might not be invertible!')\n\n    # prepare return values\n    return weights.T", "response": "This function is used to run the extended Infomax ICA decomposition on the raw data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if keys in dictionary are mat - objects. If yes Invitement todict is called to change them to nested dictionaries.", "response": "def _check_keys(dictionary):\n    \"\"\"\n    checks if entries in dictionary are mat-objects. If yes\n    todict is called to change them to nested dictionaries\n    \"\"\"\n    for key in dictionary:\n        if isinstance(dictionary[key], matlab.mio5_params.mat_struct):\n            dictionary[key] = _todict(dictionary[key])\n    return dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plainica(x, reducedim=0.99, backend=None, random_state=None):\n\n    x = atleast_3d(x)\n    t, m, l = np.shape(x)\n\n    if backend is None:\n        backend = scotbackend\n\n    # pre-transform the data with PCA\n    if reducedim == 'no pca':\n        c = np.eye(m)\n        d = np.eye(m)\n        xpca = x\n    else:\n        c, d, xpca = backend['pca'](x, reducedim)\n\n    # run on residuals ICA to estimate volume conduction\n    mx, ux = backend['ica'](cat_trials(xpca), random_state=random_state)\n\n    # correct (un)mixing matrix estimatees\n    mx = mx.dot(d)\n    ux = c.dot(ux)\n\n    class Result:\n        unmixing = ux\n        mixing = mx\n\n    return Result", "response": "This function computes the ICA from the data x with optional PCA dimensionality reduction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _msge_with_gradient_underdetermined(data, delta, xvschema, skipstep, p):\n    t, m, l = data.shape\n    d = None\n    j, k = 0, 0\n    nt = np.ceil(t / skipstep)\n    for trainset, testset in xvschema(t, skipstep):\n\n        a, b = _construct_var_eqns(atleast_3d(data[trainset, :, :]), p)\n        c, d = _construct_var_eqns(atleast_3d(data[testset, :, :]), p)\n\n        e = sp.linalg.inv(np.eye(a.shape[0]) * delta ** 2 + a.dot(a.T))\n\n        cc = c.transpose().dot(c)\n\n        be = b.transpose().dot(e)\n        bee = be.dot(e)\n        bea = be.dot(a)\n        beea = bee.dot(a)\n        beacc = bea.dot(cc)\n        dc = d.transpose().dot(c)\n\n        j += np.sum(beacc * bea - 2 * bea * dc) + np.sum(d ** 2)\n        k += np.sum(beea * dc - beacc * beea) * 4 * delta\n\n    return j / (nt * d.size), k / (nt * d.size)", "response": "Calculate mean squared generalization error and its gradient for the underdetermined equation system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _msge_with_gradient_overdetermined(data, delta, xvschema, skipstep, p):\n    t, m, l = data.shape\n    d = None\n    l, k = 0, 0\n    nt = np.ceil(t / skipstep)\n    for trainset, testset in xvschema(t, skipstep):\n\n        a, b = _construct_var_eqns(atleast_3d(data[trainset, :, :]), p)\n        c, d = _construct_var_eqns(atleast_3d(data[testset, :, :]), p)\n\n        e = sp.linalg.inv(np.eye(a.shape[1]) * delta ** 2 + a.T.dot(a))\n\n        ba = b.transpose().dot(a)\n        dc = d.transpose().dot(c)\n        bae = ba.dot(e)\n        baee = bae.dot(e)\n        baecc = bae.dot(c.transpose().dot(c))\n\n        l += np.sum(baecc * bae - 2 * bae * dc) + np.sum(d ** 2)\n        k += np.sum(baee * dc - baecc * baee) * 4 * delta\n\n    return l / (nt * d.size), k / (nt * d.size)", "response": "Calculate mean squared generalization error and its gradient for the overdetermined equation system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect which function to use for MSGE calculation.", "response": "def _get_msge_with_gradient_func(shape, p):\n    \"\"\"Select which function to use for MSGE calculation (over- or\n    underdetermined).\n    \"\"\"\n    t, m, l = shape\n\n    n = (l - p) * t\n    underdetermined = n < m * p\n\n    if underdetermined:\n        return _msge_with_gradient_underdetermined\n    else:\n        return _msge_with_gradient_overdetermined"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates mean squared generalization error and its gradient", "response": "def _get_msge_with_gradient(data, delta, xvschema, skipstep, p):\n    \"\"\"Calculate mean squared generalization error and its gradient,\n    automatically selecting the best function.\n    \"\"\"\n    t, m, l = data.shape\n\n    n = (l - p) * t\n    underdetermined = n < m * p\n\n    if underdetermined:\n        return _msge_with_gradient_underdetermined(data, delta, xvschema,\n                                                   skipstep, p)\n    else:\n        return _msge_with_gradient_overdetermined(data, delta, xvschema,\n                                                  skipstep, p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, data):\n        data = atleast_3d(data)\n\n        if self.delta == 0 or self.delta is None:\n            # ordinary least squares\n            x, y = self._construct_eqns(data)\n        else:\n            # regularized least squares (ridge regression)\n            x, y = self._construct_eqns_rls(data)\n\n        b, res, rank, s = sp.linalg.lstsq(x, y)\n\n        self.coef = b.transpose()\n\n        self.residuals = data - self.predict(data)\n        self.rescov = sp.cov(cat_trials(self.residuals[:, :, self.p:]))\n\n        return self", "response": "Fit VAR model to data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef optimize_order(self, data, min_p=1, max_p=None):\n        data = np.asarray(data)\n        if data.shape[0] < 2:\n            raise ValueError(\"At least two trials are required.\")\n\n        msge, prange = [], []\n\n        par, func = parallel_loop(_get_msge_with_gradient, n_jobs=self.n_jobs,\n                                  verbose=self.verbose)\n        if self.n_jobs is None:\n            npar = 1\n        elif self.n_jobs < 0:\n                npar = 4  # is this a sane default?\n        else:\n            npar = self.n_jobs\n\n        p = min_p\n        while True:\n            result = par(func(data, self.delta, self.xvschema, 1, p_)\n                         for p_ in range(p, p + npar))\n            j, k = zip(*result)\n            prange.extend(range(p, p + npar))\n            msge.extend(j)\n            p += npar\n            if max_p is None:\n                if len(msge) >= 2 and msge[-1] > msge[-2]:\n                    break\n            else:\n                if prange[-1] >= max_p:\n                    i = prange.index(max_p) + 1\n                    prange = prange[:i]\n                    msge = msge[:i]\n                    break\n        self.p = prange[np.argmin(msge)]\n        return zip(prange, msge)", "response": "Determine optimal model order by minimizing the mean squared of the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noptimize ridge penalty with bisection search.", "response": "def optimize_delta_bisection(self, data, skipstep=1, verbose=None):\n        \"\"\"Find optimal ridge penalty with bisection search.\n        \n        Parameters\n        ----------\n        data : array, shape (n_trials, n_channels, n_samples)\n            Epoched data set. At least two trials are required.\n        skipstep : int, optional\n            Speed up calculation by skipping samples during cost function\n            calculation.\n            \n        Returns\n        -------\n        self : :class:`VAR`\n            The :class:`VAR` object to facilitate method chaining (see usage\n            example).\n        \"\"\"\n        data = atleast_3d(data)\n        if data.shape[0] < 2:\n            raise ValueError(\"At least two trials are required.\")\n\n        if verbose is None:\n            verbose = config.getboolean('scot', 'verbose')\n\n        maxsteps = 10\n        maxdelta = 1e50\n\n        a = -10\n        b = 10\n\n        trform = lambda x: np.sqrt(np.exp(x))\n\n        msge = _get_msge_with_gradient_func(data.shape, self.p)\n\n        ja, ka = msge(data, trform(a), self.xvschema, skipstep, self.p)\n        jb, kb = msge(data, trform(b), self.xvschema, skipstep, self.p)\n\n        # before starting the real bisection, assure the interval contains 0\n        while np.sign(ka) == np.sign(kb):\n            if verbose:\n                print('Bisection initial interval (%f,%f) does not contain 0. '\n                      'New interval: (%f,%f)' % (a, b, a * 2, b * 2))\n            a *= 2\n            b *= 2\n            ja, ka = msge(data, trform(a), self.xvschema, skipstep, self.p)\n            jb, kb = msge(data, trform(b), self.xvschema, skipstep, self.p)\n\n            if trform(b) >= maxdelta:\n                if verbose:\n                    print('Bisection: could not find initial interval.')\n                    print(' ********* Delta set to zero! ************ ')\n                return 0\n\n        nsteps = 0\n\n        while nsteps < maxsteps:\n            # point where the line between a and b crosses zero\n            # this is not very stable!\n            #c = a + (b-a) * np.abs(ka) / np.abs(kb-ka)\n            c = (a + b) / 2\n            j, k = msge(data, trform(c), self.xvschema, skipstep, self.p)\n            if np.sign(k) == np.sign(ka):\n                a, ka = c, k\n            else:\n                b, kb = c, k\n\n            nsteps += 1\n            tmp = trform([a, b, a + (b - a) * np.abs(ka) / np.abs(kb - ka)])\n            if verbose:\n                print('%d Bisection Interval: %f - %f, (projected: %f)' %\n                      (nsteps, tmp[0], tmp[1], tmp[2]))\n\n        self.delta = trform(a + (b - a) * np.abs(ka) / np.abs(kb - ka))\n        if verbose:\n            print('Final point: %f' % self.delta)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mvarica(x, var, cl=None, reducedim=0.99, optimize_var=False, backend=None,\n            varfit='ensemble', random_state=None):\n    \"\"\" Performs joint VAR model fitting and ICA source separation.\n    \n    This function implements the MVARICA procedure [1]_.\n    \n    Parameters\n    ----------\n    x : array-like, shape = [n_trials, n_channels, n_samples] or [n_channels, n_samples]\n        data set\n    var : :class:`~scot.var.VARBase`-like object\n        Vector autoregressive model (VAR) object that is used for model fitting.\n    cl : list of valid dict keys, optional\n        Class labels associated with each trial.\n    reducedim : {int, float, 'no_pca', None}, optional\n        A number of less than 1 is interpreted as the fraction of variance that should remain in the data. All\n        components that describe in total less than `1-reducedim` of the variance are removed by the PCA step.\n        An integer number of 1 or greater is interpreted as the number of components to keep after applying PCA.\n        If set to None, all PCA components are retained. If set to 'no_pca', the PCA step is skipped.\n    optimize_var : bool, optional\n        Whether to call automatic optimization of the VAR fitting routine.\n    backend : dict-like, optional\n        Specify backend to use. When set to None the backend configured in config.backend is used.\n    varfit : string\n        Determines how to calculate the residuals for source decomposition.\n        'ensemble' (default) fits one model to the whole data set,\n        'class' fits a new model for each class, and\n        'trial' fits a new model for each individual trial.\n        \n    Returns\n    -------\n    result : class\n        A class with the following attributes is returned:\n            \n        +---------------+----------------------------------------------------------+\n        | mixing        | Source mixing matrix                                     |\n        +---------------+----------------------------------------------------------+\n        | unmixing      | Source unmixing matrix                                   |\n        +---------------+----------------------------------------------------------+\n        | residuals     | Residuals of the VAR model(s) in source space            |\n        +---------------+----------------------------------------------------------+\n        | var_residuals | Residuals of the VAR model(s) in EEG space (before ICA)  |\n        +---------------+----------------------------------------------------------+\n        | c             | Noise covariance of the VAR model(s) in source space     |\n        +---------------+----------------------------------------------------------+\n        | b             | VAR model coefficients (source space)                    |\n        +---------------+----------------------------------------------------------+\n        | a             | VAR model coefficients (EEG space)                       |\n        +---------------+----------------------------------------------------------+\n        \n    Notes\n    -----\n    MVARICA is performed with the following steps:        \n    1. Optional dimensionality reduction with PCA\n    2. Fitting a VAR model tho the data\n    3. Decomposing the VAR model residuals with ICA\n    4. Correcting the VAR coefficients\n        \n    References\n    ----------\n    .. [1] G. Gomez-Herrero et al. \"Measuring directional coupling between EEG sources\", NeuroImage, 2008\n    \"\"\"\n\n    x = atleast_3d(x)\n    t, m, l = np.shape(x)\n\n    if backend is None:\n        backend = scotbackend\n\n    # pre-transform the data with PCA\n    if reducedim == 'no_pca':\n        c = np.eye(m)\n        d = np.eye(m)\n        xpca = x\n    else:\n        c, d, xpca = backend['pca'](x, reducedim)\n\n    if optimize_var:\n        var.optimize(xpca)\n\n    if varfit == 'trial':\n        r = np.zeros(xpca.shape)\n        for i in range(t):\n            # fit MVAR model\n            a = var.fit(xpca[i, :, :])\n            # residuals\n            r[i, :, :] = xpca[i, :, :] - var.predict(xpca[i, :, :])[0, :, :]\n    elif varfit == 'class':\n        r = np.zeros(xpca.shape)\n        for i in np.unique(cl):\n            mask = cl == i\n            a = var.fit(xpca[mask, :, :])\n            r[mask, :, :] = xpca[mask, :, :] - var.predict(xpca[mask, :, :])\n    elif varfit == 'ensemble':\n        # fit MVAR model\n        a = var.fit(xpca)\n        # residuals\n        r = xpca - var.predict(xpca)\n    else:\n        raise ValueError('unknown VAR fitting mode: {}'.format(varfit))\n\n    # run on residuals ICA to estimate volume conduction    \n    mx, ux = backend['ica'](cat_trials(r), random_state=random_state)\n\n    # driving process\n    e = dot_special(ux.T, r)\n\n    # correct AR coefficients\n    b = a.copy()\n    for k in range(0, a.p):\n        b.coef[:, k::a.p] = mx.dot(a.coef[:, k::a.p].transpose()).dot(ux).transpose()\n\n    # correct (un)mixing matrix estimatees\n    mx = mx.dot(d)\n    ux = c.dot(ux)\n\n    class Result:\n        unmixing = ux\n        mixing = mx\n        residuals = e\n        var_residuals = r\n        c = np.cov(cat_trials(e).T, rowvar=False)\n\n    Result.b = b\n    Result.a = a\n    Result.xpca = xpca\n        \n    return Result", "response": "This function performs the MVARICA model fitting and ICA source separation procedure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize from euclidean vector", "response": "def fromvector(cls, v):\n        \"\"\"Initialize from euclidean vector\"\"\"\n        w = v.normalized()\n        return cls(w.x, w.y, w.z)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self):\n        return [self._pos3d.x, self._pos3d.y, self._pos3d.z]", "response": "position in 3d space"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distance(self, other):\n        return math.acos(self._pos3d.dot(other.vector))", "response": "Distance to another point on the sphere"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef distances(self, points):\n        return [math.acos(self._pos3d.dot(p.vector)) for p in points]", "response": "Distance to other points on the sphere"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy another vector to a new one.", "response": "def fromvector(cls, v):\n        \"\"\"Copy another vector\"\"\"\n        return cls(v.x, v.y, v.z)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef norm2(self):\n        return self.x * self.x + self.y * self.y + self.z * self.z", "response": "Squared norm of the vector"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rotate(self, l, u):\n        cl = math.cos(l)\n        sl = math.sin(l)\n        x = (cl + u.x * u.x * (1 - cl)) * self.x + (u.x * u.y * (1 - cl) - u.z * sl) * self.y + (\n        u.x * u.z * (1 - cl) + u.y * sl) * self.z\n        y = (u.y * u.x * (1 - cl) + u.z * sl) * self.x + (cl + u.y * u.y * (1 - cl)) * self.y + (\n        u.y * u.z * (1 - cl) - u.x * sl) * self.z\n        z = (u.z * u.x * (1 - cl) - u.y * sl) * self.x + (u.z * u.y * (1 - cl) + u.x * sl) * self.y + (\n        cl + u.z * u.z * (1 - cl)) * self.z\n        self.x, self.y, self.z = x, y, z\n        return self", "response": "rotate l radians around axis u"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef csp(x, cl, numcomp=None):\n\n    x = np.asarray(x)\n    cl = np.asarray(cl).ravel()\n\n    if x.ndim != 3 or x.shape[0] < 2:\n        raise AttributeError('CSP requires at least two trials.')\n\n    t, m, n = x.shape\n    \n    if t != cl.size:\n        raise AttributeError('CSP only works with multiple classes. Number of '\n                             'elements in cl ({}) must equal the first '\n                             'dimension of x ({})'.format(cl.size, t))\n\n    labels = np.unique(cl)\n    \n    if labels.size != 2:\n        raise AttributeError('CSP is currently implemented for two classes '\n                             'only (got {}).'.format(labels.size))\n        \n    x1 = x[cl == labels[0], :, :]\n    x2 = x[cl == labels[1], :, :]\n    \n    sigma1 = np.zeros((m, m))\n    for t in range(x1.shape[0]):\n        sigma1 += np.cov(x1[t, :, :]) / x1.shape[0]\n    sigma1 /= sigma1.trace()\n    \n    sigma2 = np.zeros((m, m))\n    for t in range(x2.shape[0]):\n        sigma2 += np.cov(x2[t, :, :]) / x2.shape[0]\n    sigma2 /= sigma2.trace()\n\n    e, w = eigh(sigma1, sigma1 + sigma2, overwrite_a=True, overwrite_b=True,\n                check_finite=False)\n\n    order = np.argsort(e)[::-1]\n    w = w[:, order]\n    v = np.linalg.inv(w)\n   \n    # subsequently remove unwanted components from the middle of w and v\n    if numcomp is None:\n        numcomp = w.shape[1]\n    while w.shape[1] > numcomp:\n        i = int(np.floor(w.shape[1]/2))\n        w = np.delete(w, i, 1)\n        v = np.delete(v, i, 0)\n        \n    return w, v", "response": "Calculate common spatial patterns for a set of classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cuthill_mckee(matrix):\n    matrix = np.atleast_2d(matrix)\n    n, m = matrix.shape\n    assert(n == m)\n\n    # make sure the matrix is really symmetric. This is equivalent to\n    # converting a directed adjacency matrix into a undirected adjacency matrix.\n    matrix = np.logical_or(matrix, matrix.T)\n\n    degree = np.sum(matrix, 0)\n    order = [np.argmin(degree)]\n\n    for i in range(n):\n        adj = np.nonzero(matrix[order[i]])[0]\n        adj = [a for a in adj if a not in order]\n        if not adj:\n            idx = [i for i in range(n) if i not in order]\n            order.append(idx[np.argmin(degree[idx])])\n        else:\n            if len(adj) == 1:\n                order.append(adj[0])\n            else:\n                adj = np.asarray(adj)\n                i = adj[np.argsort(degree[adj])]\n                order.extend(i.tolist())\n        if len(order) == n:\n            break\n\n    return order", "response": "Implementation of the Cuthill - McKee algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the connectivity of the given measures.", "response": "def connectivity(measure_names, b, c=None, nfft=512):\n    \"\"\"Calculate connectivity measures.\n\n    Parameters\n    ----------\n    measure_names : str or list of str\n        Name(s) of the connectivity measure(s) to calculate. See\n        :class:`Connectivity` for supported measures.\n    b : array, shape (n_channels, n_channels * model_order)\n        VAR model coefficients. See :ref:`var-model-coefficients` for details\n        about the arrangement of coefficients.\n    c : array, shape (n_channels, n_channels), optional\n        Covariance matrix of the driving noise process. Identity matrix is used\n        if set to None (default).\n    nfft : int, optional\n        Number of frequency bins to calculate. Note that these points cover the\n        range between 0 and half the sampling rate.\n\n    Returns\n    -------\n    result : array, shape (n_channels, n_channels, `nfft`)\n        An array of shape (m, m, nfft) is returned if measures is a string. If\n        measures is a list of strings, a dictionary is returned, where each key\n        is the name of the measure, and the corresponding values are arrays of\n        shape (m, m, nfft).\n\n    Notes\n    -----\n    When using this function, it is more efficient to get several measures at\n    once than calling the function multiple times.\n\n    Examples\n    --------\n    >>> c = connectivity(['DTF', 'PDC'], [[0.3, 0.6], [0.0, 0.9]])\n    \"\"\"\n    con = Connectivity(b, c, nfft)\n    try:\n        return getattr(con, measure_names)()\n    except TypeError:\n        return dict((m, getattr(con, m)()) for m in measure_names)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninverse of the noise covariance matrix.", "response": "def Cinv(self):\n        \"\"\"Inverse of the noise covariance.\"\"\"\n        try:\n            return np.linalg.inv(self.c)\n        except np.linalg.linalg.LinAlgError:\n            print('Warning: non-invertible noise covariance matrix c.')\n            return np.eye(self.c.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef S(self):\n        if self.c is None:\n            raise RuntimeError('Cross-spectral density requires noise '\n                               'covariance matrix c.')\n        H = self.H()\n        # TODO: can we do that more efficiently?\n        S = np.empty(H.shape, dtype=H.dtype)\n        for f in range(H.shape[2]):\n            S[:, :, f] = H[:, :, f].dot(self.c).dot(H[:, :, f].conj().T)\n        return S", "response": "Cross - spectral density."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninverse cross - spectral density.", "response": "def G(self):\n        \"\"\"Inverse cross-spectral density.\n\n        .. math:: \\mathbf{G}(f) = \\mathbf{A}(f) \\mathbf{C}^{-1} \\mathbf{A}'(f)\n        \"\"\"\n        if self.c is None:\n            raise RuntimeError('Inverse cross spectral density requires '\n                               'invertible noise covariance matrix c.')\n        A = self.A()\n        # TODO: can we do that more efficiently?\n        G = np.einsum('ji..., jk... ->ik...', A.conj(), self.Cinv())\n        G = np.einsum('ij..., jk... ->ik...', G, A)\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pCOH(self):\n        G = self.G()\n        # TODO: can we do that more efficiently?\n        return G / np.sqrt(np.einsum('ii..., jj... ->ij...', G, G))", "response": "Partial coherence.\n\n        .. math:: \\mathrm{pCOH}_{ij}(f) = \\\\frac{G_{ij}(f)}\n                                                {\\sqrt{G_{ii}(f) G_{jj}(f)}}\n\n        References\n        ----------\n        P. J. Franaszczuk, K. J. Blinowska, M. Kowalczyk. The application of\n        parametric multichannel spectral estimates in the study of electrical\n        brain activity. Biol. Cybernetics 51(4): 239-247, 1985."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ffPDC(self):\n        A = self.A()\n        return np.abs(A * self.nfft / np.sqrt(np.sum(A.conj() * A, axis=(0, 2),\n                                                     keepdims=True)))", "response": "Full frequency partial directed coherence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef PDCF(self):\n        A = self.A()\n        # TODO: can we do that more efficiently?\n        return np.abs(A / np.sqrt(np.einsum('aj..., ab..., bj... ->j...',\n                                            A.conj(), self.Cinv(), A)))", "response": "Partial directed coherence factor of a new concept\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GPDC(self):\n        A = self.A()\n        tmp = A / np.sqrt(np.einsum('aj..., a..., aj..., ii... ->ij...',\n                                    A.conj(), 1 / np.diag(self.c), A, self.c))\n        return np.abs(tmp)", "response": "Generalized partial directed coherence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndirect transfer function. .. math:: \\mathrm{DTF}_{ij}(f) = \\\\frac{H_{ij}(f)} {\\sqrt{H_{i:}(f) H_{i:}'(f)}} References ---------- M. J. Kaminski, K. J. Blinowska. A new method of the description of the information flow in the brain structures. Biol. Cybernetics 65(3): 203-210, 1991.", "response": "def DTF(self):\n        \"\"\"Directed transfer function.\n\n        .. math:: \\mathrm{DTF}_{ij}(f) = \\\\frac{H_{ij}(f)}\n                                               {\\sqrt{H_{i:}(f) H_{i:}'(f)}}\n\n        References\n        ----------\n        M. J. Kaminski, K. J. Blinowska. A new method of the description of the\n        information flow in the brain structures. Biol. Cybernetics 65(3):\n        203-210, 1991.\n        \"\"\"\n        H = self.H()\n        return np.abs(H / np.sqrt(np.sum(H * H.conj(), axis=1, keepdims=True)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ffDTF(self):\n        H = self.H()\n        return np.abs(H * self.nfft / np.sqrt(np.sum(H * H.conj(), axis=(1, 2),\n                                                     keepdims=True)))", "response": "Full frequency directed transfer function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngeneralize directed transfer function.", "response": "def GDTF(self):\n        \"\"\"Generalized directed transfer function.\n\n        .. math:: \\mathrm{GPDC}_{ij}(f) = \\\\frac{\\sigma_j |H_{ij}(f)|}\n            {\\sqrt{H_{i:}(f) \\mathrm{diag}(\\mathbf{C}) H_{i:}'(f)}}\n\n        References\n        ----------\n        L. Faes, S. Erla, G. Nollo. Measuring connectivity in linear\n        multivariate processes: definitions, interpretation, and practical\n        analysis. Comput. Math. Meth. Med. 2012: 140513, 2012.\n        \"\"\"\n        H = self.H()\n        tmp = H / np.sqrt(np.einsum('ia..., aa..., ia..., j... ->ij...',\n                                    H.conj(), self.c, H,\n                                    1 / self.c.diagonal()))\n        return np.abs(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nenriches this error with additional information.", "response": "def enrich(self, expected=None, provided=None, path=None, validator=None):\n        \"\"\" Enrich this error with additional information.\n\n        This works with both Invalid and MultipleInvalid (thanks to `Invalid` being iterable):\n        in the latter case, the defaults are applied to all collected errors.\n\n        The specified arguments are only set on `Invalid` errors which do not have any value on the property.\n\n        One exclusion is `path`: if provided, it is prepended to `Invalid.path`.\n        This feature is especially useful when validating the whole input with multiple different schemas:\n\n        ```python\n        from good import Schema, Invalid\n\n        schema = Schema(int)\n        input = {\n            'user': {\n                'age': 10,\n            }\n        }\n\n        try:\n            schema(input['user']['age'])\n        except Invalid as e:\n            e.enrich(path=['user', 'age'])  # Make the path reflect the reality\n            raise  # re-raise the error with updated fields\n        ```\n\n        This is used when validating a value within a container.\n\n        :param expected: Invalid.expected default\n        :type expected: unicode|None\n        :param provided: Invalid.provided default\n        :type provided: unicode|None\n        :param path: Prefix to prepend to Invalid.path\n        :type path: list|None\n        :param validator: Invalid.validator default\n        :rtype: Invalid|MultipleInvalid\n        \"\"\"\n        for e in self:\n            # defaults on fields\n            if e.expected is None and expected is not None:\n                e.expected = expected\n            if e.provided is None and provided is not None:\n                e.provided = provided\n            if e.validator is None and validator is not None:\n                e.validator = validator\n            # path prefix\n            e.path = (path or []) + e.path\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flatten(cls, errors):\n        ers = []\n        for e in errors:\n            if isinstance(e, MultipleInvalid):\n                ers.extend(cls.flatten(e.errors))\n            else:\n                ers.append(e)\n        return ers", "response": "Flatten a list of MultipleInvalid objects into a single list of Invalid objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwarp the given locations to a spherical layout.", "response": "def warp_locations(locations, y_center=None, return_ellipsoid=False, verbose=False):\n    \"\"\" Warp EEG electrode locations to spherical layout.\n\n    EEG Electrodes are warped to a spherical layout in three steps:\n        1. An ellipsoid is least-squares-fitted to the electrode locations.\n        2. Electrodes are displaced to the nearest point on the ellipsoid's surface.\n        3. The ellipsoid is transformed to a sphere, causing the new locations to lie exactly on a spherical surface\n           with unit radius.\n\n    This procedure intends to minimize electrode displacement in the original coordinate space. Simply projecting\n    electrodes on a sphere (e.g. by normalizing the x/y/z coordinates) typically gives much larger displacements.\n\n    Parameters\n    ----------\n    locations : array-like, shape = [n_electrodes, 3]\n        Eeach row of `locations` corresponds to the location of an EEG electrode in cartesian x/y/z coordinates.\n    y_center : float, optional\n        Fix the y-coordinate of the ellipsoid's center to this value (optional). This is useful to align the ellipsoid\n        with the central electrodes.\n    return_ellipsoid : bool, optional\n        If `true` center and radii of the ellipsoid are returned.\n\n    Returns\n    -------\n    newlocs : array-like, shape = [n_electrodes, 3]\n        Electrode locations on unit sphere.\n    c : array-like, shape = [3], (only returned if `return_ellipsoid` evaluates to `True`)\n        Center of the ellipsoid in the original location's coordinate space.\n    r : array-like, shape = [3], (only returned if `return_ellipsoid` evaluates to `True`)\n        Radii (x, y, z) of the ellipsoid in the original location's coordinate space.\n    \"\"\"\n    locations = np.asarray(locations)\n\n    if y_center is None:\n        c, r = _fit_ellipsoid_full(locations)\n    else:\n        c, r = _fit_ellipsoid_partial(locations, y_center)\n\n    elliptic_locations = _project_on_ellipsoid(c, r, locations)\n\n    if verbose:\n        print('Head ellipsoid center:', c)\n        print('Head ellipsoid radii:', r)\n        distance = np.sqrt(np.sum((locations - elliptic_locations)**2, axis=1))\n        print('Minimum electrode displacement:', np.min(distance))\n        print('Average electrode displacement:', np.mean(distance))\n        print('Maximum electrode displacement:', np.max(distance))\n\n    spherical_locations = (elliptic_locations - c) / r\n\n    if return_ellipsoid:\n        return spherical_locations, c, r\n\n    return spherical_locations"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nidentify all 6 ellipsoid parametes ( center radii", "response": "def _fit_ellipsoid_full(locations):\n    \"\"\"identify all 6 ellipsoid parametes (center, radii)\"\"\"\n    a = np.hstack([locations*2, locations**2])\n    lsq = sp.linalg.lstsq(a, np.ones(locations.shape[0]))\n    x = lsq[0]\n    c = -x[:3] / x[3:]\n    gam = 1 + np.sum(x[:3]**2 / x[3:])\n    r = np.sqrt(gam / x[3:])\n    return c, r"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nidentifies only 5 ellipsoid parameters ( y - center determined by e. g. Cz", "response": "def _fit_ellipsoid_partial(locations, cy):\n    \"\"\"identify only 5 ellipsoid parameters (y-center determined by e.g. Cz)\"\"\"\n    a = np.vstack([locations[:, 0]**2,\n                   locations[:, 1]**2 - 2 * locations[:, 1] * cy,\n                   locations[:, 2]**2,\n                   locations[:, 0]*2,\n                   locations[:, 2]*2]).T\n    x = sp.linalg.lstsq(a, np.ones(locations.shape[0]))[0]\n    c = [-x[3] / x[0], cy, -x[4] / x[2]]\n    gam = 1 + x[3]**2 / x[0] + x[4]**2 / x[2]\n    r = np.sqrt([gam / x[0], gam / x[1], gam / x[2]])\n    return c, r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprojecting on ellipsoid surface", "response": "def _project_on_ellipsoid(c, r, locations):\n    \"\"\"displace locations to the nearest point on ellipsoid surface\"\"\"\n    p0 = locations - c  # original locations\n\n    l2 = 1 / np.sum(p0**2 / r**2, axis=1, keepdims=True)\n    p = p0 * np.sqrt(l2)  # initial approximation (projection of points towards center of ellipsoid)\n\n    fun = lambda x: np.sum((x.reshape(p0.shape) - p0)**2)              # minimize distance between new and old points\n    con = lambda x: np.sum(x.reshape(p0.shape)**2 / r**2, axis=1) - 1  # new points constrained to surface of ellipsoid\n    res = sp.optimize.minimize(fun, p, constraints={'type': 'eq', 'fun': con}, method='SLSQP')\n\n    return res['x'].reshape(p0.shape) + c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncut continuous signal into segments.", "response": "def cut_segments(x2d, tr, start, stop):\n    \"\"\"Cut continuous signal into segments.\n\n    Parameters\n    ----------\n    x2d : array, shape (m, n)\n        Input data with m signals and n samples.\n    tr : list of int\n        Trigger positions.\n    start : int\n        Window start (offset relative to trigger).\n    stop : int\n        Window end (offset relative to trigger).\n\n    Returns\n    -------\n    x3d : array, shape (len(tr), m, stop-start)\n        Segments cut from data. Individual segments are stacked along the first\n        dimension.\n\n    See also\n    --------\n    cat_trials : Concatenate segments.\n\n    Examples\n    --------\n    >>> data = np.random.randn(5, 1000)  # 5 channels, 1000 samples\n    >>> tr = [750, 500, 250]  # three segments\n    >>> x3d = cut_segments(data, tr, 50, 100)  # each segment is 50 samples\n    >>> x3d.shape\n    (3, 5, 50)\n    \"\"\"\n    if start != int(start):\n        raise ValueError(\"start index must be an integer\")\n    if stop != int(stop):\n        raise ValueError(\"stop index must be an integer\")\n\n    x2d = np.atleast_2d(x2d)\n    tr = np.asarray(tr, dtype=int).ravel()\n    win = np.arange(start, stop, dtype=int)\n    return np.concatenate([x2d[np.newaxis, :, t + win] for t in tr])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconcatenating trials along time axis.", "response": "def cat_trials(x3d):\n    \"\"\"Concatenate trials along time axis.\n\n    Parameters\n    ----------\n    x3d : array, shape (t, m, n)\n        Segmented input data with t trials, m signals, and n samples.\n\n    Returns\n    -------\n    x2d : array, shape (m, t * n)\n        Trials are concatenated along the second axis.\n\n    See also\n    --------\n    cut_segments : Cut segments from continuous data.\n\n    Examples\n    --------\n    >>> x = np.random.randn(6, 4, 150)\n    >>> y = cat_trials(x)\n    >>> y.shape\n    (4, 900)\n    \"\"\"\n    x3d = atleast_3d(x3d)\n    t = x3d.shape[0]\n    return np.concatenate(np.split(x3d, t, 0), axis=2).squeeze(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsegment - wise dot product of two arrays with each trial of x3d.", "response": "def dot_special(x2d, x3d):\n    \"\"\"Segment-wise dot product.\n\n    This function calculates the dot product of x2d with each trial of x3d.\n\n    Parameters\n    ----------\n    x2d : array, shape (p, m)\n        Input argument.\n    x3d : array, shape (t, m, n)\n        Segmented input data with t trials, m signals, and n samples. The dot\n        product with x2d is calculated for each trial.\n\n    Returns\n    -------\n    out : array, shape (t, p, n)\n        Dot product of x2d with each trial of x3d.\n\n    Examples\n    --------\n    >>> x = np.random.randn(6, 40, 150)\n    >>> a = np.ones((7, 40))\n    >>> y = dot_special(a, x)\n    >>> y.shape\n    (6, 7, 150)\n    \"\"\"\n    x3d = atleast_3d(x3d)\n    x2d = np.atleast_2d(x2d)\n    return np.concatenate([x2d.dot(x3d[i, ...])[np.newaxis, ...]\n                           for i in range(x3d.shape[0])])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef randomize_phase(data, random_state=None):\n    rng = check_random_state(random_state)\n    data = np.asarray(data)\n    data_freq = np.fft.rfft(data)\n    data_freq = np.abs(data_freq) * np.exp(1j*rng.random_sample(data_freq.shape)*2*np.pi)\n    return np.fft.irfft(data_freq, data.shape[-1])", "response": "Phase randomization.\n\n    This function randomizes the spectral phase of the input data along the\n    last dimension.\n\n    Parameters\n    ----------\n    data : array\n        Input array.\n\n    Returns\n    -------\n    out : array\n        Array of same shape as data.\n\n    Notes\n    -----\n    The algorithm randomizes the phase component of the input's complex Fourier\n    transform.\n\n    Examples\n    --------\n    .. plot::\n        :include-source:\n\n        from pylab import *\n        from scot.datatools import randomize_phase\n        np.random.seed(1234)\n        s = np.sin(np.linspace(0,10*np.pi,1000))\n        x = np.vstack([s, np.sign(s)])\n        y = randomize_phase(x)\n        subplot(2,1,1)\n        title('Phase randomization of sine wave and rectangular function')\n        plot(x.T + [1.5, -1.5]), axis([0,1000,-3,3])\n        subplot(2,1,2)\n        plot(y.T + [1.5, -1.5]), axis([0,1000,-3,3])\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acm(x, l):\n    x = atleast_3d(x)\n\n    if l > x.shape[2]-1:\n        raise AttributeError(\"lag exceeds data length\")\n\n    ## subtract mean from each trial\n    #for t in range(x.shape[2]):\n    #    x[:, :, t] -= np.mean(x[:, :, t], axis=0)\n\n    if l == 0:\n        a, b = x, x\n    else:\n        a = x[:, :, l:]\n        b = x[:, :, 0:-l]\n\n    c = np.zeros((x.shape[1], x.shape[1]))\n    for t in range(x.shape[0]):\n        c += a[t, :, :].dot(b[t, :, :].T) / a.shape[2]\n    c /= x.shape[0]\n\n    return c.T", "response": "Compute autocovariance matrix at lag l."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the connectivity of a multivariate time series by phase.", "response": "def surrogate_connectivity(measure_names, data, var, nfft=512, repeats=100,\n                           n_jobs=1, verbose=0, random_state=None):\n    \"\"\"Calculate surrogate connectivity for a multivariate time series by phase\n    randomization [1]_.\n\n    .. note:: Parameter `var` will be modified by the function. Treat as\n    undefined after the function returns.\n\n    Parameters\n    ----------\n    measures : str or list of str\n        Name(s) of the connectivity measure(s) to calculate. See\n        :class:`Connectivity` for supported measures.\n    data : array, shape (trials, channels, samples) or (channels, samples)\n        Time series data (2D or 3D for multiple trials)\n    var : VARBase-like object\n        Instance of a VAR model.\n    nfft : int, optional\n        Number of frequency bins to calculate. Note that these points cover the\n        range between 0 and half the sampling rate.\n    repeats : int, optional\n        Number of surrogate samples to take.\n    n_jobs : int | None, optional\n        Number of jobs to run in parallel. If set to None, joblib is not used\n        at all. See `joblib.Parallel` for details.\n    verbose : int, optional\n        Verbosity level passed to joblib.\n\n    Returns\n    -------\n    result : array, shape (`repeats`, n_channels, n_channels, nfft)\n        Values of the connectivity measure for each surrogate. If\n        `measure_names` is a list of strings a dictionary is returned, where\n        each key is the name of the measure, and the corresponding values are\n        arrays of shape (`repeats`, n_channels, n_channels, nfft).\n\n    .. [1] J. Theiler et al. Testing for nonlinearity in time series: the\n           method of surrogate data. Physica D, 58: 77-94, 1992.\n    \"\"\"\n    par, func = parallel_loop(_calc_surrogate, n_jobs=n_jobs, verbose=verbose)\n    output = par(func(randomize_phase(data, random_state=random_state), var,\n                      measure_names, nfft) for _ in range(repeats))\n    return convert_output_(output, measure_names)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the jackknife estimates of each combination of the measures and data.", "response": "def jackknife_connectivity(measures, data, var, nfft=512, leaveout=1, n_jobs=1,\n                           verbose=0):\n    \"\"\"Calculate jackknife estimates of connectivity.\n\n    For each jackknife estimate a block of trials is left out. This is repeated\n    until each trial was left out exactly once. The number of estimates depends\n    on the number of trials and the value of `leaveout`. It is calculated by\n    repeats = `n_trials` // `leaveout`.\n\n    .. note:: Parameter `var` will be modified by the function. Treat as\n    undefined after the function returns.\n\n    Parameters\n    ----------\n    measures : str or list of str\n        Name(s) of the connectivity measure(s) to calculate. See\n        :class:`Connectivity` for supported measures.\n    data : array, shape (trials, channels, samples)\n        Time series data (multiple trials).\n    var : VARBase-like object\n        Instance of a VAR model.\n    nfft : int, optional\n        Number of frequency bins to calculate. Note that these points cover the\n        range between 0 and half the sampling rate.\n    leaveout : int, optional\n        Number of trials to leave out in each estimate.\n    n_jobs : int | None, optional\n        Number of jobs to run in parallel. If set to None, joblib is not used\n        at all. See `joblib.Parallel` for details.\n    verbose : int, optional\n        Verbosity level passed to joblib.\n\n    Returns\n    -------\n    result : array, shape (`repeats`, n_channels, n_channels, nfft)\n        Values of the connectivity measure for each surrogate. If\n        `measure_names` is a list of strings a dictionary is returned, where\n        each key is the name of the measure, and the corresponding values are\n        arrays of shape (`repeats`, n_channels, n_channels, nfft).\n    \"\"\"\n    data = atleast_3d(data)\n    t, m, n = data.shape\n\n    assert(t > 1)\n\n    if leaveout < 1:\n        leaveout = int(leaveout * t)\n\n    num_blocks = t // leaveout\n\n    mask = lambda block: [i for i in range(t) if i < block*leaveout or\n                                                 i >= (block + 1) * leaveout]\n\n    par, func = parallel_loop(_calc_jackknife, n_jobs=n_jobs, verbose=verbose)\n    output = par(func(data[mask(b), :, :], var, measures, nfft)\n                 for b in range(num_blocks))\n    return convert_output_(output, measures)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate bootstrap estimates of connectivity for a set of measures.", "response": "def bootstrap_connectivity(measures, data, var, nfft=512, repeats=100,\n                           num_samples=None, n_jobs=1, verbose=0,\n                           random_state=None):\n    \"\"\"Calculate bootstrap estimates of connectivity.\n\n    To obtain a bootstrap estimate trials are sampled randomly with replacement\n    from the data set.\n\n    .. note:: Parameter `var` will be modified by the function. Treat as\n    undefined after the function returns.\n\n    Parameters\n    ----------\n    measures : str or list of str\n        Name(s) of the connectivity measure(s) to calculate. See\n        :class:`Connectivity` for supported measures.\n    data : array, shape (trials, channels, samples)\n        Time series data (multiple trials).\n    var : VARBase-like object\n        Instance of a VAR model.\n    nfft : int, optional\n        Number of frequency bins to calculate. Note that these points cover the\n        range between 0 and half the sampling rate.\n    repeats : int, optional\n        Number of bootstrap estimates to take.\n    num_samples : int, optional\n        Number of samples to take for each bootstrap estimates. Defaults to the\n        same number of trials as present in the data.\n    n_jobs : int, optional\n        n_jobs : int | None, optional\n        Number of jobs to run in parallel. If set to None, joblib is not used\n        at all. See `joblib.Parallel` for details.\n    verbose : int, optional\n        Verbosity level passed to joblib.\n\n    Returns\n    -------\n    measure : array, shape (`repeats`, n_channels, n_channels, nfft)\n        Values of the connectivity measure for each bootstrap estimate. If\n        `measure_names` is a list of strings a dictionary is returned, where\n        each key is the name of the measure, and the corresponding values are\n        arrays of shape (`repeats`, n_channels, n_channels, nfft).\n    \"\"\"\n    rng = check_random_state(random_state)\n    data = atleast_3d(data)\n    n, m, t = data.shape\n\n    assert(t > 1)\n\n    if num_samples is None:\n        num_samples = t\n\n    mask = lambda r: rng.random_integers(0, data.shape[0]-1, num_samples)\n\n    par, func = parallel_loop(_calc_bootstrap, n_jobs=n_jobs, verbose=verbose)\n    output = par(func(data[mask(r), :, :], var, measures, nfft)\n                 for r in range(repeats))\n    return convert_output_(output, measures)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef significance_fdr(p, alpha):\n    i = np.argsort(p, axis=None)\n    m = i.size - np.sum(np.isnan(p))\n\n    j = np.empty(p.shape, int)\n    j.flat[i] = np.arange(1, i.size + 1)\n\n    mask = p <= alpha * j / m\n\n    if np.sum(mask) == 0:\n        return mask\n\n    # find largest k so that p_k <= alpha*k/m\n    k = np.max(j[mask])\n\n    # reject all H_i for i = 0...k\n    s = j <= k\n\n    return s", "response": "Calculate the significance of a given p - value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_type_name(t, name):\n    assert isinstance(t, type)\n    assert isinstance(name, unicode)\n    __type_names[t] = name", "response": "Register a human - friendly name for the given type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_type_name(t):\n    # Lookup in the mapping\n    try:\n        return __type_names[t]\n    except KeyError:\n        # Specific types\n        if issubclass(t, six.integer_types):\n            return _(u'Integer number')\n\n        # Get name from the Type itself\n        return six.text_type(t.__name__).capitalize()", "response": "Get a human - friendly name for the given type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a human - friendly name for the given callable.", "response": "def get_callable_name(c):\n    \"\"\" Get a human-friendly name for the given callable.\n\n    :param c: The callable to get the name for\n    :type c: callable\n    :rtype: unicode\n    \"\"\"\n    if hasattr(c, 'name'):\n        return six.text_type(c.name)\n    elif hasattr(c, '__name__'):\n        return six.text_type(c.__name__) + u'()'\n    else:\n        return six.text_type(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a human - friendly name for the given primitive.", "response": "def get_primitive_name(schema):\n    \"\"\" Get a human-friendly name for the given primitive.\n\n    :param schema: Schema\n    :type schema: *\n    :rtype: unicode\n    \"\"\"\n    try:\n        return {\n            const.COMPILED_TYPE.LITERAL: six.text_type,\n            const.COMPILED_TYPE.TYPE: get_type_name,\n            const.COMPILED_TYPE.ENUM: get_type_name,\n            const.COMPILED_TYPE.CALLABLE: get_callable_name,\n            const.COMPILED_TYPE.ITERABLE: lambda x: _(u'{type}[{content}]').format(type=get_type_name(list), content=_(u'...') if x else _(u'-')),\n            const.COMPILED_TYPE.MAPPING:  lambda x: _(u'{type}[{content}]').format(type=get_type_name(dict), content=_(u'...') if x else _(u'-')),\n        }[primitive_type(schema)](schema)\n    except KeyError:\n        return six.text_type(repr(schema))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the type of the primitive argument.", "response": "def primitive_type(schema):\n    \"\"\" Get schema type for the primitive argument.\n\n    Note: it does treats markers & schemas as callables!\n\n    :param schema: Value of a primitive type\n    :type schema: *\n    :return: const.COMPILED_TYPE.*\n    :rtype: str|None\n    \"\"\"\n    schema_type = type(schema)\n\n    # Literal\n    if schema_type in const.literal_types:\n        return const.COMPILED_TYPE.LITERAL\n    # Enum\n    elif Enum is not None and isinstance(schema, (EnumMeta, Enum)):\n        return const.COMPILED_TYPE.ENUM\n    # Type\n    elif issubclass(schema_type, six.class_types):\n        return const.COMPILED_TYPE.TYPE\n    # Mapping\n    elif isinstance(schema, collections.Mapping):\n        return const.COMPILED_TYPE.MAPPING\n    # Iterable\n    elif isinstance(schema, collections.Iterable):\n        return const.COMPILED_TYPE.ITERABLE\n    # Callable\n    elif callable(schema):\n        return const.COMPILED_TYPE.CALLABLE\n    # Not detected\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining the given iterable with commas", "response": "def commajoin_as_strings(iterable):\n    \"\"\" Join the given iterable with ',' \"\"\"\n    return _(u',').join((six.text_type(i) for i in iterable))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_topoplots(topo, values):\n    values = np.atleast_2d(values)\n\n    topomaps = []\n\n    for i in range(values.shape[0]):\n        topo.set_values(values[i, :])\n        topo.create_map()\n        topomaps.append(topo.get_map())\n\n    return topomaps", "response": "Prepare multiple topo maps for cached plotting."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_topo(axis, topo, topomap, crange=None, offset=(0,0),\n              plot_locations=True, plot_head=True):\n    \"\"\"Draw a topoplot in given axis.\n\n    .. note:: Parameter `topo` is modified by the function by calling :func:`~eegtopo.topoplot.Topoplot.set_map`.\n\n    Parameters\n    ----------\n    axis : axis\n        Axis to draw into.\n    topo : :class:`~eegtopo.topoplot.Topoplot`\n        This object draws the topo plot\n    topomap : array, shape = [w_pixels, h_pixels]\n        Scalp-projected data\n    crange : [int, int], optional\n        Range of values covered by the colormap.\n        If set to None, [-max(abs(topomap)), max(abs(topomap))] is substituted.\n    offset : [float, float], optional\n        Shift the topo plot by [x,y] in axis units.\n    plot_locations : bool, optional\n        Plot electrode locations.\n    plot_head : bool, optional\n        Plot head cartoon.\n\n    Returns\n    -------\n    h : image\n        Image object the map was plotted into\n    \"\"\"\n    topo.set_map(topomap)\n    h = topo.plot_map(axis, crange=crange, offset=offset)\n    if plot_locations:\n        topo.plot_locations(axis, offset=offset)\n    if plot_head:\n        topo.plot_head(axis, offset=offset)\n    return h", "response": "Draw a topoplot in given axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_sources(topo, mixmaps, unmixmaps, global_scale=None, fig=None):\n    urange, mrange = None, None\n\n    m = len(mixmaps)\n\n    if global_scale:\n        tmp = np.asarray(unmixmaps)\n        tmp = tmp[np.logical_not(np.isnan(tmp))]\n        umax = np.percentile(np.abs(tmp), global_scale)\n        umin = -umax\n        urange = [umin, umax]\n\n        tmp = np.asarray(mixmaps)\n        tmp = tmp[np.logical_not(np.isnan(tmp))]\n        mmax = np.percentile(np.abs(tmp), global_scale)\n        mmin = -mmax\n        mrange = [mmin, mmax]\n\n    y = np.floor(np.sqrt(m * 3 / 4))\n    x = np.ceil(m / y)\n\n    if fig is None:\n        fig = new_figure()\n\n    axes = []\n    for i in range(m):\n        axes.append(fig.add_subplot(2 * y, x, i + 1))\n        plot_topo(axes[-1], topo, unmixmaps[i], crange=urange)\n        axes[-1].set_title(str(i))\n\n        axes.append(fig.add_subplot(2 * y, x, m + i + 1))\n        plot_topo(axes[-1], topo, mixmaps[i], crange=mrange)\n        axes[-1].set_title(str(i))\n\n    for a in axes:\n        a.set_yticks([])\n        a.set_xticks([])\n        a.set_frame_on(False)\n\n    axes[0].set_ylabel('Unmixing weights')\n    axes[1].set_ylabel('Scalp projections')\n\n    return fig", "response": "Plots all scalp projections of mixing - and unmixing - maps."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the topo plots for the given layout.", "response": "def plot_connectivity_topos(layout='diagonal', topo=None, topomaps=None, fig=None):\n    \"\"\"Place topo plots in a figure suitable for connectivity visualization.\n\n    .. note:: Parameter `topo` is modified by the function by calling :func:`~eegtopo.topoplot.Topoplot.set_map`.\n\n    Parameters\n    ----------\n    layout : str\n        'diagonal' -> place topo plots on diagonal.\n        otherwise -> place topo plots in left column and top row.\n    topo : :class:`~eegtopo.topoplot.Topoplot`\n        This object draws the topo plot\n    topomaps : array, shape = [w_pixels, h_pixels]\n        Scalp-projected map\n    fig : Figure object, optional\n        Figure to plot into. If set to `None`, a new figure is created.\n\n    Returns\n    -------\n    fig : Figure object\n        The figure into which was plotted.\n    \"\"\"\n\n    m = len(topomaps)\n\n    if fig is None:\n        fig = new_figure()\n\n    if layout == 'diagonal':\n        for i in range(m):\n            ax = fig.add_subplot(m, m, i*(1+m) + 1)\n            plot_topo(ax, topo, topomaps[i])\n            ax.set_yticks([])\n            ax.set_xticks([])\n            ax.set_frame_on(False)\n    else:\n        for i in range(m):\n            for j in [i+2, (i+1)*(m+1)+1]:\n                ax = fig.add_subplot(m+1, m+1, j)\n                plot_topo(ax, topo, topomaps[i])\n                ax.set_yticks([])\n                ax.set_xticks([])\n                ax.set_frame_on(False)\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_connectivity_spectrum(a, fs=2, freq_range=(-np.inf, np.inf), diagonal=0, border=False, fig=None):\n\n    a = np.atleast_3d(a)\n    if a.ndim == 3:\n        [_, m, f] = a.shape\n        l = 0\n    else:\n        [l, _, m, f] = a.shape\n    freq = np.linspace(0, fs / 2, f)\n\n    lowest, highest = np.inf, 0\n    left = max(freq_range[0], freq[0])\n    right = min(freq_range[1], freq[-1])\n\n    if fig is None:\n        fig = new_figure()\n\n    axes = []\n    for i in range(m):\n        if diagonal == 1:\n            jrange = [i]\n        elif diagonal == 0:\n            jrange = range(m)\n        else:\n            jrange = [j for j in range(m) if j != i]\n        for j in jrange:\n            if border:\n                ax = fig.add_subplot(m+1, m+1, j + (i+1) * (m+1) + 2)\n            else:\n                ax = fig.add_subplot(m, m, j + i * m + 1)\n            axes.append((i, j, ax))\n            if l == 0:\n                ax.plot(freq, a[i, j, :])\n                lowest = min(lowest, np.min(a[i, j, :]))\n                highest = max(highest, np.max(a[i, j, :]))\n            elif l == 1:\n                ax.fill_between(freq, 0, a[0, i, j, :], facecolor=[0.25, 0.25, 0.25], alpha=0.25)\n                lowest = min(lowest, np.min(a[0, i, j, :]))\n                highest = max(highest, np.max(a[0, i, j, :]))\n            else:\n                baseline,  = ax.plot(freq, a[0, i, j, :])\n                ax.fill_between(freq, a[1, i, j, :], a[2, i, j, :], facecolor=baseline.get_color(), alpha=0.25)\n                lowest = min(lowest, np.min(a[:, i, j, :]))\n                highest = max(highest, np.max(a[:, i, j, :]))\n\n    for i, j, ax in axes:\n        ax.xaxis.set_major_locator(MaxNLocator(max(4, 10 - m)))\n        ax.yaxis.set_major_locator(MaxNLocator(max(4, 10 - m)))\n        ax.set_ylim(0, highest)\n        ax.set_xlim(left, right)\n\n        if 0 < i < m - 1:\n            ax.set_xticklabels([])\n        ax.set_yticklabels([])\n\n        if i == 0:\n            ax.xaxis.set_tick_params(labeltop=\"on\", labelbottom=\"off\")\n        if j == m-1:\n            ax.yaxis.set_tick_params(labelright=\"on\", labelleft=\"off\")\n\n        ax.tick_params(labelsize=10)\n\n    _plot_labels(fig, {'x': 0.5, 'y': 0.025, 's': 'f (Hz)',\n                       'horizontalalignment': 'center'})\n\n    return fig", "response": "Plots the connectivity spectrum of a 2D array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_connectivity_significance(s, fs=2, freq_range=(-np.inf, np.inf), diagonal=0, border=False, fig=None):\n\n    a = np.atleast_3d(s)\n    [_, m, f] = a.shape\n    freq = np.linspace(0, fs / 2, f)\n\n    left = max(freq_range[0], freq[0])\n    right = min(freq_range[1], freq[-1])\n\n    imext = (freq[0], freq[-1], -1e25, 1e25)\n\n    if fig is None:\n        fig = new_figure()\n\n    axes = []\n    for i in range(m):\n        if diagonal == 1:\n            jrange = [i]\n        elif diagonal == 0:\n            jrange = range(m)\n        else:\n            jrange = [j for j in range(m) if j != i]\n        for j in jrange:\n            if border:\n                ax = fig.add_subplot(m+1, m+1, j + (i+1) * (m+1) + 2)\n            else:\n                ax = fig.add_subplot(m, m, j + i * m + 1)\n            axes.append((i, j, ax))\n            ax.imshow(s[i, j, np.newaxis], vmin=0, vmax=2, cmap='binary', aspect='auto', extent=imext, zorder=-999)\n\n            ax.xaxis.set_major_locator(MaxNLocator(max(1, 7 - m)))\n            ax.yaxis.set_major_locator(MaxNLocator(max(1, 7 - m)))\n            ax.set_xlim(left, right)\n\n            if 0 < i < m - 1:\n                ax.set_xticks([])\n            if 0 < j < m - 1:\n                ax.set_yticks([])\n\n            if j == 0:\n                ax.yaxis.tick_left()\n            if j == m-1:\n                ax.yaxis.tick_right()\n\n    _plot_labels(fig,\n                 {'x': 0.5, 'y': 0.025, 's': 'frequency (Hz)', 'horizontalalignment': 'center'},\n                 {'x': 0.05, 'y': 0.5, 's': 'magnitude', 'horizontalalignment': 'center', 'rotation': 'vertical'})\n\n    return fig", "response": "Plot a significance of a set of states."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing time - frequency connectivity plots.", "response": "def plot_connectivity_timespectrum(a, fs=2, crange=None, freq_range=(-np.inf, np.inf), time_range=None, diagonal=0, border=False, fig=None):\n    \"\"\" Draw time/frequency connectivity plots.\n\n    Parameters\n    ----------\n    a : array, shape (n_channels, n_channels, n_fft, n_timesteps)\n        Values to draw\n    fs : float\n        Sampling frequency\n    crange : [int, int], optional\n        Range of values covered by the colormap.\n        If set to None, [min(a), max(a)] is substituted.\n    freq_range : (float, float)\n        Frequency range to plot\n    time_range : (float, float)\n        Time range covered by `a`\n    diagonal : {-1, 0, 1}\n        If diagonal == -1 nothing is plotted on the diagonal (a[i,i,:] are not plotted),\n        if diagonal == 0, a is plotted on the diagonal too (all a[i,i,:] are plotted),\n        if diagonal == 1, a is plotted on the diagonal only (only a[i,i,:] are plotted)\n    border : bool\n        If border == true the leftmost column and the topmost row are left blank\n    fig : Figure object, optional\n        Figure to plot into. If set to `None`, a new figure is created.\n\n    Returns\n    -------\n    fig : Figure object\n        The figure into which was plotted.\n    \"\"\"\n    a = np.asarray(a)\n    [_, m, _, t] = a.shape\n\n    if crange is None:\n        crange = [np.min(a), np.max(a)]\n\n    if time_range is None:\n        t0 = 0\n        t1 = t\n    else:\n        t0, t1 = time_range\n\n    f0, f1 = fs / 2, 0\n    extent = [t0, t1, f0, f1]\n\n    ymin = max(freq_range[0], f1)\n    ymax = min(freq_range[1], f0)\n\n    if fig is None:\n        fig = new_figure()\n\n    axes = []\n    for i in range(m):\n        if diagonal == 1:\n            jrange = [i]\n        elif diagonal == 0:\n            jrange = range(m)\n        else:\n            jrange = [j for j in range(m) if j != i]\n        for j in jrange:\n            if border:\n                ax = fig.add_subplot(m+1, m+1, j + (i+1) * (m+1) + 2)\n            else:\n                ax = fig.add_subplot(m, m, j + i * m + 1)\n            axes.append(ax)\n            ax.imshow(a[i, j, :, :], vmin=crange[0], vmax=crange[1], aspect='auto', extent=extent)\n            ax.invert_yaxis()\n\n            ax.xaxis.set_major_locator(MaxNLocator(max(1, 9 - m)))\n            ax.yaxis.set_major_locator(MaxNLocator(max(1, 7 - m)))\n            ax.set_ylim(ymin, ymax)\n\n            if 0 < i < m - 1:\n                ax.set_xticks([])\n            if 0 < j < m - 1:\n                ax.set_yticks([])\n\n            if i == 0:\n                ax.xaxis.tick_top()\n            if i == m-1:\n                ax.xaxis.tick_bottom()\n\n            if j == 0:\n                ax.yaxis.tick_left()\n            if j == m-1:\n                ax.yaxis.tick_right()\n\n    _plot_labels(fig,\n                 {'x': 0.5, 'y': 0.025, 's': 'time (s)', 'horizontalalignment': 'center'},\n                 {'x': 0.05, 'y': 0.5, 's': 'frequency (Hz)', 'horizontalalignment': 'center', 'rotation': 'vertical'})\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_whiteness(var, h, repeats=1000, axis=None):\n    pr, q0, q = var.test_whiteness(h, repeats, True)\n\n    if axis is None:\n        axis = current_axis()\n\n    pdf, _, _ = axis.hist(q0, 30, normed=True, label='surrogate distribution')\n    axis.plot([q,q], [0,np.max(pdf)], 'r-', label='fitted model')\n\n    #df = m*m*(h-p)\n    #x = np.linspace(np.min(q0)*0.0, np.max(q0)*2.0, 100)\n    #y = sp.stats.chi2.pdf(x, df)\n    #hc = axis.plot(x, y, label='chi-squared distribution (df=%i)' % df)\n\n    axis.set_title('significance: p = %f'%pr)\n    axis.set_xlabel('Li-McLeod statistic (Q)')\n    axis.set_ylabel('probability')\n\n    axis.legend()\n\n    return pr", "response": "Draw the distribution of the Portmanteu whiteness test."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef singletrial(num_trials, skipstep=1):\n    for t in range(0, num_trials, skipstep):\n        trainset = [t]\n        testset = [i for i in range(trainset[0])] + \\\n                  [i for i in range(trainset[-1] + 1, num_trials)]\n        testset = sort([t % num_trials for t in testset])\n        yield trainset, testset", "response": "Single - trial cross - validation schema\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef splitset(num_trials, skipstep=None):\n    split = num_trials // 2\n\n    a = list(range(0, split))\n    b = list(range(split, num_trials))\n    yield a, b\n    yield b, a", "response": "Split - set cross validation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_data(self, data, cl=None, time_offset=0):\n        self.data_ = atleast_3d(data)\n        self.cl_ = np.asarray(cl if cl is not None else [None]*self.data_.shape[0])\n        self.time_offset_ = time_offset\n        self.var_model_ = None\n        self.var_cov_ = None\n        self.connectivity_ = None\n\n        self.trial_mask_ = np.ones(self.cl_.size, dtype=bool)\n\n        if self.unmixing_ is not None:\n            self.activations_ = dot_special(self.unmixing_.T, self.data_)\n\n        return self", "response": "Assign data to the workspace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_used_labels(self, labels):\n        mask = np.zeros(self.cl_.size, dtype=bool)\n        for l in labels:\n            mask = np.logical_or(mask, self.cl_ == l)\n        self.trial_mask_ = mask\n        return self", "response": "Sets the trials used by subsequent analysis steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms MVARICA Perform MVARICA source decomposition and VAR model fitting. Parameters ---------- varfit : string Determines how to calculate the residuals for source decomposition. 'ensemble' (default) fits one model to the whole data set, 'class' fits a different model for each class, and 'trial' fits a different model for each individual trial. Returns ------- self : Workspace The Workspace object. Raises ------ RuntimeError If the :class:`Workspace` instance does not contain data. See Also -------- :func:`mvarica` : MVARICA implementation", "response": "def do_mvarica(self, varfit='ensemble', random_state=None):\n        \"\"\" Perform MVARICA\n\n        Perform MVARICA source decomposition and VAR model fitting.\n\n        Parameters\n        ----------\n        varfit : string\n            Determines how to calculate the residuals for source decomposition.\n            'ensemble' (default) fits one model to the whole data set,\n            'class' fits a different model for each class, and\n            'trial' fits a different model for each individual trial.\n\n        Returns\n        -------\n        self : Workspace\n            The Workspace object.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain data.\n\n        See Also\n        --------\n        :func:`mvarica` : MVARICA implementation\n        \"\"\"\n        if self.data_ is None:\n            raise RuntimeError(\"MVARICA requires data to be set\")\n        result = mvarica(x=self.data_[self.trial_mask_, :, :],\n                         cl=self.cl_[self.trial_mask_], var=self.var_,\n                         reducedim=self.reducedim_, backend=self.backend_,\n                         varfit=varfit, random_state=random_state)\n        self.mixing_ = result.mixing\n        self.unmixing_ = result.unmixing\n        self.var_ = result.b\n        self.connectivity_ = Connectivity(result.b.coef, result.b.rescov,\n                                          self.nfft_)\n        self.activations_ = dot_special(self.unmixing_.T, self.data_)\n        self.mixmaps_ = []\n        self.unmixmaps_ = []\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform CSPVARICA Perform CSPVARICA source decomposition and VAR model fitting. Parameters ---------- varfit : string Determines how to calculate the residuals for source decomposition. 'ensemble' (default) fits one model to the whole data set, 'class' fits a different model for each class, and 'trial' fits a different model for each individual trial. Returns ------- self : Workspace The Workspace object. Raises ------ RuntimeError If the :class:`Workspace` instance does not contain data. See Also -------- :func:`cspvarica` : CSPVARICA implementation", "response": "def do_cspvarica(self, varfit='ensemble', random_state=None):\n        \"\"\" Perform CSPVARICA\n\n        Perform CSPVARICA source decomposition and VAR model fitting.\n\n        Parameters\n        ----------\n        varfit : string\n            Determines how to calculate the residuals for source decomposition.\n            'ensemble' (default) fits one model to the whole data set,\n            'class' fits a different model for each class, and\n            'trial' fits a different model for each individual trial.\n\n        Returns\n        -------\n        self : Workspace\n            The Workspace object.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain data.\n\n        See Also\n        --------\n        :func:`cspvarica` : CSPVARICA implementation\n        \"\"\"\n        if self.data_ is None:\n            raise RuntimeError(\"CSPVARICA requires data to be set\")\n        try:\n            sorted(self.cl_)\n            for c in self.cl_:\n                assert(c is not None)\n        except (TypeError, AssertionError):\n            raise RuntimeError(\"CSPVARICA requires orderable and hashable class labels that are not None\")\n        result = cspvarica(x=self.data_, var=self.var_, cl=self.cl_,\n                           reducedim=self.reducedim_, backend=self.backend_,\n                           varfit=varfit, random_state=random_state)\n        self.mixing_ = result.mixing\n        self.unmixing_ = result.unmixing\n        self.var_ = result.b\n        self.connectivity_ = Connectivity(self.var_.coef, self.var_.rescov, self.nfft_)\n        self.activations_ = dot_special(self.unmixing_.T, self.data_)\n        self.mixmaps_ = []\n        self.unmixmaps_ = []\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming ICA source decomposition.", "response": "def do_ica(self, random_state=None):\n        \"\"\" Perform ICA\n\n        Perform plain ICA source decomposition.\n\n        Returns\n        -------\n        self : Workspace\n            The Workspace object.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain data.\n        \"\"\"\n        if self.data_ is None:\n            raise RuntimeError(\"ICA requires data to be set\")\n        result = plainica(x=self.data_[self.trial_mask_, :, :], reducedim=self.reducedim_, backend=self.backend_, random_state=random_state)\n        self.mixing_ = result.mixing\n        self.unmixing_ = result.unmixing\n        self.activations_ = dot_special(self.unmixing_.T, self.data_)\n        self.var_model_ = None\n        self.var_cov_ = None\n        self.connectivity_ = None\n        self.mixmaps_ = []\n        self.unmixmaps_ = []\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_sources(self, sources):\n        if self.unmixing_ is None or self.mixing_ is None:\n            raise RuntimeError(\"No sources available (run do_mvarica first)\")\n        self.mixing_ = np.delete(self.mixing_, sources, 0)\n        self.unmixing_ = np.delete(self.unmixing_, sources, 1)\n        if self.activations_ is not None:\n            self.activations_ = np.delete(self.activations_, sources, 1)\n        self.var_model_ = None\n        self.var_cov_ = None\n        self.connectivity_ = None\n        self.mixmaps_ = []\n        self.unmixmaps_ = []\n        return self", "response": "Removes sources from the workspace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkeeping only the specified sources in the decomposition.", "response": "def keep_sources(self, keep):\n        \"\"\"Keep only the specified sources in the decomposition.\n        \"\"\"\n        if self.unmixing_ is None or self.mixing_ is None:\n            raise RuntimeError(\"No sources available (run do_mvarica first)\")\n        n_sources = self.mixing_.shape[0]\n        self.remove_sources(np.setdiff1d(np.arange(n_sources), np.array(keep)))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits a VAR model to the source activations.", "response": "def fit_var(self):\n        \"\"\" Fit a VAR model to the source activations.\n\n        Returns\n        -------\n        self : Workspace\n            The Workspace object.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain source activations.\n        \"\"\"\n        if self.activations_ is None:\n            raise RuntimeError(\"VAR fitting requires source activations (run do_mvarica first)\")\n        self.var_.fit(data=self.activations_[self.trial_mask_, :, :])\n        self.connectivity_ = Connectivity(self.var_.coef, self.var_.rescov, self.nfft_)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noptimize the VAR model s hyperparameters.", "response": "def optimize_var(self):\n        \"\"\" Optimize the VAR model's hyperparameters (such as regularization).\n\n        Returns\n        -------\n        self : Workspace\n            The Workspace object.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain source activations.\n        \"\"\"\n        if self.activations_ is None:\n            raise RuntimeError(\"VAR fitting requires source activations (run do_mvarica first)\")\n\n        self.var_.optimize(self.activations_[self.trial_mask_, :, :])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the spectral connectivity measure of a specific variable.", "response": "def get_connectivity(self, measure_name, plot=False):\n        \"\"\" Calculate spectral connectivity measure.\n\n        Parameters\n        ----------\n        measure_name : str\n            Name of the connectivity measure to calculate. See :class:`Connectivity` for supported measures.\n        plot : {False, None, Figure object}, optional\n            Whether and where to plot the connectivity. If set to **False**, nothing is plotted. Otherwise set to the\n            Figure object. If set to **None**, a new figure is created.\n\n        Returns\n        -------\n        measure : array, shape = [n_channels, n_channels, nfft]\n            Values of the connectivity measure.\n        fig : Figure object\n            Instance of the figure in which was plotted. This is only returned if `plot` is not **False**.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain a fitted VAR model.\n        \"\"\"\n        if self.connectivity_ is None:\n            raise RuntimeError(\"Connectivity requires a VAR model (run do_mvarica or fit_var first)\")\n\n        cm = getattr(self.connectivity_, measure_name)()\n\n        cm = np.abs(cm) if np.any(np.iscomplex(cm)) else cm\n\n        if plot is None or plot:\n            fig = plot\n            if self.plot_diagonal == 'fill':\n                diagonal = 0\n            elif self.plot_diagonal == 'S':\n                diagonal = -1\n                sm = np.abs(self.connectivity_.S())\n                sm /= np.max(sm)     # scale to 1 since components are scaled arbitrarily anyway\n                fig = self.plotting.plot_connectivity_spectrum(sm, fs=self.fs_, freq_range=self.plot_f_range,\n                                                          diagonal=1, border=self.plot_outside_topo, fig=fig)\n            else:\n                diagonal = -1\n\n            fig = self.plotting.plot_connectivity_spectrum(cm, fs=self.fs_, freq_range=self.plot_f_range,\n                                                      diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n\n            return cm, fig\n\n        return cm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_surrogate_connectivity(self, measure_name, repeats=100, plot=False, random_state=None):\n        cs = surrogate_connectivity(measure_name, self.activations_[self.trial_mask_, :, :],\n                                    self.var_, self.nfft_, repeats, random_state=random_state)\n\n        if plot is None or plot:\n            fig = plot\n            if self.plot_diagonal == 'fill':\n                diagonal = 0\n            elif self.plot_diagonal == 'S':\n                diagonal = -1\n                sb = self.get_surrogate_connectivity('absS', repeats)\n                sb /= np.max(sb)     # scale to 1 since components are scaled arbitrarily anyway\n                su = np.percentile(sb, 95, axis=0)\n                fig = self.plotting.plot_connectivity_spectrum([su], fs=self.fs_, freq_range=self.plot_f_range,\n                                                          diagonal=1, border=self.plot_outside_topo, fig=fig)\n            else:\n                diagonal = -1\n            cu = np.percentile(cs, 95, axis=0)\n            fig = self.plotting.plot_connectivity_spectrum([cu], fs=self.fs_, freq_range=self.plot_f_range,\n                                                      diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n            return cs, fig\n\n        return cs", "response": "Calculates the spectral connectivity measure under the assumption of no actual connectivity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bootstrap_connectivity(self, measure_names, repeats=100, num_samples=None, plot=False, random_state=None):\n        if num_samples is None:\n            num_samples = np.sum(self.trial_mask_)\n\n        cb = bootstrap_connectivity(measure_names, self.activations_[self.trial_mask_, :, :],\n                                    self.var_, self.nfft_, repeats, num_samples, random_state=random_state)\n\n        if plot is None or plot:\n            fig = plot\n            if self.plot_diagonal == 'fill':\n                diagonal = 0\n            elif self.plot_diagonal == 'S':\n                diagonal = -1\n                sb = self.get_bootstrap_connectivity('absS', repeats, num_samples)\n                sb /= np.max(sb)     # scale to 1 since components are scaled arbitrarily anyway\n                sm = np.median(sb, axis=0)\n                sl = np.percentile(sb, 2.5, axis=0)\n                su = np.percentile(sb, 97.5, axis=0)\n                fig = self.plotting.plot_connectivity_spectrum([sm, sl, su], fs=self.fs_, freq_range=self.plot_f_range,\n                                                          diagonal=1, border=self.plot_outside_topo, fig=fig)\n            else:\n                diagonal = -1\n            cm = np.median(cb, axis=0)\n            cl = np.percentile(cb, 2.5, axis=0)\n            cu = np.percentile(cb, 97.5, axis=0)\n            fig = self.plotting.plot_connectivity_spectrum([cm, cl, cu], fs=self.fs_, freq_range=self.plot_f_range,\n                                                      diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n            return cb, fig\n\n        return cb", "response": "Calculates the bootstrap estimates of spectral connectivity measures."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the time - varying connectivity of a given time - frequency measure.", "response": "def get_tf_connectivity(self, measure_name, winlen, winstep, plot=False, baseline=None, crange='default'):\n        \"\"\" Calculate estimate of time-varying connectivity.\n\n        Connectivity is estimated in a sliding window approach on the current data set. The window is stepped\n        `n_steps` = (`n_samples` - `winlen`) // `winstep` times.\n\n        Parameters\n        ----------\n        measure_name : str\n            Name of the connectivity measure to calculate. See :class:`Connectivity` for supported measures.\n        winlen : int\n            Length of the sliding window (in samples).\n        winstep : int\n            Step size for sliding window (in samples).\n        plot : {False, None, Figure object}, optional\n            Whether and where to plot the connectivity. If set to **False**, nothing is plotted. Otherwise set to the\n            Figure object. If set to **None**, a new figure is created.\n        baseline : [int, int] or None\n            Start and end of the baseline period in samples. The baseline is subtracted from the connectivity. It is\n            computed as the average of all windows that contain start or end, or fall between start and end.\n            If set to None no baseline is subtracted.\n\n        Returns\n        -------\n        result : array, shape = [n_channels, n_channels, nfft, n_steps]\n            Values of the connectivity measure.\n        fig : Figure object, optional\n            Instance of the figure in which was plotted. This is only returned if `plot` is not **False**.\n\n        Raises\n        ------\n        RuntimeError\n            If the :class:`Workspace` instance does not contain a fitted VAR model.\n        \"\"\"\n        if self.activations_ is None:\n            raise RuntimeError(\"Time/Frequency Connectivity requires activations (call set_data after do_mvarica)\")\n        _, m, n = self.activations_.shape\n\n        steps = list(range(0, n - winlen, winstep))\n        nstep = len(steps)\n\n        result = np.zeros((m, m, self.nfft_, nstep), np.complex64)\n        for i, j in enumerate(steps):\n            win = np.arange(winlen) + j\n            data = self.activations_[:, :, win]\n            data = data[self.trial_mask_, :, :]\n            self.var_.fit(data)\n            con = Connectivity(self.var_.coef, self.var_.rescov, self.nfft_)\n            result[:, :, :, i] = getattr(con, measure_name)()\n            \n        if baseline:\n            inref = np.zeros(nstep, bool)\n            for i, j in enumerate(steps):\n                a, b = j, j + winlen - 1\n                inref[i] = b >= baseline[0] and a <= baseline[1]\n            if np.any(inref):\n                ref = np.mean(result[:, :, :, inref], axis=3, keepdims=True)\n                result -= ref\n\n        if plot is None or plot:\n            fig = plot\n            t0 = 0.5 * winlen / self.fs_ + self.time_offset_\n            t1 = self.data_.shape[2] / self.fs_ - 0.5 * winlen / self.fs_ + self.time_offset_\n            if self.plot_diagonal == 'fill':\n                diagonal = 0\n            elif self.plot_diagonal == 'S':\n                diagonal = -1\n                s = np.abs(self.get_tf_connectivity('S', winlen, winstep))\n                if crange == 'default':\n                    crange = [np.min(s), np.max(s)]\n                fig = self.plotting.plot_connectivity_timespectrum(s, fs=self.fs_, crange=[np.min(s), np.max(s)],\n                                                              freq_range=self.plot_f_range, time_range=[t0, t1],\n                                                              diagonal=1, border=self.plot_outside_topo, fig=fig)\n            else:\n                diagonal = -1\n\n            tfc = self._clean_measure(measure_name, result)\n            if crange == 'default':\n                if diagonal == -1:\n                    for m in range(tfc.shape[0]):\n                        tfc[m, m, :, :] = 0\n                crange = [np.min(tfc), np.max(tfc)]\n            fig = self.plotting.plot_connectivity_timespectrum(tfc, fs=self.fs_, crange=crange,\n                                                          freq_range=self.plot_f_range, time_range=[t0, t1],\n                                                          diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n\n            return result, fig\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_conditions(self, labels1, labels2, measure_name, alpha=0.01, repeats=100, num_samples=None, plot=False, random_state=None):\n        self.set_used_labels(labels1)\n        ca = self.get_bootstrap_connectivity(measure_name, repeats, num_samples, random_state=random_state)\n        self.set_used_labels(labels2)\n        cb = self.get_bootstrap_connectivity(measure_name, repeats, num_samples, random_state=random_state)\n\n        p = test_bootstrap_difference(ca, cb)\n        s = significance_fdr(p, alpha)\n\n        if plot is None or plot:\n            fig = plot\n            if self.plot_diagonal == 'topo':\n                diagonal = -1\n            elif self.plot_diagonal == 'fill':\n                diagonal = 0\n            elif self.plot_diagonal is 'S':\n                diagonal = -1\n                self.set_used_labels(labels1)\n                sa = self.get_bootstrap_connectivity('absS', repeats, num_samples)\n                sm = np.median(sa, axis=0)\n                sl = np.percentile(sa, 2.5, axis=0)\n                su = np.percentile(sa, 97.5, axis=0)\n                fig = self.plotting.plot_connectivity_spectrum([sm, sl, su], fs=self.fs_, freq_range=self.plot_f_range,\n                                                          diagonal=1, border=self.plot_outside_topo, fig=fig)\n\n                self.set_used_labels(labels2)\n                sb = self.get_bootstrap_connectivity('absS', repeats, num_samples)\n                sm = np.median(sb, axis=0)\n                sl = np.percentile(sb, 2.5, axis=0)\n                su = np.percentile(sb, 97.5, axis=0)\n                fig = self.plotting.plot_connectivity_spectrum([sm, sl, su], fs=self.fs_, freq_range=self.plot_f_range,\n                                                          diagonal=1, border=self.plot_outside_topo, fig=fig)\n\n                p_s = test_bootstrap_difference(ca, cb)\n                s_s = significance_fdr(p_s, alpha)\n\n                self.plotting.plot_connectivity_significance(s_s, fs=self.fs_, freq_range=self.plot_f_range,\n                                                        diagonal=1, border=self.plot_outside_topo, fig=fig)\n            else:\n                diagonal = -1\n\n            cm = np.median(ca, axis=0)\n            cl = np.percentile(ca, 2.5, axis=0)\n            cu = np.percentile(ca, 97.5, axis=0)\n\n            fig = self.plotting.plot_connectivity_spectrum([cm, cl, cu], fs=self.fs_, freq_range=self.plot_f_range,\n                                                      diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n\n            cm = np.median(cb, axis=0)\n            cl = np.percentile(cb, 2.5, axis=0)\n            cu = np.percentile(cb, 97.5, axis=0)\n\n            fig = self.plotting.plot_connectivity_spectrum([cm, cl, cu], fs=self.fs_, freq_range=self.plot_f_range,\n                                                      diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n\n            self.plotting.plot_connectivity_significance(s, fs=self.fs_, freq_range=self.plot_f_range,\n                                                    diagonal=diagonal, border=self.plot_outside_topo, fig=fig)\n\n            return p, s, fig\n\n        return p, s", "response": "Test for significant difference in connectivity of two sets of class labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the source topography of the Source decomposition.", "response": "def plot_source_topos(self, common_scale=None):\n        \"\"\" Plot topography of the Source decomposition.\n\n        Parameters\n        ----------\n        common_scale : float, optional\n            If set to None, each topoplot's color axis is scaled individually. Otherwise specifies the percentile\n            (1-99) of values in all plot. This value is taken as the maximum color scale.\n        \"\"\"\n        if self.unmixing_ is None and self.mixing_ is None:\n            raise RuntimeError(\"No sources available (run do_mvarica first)\")\n\n        self._prepare_plots(True, True)\n\n        self.plotting.plot_sources(self.topo_, self.mixmaps_, self.unmixmaps_, common_scale)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot scalp projections of the sources.", "response": "def plot_connectivity_topos(self, fig=None):\n        \"\"\" Plot scalp projections of the sources.\n\n        This function only plots the topos. Use in combination with connectivity plotting.\n\n        Parameters\n        ----------\n        fig : {None, Figure object}, optional\n            Where to plot the topos. f set to **None**, a new figure is created. Otherwise plot into the provided\n            figure object.\n\n        Returns\n        -------\n        fig : Figure object\n            Instance of the figure in which was plotted.\n        \"\"\"\n        self._prepare_plots(True, False)\n        if self.plot_outside_topo:\n            fig = self.plotting.plot_connectivity_topos('outside', self.topo_, self.mixmaps_, fig)\n        elif self.plot_diagonal == 'topo':\n            fig = self.plotting.plot_connectivity_topos('diagonal', self.topo_, self.mixmaps_, fig)\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting spectral connectivity measure under the assumption of no actual connectivity.", "response": "def plot_connectivity_surrogate(self, measure_name, repeats=100, fig=None):\n        \"\"\" Plot spectral connectivity measure under the assumption of no actual connectivity.\n\n        Repeatedly samples connectivity from phase-randomized data. This provides estimates of the connectivity\n        distribution if there was no causal structure in the data.\n\n        Parameters\n        ----------\n        measure_name : str\n            Name of the connectivity measure to calculate. See :class:`Connectivity` for supported measures.\n        repeats : int, optional\n            How many surrogate samples to take.\n        fig : {None, Figure object}, optional\n            Where to plot the topos. f set to **None**, a new figure is created. Otherwise plot into the provided\n            figure object.\n\n        Returns\n        -------\n        fig : Figure object\n            Instance of the figure in which was plotted.\n        \"\"\"\n        cb = self.get_surrogate_connectivity(measure_name, repeats)\n\n        self._prepare_plots(True, False)\n\n        cu = np.percentile(cb, 95, axis=0)\n\n        fig = self.plotting.plot_connectivity_spectrum([cu], self.fs_, freq_range=self.plot_f_range, fig=fig)\n\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parallel_loop(func, n_jobs=1, verbose=1):\n    if n_jobs:\n        try:\n            from joblib import Parallel, delayed\n        except ImportError:\n            try:\n                from sklearn.externals.joblib import Parallel, delayed\n            except ImportError:\n                n_jobs = None\n\n    if not n_jobs:\n        if verbose:\n            print('running ', func, ' serially')\n        par = lambda x: list(x)\n    else:\n        if verbose:\n            print('running ', func, ' in parallel')\n        func = delayed(func)\n        par = Parallel(n_jobs=n_jobs, verbose=verbose)\n\n    return par, func", "response": "run loops in parallel if joblib is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_compiled(self, name=None, key_schema=None, value_schema=None, as_mapping_key=None):\n        if self.name is None:\n            self.name = name\n        if self.key_schema is None:\n            self.key_schema = key_schema\n        if self.value_schema is None:\n            self.value_schema = value_schema\n        if as_mapping_key:\n            self.as_mapping_key = True\n        return self", "response": "When CompiledSchema compiles this marker it sets informational values onto it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef colorlogs(format=\"short\"):\n    try:\n        from rainbow_logging_handler import RainbowLoggingHandler\n        import sys\n        # setup `RainbowLoggingHandler`\n        logger = logging.root\n        # same as default\n        if format == \"short\":\n            fmt = \"%(message)s \"\n        else:\n            fmt = \"[%(asctime)s] %(name)s %(funcName)s():%(lineno)d\\t%(message)s [%(levelname)s]\"\n        formatter = logging.Formatter(fmt)\n        handler = RainbowLoggingHandler(sys.stderr,\n                                        color_funcName=('black', 'gray', True))\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    except ImportError:\n        # rainbow logger not found, that's ok\n        pass", "response": "Append a rainbow logging handler and a formatter to the root logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    arguments = docopt.docopt(__doc__, version=__version__)\n    colorlogs()\n    # Read input file file\n\n\n    wrapper = BMIWrapper(\n        engine=arguments['<engine>'],\n        configfile=arguments['<config>'] or ''\n    )\n\n\n    # add logger if required\n    if not arguments['--disable-logger']:\n        logging.root.setLevel(logging.DEBUG)\n        wrapper.set_logger(logging.root)\n\n    with wrapper as model:\n        # if siginfo is supported by OS (BSD)\n        def handler(signum, frame):\n            \"\"\"report progress information\"\"\"\n            t_start = model.get_start_time()\n            t_end = model.get_end_time()\n            t_current = model.get_current_time()\n            total = (t_end - t_start)\n            now = (t_current - t_start)\n            if total > 0:\n                logging.info(\"progress: %s%%\", 100.0 * now / total)\n            else:\n                logging.info(\"progress: unknown\")\n\n        if hasattr(signal, 'SIGINFO'):\n            # attach a siginfo handler (CTRL-t) to print progress\n            signal.signal(signal.SIGINFO, handler)\n\n        if arguments['--info']:\n            logging.info(\"%s\", trace(model))\n        t_end = model.get_end_time()\n        t = model.get_start_time()\n        while t < t_end:\n            model.update(-1)\n            t = model.get_current_time()\n        if arguments['--info']:\n            logging.info(\"%s\", trace(model))", "response": "main function of the bmi runner"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_def_conf():\n    '''return default configurations as simple dict'''\n    ret = dict()\n    for k,v in defConf.items():\n        ret[k] = v[0]\n    return ret", "response": "return default configurations as simple dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadvancing game by single move if possible.", "response": "def move(self):\n        \"\"\"\n        Advance game by single move, if possible.\n\n        @return: logical indicator if move was performed.\n        \"\"\"\n        if len(self.moves) == MAX_MOVES:\n            return False\n        elif len(self.moves) % 2:\n            active_engine = self.black_engine\n            active_engine_name = self.black\n            inactive_engine = self.white_engine\n            inactive_engine_name = self.white\n        else:\n            active_engine = self.white_engine\n            active_engine_name = self.white\n            inactive_engine = self.black_engine\n            inactive_engine_name = self.black\n        active_engine.setposition(self.moves)\n        movedict = active_engine.bestmove()\n        bestmove = movedict.get('move')\n        info = movedict.get('info')\n        ponder = movedict.get('ponder')\n        self.moves.append(bestmove)\n\n        if info[\"score\"][\"eval\"] == \"mate\":\n            matenum = info[\"score\"][\"value\"]\n            if matenum > 0:\n                self.winner_engine = active_engine\n                self.winner = active_engine_name\n            elif matenum < 0:\n                self.winner_engine = inactive_engine\n                self.winner = inactive_engine_name\n            return False\n\n        if ponder != '(none)':\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setposition(self, moves=[]):\n        self.put('position startpos moves %s' % Engine._movelisttostr(moves))\n        self.isready()", "response": "Set the position of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bestmove(self):\n        self.go()\n        last_info = \"\"\n        while True:\n            text = self.stdout.readline().strip()\n            split_text = text.split(' ')\n            print(text)\n            if split_text[0] == \"info\":\n                last_info = Engine._bestmove_get_info(text)\n            if split_text[0] == \"bestmove\":\n                ponder = None if len(split_text[0]) < 3 else split_text[3]\n                return {'move': split_text[1],\n                        'ponder': ponder,\n                        'info': last_info}", "response": "Get proposed best move for current position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse stockfish evaluation output as dictionary.", "response": "def _bestmove_get_info(text):\n        \"\"\"\n        Parse stockfish evaluation output as dictionary.\n\n        Examples of input:\n\n        \"info depth 2 seldepth 3 multipv 1 score cp -656 nodes 43 nps 43000 tbhits 0 \\\n        time 1 pv g7g6 h3g3 g6f7\"\n\n        \"info depth 10 seldepth 12 multipv 1 score mate 5 nodes 2378 nps 1189000 tbhits 0 \\\n        time 2 pv h3g3 g6f7 g3c7 b5d7 d1d7 f7g6 c7g3 g6h5 e6f4\"\n        \"\"\"\n        result_dict = Engine._get_info_pv(text)\n        result_dict.update(Engine._get_info_score(text))\n\n        single_value_fields = ['depth', 'seldepth', 'multipv', 'nodes', 'nps', 'tbhits', 'time']\n        for field in single_value_fields:\n            result_dict.update(Engine._get_info_singlevalue_subfield(text, field))\n\n        return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the PV field from bestmove s info and returns it in UCI notation.", "response": "def _get_info_pv(info):\n        \"\"\"\n        Helper function for _bestmove_get_info.\n\n        Extracts \"pv\" field from bestmove's info and returns move sequence in UCI notation.\n        \"\"\"\n        search = re.search(pattern=PV_REGEX, string=info)\n        return {\"pv\": search.group(\"move_list\")}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isready(self):\n        self.put('isready')\n        while True:\n            text = self.stdout.readline().strip()\n            if text == 'readyok':\n                return text", "response": "Returns the current status of the current application."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing metrics in the overview section for enriched github issues indexes.", "response": "def overview(index, start, end):\n    \"\"\"Compute metrics in the overview section for enriched github issues\n    indexes.\n    Returns a dictionary. Each key in the dictionary is the name of\n    a metric, the value is the value of that metric. Value can be\n    a complex object (eg, a time series).\n\n    :param index: index object\n    :param start: date to apply the filters from\n    :param end: date to apply the filters upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"activity_metrics\": [SubmittedPRs(index, start, end),\n                             ClosedPRs(index, start, end)],\n        \"author_metrics\": [],\n        \"bmi_metrics\": [BMIPR(index, start, end)],\n        \"time_to_close_metrics\": [DaysToClosePRMedian(index, start, end)],\n        \"projects_metrics\": []\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project_activity(index, start, end):\n\n    results = {\n        \"metrics\": [SubmittedPRs(index, start, end),\n                    ClosedPRs(index, start, end)]\n    }\n\n    return results", "response": "Compute the metrics for the project activity section of the enriched\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the metrics for the project process section of the enriched github issues index.", "response": "def project_process(index, start, end):\n    \"\"\"Compute the metrics for the project process section of the enriched\n    github issues index.\n\n    Returns a dictionary containing \"bmi_metrics\", \"time_to_close_metrics\",\n    \"time_to_close_review_metrics\" and patchsets_metrics as the keys and\n    the related Metrics as the values.\n    time_to_close_title and time_to_close_review_title contain the file names\n    to be used for time_to_close_metrics and time_to_close_review_metrics\n    metrics data.\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"bmi_metrics\": [BMIPR(index, start, end)],\n        \"time_to_close_metrics\": [],\n        \"time_to_close_review_metrics\": [DaysToClosePRAverage(index, start, end),\n                                         DaysToClosePRMedian(index, start, end)],\n        \"patchsets_metrics\": []\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef aggregations(self):\n\n        prev_month_start = get_prev_month(self.end, self.query.interval_)\n        self.query.since(prev_month_start)\n        agg = super().aggregations()\n        if agg is None:\n            agg = 0  # None is because NaN in ES. Let's convert to 0\n        return agg", "response": "Get the single valued aggregations with respect to the\n        previous time interval."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeseries(self, dataframe=False):\n\n        self.query.by_period()\n        ts = super().timeseries(dataframe=dataframe)\n        ts['value'] = ts['value'].apply(lambda x: float(\"%.2f\" % x))\n        return ts", "response": "Get the date histogram aggregations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timeseries(self, dataframe=False):\n\n        closed_timeseries = self.closed.timeseries(dataframe=dataframe)\n        opened_timeseries = self.opened.timeseries(dataframe=dataframe)\n        return calculate_bmi(closed_timeseries, opened_timeseries)", "response": "Get BMIPR as a time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_section_metrics(cls):\n\n        # Those metrics are only for Pull Requests\n        # github issues is covered as ITS\n        return {\n            \"overview\": {\n                \"activity_metrics\": [ClosedPR, SubmittedPR],\n                \"author_metrics\": [],\n                \"bmi_metrics\": [BMIPR],\n                \"time_to_close_metrics\": [DaysToClosePRMedian],\n                \"projects_metrics\": [Projects]\n            },\n            \"com_channels\": {\n                \"activity_metrics\": [],\n                \"author_metrics\": []\n            },\n            \"project_activity\": {\n                \"metrics\": [SubmittedPR, ClosedPR]\n            },\n            \"project_community\": {\n                \"author_metrics\": [],\n                \"people_top_metrics\": [],\n                \"orgs_top_metrics\": [],\n            },\n            \"project_process\": {\n                \"bmi_metrics\": [BMIPR],\n                \"time_to_close_metrics\": [],\n                \"time_to_close_title\": \"Days to close (median and average)\",\n                \"time_to_close_review_metrics\": [DaysToClosePRAverage, DaysToClosePRMedian],\n                \"time_to_close_review_title\": \"Days to close review (median and average)\",\n                \"patchsets_metrics\": []\n            }\n        }", "response": "Get the mapping between metrics and sections in Manuscripts report\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_metrics(self):\n        esfilters_close = None\n        esfilters_submit = None\n        if self.esfilters:\n            esfilters_close = self.esfilters.copy()\n            esfilters_submit = self.esfilters.copy()\n\n        closed = ClosedPR(self.es_url, self.es_index,\n                          start=self.start, end=self.end,\n                          esfilters=esfilters_close, interval=self.interval)\n        # For BMI we need when the ticket was closed\n        closed.FIELD_DATE = 'closed_at'\n        submitted = SubmittedPR(self.es_url, self.es_index,\n                                start=self.start, end=self.end,\n                                esfilters=esfilters_submit,\n                                interval=self.interval)\n\n        return (closed, submitted)", "response": "Get the closed and submitted metrics from the index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_definition(self):\n        def_ = {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"desc\": self.desc\n        }\n        return def_", "response": "Get the dict with the basic fields used to describe a metrics"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_query(self, evolutionary=False):\n\n        if not evolutionary:\n            interval = None\n            offset = None\n        else:\n            interval = self.interval\n            offset = self.offset\n            if not interval:\n                raise RuntimeError(\"Evolutionary query without an interval.\")\n\n        query = ElasticQuery.get_agg(field=self.FIELD_COUNT,\n                                     date_field=self.FIELD_DATE,\n                                     start=self.start, end=self.end,\n                                     filters=self.esfilters,\n                                     agg_type=self.AGG_TYPE,\n                                     interval=interval,\n                                     offset=offset)\n\n        logger.debug(\"Metric: '%s' (%s); Query: %s\",\n                     self.name, self.id, query)\n        return query", "response": "Basic query to get the metric values time series"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts from a DSL aggregated response the values for each bucket", "response": "def get_list(self):\n        \"\"\"\n        Extract from a DSL aggregated response the values for each bucket\n\n        :return: a list with the values in a DSL aggregated response\n        \"\"\"\n        field = self.FIELD_NAME\n        query = ElasticQuery.get_agg(field=field,\n                                     date_field=self.FIELD_DATE,\n                                     start=self.start, end=self.end,\n                                     filters=self.esfilters)\n        logger.debug(\"Metric: '%s' (%s); Query: %s\",\n                     self.name, self.id, query)\n        res = self.get_metrics_data(query)\n        list_ = {field: [], \"value\": []}\n        for bucket in res['aggregations'][str(ElasticQuery.AGGREGATION_ID)]['buckets']:\n            list_[field].append(bucket['key'])\n            list_['value'].append(bucket['doc_count'])\n        return list_"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the metrics data from Elasticsearch given a DSL query", "response": "def get_metrics_data(self, query):\n        \"\"\"\n        Get the metrics data from Elasticsearch given a DSL query\n\n        :param query: query to be sent to Elasticsearch\n        :return: a dict with the results of executing the query\n        \"\"\"\n        if self.es_url.startswith(\"http\"):\n            url = self.es_url\n        else:\n            url = 'http://' + self.es_url\n        es = Elasticsearch(url)\n        s = Search(using=es, index=self.es_index)\n        s = s.update_from_dict(query)\n        try:\n            response = s.execute()\n            return response.to_dict()\n        except Exception as e:\n            print()\n            print(\"In get_metrics_data: Failed to fetch data.\\n Query: {}, \\n Error Info: {}\"\n                  .format(query, e.info))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the metric", "response": "def get_agg(self):\n        \"\"\"\n        Returns the aggregated value for the metric\n\n        :return: the value of the metric\n        \"\"\"\n        \"\"\" Returns an aggregated value \"\"\"\n        query = self.get_query(False)\n        res = self.get_metrics_data(query)\n        # We need to extract the data from the JSON res\n        # If we have agg data use it\n        agg_id = str(ElasticQuery.AGGREGATION_ID)\n        if 'aggregations' in res and 'values' in res['aggregations'][agg_id]:\n            if self.AGG_TYPE == 'median':\n                agg = res['aggregations'][agg_id]['values'][\"50.0\"]\n                if agg == 'NaN':\n                    # ES returns NaN. Convert to None for matplotlib graph\n                    agg = None\n            else:\n                raise RuntimeError(\"Multivalue aggregation result not supported\")\n        elif 'aggregations' in res and 'value' in res['aggregations'][agg_id]:\n            agg = res['aggregations'][agg_id]['value']\n        else:\n            agg = res['hits']['total']\n\n        return agg"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the trend for the last two metric values using the interval defined in the metric clf.", "response": "def get_trend(self):\n        \"\"\"\n        Get the trend for the last two metric values using the interval defined in the metric\n\n        :return: a tuple with the metric value for the last interval and the\n                 trend percentage between the last two intervals\n        \"\"\"\n        \"\"\"  \"\"\"\n\n        # TODO: We just need the last two periods, not the full ts\n        ts = self.get_ts()\n        last = ts['value'][len(ts['value']) - 1]\n        prev = ts['value'][len(ts['value']) - 2]\n\n        trend = last - prev\n        trend_percentage = None\n        if last == 0:\n            if prev > 0:\n                trend_percentage = -100\n            else:\n                trend_percentage = 0\n        else:\n            trend_percentage = int((trend / last) * 100)\n\n        return (last, trend_percentage)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the mapping between metrics and sections in Manuscripts report", "response": "def get_section_metrics(cls):\n        \"\"\"\n        Get the mapping between metrics and sections in Manuscripts report\n        :return: a dict with the mapping between metrics and sections in Manuscripts report\n        \"\"\"\n\n        return {\n            \"overview\": {\n                \"activity_metrics\": [Closed, Opened],\n                \"author_metrics\": [],\n                \"bmi_metrics\": [BMI],\n                \"time_to_close_metrics\": [DaysToCloseMedian],\n                \"projects_metrics\": [Projects]\n            },\n            \"com_channels\": {\n                \"activity_metrics\": [],\n                \"author_metrics\": []\n            },\n            \"project_activity\": {\n                \"metrics\": [Opened, Closed]\n            },\n            \"project_community\": {\n                \"author_metrics\": [],\n                \"people_top_metrics\": [],\n                \"orgs_top_metrics\": [],\n            },\n            \"project_process\": {\n                \"bmi_metrics\": [BMI],\n                \"time_to_close_metrics\": [DaysToCloseAverage, DaysToCloseMedian],\n                \"time_to_close_title\": \"Days to close (median and average)\",\n                \"time_to_close_review_metrics\": [],\n                \"time_to_close_review_title\": \"\",\n                \"patchsets_metrics\": []\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_metrics(self):\n        esfilters_closed = None\n        esfilters_opened = None\n        if self.esfilters:\n            esfilters_closed = self.esfilters.copy()\n            esfilters_opened = self.esfilters.copy()\n\n        closed = self.closed_class(self.es_url, self.es_index,\n                                   start=self.start, end=self.end,\n                                   esfilters=esfilters_closed, interval=self.interval)\n        opened = self.opened_class(self.es_url, self.es_index,\n                                   start=self.start, end=self.end,\n                                   esfilters=esfilters_opened, interval=self.interval)\n        return (closed, opened)", "response": "Get the closed and opened metrics from the current instance of this class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_paths(self, paths, depth=0):\n        '''\n        Goes recursevly through the given list of paths\n        in order to find and pass all preset files to ```_load_preset()```\n        '''\n\n        if depth > self.MAX_DEPTH:\n            return\n\n        for path in paths:\n            try:\n                # avoid empty string\n                if not path:\n                    continue\n\n                # cleanUp path\n                if depth == 0:\n                    path = os.path.expanduser(path)  # replace ~\n                    path = os.path.expandvars(path)  # replace vars\n                    path = os.path.normpath(path)  # replace /../ , \"\" will be converted to \".\"\n\n                if not os.path.exists(path):\n                    raise PresetException(\"does not exists or is a broken link or not enough permissions to read\")\n                elif os.path.isdir(path):\n                    try:\n                        for child in os.listdir(path):\n                            self._load_paths([os.path.join(path, child)], depth + 1)\n                    except OSError as e:\n                        raise PresetException(\"IOError: \" + e.strerror)\n                elif os.path.isfile(path):\n                    if path.endswith(\".json\"):\n                        self._load_preset(path)\n                else:\n                    raise PresetException(\"not regular file\")\n            except PresetException as e:\n                e.message = \"Failed to load preset: \\\"{}\\\" [ {} ]\".format(path, e.message)\n                if self.strict:\n                    raise\n                logger.error(str(e))", "response": "Goes through the given list of paths and loads all preset files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads validate and store a single preset file", "response": "def _load_preset(self, path):\n        ''' load, validate and store a single preset file'''\n\n        try:\n            with open(path, 'r') as f:\n                presetBody = json.load(f)\n        except IOError as e:\n            raise PresetException(\"IOError: \" + e.strerror)\n        except ValueError as e:\n            raise PresetException(\"JSON decoding error: \" + str(e))\n        except Exception as e:\n            raise PresetException(str(e))\n\n        try:\n            preset = Preset(presetBody)\n        except PresetException as e:\n            e.message = \"Bad format: \" + e.message\n            raise\n\n        if(preset.id in self.presets):\n            raise PresetException(\"Duplicate preset id: \" + preset.id)\n        else:\n            self.presets[preset.id] = preset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the response format requested by client", "response": "def requestedFormat(request,acceptedFormat):\n        \"\"\"Return the response format requested by client\n\n        Client could specify requested format using:\n        (options are processed in this order)\n            - `format` field in http request\n            - `Accept` header in http request\n        Example:\n            chooseFormat(request, ['text/html','application/json'])\n        Args:\n            acceptedFormat: list containing all the accepted format\n        Returns:\n            string: the user requested mime-type (if supported)\n        Raises:\n            ValueError: if user request a mime-type not supported\n        \"\"\"\n        if 'format' in request.args:\n            fieldFormat = request.args.get('format')\n            if fieldFormat not in acceptedFormat:\n                raise ValueError(\"requested format not supported: \"+ fieldFormat)\n            return fieldFormat\n        else:\n            return request.accept_mimetypes.best_match(acceptedFormat)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbatch routes registering Register routes to a blueprint/flask_app previously collected with :func:`routes_collector`. :param fapp: bluprint or flask_app to whom attach new routes. :param routes: dict of routes collected by :func:`routes_collector` :param prefix: url prefix under which register all routes", "response": "def add_routes(fapp, routes, prefix=\"\"):\n    \"\"\"Batch routes registering\n\n    Register routes to a blueprint/flask_app previously collected\n    with :func:`routes_collector`.\n\n    :param fapp: bluprint or flask_app to whom attach new routes.\n    :param routes: dict of routes collected by :func:`routes_collector`\n    :param prefix: url prefix under which register all routes\n    \"\"\"\n    for r in routes:\n        r['rule'] = prefix + r['rule']\n        fapp.add_url_rule(**r)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the range of pages to render in a pagination menu.", "response": "def get_centered_pagination(current, total, visible=5):\n    ''' Return the range of pages to render in a pagination menu.\n\n        The current page is always kept in the middle except\n        for the edge cases.\n\n        Reeturns a dict\n            { prev, first, current, last, next }\n\n        :param current: the current page\n        :param total: total number of pages available\n        :param visible: number of pages visible\n    '''\n    inc = visible/2\n    first = current - inc\n    last = current + inc\n    if (total <= visible):\n        first = 1\n        last = total\n    elif (last > total):\n        first = total - (visible-1)\n        last = total\n    elif (first < 1):\n        first = 1\n        last = visible\n    return dict(prev = current-1 if(current > 1) else None,\n                first=first,\n                current = current,\n                last=last,\n                next = current+1 if(current < total) else None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_section_metrics(cls):\n\n        return {\n            \"overview\": {\n                \"activity_metrics\": [Closed, Submitted],\n                \"author_metrics\": None,\n                \"bmi_metrics\": [BMI],\n                \"time_to_close_metrics\": [DaysToMergeMedian],\n                \"projects_metrics\": [Projects],\n            },\n            \"com_channels\": {\n                \"activity_metrics\": [],\n                \"author_metrics\": []\n            },\n            \"project_activity\": {\n                \"metrics\": [Submitted, Closed]\n            },\n            \"project_community\": {\n                \"author_metrics\": [],\n                \"people_top_metrics\": [],\n                \"orgs_top_metrics\": [],\n            },\n            \"project_process\": {\n                \"bmi_metrics\": [BMI],\n                \"time_to_close_metrics\": [],\n                \"time_to_close_title\": \"\",\n                \"time_to_close_review_metrics\": [DaysToMergeAverage, DaysToMergeMedian],\n                \"time_to_close_review_title\": \"Days to close review (median and average)\",\n                \"patchsets_metrics\": [PatchsetsMedian, PatchsetsAverage],\n                \"patchsets_title\": \"Number of patchsets per review (median and average)\"\n            }\n        }", "response": "Get the mapping between metrics and sections in Manuscripts report\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __get_metrics(self):\n        esfilters_merge = None\n        esfilters_abandon = None\n        if self.esfilters:\n            esfilters_merge = self.esfilters.copy()\n            esfilters_abandon = self.esfilters.copy()\n\n        merged = Merged(self.es_url, self.es_index,\n                        start=self.start, end=self.end,\n                        esfilters=esfilters_merge, interval=self.interval)\n\n        abandoned = Abandoned(self.es_url, self.es_index,\n                              start=self.start, end=self.end,\n                              esfilters=esfilters_abandon, interval=self.interval)\n        return (merged, abandoned)", "response": "Get the merged and abandoned metrics from the index"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the metrics that are used to create the index.", "response": "def __get_metrics(self):\n        \"\"\" Each metric must have its own filters copy to modify it freely\"\"\"\n        esfilters_merge = None\n        esfilters_abandon = None\n        esfilters_submit = None\n        if self.esfilters:\n            esfilters_merge = self.esfilters.copy()\n            esfilters_abandon = self.esfilters.copy()\n            esfilters_submit = self.esfilters.copy()\n\n        merged = Merged(self.es_url, self.es_index,\n                        start=self.start, end=self.end,\n                        esfilters=esfilters_merge, interval=self.interval)\n        # For BMI we need when the ticket was closed\n        merged.FIELD_DATE = 'closed'\n\n        abandoned = Abandoned(self.es_url, self.es_index,\n                              start=self.start, end=self.end,\n                              esfilters=esfilters_abandon, interval=self.interval)\n        # For BMI we need when the ticket was closed\n        abandoned.FIELD_DATE = 'closed'\n\n        submitted = Submitted(self.es_url, self.es_index,\n                              start=self.start, end=self.end,\n                              esfilters=esfilters_submit,\n                              interval=self.interval)\n\n        return (merged, abandoned, submitted)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fwhm(x, y, k=10):  # http://stackoverflow.com/questions/10582795/finding-the-full-width-half-maximum-of-a-peak\n\n    class MultiplePeaks(Exception):\n        pass\n\n    class NoPeaksFound(Exception):\n        pass\n\n    half_max = np.amax(y) / 2.0\n    s = splrep(x, y - half_max)\n    roots = sproot(s)\n\n    if len(roots) > 2:\n        raise MultiplePeaks(\"The dataset appears to have multiple peaks, and \"\n                            \"thus the FWHM can't be determined.\")\n    elif len(roots) < 2:\n        raise NoPeaksFound(\"No proper peaks were found in the data set; likely \"\n                           \"the dataset is flat (e.g. all zeros).\")\n    else:\n        return roots[0], roots[1]", "response": "This function determines the full - with - half - maximum of a peaked set of points x and y."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild an argument parser using argparse.", "response": "def build_arg_parser():\n    \"\"\"\n    Build an argument parser using argparse. Use it when python version is 2.7 or later.\n\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Smatch calculator -- arguments\")\n    parser.add_argument('-f', nargs=2, required=True, type=argparse.FileType('r'),\n                        help='Two files containing AMR pairs. AMRs in each file are separated by a single blank line')\n    parser.add_argument('-r', type=int, default=4, help='Restart number (Default:4)')\n    parser.add_argument('--significant', type=int, default=2, help='significant digits to output (default: 2)')\n    parser.add_argument('-v', action='store_true', help='Verbose output (Default:false)')\n    parser.add_argument('--vv', action='store_true', help='Very Verbose output (Default:false)')\n    parser.add_argument('--ms', action='store_true', default=False,\n                        help='Output multiple scores (one AMR pair a score)'\n                             'instead of a single document-level smatch score (Default: false)')\n    parser.add_argument('--pr', action='store_true', default=False,\n                        help=\"Output precision and recall as well as the f-score. Default: false\")\n    parser.add_argument('--justinstance', action='store_true', default=False,\n                        help=\"just pay attention to matching instances\")\n    parser.add_argument('--justattribute', action='store_true', default=False,\n                        help=\"just pay attention to matching attributes\")\n    parser.add_argument('--justrelation', action='store_true', default=False,\n                        help=\"just pay attention to matching relations\")\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_arg_parser2():\n    usage_str = \"Smatch calculator -- arguments\"\n    parser = optparse.OptionParser(usage=usage_str)\n    parser.add_option(\"-f\", \"--files\", nargs=2, dest=\"f\", type=\"string\",\n                      help='Two files containing AMR pairs. AMRs in each file are ' \\\n                           'separated by a single blank line. This option is required.')\n    parser.add_option(\"-r\", \"--restart\", dest=\"r\", type=\"int\", help='Restart number (Default: 4)')\n    parser.add_option('--significant', dest=\"significant\", type=\"int\", default=2,\n                      help='significant digits to output (default: 2)')\n    parser.add_option(\"-v\", \"--verbose\", action='store_true', dest=\"v\", help='Verbose output (Default:False)')\n    parser.add_option(\"--vv\", \"--veryverbose\", action='store_true', dest=\"vv\",\n                      help='Very Verbose output (Default:False)')\n    parser.add_option(\"--ms\", \"--multiple_score\", action='store_true', dest=\"ms\",\n                      help='Output multiple scores (one AMR pair a score) instead of ' \\\n                           'a single document-level smatch score (Default: False)')\n    parser.add_option('--pr', \"--precision_recall\", action='store_true', dest=\"pr\",\n                      help=\"Output precision and recall as well as the f-score. Default: false\")\n    parser.add_option('--justinstance', action='store_true', default=False,\n                      help=\"just pay attention to matching instances\")\n    parser.add_option('--justattribute', action='store_true', default=False,\n                      help=\"just pay attention to matching attributes\")\n    parser.add_option('--justrelation', action='store_true', default=False,\n                      help=\"just pay attention to matching relations\")\n    parser.set_defaults(r=4, v=False, ms=False, pr=False)\n    return parser", "response": "Build an argument parser using optparse. Use it when python version is 2. 5 or 2. 6."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the best match mapping between two nodes.", "response": "def get_best_match(instance1, attribute1, relation1,\n                   instance2, attribute2, relation2,\n                   prefix1, prefix2, doinstance=True, doattribute=True, dorelation=True):\n    \"\"\"\n    Get the highest triple match number between two sets of triples via hill-climbing.\n    Arguments:\n        instance1: instance triples of AMR 1 (\"instance\", node name, node value)\n        attribute1: attribute triples of AMR 1 (attribute name, node name, attribute value)\n        relation1: relation triples of AMR 1 (relation name, node 1 name, node 2 name)\n        instance2: instance triples of AMR 2 (\"instance\", node name, node value)\n        attribute2: attribute triples of AMR 2 (attribute name, node name, attribute value)\n        relation2: relation triples of AMR 2 (relation name, node 1 name, node 2 name)\n        prefix1: prefix label for AMR 1\n        prefix2: prefix label for AMR 2\n    Returns:\n        best_match: the node mapping that results in the highest triple matching number\n        best_match_num: the highest triple matching number\n\n    \"\"\"\n    # Compute candidate pool - all possible node match candidates.\n    # In the hill-climbing, we only consider candidate in this pool to save computing time.\n    # weight_dict is a dictionary that maps a pair of node\n    (candidate_mappings, weight_dict) = compute_pool(instance1, attribute1, relation1,\n                                                     instance2, attribute2, relation2,\n                                                     prefix1, prefix2, doinstance=doinstance, doattribute=doattribute,\n                                                     dorelation=dorelation)\n    if veryVerbose:\n        print(\"Candidate mappings:\", file=DEBUG_LOG)\n        print(candidate_mappings, file=DEBUG_LOG)\n        print(\"Weight dictionary\", file=DEBUG_LOG)\n        print(weight_dict, file=DEBUG_LOG)\n\n    best_match_num = 0\n    # initialize best match mapping\n    # the ith entry is the node index in AMR 2 which maps to the ith node in AMR 1\n    best_mapping = [-1] * len(instance1)\n    for i in range(iteration_num):\n        if veryVerbose:\n            print(\"Iteration\", i, file=DEBUG_LOG)\n        if i == 0:\n            # smart initialization used for the first round\n            cur_mapping = smart_init_mapping(candidate_mappings, instance1, instance2)\n        else:\n            # random initialization for the other round\n            cur_mapping = random_init_mapping(candidate_mappings)\n        # compute current triple match number\n        match_num = compute_match(cur_mapping, weight_dict)\n        if veryVerbose:\n            print(\"Node mapping at start\", cur_mapping, file=DEBUG_LOG)\n            print(\"Triple match number at start:\", match_num, file=DEBUG_LOG)\n        while True:\n            # get best gain\n            (gain, new_mapping) = get_best_gain(cur_mapping, candidate_mappings, weight_dict,\n                                                len(instance2), match_num)\n            if veryVerbose:\n                print(\"Gain after the hill-climbing\", gain, file=DEBUG_LOG)\n            # hill-climbing until there will be no gain for new node mapping\n            if gain <= 0:\n                break\n            # otherwise update match_num and mapping\n            match_num += gain\n            cur_mapping = new_mapping[:]\n            if veryVerbose:\n                print(\"Update triple match number to:\", match_num, file=DEBUG_LOG)\n                print(\"Current mapping:\", cur_mapping, file=DEBUG_LOG)\n        if match_num > best_match_num:\n            best_mapping = cur_mapping[:]\n            best_match_num = match_num\n    return best_mapping, best_match_num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute all possible node mapping candidates and their weights (the triple matching number gain resulting from mapping one node in AMR 1 to another node in AMR2) Arguments: instance1: instance triples of AMR 1 attribute1: attribute triples of AMR 1 (attribute name, node name, attribute value) relation1: relation triples of AMR 1 (relation name, node 1 name, node 2 name) instance2: instance triples of AMR 2 attribute2: attribute triples of AMR 2 (attribute name, node name, attribute value) relation2: relation triples of AMR 2 (relation name, node 1 name, node 2 name prefix1: prefix label for AMR 1 prefix2: prefix label for AMR 2 Returns: candidate_mapping: a list of candidate nodes. The ith element contains the node indices (in AMR 2) the ith node (in AMR 1) can map to. (resulting in non-zero triple match) weight_dict: a dictionary which contains the matching triple number for every pair of node mapping. The key is a node pair. The value is another dictionary. key {-1} is triple match resulting from this node pair alone (instance triples and attribute triples), and other keys are node pairs that can result in relation triple match together with the first node pair.", "response": "def compute_pool(instance1, attribute1, relation1,\n                 instance2, attribute2, relation2,\n                 prefix1, prefix2, doinstance=True, doattribute=True, dorelation=True):\n    \"\"\"\n    compute all possible node mapping candidates and their weights (the triple matching number gain resulting from\n    mapping one node in AMR 1 to another node in AMR2)\n\n    Arguments:\n        instance1: instance triples of AMR 1\n        attribute1: attribute triples of AMR 1 (attribute name, node name, attribute value)\n        relation1: relation triples of AMR 1 (relation name, node 1 name, node 2 name)\n        instance2: instance triples of AMR 2\n        attribute2: attribute triples of AMR 2 (attribute name, node name, attribute value)\n        relation2: relation triples of AMR 2 (relation name, node 1 name, node 2 name\n        prefix1: prefix label for AMR 1\n        prefix2: prefix label for AMR 2\n    Returns:\n      candidate_mapping: a list of candidate nodes.\n                       The ith element contains the node indices (in AMR 2) the ith node (in AMR 1) can map to.\n                       (resulting in non-zero triple match)\n      weight_dict: a dictionary which contains the matching triple number for every pair of node mapping. The key\n                   is a node pair. The value is another dictionary. key {-1} is triple match resulting from this node\n                   pair alone (instance triples and attribute triples), and other keys are node pairs that can result\n                   in relation triple match together with the first node pair.\n\n\n    \"\"\"\n    candidate_mapping = []\n    weight_dict = {}\n    for instance1_item in instance1:\n        # each candidate mapping is a set of node indices\n        candidate_mapping.append(set())\n        if doinstance:\n            for instance2_item in instance2:\n                # if both triples are instance triples and have the same value\n                if normalize(instance1_item[0]) == normalize(instance2_item[0]) and \\\n                        normalize(instance1_item[2]) == normalize(instance2_item[2]):\n                    # get node index by stripping the prefix\n                    node1_index = int(instance1_item[1][len(prefix1):])\n                    node2_index = int(instance2_item[1][len(prefix2):])\n                    candidate_mapping[node1_index].add(node2_index)\n                    node_pair = (node1_index, node2_index)\n                    # use -1 as key in weight_dict for instance triples and attribute triples\n                    if node_pair in weight_dict:\n                        weight_dict[node_pair][-1] += 1\n                    else:\n                        weight_dict[node_pair] = {}\n                        weight_dict[node_pair][-1] = 1\n    if doattribute:\n        for attribute1_item in attribute1:\n            for attribute2_item in attribute2:\n                # if both attribute relation triple have the same relation name and value\n                if normalize(attribute1_item[0]) == normalize(attribute2_item[0]) \\\n                        and normalize(attribute1_item[2]) == normalize(attribute2_item[2]):\n                    node1_index = int(attribute1_item[1][len(prefix1):])\n                    node2_index = int(attribute2_item[1][len(prefix2):])\n                    candidate_mapping[node1_index].add(node2_index)\n                    node_pair = (node1_index, node2_index)\n                    # use -1 as key in weight_dict for instance triples and attribute triples\n                    if node_pair in weight_dict:\n                        weight_dict[node_pair][-1] += 1\n                    else:\n                        weight_dict[node_pair] = {}\n                        weight_dict[node_pair][-1] = 1\n    if dorelation:\n        for relation1_item in relation1:\n            for relation2_item in relation2:\n                # if both relation share the same name\n                if normalize(relation1_item[0]) == normalize(relation2_item[0]):\n                    node1_index_amr1 = int(relation1_item[1][len(prefix1):])\n                    node1_index_amr2 = int(relation2_item[1][len(prefix2):])\n                    node2_index_amr1 = int(relation1_item[2][len(prefix1):])\n                    node2_index_amr2 = int(relation2_item[2][len(prefix2):])\n                    # add mapping between two nodes\n                    candidate_mapping[node1_index_amr1].add(node1_index_amr2)\n                    candidate_mapping[node2_index_amr1].add(node2_index_amr2)\n                    node_pair1 = (node1_index_amr1, node1_index_amr2)\n                    node_pair2 = (node2_index_amr1, node2_index_amr2)\n                    if node_pair2 != node_pair1:\n                        # update weight_dict weight. Note that we need to update both entries for future search\n                        # i.e weight_dict[node_pair1][node_pair2]\n                        #     weight_dict[node_pair2][node_pair1]\n                        if node1_index_amr1 > node2_index_amr1:\n                            # swap node_pair1 and node_pair2\n                            node_pair1 = (node2_index_amr1, node2_index_amr2)\n                            node_pair2 = (node1_index_amr1, node1_index_amr2)\n                        if node_pair1 in weight_dict:\n                            if node_pair2 in weight_dict[node_pair1]:\n                                weight_dict[node_pair1][node_pair2] += 1\n                            else:\n                                weight_dict[node_pair1][node_pair2] = 1\n                        else:\n                            weight_dict[node_pair1] = {-1: 0, node_pair2: 1}\n                        if node_pair2 in weight_dict:\n                            if node_pair1 in weight_dict[node_pair2]:\n                                weight_dict[node_pair2][node_pair1] += 1\n                            else:\n                                weight_dict[node_pair2][node_pair1] = 1\n                        else:\n                            weight_dict[node_pair2] = {-1: 0, node_pair1: 1}\n                    else:\n                        # two node pairs are the same. So we only update weight_dict once.\n                        # this generally should not happen.\n                        if node_pair1 in weight_dict:\n                            weight_dict[node_pair1][-1] += 1\n                        else:\n                            weight_dict[node_pair1] = {-1: 1}\n    return candidate_mapping, weight_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the node mapping based on the concept mapping of two instances.", "response": "def smart_init_mapping(candidate_mapping, instance1, instance2):\n    \"\"\"\n    Initialize mapping based on the concept mapping (smart initialization)\n    Arguments:\n        candidate_mapping: candidate node match list\n        instance1: instance triples of AMR 1\n        instance2: instance triples of AMR 2\n    Returns:\n        initialized node mapping between two AMRs\n\n    \"\"\"\n    random.seed()\n    matched_dict = {}\n    result = []\n    # list to store node indices that have no concept match\n    no_word_match = []\n    for i, candidates in enumerate(candidate_mapping):\n        if not candidates:\n            # no possible mapping\n            result.append(-1)\n            continue\n        # node value in instance triples of AMR 1\n        value1 = instance1[i][2]\n        for node_index in candidates:\n            value2 = instance2[node_index][2]\n            # find the first instance triple match in the candidates\n            # instance triple match is having the same concept value\n            if value1 == value2:\n                if node_index not in matched_dict:\n                    result.append(node_index)\n                    matched_dict[node_index] = 1\n                    break\n        if len(result) == i:\n            no_word_match.append(i)\n            result.append(-1)\n    # if no concept match, generate a random mapping\n    for i in no_word_match:\n        candidates = list(candidate_mapping[i])\n        while candidates:\n            # get a random node index from candidates\n            rid = random.randint(0, len(candidates) - 1)\n            candidate = candidates[rid]\n            if candidate in matched_dict:\n                candidates.pop(rid)\n            else:\n                matched_dict[candidate] = 1\n                result[i] = candidate\n                break\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a random node mapping between two AMRs.", "response": "def random_init_mapping(candidate_mapping):\n    \"\"\"\n    Generate a random node mapping.\n    Args:\n        candidate_mapping: candidate_mapping: candidate node match list\n    Returns:\n        randomly-generated node mapping between two AMRs\n\n    \"\"\"\n    # if needed, a fixed seed could be passed here to generate same random (to help debugging)\n    random.seed()\n    matched_dict = {}\n    result = []\n    for c in candidate_mapping:\n        candidates = list(c)\n        if not candidates:\n            # -1 indicates no possible mapping\n            result.append(-1)\n            continue\n        found = False\n        while candidates:\n            # randomly generate an index in [0, length of candidates)\n            rid = random.randint(0, len(candidates) - 1)\n            candidate = candidates[rid]\n            # check if it has already been matched\n            if candidate in matched_dict:\n                candidates.pop(rid)\n            else:\n                matched_dict[candidate] = 1\n                result.append(candidate)\n                found = True\n                break\n        if not found:\n            result.append(-1)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_match(mapping, weight_dict):\n    # If this mapping has been investigated before, retrieve the value instead of re-computing.\n    if veryVerbose:\n        print(\"Computing match for mapping\", file=DEBUG_LOG)\n        print(mapping, file=DEBUG_LOG)\n    if tuple(mapping) in match_triple_dict:\n        if veryVerbose:\n            print(\"saved value\", match_triple_dict[tuple(mapping)], file=DEBUG_LOG)\n        return match_triple_dict[tuple(mapping)]\n    match_num = 0\n    # i is node index in AMR 1, m is node index in AMR 2\n    for i, m in enumerate(mapping):\n        if m == -1:\n            # no node maps to this node\n            continue\n        # node i in AMR 1 maps to node m in AMR 2\n        current_node_pair = (i, m)\n        if current_node_pair not in weight_dict:\n            continue\n        if veryVerbose:\n            print(\"node_pair\", current_node_pair, file=DEBUG_LOG)\n        for key in weight_dict[current_node_pair]:\n            if key == -1:\n                # matching triple resulting from instance/attribute triples\n                match_num += weight_dict[current_node_pair][key]\n                if veryVerbose:\n                    print(\"instance/attribute match\", weight_dict[current_node_pair][key], file=DEBUG_LOG)\n            # only consider node index larger than i to avoid duplicates\n            # as we store both weight_dict[node_pair1][node_pair2] and\n            #     weight_dict[node_pair2][node_pair1] for a relation\n            elif key[0] < i:\n                continue\n            elif mapping[key[0]] == key[1]:\n                match_num += weight_dict[current_node_pair][key]\n                if veryVerbose:\n                    print(\"relation match with\", key, weight_dict[current_node_pair][key], file=DEBUG_LOG)\n    if veryVerbose:\n        print(\"match computing complete, result:\", match_num, file=DEBUG_LOG)\n    # update match_triple_dict\n    match_triple_dict[tuple(mapping)] = match_num\n    return match_num", "response": "Given a node mapping compute the match number based on weight_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_gain(mapping, node_id, old_id, new_id, weight_dict, match_num):\n    # new node mapping after moving\n    new_mapping = (node_id, new_id)\n    # node mapping before moving\n    old_mapping = (node_id, old_id)\n    # new nodes mapping list (all node pairs)\n    new_mapping_list = mapping[:]\n    new_mapping_list[node_id] = new_id\n    # if this mapping is already been investigated, use saved one to avoid duplicate computing\n    if tuple(new_mapping_list) in match_triple_dict:\n        return match_triple_dict[tuple(new_mapping_list)] - match_num\n    gain = 0\n    # add the triple match incurred by new_mapping to gain\n    if new_mapping in weight_dict:\n        for key in weight_dict[new_mapping]:\n            if key == -1:\n                # instance/attribute triple match\n                gain += weight_dict[new_mapping][-1]\n            elif new_mapping_list[key[0]] == key[1]:\n                # relation gain incurred by new_mapping and another node pair in new_mapping_list\n                gain += weight_dict[new_mapping][key]\n    # deduct the triple match incurred by old_mapping from gain\n    if old_mapping in weight_dict:\n        for k in weight_dict[old_mapping]:\n            if k == -1:\n                gain -= weight_dict[old_mapping][-1]\n            elif mapping[k[0]] == k[1]:\n                gain -= weight_dict[old_mapping][k]\n    # update match number dictionary\n    match_triple_dict[tuple(new_mapping_list)] = match_num + gain\n    return gain", "response": "Compute the triple match number gain from the move operation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef swap_gain(mapping, node_id1, mapping_id1, node_id2, mapping_id2, weight_dict, match_num):\n    new_mapping_list = mapping[:]\n    # Before swapping, node_id1 maps to mapping_id1, and node_id2 maps to mapping_id2\n    # After swapping, node_id1 maps to mapping_id2 and node_id2 maps to mapping_id1\n    new_mapping_list[node_id1] = mapping_id2\n    new_mapping_list[node_id2] = mapping_id1\n    if tuple(new_mapping_list) in match_triple_dict:\n        return match_triple_dict[tuple(new_mapping_list)] - match_num\n    gain = 0\n    new_mapping1 = (node_id1, mapping_id2)\n    new_mapping2 = (node_id2, mapping_id1)\n    old_mapping1 = (node_id1, mapping_id1)\n    old_mapping2 = (node_id2, mapping_id2)\n    if node_id1 > node_id2:\n        new_mapping2 = (node_id1, mapping_id2)\n        new_mapping1 = (node_id2, mapping_id1)\n        old_mapping1 = (node_id2, mapping_id2)\n        old_mapping2 = (node_id1, mapping_id1)\n    if new_mapping1 in weight_dict:\n        for key in weight_dict[new_mapping1]:\n            if key == -1:\n                gain += weight_dict[new_mapping1][-1]\n            elif new_mapping_list[key[0]] == key[1]:\n                gain += weight_dict[new_mapping1][key]\n    if new_mapping2 in weight_dict:\n        for key in weight_dict[new_mapping2]:\n            if key == -1:\n                gain += weight_dict[new_mapping2][-1]\n            # to avoid duplicate\n            elif key[0] == node_id1:\n                continue\n            elif new_mapping_list[key[0]] == key[1]:\n                gain += weight_dict[new_mapping2][key]\n    if old_mapping1 in weight_dict:\n        for key in weight_dict[old_mapping1]:\n            if key == -1:\n                gain -= weight_dict[old_mapping1][-1]\n            elif mapping[key[0]] == key[1]:\n                gain -= weight_dict[old_mapping1][key]\n    if old_mapping2 in weight_dict:\n        for key in weight_dict[old_mapping2]:\n            if key == -1:\n                gain -= weight_dict[old_mapping2][-1]\n            # to avoid duplicate\n            elif key[0] == node_id1:\n                continue\n            elif mapping[key[0]] == key[1]:\n                gain -= weight_dict[old_mapping2][key]\n    match_triple_dict[tuple(new_mapping_list)] = match_num + gain\n    return gain", "response": "This function computes the triple match number gain from the swapping ArcGIS node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_best_gain(mapping, candidate_mappings, weight_dict, instance_len, cur_match_num):\n    largest_gain = 0\n    # True: using swap; False: using move\n    use_swap = True\n    # the node to be moved/swapped\n    node1 = None\n    # store the other node affected. In swap, this other node is the node swapping with node1. In move, this other\n    # node is the node node1 will move to.\n    node2 = None\n    # unmatched nodes in AMR 2\n    unmatched = set(range(instance_len))\n    # exclude nodes in current mapping\n    # get unmatched nodes\n    for nid in mapping:\n        if nid in unmatched:\n            unmatched.remove(nid)\n    for i, nid in enumerate(mapping):\n        # current node i in AMR 1 maps to node nid in AMR 2\n        for nm in unmatched:\n            if nm in candidate_mappings[i]:\n                # remap i to another unmatched node (move)\n                # (i, m) -> (i, nm)\n                if veryVerbose:\n                    print(\"Remap node\", i, \"from \", nid, \"to\", nm, file=DEBUG_LOG)\n                mv_gain = move_gain(mapping, i, nid, nm, weight_dict, cur_match_num)\n                if veryVerbose:\n                    print(\"Move gain:\", mv_gain, file=DEBUG_LOG)\n                    new_mapping = mapping[:]\n                    new_mapping[i] = nm\n                    new_match_num = compute_match(new_mapping, weight_dict)\n                    if new_match_num != cur_match_num + mv_gain:\n                        print(mapping, new_mapping, file=ERROR_LOG)\n                        print(\"Inconsistency in computing: move gain\", cur_match_num, mv_gain, new_match_num,\n                              file=ERROR_LOG)\n                if mv_gain > largest_gain:\n                    largest_gain = mv_gain\n                    node1 = i\n                    node2 = nm\n                    use_swap = False\n    # compute swap gain\n    for i, m in enumerate(mapping):\n        for j in range(i + 1, len(mapping)):\n            m2 = mapping[j]\n            # swap operation (i, m) (j, m2) -> (i, m2) (j, m)\n            # j starts from i+1, to avoid duplicate swap\n            if veryVerbose:\n                print(\"Swap node\", i, \"and\", j, file=DEBUG_LOG)\n                print(\"Before swapping:\", i, \"-\", m, \",\", j, \"-\", m2, file=DEBUG_LOG)\n                print(mapping, file=DEBUG_LOG)\n                print(\"After swapping:\", i, \"-\", m2, \",\", j, \"-\", m, file=DEBUG_LOG)\n            sw_gain = swap_gain(mapping, i, m, j, m2, weight_dict, cur_match_num)\n            if veryVerbose:\n                print(\"Swap gain:\", sw_gain, file=DEBUG_LOG)\n                new_mapping = mapping[:]\n                new_mapping[i] = m2\n                new_mapping[j] = m\n                print(new_mapping, file=DEBUG_LOG)\n                new_match_num = compute_match(new_mapping, weight_dict)\n                if new_match_num != cur_match_num + sw_gain:\n                    print(mapping, new_mapping, file=ERROR_LOG)\n                    print(\"Inconsistency in computing: swap gain\", cur_match_num, sw_gain, new_match_num,\n                          file=ERROR_LOG)\n            if sw_gain > largest_gain:\n                largest_gain = sw_gain\n                node1 = i\n                node2 = j\n                use_swap = True\n    # generate a new mapping based on swap/move\n    cur_mapping = mapping[:]\n    if node1 is not None:\n        if use_swap:\n            if veryVerbose:\n                print(\"Use swap gain\", file=DEBUG_LOG)\n            temp = cur_mapping[node1]\n            cur_mapping[node1] = cur_mapping[node2]\n            cur_mapping[node2] = temp\n        else:\n            if veryVerbose:\n                print(\"Use move gain\", file=DEBUG_LOG)\n            cur_mapping[node1] = node2\n    else:\n        if veryVerbose:\n            print(\"no move/swap gain found\", file=DEBUG_LOG)\n    if veryVerbose:\n        print(\"Original mapping\", mapping, file=DEBUG_LOG)\n        print(\"Current mapping\", cur_mapping, file=DEBUG_LOG)\n    return largest_gain, cur_mapping", "response": "This method returns the best gain for a given candidate_mappings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints the alignment based on a node mapping", "response": "def print_alignment(mapping, instance1, instance2):\n    \"\"\"\n    print the alignment based on a node mapping\n    Args:\n        mapping: current node mapping list\n        instance1: nodes of AMR 1\n        instance2: nodes of AMR 2\n\n    \"\"\"\n    result = []\n    for instance1_item, m in zip(instance1, mapping):\n        r = instance1_item[1] + \"(\" + instance1_item[2] + \")\"\n        if m == -1:\n            r += \"-Null\"\n        else:\n            instance2_item = instance2[m]\n            r += \"-\" + instance2_item[1] + \"(\" + instance2_item[2] + \")\"\n        result.append(r)\n    return \" \".join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_f(match_num, test_num, gold_num):\n    if test_num == 0 or gold_num == 0:\n        return 0.00, 0.00, 0.00\n    precision = float(match_num) / float(test_num)\n    recall = float(match_num) / float(gold_num)\n    if (precision + recall) != 0:\n        f_score = 2 * precision * recall / (precision + recall)\n        if veryVerbose:\n            print(\"F-score:\", f_score, file=DEBUG_LOG)\n        return precision, recall, f_score\n    else:\n        if veryVerbose:\n            print(\"F-score:\", \"0.0\", file=DEBUG_LOG)\n        return precision, recall, 0.00", "response": "Compute the f - score based on the matching triple number and the matching triple number of AMR set 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_amr_lines(f1, f2):\n    while True:\n        cur_amr1 = amr.AMR.get_amr_line(f1)\n        cur_amr2 = amr.AMR.get_amr_line(f2)\n        if not cur_amr1 and not cur_amr2:\n            pass\n        elif not cur_amr1:\n            print(\"Error: File 1 has less AMRs than file 2\", file=ERROR_LOG)\n            print(\"Ignoring remaining AMRs\", file=ERROR_LOG)\n        elif not cur_amr2:\n            print(\"Error: File 2 has less AMRs than file 1\", file=ERROR_LOG)\n            print(\"Ignoring remaining AMRs\", file=ERROR_LOG)\n        else:\n            yield cur_amr1, cur_amr2\n            continue\n        break", "response": "Generator for one - line AMR lines from the given file handles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a generator of one - line AMR pairs from two files.", "response": "def score_amr_pairs(f1, f2, justinstance=False, justattribute=False, justrelation=False):\n    \"\"\"\n    Score one pair of AMR lines at a time from each file handle\n    :param f1: file handle (or any iterable of strings) to read AMR 1 lines from\n    :param f2: file handle (or any iterable of strings) to read AMR 2 lines from\n    :param justinstance: just pay attention to matching instances\n    :param justattribute: just pay attention to matching attributes\n    :param justrelation: just pay attention to matching relations\n    :return: generator of cur_amr1, cur_amr2 pairs: one-line AMR strings\n    \"\"\"\n    # matching triple number, triple number in test file, triple number in gold file\n    total_match_num = total_test_num = total_gold_num = 0\n    # Read amr pairs from two files\n    for sent_num, (cur_amr1, cur_amr2) in enumerate(generate_amr_lines(f1, f2), start=1):\n        best_match_num, test_triple_num, gold_triple_num = get_amr_match(cur_amr1, cur_amr2,\n                                                                         sent_num=sent_num,  # sentence number\n                                                                         justinstance=justinstance,\n                                                                         justattribute=justattribute,\n                                                                         justrelation=justrelation)\n        total_match_num += best_match_num\n        total_test_num += test_triple_num\n        total_gold_num += gold_triple_num\n        # clear the matching triple dictionary for the next AMR pair\n        match_triple_dict.clear()\n        if not single_score:  # if each AMR pair should have a score, compute and output it here\n            yield compute_f(best_match_num, test_triple_num, gold_triple_num)\n    if verbose:\n        print(\"Total match number, total triple number in AMR 1, and total triple number in AMR 2:\", file=DEBUG_LOG)\n        print(total_match_num, total_test_num, total_gold_num, file=DEBUG_LOG)\n        print(\"---------------------------------------------------------------------------------\", file=DEBUG_LOG)\n    if single_score:  # output document-level smatch score (a single f-score for all AMR pairs in two files)\n        yield compute_f(total_match_num, total_test_num, total_gold_num)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting volume metadata from es to archivant format", "response": "def normalize_volume(volume):\n        '''convert volume metadata from es to archivant format\n\n           This function makes side effect on input volume\n\n           output example::\n\n            {\n                'id': 'AU0paPZOMZchuDv1iDv8',\n                'type': 'volume',\n                'metadata': {'_language': 'en',\n                            'key1': 'value1',\n                            'key2': 'value2',\n                            'key3': 'value3'},\n                'attachments': [{'id': 'a910e1kjdo2d192d1dko1p2kd1209d',\n                                'type' : 'attachment',\n                                'url': 'fsdb:///624bffa8a6f90813b7982d0e5b4c1475ebec40e3',\n                                'metadata': {'download_count': 0,\n                                            'mime': 'application/json',\n                                            'name': 'tmp9fyat_',\n                                            'notes': 'this file is awsome',\n                                            'sha1': '624bffa8a6f90813b7982d0e5b4c1475ebec40e3',\n                                            'size': 10}\n                            }]\n            }\n\n        '''\n        res = dict()\n        res['type'] = 'volume'\n        res['id'] = volume['_id']\n        if '_score' in volume:\n            res['score'] = volume['_score']\n\n        source = volume['_source']\n        attachments = source['_attachments']\n        del(source['_attachments'])\n        del(source['_text_' + source['_language']])\n        res['metadata'] = source\n\n        atts = list()\n        for attachment in attachments:\n            atts.append(Archivant.normalize_attachment(attachment))\n        res['attachments'] = atts\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting attachment metadata from es to archivant format", "response": "def normalize_attachment(attachment):\n        ''' Convert attachment metadata from es to archivant format\n\n            This function makes side effect on input attachment\n        '''\n        res = dict()\n        res['type'] = 'attachment'\n        res['id'] = attachment['id']\n        del(attachment['id'])\n        res['url'] = attachment['url']\n        del(attachment['url'])\n        res['metadata'] = attachment\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting volume metadata from archivant to es format", "response": "def denormalize_volume(volume):\n        '''convert volume metadata from archivant to es format'''\n        id = volume.get('id', None)\n        res = dict()\n        res.update(volume['metadata'])\n        denorm_attachments = list()\n        for a in volume['attachments']:\n            denorm_attachments.append(Archivant.denormalize_attachment(a))\n        res['_attachments'] = denorm_attachments\n        return id, res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting attachment metadata from archivant to es format", "response": "def denormalize_attachment(attachment):\n        '''convert attachment metadata from archivant to es format'''\n        res = dict()\n        ext = ['id', 'url']\n        for k in ext:\n            if k in attachment['metadata']:\n                raise ValueError(\"metadata section could not contain special key '{}'\".format(k))\n            res[k] = attachment[k]\n        res.update(attachment['metadata'])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all stored volumes", "response": "def iter_all_volumes(self):\n        '''iterate over all stored volumes'''\n        for raw_volume in self._db.iterate_all():\n            v = self.normalize_volume(raw_volume)\n            del v['score']\n            yield v"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_attachments(self, volumeID, attachmentsID):\n        ''' delete attachments from a volume '''\n        log.debug(\"deleting attachments from volume '{}': {}\".format(volumeID, attachmentsID))\n        rawVolume = self._req_raw_volume(volumeID)\n        insID = [a['id'] for a in rawVolume['_source']['_attachments']]\n        # check that all requested file are present\n        for id in attachmentsID:\n            if id not in insID:\n                raise NotFoundException(\"could not found attachment '{}' of the volume '{}'\".format(id, volumeID))\n        for index, id in enumerate(attachmentsID):\n            rawVolume['_source']['_attachments'].pop(insID.index(id))\n        self._db.modify_book(volumeID, rawVolume['_source'], version=rawVolume['_version'])", "response": "delete attachments from a volume"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_attachments(self, volumeID, attachments):\n        ''' add attachments to an already existing volume '''\n        log.debug(\"adding new attachments to volume '{}': {}\".format(volumeID, attachments))\n        if not attachments:\n            return\n        rawVolume = self._req_raw_volume(volumeID)\n        attsID = list()\n        for index, a in enumerate(attachments):\n            try:\n                rawAttachment = self._assemble_attachment(a['file'], a)\n                rawVolume['_source']['_attachments'].append(rawAttachment)\n                attsID.append(rawAttachment['id'])\n            except Exception:\n                log.exception(\"Error while elaborating attachments array at index: {}\".format(index))\n                raise\n        self._db.modify_book(volumeID, rawVolume['_source'], version=rawVolume['_version'])\n        return attsID", "response": "add attachments to an already existing volume"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_volume(self, metadata, attachments=[]):\n        '''Insert a new volume\n\n        Returns the ID of the added volume\n\n        `metadata` must be a dict containg metadata of the volume::\n\n            {\n              \"_language\" : \"it\",  # language of the metadata\n              \"key1\" : \"value1\",   # attribute\n              \"key2\" : \"value2\",\n              ...\n              \"keyN\" : \"valueN\"\n            }\n\n            The only required key is `_language`\n\n\n        `attachments` must be an array of dict::\n\n            {\n              \"file\"  : \"/prova/una/path/a/caso\" # path or fp\n              \"name\"  : \"nome_buffo.ext\"         # name of the file (extension included) [optional if a path was given]\n              \"mime\"  : \"application/json\"       # mime type of the file [optional]\n              \"notes\" : \"this file is awesome\"   # notes that will be attached to this file [optional]\n            }\n\n        '''\n\n        log.debug(\"adding new volume:\\n\\tdata: {}\\n\\tfiles: {}\".format(metadata, attachments))\n\n        requiredFields = ['_language']\n        for requiredField in requiredFields:\n            if requiredField not in metadata:\n                raise KeyError(\"Required field '{}' is missing\".format(requiredField))\n\n        volume = deepcopy(metadata)\n\n        attsData = []\n        for index, a in enumerate(attachments):\n            try:\n                attData = self._assemble_attachment(a['file'], a)\n                attsData.append(attData)\n            except Exception:\n                log.exception(\"Error while elaborating attachments array at index: {}\".format(index))\n                raise\n        volume['_attachments'] = attsData\n\n        log.debug('constructed volume for insertion: {}'.format(volume))\n        addedVolume = self._db.add_book(body=volume)\n        log.debug(\"added new volume: '{}'\".format(addedVolume['_id']))\n        return addedVolume['_id']", "response": "Insert a new volume into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassembles an attachment into a dictionary", "response": "def _assemble_attachment(self, file, metadata):\n        ''' store file and return a dict containing assembled metadata\n\n            param `file` must be a path or a File Object\n            param `metadata` must be a dict:\n                {\n                  \"name\"  : \"nome_buffo.ext\"         # name of the file (extension included) [optional if a path was given]\n                  \"mime\"  : \"application/json\"       # mime type of the file [optional]\n                  \"notes\" : \"this file is awesome\"   # notes about this file [optional]\n                }\n        '''\n        res = dict()\n\n        if isinstance(file, basestring) and os.path.isfile(file):\n            res['name'] = metadata['name'] if 'name' in metadata else os.path.basename(file)\n            res['size'] = os.path.getsize(file)\n            res['sha1'] = calc_file_digest(file, algorithm=\"sha1\")\n\n        elif hasattr(file, 'read') and hasattr(file, 'seek'):\n            if 'name' in metadata and metadata['name']:\n                res['name'] = metadata['name']\n            elif hasattr(file, 'name'):\n                file['name'] = file.name\n            else:\n                raise ValueError(\"Could not assign a name to the file\")\n\n            old_position = file.tell()\n\n            file.seek(0, os.SEEK_END)\n            res['size'] = file.tell() - old_position\n            file.seek(old_position, os.SEEK_SET)\n\n            res['sha1'] = calc_digest(file, algorithm=\"sha1\")\n            file.seek(old_position, os.SEEK_SET)\n\n        else:\n            raise ValueError(\"Unsupported file value type: {}\".format(type(file)))\n\n        res['id'] = uuid4().hex\n        res['mime'] = metadata['mime'] if 'mime' in metadata else None\n        res['notes'] = metadata['notes'] if 'notes' in metadata else \"\"\n        res['download_count'] = 0\n        fsdb_id = self._fsdb.add(file)\n        res['url'] = \"fsdb:///\" + fsdb_id\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates existing volume metadata", "response": "def update_volume(self, volumeID, metadata):\n        '''update existing volume metadata\n           the given metadata will substitute the old one\n        '''\n        log.debug('updating volume metadata: {}'.format(volumeID))\n        rawVolume = self._req_raw_volume(volumeID)\n        normalized = self.normalize_volume(rawVolume)\n        normalized['metadata'] = metadata\n        _, newRawVolume = self.denormalize_volume(normalized)\n        self._db.modify_book(volumeID, newRawVolume)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating an existing attachment with the given metadata dict", "response": "def update_attachment(self, volumeID, attachmentID, metadata):\n        '''update an existing attachment\n\n        the given metadata dict will be merged with the old one.\n        only the following fields could be updated:\n        [name, mime, notes, download_count]\n        '''\n        log.debug('updating metadata of attachment {} from volume {}'.format(attachmentID, volumeID))\n        modifiable_fields = ['name', 'mime', 'notes', 'download_count']\n        for k in metadata.keys():\n            if k not in modifiable_fields:\n                raise ValueError('Not modifiable field given: {}'.format(k))\n        if 'name' in metadata and not isinstance(metadata['name'], basestring):\n            raise ValueError(\"'name' must be a string\")\n        if 'mime' in metadata and not isinstance(metadata['mime'], basestring):\n            raise ValueError(\"'mime' must be a string\")\n        if 'notes' in metadata and not isinstance(metadata['notes'], basestring):\n            raise ValueError(\"'notes' must be a string\")\n        if 'download_count' in metadata and not isinstance(metadata['download_count'], Integral):\n            raise ValueError(\"'download_count' must be a number\")\n        rawVolume = self._req_raw_volume(volumeID)\n        for attachment in rawVolume['_source']['_attachments']:\n            if attachment['id'] == attachmentID:\n                attachment.update(metadata)\n                self._db.modify_book(id=volumeID, body=rawVolume['_source'], version=rawVolume['_version'])\n                return\n        raise NotFoundException('Could not found attachment with id {} in volume {}'.format(attachmentID, volumeID))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over fsdb files no more attached to any volume", "response": "def dangling_files(self):\n        '''iterate over fsdb files no more attached to any volume'''\n        for fid in self._fsdb:\n            if not self._db.file_is_attached('fsdb:///' + fid):\n                yield fid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shrink_local_fsdb(self, dangling=True, corrupted=True, dryrun=False):\n        '''shrink local fsdb by removing dangling and/or corrupted files\n\n           return number of deleted files\n        '''\n        log.debug('shrinking local fsdb [danglings={}, corrupted={}]'.format(dangling, corrupted))\n        count = 0\n        if dangling:\n            for fid in self.dangling_files():\n                log.info(\"shrinking: removing dangling  '{}'\".format(fid))\n                if not dryrun:\n                    self._fsdb.remove(fid)\n                count += 1\n        if corrupted:\n            for fid in self._fsdb.corrupted():\n                log.info(\"shrinking: removing corrupted '{}'\".format(fid))\n                if not dryrun:\n                    self._fsdb.remove(fid)\n                count += 1\n        return count", "response": "shrink local fsdb by removing dangling and corrupted files\n        return number of deleted files\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a BSON string to python unicode string.", "response": "def _get_string(data, position, obj_end, dummy):\n    \"\"\"Decode a BSON string to python unicode string.\"\"\"\n    length = _UNPACK_INT(data[position:position + 4])[0]\n    position += 4\n    if length < 1 or obj_end - position < length:\n        raise InvalidBSON(\"invalid string length\")\n    end = position + length - 1\n    if data[end:end + 1] != b\"\\x00\":\n        raise InvalidBSON(\"invalid end of string\")\n    return _utf_8_decode(data[position:end], None, True)[0], end + 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a BSON subdocument to opts. document_class or bson. dbref. DBRef.", "response": "def _get_object(data, position, obj_end, opts):\n    \"\"\"Decode a BSON subdocument to opts.document_class or bson.dbref.DBRef.\"\"\"\n    obj_size = _UNPACK_INT(data[position:position + 4])[0]\n    end = position + obj_size - 1\n    if data[end:position + obj_size] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    if end >= obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    obj = _elements_to_dict(data, position + 4, end, opts, subdocument=True)\n\n    position += obj_size\n    if \"$ref\" in obj:\n        return (DBRef(obj.pop(\"$ref\"), obj.pop(\"$id\", None),\n                      obj.pop(\"$db\", None), obj), position)\n    return obj, position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a BSON true or false to python True or False.", "response": "def _get_boolean(data, position, dummy0, dummy1):\n    \"\"\"Decode a BSON true/false to python True/False.\"\"\"\n    end = position + 1\n    return data[position:end] == b\"\\x01\", end"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes a BSON datetime to python datetime. datetime.", "response": "def _get_date(data, position, dummy, opts):\n    \"\"\"Decode a BSON datetime to python datetime.datetime.\"\"\"\n    end = position + 8\n    millis = _UNPACK_LONG(data[position:end])[0]\n    diff = ((millis % 1000) + 1000) % 1000\n    seconds = (millis - diff) / 1000\n    micros = diff * 1000\n    if opts.tz_aware:\n        return EPOCH_AWARE + datetime.timedelta(\n            seconds=seconds, microseconds=micros), end\n    else:\n        return EPOCH_NAIVE + datetime.timedelta(\n            seconds=seconds, microseconds=micros), end"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_code_w_scope(data, position, obj_end, opts):\n    code, position = _get_string(data, position + 4, obj_end, opts)\n    scope, position = _get_object(data, position, obj_end, opts)\n    return Code(code, scope), position", "response": "Decode a BSON code_w_scope to bson. code. Code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_regex(data, position, dummy0, dummy1):\n    pattern, position = _get_c_string(data, position)\n    bson_flags, position = _get_c_string(data, position)\n    bson_re = Regex(pattern, bson_flags)\n    return bson_re, position", "response": "Decode a BSON regex to bson. regex. Regex or a python pattern object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a BSON document.", "response": "def _elements_to_dict(data, position, obj_end, opts, subdocument=None):\n    \"\"\"Decode a BSON document.\"\"\"\n    if type(opts.document_class) == tuple:\n        result = opts.document_class[0](**opts.document_class[1]) if not subdocument else dict()\n    else:\n        result = opts.document_class() if not subdocument else dict()\n    end = obj_end - 1\n    while position < end:\n        (key, value, position) = _element_to_dict(data, position, obj_end, opts)\n        result[key] = value\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bson_to_dict(data, opts):\n    try:\n        obj_size = _UNPACK_INT(data[:4])[0]\n    except struct.error as exc:\n        raise InvalidBSON(str(exc))\n    if obj_size != len(data):\n        raise InvalidBSON(\"invalid object size\")\n    if data[obj_size - 1:obj_size] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    try:\n        return _elements_to_dict(data, 4, obj_size - 1, opts)\n    except InvalidBSON:\n        raise\n    except Exception:\n        # Change exception type to InvalidBSON but preserve traceback.\n        _, exc_value, exc_tb = sys.exc_info()\n        reraise(InvalidBSON, exc_value, exc_tb)", "response": "Decode a BSON string to document_class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a mapping type.", "response": "def _encode_mapping(name, value, check_keys, opts):\n    \"\"\"Encode a mapping type.\"\"\"\n    data = b\"\".join([_element_to_bson(key, val, check_keys, opts)\n                     for key, val in iteritems(value)])\n    return b\"\\x03\" + name + _PACK_INT(len(data) + 5) + data + b\"\\x00\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding bson. code. Code.", "response": "def _encode_code(name, value, dummy, opts):\n    \"\"\"Encode bson.code.Code.\"\"\"\n    cstring = _make_c_string(value)\n    cstrlen = len(cstring)\n    if not value.scope:\n        return b\"\\x0D\" + name + _PACK_INT(cstrlen) + cstring\n    scope = _dict_to_bson(value.scope, False, opts, False)\n    full_length = _PACK_INT(8 + cstrlen + len(scope))\n    return b\"\\x0F\" + name + full_length + _PACK_INT(cstrlen) + cstring + scope"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simToReg(self, sim):\n        # remove initial slash if present\n        res = re.sub('^/', '', sim)\n        res = re.sub('/$', '', res)\n        return '^/?' + re.sub('\\*', '[^/]+', res) + '/?$'", "response": "Convert simplified domain expression to regular expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(self, dom, act):\n        return self.match_domain(dom) and self.match_action(act)", "response": "Check if the given domain and action are allowed by this capability\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert an actions bitmask into a list of action strings", "response": "def to_list(self):\n        '''convert an actions bitmask into a list of action strings'''\n        res = []\n        for a in self.__class__.ACTIONS:\n            aBit = self.__class__.action_bitmask(a)\n            if ((self & aBit) == aBit):\n                res.append(a)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_list(cls, actions):\n        '''convert list of actions into the corresponding bitmask'''\n        bitmask = 0\n        for a in actions:\n            bitmask |= cls.action_bitmask(a)\n        return Action(bitmask)", "response": "convert list of actions into the corresponding bitmask"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_section_metrics(cls):\n\n        return {\n            \"overview\": {\n                \"activity_metrics\": [Commits],\n                \"author_metrics\": [Authors],\n                \"bmi_metrics\": [],\n                \"time_to_close_metrics\": [],\n                \"projects_metrics\": [Projects]\n            },\n            \"com_channels\": {\n                \"activity_metrics\": [],\n                \"author_metrics\": []\n            },\n            \"project_activity\": {\n                # TODO: Authors is not activity but we need two metrics here\n                \"metrics\": [Commits, Authors]\n            },\n            \"project_community\": {\n                \"author_metrics\": [Authors],\n                \"people_top_metrics\": [Authors],\n                \"orgs_top_metrics\": [Organizations],\n            },\n            \"project_process\": {\n                \"bmi_metrics\": [],\n                \"time_to_close_metrics\": [],\n                \"time_to_close_title\": \"\",\n                \"time_to_close_review_metrics\": [],\n                \"time_to_close_review_title\": \"\",\n                \"patchsets_metrics\": []\n            }\n        }", "response": "Get the mapping between metrics and sections in Manuscripts report\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef str_val(val):\n    str_val = val\n    if val is None:\n        str_val = \"NA\"\n    elif type(val) == float:\n        str_val = '%0.2f' % val\n    else:\n        str_val = str(val)\n    return str_val", "response": "Format the value of a metric value to a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_cfg(path, envvar_prefix='LIBREANT_', debug=False):\n    '''wrapper of config_utils.load_configs'''\n    try:\n        return load_configs(envvar_prefix, path=path)\n    except Exception as e:\n        if debug:\n            raise\n        else:\n            die(str(e))", "response": "wrapper of config_utils. load_configs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef files_in_subdir(dir, subdir):\n    paths = []\n    for (path, dirs, files) in os.walk(os.path.join(dir, subdir)):\n        for file in files:\n            paths.append(os.path.relpath(os.path.join(path, file), dir))\n    return paths", "response": "Find all files in a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes metrics in the overview section for enriched git indexes.", "response": "def overview(index, start, end):\n    \"\"\"Compute metrics in the overview section for enriched git indexes.\n\n    Returns a dictionary. Each key in the dictionary is the name of\n    a metric, the value is the value of that metric. Value can be\n    a complex object (eg, a time series).\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"activity_metrics\": [Commits(index, start, end)],\n        \"author_metrics\": [Authors(index, start, end)],\n        \"bmi_metrics\": [],\n        \"time_to_close_metrics\": [],\n        \"projects_metrics\": []\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef project_activity(index, start, end):\n\n    results = {\n        \"metrics\": [Commits(index, start, end),\n                    Authors(index, start, end)]\n    }\n\n    return results", "response": "Compute the metrics for the project activity section of the enriched\n    git index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the metrics for the project community section of the enriched git index.", "response": "def project_community(index, start, end):\n    \"\"\"Compute the metrics for the project community section of the enriched\n    git index.\n\n    Returns a dictionary containing \"author_metrics\", \"people_top_metrics\"\n    and \"orgs_top_metrics\" as the keys and the related Metrics as the values.\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"author_metrics\": [Authors(index, start, end)],\n        \"people_top_metrics\": [Authors(index, start, end)],\n        \"orgs_top_metrics\": [Organizations(index, start, end)],\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding parent method to obtain list of the terms and their corresponding values using terms aggregations for the previous time period.", "response": "def aggregations(self):\n        \"\"\"\n        Override parent method. Obtain list of the terms and their corresponding\n        values using \"terms\" aggregations for the previous time period.\n\n        :returns: a data frame containing terms and their corresponding values\n        \"\"\"\n\n        prev_month_start = get_prev_month(self.end, self.query.interval_)\n        self.query.since(prev_month_start)\n        self.query.get_terms(\"author_name\")\n        return self.query.get_list(dataframe=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the date histogram aggregations.", "response": "def timeseries(self, dataframe=False):\n        \"\"\"Get the date histogram aggregations.\n\n        :param dataframe: if true, return a pandas.DataFrame object\n        \"\"\"\n\n        self.query.get_cardinality(\"author_uuid\").by_period()\n        return super().timeseries(dataframe)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute metrics in the overview section for enriched github issues indexes.", "response": "def overview(index, start, end):\n    \"\"\"Compute metrics in the overview section for enriched github issues\n    indexes.\n    Returns a dictionary. Each key in the dictionary is the name of\n    a metric, the value is the value of that metric. Value can be\n    a complex object (eg, a time series).\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"activity_metrics\": [OpenedIssues(index, start, end),\n                             ClosedIssues(index, start, end)],\n        \"author_metrics\": [],\n        \"bmi_metrics\": [BMI(index, start, end)],\n        \"time_to_close_metrics\": [DaysToCloseMedian(index, start, end)],\n        \"projects_metrics\": []\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the metrics for the project activity section of the enriched github issues index.", "response": "def project_activity(index, start, end):\n    \"\"\"Compute the metrics for the project activity section of the enriched\n    github issues index.\n\n    Returns a dictionary containing a \"metric\" key. This key contains the\n    metrics for this section.\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"metrics\": [OpenedIssues(index, start, end),\n                    ClosedIssues(index, start, end)]\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the metrics for the project process section of the enriched github issues index.", "response": "def project_process(index, start, end):\n    \"\"\"Compute the metrics for the project process section of the enriched\n    github issues index.\n\n    Returns a dictionary containing \"bmi_metrics\", \"time_to_close_metrics\",\n    \"time_to_close_review_metrics\" and patchsets_metrics as the keys and\n    the related Metrics as the values.\n    time_to_close_title and time_to_close_review_title contain the file names\n    to be used for time_to_close_metrics and time_to_close_review_metrics\n    metrics data.\n\n    :param index: index object\n    :param start: start date to get the data from\n    :param end: end date to get the data upto\n    :return: dictionary with the value of the metrics\n    \"\"\"\n\n    results = {\n        \"bmi_metrics\": [BMI(index, start, end)],\n        \"time_to_close_metrics\": [DaysToCloseAverage(index, start, end),\n                                  DaysToCloseMedian(index, start, end)],\n        \"time_to_close_review_metrics\": [],\n        \"patchsets_metrics\": []\n    }\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef aggregations(self):\n\n        prev_month_start = get_prev_month(self.end,\n                                          self.closed.query.interval_)\n        self.closed.query.since(prev_month_start,\n                                field=\"closed_at\")\n        closed_agg = self.closed.aggregations()\n        self.opened.query.since(prev_month_start)\n        opened_agg = self.opened.aggregations()\n        if opened_agg == 0:\n            bmi = 1.0  # if no submitted issues/prs, bmi is at 100%\n        else:\n            bmi = closed_agg / opened_agg\n        return bmi", "response": "Get the aggregation value for BMI with respect to the previous\n        time interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef c_log(level, message):\n    c_level = level\n    level = LEVELS_F2PY[c_level]\n    logger.log(level, message)", "response": "Python logger to be called from fortran"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a ctypes structure to a dictionary", "response": "def struct2dict(struct):\n    \"\"\"convert a ctypes structure to a dictionary\"\"\"\n    return {x: getattr(struct, x) for x in dict(struct._fields_).keys()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef structs2records(structs):\n    try:\n        n = len(structs)\n    except TypeError:\n        # no array\n        yield struct2dict(structs)\n        # just 1\n        return\n    for i in range(n):\n        struct = structs[i]\n        yield struct2dict(struct)", "response": "convert one or more structs and generate dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef structs2pandas(structs):\n    try:\n        import pandas\n        records = list(structs2records(structs))\n        df = pandas.DataFrame.from_records(records)\n        # TODO: do this for string columns, for now just for id\n        # How can we check for string columns, this is not nice:\n        # df.columns[df.dtypes == object]\n        if 'id' in df:\n            df[\"id\"] = df[\"id\"].apply(str.rstrip)\n        return df\n    except ImportError:\n        # pandas not found, that's ok        \n        return structs", "response": "convert ctypes structure or structure array to pandas data frame"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap a function with type conversion and sanity checks.", "response": "def wrap(func):\n    \"\"\"Return wrapped function with type conversion and sanity checks.\n    \"\"\"\n    @functools.wraps(func, assigned=('restype', 'argtypes'))\n    def wrapped(*args):\n        if len(args) != len(func.argtypes):\n            logger.warn(\"{} {} not of same length\",\n                        args, func.argtypes)\n\n        typed_args = []\n        for (arg, argtype) in zip(args, func.argtypes):\n            if argtype == c_char_p:\n                # create a string buffer for strings\n                typed_arg = create_string_buffer(arg)\n            else:\n                # for other types, use the type to do the conversion\n                if hasattr(argtype, 'contents'):\n                    # type is a pointer\n                    typed_arg = argtype(argtype._type_(arg))\n                else:\n                    typed_arg = argtype(arg)\n            typed_args.append(typed_arg)\n        result = func(*typed_args)\n        if hasattr(result, 'contents'):\n            return result.contents\n        else:\n            return result\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn platform - specific modelf90 shared library name.", "response": "def _libname(self):\n        \"\"\"Return platform-specific modelf90 shared library name.\"\"\"\n        prefix = 'lib'\n        suffix = '.so'\n        if platform.system() == 'Darwin':\n            suffix = '.dylib'\n        if platform.system() == 'Windows':\n            prefix = ''\n            suffix = '.dll'\n        return prefix + self.engine + suffix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning full path to the shared library.", "response": "def _library_path(self):\n        \"\"\"Return full path to the shared library.\n\n        A couple of regular unix paths like ``/usr/lib/`` is searched by\n        default. If your library is not in one of those, set a\n        ``LD_LIBRARY_PATH`` environment variable to the directory with your\n        shared library.\n\n        If the library cannot be found, a ``RuntimeError`` with debug\n        information is raised.\n        \"\"\"\n\n        # engine is an existing library name\n        # TODO change add directory to library path\n        if os.path.isfile(self.engine):\n            return self.engine\n\n        pathname = 'LD_LIBRARY_PATH'\n        separator = ':'\n        if platform.system() == 'Darwin':\n            pathname = 'DYLD_LIBRARY_PATH'\n            separator = ':'\n        if platform.system() == 'Windows':\n            # windows does not separate between dll path's and exe paths\n            pathname = 'PATH'\n            separator = ';'\n\n        lib_path_from_environment = os.environ.get(pathname, '')\n        # Expand the paths with the system path if it exists\n        if lib_path_from_environment:\n            known_paths = [\n                path for path in lib_path_from_environment.split(separator)] + self.known_paths\n        else:\n            known_paths = self.known_paths\n        # expand ~\n        known_paths = [os.path.expanduser(path) for path in known_paths]\n\n        possible_libraries = [os.path.join(path, self._libname())\n                              for path in known_paths]\n        for library in possible_libraries:\n            if os.path.exists(library):\n                logger.info(\"Using model fortran library %s\", library)\n                return library\n        msg = \"Library not found, looked in %s\" % ', '.join(possible_libraries)\n        raise RuntimeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the fortran library and return the loaded object", "response": "def _load_library(self):\n        \"\"\"Return the fortran library, loaded with \"\"\"\n        path = self._library_path()\n        logger.info(\"Loading library from path {}\".format(path))\n        library_dir = os.path.dirname(path)\n        if platform.system() == 'Windows':\n            import win32api\n            olddir = os.getcwd()\n            os.chdir(library_dir)\n            win32api.SetDllDirectory('.')\n\n        result = cdll.LoadLibrary(path)\n\n        if platform.system() == 'Windows':\n            os.chdir(olddir)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize(self, configfile=None):\n\n        if configfile is not None:\n            self.configfile = configfile\n        try:\n            self.configfile\n        except AttributeError:\n            raise ValueError(\"Specify configfile during construction or during initialize\")\n        abs_name = os.path.abspath(self.configfile)\n        os.chdir(os.path.dirname(self.configfile) or '.')\n        logmsg = \"Loading model {} in directory {}\".format(\n            self.configfile,\n            os.path.abspath(os.getcwd())\n        )\n        logger.info(logmsg)\n        # Fortran init function.\n        self.library.initialize.argtypes = [c_char_p]\n        self.library.initialize.restype = None\n        # initialize by abs_name because we already chdirred\n        # if configfile is a relative path  we would have a problem\n        ierr = wrap(self.library.initialize)(abs_name)\n        if ierr:\n            errormsg = \"Loading model {config} failed with exit code {code}\"\n            raise RuntimeError(errormsg.format(config=self.configfile,\n                                               code=ierr))", "response": "Initialize and load the Fortran library and model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn type string compatible with numpy.", "response": "def update(self, dt=-1):\n        \"\"\"\n        Return type string, compatible with numpy.\n        \"\"\"\n        self.library.update.argtypes = [c_double]\n        self.library.update.restype = c_int\n        if dt == -1:\n            # use default timestep\n            dt = self.get_time_step()\n        result = wrap(self.library.update)(dt)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the number of variables in the current locale.", "response": "def get_var_count(self):\n        \"\"\"\n        Return number of variables\n        \"\"\"\n        n = c_int()\n        self.library.get_var_count.argtypes = [POINTER(c_int)]\n        self.library.get_var_count(byref(n))\n        return n.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets variable name of the named entry.", "response": "def get_var_name(self, i):\n        \"\"\"\n        Return variable name\n        \"\"\"\n        i = c_int(i)\n        name = create_string_buffer(MAXSTRLEN)\n        self.library.get_var_name.argtypes = [c_int, c_char_p]\n        self.library.get_var_name(i, name)\n        return name.value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_var_type(self, name):\n        name = create_string_buffer(name)\n        type_ = create_string_buffer(MAXSTRLEN)\n        self.library.get_var_type.argtypes = [c_char_p, c_char_p]\n        self.library.get_var_type(name, type_)\n        return type_.value", "response": "Get the type of a variable in the current library."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inq_compound(self, name):\n        name = create_string_buffer(name)\n        self.library.inq_compound.argtypes = [c_char_p, POINTER(c_int)]\n        self.library.inq_compound.restype = None\n        nfields = c_int()\n        self.library.inq_compound(name, byref(nfields))\n        return nfields.value", "response": "Return the number of fields and size of a compound type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inq_compound_field(self, name, index):\n        typename = create_string_buffer(name)\n        index = c_int(index + 1)\n        fieldname = create_string_buffer(MAXSTRLEN)\n        fieldtype = create_string_buffer(MAXSTRLEN)\n        rank = c_int()\n        arraytype = ndpointer(dtype='int32',\n                              ndim=1,\n                              shape=(MAXDIMS, ),\n                              flags='F')\n        shape = np.empty((MAXDIMS, ), dtype='int32', order='F')\n        self.library.inq_compound_field.argtypes = [c_char_p,\n                                                    POINTER(c_int),\n                                                    c_char_p,\n                                                    c_char_p,\n                                                    POINTER(c_int),\n                                                    arraytype]\n        self.library.inq_compound_field.restype = None\n        self.library.inq_compound_field(typename,\n                                        byref(index),\n                                        fieldname,\n                                        fieldtype,\n                                        byref(rank),\n                                        shape)\n        return (fieldname.value,\n                fieldtype.value,\n                rank.value,\n                tuple(shape[:rank.value]))", "response": "Lookup the type rank and shape of a compound field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_compound_ctype(self, varname):\n\n        # look up the type name\n        compoundname = self.get_var_type(varname)\n        nfields = self.inq_compound(compoundname)\n        # for all the fields look up the type, rank and shape\n        fields = []\n        for i in range(nfields):\n            (fieldname, fieldtype,\n             fieldrank, fieldshape) = self.inq_compound_field(compoundname, i)\n            assert fieldrank <= 1\n            fieldctype = CTYPESMAP[fieldtype]\n            if fieldrank == 1:\n                fieldctype = fieldctype * fieldshape[0]\n            fields.append((fieldname, fieldctype))\n        # create a new structure\n\n        class COMPOUND(Structure):\n            _fields_ = fields\n\n        # if we have a rank 1 array, create an array\n        rank = self.get_var_rank(varname)\n        assert rank <= 1, \"we can't handle >=2 dimensional compounds yet\"\n        if rank == 1:\n            shape = self.get_var_shape(varname)\n            valtype = POINTER(ARRAY(COMPOUND, shape[0]))\n        else:\n            valtype = POINTER(COMPOUND)\n        # return the custom type\n        return valtype", "response": "Create a ctypes type that corresponds to a compound type in memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets rank of a variable from the cache.", "response": "def get_var_rank(self, name):\n        \"\"\"\n        Return array rank or 0 for scalar.\n        \"\"\"\n        name = create_string_buffer(name)\n        rank = c_int()\n        self.library.get_var_rank.argtypes = [c_char_p, POINTER(c_int)]\n        self.library.get_var_rank.restype = None\n        self.library.get_var_rank(name, byref(rank))\n        return rank.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the shape of the array.", "response": "def get_var_shape(self, name):\n        \"\"\"\n        Return shape of the array.\n        \"\"\"\n        rank = self.get_var_rank(name)\n        name = create_string_buffer(name)\n        arraytype = ndpointer(dtype='int32',\n                              ndim=1,\n                              shape=(MAXDIMS, ),\n                              flags='F')\n        shape = np.empty((MAXDIMS, ), dtype='int32', order='F')\n        self.library.get_var_shape.argtypes = [c_char_p, arraytype]\n        self.library.get_var_shape(name, shape)\n        return tuple(shape[:rank])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_start_time(self):\n        start_time = c_double()\n        self.library.get_start_time.argtypes = [POINTER(c_double)]\n        self.library.get_start_time.restype = None\n        self.library.get_start_time(byref(start_time))\n        return start_time.value", "response": "returns the start time of the current log entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the end time of simulation", "response": "def get_end_time(self):\n        \"\"\"\n        returns end time of simulation\n        \"\"\"\n        end_time = c_double()\n        self.library.get_end_time.argtypes = [POINTER(c_double)]\n        self.library.get_end_time.restype = None\n        self.library.get_end_time(byref(end_time))\n        return end_time.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn current time of simulation", "response": "def get_current_time(self):\n        \"\"\"\n        returns current time of simulation\n        \"\"\"\n        current_time = c_double()\n        self.library.get_current_time.argtypes = [POINTER(c_double)]\n        self.library.get_current_time.restype = None\n        self.library.get_current_time(byref(current_time))\n        return current_time.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_time_step(self):\n        time_step = c_double()\n        self.library.get_time_step.argtypes = [POINTER(c_double)]\n        self.library.get_time_step.restype = None\n        self.library.get_time_step(byref(time_step))\n        return time_step.value", "response": "returns current time step of simulation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_var(self, name):\n        # How many dimensions.\n        rank = self.get_var_rank(name)\n        # The shape array is fixed size\n        shape = np.empty((MAXDIMS, ), dtype='int32', order='F')\n        shape = self.get_var_shape(name)\n        # there should be nothing here...\n        assert sum(shape[rank:]) == 0\n        # variable type name\n        type_ = self.get_var_type(name)\n\n        is_numpytype = type_ in TYPEMAP\n\n        if is_numpytype:\n            # Store the data in this type\n            arraytype = ndpointer(dtype=TYPEMAP[type_],\n                                  ndim=rank,\n                                  shape=shape,\n                                  flags='F')\n        # '' or b''\n        elif not type_:\n            raise ValueError('type not found for variable {}'.format(name))\n        else:\n            arraytype = self.make_compound_ctype(name)\n        # Create a pointer to the array type\n        data = arraytype()\n        # The functions get_var_type/_shape/_rank are already wrapped with\n        # python function converter, get_var isn't.\n        c_name = create_string_buffer(name)\n        get_var = self.library.get_var\n        get_var.argtypes = [c_char_p, POINTER(arraytype)]\n        get_var.restype = None\n        # Get the array\n        get_var(c_name, byref(data))\n        if not data:\n            logger.info(\"NULL pointer returned\")\n            return None\n\n        if is_numpytype:\n            # for now always a pointer, see python-subgrid for advanced examples\n            array = np.ctypeslib.as_array(data)\n        else:\n            array = structs2pandas(data.contents)\n\n        return array", "response": "Return an nd array from model library"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_logger(self, logger):\n\n        # we don't expect anything back\n        try:\n            self.library.set_logger.restype = None\n        except AttributeError:\n            logger.warn(\"Tried to set logger but method is not implemented in %s\", self.engine)\n            return\n        # as an argument we need a pointer to a fortran log func...\n        self.library.set_logger.argtypes = [\n            (fortran_log_functype)]\n\n        self.library.set_logger(fortran_log_func)", "response": "subscribe to fortran log messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the current time of simulation.", "response": "def set_current_time(self, current_time):\n        \"\"\"\n        sets current time of simulation\n        \"\"\"\n        current_time = c_double(current_time)\n        try:\n            self.library.set_current_time.argtypes = [POINTER(c_double)]\n            self.library.set_current_time.restype = None\n            self.library.set_current_time(byref(current_time))\n        except AttributeError:\n            logger.warn(\"Tried to set current time but method is not implemented in %s\", self.engine)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_book(body):\n    '''\n    This does not only accept/refuse a book. It also returns an ENHANCED\n    version of body, with (mostly fts-related) additional fields.\n\n    This function is idempotent.\n    '''\n    if '_language' not in body:\n        raise ValueError('language needed')\n    if len(body['_language']) > 2:\n        raise ValueError('invalid language: %s' % body['_language'])\n\n    # remove old _text_* fields\n    for k in body.keys():\n        if k.startswith('_text'):\n            del(body[k])\n\n    allfields = collectStrings(body)\n    body['_text_%s' % body['_language']] = ' '.join(allfields)\n    return body", "response": "This function is idempotent and returns a body dictionary that can be used to create a new ENHANCED\n   . It also returns an ENHANCED\n    version of body with additional fields added to the _text_ * field."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and configure index and configure cluster", "response": "def setup_db(self, wait_for_ready=True):\n        ''' Create and configure index\n\n            If `wait_for_ready` is True, this function will block until\n            status for `self.index_name` will be `yellow`\n        '''\n\n        if self.es.indices.exists(self.index_name):\n            try:\n                self.update_mappings()\n            except MappingsException as ex:\n                log.error(ex)\n                log.warn('An old or wrong properties mapping has been found for index: \"{0}\",\\\n                          this could led to some errors. It is recomanded to run \"libreant-db upgrade\"'.format(self.index_name))\n        else:\n            log.debug(\"Index is missing: '{0}'\".format(self.index_name))\n            self.create_index()\n\n        if wait_for_ready:\n            log.debug('waiting for index \"{}\" to be ready'.format(self.index_name))\n            self.es.cluster.health(index=self.index_name, level='index', wait_for_status='yellow')\n            log.debug('index \"{}\" is now ready'.format(self.index_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the index with given configuration.", "response": "def create_index(self, indexname=None, index_conf=None):\n        ''' Create the index\n\n            Create the index with given configuration.\n            If `indexname` is provided it will be used as the new index name\n            instead of the class one (:py:attr:`DB.index_name`)\n\n            :param index_conf: configuration to be used in index creation. If this\n                              is not specified the default index configuration will be used.\n            :raises Exception: if the index already exists.\n        '''\n        if indexname is None:\n            indexname = self.index_name\n        log.debug(\"Creating new index: '{0}'\".format(indexname))\n        if index_conf is None:\n            index_conf = {'settings': self.settings,\n                          'mappings': {'book': {'properties': self.properties}}}\n        try:\n            self.es.indices.create(index=indexname, body=index_conf)\n        except TransportError as te:\n            if te.error.startswith(\"IndexAlreadyExistsException\"):\n                raise Exception(\"Cannot create index '{}', already exists\".format(indexname))\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclones current index into new index", "response": "def clone_index(self, new_indexname, index_conf=None):\n        '''Clone current index\n\n           All entries of the current index will be copied into the newly\n           created one named `new_indexname`\n\n           :param index_conf: Configuration to be used in the new index creation.\n                              This param will be passed directly to :py:func:`DB.create_index`\n        '''\n        log.debug(\"Cloning index '{}' into '{}'\".format(self.index_name, new_indexname))\n        self.create_index(indexname=new_indexname, index_conf=index_conf)\n        reindex(self.es, self.index_name, new_indexname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reindex(self, new_index=None, index_conf=None):\n        '''Rebuilt the current index\n\n           This function could be useful in the case you want to change some index settings/mappings\n           and you don't want to loose all the entries belonging to that index.\n\n           This function is built in such a way that you can continue to use the old index name,\n           this is achieved using index aliases.\n\n           The old index will be cloned into a new one with the given `index_conf`.\n           If we are working on an alias, it is redirected to the new index.\n           Otherwise a brand new alias with the old index name is created in such a way that\n           points to the newly create index.\n\n           Keep in mind that even if you can continue to use the same index name,\n           the old index will be deleted.\n\n           :param index_conf: Configuration to be used in the new index creation.\n                              This param will be passed directly to :py:func:`DB.create_index`\n        '''\n\n        alias = self.index_name if self.es.indices.exists_alias(name=self.index_name) else None\n        if alias:\n            original_index=self.es.indices.get_alias(self.index_name).popitem()[0]\n        else:\n            original_index=self.index_name\n\n        if new_index is None:\n            mtc = re.match(r\"^.*_v(\\d)*$\", original_index)\n            if mtc:\n                new_index = original_index[:mtc.start(1)] + str(int(mtc.group(1)) + 1)\n            else:\n                new_index = original_index + '_v1'\n\n        log.debug(\"Reindexing {{ alias: '{}', original_index: '{}', new_index: '{}'}}\".format(alias, original_index, new_index))\n        self.clone_index(new_index, index_conf=index_conf)\n\n        if alias:\n            log.debug(\"Moving alias from ['{0}' -> '{1}'] to ['{0}' -> '{2}']\".format(alias, original_index, new_index))\n            self.es.indices.update_aliases(body={\n                \"actions\" : [\n                    { \"remove\" : { \"alias\": alias, \"index\" : original_index} },\n                    { \"add\" : { \"alias\": alias, \"index\" : new_index } }\n                ]})\n\n        log.debug(\"Deleting old index: '{}'\".format(original_index))\n        self.es.indices.delete(original_index)\n\n        if not alias:\n            log.debug(\"Crating new alias: ['{0}' -> '{1}']\".format(original_index, new_index))\n            self.es.indices.update_aliases(body={\n                \"actions\" : [\n                    { \"add\" : { \"alias\": original_index, \"index\" : new_index } }\n                ]})", "response": "Rebuild the current index and clone it if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mlt(self, _id):\n        '''\n        High-level method to do \"more like this\".\n\n        Its exact implementation can vary.\n        '''\n        query = {\n            'query': {'more_like_this': {\n                      'like': {'_id': _id},\n                      'min_term_freq': 1,\n                      'min_doc_freq': 1,\n             }}\n        }\n        if es_version[0] <= 1:\n            mlt = query['query']['more_like_this']\n            mlt['ids'] = [_id]\n            del mlt['like']\n        return self._search(query)", "response": "High - level method to do more like this."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning true if at least one book has file with the given url as attachment", "response": "def file_is_attached(self, url):\n        '''return true if at least one book has\n           file with the given url as attachment\n        '''\n        body = self._get_search_field('_attachments.url', url)\n        return self.es.count(index=self.index_name, body=body)['count'] > 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_book(self, body, doc_type='book'):\n        '''\n        Call it like this:\n            db.add_book(doc_type='book',\n            body={'title': 'foobar', '_language': 'it'})\n        '''\n        body = validate_book(body)\n        body['_insertion_date'] = current_time_millisec()\n        return self.es.index(index=self.index_name, doc_type=doc_type, body=body)", "response": "Add a book to the index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_all(self):\n        '''Delete all books from the index'''\n        def delete_action_gen():\n            scanner = scan(self.es,\n                           index=self.index_name,\n                           query={'query': {'match_all':{}}})\n            for v in scanner:\n                yield { '_op_type': 'delete',\n                        '_index': self.index_name,\n                        '_type': v['_type'],\n                        '_id': v['_id'],\n                      }\n        bulk(self.es, delete_action_gen())", "response": "Delete all books from the index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_book(self, id, body, doc_type='book'):\n        ''' Update a book\n\n            The \"body\" is merged with the current one.\n            Yes, it is NOT overwritten.\n\n            In case of concurrency conflict\n            this function could raise `elasticsearch.ConflictError`\n        '''\n        # note that we are NOT overwriting all the _source, just merging\n        book = self.get_book_by_id(id)\n        book['_source'].update(body)\n        validated = validate_book(book['_source'])\n        ret = self.es.index(index=self.index_name, id=id,\n                            doc_type=doc_type, body=validated, version=book['_version'])\n        return ret", "response": "Update a book by id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modify_book(self, id, body, doc_type='book', version=None):\n        ''' replace the entire book body\n\n            Instead of `update_book` this function\n            will overwrite the book content with param body\n\n            If param `version` is given, it will be checked that the\n            changes are applied upon that document version.\n            If the document version provided is different from the one actually found,\n            an `elasticsearch.ConflictError` will be raised\n        '''\n        validatedBody = validate_book(body)\n        params = dict(index=self.index_name, id=id, doc_type=doc_type, body=validatedBody)\n        if version:\n            params['version'] = version\n        ret = self.es.index(**params)\n        return ret", "response": "replace the entire book body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the download count of a specific file", "response": "def increment_download_count(self, id, attachmentID, doc_type='book'):\n        '''\n        Increment the download counter of a specific file\n        '''\n        body = self.es.get(index=self.index_name, id=id, doc_type='book', _source_include='_attachments')['_source']\n\n        for attachment in body['_attachments']:\n            if attachment['id'] == attachmentID:\n                attachment['download_count'] += 1\n                self.es.update(index=self.index_name,\n                               id=id,\n                               doc_type=doc_type,\n                               body={\"doc\": {'_attachments': body['_attachments']}})\n                return\n        raise NotFoundError(\"No attachment could be found with id: {}\".format(attachmentID))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine all the possible config params for the current class", "response": "def general_params(cls):\n        \"\"\" Define all the possible config params \"\"\"\n\n        optional_bool_none = {\n            \"optional\": True,\n            \"default\": None,\n            \"type\": bool\n        }\n        optional_string_none = {\n            \"optional\": True,\n            \"default\": None,\n            \"type\": str\n        }\n        optional_int_none = {\n            \"optional\": True,\n            \"default\": None,\n            \"type\": int\n        }\n        optional_empty_list = {\n            \"optional\": True,\n            \"default\": [],\n            \"type\": list\n        }\n        no_optional_empty_string = {\n            \"optional\": False,\n            \"default\": \"\",\n            \"type\": str\n        }\n        no_optional_true = {\n            \"optional\": False,\n            \"default\": True,\n            \"type\": bool\n        }\n        optional_false = {\n            \"optional\": True,\n            \"default\": False,\n            \"type\": bool\n        }\n\n        params = {}\n\n        # GENERAL CONFIG\n        params_general = {\n            \"general\": {\n                \"sleep\": optional_int_none,  # we are not using it\n                \"min_update_delay\": {\n                    \"optional\": True,\n                    \"default\": 60,\n                    \"type\": int\n                },\n                \"kibana\": {\n                    \"optional\": True,\n                    \"default\": \"5\",\n                    \"type\": str\n                },\n                \"update\": {\n                    \"optional\": False,\n                    \"default\": False,\n                    \"type\": bool\n                },\n                \"short_name\": {\n                    \"optional\": False,\n                    \"default\": \"Short name\",\n                    \"type\": str\n                },\n                \"debug\": {\n                    \"optional\": False,\n                    \"default\": True,\n                    \"type\": bool\n                },\n                \"from_date\": optional_string_none,  # per data source param now\n                \"logs_dir\": {\n                    \"optional\": False,\n                    \"default\": \"logs\",\n                    \"type\": str\n                },\n                \"bulk_size\": {\n                    \"optional\": True,\n                    \"default\": 1000,\n                    \"type\": int\n                },\n                \"scroll_size\": {\n                    \"optional\": True,\n                    \"default\": 100,\n                    \"type\": int\n                }\n\n            }\n        }\n        params_projects = {\n            \"projects\": {\n                \"projects_file\": {\n                    \"optional\": True,\n                    \"default\": \"projects.json\",\n                    \"type\": str\n                },\n                \"projects_url\": {\n                    \"optional\": True,\n                    \"default\": None,\n                    \"type\": str\n                },\n                \"load_eclipse\": {\n                    \"optional\": True,\n                    \"default\": False,\n                    \"type\": bool\n                }\n            }\n        }\n\n        params_phases = {\n            \"phases\": {\n                \"collection\": no_optional_true,\n                \"enrichment\": no_optional_true,\n                \"identities\": no_optional_true,\n                \"panels\": no_optional_true,\n                \"track_items\": optional_false,\n                \"report\": optional_false\n            }\n        }\n\n        general_config_params = [params_general, params_projects, params_phases]\n\n        for section_params in general_config_params:\n            params.update(section_params)\n\n        # Config provided by tasks\n        params_collection = {\n            \"es_collection\": {\n                \"password\": optional_string_none,\n                \"user\": optional_string_none,\n                \"url\": {\n                    \"optional\": False,\n                    \"default\": \"http://172.17.0.1:9200\",\n                    \"type\": str\n                },\n                \"arthur\": optional_false,\n                \"arthur_url\": optional_string_none,\n                \"redis_url\": optional_string_none\n            }\n        }\n\n        params_enrichment = {\n            \"es_enrichment\": {\n                \"url\": {\n                    \"optional\": False,\n                    \"default\": \"http://172.17.0.1:9200\",\n                    \"type\": str\n                },\n                \"studies\": optional_bool_none,\n                \"autorefresh\": {\n                    \"optional\": True,\n                    \"default\": True,\n                    \"type\": bool\n                },\n                \"user\": optional_string_none,\n                \"password\": optional_string_none\n            }\n        }\n\n        params_panels = {\n            \"panels\": {\n                \"kibiter_time_from\": {\n                    \"optional\": True,\n                    \"default\": \"now-90d\",\n                    \"type\": str\n                },\n                \"kibiter_default_index\": {\n                    \"optional\": True,\n                    \"default\": \"git\",\n                    \"type\": str\n                }\n            }\n        }\n\n        params_report = {\n            \"report\": {\n                \"start_date\": {\n                    \"optional\": False,\n                    \"default\": \"1970-01-01\",\n                    \"type\": str\n                },\n                \"end_date\": {\n                    \"optional\": False,\n                    \"default\": \"2100-01-01\",\n                    \"type\": str\n                },\n                \"interval\": {\n                    \"optional\": False,\n                    \"default\": \"quarter\",\n                    \"type\": str\n                },\n                \"config_file\": {\n                    \"optional\": False,\n                    \"default\": \"report.cfg\",\n                    \"type\": str\n                },\n                \"data_dir\": {\n                    \"optional\": False,\n                    \"default\": \"report_data\",\n                    \"type\": str\n                },\n                \"filters\": optional_empty_list,\n                \"offset\": optional_string_none\n            }\n        }\n\n        params_sortinghat = {\n            \"sortinghat\": {\n                \"affiliate\": {\n                    \"optional\": False,\n                    \"default\": \"True\",\n                    \"type\": bool\n                },\n                \"unaffiliated_group\": {\n                    \"optional\": False,\n                    \"default\": \"Unknown\",\n                    \"type\": str\n                },\n                \"unify_method\": {  # not used\n                    \"optional\": True,\n                    \"default\": \"fast-matching\",\n                    \"type\": str\n                },\n                \"matching\": {\n                    \"optional\": False,\n                    \"default\": [\"email\"],\n                    \"type\": list\n                },\n                \"sleep_for\": {\n                    \"optional\": False,\n                    \"default\": 3600,\n                    \"type\": int\n                },\n                \"database\": {\n                    \"optional\": False,\n                    \"default\": \"sortinghat_db\",\n                    \"type\": str\n                },\n                \"host\": {\n                    \"optional\": False,\n                    \"default\": \"mariadb\",\n                    \"type\": str\n                },\n                \"user\": {\n                    \"optional\": False,\n                    \"default\": \"root\",\n                    \"type\": str\n                },\n                \"password\": no_optional_empty_string,\n                \"autoprofile\": {\n                    \"optional\": False,\n                    \"default\": [\"customer\", \"git\", \"github\"],\n                    \"type\": list\n                },\n                \"load_orgs\": {\n                    \"optional\": True,\n                    \"default\": False,\n                    \"type\": bool,\n                    \"deprecated\": \"Orgs are loaded if defined always\"\n                },\n                \"identities_format\": {\n                    \"optional\": True,\n                    \"default\": \"sortinghat\",\n                    \"type\": str,\n                    \"doc\": \"Format of the identities data to be loaded\"\n                },\n                \"github_api_token\": {\n                    \"optional\": True,\n                    \"default\": None,\n                    \"type\": str,\n                    \"deprecated\": \"Use identities_api_token\"\n                },\n                \"strict_mapping\": {\n                    \"optional\": True,\n                    \"default\": True,\n                    \"type\": bool,\n                    \"doc\": \"rigorous check of values in identities matching \" + \\\n                           \"(i.e, well formed email addresses)\"\n                },\n                \"reset_on_load\": {\n                    \"optional\": True,\n                    \"default\": False,\n                    \"type\": bool,\n                    \"doc\": \"Unmerge and remove affiliations for all identities on load\"\n                },\n                \"orgs_file\": optional_string_none,\n                \"identities_file\": optional_empty_list,\n                \"identities_export_url\": optional_string_none,\n                \"identities_api_token\": optional_string_none,\n                \"bots_names\": optional_empty_list,\n                \"no_bots_names\": optional_empty_list  # to clean bots in SH\n            }\n        }\n\n        params_track_items = {\n            \"track_items\": {\n                \"project\": {\n                    \"optional\": False,\n                    \"default\": \"TrackProject\",\n                    \"type\": str\n                },\n                \"upstream_raw_es_url\": no_optional_empty_string,\n                \"raw_index_gerrit\": no_optional_empty_string,\n                \"raw_index_git\": no_optional_empty_string\n            }\n        }\n\n        params_enrich_onion = {\n            \"enrich_onion\": {\n                \"in_index\": {\n                    \"optional\": False,\n                    \"default\": None,\n                    \"type\": str\n                },\n                \"out_index\": {\n                    \"optional\": False,\n                    \"default\": None,\n                    \"type\": str\n                },\n                \"contribs_field\": {\n                    \"optional\": True,\n                    \"default\": \"hash\",\n                    \"type\": str\n                },\n                \"no_incremental\": {\n                    \"optional\": True,\n                    \"default\": True,\n                    \"type\": bool\n                }\n            }\n        }\n\n        params_enrich_areas_of_code = {\n            \"enrich_areas_of_code\": {\n                \"in_index\": {\n                    \"optional\": False,\n                    \"default\": None,\n                    \"type\": str\n                },\n                \"out_index\": {\n                    \"optional\": False,\n                    \"default\": None,\n                    \"type\": str\n                },\n                \"no_incremental\": {\n                    \"optional\": True,\n                    \"default\": True,\n                    \"type\": bool\n                }\n            }\n        }\n\n        params_enrich_demography = {\n            \"enrich_demography\": {\n                \"no_incremental\": {\n                    \"optional\": True,\n                    \"default\": True,\n                    \"type\": bool\n                }\n            }\n        }\n\n        tasks_config_params = [params_collection, params_enrichment, params_panels,\n                               params_report, params_sortinghat, params_track_items,\n                               params_enrich_areas_of_code, params_enrich_demography,\n                               params_enrich_onion]\n\n        for section_params in tasks_config_params:\n            params.update(section_params)\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting to int boolean list None types config items", "response": "def __add_types(self, raw_conf):\n        \"\"\" Convert to int, boolean, list, None types config items \"\"\"\n\n        typed_conf = {}\n\n        for s in raw_conf.keys():\n            typed_conf[s] = {}\n            for option in raw_conf[s]:\n                val = raw_conf[s][option]\n                if len(val) > 1 and (val[0] == '\"' and val[-1] == '\"'):\n                    # It is a string\n                    typed_conf[s][option] = val[1:-1]\n                # Check list\n                elif len(val) > 1 and (val[0] == '[' and val[-1] == ']'):\n                    # List value\n                    typed_conf[s][option] = val[1:-1].replace(' ', '').split(',')\n                # Check boolean\n                elif val.lower() in ['true', 'false']:\n                    typed_conf[s][option] = True if val.lower() == 'true' else False\n                # Check None\n                elif val.lower() is 'none':\n                    typed_conf[s][option] = None\n                else:\n                    try:\n                        # Check int\n                        typed_conf[s][option] = int(val)\n                    except ValueError:\n                        # Is a string\n                        typed_conf[s][option] = val\n        return typed_conf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Elasticsearch(*args, **kwargs):\n    check_version = kwargs.pop('check_version', True)\n    es = Elasticsearch_official(*args, **kwargs)\n    if check_version:\n        es_version = es.info()['version']['number'].split('.')\n        if(int(es_version[0]) != int(es_pylib_version[0])):\n            raise RuntimeError(\"The Elasticsearch python library version does not match the one of the running cluster: {} != {}. Please install the correct elasticsearch-py version\".format(es_pylib_version[0], es_version[0]))\n    return es", "response": "Wrapper function around the Elasticsearch class that adds a simple version check upon initialization."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_envvars(prefix=None, environ=None, envvars=None, as_json=True):\n    conf = {}\n    if environ is None:\n        environ = os.environ\n    if prefix is None and envvars is None:\n        raise RuntimeError('Must either give prefix or envvars argument')\n\n    # if it's a list, convert to dict\n    if isinstance(envvars, list):\n        envvars = {k: k for k in envvars}\n\n    if not envvars:\n        envvars = {k: k[len(prefix):] for k in environ.keys()\n                   if k.startswith(prefix)}\n\n    for env_name, name in envvars.items():\n        if env_name not in environ:\n            continue\n\n        if as_json:\n            try:\n                conf[name] = json.loads(environ[env_name])\n            except ValueError:\n                conf[name] = environ[env_name]\n        else:\n            conf[name] = environ[env_name]\n\n    return conf", "response": "Load environment variables from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_configs(envvar_prefix, path=None):\n    '''Load configuration\n\n    The following steps will be undertake:\n        * It will attempt to load configs from file:\n          if `path` is provided, it will be used, otherwise the path\n          will be taken from envvar `envvar_prefix` + \"SETTINGS\".\n        * all envvars starting with `envvar_prefix` will be loaded.\n\n    '''\n    conf = {}\n    if path:\n        conf.update(from_file(path))\n    else:\n        conf.update(from_envvar_file(envvar_prefix + \"SETTINGS\"))\n    conf.update(from_envvars(prefix=envvar_prefix))\n    return conf", "response": "Load configuration from file or envvar_prefix + \"SETTINGS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses the values returned by get_timeseries(), compare the current Metric value with it's previous period's value :param timeseries: data returned from the get_timeseries() method :returns: the last period value and relative change", "response": "def get_trend(timeseries):\n    \"\"\"\n    Using the values returned by get_timeseries(), compare the current\n    Metric value with it's previous period's value\n\n    :param timeseries: data returned from the get_timeseries() method\n    :returns: the last period value and relative change\n    \"\"\"\n\n    last = timeseries['value'][len(timeseries['value']) - 1]\n    prev = timeseries['value'][len(timeseries['value']) - 2]\n    trend = last - prev\n    trend_percentage = None\n\n    if last == 0:\n        if prev > 0:\n            trend_percentage = -100\n        else:\n            trend_percentage = 0\n    else:\n        trend_percentage = int((trend / last) * 100)\n    return (last, trend_percentage)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_bmi(closed, submitted):\n\n    if sorted(closed.keys()) != sorted(submitted.keys()):\n        raise AttributeError(\"The buckets supplied are not congruent!\")\n\n    dates = closed.index.values\n    closed_values = closed['value']\n    submitted_values = submitted['value']\n    ratios = []\n    for x, y in zip(closed_values, submitted_values):\n        if y == 0:\n            ratios.append(0.0)\n        else:\n            ratios.append(float(\"%.2f\" % (x / y)))\n\n    df = pd.DataFrame.from_records({\"date\": dates, \"bmi\": ratios}, index=\"date\")\n    return df.fillna(0)", "response": "Calculates the BMI of the items in the order of the items in the closed and submitted buckets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef buckets_to_df(buckets):\n    cleaned_buckets = []\n    for item in buckets:\n        if type(item) == str:\n            return item\n\n        temp = {}\n        for key, val in item.items():\n            try:\n                temp[key] = val['value']\n            except Exception as e:\n                temp[key] = val\n\n        cleaned_buckets.append(temp)\n\n    if \"key_as_string\" in temp.keys():\n        ret_df = pd.DataFrame.from_records(cleaned_buckets)\n        ret_df = ret_df.rename(columns={\"key\": \"date_in_seconds\"})\n        ret_df['key'] = pd.to_datetime(ret_df['key_as_string'])\n        ret_df = ret_df.drop([\"key_as_string\", \"doc_count\"], axis=1)\n        ret_df = ret_df.set_index(\"key\")\n    elif \"key\" in cleaned_buckets[0].keys():\n        ret_df = pd.DataFrame.from_records(cleaned_buckets)  # index=\"key\")\n    else:\n        ret_df = pd.DataFrame(cleaned_buckets)\n\n    return ret_df.fillna(0)", "response": "Takes in aggregation buckets and converts them into a pandas dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_inverse_query(self, key_val={}):\n\n        q = Q(\"match\", **key_val)\n        self.search = self.search.query(~q)\n        return self", "response": "Adds an es_dsl inverse query object to the Search object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a sum aggregation object and add it to the aggregation dict", "response": "def get_sum(self, field=None):\n        \"\"\"\n        Create a sum aggregation object and add it to the aggregation dict\n\n        :param field: the field present in the index that is to be aggregated\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"sum\", field=field)\n        self.aggregations['sum_' + field] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_average(self, field=None):\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"avg\", field=field)\n        self.aggregations['avg_' + field] = agg\n        return self", "response": "Create an avg aggregation object and add it to the aggregation dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an aggregation object and add it to the aggregation dict", "response": "def get_percentiles(self, field=None, percents=None):\n        \"\"\"\n        Create a percentile aggregation object and add it to the aggregation dict\n\n        :param field: the field present in the index that is to be aggregated\n        :param percents: the specific percentiles to be calculated\n                         default: [1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0]\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        if not percents:\n            percents = [1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0]\n        agg = A(\"percentiles\", field=field, percents=percents)\n\n        self.aggregations['percentiles_' + field] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_terms(self, field=None):\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"terms\", field=field, size=self.size, order={\"_count\": \"desc\"})\n        self.aggregations['terms_' + field] = agg\n        return self", "response": "Create a terms aggregation object and add it to the aggregation dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new object with the minimum value of the key field", "response": "def get_min(self, field=None):\n        \"\"\"\n        Create a min aggregation object and add it to the aggregation dict\n\n        :param field: the field present in the index that is to be aggregated\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"min\", field=field)\n        self.aggregations['min_' + field] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a max aggregation object and add it to the aggregation dict", "response": "def get_max(self, field=None):\n        \"\"\"\n        Create a max aggregation object and add it to the aggregation dict\n\n        :param field: the field present in the index that is to be aggregated\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"max\", field=field)\n        self.aggregations['max_' + field] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cardinality(self, field=None):\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"cardinality\", field=field, precision_threshold=self.precision_threshold)\n        self.aggregations['cardinality_' + field] = agg\n        return self", "response": "Create a cardinality aggregation object and add it to the aggregation dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an extended_stats aggregation object and add it to the aggregation dict", "response": "def get_extended_stats(self, field=None):\n        \"\"\"\n        Create an extended_stats aggregation object and add it to the aggregation dict\n\n        :param field: the field present in the index that is to be aggregated\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            raise AttributeError(\"Please provide field to apply aggregation to!\")\n        agg = A(\"extended_stats\", field=field)\n        self.aggregations['extended_stats_' + field] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a custom aggregation to the aggregation dict", "response": "def add_custom_aggregation(self, agg, name=None):\n        \"\"\"\n        Takes in an es_dsl Aggregation object and adds it to the aggregation dict.\n        Can be used to add custom aggregations such as moving averages\n\n        :param agg: aggregation to be added to the es_dsl search object\n        :param name: name of the aggregation object (optional)\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        agg_name = name if name else 'custom_agg'\n        self.aggregations[agg_name] = agg\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the start date to the query data starting from that date", "response": "def since(self, start, field=None):\n        \"\"\"\n        Add the start date to query data starting from that date\n        sets the default start date for each query\n\n        :param start: date to start looking at the fields (from date)\n        :param field: specific field for the start date in range filter\n                      for the Search object\n        :returns: self, which allows the method to be chainable with the other methods\n        \"\"\"\n\n        if not field:\n            field = \"grimoire_creation_date\"\n        self.start_date = start\n\n        date_dict = {field: {\"gte\": \"{}\".format(self.start_date.isoformat())}}\n        self.search = self.search.filter(\"range\", **date_dict)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef until(self, end, field=None):\n\n        if not field:\n            field = \"grimoire_creation_date\"\n        self.end_date = end\n\n        date_dict = {field: {\"lte\": \"{}\".format(self.end_date.isoformat())}}\n        self.search = self.search.filter(\"range\", **date_dict)\n        return self", "response": "Add the end date to the query data upto that date"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef by_organizations(self, field=None):\n\n        # this functions is currently only for issues and PRs\n        agg_field = field if field else \"author_org_name\"\n        agg_key = \"terms_\" + agg_field\n        if agg_key in self.aggregations.keys():\n            agg = self.aggregations[agg_key]\n        else:\n            agg = A(\"terms\", field=agg_field, missing=\"others\", size=self.size)\n\n        child_agg_counter = self.child_agg_counter_dict[agg_key]  # 0 if not present because defaultdict\n        child_name, child_agg = self.aggregations.popitem()\n\n        agg.metric(child_agg_counter, child_agg)\n        self.aggregations[agg_key] = agg\n        self.child_agg_counter_dict[agg_key] += 1\n        return self", "response": "This method seggregates the data acording to organizations. This method creates a nested aggregation under the current aggregation and adds it as a nested aggregation under itself."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef by_period(self, field=None, period=None, timezone=None, start=None, end=None):\n\n        hist_period = period if period else self.interval_\n        time_zone = timezone if timezone else \"UTC\"\n\n        start_ = start if start else self.start_date\n        end_ = end if end else self.end_date\n        bounds = self.get_bounds(start_, end_)\n\n        date_field = field if field else \"grimoire_creation_date\"\n        agg_key = \"date_histogram_\" + date_field\n        if agg_key in self.aggregations.keys():\n            agg = self.aggregations[agg_key]\n        else:\n            agg = A(\"date_histogram\", field=date_field, interval=hist_period,\n                    time_zone=time_zone, min_doc_count=0, **bounds)\n\n        child_agg_counter = self.child_agg_counter_dict[agg_key]\n        child_name, child_agg = self.aggregations.popitem()\n\n        agg.metric(child_agg_counter, child_agg)\n        self.aggregations[agg_key] = agg\n        self.child_agg_counter_dict[agg_key] += 1\n        return self", "response": "Create a date histogram aggregation using the last added aggregation for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the bounds for the date_histogram method MimeType", "response": "def get_bounds(self, start=None, end=None):\n        \"\"\"\n        Get bounds for the date_histogram method\n\n        :param start: start date to set the extended_bounds min field\n        :param end: end date to set the extended_bounds max field\n        :returns bounds: a dictionary containing the min and max fields\n                         required to set the bounds in date_histogram aggregation\n        \"\"\"\n\n        bounds = {}\n        if start or end:\n            # Extend bounds so we have data until start and end\n            start_ts = None\n            end_ts = None\n\n            if start:\n                start = start.replace(microsecond=0)\n                start_ts = start.replace(tzinfo=timezone.utc).timestamp()\n                start_ts_ms = start_ts * 1000  # ES uses ms\n            if end:\n                end = end.replace(microsecond=0)\n                end_ts = end.replace(tzinfo=timezone.utc).timestamp()\n                end_ts_ms = end_ts * 1000  # ES uses ms\n\n            bounds_data = {}\n            if start:\n                bounds_data[\"min\"] = start_ts_ms\n            if end:\n                bounds_data[\"max\"] = end_ts_ms\n\n            bounds[\"extended_bounds\"] = bounds_data\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the aggregations of the current object to 0.", "response": "def reset_aggregations(self):\n        \"\"\"\n        Remove all aggregations added to the search object\n        \"\"\"\n\n        temp_search = self.search.to_dict()\n        if 'aggs' in temp_search.keys():\n            del temp_search['aggs']\n            self.search.from_dict(temp_search)\n        self.parent_agg_counter = 0\n        self.child_agg_counter = 0\n        self.child_agg_counter_dict = defaultdict(int)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery elasticsearch and returns the results from the aggregation query", "response": "def fetch_aggregation_results(self):\n        \"\"\"\n        Loops though the self.aggregations dict and adds them to the Search object\n        in order in which they were created. Queries elasticsearch and returns a dict\n        containing the results\n\n        :returns: a dictionary containing the response from elasticsearch\n        \"\"\"\n\n        self.reset_aggregations()\n\n        for key, val in self.aggregations.items():\n            self.search.aggs.bucket(self.parent_agg_counter, val)\n            self.parent_agg_counter += 1\n\n        self.search = self.search.extra(size=0)\n        response = self.search.execute()\n        self.flush_aggregations()\n        return response.to_dict()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets values for specific fields in the elasticsearch index from source", "response": "def fetch_results_from_source(self, *fields, dataframe=False):\n        \"\"\"\n        Get values for specific fields in the elasticsearch index, from source\n\n        :param fields: a list of fields that have to be retrieved from the index\n        :param dataframe: if true, will return the data in the form of a pandas.DataFrame\n        :returns: a list of dicts(key_val pairs) containing the values for the applied fields\n                  if dataframe=True, will return the a dataframe containing the data in rows\n                  and the fields representing column names\n        \"\"\"\n\n        if not fields:\n            raise AttributeError(\"Please provide the fields to get from elasticsearch!\")\n\n        self.reset_aggregations()\n\n        self.search = self.search.extra(_source=fields)\n        self.search = self.search.extra(size=self.size)\n        response = self.search.execute()\n        hits = response.to_dict()['hits']['hits']\n        data = [item[\"_source\"] for item in hits]\n\n        if dataframe:\n            df = pd.DataFrame.from_records(data)\n            return df.fillna(0)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the time series data for the specified fields and period of analysis.", "response": "def get_timeseries(self, child_agg_count=0, dataframe=False):\n        \"\"\"\n        Get time series data for the specified fields and period of analysis\n\n        :param child_agg_count: the child aggregation count to be used\n                                default = 0\n        :param dataframe: if dataframe=True, return a pandas.DataFrame object\n        :returns: dictionary containing \"date\", \"value\" and \"unixtime\" keys\n                  with lists as values containing data from each bucket in the\n                  aggregation\n        \"\"\"\n\n        res = self.fetch_aggregation_results()\n\n        ts = {\"date\": [], \"value\": [], \"unixtime\": []}\n\n        if 'buckets' not in res['aggregations'][str(self.parent_agg_counter - 1)]:\n            raise RuntimeError(\"Aggregation results have no buckets in time series results.\")\n\n        for bucket in res['aggregations'][str(self.parent_agg_counter - 1)]['buckets']:\n            ts['date'].append(parser.parse(bucket['key_as_string']).date())\n            if str(child_agg_count) in bucket:\n                # We have a subaggregation with the value\n                # If it is percentiles we get the median\n                if 'values' in bucket[str(child_agg_count)]:\n                    val = bucket[str(child_agg_count)]['values']['50.0']\n                    if val == 'NaN':\n                        # ES returns NaN. Convert to None for matplotlib graph\n                        val = None\n                    ts['value'].append(val)\n                else:\n                    ts['value'].append(bucket[str(child_agg_count)]['value'])\n            else:\n                ts['value'].append(bucket['doc_count'])\n            # unixtime comes in ms from ElasticSearch\n            ts['unixtime'].append(bucket['key'] / 1000)\n\n        if dataframe:\n            df = pd.DataFrame.from_records(ts, index=\"date\")\n            return df.fillna(0)\n        return ts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the values for single valued aggregations COOKIEID", "response": "def get_aggs(self):\n        \"\"\"\n        Compute the values for single valued aggregations\n\n        :returns: the single aggregation value\n        \"\"\"\n\n        res = self.fetch_aggregation_results()\n        if 'aggregations' in res and 'values' in res['aggregations'][str(self.parent_agg_counter - 1)]:\n            try:\n                agg = res['aggregations'][str(self.parent_agg_counter - 1)]['values'][\"50.0\"]\n                if agg == 'NaN':\n                    # ES returns NaN. Convert to None for matplotlib graph\n                    agg = None\n            except Exception as e:\n                raise RuntimeError(\"Multivalue aggregation result not supported\")\n\n        elif 'aggregations' in res and 'value' in res['aggregations'][str(self.parent_agg_counter - 1)]:\n            agg = res['aggregations'][str(self.parent_agg_counter - 1)]['value']\n\n        else:\n            agg = res['hits']['total']\n\n        return agg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the value for multi - valued aggregations", "response": "def get_list(self, dataframe=False):\n        \"\"\"\n        Compute the value for multi-valued aggregations\n\n        :returns: a dict containing 'keys' and their corresponding 'values'\n        \"\"\"\n\n        res = self.fetch_aggregation_results()\n        keys = []\n        values = []\n        for bucket in res['aggregations'][str(self.parent_agg_counter - 1)]['buckets']:\n            keys.append(bucket['key'])\n            values.append(bucket['doc_count'])\n\n        result = {\"keys\": keys, \"values\": values}\n        if dataframe:\n            result = pd.DataFrame.from_records(result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupgrade libreant database. This command can be used after an update of libreant in order to upgrade the database and make it aligned with the new version.", "response": "def upgrade(check_only, yes):\n    '''\n    Upgrade libreant database.\n\n    This command can be used after an update of libreant\n    in order to upgrade the database and make it aligned with the new version.\n    '''\n    from utils.es import Elasticsearch\n    from libreantdb import DB, migration\n    from libreantdb.exceptions import MappingsException\n\n    try:\n        db = DB(Elasticsearch(hosts=conf['ES_HOSTS']),\n                              index_name=conf['ES_INDEXNAME'])\n        if not db.es.indices.exists(db.index_name):\n            die(\"The specified index does not exists: {}\".format(db.index_name))\n\n        # Migrate old special `_timestamp` field into the new `_insertion_date`\n        num_to_update = migration.elements_without_insertion_date(db.es, db.index_name)\n        if num_to_update > 0:\n            if check_only:\n                exit(123)\n\n            if yes or click.confirm(\"{} entries miss the '_insertion_date' field. Do you want to proceed and update those entries?\".format(num_to_update),\n                             prompt_suffix='',\n                             default=False):\n                migration.migrate_timestamp(db.es, db.index_name)\n            else:\n                exit(0)\n\n        # Upgrade the index mappings and reindex if necessary\n        try:\n            db.update_mappings()\n        except MappingsException:\n            if check_only:\n                exit(123)\n            count = db.es.count(index=db.index_name)['count']\n            if yes or click.confirm(\"Some old or wrong mappings has been found for the index '\"+ db.index_name +\"'.\\n\"\\\n                                    \"In order to upgrade them it is necessary to reindex '\"+ str(count) +\"' entries.\\n\"\\\n                                    \"Are you sure you want to proceed?\",\n                                     prompt_suffix='',\n                                     default=False):\n                    db.reindex()\n\n    except Exception as e:\n        if conf.get('DEBUG', False):\n            raise\n        else:\n            die(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport the volumes from a json file into the index.", "response": "def import_volumes(source, ignore_conflicts, yes):\n    '''Import volumes\n\n    SOURCE must be a json file and must follow the same structure used in `libreant-db export`.\n    Pass - to read from standard input.\n    '''\n    volumes = json.load(source)\n    tot = len(volumes)\n    if not yes:\n        click.confirm(\"Are you sure you want to import {} volumes into index '{}'\".format(tot, arc._config['ES_INDEXNAME']))\n    conflicts=0\n    with click.progressbar(volumes, label='adding volumes') as bar:\n        for v in bar:\n            try:\n                arc.import_volume(v)\n            except ConflictException as ce:\n                if not ignore_conflicts:\n                    die(str(ce))\n                conflicts += 1\n            except Exception as e:\n                if conf.get('DEBUG', False):\n                    raise\n                else:\n                    die(str(e))\n\n    if conflicts > 0:\n        click.echo(\"{} volumes has been skipped beacause of a conflict\".format(conflicts))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_volume(language, filepath, notes, metadata):\n    '''\n    Add a new volume to libreant.\n\n    The metadata of the volume are taken from a json file whose path must be\n    passed as argument. Passing \"-\" as argument will read the file from stdin.\n    language is an exception, because it must be set using --language\n\n    For every attachment you must add a --file AND a --notes.\n\n    \\b\n    Examples:\n        Adds a volume with no metadata. Yes, it makes no sense but you can\n          libreant-db insert-volume -l en - <<<'{}'\n        Adds a volume with no files attached\n          libreant-db insert-volume -l en - <<EOF\n          {\n            \"title\": \"How to create volumes\",\n            \"actors\": [\"libreant devs\", \"open access conspiration\"]\n          }\n          EOF\n        Adds a volume with one attachment but no metadata\n          libreant-db insert-volume -l en -f /path/book.epub --notes 'poor quality'\n        Adds a volume with two attachments but no metadata\n          libreant-db insert-volume -l en -f /path/book.epub --notes 'poor quality' -f /path/someother.epub --notes 'preprint'\n\n    '''\n    meta = {\"_language\": language}\n    if metadata:\n        meta.update(json.load(metadata))\n    attachments = attach_list(filepath, notes)\n    try:\n        out = arc.insert_volume(meta, attachments)\n    except Exception:\n        die('An upload error have occurred!', exit_code=4)\n    click.echo(out)", "response": "Adds a new volume to libreant."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach_list(filepaths, notes):\n    '''\n    all the arguments are lists\n    returns a list of dictionaries; each dictionary \"represent\" an attachment\n    '''\n    assert type(filepaths) in (list, tuple)\n    assert type(notes) in (list, tuple)\n\n    # this if clause means \"if those lists are not of the same length\"\n    if len(filepaths) != len(notes):\n        die('The number of --filepath, and --notes must be the same')\n\n    attach_list = []\n    for fname, note in zip(filepaths, notes):\n        name = os.path.basename(fname)\n        assert os.path.exists(fname)\n        mime = mimetypes.guess_type(fname)[0]\n        if mime is not None and '/' not in mime:\n            mime = None\n        attach_list.append({\n            'file': fname,\n            'name': name,\n            'mime': mime,\n            'note': note\n        })\n    return attach_list", "response": "returns a list of dictionaries ; each dictionary represent an attachment\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _param_fields(kwargs, fields):\n  if fields is None:\n    return\n  if type(fields) in [list, set, frozenset, tuple]:\n    fields = {x: True for x in fields}\n  if type(fields) == dict:\n    fields.setdefault(\"_id\", False)\n  kwargs[\"projection\"] = fields", "response": "Normalize the fields argument to most find methods\n ethernet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npatch a cursor to include batch_size limit and sort parameters.", "response": "def patch_cursor(cursor, batch_size=None, limit=None, skip=None, sort=None, **kwargs):\n  \"\"\"\n    Adds batch_size, limit, sort parameters to a DB cursor\n  \"\"\"\n\n  if type(batch_size) == int:\n    cursor.batch_size(batch_size)\n\n  if limit is not None:\n    cursor.limit(limit)\n\n  if sort is not None:\n    cursor.sort(sort)\n\n  if skip is not None:\n    cursor.skip(skip)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the search matches at least one document", "response": "def exists(self, query, **args):\n        \"\"\"\n        Returns True if the search matches at least one document\n        \"\"\"\n        return bool(self.find(query, **args).limit(1).count())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of the pymongo collection with various options set up.", "response": "def _collection_with_options(self, kwargs):\n        \"\"\" Returns a copy of the pymongo collection with various options set up \"\"\"\n\n        # class DocumentClassWithFields(self.document_class):\n        #     _fetched_fields = kwargs.get(\"projection\")\n        #     mongokat_collection = self\n\n        read_preference = kwargs.get(\"read_preference\") or getattr(self.collection, \"read_preference\", None) or ReadPreference.PRIMARY\n\n        if \"read_preference\" in kwargs:\n            del kwargs[\"read_preference\"]\n\n        # Simplified tag usage\n        if \"read_use\" in kwargs:\n            if kwargs[\"read_use\"] == \"primary\":\n                read_preference = ReadPreference.PRIMARY\n            elif kwargs[\"read_use\"] == \"secondary\":\n                read_preference = ReadPreference.SECONDARY\n            elif kwargs[\"read_use\"] == \"nearest\":\n                read_preference = ReadPreference.NEAREST\n            elif kwargs[\"read_use\"]:\n                read_preference = read_preferences.Secondary(tag_sets=[{\"use\": kwargs[\"read_use\"]}])\n            del kwargs[\"read_use\"]\n\n        write_concern = None\n        if kwargs.get(\"w\") is 0:\n            write_concern = WriteConcern(w=0)\n        elif kwargs.get(\"write_concern\"):\n            write_concern = kwargs.get(\"write_concern\")\n\n        codec_options = CodecOptions(\n            document_class=(\n                self.document_class,\n                {\n                    \"fetched_fields\": kwargs.get(\"projection\"),\n                    \"mongokat_collection\": self\n                }\n            )\n        )\n\n        return self.collection.with_options(\n            codec_options=codec_options,\n            read_preference=read_preference,\n            write_concern=write_concern\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_one(self, *args, **kwargs):\n        doc = self._collection_with_options(kwargs).find_one(*args, **kwargs)\n        if doc is None:\n            return None\n\n        return doc", "response": "Find a single document in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a record by id", "response": "def find_by_id(self, _id, **kwargs):\n        \"\"\"\n        Pass me anything that looks like an _id : str, ObjectId, {\"_id\": str}, {\"_id\": ObjectId}\n        \"\"\"\n\n        if type(_id) == dict and _id.get(\"_id\"):\n            return self.find_one({\"_id\": ObjectId(_id[\"_id\"])}, **kwargs)\n\n        return self.find_one({\"_id\": ObjectId(_id)}, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_by_b64id(self, _id, **kwargs):\n\n        return self.find_one({\"_id\": ObjectId(base64.b64decode(_id))}, **kwargs)", "response": "Find a record by base64 - encoded ObjectId"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding objects by base64 - encoded ObjectId s", "response": "def find_by_b64ids(self, _ids, **kwargs):\n        \"\"\"\n        Pass me a list of base64-encoded ObjectId\n        \"\"\"\n\n        return self.find_by_ids([ObjectId(base64.b64decode(_id)) for _id in _ids], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_column(self, query=None, field=\"_id\", **kwargs):\n        find_kwargs = {\n            \"projection\": {\"_id\": False}\n        }\n        find_kwargs[\"projection\"][field] = True\n\n        cursor = self._collection_with_options(kwargs).find(query, **find_kwargs)  # We only want 1 field: bypass the ORM\n\n        patch_cursor(cursor, **kwargs)\n\n        return (dotdict(x)[field] for x in cursor)", "response": "Return one field as an iterator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_random(self, **kwargs):\n        import random\n        max = self.count(**kwargs)\n        if max:\n            num = random.randint(0, max - 1)\n            return next(self.find(**kwargs).skip(num))", "response": "find one random document from the collection\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert the data as a new document.", "response": "def insert(self, data, return_object=False):\n        \"\"\" Inserts the data as a new document. \"\"\"\n\n        obj = self(data)  # pylint: disable=E1102\n        obj.save()\n\n        if return_object:\n            return obj\n        else:\n            return obj[\"_id\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trigger(self, event, filter=None, update=None, documents=None, ids=None, replacements=None):\n\n        if not self.has_trigger(event):\n            return\n\n        if documents is not None:\n            pass\n        elif ids is not None:\n            documents = self.find_by_ids(ids, read_use=\"primary\")\n        elif filter is not None:\n            documents = self.find(filter, read_use=\"primary\")\n        else:\n            raise Exception(\"Trigger couldn't filter documents\")\n\n        for doc in documents:\n            getattr(doc, event)(update=update, replacements=replacements)", "response": "Trigger the after_save hook on documents if present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching all the objects in the database that match the structure of the object.", "response": "def fetch(self, spec=None, *args, **kwargs):\n        \"\"\"\n        return all document which match the structure of the object\n        `fetch()` takes the same arguments than the the pymongo.collection.find method.\n        The query is launch against the db and collection of the object.\n        \"\"\"\n        if spec is None:\n            spec = {}\n        for key in self.structure:\n            if key in spec:\n                if isinstance(spec[key], dict):\n                    spec[key].update({'$exists': True})\n            else:\n                spec[key] = {'$exists': True}\n        return self.find(spec, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching one document from the database.", "response": "def fetch_one(self, *args, **kwargs):\n        \"\"\"\n        return one document which match the structure of the object\n        `fetch_one()` takes the same arguments than the the pymongo.collection.find method.\n        If multiple documents are found, raise a MultipleResultsFound exception.\n        If no document is found, return None\n        The query is launch against the db and collection of the object.\n        \"\"\"\n        bson_obj = self.fetch(*args, **kwargs)\n        count = bson_obj.count()\n        if count > 1:\n            raise MultipleResultsFound(\"%s results found\" % count)\n        elif count == 1:\n            # return self(bson_obj.next(), fetched_fields=kwargs.get(\"projection\"))\n            return next(bson_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint usage and exit with given code.", "response": "def usage(ecode, msg=''):\n    \"\"\"\n    Print usage and msg and exit with given code.\n    \"\"\"\n    print >> sys.stderr, __doc__\n    if msg:\n        print >> sys.stderr, msg\n    sys.exit(ecode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a translation to the dictionary.", "response": "def add(msgid, transtr, fuzzy):\n    \"\"\"\n    Add a non-fuzzy translation to the dictionary.\n    \"\"\"\n    global MESSAGES\n    if not fuzzy and transtr and not transtr.startswith('\\0'):\n        MESSAGES[msgid] = transtr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate():\n    global MESSAGES\n    keys = MESSAGES.keys()\n    # the keys are sorted in the .mo file\n    keys.sort()\n    offsets = []\n    ids = strs = ''\n    for _id in keys:\n        # For each string, we need size and file offset.  Each string is NUL\n        # terminated; the NUL does not count into the size.\n        offsets.append((len(ids), len(_id), len(strs), len(MESSAGES[_id])))\n        ids += _id + '\\0'\n        strs += MESSAGES[_id] + '\\0'\n    output = ''\n    # The header is 7 32-bit unsigned integers.  We don't use hash tables, so\n    # the keys start right after the index tables.\n    # translated string.\n    keystart = 7 * 4 + 16 * len(keys)\n    # and the values start after the keys\n    valuestart = keystart + len(ids)\n    koffsets = []\n    voffsets = []\n    # The string table first has the list of keys, then the list of values.\n    # Each entry has first the size of the string, then the file offset.\n    for o1, l1, o2, l2 in offsets:\n        koffsets += [l1, o1 + keystart]\n        voffsets += [l2, o2 + valuestart]\n    offsets = koffsets + voffsets\n    output = struct.pack(\"Iiiiiii\",\n                         0x950412deL,       # Magic\n                         0,                 # Version\n                         len(keys),         # # of entries\n                         7 * 4,             # start of key index\n                         7 * 4 + len(keys) * 8,   # start of value index\n                         0, 0)              # size and offset of hash table\n    output += array.array(\"i\", offsets).tostring()\n    output += ids\n    output += strs\n    return output", "response": "Generate the NUL file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the requirements string for the elsaticsearch - py library Returns a suitable requirements string for the elsaticsearch - py library according to the elasticsearch version to be supported ( es_version", "response": "def get_es_requirements(es_version):\n    '''Get the requirements string for elasticsearch-py library\n\n    Returns a suitable requirements string for the elsaticsearch-py library\n    according to the elasticsearch version to be supported (es_version)'''\n\n    # accepts version range in the form `2.x`\n    es_version = es_version.replace('x', '0')\n    es_version = map(int, es_version.split('.'))\n    if es_version >= [6]:\n        return \">=6.0.0, <7.0.0\"\n    elif es_version >= [5]:\n        return \">=5.0.0, <6.0.0\"\n    elif es_version >= [2]:\n        return \">=2.0.0, <3.0.0\"\n    elif es_version >= [1]:\n        return \">=1.0.0, <2.0.0\"\n    else:\n        return \"<1.0.0\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile all message catalogs. po files into. mo files.", "response": "def run(self):\n        \"\"\"\n           Compile all message catalogs .po files into .mo files.\n           Skips not changed file based on source mtime.\n        \"\"\"\n        # thanks to deluge guys ;)\n        po_dir = os.path.join(os.path.dirname(__file__), 'webant', 'translations')\n        print('Compiling po files from \"{}\"...'.format(po_dir))\n        for lang in os.listdir(po_dir):\n            sys.stdout.write(\"\\tCompiling {}... \".format(lang))\n            sys.stdout.flush()\n            curr_lang_path = os.path.join(po_dir, lang)\n            for path, dirs, filenames in os.walk(curr_lang_path):\n                for f in filenames:\n                    if f.endswith('.po'):\n                        src = os.path.join(path, f)\n                        dst = os.path.join(path, f[:-3] + \".mo\")\n                        if not os.path.exists(dst) or self.force:\n                            msgfmt.make(src, dst)\n                            print(\"ok.\")\n                        else:\n                            src_mtime = os.stat(src)[8]\n                            dst_mtime = os.stat(dst)[8]\n                            if src_mtime > dst_mtime:\n                                msgfmt.make(src, dst)\n                                print(\"ok.\")\n                            else:\n                                print(\"already up to date.\")\n        print('Finished compiling translation files.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_response_element_data(self, key1, key2):\n        if not self.dict_response[key1][key2]:\n            l = self.response\n            for i, orig in enumerate(self.origins):\n                self.dict_response[key1][key2][orig] = {}\n                for j, dest in enumerate(self.destinations):\n                    if l[i]['elements'][j]['status'] == 'OK':\n                        self.dict_response[key1][key2][orig][dest] = l[i]['elements'][j][key1][key2]\n                    else:\n                        self.dict_response[key1][key2][orig][dest] = l[i]['elements'][j]['status']\n\n        return self.dict_response[key1][key2]", "response": "Get the response element data for the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget closest points to a given origin. Returns a list of 2 element tuples where first element is the destination and second is the distance.", "response": "def get_closest_points(self, max_distance=None, origin_index=0, origin_raw=None):\n        \"\"\"\n        Get closest points to a given origin. Returns a list of 2 element tuples where first element is the destination and the second is the distance.\n        \"\"\"\n        if not self.dict_response['distance']['value']:\n            self.get_distance_values()\n\n        if origin_raw:\n            origin = copy.deepcopy(self.dict_response['distance']['value'][origin_raw])\n        else:\n            origin = copy.deepcopy(self.dict_response['distance']['value'][self.origins[origin_index]])\n\n        tmp_origin = copy.deepcopy(origin)\n        if max_distance is not None:\n            for k, v in tmp_origin.iteritems():\n                if v > max_distance or v == 'ZERO_RESULTS':\n                    del(origin[k])\n\n        return origin"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrename nodes in the AMR graph to prefix + node_index.", "response": "def rename_node(self, prefix):\n        \"\"\"\n        Rename AMR graph nodes to prefix + node_index to avoid nodes with the same name in two different AMRs.\n\n        \"\"\"\n        node_map_dict = {}\n        # map each node to its new name (e.g. \"a1\")\n        for i in range(0, len(self.nodes)):\n            node_map_dict[self.nodes[i]] = prefix + str(i)\n        # update node name\n        for i, v in enumerate(self.nodes):\n            self.nodes[i] = node_map_dict[v]\n        # update node name in relations\n        for node_relations in self.relations:\n            for i, l in enumerate(node_relations):\n                node_relations[i][1] = node_map_dict[l[1]]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_triples(self):\n        instance_triple = []\n        relation_triple = []\n        attribute_triple = []\n        for i in range(len(self.nodes)):\n            instance_triple.append((\"instance\", self.nodes[i], self.node_values[i]))\n            # l[0] is relation name\n            # l[1] is the other node this node has relation with\n            for l in self.relations[i]:\n                relation_triple.append((l[0], self.nodes[i], l[1]))\n            # l[0] is the attribute name\n            # l[1] is the attribute value\n            for l in self.attributes[i]:\n                attribute_triple.append((l[0], self.nodes[i], l[1]))\n        return instance_triple, attribute_triple, relation_triple", "response": "Get the triples in three lists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_amr_line(input_f):\n        cur_amr = []\n        has_content = False\n        for line in input_f:\n            line = line.strip()\n            if line == \"\":\n                if not has_content:\n                    # empty lines before current AMR\n                    continue\n                else:\n                    # end of current AMR\n                    break\n            if line.strip().startswith(\"#\"):\n                # ignore the comment line (starting with \"#\") in the AMR file\n                continue\n            else:\n                has_content = True\n                cur_amr.append(line.strip())\n        return \"\".join(cur_amr)", "response": "Reads the file containing AMRs and returns the next available AMR."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_AMR_line(line):\n        # Current state. It denotes the last significant symbol encountered. 1 for (, 2 for :, 3 for /,\n        # and 0 for start state or ')'\n        # Last significant symbol is ( --- start processing node name\n        # Last significant symbol is : --- start processing relation name\n        # Last significant symbol is / --- start processing node value (concept name)\n        # Last significant symbol is ) --- current node processing is complete\n        # Note that if these symbols are inside parenthesis, they are not significant symbols.\n        state = 0\n        # node stack for parsing\n        stack = []\n        # current not-yet-reduced character sequence\n        cur_charseq = []\n        # key: node name value: node value\n        node_dict = {}\n        # node name list (order: occurrence of the node)\n        node_name_list = []\n        # key: node name:  value: list of (relation name, the other node name)\n        node_relation_dict1 = defaultdict(list)\n        # key: node name, value: list of (attribute name, const value) or (relation name, unseen node name)\n        node_relation_dict2 = defaultdict(list)\n        # current relation name\n        cur_relation_name = \"\"\n        # having unmatched quote string\n        in_quote = False\n        for i, c in enumerate(line.strip()):\n            if c == \" \":\n                # allow space in relation name\n                if state == 2:\n                    cur_charseq.append(c)\n                continue\n            if c == \"\\\"\":\n                # flip in_quote value when a quote symbol is encountered\n                # insert placeholder if in_quote from last symbol\n                if in_quote:\n                    cur_charseq.append('_')\n                in_quote = not in_quote\n            elif c == \"(\":\n                # not significant symbol if inside quote\n                if in_quote:\n                    cur_charseq.append(c)\n                    continue\n                # get the attribute name\n                # e.g :arg0 (x ...\n                # at this point we get \"arg0\"\n                if state == 2:\n                    # in this state, current relation name should be empty\n                    if cur_relation_name != \"\":\n                        print(\"Format error when processing \", line[0:i + 1], file=ERROR_LOG)\n                        return None\n                    # update current relation name for future use\n                    cur_relation_name = \"\".join(cur_charseq).strip()\n                    cur_charseq[:] = []\n                state = 1\n            elif c == \":\":\n                # not significant symbol if inside quote\n                if in_quote:\n                    cur_charseq.append(c)\n                    continue\n                # Last significant symbol is \"/\". Now we encounter \":\"\n                # Example:\n                # :OR (o2 / *OR*\n                #    :mod (o3 / official)\n                #  gets node value \"*OR*\" at this point\n                if state == 3:\n                    node_value = \"\".join(cur_charseq)\n                    # clear current char sequence\n                    cur_charseq[:] = []\n                    # pop node name (\"o2\" in the above example)\n                    cur_node_name = stack[-1]\n                    # update node name/value map\n                    node_dict[cur_node_name] = node_value\n                # Last significant symbol is \":\". Now we encounter \":\"\n                # Example:\n                # :op1 w :quant 30\n                # or :day 14 :month 3\n                # the problem is that we cannot decide if node value is attribute value (constant)\n                # or node value (variable) at this moment\n                elif state == 2:\n                    temp_attr_value = \"\".join(cur_charseq)\n                    cur_charseq[:] = []\n                    parts = temp_attr_value.split()\n                    if len(parts) < 2:\n                        print(\"Error in processing; part len < 2\", line[0:i + 1], file=ERROR_LOG)\n                        return None\n                    # For the above example, node name is \"op1\", and node value is \"w\"\n                    # Note that this node name might not be encountered before\n                    relation_name = parts[0].strip()\n                    relation_value = parts[1].strip()\n                    # We need to link upper level node to the current\n                    # top of stack is upper level node\n                    if len(stack) == 0:\n                        print(\"Error in processing\", line[:i], relation_name, relation_value, file=ERROR_LOG)\n                        return None\n                    # if we have not seen this node name before\n                    if relation_value not in node_dict:\n                        node_relation_dict2[stack[-1]].append((relation_name, relation_value))\n                    else:\n                        node_relation_dict1[stack[-1]].append((relation_name, relation_value))\n                state = 2\n            elif c == \"/\":\n                if in_quote:\n                    cur_charseq.append(c)\n                    continue\n                # Last significant symbol is \"(\". Now we encounter \"/\"\n                # Example:\n                # (d / default-01\n                # get \"d\" here\n                if state == 1:\n                    node_name = \"\".join(cur_charseq)\n                    cur_charseq[:] = []\n                    # if this node name is already in node_dict, it is duplicate\n                    if node_name in node_dict:\n                        print(\"Duplicate node name \", node_name, \" in parsing AMR\", file=ERROR_LOG)\n                        return None\n                    # push the node name to stack\n                    stack.append(node_name)\n                    # add it to node name list\n                    node_name_list.append(node_name)\n                    # if this node is part of the relation\n                    # Example:\n                    # :arg1 (n / nation)\n                    # cur_relation_name is arg1\n                    # node name is n\n                    # we have a relation arg1(upper level node, n)\n                    if cur_relation_name != \"\":\n                        # if relation name ends with \"-of\", e.g.\"arg0-of\",\n                        # it is reverse of some relation. For example, if a is \"arg0-of\" b,\n                        # we can also say b is \"arg0\" a.\n                        # If the relation name ends with \"-of\", we store the reverse relation.\n                        if not cur_relation_name.endswith(\"-of\"):\n                            # stack[-2] is upper_level node we encountered, as we just add node_name to stack\n                            node_relation_dict1[stack[-2]].append((cur_relation_name, node_name))\n                        else:\n                            # cur_relation_name[:-3] is to delete \"-of\"\n                            node_relation_dict1[node_name].append((cur_relation_name[:-3], stack[-2]))\n                        # clear current_relation_name\n                        cur_relation_name = \"\"\n                else:\n                    # error if in other state\n                    print(\"Error in parsing AMR\", line[0:i + 1], file=ERROR_LOG)\n                    return None\n                state = 3\n            elif c == \")\":\n                if in_quote:\n                    cur_charseq.append(c)\n                    continue\n                # stack should be non-empty to find upper level node\n                if len(stack) == 0:\n                    print(\"Unmatched parenthesis at position\", i, \"in processing\", line[0:i + 1], file=ERROR_LOG)\n                    return None\n                # Last significant symbol is \":\". Now we encounter \")\"\n                # Example:\n                # :op2 \"Brown\") or :op2 w)\n                # get \\\"Brown\\\" or w here\n                if state == 2:\n                    temp_attr_value = \"\".join(cur_charseq)\n                    cur_charseq[:] = []\n                    parts = temp_attr_value.split()\n                    if len(parts) < 2:\n                        print(\"Error processing\", line[:i + 1], temp_attr_value, file=ERROR_LOG)\n                        return None\n                    relation_name = parts[0].strip()\n                    relation_value = parts[1].strip()\n                    # store reverse of the relation\n                    # we are sure relation_value is a node here, as \"-of\" relation is only between two nodes\n                    if relation_name.endswith(\"-of\"):\n                        node_relation_dict1[relation_value].append((relation_name[:-3], stack[-1]))\n                    # attribute value not seen before\n                    # Note that it might be a constant attribute value, or an unseen node\n                    # process this after we have seen all the node names\n                    elif relation_value not in node_dict:\n                        node_relation_dict2[stack[-1]].append((relation_name, relation_value))\n                    else:\n                        node_relation_dict1[stack[-1]].append((relation_name, relation_value))\n                # Last significant symbol is \"/\". Now we encounter \")\"\n                # Example:\n                # :arg1 (n / nation)\n                # we get \"nation\" here\n                elif state == 3:\n                    node_value = \"\".join(cur_charseq)\n                    cur_charseq[:] = []\n                    cur_node_name = stack[-1]\n                    # map node name to its value\n                    node_dict[cur_node_name] = node_value\n                # pop from stack, as the current node has been processed\n                stack.pop()\n                cur_relation_name = \"\"\n                state = 0\n            else:\n                # not significant symbols, so we just shift.\n                cur_charseq.append(c)\n        #create data structures to initialize an AMR\n        node_value_list = []\n        relation_list = []\n        attribute_list = []\n        for v in node_name_list:\n            if v not in node_dict:\n                print(\"Error: Node name not found\", v, file=ERROR_LOG)\n                return None\n            else:\n                node_value_list.append(node_dict[v])\n            # build relation list and attribute list for this node\n            node_rel_list = []\n            node_attr_list = []\n            if v in node_relation_dict1:\n                for v1 in node_relation_dict1[v]:\n                    node_rel_list.append([v1[0], v1[1]])\n            if v in node_relation_dict2:\n                for v2 in node_relation_dict2[v]:\n                    # if value is in quote, it is a constant value\n                    # strip the quote and put it in attribute map\n                    if v2[1][0] == \"\\\"\" and v2[1][-1] == \"\\\"\":\n                        node_attr_list.append([[v2[0]], v2[1][1:-1]])\n                    # if value is a node name\n                    elif v2[1] in node_dict:\n                        node_rel_list.append([v2[0], v2[1]])\n                    else:\n                        node_attr_list.append([v2[0], v2[1]])\n            # each node has a relation list and attribute list\n            relation_list.append(node_rel_list)\n            attribute_list.append(node_attr_list)\n        # add TOP as an attribute. The attribute value is the top node value\n        attribute_list[0].append([\"TOP\", node_value_list[0]])\n        result_amr = AMR(node_name_list, node_value_list, relation_list, attribute_list)\n        return result_amr", "response": "Parses a line representation of an AMR into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_config(self, data_sources=None):\n\n        if not data_sources:\n            # For testing\n            data_sources = [\"gerrit\", \"git\", \"github_issues\", \"mls\"]\n\n        # In new_config a dict with all the metrics for all data sources is created\n        new_config = {}\n        for ds in data_sources:\n            ds_config = self.ds2class[ds].get_section_metrics()\n            for section in ds_config:\n                if section not in new_config:\n                    # Just create the section with the data for the ds\n                    new_config[section] = ds_config[section]\n                else:\n                    for metric_section in ds_config[section]:\n                        if ds_config[section][metric_section] is not None:\n                            if (metric_section not in new_config[section] or\n                                new_config[section][metric_section] is None):\n                                new_config[section][metric_section] = ds_config[section][metric_section]\n                            else:\n                                new_config[section][metric_section] += ds_config[section][metric_section]\n\n        # Fields that are not linked to a data source\n        new_config['overview']['activity_file_csv'] = \"data_source_evolution.csv\"\n        new_config['overview']['efficiency_file_csv'] = \"efficiency.csv\"\n        new_config['project_process']['time_to_close_title'] = \"Days to close (median and average)\"\n        new_config['project_process']['time_to_close_review_title'] = \"Days to close review (median and average)\"\n\n        for i in range(0, len(data_sources)):\n            ds = data_sources[i]\n            ds_config = self.ds2class[ds].get_section_metrics()\n            activity_metrics = ds_config['project_activity']['metrics']\n            new_config['project_activity']['ds' + str(i + 1) + \"_metrics\"] = activity_metrics\n\n        return new_config", "response": "Build a dictionary with the Report configuration with the data sources and metrics to be included in each section of the report."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts None values to 0 so the data works with Matplotlib", "response": "def __convert_none_to_zero(self, ts):\n        \"\"\"\n        Convert None values to 0 so the data works with Matplotlib\n        :param ts:\n        :return: a list with 0s where Nones existed\n        \"\"\"\n\n        if not ts:\n            return ts\n\n        ts_clean = [val if val else 0 for val in ts]\n\n        return ts_clean"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bar3_chart(self, title, labels, data1, file_name, data2, data3, legend=[\"\", \"\"]):\n\n        colors = [\"orange\", \"grey\"]\n\n        data1 = self.__convert_none_to_zero(data1)\n        data2 = self.__convert_none_to_zero(data2)\n        data3 = self.__convert_none_to_zero(data3)\n\n        fig, ax = plt.subplots(1)\n        xpos = np.arange(len(data1))\n        width = 0.28\n\n        plt.title(title)\n        y_pos = np.arange(len(data1))\n\n        ppl.bar(xpos + width + width, data3, color=\"orange\", width=0.28, annotate=True)\n        ppl.bar(xpos + width, data1, color='grey', width=0.28, annotate=True)\n        ppl.bar(xpos, data2, grid='y', width=0.28, annotate=True)\n        plt.xticks(xpos + width, labels)\n        plt.legend(legend, loc=2)\n\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        plt.savefig(file_name)\n        plt.close()", "response": "Generate a bar plot with three columns in each x position and save it to file_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_metric_index(self, metric_cls):\n        ds = self.class2ds[metric_cls.ds]\n        if self.index_dict[ds]:\n            index_name = self.index_dict[ds]\n        else:\n            index_name = self.ds2index[metric_cls.ds]\n        return index_name", "response": "Get the index name with the data for a metric class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sec_overview(self):\n\n        \"\"\" Data sources overview: table with metric summaries\"\"\"\n        metrics = self.config['overview']['activity_metrics']\n        file_name = self.config['overview']['activity_file_csv']\n\n        data_path = os.path.join(self.data_dir, \"data\")\n        file_name = os.path.join(data_path, file_name)\n\n        logger.debug(\"CSV file %s generation in progress\", file_name)\n\n        csv = 'metricsnames,netvalues,relativevalues,datasource\\n'\n        for metric in metrics:\n            # comparing current metric month count with previous month\n            es_index = self.get_metric_index(metric)\n            ds = metric.ds.name\n            m = metric(self.es_url, es_index, start=self.start, end=self.end)\n            (last, percentage) = m.get_trend()\n            csv += \"%s,%i,%i,%s\" % (metric.name, last, percentage, ds)\n            csv += \"\\n\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        with open(file_name, \"w\") as f:\n            # Hack, we need to fix LaTeX escaping in a central place\n            csv = csv.replace(\"_\", r\"\\_\")\n            f.write(csv)\n\n        logger.debug(\"CSV file: %s was generated\", file_name)\n\n        \"\"\"\n        Git Authors:\n\n        description: average number of developers per month by quarters\n        (so we have the average number of developers per month during\n        those three months). If the approach is to work at the level of month,\n        then just the number of developers per month.\n        \"\"\"\n\n        author = self.config['overview']['author_metrics'][0]\n        csv_labels = 'labels,' + author.id\n        file_label = author.ds.name + \"_\" + author.id\n        title_label = author.name + \" per \" + self.interval\n        self.__create_csv_eps(author, None, csv_labels, file_label, title_label)\n\n        logger.debug(\"CSV file %s generation in progress\", file_name)\n\n        bmi = []\n        ttc = []  # time to close\n\n        csv_labels = ''\n        for m in self.config['overview']['bmi_metrics']:\n            metric = m(self.es_url, self.get_metric_index(m),\n                       start=self.end_prev_month, end=self.end)\n            csv_labels += m.id + \",\"\n            bmi.append(metric.get_agg())\n\n        for m in self.config['overview']['time_to_close_metrics']:\n            metric = m(self.es_url, self.get_metric_index(m),\n                       start=self.end_prev_month, end=self.end)\n            csv_labels += m.id + \",\"\n            ttc.append(metric.get_agg())\n\n        csv = csv_labels[:-1] + \"\\n\"  # remove last comma\n        csv = csv.replace(\"_\", \"\")\n        for val in bmi:\n            csv += \"%s,\" % (self.str_val(val))\n        for val in ttc:\n            csv += \"%s,\" % (self.str_val(val))\n        if csv[-1] == ',':\n            csv = csv[:-1]\n\n        data_path = os.path.join(self.data_dir, \"data\")\n        file_name = os.path.join(data_path, 'efficiency.csv')\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        with open(file_name, \"w\") as f:\n            f.write(csv)\n\n        logger.debug(\"CSV file: %s was generated\", file_name)", "response": "Generate the data sources overview for the current activity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sec_com_channels(self):\n\n        metrics = self.config['com_channels']['activity_metrics']\n        metrics += self.config['com_channels']['author_metrics']\n        for metric in metrics:\n            csv_labels = 'labels,' + metric.id\n            file_label = metric.ds.name + \"_\" + metric.id\n            title_label = metric.name + \" per \" + self.interval\n            self.__create_csv_eps(metric, None, csv_labels, file_label, title_label)", "response": "Generate the data for the Communication Channels section in the report"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __create_csv_eps(self, metric1, metric2, csv_labels, file_label,\n                         title_label, project=None):\n        \"\"\"\n        Generate the CSV data and EPS figs files for two metrics\n        :param metric1: first metric class\n        :param metric2: second metric class\n        :param csv_labels: labels to be used in the CSV file\n        :param file_label: shared filename token to be included in csv and eps files\n        :param title_label: title for the EPS figures\n        :param project: name of the project for which to generate the data\n        :return:\n        \"\"\"\n\n        logger.debug(\"CSV file %s generation in progress\", file_label)\n\n        esfilters = None\n        csv_labels = csv_labels.replace(\"_\", \"\")  # LaTeX not supports\n\n        if project and project != self.GLOBAL_PROJECT:\n            esfilters = {\"project\": project}\n        m1 = metric1(self.es_url, self.get_metric_index(metric1),\n                     esfilters=esfilters,\n                     start=self.start, end=self.end)\n        m1_ts = m1.get_ts()\n\n        if metric2:\n            m2 = metric2(self.es_url, self.get_metric_index(metric2),\n                         esfilters=esfilters,\n                         start=self.start, end=self.end)\n            m2_ts = m2.get_ts()\n\n        csv = csv_labels + '\\n'\n        for i in range(0, len(m1_ts['date'])):\n            if self.interval == 'quarter':\n                date_str = self.build_period_name(parser.parse(m1_ts['date'][i]), start_date=True)\n            else:\n                date_str = parser.parse(m1_ts['date'][i]).strftime(\"%y-%m\")\n            csv += date_str\n            csv += \",\" + self.str_val(m1_ts['value'][i])\n            if metric2:\n                csv += \",\" + self.str_val(m2_ts['value'][i])\n            csv += \"\\n\"\n\n        data_path = os.path.join(self.data_dir, \"data\")\n\n        if project:\n            file_name = os.path.join(data_path, file_label + \"_\" + project + \".csv\")\n        else:\n            file_name = os.path.join(data_path, file_label + \".csv\")\n\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        with open(file_name, \"w\") as f:\n            f.write(csv)\n\n        logger.debug(\"CSV file %s was generated\", file_label)\n\n        fig_path = os.path.join(self.data_dir, \"figs\")\n\n        if project:\n            file_name = os.path.join(fig_path, file_label + \"_\" + project + \".eps\")\n            title = title_label + \": \" + project\n        else:\n            file_name = os.path.join(fig_path, file_label + \".eps\")\n            title = title_label\n\n        if self.interval != 'quarter':\n            x_val = [parser.parse(val).strftime(\"%y-%m\") for val in m1_ts['date']]\n        else:\n            x_val = []\n            for val in m1_ts['date']:\n                period = self.build_period_name(parser.parse(val), start_date=True)\n                x_val.append(period)\n        if metric2:\n            self.bar_chart(title, x_val, m1_ts['value'],\n                           file_name, m2_ts['value'],\n                           legend=[m1.name, m2.name])\n        else:\n            self.bar_chart(title, x_val, m1_ts['value'], file_name,\n                           legend=[m1.name])", "response": "Generate the CSV data and EPS figs files for two metrics."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the data for the Activity section in the report", "response": "def sec_project_activity(self, project=None):\n        \"\"\"\n        Generate the data for the Activity section in the report\n        :return:\n        \"\"\"\n\n        def create_data(metrics, project):\n            csv_labels = \"labels\" + ',' + metrics[0].id + \",\" + metrics[1].id\n            file_label = metrics[0].ds.name + \"_\" + metrics[0].id + \"_\"\n            file_label += metrics[1].ds.name + \"_\" + metrics[1].id\n            title_label = metrics[0].name + \", \" + metrics[1].name + \" per \" + self.interval\n            self.__create_csv_eps(metrics[0], metrics[1], csv_labels,\n                                  file_label, title_label, project)\n\n        logger.info(\"Activity data for: %s\", project)\n\n        for activity_ds in self.config['project_activity']:\n            if activity_ds == 'metrics':\n                continue  # all metrics included\n            metrics = self.config['project_activity'][activity_ds]\n            create_data(metrics, project)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the data for the Communication section in a Project report", "response": "def sec_project_community(self, project=None):\n        \"\"\"\n        Generate the data for the Communication section in a Project report\n        :return:\n        \"\"\"\n\n        def create_csv(metric1, csv_labels, file_label):\n            esfilters = None\n            csv_labels = csv_labels.replace(\"_\", \"\")  # LaTeX not supports \"_\"\n            if project != self.GLOBAL_PROJECT:\n                esfilters = {\"project\": project}\n\n            data_path = os.path.join(self.data_dir, \"data\")\n\n            file_name = os.path.join(data_path, file_label + \"_\" + project + \".csv\")\n\n            logger.debug(\"CSV file %s generation in progress\", file_name)\n\n            m1 = metric1(self.es_url, self.get_metric_index(metric1),\n                         esfilters=esfilters, start=self.end_prev_month, end=self.end)\n            top = m1.get_list()\n            csv = csv_labels + '\\n'\n            for i in range(0, len(top['value'])):\n                if i > self.TOP_MAX:\n                    break\n                csv += top[metric1.FIELD_NAME][i] + \",\" + self.str_val(top['value'][i])\n                csv += \"\\n\"\n\n            with open(file_name, \"w\") as f:\n                f.write(csv)\n\n            logger.debug(\"CSV file %s was generated\", file_name)\n\n        logger.info(\"Community data for: %s\", project)\n\n        author = self.config['project_community']['author_metrics'][0]\n        csv_labels = 'labels,' + author.id\n        file_label = author.ds.name + \"_\" + author.id\n        title_label = author.name + \" per \" + self.interval\n        self.__create_csv_eps(author, None, csv_labels, file_label, title_label,\n                              project)\n\n        \"\"\"\n        Main developers\n\n        \"\"\"\n        metric = self.config['project_community']['people_top_metrics'][0]\n        # TODO: Commits must be extracted from metric\n        csv_labels = author.id + \",commits\"\n        file_label = author.ds.name + \"_top_\" + author.id\n        create_csv(metric, csv_labels, file_label)\n\n        \"\"\"\n        Main organizations\n\n        \"\"\"\n        orgs = self.config['project_community']['orgs_top_metrics'][0]\n        # TODO: Commits must be extracted from metric\n        csv_labels = orgs.id + \",commits\"\n        file_label = orgs.ds.name + \"_top_\" + orgs.id\n        create_csv(orgs, csv_labels, file_label)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the data for the Project Process section in a Project report.", "response": "def sec_project_process(self, project=None):\n        \"\"\"\n        Generate the data for the Process section in a Project report\n        :return:\n        \"\"\"\n\n        logger.info(\"Process data for: %s\", project)\n\n        \"\"\"\n        BMI Pull Requests, BMI Issues\n\n        description: closed PRs/issues out of open PRs/issues in a period of time\n        \"\"\"\n        for i in range(0, len(self.config['project_process']['bmi_metrics'])):\n            metric = self.config['project_process']['bmi_metrics'][i]\n            csv_labels = \"labels\" + \",\" + metric.id\n            file_label = metric.ds.name + \"_\" + metric.id\n            title_label = metric.name\n            self.__create_csv_eps(metric, None, csv_labels, file_label, title_label,\n                                  project)\n\n        \"\"\"\n        Time to close Issues\n\n        description: median and mean time to close issues\n        \"\"\"\n\n        if len(self.config['project_process']['time_to_close_metrics']) > 0:\n            metrics = self.config['project_process']['time_to_close_metrics']\n            csv_labels = \"labels\" + ',' + metrics[0].id + \",\" + metrics[1].id\n            file_label = metrics[0].ds.name + \"_\" + metrics[0].id + \"_\"\n            file_label += metrics[1].ds.name + \"_\" + metrics[1].id\n            # title_label = metrics[0].name + \", \" + metrics[1].name + \" per \"+ self.interval\n            title_label = self.config['project_process']['time_to_close_title']\n            self.__create_csv_eps(metrics[0], metrics[1], csv_labels, file_label,\n                                  title_label, project)\n\n        \"\"\"\n        Time to close PRs and gerrit reviews\n\n        description: median and mean time to close prs and gerrit reviews\n        \"\"\"\n        if len(self.config['project_process']['time_to_close_review_metrics']) > 0:\n            metrics = self.config['project_process']['time_to_close_review_metrics']\n            i = 0\n            while i < len(metrics):\n                # Generate the metrics in data source pairs.\n                # TODO: convert to tuples metrics\n                csv_labels = \"labels\" + ',' + metrics[i].id + \",\" + metrics[i + 1].id\n                file_label = metrics[i].ds.name + \"_\" + metrics[i].id + \"_\"\n                file_label += metrics[i + 1].ds.name + \"_\" + metrics[i + 1].id\n                # title_label = metrics[0].name+\", \"+ metrics[1].name + \" per \"+ self.interval\n                title_label = self.config['project_process']['time_to_close_review_title']\n                self.__create_csv_eps(metrics[i], metrics[i + 1], csv_labels, file_label,\n                                      title_label, project)\n                i = i + 2\n\n        \"\"\"\n        Patchsets per review\n\n        description: median and average of the number of patchsets per review\n        \"\"\"\n        if self.config['project_process']['patchsets_metrics']:\n            metrics = self.config['project_process']['patchsets_metrics']\n            csv_labels = \"labels\" + ',' + metrics[0].id + \",\" + metrics[1].id\n            file_label = metrics[0].ds.name + \"_\" + metrics[0].id + \"_\"\n            file_label += metrics[1].ds.name + \"_\" + metrics[1].id\n            # title_label = metrics[0].name+\", \"+ metrics[1].name + \" per \"+ self.interval\n            title_label = self.config['project_process']['patchsets_title']\n            self.__create_csv_eps(metrics[0], metrics[1], csv_labels, file_label,\n                                  title_label, project)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the report projects related sections", "response": "def sec_projects(self):\n        \"\"\"\n        Generate the report projects related sections: the general project is included always and add\n        to the global report, and an specific report for each project is generated if configured.\n        :return:\n        \"\"\"\n\n        \"\"\"\n        This activity is displayed at the general level, aggregating all\n        of the projects, with the name 'general' and per project using\n        the name of each project. This activity is divided into three main\n        sections: activity, community and process.\n        \"\"\"\n\n        # First the 'general' project\n        self.sec_project_activity(self.GLOBAL_PROJECT)\n        self.sec_project_community(self.GLOBAL_PROJECT)\n        self.sec_project_process(self.GLOBAL_PROJECT)\n\n        if not self.projects:\n            # Don't generate per project data\n            return\n\n        # Just one level projects supported yet\n\n        # First we need to get the list of projects per data source and\n        # join all lists in the overall projects list\n\n        projects_lists = self.config['overview']['projects_metrics']\n\n        projects = []\n        for p in projects_lists:\n            p_list = p(self.es_url, self.get_metric_index(p),\n                       start=self.start).get_list()['project']\n            projects += p_list\n\n        projects = list(set(projects))\n\n        project_str = \"\\n\".join(projects)\n\n        with open(os.path.join(self.data_dir, \"projects.txt\"), \"w\") as f:\n            f.write(project_str)\n\n        for project in projects:\n            # The name of the project is used to create files\n            project = project.replace(\"/\", \"_\")\n            self.sec_project_activity(project)\n            self.sec_project_community(project)\n            self.sec_project_process(project)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sections(self):\n        secs = OrderedDict()\n        secs['Overview'] = self.sec_overview\n        secs['Communication Channels'] = self.sec_com_channels\n        secs['Detailed Activity by Project'] = self.sec_projects\n\n        return secs", "response": "Get the sections of the report and howto build them"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the data and figs files for the report.", "response": "def create_data_figs(self):\n        \"\"\"\n        Generate the data and figs files for the report\n\n        :return:\n        \"\"\"\n\n        logger.info(\"Generating the report data and figs from %s to %s\",\n                    self.start, self.end)\n\n        for section in self.sections():\n            logger.info(\"Generating %s\", section)\n            self.sections()[section]()\n\n        logger.info(\"Data and figs done\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the period name for the report.", "response": "def build_period_name(cls, pdate, interval='quarter', offset=None, start_date=False):\n        \"\"\"\n        Build the period name for humans (eg, 18-Q2) to be used in the reports.\n        Just supporting quarters right now.\n        The name is built using the last month for the quarter.\n\n        :param pdate: the date (datetime) which defines the period\n        :param interval: the time interval (string)  used in the report\n        :param start_date: if False, pdate is the end of the period date\n        :return: the period name for humans (eg, 18-Q2)\n        \"\"\"\n\n        if interval not in ['quarter']:\n            raise RuntimeError(\"Interval not support in build_period_name\", interval)\n\n        name = pdate.strftime('%Y-%m-%d') + ' ' + interval\n        months_in_quarter = 3\n        end_quarter_month = None\n\n        if interval == 'quarter':\n            if offset:\n                # +31d offset format\n                offset_days = int(offset[1:-1])\n                pdate = pdate - timedelta(days=offset_days)\n            if not start_date:\n                # pdate param is the first day of the next quarter\n                # Remove one day to have a date in the end_quarter_month\n                pdate = pdate.replace(day=1) - timedelta(days=1)\n                year = pdate.strftime('%y')\n                end_quarter_month = int(pdate.strftime('%m'))\n            else:\n                # pdate param is the first day of the period\n                # add months_in_quarter to have the last month of the quarter\n                year = pdate.strftime('%y')\n                end_quarter_month = int(pdate.strftime('%m')) + months_in_quarter - 1\n\n            quarter = int(round(int(end_quarter_month) / months_in_quarter, 0))\n            name = year + \"-Q\" + str(quarter)\n\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_text(filepath, to_replace, replacement):\n        with open(filepath) as file:\n            s = file.read()\n        s = s.replace(to_replace, replacement)\n        with open(filepath, 'w') as file:\n            file.write(s)", "response": "Replaces a string in a given file with another string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing a string with its replacement in all the files in the directory.", "response": "def replace_text_dir(self, directory, to_replace, replacement, file_type=None):\n        \"\"\"\n        Replaces a string with its replacement in all the files in the directory\n\n        :param directory: the directory in which the files have to be modified\n        :param to_replace: the string to be replaced in the files\n        :param replacement: the string which replaces 'to_replace' in the files\n        :param file_type: file pattern to match the files in which the string has to be replaced\n        \"\"\"\n        if not file_type:\n            file_type = \"*.tex\"\n        for file in glob.iglob(os.path.join(directory, file_type)):\n            self.replace_text(file, to_replace, replacement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the PDF file for the LaTeX report.", "response": "def create_pdf(self):\n        \"\"\"\n        Create the report pdf file filling the LaTeX templates with the figs and data for the report\n\n        :return:\n        \"\"\"\n\n        logger.info(\"Generating PDF report\")\n\n        # First step is to create the report dir from the template\n        report_path = self.data_dir\n        templates_path = os.path.join(os.path.dirname(__file__),\n                                      \"latex_template\")\n\n        # Copy the data generated to be used in LaTeX template\n        copy_tree(templates_path, report_path)\n\n        # if user specified a logo then replace it with default logo\n        if self.logo:\n            os.remove(os.path.join(report_path, \"logo.eps\"))\n            os.remove(os.path.join(report_path, \"logo-eps-converted-to.pdf\"))\n            print(copy_file(self.logo, os.path.join(report_path, \"logo.\" + self.logo.split('/')[-1].split('.')[-1])))\n\n        # Change the project global name\n        project_replace = self.report_name.replace(' ', r'\\ ')\n        self.replace_text_dir(report_path, 'PROJECT-NAME', project_replace)\n        self.replace_text_dir(os.path.join(report_path, 'overview'), 'PROJECT-NAME', project_replace)\n\n        # Change the quarter subtitle\n        if self.interval == \"quarter\":\n            build_period_name = self.build_period_name(self.end, self.interval, self.offset)\n        else:\n            build_period_name = self.start.strftime(\"%y-%m\") + \"-\" + self.end.strftime(\"%y-%m\")\n        period_replace = build_period_name.replace(' ', r'\\ ')\n        self.replace_text_dir(report_path, '2016-QUARTER', period_replace)\n        self.replace_text_dir(os.path.join(report_path, 'overview'), '2016-QUARTER', period_replace)\n\n        # Report date frame\n        quarter_start = self.end - relativedelta.relativedelta(months=3)\n        quarter_start += relativedelta.relativedelta(days=1)\n        dateframe = (quarter_start.strftime('%Y-%m-%d') + \" to \" + self.end.strftime('%Y-%m-%d')).replace(' ', r'\\ ')\n        self.replace_text_dir(os.path.join(report_path, 'overview'), 'DATEFRAME', dateframe)\n\n        # Change the date Copyright\n        self.replace_text_dir(report_path, '(cc) 2016', '(cc) ' + datetime.now().strftime('%Y'))\n\n        # Fix LaTeX special chars\n        self.replace_text_dir(report_path, '&', '\\&', 'data/git_top_organizations_*')\n        self.replace_text_dir(report_path, '^#', '', 'data/git_top_organizations_*')\n\n        # Activity section\n        activity = ''\n        for activity_ds in ['git', 'github', 'gerrit', 'mls']:\n            if activity_ds in self.data_sources:\n                activity += r\"\\input{activity/\" + activity_ds + \".tex}\"\n\n        with open(os.path.join(report_path, \"activity.tex\"), \"w\") as flatex:\n            flatex.write(activity)\n\n        # Community section\n        community = ''\n        for community_ds in ['git', 'mls']:\n            if community_ds in self.data_sources:\n                community += r\"\\input{community/\" + community_ds + \".tex}\"\n\n        with open(os.path.join(report_path, \"community.tex\"), \"w\") as flatex:\n            flatex.write(community)\n\n        # Overview section\n        overview = r'\\input{overview/summary.tex}'\n        for overview_ds in ['github', 'gerrit']:\n            if overview_ds in self.data_sources:\n                overview += r\"\\input{overview/efficiency-\" + overview_ds + \".tex}\"\n\n        with open(os.path.join(report_path, \"overview.tex\"), \"w\") as flatex:\n            flatex.write(overview)\n\n        # Process section\n        process = ''\n        for process_ds in ['github_prs', 'gerrit']:\n            if process_ds in self.data_sources:\n                process += r\"\\input{process/\" + process_ds + \".tex}\"\n\n        if not process:\n            process = \"Unfortunately, this section is empty because there are no supported sources available to perform this kind of analysis.\"\n\n        with open(os.path.join(report_path, \"process.tex\"), \"w\") as flatex:\n            flatex.write(process)\n\n        # Time to generate the pdf report\n        res = subprocess.call(\"pdflatex report.tex\", shell=True, cwd=report_path)\n        if res > 0:\n            logger.error(\"Error generating PDF\")\n            return\n        # A second time so the TOC is generated\n        subprocess.call(\"pdflatex report.tex\", shell=True, cwd=report_path)\n\n        logger.info(\"PDF report done %s\", report_path + \"/report.pdf\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the LaTeX report and fill the LaTeX templates with them", "response": "def create(self):\n        \"\"\"\n        Generate the data and figs for the report and fill the LaTeX templates with them\n        to generate a PDF file with the report.\n\n        :return:\n        \"\"\"\n        logger.info(\"Generating the report from %s to %s\", self.start, self.end)\n\n        self.create_data_figs()\n        self.create_pdf()\n\n        logger.info(\"Report completed\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_names(file_dir, files):\n    # for each user, check if they have files available\n    # return user name list\n    total_list = []\n    name_list = []\n    get_sub = False\n    for path, subdir, dir_files in os.walk(file_dir):\n        if not get_sub:\n            total_list = subdir[:]\n            get_sub = True\n        else:\n            break\n    for user in total_list:\n        has_file = True\n        for f in files:\n            file_path = file_dir + user + \"/\" + f + \".txt\"\n            if not os.path.exists(file_path):\n                has_file = False\n                break\n        if has_file:\n            name_list.append(user)\n    if len(name_list) == 0:\n        print(\"********Error: Cannot find any user who completes the files*************\", file=ERROR_LOG)\n    return name_list", "response": "Get the annotator name list based on a list of files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_files(user1, user2, file_list, dir_pre, start_num):\n\n    \"\"\"\n    Compute the smatch scores for a file list between two users\n    Args:\n    user1: user 1 name\n    user2: user 2 name\n    file_list: file list\n    dir_pre: the file location prefix\n    start_num: the number of restarts in smatch\n    Returns:\n    smatch f score.\n\n    \"\"\"\n    match_total = 0\n    test_total = 0\n    gold_total = 0\n    for fi in file_list:\n        file1 = dir_pre + user1 + \"/\" + fi + \".txt\"\n        file2 = dir_pre + user2 + \"/\" + fi + \".txt\"\n        if not os.path.exists(file1):\n            print(\"*********Error: \", file1, \"does not exist*********\", file=ERROR_LOG)\n            return -1.00\n        if not os.path.exists(file2):\n            print(\"*********Error: \", file2, \"does not exist*********\", file=ERROR_LOG)\n            return -1.00\n        try:\n            file1_h = open(file1, \"r\")\n            file2_h = open(file2, \"r\")\n        except IOError:\n            print(\"Cannot open the files\", file1, file2, file=ERROR_LOG)\n            break\n        cur_amr1 = smatch.get_amr_line(file1_h)\n        cur_amr2 = smatch.get_amr_line(file2_h)\n        if cur_amr1 == \"\":\n            print(\"AMR 1 is empty\", file=ERROR_LOG)\n            continue\n        if cur_amr2 == \"\":\n            print(\"AMR 2 is empty\", file=ERROR_LOG)\n            continue\n        amr1 = amr.AMR.parse_AMR_line(cur_amr1)\n        amr2 = amr.AMR.parse_AMR_line(cur_amr2)\n        test_label = \"a\"\n        gold_label = \"b\"\n        amr1.rename_node(test_label)\n        amr2.rename_node(gold_label)\n        (test_inst, test_rel1, test_rel2) = amr1.get_triples()\n        (gold_inst, gold_rel1, gold_rel2) = amr2.get_triples()\n        if verbose:\n            print(\"Instance triples of file 1:\", len(test_inst), file=DEBUG_LOG)\n            print(test_inst, file=DEBUG_LOG)\n            print(\"Attribute triples of file 1:\", len(test_rel1), file=DEBUG_LOG)\n            print(test_rel1, file=DEBUG_LOG)\n            print(\"Relation triples of file 1:\", len(test_rel2), file=DEBUG_LOG)\n            print(test_rel2, file=DEBUG_LOG)\n            print(\"Instance triples of file 2:\", len(gold_inst), file=DEBUG_LOG)\n            print(gold_inst, file=DEBUG_LOG)\n            print(\"Attribute triples of file 2:\", len(gold_rel1), file=DEBUG_LOG)\n            print(gold_rel1, file=DEBUG_LOG)\n            print(\"Relation triples of file 2:\", len(gold_rel2), file=DEBUG_LOG)\n            print(gold_rel2, file=DEBUG_LOG)\n        (best_match, best_match_num) = smatch.get_best_match(test_inst, test_rel1, test_rel2,\n                                                             gold_inst, gold_rel1, gold_rel2,\n                                                             test_label, gold_label)\n        if verbose:\n            print(\"best match number\", best_match_num, file=DEBUG_LOG)\n            print(\"Best Match:\", smatch.print_alignment(best_match, test_inst, gold_inst), file=DEBUG_LOG)\n        match_total += best_match_num\n        test_total += (len(test_inst) + len(test_rel1) + len(test_rel2))\n        gold_total += (len(gold_inst) + len(gold_rel1) + len(gold_rel2))\n        smatch.match_triple_dict.clear()\n    (precision, recall, f_score) = smatch.compute_f(match_total, test_total, gold_total)\n    return \"%.2f\" % f_score", "response": "Compute the smatch scores for a list of files between two users."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a table in pretty format", "response": "def pprint_table(table):\n    \"\"\"\n    Print a table in pretty format\n\n    \"\"\"\n    col_paddings = []\n    for i in range(len(table[0])):\n        col_paddings.append(get_max_width(table,i))\n    for row in table:\n        print(row[0].ljust(col_paddings[0] + 1), end=\"\")\n        for i in range(1, len(row)):\n            col = str(row[i]).rjust(col_paddings[i]+2)\n            print(col, end='')\n        print(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_arg_parser():\n    parser = argparse.ArgumentParser(description=\"Smatch table calculator -- arguments\")\n    parser.add_argument(\"--fl\", type=argparse.FileType('r'), help='AMR ID list file')\n    parser.add_argument('-f', nargs='+', help='AMR IDs (at least one)')\n    parser.add_argument(\"-p\", nargs='*', help=\"User list (can be none)\")\n    parser.add_argument(\"--fd\", default=isi_dir_pre, help=\"AMR File directory. Default=location on isi machine\")\n    parser.add_argument('-r', type=int, default=4, help='Restart number (Default:4)')\n    parser.add_argument('-v', action='store_true', help='Verbose output (Default:False)')\n    return parser", "response": "Build an argument parser using argparse."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_arg_parser2():\n    usage_str = \"Smatch table calculator -- arguments\"\n    parser = optparse.OptionParser(usage=usage_str)\n    parser.add_option(\"--fl\", dest=\"fl\", type=\"string\", help='AMR ID list file')\n    parser.add_option(\"-f\", dest=\"f\", type=\"string\", action=\"callback\", callback=cb, help=\"AMR IDs (at least one)\")\n    parser.add_option(\"-p\", dest=\"p\", type=\"string\", action=\"callback\", callback=cb, help=\"User list\")\n    parser.add_option(\"--fd\", dest=\"fd\", type=\"string\", help=\"file directory\")\n    parser.add_option(\"-r\", \"--restart\", dest=\"r\", type=\"int\", help='Restart number (Default: 4)')\n    parser.add_option(\"-v\", \"--verbose\", action='store_true', dest=\"v\", help='Verbose output (Default:False)')\n    parser.set_defaults(r=4, v=False, ms=False, fd=isi_dir_pre)\n    return parser", "response": "Build an argument parser using optparse. Use it when python version is 2. 5 or 2. 6."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse arguments and check if the arguments are valid.", "response": "def check_args(args):\n    \"\"\"\n    Parse arguments and check if the arguments are valid\n\n    \"\"\"\n    if not os.path.exists(args.fd):\n        print(\"Not a valid path\", args.fd, file=ERROR_LOG)\n        return [], [], False\n    if args.fl is not None:\n        # we already ensure the file can be opened and opened the file\n        file_line = args.fl.readline()\n        amr_ids = file_line.strip().split()\n    elif args.f is None:\n        print(\"No AMR ID was given\", file=ERROR_LOG)\n        return [], [], False\n    else:\n        amr_ids = args.f\n    names = []\n    check_name = True\n    if args.p is None:\n        names = get_names(args.fd, amr_ids)\n        # no need to check names\n        check_name = False\n        if len(names) == 0:\n            print(\"Cannot find any user who tagged these AMR\", file=ERROR_LOG)\n            return [], [], False\n        else:\n            names = args.p\n    if len(names) == 0:\n        print(\"No user was given\", file=ERROR_LOG)\n        return [], [], False\n    if len(names) == 1:\n        print(\"Only one user is given. Smatch calculation requires at least two users.\", file=ERROR_LOG)\n        return [], [], False\n    if \"consensus\" in names:\n        con_index = names.index(\"consensus\")\n        names.pop(con_index)\n        names.append(\"consensus\")\n    # check if all the AMR_id and user combinations are valid\n    if check_name:\n        pop_name = []\n        for i, name in enumerate(names):\n            for amr in amr_ids:\n                amr_path = args.fd + name + \"/\" + amr + \".txt\"\n                if not os.path.exists(amr_path):\n                    print(\"User\", name, \"fails to tag AMR\", amr, file=ERROR_LOG)\n                    pop_name.append(i)\n                    break\n        if len(pop_name) != 0:\n            pop_num = 0\n            for p in pop_name:\n                print(\"Deleting user\", names[p - pop_num], \"from the name list\", file=ERROR_LOG)\n                names.pop(p - pop_num)\n                pop_num += 1\n        if len(names) < 2:\n            print(\"Not enough users to evaluate. Smatch requires >2 users who tag all the AMRs\", file=ERROR_LOG)\n            return \"\", \"\", False\n    return amr_ids, names, True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a CSV file with the given data and store it in the file named filename.", "response": "def create_csv(filename, csv_data, mode=\"w\"):\n    \"\"\"\n    Create a CSV file with the given data and store it in the\n    file with the given name.\n\n    :param filename: name of the file to store the data in\n    :pram csv_data: the data to be stored in the file\n    :param mode: the mode in which we have to open the file. It can\n                 be 'w', 'a', etc. Default is 'w'\n    \"\"\"\n\n    with open(filename, mode) as f:\n        csv_data.replace(\"_\", r\"\\_\")\n        f.write(csv_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the overview section of the report.", "response": "def get_sec_overview(self):\n        \"\"\"\n        Generate the \"overview\" section of the report.\n        \"\"\"\n\n        logger.debug(\"Calculating Overview metrics.\")\n\n        data_path = os.path.join(self.data_dir, \"overview\")\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n\n        overview_config = {\n            \"activity_metrics\": [],\n            \"author_metrics\": [],\n            \"bmi_metrics\": [],\n            \"time_to_close_metrics\": [],\n            \"projects_metrics\": []\n        }\n\n        for ds in self.data_sources:\n            metric_file = self.ds2class[ds]\n            metric_index = self.get_metric_index(ds)\n            overview = metric_file.overview(metric_index, self.start_date, self.end_date)\n            for section in overview_config:\n                overview_config[section] += overview[section]\n\n        overview_config['activity_file_csv'] = \"data_source_evolution.csv\"\n        overview_config['efficiency_file_csv'] = \"efficiency.csv\"\n\n        # ACTIVITY METRICS\n        metrics = overview_config['activity_metrics']\n        file_name = overview_config['activity_file_csv']\n        file_name = os.path.join(data_path, file_name)\n\n        csv = \"metricsnames, netvalues, relativevalues, datasource\\n\"\n\n        for metric in metrics:\n            (last, percentage) = get_trend(metric.timeseries())\n            csv += \"{}, {}, {}, {}\\n\".format(metric.name, last,\n                                             percentage, metric.DS_NAME)\n        csv = csv.replace(\"_\", \"\\_\")\n        create_csv(file_name, csv)\n\n        # AUTHOR METRICS\n        \"\"\"\n        Git Authors:\n        -----------\n        Description: average number of developers per month by quarters\n        (so we have the average number of developers per month during\n        those three months). If the approach is to work at the level of month,\n        then just the number of developers per month.\n        \"\"\"\n\n        author = overview_config['author_metrics']\n        if author:\n            authors_by_period = author[0]\n            title_label = file_label = authors_by_period.name + ' per ' + self.interval\n            file_path = os.path.join(data_path, file_label)\n            csv_data = authors_by_period.timeseries(dataframe=True)\n            # generate the CSV and the image file displaying the data\n            self.create_csv_fig_from_df([csv_data], file_path, [authors_by_period.name],\n                                        fig_type=\"bar\", title=title_label, xlabel=\"time_period\",\n                                        ylabel=authors_by_period.id)\n        # BMI METRICS\n        bmi = []\n        bmi_metrics = overview_config['bmi_metrics']\n        csv = \"\"\n        for metric in bmi_metrics:\n            bmi.append(metric.aggregations())\n            csv += metric.id + \", \"\n\n        # Time to close METRICS\n        ttc = []\n        ttc_metrics = overview_config['time_to_close_metrics']\n        for metric in ttc_metrics:\n            ttc.append(metric.aggregations())\n            csv += metric.id + \", \"\n\n        # generate efficiency file\n        csv = csv[:-2] + \"\\n\"\n        csv = csv.replace(\"_\", \"\")\n        bmi.extend(ttc)\n        for val in bmi:\n            csv += \"%s, \" % str_val(val)\n        if csv[-2:] == \", \":\n            csv = csv[:-2]\n\n        file_name = os.path.join(data_path, 'efficiency.csv')\n        create_csv(file_name, csv)\n        logger.debug(\"Overview metrics generation complete!\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the project activity section of the report.", "response": "def get_sec_project_activity(self):\n        \"\"\"\n        Generate the \"project activity\" section of the report.\n        \"\"\"\n\n        logger.debug(\"Calculating Project Activity metrics.\")\n\n        data_path = os.path.join(self.data_dir, \"activity\")\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n\n        for ds in self.data_sources:\n            metric_file = self.ds2class[ds]\n            metric_index = self.get_metric_index(ds)\n            project_activity = metric_file.project_activity(metric_index, self.start_date,\n                                                            self.end_date)\n            headers = []\n            data_frames = []\n            title_names = []\n            file_name = \"\"\n            for metric in project_activity['metrics']:\n                file_name += metric.DS_NAME + \"_\" + metric.id + \"_\"\n                title_names.append(metric.name)\n                headers.append(metric.id)\n                data_frames.append(metric.timeseries(dataframe=True))\n\n            file_name = file_name[:-1]  # remove trailing underscore\n            file_path = os.path.join(data_path, file_name)\n            title_name = \" & \".join(title_names) + ' per ' + self.interval\n            self.create_csv_fig_from_df(data_frames, file_path, headers,\n                                        fig_type=\"bar\", title=title_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sec_project_community(self):\n\n        logger.debug(\"Calculating Project Community metrics.\")\n\n        data_path = os.path.join(self.data_dir, \"community\")\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n\n        project_community_config = {\n            \"author_metrics\": [],\n            \"people_top_metrics\": [],\n            \"orgs_top_metrics\": []\n        }\n\n        for ds in self.data_sources:\n            metric_file = self.ds2class[ds]\n            metric_index = self.get_metric_index(ds)\n            project_community = metric_file.project_community(metric_index, self.start_date,\n                                                              self.end_date)\n            for section in project_community_config:\n                project_community_config[section] += project_community[section]\n\n        # Get git authors:\n        author = project_community_config['author_metrics'][0]\n        author_ts = author.timeseries(dataframe=True)\n        csv_labels = [author.id]\n        file_label = author.DS_NAME + \"_\" + author.id\n        file_path = os.path.join(data_path, file_label)\n        title_label = author.name + \" per \" + self.interval\n        self.create_csv_fig_from_df([author_ts], file_path, csv_labels, fig_type=\"bar\",\n                                    title=title_label)\n\n        \"\"\"Main developers\"\"\"\n        authors = project_community_config['people_top_metrics'][0]\n        authors_df = authors.aggregations()\n        authors_df = authors_df.head(self.TOP_MAX)\n        authors_df.columns = [authors.id, \"commits\"]\n        file_label = authors.DS_NAME + \"_top_\" + authors.id + \".csv\"\n        file_path = os.path.join(data_path, file_label)\n        authors_df.to_csv(file_path, index=False)\n\n        \"\"\"Main organizations\"\"\"\n        orgs = project_community_config['orgs_top_metrics'][0]\n        orgs_df = orgs.aggregations()\n        orgs_df = orgs_df.head(self.TOP_MAX)\n        orgs_df.columns = [orgs.id, \"commits\"]\n        file_label = orgs.DS_NAME + \"_top_\" + orgs.id + \".csv\"\n        file_path = os.path.join(data_path, file_label)\n        orgs_df.to_csv(file_path, index=False)", "response": "Generate the project community section of the report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sec_project_process(self):\n\n        logger.debug(\"Calculating Project Process metrics.\")\n\n        data_path = os.path.join(self.data_dir, \"process\")\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n\n        project_process_config = {\n            \"bmi_metrics\": [],\n            \"time_to_close_metrics\": [],\n            \"time_to_close_review_metrics\": [],\n            \"patchsets_metrics\": []\n        }\n\n        for ds in self.data_sources:\n            metric_file = self.ds2class[ds]\n            metric_index = self.get_metric_index(ds)\n            project_process = metric_file.project_process(metric_index, self.start_date,\n                                                          self.end_date)\n            for section in project_process_config:\n                project_process_config[section].extend(project_process[section])\n\n        project_process_config[\"time_to_close_title\"] = \"Days to close (median and average)\"\n        project_process_config[\"time_to_close_review_title\"] = \"Days to close review (median and average)\"\n\n        \"\"\"\n        BMI Pull Requests, BMI Issues\n        description: closed PRs/issues out of open PRs/issues in a period of time\n        \"\"\"\n        for bmi_metric in project_process_config['bmi_metrics']:\n            headers = bmi_metric.id\n            dataframe = bmi_metric.timeseries(dataframe=True)\n            file_label = bmi_metric.DS_NAME + \"_\" + bmi_metric.id\n            file_path = os.path.join(data_path, file_label)\n            title_name = bmi_metric.name\n            self.create_csv_fig_from_df([dataframe], file_path, [headers],\n                                        fig_type=\"bar\", title=title_name)\n        \"\"\"\n        Time to close Issues and gerrit and ITS tickets\n        description: median and mean time to close issues\n        \"\"\"\n        i = 0\n        if len(project_process_config['time_to_close_metrics']) > 0:\n            metrics = project_process_config['time_to_close_metrics']\n            while i < len(metrics):\n                file_label = \"\"\n                headers = [metrics[i].id, metrics[i + 1].id]\n                file_label += metrics[i].DS_NAME + \"_\" + metrics[i].id + \"_\"\n                file_label += metrics[i + 1].DS_NAME + \"_\" + metrics[i + 1].id\n                dataframes = [metrics[i].timeseries(dataframe=True),\n                              metrics[i + 1].timeseries(dataframe=True)]\n                title_name = project_process_config['time_to_close_title']\n                file_path = os.path.join(data_path, file_label)\n                self.create_csv_fig_from_df(dataframes, file_path, headers,\n                                            fig_type=\"bar\", title=title_name)\n                i += 2\n        \"\"\"\n        Time to close PRs and gerrit reviews\n        description: median and mean time to close prs and gerrit reviews\n        \"\"\"\n        i = 0\n        if len(project_process_config['time_to_close_review_metrics']) > 0:\n            metrics = project_process_config['time_to_close_review_metrics']\n            while i < len(metrics):\n                file_label = \"\"\n                headers = [metrics[i].id, metrics[i + 1].id]\n                file_label += metrics[i].DS_NAME + \"_\" + metrics[i].id + \"_\"\n                file_label += metrics[i + 1].DS_NAME + \"_\" + metrics[i + 1].id\n                dataframes = [metrics[i].timeseries(dataframe=True),\n                              metrics[i + 1].timeseries(dataframe=True)]\n                title_name = project_process_config['time_to_close_review_title']\n                file_path = os.path.join(data_path, file_label)\n                self.create_csv_fig_from_df(dataframes, file_path, headers,\n                                            fig_type=\"bar\", title=title_name)\n                i += 2\n        \"\"\"\n        Patchsets per review\n        description: median and average of the number of patchsets per review\n        \"\"\"\n        if project_process_config['patchsets_metrics']:\n            file_label = \"\"\n            metrics = project_process_config['patchsets_metrics']\n            headers = [metrics[0].id, metrics[1].id]\n            dataframes = [metrics[0].timeseries(dataframe=True),\n                          metrics[1].timeseries(dataframe=True)]\n            file_label = metrics[0].DS_NAME + \"_\" + metrics[0].id + \"_\"\n            file_label += metrics[1].DS_NAME + \"_\" + metrics[1].id\n            file_path = os.path.join(data_path, file_label)\n            title_name = project_process_config['patchsets_title']\n            self.create_csv_fig_from_df(dataframes, file_path, headers,\n                                        fig_type=\"bar\", title=title_name)", "response": "Generate the project process section of the report."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a CSV file from a dataframe containing timeseries data.", "response": "def create_csv_fig_from_df(self, data_frames=[], filename=None, headers=[], index_label=None,\n                               fig_type=None, title=None, xlabel=None, ylabel=None, xfont=10,\n                               yfont=10, titlefont=15, fig_size=(8, 10), image_type=\"eps\"):\n        \"\"\"\n        Joins all the datafarames horizontally and creates a CSV and an image file from\n        those dataframes.\n\n        :param data_frames: a list of dataframes containing timeseries data from various metrics\n        :param filename: the name of the csv and image file\n        :param headers: a list of headers to be applied to columns of the dataframes\n        :param index_label: name of the index column\n        :param fig_type: figure type. Currently we support 'bar' graphs\n                         default: normal graph\n        :param title: display title of the figure\n        :param filename: file name to save the figure as\n        :param xlabel: label for x axis\n        :param ylabel: label for y axis\n        :param xfont: font size of x axis label\n        :param yfont: font size of y axis label\n        :param titlefont: font size of title of the figure\n        :param fig_size: tuple describing size of the figure (in centimeters) (W x H)\n        :param image_type: the image type to save the image as: jpg, png, etc\n                           default: png\n\n        :returns: creates a csv having name as \"filename\".csv and an image file\n                  having the name as \"filename\".\"image_type\"\n        \"\"\"\n\n        if not data_frames:\n            logger.error(\"No dataframes provided to create CSV\")\n            sys.exit(1)\n        assert(len(data_frames) == len(headers))\n        dataframes = []\n\n        for index, df in enumerate(data_frames):\n            df = df.rename(columns={\"value\": headers[index].replace(\"_\", \"\")})\n            dataframes.append(df)\n        res_df = pd.concat(dataframes, axis=1)\n\n        if \"unixtime\" in res_df:\n            del res_df['unixtime']\n        if not index_label:\n            index_label = \"Date\"\n\n        # Create the CSV file:\n        csv_name = filename + \".csv\"\n        res_df.to_csv(csv_name, index_label=index_label)\n        logger.debug(\"file: {} was created.\".format(csv_name))\n\n        # Create the Image:\n        image_name = filename + \".\" + image_type\n        title = title.replace(\"_\", \"\")\n        figure(figsize=fig_size)\n        plt.subplot(111)\n\n        if fig_type == \"bar\":\n            ax = res_df.plot.bar(figsize=fig_size)\n            ticklabels = res_df.index\n            ax.xaxis.set_major_formatter(matplotlib.ticker.FixedFormatter(ticklabels))\n        else:\n            plt.plot(res_df)\n\n        if not ylabel:\n            ylabel = \"num \" + \" & \".join(headers)\n        if not xlabel:\n            xlabel = index_label\n\n        plt.title(title, fontsize=titlefont)\n        plt.ylabel(ylabel, fontsize=yfont)\n        plt.xlabel(xlabel, fontsize=xfont)\n        plt.grid(True)\n        plt.savefig(image_name)\n        logger.debug(\"Figure {} was generated.\".format(image_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_data_figs(self):\n\n        logger.info(\"Generating the report data and figs from %s to %s\",\n                    self.start_date, self.end_date)\n\n        self.get_sec_overview()\n        self.get_sec_project_activity()\n        self.get_sec_project_community()\n        self.get_sec_project_process()\n\n        logger.info(\"Data and figs done\")", "response": "Generate the data and figs files for the report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the LaTeX report and fill the LaTeX templates with them", "response": "def create(self):\n        \"\"\"\n        Generate the data and figs for the report and fill the LaTeX templates with them\n        to generate a PDF file with the report.\n\n        :return:\n        \"\"\"\n        logger.info(\"Generating the report from %s to %s\", self.start_date, self.end_date)\n\n        self.create_data_figs()\n        self.create_pdf()\n\n        logger.info(\"Report completed\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the values in variable name with data from var in the range start to start + count.", "response": "def set_var_slice(self, name, start, count, var):\n        \"\"\"\n        Overwrite the values in variable name with data\n        from var, in the range (start:start+count).\n        Start, count can be integers for rank 1, and can be\n        tuples of integers for higher ranks.\n        For some implementations it can be equivalent and more efficient to do:\n        `get_var(name)[start[0]:start[0]+count[0], ..., start[n]:start[n]+count[n]] = var`\n        \"\"\"\n        tmp = self.get_var(name).copy()\n        # sometimes we want to slice in 1 dimension, sometimes in more\n        # always slice in arrays\n        start = np.atleast_1d(start)\n        count = np.atleast_1d(count)\n        slices = [np.s_[i:(i+n)] for i,n in zip(start, count)]\n        tmp[slices] = var\n        self.set_var(name, tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_var_index(self, name, index, var):\n        tmp = self.get_var(name).copy()\n        tmp.flat[index] = var\n        self.set_var(name, tmp)", "response": "Set the values in variable name at the flattened ( C - contiguous style )\n        indices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_tables(database):\n    '''Create all tables in the given database'''\n    logging.getLogger(__name__).debug(\"Creating missing database tables\")\n    database.connect()\n    database.create_tables([User,\n                            Group,\n                            UserToGroup,\n                            GroupToCapability,\n                            Capability], safe=True)", "response": "Create all tables in the given database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef populate_with_defaults():\n    '''Create user admin and grant him all permission\n\n    If the admin user already exists the function will simply return\n    '''\n    logging.getLogger(__name__).debug(\"Populating with default users\")\n    if not User.select().where(User.name == 'admin').exists():\n        admin = User.create(name='admin', password='admin')\n        admins = Group.create(name='admins')\n        starCap = Capability.create(domain='.+',\n                                    action=(Action.CREATE |\n                                            Action.READ |\n                                            Action.UPDATE |\n                                            Action.DELETE))\n        admins.capabilities.add(starCap)\n        admin.groups.add(admins)\n        admin.save()\n    if not User.select().where(User.name == 'anonymous').exists():\n        anon = User.create(name='anonymous', password='')\n        anons = Group.create(name='anonymous')\n        readCap = Capability.create(domain=Capability.simToReg('/volumes/*'),\n                                    action=Action.READ)\n        anons.capabilities.add(readCap)\n        anon.groups.add(anons)\n        anon.save()", "response": "Create user admin and grant him all permission\n    Create user anonymous and anonymous groups and return\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize users database initialize database and create necessary tables to handle users oprations.", "response": "def init_db(dbURL, pwd_salt_size=None, pwd_rounds=None):\n    '''Initialize users database\n\n    initialize database and create necessary tables\n    to handle users oprations.\n\n    :param dbURL: database url, as described in :func:`init_proxy`\n    '''\n    if not dbURL:\n        dbURL = 'sqlite:///:memory:'\n    logging.getLogger(__name__).debug(\"Initializing database: {}\".format(dict(url=dbURL,\n                                                                              pwd_salt_size=pwd_salt_size,\n                                                                              pwd_rounds=pwd_rounds)))\n    try:\n        db = init_proxy(dbURL)\n        global pwdCryptCtx\n        pwdCryptCtx = gen_crypt_context(salt_size=pwd_salt_size, rounds=pwd_rounds)\n        create_tables(db)\n        return db\n    except Exception as e:\n        e.args = (e.args[0] + ' [users database]',)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize(s, replace_spaces=True):\n    whitelist = (' -' + string.ascii_letters + string.digits)\n\n    if type(s) == six.binary_type:\n        s = six.text_type(s, 'utf-8', 'ignore')\n\n    table = {}\n    for ch in [ch for ch in s if ch not in whitelist]:\n        if ch not in table:\n            try:\n                replacement = unicodedata.normalize('NFKD', ch)[0]\n                if replacement in whitelist:\n                    table[ord(ch)] = replacement\n                else:\n                    table[ord(ch)] = u'_'\n            except:\n                table[ord(ch)] = u'_'\n\n    if replace_spaces:\n        return s.translate(table).replace(u'_', u'').replace(' ', '_')\n    else:\n        return s.translate(table).replace(u'_', u'')", "response": "Normalize non - ascii characters to their closest ascii counterparts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of Robot Framework - compatible cli - variables parsed from ROBOT_ - prefixed environment variable", "response": "def get_robot_variables():\n    \"\"\"Return list of Robot Framework -compatible cli-variables parsed\n    from ROBOT_-prefixed environment variable\n\n    \"\"\"\n    prefix = 'ROBOT_'\n    variables = []\n\n    def safe_str(s):\n        if isinstance(s, six.text_type):\n            return s\n        else:\n            return six.text_type(s, 'utf-8', 'ignore')\n    for key in os.environ:\n        if key.startswith(prefix) and len(key) > len(prefix):\n            variables.append(safe_str(\n                '%s:%s' % (key[len(prefix):], os.environ[key]),\n            ))\n    return variables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge(a, b):\n    global loglevel\n\n    # Iterate throughout the currently merged node set\n    for child in b.iterchildren():\n\n        # Merge suites\n        if child.tag == 'suite':\n            source = child.get('source')\n            suites = a.xpath('suite[@source=\"%s\"]' % source)\n            # When matching suite is found, merge\n            if suites:\n                merge(suites[0], child)\n            # When no matching suite is found, append and fix ids\n            else:\n                suites = a.xpath('suite')\n                child_id = child.get('id')\n                parent_id = a.get('id', '')\n                if parent_id:\n                    suite_id = '%s-s%s' % (parent_id, str(len(suites) + 1))\n                else:\n                    suite_id = 's%s' % str(len(suites) + 1)\n                for node in child.xpath('//*[contains(@id, \"%s\")]' % child_id):\n                    node.set('id', re.sub('^%s' % child_id, suite_id,\n                                          node.get('id')))\n                a.append(child)\n                # Gather separate top-level suites into a single top-level\n                # suite\n            if a.tag == 'robot' and a.xpath('suite[@source]'):\n                for merge_root in a.xpath('suite[not(@source)]'):\n                    mergeable = etree.Element('robot')\n                    for suite in a.xpath('suite[@source]'):\n                        mergeable.append(suite)\n                    merge(merge_root, mergeable)\n        # Merge keywords\n        elif child.tag == 'kw':\n            name = child.get('name')\n            type_ = child.get('type')\n            keywords = a.xpath('kw[@name=\"%s\" and @type=\"%s\"]' % (name, type_))\n            # When matching keyword is found, merge\n            if len(keywords) == 1:\n                merge(keywords[0], child)\n            # When multiple matching keywords is found, merge with position\n            elif len(keywords) > 1:\n                child_keywords = child.getparent().xpath(\n                    'kw[@name=\"%s\" and @type=\"%s\"]' % (name, type_))\n                child_position = child_keywords.index(child)\n                merge(keywords[min(child_position, len(keywords) - 1)], child)\n            # When no matching suite is found, append\n            else:\n                a.append(child)\n\n        # Merge (append) tests\n        elif child.tag == 'test':\n            tests = a.xpath('test')\n            child.set('id', '%s-t%s' % (a.get('id'), str(len(tests) + 1)))\n            a.append(child)\n\n        # Merge (append) statuses\n        elif child.tag == 'status':\n            a.append(child)\n\n        # Merge statistics\n        elif child.tag == 'statistics':\n            statistics = a.xpath('statistics')\n            # When no statistics are found, append to root\n            if not statistics:\n                a.append(child)\n            # When statistics are found, merge matching or append\n            else:\n                for grandchild in child.xpath('total'):\n                    totals = a.xpath('statistics/total')\n                    if totals:\n                        merge(totals[0], grandchild)\n                    else:\n                        statistics.append(child)\n                for grandchild in child.xpath('suite'):\n                    suites = a.xpath('statistics/suite')\n                    if suites:\n                        merge(suites[0], grandchild)\n                    else:\n                        statistics.append(child)\n\n        # Merge individual statistics\n        elif child.tag == 'stat':\n            stats = a.xpath('stat[text() = \"%s\"]' % child.text)\n            if stats:\n                stats[0].set('fail', str(int(stats[0].get('fail'))\n                                         + int(child.get('fail'))))\n                stats[0].set('pass', str(int(stats[0].get('pass'))\n                                         + int(child.get('pass'))))\n            else:\n                suites = a.xpath('//suite[@name=\"%s\"]' % child.get('name'))\n                if suites:\n                    child.set('id', suites[0].get('id'))\n                a.append(child)\n\n        # Merge errors\n        elif child.tag == 'errors':\n            errors = a.xpath('errors')\n            # Filter by loglevel\n            for grandchild in tuple(child.iterchildren()):\n                try:\n                    level = int(getattr(logging, grandchild.get('level')))\n                except (TypeError, AttributeError, ValueError):\n                    level = 0\n                if level < loglevel:\n                    child.remove(grandchild)\n            # When no errors are found, append to root\n            if not errors:\n                a.append(child)\n            # When errors are found, append the children\n            else:\n                for grandchild in child.iterchildren():\n                    errors[0].append(grandchild)", "response": "Merge two unicode Robot Framework reports so that report a is merged\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconfigure the device. Send the device configuration saved inside the MCP342x object to the target device.", "response": "def configure(self):\n        \"\"\"Configure the device.\n\n        Send the device configuration saved inside the MCP342x object to the target device.\"\"\"\n        logger.debug('Configuring ' + hex(self.get_address())\n                     + ' ch: ' + str(self.get_channel())\n                     + ' res: ' + str(self.get_resolution())\n                     + ' gain: ' + str(self.get_gain()))\n        self.bus.write_byte(self.address, self.config)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(self):\n        c = self.config\n        c &= (~MCP342x._continuous_mode_mask & 0x7f)  # Force one-shot\n        c |= MCP342x._not_ready_mask                  # Convert\n        logger.debug('Convert ' + hex(self.address) + ' config: ' + bin(c))\n        self.bus.write_byte(self.address, c)", "response": "Initiate one - shot conversion."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the first date present in the index", "response": "def get_first_date_of_index(elastic_url, index):\n    \"\"\"Get the first/min date present in the index\"\"\"\n    es = Elasticsearch(elastic_url)\n    search = Search(using=es, index=index)\n    agg = A(\"min\", field=\"grimoire_creation_date\")\n    search.aggs.bucket(\"1\", agg)\n    search = search.extra(size=0)\n    response = search.execute()\n    start_date = response.to_dict()['aggregations']['1']['value_as_string'][:10]\n    return start_date"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a dict with the filters to be applied ({ name1 : value1 name2 : value2... } to a list of query objects which can be used together in a query using boolean MimeType combination logic.", "response": "def __get_query_filters(cls, filters={}, inverse=False):\n        \"\"\"\n        Convert a dict with the filters to be applied ({\"name1\":\"value1\", \"name2\":\"value2\"})\n        to a list of query objects which can be used together in a query using boolean\n        combination logic.\n\n        :param filters: dict with the filters to be applied\n        :param inverse: if True include all the inverse filters (the one starting with *)\n        :return: a list of es_dsl 'MatchPhrase' Query objects\n                 Ex: [MatchPhrase(name1=\"value1\"), MatchPhrase(name2=\"value2\"), ..]\n                 Dict representation of the object: {'match_phrase': {'field': 'home'}}\n        \"\"\"\n        query_filters = []\n\n        for name in filters:\n            if name[0] == '*' and not inverse:\n                # An inverse filter and not inverse mode\n                continue\n            if name[0] != '*' and inverse:\n                # A direct filter and inverse mode\n                continue\n            field_name = name[1:] if name[0] == '*' else name\n\n            params = {field_name: filters[name]}\n            # trying to use es_dsl only and not creating hard coded queries\n            query_filters.append(Q('match_phrase', **params))\n\n        return query_filters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a filter dict with date_field from start to end dates.", "response": "def __get_query_range(cls, date_field, start=None, end=None):\n        \"\"\"\n        Create a filter dict with date_field from start to end dates.\n\n        :param date_field: field with the date value\n        :param start: date with the from value. Should be a datetime.datetime object\n                      of the form: datetime.datetime(2018, 5, 25, 15, 17, 39)\n        :param end: date with the to value. Should be a datetime.datetime object\n                      of the form: datetime.datetime(2018, 5, 25, 15, 17, 39)\n        :return: a dict containing a range filter which can be used later in an\n                 es_dsl Search object using the `filter` method.\n        \"\"\"\n        if not start and not end:\n            return ''\n\n        start_end = {}\n        if start:\n            start_end[\"gte\"] = \"%s\" % start.isoformat()\n        if end:\n            start_end[\"lte\"] = \"%s\" % end.isoformat()\n\n        query_range = {date_field: start_end}\n        return query_range"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a query object containing the required parameters for the get_resource_list method.", "response": "def __get_query_basic(cls, date_field=None, start=None, end=None,\n                          filters={}):\n        \"\"\"\n        Create a es_dsl query object with the date range and filters.\n\n        :param date_field: field with the date value\n        :param start: date with the from value, should be a datetime.datetime object\n        :param end: date with the to value, should be a datetime.datetime object\n        :param filters: dict with the filters to be applied\n        :return: a DSL query containing the required parameters\n                 Ex: {'query': {'bool': {'filter': [{'range': {'DATE_FIELD':\n                                                        {'gte': '2015-05-19T00:00:00',\n                                                         'lte': '2018-05-18T00:00:00'}}}],\n                    'must': [{'match_phrase': {'first_name': 'Jhon'}},\n                             {'match_phrase': {'last_name': 'Doe'}},\n                             {'match_phrase': {'Phone': 2222222}}\n                             ]}}}\n        \"\"\"\n        query_basic = Search()\n\n        query_filters = cls.__get_query_filters(filters)\n        for f in query_filters:\n            query_basic = query_basic.query(f)\n\n        query_filters_inverse = cls.__get_query_filters(filters, inverse=True)\n        # Here, don't forget the '~'. That is what makes this an inverse filter.\n        for f in query_filters_inverse:\n            query_basic = query_basic.query(~f)\n\n        if not date_field:\n            query_range = {}\n        else:\n            query_range = cls.__get_query_range(date_field, start, end)\n\n        # Applying the range filter\n        query_basic = query_basic.filter('range', **query_range)\n\n        return query_basic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an es_dsl aggregation object based on a term.", "response": "def __get_query_agg_terms(cls, field, agg_id=None):\n        \"\"\"\n        Create a es_dsl aggregation object based on a term.\n\n        :param field: field to be used to aggregate\n        :return: a tuple with the aggregation id and es_dsl aggregation object. Ex:\n                {\n                    \"terms\": {\n                        \"field\": <field>,\n                        \"size:\": <size>,\n                        \"order\":{\n                            \"_count\":\"desc\"\n                    }\n                }\n        Which will then be used as Search.aggs.bucket(agg_id, query_agg) method\n        to add aggregations to the es_dsl Search object\n        \"\"\"\n        if not agg_id:\n            agg_id = cls.AGGREGATION_ID\n        query_agg = A(\"terms\", field=field, size=cls.AGG_SIZE, order={\"_count\": \"desc\"})\n        return (agg_id, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an es_dsl aggregation object for getting the max value of a field.", "response": "def __get_query_agg_max(cls, field, agg_id=None):\n        \"\"\"\n        Create an es_dsl aggregation object for getting the max value of a field.\n\n        :param field: field from which the get the max value\n        :return: a tuple with the aggregation id and es_dsl aggregation object. Ex:\n                {\n                    \"max\": {\n                        \"field\": <field>\n                }\n        \"\"\"\n        if not agg_id:\n            agg_id = cls.AGGREGATION_ID\n        query_agg = A(\"max\", field=field)\n        return (agg_id, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an es_dsl aggregation object for getting the percentiles values of a field.", "response": "def __get_query_agg_percentiles(cls, field, agg_id=None):\n        \"\"\"\n        Create an es_dsl aggregation object for getting the percentiles value of a field.\n        In general this is used to get the median (0.5) percentile.\n\n        :param field: field from which the get the percentiles values\n        :return: a tuple with the aggregation id and es_dsl aggregation object. Ex:\n                {\n                    \"percentile\": {\n                        \"field\": <field>\n                }\n        \"\"\"\n        if not agg_id:\n            agg_id = cls.AGGREGATION_ID\n        query_agg = A(\"percentiles\", field=field)\n        return (agg_id, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an es_dsl aggregation object for getting the average value of a field.", "response": "def __get_query_agg_avg(cls, field, agg_id=None):\n        \"\"\"\n        Create an es_dsl aggregation object for getting the average value of a field.\n\n        :param field: field from which the get the average value\n        :return: a tuple with the aggregation id and es_dsl aggregation object. Ex:\n                {\n                    \"avg\": {\n                        \"field\": <field>\n                }\n        \"\"\"\n        if not agg_id:\n            agg_id = cls.AGGREGATION_ID\n        query_agg = A(\"avg\", field=field)\n        return (agg_id, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an es_dsl aggregation object for getting the approximate count of distinct values of a field.", "response": "def __get_query_agg_cardinality(cls, field, agg_id=None):\n        \"\"\"\n        Create an es_dsl aggregation object for getting the approximate count of distinct values of a field.\n\n        :param field: field from which the get count of distinct values\n        :return: a tuple with the aggregation id and es_dsl aggregation object. Ex:\n                {\n                    \"cardinality\": {\n                        \"field\": <field>,\n                        \"precision_threshold\": 3000\n                }\n        \"\"\"\n        if not agg_id:\n            agg_id = cls.AGGREGATION_ID\n        query_agg = A(\"cardinality\", field=field, precision_threshold=cls.ES_PRECISION)\n        return (agg_id, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dict with the bounds for a date_histogram aggregation.", "response": "def __get_bounds(cls, start=None, end=None):\n        \"\"\"\n        Return a dict with the bounds for a date_histogram agg.\n\n        :param start: date from for the date_histogram agg, should be a datetime.datetime object\n        :param end: date to for the date_histogram agg, should be a datetime.datetime object\n        :return: a dict with the DSL bounds for a date_histogram aggregation\n        \"\"\"\n        bounds = {}\n        if start or end:\n            # Extend bounds so we have data until start and end\n            start_ts = None\n            end_ts = None\n            if start:\n                # elasticsearch is unable to convert date with microseconds into long\n                # format for processing, hence we convert microseconds to zero\n                start = start.replace(microsecond=0)\n                start_ts = start.replace(tzinfo=timezone.utc).timestamp()\n                start_ts_ms = start_ts * 1000  # ES uses ms\n            if end:\n                end = end.replace(microsecond=0)\n                end_ts = end.replace(tzinfo=timezone.utc).timestamp()\n                end_ts_ms = end_ts * 1000  # ES uses ms\n\n            bounds_data = {}\n            if start:\n                bounds_data[\"min\"] = start_ts_ms\n            if end:\n                bounds_data[\"max\"] = end_ts_ms\n\n            bounds[\"extended_bounds\"] = bounds_data\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an es_dsl aggregation object for calculating the time series values for a field.", "response": "def __get_query_agg_ts(cls, field, time_field, interval=None,\n                           time_zone=None, start=None, end=None,\n                           agg_type='count', offset=None):\n        \"\"\"\n        Create an es_dsl aggregation object for getting the time series values for a field.\n\n        :param field: field to get the time series values\n        :param time_field: field with the date\n        :param interval: interval to be used to generate the time series values, such as:(year(y),\n                         quarter(q), month(M), week(w), day(d), hour(h), minute(m), second(s))\n        :param time_zone: time zone for the time_field\n        :param start: date from for the time series, should be a datetime.datetime object\n        :param end: date to for the time series, should be a datetime.datetime object\n        :param agg_type: kind of aggregation for the field (cardinality, avg, percentiles)\n        :param offset: offset to be added to the time_field in days\n        :return: a aggregation object to calculate timeseries values of a field\n        \"\"\"\n        \"\"\" Time series for an aggregation metric \"\"\"\n        if not interval:\n            interval = '1M'\n        if not time_zone:\n            time_zone = 'UTC'\n\n        if not field:\n            field_agg = ''\n        else:\n            if agg_type == \"cardinality\":\n                agg_id, field_agg = cls.__get_query_agg_cardinality(field, agg_id=cls.AGGREGATION_ID + 1)\n            elif agg_type == \"avg\":\n                agg_id, field_agg = cls.__get_query_agg_avg(field, agg_id=cls.AGGREGATION_ID + 1)\n            elif agg_type == \"percentiles\":\n                agg_id, field_agg = cls.__get_query_agg_percentiles(field, agg_id=cls.AGGREGATION_ID + 1)\n            else:\n                raise RuntimeError(\"Aggregation of %s in ts not supported\" % agg_type)\n\n        bounds = {}\n        if start or end:\n            if not offset:\n                # With offset and quarter interval bogus buckets are added\n                # to the start and to the end if extended_bounds is used\n                # https://github.com/elastic/elasticsearch/issues/23776\n                bounds = cls.__get_bounds(start, end)\n            else:\n                bounds = {'offset': offset}\n\n        query_agg = A(\"date_histogram\", field=time_field, interval=interval,\n                      time_zone=time_zone, min_doc_count=0, **bounds)\n\n        agg_dict = field_agg.to_dict()[field_agg.name]\n        query_agg.bucket(agg_id, field_agg.name, **agg_dict)\n\n        return (cls.AGGREGATION_ID, query_agg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a DSL query for counting the number of items in a resource.", "response": "def get_count(cls, date_field=None, start=None, end=None, filters={}):\n        \"\"\"\n        Build the DSL query for counting the number of items.\n\n        :param date_field: field with the date\n        :param start: date from which to start counting, should be a datetime.datetime object\n        :param end: date until which to count items, should be a datetime.datetime object\n        :param filters: dict with the filters to be applied\n        :return: a DSL query with size parameter\n        \"\"\"\n        \"\"\" Total number of items \"\"\"\n        query_basic = cls.__get_query_basic(date_field=date_field,\n                                            start=start, end=end,\n                                            filters=filters)\n        # size=0 gives only the count and not the hits\n        query = query_basic.extra(size=0)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_agg(cls, field=None, date_field=None, start=None, end=None,\n                filters={}, agg_type=\"terms\", offset=None, interval=None):\n        \"\"\"\n        Compute the aggregated value for a field.\n        :param field: field to get the time series values\n        :param date_field: field with the date\n        :param interval: interval to be used to generate the time series values, such as:(year(y),\n                         quarter(q), month(M), week(w), day(d), hour(h), minute(m), second(s))\n        :param start: date from for the time series, should be a datetime.datetime object\n        :param end: date to for the time series, should be a datetime.datetime object\n        :param agg_type: kind of aggregation for the field (cardinality, avg, percentiles)\n        :param offset: offset to be added to the time_field in days\n        :return: a query containing the aggregation, filters and range for the specified term\n        \"\"\"\n        # This gives us the basic structure of the query, including:\n        # Normal and inverse filters and range.\n        s = cls.__get_query_basic(date_field=date_field, start=start, end=end, filters=filters)\n\n        # Get only the aggs not the hits\n        # Default count starts from 0\n        s = s.extra(size=0)\n\n        if agg_type == \"count\":\n            agg_type = 'cardinality'\n        elif agg_type == \"median\":\n            agg_type = 'percentiles'\n        elif agg_type == \"average\":\n            agg_type = 'avg'\n\n        if not interval:\n            if agg_type == \"terms\":\n                agg_id, query_agg = ElasticQuery.__get_query_agg_terms(field)\n            elif agg_type == \"max\":\n                agg_id, query_agg = ElasticQuery.__get_query_agg_max(field)\n            elif agg_type == \"cardinality\":\n                agg_id, query_agg = ElasticQuery.__get_query_agg_cardinality(field)\n            elif agg_type == \"percentiles\":\n                agg_id, query_agg = ElasticQuery.__get_query_agg_percentiles(field)\n            elif agg_type == \"avg\":\n                agg_id, query_agg = ElasticQuery.__get_query_agg_avg(field)\n            else:\n                raise RuntimeError(\"Aggregation of %s not supported\" % agg_type)\n        else:\n            agg_id, query_agg = ElasticQuery.__get_query_agg_ts(field, date_field,\n                                                                start=start, end=end,\n                                                                interval=interval,\n                                                                agg_type=agg_type,\n                                                                offset=offset)\n        s.aggs.bucket(agg_id, query_agg)\n\n        return s.to_dict()", "response": "Compute the aggregated value for a field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the fields are fetched and populate them if not.", "response": "def ensure_fields(self, fields, force_refetch=False):\n        \"\"\" Makes sure we fetched the fields, and populate them if not. \"\"\"\n\n        # We fetched with fields=None, we should have fetched them all\n        if self._fetched_fields is None or self._initialized_with_doc:\n            return\n\n        if force_refetch:\n            missing_fields = fields\n        else:\n            missing_fields = [f for f in fields if f not in self._fetched_fields]\n\n        if len(missing_fields) == 0:\n            return\n\n        if \"_id\" not in self:\n            raise Exception(\"Can't ensure_fields because _id is missing\")\n\n        self.refetch_fields(missing_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refetch_fields(self, missing_fields):\n        db_fields = self.mongokat_collection.find_one({\"_id\": self[\"_id\"]}, fields={k: 1 for k in missing_fields})\n\n        self._fetched_fields += tuple(missing_fields)\n\n        if not db_fields:\n            return\n\n        for k, v in db_fields.items():\n            self[k] = v", "response": "Refetches a list of fields from the DB"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the specified list of fields from both the local object and the DB.", "response": "def unset_fields(self, fields):\n        \"\"\" Removes this list of fields from both the local object and the DB. \"\"\"\n\n        self.mongokat_collection.update_one({\"_id\": self[\"_id\"]}, {\"$unset\": {\n            f: 1 for f in fields\n        }})\n\n        for f in fields:\n            if f in self:\n                del self[f]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreload the _id of the object.", "response": "def reload(self):\n        \"\"\"\n        allow to refresh the document, so after using update(), it could reload\n        its value from the database.\n\n        Be carreful : reload() will erase all unsaved values.\n\n        If no _id is set in the document, a KeyError is raised.\n\n        \"\"\"\n\n        old_doc = self.mongokat_collection.find_one({\"_id\": self['_id']}, read_use=\"primary\")\n\n        if not old_doc:\n            raise OperationFailure('Can not reload an unsaved document.'\n                                   ' %s is not found in the database. Maybe _id was a string and not ObjectId?' % self['_id'])\n        else:\n            for k in list(self.keys()):\n                del self[k]\n            self.update(dotdict(old_doc))\n\n        self._initialized_with_doc = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, force=False, uuid=False, **kwargs):\n\n        if not self._initialized_with_doc and not force:\n            raise Exception(\"Cannot save a document not initialized from a Python dict. This might remove fields from the DB!\")\n\n        self._initialized_with_doc = False\n\n        if '_id' not in self:\n            if uuid:\n                self['_id'] = str(\"%s-%s\" % (self.mongokat_collection.__class__.__name__, uuid4()))\n\n        return self.mongokat_collection.save(self, **kwargs)", "response": "Saves the object to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_partial(self, data=None, allow_protected_fields=False, **kwargs):\n\n        # Backwards compat, deprecated argument\n        if \"dotnotation\" in kwargs:\n            del kwargs[\"dotnotation\"]\n\n        if data is None:\n\n            data = dotdict(self)\n            if \"_id\" not in data:\n                raise KeyError(\"_id must be set in order to do a save_partial()\")\n            del data[\"_id\"]\n\n        if len(data) == 0:\n          return\n\n        if not allow_protected_fields:\n            self.mongokat_collection._check_protected_fields(data)\n\n        apply_on = dotdict(self)\n\n        self._initialized_with_doc = False\n\n        self.mongokat_collection.update_one({\"_id\": self[\"_id\"]}, {\"$set\": data}, **kwargs)\n\n        for k, v in data.items():\n            apply_on[k] = v\n\n        self.update(dict(apply_on))", "response": "Saves the currently set fields in the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_default_args(tool_name):\n    global opinel_arg_dir\n    \n    profile_name = 'default'\n    # h4ck to have an early read of the profile name\n    for i, arg in enumerate(sys.argv):\n        if arg == '--profile' and len(sys.argv) >= i + 1:\n            profile_name = sys.argv[i + 1]\n    #if not os.path.isdir(opinel_arg_dir):\n    #    os.makedirs(opinel_arg_dir)            \n    if not os.path.isdir(opinel_arg_dir):\n        try:\n            os.makedirs(opinel_arg_dir)\n        except:\n            # Within AWS Lambda, home directories are not writable. This attempts to detect that...\n            #  ...and uses the /tmp folder, which *is* writable in AWS Lambda\n            opinel_arg_dir = os.path.join(tempfile.gettempdir(), '.aws/opinel')\n            if not os.path.isdir(opinel_arg_dir):\n                os.makedirs(opinel_arg_dir)\n    opinel_arg_file = os.path.join(opinel_arg_dir, '%s.json' % profile_name)\n    default_args = {}\n    if os.path.isfile(opinel_arg_file):\n        with open(opinel_arg_file, 'rt') as f:\n            all_args = json.load(f)\n        for target in all_args:\n            if tool_name.endswith(target):\n                default_args.update(all_args[target])\n        for k in all_args['shared']:\n            if k not in default_args:\n                default_args[k] = all_args['shared'][k]\n    return default_args", "response": "Reads the default arguments for a given tool."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prompt(test_input = None):\n    if test_input != None:\n        if type(test_input) == list and len(test_input):\n            choice = test_input.pop(0)\n        elif type(test_input) == list:\n            choice = ''\n        else:\n            choice = test_input\n    else:\n        # Coverage: 4 missed statements\n        try:\n            choice = raw_input()\n        except:\n            choice = input()\n    return choice", "response": "Prompt user for a new item from the list of items in the order they are in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt the user for a MFA code.", "response": "def prompt_4_mfa_code(activate = False, input = None):\n    \"\"\"\n    Prompt for an MFA code\n\n    :param activate:                    Set to true when prompting for the 2nd code when activating a new MFA device\n    :param input:                       Used for unit testing\n\n    :return:                            The MFA code\n    \"\"\"\n    while True:\n        if activate:\n            prompt_string = 'Enter the next value: '\n        else:\n            prompt_string = 'Enter your MFA code (or \\'q\\' to abort): '\n        mfa_code = prompt_4_value(prompt_string, no_confirm = True, input = input)\n        try:\n            if mfa_code == 'q':\n                return mfa_code\n            int(mfa_code)\n            mfa_code[5]\n            break\n        except:\n            printError('Error: your MFA code must only consist of digits and be at least 6 characters long.')\n    return mfa_code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prompt_4_mfa_serial(input = None):\n    return prompt_4_value('Enter your MFA serial:', required = False, regex = re_mfa_serial_format, regex_format = mfa_serial_format, input = input)", "response": "Prompts the user for a MFA serial number"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prompt_4_overwrite(filename, force_write, input = None):\n    if not os.path.exists(filename) or force_write:\n        return True\n    return prompt_4_yes_no('File \\'{}\\' already exists. Do you want to overwrite it'.format(filename), input = input)", "response": "Prompt whether the file should be overwritten"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprompting the user for a value for a given list of authorized answers and returns the value for that answer.", "response": "def prompt_4_value(question, choices = None, default = None, display_choices = True, display_indices = False, authorize_list = False, is_question = False, no_confirm = False, required = True, regex = None, regex_format = '', max_laps = 5, input = None, return_index = False):\n    \"\"\"\n    Prompt for a value\n                                        .                    .\n    :param question:                    Question to be asked\n    :param choices:                     List of authorized answers\n    :param default:                     Value suggested by default\n    :param display_choices:             Display accepted choices\n    :param display_indices:             Display the indice in the list next to the choice\n    :param authorize_list:              Set to true if a list of answers may be accepted\n    :param is_question:                 Set to true to append a question mark\n    :param no_confirm:                  Set to true to not prompt for a confirmation of the value\n    :param required:                    Set to false if an empty answer is authorized\n    :param regex:                       TODO\n    :param regex_format                 TODO\n    :param max_laps:                    Exit after N laps\n    :param input:                       Used for unit testing\n\n    :return:\n    \"\"\"\n    if choices and display_choices and not display_indices:\n        question = question + ' (' + '/'.join(choices) + ')'\n    lap_n = 0\n    while True:\n        if lap_n >= max_laps:\n            printError('Automatically abording prompt loop after 5 failures')\n            return None\n        lap_n += 1\n        can_return = False\n        # Display the question, choices, and prompt for the answer\n        if is_question:\n            question = question + '? '\n        printError(question)\n        if choices and display_indices:\n            for c in choices:\n                printError('%3d. %s' % (choices.index(c), c))\n            printError('Enter the number corresponding to your choice: ', False)\n        choice = prompt(input)\n        # Set the default value if empty choice\n        if not choice or choice == '':\n            if default:\n                if no_confirm or prompt_4_yes_no('Use the default value (' + default + ')'):\n                    #return default\n                    choice = default\n                    can_return = True\n            elif not required:\n                can_return = True\n            else:\n                printError('Error: you cannot leave this parameter empty.')\n        # Validate the value against a whitelist of choices\n        elif choices:\n            user_choices = [item.strip() for item in choice.split(',')]\n            if not authorize_list and len(user_choices) > 1:\n                printError('Error: multiple values are not supported; please enter a single value.')\n            else:\n                choice_valid = True\n                if display_indices and int(choice) < len(choices):\n                    int_choice = choice\n                    choice = choices[int(choice)]\n                else:\n                    for c in user_choices:\n                        if not c in choices:\n                            printError('Invalid value (%s).' % c)\n                            choice_valid = False\n                            break\n                if choice_valid:\n                    can_return = True\n        # Validate against a regex\n        elif regex:\n            if regex.match(choice):\n                #return choice\n                can_return = True\n            else:\n                printError('Error: expected format is: %s' % regex_format)\n        else:\n            # No automated validation, can attempt to return\n            can_return = True\n        if can_return:\n            # Manually onfirm that the entered value is correct if needed\n            if no_confirm or prompt_4_yes_no('You entered \"' + choice + '\". Is that correct', input=input):\n                return int(int_choice) if return_index else choice"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprompting the user for a yes or no answer.", "response": "def prompt_4_yes_no(question, input = None):\n    \"\"\"\n    Prompt for a yes/no or y/n answer\n                                        .\n    :param question:                    Question to be asked\n    :param input:                       Used for unit testing\n\n    :return:                            True for yes/y, False for no/n\n    \"\"\"\n    count = 0\n    while True:\n        printError(question + ' (y/n)? ')\n        choice = prompt(input).lower()\n        if choice == 'yes' or choice == 'y':\n            return True\n        elif choice == 'no' or choice == 'n':\n            return False\n        else:\n            count += 1\n            printError('\\'%s\\' is not a valid answer. Enter \\'yes\\'(y) or \\'no\\'(n).' % choice)\n            if count > 3:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to import the specified module from the given Python path and return the object or None.", "response": "def safe_import_module(path, default=None):\n    \"\"\"\n    Try to import the specified module from the given Python path\n    \n    @path is a string containing a Python path to the wanted module, @default is \n    an object to return if import fails, it can be None, a callable or whatever you need.\n    \n    Return a object or None\n    \"\"\"\n    if path is None:\n        return default\n    \n    dot = path.rindex('.')\n    module_name = path[:dot]\n    class_name = path[dot + 1:]\n    try:\n        _class = getattr(import_module(module_name), class_name)\n        return _class\n    except (ImportError, AttributeError) as e:\n        raise FeedparserError(\"{}: {}\".format(e.__class__.__name__, e.message))\n    \n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_feed_renderer(engines, name):\n    if name not in engines:\n        raise FeedparserError(\"Given feed name '{}' does not exists in 'settings.FEED_RENDER_ENGINES'\".format(name))\n    \n    renderer = safe_import_module(engines[name])\n    \n    return renderer", "response": "Load the renderer class from the given engine name and return the renderer class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear_line(mode=2):\n    ''' Clear the current line.\n\n        Arguments:\n\n            mode:  | 0 | 'forward'  | 'right' - Clear cursor to end of line.\n                   | 1 | 'backward' | 'left'  - Clear cursor to beginning of line.\n                   | 2 | 'full'               - Clear entire line.\n\n        Note:\n            Cursor position does not change.\n    '''\n    text = sc.erase_line(_mode_map.get(mode, mode))\n    _write(text)\n    return text", "response": "Clear the current line."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the terminal screen.", "response": "def clear_screen(mode=2):\n    ''' Clear the terminal/console screen. (Also aliased to clear.)\n\n        Arguments:\n\n            mode:  | 0 | 'forward'   - Clear cursor to end of screen, cursor stays.\n                   | 1 | 'backward'  - Clear cursor to beginning of screen, \"\"\n                   | 2 | 'full'      - Clear entire visible screen, cursor to 1,1.\n                   | 3 | 'history'   - Clear entire visible screen and scrollback\n                                       buffer (xterm).\n    '''\n    text = sc.erase(_mode_map.get(mode, mode))\n    _write(text)\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset_terminal():\n    ''' Reset the terminal/console screen. (Also aliased to cls.)\n\n        Greater than a fullscreen terminal clear, also clears the scrollback\n        buffer.  May expose bugs in dumb terminals.\n    '''\n    if os.name == 'nt':\n        from .windows import cls\n        cls()\n    else:\n        text = sc.reset\n        _write(text)\n        return text", "response": "Reset the terminal screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_title(title, mode=0):\n    ''' Set the title of the terminal window/tab/icon.\n\n        Arguments:\n            title:  str\n            mode:  | 0 | 'both'   - Set icon/taskbar and window/tab title\n                   | 1 | 'icon'   - Set only icon/taskbar title\n                   | 2 | 'title'  - Set only window/tab title\n    '''\n    if os.name == 'nt':\n        from .windows import set_title\n        return set_title(title)\n    else:\n        if _CHOSEN_PALETTE:\n            text = f'{OSC}{_title_mode_map.get(mode, mode)};{title}{BEL}'\n            _write(text)\n            return text", "response": "Set the title of the terminal window / tab / icon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_ansi(text, c1=False, osc=False):\n    ''' Strip ANSI escape sequences from a portion of text.\n        https://stackoverflow.com/a/38662876/450917\n\n        Arguments:\n            line: str\n            osc: bool  - include OSC commands in the strippage.\n            c1:  bool  - include C1 commands in the strippage.\n\n        Notes:\n            Enabling c1 and osc stripping is less efficient and the two options\n            can mildly conflict with one another.\n            The less problematic order was chosen, so there may still be rare\n            C1 OSC fragments left over.\n    '''\n    text = ansi_csi0_finder.sub('', text)\n    if osc:\n        text = ansi_osc0_finder.sub('', text)\n    if c1:\n        text = ansi_csi1_finder.sub('', text)  # go first, less destructive\n        if osc:\n            text = ansi_osc1_finder.sub('', text)\n    return text", "response": "Strip ANSI escape sequences from a portion of text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait_key(keys=None):\n    ''' Waits for a keypress at the console and returns it.\n        \"Where's the any key?\"\n\n        Arguments:\n            keys - if passed, wait for this specific key, e.g. ESC.\n                   may be a tuple.\n        Returns:\n            char or ESC - depending on key hit.\n            None - immediately under i/o redirection, not an interactive tty.\n    '''\n    if is_a_tty():\n        if keys:\n            if not isinstance(keys, tuple):\n                keys = (keys,)\n            while True:\n                key = _getch()\n                if key in keys:\n                    return key\n        else:\n            return _getch()", "response": "Waits for a keypress at the console and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a query fragment from a query string.", "response": "def build_query_fragment(query):\n    \"\"\"\n    <query xmlns=\"http://basex.org/rest\">\n        <text><![CDATA[ (//city/name)[position() <= 5] ]]></text>\n    </query>\n    \"\"\"\n    root = etree.Element('query', nsmap={None: 'http://basex.org/rest'})\n    text = etree.SubElement(root, 'text')\n    text.text = etree.CDATA(query.strip())\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef feedparser_render(url, *args, **kwargs):\n    renderer_name = kwargs.get('renderer', settings.FEED_DEFAULT_RENDERER_ENGINE)\n    renderer_template = kwargs.get('template', None)\n    expiration = kwargs.get('expiration', 0)\n    \n    renderer = get_feed_renderer(settings.FEED_RENDER_ENGINES, renderer_name)\n    return renderer().render(url, template=renderer_template, expiration=expiration)", "response": "Render a feed and return its builded html"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_region_list(service, chosen_regions = [], partition_name = 'aws'):\n    service = 'ec2containerservice' if service == 'ecs' else service # Of course things aren't that easy...\n    # Get list of regions from botocore\n    regions = Session().get_available_regions(service, partition_name = partition_name)\n    if len(chosen_regions):\n        return list((Counter(regions) & Counter(chosen_regions)).elements())\n    else:\n        return regions", "response": "Build the list of target region names for a service"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_service(service, credentials, region_name = None, config = None, silent = False):\n    api_client = None\n    try:\n        client_params = {}\n        client_params['service_name'] = service.lower()\n        session_params = {}\n        session_params['aws_access_key_id'] = credentials['AccessKeyId']\n        session_params['aws_secret_access_key'] = credentials['SecretAccessKey']\n        session_params['aws_session_token'] = credentials['SessionToken']\n        if region_name:\n            client_params['region_name'] = region_name\n            session_params['region_name'] = region_name\n        if config:\n            client_params['config'] = config\n        aws_session = boto3.session.Session(**session_params)\n        if not silent:\n            infoMessage = 'Connecting to AWS %s' % service\n            if region_name:\n                infoMessage = infoMessage + ' in %s' % region_name\n            printInfo('%s...' % infoMessage)\n        api_client = aws_session.client(**client_params)\n    except Exception as e:\n        printException(e)\n    return api_client", "response": "Connect to a service and return a client object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles truncated responses from the server.", "response": "def handle_truncated_response(callback, params, entities):\n    \"\"\"\n    Handle truncated responses\n\n    :param callback:\n    :param params:\n    :param entities:\n\n    :return:\n    \"\"\"\n    results = {}\n    for entity in entities:\n        results[entity] = []\n    while True:\n        try:\n            marker_found = False\n            response = callback(**params)\n            for entity in entities:\n                if entity in response:\n                    results[entity] = results[entity] + response[entity]\n            for marker_name in ['NextToken', 'Marker', 'PaginationToken']:\n                if marker_name in response and response[marker_name]:\n                    params[marker_name] = response[marker_name]\n                    marker_found = True\n            if not marker_found:\n                break\n        except Exception as e:\n            if is_throttled(e):\n                time.sleep(1)\n            else:\n                raise e\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pdftojpg(filehandle, meta):\n    resolution = meta.get('resolution', 300)\n    width = meta.get('width', 1080)\n    bgcolor = Color(meta.get('bgcolor', 'white'))\n    stream = BytesIO()\n\n    with Image(blob=filehandle.stream, resolution=resolution) as img:\n        img.background_color = bgcolor\n        img.alpha_channel = False\n        img.format = 'jpeg'\n        ratio = width / img.width\n        img.resize(width, int(ratio * img.height))\n        img.compression_quality = 90\n        img.save(file=stream)\n\n    stream.seek(0)\n    filehandle.stream = stream\n    return filehandle", "response": "Converts a PDF to a JPG and places it back onto the FileStorage instance\n    passed to it as a BytesIO object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the filename to reflect the conversion from PDF to JPG.", "response": "def change_filename(filehandle, meta):\n    \"\"\"Changes the filename to reflect the conversion from PDF to JPG.\n    This method will preserve the original filename in the meta dictionary.\n    \"\"\"\n    filename = secure_filename(meta.get('filename', filehandle.filename))\n    basename, _ = os.path.splitext(filename)\n    meta['original_filename'] = filehandle.filename\n    filehandle.filename = filename + '.jpg'\n    return filehandle"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef avoid_name_collisions(filehandle, meta):\n    if meta.get('avoid_name_collision', True):\n        filename = filehandle.filename\n        original, ext = os.path.splitext(filehandle.filename)\n        counter = count()\n        while os.path.exists(get_save_path(filename)):\n            fixer = str(next(counter))\n            filename = '{}_{}{}'.format(original, fixer, ext)\n        filehandle.filename = filename\n    return filehandle", "response": "Manipulates a filename until it s unique."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing werkzeug. FileStorage instance to save the converted image.", "response": "def pdf_saver(filehandle, *args, **kwargs):\n    \"Uses werkzeug.FileStorage instance to save the converted image.\"\n    fullpath = get_save_path(filehandle.filename)\n    filehandle.save(fullpath, buffer_size=kwargs.get('buffer_size', 16384))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pass_condition(b, test, a):\n\n    # Return false by default\n    result = False\n\n    # Equality tests\n    if test == 'equal':\n        a = str(a)\n        b = str(b)\n        result = (a == b)\n    elif test == 'notEqual':\n        result = (not pass_condition(b, 'equal', a))\n\n    # More/Less tests\n    elif test == 'lessThan':\n        result = (int(b) < int(a))\n    elif test == 'lessOrEqual':\n        result = (int(b) <= int(a))\n    elif test == 'moreThan':\n        result = (int(b) > int(a))\n    elif test == 'moreOrEqual':\n        result = (int(b) >= int(a))\n\n    # Empty tests\n    elif test == 'empty':\n        result = ((type(b) == dict and b == {}) or (type(b) == list and b == []) or (type(b) == list and b == [None]))\n    elif test == 'notEmpty':\n        result = (not pass_condition(b, 'empty', 'a'))\n    elif test == 'null':\n        result = ((b == None) or (type(b) == str and b == 'None'))\n    elif test == 'notNull':\n        result = (not pass_condition(b, 'null', a))\n\n    # Boolean tests\n    elif test == 'true':\n        result = (str(b).lower() == 'true')\n    elif test == 'notTrue' or test == 'false':\n        result = (str(b).lower() == 'false')\n\n    # Object length tests\n    elif test == 'lengthLessThan':\n        result = (len(b) < int(a))\n    elif test == 'lengthMoreThan':\n        result = (len(b) > int(a))\n    elif test == 'lengthEqual':\n        result = (len(b) == int(a))\n\n    # Dictionary keys tests\n    elif test == 'withKey':\n        result = (a in b)\n    elif test == 'withoutKey':\n        result = (not a in b)\n\n    # List tests\n    elif test == 'containAtLeastOneOf':\n        result = False\n        if not type(b) == list:\n            b = [ b ]\n        if not type(a) == list:\n            a = [ a ]\n        for c in b:\n            if type(c):\n                c = str(c)\n            if c in a:\n                result = True\n                break\n    elif test == 'containAtLeastOneDifferentFrom':\n        result = False\n        if not type(b) == list:\n            b = [ b ]\n        if not type(a) == list:\n            a = [ a ]\n        for c in b:\n            if c != None and c != '' and c not in a:\n                result = True\n                break\n    elif test == 'containNoneOf':\n        result = True\n        if not type(b) == list:\n            b = [ b ]\n        if not type(a) == list:\n            a = [ a ]\n        for c in b:\n            if c in a:\n                result = False\n                break\n\n    # Regex tests\n    elif test == 'match':\n        if type(a) != list:\n            a = [ a ]\n        b = str(b)\n        for c in a:\n            if re.match(c, b) != None:\n                result = True\n                break\n    elif test == 'notMatch':\n        result = (not pass_condition(b, 'match', a))\n\n    # Date tests\n    elif test == 'priorToDate':\n        b = dateutil.parser.parse(str(b)).replace(tzinfo=None)\n        a = dateutil.parser.parse(str(a)).replace(tzinfo=None)\n        result = (b < a)\n    elif test == 'olderThan':\n        age, threshold = __prepare_age_test(a, b)\n        result = (age > threshold)\n    elif test == 'newerThan':\n        age, threshold = __prepare_age_test(a, b)\n        result = (age < threshold)\n\n    # CIDR tests\n    elif test == 'inSubnets':\n        result = False\n        grant = netaddr.IPNetwork(b)\n        if type(a) != list:\n            a = [ a ]\n        for c in a:\n            known_subnet = netaddr.IPNetwork(c)\n            if grant in known_subnet:\n                result = True\n                break\n    elif test == 'notInSubnets':\n        result = (not pass_condition(b, 'inSubnets', a))\n\n    # Policy statement tests\n    elif test == 'containAction':\n        result = False\n        if type(b) != dict:\n            b = json.loads(b)\n        statement_actions = get_actions_from_statement(b)\n        rule_actions = _expand_wildcard_action(a)\n        for action in rule_actions:\n            if action.lower() in statement_actions:\n                result = True\n                break\n    elif test == 'notContainAction':\n        result = (not pass_condition(b, 'containAction', a))\n    elif test == 'containAtLeastOneAction':\n        result = False\n        if type(b) != dict:\n            b = json.loads(b)\n        if type(a) != list:\n            a = [ a ]\n        actions = get_actions_from_statement(b)\n        for c in a:\n            if c.lower() in actions:\n                result = True\n                break\n\n    # Policy principal tests\n    elif test == 'isCrossAccount':\n        result = False\n        if type(b) != list:\n            b = [b]\n        for c in b:\n            if c != a and not re.match(r'arn:aws:iam:.*?:%s:.*' % a, c):\n                result = True\n                break\n    elif test == 'isSameAccount':\n        result = False\n        if type(b) != list:\n            b = [b]\n        for c in b:\n            if c == a or re.match(r'arn:aws:iam:.*?:%s:.*' % a, c):\n                result = True\n                break\n\n    # Unknown test case\n    else:\n        printError('Error: unknown test case %s' % test)\n        raise Exception\n\n    return result", "response": "Generic test function used by Scout2 / AWS recipes\n                                       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a JSON data file and return a dictionary of the data.", "response": "def load_data(data_file, key_name = None, local_file = False, format = 'json'):\n    \"\"\"\n    Load a JSON data file\n\n    :param data_file:\n    :param key_name:\n    :param local_file:\n    :return:\n    \"\"\"\n    if local_file:\n        if data_file.startswith('/'):\n            src_file = data_file\n        else:\n            src_dir = os.getcwd()\n            src_file = os.path.join(src_dir, data_file)\n    else:\n        src_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data')\n        if not os.path.isdir(src_dir):\n            src_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../data')\n        src_file = os.path.join(src_dir, data_file)\n    with open(src_file) as f:\n        if format == 'json':\n            data = json.load(f)\n        elif format == 'yaml':\n            data = yaml.load(f)\n        elif format not in ['json', 'yaml'] and not key_name:\n            data = f.read()\n        else:\n            printError('Error, argument \\'key_name\\' may not be used with data in %s format.' % format)\n            return None\n    if key_name:\n        data = data[key_name]\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads an ip - ranges file and returns a list of IP prefixes", "response": "def read_ip_ranges(filename, local_file = True, ip_only = False, conditions = []):\n    \"\"\"\n    Returns the list of IP prefixes from an ip-ranges file\n\n    :param filename:\n    :param local_file:\n    :param conditions:\n    :param ip_only:\n    :return:\n    \"\"\"\n    targets = []\n    data = load_data(filename, local_file = local_file)\n    if 'source' in data:\n        # Filtered IP ranges\n        conditions = data['conditions']\n        local_file = data['local_file'] if 'local_file' in data else False\n        data = load_data(data['source'], local_file = local_file, key_name = 'prefixes')\n    else:\n        # Plain IP ranges\n        data = data['prefixes']\n    for d in data:\n        condition_passed = True\n        for condition in conditions:\n            if type(condition) != list or len(condition) < 3:\n                continue\n            condition_passed = pass_condition(d[condition[0]], condition[1], condition[2])\n            if not condition_passed:\n                break\n        if condition_passed:\n            targets.append(d)\n    if ip_only:\n        ips = []\n        for t in targets:\n            ips.append(t['ip_prefix'])\n        return ips\n    else:\n        return targets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the contents of a file in the specified mode", "response": "def read_file(file_path, mode = 'rt'):\n    \"\"\"\n    Read the contents of a file\n\n    :param file_path:                   Path of the file to be read\n\n    :return:                            Contents of the file\n    \"\"\"\n    contents = ''\n    with open(file_path, mode) as f:\n        contents = f.read()\n    return contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_blob_as_json(filename, blob, force_write, debug):\n    try:\n        if prompt_4_overwrite(filename, force_write):\n            with open(filename, 'wt') as f:\n                print('%s' % json.dumps(blob, indent=4 if debug else None, separators=(',', ': '), sort_keys=True, cls=CustomJSONEncoder), file=f)\n    except Exception as e:\n        printException(e)\n        pass", "response": "Saves a blob as JSON"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves a list of ip - ranges for a given profile.", "response": "def save_ip_ranges(profile_name, prefixes, force_write, debug, output_format = 'json'):\n    \"\"\"\n    Creates/Modifies an ip-range-XXX.json file\n\n    :param profile_name:\n    :param prefixes:\n    :param force_write:\n    :param debug:\n\n    :return:\n    \"\"\"\n    filename = 'ip-ranges-%s.json' % profile_name\n    ip_ranges = {}\n    ip_ranges['createDate'] = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n    # Unique prefixes\n    unique_prefixes = {}\n    for prefix in prefixes:\n        if type(prefix) == dict:\n            unique_prefixes[prefix['ip_prefix']] = prefix\n        else:\n            unique_prefixes[prefix] = {'ip_prefix': prefix}\n    unique_prefixes = list(unique_prefixes.values())\n    ip_ranges['prefixes'] = unique_prefixes\n    if output_format == 'json':\n        save_blob_as_json(filename, ip_ranges, force_write, debug)\n    else:\n        # Write as CSV\n        output = 'account_id, region, ip, instance_id, instance_name\\n'\n        for prefix in unique_prefixes:\n            output += '%s, %s, %s, %s, %s\\n' % (prefix['account_id'], prefix['region'], prefix['ip_prefix'], prefix['instance_id'], prefix['name'])\n        with open('ip-ranges-%s.csv' % profile_name, 'wt') as f:\n            f.write(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_requirements(script_path, requirements_file = None):\n    script_dir = os.path.dirname(script_path)\n    opinel_min_version = opinel_max_version = boto3_min_version = boto3_max_version = None\n    # Requirements file is either next to the script or in data/requirements\n    if not requirements_file:\n        requirements_file = os.path.join(script_dir, 'data/requirements.txt')\n        if not os.path.isfile(requirements_file):\n            requirements_file = os.path.join(script_dir, 'requirements.txt')\n    with open(requirements_file, 'rt') as f:\n        for requirement in f.readlines():\n            opinel_requirements = re_opinel.match(requirement)\n            if opinel_requirements:\n                opinel_requirements = opinel_requirements.groups()\n                opinel_min_version = opinel_requirements[0]\n                opinel_max_version = opinel_requirements[1]\n            boto3_requirements = re_boto3.match(requirement)\n            if boto3_requirements:\n                boto3_requirements = boto3_requirements.groups()\n                boto3_min_version = boto3_requirements[0]\n                boto3_max_version = boto3_requirements[1]\n    if not check_versions(opinel_min_version, OPINEL_VERSION, opinel_max_version, 'opinel'):\n        return False\n    if not check_versions(boto3_min_version, boto3.__version__, boto3_max_version, 'boto3'):\n        return False\n    return True", "response": "Check if the script_path contains the opinel and boto3 requirements file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_app(self, app):\n        assert 'zodb' not in app.extensions, \\\n               'app already initiated for zodb'\n        app.extensions['zodb'] = _ZODBState(self, app)\n        app.teardown_request(self.close_db)", "response": "Configure a Flask application to use this ZODB extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall by the application when the application is done with ZODB.", "response": "def close_db(self, exception):\n        \"\"\"Added as a `~flask.Flask.teardown_request` to applications to\n        commit the transaction and disconnect ZODB if it was used during\n        the request.\"\"\"\n        if self.is_connected:\n            if exception is None and not transaction.isDoomed():\n                transaction.commit()\n            else:\n                transaction.abort()\n            self.connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a ZODB connection pool from the app configuration.", "response": "def create_db(self, app):\n        \"\"\"Create a ZODB connection pool from the *app* configuration.\"\"\"\n        assert 'ZODB_STORAGE' in app.config, \\\n               'ZODB_STORAGE not configured'\n        storage = app.config['ZODB_STORAGE']\n        if isinstance(storage, basestring):\n            factory, dbargs = zodburi.resolve_uri(storage)\n        elif isinstance(storage, tuple):\n            factory, dbargs = storage\n        else:\n            factory, dbargs = storage, {}\n        return DB(factory(), **dbargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting - bound database connection.", "response": "def connection(self):\n        \"\"\"Request-bound database connection.\"\"\"\n        assert flask.has_request_context(), \\\n               'tried to connect zodb outside request'\n        if not self.is_connected:\n            connector = flask.current_app.extensions['zodb']\n            flask._request_ctx_stack.top.zodb_connection = connector.db.open()\n            transaction.begin()\n        return flask._request_ctx_stack.top.zodb_connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_user_to_group(iam_client, user, group, quiet = False):\n    if not quiet:\n        printInfo('Adding user to group %s...' % group)\n    iam_client.add_user_to_group(GroupName = group, UserName = user)", "response": "Add an IAM user to an IAM group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a number of IAM groups silently handling exceptions when entity already exists WorkItem.", "response": "def create_groups(iam_client, groups):\n    \"\"\"\n    Create a number of IAM group, silently handling exceptions when entity already exists\n                                        .\n    :param iam_client:                  AWS API client for IAM\n    :param groups:                      Name of IAM groups to be created.\n\n    :return:                            None\n    \"\"\"\n    groups_data = []\n    if type(groups) != list:\n        groups = [ groups ]\n    for group in groups:\n        errors = []\n        try:\n            printInfo('Creating group %s...' % group)\n            iam_client.create_group(GroupName = group)\n        except  Exception as e:\n            if e.response['Error']['Code'] != 'EntityAlreadyExists':\n                printException(e)\n                errors.append('iam:creategroup')\n        groups_data.append({'groupname': group, 'errors': errors})\n    return groups_data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_user(iam_client, user, mfa_serial = None, keep_user = False, terminated_groups = []):\n    errors = []\n    printInfo('Deleting user %s...' % user)\n    # Delete access keys\n    try:\n        aws_keys = get_access_keys(iam_client, user)\n        for aws_key in aws_keys:\n            try:\n                printInfo('Deleting access key ID %s... ' % aws_key['AccessKeyId'], False)\n                iam_client.delete_access_key(AccessKeyId = aws_key['AccessKeyId'], UserName = user)\n                printInfo('Success')\n            except Exception as e:\n                printInfo('Failed')\n                printException(e)\n                errors.append(e.response['Error']['Code'])\n    except Exception as e:\n        printException(e)\n        printError('Failed to get access keys for user %s.' % user)\n    # Deactivate and delete MFA devices\n    try:\n        mfa_devices = iam_client.list_mfa_devices(UserName = user)['MFADevices']\n        for mfa_device in mfa_devices:\n            serial = mfa_device['SerialNumber']\n            try:\n                printInfo('Deactivating MFA device %s... ' % serial, False)\n                iam_client.deactivate_mfa_device(SerialNumber = serial, UserName = user)\n                printInfo('Success')\n            except Exception as e:\n                printInfo('Failed')\n                printException(e)\n                errors.append(e.response['Error']['Code'])\n            delete_virtual_mfa_device(iam_client, serial)\n        if mfa_serial:\n            delete_virtual_mfa_device(iam_client, mfa_serial)\n    except Exception as e:\n        printException(e)\n        printError('Faile to fetch/delete MFA device serial number for user %s.' % user)\n        errors.append(e.response['Error']['Code'])\n    # Remove IAM user from groups\n    try:\n        groups = iam_client.list_groups_for_user(UserName = user)['Groups']\n        for group in groups:\n            try:\n                printInfo('Removing from group %s... ' % group['GroupName'], False)\n                iam_client.remove_user_from_group(GroupName = group['GroupName'], UserName = user)\n                printInfo('Success')\n            except Exception as e:\n                printInfo('Failed')\n                printException(e)\n                errors.append(e.response['Error']['Code'])\n    except Exception as e:\n        printException(e)\n        printError('Failed to fetch IAM groups for user %s.' % user)\n        errors.append(e.response['Error']['Code'])\n    # Delete login profile\n    login_profile = []\n    try:\n        login_profile = iam_client.get_login_profile(UserName = user)['LoginProfile']\n    except Exception as e:\n        pass\n    try:\n        if len(login_profile):\n            printInfo('Deleting login profile... ', False)\n            iam_client.delete_login_profile(UserName = user)\n            printInfo('Success')\n    except Exception as e:\n        printInfo('Failed')\n        printException(e)\n        errors.append(e.response['Error']['Code'])\n    # Delete inline policies\n    try:\n        printInfo('Deleting inline policies... ', False)\n        policies = iam_client.list_user_policies(UserName = user)\n        for policy in policies['PolicyNames']:\n            iam_client.delete_user_policy(UserName = user, PolicyName = policy)\n        printInfo('Success')\n    except Exception as e:\n        printInfo('Failed')\n        printException(e)\n        errors.append(e.response['Error']['Code'])\n    # Detach managed policies\n    try:\n        printInfo('Detaching managed policies... ', False)\n        policies = iam_client.list_attached_user_policies(UserName = user)\n        for policy in policies['AttachedPolicies']:\n            iam_client.detach_user_policy(UserName = user, PolicyArn = policy['PolicyArn'])\n        printInfo('Success')\n    except Exception as e:\n        printInfo('Failed')\n        printException(e)\n        errors.append(e.response['Error']['Code'])\n    # Delete IAM user\n    try:\n        if not keep_user:\n            iam_client.delete_user(UserName = user)\n            printInfo('User %s deleted.' % user)\n        else:\n            for group in terminated_groups:\n                add_user_to_group(iam_client, group, user)\n    except Exception as e:\n        printException(e)\n        printError('Failed to delete user.')\n        errors.append(e.response['Error']['Code'])\n        pass\n    return errors", "response": "Delete IAM user and MFA user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_virtual_mfa_device(iam_client, mfa_serial):\n    try:\n        printInfo('Deleting MFA device %s...' % mfa_serial)\n        iam_client.delete_virtual_mfa_device(SerialNumber = mfa_serial)\n    except Exception as e:\n        printException(e)\n        printError('Failed to delete MFA device %s' % mfa_serial)\n        pass", "response": "Delete a vritual MFA device given its serial number."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_group_category_regex(category_groups, category_regex_args):\n    category_regex = []\n    authorized_empty_regex = 1\n    if len(category_regex_args) and len(category_groups) != len(category_regex_args):\n        printError('Error: you must provide as many regex as category groups.')\n        return None\n    for regex in category_regex_args:\n        if len(regex) < 1:\n            if authorized_empty_regex > 0:\n                category_regex.append(None)\n                authorized_empty_regex -= 1\n            else:\n                printError('Error: you cannot have more than one empty regex to automatically assign groups to users.')\n                return None\n        else:\n            category_regex.append(re.compile(regex))\n    return category_regex", "response": "Initialize and compile regular expression for category groups\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing extended entry once on the fly.", "response": "def _get_extended_palette_entry(self, name, index, is_hex=False):\n        ''' Compute extended entry, once on the fly. '''\n        values = None\n        is_fbterm = (env.TERM == 'fbterm')  # sigh\n\n        if 'extended' in self._palette_support:  # build entry\n            if is_hex:\n                index = str(find_nearest_color_hexstr(index,\n                                                      method=self._dg_method))\n            start_codes = self._start_codes_extended\n            if is_fbterm:\n                start_codes = self._start_codes_extended_fbterm\n\n            values = [start_codes, index]\n\n        # downgrade section\n        elif 'basic' in self._palette_support:\n            if is_hex:\n                nearest_idx = find_nearest_color_hexstr(index, color_table4,\n                                                        method=self._dg_method)\n            else:\n                from .color_tables import index_to_rgb8  # find rgb for idx\n                nearest_idx = find_nearest_color_index(*index_to_rgb8[index],\n                                                       color_table=color_table4,\n                                                       method=self._dg_method)\n            values = self._index_to_ansi_values(nearest_idx)\n\n        return (self._create_entry(name, values, fbterm=is_fbterm)\n                if values else empty)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute truecolor entry once on the fly.", "response": "def _get_true_palette_entry(self, name, digits):\n        ''' Compute truecolor entry, once on the fly.\n\n            values must become sequence of decimal int strings: ('1', '2', '3')\n        '''\n        values = None\n        type_digits = type(digits)\n        is_fbterm = (env.TERM == 'fbterm')  # sigh\n\n        if 'truecolor' in self._palette_support:  # build entry\n            values = [self._start_codes_true]\n            if type_digits is str:  # convert hex string\n                if len(digits) == 3:\n                    values.extend(str(int(ch + ch, 16)) for ch in digits)\n                else:  # chunk 'BB00BB', to ints to 'R', 'G', 'B':\n                    values.extend(str(int(digits[i:i+2], 16)) for i in (0, 2 ,4))\n            else:  # tuple of str-digit or int, may not matter to bother:\n                values.extend(str(digit) for digit in digits)\n\n        # downgrade section\n        elif 'extended' in self._palette_support:\n            if type_digits is str:\n                nearest_idx = find_nearest_color_hexstr(digits,\n                                                       method=self._dg_method)\n            else:  # tuple\n                if type(digits[0]) is str:  # convert to ints\n                    digits = tuple(int(digit) for digit in digits)\n                nearest_idx = find_nearest_color_index(*digits,\n                                                       method=self._dg_method)\n\n            start_codes = self._start_codes_extended\n            if is_fbterm:\n                start_codes = self._start_codes_extended_fbterm\n\n            values = [start_codes, str(nearest_idx)]\n\n        elif 'basic' in self._palette_support:\n            if type_digits is str:\n                nearest_idx = find_nearest_color_hexstr(digits, color_table4,\n                                                       method=self._dg_method)\n            else:  # tuple\n                if type(digits[0]) is str:  # convert to ints\n                    digits = tuple(int(digit) for digit in digits)\n                nearest_idx = find_nearest_color_index(*digits,\n                                                       color_table=color_table4,\n                                                       method=self._dg_method)\n            values = self._index_to_ansi_values(nearest_idx)\n\n        return (self._create_entry(name, values, fbterm=is_fbterm)\n                if values else empty)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _index_to_ansi_values(self, index):\n        ''' Converts an palette index to the corresponding ANSI color.\n\n            Arguments:\n                index   - an int (from 0-15)\n            Returns:\n                index as str in a list for compatibility with values.\n        '''\n        if self.__class__.__name__[0] == 'F':   # Foreground\n            if index < 8:\n                index += ANSI_FG_LO_BASE\n            else:\n                index += (ANSI_FG_HI_BASE - 8)  # 82\n        else:                                   # Background\n            if index < 8:\n                index += ANSI_BG_LO_BASE\n            else:\n                index += (ANSI_BG_HI_BASE - 8)  # 92\n        return [str(index)]", "response": "Converts an index to the corresponding ANSI color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_entry(self, name, values, fbterm=False):\n        ''' Render first values as string and place as first code,\n            save, and return attr.\n        '''\n        if fbterm:\n            attr = _PaletteEntryFBTerm(self, name.upper(), ';'.join(values))\n        else:\n            attr = _PaletteEntry(self, name.upper(), ';'.join(values))\n        setattr(self, name, attr)  # now cached\n        return attr", "response": "Create a new entry with the given name and values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, data):\n        ''' This could be a bit less clumsy. '''\n        if data == '\\n':  # print does this\n            return self.stream.write(data)\n        else:\n            bytes_ = 0\n            for line in data.splitlines(True):\n                nl = ''\n                if line.endswith('\\n'):  # mv nl to end:\n                    line = line[:-1]\n                    nl = '\\n'\n                bytes_ += self.stream.write(\n                                f'{self.start}{line}{self.default}{nl}'\n                          ) or 0  # in case None returned (on Windows)\n            return bytes_", "response": "This function writes the data to the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the output file for this context manager.", "response": "def set_output(self, outfile):\n        ''' Set's the output file, currently only useful with context-managers.\n\n            Note:\n                This function is experimental and may not last.\n        '''\n        if self._orig_stdout:  # restore Usted\n            sys.stdout = self._orig_stdout\n\n        self._stream = outfile\n        sys.stdout = _LineWriter(self, self._stream, self.default)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_status(self):\n        ''' Check bounds for errors and update label accordingly. '''\n        label = ''\n        #\u00a0label format, based on time - when slow, go to higher-res display\n        delta = time.time() - self.start\n        if delta > self.timedelta2:\n            self.label_fmt = '%5.2f%%'\n        elif delta > self.timedelta1:\n            self.label_fmt = '%4.1f%%'\n\n        ratio = self.ratio\n        if 0 <= ratio < 1:\n            if self.label_mode:\n                label = self.label_fmt % (ratio * 100)\n            if self.oob_error:  # now fixed, reset\n                self._first = self.styles[_if](self.icons[_if])\n                self._last = self.styles[_il](self.icons[_il])\n                self.oob_error = False\n                self._comp_style = self.styles[_ic]\n                self.done = False\n        else:\n            if ratio == 1:\n                self.done = True\n                self._comp_style = self.styles[_id]\n                self._last = self.styles[_if](self.icons[_il])\n                if self.label_mode:\n                    label = '\u2713' if self.unicode_support else '+'\n                if self.oob_error:  # now fixed, reset\n                    self._first = self.styles[_if](self.icons[_if])\n                    self.oob_error = False\n\n            elif ratio > 1:\n                self.done = True\n                self.oob_error = True\n                if self.unicode_support:\n                    self._last = self._err_style('\u23f5')\n                    if self.label_mode:\n                        label = '\u2717'\n                else:\n                    self._last = self._err_style('>')\n                    if self.label_mode:\n                        label = 'ERR'\n            else:  # < 0\n                self.oob_error = True\n                if self.unicode_support:\n                    self._first = self._err_style('\u23f4')\n                    if self.label_mode:\n                        label = '\u2717'\n                else:\n                    self._first = self._err_style('<')\n                    if self.label_mode:\n                        label = 'ERR'\n        self._lbl = label", "response": "Update status of the current object based on the current status of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render(self):\n        ''' Standard rendering of bar graph. '''\n        cm_chars = self._comp_style(self.icons[_ic] * self._num_complete_chars)\n        em_chars = self._empt_style(self.icons[_ie] * self._num_empty_chars)\n        return f'{self._first}{cm_chars}{em_chars}{self._last} {self._lbl}'", "response": "Standard rendering of bar graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders with a label inside the bar graph.", "response": "def _render_internal_label(self):\n        ''' Render with a label inside the bar graph. '''\n        ncc = self._num_complete_chars\n        bar = self._lbl.center(self.iwidth)\n        cm_chars = self._comp_style(bar[:ncc])\n        em_chars = self._empt_style(bar[ncc:])\n        return f'{self._first}{cm_chars}{em_chars}{self._last}'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the number of complete chars.", "response": "def _get_ncc(self, width, ratio):\n        ''' Get the number of complete chars.\n\n            This one figures the remainder for the partial char as well.\n        '''\n        sub_chars = round(width * ratio * self.partial_chars_len)\n        ncc, self.remainder = divmod(sub_chars, self.partial_chars_len)\n        return ncc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef thread_work(targets, function, params = {}, num_threads = 0):\n    q = Queue(maxsize=0)\n    if not num_threads:\n        num_threads = len(targets)\n    for i in range(num_threads):\n        worker = Thread(target=function, args=(q, params))\n        worker.setDaemon(True)\n        worker.start()\n    for target in targets:\n        q.put(target)\n    q.join()", "response": "Generic multithreading helper for thread_work."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef threaded_per_region(q, params):\n    while True:\n        try:\n            params['region'] = q.get()\n            method = params['method']\n            method(params)\n        except Exception as e:\n            printException(e)\n        finally:\n            q.task_done()", "response": "This is a thread that runs the method in a per - region basis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef location(self, x=None, y=None):\n        ''' Temporarily move the cursor, perform work, and return to the\n            previous location.\n\n            ::\n\n                with screen.location(40, 20):\n                    print('Hello, world!')\n        '''\n        stream = self._stream\n        stream.write(self.save_pos)  # cursor position\n\n        if x is not None and y is not None:\n            stream.write(self.mv(y, x))\n        elif x is not None:\n            stream.write(self.mv_x(x))\n        elif y is not None:\n            stream.write(self.mv_y(y))\n\n        stream.flush()\n        try:\n            yield self\n        finally:\n            stream.write(self.rest_pos)\n            stream.flush()", "response": "Temporarily move the cursor perform work and return to the\n            previous location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fullscreen(self):\n        ''' Context Manager that enters full-screen mode and restores normal\n            mode on exit.\n\n            ::\n\n                with screen.fullscreen():\n                    print('Hello, world!')\n        '''\n        stream = self._stream\n        stream.write(self.alt_screen_enable)\n        stream.write(str(self.save_title(0)))     # 0 = both icon, title\n        stream.flush()\n        try:\n            yield self\n        finally:\n            stream.write(self.alt_screen_disable)\n            stream.write(str(self.restore_title(0)))  # 0 = icon & title\n            stream.flush()", "response": "Context Manager that enters full - screen mode and restores normal\n            on exit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef undecorated(o):\n    # class decorator\n    if type(o) is type:\n        return o\n\n    try:\n        # python2\n        closure = o.func_closure\n    except AttributeError:\n        pass\n\n    try:\n        # python3\n        closure = o.__closure__\n    except AttributeError:\n        return\n\n    if closure:\n        for cell in closure:\n            # avoid infinite recursion\n            if cell.cell_contents is o:\n                continue\n\n            # check if the contents looks like a decorator; in that case\n            # we need to go one level down into the dream, otherwise it\n            # might just be a different closed-over variable, which we\n            # can ignore.\n\n            # Note: this favors supporting decorators defined without\n            # @wraps to the detriment of function/method/class closures\n            if looks_like_a_decorator(cell.cell_contents):\n                undecd = undecorated(cell.cell_contents)\n                if undecd:\n                    return undecd\n        else:\n            return o\n    else:\n        return o", "response": "Removes all decorators from a function method or class or class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassuming role and save credentials to STS file", "response": "def assume_role(role_name, credentials, role_arn, role_session_name, silent = False):\n    \"\"\"\n    Assume role and save credentials\n\n    :param role_name:\n    :param credentials:\n    :param role_arn:\n    :param role_session_name:\n    :param silent:\n    :return:\n    \"\"\"\n    external_id = credentials.pop('ExternalId') if 'ExternalId' in credentials else None\n    # Connect to STS\n    sts_client = connect_service('sts', credentials, silent = silent)\n    # Set required arguments for assume role call\n    sts_args = {\n      'RoleArn': role_arn,\n      'RoleSessionName': role_session_name\n    }\n    # MFA used ?\n    if 'mfa_serial' in credentials and 'mfa_code' in credentials:\n      sts_args['TokenCode'] = credentials['mfa_code']\n      sts_args['SerialNumber'] = credentials['mfa_serial']\n    # External ID used ?\n    if external_id:\n      sts_args['ExternalId'] = external_id\n    # Assume the role\n    sts_response = sts_client.assume_role(**sts_args)\n    credentials = sts_response['Credentials']\n    cached_credentials_filename = get_cached_credentials_filename(role_name, role_arn)\n    #with open(cached_credentials_filename, 'wt+') as f:\n    #   write_data_to_file(f, sts_response, True, False)\n    cached_credentials_path = os.path.dirname(cached_credentials_filename)\n    if not os.path.isdir(cached_credentials_path):\n        os.makedirs(cached_credentials_path)\n    save_blob_as_json(cached_credentials_filename, sts_response, True, False) # blob, force_write, debug):\n    return credentials"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cached_credentials_filename(role_name, role_arn):\n    filename_p1 = role_name.replace('/','-')\n    filename_p2 = role_arn.replace('/', '-').replace(':', '_')\n    return os.path.join(os.path.join(os.path.expanduser('~'), '.aws'), 'cli/cache/%s--%s.json' %\n                        (filename_p1, filename_p2))", "response": "Construct a filepath for the cached credentials file for the given role name and ARN."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_password(length=16):\n    chars = string.ascii_letters + string.digits + '!@#$%^&*()_+-=[]{};:,<.>?|'\n    modulus = len(chars)\n    pchars = os.urandom(16)\n    if type(pchars) == str:\n        return ''.join(chars[i % modulus] for i in map(ord, pchars))\n    else:\n        return ''.join(chars[i % modulus] for i in pchars)", "response": "Generate a random password using random characters from uppercase lowercase digits and symbols."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches STS credentials :param profile_name: :param credentials: :param duration: :param session_name: :param save_creds: :return:", "response": "def init_sts_session(profile_name, credentials, duration = 28800, session_name = None, save_creds = True):\n    \"\"\"\n    Fetch STS credentials\n\n    :param profile_name:\n    :param credentials:\n    :param duration:\n    :param session_name:\n    :param save_creds:\n    :return:\n    \"\"\"\n    # Set STS arguments\n    sts_args = {\n        'DurationSeconds': duration\n    }\n    # Prompt for MFA code if MFA serial present\n    if 'SerialNumber' in credentials and credentials['SerialNumber']:\n        if not credentials['TokenCode']:\n            credentials['TokenCode'] = prompt_4_mfa_code()\n            if credentials['TokenCode'] == 'q':\n                credentials['SerialNumber'] = None\n        sts_args['TokenCode'] = credentials['TokenCode']\n        sts_args['SerialNumber'] = credentials['SerialNumber']\n    # Init session\n    sts_client = boto3.session.Session(credentials['AccessKeyId'], credentials['SecretAccessKey']).client('sts')\n    sts_response = sts_client.get_session_token(**sts_args)\n    if save_creds:\n        # Move long-lived credentials if needed\n        if not profile_name.endswith('-nomfa') and credentials['AccessKeyId'].startswith('AKIA'):\n            write_creds_to_aws_credentials_file(profile_name + '-nomfa', credentials)\n        # Save STS values in the .aws/credentials file\n        write_creds_to_aws_credentials_file(profile_name, sts_response['Credentials'])\n    return sts_response['Credentials']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread credentials from AWS config file.", "response": "def read_creds_from_aws_credentials_file(profile_name, credentials_file = aws_credentials_file):\n    \"\"\"\n    Read credentials from AWS config file\n\n    :param profile_name:\n    :param credentials_file:\n    :return:\n    \"\"\"\n    credentials = init_creds()\n    profile_found = False\n    try:\n        # Make sure the ~.aws folder exists\n        if not os.path.exists(aws_config_dir):\n            os.makedirs(aws_config_dir)\n        with open(credentials_file, 'rt') as cf:\n            for line in cf:\n                profile_line = re_profile_name.match(line)\n                if profile_line:\n                    if profile_line.groups()[0] == profile_name:\n                        profile_found = True\n                    else:\n                        profile_found = False\n                if profile_found:\n                    if re_access_key.match(line):\n                        credentials['AccessKeyId'] = line.split(\"=\")[1].strip()\n                    elif re_secret_key.match(line):\n                        credentials['SecretAccessKey'] = line.split(\"=\")[1].strip()\n                    elif re_mfa_serial.match(line):\n                        credentials['SerialNumber'] = (line.split('=')[1]).strip()\n                    elif re_session_token.match(line) or re_security_token.match(line):\n                        credentials['SessionToken'] = ('='.join(x for x in line.split('=')[1:])).strip()\n                    elif re_expiration.match(line):\n                        credentials['Expiration'] = ('='.join(x for x in line.split('=')[1:])).strip()\n    except Exception as e:\n        # Silent if error is due to no ~/.aws/credentials file\n        if not hasattr(e, 'errno') or e.errno != 2:\n            printException(e)\n    return credentials"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread credentials from a CSV file.", "response": "def read_creds_from_csv(filename):\n    \"\"\"\n    Read credentials from a CSV file\n\n    :param filename:\n    :return:\n    \"\"\"\n    key_id = None\n    secret = None\n    mfa_serial = None\n    secret_next = False\n    with open(filename, 'rt') as csvfile:\n        for i, line in enumerate(csvfile):\n            values = line.split(',')\n            for v in values:\n                if v.startswith('AKIA'):\n                    key_id = v.strip()\n                    secret_next = True\n                elif secret_next:\n                    secret = v.strip()\n                    secret_next = False\n                elif re_mfa_serial_format.match(v):\n                    mfa_serial = v.strip()\n    return key_id, secret, mfa_serial"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_creds_from_ec2_instance_metadata():\n    creds = init_creds()\n    try:\n        has_role = requests.get('http://169.254.169.254/latest/meta-data/iam/security-credentials', timeout = 1)\n        if has_role.status_code == 200:\n            iam_role = has_role.text\n            credentials = requests.get('http://169.254.169.254/latest/meta-data/iam/security-credentials/%s/' %\n                                       iam_role.strip()).json()\n            for c in ['AccessKeyId', 'SecretAccessKey']:\n                creds[c] = credentials[c]\n            creds['SessionToken'] = credentials['Token']\n        return creds\n    except Exception as e:\n        return False", "response": "Read credentials from EC2 instance metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading credentials from ECS instance metadata", "response": "def read_creds_from_ecs_container_metadata():\n    \"\"\"\n    Read credentials from ECS instance metadata (IAM role)\n\n    :return:\n    \"\"\"\n    creds = init_creds()\n    try:\n        ecs_metadata_relative_uri = os.environ['AWS_CONTAINER_CREDENTIALS_RELATIVE_URI']\n        credentials = requests.get('http://169.254.170.2' + ecs_metadata_relative_uri, timeout = 1).json()\n        for c in ['AccessKeyId', 'SecretAccessKey']:\n            creds[c] = credentials[c]\n            creds['SessionToken'] = credentials['Token']\n        return creds\n    except Exception as e:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_creds_from_environment_variables():\n    creds = init_creds()\n    # Check environment variables\n    if 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ:\n        creds['AccessKeyId'] = os.environ['AWS_ACCESS_KEY_ID']\n        creds['SecretAccessKey'] = os.environ['AWS_SECRET_ACCESS_KEY']\n        if 'AWS_SESSION_TOKEN' in os.environ:\n            creds['SessionToken'] = os.environ['AWS_SESSION_TOKEN']\n    return creds", "response": "Read credentials from environment variables and return them as a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads profiles from environment variables.", "response": "def read_profile_from_environment_variables():\n    \"\"\"\n    Read profiles from env\n\n    :return:\n    \"\"\"\n    role_arn = os.environ.get('AWS_ROLE_ARN', None)\n    external_id = os.environ.get('AWS_EXTERNAL_ID', None)\n    return role_arn, external_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading profiles from AWS config file and return them as a tuple.", "response": "def read_profile_from_aws_config_file(profile_name, config_file = aws_config_file):\n    \"\"\"\n    Read profiles from AWS config file\n\n    :param profile_name:\n    :param config_file:\n    :return:\n    \"\"\"\n    role_arn = None\n    source_profile = 'default'\n    mfa_serial = None\n    profile_found = False\n    external_id = None\n    try:\n        with open(config_file, 'rt') as config:\n            for line in config:\n                profile_line = re_profile_name.match(line)\n                if profile_line:\n                    role_profile_name = profile_line.groups()[0].split()[-1]\n                    if role_profile_name == profile_name:\n                        profile_found = True\n                    else:\n                        profile_found = False\n                if profile_found:\n                    if re_role_arn.match(line):\n                        role_arn = line.split('=')[1].strip()\n                    elif re_source_profile.match(line):\n                        source_profile = line.split('=')[1].strip()\n                    elif re_mfa_serial.match(line):\n                        mfa_serial = line.split('=')[1].strip()\n                    elif re_external_id.match(line):\n                        external_id = line.split('=')[1].strip()\n    except Exception as e:\n        # Silent if error is due to no .aws/config file\n        if not hasattr(e, 'errno') or e.errno != 2:\n            printException(e)\n    return role_arn, source_profile, mfa_serial, external_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows profiles from AWS credentials file", "response": "def show_profiles_from_aws_credentials_file(credentials_files = [aws_credentials_file, aws_config_file]):\n    \"\"\"\n    Show profile names from ~/.aws/credentials\n\n    :param credentials_files:\n    :return:\n    \"\"\"\n    profiles = get_profiles_from_aws_credentials_file(credentials_files)\n    for profile in set(profiles):\n        printInfo(' * %s' % profile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites credentials to AWS config file", "response": "def write_creds_to_aws_credentials_file(profile_name, credentials, credentials_file = aws_credentials_file):\n    \"\"\"\n    Write credentials to AWS config file\n\n    :param profile_name:\n    :param credentials:\n    :param credentials_file:\n    :return:\n    \"\"\"\n    profile_found = False\n    profile_ever_found = False\n    session_token_written = False\n    security_token_written = False\n    mfa_serial_written = False\n    expiration_written = False\n    # Create the .aws folder if needed\n    if not os.path.isdir(aws_config_dir):\n        os.mkdir(aws_config_dir)\n    # Create an empty file if target does not exist\n    if not os.path.isfile(credentials_file):\n        open(credentials_file, 'a').close()\n    # Open and parse/edit file\n    for line in fileinput.input(credentials_file, inplace=True):\n        profile_line = re_profile_name.match(line)\n        if profile_line:\n            if profile_line.groups()[0] == profile_name:\n                profile_found = True\n                profile_ever_found = True\n            else:\n                profile_found = False\n            print(line.rstrip())\n        elif profile_found:\n            if re_access_key.match(line) and 'AccessKeyId' in credentials and credentials['AccessKeyId']:\n                print('aws_access_key_id = %s' % credentials['AccessKeyId'])\n            elif re_secret_key.match(line) and 'SecretAccessKey' in credentials and credentials['SecretAccessKey']:\n                print('aws_secret_access_key = %s' % credentials['SecretAccessKey'])\n            elif re_mfa_serial.match(line) and 'SerialNumber' in credentials and credentials['SerialNumber']:\n                print('aws_mfa_serial = %s' % credentials['SerialNumber'])\n                mfa_serial_written = True\n            elif re_session_token.match(line) and 'SessionToken' in credentials and credentials['SessionToken']:\n                print('aws_session_token = %s' % credentials['SessionToken'])\n                session_token_written = True\n            elif re_security_token.match(line) and 'SessionToken' in credentials and credentials['SessionToken']:\n                print('aws_security_token = %s' % credentials['SessionToken'])\n                security_token_written = True\n            elif re_expiration.match(line) and 'Expiration' in credentials and credentials['Expiration']:\n                print('expiration = %s' % credentials['Expiration'])\n                expiration_written = True\n            else:\n                print(line.rstrip())\n        else:\n            print(line.rstrip())\n\n    # Complete the profile if needed\n    if profile_found:\n        with open(credentials_file, 'a') as f:\n            complete_profile(f, credentials, session_token_written, mfa_serial_written)\n\n    # Add new profile if not found\n    if not profile_ever_found:\n        with open(credentials_file, 'a') as f:\n            f.write('[%s]\\n' % profile_name)\n            f.write('aws_access_key_id = %s\\n' % credentials['AccessKeyId'])\n            f.write('aws_secret_access_key = %s\\n' % credentials['SecretAccessKey'])\n            complete_profile(f, credentials, session_token_written, mfa_serial_written)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complete_profile(f, credentials, session_token_written, mfa_serial_written):\n    session_token = credentials['SessionToken'] if 'SessionToken' in credentials else None\n    mfa_serial = credentials['SerialNumber'] if 'SerialNumber' in credentials else None\n    if session_token and not session_token_written:\n        f.write('aws_session_token = %s\\n' % session_token)\n    if mfa_serial and not mfa_serial_written:\n        f.write('aws_mfa_serial = %s\\n' % mfa_serial)", "response": "Append session token and mfa serial if needed\n\n    :param f:\n    :param credentials:\n    :param session_token_written:\n    :param mfa_serial_written:\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_creds(profile_name, csv_file = None, mfa_serial_arg = None, mfa_code = None, force_init = False,\n               role_session_name = 'opinel'):\n    \"\"\"\n    Read credentials from anywhere (CSV, Environment, Instance metadata, config/credentials)\n\n    :param profile_name:\n    :param csv_file:\n    :param mfa_serial_arg:\n    :param mfa_code:\n    :param force_init:\n    :param role_session_name:\n\n    :return:\n    \"\"\"\n    first_sts_session = False\n    source_profile = None\n    role_mfa_serial = None\n    expiration = None\n    credentials = init_creds()\n    role_arn, external_id = read_profile_from_environment_variables()\n    if csv_file:\n        # Read credentials from a CSV file that was provided\n        credentials['AccessKeyId'], credentials['SecretAccessKey'], credentials['SerialNumber'] = \\\n            read_creds_from_csv(csv_file)\n    elif profile_name == 'default':\n        # Try reading credentials from environment variables (Issue #11) if the profile name is 'default'\n        credentials = read_creds_from_environment_variables()\n    if ('AccessKeyId' not in credentials or not credentials['AccessKeyId']) \\\n            and not csv_file and profile_name == 'default':\n        ec2_credentials = read_creds_from_ec2_instance_metadata()\n        if ec2_credentials:\n            credentials = ec2_credentials\n        else:\n            ecs_credentials = read_creds_from_ecs_container_metadata()\n            if ecs_credentials:\n                credentials = ecs_credentials\n        # TODO support lambda\n    if role_arn or (not credentials['AccessKeyId'] and not csv_file):\n        # Lookup if a role is defined in ~/.aws/config\n        if not role_arn:\n            role_arn, source_profile, role_mfa_serial, external_id = read_profile_from_aws_config_file(profile_name)\n        # Scout2 issue 237 - credentials file may be used to configure role-based profiles...\n        if not role_arn:\n            role_arn, source_profile, role_mfa_serial, external_id = \\\n                read_profile_from_aws_config_file(profile_name, config_file = aws_credentials_file)\n        if role_arn:\n            # Lookup cached credentials\n            try:\n                cached_credentials_filename = get_cached_credentials_filename(profile_name, role_arn)\n                with open(cached_credentials_filename, 'rt') as f:\n                    assume_role_data = json.load(f)\n                    oldcred = credentials\n                    credentials = assume_role_data['Credentials']\n                    expiration = dateutil.parser.parse(credentials['Expiration'])\n                    expiration = expiration.replace(tzinfo=None)\n                    current = datetime.datetime.utcnow()\n                    if expiration < current:\n                        print('Role\\'s credentials have expired on %s' % credentials['Expiration'])\n                        credentials = oldcred\n            except Exception as e:\n                pass\n            if not expiration or expiration < current or credentials['AccessKeyId'] == None:\n                if source_profile:\n                    credentials = read_creds(source_profile)\n                if role_mfa_serial:\n                    credentials['SerialNumber'] = role_mfa_serial\n                    # Auto prompt for a code...\n                    if not mfa_code:\n                        credentials['TokenCode'] = prompt_4_mfa_code()\n                if external_id:\n                    credentials['ExternalId'] = external_id\n                credentials = assume_role(profile_name, credentials, role_arn, role_session_name)\n        # Read from ~/.aws/credentials\n        else:\n            credentials = read_creds_from_aws_credentials_file(profile_name)\n            if credentials['SessionToken']:\n                if 'Expiration' in credentials and credentials['Expiration']:\n                    expiration = dateutil.parser.parse(credentials['Expiration'])\n                    expiration = expiration.replace(tzinfo=None)\n                    current = datetime.datetime.utcnow()\n                    if expiration < current:\n                        printInfo('Saved STS credentials expired on %s' % credentials['Expiration'])\n                        force_init = True\n                else:\n                    force_init = True\n                    sts_credentials = credentials\n            else:\n                first_sts_session = True\n            if force_init or (mfa_serial_arg and mfa_code):\n                credentials = read_creds_from_aws_credentials_file(profile_name if first_sts_session\n                                                                   else '%s-nomfa' % profile_name)\n                if not credentials['AccessKeyId']:\n                    printInfo('Warning: Unable to determine STS token expiration; later API calls may fail.')\n                    credentials = sts_credentials\n                else:\n                    if mfa_serial_arg:\n                        credentials['SerialNumber'] = mfa_serial_arg\n                    if mfa_code:\n                        credentials['TokenCode'] = mfa_code\n                    if 'AccessKeyId' in credentials and credentials['AccessKeyId']:\n                        credentials = init_sts_session(profile_name, credentials)\n    # If we don't have valid creds by now, print an error message\n    if 'AccessKeyId' not in credentials or credentials['AccessKeyId'] == None or \\\n            'SecretAccessKey' not in credentials or credentials['SecretAccessKey'] == None:\n        printError('Error: could not find AWS credentials. Use the --help option for more information.')\n    if not 'AccessKeyId' in credentials:\n        credentials = { 'AccessKeyId': None }\n    return credentials", "response": "Read credentials from any file that was provided in the user s profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_stackset_ready_accounts(credentials, account_ids, quiet=True):\n    api_client = connect_service('sts', credentials, silent=True)\n    configured_account_ids = []\n    for account_id in account_ids:\n        try:\n            role_arn = 'arn:aws:iam::%s:role/AWSCloudFormationStackSetExecutionRole' % account_id\n            api_client.assume_role(RoleArn=role_arn, RoleSessionName='opinel-get_stackset_ready_accounts')\n            configured_account_ids.append(account_id)\n        except Exception as e:\n            pass\n\n    if len(configured_account_ids) != len(account_ids) and not quiet:\n        printInfo('Only %d of these accounts have the necessary stack set execution role:' % len(configured_account_ids))\n        printDebug(str(configured_account_ids))\n    return configured_account_ids", "response": "Verify which AWS accounts have been configured for CloudFormation stack set execution role"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the content of a feed using requests. get", "response": "def fetch(self, url):\n        \"\"\"\n        Get the feed content using 'requests'\n        \"\"\"\n        try:\n            r = requests.get(url, timeout=self.timeout)\n        except requests.exceptions.Timeout:\n            if not self.safe:\n                raise\n            else:\n                return None\n        \n        # Raise 404/500 error if any\n        if r and not self.safe:\n            r.raise_for_status()\n        \n        return r.text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the feed content and return a feed dict.", "response": "def parse(self, content):\n        \"\"\"\n        Parse the fetched feed content\n        \n        Feedparser returned dict contain a 'bozo' key which can be '1' if the feed \n        is malformed.\n        \n        Return None if the feed is malformed and 'bozo_accept' \n        is 'False', else return the feed content dict.\n        \n        If the feed is malformed but 'bozo_accept' is 'True', the feed content dict will \n        contain the parsing error exception informations in 'bozo_exception'.\n        \"\"\"\n        if content is None:\n            return None\n        \n        feed = feedparser.parse(content)\n        \n        # When feed is malformed\n        if feed['bozo']:\n            # keep track of the parsing error exception but as string \n            # infos, not an exception object\n            exception_content = {\n                \"exception\": str(type(feed['bozo_exception'])),\n                \"content\": str(feed['bozo_exception'].getException()),\n                \"line\": feed['bozo_exception'].getLineNumber(),\n                \"message\": feed['bozo_exception'].getMessage(),\n            }\n            # Overwrite the bozo content from feedparser\n            feed['bozo_exception'] = exception_content\n            # bozo feeds are not accepted\n            if not self.bozo_accept:\n                feed = None\n\n        return feed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhash the URL to an md5sum.", "response": "def _hash_url(self, url):\n        \"\"\"\n        Hash the URL to an md5sum.\n        \"\"\"\n\n        if isinstance(url, six.text_type):\n            url = url.encode('utf-8')\n\n        return hashlib.md5(url).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches the feed from cache if no cache exists and if no cache exists and no expiration time is given", "response": "def get(self, url, expiration):\n        \"\"\"\n        Fetch the feed if no cache exist or if cache is stale\n        \"\"\"\n        # Hash url to have a shorter key and add it expiration time to avoid clash for \n        # other url usage with different expiration\n        cache_key = self.cache_key.format(**{\n            'id': self._hash_url(url),\n            'expire': str(expiration)\n        })\n        \n        # Get feed from cache if any\n        feed = cache.get(cache_key)\n        # Else fetch it\n        if feed is None:\n            #print \"No feed cache, have to fetch it\"\n            feed = self.fetch(url)\n            cache.set(cache_key, feed, expiration)\n            \n        return self.parse(feed)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding template context with formatted feed content", "response": "def get_context(self, url, expiration):\n        \"\"\"\n        Build template context with formatted feed content\n        \"\"\"\n        self._feed = self.get(url, expiration)\n        \n        return {\n            self.feed_context_name: self.format_feed_content(self._feed),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks to see if this version of Windows is recent enough to support ANSI VT processing.", "response": "def is_ansi_capable():\n    ''' Check to see whether this version of Windows is recent enough to\n        support \"ANSI VT\"\" processing.\n    '''\n    BUILD_ANSI_AVAIL = 10586  # Win10 TH2\n    CURRENT_VERS = sys.getwindowsversion()[:3]\n\n    if CURRENT_VERS[2] > BUILD_ANSI_AVAIL:\n        result = True\n    else:\n        result = False\n    log.debug('version %s %s', CURRENT_VERS, result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn current colors of console.", "response": "def get_color(name, stream=STD_OUTPUT_HANDLE):\n    ''' Returns current colors of console.\n\n        https://docs.microsoft.com/en-us/windows/console/getconsolescreenbufferinfo\n\n        Arguments:\n            name:   one of ('background', 'bg', 'foreground', 'fg')\n            stream: Handle to stdout, stderr, etc.\n\n        Returns:\n            int:  a color id from the conhost palette.\n                  Ids under 0x8 (8) are dark colors, above light.\n    '''\n    stream = kernel32.GetStdHandle(stream)\n    csbi = CONSOLE_SCREEN_BUFFER_INFO()\n    kernel32.GetConsoleScreenBufferInfo(stream, byref(csbi))\n    color_id = csbi.wAttributes & _mask_map.get(name, name)\n    log.debug('color_id from conhost: %d', color_id)\n    if name in ('background', 'bg'):\n        color_id /= 16  # divide by 16\n        log.debug('color_id divided: %d', color_id)\n\n    # convert to ansi order\n    color_id = _win_to_ansi_offset_map.get(color_id, color_id)\n    log.debug('ansi color_id: %d', color_id)\n    return color_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn current position of cursor starts at 1.", "response": "def get_position(stream=STD_OUTPUT_HANDLE):\n    ''' Returns current position of cursor, starts at 1. '''\n    stream = kernel32.GetStdHandle(stream)\n    csbi = CONSOLE_SCREEN_BUFFER_INFO()\n    kernel32.GetConsoleScreenBufferInfo(stream, byref(csbi))\n\n    pos = csbi.dwCursorPosition\n    # zero based, add ones for compatibility.\n    return (pos.X + 1, pos.Y + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_position(x, y, stream=STD_OUTPUT_HANDLE):\n    ''' Sets current position of the cursor. '''\n    stream = kernel32.GetStdHandle(stream)\n    value = x + (y << 16)\n    kernel32.SetConsoleCursorPosition(stream, c_long(value))", "response": "Sets the current position of the cursor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn console title string.", "response": "def get_title():\n    ''' Returns console title string.\n\n        https://docs.microsoft.com/en-us/windows/console/getconsoletitle\n    '''\n    MAX_LEN = 256\n    buffer_ = create_unicode_buffer(MAX_LEN)\n    kernel32.GetConsoleTitleW(buffer_, MAX_LEN)\n    log.debug('%s', buffer_.value)\n    return buffer_.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the header of a MPQ archive.", "response": "def read_header(self):\n        \"\"\"Read the header of a MPQ archive.\"\"\"\n\n        def read_mpq_header(offset=None):\n            if offset:\n                self.file.seek(offset)\n            data = self.file.read(32)\n            header = MPQFileHeader._make(\n                struct.unpack(MPQFileHeader.struct_format, data))\n            header = header._asdict()\n            if header['format_version'] == 1:\n                data = self.file.read(12)\n                extended_header = MPQFileHeaderExt._make(\n                    struct.unpack(MPQFileHeaderExt.struct_format, data))\n                header.update(extended_header._asdict())\n            return header\n\n        def read_mpq_user_data_header():\n            data = self.file.read(16)\n            header = MPQUserDataHeader._make(\n                struct.unpack(MPQUserDataHeader.struct_format, data))\n            header = header._asdict()\n            header['content'] = self.file.read(header['user_data_header_size'])\n            return header\n\n        magic = self.file.read(4)\n        self.file.seek(0)\n\n        if magic == b'MPQ\\x1a':\n            header = read_mpq_header()\n            header['offset'] = 0\n        elif magic == b'MPQ\\x1b':\n            user_data_header = read_mpq_user_data_header()\n            header = read_mpq_header(user_data_header['mpq_header_offset'])\n            header['offset'] = user_data_header['mpq_header_offset']\n            header['user_data_header'] = user_data_header\n        else:\n            raise ValueError(\"Invalid file header.\")\n\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_table(self, table_type):\n\n        if table_type == 'hash':\n            entry_class = MPQHashTableEntry\n        elif table_type == 'block':\n            entry_class = MPQBlockTableEntry\n        else:\n            raise ValueError(\"Invalid table type.\")\n\n        table_offset = self.header['%s_table_offset' % table_type]\n        table_entries = self.header['%s_table_entries' % table_type]\n        key = self._hash('(%s table)' % table_type, 'TABLE')\n\n        self.file.seek(table_offset + self.header['offset'])\n        data = self.file.read(table_entries * 16)\n        data = self._decrypt(data, key)\n\n        def unpack_entry(position):\n            entry_data = data[position*16:position*16+16]\n            return entry_class._make(\n                struct.unpack(entry_class.struct_format, entry_data))\n\n        return [unpack_entry(i) for i in range(table_entries)]", "response": "Read either the hash or block table of a MPQ archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the hash table entry corresponding to a given filename.", "response": "def get_hash_table_entry(self, filename):\n        \"\"\"Get the hash table entry corresponding to a given filename.\"\"\"\n        hash_a = self._hash(filename, 'HASH_A')\n        hash_b = self._hash(filename, 'HASH_B')\n        for entry in self.hash_table:\n            if (entry.hash_a == hash_a and entry.hash_b == hash_b):\n                return entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_file(self, filename, force_decompress=False):\n\n        def decompress(data):\n            \"\"\"Read the compression type and decompress file data.\"\"\"\n            compression_type = ord(data[0:1])\n            if compression_type == 0:\n                return data\n            elif compression_type == 2:\n                return zlib.decompress(data[1:], 15)\n            elif compression_type == 16:\n                return bz2.decompress(data[1:])\n            else:\n                raise RuntimeError(\"Unsupported compression type.\")\n\n        hash_entry = self.get_hash_table_entry(filename)\n        if hash_entry is None:\n            return None\n        block_entry = self.block_table[hash_entry.block_table_index]\n\n        # Read the block.\n        if block_entry.flags & MPQ_FILE_EXISTS:\n            if block_entry.archived_size == 0:\n                return None\n\n            offset = block_entry.offset + self.header['offset']\n            self.file.seek(offset)\n            file_data = self.file.read(block_entry.archived_size)\n\n            if block_entry.flags & MPQ_FILE_ENCRYPTED:\n                raise NotImplementedError(\"Encryption is not supported yet.\")\n\n            if not block_entry.flags & MPQ_FILE_SINGLE_UNIT:\n                # File consists of many sectors. They all need to be\n                # decompressed separately and united.\n                sector_size = 512 << self.header['sector_size_shift']\n                sectors = block_entry.size // sector_size + 1\n                if block_entry.flags & MPQ_FILE_SECTOR_CRC:\n                    crc = True\n                    sectors += 1\n                else:\n                    crc = False\n                positions = struct.unpack('<%dI' % (sectors + 1),\n                                          file_data[:4*(sectors+1)])\n                result = BytesIO()\n                sector_bytes_left = block_entry.size\n                for i in range(len(positions) - (2 if crc else 1)):\n                    sector = file_data[positions[i]:positions[i+1]]\n                    if (block_entry.flags & MPQ_FILE_COMPRESS and\n                        (force_decompress or sector_bytes_left > len(sector))):\n                        sector = decompress(sector)\n\n                    sector_bytes_left -= len(sector)\n                    result.write(sector)\n                file_data = result.getvalue()\n            else:\n                # Single unit files only need to be decompressed, but\n                # compression only happens when at least one byte is gained.\n                if (block_entry.flags & MPQ_FILE_COMPRESS and\n                    (force_decompress or block_entry.size > block_entry.archived_size)):\n                    file_data = decompress(file_data)\n\n            return file_data", "response": "Read a file from the MPQ archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts all the files inside the MPQ archive in memory.", "response": "def extract(self):\n        \"\"\"Extract all the files inside the MPQ archive in memory.\"\"\"\n        if self.files:\n            return dict((f, self.read_file(f)) for f in self.files)\n        else:\n            raise RuntimeError(\"Can't extract whole archive without listfile.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_to_disk(self):\n        archive_name, extension = os.path.splitext(os.path.basename(self.file.name))\n        if not os.path.isdir(os.path.join(os.getcwd(), archive_name)):\n            os.mkdir(archive_name)\n        os.chdir(archive_name)\n        for filename, data in self.extract().items():\n            f = open(filename, 'wb')\n            f.write(data or b'')\n            f.close()", "response": "Extract all files and write them to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_files(self, *filenames):\n        for filename in filenames:\n            data = self.read_file(filename)\n            f = open(filename, 'wb')\n            f.write(data or b'')\n            f.close()", "response": "Extract given files from the archive to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhash a string using MPQ s hash function.", "response": "def _hash(self, string, hash_type):\n        \"\"\"Hash a string using MPQ's hash function.\"\"\"\n        hash_types = {\n            'TABLE_OFFSET': 0,\n            'HASH_A': 1,\n            'HASH_B': 2,\n            'TABLE': 3\n        }\n        seed1 = 0x7FED7FED\n        seed2 = 0xEEEEEEEE\n\n        for ch in string.upper():\n            if not isinstance(ch, int): ch = ord(ch)\n            value = self.encryption_table[(hash_types[hash_type] << 8) + ch]\n            seed1 = (value ^ (seed1 + seed2)) & 0xFFFFFFFF\n            seed2 = ch + seed1 + seed2 + (seed2 << 5) + 3 & 0xFFFFFFFF\n\n        return seed1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decrypt(self, data, key):\n        seed1 = key\n        seed2 = 0xEEEEEEEE\n        result = BytesIO()\n\n        for i in range(len(data) // 4):\n            seed2 += self.encryption_table[0x400 + (seed1 & 0xFF)]\n            seed2 &= 0xFFFFFFFF\n            value = struct.unpack(\"<I\", data[i*4:i*4+4])[0]\n            value = (value ^ (seed1 + seed2)) & 0xFFFFFFFF\n\n            seed1 = ((~seed1 << 0x15) + 0x11111111) | (seed1 >> 0x0B)\n            seed1 &= 0xFFFFFFFF\n            seed2 = value + seed2 + (seed2 << 5) + 3 & 0xFFFFFFFF\n\n            result.write(struct.pack(\"<I\", value))\n\n        return result.getvalue()", "response": "Decrypt hash or block table or a sector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing the encryption table for MPQ hash function.", "response": "def _prepare_encryption_table():\n        \"\"\"Prepare encryption table for MPQ hash function.\"\"\"\n        seed = 0x00100001\n        crypt_table = {}\n\n        for i in range(256):\n            index = i\n            for j in range(5):\n                seed = (seed * 125 + 3) % 0x2AAAAB\n                temp1 = (seed & 0xFFFF) << 0x10\n\n                seed = (seed * 125 + 3) % 0x2AAAAB\n                temp2 = (seed & 0xFFFF)\n\n                crypt_table[index] = (temp1 | temp2)\n\n                index += 0x100\n\n        return crypt_table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef key_for_request(self, method, url, **kwargs):\n        if method != 'get':\n            return None\n\n        return requests.Request(url=url, params=kwargs.get('params', {})).prepare().url", "response": "Return a cache key for a given request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride Session. request in caching. Cache is only used if should_cache_response was true.", "response": "def request(self, method, url, **kwargs):\n        \"\"\" Override, wraps Session.request in caching.\n\n            Cache is only used if key_for_request returns a valid key\n            and should_cache_response was true as well.\n        \"\"\"\n        # short circuit if cache isn't configured\n        if not self.cache_storage:\n            resp = super(CachingSession, self).request(method, url, **kwargs)\n            resp.fromcache = False\n            return resp\n\n        resp = None\n        method = method.lower()\n\n        request_key = self.key_for_request(method, url, **kwargs)\n\n        if request_key and not self.cache_write_only:\n            resp = self.cache_storage.get(request_key)\n\n        if resp:\n            resp.fromcache = True\n        else:\n            resp = super(CachingSession, self).request(method, url, **kwargs)\n            # save to cache if request and response meet criteria\n            if request_key and self.should_cache_response(resp):\n                self.cache_storage.set(request_key, resp)\n            resp.fromcache = False\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, orig_key):\n        resp = requests.Response()\n\n        key = self._clean_key(orig_key)\n        path = os.path.join(self.cache_dir, key)\n\n        try:\n            with open(path, 'rb') as f:\n                # read lines one at a time\n                while True:\n                    line = f.readline().decode('utf8').strip('\\r\\n')\n                    # set headers\n\n                    if self.check_last_modified and re.search(\"last-modified\", line, flags=re.I):\n                        # line contains last modified header\n                        head_resp = requests.head(orig_key)\n\n                        try:\n                            new_lm = head_resp.headers['last-modified']\n                            old_lm = line[string.find(line, ':') + 1:].strip()\n                            if old_lm != new_lm:\n                                # last modified timestamps don't match, need to download again\n                                return None\n                        except KeyError:\n                            # no last modified header present, so redownload\n                            return None\n\n                    header = self._header_re.match(line)\n                    if header:\n                        resp.headers[header.group(1)] = header.group(2)\n                    else:\n                        break\n                # everything left is the real content\n                resp._content = f.read()\n\n            # status & encoding will be in headers, but are faked\n            # need to split spaces out of status to get code (e.g. '200 OK')\n            resp.status_code = int(resp.headers.pop('status').split(' ')[0])\n            resp.encoding = resp.headers.pop('encoding')\n            resp.url = resp.headers.get('content-location', orig_key)\n            # TODO: resp.request = request\n            return resp\n        except IOError:\n            return None", "response": "Get the cache entry for a key or return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets cache entry for key with contents of response.", "response": "def set(self, key, response):\n        \"\"\"Set cache entry for key with contents of response.\"\"\"\n        key = self._clean_key(key)\n        path = os.path.join(self.cache_dir, key)\n\n        with open(path, 'wb') as f:\n            status_str = 'status: {0}\\n'.format(response.status_code)\n            f.write(status_str.encode('utf8'))\n            encoding_str = 'encoding: {0}\\n'.format(response.encoding)\n            f.write(encoding_str.encode('utf8'))\n            for h, v in response.headers.items():\n                # header: value\\n\n                f.write(h.encode('utf8'))\n                f.write(b': ')\n                f.write(v.encode('utf8'))\n                f.write(b'\\n')\n            # one blank line\n            f.write(b'\\n')\n            f.write(response.content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets cache entry for key with contents of response.", "response": "def set(self, key, response):\n        \"\"\"Set cache entry for key with contents of response.\"\"\"\n        mod = response.headers.pop('last-modified', None)\n        status = int(response.status_code)\n        rec = (key, status, mod, response.encoding, response.content,\n               json.dumps(dict(response.headers)))\n        with self._conn:\n            self._conn.execute(\"DELETE FROM cache WHERE key=?\", (key, ))\n            self._conn.execute(\"INSERT INTO cache VALUES (?,?,?,?,?,?)\", rec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, key):\n        query = self._conn.execute(\"SELECT * FROM cache WHERE key=?\", (key,))\n        rec = query.fetchone()\n        if rec is None:\n            return None\n        rec = dict(zip(self._columns, rec))\n\n        if self.check_last_modified:\n            if rec['modified'] is None:\n                return None  # no last modified header present, so redownload\n\n            head_resp = requests.head(key)\n            new_lm = head_resp.headers.get('last-modified', None)\n            if rec['modified'] != new_lm:\n                return None\n\n        resp = requests.Response()\n        resp._content = rec['data']\n        resp.status_code = rec['status']\n        resp.encoding = rec['encoding']\n        resp.headers = json.loads(rec['headers'])\n        resp.url = key\n        return resp", "response": "Get the cache entry for a key or return None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a callable out of the destination.", "response": "def _make_destination_callable(dest):\n    \"\"\"Creates a callable out of the destination. If it's already callable,\n    the destination is returned. Instead, if the object is a string or a\n    writable object, it's wrapped in a closure to be used later.\n    \"\"\"\n    if callable(dest):\n        return dest\n    elif hasattr(dest, 'write') or isinstance(dest, string_types):\n        return _use_filehandle_to_save(dest)\n    else:\n        raise TypeError(\"Destination must be a string, writable or callable object.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate(self, filehandle, metadata, catch_all_errors=False):\n        errors = []\n        DEFAULT_ERROR_MSG = '{0!r}({1!r}, {2!r}) returned False'\n\n        for validator in self._validators:\n            try:\n                if not validator(filehandle, metadata):\n                    msg = DEFAULT_ERROR_MSG.format(validator, filehandle, metadata)\n                    raise UploadError(msg)\n            except UploadError as e:\n                if catch_all_errors:\n                    errors.append(e.args[0])\n                else:\n                    raise\n\n        if errors:\n            raise UploadError(errors)", "response": "Runs all attached validators on the provided filehandle and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _preprocess(self, filehandle, metadata):\n        \"Runs all attached preprocessors on the provided filehandle.\"\n        for process in self._preprocessors:\n            filehandle = process(filehandle, metadata)\n        return filehandle", "response": "Runs all attached preprocessors on the provided filehandle."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns all attached postprocessors on the provided filehandle.", "response": "def _postprocess(self, filehandle, metadata):\n        \"Runs all attached postprocessors on the provided filehandle.\"\n        for process in self._postprocessors:\n            filehandle = process(filehandle, metadata)\n        return filehandle"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, filehandle, destination=None, metadata=None,\n             validate=True, catch_all_errors=False, *args, **kwargs):\n        \"\"\"Saves the filehandle to the provided destination or the attached\n        default destination. Allows passing arbitrary positional and keyword\n        arguments to the saving mechanism\n\n        :param filehandle: werkzeug.FileStorage instance\n        :param dest: String path, callable or writable destination to pass the\n            filehandle off to. Transfer handles transforming a string or\n            writable object into a callable automatically.\n        :param metadata: Optional mapping of metadata to pass to validators,\n            preprocessors, and postprocessors.\n        :param validate boolean: Toggle validation, defaults to True\n        :param catch_all_errors boolean: Toggles if validation should collect\n            all UploadErrors and raise a collected error message or bail out on\n            the first one.\n        \"\"\"\n        destination = destination or self._destination\n        if destination is None:\n            raise RuntimeError(\"Destination for filehandle must be provided.\")\n\n        elif destination is not self._destination:\n            destination = _make_destination_callable(destination)\n\n        if metadata is None:\n            metadata = {}\n\n        if validate:\n            self._validate(filehandle, metadata)\n\n        filehandle = self._preprocess(filehandle, metadata)\n        destination(filehandle, metadata)\n        filehandle = self._postprocess(filehandle, metadata)\n        return filehandle", "response": "Saves the filehandle to the provided destination or the attached\n            s default destination."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave result of a request to a file, similarly to :func:`urllib.urlretrieve`. If an error is encountered may raise any of the scrapelib `exceptions`_. A filename may be provided or :meth:`urlretrieve` will safely create a temporary file. If a directory is provided, a file will be given a random name within the specified directory. Either way, it is the responsibility of the caller to ensure that the temporary file is deleted when it is no longer needed. :param url: URL for request :param filename: optional name for file :param method: any valid HTTP method, but generally GET or POST :param body: optional body for request, to turn parameters into an appropriate string use :func:`urllib.urlencode()` :param dir: optional directory to place file in :returns filename, response: tuple with filename for saved response (will be same as given filename if one was given, otherwise will be a temp file in the OS temp directory) and a :class:`Response` object that can be used to inspect the response headers.", "response": "def urlretrieve(self, url, filename=None, method='GET', body=None, dir=None, **kwargs):\n        \"\"\"\n        Save result of a request to a file, similarly to\n        :func:`urllib.urlretrieve`.\n\n        If an error is encountered may raise any of the scrapelib\n        `exceptions`_.\n\n        A filename may be provided or :meth:`urlretrieve` will safely create a\n        temporary file. If a directory is provided, a file will be given a random\n        name within the specified directory. Either way, it is the responsibility\n        of the caller to ensure that the temporary file is deleted when it is no\n        longer needed.\n\n        :param url: URL for request\n        :param filename: optional name for file\n        :param method: any valid HTTP method, but generally GET or POST\n        :param body: optional body for request, to turn parameters into\n            an appropriate string use :func:`urllib.urlencode()`\n        :param dir: optional directory to place file in\n        :returns filename, response: tuple with filename for saved\n            response (will be same as given filename if one was given,\n            otherwise will be a temp file in the OS temp directory) and\n            a :class:`Response` object that can be used to inspect the\n            response headers.\n        \"\"\"\n        result = self.request(method, url, data=body, **kwargs)\n        result.code = result.status_code    # backwards compat\n\n        if not filename:\n            fd, filename = tempfile.mkstemp(dir=dir)\n            f = os.fdopen(fd, 'wb')\n        else:\n            f = open(filename, 'wb')\n\n        f.write(result.content)\n        f.close()\n\n        return filename, result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef choose_palette(stream=sys.stdout, basic_palette=None):\n    ''' Make a best effort to automatically determine whether to enable\n        ANSI sequences, and if so, which color palettes are available.\n\n        This is the main function of the module\u2014meant to be used unless\n        something more specific is needed.\n\n        Takes the following factors into account:\n\n        - Whether output stream is a TTY.\n        - ``TERM``, ``ANSICON`` environment variables\n        - ``CLICOLOR``, ``NO_COLOR`` environment variables\n\n        Arguments:\n            stream:             Which output file to check: stdout, stderr\n            basic_palette:      Force the platform-dependent 16 color palette,\n                                for testing.  List of 16 rgb-int tuples.\n        Returns:\n            None, str: 'basic', 'extended', or 'truecolor'\n    '''\n    result = None\n    pal = basic_palette\n    log.debug('console version: %s', __version__)\n    log.debug('X11_RGB_PATHS: %r', X11_RGB_PATHS)\n\n    if color_is_forced():\n        result, pal = detect_palette_support(basic_palette=pal) or 'basic'\n\n    elif is_a_tty(stream=stream) and color_is_allowed():\n        result, pal = detect_palette_support(basic_palette=pal)\n\n    proximity.build_color_tables(pal)\n    log.debug('Basic palette: %r', pal)\n    log.debug('%r', result)\n    return result", "response": "A function that returns a best effort to determine whether to enable ANSI sequences and if so which color palettes are available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if color is allowed.", "response": "def color_is_allowed():\n    ''' Look for clues in environment, e.g.:\n\n        - https://bixense.com/clicolors/\n        - http://no-color.org/\n\n        Returns:\n            Bool:  Allowed\n    '''\n    result = True  # generally yes - env.CLICOLOR != '0'\n\n    if color_is_disabled():\n        result = False\n\n    log.debug('%r', result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the color of the current user is disabled.", "response": "def color_is_disabled(**envars):\n    ''' Look for clues in environment, e.g.:\n\n        - https://bixense.com/clicolors/\n        - http://no-color.org/\n\n        Arguments:\n            envars:     Additional environment variables to check for\n                        equality, i.e. ``MYAPP_COLOR_DISABLED='1'``\n\n        Returns:\n            None, Bool:  Disabled\n    '''\n    result = None\n    if 'NO_COLOR' in env:\n        result = True\n    elif env.CLICOLOR == '0':\n        result = True\n\n    log.debug('%r (NO_COLOR=%s, CLICOLOR=%s)', result,\n              env.NO_COLOR or '',\n              env.CLICOLOR or ''\n    )\n    for name, value in envars.items():\n        envar = getattr(env, name)\n        if envar.value == value:\n            result = True\n        log.debug('%s == %r: %r', name, value, result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef color_is_forced(**envars):\n    ''' Look for clues in environment, e.g.:\n\n        - https://bixense.com/clicolors/\n\n        Arguments:\n            envars:     Additional environment variables to check for\n                        equality, i.e. ``MYAPP_COLOR_FORCED='1'``\n        Returns:\n            Bool:  Forced\n    '''\n    result = env.CLICOLOR_FORCE and env.CLICOLOR_FORCE != '0'\n    log.debug('%s (CLICOLOR_FORCE=%s)', result, env.CLICOLOR_FORCE or '')\n\n    for name, value in envars.items():\n        envar = getattr(env, name)\n        if envar.value == value:\n            result = True\n        log.debug('%s == %r: %r', name, value, result)\n\n    return result", "response": "Check if color for the current node is forced."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detect_palette_support(basic_palette=None):\n    ''' Returns whether we think the terminal supports basic, extended, or\n        truecolor.  None if not able to tell.\n\n        Returns:\n            None or str: 'basic', 'extended', 'truecolor'\n    '''\n    result = col_init = win_enabled = None\n    TERM = env.TERM or ''\n    if os_name == 'nt':\n        from .windows import (is_ansi_capable, enable_vt_processing,\n                              is_colorama_initialized)\n        if is_ansi_capable():\n            win_enabled = all(enable_vt_processing())\n        col_init = is_colorama_initialized()\n\n    # linux, older Windows + colorama\n    if TERM.startswith('xterm') or (TERM == 'linux') or col_init:\n            result = 'basic'\n\n    # xterm, fbterm, older Windows + ansicon\n    if ('256color' in TERM) or (TERM == 'fbterm') or env.ANSICON:\n        result = 'extended'\n\n    # https://bugzilla.redhat.com/show_bug.cgi?id=1173688 - obsolete?\n    if env.COLORTERM in ('truecolor', '24bit') or win_enabled:\n        result = 'truecolor'\n\n    # find the platform-dependent 16-color basic palette\n    pal_name = 'Unknown'\n    if result and not basic_palette:\n        result, pal_name, basic_palette = _find_basic_palette(result)\n\n    try:\n        import webcolors\n    except ImportError:\n        webcolors = None\n\n    log.debug(\n        f'{result!r} ({os_name}, TERM={env.TERM or \"\"}, '\n        f'COLORTERM={env.COLORTERM or \"\"}, ANSICON={env.ANSICON}, '\n        f'webcolors={bool(webcolors)}, basic_palette={pal_name})'\n    )\n    return (result, basic_palette)", "response": "Detects whether the terminal supports basic extended or truecolor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_unicode_support():\n    ''' Try to detect unicode (utf8?) support in the terminal.\n\n        Experimental, implementation idea is from the link below:\n           https://unix.stackexchange.com/questions/184345/detect-how-much-of-unicode-my-terminal-supports-even-through-screen\n\n        TODO:\n            needs improvement.\n            # should return None or True on redirection?\n\n        Returns:\n            Boolean | None if not a TTY\n    '''\n    result = None\n\n    if env.LANG and env.LANG.endswith('UTF-8'):  #\u00a0approximation\n        result = True\n\n    elif is_a_tty():\n        if os_name == 'nt':\n            from .windows import get_position as _get_position\n        else:\n            _get_position = get_position\n\n        out = sys.stdout\n        # what if cursor is not at beginning of line?\n        x, _ = _get_position()\n        out.write('\u00e9')\n        out.flush()\n        x2, _ = _get_position()\n\n        difference = x2 - x\n        if difference == 1:\n            result = True\n        else:\n            result = False  # 0, 2 - no\n\n        # clean up\n        out.write(BS)\n        out.flush()\n\n    return result", "response": "Detect unicode support in the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_basic_palette(result):\n    ''' Find the platform-dependent 16-color basic palette.\n\n        This is used for \"downgrading to the nearest color\" support.\n    '''\n    pal_name = 'default (xterm)'\n    basic_palette = color_tables.xterm_palette4\n    if env.SSH_CLIENT:  # fall back to xterm over ssh, info often wrong\n        pal_name = 'ssh (xterm)'\n    else:\n        if os_name == 'nt':\n            if sys.getwindowsversion()[2] > 16299: # Win10 FCU, new palette\n                pal_name = 'cmd_1709'\n                basic_palette = color_tables.cmd1709_palette4\n            else:\n                pal_name = 'cmd_legacy'\n                basic_palette = color_tables.cmd_palette4\n        elif sys.platform == 'darwin':\n            if env.TERM_PROGRAM == 'Apple_Terminal':\n                pal_name = 'termapp'\n                basic_palette = color_tables.termapp_palette4\n            elif env.TERM_PROGRAM == 'iTerm.app':\n                pal_name = 'iterm'\n                basic_palette = color_tables.iterm_palette4\n        elif os_name == 'posix':\n            if env.TERM in ('linux', 'fbterm'):\n                pal_name = 'vtrgb'\n                basic_palette = parse_vtrgb()\n            elif env.TERM.startswith('xterm'):\n                # fix: LOW64 - Python on Linux on Windows!\n                if 'Microsoft' in os.uname().release:\n                    pal_name = 'cmd_1709'\n                    basic_palette = color_tables.cmd1709_palette4\n                    result = 'truecolor'  #\u00a0override\n                elif sys.platform.startswith('freebsd'):  # vga console :-/\n                    pal_name = 'vga'\n                    basic_palette = color_tables.vga_palette4\n                else:\n                    try:  # TODO: check green to identify palette, others?\n                        if get_color('index', 2)[0][:2] == '4e':\n                            pal_name = 'tango'\n                            basic_palette = color_tables.tango_palette4\n                        else:\n                            raise RuntimeError('not the color scheme.')\n                    except (IndexError, RuntimeError):\n                        pass\n        else:  # Amiga/Atari :-P\n            log.warn('Unexpected OS: os.name: %s', os_name)\n\n    return result, pal_name, basic_palette", "response": "Find the platform - dependent 16 - color basic palette."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a chosen palette returns tuple of those available and None when not found.", "response": "def get_available_palettes(chosen_palette):\n    ''' Given a chosen palette, returns tuple of those available,\n        or None when not found.\n\n        Because palette support of a particular level is almost always a\n        superset of lower levels, this should return all available palettes.\n\n        Returns:\n            Boolean, None: is tty or None if not found.\n    '''\n    result = None\n    try:\n        result = ALL_PALETTES[:ALL_PALETTES.index(chosen_palette)+1]\n    except ValueError:\n        pass\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects terminal or something else.", "response": "def is_a_tty(stream=sys.stdout):\n    ''' Detect terminal or something else, such as output redirection.\n\n        Returns:\n            Boolean, None: is tty or None if not found.\n    '''\n    result = stream.isatty() if hasattr(stream, 'isatty') else None\n    log.debug(result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_x11_color_map(paths=X11_RGB_PATHS):\n    ''' Load and parse X11's rgb.txt.\n\n        Loads:\n            x11_color_map: { name_lower: ('R', 'G', 'B') }\n    '''\n    if type(paths) is str:\n        paths = (paths,)\n\n    x11_color_map = color_tables.x11_color_map\n    for path in paths:\n        try:\n            with open(path) as infile:\n                for line in infile:\n                    if line.startswith('!') or line.isspace():\n                        continue\n\n                    tokens = line.rstrip().split(maxsplit=3)\n                    key = tokens[3]\n                    if ' ' in key:  # skip names with spaces to match webcolors\n                        continue\n\n                    x11_color_map[key.lower()] = tuple(tokens[:3])\n            log.debug('X11 palette found at %r.', path)\n            break\n        except FileNotFoundError as err:\n            log.debug('X11 palette file not found: %r', path)\n        except IOError as err:\n            log.debug('X11 palette file not read: %s', err)", "response": "Load and parse X11 s rgb. txt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_vtrgb(path='/etc/vtrgb'):\n    ''' Parse the color table for the Linux console. '''\n    palette = ()\n    table = []\n    try:\n        with open(path) as infile:\n            for i, line in enumerate(infile):\n                row = tuple(int(val) for val in line.split(','))\n                table.append(row)\n                if i == 2:  # failsafe\n                    break\n\n        palette = tuple(zip(*table))  # swap rows to columns\n\n    except IOError as err:\n        palette = color_tables.vga_palette4\n\n    return palette", "response": "Parse the color table for Linux console."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getch():\n    ''' POSIX\u00a0implementation of get char/key. '''\n    import tty\n\n    with TermStack() as fd:\n        tty.setraw(fd)\n        return sys.stdin.read(1)", "response": "POSIX\u00a0implementation of get char / key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a terminal response of up to a few characters from stdin.", "response": "def _read_until(infile=sys.stdin, maxchars=20, end=RS):\n    ''' Read a terminal response of up to a few characters from stdin.  '''\n    chars = []\n    read = infile.read\n    if not isinstance(end, tuple):\n        end = (end,)\n\n    # count down, stopping at 0\n    while maxchars:\n        char = read(1)\n        if char in end:\n            break\n        chars.append(char)\n        maxchars -= 1\n\n    return ''.join(chars)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying the default terminal for colors.", "response": "def get_color(name, number=None):\n    ''' Query the default terminal, for colors, etc.\n\n        Direct queries supported on xterm, iTerm, perhaps others.\n\n        Arguments:\n            str:  name,  one of ('foreground', 'fg', 'background', 'bg',\n                                 or 'index')  # index grabs a palette index\n            int:  or a \"dynamic color number of (4, 10-19),\" see links below.\n            str:  number - if name is index, number should be an int from 0\u2026255\n\n        Queries terminal using ``OSC # ? BEL`` sequence,\n        call responds with a color in this X Window format syntax:\n\n            - ``rgb:DEAD/BEEF/CAFE``\n            - `Control sequences\n              <http://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Operating-System-Commands>`_\n            - `X11 colors\n              <https://www.x.org/releases/X11R7.7/doc/libX11/libX11/libX11.html#RGB_Device_String_Specification>`_\n\n        Returns:\n            tuple[int]:\u00a0\n                A tuple of four-digit hex strings after parsing,\n                the last two digits are the least significant and can be\n                chopped if needed:\n\n                ``('DEAD', 'BEEF', 'CAFE')``\n\n                If an error occurs during retrieval or parsing,\n                the tuple will be empty.\n\n        Examples:\n            >>> get_color('bg')\n            ('0000', '0000', '0000')\n\n            >>> get_color('index', 2)   # second color in indexed\n            ('4e4d', '9a9a', '0605')    # palette, 2 aka 32 in basic\n\n        Note:\n            Blocks if terminal does not support the function.\n            Checks is_a_tty() first, since function would also block if i/o\n            were redirected through a pipe.\n\n            On Windows, only able to find palette defaults,\n            which may be different if they were customized.\n            To find the palette index instead,\n            see ``windows.get_color``.\n    '''\n    colors = ()\n    if is_a_tty() and not env.SSH_CLIENT:\n        if not 'index' in _color_code_map:\n            _color_code_map['index'] = '4;' + str(number or '')\n\n        if os_name == 'nt':\n            from .windows import get_color\n            color_id = get_color(name)\n            if sys.getwindowsversion()[2] > 16299:  # Win10 FCU, new palette\n                basic_palette = color_tables.cmd1709_palette4\n            else:\n                basic_palette = color_tables.cmd_palette4\n            colors = (f'{i:02x}' for i in basic_palette[color_id]) # compat\n\n        elif sys.platform == 'darwin':\n            if env.TERM_PROGRAM == 'iTerm.app':\n                # supports, though returns two chars per\n                colors = _get_color_xterm(name, number)\n\n        elif os_name == 'posix':\n            if sys.platform.startswith('freebsd'):\n                pass\n            elif env.TERM and env.TERM.startswith('xterm'):\n                colors = _get_color_xterm(name, number)\n\n    return tuple(colors)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_position(fallback=CURSOR_POS_FALLBACK):\n    ''' Return the current column number of the terminal cursor.\n        Used to figure out if we need to print an extra newline.\n\n        Returns:\n            tuple(int): (x, y), (,)  - empty, if an error occurred.\n\n        TODO:\u00a0needs non-ansi mode for Windows\n        Note:\n            Checks is_a_tty() first, since function would block if i/o were\n            redirected through a pipe.\n    '''\n    values = fallback\n    if is_a_tty():\n        import tty, termios\n        try:\n            with TermStack() as fd:\n                tty.setcbreak(fd, termios.TCSANOW)      # shut off echo\n                sys.stdout.write(CSI + '6n')            # screen.dsr, avoid import\n                sys.stdout.flush()\n                resp = _read_until(maxchars=10, end='R')\n        except AttributeError:  #\u00a0no .fileno()\n            return values\n\n        # parse response\n        resp = resp.lstrip(CSI)\n        try:  # reverse\n            values = tuple( int(token) for token in resp.partition(';')[::-2] )\n        except Exception as err:\n            log.error('parse error: %s on %r', err, resp)\n\n    return values", "response": "Return the current column number of the terminal cursor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_title(mode='title'):\n    ''' Return the terminal/console title.\n\n        Arguments:\n            str:  mode,  one of ('title', 'icon') or int (20-21):\n                  see links below.\n\n        - `Control sequences\n          <http://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Operating-System-Commands>`_\n\n        Returns:\n            title string, or None if not able to be found.\n\n        Note:\n            Experimental, few terms outside xterm support this correctly.\n            MATE Terminal returns \"Terminal\".\n            iTerm returns \"\".\n    '''\n    title = None\n    if is_a_tty() and not env.SSH_CLIENT:\n        if os_name == 'nt':\n            from .windows import get_title\n            return get_title()\n\n        elif sys.platform == 'darwin':\n            if env.TERM_PROGRAM and env.TERM_PROGRAM == 'iTerm.app':\n                pass\n            else:\n                return\n        elif os_name == 'posix':\n            pass\n\n        # xterm (maybe iterm) only support\n        import tty, termios\n        mode = _query_mode_map.get(mode, mode)\n        query_sequence = f'{CSI}{mode}t'\n        try:\n            with TermStack() as fd:\n                termios.tcflush(fd, termios.TCIFLUSH)   # clear input\n\n                tty.setcbreak(fd, termios.TCSANOW)      # shut off echo\n                sys.stdout.write(query_sequence)\n                sys.stdout.flush()\n                resp = _read_until(maxchars=100, end=ST)\n        except AttributeError:  #\u00a0no .fileno()\n            return title\n\n        # parse response\n        title = resp.lstrip(OSC)[1:].rstrip(ESC)\n\n    log.debug('%r', title)\n    return title", "response": "Return the terminal title."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks system for theme information.", "response": "def get_theme():\n    ''' Checks system for theme information.\n\n        First checks for the environment variable COLORFGBG.\n        Next, queries terminal, supported on Windows and xterm, perhaps others.\n        See notes on get_color().\n\n        Returns:\n            str, None: 'dark', 'light', None if no information.\n    '''\n    theme = None\n    log.debug('COLORFGBG: %s', env.COLORFGBG)\n    if env.COLORFGBG:\n        FG, _, BG = env.COLORFGBG.partition(';')\n        theme = 'dark' if BG < '8' else 'light'  # background wins\n    else:\n        if os_name == 'nt':\n            from .windows import get_color as _get_color  # avoid Unbound Local\n            color_id = _get_color('background')\n            theme = 'dark' if color_id < 8 else 'light'\n        elif os_name == 'posix':\n            if env.TERM in ('linux', 'fbterm'):  # default\n                theme = 'dark'\n            elif sys.platform.startswith('freebsd'):  # vga console :-/\n                theme = 'dark'\n            else:\n                # try xterm - find average across rgb\n                colors = get_color('background')  # bg wins\n                if colors:\n                    colors = tuple(int(cm[:2], 16) for cm in colors)\n                    avg = sum(colors) / len(colors)\n                    theme = 'dark' if avg < 128 else 'light'\n\n    log.debug('%r', theme)\n    return theme"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndons t actually use this it s just an example.", "response": "def really_bad_du(path):\n    \"Don't actually use this, it's just an example.\"\n    return sum([os.path.getsize(fp) for fp in list_files(path)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the upload directory to see if the uploaded file would exceed the total disk allotment.", "response": "def check_disk_usage(filehandle, meta):\n    \"\"\"Checks the upload directory to see if the uploaded file would exceed\n    the total disk allotment. Meant as a quick and dirty example.\n    \"\"\"\n    # limit it at twenty kilobytes if no default is provided\n    MAX_DISK_USAGE = current_app.config.get('MAX_DISK_USAGE', 20 * 1024)\n    CURRENT_USAGE = really_bad_du(current_app.config['UPLOAD_PATH'])\n    filehandle.seek(0, os.SEEK_END)\n\n    if CURRENT_USAGE + filehandle.tell() > MAX_DISK_USAGE:\n        filehandle.close()\n        raise UploadError(\"Upload exceeds allotment.\")\n    filehandle.seek(0)\n    return filehandle"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading version as text to avoid machinations at import time.", "response": "def get_version(filename, version='1.00'):\n    ''' Read version as text to avoid machinations at import time. '''\n    with open(filename) as infile:\n        for line in infile:\n            if line.startswith('__version__'):\n                try:\n                    version = line.split(\"'\")[1]\n                except IndexError:\n                    pass\n                break\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a HTTP_RANGE request header into a list of two - tuples.", "response": "def parse_range_header(self, header, resource_size):\n        \"\"\"\n        Parses a range header into a list of two-tuples (start, stop) where\n        `start` is the starting byte of the range (inclusive) and\n        `stop` is the ending byte position of the range (exclusive).\n\n        Args:\n            header (str): The HTTP_RANGE request header.\n            resource_size (int): The size of the file in bytes.\n\n        Returns:\n            None if the value of the header is not syntatically valid.\n        \"\"\"\n        if not header or '=' not in header:\n            return None\n\n        ranges = []\n        units, range_ = header.split('=', 1)\n        units = units.strip().lower()\n\n        if units != 'bytes':\n            return None\n\n        for val in range_.split(','):\n            val = val.strip()\n            if '-' not in val:\n                return None\n\n            if val.startswith('-'):\n                # suffix-byte-range-spec: this form specifies the last N bytes\n                # of an entity-body.\n                start = resource_size + int(val)\n                if start < 0:\n                    start = 0\n                stop = resource_size\n            else:\n                # byte-range-spec: first-byte-pos \"-\" [last-byte-pos].\n                start, stop = val.split('-', 1)\n                start = int(start)\n                # The +1 is here since we want the stopping point to be\n                # exclusive, whereas in the HTTP spec, the last-byte-pos\n                # is inclusive.\n                stop = int(stop) + 1 if stop else resource_size\n                if start >= stop:\n                    return None\n\n            ranges.append((start, stop))\n\n        return ranges"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd several headers that are necessary for a streaming file response.", "response": "def add_range_headers(self, range_header):\n        \"\"\"\n        Adds several headers that are necessary for a streaming file\n        response, in order for Safari to play audio files. Also\n        sets the HTTP status_code to 206 (partial content).\n\n        Args:\n            range_header (str): Browser HTTP_RANGE request header.\n        \"\"\"\n        self['Accept-Ranges'] = 'bytes'\n        size = self.ranged_file.size\n        try:\n            ranges = self.ranged_file.parse_range_header(range_header, size)\n        except ValueError:\n            ranges = None\n        # Only handle syntactically valid headers, that are simple (no\n        # multipart byteranges).\n        if ranges is not None and len(ranges) == 1:\n            start, stop = ranges[0]\n            if start >= size:\n                # Requested range not satisfiable.\n                self.status_code = 416\n                return\n\n            if stop >= size:\n                stop = size\n\n            self.ranged_file.start = start\n            self.ranged_file.stop = stop\n            self['Content-Range'] = 'bytes %d-%d/%d' % (start, stop - 1, size)\n            self['Content-Length'] = stop - start\n            self.status_code = 206"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_color_tables(base=color_tables.vga_palette4):\n    '''\n        Create the color tables for palette downgrade support,\n        starting with the platform-specific 16 from the color tables module.\n        Save as global state. :-/\n    '''\n    base = [] if base is None else base\n\n    # make sure we have them before clearing\n    table4 = _build_color_table(base, extended=False)\n    if table4:\n        color_table4.clear()\n        color_table4.extend(table4)\n\n    table8 = _build_color_table(base)\n    if table8:\n        color_table8.clear()\n        color_table8.extend(table8)", "response": "Create the color tables for palette downgrade support."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_nearest_color_index(r, g, b, color_table=None, method='euclid'):\n    ''' Given three integers representing R, G, and B,\n        return the nearest color index.\n\n        Arguments:\n            r:    int - of range 0\u2026255\n            g:    int - of range 0\u2026255\n            b:    int - of range 0\u2026255\n\n        Returns:\n            int, None: index, or None on error.\n    '''\n    shortest_distance = 257*257*3  # max eucl. distance from #000000 to #ffffff\n    index = 0                      #\u00a0default to black\n    if not color_table:\n        if not color_table8:\n            build_color_tables()\n        color_table = color_table8\n\n    for i, values in enumerate(color_table):\n        rd = r - values[0]\n        gd = g - values[1]\n        bd = b - values[2]\n\n        this_distance = (rd * rd) + (gd * gd) + (bd * bd)\n\n        if this_distance < shortest_distance:  # closer\n            index = i\n            shortest_distance = this_distance\n\n    return index", "response": "Given three integers representing R G and B return the nearest color index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a three or six - character hex digit string return the nearest color index.", "response": "def find_nearest_color_hexstr(hexdigits, color_table=None, method='euclid'):\n    ''' Given a three or six-character hex digit string, return the nearest\n        color index.\n\n        Arguments:\n            hexdigits:  a three/6 digit hex string, e.g. 'b0b', '123456'\n\n        Returns:\n            int, None: index, or None on error.\n    '''\n    triplet = []\n    try:\n        if len(hexdigits) == 3:\n            for digit in hexdigits:\n                digit = int(digit, 16)\n                triplet.append((digit * 16) + digit)\n        elif len(hexdigits) == 6:\n            triplet.extend(int(hexdigits[i:i+2], 16) for i in (0, 2, 4))\n        else:\n            raise ValueError('wrong length: %r' % hexdigits)\n    except ValueError:\n        return None\n\n    return find_nearest_color_index(*triplet,\n                                    color_table=color_table,\n                                    method=method)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_interval(self, start, end, data=None):\n        '''\n        Inserts an interval to the tree. \n        Note that when inserting we do not maintain appropriate sorting of the \"mid\" data structure.\n        This should be done after all intervals are inserted.\n        '''\n        # Ignore intervals of 0 or negative length\n        if (end - start) <= 0:\n            return\n        if self.single_interval is None:\n            # This is an empty tree and we are adding the first interval. Just record it in a field.\n            self.single_interval = (start, end, data)\n        elif self.single_interval == 0:\n            # This is a usual tree, use standard addition method\n            self._add_interval(start, end, data)\n        else:\n            # This is a tree with a single interval. Convert to a usual tree.\n            self._add_interval(*self.single_interval)\n            self.single_interval = 0\n            self._add_interval(start, end, data)", "response": "Inserts an interval into the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sort(self):\n        '''\n        Must be invoked after all intevals have been added to sort mid_** arrays.\n        '''\n        if self.single_interval is None or self.single_interval != 0:\n            return # Nothing to do for empty and leaf trees.\n        self.mid_sorted_by_start.sort(key = lambda x: x[0])\n        self.mid_sorted_by_end.sort(key = lambda x: x[1], reverse=True)\n        if self.left_subtree is not None:\n            self.left_subtree.sort()\n        if self.right_subtree is not None:\n            self.right_subtree.sort()", "response": "Sort the mid_** arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the related elements of the hierarchy for the specified entry.", "response": "def _query(self, x, result):\n        '''\n        Same as self.query, but uses a provided list to accumulate results into.\n        '''\n        if self.single_interval is None: # Empty\n            return\n        elif self.single_interval != 0:  # Single interval, just check whether x is in it\n            if self.single_interval[0] <= x < self.single_interval[1]:\n                result.append(self.single_interval)\n        elif x < self.center:            # Normal tree, query point to the left of center\n            if self.left_subtree is not None:\n                self.left_subtree._query(x, result)\n            for int in self.mid_sorted_by_start:\n                if int[0] <= x:\n                    result.append(int)\n                else:\n                    break\n        else:  # Normal tree, query point to the right of center\n            for int in self.mid_sorted_by_end:\n                if int[1] > x:\n                    result.append(int)\n                else:\n                    break\n            if self.right_subtree is not None:\n                self.right_subtree._query(x, result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a given word is accepted by a DFA returning True or False.", "response": "def dfa_word_acceptance(dfa: dict, word: list) -> bool:\n    \"\"\" Checks if a given **word** is accepted by a DFA,\n    returning True/false.\n\n    The word w is accepted by a DFA if DFA has an accepting run\n    on w. Since A is deterministic,\n    :math:`w \u2208 L(A)` if and only if :math:`\u03c1(s_0 , w) \u2208 F` .\n\n    :param dict dfa: input DFA;\n    :param list word: list of actions \u2208 dfa['alphabet'].\n    :return: *(bool)*, True if the word is accepted, False in the\n             other case.\n    \"\"\"\n    current_state = dfa['initial_state']\n    for action in word:\n        if (current_state, action) in dfa['transitions']:\n            current_state = dfa['transitions'][current_state, action]\n        else:\n            return False\n\n    if current_state in dfa['accepting_states']:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncomplete the DFA with all states and transitions assigned to each letter in the alphabet.", "response": "def dfa_completion(dfa: dict) -> dict:\n    \"\"\" Side effects on input! Completes the DFA assigning to\n    each state a transition for each letter in the alphabet (if\n    not already defined).\n\n    We say that a DFA is complete if its transition function\n    :math:`\u03c1:S\u00d7\u03a3\u2192S` is a total function, that is,\n    for all :math:`s \u2208 S` and all :math:`a \u2208 \u03a3` we have that\n    exists a :math:`\u03c1(s,a)=s_x` for some :math:`s_x \u2208 S`.\n    Given an arbitrary DFA A, its completed version :math:`A_T`\n    is obtained as follows:\n    :math:`A_T = (\u03a3, S \u222a \\{sink\\}, s_0 , \u03c1_T , F )` with\n    :math:`\u03c1_T(s,a)=sink`\n    when :math:`\u03c1(s,a)` is not defined in A and :math:`\u03c1_T=\u03c1` in\n    the other cases.\n\n    :param dict dfa: input DFA.\n    :return: *(dict)* representing the completed DFA.\n    \"\"\"\n    dfa['states'].add('sink')\n    for state in dfa['states']:\n        for action in dfa['alphabet']:\n            if (state, action) not in dfa['transitions']:\n                dfa['transitions'][state, action] = 'sink'\n    return dfa"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dfa_complementation(dfa: dict) -> dict:\n    dfa_complement = dfa_completion(deepcopy(dfa))\n    dfa_complement['accepting_states'] = \\\n        dfa_complement['states'].difference(dfa_complement['accepting_states'])\n    return dfa_complement", "response": "Returns a DFA that accepts any word but he ones accepted by the input DFA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a DFA accepting the intersection of the input DFAs.", "response": "def dfa_intersection(dfa_1: dict, dfa_2: dict) -> dict:\n    \"\"\" Returns a DFA accepting the intersection of the DFAs in\n    input.\n\n    Let :math:`A_1 = (\u03a3, S_1 , s_{01} , \u03c1_1 , F_1 )` and\n    :math:`A_2 = (\u03a3, S_2 , s_{02} , \u03c1_2 , F_2 )` be two DFAs.\n    Then there is a DFA :math:`A_\u2227` that runs simultaneously both\n    :math:`A_1` and :math:`A_2` on the input word and\n    accepts when both accept.\n    It is defined as:\n\n    :math:`A_\u2227 = (\u03a3, S_1 \u00d7 S_2 , (s_{01} , s_{02} ), \u03c1, F_1 \u00d7 F_2 )`\n\n    where\n\n    :math:`\u03c1((s_1 , s_2 ), a) = (s_{X1} , s_{X2} )` iff\n    :math:`s_{X1} = \u03c1_1 (s_1 , a)` and :math:`s_{X2}= \u03c1_2 (s_2 , a)`\n\n    Implementation proposed guarantees the resulting DFA has only\n    **reachable** states.\n\n    :param dict dfa_1: first input DFA;\n    :param dict dfa_2: second input DFA.\n    :return: *(dict)* representing the intersected DFA.\n    \"\"\"\n    intersection = {\n        'alphabet': dfa_1['alphabet'].intersection(dfa_2['alphabet']),\n        'states': {(dfa_1['initial_state'], dfa_2['initial_state'])},\n        'initial_state': (dfa_1['initial_state'], dfa_2['initial_state']),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n\n    boundary = set()\n    boundary.add(intersection['initial_state'])\n    while boundary:\n        (state_dfa_1, state_dfa_2) = boundary.pop()\n        if state_dfa_1 in dfa_1['accepting_states'] \\\n                and state_dfa_2 in dfa_2['accepting_states']:\n            intersection['accepting_states'].add((state_dfa_1, state_dfa_2))\n\n        for a in intersection['alphabet']:\n            if (state_dfa_1, a) in dfa_1['transitions'] \\\n                    and (state_dfa_2, a) in dfa_2['transitions']:\n                next_state_1 = dfa_1['transitions'][state_dfa_1, a]\n                next_state_2 = dfa_2['transitions'][state_dfa_2, a]\n                if (next_state_1, next_state_2) not in intersection['states']:\n                    intersection['states'].add((next_state_1, next_state_2))\n                    boundary.add((next_state_1, next_state_2))\n                intersection['transitions'][(state_dfa_1, state_dfa_2), a] = \\\n                    (next_state_1, next_state_2)\n\n    return intersection"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dfa_union(dfa_1: dict, dfa_2: dict) -> dict:\n    dfa_1 = deepcopy(dfa_1)\n    dfa_2 = deepcopy(dfa_2)\n    dfa_1['alphabet'] = dfa_2['alphabet'] = dfa_1['alphabet'].union(\n        dfa_2['alphabet'])  # to complete the DFAs over all possible transition\n    dfa_1 = dfa_completion(dfa_1)\n    dfa_2 = dfa_completion(dfa_2)\n\n    union = {\n        'alphabet': dfa_1['alphabet'].copy(),\n        'states': {(dfa_1['initial_state'], dfa_2['initial_state'])},\n        'initial_state': (dfa_1['initial_state'], dfa_2['initial_state']),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n\n    boundary = set()\n    boundary.add(union['initial_state'])\n    while boundary:\n        (state_dfa_1, state_dfa_2) = boundary.pop()\n        if state_dfa_1 in dfa_1['accepting_states'] \\\n                or state_dfa_2 in dfa_2['accepting_states']:\n            union['accepting_states'].add((state_dfa_1, state_dfa_2))\n        for a in union['alphabet']:\n            # as DFAs are completed they surely have the transition\n            next_state_1 = dfa_1['transitions'][state_dfa_1, a]\n            next_state_2 = dfa_2['transitions'][state_dfa_2, a]\n            if (next_state_1, next_state_2) not in union['states']:\n                union['states'].add((next_state_1, next_state_2))\n                boundary.add((next_state_1, next_state_2))\n            union['transitions'][(state_dfa_1, state_dfa_2), a] = \\\n                (next_state_1, next_state_2)\n\n    return union", "response": "Returns a DFA accepting the union of the input DFAs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the minimization of the DFA in input through a greatest fix - point method.", "response": "def dfa_minimization(dfa: dict) -> dict:\n    \"\"\" Returns the minimization of the DFA in input through a\n    greatest fix-point method.\n\n    Given a completed DFA :math:`A = (\u03a3, S, s_0 , \u03c1, F )` there\n    exists a single minimal DFA :math:`A_m`\n    which is equivalent to A, i.e. reads the same language\n    :math:`L(A) = L(A_m)` and with a minimal number of states.\n    To construct such a DFA we exploit bisimulation as a suitable\n    equivalence relation between states.\n\n    A bisimulation relation :math:`E \u2208 S \u00d7 S` is a relation\n    between states that satisfies the following condition:\n    if :math:`(s, t) \u2208 E` then:\n\n     \u2022 s \u2208 F iff t \u2208 F;\n     \u2022 For all :math:`(s_X,a)` such that :math:`\u03c1(s, a) = s_X`,\n       there exists :math:`t_X` such that :math:`\u03c1(t, a) = t_X`\n       and :math:`(s_X , t_X ) \u2208 E`;\n     \u2022 For all :math:`(t_X,a)` such that :math:`\u03c1(t, a) = t_X` ,\n       there exists :math:`s_X` such that :math:`\u03c1(s, a) = s_X`\n       and :math:`(s_X , t_X ) \u2208 E`.\n\n    :param dict dfa: input DFA.\n    :return: *(dict)* representing the minimized DFA.\n    \"\"\"\n    dfa = dfa_completion(deepcopy(dfa))\n\n    ################################################################\n    ### Greatest-fixpoint\n\n    z_current = set()\n    z_next = set()\n\n    # First bisimulation condition check (can be done just once)\n    # s \u2208 F iff t \u2208 F\n    for state_s in dfa['states']:\n        for state_t in dfa['states']:\n            if (\n                            state_s in dfa['accepting_states']\n                    and state_t in dfa['accepting_states']\n            ) or (\n                            state_s not in dfa['accepting_states']\n                    and state_t not in dfa['accepting_states']\n            ):\n                z_next.add((state_s, state_t))\n\n    # Second and third condition of bisimularity check\n    while z_current != z_next:\n        z_current = z_next\n        z_next = z_current.copy()\n        for (state_1, state_2) in z_current:\n            # for all s0,a s.t. \u03c1(s, a) = s_0 , there exists t 0\n            # s.t. \u03c1(t, a) = t 0 and (s_0 , t 0 ) \u2208 Z i ;\n            for a in dfa['alphabet']:\n                if (state_1, a) in dfa['transitions'] \\\n                        and (state_2, a) in dfa['transitions']:\n                    if (\n                            dfa['transitions'][state_1, a],\n                            dfa['transitions'][state_2, a]\n                    ) not in z_current:\n                        z_next.remove((state_1, state_2))\n                        break\n                else:\n                    # action a not possible in state element[0]\n                    # or element[1]\n                    z_next.remove((state_1, state_2))\n                    break\n\n    ################################################################\n    ### Equivalence Sets\n\n    equivalence = dict()\n    for (state_1, state_2) in z_current:\n        equivalence.setdefault(state_1, set()).add(state_2)\n\n    ################################################################\n    ### Minimal DFA construction\n\n    dfa_min = {\n        'alphabet': dfa['alphabet'].copy(),\n        'states': set(),\n        'initial_state': dfa['initial_state'],\n        'accepting_states': set(),\n        'transitions': dfa['transitions'].copy()\n    }\n\n    # select one element for each equivalence set\n    for equivalence_set in equivalence.values():\n        if dfa_min['states'].isdisjoint(equivalence_set):\n            e = equivalence_set.pop()\n            dfa_min['states'].add(e)  # TODO highlight this instruction\n            equivalence_set.add(e)\n\n    dfa_min['accepting_states'] = \\\n        dfa_min['states'].intersection(dfa['accepting_states'])\n\n    for t in dfa['transitions']:\n        if t[0] not in dfa_min['states']:\n            dfa_min['transitions'].pop(t)\n        elif dfa['transitions'][t] not in dfa_min['states']:\n            dfa_min['transitions'][t] = \\\n                equivalence[dfa['transitions'][t]]. \\\n                    intersection(dfa_min['states']).pop()\n\n    return dfa_min"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dfa_reachable(dfa: dict) -> dict:\n    reachable_states = set()  # set of reachable states from root\n    boundary = set()\n    reachable_states.add(dfa['initial_state'])\n    boundary.add(dfa['initial_state'])\n\n    while boundary:\n        s = boundary.pop()\n        for a in dfa['alphabet']:\n            if (s, a) in dfa['transitions']:\n                if dfa['transitions'][s, a] not in reachable_states:\n                    reachable_states.add(dfa['transitions'][s, a])\n                    boundary.add(dfa['transitions'][s, a])\n    dfa['states'] = reachable_states\n    dfa['accepting_states'] = \\\n        dfa['accepting_states'].intersection(dfa['states'])\n\n    transitions = dfa[\n        'transitions'].copy()  # TODO why copy? because for doesn't cycle\n    # mutable set....\n    for t in transitions:\n        if t[0] not in dfa['states']:\n            dfa['transitions'].pop(t)\n        elif dfa['transitions'][t] not in dfa['states']:\n            dfa['transitions'].pop(t)\n\n    return dfa", "response": "Removes unreachable states from a DFA and returns the pruned DFA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a pruned DFA with all states that do not reach a final state and all transitions that do not reach a final state.", "response": "def dfa_co_reachable(dfa: dict) -> dict:\n    \"\"\" Side effects on input! Removes from the DFA all states that\n    do not reach a final state and returns the pruned DFA.\n\n    It is possible to remove from a DFA A all states that do not\n    reach a final state without altering the language.\n    The co-reachable dfa :math:`A_F` corresponding to A is\n    defined as:\n\n    :math:`A_F = (\u03a3, S_F , s_0 , \u03c1|S_F , F )`\n\n    where\n\n    \u2022 :math:`S_F` is the set of states that reach a final state\n    \u2022 :math:`\u03c1|S_F` is the restriction on :math:`S_F \u00d7 \u03a3` of \u03c1.\n\n    :param dict dfa: input DFA.\n    :return: *(dict)* representing the pruned DFA.\n    \"\"\"\n\n    co_reachable_states = dfa['accepting_states'].copy()\n    boundary = co_reachable_states.copy()\n\n    # inverse transition function\n    inverse_transitions = dict()\n    for key, value in dfa['transitions'].items():\n        inverse_transitions.setdefault(value, set()).add(key)\n\n    while boundary:\n        s = boundary.pop()\n        if s in inverse_transitions:\n            for (state, action) in inverse_transitions[s]:\n                if state not in co_reachable_states:\n                    boundary.add(state)\n                    co_reachable_states.add(state)\n\n    dfa['states'] = co_reachable_states\n\n    # If not s_0 \u2208 S_F the resulting dfa is empty\n    if dfa['initial_state'] not in dfa['states']:\n        dfa = {\n            'alphabet': set(),\n            'states': set(),\n            'initial_state': None,\n            'accepting_states': set(),\n            'transitions': dict()\n        }\n        return dfa\n\n    transitions = dfa['transitions'].copy()\n    for t in transitions:\n        if t[0] not in dfa['states']:\n            dfa['transitions'].pop(t)\n        elif dfa['transitions'][t] not in dfa['states']:\n            dfa['transitions'].pop(t)\n\n    return dfa"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dfa_trimming(dfa: dict) -> dict:\n    # Reachable DFA\n    dfa = dfa_reachable(dfa)\n    # Co-reachable DFA\n    dfa = dfa_co_reachable(dfa)\n    # trimmed DFA\n    return dfa", "response": "Returns the DFA in input trimmed and co - reachable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dfa_projection(dfa: dict, symbols_to_remove: set) -> dict:\n    nfa = {\n        'alphabet': dfa['alphabet'].difference(symbols_to_remove),\n        'states': dfa['states'].copy(),\n        'initial_states': {dfa['initial_state']},\n        'accepting_states': dfa['accepting_states'].copy(),\n        'transitions': dict()\n    }\n\n    current_nfa_transitions = None\n    current_e_x = None\n    e_x = dict()  # equivalence relation dictionary\n\n    # while no more changes are possible\n    while current_nfa_transitions != nfa['transitions'] or current_e_x != e_x:\n        current_nfa_transitions = nfa['transitions'].copy()\n        current_e_x = deepcopy(e_x)\n        for (state, a) in dfa['transitions']:\n            next_state = dfa['transitions'][state, a]\n            if a in symbols_to_remove:\n                # mark next_state as equivalent to state\n                e_x.setdefault(state, set()).add(next_state)\n\n                app_set = set()\n                for equivalent in e_x[state]:\n                    # mark states equivalent to next_states also to state\n                    if equivalent in e_x:\n                        app_set.update(e_x[equivalent])\n                    # add all transitions of equivalent states to state\n                    for act in nfa['alphabet']:\n                        if (equivalent, act) in dfa['transitions']:\n                            equivalent_next = dfa['transitions'][\n                                equivalent, act]\n                            nfa['transitions'].setdefault(\n                                (state, act), set()).add(equivalent_next)\n                            # if equivalent_next has equivalent states\n                            if equivalent_next in e_x:\n                                # the transition leads also to these states\n                                nfa['transitions'][state, act].update(\n                                    e_x[equivalent_next])\n                e_x[state].update(app_set)\n            else:\n                # add the transition to the NFA\n                nfa['transitions'].setdefault((state, a), set()).add(\n                    next_state)\n                # if next_state has equivalent states\n                if next_state in e_x:\n                    # the same transition arrive also to all these other states\n                    nfa['transitions'][state, a].update(e_x[next_state])\n\n    # Add all state equivalent to the initial one to NFA initial states set\n    if dfa['initial_state'] in e_x:\n        nfa['initial_states'].update(e_x[dfa['initial_state']])\n\n    return nfa", "response": "Returns a NFA that reads the language recognized by the DFA and removes all the symbols in symbols_to_project from the alphabet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the input DFA is nonempty and returns True if it recognizes a language except the empty one.", "response": "def dfa_nonemptiness_check(dfa: dict) -> bool:\n    \"\"\" Checks if the input DFA is nonempty (i.e. if it recognizes a\n    language except the empty one), returning True/False.\n\n    An automaton A is nonempty if :math:`L(A) \u2260 \u2205`. L(A) is\n    nonempty iff there are states :math:`s_0 and t \u2208 F` such\n    that t is connected to :math:`s_0`. Thus, automata\n    nonemptiness is equivalent to graph reachability, where a\n    breadth-first-search algorithm can construct in linear time\n    the set of all states connected to initial state\n    :math:`s_0`.\n    A is nonempty iff this set intersects F nontrivially.\n\n    :param dict dfa: input DFA.\n    :return: *(bool)*, True if the DFA is nonempty, False otherwise\n    \"\"\"\n    # BFS\n    queue = [dfa['initial_state']]\n    visited = set()\n    visited.add(dfa['initial_state'])\n    while queue:\n        state = queue.pop(0)  # TODO note that this pop is applied to a list\n        # not like in sets\n        visited.add(state)\n        for a in dfa['alphabet']:\n            if (state, a) in dfa['transitions']:\n                if dfa['transitions'][state, a] in dfa['accepting_states']:\n                    return True\n                if dfa['transitions'][state, a] not in visited:\n                    queue.append(dfa['transitions'][state, a])\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_dfa_states(dfa: dict, suffix: str):\n    conversion_dict = dict()\n    new_states = set()\n    new_accepting = set()\n    for state in dfa['states']:\n        conversion_dict[state] = '' + suffix + state\n        new_states.add('' + suffix + state)\n        if state in dfa['accepting_states']:\n            new_accepting.add('' + suffix + state)\n\n    dfa['states'] = new_states\n    dfa['initial_state'] = '' + suffix + dfa['initial_state']\n    dfa['accepting_states'] = new_accepting\n\n    new_transitions = dict()\n    for transition in dfa['transitions']:\n        new_transitions[conversion_dict[transition[0]], transition[1]] = \\\n            conversion_dict[dfa['transitions'][transition]]\n    dfa['transitions'] = new_transitions\n    return dfa", "response": "Side effect on input! Renames all the states of the DFA adding a suffix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_liftover_chain_file(from_db, to_db, search_dir='.', cache_dir=os.path.expanduser(\"~/.pyliftover\"), use_web=True, write_cache=True):\n    '''\n    A \"smart\" way of obtaining liftover chain files.\n    By default acts as follows:\n     1. If the file ``<from_db>To<to_db>.over.chain.gz`` exists in <search_dir>,\n        opens it for reading via gzip.open.\n     2. Otherwise, if the file ``<from_db>To<to_db>.over.chain`` exists\n        in the <search_dir> opens it (as uncompressed file).\n        Steps 1 and 2 may be disabled if search_dir is set to None.\n     3. Otherwise, checks whether ``<cache_dir>/<from_db>To<to_db>.over.chain.gz`` exists.\n        This step may be disabled by specifying cache_dir = None.\n     4. If file still not found attempts to download the file from the URL\n        'http://hgdownload.cse.ucsc.edu/goldenPath/<from_db>/liftOver/<from_db>To<to_db>.over.chain.gz'\n        to a temporary location. This step may be disabled by specifying use_web=False. In this case the operation fails and \n        the function returns None.\n     5. At this point, if write_cache=True and cache_dir is not None and writable, the file is copied to cache_dir and opened from there.\n        Otherwise it is opened from the temporary location.\n        \n    In case of errors (e.g. URL cannot be opened), None is returned.\n    '''\n    to_db = to_db[0].upper() + to_db[1:]\n    FILE_NAME_GZ = '%sTo%s.over.chain.gz' % (from_db, to_db)\n    FILE_NAME = '%sTo%s.over.chain' % (from_db, to_db)\n    \n    if search_dir is not None:\n        FILE_GZ = os.path.join(search_dir, FILE_NAME_GZ)\n        FILE = os.path.join(search_dir, FILE_NAME)\n        if os.path.isfile(FILE_GZ):\n            return gzip.open(FILE_GZ, 'rb')\n        elif os.path.isfile(FILE):\n            return open(FILE, 'rb')\n    if cache_dir is not None:\n        FILE_GZ = os.path.join(cache_dir, FILE_NAME_GZ)\n        if os.path.isfile(FILE_GZ):\n            return gzip.open(FILE_GZ, 'rb')\n    if use_web:\n        # Download file from the web.\n        try:\n            url = 'http://hgdownload.cse.ucsc.edu/goldenPath/%s/liftOver/%sTo%s.over.chain.gz' % (from_db, from_db, to_db)\n            (filename, headers) = _urlopener.retrieve(url)\n        except:\n            # Download failed, exit\n            return None\n        # Move the file to cache?\n        if write_cache and (cache_dir is not None):\n            try:\n                if not os.path.isdir(cache_dir):\n                    os.mkdir(cache_dir)\n                shutil.move(filename, FILE_GZ)\n                # Move successful, open from cache\n                return gzip.open(FILE_GZ, 'rb')\n            except:\n                # Move failed, open file from temp location\n                return gzip.open(filename, 'rb')\n        else:\n            # Open from temp location\n            return gzip.open(filename, 'rb')\n    # If we didn't quit before this place, all failed.\n    return None", "response": "Open the liftover chain file for reading."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading all LiftOverChain objects from a file into an array. Returns the result.", "response": "def _load_chains(f):\n        '''\n        Loads all LiftOverChain objects from a file into an array. Returns the result.\n        '''\n        chains = []\n        while True:\n            line = f.readline()\n            if not line:\n                break\n            if line.startswith(b'#') or line.startswith(b'\\n') or line.startswith(b'\\r'):\n                continue\n            if line.startswith(b'chain'):\n                # Read chain\n                chains.append(LiftOverChain(line, f))\n                continue\n        return chains"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a dictionary that maps source_name to intervals and intervals in the given list of LiftOverChain objects.", "response": "def _index_chains(chains):\n        '''\n        Given a list of LiftOverChain objects, creates a\n         dict: source_name --> \n            IntervalTree: <source_from, source_to> -->\n                (target_from, target_to, chain)\n        Returns the resulting dict.\n        Throws an exception on any errors or inconsistencies among chains (e.g. different sizes specified for the same chromosome in various chains).\n        '''\n        chain_index = {}\n        source_size = {}\n        target_size = {}\n        for c in chains:\n            # Verify that sizes of chromosomes are consistent over all chains\n            source_size.setdefault(c.source_name, c.source_size)\n            if source_size[c.source_name] != c.source_size:\n                raise Exception(\"Chains have inconsistent specification of source chromosome size for %s (%d vs %d)\" % (c.source_name, source_size[c.source_name], c.source_size))\n            target_size.setdefault(c.target_name, c.target_size)\n            if target_size[c.target_name] != c.target_size:\n                raise Exception(\"Chains have inconsistent specification of target chromosome size for %s (%d vs %d)\" % (c.target_name, target_size[c.target_name], c.target_size))\n            chain_index.setdefault(c.source_name, IntervalTree(0, c.source_size))\n            # Register all blocks from the chain in the corresponding interval tree\n            tree = chain_index[c.source_name]\n            for (sfrom, sto, tfrom, tto) in c.blocks:\n                tree.add_interval(sfrom, sto, (tfrom, tto, c))\n\n        # Sort all interval trees\n        for k in chain_index:\n            chain_index[k].sort()\n        return chain_index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a chromosome and position returns all matching records from the chain index.", "response": "def query(self, chromosome, position):\n        '''\n        Given a chromosome and position, returns all matching records from the chain index.\n        Each record is an interval (source_from, source_to, data)\n        where data = (target_from, target_to, chain). Note that depending on chain.target_strand, the target values may need to be reversed (e.g. pos --> chain.target_size - pos).\n        \n        If chromosome is not found in the index, None is returned.\n        '''\n        # A somewhat-ugly hack to allow both 'bytes' and 'str' objects to be used as\n        # chromosome names in Python 3. As we store chromosome names as strings,\n        # we'll transparently translate the query to a string too.\n        if type(chromosome).__name__ == 'bytes':\n            chromosome = chromosome.decode('ascii')\n        if chromosome not in self.chain_index:\n            return None\n        else:\n            return self.chain_index[chromosome].query(position)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmonitors a stop in a specific city.", "response": "def monitor(stop, offset=0, limit=10, city='Dresden', *, raw=False):\n    \"\"\"\n    VVO Online Monitor\n    (GET http://widgets.vvo-online.de/abfahrtsmonitor/Abfahrten.do)\n\n    :param stop: Name of Stop\n    :param offset: Minimum time of arrival\n    :param limit: Count of returned results\n    :param city: Name of City\n    :param raw: Return raw response\n    :return: Dict of stops\n    \"\"\"\n    try:\n        r = requests.get(\n            url='http://widgets.vvo-online.de/abfahrtsmonitor/Abfahrten.do',\n            params={\n                'ort': city,\n                'hst': stop,\n                'vz': offset,\n                'lim': limit,\n            },\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.RequestException as e:\n        print('Failed to access VVO monitor. Request Exception', e)\n        response = None\n\n    if response is None:\n        return None\n    return response if raw else [\n        {\n            'line': line,\n            'direction': direction,\n            'arrival': 0 if arrival == '' else int(arrival)\n        } for line, direction, arrival in response\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef route(origin, destination, city_origin='Dresden', city_destination='Dresden', time=None,\n          deparr='dep', eduroam=False, recommendations=False, *, raw=False):\n    \"\"\"\n    VVO Online EFA TripRequest\n    (GET http://efa.vvo-online.de:8080/dvb/XML_TRIP_REQUEST2)\n\n    :param origin: Origin of route\n    :param destination: Destination of route\n    :param city_origin: City of origin\n    :param city_destination: City of destination\n    :param time: Unix timestamp of departure\n    :param deparr: 'dep' for departure time (default), or 'arr' for arrival\n    :param eduroam: Request from eduroam\n    :param recommendations: Recommendations for interchange\n    :param raw: Return raw response\n    :return: List of single trips\n    \"\"\"\n\n    assert deparr == 'dep' or deparr == 'arr'\n\n    time = datetime.now() if time is None else datetime.fromtimestamp(int(time))\n\n    url = 'http://efa.faplino.de/dvb/XML_TRIP_REQUEST2' if eduroam \\\n        else 'http://efa.vvo-online.de:8080/dvb/XML_TRIP_REQUEST2'\n\n    try:\n        r = requests.get(\n            url=url,\n            params={\n                'sessionID': '0',\n                'requestID': '0',\n                'language': 'de',\n                'execInst': 'normal',\n                'command': '',\n                'ptOptionsActive': '-1',\n                'itOptionsActive': '',\n                'itDateDay': time.day,\n                'itDateMonth': time.month,\n                'itDateYear': time.year,\n                'place_origin': city_origin,\n                'placeState_origin': 'empty',\n                'type_origin': 'stop',\n                'name_origin': origin,\n                'nameState_origin': 'empty',\n                'place_destination': city_destination,\n                'placeState_destination': 'empty',\n                'type_destination': 'stop',\n                'name_destination': destination,\n                'nameState_destination': 'empty',\n                'itdTripDateTimeDepArr': deparr,\n                'itdTimeHour': time.hour,\n                'idtTimeMinute': time.minute,\n                'outputFormat': 'JSON',\n                'coordOutputFormat': 'WGS84',\n                'coordOutputFormatTail': '0',\n            },\n            timeout=10\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.Timeout:\n        print('Failed to access VVO TripRequest. Connection timed out. Are you connected to eduroam?')\n        response = None\n    except requests.RequestException as e:\n        print('Failed to access VVO TripRequest. Request Exception', e)\n        response = None\n\n    if response is None:\n        return None\n\n    if raw:\n        return response\n\n    prettified = {\n        'origin': response['origin']['points']['point']['name'],\n        'destination': response['destination']['points']['point']['name'],\n        'trips': [\n            process_single_trip(single_trip) for single_trip in response['trips']\n            ]\n    }\n\n    if recommendations:\n        prettified['trips'] = [interchange_prediction(trip) for trip in prettified['trips']]\n\n    return prettified", "response": "This function returns a list of trips from origin to destination."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find(search, eduroam=False, *, raw=False):\n\n    url = 'http://efa.faplino.de/dvb/XML_STOPFINDER_REQUEST' if eduroam \\\n        else 'http://efa.vvo-online.de:8080/dvb/XML_STOPFINDER_REQUEST'\n\n    try:\n        r = requests.get(\n            url=url,\n            params={\n                'locationServerActive': '1',\n                'outputFormat': 'JSON',\n                'type_sf': 'any',\n                'name_sf': search,\n                'coordOutputFormat': 'WGS84',\n                'coordOutputFormatTail': '0',\n            },\n            timeout=10\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.Timeout:\n        print('Failed to access VVO StopFinder. Connection timed out. Are you connected to eduroam?')\n        response = None\n    except requests.RequestException as e:\n        print('Failed to access VVO StopFinder. Request Exception', e)\n        response = None\n\n    if response is None or raw:\n        return response\n\n    points = response['stopFinder']['points']\n    return [\n        # single result\n        find_return_results(points['point'])\n    ] if 'point' in points else [\n        # multiple results\n        find_return_results(stop)\n        for stop in points\n        ]", "response": "Find a stop in the NCBI stop finder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the DVB Map Pins for a given location.", "response": "def pins(swlat, swlng, nelat, nelng, pintypes='stop', *, raw=False):\n    \"\"\"\n    DVB Map Pins\n    (GET https://www.dvb.de/apps/map/pins)\n\n    :param swlat: South-West Bounding Box Latitude\n    :param swlng: South-West Bounding Box Longitude\n    :param nelat: North-East Bounding Box Latitude\n    :param nelng: North-East Bounding Box Longitude\n    :param pintypes: Types to search for, defaults to 'stop'\n    :param raw: Return raw response\n    :return:\n    \"\"\"\n    try:\n        swlat, swlng = wgs_to_gk4(swlat, swlng)\n        nelat, nelng = wgs_to_gk4(nelat, nelng)\n        r = requests.get(\n            url='https://www.dvb.de/apps/map/pins',\n            params={\n                'showlines': 'true',\n                'swlat': swlat,\n                'swlng': swlng,\n                'nelat': nelat,\n                'nelng': nelng,\n                'pintypes': pintypes,\n            },\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.RequestException as e:\n        print('Failed to access DVB map pins app. Request Exception', e)\n        response = None\n\n    if response is None:\n        return None\n\n    return response if raw else [pins_return_results(line, pintypes) for line in response]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poi_coords(poi_id, *, raw=False):\n    try:\n        r = requests.get(\n            url='https://www.dvb.de/apps/map/coordinates',\n            params={\n                'id': poi_id,\n            },\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.RequestException as e:\n        print('Failed to access DVB map coordinates app. Request Exception', e)\n        response = None\n\n    if response is None or raw:\n        return response\n\n    coords = [int(i) for i in response.split('|')]\n    lat, lng = gk4_to_wgs(coords[0], coords[1])\n    return {\n        'lat': lat,\n        'lng': lng\n    }", "response": "Get DVB map coordinates of a poi."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef address(lat, lng, *, raw=False):\n    try:\n        lat, lng = wgs_to_gk4(lat, lng)\n        r = requests.get(\n            url='https://www.dvb.de/apps/map/address',\n            params={\n                'lat': lat,\n                'lng': lng,\n            },\n        )\n        if r.status_code == 200:\n            response = json.loads(r.content.decode('utf-8'))\n        else:\n            raise requests.HTTPError('HTTP Status: {}'.format(r.status_code))\n    except requests.RequestException as e:\n        print('Failed to access DVB map address app. Request Exception', e)\n        response = None\n\n    if response is None:\n        return None\n    return response if raw else process_address(response)", "response": "Get the DVB map address for a given latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checksum(string):\n    digits = list(map(int, string))\n    odd_sum = sum(digits[-1::-2])\n    even_sum = sum([sum(divmod(2 * d, 10)) for d in digits[-2::-2]])\n    return (odd_sum + even_sum) % 10", "response": "Compute the Luhn checksum for the provided string of digits."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_blocked(self, ip):\n        blocked = True\n\n        if ip in self.allowed_admin_ips:\n            blocked = False\n\n        for allowed_range in self.allowed_admin_ip_ranges:\n            if ipaddress.ip_address(ip) in ipaddress.ip_network(allowed_range):\n                blocked = False\n\n        return blocked", "response": "Determine if an IP address should be blocked."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a server which will watch for changes and serve the contents of the current directory.", "response": "def serve(args):\n    \"\"\"Start a server which will watch .md and .rst files for changes.\n    If a md file changes, the Home Documentation is rebuilt. If a .rst\n    file changes, the updated sphinx project is rebuilt\n\n    Args:\n        args (ArgumentParser): flags from the CLI\n    \"\"\"\n    # Sever's parameters\n    port = args.serve_port or PORT\n    host = \"0.0.0.0\"\n\n    # Current working directory\n    dir_path = Path().absolute()\n    web_dir = dir_path / \"site\"\n\n    # Update routes\n    utils.set_routes()\n\n    # Offline mode\n    if args.offline:\n        os.environ[\"MKINX_OFFLINE\"] = \"true\"\n        _ = subprocess.check_output(\"mkdocs build > /dev/null\", shell=True)\n        utils.make_offline()\n\n    class MkinxHTTPHandler(SimpleHTTPRequestHandler):\n        \"\"\"Class routing urls (paths) to projects (resources)\n        \"\"\"\n\n        def translate_path(self, path):\n            # default root -> cwd\n            location = str(web_dir)\n            route = location\n\n            if len(path) != 0 and path != \"/\":\n                for key, loc in utils.get_routes():\n                    if path.startswith(key):\n                        location = loc\n                        path = path[len(key) :]\n                        break\n\n            if location[-1] == \"/\" or not path or path[0] == \"/\":\n                route = location + path\n            else:\n                route = location + \"/\" + path\n\n            return route.split(\"?\")[0]\n\n    # Serve as deamon thread\n    success = False\n    count = 0\n    print(\"Waiting for server port...\")\n    try:\n        while not success:\n            try:\n                httpd = socketserver.TCPServer((host, port), MkinxHTTPHandler)\n                success = True\n            except OSError:\n                count += 1\n            finally:\n                if not success and count > 20:\n                    s = \"port {} seems occupied. Try with {} ? (y/n)\"\n                    if \"y\" in input(s.format(port, port + 1)):\n                        port += 1\n                        count = 0\n                    else:\n                        print(\"You can specify a custom port with mkinx serve -s\")\n                        return\n                time.sleep(0.5)\n    except KeyboardInterrupt:\n        print(\"Aborting.\")\n        return\n\n    httpd.allow_reuse_address = True\n    print(\"\\nServing at http://{}:{}\\n\".format(host, port))\n    thread = threading.Thread(target=httpd.serve_forever)\n    thread.daemon = True\n    thread.start()\n\n    # Watch for changes\n    event_handler = utils.MkinxFileHandler(\n        patterns=[\"*.rst\", \"*.md\", \"*.yml\", \"*.yaml\"]\n    )\n    observer = Observer()\n    observer.schedule(event_handler, path=str(dir_path), recursive=True)\n    observer.start()\n\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n        httpd.server_close()\n    observer.join()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the documentation for the projects specified in the CLI.", "response": "def build(args):\n    \"\"\"Build the documentation for the projects specified in the CLI.\n    It will do 4 different things for each project the\n    user asks for (see flags):\n        1. Update mkdocs's index.md file with links to project\n           documentations\n        2. Build these documentations\n        3. Update the documentations' index.html file to add a link\n           back to the home of all documentations\n        4. Build mkdoc's home documentation\n\n    Args:\n        args (ArgumentParser): parsed args from an ArgumentParser\n    \"\"\"\n    # Proceed?\n    go = False\n\n    # Current working directory\n    dir_path = Path().resolve()\n\n    # Set of all available projects in the dir\n    # Projects must contain a PROJECT_MARKER file.\n    all_projects = {\n        m\n        for m in os.listdir(dir_path)\n        if os.path.isdir(m) and \"source\" in os.listdir(dir_path / m)\n    }\n\n    if args.all and args.projects:\n        print(\n            \"{}Can't use both the 'projects' and 'all' flags{}\".format(\n                utils.colors.FAIL, utils.colors.ENDC\n            )\n        )\n        return\n\n    if not args.all and not args.projects:\n        print(\n            \"{}You have to specify at least one project (or all){}\".format(\n                utils.colors.FAIL, utils.colors.ENDC\n            )\n        )\n        return\n\n    if args.force:\n        go = True\n        projects = (\n            all_projects if args.all else all_projects.intersection(set(args.projects))\n        )\n\n    elif args.projects:\n        s = \"You are about to build the docs for: \"\n        s += \"\\n- {}\\nContinue? (y/n) \".format(\"\\n- \".join(args.projects))\n        if \"y\" in input(s):\n            go = True\n            projects = all_projects.intersection(set(args.projects))\n    elif args.all:\n        s = \"You're about to build the docs for ALL projects.\"\n        s += \"\\nContinue? (y/n) \"\n        if \"y\" in input(s):\n            go = True\n            projects = all_projects\n\n    if go:\n        # Update projects links\n        listed_projects = utils.get_listed_projects()\n\n        # Don't update projects which are not listed in the Documentation's\n        # Home if the -o flag was used\n        if args.only_index:\n            projects = listed_projects.intersection(projects)\n        print(\"projects\", projects)\n        for project_to_build in projects:\n            # Re-build documentation\n            warnings.warn(\"[sphinx]\")\n            if args.verbose:\n                os.system(\n                    \"cd {} && make clean && make html\".format(\n                        dir_path / project_to_build\n                    )\n                )\n            else:\n                os.system(\n                    \"cd {} && make clean && make html > /dev/null\".format(\n                        dir_path / project_to_build\n                    )\n                )\n\n            # Add link to Documentation's Home\n            utils.overwrite_view_source(project_to_build, dir_path)\n\n            if args.verbose:\n                print(\"\\n>>>>>> Done {}\\n\\n\\n\".format(project_to_build))\n        # Build Documentation\n        if args.verbose:\n            os.system(\"mkdocs build\")\n            print(\"\\n\\n>>>>>> Build Complete.\")\n        else:\n            warnings.warn(\"[mkdocs]\")\n            os.system(\"mkdocs build > /dev/null\")\n\n        if args.offline:\n            utils.make_offline()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a Home Documentation s folder", "response": "def init(args):\n    \"\"\"Initialize a Home Documentation's folder\n\n    Args:\n        args (ArgumentParser): Flags from the CLI\n    \"\"\"\n    # working directory\n    dir_path = Path().absolute()\n\n    if not args.project_name or args.project_name.find(\"/\") >= 0:\n        print(\n            \"{}You should specify a valid project name{}\".format(\n                utils.colors.FAIL, utils.colors.ENDC\n            )\n        )\n        return\n\n    project_path = dir_path / args.project_name\n\n    # Create the Home Documentation's directory\n    if not project_path.exists():\n        project_path.mkdir()\n    else:\n        print(\n            \"{}This project already exists{}\".format(\n                utils.colors.FAIL, utils.colors.ENDC\n            )\n        )\n        return\n\n    # Directory with the Home Documentation's source code\n    home_doc_path = project_path / \"docs\"\n    home_doc_path.mkdir()\n    help_doc_path = home_doc_path / \"help\"\n    help_doc_path.mkdir()\n\n    file_path = Path(__file__).resolve().parent / \"include\"\n\n    # Add initial files\n    copyfile(file_path / \"index.md\", home_doc_path / \"index.md\")\n    copyfile(file_path / \"How_To_Use_Mkinx.md\", help_doc_path / \"How_To_Use_Mkinx.md\")\n    copyfile(\n        file_path / \"Writing_Sphinx_Documentation.md\",\n        help_doc_path / \"Writing_Sphinx_Documentation.md\",\n    )\n\n    with open(file_path / \"mkdocs.yml\", \"r\") as f:\n        lines = f.readlines()\n\n    input_text = \"What is your Documentation's name\"\n    input_text += \" (it can be changed later in mkdocs.yml)?\\n\"\n    input_text += \"[Default: {} - Home Documentation]\\n\"\n\n    site_name = input(input_text.format(args.project_name.capitalize()))\n    if not site_name:\n        site_name = \"{} - Home Documentation\".format(args.project_name.capitalize())\n\n    lines[0] = \"site_name: {}\\n\".format(site_name)\n\n    with open(project_path / \"mkdocs.yml\", \"w\") as f:\n        f.writelines(lines)\n\n    example_project_path = project_path / \"example_project\" / \"example_project\"\n\n    windows = \"y\" if sys.platform in {\"win32\", \"cygwin\"} else \"n\"\n\n    copytree(file_path / \"example_project\", example_project_path)\n    move(str(example_project_path / \"source\"), str(project_path / \"example_project\"))\n    move(\n        str(project_path / \"example_project\" / \"example_project\" / \"Makefile\"),\n        str(project_path / \"example_project\"),\n    )\n    if windows == \"y\":\n        move(\n            str(project_path / \"example_project\" / \"example_project\" / \"make.bat\"),\n            str(project_path / \"example_project\"),\n        )\n    else:\n        os.remove(\n            str(project_path / \"example_project\" / \"example_project\" / \"make.bat\")\n        )\n\n    static = project_path / \"example_project\" / \"source\"\n    static /= \"_static\"\n    if not static.exists():\n        static.mkdir()\n\n    _ = subprocess.check_output(\n        \"cd {} && mkinx build -F -A > /dev/null\".format(args.project_name), shell=True\n    )\n\n    print(\n        \"\\n\\n\",\n        utils.colors.OKBLUE,\n        \"{}/{} created as a showcase of how mkinx works\".format(\n            args.project_name, \"example_project\"\n        ),\n        utils.colors.ENDC,\n    )\n\n    print(\n        \"\\n\",\n        utils.colors.OKGREEN,\n        \"Success!\",\n        utils.colors.ENDC,\n        \"You can now start your Docs in ./{}\\n\".format(args.project_name),\n        utils.colors.HEADER,\n        \"$ cd ./{}\".format(args.project_name),\n        utils.colors.ENDC,\n    )\n    print(\n        \"  Start the server from within your Docs to see them \\n  (default\",\n        \"port is 8443 but you can change it with the -s flag):\",\n    )\n    print(\n        utils.colors.HEADER,\n        \" {} $ mkinx serve\\n\".format(args.project_name),\n        utils.colors.ENDC,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(self, X_train, Y_train, X_test, Y_test):\n\n        while True:\n            print(1)\n            time.sleep(1)\n            if random.randint(0, 9) >= 5:\n                break", "response": "Train and validate the LR on a train and test dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a file from a URL and store it in a folder.", "response": "def download(url, path, kind='file',\n             progressbar=True, replace=False, timeout=10., verbose=True):\n    \"\"\"Download a URL.\n\n    This will download a file and store it in a '~/data/` folder,\n    creating directories if need be. It will also work for zip\n    files, in which case it will unzip all of the files to the\n    desired location.\n\n    Parameters\n    ----------\n    url : string\n        The url of the file to download. This may be a dropbox\n        or google drive \"share link\", or a regular URL. If it\n        is a share link, then it should point to a single file and\n        not a folder. To download folders, zip them first.\n    path : string\n        The path where the downloaded file will be stored. If ``zipfile``\n        is True, then this must be a folder into which files will be zipped.\n    kind : one of ['file', 'zip', 'tar', 'tar.gz']\n        The kind of file to be downloaded. If not 'file', then the file\n        contents will be unpackaged according to the kind specified. Package\n        contents will be placed in ``root_destination/<name>``.\n    progressbar : bool\n        Whether to display a progress bar during file download.\n    replace : bool\n        If True and the URL points to a single file, overwrite the\n        old file if possible.\n    timeout : float\n        The URL open timeout.\n    verbose : bool\n        Whether to print download status to the screen.\n\n    Returns\n    -------\n    out_path : string\n        A path to the downloaded file (or folder, in the case of\n        a zip file).\n    \"\"\"\n    if kind not in ALLOWED_KINDS:\n        raise ValueError('`kind` must be one of {}, got {}'.format(\n            ALLOWED_KINDS, kind))\n\n    # Make sure we have directories to dump files\n    path = op.expanduser(path)\n\n    if len(path) == 0:\n        raise ValueError('You must specify a path. For current directory use .')\n\n    download_url = _convert_url_to_downloadable(url)\n\n    if replace is False and op.exists(path):\n        msg = ('Replace is False and data exists, so doing nothing. '\n               'Use replace==True to re-download the data.')\n    elif kind in ZIP_KINDS:\n        # Create new folder for data if we need it\n        if not op.isdir(path):\n            if verbose:\n                tqdm.write('Creating data folder...')\n            os.makedirs(path)\n\n        # Download the file to a temporary folder to unzip\n        path_temp = _TempDir()\n        path_temp_file = op.join(path_temp, \"tmp.{}\".format(kind))\n        _fetch_file(download_url, path_temp_file, timeout=timeout,\n                    verbose=verbose)\n\n        # Unzip the file to the out path\n        if verbose:\n            tqdm.write('Extracting {} file...'.format(kind))\n        if kind == 'zip':\n            zipper = ZipFile\n        elif kind == 'tar':\n            zipper = tarfile.open\n        elif kind == 'tar.gz':\n            zipper = partial(tarfile.open, mode='r:gz')\n        with zipper(path_temp_file) as myobj:\n            myobj.extractall(path)\n        msg = 'Successfully downloaded / unzipped to {}'.format(path)\n    else:\n        if not op.isdir(op.dirname(path)):\n            os.makedirs(op.dirname(path))\n        _fetch_file(download_url, path, timeout=timeout, verbose=verbose)\n        msg = 'Successfully downloaded file to {}'.format(path)\n    if verbose:\n        tqdm.write(msg)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_url_to_downloadable(url):\n\n    if 'drive.google.com' in url:\n        # For future support of google drive\n        file_id = url.split('d/')[1].split('/')[0]\n        base_url = 'https://drive.google.com/uc?export=download&id='\n        out = '{}{}'.format(base_url, file_id)\n    elif 'dropbox.com' in url:\n        if url.endswith('.png'):\n            out = url + '?dl=1'\n        else:\n            out = url.replace('dl=0', 'dl=1')\n    elif 'github.com' in url:\n        out = url.replace('github.com', 'raw.githubusercontent.com')\n        out = out.replace('blob/', '')\n    else:\n        out = url\n    return out", "response": "Convert a url to the proper style depending on its website."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload a file from Dropbox and save it in a temporary file.", "response": "def _fetch_file(url, file_name, resume=True,\n                hash_=None, timeout=10., progressbar=True, verbose=True):\n    \"\"\"Load requested file, downloading it if needed or requested.\n\n    Parameters\n    ----------\n    url: string\n        The url of file to be downloaded.\n    file_name: string\n        Name, along with the path, of where downloaded file will be saved.\n    resume: bool, optional\n        If true, try to resume partially downloaded files.\n    hash_ : str | None\n        The hash of the file to check. If None, no checking is\n        performed.\n    timeout : float\n        The URL open timeout.\n    verbose : bool\n        Whether to print download status.\n    \"\"\"\n    # Adapted from NISL and MNE-python:\n    # https://github.com/nisl/tutorial/blob/master/nisl/datasets.py\n    # https://martinos.org/mne\n    if hash_ is not None and (not isinstance(hash_, string_types) or\n                              len(hash_) != 32):\n        raise ValueError('Bad hash value given, should be a 32-character '\n                         'string:\\n%s' % (hash_,))\n    temp_file_name = file_name + \".part\"\n\n    try:\n        if 'dropbox.com' in url:\n            # Use requests to handle cookies.\n            # XXX In the future, we should probably use requests everywhere.\n            # Unless we want to minimize dependencies.\n            try:\n                import requests\n            except ModuleNotFoundError:\n                raise ValueError('To download Dropbox links, you need to '\n                                 'install the `requests` module.')\n            resp = requests.get(url)\n            chunk_size = 8192  # 2 ** 13\n            with open(temp_file_name, 'wb') as ff:\n                for chunk in resp.iter_content(chunk_size=chunk_size):\n                    if chunk:  # filter out keep-alive new chunks\n                        ff.write(chunk)\n        else:\n            # Check file size and displaying it alongside the download url\n            u = urllib.request.urlopen(url, timeout=timeout)\n            u.close()\n            # this is necessary to follow any redirects\n            url = u.geturl()\n            u = urllib.request.urlopen(url, timeout=timeout)\n            try:\n                file_size = int(u.headers.get('Content-Length', '1').strip())\n            finally:\n                u.close()\n                del u\n            if verbose:\n                tqdm.write('Downloading data from %s (%s)\\n'\n                           % (url, sizeof_fmt(file_size)))\n\n            # Triage resume\n            if not os.path.exists(temp_file_name):\n                resume = False\n            if resume:\n                with open(temp_file_name, 'rb', buffering=0) as local_file:\n                    local_file.seek(0, 2)\n                    initial_size = local_file.tell()\n                del local_file\n            else:\n                initial_size = 0\n            # This should never happen if our functions work properly\n            if initial_size > file_size:\n                raise RuntimeError('Local file (%s) is larger than remote '\n                                   'file (%s), cannot resume download'\n                                   % (sizeof_fmt(initial_size),\n                                      sizeof_fmt(file_size)))\n\n            scheme = urllib.parse.urlparse(url).scheme\n            fun = _get_http if scheme in ('http', 'https') else _get_ftp\n            fun(url, temp_file_name, initial_size, file_size, verbose,\n                progressbar, ncols=80)\n\n            # check md5sum\n            if hash_ is not None:\n                if verbose:\n                    tqdm.write('Verifying download hash.')\n                md5 = md5sum(temp_file_name)\n                if hash_ != md5:\n                    raise RuntimeError('Hash mismatch for downloaded file %s, '\n                                       'expected %s but got %s'\n                                       % (temp_file_name, hash_, md5))\n        shutil.move(temp_file_name, file_name)\n    except Exception as ee:\n        raise RuntimeError('Error while fetching file %s.'\n                           ' Dataset fetching aborted.\\nError: %s' % (url, ee))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the md5sum for a file.", "response": "def md5sum(fname, block_size=1048576):  # 2 ** 20\n    \"\"\"Calculate the md5sum for a file.\n\n    Parameters\n    ----------\n    fname : str\n        Filename.\n    block_size : int\n        Block size to use when reading.\n\n    Returns\n    -------\n    hash_ : str\n        The hexadecimal digest of the hash.\n    \"\"\"\n    md5 = hashlib.md5()\n    with open(fname, 'rb') as fid:\n        while True:\n            data = fid.read(block_size)\n            if not data:\n                break\n            md5.update(data)\n    return md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _chunk_write(chunk, local_file, progress):\n    local_file.write(chunk)\n    if progress is not None:\n        progress.update(len(chunk))", "response": "Write a chunk to file and update the progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn number of bytes into human - readable str.", "response": "def sizeof_fmt(num):\n    \"\"\"Turn number of bytes into human-readable str.\n\n    Parameters\n    ----------\n    num : int\n        The number of bytes.\n\n    Returns\n    -------\n    size : str\n        The size in human-readable format.\n    \"\"\"\n    units = ['bytes', 'kB', 'MB', 'GB', 'TB', 'PB']\n    decimals = [0, 0, 1, 2, 2, 2]\n    if num > 1:\n        exponent = min(int(log(num, 1024)), len(units) - 1)\n        quotient = float(num) / 1024 ** exponent\n        unit = units[exponent]\n        num_decimals = decimals[exponent]\n        format_string = '{0:.%sf} {1}' % (num_decimals)\n        return format_string.format(quotient, unit)\n    if num == 0:\n        return '0 bytes'\n    if num == 1:\n        return '1 byte'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retry(f, exc_classes=DEFAULT_EXC_CLASSES, logger=None,\n          retry_log_level=logging.INFO,\n          retry_log_message=\"Connection broken in '{f}' (error: '{e}'); \"\n                            \"retrying with new connection.\",\n          max_failures=None, interval=0,\n          max_failure_log_level=logging.ERROR,\n          max_failure_log_message=\"Max retries reached for '{f}'. Aborting.\"):\n    \"\"\"\n    Decorator to automatically reexecute a function if the connection is\n    broken for any reason.\n    \"\"\"\n    exc_classes = tuple(exc_classes)\n\n    @wraps(f)\n    def deco(*args, **kwargs):\n        failures = 0\n        while True:\n            try:\n                return f(*args, **kwargs)\n            except exc_classes as e:\n                if logger is not None:\n                    logger.log(retry_log_level,\n                               retry_log_message.format(f=f.func_name, e=e))\n                gevent.sleep(interval)\n                failures += 1\n                if max_failures is not None \\\n                        and failures > max_failures:\n                    if logger is not None:\n                        logger.log(max_failure_log_level,\n                                   max_failure_log_message.format(\n                                       f=f.func_name, e=e))\n                    raise\n    return deco", "response": "Decorator to automatically reexecute a function if the connection is broken."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self):\n        self.lock.acquire()\n        try:\n            c = self.conn.popleft()\n            yield c\n        except self.exc_classes:\n            # The current connection has failed, drop it and create a new one\n            gevent.spawn_later(1, self._addOne)\n            raise\n        except:\n            self.conn.append(c)\n            self.lock.release()\n            raise\n        else:\n            # NOTE: cannot use finally because MUST NOT reuse the connection\n            # if it failed (socket.error)\n            self.conn.append(c)\n            self.lock.release()", "response": "Get a connection from the pool to make and receive traffic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a transformer function for parsing EFF annotations. N. B., fills are a list of integers. N. B., fills are a list of integers representing the amount of fills that are used to fill the EFF.", "response": "def eff_default_transformer(fills=EFF_DEFAULT_FILLS):\n    \"\"\"\n    Return a simple transformer function for parsing EFF annotations. N.B.,\n    ignores all but the first effect.\n\n    \"\"\"\n    def _transformer(vals):\n        if len(vals) == 0:\n            return fills\n        else:\n            # ignore all but first effect\n            match_eff_main = _prog_eff_main.match(vals[0])\n            if match_eff_main is None:\n                logging.warning(\n                    'match_eff_main is None: vals={}'.format(str(vals[0]))\n                )\n                return fills\n            eff = [match_eff_main.group(1)] \\\n                + match_eff_main.group(2).split(b'|')\n            result = tuple(\n                fill if v == b''\n                else int(v) if i == 5 or i == 10\n                else (1 if v == b'CODING' else 0) if i == 8\n                else v\n                for i, (v, fill) in enumerate(list(zip(eff, fills))[:11])\n            )\n            return result\n    return _transformer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a transformer function for parsing ANN annotations. N. B., fills can be either ANN_DEFAULT_FILLS or ANN_DEFAULT_FILLS.", "response": "def ann_default_transformer(fills=ANN_DEFAULT_FILLS):\n    \"\"\"\n    Return a simple transformer function for parsing ANN annotations. N.B.,\n    ignores all but the first effect.\n\n    \"\"\"\n    def _transformer(vals):\n        if len(vals) == 0:\n            return fills\n        else:\n            # ignore all but first effect\n            ann = vals[0].split(b'|')\n            ann = ann[:11] + _ann_split2(ann[11]) + _ann_split2(ann[12]) + \\\n                _ann_split2(ann[13]) + ann[14:]\n            result = tuple(\n                fill if v == b''\n                else int(v.partition(b'/')[0]) if i == 8\n                else int(v) if 11 <= i < 18\n                else v\n                for i, (v, fill) in enumerate(list(zip(ann, fills))[:18])\n            )\n            return result\n    return _transformer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef overload(func):\n    if sys.version_info < (3, 3):\n        raise OverloadingError(\"The 'overload' syntax requires Python version 3.3 or higher.\")\n    fn = unwrap(func)\n    ensure_function(fn)\n    fname = get_full_name(fn)\n    if fname.find('<locals>') >= 0:\n        raise OverloadingError(\"The 'overload' syntax cannot be used with nested functions. \"\n                               \"Decorators must use functools.wraps().\")\n    try:\n        return register(__registry[fname], func)\n    except KeyError:\n        __registry[fname] = overloaded(func)\n        return __registry[fname]", "response": "A decorator that can be used to register a function as an overloaded function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overloaded(func):\n    fn = unwrap(func)\n    ensure_function(fn)\n\n    def dispatcher(*args, **kwargs):\n\n        resolved = None\n        if dispatcher.__complex_parameters:\n            cache_key_pos = []\n            cache_key_kw = []\n            for argset in (0, 1) if kwargs else (0,):\n                if argset == 0:\n                    arg_pairs = enumerate(args)\n                    complexity_mapping = dispatcher.__complex_positions\n                else:\n                    arg_pairs = kwargs.items()\n                    complexity_mapping = dispatcher.__complex_parameters\n                for id, arg in arg_pairs:\n                    type_ = type(arg)\n                    element_type = None\n                    if id in complexity_mapping:\n                        try:\n                            element = next(iter(arg))\n                        except TypeError:\n                            pass\n                        except StopIteration:\n                            element_type = _empty\n                        else:\n                            complexity = complexity_mapping[id]\n                            if complexity & 8 and isinstance(arg, tuple):\n                                element_type = tuple(type(el) for el in arg)\n                            elif complexity & 4 and hasattr(arg, 'keys'):\n                                element_type = (type(element), type(arg[element]))\n                            else:\n                                element_type = type(element)\n                    if argset == 0:\n                        cache_key_pos.append((type_, element_type))\n                    else:\n                        cache_key_kw.append((id, type_, element_type))\n        else:\n            cache_key_pos = (type(arg) for arg in args)\n            cache_key_kw = ((name, type(arg)) for (name, arg) in kwargs.items()) if kwargs else None\n\n        cache_key = (tuple(cache_key_pos),\n                     tuple(sorted(cache_key_kw)) if kwargs else None)\n\n        try:\n            resolved = dispatcher.__cache[cache_key]\n        except KeyError:\n            resolved = find(dispatcher, args, kwargs)\n            if resolved:\n                dispatcher.__cache[cache_key] = resolved\n        if resolved:\n            before = dispatcher.__hooks['before']\n            after = dispatcher.__hooks['after']\n            if before:\n                before(*args, **kwargs)\n            result = resolved(*args, **kwargs)\n            if after:\n                after(*args, **kwargs)\n            return result\n        else:\n            return error(dispatcher.__name__)\n\n    dispatcher.__dict__.update(\n        __functions = [],\n        __hooks = {'before': None, 'after': None},\n        __cache = {},\n        __complex_positions = {},\n        __complex_parameters = {},\n        __maxlen = 0,\n    )\n    for attr in ('__module__', '__name__', '__qualname__', '__doc__'):\n        setattr(dispatcher, attr, getattr(fn, attr, None))\n    if is_void(fn):\n        update_docstring(dispatcher, fn)\n        return dispatcher\n    else:\n        update_docstring(dispatcher)\n        return register(dispatcher, func)", "response": "Introduces a new overloaded function and registers its first implementation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(dispatcher, func, *, hook=None):\n    wrapper = None\n    if isinstance(func, (classmethod, staticmethod)):\n        wrapper = type(func)\n        func = func.__func__\n    ensure_function(func)\n    if isinstance(dispatcher, (classmethod, staticmethod)):\n        wrapper = None\n    dp = unwrap(dispatcher)\n    try:\n        dp.__functions\n    except AttributeError:\n        raise OverloadingError(\"%r has not been set up as an overloaded function.\" % dispatcher)\n    fn = unwrap(func)\n    if hook:\n        dp.__hooks[hook] = func\n    else:\n        signature = get_signature(fn)\n        for i, type_ in enumerate(signature.types):\n            if not isinstance(type_, type):\n                raise OverloadingError(\n                  \"Failed to overload function '{0}': parameter '{1}' has \"\n                  \"an annotation that is not a type.\"\n                  .format(dp.__name__, signature.parameters[i]))\n        for fninfo in dp.__functions:\n            dup_sig = sig_cmp(signature, fninfo.signature)\n            if dup_sig and signature.has_varargs == fninfo.signature.has_varargs:\n                raise OverloadingError(\n                  \"Failed to overload function '{0}': non-unique signature ({1}).\"\n                  .format(dp.__name__, str.join(', ', (_repr(t) for t in dup_sig))))\n        # All clear; register the function.\n        dp.__functions.append(FunctionInfo(func, signature))\n        dp.__cache.clear()\n        dp.__maxlen = max(dp.__maxlen, len(signature.parameters))\n        if typing:\n            # For each parameter position and name, compute a bitwise union of complexity\n            # values over all registered signatures. Retain the result for parameters where\n            # a nonzero value occurs at least twice and at least one of those values is >= 2.\n            # Such parameters require deep type-checking during function resolution.\n            position_values = defaultdict(lambda: 0)\n            keyword_values = defaultdict(lambda: 0)\n            position_counter = Counter()\n            keyword_counter = Counter()\n            for fninfo in dp.__functions:\n                sig = fninfo.signature\n                complex_positions = {i: v for i, v in enumerate(sig.complexity) if v}\n                complex_keywords = {p: v for p, v in zip(sig.parameters, sig.complexity) if v}\n                for i, v in complex_positions.items():\n                    position_values[i] |= v\n                for p, v in complex_keywords.items():\n                    keyword_values[p] |= v\n                position_counter.update(complex_positions.keys())\n                keyword_counter.update(complex_keywords.keys())\n            dp.__complex_positions = {\n                i: v for i, v in position_values.items() if v >= 2 and position_counter[i] > 1}\n            dp.__complex_parameters = {\n                p: v for p, v in keyword_values.items() if v >= 2 and keyword_counter[p] > 1}\n    if wrapper is None:\n        wrapper = lambda x: x\n    if func.__name__ == dp.__name__:\n        # The returned function is going to be bound to the invocation name\n        # in the calling scope, so keep returning the dispatcher.\n        return wrapper(dispatcher)\n    else:\n        return wrapper(func)", "response": "Registers a function as an implementation on dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a signature for the given function.", "response": "def get_signature(func):\n    \"\"\"\n    Gathers information about the call signature of `func`.\n    \"\"\"\n    code = func.__code__\n\n    # Names of regular parameters\n    parameters = tuple(code.co_varnames[:code.co_argcount])\n\n    # Flags\n    has_varargs = bool(code.co_flags & inspect.CO_VARARGS)\n    has_varkw = bool(code.co_flags & inspect.CO_VARKEYWORDS)\n    has_kwonly = bool(code.co_kwonlyargcount)\n\n    # A mapping of parameter names to default values\n    default_values = func.__defaults__ or ()\n    defaults = dict(zip(parameters[-len(default_values):], default_values))\n\n    # Type annotations for all parameters\n    type_hints = typing.get_type_hints(func) if typing else func.__annotations__\n    types = tuple(normalize_type(type_hints.get(param, AnyType)) for param in parameters)\n\n    # Type annotations for required parameters\n    required = types[:-len(defaults)] if defaults else types\n\n    # Complexity\n    complexity = tuple(map(type_complexity, types)) if typing else None\n\n    return Signature(parameters, types, complexity, defaults, required,\n                     has_varargs, has_varkw, has_kwonly)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize an arbitrarily complex type declaration into something manageable.", "response": "def normalize_type(type_, level=0):\n    \"\"\"\n    Reduces an arbitrarily complex type declaration into something manageable.\n    \"\"\"\n    if not typing or not isinstance(type_, typing.TypingMeta) or type_ is AnyType:\n        return type_\n    if isinstance(type_, typing.TypeVar):\n        if type_.__constraints__ or type_.__bound__:\n            return type_\n        else:\n            return AnyType\n    if issubclass(type_, typing.Union):\n        if not type_.__union_params__:\n            raise OverloadingError(\"typing.Union must be parameterized\")\n        return typing.Union[tuple(normalize_type(t, level) for t in type_.__union_params__)]\n    if issubclass(type_, typing.Tuple):\n        params = type_.__tuple_params__\n        if level > 0 or params is None:\n            return typing.Tuple\n        elif type_.__tuple_use_ellipsis__:\n            return typing.Tuple[normalize_type(params[0], level + 1), ...]\n        else:\n            return typing.Tuple[tuple(normalize_type(t, level + 1) for t in params)]\n    if issubclass(type_, typing.Callable):\n        return typing.Callable\n    if isinstance(type_, typing.GenericMeta):\n        base = find_base_generic(type_)\n        if base is typing.Generic:\n            return type_\n        else:\n            return GenericWrapper(type_, base, level > 0)\n    raise OverloadingError(\"%r not supported yet\" % type_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute an indicator for the complexity of type_.", "response": "def type_complexity(type_):\n    \"\"\"Computes an indicator for the complexity of `type_`.\n\n    If the return value is 0, the supplied type is not parameterizable.\n    Otherwise, set bits in the return value denote the following features:\n    - bit 0: The type could be parameterized but is not.\n    - bit 1: The type represents an iterable container with 1 constrained type parameter.\n    - bit 2: The type represents a mapping with a constrained value type (2 parameters).\n    - bit 3: The type represents an n-tuple (n parameters).\n    Since these features are mutually exclusive, only a `Union` can have more than one bit set.\n    \"\"\"\n    if (not typing\n      or not isinstance(type_, (typing.TypingMeta, GenericWrapperMeta))\n      or type_ is AnyType):\n        return 0\n    if issubclass(type_, typing.Union):\n        return reduce(operator.or_, map(type_complexity, type_.__union_params__))\n    if issubclass(type_, typing.Tuple):\n        if type_.__tuple_params__ is None:\n            return 1\n        elif type_.__tuple_use_ellipsis__:\n            return 2\n        else:\n            return 8\n    if isinstance(type_, GenericWrapperMeta):\n        type_count = 0\n        for p in reversed(type_.parameters):\n            if type_count > 0:\n                type_count += 1\n            if p is AnyType:\n                continue\n            if not isinstance(p, typing.TypeVar) or p.__constraints__ or p.__bound__:\n                type_count += 1\n        return 1 << min(type_count, 2)\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlocating the underlying generic whose structure and behavior are known.", "response": "def find_base_generic(type_):\n    \"\"\"Locates the underlying generic whose structure and behavior are known.\n\n    For example, the base generic of a type that inherits from `typing.Mapping[T, int]`\n    is `typing.Mapping`.\n    \"\"\"\n    for t in type_.__mro__:\n        if t.__module__ == typing.__name__:\n            return first_origin(t)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_generic_bases(type_):\n    for t in type_.__mro__:\n        if not isinstance(t, typing.GenericMeta):\n            continue\n        yield t\n        t = t.__origin__\n        while t:\n            yield t\n            t = t.__origin__", "response": "Iterates over all generics of type_ derives from including origins."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sig_cmp(sig1, sig2):\n    types1 = sig1.required\n    types2 = sig2.required\n    if len(types1) != len(types2):\n        return False\n    dup_pos = []\n    dup_kw = {}\n    for t1, t2 in zip(types1, types2):\n        match = type_cmp(t1, t2)\n        if match:\n            dup_pos.append(match)\n        else:\n            break\n    else:\n        return tuple(dup_pos)\n    kw_range = slice(len(dup_pos), len(types1))\n    kwds1 = sig1.parameters[kw_range]\n    kwds2 = sig2.parameters[kw_range]\n    if set(kwds1) != set(kwds2):\n        return False\n    kwtypes1 = dict(zip(sig1.parameters, types1))\n    kwtypes2 = dict(zip(sig2.parameters, types2))\n    for kw in kwds1:\n        match = type_cmp(kwtypes1[kw], kwtypes2[kw])\n        if match:\n            dup_kw[kw] = match\n        else:\n            break\n    else:\n        return tuple(dup_pos), dup_kw\n    return False", "response": "Compares two normalized type signatures for validation purposes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_void(func):\n    try:\n        source = dedent(inspect.getsource(func))\n    except (OSError, IOError):\n        return False\n    fdef = next(ast.iter_child_nodes(ast.parse(source)))\n    return (\n      type(fdef) is ast.FunctionDef and len(fdef.body) == 1 and\n      type(fdef.body[0]) is ast.Expr and\n      type(fdef.body[0].value) in {ast.Str, ast.Ellipsis})", "response": "Determines if a function is a void function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the dispatcher s docstring with the signature taken from func.", "response": "def update_docstring(dispatcher, func=None):\n    \"\"\"\n    Inserts a call signature at the beginning of the docstring on `dispatcher`.\n    The signature is taken from `func` if provided; otherwise `(...)` is used.\n    \"\"\"\n    doc = dispatcher.__doc__ or ''\n    if inspect.cleandoc(doc).startswith('%s(' % dispatcher.__name__):\n        return\n    sig = '(...)'\n    if func and func.__code__.co_argcount:\n        argspec = inspect.getfullargspec(func) # pylint: disable=deprecated-method\n        if argspec.args and argspec.args[0] in {'self', 'cls'}:\n            argspec.args.pop(0)\n        if any(argspec):\n            sig = inspect.formatargspec(*argspec) # pylint: disable=deprecated-method\n            sig = re.sub(r' at 0x[0-9a-f]{8,16}(?=>)', '', sig)\n    sep = '\\n' if doc.startswith('\\n') else '\\n\\n'\n    dispatcher.__doc__ = dispatcher.__name__ + sig + sep + doc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derive_configuration(cls):\n        base_params = cls.base.__parameters__\n        if hasattr(cls.type, '__args__'):\n            # typing as of commit abefbe4\n            tvars = {p: p for p in base_params}\n            types = {}\n            for t in iter_generic_bases(cls.type):\n                if t is cls.base:\n                    type_vars = tuple(tvars[p] for p in base_params)\n                    parameters = (types.get(tvar, tvar) for tvar in type_vars)\n                    break\n                if t.__args__:\n                    for arg, tvar in zip(t.__args__, t.__origin__.__parameters__):\n                        if isinstance(arg, typing.TypeVar):\n                            tvars[tvar] = tvars.get(arg, arg)\n                        else:\n                            types[tvar] = arg\n        else:\n            # typing 3.5.0\n            tvars = [None] * len(base_params)\n            for t in iter_generic_bases(cls.type):\n                for i, p in enumerate(t.__parameters__):\n                    if tvars[i] is None and isinstance(p, typing.TypeVar):\n                        tvars[i] = p\n                if all(tvars):\n                    type_vars = tvars\n                    parameters = cls.type.__parameters__\n                    break\n        cls.type_vars = type_vars\n        cls.parameters = tuple(normalize_type(p, 1) for p in parameters)", "response": "Derive the configuration from the type and its bases."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nfa_intersection(nfa_1: dict, nfa_2: dict) -> dict:\n    intersection = {\n        'alphabet': nfa_1['alphabet'].intersection(nfa_2['alphabet']),\n        'states': set(),\n        'initial_states': set(),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n    for init_1 in nfa_1['initial_states']:\n        for init_2 in nfa_2['initial_states']:\n            intersection['initial_states'].add((init_1, init_2))\n\n    intersection['states'].update(intersection['initial_states'])\n\n    boundary = set()\n    boundary.update(intersection['initial_states'])\n    while boundary:\n        (state_nfa_1, state_nfa_2) = boundary.pop()\n        if state_nfa_1 in nfa_1['accepting_states'] \\\n                and state_nfa_2 in nfa_2['accepting_states']:\n            intersection['accepting_states'].add((state_nfa_1, state_nfa_2))\n        for a in intersection['alphabet']:\n            if (state_nfa_1, a) not in nfa_1['transitions'] \\\n                    or (state_nfa_2, a) not in nfa_2['transitions']:\n                continue\n            s1 = nfa_1['transitions'][state_nfa_1, a]\n            s2 = nfa_2['transitions'][state_nfa_2, a]\n\n            for destination_1 in s1:\n                for destination_2 in s2:\n                    next_state = (destination_1, destination_2)\n                    if next_state not in intersection['states']:\n                        intersection['states'].add(next_state)\n                        boundary.add(next_state)\n                    intersection['transitions'].setdefault(\n                        ((state_nfa_1, state_nfa_2), a), set()).add(next_state)\n                    if destination_1 in nfa_1['accepting_states'] \\\n                            and destination_2 in nfa_2['accepting_states']:\n                        intersection['accepting_states'].add(next_state)\n\n    return intersection", "response": "Returns a NFA that reads the intersection of the input NFAs in nfa_1 and nfa_2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nfa_union(nfa_1: dict, nfa_2: dict) -> dict:\n    union = {\n        'alphabet': nfa_1['alphabet'].union(nfa_2['alphabet']),\n        'states': nfa_1['states'].union(nfa_2['states']),\n        'initial_states':\n            nfa_1['initial_states'].union(nfa_2['initial_states']),\n        'accepting_states':\n            nfa_1['accepting_states'].union(nfa_2['accepting_states']),\n        'transitions': nfa_1['transitions'].copy()}\n\n    for trans in nfa_2['transitions']:\n        for elem in nfa_2['transitions'][trans]:\n            union['transitions'].setdefault(trans, set()).add(elem)\n\n    return union", "response": "Returns a NFA that reads the union of the NFAs in input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a DFA that reads the same language of the input NFA and returns the DFA that reads the same language of the input NFA.", "response": "def nfa_determinization(nfa: dict) -> dict:\n    \"\"\" Returns a DFA that reads the same language of the input NFA.\n\n    Let A be an NFA, then there exists a DFA :math:`A_d` such\n    that :math:`L(A_d) = L(A)`. Intuitively, :math:`A_d`\n    collapses all possible runs of A on a given input word into\n    one run over a larger state set.\n    :math:`A_d` is defined as:\n\n    :math:`A_d = (\u03a3, 2^S , s_0 , \u03c1_d , F_d )`\n\n    where:\n\n    \u2022 :math:`2^S` , i.e., the state set of :math:`A_d` , consists\n      of all sets of states S in A;\n    \u2022 :math:`s_0 = S^0` , i.e., the single initial state of\n      :math:`A_d` is the set :math:`S_0` of initial states of A;\n    \u2022 :math:`F_d = \\{Q | Q \u2229 F \u2260 \u2205\\}`, i.e., the collection of\n      sets of states that intersect F nontrivially;\n    \u2022 :math:`\u03c1_d(Q, a) = \\{s' | (s,a, s' ) \u2208 \u03c1\\ for\\ some\\ s \u2208 Q\\}`.\n\n    :param dict nfa: input NFA.\n    :return: *(dict)* representing a DFA\n    \"\"\"\n    def state_name(s):\n        return str(set(sorted(s)))\n\n    dfa = {\n        'alphabet': nfa['alphabet'].copy(),\n        'initial_state': None,\n        'states': set(),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n\n    if len(nfa['initial_states']) > 0:\n        dfa['initial_state'] = state_name(nfa['initial_states'])\n        dfa['states'].add(state_name(nfa['initial_states']))\n\n    sets_states = list()\n    sets_queue = list()\n    sets_queue.append(nfa['initial_states'])\n    sets_states.append(nfa['initial_states'])\n    if len(sets_states[0].intersection(nfa['accepting_states'])) > 0:\n        dfa['accepting_states'].add(state_name(sets_states[0]))\n\n    while sets_queue:\n        current_set = sets_queue.pop(0)\n        for a in dfa['alphabet']:\n            next_set = set()\n            for state in current_set:\n                if (state, a) in nfa['transitions']:\n                    for next_state in nfa['transitions'][state, a]:\n                        next_set.add(next_state)\n            if len(next_set) == 0:\n                continue\n            if next_set not in sets_states:\n                sets_states.append(next_set)\n                sets_queue.append(next_set)\n                dfa['states'].add(state_name(next_set))\n                if next_set.intersection(nfa['accepting_states']):\n                    dfa['accepting_states'].add(state_name(next_set))\n\n            dfa['transitions'][state_name(current_set), a] = state_name(next_set)\n\n    return dfa"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nfa_complementation(nfa: dict) -> dict:\n    determinized_nfa = nfa_determinization(nfa)\n    return DFA.dfa_complementation(determinized_nfa)", "response": "Returns a DFA reading the complemented language read by\n    input NFA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the input NFA reads any language other than the empty one returning True or False.", "response": "def nfa_nonemptiness_check(nfa: dict) -> bool:\n    \"\"\" Checks if the input NFA reads any language other than the\n    empty one, returning True/False.\n\n    The language L(A) recognized by the automaton A is nonempty iff\n    there are states :math:`s \u2208 S_0` and :math:`t \u2208 F` such that\n    t is connected to s.\n    Thus, automata nonemptiness is equivalent to graph reachability.\n\n    A breadth-first-search algorithm can construct in linear time\n    the set of all states connected to a state in :math:`S_0`. A\n    is nonempty iff this set intersects F nontrivially.\n\n    :param dict nfa: input NFA.\n    :return: *(bool)*, True if the input nfa is nonempty, False\n             otherwise.\n    \"\"\"\n    # BFS\n    queue = list()\n    visited = set()\n    for state in nfa['initial_states']:\n        visited.add(state)\n        queue.append(state)\n    while queue:\n        state = queue.pop(0)\n        visited.add(state)\n        for a in nfa['alphabet']:\n            if (state, a) in nfa['transitions']:\n                for next_state in nfa['transitions'][state, a]:\n                    if next_state in nfa['accepting_states']:\n                        return True\n                    if next_state not in visited:\n                        queue.append(next_state)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the language read by the input NFA is different from \u03a3\u2217.", "response": "def nfa_nonuniversality_check(nfa: dict) -> bool:\n    \"\"\" Checks if the language read by the input NFA is different\n    from \u03a3\u2217 (i.e. contains all possible words), returning\n    True/False.\n\n    To test nfa A for nonuniversality, it suffices to test \u0100 (\n    complementary automaton of A) for nonemptiness\n\n    :param dict nfa: input NFA.\n    :return: *(bool)*, True if input nfa is nonuniversal,\n             False otherwise.\n    \"\"\"\n    # NAIVE Very inefficient (exponential space) : simply\n    # construct \u0100 and then test its nonemptiness\n    complemented_nfa = nfa_complementation(nfa)\n    return DFA.dfa_nonemptiness_check(complemented_nfa)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nfa_word_acceptance(nfa: dict, word: list) -> bool:\n    current_level = set()\n    current_level = current_level.union(nfa['initial_states'])\n    next_level = set()\n    for action in word:\n        for state in current_level:\n            if (state, action) in nfa['transitions']:\n                next_level.update(nfa['transitions'][state, action])\n        if len(next_level) < 1:\n            return False\n        current_level = next_level\n        next_level = set()\n\n    if current_level.intersection(nfa['accepting_states']):\n        return True\n    else:\n        return False", "response": "Checks if a given word is accepted by a NFA."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsides effect on input! Renames all the states of the NFA with a given suffix.", "response": "def rename_nfa_states(nfa: dict, suffix: str):\n    \"\"\" Side effect on input! Renames all the states of the NFA\n    adding a **suffix**.\n\n    It is an utility function to be used to avoid automata to have\n    states with names in common.\n\n    Avoid suffix that can lead to special name like \"as\", \"and\",...\n\n    :param dict nfa: input NFA.\n    :param str suffix: string to be added at beginning of each state name.\n    \"\"\"\n    conversion_dict = {}\n    new_states = set()\n    new_initials = set()\n    new_accepting = set()\n    for state in nfa['states']:\n        conversion_dict[state] = '' + suffix + state\n        new_states.add('' + suffix + state)\n        if state in nfa['initial_states']:\n            new_initials.add('' + suffix + state)\n        if state in nfa['accepting_states']:\n            new_accepting.add('' + suffix + state)\n\n    nfa['states'] = new_states\n    nfa['initial_states'] = new_initials\n    nfa['accepting_states'] = new_accepting\n\n    new_transitions = {}\n    for transition in nfa['transitions']:\n        new_arrival = set()\n        for arrival in nfa['transitions'][transition]:\n            new_arrival.add(conversion_dict[arrival])\n        new_transitions[\n            conversion_dict[transition[0]], transition[1]] = new_arrival\n    nfa['transitions'] = new_transitions\n    return nfa"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef overwrite_view_source(project, dir_path):\n\n    project_html_location = dir_path / project / HTML_LOCATION\n    if not project_html_location.exists():\n        return\n\n    files_to_overwrite = [\n        f for f in project_html_location.iterdir() if \"html\" in f.suffix\n    ]\n\n    for html_file in files_to_overwrite:\n        with open(html_file, \"r\") as f:\n            html = f.readlines()\n        for i, l in enumerate(html):\n            if TO_REPLACE_WITH_HOME in l:\n                html[i] = NEW_HOME_LINK\n                break\n        with open(html_file, \"w\") as f:\n            f.writelines(html)", "response": "In the project s index. html built file replace the top source link with a link to mkdoc s home"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_listed_projects():\n    index_path = Path().resolve() / \"docs\" / \"index.md\"\n    with open(index_path, \"r\") as index_file:\n        lines = index_file.readlines()\n\n    listed_projects = set()\n    project_section = False\n    for _, l in enumerate(lines):\n        idx = l.find(PROJECT_KEY)\n        if idx >= 0:\n            project_section = True\n        if project_section:\n            # Find first parenthesis after the key\n            start = l.find(\"](\")\n            if start > 0:\n                closing_parenthesis = sorted(\n                    [m.start() for m in re.finditer(r\"\\)\", l) if m.start() > start]\n                )[0]\n                project = l[start + 2 : closing_parenthesis]\n                listed_projects.add(project)\n        # If the Projects section is over, stop iteration.\n        # It will stop before seeing ## but wainting for it\n        # Allows the user to use single # in the projects' descriptions\n        if len(listed_projects) > 0 and l.startswith(\"#\"):\n            return listed_projects\n    return listed_projects", "response": "Find the projects listed in the Home Documentation s index. md file\n    Returns a set of projects listed in the Home Documentation s index. md file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_routes():\n    os.system(\"pwd\")\n    dir_path = Path(os.getcwd()).absolute()\n    projects = get_listed_projects()\n    routes = [\n        [p if p[0] == \"/\" else \"/\" + p, str(dir_path) + \"{}/build/html\".format(p)]\n        for p in projects\n    ]\n    os.environ[\"MKINX_ROUTES\"] = json.dumps(routes)", "response": "Set the MKINX_ROUTES environment variable with a serialized list of routes one route being a list of routes one route being a string that represents the path to the project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_offline():\n    dir_path = Path(os.getcwd()).absolute()\n\n    css_path = dir_path / \"site\" / \"assets\" / \"stylesheets\"\n    material_css = css_path / \"material-style.css\"\n    if not material_css.exists():\n        file_path = Path(__file__).resolve().parent\n        copyfile(file_path / \"material-style.css\", material_css)\n        copyfile(file_path / \"material-icons.woff2\", css_path / \"material-icons.woff2\")\n\n    indexes = []\n    for root, _, filenames in os.walk(dir_path / \"site\"):\n        for filename in fnmatch.filter(filenames, \"index.html\"):\n            indexes.append(os.path.join(root, filename))\n    for index_file in indexes:\n        update_index_to_offline(index_file)", "response": "Creates a new empty site node that is not part of the web site."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef variants(vcf_fn, region=None, fields=None, exclude_fields=None,\n             dtypes=None, arities=None, fills=None, transformers=None,\n             vcf_types=None, count=None, progress=0, logstream=None,\n             condition=None, slice_args=None, flatten_filter=False,\n             verbose=True, cache=False, cachedir=None, skip_cached=False,\n             compress_cache=False, truncate=True):\n    \"\"\"\n    Load an numpy structured array with data from the fixed fields of a VCF\n    file (including INFO).\n\n    Parameters\n    ----------\n\n    vcf_fn: string or list\n        Name of the VCF file or list of file names.\n    region: string, optional\n        Region to extract, e.g., 'chr1' or 'chr1:0-100000'.\n    fields: list or array-like, optional\n        List of fields to extract from the VCF.\n    exclude_fields: list or array-like, optional\n        Fields to exclude from extraction.\n    dtypes: dict or dict-like, optional\n        Dictionary cotaining dtypes to use instead of the default inferred\n        ones.\n    arities: dict or dict-like, optional\n        Dictionary containing field:integer mappings used to override the\n        number of values to expect.\n    fills: dict or dict-like, optional\n        Dictionary containing field:fillvalue mappings used to override the\n        defaults used for missing values.\n    transformers: dict or dict-like, optional\n        Dictionary containing field:function mappings used to preprocess\n        any values prior to loading into array.\n    vcf_types: dict or dict-like, optional\n        Dictionary containing field:string mappings used to override any\n        bogus type declarations in the VCF header (e.g., MQ0Fraction declared\n        as Integer).\n    count: int, optional\n        Attempt to extract a specific number of records.\n    progress: int, optional\n        If greater than 0, log progress.\n    logstream: file or file-like object, optional\n        Stream to use for logging progress.\n    condition: array, optional\n        Boolean array defining which rows to load.\n    slice_args: tuple or list, optional\n        Slice of the underlying iterator, e.g., (0, 1000, 10) takes every\n        10th row from the first 1000.\n    flatten_filter: bool, optional\n        Return FILTER as multiple boolean fields, e.g., FILTER_PASS,\n        FILTER_LowQuality, etc.\n    verbose: bool, optional\n        Log more messages.\n    cache: bool, optional\n        If True, save the resulting numpy array to disk, and load from the\n        cache if present rather than rebuilding from the VCF.\n    cachedir: string, optional\n        Manually specify the directory to use to store cache files.\n    skip_cached: bool, optional\n        If True and cache file is fresh, do not load and return None.\n    compress_cache: bool, optional\n        If True, compress the cache file.\n    truncate: bool, optional\n        If True (default) only include variants whose start position is within\n        the given region. If False, use default tabix behaviour.\n\n    Examples\n    --------\n\n    >>> from vcfnp import variants\n    >>> v = variants('fixture/sample.vcf')\n    >>> v\n    array([ (b'19', 111, b'.', b'A', b'C', 9.600000381469727, (False, False, False), 2, True, 0, b'', 0, 0.0, 0, False, 0, False, 0),\n           (b'19', 112, b'.', b'A', b'G', 10.0, (False, False, False), 2, True, 0, b'', 0, 0.0, 0, False, 0, False, 0),\n           (b'20', 14370, b'rs6054257', b'G', b'A', 29.0, (False, False, True), 2, True, 0, b'', 0, 0.5, 0, True, 14, True, 3),\n           (b'20', 17330, b'.', b'T', b'A', 3.0, (True, False, False), 2, True, 0, b'', 0, 0.016998291015625, 0, False, 11, False, 3),\n           (b'20', 1110696, b'rs6040355', b'A', b'G', 67.0, (False, False, True), 3, True, 0, b'T', 0, 0.3330078125, 0, True, 10, False, 2),\n           (b'20', 1230237, b'.', b'T', b'.', 47.0, (False, False, True), 2, False, 0, b'T', 0, 0.0, 0, False, 13, False, 3),\n           (b'20', 1234567, b'microsat1', b'G', b'GA', 50.0, (False, False, True), 3, False, 1, b'G', 3, 0.0, 6, False, 9, False, 3),\n           (b'20', 1235237, b'.', b'T', b'.', 0.0, (False, False, False), 2, False, 0, b'', 0, 0.0, 0, False, 0, False, 0),\n           (b'X', 10, b'rsTest', b'AC', b'A', 10.0, (False, False, True), 3, False, -1, b'', 0, 0.0, 0, False, 0, False, 0)],\n          dtype=[('CHROM', 'S12'), ('POS', '<i4'), ('ID', 'S12'), ('REF', 'S12'), ('ALT', 'S12'), ('QUAL', '<f4'), ('FILTER', [('q10', '?'), ('s50', '?'), ('PASS', '?')]), ('num_alleles', 'u1'), ('is_snp', '?'), ('svlen', '<i4'), ('AA', 'S12'), ('AC', '<u2'), ('AF', '<f2'), ('AN', '<u2'), ('DB', '?'), ('DP', '<i4'), ('H2', '?'), ('NS', '<i4')])\n    >>> v['QUAL']\n    array([  9.60000038,  10.        ,  29.        ,   3.        ,\n            67.        ,  47.        ,  50.        ,   0.        ,  10.        ], dtype=float32)\n    >>> v['FILTER']['PASS']\n    array([False, False,  True, False,  True,  True,  True, False,  True], dtype=bool)\n    >>> v['AF']\n    array([ 0.        ,  0.        ,  0.5       ,  0.01699829,  0.33300781,\n            0.        ,  0.        ,  0.        ,  0.        ], dtype=float16)\n\n    \"\"\"  # flake8: noqa\n\n    loader = _VariantsLoader(vcf_fn, region=region, fields=fields,\n                             exclude_fields=exclude_fields, dtypes=dtypes,\n                             arities=arities, fills=fills,\n                             transformers=transformers, vcf_types=vcf_types,\n                             count=count, progress=progress,\n                             logstream=logstream, condition=condition,\n                             slice_args=slice_args,\n                             flatten_filter=flatten_filter, verbose=verbose,\n                             cache=cache, cachedir=cachedir,\n                             skip_cached=skip_cached,\n                             compress_cache=compress_cache, truncate=truncate)\n    return loader.load()", "response": "Load a numpy structured array of variants from a VCF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _variants_fields(fields, exclude_fields, info_ids):\n    if fields is None:\n        # no fields specified by user\n        # by default extract all standard and INFO fields\n        fields = config.STANDARD_VARIANT_FIELDS + info_ids\n    else:\n        # fields have been specified\n        for f in fields:\n            # check for non-standard fields not declared in INFO header\n            if f not in config.STANDARD_VARIANT_FIELDS and f not in info_ids:\n                # support extracting INFO even if not declared in header,\n                # but warn...\n                print('WARNING: no INFO definition found for field %s' % f,\n                      file=sys.stderr)\n    # process any exclusions\n    if exclude_fields is not None:\n        fields = [f for f in fields if f not in exclude_fields]\n    return tuple(f for f in fields)", "response": "Utility function to determine which fields to extract when loading\n    variants."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _variants_dtype(fields, dtypes, arities, filter_ids, flatten_filter,\n                    info_types):\n    \"\"\"Utility function to build a numpy dtype for a variants array,\n    given user arguments and information available from VCF header.\"\"\"\n    dtype = list()\n    for f, n, vcf_type in zip(fields, arities, info_types):\n        if f == 'FILTER' and flatten_filter:\n            # split FILTER into multiple boolean fields\n            for flt in filter_ids:\n                nm = 'FILTER_' + flt\n                dtype.append((nm, 'b1'))\n        elif f == 'FILTER' and not flatten_filter:\n            # represent FILTER as a structured field\n            t = [(flt, 'b1') for flt in filter_ids]\n            dtype.append((f, t))\n        else:\n            if dtypes is not None and f in dtypes:\n                # user overrides default dtype\n                t = dtypes[f]\n            elif f in config.STANDARD_VARIANT_FIELDS:\n                t = config.DEFAULT_VARIANT_DTYPE[f]\n            elif f in config.DEFAULT_INFO_DTYPE:\n                # known INFO field\n                t = config.DEFAULT_INFO_DTYPE[f]\n            else:\n                t = config.DEFAULT_TYPE_MAP[vcf_type]\n            # deal with arity\n            if n == 1:\n                dtype.append((f, t))\n            else:\n                dtype.append((f, t, (n,)))\n    return dtype", "response": "Utility function to build a numpy dtype for a variants array given user arguments and information available from VCF header."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iter_withprogress(iterable, progress, log):\n    before_all = time.time()\n    before = before_all\n    n = 0\n    for i, o in enumerate(iterable):\n        yield o\n        n = i+1\n        if n % progress == 0:\n            after = time.time()\n            log('%s rows in %.2fs; batch in %.2fs (%d rows/s)'\n                % (n, after-before_all, after-before, progress/(after-before)))\n            before = after\n    after_all = time.time()\n    log('%s rows in %.2fs (%d rows/s)'\n        % (n, after_all-before_all, n/(after_all-before_all)))", "response": "Utility function to load an array from an iterator reporting progress as we go."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a numpy array with data from a sample VCF file.", "response": "def calldata(vcf_fn, region=None, samples=None, ploidy=2, fields=None,\n             exclude_fields=None, dtypes=None, arities=None, fills=None,\n             vcf_types=None, count=None, progress=0, logstream=None,\n             condition=None, slice_args=None, verbose=True, cache=False,\n             cachedir=None, skip_cached=False, compress_cache=False,\n             truncate=True):\n    \"\"\"\n    Load a numpy 1-dimensional structured array with data from the sample\n    columns of a VCF file.\n\n    Parameters\n    ----------\n\n    vcf_fn: string or list\n        Name of the VCF file or list of file names.\n    region: string\n        Region to extract, e.g., 'chr1' or 'chr1:0-100000'.\n    fields: list or array-like\n        List of fields to extract from the VCF.\n    exclude_fields: list or array-like\n        Fields to exclude from extraction.\n    dtypes: dict or dict-like\n        Dictionary cotaining dtypes to use instead of the default inferred ones\n    arities: dict or dict-like\n        Override the amount of values to expect.\n    fills: dict or dict-like\n        Dictionary containing field:fillvalue mappings used to override the\n        default fill in values in VCF fields.\n    vcf_types: dict or dict-like\n        Dictionary containing field:string mappings used to override any\n        bogus type declarations in the VCF header.\n    count: int\n        Attempt to extract a specific number of records.\n    progress: int\n        If greater than 0, log parsing progress.\n    logstream: file or file-like object\n        Stream to use for logging progress.\n    condition: array\n        Boolean array defining which rows to load.\n    slice_args: tuple or list\n        Slice of the underlying iterator, e.g., (0, 1000, 10) takes every\n        10th row from the first 1000.\n    verbose: bool\n        Log more messages.\n    cache: bool\n        If True, save the resulting numpy array to disk, and load from the\n        cache if present rather than rebuilding from the VCF.\n    cachedir: string\n        Manually specify the directory to use to store cache files.\n    skip_cached: bool\n        If True and cache file is fresh, do not load and return None.\n    compress_cache: bool, optional\n        If True, compress the cache file.\n    truncate: bool, optional\n        If True (default) only include variants whose start position is within\n        the given region. If False, use default tabix behaviour.\n\n    Examples\n    --------\n\n    >>> from vcfnp import calldata, view2d\n    >>> c = calldata('fixture/sample.vcf')\n    >>> c\n    array([ ((True, True, [0, 0], 0, 0, b'0|0', [10, 10]), (True, True, [0, 0], 0, 0, b'0|0', [10, 10]), (True, False, [0, 1], 0, 0, b'0/1', [3, 3])),\n           ((True, True, [0, 0], 0, 0, b'0|0', [10, 10]), (True, True, [0, 0], 0, 0, b'0|0', [10, 10]), (True, False, [0, 1], 0, 0, b'0/1', [3, 3])),\n           ((True, True, [0, 0], 1, 48, b'0|0', [51, 51]), (True, True, [1, 0], 8, 48, b'1|0', [51, 51]), (True, False, [1, 1], 5, 43, b'1/1', [0, 0])),\n           ((True, True, [0, 0], 3, 49, b'0|0', [58, 50]), (True, True, [0, 1], 5, 3, b'0|1', [65, 3]), (True, False, [0, 0], 3, 41, b'0/0', [0, 0])),\n           ((True, True, [1, 2], 6, 21, b'1|2', [23, 27]), (True, True, [2, 1], 0, 2, b'2|1', [18, 2]), (True, False, [2, 2], 4, 35, b'2/2', [0, 0])),\n           ((True, True, [0, 0], 0, 54, b'0|0', [56, 60]), (True, True, [0, 0], 4, 48, b'0|0', [51, 51]), (True, False, [0, 0], 2, 61, b'0/0', [0, 0])),\n           ((True, False, [0, 1], 4, 0, b'0/1', [0, 0]), (True, False, [0, 2], 2, 17, b'0/2', [0, 0]), (False, False, [-1, -1], 3, 40, b'./.', [0, 0])),\n           ((True, False, [0, 0], 0, 0, b'0/0', [0, 0]), (True, True, [0, 0], 0, 0, b'0|0', [0, 0]), (False, False, [-1, -1], 0, 0, b'./.', [0, 0])),\n           ((True, False, [0, -1], 0, 0, b'0', [0, 0]), (True, False, [0, 1], 0, 0, b'0/1', [0, 0]), (True, True, [0, 2], 0, 0, b'0|2', [0, 0]))],\n          dtype=[('NA00001', [('is_called', '?'), ('is_phased', '?'), ('genotype', 'i1', (2,)), ('DP', '<u2'), ('GQ', 'u1'), ('GT', 'S3'), ('HQ', '<i4', (2,))]), ('NA00002', [('is_called', '?'), ('is_phased', '?'), ('genotype', 'i1', (2,)), ('DP', '<u2'), ('GQ', 'u1'), ('GT', 'S3'), ('HQ', '<i4', (2,))]), ('NA00003', [('is_called', '?'), ('is_phased', '?'), ('genotype', 'i1', (2,)), ('DP', '<u2'), ('GQ', 'u1'), ('GT', 'S3'), ('HQ', '<i4', (2,))])])\n    >>> c['NA00001']\n    array([(True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n           (True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n           (True, True, [0, 0], 1, 48, b'0|0', [51, 51]),\n           (True, True, [0, 0], 3, 49, b'0|0', [58, 50]),\n           (True, True, [1, 2], 6, 21, b'1|2', [23, 27]),\n           (True, True, [0, 0], 0, 54, b'0|0', [56, 60]),\n           (True, False, [0, 1], 4, 0, b'0/1', [0, 0]),\n           (True, False, [0, 0], 0, 0, b'0/0', [0, 0]),\n           (True, False, [0, -1], 0, 0, b'0', [0, 0])],\n          dtype=[('is_called', '?'), ('is_phased', '?'), ('genotype', 'i1', (2,)), ('DP', '<u2'), ('GQ', 'u1'), ('GT', 'S3'), ('HQ', '<i4', (2,))])\n    >>> c2d = view2d(c)\n    >>> c2d\n    array([[(True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n            (True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n            (True, False, [0, 1], 0, 0, b'0/1', [3, 3])],\n           [(True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n            (True, True, [0, 0], 0, 0, b'0|0', [10, 10]),\n            (True, False, [0, 1], 0, 0, b'0/1', [3, 3])],\n           [(True, True, [0, 0], 1, 48, b'0|0', [51, 51]),\n            (True, True, [1, 0], 8, 48, b'1|0', [51, 51]),\n            (True, False, [1, 1], 5, 43, b'1/1', [0, 0])],\n           [(True, True, [0, 0], 3, 49, b'0|0', [58, 50]),\n            (True, True, [0, 1], 5, 3, b'0|1', [65, 3]),\n            (True, False, [0, 0], 3, 41, b'0/0', [0, 0])],\n           [(True, True, [1, 2], 6, 21, b'1|2', [23, 27]),\n            (True, True, [2, 1], 0, 2, b'2|1', [18, 2]),\n            (True, False, [2, 2], 4, 35, b'2/2', [0, 0])],\n           [(True, True, [0, 0], 0, 54, b'0|0', [56, 60]),\n            (True, True, [0, 0], 4, 48, b'0|0', [51, 51]),\n            (True, False, [0, 0], 2, 61, b'0/0', [0, 0])],\n           [(True, False, [0, 1], 4, 0, b'0/1', [0, 0]),\n            (True, False, [0, 2], 2, 17, b'0/2', [0, 0]),\n            (False, False, [-1, -1], 3, 40, b'./.', [0, 0])],\n           [(True, False, [0, 0], 0, 0, b'0/0', [0, 0]),\n            (True, True, [0, 0], 0, 0, b'0|0', [0, 0]),\n            (False, False, [-1, -1], 0, 0, b'./.', [0, 0])],\n           [(True, False, [0, -1], 0, 0, b'0', [0, 0]),\n            (True, False, [0, 1], 0, 0, b'0/1', [0, 0]),\n            (True, True, [0, 2], 0, 0, b'0|2', [0, 0])]],\n          dtype=[('is_called', '?'), ('is_phased', '?'), ('genotype', 'i1', (2,)), ('DP', '<u2'), ('GQ', 'u1'), ('GT', 'S3'), ('HQ', '<i4', (2,))])\n    >>> c2d['genotype']\n    array([[[ 0,  0],\n            [ 0,  0],\n            [ 0,  1]],\n           [[ 0,  0],\n            [ 0,  0],\n            [ 0,  1]],\n           [[ 0,  0],\n            [ 1,  0],\n            [ 1,  1]],\n           [[ 0,  0],\n            [ 0,  1],\n            [ 0,  0]],\n           [[ 1,  2],\n            [ 2,  1],\n            [ 2,  2]],\n           [[ 0,  0],\n            [ 0,  0],\n            [ 0,  0]],\n           [[ 0,  1],\n            [ 0,  2],\n            [-1, -1]],\n           [[ 0,  0],\n            [ 0,  0],\n            [-1, -1]],\n           [[ 0, -1],\n            [ 0,  1],\n            [ 0,  2]]], dtype=int8)\n    >>> c2d['genotype'][3, :]\n    array([[0, 0],\n           [0, 1],\n           [0, 0]], dtype=int8)\n\n    \"\"\"  # flake8: noqa\n\n    loader = _CalldataLoader(vcf_fn, region=region, samples=samples,\n                             ploidy=ploidy, fields=fields,\n                             exclude_fields=exclude_fields, dtypes=dtypes,\n                             arities=arities, fills=fills, vcf_types=vcf_types,\n                             count=count, progress=progress,\n                             logstream=logstream, condition=condition,\n                             slice_args=slice_args, verbose=verbose,\n                             cache=cache, cachedir=cachedir,\n                             skip_cached=skip_cached,\n                             compress_cache=compress_cache,\n                             truncate=truncate)\n    arr = loader.load()\n    return arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _datetime_to_timestamp(self, v):\n\n        # stole from https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp\n        if timezone.is_aware(v):\n            return (v - timezone.datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()\n        else:\n            return (v - timezone.datetime(1970, 1, 1)).total_seconds()", "response": "Convert a datetime to a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert value to timestamp format", "response": "def to_timestamp(self, value):\n        \"\"\"\n        from value to timestamp format(float)\n        \"\"\"\n        if isinstance(value, (six.integer_types, float, six.string_types)):\n            try:\n                return float(value)\n            except ValueError:\n                value = self.datetime_str_to_datetime(value)\n\n        if isinstance(value, datetime.datetime):\n            return self._datetime_to_timestamp(value)\n\n        if value is None:\n            try:\n                return float(self.default)\n            except:\n                return 0.0\n\n        raise exceptions.ValidationError(\n            \"Unable to convert value: '%s' to timestamp\" % value,\n            code=\"invalid_timestamp\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_utc_datetime(self, value):\n        value = self.to_naive_datetime(value)\n\n        if timezone.is_naive(value):\n            value = timezone.make_aware(value, timezone.utc)\n        else:\n            value = timezone.localtime(value, timezone.utc)\n        return value", "response": "Converts value to datetime with tzinfo format"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert to default timezone datetime convert to default timezone datetime", "response": "def to_default_timezone_datetime(self, value):\n        \"\"\"\n        convert to default timezone datetime\n        \"\"\"\n        return timezone.localtime(self.to_utc_datetime(value), timezone.get_default_timezone())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_timestamp(self, value):\n        if isinstance(value, (six.integer_types, float, six.string_types)):\n            try:\n                return int(value)\n            except ValueError:\n                value = self.datetime_str_to_datetime(value)\n\n        if isinstance(value, datetime.datetime):\n            if timezone.is_aware(value):\n                value = timezone.localtime(value, timezone.utc)\n            return self._datetime_to_timestamp(value)\n\n        raise exceptions.ValidationError(\n            \"Unable to convert value: '%s' to timestamp\" % value,\n            code=\"invalid_timestamp\"\n        )", "response": "Converts value to timestamp format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_naive_datetime(self, value):\n        if isinstance(value, (six.integer_types, float, six.string_types)):\n            try:\n                return self.from_number(value)\n            except ValueError:\n                return self.datetime_str_to_datetime(value)\n\n        if isinstance(value, datetime.datetime):\n            return value\n\n        raise exceptions.ValidationError(\n            \"Unable to convert value: '%s' to python data type\" % value,\n            code=\"invalid_datetime\"\n        )", "response": "Converts value to datetime with tzinfo format"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_utc_datetime(self, value):\n        if isinstance(value, (six.integer_types, float, six.string_types)):\n            value = self.to_naive_datetime(value)\n\n        if isinstance(value, datetime.datetime):\n            if timezone.is_naive(value):\n                value = timezone.make_aware(value, timezone.utc)\n            else:\n                value = timezone.localtime(value, timezone.utc)\n            return value\n\n        raise exceptions.ValidationError(\n            \"Unable to convert value: '%s' to python data type\" % value,\n            code=\"invalid_datetime\"\n        )", "response": "Converts value to datetime with tzinfo format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace all occurrences of some symbols according to mapping repls.", "response": "def __replace_all(repls: dict, input: str) -> str:\n    \"\"\" Replaces from a string **input** all the occurrences of some\n    symbols according to mapping **repls**.\n\n    :param dict repls: where #key is the old character and\n    #value is the one to substitute with;\n    :param str input: original string where to apply the\n    replacements;\n    :return: *(str)* the string with the desired characters replaced\n    \"\"\"\n    return re.sub('|'.join(re.escape(key) for key in repls.keys()),\n                  lambda k: repls[k.group(0)], input)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dfa_json_importer(input_file: str) -> dict:\n    file = open(input_file)\n    json_file = json.load(file)\n\n    transitions = {}  # key [state \u2208 states, action \u2208 alphabet]\n    #                   value [arriving state \u2208 states]\n    for (origin, action, destination) in json_file['transitions']:\n        transitions[origin, action] = destination\n\n    dfa = {\n        'alphabet': set(json_file['alphabet']),\n        'states': set(json_file['states']),\n        'initial_state': json_file['initial_state'],\n        'accepting_states': set(json_file['accepting_states']),\n        'transitions': transitions\n    }\n    return dfa", "response": "Imports a DFA from a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport a DFA to a JSON file.", "response": "def dfa_to_json(dfa: dict, name: str, path: str = './'):\n    \"\"\" Exports a DFA to a JSON file.\n\n    If *path* do not exists, it will be created.\n\n    :param dict dfa: DFA to export;\n    :param str name: name of the output file;\n    :param str path: path where to save the JSON file (default:\n                     working directory)\n    \"\"\"\n    out = {\n        'alphabet': list(dfa['alphabet']),\n        'states': list(dfa['states']),\n        'initial_state': dfa['initial_state'],\n        'accepting_states': list(dfa['accepting_states']),\n        'transitions': list()\n    }\n\n    for t in dfa['transitions']:\n        out['transitions'].append(\n            [t[0], t[1], dfa['transitions'][t]])\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file = open(os.path.join(path, name + '.json'), 'w')\n    json.dump(out, file, sort_keys=True, indent=4)\n    file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting a DFA from a DOT file.", "response": "def dfa_dot_importer(input_file: str) -> dict:\n    \"\"\" Imports a DFA from a DOT file.\n\n    Of DOT files are recognized the following attributes:\n\n      \u2022 nodeX   shape=doublecircle -> accepting node;\n      \u2022 nodeX   root=true -> initial node;\n      \u2022 edgeX   label=\"a\" -> action in alphabet;\n      \u2022 fake    [style=invisible] -> dummy invisible node pointing\n                to initial state (they will be skipped);\n      \u2022 fake-> S [style=bold] -> dummy transition to draw the arrow\n                pointing to initial state (it will be skipped).\n\n    Forbidden names:\n\n      \u2022 'fake'  used for graphical purpose to drawn the arrow of\n        the initial state;\n      \u2022 'sink'  used as additional state when completing a DFA;\n      \u2022 'None'  used when no initial state is present.\n\n    Forbidden characters:\n      \u2022 \"\n      \u2022 '\n      \u2022 (\n      \u2022 )\n      \u2022 spaces\n\n    :param str input_file: path to the DOT file;\n    :return: *(dict)* representing a DFA.\n    \"\"\"\n\n    # pyDot Object\n    g = pydot.graph_from_dot_file(input_file)[0]\n\n    states = set()\n    initial_state = None\n    accepting_states = set()\n\n    replacements = {'\"': '', \"'\": '', '(': '', ')': '', ' ': ''}\n    for node in g.get_nodes():\n        if node.get_name() == 'fake' \\\n                or node.get_name() == 'None' \\\n                or node.get_name() == 'graph' \\\n                or node.get_name() == 'node':\n            continue\n        if 'style' in node.get_attributes() \\\n                and node.get_attributes()['style'] == 'invisible':\n            continue\n        node_reference = __replace_all(replacements,\n                                       node.get_name()).split(',')\n        if len(node_reference) > 1:\n            node_reference = tuple(node_reference)\n        else:\n            node_reference = node_reference[0]\n        states.add(node_reference)\n        for attribute in node.get_attributes():\n            if attribute == 'root':\n                initial_state = node_reference\n            if attribute == 'shape' and node.get_attributes()[\n                'shape'] == 'doublecircle':\n                accepting_states.add(node_reference)\n\n    alphabet = set()\n    transitions = {}\n    for edge in g.get_edges():\n        if edge.get_source() == 'fake':\n            continue\n        label = __replace_all(replacements, edge.get_label())\n        alphabet.add(label)\n        source = __replace_all(replacements,\n                               edge.get_source()).split(',')\n        if len(source) > 1:\n            source = tuple(source)\n        else:\n            source = source[0]\n        destination = __replace_all(replacements,\n                                    edge.get_destination()).split(',')\n        if len(destination) > 1:\n            destination = tuple(destination)\n        else:\n            destination = destination[0]\n        transitions[source, label] = destination\n\n    dfa = {\n        'alphabet': alphabet,\n        'states': states,\n        'initial_state': initial_state,\n        'accepting_states': accepting_states,\n        'transitions': transitions}\n    return dfa"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports a DFA to a DOT file.", "response": "def dfa_to_dot(dfa: dict, name: str, path: str = './'):\n    \"\"\" Generates a DOT file and a relative SVG image in **path**\n    folder of the input DFA using graphviz library.\n\n    :param dict dfa: DFA to export;\n    :param str name: name of the output file;\n    :param str path: path where to save the DOT/SVG files (default:\n                     working directory)\n    \"\"\"\n    g = graphviz.Digraph(format='svg')\n    g.node('fake', style='invisible')\n    for state in dfa['states']:\n        if state == dfa['initial_state']:\n            if state in dfa['accepting_states']:\n                g.node(str(state), root='true',\n                       shape='doublecircle')\n            else:\n                g.node(str(state), root='true')\n        elif state in dfa['accepting_states']:\n            g.node(str(state), shape='doublecircle')\n        else:\n            g.node(str(state))\n\n    g.edge('fake', str(dfa['initial_state']), style='bold')\n    for transition in dfa['transitions']:\n        g.edge(str(transition[0]),\n               str(dfa['transitions'][transition]),\n               label=transition[1])\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    g.render(filename=os.path.join(path, name + '.dot'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports a NFA from a JSON file.", "response": "def nfa_json_importer(input_file: str) -> dict:\n    \"\"\" Imports a NFA from a JSON file.\n\n    :param str input_file: path+filename to JSON file;\n    :return: *(dict)* representing a NFA.\n    \"\"\"\n    file = open(input_file)\n    json_file = json.load(file)\n\n    transitions = {}  # key [state in states, action in alphabet]\n    #                   value [Set of arriving states in states]\n    for p in json_file['transitions']:\n        transitions.setdefault((p[0], p[1]), set()).add(p[2])\n\n    nfa = {\n        'alphabet': set(json_file['alphabet']),\n        'states': set(json_file['states']),\n        'initial_states': set(json_file['initial_states']),\n        'accepting_states': set(json_file['accepting_states']),\n        'transitions': transitions\n    }\n\n    return nfa"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting a NFA to a JSON file.", "response": "def nfa_to_json(nfa: dict, name: str, path: str = './'):\n    \"\"\" Exports a NFA to a JSON file.\n\n    :param dict nfa: NFA to export;\n    :param str name: name of the output file;\n    :param str path: path where to save the JSON file (default:\n                     working directory).\n    \"\"\"\n    transitions = list()  # key[state in states, action in alphabet]\n    #                       value [Set of arriving states in states]\n    for p in nfa['transitions']:\n        for dest in nfa['transitions'][p]:\n            transitions.append([p[0], p[1], dest])\n\n    out = {\n        'alphabet': list(nfa['alphabet']),\n        'states': list(nfa['states']),\n        'initial_states': list(nfa['initial_states']),\n        'accepting_states': list(nfa['accepting_states']),\n        'transitions': transitions\n    }\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file = open(os.path.join(path, name + '.json'), 'w')\n    json.dump(out, file, sort_keys=True, indent=4)\n    file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nfa_dot_importer(input_file: str) -> dict:\n\n    # pyDot Object\n    g = pydot.graph_from_dot_file(input_file)[0]\n\n    states = set()\n    initial_states = set()\n    accepting_states = set()\n\n    replacements = {'\"': '', \"'\": '', '(': '', ')': '', ' ': ''}\n\n    for node in g.get_nodes():\n        attributes = node.get_attributes()\n        if node.get_name() == 'fake' \\\n                or node.get_name() == 'None' \\\n                or node.get_name() == 'graph' \\\n                or node.get_name() == 'node':\n            continue\n        if 'style' in attributes \\\n                and attributes['style'] == 'invisible':\n            continue\n\n        node_reference = __replace_all(replacements,\n                                       node.get_name()).split(',')\n        if len(node_reference) > 1:\n            node_reference = tuple(node_reference)\n        else:\n            node_reference = node_reference[0]\n        states.add(node_reference)\n        for attribute in attributes:\n            if attribute == 'root':\n                initial_states.add(node_reference)\n            if attribute == 'shape' \\\n                    and attributes['shape'] == 'doublecircle':\n                accepting_states.add(node_reference)\n\n    alphabet = set()\n    transitions = {}\n    for edge in g.get_edges():\n        source = __replace_all(replacements,\n                               edge.get_source()).split(',')\n        if len(source) > 1:\n            source = tuple(source)\n        else:\n            source = source[0]\n        destination = __replace_all(replacements,\n                                    edge.get_destination()).split(',')\n        if len(destination) > 1:\n            destination = tuple(destination)\n        else:\n            destination = destination[0]\n\n        if source not in states or destination not in states:\n            continue\n\n        label = __replace_all(replacements, edge.get_label())\n        alphabet.add(label)\n\n        transitions.setdefault((source, label), set()).add(\n            destination)\n\n    nfa = {\n        'alphabet': alphabet,\n        'states': states,\n        'initial_states': initial_states,\n        'accepting_states': accepting_states,\n        'transitions': transitions\n    }\n\n    return nfa", "response": "Imports a NFA from a DOT file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a DOT file and a relative SVG image in path from the input NFA.", "response": "def nfa_to_dot(nfa: dict, name: str, path: str = './'):\n    \"\"\" Generates a DOT file and a relative SVG image in **path**\n    folder of the input NFA using graphviz library.\n\n    :param dict nfa: input NFA;\n    :param str name: string with the name of the output file;\n    :param str path: path where to save the DOT/SVG files (default:\n                     working directory).\n    \"\"\"\n    g = graphviz.Digraph(format='svg')\n\n    fakes = []\n    for i in range(len(nfa['initial_states'])):\n        fakes.append('fake' + str(i))\n        g.node('fake' + str(i), style='invisible')\n\n    for state in nfa['states']:\n        if state in nfa['initial_states']:\n            if state in nfa['accepting_states']:\n                g.node(str(state), root='true',\n                       shape='doublecircle')\n            else:\n                g.node(str(state), root='true')\n        elif state in nfa['accepting_states']:\n            g.node(str(state), shape='doublecircle')\n        else:\n            g.node(str(state))\n\n    for initial_state in nfa['initial_states']:\n        g.edge(fakes.pop(), str(initial_state), style='bold')\n    for transition in nfa['transitions']:\n        for destination in nfa['transitions'][transition]:\n            g.edge(str(transition[0]), str(destination),\n                   label=transition[1])\n\n    g.render(filename=os.path.join(path, name + '.dot'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef afw_json_importer(input_file: str) -> dict:\n    file = open(input_file)\n    json_file = json.load(file)\n\n    transitions = {}  # key [state in states, action in alphabet]\n    #  value [string representing boolean expression]\n    for p in json_file['transitions']:\n        transitions[p[0], p[1]] = p[2]\n\n    # return map\n    afw = {\n        'alphabet': set(json_file['alphabet']),\n        'states': set(json_file['states']),\n        'initial_state': json_file['initial_state'],\n        'accepting_states': set(json_file['accepting_states']),\n        'transitions': transitions\n    }\n    return afw", "response": "Imports a AFW from a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __recursive_acceptance(afw, state, remaining_word):\n    # the word is accepted only if all the final states are\n    # accepting states\n    if len(remaining_word) == 0:\n        if state in afw['accepting_states']:\n            return True\n        else:\n            return False\n\n    action = remaining_word[0]\n    if (state, action) not in afw['transitions']:\n        return False\n\n    if afw['transitions'][state, action] == 'True':\n        return True\n    elif afw['transitions'][state, action] == 'False':\n        return False\n\n    transition = (state, action)\n    # extract from the boolean formula of the transition the\n    # states involved in it\n    involved_states = list(\n        set(\n            re.findall(r\"[\\w']+\", afw['transitions'][transition])\n        ).difference({'and', 'or', 'True', 'False'})\n    )\n    possible_assignments = set(\n        itertools.product([True, False], repeat=len(involved_states)))\n    # For all possible assignment of the the transition (a\n    # boolean formula over the states)\n    for assignment in possible_assignments:\n        mapping = dict(zip(involved_states, assignment))\n        # If the assignment evaluation is positive\n        if eval(afw['transitions'][transition], mapping):\n            ok = True\n            mapping.pop('__builtins__')  # removes useless entry\n            # added by the function eval()\n\n            # Check if the word is accepted in ALL the states\n            # mapped to True by the assignment\n            for mapped_state in mapping:\n                if mapping[mapped_state] == False:\n                    continue\n                if not __recursive_acceptance(afw,\n                                              mapped_state,\n                                              remaining_word[1:]):\n                    # if one positive state of the assignment\n                    # doesn't accepts the word,the whole\n                    # assignment is discarded\n                    ok = False\n                    break\n            if ok:\n                # If at least one assignment accepts the word,\n                # the word is accepted by the afw\n                return True\n    return False", "response": "Recursive call for word acceptance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef afw_word_acceptance(afw: dict, word: list) -> bool:\n    return __recursive_acceptance(afw, afw['initial_state'], word)", "response": "Checks if a word is accepted by input AFW returning True if the word is accepted False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nside effect on input! Complete the AFW adding not racket present transitions and marking them as False.", "response": "def afw_completion(afw):\n    \"\"\" Side effect on input! Complete the afw adding not\n    present transitions and marking them as False.\n\n    :param dict afw: input AFW.\n    \"\"\"\n\n    for state in afw['states']:\n        for a in afw['alphabet']:\n            if (state, a) not in afw['transitions']:\n                afw['transitions'][state, a] = 'False'\n    return afw"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nfa_to_afw_conversion(nfa: dict) -> dict:\n    afw = {\n        'alphabet': nfa['alphabet'].copy(),\n        'states': nfa['states'].copy(),\n        'initial_state': 'root',\n        'accepting_states': nfa['accepting_states'].copy(),\n        'transitions': dict()\n    }\n\n    # Make sure \"root\" node doesn't already exists, in case rename it\n    i = 0\n    while afw['initial_state'] in nfa['states']:\n        afw['initial_state'] = 'root' + str(i)\n        i += 1\n    afw['states'].add(afw['initial_state'])\n\n    for (state, action) in nfa['transitions']:\n        boolean_formula = str()\n        for destination in nfa['transitions'][state, action]:\n            boolean_formula += destination + ' or '\n        # strip last ' or ' from the formula string\n        boolean_formula = boolean_formula[0:-4]\n        afw['transitions'][state, action] = boolean_formula\n        if state in nfa['initial_states']:\n            afw['transitions'][afw['initial_state'], action] = boolean_formula\n\n    return afw", "response": "Returns a AFW reading the same language of input NFA."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a AFW to a NFA.", "response": "def afw_to_nfa_conversion(afw: dict) -> dict:\n    \"\"\" Returns a NFA reading the same language of input AFW.\n\n    Let :math:`A = (\u03a3, S, s^0 , \u03c1, F )`  be an afw. Then we\n    define the nfa :math:`A_N` such that :math:`L(A_N) = L(A)`\n    as follows :math:`AN = (\u03a3, S_N , S^0_N , \u03c1_N , F_N )` where:\n\n     \u2022 :math:`S_N = 2^S`\n     \u2022 :math:`S^0_N= \\{\\{s^0 \\}\\}`\n     \u2022 :math:`F_N=2^F`\n     \u2022 :math:`(Q,a,Q') \u2208 \u03c1_N` iff :math:`Q'` satisfies :math:`\u22c0_{\n       s\u2208Q} \u03c1(s, a)`\n\n     We take an empty conjunction in the definition of\n     :math:`\u03c1_N` to be equivalent to true; thus, :math:`(\u2205, a,\n     \u2205) \u2208 \u03c1_N`.\n\n    :param dict afw: input AFW.\n    :return: *(dict)* representing a NFA.\n    \"\"\"\n\n    nfa = {\n        'alphabet': afw['alphabet'].copy(),\n        'initial_states': {(afw['initial_state'],)},\n        'states': {(afw['initial_state'],)},\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n\n    # State of the NFA are composed by the union of more states of the AFW\n\n    boundary = deepcopy(nfa['states'])\n    possible_assignments = set(\n        itertools.product([True, False], repeat=len(afw['states'])))\n\n    while boundary:\n        state = boundary.pop()\n        # The state is accepting only if composed exclusively of final states\n        if set(state).issubset(afw['accepting_states']):\n            nfa['accepting_states'].add(state)\n\n        for action in nfa['alphabet']:\n            boolean_formula = 'True'\n            # join the boolean formulas of the single states given the action\n            for s in state:\n                if (s, action) not in afw['transitions']:\n                    boolean_formula += ' and False'\n                else:\n                    boolean_formula += \\\n                        ' and (' + \\\n                        afw['transitions'][s, action] + \\\n                        ')'\n\n            for assignment in possible_assignments:\n                mapping = dict(zip(afw['states'], assignment))\n\n                # If the formula is satisfied\n                if eval(boolean_formula, mapping):\n                    # add the transition to the resulting NFA\n\n                    evaluation = \\\n                        tuple(k for k in mapping if mapping[k] is True)\n\n                    if evaluation not in nfa['states']:\n                        nfa['states'].add(evaluation)\n                        boundary.add(evaluation)\n                    nfa['transitions'].setdefault(\n                        (state, action), set()).add(evaluation)\n\n    return nfa"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dual of the input formula.", "response": "def formula_dual(input_formula: str) -> str:\n    \"\"\" Returns the dual of the input formula.\n\n    The dual operation on formulas in :math:`B^+(X)` is defined as:\n    the dual :math:`\\overline{\u03b8}` of a formula :math:`\u03b8` is obtained from \u03b8 by\n    switching :math:`\u2227` and :math:`\u2228`, and\n    by switching :math:`true` and :math:`false`.\n\n    :param str input_formula: original string.\n    :return: *(str)*, dual of input formula.\n    \"\"\"\n    conversion_dictionary = {\n        'and': 'or',\n        'or': 'and',\n        'True': 'False',\n        'False': 'True'\n    }\n\n    return re.sub(\n        '|'.join(re.escape(key) for key in conversion_dictionary.keys()),\n        lambda k: conversion_dictionary[k.group(0)], input_formula)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef afw_complementation(afw: dict) -> dict:\n    completed_input = afw_completion(deepcopy(afw))\n\n    complemented_afw = {\n        'alphabet': completed_input['alphabet'],\n        'states': completed_input['states'],\n        'initial_state': completed_input['initial_state'],\n        'accepting_states':\n            completed_input['states'].difference(afw['accepting_states']),\n        'transitions': dict()\n    }\n\n    for transition in completed_input['transitions']:\n        complemented_afw['transitions'][transition] = \\\n            formula_dual(completed_input['transitions'][transition])\n    return complemented_afw", "response": "Returns a AFW reading the complemented language read by AFW input AFW."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename_afw_states(afw: dict, suffix: str):\n    conversion_dict = {}\n    new_states = set()\n    new_accepting = set()\n    for state in afw['states']:\n        conversion_dict[state] = '' + suffix + state\n        new_states.add('' + suffix + state)\n        if state in afw['accepting_states']:\n            new_accepting.add('' + suffix + state)\n\n    afw['states'] = new_states\n    afw['initial_state'] = '' + suffix + afw['initial_state']\n    afw['accepting_states'] = new_accepting\n\n    new_transitions = {}\n    for transition in afw['transitions']:\n        new_transition = __replace_all(conversion_dict, transition[0])\n        new_transitions[new_transition, transition[1]] = \\\n            __replace_all(conversion_dict, afw['transitions'][transition])\n    afw['transitions'] = new_transitions", "response": "Side effect on input! Renames all the states of the AFW with a given suffix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef afw_union(afw_1: dict, afw_2: dict) -> dict:\n    # make sure new root state is unique\n    initial_state = 'root'\n    i = 0\n    while initial_state in afw_1['states'] or initial_state in afw_2['states']:\n        initial_state = 'root' + str(i)\n        i += 1\n\n    union = {\n        'alphabet': afw_1['alphabet'].union(afw_2['alphabet']),\n        'states':\n            afw_1['states'].union(afw_2['states']).union({initial_state}),\n        'initial_state': initial_state,\n        'accepting_states':\n            afw_1['accepting_states'].union(afw_2['accepting_states']),\n        'transitions': deepcopy(afw_1['transitions'])\n    }\n\n    # add also afw_2 transitions\n    union['transitions'].update(afw_2['transitions'])\n\n    # if just one initial state is accepting, so the new one is\n    if afw_1['initial_state'] in afw_1['accepting_states'] \\\n            or afw_2['initial_state'] in afw_2['accepting_states']:\n        union['accepting_states'].add(union['initial_state'])\n\n    # copy all transitions of initial states and eventually their conjunction\n    # into the new initial state\n    for action in union['alphabet']:\n        if (afw_1['initial_state'], action) in afw_1['transitions']:\n            union['transitions'][initial_state, action] = \\\n                '(' + \\\n                afw_1['transitions'][afw_1['initial_state'], action] + \\\n                ')'\n            if (afw_2['initial_state'], action) in afw_2['transitions']:\n                union['transitions'][initial_state, action] += \\\n                    ' or (' + \\\n                    afw_2['transitions'][afw_2['initial_state'], action] + \\\n                    ')'\n        elif (afw_2['initial_state'], action) in afw_2['transitions']:\n            union['transitions'][initial_state, action] = \\\n                '(' + \\\n                afw_2['transitions'][afw_2['initial_state'], action] + \\\n                ')'\n\n    return union", "response": "Returns a AFW that reads the union of the languages in input AFWs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a AFW that reads the intersection of the languages in two AFWs.", "response": "def afw_intersection(afw_1: dict, afw_2: dict) -> dict:\n    \"\"\" Returns a AFW that reads the intersection of the\n    languages read by input AFWs.\n\n    Let :math:`A_1 = (\u03a3, S_1 , s^0_1, \u03c1_1 , F_1 )` and :math:`A_2\n    = (\u03a3, S_2 , s^0_2, \u03c1_2 , F_2 )`\n    be alternating automata accepting the languages :math:`L(\n    A_1)` and :math:`L(A_2)`.\n    Then, :math:`B_\u2229 = (\u03a3, S_1 \u222a S_2 \u222a {root}, root, \u03c1_\u2229 , F_1 \u222a\n    F_2 )` with\n    :math:`\u03c1_\u2229 = \u03c1_1 \u222a \u03c1_2 \u222a [(root, a): \u03c1(s^0_1 , a) \u2227 \u03c1(s^0_2 ,\n    a)]` accepts :math:`L(A_1) \u2229 L(A_2)`.\n\n    :param dict afw_1: first input AFW;\n    :param dict afw_2: second input AFW.\n    :return: *(dict)* representing a AFW.\n    \"\"\"\n    # make sure new root state is unique\n    initial_state = 'root'\n    i = 0\n    while initial_state in afw_1['states'] or initial_state in afw_2['states']:\n        initial_state = 'root' + str(i)\n        i += 1\n\n    intersection = {\n        'alphabet': afw_1['alphabet'].union(afw_2['alphabet']),\n        'states':\n            afw_1['states'].union(afw_2['states']).union({initial_state}),\n        'initial_state': initial_state,\n        'accepting_states':\n            afw_1['accepting_states'].union(afw_2['accepting_states']),\n        'transitions': deepcopy(afw_1['transitions'])\n    }\n\n    # add also afw_2 transitions\n    intersection['transitions'].update(afw_2['transitions'])\n\n    # if both initial states are accepting, so the new one is\n    if afw_1['initial_state'] in afw_1['accepting_states'] \\\n            and afw_2['initial_state'] in afw_2['accepting_states']:\n        intersection['accepting_states'].add(\n            intersection['initial_state'])\n\n    # New initial state transitions will be the conjunction of\n    # precedent inital states ones\n    for action in intersection['alphabet']:\n        if (afw_1['initial_state'], action) in afw_1['transitions']:\n            intersection['transitions'][initial_state, action] = \\\n                '(' + \\\n                afw_1['transitions'][afw_1['initial_state'], action] + \\\n                ')'\n            if (afw_2['initial_state'], action) in afw_2['transitions']:\n                intersection['transitions'][initial_state, action] += \\\n                    ' and (' + \\\n                    afw_2['transitions'][afw_2['initial_state'], action] + \\\n                    ')'\n            else:\n                intersection['transitions'][\n                    initial_state, action] += ' and False'\n        elif (afw_2['initial_state'], action) in afw_2['transitions']:\n            intersection['transitions'][initial_state, action] = \\\n                'False and (' + \\\n                afw_2['transitions'][afw_2['initial_state'], action] + \\\n                ')'\n\n    return intersection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef afw_nonemptiness_check(afw: dict) -> bool:\n    nfa = afw_to_nfa_conversion(afw)\n    return NFA.nfa_nonemptiness_check(nfa)", "response": "Checks if the input AFW reads any other than the\n    empty one returning True or False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the language read by the input AFW is different from \u03a3\u2217.", "response": "def afw_nonuniversality_check(afw: dict) -> bool:\n    \"\"\" Checks if the language read by the input AFW is different\n    from \u03a3\u2217, returning True/False.\n\n    The afw is translated into a nfa and then its nonuniversality\n    is checked.\n\n    :param dict afw: input AFW.\n    :return: *(bool)*, True if input afw is nonuniversal, False\n             otherwise.\n    \"\"\"\n    nfa = afw_to_nfa_conversion(afw)\n    return NFA.nfa_nonuniversality_check(nfa)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntranslate value to dbus types", "response": "def translate_to_dbus_type(typeof, value):\n    \"\"\"\n    Helper function to map values from their native Python types\n    to Dbus types.\n\n    :param type typeof: Target for type conversion e.g., 'dbus.Dictionary'\n    :param value: Value to assign using type 'typeof'\n    :return: 'value' converted to type 'typeof'\n    :rtype: typeof\n    \"\"\"\n    if ((isinstance(value, types.UnicodeType) or\n         isinstance(value, str)) and typeof is not dbus.String):\n        # FIXME: This is potentially dangerous since it evaluates\n        # a string in-situ\n        return typeof(eval(value))\n    else:\n        return typeof(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef signal_handler(self, *args):\n        self.user_callback(self.signal, self.user_arg, *args)", "response": "Method to invoke the user callback when the signal is received."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a signal receiver callback with user argument", "response": "def add_signal_receiver(self, callback_fn, signal, user_arg):\n        \"\"\"\n        Add a signal receiver callback with user argument\n\n        See also :py:meth:`remove_signal_receiver`,\n        :py:exc:`.BTSignalNameNotRecognisedException`\n\n        :param func callback_fn: User-defined callback function to call when\n            signal triggers\n        :param str signal: Signal name e.g.,\n            :py:attr:`.BTInterface.SIGNAL_PROPERTY_CHANGED`\n        :param user_arg: User-defined callback argument to be passed with\n            callback function\n        :return:\n        :raises BTSignalNameNotRecognisedException: if the signal name is\n            not registered\n        \"\"\"\n        if (signal in self._signal_names):\n            s = Signal(signal, callback_fn, user_arg)\n            self._signals[signal] = s\n            self._bus.add_signal_receiver(s.signal_handler,\n                                          signal,\n                                          dbus_interface=self._dbus_addr,\n                                          path=self._path)\n        else:\n            raise BTSignalNameNotRecognisedException"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_property(self, name=None):\n        if (name):\n            return self._interface.GetProperties()[name]\n        else:\n            return self._interface.GetProperties()", "response": "Get a property value by name or all properties."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a property value by name translating to correct dbus type", "response": "def set_property(self, name, value):\n        \"\"\"\n        Helper to set a property value by name, translating to correct\n        dbus type\n\n        See also :py:meth:`get_property`\n\n        :param str name: The property name in the object's dictionary\n            whose value shall be set.\n        :param value: Properties new value to be assigned.\n        :return:\n        :raises KeyError: if the property key is not found in the\n            object's dictionary\n        :raises dbus.Exception: org.bluez.Error.DoesNotExist\n        :raises dbus.Exception: org.bluez.Error.InvalidArguments\n        \"\"\"\n        typeof = type(self.get_property(name))\n        self._interface.SetProperty(name,\n                                    translate_to_dbus_type(typeof, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_mode(self, mode):\n        values = {\"desired_state\": {\"mode\": mode}}\n        response = self.api_interface.set_device_state(self, values)\n        self._update_state_from_response(response)", "response": "Set the mode of the night or home"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_privacy(self, state):\n        values = {\"desired_state\": {\"private\": state}}\n        response = self.api_interface.set_device_state(self, values)\n        self._update_state_from_response(response)", "response": "Set the privacy state of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default(self, o):\n        if isinstance(o, (datetime.datetime, datetime.date, datetime.time)):\n            return o.isoformat()\n\n        if isinstance(o, decimal.Decimal):\n            return float(o)\n\n        return json.JSONEncoder.default(self, o)", "response": "Encode JSON.\n\n        :return str: A JSON encoded string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the document for the current model instance.", "response": "def dump_document(cls, instance, fields_own=None, fields_to_many=None):\n        \"\"\" Get document for model_instance.\n\n        redefine dump rule for field x: def dump_document_x\n\n        :param django.db.models.Model instance: model instance\n        :param list<Field> or None fields: model_instance field to dump\n        :return dict: document\n\n        Related documents are not included to current one. In case of to-many\n        field serialization ensure that models_instance has been select_related\n        so, no database calls would be executed.\n\n        Method ensures that document has cls.Meta.fieldnames_include and does\n        not have cls.Meta.fieldnames_exclude\n\n        Steps:\n        1) fieldnames_include could be properties, but not related models.\n        Add them to fields_own.\n\n        \"\"\"\n        if fields_own is not None:\n            fields_own = {f.name for f in fields_own}\n        else:\n            fields_own = {\n                f.name for f in instance._meta.fields\n                if f.rel is None and f.serialize\n            }\n        fields_own.add('id')\n\n        fields_own = (fields_own | set(cls.Meta.fieldnames_include))\\\n            - set(cls.Meta.fieldnames_exclude)\n\n        document = {}\n        # Include own fields\n        for fieldname in fields_own:\n            field_serializer = getattr(\n                cls, \"dump_document_{}\".format(fieldname), None)\n\n            if field_serializer is not None:\n                value = field_serializer(instance)\n            else:\n                value = getattr(instance, fieldname)\n                try:\n                    field = instance._meta.get_field(fieldname)\n                except models.fields.FieldDoesNotExist:\n                    # Field is property, value already calculated\n                    pass\n                else:\n                    if isinstance(field, models.fields.files.FileField):\n                        # TODO: Serializer depends on API here.\n                        value = cls.Meta.api.base_url + value.url\n                    elif isinstance(field, models.CommaSeparatedIntegerField):\n                        value = [v for v in value]\n\n            document[fieldname] = value\n\n        # Include to-one fields. It does not require database calls\n        for field in instance._meta.fields:\n            fieldname = \"{}_id\".format(field.name)\n            # NOTE: check field is not related to parent model to exclude\n            # <class>_ptr fields. OneToOne relationship field.rel.multiple =\n            # False. Here make sure relationship is to parent model.\n            if field.rel and not field.rel.multiple \\\n                    and isinstance(instance, field.rel.to):\n                continue\n\n            if field.rel and fieldname not in cls.Meta.fieldnames_exclude:\n                document[\"links\"] = document.get(\"links\") or {}\n                document[\"links\"][field.name] = getattr(instance, fieldname)\n\n        # Include to-many fields. It requires database calls. At this point we\n        # assume that model was prefetch_related with child objects, which would\n        # be included into 'linked' attribute. Here we need to add ids of linked\n        # objects. To avoid database calls, iterate over objects manually and\n        # get ids.\n        fields_to_many = fields_to_many or []\n        for field in fields_to_many:\n            document[\"links\"] = document.get(\"links\") or {}\n            document[\"links\"][field.related_resource_name] = [\n                obj.id for obj in getattr(instance, field.name).all()]\n\n        return document"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the state of the object based on the response json obj returned from query", "response": "def _update_state_from_response(self, response_json):\n        \"\"\"\n        :param response_json: the json obj returned from query\n        :return:\n        \"\"\"\n        _response_json = response_json.get('data')\n        if _response_json is not None:\n            self.json_state = _response_json\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering out related model fields.", "response": "def _filter_child_model_fields(cls, fields):\n        \"\"\" Keep only related model fields.\n\n        Example: Inherited models: A -> B -> C\n        B has one-to-many relationship to BMany.\n        after inspection BMany would have links to B and C. Keep only B. Parent\n        model A could not be used (It would not be in fields)\n\n        :param list fields: model fields.\n        :return list fields: filtered fields.\n\n        \"\"\"\n        indexes_to_remove = set([])\n        for index1, field1 in enumerate(fields):\n            for index2, field2 in enumerate(fields):\n                if index1 < index2 and index1 not in indexes_to_remove and\\\n                        index2 not in indexes_to_remove:\n                    if issubclass(field1.related_model, field2.related_model):\n                        indexes_to_remove.add(index1)\n\n                    if issubclass(field2.related_model, field1.related_model):\n                        indexes_to_remove.add(index2)\n\n        fields = [field for index, field in enumerate(fields)\n                  if index not in indexes_to_remove]\n\n        return fields"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef major_service_class(self):\n        major_service = []\n        for i in BTCoD._MAJOR_SERVICE_CLASS.keys():\n            if (self.cod & i):\n                major_service.append(BTCoD._MAJOR_SERVICE_CLASS[i])\n        return major_service", "response": "Return the major service class property decoded e. g.. Auxiliary class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minor_device_class(self):\n        minor_device = []\n        minor_lookup = BTCoD._MINOR_DEVICE_CLASS.get(self.cod &\n                                                     BTCoD._MAJOR_DEVICE_MASK,\n                                                     [])\n        for i in minor_lookup:\n            minor_value = self.cod & i.get('mask')\n            minor_device.append(i.get(minor_value, 'Unknown'))\n        return minor_device", "response": "Return the minor device class property decoded e. g.."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_devices_from_response_dict(response_dict, device_type):\n    items = response_dict.get('data')\n\n    devices = []\n\n    api_interface = WinkApiInterface()\n    check_list = isinstance(device_type, (list,))\n\n    for item in items:\n        if (check_list and get_object_type(item) in device_type) or \\\n                (not check_list and get_object_type(item) == device_type):\n            _devices = build_device(item, api_interface)\n            for device in _devices:\n                devices.append(device)\n\n    return devices", "response": "Get list of WinkDevice objects from response dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the state of a specific device.", "response": "def set_device_state(self, device, state, id_override=None, type_override=None):\n        \"\"\"\n        Set device state via online API.\n\n        Args:\n            device (WinkDevice): The device the change is being requested for.\n            state (Dict): The state being requested.\n            id_override (String, optional): A device ID used to override the\n                passed in device's ID. Used to make changes on sub-devices.\n                i.e. Outlet in a Powerstrip. The Parent device's ID.\n            type_override (String, optional): Used to override the device type\n                when a device inherits from a device other than WinkDevice.\n        Returns:\n            response_json (Dict): The API's response in dictionary format\n        \"\"\"\n        _LOGGER.info(\"Setting state via online API\")\n        object_id = id_override or device.object_id()\n        object_type = type_override or device.object_type()\n        url_string = \"{}/{}s/{}\".format(self.BASE_URL,\n                                        object_type,\n                                        object_id)\n        if state is None or object_type == \"group\":\n            url_string += \"/activate\"\n            if state is None:\n                arequest = requests.post(url_string,\n                                         headers=API_HEADERS)\n            else:\n                arequest = requests.post(url_string,\n                                         data=json.dumps(state),\n                                         headers=API_HEADERS)\n        else:\n            arequest = requests.put(url_string,\n                                    data=json.dumps(state),\n                                    headers=API_HEADERS)\n        if arequest.status_code == 401:\n            new_token = refresh_access_token()\n            if new_token:\n                arequest = requests.put(url_string,\n                                        data=json.dumps(state),\n                                        headers=API_HEADERS)\n            else:\n                raise WinkAPIException(\"Failed to refresh access token.\")\n        response_json = arequest.json()\n        _LOGGER.debug('%s', response_json)\n        return response_json"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef local_set_state(self, device, state, id_override=None, type_override=None):\n        if ALLOW_LOCAL_CONTROL:\n            if device.local_id() is not None:\n                hub = HUBS.get(device.hub_id())\n                if hub is None or hub[\"token\"] is None:\n                    return self.set_device_state(device, state, id_override, type_override)\n            else:\n                return self.set_device_state(device, state, id_override, type_override)\n            _LOGGER.info(\"Setting local state\")\n            local_id = id_override or device.local_id().split(\".\")[0]\n            object_type = type_override or device.object_type()\n            LOCAL_API_HEADERS['Authorization'] = \"Bearer \" + hub[\"token\"]\n            url_string = \"https://{}:8888/{}s/{}\".format(hub[\"ip\"],\n                                                         object_type,\n                                                         local_id)\n            try:\n                arequest = requests.put(url_string,\n                                        data=json.dumps(state),\n                                        headers=LOCAL_API_HEADERS,\n                                        verify=False, timeout=3)\n            except requests.exceptions.RequestException:\n                _LOGGER.error(\"Error sending local control request. Sending request online\")\n                return self.set_device_state(device, state, id_override, type_override)\n            response_json = arequest.json()\n            _LOGGER.debug('%s', response_json)\n            temp_state = device.json_state\n            for key, value in response_json[\"data\"][\"last_reading\"].items():\n                temp_state[\"last_reading\"][key] = value\n            return temp_state\n        else:\n            return self.set_device_state(device, state, id_override, type_override)", "response": "Set the state of a specific object in the local API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_device_state(self, device, id_override=None, type_override=None):\n        _LOGGER.info(\"Getting state via online API\")\n        object_id = id_override or device.object_id()\n        object_type = type_override or device.object_type()\n        url_string = \"{}/{}s/{}\".format(self.BASE_URL,\n                                        object_type, object_id)\n        arequest = requests.get(url_string, headers=API_HEADERS)\n        response_json = arequest.json()\n        _LOGGER.debug('%s', response_json)\n        return response_json", "response": "Get the state of a specific object in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the state of a specific object from the local API.", "response": "def local_get_state(self, device, id_override=None, type_override=None):\n        \"\"\"\n        Get device state via local API, and fall back to online API.\n\n        Args:\n            device (WinkDevice): The device the change is being requested for.\n            id_override (String, optional): A device ID used to override the\n                passed in device's ID. Used to make changes on sub-devices.\n                i.e. Outlet in a Powerstrip. The Parent device's ID.\n            type_override (String, optional): Used to override the device type\n                when a device inherits from a device other than WinkDevice.\n        Returns:\n            response_json (Dict): The API's response in dictionary format\n        \"\"\"\n        if ALLOW_LOCAL_CONTROL:\n            if device.local_id() is not None:\n                hub = HUBS.get(device.hub_id())\n                if hub is not None and hub[\"token\"] is not None:\n                    ip = hub[\"ip\"]\n                    access_token = hub[\"token\"]\n                else:\n                    return self.get_device_state(device, id_override, type_override)\n            else:\n                return self.get_device_state(device, id_override, type_override)\n            _LOGGER.info(\"Getting local state\")\n            local_id = id_override or device.local_id()\n            object_type = type_override or device.object_type()\n            LOCAL_API_HEADERS['Authorization'] = \"Bearer \" + access_token\n            url_string = \"https://{}:8888/{}s/{}\".format(ip,\n                                                         object_type,\n                                                         local_id)\n            try:\n                arequest = requests.get(url_string,\n                                        headers=LOCAL_API_HEADERS,\n                                        verify=False, timeout=3)\n            except requests.exceptions.RequestException:\n                _LOGGER.error(\"Error sending local control request. Sending request online\")\n                return self.get_device_state(device, id_override, type_override)\n            response_json = arequest.json()\n            _LOGGER.debug('%s', response_json)\n            temp_state = device.json_state\n            for key, value in response_json[\"data\"][\"last_reading\"].items():\n                temp_state[\"last_reading\"][key] = value\n            return temp_state\n        else:\n            return self.get_device_state(device, id_override, type_override)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a call to the update_firmware endpoint. As far as I know this is only valid for Wink hubs. Args: device (WinkDevice): The device the change is being requested for. id_override (String, optional): A device ID used to override the passed in device's ID. Used to make changes on sub-devices. i.e. Outlet in a Powerstrip. The Parent device's ID. type_override (String, optional): Used to override the device type when a device inherits from a device other than WinkDevice. Returns: response_json (Dict): The API's response in dictionary format", "response": "def update_firmware(self, device, id_override=None, type_override=None):\n        \"\"\"\n        Make a call to the update_firmware endpoint. As far as I know this\n        is only valid for Wink hubs.\n\n        Args:\n            device (WinkDevice): The device the change is being requested for.\n            id_override (String, optional): A device ID used to override the\n                passed in device's ID. Used to make changes on sub-devices.\n                i.e. Outlet in a Powerstrip. The Parent device's ID.\n            type_override (String, optional): Used to override the device type\n                when a device inherits from a device other than WinkDevice.\n        Returns:\n            response_json (Dict): The API's response in dictionary format\n        \"\"\"\n        object_id = id_override or device.object_id()\n        object_type = type_override or device.object_type()\n        url_string = \"{}/{}s/{}/update_firmware\".format(self.BASE_URL,\n                                                        object_type,\n                                                        object_id)\n        try:\n            arequest = requests.post(url_string,\n                                     headers=API_HEADERS)\n            response_json = arequest.json()\n            return response_json\n        except requests.exceptions.RequestException:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a device from the cache.", "response": "def remove_device(self, device, id_override=None, type_override=None):\n        \"\"\"\n        Remove a device.\n\n        Args:\n            device (WinkDevice): The device the change is being requested for.\n            id_override (String, optional): A device ID used to override the\n                passed in device's ID. Used to make changes on sub-devices.\n                i.e. Outlet in a Powerstrip. The Parent device's ID.\n            type_override (String, optional): Used to override the device type\n                when a device inherits from a device other than WinkDevice.\n        Returns:\n            (boolean): True if the device was removed.\n        \"\"\"\n        object_id = id_override or device.object_id()\n        object_type = type_override or device.object_type()\n        url_string = \"{}/{}s/{}\".format(self.BASE_URL,\n                                        object_type,\n                                        object_id)\n\n        try:\n            arequest = requests.delete(url_string,\n                                       headers=API_HEADERS)\n            if arequest.status_code == 204:\n                return True\n            _LOGGER.error(\"Failed to remove device. Status code: %s\", arequest.status_code)\n            return False\n        except requests.exceptions.RequestException:\n            _LOGGER.error(\"Failed to remove device.\")\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new lock key code for a device.", "response": "def create_lock_key(self, device, new_device_json, id_override=None, type_override=None):\n        \"\"\"\n        Create a new lock key code.\n\n        Args:\n            device (WinkDevice): The device the change is being requested for.\n            new_device_json (String): The JSON string required to create the device.\n            id_override (String, optional): A device ID used to override the\n                passed in device's ID. Used to make changes on sub-devices.\n                i.e. Outlet in a Powerstrip. The Parent device's ID.\n            type_override (String, optional): Used to override the device type\n                when a device inherits from a device other than WinkDevice.\n        Returns:\n            response_json (Dict): The API's response in dictionary format\n        \"\"\"\n        object_id = id_override or device.object_id()\n        object_type = type_override or device.object_type()\n        url_string = \"{}/{}s/{}/keys\".format(self.BASE_URL,\n                                             object_type,\n                                             object_id)\n        try:\n            arequest = requests.post(url_string,\n                                     data=json.dumps(new_device_json),\n                                     headers=API_HEADERS)\n            response_json = arequest.json()\n            return response_json\n        except requests.exceptions.RequestException:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_concrete_model(model):\n    if not(inspect.isclass(model) and issubclass(model, models.Model)):\n        model = get_model_by_name(model)\n\n    return model", "response": "Get model defined in Meta.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine resource name based on Meta information.", "response": "def get_resource_name(meta):\n    \"\"\" Define resource name based on Meta information.\n\n    :param Resource.Meta meta: resource meta information\n    :return: name of resource\n    :rtype: str\n    :raises ValueError:\n\n    \"\"\"\n    if meta.name is None and not meta.is_model:\n        msg = \"Either name or model for resource.Meta shoud be provided\"\n        raise ValueError(msg)\n\n    name = meta.name or get_model_name(get_concrete_model(meta.model))\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging meta parameters. next meta has priority over current, it will overwrite attributes. :param class or None meta: class with properties. :return class: merged meta.", "response": "def merge_metas(*metas):\n    \"\"\" Merge meta parameters.\n\n    next meta has priority over current, it will overwrite attributes.\n\n    :param class or None meta: class with properties.\n    :return class: merged meta.\n\n    \"\"\"\n    metadict = {}\n    for meta in metas:\n        metadict.update(meta.__dict__)\n\n    metadict = {k: v for k, v in metadict.items() if not k.startswith('__')}\n    return type('Meta', (object, ), metadict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_state(self, state, speed=None):\n        speed = speed or self.current_fan_speed()\n        if state:\n            desired_state = {\"powered\": state, \"mode\": speed}\n        else:\n            desired_state = {\"powered\": state}\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "response": "Set the state of the current object in the current state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_fan_direction(self, direction):\n        desired_state = {\"direction\": direction}\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "response": ":param direction: a string one of [\"forward\", \"reverse\"]\n        :return: nothing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the fan timer for the current device.", "response": "def set_fan_timer(self, timer):\n        \"\"\"\n        :param timer: an int between fan_timer_range\n        :return: nothing\n        \"\"\"\n        desired_state = {\"timer\": timer}\n\n        resp = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_state(self, state, speed=None):\n        desired_state = {\"powered\": state}\n        if state:\n            brightness = self._to_brightness.get(speed or self.current_fan_speed(), 0.33)\n            desired_state.update({'brightness': brightness})\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "response": "Set the state of the current object to state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a table containing data from a VCF file.", "response": "def fromvcf(filename, chrom=None, start=None, stop=None, samples=True):\n    \"\"\"\n    Returns a table providing access to data from a variant call file (VCF).\n    E.g.::\n\n        >>> import petl as etl\n        >>> # activate bio extensions\n        ... import petlx.bio\n        >>> table1 = etl.fromvcf('fixture/sample.vcf')\n        >>> table1.look(truncate=20)\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        | CHROM | POS     | ID          | REF | ALT    | QUAL | FILTER  | INFO                 | NA00001              | NA00002              | NA00003              |\n        +=======+=========+=============+=====+========+======+=========+======================+======================+======================+======================+\n        | '19'  |     111 | None        | 'A' | [C]    |  9.6 | None    | {}                   | Call(sample=NA00001, | Call(sample=NA00002, | Call(sample=NA00003, |\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        | '19'  |     112 | None        | 'A' | [G]    |   10 | None    | {}                   | Call(sample=NA00001, | Call(sample=NA00002, | Call(sample=NA00003, |\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        | '20'  |   14370 | 'rs6054257' | 'G' | [A]    |   29 | []      | {'DP': 14, 'H2': Tru | Call(sample=NA00001, | Call(sample=NA00002, | Call(sample=NA00003, |\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        | '20'  |   17330 | None        | 'T' | [A]    |    3 | ['q10'] | {'DP': 11, 'NS': 3,  | Call(sample=NA00001, | Call(sample=NA00002, | Call(sample=NA00003, |\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        | '20'  | 1110696 | 'rs6040355' | 'A' | [G, T] |   67 | []      | {'DP': 10, 'AA': 'T' | Call(sample=NA00001, | Call(sample=NA00002, | Call(sample=NA00003, |\n        +-------+---------+-------------+-----+--------+------+---------+----------------------+----------------------+----------------------+----------------------+\n        ...\n\n    \"\"\"\n\n    return VCFView(filename, chrom=chrom, start=start, stop=stop,\n                   samples=samples)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nunpack the INFO field into separate fields.", "response": "def vcfunpackinfo(table, *keys):\n    \"\"\"\n    Unpack the INFO field into separate fields. E.g.::\n\n        >>> import petl as etl\n        >>> # activate bio extensions\n        ... import petlx.bio\n        >>> table1 = (\n        ...     etl\n        ...     .fromvcf('fixture/sample.vcf', samples=None)\n        ...     .vcfunpackinfo()\n        ... )\n        >>> table1\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        | CHROM | POS     | ID          | REF | ALT    | QUAL | FILTER  | AA   | AC   | AF             | AN   | DB   | DP   | H2   | NS   |\n        +=======+=========+=============+=====+========+======+=========+======+======+================+======+======+======+======+======+\n        | '19'  |     111 | None        | 'A' | [C]    |  9.6 | None    | None | None | None           | None | None | None | None | None |\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        | '19'  |     112 | None        | 'A' | [G]    |   10 | None    | None | None | None           | None | None | None | None | None |\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        | '20'  |   14370 | 'rs6054257' | 'G' | [A]    |   29 | []      | None | None | [0.5]          | None | True |   14 | True |    3 |\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        | '20'  |   17330 | None        | 'T' | [A]    |    3 | ['q10'] | None | None | [0.017]        | None | None |   11 | None |    3 |\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        | '20'  | 1110696 | 'rs6040355' | 'A' | [G, T] |   67 | []      | 'T'  | None | [0.333, 0.667] | None | True |   10 | None |    2 |\n        +-------+---------+-------------+-----+--------+------+---------+------+------+----------------+------+------+------+------+------+\n        ...\n\n    \"\"\"\n\n    result = etl.unpackdict(table, 'INFO', keys=keys)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmelting the samples columns. E.g.:: >>> import petl as etl >>> # activate bio extensions ... import petlx.bio >>> table1 = ( ... etl ... .fromvcf('fixture/sample.vcf') ... .vcfmeltsamples() ... ) >>> table1 +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ | CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | SAMPLE | CALL | +=======+=====+======+=====+=====+======+========+======+===========+=====================================================+ | '19' | 111 | None | 'A' | [C] | 9.6 | None | {} | 'NA00001' | Call(sample=NA00001, CallData(GT=0|0, HQ=[10, 10])) | +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ | '19' | 111 | None | 'A' | [C] | 9.6 | None | {} | 'NA00002' | Call(sample=NA00002, CallData(GT=0|0, HQ=[10, 10])) | +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ | '19' | 111 | None | 'A' | [C] | 9.6 | None | {} | 'NA00003' | Call(sample=NA00003, CallData(GT=0/1, HQ=[3, 3])) | +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ | '19' | 112 | None | 'A' | [G] | 10 | None | {} | 'NA00001' | Call(sample=NA00001, CallData(GT=0|0, HQ=[10, 10])) | +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ | '19' | 112 | None | 'A' | [G] | 10 | None | {} | 'NA00002' | Call(sample=NA00002, CallData(GT=0|0, HQ=[10, 10])) | +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+ ...", "response": "def vcfmeltsamples(table, *samples):\n    \"\"\"\n    Melt the samples columns. E.g.::\n    \n        >>> import petl as etl\n        >>> # activate bio extensions\n        ... import petlx.bio\n        >>> table1 = (\n        ...     etl\n        ...     .fromvcf('fixture/sample.vcf')\n        ...     .vcfmeltsamples()\n        ... )\n        >>> table1\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        | CHROM | POS | ID   | REF | ALT | QUAL | FILTER | INFO | SAMPLE    | CALL                                                |\n        +=======+=====+======+=====+=====+======+========+======+===========+=====================================================+\n        | '19'  | 111 | None | 'A' | [C] |  9.6 | None   | {}   | 'NA00001' | Call(sample=NA00001, CallData(GT=0|0, HQ=[10, 10])) |\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        | '19'  | 111 | None | 'A' | [C] |  9.6 | None   | {}   | 'NA00002' | Call(sample=NA00002, CallData(GT=0|0, HQ=[10, 10])) |\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        | '19'  | 111 | None | 'A' | [C] |  9.6 | None   | {}   | 'NA00003' | Call(sample=NA00003, CallData(GT=0/1, HQ=[3, 3]))   |\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        | '19'  | 112 | None | 'A' | [G] |   10 | None   | {}   | 'NA00001' | Call(sample=NA00001, CallData(GT=0|0, HQ=[10, 10])) |\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        | '19'  | 112 | None | 'A' | [G] |   10 | None   | {}   | 'NA00002' | Call(sample=NA00002, CallData(GT=0|0, HQ=[10, 10])) |\n        +-------+-----+------+-----+-----+------+--------+------+-----------+-----------------------------------------------------+\n        ...\n\n    \"\"\"\n\n    result = etl.melt(table, key=VCF_HEADER, variables=samples,\n                      variablefield='SAMPLE', valuefield='CALL')\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vcfunpackcall(table, *keys):\n\n    result = (\n        etl.wrap(table)\n        .convert('CALL', lambda v: v.data._asdict())\n        .unpackdict('CALL', keys=keys)\n    )\n    return result", "response": "Unpack the call column. E. g."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a model by its name.", "response": "def get_model_by_name(model_name):\n    \"\"\" Get model by its name.\n\n    :param str model_name: name of model.\n    :return django.db.models.Model:\n\n    Example:\n        get_concrete_model_by_name('auth.User')\n        django.contrib.auth.models.User\n\n    \"\"\"\n    if isinstance(model_name, six.string_types) and \\\n            len(model_name.split('.')) == 2:\n        app_name, model_name = model_name.split('.')\n\n        if django.VERSION[:2] < (1, 8):\n            model = models.get_model(app_name, model_name)\n        else:\n            from django.apps import apps\n            model = apps.get_model(app_name, model_name)\n    else:\n        raise ValueError(\"{0} is not a Django model\".format(model_name))\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_model_name(model):\n    opts = model._meta\n    if django.VERSION[:2] < (1, 7):\n        model_name = opts.module_name\n    else:\n        model_name = opts.model_name\n\n    return model_name", "response": "Get the model name for the field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_app_cache(app_name):\n    loading_cache = django.db.models.loading.cache\n\n    if django.VERSION[:2] < (1, 7):\n        loading_cache.app_models[app_name].clear()\n    else:\n        loading_cache.all_models[app_name].clear()", "response": "Clear django cache for models."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_state(self, color_hex):\r\n        root_name = self.json_state.get('piggy_bank_id', self.name())\r\n        response = self.api_interface.set_device_state(self, {\r\n            \"nose_color\": color_hex\r\n        }, root_name)\r\n        self._update_state_from_response(response)", "response": "Set the state of the object to the desired state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeposits the given amount of cents in cents", "response": "def deposit(self, amount):\r\n        \"\"\"\r\n\r\n        :param amount: (int +/-) amount to be deposited or withdrawn in cents\r\n        \"\"\"\r\n        _json = {\"amount\": amount}\r\n        self.api_interface.piggy_bank_deposit(self, _json)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_sbc_config(self, config):\n        if (config.channel_mode == SBCChannelMode.CHANNEL_MODE_MONO):\n            self.config.mode = self.codec.SBC_MODE_MONO\n        elif (config.channel_mode == SBCChannelMode.CHANNEL_MODE_STEREO):\n            self.config.mode = self.codec.SBC_MODE_STEREO\n        elif (config.channel_mode == SBCChannelMode.CHANNEL_MODE_DUAL):\n            self.config.mode = self.codec.SBC_MODE_DUAL_CHANNEL\n        elif (config.channel_mode == SBCChannelMode.CHANNEL_MODE_JOINT_STEREO):\n            self.config.mode = self.codec.SBC_MODE_JOINT_STEREO\n\n        if (config.frequency == SBCSamplingFrequency.FREQ_16KHZ):\n            self.config.frequency = self.codec.SBC_FREQ_16000\n        elif (config.frequency == SBCSamplingFrequency.FREQ_32KHZ):\n            self.config.frequency = self.codec.SBC_FREQ_32000\n        elif (config.frequency == SBCSamplingFrequency.FREQ_44_1KHZ):\n            self.config.frequency = self.codec.SBC_FREQ_44100\n        elif (config.frequency == SBCSamplingFrequency.FREQ_48KHZ):\n            self.config.frequency = self.codec.SBC_FREQ_48000\n\n        if (config.allocation_method == SBCAllocationMethod.LOUDNESS):\n            self.config.allocation = self.codec.SBC_AM_LOUDNESS\n        elif (config.allocation_method == SBCAllocationMethod.SNR):\n            self.config.allocation = self.codec.SBC_AM_SNR\n\n        if (config.subbands == SBCSubbands.SUBBANDS_4):\n            self.config.subbands = self.codec.SBC_SB_4\n        elif (config.subbands == SBCSubbands.SUBBANDS_8):\n            self.config.subbands = self.codec.SBC_SB_8\n\n        if (config.block_length == SBCBlocks.BLOCKS_4):\n            self.config.blocks = self.codec.SBC_BLK_4\n        elif (config.block_length == SBCBlocks.BLOCKS_8):\n            self.config.blocks = self.codec.SBC_BLK_8\n        elif (config.block_length == SBCBlocks.BLOCKS_12):\n            self.config.blocks = self.codec.SBC_BLK_12\n        elif (config.block_length == SBCBlocks.BLOCKS_16):\n            self.config.blocks = self.codec.SBC_BLK_16\n\n        self.config.bitpool = config.max_bitpool\n        self.config.endian = self.codec.SBC_LE", "response": "Initializes the SB codec configuration from namedtuple config representation to\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(self, fd, mtu, data):\n        self.codec.rtp_sbc_encode_to_fd(self.config,\n                                        ffi.new('char[]',\n                                                data),\n                                        len(data),\n                                        mtu,\n                                        self.ts,\n                                        self.seq_num,\n                                        fd)", "response": "Encode the supplied data and write to the specified file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the media transport file descriptor depay the RTP payload and decode the SBC frames into a byte array.", "response": "def decode(self, fd, mtu, max_len=2560):\n        \"\"\"\n        Read the media transport descriptor, depay\n        the RTP payload and decode the SBC frames into\n        a byte array.  The maximum number of bytes to\n        be returned may be passed as an argument and all\n        available bytes are returned to the caller.\n\n        :param int fd: Media transport file descriptor\n        :param int mtu: Media transport MTU size as returned\n            when the media transport was acquired.\n        :param int max_len: Optional.  Set maximum number of\n            bytes to read.\n        :return data: Decoded data bytes as an array.\n        :rtype: array{byte}\n        \"\"\"\n        output_buffer = ffi.new('char[]', max_len)\n        sz = self.codec.rtp_sbc_decode_from_fd(self.config,\n                                               output_buffer,\n                                               max_len,\n                                               mtu,\n                                               fd)\n        return ffi.buffer(output_buffer[0:sz])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a GFF3 attributes string and return a dictionary.", "response": "def gff3_parse_attributes(attributes_string):\n    \"\"\"\n    Parse a string of GFF3 attributes ('key=value' pairs delimited by ';') \n    and return a dictionary.\n  \n    \"\"\"\n    \n    attributes = dict()\n    fields = attributes_string.split(';')\n    for f in fields:\n        if '=' in f:\n            key, value = f.split('=')\n            attributes[unquote_plus(key).strip()] = unquote_plus(value.strip())\n        elif len(f) > 0:\n            # not strictly kosher\n            attributes[unquote_plus(f).strip()] = True            \n    return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract feature rows from a GFF3 file.", "response": "def fromgff3(filename, region=None):\n    \"\"\"\n    Extract feature rows from a GFF3 file, e.g.::\n\n        >>> import petl as etl\n        >>> # activate bio extensions\n        ... import petlx.bio\n        >>> table1 = etl.fromgff3('fixture/sample.gff')\n        >>> table1.look(truncate=30)\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        | seqid        | source  | type          | start | end     | score | strand | phase | attributes                     |\n        +==============+=========+===============+=======+=========+=======+========+=======+================================+\n        | 'apidb|MAL1' | 'ApiDB' | 'supercontig' |     1 |  643292 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL2' | 'ApiDB' | 'supercontig' |     1 |  947102 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL3' | 'ApiDB' | 'supercontig' |     1 | 1060087 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL4' | 'ApiDB' | 'supercontig' |     1 | 1204112 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL5' | 'ApiDB' | 'supercontig' |     1 | 1343552 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+-------+---------+-------+--------+-------+--------------------------------+\n        ...\n\n    A region query string of the form '[seqid]' or '[seqid]:[start]-[end]'\n    may be given for the `region` argument. If given, requires the GFF3\n    file to be position sorted, bgzipped and tabix indexed. Requires pysam to be\n    installed. E.g.::\n\n        >>> # extract from a specific genome region via tabix\n        ... table2 = etl.fromgff3('fixture/sample.sorted.gff.gz',\n        ...                       region='apidb|MAL5:1289593-1289595')\n        >>> table2.look(truncate=30)\n        +--------------+---------+---------------+---------+---------+-------+--------+-------+--------------------------------+\n        | seqid        | source  | type          | start   | end     | score | strand | phase | attributes                     |\n        +==============+=========+===============+=========+=========+=======+========+=======+================================+\n        | 'apidb|MAL5' | 'ApiDB' | 'supercontig' |       1 | 1343552 | '.'   | '+'    | '.'   | {'localization': 'nuclear', 'o |\n        +--------------+---------+---------------+---------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL5' | 'ApiDB' | 'exon'        | 1289594 | 1291685 | '.'   | '+'    | '.'   | {'size': '2092', 'Parent': 'ap |\n        +--------------+---------+---------------+---------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL5' | 'ApiDB' | 'gene'        | 1289594 | 1291685 | '.'   | '+'    | '.'   | {'ID': 'apidb|MAL5_18S', 'web_ |\n        +--------------+---------+---------------+---------+---------+-------+--------+-------+--------------------------------+\n        | 'apidb|MAL5' | 'ApiDB' | 'rRNA'        | 1289594 | 1291685 | '.'   | '+'    | '.'   | {'ID': 'apidb|rna_MAL5_18S-1', |\n        +--------------+---------+---------------+---------+---------+-------+--------+-------+--------------------------------+\n\n    \"\"\"\n\n    if region is None:\n\n        # parse file as tab-delimited\n        table = etl.fromtsv(filename)\n\n    else:\n\n        # extract via tabix\n        table = etl.fromtabix(filename, region=region)\n\n    return (\n        table\n        .pushheader(GFF3_HEADER)\n        .skipcomments('#')\n        # ignore any row not 9 values long (e.g., trailing fasta)\n        .rowlenselect(9)\n        # parse attributes into a dict\n        .convert('attributes', gff3_parse_attributes)\n        # parse coordinates\n        .convert(('start', 'end'), int)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _transport_ready_handler(self, fd, cb_condition):\n        if(self.user_cb):\n            self.user_cb(self.user_arg)\n        return True", "response": "Wrapper for calling user callback routine to notify user when data is ready to read."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_transport_ready_event(self, user_cb, user_arg):\n        self.user_cb = user_cb\n        self.user_arg = user_arg", "response": "Register for transport ready events."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_transport(self):\n        if ('r' not in self.access_type):\n            raise BTIncompatibleTransportAccessType\n        return self.codec.decode(self.fd, self.read_mtu)", "response": "Read data from media transport."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting data to the media transport.", "response": "def write_transport(self, data):\n        \"\"\"\n        Write data to media transport.  The data is\n        encoded using the SBC codec and RTP encapsulated\n        before being written to the transport file\n        descriptor.\n\n        :param array{byte} data: Payload data to encode,\n            encapsulate and send.\n        \"\"\"\n        if ('w' not in self.access_type):\n            raise BTIncompatibleTransportAccessType\n        return self.codec.encode(self.fd, self.write_mtu, data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close_transport(self):\n        if (self.path):\n            self._release_media_transport(self.path,\n                                          self.access_type)\n            self.path = None", "response": "Forcibly close previously acquired media transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _acquire_media_transport(self, path, access_type):\n        transport = BTMediaTransport(path=path)\n        (fd, read_mtu, write_mtu) = transport.acquire(access_type)\n        self.fd = fd.take()   # We must do the clean-up later\n        self.write_mtu = write_mtu\n        self.read_mtu = read_mtu\n        self.access_type = access_type\n        self.path = path\n        self._install_transport_ready()", "response": "Acquire the media transport file descriptor and set the attributes of the current object to the values in the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreleases the media transport file descriptor and remove it from the cache.", "response": "def _release_media_transport(self, path, access_type):\n        \"\"\"\n        Should be called by subclass when it is finished\n        with the media transport file descriptor\n        \"\"\"\n        try:\n            self._uninstall_transport_ready()\n            os.close(self.fd)   # Clean-up previously taken fd\n            transport = BTMediaTransport(path=path)\n            transport.release(access_type)\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_config(config):\n        # The SBC config encoding is taken from a2dp_codecs.h, in particular,\n        # the a2dp_sbc_t type is converted into a 4-byte array:\n        #   uint8_t channel_mode:4\n        #   uint8_t frequency:4\n        #   uint8_t allocation_method:2\n        #   uint8_t subbands:2\n        #   uint8_t block_length:4\n        #   uint8_t min_bitpool\n        #   uint8_t max_bitpool\n        return dbus.Array([dbus.Byte(config.channel_mode |\n                                     (config.frequency << 4)),\n                           dbus.Byte(config.allocation_method |\n                                     (config.subbands << 2) |\n                                     (config.block_length << 4)),\n                           dbus.Byte(config.min_bitpool),\n                           dbus.Byte(config.max_bitpool)])", "response": "Helper function to turn the SBC codec configuration params into a2dp_sbc_t structure usable by bluez"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _property_change_event_handler(self, signal, transport, *args):\n        current_state = self.source.State\n        if (self.state == 'connected' and current_state == 'playing'):\n            self._acquire_media_transport(transport, 'r')\n        elif (self.state == 'playing' and current_state == 'connected'):\n            self._release_media_transport(transport, 'r')\n        self.state = current_state", "response": "This is the event handler for the property change event."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls by the endpoint when a new media transport is available", "response": "def _notify_media_transport_available(self, path, transport):\n        \"\"\"\n        Called by the endpoint when a new media transport is\n        available\n        \"\"\"\n        self.source = BTAudioSource(dev_path=path)\n        self.state = self.source.State\n        self.source.add_signal_receiver(self._property_change_event_handler,\n                                        BTAudioSource.SIGNAL_PROPERTY_CHANGED,  # noqa\n                                        transport)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _notify_media_transport_available(self, path, transport):\n        self.sink = BTAudioSink(dev_path=path)\n        self.state = self.sink.State\n        self.sink.add_signal_receiver(self._property_change_event_handler,\n                                      BTAudioSource.SIGNAL_PROPERTY_CHANGED,  # noqa\n                                      transport)", "response": "Called by the endpoint when a new media transport is available"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_operation_mode(self, mode):\n        if mode == \"off\":\n            desired_state = {\"powered\": False}\n        else:\n            desired_state = {\"powered\": True, \"mode\": mode}\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "response": ":param mode: a string one of self.modes()\n        :return: nothing"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_temperature(self, set_point):\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": {'set_point': set_point}\n        })\n\n        self._update_state_from_response(response)", "response": "Set the temperature of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the vhost mode of the resource.", "response": "def set_vacation_mode(self, state):\n        \"\"\"\n        :param state: a boolean of ture (on) or false ('off')\n        :return: nothing\n        \"\"\"\n        values = {\"desired_state\": {\"vacation_mode\": state}}\n        response = self.api_interface.local_set_state(self, values)\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort rows based on some key field or fields.", "response": "def sort(key=None, reverse=False, buffersize=None):\n    \"\"\"Sort rows based on some key field or fields. E.g.::\n\n        >>> from petlx.push import sort, tocsv\n        >>> p = sort('foo')\n        >>> p.pipe(tocsv('sorted_by_foo.csv'))\n        >>> p.push(sometable)\n\n    \"\"\"\n\n    return SortComponent(key=key, reverse=reverse, buffersize=buffersize)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the XY colour value of the last reading.", "response": "def color_xy(self):\n        \"\"\"\n        XY colour value: [float, float] or None\n        :rtype: list float\n        \"\"\"\n        color_x = self._last_reading.get('color_x')\n        color_y = self._last_reading.get('color_y')\n        if color_x is not None and color_y is not None:\n            return [float(color_x), float(color_y)]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_state(self, state, brightness=None,\n                  color_kelvin=None, color_xy=None,\n                  color_hue_saturation=None):\n        \"\"\"\n        :param state:   a boolean of true (on) or false ('off')\n        :param brightness: a float from 0 to 1 to set the brightness of\n         this bulb\n        :param color_kelvin: an integer greater than 0 which is a color in\n         degrees Kelvin\n        :param color_xy: a pair of floats in a list which specify the desired\n        CIE 1931 x,y color coordinates\n        :param color_hue_saturation: a pair of floats in a list which specify\n        the desired hue and saturation in that order.  Brightness can be\n        supplied via the brightness param\n        :return: nothing\n        \"\"\"\n        desired_state = {\"powered\": state}\n\n        color_state = self._format_color_data(color_hue_saturation, color_kelvin, color_xy)\n        if color_state is not None:\n            desired_state.update(color_state)\n\n        if brightness is not None:\n            desired_state.update({'brightness': brightness})\n\n        response = self.api_interface.local_set_state(self, {\n            \"desired_state\": desired_state\n        })\n        self._update_state_from_response(response)", "response": "Set the state of the bulb in the state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_tare(self, tare):\n        response = self.api_interface.set_device_state(self, {\"tare\": tare})\n        self._update_state_from_response(response)", "response": "set the tank weight of tank as printed on the main device"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_alarm_sensitivity(self, mode):\r\n        values = {\"desired_state\": {\"alarm_sensitivity\": mode}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)", "response": "Set the alarm sensitivity of the device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the alarm mode of the device.", "response": "def set_alarm_mode(self, mode):\r\n        \"\"\"\r\n        :param mode: one of [None, \"activity\", \"tamper\", \"forced_entry\"]\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\"desired_state\": {\"alarm_mode\": mode}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_alarm_state(self, state):\r\n        values = {\"desired_state\": {\"alarm_enabled\": state}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)", "response": "Sets the state of the alarm for the current device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the status of the related resources in the virtual acation mode.", "response": "def set_vacation_mode(self, state):\r\n        \"\"\"\r\n        :param state: a boolean of ture (on) or false ('off')\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\"desired_state\": {\"vacation_mode_enabled\": state}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_beeper_mode(self, state):\r\n        values = {\"desired_state\": {\"beeper_enabled\": state}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)", "response": ":param state: a boolean of ture (on) or false ('off')\r\n        :return: nothing"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the state of the current object.", "response": "def set_state(self, state):\r\n        \"\"\"\r\n        :param state:   a boolean of true (on) or false ('off')\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\"desired_state\": {\"locked\": state}}\r\n        response = self.api_interface.local_set_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_new_key(self, code, name):\r\n        device_json = {\"code\": code, \"name\": name}\r\n        return self.api_interface.create_lock_key(self, device_json)", "response": "Add a new user key code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new object path for a remote device. This method will connect to the remote device and retrieve all SDP records and then initiate the pairing. If a previously :py:meth:`create_device` was used successfully, this method will only initiate the pairing. Compared to :py:meth:`create_device` this method will fail if the pairing already exists, but not if the object path already has been created. This allows applications to use :py:meth:`create_device` first and then, if needed, use :py:meth:`create_paired_device` to initiate pairing. The agent object path is assumed to reside within the process (D-Bus connection instance) that calls this method. No separate registration procedure is needed for it and it gets automatically released once the pairing operation is complete. :param str dev_id: New device MAC address create e.g., '11:22:33:44:55:66' :param str agent_path: Path used when creating the bluetooth agent e.g., '/test/agent' :param str capability: Pairing agent capability e.g., 'DisplayYesNo', etc :param func cb_notify_device: Callback on success. The callback is called with the new device's object path as an argument. :param func cb_notify_error: Callback on error. The callback is called with the error reason. :return: :raises dbus.Exception: org.bluez.Error.InvalidArguments :raises dbus.Exception: org.bluez.Error.Failed", "response": "def create_paired_device(self, dev_id, agent_path,\n                             capability, cb_notify_device, cb_notify_error):\n        \"\"\"\n        Creates a new object path for a remote device. This\n        method will connect to the remote device and retrieve\n        all SDP records and then initiate the pairing.\n\n        If a previously :py:meth:`create_device` was used\n        successfully, this method will only initiate the pairing.\n\n        Compared to :py:meth:`create_device` this method will\n        fail if the pairing already exists, but not if the object\n        path already has been created. This allows applications\n        to use :py:meth:`create_device` first and then, if needed,\n        use :py:meth:`create_paired_device` to initiate pairing.\n\n        The agent object path is assumed to reside within the\n        process (D-Bus connection instance) that calls this\n        method. No separate registration procedure is needed\n        for it and it gets automatically released once the\n        pairing operation is complete.\n\n        :param str dev_id: New device MAC address create\n            e.g., '11:22:33:44:55:66'\n        :param str agent_path: Path used when creating the\n            bluetooth agent e.g., '/test/agent'\n        :param str capability: Pairing agent capability\n            e.g., 'DisplayYesNo', etc\n        :param func cb_notify_device: Callback on success.  The\n            callback is called with the new device's object\n            path as an argument.\n        :param func cb_notify_error: Callback on error.  The\n            callback is called with the error reason.\n        :return:\n        :raises dbus.Exception: org.bluez.Error.InvalidArguments\n        :raises dbus.Exception: org.bluez.Error.Failed\n        \"\"\"\n        return self._interface.CreatePairedDevice(dev_id,\n                                                  agent_path,\n                                                  capability,\n                                                  reply_handler=cb_notify_device,  # noqa\n                                                  error_handler=cb_notify_error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _visit_body(self, node):\n        if (node.body and isinstance(node.body[0], ast.Expr) and\n                self.is_base_string(node.body[0].value)):\n            node.body[0].value.is_docstring = True\n            self.visit(node.body[0].value)\n\n        for sub_node in node.body:\n            self.visit(sub_node)", "response": "Traverse the body of the node manually."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate state with latest info from Wink API.", "response": "def update_state(self):\n        \"\"\"\n        Update state with latest info from Wink API.\n        \"\"\"\n        response = self.api_interface.get_device_state(self, type_override=\"button\")\n        return self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup(self, domain, get_last_full_query=True):\n\n        last_full_builtwith_scan_date = None\n\n        if self.api_version == 7 and isinstance(domain, list):\n            domain = ','.join(domain)\n\n        if self.api_version in [2, 7]:\n            last_updates_resp = requests.get(ENDPOINTS_BY_API_VERSION[self.api_version], params={'UPDATE': 1})\n            last_updated_data = last_updates_resp.json()\n\n            if get_last_full_query and last_updated_data['FULL']:\n              last_full_builtwith_scan_date = datetime.datetime.strptime(last_updated_data['FULL'], '%Y-%m-%d').date()\n\n        params = {\n            'KEY': self.key,\n            'LOOKUP': domain,\n        }\n\n        response = requests.get(ENDPOINTS_BY_API_VERSION[self.api_version], params=params)\n\n        if self.api_version == 1:\n            return response.json()\n        elif self.api_version == 2:\n            return BuiltWithDomainInfo(response.json(), last_full_builtwith_scan_date)\n        elif self.api_version == 7:\n            domain_info = list()\n            for result in response.json()['Results']:\n                domain_info.append(BuiltWithDomainInfo(result['Result'], last_full_builtwith_scan_date))\n            return domain_info", "response": "Lookup BuiltWith results for the given domain."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the schedule enabled state of the resource.", "response": "def set_schedule_enabled(self, state):\n        \"\"\"\n        :param state: a boolean True (on) or False (off)\n        :return: nothing\n        \"\"\"\n        desired_state = {\"schedule_enabled\": state}\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ac_fan_speed(self, speed):\n        desired_state = {\"fan_speed\": speed}\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "response": "Set the fan speed of the ac - related resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the temperature of the current device.", "response": "def set_temperature(self, max_set_point=None):\n        \"\"\"\n        :param max_set_point: a float for the max set point value in celsius\n        :return: nothing\n        \"\"\"\n        desired_state = {}\n\n        if max_set_point:\n            desired_state['max_set_point'] = max_set_point\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(self, resource=None, **kwargs):\n        if resource is None:\n            def wrapper(resource):\n                return self.register(resource, **kwargs)\n            return wrapper\n\n        for key, value in kwargs.items():\n            setattr(resource.Meta, key, value)\n\n        if resource.Meta.name in self.resource_map:\n            raise ValueError('Resource {} already registered'.format(\n                resource.Meta.name))\n\n        if resource.Meta.name_plural in self.resource_map:\n            raise ValueError(\n                'Resource plural name {} conflicts with registered resource'.\n                format(resource.Meta.name))\n\n        resource_plural_names = {\n            r.Meta.name_plural for r in self.resource_map.values()\n        }\n        if resource.Meta.name in resource_plural_names:\n            raise ValueError(\n                'Resource name {} conflicts with other resource plural name'.\n                format(resource.Meta.name)\n            )\n\n        resource.Meta.api = self\n        self._resources.append(resource)\n        return resource", "response": "Register resource for currnet API."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all of the api endpoints.", "response": "def urls(self):\n        \"\"\" Get all of the api endpoints.\n\n        NOTE: only for django as of now.\n        NOTE: urlpatterns are deprecated since Django1.8\n\n        :return list: urls\n\n        \"\"\"\n        from django.conf.urls import url\n        urls = [\n            url(r'^$', self.documentation),\n            url(r'^map$', self.map_view),\n        ]\n\n        for resource_name in self.resource_map:\n            urls.extend([\n                url(r'(?P<resource_name>{})$'.format(\n                    resource_name), self.handler_view),\n                url(r'(?P<resource_name>{})/(?P<ids>[\\w\\-\\,]+)$'.format(\n                    resource_name), self.handler_view),\n            ])\n\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_urls(self, request, resource_name=None, ids=None):\n        http_host = request.META.get('HTTP_HOST', None)\n\n        if http_host is None:\n            http_host = request.META['SERVER_NAME']\n            if request.META['SERVER_PORT'] not in ('80', '443'):\n                http_host = \"{}:{}\".format(\n                    http_host, request.META['SERVER_PORT'])\n\n        self.base_url = \"{}://{}\".format(\n            request.META['wsgi.url_scheme'],\n            http_host\n        )\n        self.api_url = \"{}{}\".format(self.base_url, request.path)\n        self.api_url = self.api_url.rstrip(\"/\")\n\n        if ids is not None:\n            self.api_url = self.api_url.rsplit(\"/\", 1)[0]\n\n        if resource_name is not None:\n            self.api_url = self.api_url.rsplit(\"/\", 1)[0]", "response": "Update url configuration.\n\n        :param request:\n        :param resource_name:\n        :type resource_name: str or None\n        :param ids:\n        :rtype: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map_view(self, request):\n        self.update_urls(request)\n        resource_info = {\n            \"resources\": [{\n                \"id\": index + 1,\n                \"href\": \"{}/{}\".format(self.api_url, resource_name),\n            } for index, (resource_name, resource) in enumerate(\n                sorted(self.resource_map.items()))\n                if not resource.Meta.authenticators or\n                resource.authenticate(request) is not None\n            ]\n        }\n        response = json.dumps(resource_info)\n        return HttpResponse(response, content_type=\"application/vnd.api+json\")", "response": "Show information about available resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef documentation(self, request):\n        self.update_urls(request)\n        context = {\n            \"resources\": sorted(self.resource_map.items())\n        }\n        return render(request, \"jsonapi/index.html\", context)", "response": "Return a list of all the resource names that are available in the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the view of the resource.", "response": "def handler_view(self, request, resource_name, ids=None):\n        \"\"\" Handler for resources.\n\n        .. versionadded:: 0.5.7\n            Content-Type check\n\n        :return django.http.HttpResponse\n\n        \"\"\"\n        signal_request.send(sender=self, request=request)\n        time_start = time.time()\n        self.update_urls(request, resource_name=resource_name, ids=ids)\n        resource = self.resource_map[resource_name]\n\n        allowed_http_methods = resource.Meta.allowed_methods\n        if request.method not in allowed_http_methods:\n            response = HttpResponseNotAllowed(\n                permitted_methods=allowed_http_methods)\n            signal_response.send(\n                sender=self, request=request, response=response,\n                duration=time.time() - time_start)\n            return response\n\n        if resource.Meta.authenticators and not (\n                request.method == \"GET\" and\n                resource.Meta.disable_get_authentication):\n            user = resource.authenticate(request)\n            if user is None or not user.is_authenticated():\n                response = HttpResponse(\"Not Authenticated\", status=401)\n                signal_response.send(\n                    sender=self, request=request, response=response,\n                    duration=time.time() - time_start)\n                return response\n\n        kwargs = dict(request=request)\n        if ids is not None:\n            kwargs['ids'] = ids.split(\",\")\n\n        try:\n            if request.method == \"GET\":\n                response = self.handler_view_get(resource, **kwargs)\n            elif request.method == \"POST\":\n                response = self.handler_view_post(resource, **kwargs)\n            elif request.method == \"PUT\":\n                response = self.handler_view_put(resource, **kwargs)\n            elif request.method == \"DELETE\":\n                response = self.handler_view_delete(resource, **kwargs)\n        except JSONAPIError as e:\n            response = HttpResponse(\n                json.dumps({\"errors\": [e.data]}, cls=DatetimeDecimalEncoder),\n                content_type=self.CONTENT_TYPE, status=e.status)\n\n        signal_response.send(sender=self, request=request, response=response,\n                             duration=time.time() - time_start)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef away(self):\n        nest = self._last_reading.get('users_away', None)\n        ecobee = self.profile()\n        if nest is not None:\n            return nest\n        if ecobee is not None:\n            if ecobee == \"home\":\n                return False\n            return True\n        return None", "response": "Returns the unique ID of the user that is away or home."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the ical string and return a datetime object and the list of days.", "response": "def _parse_ical_string(ical_string):\r\n    \"\"\"\r\n    SU,MO,TU,WE,TH,FR,SA\r\n    DTSTART;TZID=America/New_York:20180804T233251\\nRRULE:FREQ=WEEKLY;BYDAY=SA\r\n    DTSTART;TZID=America/New_York:20180804T233251\\nRRULE:FREQ=DAILY\r\n    DTSTART;TZID=America/New_York:20180804T233251\\nRRULE:FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR,SA\r\n    DTSTART;TZID=America/New_York:20180718T174500\r\n    \"\"\"\r\n    start_time = ical_string.splitlines()[0].replace(DTSTART, '')\r\n    if \"RRULE\" in ical_string:\r\n        days = ical_string.splitlines()[1].replace(REPEAT, '')\r\n        if days == \"RRULE:FREQ=DAILY\":\r\n            days = ['DAILY']\r\n        else:\r\n            days = days.split(',')\r\n    else:\r\n        days = None\r\n    start_time = start_time.splitlines()[0].split(':')[1]\r\n    datetime_object = datetime.strptime(start_time, '%Y%m%dT%H%M%S')\r\n    return datetime_object, days"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the dials of a specific channel.", "response": "def set_dial(self, json_value, index, timezone=None):\r\n        \"\"\"\r\n        :param json_value: The value to set\r\n        :param index: The dials index\r\n        :param timezone: The time zone to use for a time dial\r\n        :return:\r\n        \"\"\"\r\n\r\n        values = self.json_state\r\n        values[\"nonce\"] = str(random.randint(0, 1000000000))\r\n        if timezone is None:\r\n            json_value[\"channel_configuration\"] = {\"channel_id\": \"10\"}\r\n            values[\"dials\"][index] = json_value\r\n            response = self.api_interface.set_device_state(self, values)\r\n        else:\r\n            json_value[\"channel_configuration\"] = {\"channel_id\": \"1\", \"timezone\": timezone}\r\n            values[\"dials\"][index] = json_value\r\n            response = self.api_interface.set_device_state(self, values)\r\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_state(self):\r\n        response = self.api_interface.get_device_state(self, id_override=self.parent.object_id(),\r\n                                                       type_override=self.parent.object_type())\r\n        self._update_state_from_response(response)", "response": "Update state with latest info from Wink API."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the state of the object based on the response json.", "response": "def _update_state_from_response(self, response_json):\r\n        \"\"\"\r\n        :param response_json: the json obj returned from query\r\n        :return:\r\n        \"\"\"\r\n        if 'data' in response_json and response_json['data']['object_type'] == \"cloud_clock\":\r\n            cloud_clock = response_json.get('data')\r\n            if cloud_clock is None:\r\n                return False\r\n\r\n            alarms = cloud_clock.get('alarms')\r\n            for alarm in alarms:\r\n                if alarm.get('object_id') == self.object_id():\r\n                    self.json_state = alarm\r\n                    return True\r\n            return False\r\n        if 'data' in response_json:\r\n            alarm = response_json.get('data')\r\n            self.json_state = alarm\r\n            return True\r\n        self.json_state = response_json\r\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_recurrence(self, date, days=None):\r\n        if self.parent.get_time_dial() is None:\r\n            _LOGGER.error(\"Not setting alarm, no time dial.\")\r\n            return False\r\n        timezone_string = self.parent.get_time_dial()[\"channel_configuration\"][\"timezone\"]\r\n        ical_string = _create_ical_string(timezone_string, date, days)\r\n        _json = {'recurrence': ical_string, 'enabled': True}\r\n\r\n        self.api_interface.set_device_state(self, _json)\r\n        return True", "response": "Set the recurrence of the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_state(self):\r\n        response = self.api_interface.get_device_state(self, id_override=self.parent_id(),\r\n                                                       type_override=self.parent_object_type())\r\n        self._update_state_from_response(response)", "response": "Update state with latest info from Wink API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the state of the object based on the response json.", "response": "def _update_state_from_response(self, response_json):\r\n        \"\"\"\r\n        :param response_json: the json obj returned from query\r\n        :return:\r\n        \"\"\"\r\n        if response_json.get('data') is not None:\r\n            cloud_clock = response_json.get('data')\r\n        else:\r\n            cloud_clock = response_json\r\n        self.parent.json_state = cloud_clock\r\n\r\n        cloud_clock_last_reading = cloud_clock.get('last_reading')\r\n        dials = cloud_clock.get('dials')\r\n        for dial in dials:\r\n            if dial.get('object_id') == self.object_id():\r\n                dial['connection'] = cloud_clock_last_reading.get('connection')\r\n                self.json_state = dial\r\n                return True\r\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_configuration(self, min_value, max_value, rotation=\"cw\", scale=\"linear\", ticks=12, min_position=0,\r\n                          max_position=360):\r\n        \"\"\"\r\n\r\n        :param min_value: Any number\r\n        :param max_value: Any number above min_value\r\n        :param rotation: (String) cw or ccw\r\n        :param scale: (String) linear or log\r\n        :param ticks:(Int) number of ticks of the clock up to 360?\r\n        :param min_position: (Int) 0-360\r\n        :param max_position: (Int) 0-360\r\n        :return:\r\n        \"\"\"\r\n\r\n        _json = {\"min_value\": min_value, \"max_value\": max_value, \"rotation\": rotation, \"scale_type\": scale,\r\n                 \"num_ticks\": ticks, \"min_position\": min_position, \"max_position\": max_position}\r\n\r\n        dial_config = {\"dial_configuration\": _json}\r\n\r\n        self._update_state_from_response(self.parent.set_dial(dial_config, self.index()))", "response": "Set the dial configuration of the master node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the state of the most recent version of the user.", "response": "def set_state(self, value, labels=None):\r\n        \"\"\"\r\n\r\n        :param value: Any number\r\n        :param labels: A list of two Strings sending None won't change the current values.\r\n        :return:\r\n        \"\"\"\r\n\r\n        values = {\"value\": value}\r\n        json_labels = []\r\n        if labels:\r\n            for label in labels:\r\n                json_labels.append(str(label).upper())\r\n            values[\"labels\"] = json_labels\r\n\r\n        self._update_state_from_response(self.parent.set_dial(values, self.index()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_time_dial(self, timezone_string):\r\n        self._update_state_from_response(self.parent.set_dial({}, self.index(), timezone_string))", "response": "Make the time dial for the current session"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromtabix(filename, reference=None, start=None, stop=None, region=None,\n              header=None):\n    \"\"\"\n    Extract rows from a tabix indexed file, e.g.::\n\n        >>> import petl as etl\n        >>> # activate bio extensions\n        ... import petlx.bio\n        >>> table1 = etl.fromtabix('fixture/test.bed.gz',\n        ...                        region='Pf3D7_02_v3')\n        >>> table1\n        +---------------+----------+----------+-----------------------------+\n        | #chrom        | start    | end      | region                      |\n        +===============+==========+==========+=============================+\n        | 'Pf3D7_02_v3' | '0'      | '23100'  | 'SubtelomericRepeat'        |\n        +---------------+----------+----------+-----------------------------+\n        | 'Pf3D7_02_v3' | '23100'  | '105800' | 'SubtelomericHypervariable' |\n        +---------------+----------+----------+-----------------------------+\n        | 'Pf3D7_02_v3' | '105800' | '447300' | 'Core'                      |\n        +---------------+----------+----------+-----------------------------+\n        | 'Pf3D7_02_v3' | '447300' | '450450' | 'Centromere'                |\n        +---------------+----------+----------+-----------------------------+\n        | 'Pf3D7_02_v3' | '450450' | '862500' | 'Core'                      |\n        +---------------+----------+----------+-----------------------------+\n        ...\n\n        >>> table2 = etl.fromtabix('fixture/test.bed.gz',\n        ...                        region='Pf3D7_02_v3:110000-120000')\n        >>> table2\n        +---------------+----------+----------+--------+\n        | #chrom        | start    | end      | region |\n        +===============+==========+==========+========+\n        | 'Pf3D7_02_v3' | '105800' | '447300' | 'Core' |\n        +---------------+----------+----------+--------+\n\n    \"\"\"\n    \n    return TabixView(filename, reference, start, stop, region, header)", "response": "Extract rows from a tabix indexed file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new device pairing mode and duration.", "response": "def pair_new_device(self, pairing_mode, pairing_mode_duration=60, pairing_device_type_selector=None,\r\n                        kidde_radio_code=None):\r\n        \"\"\"\r\n        :param pairing_mode: a string one of [\"zigbee\", \"zwave\", \"zwave_exclusion\",\r\n            \"zwave_network_rediscovery\", \"lutron\", \"bluetooth\", \"kidde\"]\r\n        :param pairing_mode_duration: an int in seconds defaults to 60\r\n        :param pairing_device_type_selector: a string I believe this is only for bluetooth devices.\r\n        :param kidde_radio_code: a string of 8 1s and 0s one for each dip switch on the kidde device\r\n            left --> right = 1 --> 8\r\n        :return: nothing\r\n        \"\"\"\r\n        if pairing_mode == \"lutron\" and pairing_mode_duration < 120:\r\n            pairing_mode_duration = 120\r\n        elif pairing_mode == \"zwave_network_rediscovery\":\r\n            pairing_mode_duration = 0\r\n        elif pairing_mode == \"bluetooth\" and pairing_device_type_selector is None:\r\n            pairing_device_type_selector = \"switchmate\"\r\n\r\n        desired_state = {\"pairing_mode\": pairing_mode,\r\n                         \"pairing_mode_duration\": pairing_mode_duration}\r\n\r\n        if pairing_mode == \"kidde\" and kidde_radio_code is not None:\r\n            # Convert dip switch 1 and 0s to an int\r\n            try:\r\n                kidde_radio_code_int = int(kidde_radio_code, 2)\r\n                desired_state = {\"kidde_radio_code\": kidde_radio_code_int, \"pairing_mode\": None}\r\n            except (TypeError, ValueError):\r\n                _LOGGER.error(\"An invalid Kidde radio code was provided. %s\", kidde_radio_code)\r\n\r\n        if pairing_device_type_selector is not None:\r\n            desired_state.update({\"pairing_device_type_selector\": pairing_device_type_selector})\r\n\r\n        response = self.api_interface.set_device_state(self, {\r\n            \"desired_state\": desired_state\r\n        })\r\n\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cipher(self):\n        # If no offset is selected, pick random one with sufficient distance\n        # from original.\n        if self.offset is False:\n            self.offset = randrange(5, 25)\n            logging.info(\"Random offset selected: {0}\".format(self.offset))\n        logging.debug(\"Offset set: {0}\".format(self.offset))\n\n        # Cipher\n        ciphered_message_list = list(self.message)\n        for i, letter in enumerate(ciphered_message_list):\n            if letter.isalpha():\n                # Use default upper and lower case characters if alphabet\n                # not supplied by user.\n                if letter.isupper():\n                    alphabet = [character.upper()\n                                for character in self.alphabet]\n                else:\n                    alphabet = self.alphabet\n\n                logging.debug(\"Letter: {0}\".format(letter))\n                logging.debug(\"Alphabet: {0}\".format(alphabet))\n                value = alphabet.index(letter)\n                cipher_value = value + self.offset\n                if cipher_value > 25 or cipher_value < 0:\n                    cipher_value = cipher_value % 26\n                logging.debug(\"Cipher value: {0}\".format(cipher_value))\n                ciphered_message_list[i] = alphabet[cipher_value]\n                logging.debug(\"Ciphered letter: {0}\".format(letter))\n        self.message = ''.join(ciphered_message_list)\n        return self.message", "response": "Applies the Caesar shift cipher to the message attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the entropy of a string based on the known frequency of the entry in the dictionary.", "response": "def calculate_entropy(self, entropy_string):\n        \"\"\"Calculates the entropy of a string based on known frequency of\n        English letters.\n\n        Args:\n            entropy_string: A str representing the string to calculate.\n\n        Returns:\n            A negative float with the total entropy of the string (higher\n            is better).\n        \"\"\"\n        total = 0\n        for char in entropy_string:\n            if char.isalpha():\n                prob = self.frequency[char.lower()]\n                total += - math.log(prob) / math.log(2)\n        logging.debug(\"Entropy score: {0}\".format(total))\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cracked(self):\n        logging.info(\"Cracking message: {0}\".format(self.message))\n        entropy_values = {}\n        attempt_cache = {}\n        message = self.message\n        for i in range(25):\n            self.message = message\n            self.offset = i * -1\n            logging.debug(\"Attempting crack with offset: \"\n                          \"{0}\".format(self.offset))\n            test_cipher = self.cipher()\n            logging.debug(\"Attempting plaintext: {0}\".format(test_cipher))\n            entropy_values[i] = self.calculate_entropy(test_cipher)\n            attempt_cache[i] = test_cipher\n\n        sorted_by_entropy = sorted(entropy_values, key=entropy_values.get)\n        self.offset = sorted_by_entropy[0] * -1\n        cracked_text = attempt_cache[sorted_by_entropy[0]]\n        self.message = cracked_text\n\n        logging.debug(\"Entropy scores: {0}\".format(entropy_values))\n        logging.debug(\"Lowest entropy score: \"\n                      \"{0}\".format(str(entropy_values[sorted_by_entropy[0]])))\n        logging.debug(\"Most likely offset: {0}\".format(self.offset))\n        logging.debug(\"Most likely message: {0}\".format(cracked_text))\n\n        return cracked_text", "response": "Attempts to crack ciphertext using frequency of letters in English."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding message using Caesar shift cipher Inverse operation of encoding applies negative offset to Caesar shift cipher Returns a string decoded with the message", "response": "def decoded(self):\n        \"\"\"Decodes message using Caesar shift cipher\n\n        Inverse operation of encoding, applies negative offset to Caesar shift\n        cipher.\n\n        Returns:\n            String decoded with cipher.\n        \"\"\"\n        logging.info(\"Decoding message: {0}\".format(self.message))\n        self.offset = self.offset * -1\n        return self.cipher()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(cls, querydict):\n        for key in querydict.keys():\n            if not any((key in JSONAPIQueryDict._fields,\n                        cls.RE_FIELDS.match(key))):\n\n                msg = \"Query parameter {} is not known\".format(key)\n                raise ValueError(msg)\n\n        result = JSONAPIQueryDict(\n            distinct=cls.prepare_values(querydict.getlist('distinct')),\n            fields=cls.parse_fields(querydict),\n            filter=querydict.getlist('filter'),\n            include=cls.prepare_values(querydict.getlist('include')),\n            page=int(querydict.get('page')) if querydict.get('page') else None,\n            sort=cls.prepare_values(querydict.getlist('sort'))\n        )\n\n        return result", "response": "Parse the querydict data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the state of the device.", "response": "def set_state(self, state):\r\n        \"\"\"\r\n        :param state:   a boolean of true (on) or false ('off')\r\n        :return: nothing\r\n        \"\"\"\r\n        desired_state = {\"desired_state\": {\"powered\": state}}\r\n        response = self.api_interface.set_device_state(self, desired_state)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_state_from_response(self, response_json):\r\n        power_strip = response_json.get('data')\r\n\r\n        power_strip_reading = power_strip.get('last_reading')\r\n        outlets = power_strip.get('outlets')\r\n        for outlet in outlets:\r\n            if outlet.get('outlet_id') == str(self.object_id()):\r\n                outlet['last_reading']['connection'] = power_strip_reading.get('connection')\r\n                self.json_state = outlet", "response": "Update the state of the object based on the response json."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_state(self, state):\r\n        if self.index() == 0:\r\n            values = {\"outlets\": [{\"desired_state\": {\"powered\": state}}, {}]}\r\n        else:\r\n            values = {\"outlets\": [{}, {\"desired_state\": {\"powered\": state}}]}\r\n\r\n        response = self.api_interface.set_device_state(self, values, id_override=self.parent_id(),\r\n                                                       type_override=\"powerstrip\")\r\n        self._update_state_from_response(response)", "response": "Sets the state of the current user in the outlet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the state of the current binary entry.", "response": "def set_state(self, state):\r\n        \"\"\"\r\n        :param state:   a boolean of true (on) or false ('off')\r\n        :return: nothing\r\n        \"\"\"\r\n        _field = self.binary_state_name()\r\n        values = {\"desired_state\": {_field: state}}\r\n        response = self.api_interface.local_set_state(self, values, type_override=\"binary_switche\")\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the name of the binary state field that is used in the device.", "response": "def binary_state_name(self):\r\n        \"\"\"\r\n        Search all of the capabilities of the device and return the supported binary state field.\r\n        Default to returning powered.\r\n        \"\"\"\r\n        return_field = \"powered\"\r\n        _capabilities = self.json_state.get('capabilities')\r\n        if _capabilities is not None:\r\n            _fields = _capabilities.get('fields')\r\n            if _fields is not None:\r\n                for field in _fields:\r\n                    if field.get('field') in SUPPORTED_BINARY_STATE_FIELDS:\r\n                        return_field = field.get('field')\r\n        return return_field"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates state with latest info from Wink API.", "response": "def update_state(self):\r\n        \"\"\"\r\n        Update state with latest info from Wink API.\r\n        \"\"\"\r\n        response = self.api_interface.local_get_state(self, type_override=\"binary_switche\")\r\n        return self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the siren volume of the device.", "response": "def set_siren_volume(self, volume):\r\n        \"\"\"\r\n        :param volume: one of [low, medium, high]\r\n        \"\"\"\r\n        values = {\r\n            \"desired_state\": {\r\n                \"siren_volume\": volume\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_chime_volume(self, volume):\r\n        values = {\r\n            \"desired_state\": {\r\n                \"chime_volume\": volume\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)", "response": "Set the current chime volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the state of the siren strobe.", "response": "def set_siren_strobe_enabled(self, enabled):\r\n        \"\"\"\r\n        :param enabled:  True or False\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\r\n            \"desired_state\": {\r\n                \"strobe_enabled\": enabled\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the state of the most recent Chime - Stebe.", "response": "def set_chime_strobe_enabled(self, enabled):\r\n        \"\"\"\r\n        :param enabled:  True or False\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\r\n            \"desired_state\": {\r\n                \"chime_strobe_enabled\": enabled\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the siren sound for the current user.", "response": "def set_siren_sound(self, sound):\r\n        \"\"\"\r\n        :param sound: a str, one of [\"doorbell\", \"fur_elise\", \"doorbell_extended\", \"alert\",\r\n                                     \"william_tell\", \"rondo_alla_turca\", \"police_siren\",\r\n                                     \"\"evacuation\", \"beep_beep\", \"beep\"]\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\r\n            \"desired_state\": {\r\n                \"siren_sound\": sound\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_chime(self, sound, cycles=None):\r\n        desired_state = {\"activate_chime\": sound}\r\n        if cycles is not None:\r\n            desired_state.update({\"chime_cycles\": cycles})\r\n        response = self.api_interface.set_device_state(self,\r\n                                                       {\"desired_state\": desired_state})\r\n        self._update_state_from_response(response)", "response": "Set the current Chime of the device."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets auto shutoff for the related user.", "response": "def set_auto_shutoff(self, timer):\r\n        \"\"\"\r\n        :param timer: an int, one of [None (never), -1, 30, 60, 120]\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\r\n            \"desired_state\": {\r\n                \"auto_shutoff\": timer\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the state of the device.", "response": "def set_state(self, state):\r\n        \"\"\"\r\n        :param state:   a boolean of true (on) or false ('off')\r\n        :return: nothing\r\n        \"\"\"\r\n        values = {\"desired_state\": {\"powered\": state}}\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_netflux(flux):\n    netflux = flux - np.transpose(flux)\n    \"\"\"Set negative fluxes to zero\"\"\"\n    ind = (netflux < 0.0)\n    netflux[ind] = 0.0\n    return netflux", "response": "r Compute the netflux from the gross flux."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coarsegrain(F, sets):\n    nnew = len(sets)\n    Fc = np.zeros((nnew, nnew))\n    for i in range(0, nnew - 1):\n        for j in range(i + 1, nnew):\n            I = list(sets[i])\n            J = list(sets[j])\n            Fc[i, j] = np.sum(F[I, :][:, J])\n            Fc[j, i] = np.sum(F[J, :][:, I])\n    return Fc", "response": "r Coarse - grain the flux to the given sets of states."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef total_flux(F, A=None):\n    if A is None:\n        prod = flux_production(F)\n        zeros = np.zeros(len(prod))\n        outflux = np.sum(np.maximum(prod, zeros))\n        return outflux\n    else:\n        X = set(np.arange(F.shape[0]))  # total state space\n        A = set(A)\n        notA = X.difference(A)\n        outflux = (F[list(A), :])[:, list(notA)].sum()\n        return outflux", "response": "r Compute the total flux or turnover flux that is produced by the\n        flux sources and consumed by the\n        flux sinks."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rate(totflux, pi, qminus):\n    kAB = totflux / (pi * qminus).sum()\n    return kAB", "response": "r Returns the transition rate for reaction A to B."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the initialization lines to the journal.", "response": "def _init_journal(self, permissive=True):\n        \"\"\"Add the initialization lines to the journal.\n\n        By default adds JrnObj variable and timestamp to the journal contents.\n\n        Args:\n            permissive (bool): if True most errors in journal will not\n                               cause Revit to stop journal execution.\n                               Some still do.\n        \"\"\"\n        nowstamp = datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")[:-3]\n        self._add_entry(templates.INIT\n                                 .format(time_stamp=nowstamp))\n        if permissive:\n            self._add_entry(templates.INIT_DEBUG)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends a new file from. rft entry to the journal.", "response": "def _new_from_rft(self, base_template, rft_file):\n        \"\"\"Append a new file from .rft entry to the journal.\n\n        This instructs Revit to create a new model based on\n        the provided .rft template.\n\n        Args:\n            base_template (str): new file journal template from rmj.templates\n            rft_file (str): full path to .rft template to be used\n        \"\"\"\n        self._add_entry(base_template)\n        self._add_entry(templates.NEW_FROM_RFT\n                                 .format(rft_file_path=rft_file,\n                                         rft_file_name=op.basename(rft_file)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nappend a new model from the current resource to the journal.", "response": "def new_model(self, template_name='<None>'):\n        \"\"\"Append a new model from .rft entry to the journal.\n\n        This instructs Revit to create a new model based on the\n        provided .rft template.\n\n        Args:\n            template_name (str): optional full path to .rft template\n                                 to be used. default value is <None>\n        \"\"\"\n        self._add_entry(templates.NEW_MODEL\n                                 .format(template_name=template_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends a new template from the current resource to the journal.", "response": "def new_template(self, template_name='<None>'):\n        \"\"\"Append a new template from .rft entry to the journal.\n\n        This instructs Revit to create a new template model based on the\n        provided .rft template.\n\n        Args:\n            template_name (str): optional full path to .rft template\n                                 to be used. default value is <None>\n        \"\"\"\n        self._add_entry(templates.NEW_MODEL_TEMPLATE\n                                 .format(template_name=template_name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_workshared_model(self, model_path, central=False,\n                              detached=False, keep_worksets=True, audit=False,\n                              show_workset_config=1):\n        \"\"\"Append a open workshared model entry to the journal.\n\n        This instructs Revit to open a workshared model.\n\n        Args:\n            model_path (str): full path to workshared model\n            central (bool): if True opens central model and not local\n            detached (bool): if True opens a detached model\n            keep_worksets (bool): if True keeps worksets when detaching\n            audit (bool): if True audits the model when opening\n        \"\"\"\n        if detached:\n            if audit:\n                if keep_worksets:\n                    self._add_entry(\n                        templates.CENTRAL_OPEN_DETACH_AUDIT\n                                 .format(model_path=model_path,\n                                         workset_config=show_workset_config)\n                                 )\n                else:\n                    self._add_entry(\n                        templates.CENTRAL_OPEN_DETACH_AUDIT_DISCARD\n                                 .format(model_path=model_path,\n                                         workset_config=show_workset_config)\n                                 )\n            else:\n                if keep_worksets:\n                    self._add_entry(\n                        templates.CENTRAL_OPEN_DETACH\n                                 .format(model_path=model_path,\n                                         workset_config=show_workset_config)\n                                 )\n                else:\n                    self._add_entry(\n                        templates.CENTRAL_OPEN_DETACH_DISCARD\n                                 .format(model_path=model_path,\n                                         workset_config=show_workset_config)\n                                 )\n        elif central:\n            if audit:\n                self._add_entry(\n                    templates.CENTRAL_OPEN_AUDIT\n                             .format(model_path=model_path,\n                                     workset_config=show_workset_config)\n                             )\n            else:\n                self._add_entry(\n                    templates.CENTRAL_OPEN\n                             .format(model_path=model_path,\n                                     workset_config=show_workset_config)\n                             )\n        else:\n            if audit:\n                self._add_entry(\n                    templates.WORKSHARED_OPEN_AUDIT\n                             .format(model_path=model_path,\n                                     workset_config=show_workset_config)\n                             )\n            else:\n                self._add_entry(\n                    templates.WORKSHARED_OPEN\n                             .format(model_path=model_path,\n                                     workset_config=show_workset_config)\n                             )", "response": "Append a open workshared model entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a open non - workshared model entry to the journal.", "response": "def open_model(self, model_path, audit=False):\n        \"\"\"Append a open non-workshared model entry to the journal.\n\n        This instructs Revit to open a non-workshared model.\n\n        Args:\n            model_path (str): full path to non-workshared model\n            audit (bool): if True audits the model when opening\n        \"\"\"\n        if audit:\n            self._add_entry(templates.FILE_OPEN_AUDIT\n                                     .format(model_path=model_path))\n        else:\n            self._add_entry(templates.FILE_OPEN\n                                     .format(model_path=model_path))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_command(self, tab_name, panel_name,\n                        command_module, command_class, command_data=None):\n        \"\"\"Append an execute external command entry to the journal.\n\n        This instructs Revit to execute the provided command from the\n        provided module, tab, and panel.\n\n        Args:\n            tab_name (str): name of ribbon tab that contains the command\n            panel_name (str): name of ribbon panel that contains the command\n            command_module (str): name of module that provides the command\n            command_class (str): name of command class inside command module\n            command_data (dict): dict of string data to be passed to command\n\n        Examples:\n            >>> jm = JournalMaker()\n            >>> cmdata = {'key1':'value1', 'key2':'value2'}\n            >>> jm.execute_command(tab_name='Add-Ins',\n            ...                    panel_name='Panel Name',\n            ...                    command_module='Addon App Namespace',\n            ...                    command_class='Command Classname',\n            ...                    command_data=cmdata)\n        \"\"\"\n        # make sure command_data is not empty\n        command_data = {} if command_data is None else command_data\n        # make the canonical name for the command\n        cmdclassname = '{}.{}'.format(command_module, command_class)\n\n        self._add_entry(templates.EXTERNAL_COMMAND\n                                 .format(external_command_tab=tab_name,\n                                         external_command_panel=panel_name,\n                                         command_class_name=command_class,\n                                         command_class=cmdclassname))\n\n        # count the data\n        data_count = len(command_data.keys())\n\n        # create the entry for the command data\n        if data_count > 0:\n            data_str_list = []\n            for k, v in command_data.items():\n                data_str_list.append(' \"{}\" , \"{}\"'.format(k, v))\n\n            data_str = '_\\n    ,'.join(data_str_list)\n            self._add_entry(templates.EXTERNAL_COMMANDDATA\n                                     .format(data_count=data_count,\n                                             data_string=data_str))", "response": "Append an execute external command entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_dynamo_definition(self, definition_path,\n                                  show_ui=False, shutdown=True,\n                                  automation=False, path_exec=True):\n        \"\"\"Execute a dynamo definition.\n\n        Args:\n            definition_path (str): full path to dynamo definition file\n            show_ui (bool): show dynamo UI at execution\n            shutdown (bool): shutdown model after execution\n            automation (bool): activate dynamo automation\n            path_exec (bool): activate dynamo path execute\n\n        Examples:\n            >>> jm = JournalMaker()\n            >>> jm.execute_dynamo_definition(\n            ...     definition_path='C:/testdef.dyn',\n            ...     show_ui=True,\n            ...     shutdown=True\n            ... )\n        \"\"\"\n        self._add_entry(templates.DYNAMO_COMMAND\n                                 .format(dynamo_def_path=definition_path,\n                                         dyn_show_ui=show_ui,\n                                         dyn_automation=automation,\n                                         dyn_path_exec=path_exec,\n                                         dyn_shutdown=shutdown))", "response": "Execute a dynamo definition file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_family(self, rfa_file):\n        self._add_entry(templates.IMPORT_FAMILY\n                                 .format(family_file=rfa_file))", "response": "Append a import family entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_warnings(self, export_file):\n        warn_filepath = op.dirname(export_file)\n        warn_filename = op.splitext(op.basename(export_file))[0]\n        self._add_entry(templates.EXPORT_WARNINGS\n                                 .format(warnings_export_path=warn_filepath,\n                                         warnings_export_file=warn_filename))", "response": "Append an export warnings entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef purge_unused(self, pass_count=3):\n        for purge_count in range(0, pass_count):\n            self._add_entry(templates.PROJECT_PURGE)", "response": "Append an entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sync_model(self, comment='', compact_central=False,\n                   release_borrowed=True, release_workset=True,\n                   save_local=False):\n        \"\"\"Append a sync model entry to the journal.\n\n        This instructs Revit to sync the currently open workshared model.\n\n        Args:\n            comment (str): comment to be provided for the sync step\n            compact_central (bool): if True compacts the central file\n            release_borrowed (bool): if True releases the borrowed elements\n            release_workset (bool): if True releases the borrowed worksets\n            save_local (bool): if True saves the local file as well\n        \"\"\"\n        self._add_entry(templates.FILE_SYNC_START)\n\n        if compact_central:\n            self._add_entry(templates.FILE_SYNC_COMPACT)\n        if release_borrowed:\n            self._add_entry(templates.FILE_SYNC_RELEASE_BORROWED)\n        if release_workset:\n            self._add_entry(templates.FILE_SYNC_RELEASE_USERWORKSETS)\n        if save_local:\n            self._add_entry(templates.FILE_SYNC_RELEASE_SAVELOCAL)\n\n        self._add_entry(templates.FILE_SYNC_COMMENT_OK\n                                 .format(sync_comment=comment))", "response": "Append a sync model entry to the journal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_journal(self, journal_file_path):\n        # TODO: assert the extension is txt and not other\n        with open(journal_file_path, \"w\") as jrn_file:\n            jrn_file.write(self._journal_contents)", "response": "Write the constructed journal in to the provided file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef endswith(self, search_str):\n        for entry in reversed(list(open(self._jrnl_file, 'r'))[-5:]):\n            if search_str in entry:\n                return True\n\n        return False", "response": "Check whether the provided string exists in the Journal file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward_committor(T, A, B):\n    X = set(range(T.shape[0]))\n    A = set(A)\n    B = set(B)\n    AB = A.intersection(B)\n    notAB = X.difference(A).difference(B)\n    if len(AB) > 0:\n        raise ValueError(\"Sets A and B have to be disjoint\")\n    L = T - np.eye(T.shape[0])  # Generator matrix\n\n    \"\"\"Assemble left hand-side W for linear system\"\"\"\n    \"\"\"Equation (I)\"\"\"\n    W = 1.0 * L\n    \"\"\"Equation (II)\"\"\"\n    W[list(A), :] = 0.0\n    W[list(A), list(A)] = 1.0\n    \"\"\"Equation (III)\"\"\"\n    W[list(B), :] = 0.0\n    W[list(B), list(B)] = 1.0\n\n    \"\"\"Assemble right hand side r for linear system\"\"\"\n    \"\"\"Equation (I+II)\"\"\"\n    r = np.zeros(T.shape[0])\n    \"\"\"Equation (III)\"\"\"\n    r[list(B)] = 1.0\n\n    u = solve(W, r)\n    return u", "response": "r Forward committor between given sets A and B."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backward_committor(T, A, B, mu=None):\n    X = set(range(T.shape[0]))\n    A = set(A)\n    B = set(B)\n    AB = A.intersection(B)\n    notAB = X.difference(A).difference(B)\n    if len(AB) > 0:\n        raise ValueError(\"Sets A and B have to be disjoint\")\n    if mu is None:\n        mu = stationary_distribution(T)\n    K = np.transpose(mu[:, np.newaxis] * (T - np.eye(T.shape[0])))\n\n    \"\"\"Assemble left-hand side W for linear system\"\"\"\n    \"\"\"Equation (I)\"\"\"\n    W = 1.0 * K\n    \"\"\"Equation (II)\"\"\"\n    W[list(A), :] = 0.0\n    W[list(A), list(A)] = 1.0\n    \"\"\"Equation (III)\"\"\"\n    W[list(B), :] = 0.0\n    W[list(B), list(B)] = 1.0\n\n    \"\"\"Assemble right-hand side r for linear system\"\"\"\n    \"\"\"Equation (I)+(III)\"\"\"\n    r = np.zeros(T.shape[0])\n    \"\"\"Equation (II)\"\"\"\n    r[list(A)] = 1.0\n\n    u = solve(W, r)\n\n    return u", "response": "r Returns the backward committor between given sets A and B."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prior_const(C, alpha=0.001):\n    B = alpha * np.ones(C.shape)\n    return B", "response": "Constant prior of strength alpha."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_states(dtrajs, subset = None):\n    # check input\n    dtrajs = _ensure_dtraj_list(dtrajs)\n    # select subset unless given\n    n = number_of_states(dtrajs)\n    if subset is None:\n        subset = list(range(n))\n    else:\n        if np.max(subset) >= n:\n            raise ValueError('Selected subset is not a subset of the states in dtrajs.')\n    # histogram states\n    hist = count_states(dtrajs)\n    # efficient access to which state are accessible\n    is_requested = np.ndarray((n), dtype=bool)\n    is_requested[:] = False\n    is_requested[subset] = True\n    # efficient access to requested state indexes\n    full2states = np.zeros((n), dtype=int)\n    full2states[subset] = list(range(len(subset)))\n    # initialize results\n    res    = np.ndarray((len(subset)), dtype=object)\n    counts = np.zeros((len(subset)), dtype=int)\n    for i,s in enumerate(subset):\n        res[i] = np.zeros((hist[s],2), dtype=int)\n    # walk through trajectories and remember requested state indexes\n    for i,dtraj in enumerate(dtrajs):\n        for t,s in enumerate(dtraj):\n            if is_requested[s]:\n                k = full2states[s]\n                res[k][counts[k],0] = i\n                res[k][counts[k],1] = t\n                counts[k] += 1\n    return res", "response": "Generates a trajectory or time indexes for the given list of states in a single trajectory or list of discretized trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsample trajectory or time indexes according to the given sequence of states.", "response": "def sample_indexes_by_state(indexes, nsample, subset=None, replace=True):\n    \"\"\"Samples trajectory/time indexes according to the given sequence of states\n\n    Parameters\n    ----------\n    indexes : list of ndarray( (N_i, 2) )\n        For each state, all trajectory and time indexes where this state occurs.\n        Each matrix has a number of rows equal to the number of occurrences of the corresponding state,\n        with rows consisting of a tuple (i, t), where i is the index of the trajectory and t is the time index\n        within the trajectory.\n    nsample : int\n        Number of samples per state. If replace = False, the number of returned samples per state could be smaller\n        if less than nsample indexes are available for a state.\n    subset : ndarray((n)), optional, default = None\n        array of states to be indexed. By default all states in dtrajs will be used\n    replace : boolean, optional\n        Whether the sample is with or without replacement\n\n    Returns\n    -------\n    indexes : list of ndarray( (N, 2) )\n        List of the sampled indices by state.\n        Each element is an index array with a number of rows equal to N=len(sequence), with rows consisting of a\n        tuple (i, t), where i is the index of the trajectory and t is the time index within the trajectory.\n\n    \"\"\"\n    # how many states in total?\n    n = len(indexes)\n    # define set of states to work on\n    if subset is None:\n        subset = list(range(n))\n\n    # list of states\n    res = np.ndarray((len(subset)), dtype=object)\n    for i in range(len(subset)):\n        # sample the following state\n        s = subset[i]\n        # how many indexes are available?\n        m_available = indexes[s].shape[0]\n        # do we have no indexes for this state? Then insert empty array.\n        if (m_available == 0):\n            res[i] = np.zeros((0,2), dtype=int)\n        elif replace:\n            I = np.random.choice(m_available, nsample, replace=True)\n            res[i] = indexes[s][I,:]\n        else:\n            I = np.random.choice(m_available, min(m_available,nsample), replace=False)\n            res[i] = indexes[s][I,:]\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sample_indexes_by_distribution(indexes, distributions, nsample):\n    # how many states in total?\n    n = len(indexes)\n    for dist in distributions:\n        if len(dist) != n:\n            raise ValueError('Size error: Distributions must all be of length n (number of states).')\n\n    # list of states\n    res = np.ndarray((len(distributions)), dtype=object)\n    for i in range(len(distributions)):\n        # sample states by distribution\n        sequence = np.random.choice(n, size=nsample, p=distributions[i])\n        res[i] = sample_indexes_by_sequence(indexes, sequence)\n    #\n    return res", "response": "Samples trajectory or time indexes according to the given probability distributions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_transition_matrix(T, tol=1e-10):\n    if T.ndim != 2:\n        return False\n    if T.shape[0] != T.shape[1]:\n        return False\n    dim = T.shape[0]\n    X = np.abs(T) - T\n    x = np.sum(T, axis=1)\n    return np.abs(x - np.ones(dim)).max() < dim * tol and X.max() < 2.0 * tol", "response": "Tests whether T is a transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_rate_matrix(K, tol=1e-10):\n    R = K - K.diagonal()\n    off_diagonal_positive = np.allclose(R, abs(R), 0.0, atol=tol)\n\n    row_sum = K.sum(axis=1)\n    row_sum_eq_0 = np.allclose(row_sum, 0.0, atol=tol)\n\n    return off_diagonal_positive and row_sum_eq_0", "response": "True if K is a rate matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_reversible(T, mu=None, tol=1e-10):\n    if is_transition_matrix(T, tol):\n        if mu is None:\n            mu = stationary_distribution(T)\n        X = mu[:, np.newaxis] * T\n        return np.allclose(X, np.transpose(X),  atol=tol)\n    else:\n        raise ValueError(\"given matrix is not a valid transition matrix.\")", "response": "r Checks whether T is reversible in terms of given stationary distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_transition_matrix(T, tol):\n    T = T.tocsr()  # compressed sparse row for fast row slicing\n    values = T.data  # non-zero entries of T\n\n    \"\"\"Check entry-wise positivity\"\"\"\n    is_positive = np.allclose(values, np.abs(values), rtol=tol)\n\n    \"\"\"Check row normalization\"\"\"\n    is_normed = np.allclose(T.sum(axis=1), 1.0, rtol=tol)\n\n    return is_positive and is_normed", "response": "Check if T is a transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a matrix K is a rate matrix.", "response": "def is_rate_matrix(K, tol):\n    \"\"\"\n    True if K is a rate matrix\n    Parameters\n    ----------\n    K : scipy.sparse matrix\n        Matrix to check\n    tol : float\n        tolerance to check with\n\n    Returns\n    -------\n    Truth value : bool\n        True, if K negated diagonal is positive and row sums up to zero.\n        False, otherwise\n    \"\"\"\n    K = K.tocsr()\n\n    # check rows sum up to zero.\n    row_sum = K.sum(axis=1)\n    sum_eq_zero = np.allclose(row_sum, np.zeros(shape=row_sum.shape), atol=tol)\n\n    # store copy of original diagonal\n    org_diag = K.diagonal()\n\n    # substract diagonal\n    K = K - diags(org_diag, 0)\n\n    # check off diagonals are > 0\n    values = K.data\n    values_gt_zero = np.allclose(values, np.abs(values), atol=tol)\n\n    # add diagonal\n    K = K + diags(org_diag, 0)\n\n    return values_gt_zero and sum_eq_zero"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_reversible(T, mu=None, tol=1e-15):\n    if not is_transition_matrix(T, tol):\n        raise ValueError(\"given matrix is not a valid transition matrix.\")\n\n    T = T.tocsr()\n\n    if mu is None:\n        from .decomposition import stationary_distribution\n        mu = stationary_distribution(T)\n\n    Mu = diags(mu, 0)\n    prod = Mu * T\n\n    return allclose_sparse(prod, prod.transpose(), rtol=tol)", "response": "r Checks whether T is reversible in terms of given stationary distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_connected(T, directed=True):\n    nc = connected_components(T, directed=directed, connection='strong', \\\n                              return_labels=False)\n    return nc == 1", "response": "r Check connectivity of the transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if T is egodic.", "response": "def is_ergodic(T, tol):\n    \"\"\"\n    checks if T is 'ergodic'\n\n    Parameters\n    ----------\n    T : scipy.sparse matrix\n        Transition matrix\n    tol : float\n        tolerance\n\n    Returns\n    -------\n    Truth value : bool\n    True, if # strongly connected components = 1\n    False, otherwise\n    \"\"\"\n    if isdense(T):\n        T = T.tocsr()\n    if not is_transition_matrix(T, tol):\n        raise ValueError(\"given matrix is not a valid transition matrix.\")\n\n    num_components = connected_components(T, directed=True, \\\n                                          connection='strong', \\\n                                          return_labels=False)\n\n    return num_components == 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_bottleneck(F, A, B):\n    if F.nnz == 0:\n        raise PathwayError('no more pathways left: Flux matrix does not contain any positive entries')\n    F = F.tocoo()\n    n = F.shape[0]\n\n    \"\"\"Get exdges and corresponding flux values\"\"\"\n    val = F.data\n    row = F.row\n    col = F.col\n\n    \"\"\"Sort edges according to flux\"\"\"\n    ind = np.argsort(val)\n    val = val[ind]\n    row = row[ind]\n    col = col[ind]\n\n    \"\"\"Check if edge with largest conductivity connects A and B\"\"\"\n    b = np.array(row[-1], col[-1])\n    if has_path(b, A, B):\n        return b\n    else:\n        \"\"\"Bisection of flux-value array\"\"\"\n        r = val.size\n        l = 0\n        N = 0\n        while r - l > 1:\n            m = np.int(np.floor(0.5 * (r + l)))\n            valtmp = val[m:]\n            rowtmp = row[m:]\n            coltmp = col[m:]\n            C = coo_matrix((valtmp, (rowtmp, coltmp)), shape=(n, n))\n            \"\"\"Check if there is a path connecting A and B by\n            iterating over all starting nodes in A\"\"\"\n            if has_connection(C, A, B):\n                l = 1 * m\n            else:\n                r = 1 * m\n\n        E_AB = coo_matrix((val[l + 1:], (row[l + 1:], col[l + 1:])), shape=(n, n))\n        b1 = row[l]\n        b2 = col[l]\n        return b1, b2, E_AB", "response": "r Finds dynamic bottleneck of flux network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_path(F, path):\n    c = capacity(F, path)\n    F = F.todok()\n    L = len(path)\n    for l in range(L - 1):\n        i = path[l]\n        j = path[l + 1]\n        F[i, j] -= c\n    return F", "response": "r Removes capacity along a path from flux network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_utf_8(txt):\n    assert isinstance(txt, six.binary_type)\n\n    try:\n        _ = six.text_type(txt, 'utf-8')\n    except (TypeError, UnicodeEncodeError):\n        return False\n    else:\n        return True", "response": "Check a string is utf - 8 encoded and return a boolean indicating if it is utf - 8 encoded or not"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_libs(self, scripts_paths):\n        for path in scripts_paths:\n            self.run_script(_read_file(path), identifier=path)", "response": "Load the scripts into the context."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a JS script within the context.", "response": "def run_script(self, script, identifier=_DEFAULT_SCRIPT_NAME):\n        \"\"\"\n        Run a JS script within the context.\\\n        All code is ran synchronously,\\\n        there is no event loop. It's thread-safe\n\n        :param script: utf-8 encoded or unicode string\n        :type script: bytes or str\n        :param identifier: utf-8 encoded or unicode string.\\\n        This is used as the name of the script\\\n        (ie: in stack-traces)\n        :type identifier: bytes or str\n        :return: Result of running the JS script\n        :rtype: str\n        :raises V8Error: if there was\\\n        an error running the JS script\n        \"\"\"\n        assert isinstance(script, six.text_type) or _is_utf_8(script)\n        assert isinstance(identifier, six.text_type) or _is_utf_8(identifier)\n\n        if isinstance(script, six.text_type):\n            script = script.encode('utf-8')\n\n        if isinstance(identifier, six.text_type):\n            identifier = identifier.encode('utf-8')\n\n        with _String() as output:\n            with _String() as error:\n                code = lib.v8cffi_run_script(\n                    self._c_context[0],\n                    script,\n                    len(script),\n                    identifier,\n                    len(identifier),\n                    output.string_ptr,\n                    output.len_ptr,\n                    error.string_ptr,\n                    error.len_ptr)\n\n                if code != lib.E_V8_OK:\n                    raise exceptions.get_exception(code)(six.text_type(error))\n\n                return six.text_type(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eigenvalues(T, k=None, reversible=False, mu=None):\n    if reversible:\n        try:\n            evals = eigenvalues_rev(T, k=k, mu=mu)\n        except:\n            evals = eigvals(T).real  # use fallback code but cast to real\n    else:\n        evals = eigvals(T)  # nonreversible\n\n    \"\"\"Sort by decreasing absolute value\"\"\"\n    ind = np.argsort(np.abs(evals))[::-1]\n    evals = evals[ind]\n\n    if isinstance(k, (list, set, tuple)):\n        try:\n            return [evals[n] for n in k]\n        except IndexError:\n            raise ValueError(\"given indices do not exist: \", k)\n    elif k is not None:\n        return evals[: k]\n    else:\n        return evals", "response": "r Compute the eigenvalues of a given transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eigenvectors(T, k=None, right=True, reversible=False, mu=None):\n    if reversible:\n        eigvec = eigenvectors_rev(T, right=right, mu=mu)\n    else:\n        eigvec = eigenvectors_nrev(T, right=right)\n\n    \"\"\" Return eigenvectors \"\"\"\n    if k is None:\n        return eigvec\n    elif isinstance(k, numbers.Integral):\n        return eigvec[:, 0:k]\n    else:\n        ind = np.asarray(k)\n        return eigvec[:, ind]", "response": "r Compute the eigenvectors of the transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eigenvectors_nrev(T, right=True):\n    if right:\n        val, R = eig(T, left=False, right=True)\n        \"\"\" Sorted eigenvalues and left and right eigenvectors. \"\"\"\n        perm = np.argsort(np.abs(val))[::-1]\n        # eigval=val[perm]\n        eigvec = R[:, perm]\n\n    else:\n        val, L = eig(T, left=True, right=False)\n\n        \"\"\" Sorted eigenvalues and left and right eigenvectors. \"\"\"\n        perm = np.argsort(np.abs(val))[::-1]\n        # eigval=val[perm]\n        eigvec = L[:, perm]\n    return eigvec", "response": "r Compute the eigenvectors of transition matrix T ordered with decreasing absolute value T and the first k eigenvalues of the corresponding eigenvalue T."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rdl_decomposition_nrev(T, norm='standard'):\n    d = T.shape[0]\n    w, R = eig(T)\n\n    \"\"\"Sort by decreasing magnitude of eigenvalue\"\"\"\n    ind = np.argsort(np.abs(w))[::-1]\n    w = w[ind]\n    R = R[:, ind]\n\n    \"\"\"Diagonal matrix containing eigenvalues\"\"\"\n    D = np.diag(w)\n\n    # Standard norm: Euclidean norm is 1 for r and LR = I.\n    if norm == 'standard':\n        L = solve(np.transpose(R), np.eye(d))\n\n        \"\"\"l1- normalization of L[:, 0]\"\"\"\n        R[:, 0] = R[:, 0] * np.sum(L[:, 0])\n        L[:, 0] = L[:, 0] / np.sum(L[:, 0])\n\n        return R, D, np.transpose(L)\n\n    # Reversible norm:\n    elif norm == 'reversible':\n        b = np.zeros(d)\n        b[0] = 1.0\n\n        A = np.transpose(R)\n        nu = solve(A, b)\n        mu = nu / np.sum(nu)\n\n        \"\"\"Ensure that R[:,0] is positive\"\"\"\n        R[:, 0] = R[:, 0] / np.sign(R[0, 0])\n\n        \"\"\"Use mu to connect L and R\"\"\"\n        L = mu[:, np.newaxis] * R\n\n        \"\"\"Compute overlap\"\"\"\n        s = np.diag(np.dot(np.transpose(L), R))\n\n        \"\"\"Renormalize left-and right eigenvectors to ensure L'R=Id\"\"\"\n        R = R / np.sqrt(s[np.newaxis, :])\n        L = L / np.sqrt(s[np.newaxis, :])\n\n        return R, D, np.transpose(L)\n\n    else:\n        raise ValueError(\"Keyword 'norm' has to be either 'standard' or 'reversible'\")", "response": "rDecomposition into left and right eigenvectors of a single - level tree T."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timescales_from_eigenvalues(evals, tau=1):\n\n    \"\"\"Check for dominant eigenvalues with large imaginary part\"\"\"\n\n    if not np.allclose(evals.imag, 0.0):\n        warnings.warn('Using eigenvalues with non-zero imaginary part', ImaginaryEigenValueWarning)\n\n    \"\"\"Check for multiple eigenvalues of magnitude one\"\"\"\n    ind_abs_one = np.isclose(np.abs(evals), 1.0, rtol=0.0, atol=1e-14)\n    if sum(ind_abs_one) > 1:\n        warnings.warn('Multiple eigenvalues with magnitude one.', SpectralWarning)\n\n    \"\"\"Compute implied time scales\"\"\"\n    ts = np.zeros(len(evals))\n\n    \"\"\"Eigenvalues of magnitude one imply infinite timescale\"\"\"\n    ts[ind_abs_one] = np.inf\n\n    \"\"\"All other eigenvalues give rise to finite timescales\"\"\"\n    ts[np.logical_not(ind_abs_one)] = \\\n        -1.0 * tau / np.log(np.abs(evals[np.logical_not(ind_abs_one)]))\n    return ts", "response": "r Compute implied time scales from given eigenvalues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_sparse_file(filename):\n    dirname, basename = os.path.split(filename)\n    name, ext = os.path.splitext(basename)\n    matrix_name, matrix_ext = os.path.splitext(name)\n    if matrix_ext == '.coo':\n        return True\n    else:\n        return False", "response": "Determine if the given filename indicates a dense or sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a sparse matrix from a file.", "response": "def read_matrix_sparse(filename, dtype=float, comments='#'):\n    coo = np.loadtxt(filename, comments=comments, dtype=dtype)\n\n    \"\"\"Check if coo is (M, 3) ndarray\"\"\"\n    if len(coo.shape) == 2 and coo.shape[1] == 3:\n        row = coo[:, 0]\n        col = coo[:, 1]\n        values = coo[:, 2]\n\n        \"\"\"Check if imaginary part of row and col is zero\"\"\"\n        if np.all(np.isreal(row)) and np.all(np.isreal(col)):\n            row = row.real\n            col = col.real\n\n            \"\"\"Check if first and second column contain only integer entries\"\"\"\n            if np.all(is_integer(row)) and np.all(is_integer(col)):\n\n                \"\"\"Convert row and col to int\"\"\"\n                row = row.astype(int)\n                col = col.astype(int)\n\n                \"\"\"Create coo-matrix\"\"\"\n                A = scipy.sparse.coo_matrix((values, (row, col)))\n                return A\n            else:\n                raise ValueError('File contains non-integer entries for row and col.')\n        else:\n            raise ValueError('File contains complex entries for row and col.')\n    else:\n        raise ValueError('Given file is not a sparse matrix in coo-format.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a sparse matrix from a file.", "response": "def load_matrix_sparse(filename):\n    coo = np.load(filename)\n\n    \"\"\"Check if coo is (M, 3) ndarray\"\"\"\n    if len(coo.shape) == 2 and coo.shape[1] == 3:\n        row = coo[:, 0]\n        col = coo[:, 1]\n        values = coo[:, 2]\n\n        \"\"\"Check if imaginary part of row and col is zero\"\"\"\n        if np.all(np.isreal(row)) and np.all(np.isreal(col)):\n            row = row.real\n            col = col.real\n\n            \"\"\"Check if first and second column contain only integer entries\"\"\"\n            if np.all(is_integer(row)) and np.all(is_integer(col)):\n\n                \"\"\"Convert row and col to int\"\"\"\n                row = row.astype(int)\n                col = col.astype(int)\n\n                \"\"\"Create coo-matrix\"\"\"\n                A = scipy.sparse.coo_matrix((values, (row, col)))\n                return A\n            else:\n                raise ValueError('File contains non-integer entries for row and col.')\n        else:\n            raise ValueError('File contains complex entries for row and col.')\n    else:\n        raise ValueError('Given file is not a sparse matrix in coo-format.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backward_iteration(A, mu, x0, tol=1e-14, maxiter=100):\n    T = A - mu * eye(A.shape[0], A.shape[0])\n    T = T.tocsc()\n    \"\"\"Prefactor T and return a function for solution\"\"\"\n    solve = factorized(T)\n    \"\"\"Starting iterate with ||y_0||=1\"\"\"\n    r0 = 1.0 / np.linalg.norm(x0)\n    y0 = x0 * r0\n    \"\"\"Local variables for inverse iteration\"\"\"\n    y = 1.0 * y0\n    r = 1.0 * r0\n    N = 0\n    for i in range(maxiter):\n        x = solve(y)\n        r = 1.0 / np.linalg.norm(x)\n        y = x * r\n        if r <= tol:\n            return y\n    msg = \"Failed to converge after %d iterations, residuum is %e\" % (maxiter, r)\n    raise RuntimeError(msg)", "response": "r Find eigenvector to approximate eigenvalue via backward iteration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stationary_distribution_from_backward_iteration(P, eps=1e-15):\n    A = P.transpose()\n    mu = 1.0 - eps\n    x0 = np.ones(P.shape[0])\n    y = backward_iteration(A, mu, x0)\n    pi = y / y.sum()\n    return pi", "response": "r Fast computation of the stationary vector using backward iteration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef eigenvalues(T, k=None, ncv=None, reversible=False, mu=None):\n    if k is None:\n        raise ValueError(\"Number of eigenvalues required for decomposition of sparse matrix\")\n    else:\n        if reversible:\n            try:\n                v = eigenvalues_rev(T, k, ncv=ncv, mu=mu)\n            except:  # use fallback code, but cast to real\n                v = scipy.sparse.linalg.eigs(T, k=k, which='LM', return_eigenvectors=False, ncv=ncv).real\n        else:\n            v = scipy.sparse.linalg.eigs(T, k=k, which='LM', return_eigenvectors=False, ncv=ncv)\n\n    ind = np.argsort(np.abs(v))[::-1]\n    return v[ind]", "response": "r Compute the eigenvalues of a sparse transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigenvectors_rev(T, k, right=True, ncv=None, mu=None):\n    if mu is None:\n        mu = stationary_distribution(T)\n    \"\"\" symmetrize T \"\"\"\n    smu = np.sqrt(mu)\n    D = diags(smu, 0)\n    Dinv = diags(1.0/smu, 0)\n    S = (D.dot(T)).dot(Dinv)\n    \"\"\"Compute eigenvalues, eigenvecs using a solver for\n    symmetric/hermititan eigenproblems\"\"\"\n    val, eigvec = scipy.sparse.linalg.eigsh(S, k=k, ncv=ncv, which='LM',\n                                            return_eigenvectors=True)\n    \"\"\"Sort eigenvectors\"\"\"\n    ind = np.argsort(np.abs(val))[::-1]\n    eigvec = eigvec[:, ind]\n    if right:\n        return eigvec / smu[:, np.newaxis]\n    else:\n        return eigvec * smu[:, np.newaxis]", "response": "r Compute the eigenvectors of reversible transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rdl_decomposition_nrev(T, k, norm='standard', ncv=None):\n    # Standard norm: Euclidean norm is 1 for r and LR = I.\n    if norm == 'standard':\n        v, R = scipy.sparse.linalg.eigs(T, k=k, which='LM', ncv=ncv)\n        r, L = scipy.sparse.linalg.eigs(T.transpose(), k=k, which='LM', ncv=ncv)\n\n        \"\"\"Sort right eigenvectors\"\"\"\n        ind = np.argsort(np.abs(v))[::-1]\n        v = v[ind]\n        R = R[:, ind]\n\n        \"\"\"Sort left eigenvectors\"\"\"\n        ind = np.argsort(np.abs(r))[::-1]\n        r = r[ind]\n        L = L[:, ind]\n\n        \"\"\"l1-normalization of L[:, 0]\"\"\"\n        L[:, 0] = L[:, 0] / np.sum(L[:, 0])\n\n        \"\"\"Standard normalization L'R=Id\"\"\"\n        ov = np.diag(np.dot(np.transpose(L), R))\n        R = R / ov[np.newaxis, :]\n\n        \"\"\"Diagonal matrix with eigenvalues\"\"\"\n        D = np.diag(v)\n\n        return R, D, np.transpose(L)\n\n    # Reversible norm:\n    elif norm == 'reversible':\n        v, R = scipy.sparse.linalg.eigs(T, k=k, which='LM', ncv=ncv)\n        mu = stationary_distribution(T)\n\n        \"\"\"Sort right eigenvectors\"\"\"\n        ind = np.argsort(np.abs(v))[::-1]\n        v = v[ind]\n        R = R[:, ind]\n\n        \"\"\"Ensure that R[:,0] is positive\"\"\"\n        R[:, 0] = R[:, 0] / np.sign(R[0, 0])\n\n        \"\"\"Diagonal matrix with eigenvalues\"\"\"\n        D = np.diag(v)\n\n        \"\"\"Compute left eigenvectors from right ones\"\"\"\n        L = mu[:, np.newaxis] * R\n\n        \"\"\"Compute overlap\"\"\"\n        s = np.diag(np.dot(np.transpose(L), R))\n\n        \"\"\"Renormalize left-and right eigenvectors to ensure L'R=Id\"\"\"\n        R = R / np.sqrt(s[np.newaxis, :])\n        L = L / np.sqrt(s[np.newaxis, :])\n\n        return R, D, np.transpose(L)\n    else:\n        raise ValueError(\"Keyword 'norm' has to be either 'standard' or 'reversible'\")", "response": "r Compute the decomposition into left and right eigenvectors of a set of entries in the n - reversible tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rdl_decomposition_rev(T, k, norm='reversible', ncv=None, mu=None):\n    if mu is None:\n        mu = stationary_distribution(T)\n    \"\"\" symmetrize T \"\"\"\n    smu = np.sqrt(mu)\n    Dpi = diags(smu, 0)\n    Dinv = diags(1.0/smu, 0)\n    S = (Dpi.dot(T)).dot(Dinv)\n    \"\"\"Compute eigenvalues, eigenvecs using a solver for\n    symmetric/hermititan eigenproblems\"\"\"\n    val, eigvec = scipy.sparse.linalg.eigsh(S, k=k, ncv=ncv, which='LM',\n                                            return_eigenvectors=True)\n    \"\"\"Sort eigenvalues and eigenvectors\"\"\"\n    ind = np.argsort(np.abs(val))[::-1]\n    val = val[ind]\n    eigvec = eigvec[:, ind]\n\n    \"\"\"Diagonal matrix of eigenvalues\"\"\"\n    D = np.diag(val)\n\n    \"\"\"Right and left eigenvectors\"\"\"\n    R = eigvec / smu[:, np.newaxis]\n    L = eigvec * smu[:, np.newaxis]\n\n    \"\"\"Ensure that R[:,0] is positive and unity\"\"\"\n    tmp = R[0, 0]\n    R[:, 0] = R[:, 0] / tmp\n\n    \"\"\"Ensure that L[:, 0] is probability vector\"\"\"\n    L[:, 0] = L[:, 0] *  tmp\n\n    if norm == 'reversible':\n        return R, D, L.T\n    elif norm == 'standard':\n        \"\"\"Standard l2-norm of right eigenvectors\"\"\"\n        w = np.diag(np.dot(R.T, R))\n        sw = np.sqrt(w)\n        \"\"\"Don't change normalization of eigenvectors for dominant eigenvalue\"\"\"\n        sw[0] = 1.0\n\n        R = R / sw[np.newaxis, :]\n        L = L * sw[np.newaxis, :]\n        return R, D, L.T\n    else:\n        raise ValueError(\"Keyword 'norm' has to be either 'standard' or 'reversible'\")", "response": "r Compute the decomposition into left and right eigenvectors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timescales(T, tau=1, k=None, ncv=None, reversible=False, mu=None):\n    if k is None:\n        raise ValueError(\"Number of time scales required for decomposition of sparse matrix\")\n    values = eigenvalues(T, k=k, ncv=ncv, reversible=reversible)\n\n    \"\"\"Check for dominant eigenvalues with large imaginary part\"\"\"\n    if not np.allclose(values.imag, 0.0):\n        warnings.warn('Using eigenvalues with non-zero imaginary part '\n                      'for implied time scale computation', ImaginaryEigenValueWarning)\n\n    \"\"\"Check for multiple eigenvalues of magnitude one\"\"\"\n    ind_abs_one = np.isclose(np.abs(values), 1.0)\n    if sum(ind_abs_one) > 1:\n        warnings.warn('Multiple eigenvalues with magnitude one.', SpectralWarning)\n\n    \"\"\"Compute implied time scales\"\"\"\n    ts = np.zeros(len(values))\n\n    \"\"\"Eigenvalues of magnitude one imply infinite rate\"\"\"\n    ts[ind_abs_one] = np.inf\n\n    \"\"\"All other eigenvalues give rise to finite rates\"\"\"\n    ts[np.logical_not(ind_abs_one)] = \\\n        -1.0 * tau / np.log(np.abs(values[np.logical_not(ind_abs_one)]))\n    return ts", "response": "r Compute implied time scales of given transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef number_of_states(dtrajs):\n    # determine number of states n\n    nmax = 0\n    for dtraj in dtrajs:\n        nmax = max(nmax, np.max(dtraj))\n    # return number of states\n    return nmax + 1", "response": "r Determine the number of states from a set of discrete trajectories\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef determine_lengths(dtrajs):\n    if (isinstance(dtrajs[0], (int))):\n        return len(dtrajs) * np.ones((1))\n    lengths = np.zeros((len(dtrajs)))\n    for i in range(len(dtrajs)):\n        lengths[i] = len(dtrajs[i])\n    return lengths", "response": "r Determines the lengths of all discrete trajectories\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bootstrap_trajectories(trajs, correlation_length):\n    from scipy.stats import rv_discrete\n    # if we have just one trajectory, put it into a one-element list:\n    if (isinstance(trajs[0], (int, int, float))):\n        trajs = [trajs]\n    ntraj = len(trajs)\n\n    # determine correlation length to be used\n    lengths = determine_lengths(trajs)\n    Ltot = np.sum(lengths)\n    Lmax = np.max(lengths)\n    if (correlation_length < 1):\n        correlation_length = Lmax\n\n        # assign probabilites to select trajectories\n    w_trajs = np.zeros((len(trajs)))\n    for i in range(ntraj):\n        w_trajs[i] = len(trajs[i])\n    w_trajs /= np.sum(w_trajs)  # normalize to sum 1.0\n    distrib_trajs = rv_discrete(values=(list(range(ntraj)), w_trajs))\n\n    # generate subtrajectories\n    Laccum = 0\n    subs = []\n    while (Laccum < Ltot):\n        # pick a random trajectory\n        itraj = distrib_trajs.rvs()\n        # pick a starting frame\n        t0 = random.randint(0, max(1, len(trajs[itraj]) - correlation_length))\n        t1 = min(len(trajs[itraj]), t0 + correlation_length)\n        # add new subtrajectory\n        subs.append(trajs[itraj][t0:t1])\n        # increment available states\n        Laccum += (t1 - t0)\n\n    # and return\n    return subs", "response": "Generates a random count matrix given the input coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns n counts at the given lagtime from the given trajectory", "response": "def bootstrap_counts_singletraj(dtraj, lagtime, n):\n    \"\"\"\n    Samples n counts at the given lagtime from the given trajectory\n    \"\"\"\n    # check if length is sufficient\n    L = len(dtraj)\n    if (lagtime > L):\n        raise ValueError(\n            'Cannot sample counts with lagtime ' + str(lagtime) + ' from a trajectory with length ' + str(L))\n    # sample\n    I = np.random.randint(0, L - lagtime - 1, size=n)\n    J = I + lagtime\n\n    # return state pairs\n    return (dtraj[I], dtraj[J])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bootstrap_counts(dtrajs, lagtime, corrlength=None):\n    from scipy.stats import rv_discrete\n    # if we have just one trajectory, put it into a one-element list:\n    if (isinstance(dtrajs[0], six.integer_types)):\n        dtrajs = [dtrajs]\n    ntraj = len(dtrajs)\n\n    # can we do the estimate?\n    lengths = determine_lengths(dtrajs)\n    Lmax = np.max(lengths)\n    Ltot = np.sum(lengths)\n    if (lagtime >= Lmax):\n        raise ValueError('Cannot estimate count matrix: lag time '\n                         + str(lagtime) + ' is longer than the longest trajectory length ' + str(Lmax))\n\n    # how many counts can we sample?\n    if corrlength is None:\n        corrlength = lagtime\n    nsample = int(Ltot / corrlength)\n\n    # determine number of states n\n    n = number_of_states(dtrajs)\n\n    # assigning trajectory sampling weights\n    w_trajs = np.maximum(0.0, lengths - lagtime)\n    w_trajs /= np.sum(w_trajs)  # normalize to sum 1.0\n    distrib_trajs = rv_discrete(values=(list(range(ntraj)), w_trajs))\n    # sample number of counts from each trajectory\n    n_from_traj = np.bincount(distrib_trajs.rvs(size=nsample), minlength=ntraj)\n\n    # for each trajectory, sample counts and stack them\n    rows = np.zeros((nsample))\n    cols = np.zeros((nsample))\n    ones = np.ones((nsample))\n    ncur = 0\n    for i in range(len(n_from_traj)):\n        if n_from_traj[i] > 0:\n            (r, c) = bootstrap_counts_singletraj(dtrajs[i], lagtime, n_from_traj[i])\n            rows[ncur:ncur + n_from_traj[i]] = r\n            cols[ncur:ncur + n_from_traj[i]] = c\n            ncur += n_from_traj[i]\n    # sum over counts\n    Csparse = scipy.sparse.coo_matrix((ones, (rows, cols)), shape=(n, n))\n\n    return Csparse.tocsr()", "response": "Generates a random resampled count matrix given the input coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connected_sets(C, directed=True):\n    M = C.shape[0]\n    \"\"\" Compute connected components of C. nc is the number of\n    components, indices contain the component labels of the states\n    \"\"\"\n    nc, indices = csgraph.connected_components(C, directed=directed, connection='strong')\n\n    states = np.arange(M)  # Discrete states\n\n    \"\"\"Order indices\"\"\"\n    ind = np.argsort(indices)\n    indices = indices[ind]\n\n    \"\"\"Order states\"\"\"\n    states = states[ind]\n    \"\"\" The state index tuple is now of the following form (states,\n    indices)=([s_23, s_17,...,s_3, s_2, ...], [0, 0, ..., 1, 1, ...])\n    \"\"\"\n\n    \"\"\"Find number of states per component\"\"\"\n    count = np.bincount(indices)\n\n    \"\"\"Cumulative sum of count gives start and end indices of\n    components\"\"\"\n    csum = np.zeros(len(count) + 1, dtype=int)\n    csum[1:] = np.cumsum(count)\n\n    \"\"\"Generate list containing components, sort each component by\n    increasing state label\"\"\"\n    cc = []\n    for i in range(nc):\n        cc.append(np.sort(states[csum[i]:csum[i + 1]]))\n\n    \"\"\"Sort by size of component - largest component first\"\"\"\n    cc = sorted(cc, key=lambda x: -len(x))\n\n    return cc", "response": "r Compute the connected components of a directed or undirected graph with weights C."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef largest_connected_submatrix(C, directed=True, lcc=None):\n    if lcc is None:\n        lcc = largest_connected_set(C, directed=directed)\n\n    \"\"\"Row slicing\"\"\"\n    if scipy.sparse.issparse(C):\n        C_cc = C.tocsr()\n    else:\n        C_cc = C\n    C_cc = C_cc[lcc, :]\n\n    \"\"\"Column slicing\"\"\"\n    if scipy.sparse.issparse(C):\n        C_cc = C_cc.tocsc()\n    C_cc = C_cc[:, lcc]\n\n    if scipy.sparse.issparse(C):\n        return C_cc.tocoo()\n    else:\n        return C_cc", "response": "r Compute the count matrix of the largest connected set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flux_matrix(T, pi, qminus, qplus, netflux=True):\n    if issparse(T):\n        return sparse.tpt.flux_matrix(T, pi, qminus, qplus, netflux=netflux)\n    elif isdense(T):\n        return dense.tpt.flux_matrix(T, pi, qminus, qplus, netflux=netflux)\n    else:\n        raise _type_not_supported", "response": "r Computes the TPT net flux matrix for the reaction A and B."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_netflux(flux):\n    if issparse(flux):\n        return sparse.tpt.to_netflux(flux)\n    elif isdense(flux):\n        return dense.tpt.to_netflux(flux)\n    else:\n        raise _type_not_supported", "response": "r Compute the netflux from the gross flux."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rate(totflux, pi, qminus):\n    return dense.tpt.rate(totflux, pi, qminus)", "response": "r Returns the transition rate for reaction A to B."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pathways(F, A, B, fraction=1.0, maxiter=1000):\n    if issparse(F):\n        return sparse.pathways.pathways(F, A, B, fraction=fraction, maxiter=maxiter)\n    elif isdense(F):\n        return sparse.pathways.pathways(csr_matrix(F), A, B, fraction=fraction, maxiter=maxiter)\n    else:\n        raise _type_not_supported", "response": "r Decomposes flux network into dominant reaction paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noptimizes the PCCA+ rotation matrix such that the memberships are exclusively nonnegative. Parameters ---------- eigenvectors : ndarray A matrix with the sorted eigenvectors in the columns. The stationary eigenvector should be first, then the one to the slowest relaxation process, etc. rot_mat : ndarray (m x m) nonoptimized rotation matrix n_clusters : int Number of clusters to group to. Returns ------- rot_mat : ndarray (m x m) Optimized rotation matrix that rotates the dominant eigenvectors to yield the PCCA memberships, i.e.: chi = np.dot(evec, rot_matrix References ---------- [1] S. Roeblitz and M. Weber, Fuzzy spectral clustering by PCCA+: application to Markov state models and data classification. Adv Data Anal Classif 7, 147-179 (2013).", "response": "def _opt_soft(eigvectors, rot_matrix, n_clusters):\n    \"\"\"\n    Optimizes the PCCA+ rotation matrix such that the memberships are exclusively nonnegative.\n\n    Parameters\n    ----------\n    eigenvectors : ndarray\n        A matrix with the sorted eigenvectors in the columns. The stationary eigenvector should\n        be first, then the one to the slowest relaxation process, etc.\n\n    rot_mat : ndarray (m x m)\n        nonoptimized rotation matrix\n\n    n_clusters : int\n        Number of clusters to group to.\n\n    Returns\n    -------\n    rot_mat : ndarray (m x m)\n        Optimized rotation matrix that rotates the dominant eigenvectors to yield the PCCA memberships, i.e.:\n        chi = np.dot(evec, rot_matrix\n\n    References\n    ----------\n    [1] S. Roeblitz and M. Weber, Fuzzy spectral clustering by PCCA+:\n        application to Markov state models and data classification.\n        Adv Data Anal Classif 7, 147-179 (2013).\n\n    \"\"\"\n    # only consider first n_clusters eigenvectors\n    eigvectors = eigvectors[:, :n_clusters]\n\n    # crop first row and first column from rot_matrix\n    # rot_crop_matrix = rot_matrix[1:,1:]\n    rot_crop_matrix = rot_matrix[1:][:, 1:]\n\n    (x, y) = rot_crop_matrix.shape\n\n    # reshape rot_crop_matrix into linear vector\n    rot_crop_vec = np.reshape(rot_crop_matrix, x * y)\n\n    # Susanna Roeblitz' target function for optimization\n    def susanna_func(rot_crop_vec, eigvectors):\n        # reshape into matrix\n        rot_crop_matrix = np.reshape(rot_crop_vec, (x, y))\n        # fill matrix\n        rot_matrix = _fill_matrix(rot_crop_matrix, eigvectors)\n\n        result = 0\n        for i in range(0, n_clusters):\n            for j in range(0, n_clusters):\n                result += np.power(rot_matrix[j, i], 2) / rot_matrix[0, i]\n        return -result\n\n    from scipy.optimize import fmin\n\n    rot_crop_vec_opt = fmin(susanna_func, rot_crop_vec, args=(eigvectors,), disp=False)\n\n    rot_crop_matrix = np.reshape(rot_crop_vec_opt, (x, y))\n    rot_matrix = _fill_matrix(rot_crop_matrix, eigvectors)\n\n    return rot_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fill_matrix(rot_crop_matrix, eigvectors):\n\n    (x, y) = rot_crop_matrix.shape\n\n    row_sums = np.sum(rot_crop_matrix, axis=1)\n    row_sums = np.reshape(row_sums, (x, 1))\n\n    # add -row_sums as leftmost column to rot_crop_matrix\n    rot_crop_matrix = np.concatenate((-row_sums, rot_crop_matrix), axis=1)\n\n    tmp = -np.dot(eigvectors[:, 1:], rot_crop_matrix)\n\n    tmp_col_max = np.max(tmp, axis=0)\n    tmp_col_max = np.reshape(tmp_col_max, (1, y + 1))\n\n    tmp_col_max_sum = np.sum(tmp_col_max)\n\n    # add col_max as top row to rot_crop_matrix and normalize\n    rot_matrix = np.concatenate((tmp_col_max, rot_crop_matrix), axis=0)\n    rot_matrix /= tmp_col_max_sum\n\n    return rot_matrix", "response": "Helper function for opt_soft\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pcca_connected(P, n, return_rot=False):\n\n    # test connectivity\n    from msmtools.estimation import connected_sets\n\n    labels = connected_sets(P)\n    n_components = len(labels)  # (n_components, labels) = connected_components(P, connection='strong')\n    if (n_components > 1):\n        raise ValueError(\"Transition matrix is disconnected. Cannot use pcca_connected.\")\n\n    from msmtools.analysis import stationary_distribution\n\n    pi = stationary_distribution(P)\n    # print \"statdist = \",pi\n\n    from msmtools.analysis import is_reversible\n\n    if not is_reversible(P, mu=pi):\n        raise ValueError(\"Transition matrix does not fulfill detailed balance. \"\n                         \"Make sure to call pcca with a reversible transition matrix estimate\")\n    # TODO: Susanna mentioned that she has a potential fix for nonreversible matrices by replacing each complex conjugate\n    #      pair by the real and imaginary components of one of the two vectors. We could use this but would then need to\n    #      orthonormalize all eigenvectors e.g. using Gram-Schmidt orthonormalization. Currently there is no theoretical\n    #      foundation for this, so I'll skip it for now.\n\n    # right eigenvectors, ordered\n    from msmtools.analysis import eigenvectors\n\n    evecs = eigenvectors(P, n)\n\n    # orthonormalize\n    for i in range(n):\n        evecs[:, i] /= math.sqrt(np.dot(evecs[:, i] * pi, evecs[:, i]))\n    # make first eigenvector positive\n    evecs[:, 0] = np.abs(evecs[:, 0])\n\n    # Is there a significant complex component?\n    if not np.alltrue(np.isreal(evecs)):\n        warnings.warn(\n            \"The given transition matrix has complex eigenvectors, so it doesn't exactly fulfill detailed balance \"\n            + \"forcing eigenvectors to be real and continuing. Be aware that this is not theoretically solid.\")\n    evecs = np.real(evecs)\n\n    # create initial solution using PCCA+. This could have negative memberships\n    (chi, rot_matrix) = _pcca_connected_isa(evecs, n)\n\n    #print \"initial chi = \\n\",chi\n\n    # optimize the rotation matrix with PCCA++.\n    rot_matrix = _opt_soft(evecs, rot_matrix, n)\n\n    # These memberships should be nonnegative\n    memberships = np.dot(evecs[:, :], rot_matrix)\n\n    # We might still have numerical errors. Force memberships to be in [0,1]\n    # print \"memberships unnormalized: \",memberships\n    memberships = np.maximum(0.0, memberships)\n    memberships = np.minimum(1.0, memberships)\n    # print \"memberships unnormalized: \",memberships\n    for i in range(0, np.shape(memberships)[0]):\n        memberships[i] /= np.sum(memberships[i])\n\n    # print \"final chi = \\n\",chi\n\n    return memberships", "response": "This function is used to generate the PCCA - connected state model for a given transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pcca(P, m):\n    # imports\n    from msmtools.estimation import connected_sets\n    from msmtools.analysis import eigenvalues, is_transition_matrix, hitting_probability\n\n    # validate input\n    n = np.shape(P)[0]\n    if (m > n):\n        raise ValueError(\"Number of metastable states m = \" + str(m)+\n                         \" exceeds number of states of transition matrix n = \" + str(n))\n    if not is_transition_matrix(P):\n        raise ValueError(\"Input matrix is not a transition matrix.\")\n\n    # prepare output\n    chi = np.zeros((n, m))\n\n    # test connectivity\n    components = connected_sets(P)\n    # print \"all labels \",labels\n    n_components = len(components)  # (n_components, labels) = connected_components(P, connection='strong')\n    # print 'n_components'\n\n    # store components as closed (with positive equilibrium distribution)\n    # or as transition states (with vanishing equilibrium distribution)\n    closed_components = []\n    transition_states = []\n    for i in range(n_components):\n        component = components[i]  # np.argwhere(labels==i).flatten()\n        rest = list(set(range(n)) - set(component))\n        # is component closed?\n        if (np.sum(P[component, :][:, rest]) == 0):\n            closed_components.append(component)\n        else:\n            transition_states.append(component)\n    n_closed_components = len(closed_components)\n    closed_states = np.concatenate(closed_components)\n    if len(transition_states) == 0:\n        transition_states = np.array([], dtype=int)\n    else:\n        transition_states = np.concatenate(transition_states)\n\n    # check if we have enough clusters to support the disconnected sets\n    if (m < len(closed_components)):\n        raise ValueError(\"Number of metastable states m = \" + str(m) + \" is too small. Transition matrix has \" +\n                         str(len(closed_components)) + \" disconnected components\")\n\n    # We collect eigenvalues in order to decide which\n    closed_components_Psub = []\n    closed_components_ev = []\n    closed_components_enum = []\n    for i in range(n_closed_components):\n        component = closed_components[i]\n        # print \"component \",i,\" \",component\n        # compute eigenvalues in submatrix\n        Psub = P[component, :][:, component]\n        closed_components_Psub.append(Psub)\n        closed_components_ev.append(eigenvalues(Psub))\n        closed_components_enum.append(i * np.ones((component.size), dtype=int))\n\n    # flatten\n    closed_components_ev_flat = np.hstack(closed_components_ev)\n    closed_components_enum_flat = np.hstack(closed_components_enum)\n    # which components should be clustered?\n    component_indexes = closed_components_enum_flat[np.argsort(closed_components_ev_flat)][0:m]\n    # cluster each component\n    ipcca = 0\n    for i in range(n_closed_components):\n        component = closed_components[i]\n        # how many PCCA states in this component?\n        m_by_component = np.shape(np.argwhere(component_indexes == i))[0]\n\n        # if 1, then the result is trivial\n        if (m_by_component == 1):\n            chi[component, ipcca] = 1.0\n            ipcca += 1\n        elif (m_by_component > 1):\n            #print \"submatrix: \",closed_components_Psub[i]\n            chi[component, ipcca:ipcca + m_by_component] = _pcca_connected(closed_components_Psub[i], m_by_component)\n            ipcca += m_by_component\n        else:\n            raise RuntimeError(\"Component \" + str(i) + \" spuriously has \" + str(m_by_component) + \" pcca sets\")\n\n    # finally assign all transition states\n    # print \"chi\\n\", chi\n    # print \"transition states: \",transition_states\n    # print \"closed states: \", closed_states\n    if (transition_states.size > 0):\n        # make all closed states absorbing, so we can see which closed state we hit first\n        Pabs = P.copy()\n        Pabs[closed_states, :] = 0.0\n        Pabs[closed_states, closed_states] = 1.0\n        for i in range(closed_states.size):\n            # hitting probability to each closed state\n            h = hitting_probability(Pabs, closed_states[i])\n            for j in range(transition_states.size):\n                # transition states belong to closed states with the hitting probability, and inherit their chi\n                chi[transition_states[j]] += h[transition_states[j]] * chi[closed_states[i]]\n\n    # check if we have m metastable sets. If less than m, we must raise\n    nmeta = np.count_nonzero(chi.sum(axis=0))\n    assert m <= nmeta, str(m) + \" metastable states requested, but transition matrix only has \" + str(nmeta) \\\n                       + \". Consider using a prior or request less metastable states. \"\n\n    # print \"chi\\n\", chi\n    return chi", "response": "This function is used to compute the PCCA - based clustering algorithm for the Markov state models and data classification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coarsegrain(P, n):\n    M = pcca(P, n)\n    # coarse-grained transition matrix\n    W = np.linalg.inv(np.dot(M.T, M))\n    A = np.dot(np.dot(M.T, P), M)\n    P_coarse = np.dot(W, A)\n\n    # symmetrize and renormalize to eliminate numerical errors\n    from msmtools.analysis import stationary_distribution\n    pi_coarse = np.dot(M.T, stationary_distribution(P))\n    X = np.dot(np.diag(pi_coarse), P_coarse)\n    P_coarse = X / X.sum(axis=1)[:, None]\n\n    return P_coarse", "response": "Coarse - graining transition matrix P to n sets using PCCA\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_transition_matrix(T, tol=1e-12):\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    if _issparse(T):\n        return sparse.assessment.is_transition_matrix(T, tol)\n    else:\n        return dense.assessment.is_transition_matrix(T, tol)", "response": "r Check if the given matrix is a transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_connected(T, directed=True):\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    if _issparse(T):\n        return sparse.assessment.is_connected(T, directed=directed)\n    else:\n        T = _csr_matrix(T)\n        return sparse.assessment.is_connected(T, directed=directed)", "response": "r Check connectivity of the given transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stationary_distribution(T):\n    # is this a transition matrix?\n    if not is_transition_matrix(T):\n        raise ValueError(\"Input matrix is not a transition matrix.\"\n                         \"Cannot compute stationary distribution\")\n    # is the stationary distribution unique?\n    if not is_connected(T, directed=False):\n        raise ValueError(\"Input matrix is not weakly connected. \"\n                         \"Therefore it has no unique stationary \"\n                         \"distribution. Separate disconnected components \"\n                         \"and handle them separately\")\n    # we're good to go...\n    if _issparse(T):\n        mu = sparse.stationary_vector.stationary_distribution(T)\n    else:\n        mu = dense.stationary_vector.stationary_distribution(T)\n    return mu", "response": "r Compute stationary distribution of stochastic matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigenvectors(T, k=None, right=True, ncv=None, reversible=False, mu=None):\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    if _issparse(T):\n        if right:\n            return sparse.decomposition.eigenvectors(T, k=k, right=right, ncv=ncv)\n        else:\n            return sparse.decomposition.eigenvectors(T, k=k, right=right, ncv=ncv).T\n\n    else:\n        if right:\n            return dense.decomposition.eigenvectors(T, k=k, right=right)\n        else:\n            return dense.decomposition.eigenvectors(T, k=k, right=right).T", "response": "r Compute the eigenvectors of a transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mfpt(T, target, origin=None, tau=1, mu=None):\n    # check inputs\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    target = _types.ensure_int_vector(target)\n    origin = _types.ensure_int_vector_or_None(origin)\n    # go\n    if _issparse(T):\n        if origin is None:\n            t_tau = sparse.mean_first_passage_time.mfpt(T, target)\n        else:\n            t_tau = sparse.mean_first_passage_time.mfpt_between_sets(T, target, origin, mu=mu)\n    else:\n        if origin is None:\n            t_tau = dense.mean_first_passage_time.mfpt(T, target)\n        else:\n            t_tau = dense.mean_first_passage_time.mfpt_between_sets(T, target, origin, mu=mu)\n\n    # scale answer by lag time used.\n    return tau * t_tau", "response": "r Mean first passage times from a set of starting states to a set of target states."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the hitting probability of all states to the target states.", "response": "def hitting_probability(T, target):\n    \"\"\"\n    Computes the hitting probabilities for all states to the target states.\n\n    The hitting probability of state i to the target set A is defined as the minimal,\n    non-negative solution of:\n\n    .. math::\n        h_i^A &= 1                    \\:\\:\\:\\:  i\\in A \\\\\n        h_i^A &= \\sum_j p_{ij} h_i^A  \\:\\:\\:\\:  i \\notin A\n\n    Parameters\n    ----------\n    T : (M, M) ndarray or scipy.sparse matrix\n        Transition matrix\n    B : array_like\n        List of integer state labels for the target set\n\n    Returns\n    -------\n    h : ndarray(n)\n        a vector with hitting probabilities\n    \"\"\"\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    target = _types.ensure_int_vector(target)\n    if _issparse(T):\n        _showSparseConversionWarning()  # currently no sparse implementation!\n        return dense.hitting_probability.hitting_probability(T.toarray(), target)\n    else:\n        return dense.hitting_probability.hitting_probability(T, target)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expected_counts(T, p0, N):\n    # check input\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    p0 = _types.ensure_float_vector(p0, require_order=True)\n    # go\n    if _issparse(T):\n        return sparse.expectations.expected_counts(p0, T, N)\n    else:\n        return dense.expectations.expected_counts(p0, T, N)", "response": "r Compute expected transition counts for Markov chain with n steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expected_counts_stationary(T, N, mu=None):\n    # check input\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    mu = _types.ensure_float_vector_or_None(mu, require_order=True)\n    # go\n    if _issparse(T):\n        return sparse.expectations.expected_counts_stationary(T, N, mu=mu)\n    else:\n        return dense.expectations.expected_counts_stationary(T, N, mu=mu)", "response": "r Returns the expected transition counts for Markov chain in equilibrium."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fingerprint_relaxation(T, p0, obs, tau=1, k=None, ncv=None):\n    # check if square matrix and remember size\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    n = T.shape[0]\n    # will not do fingerprint analysis for nonreversible matrices\n    if not is_reversible(T):\n        raise ValueError('Fingerprint calculation is not supported for nonreversible transition matrices. ')\n    p0 = _types.ensure_ndarray(p0, ndim=1, size=n, kind='numeric')\n    obs = _types.ensure_ndarray(obs, ndim=1, size=n, kind='numeric')\n    # go\n    if _issparse(T):\n        return sparse.fingerprints.fingerprint_relaxation(T, p0, obs, tau=tau, k=k, ncv=ncv)\n    else:\n        return dense.fingerprints.fingerprint_relaxation(T, p0, obs, tau=tau, k=k)", "response": "r Returns a Dynamical fingerprint for relaxation experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expectation(T, a, mu=None):\n    # check if square matrix and remember size\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    n = T.shape[0]\n    a = _types.ensure_ndarray(a, ndim=1, size=n, kind='numeric')\n    mu = _types.ensure_ndarray_or_None(mu, ndim=1, size=n, kind='numeric')\n    # go\n    if not mu:\n        mu = stationary_distribution(T)\n    return _np.dot(mu, a)", "response": "r Returns the expectation value of a given observable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef relaxation(T, p0, obs, times=(1), k=None, ncv=None):\n    # check if square matrix and remember size\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    n = T.shape[0]\n    p0 = _types.ensure_ndarray(p0, ndim=1, size=n, kind='numeric')\n    obs = _types.ensure_ndarray(obs, ndim=1, size=n, kind='numeric')\n    times = _types.ensure_int_vector(times, require_order=True)\n    # go\n    if _issparse(T):\n        return sparse.fingerprints.relaxation(T, p0, obs, k=k, times=times)\n    else:\n        return dense.fingerprints.relaxation(T, p0, obs, k=k, times=times)", "response": "r Return a relaxation of the time - evolution of an expectation value starting at a non - equilibrium state space T p0 obs times and k."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pcca_object(T, m):\n    if _issparse(T):\n        _showSparseConversionWarning()\n        T = T.toarray()\n    T = _types.ensure_ndarray(T, ndim=2, uniform=True, kind='numeric')\n    return dense.pcca.PCCA(T, m)", "response": "Constructs the pcca object from dense or sparse matrix T and m."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eigenvector_sensitivity(T, k, j, right=True):\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    if _issparse(T):\n        _showSparseConversionWarning()\n        eigenvector_sensitivity(T.todense(), k, j, right=right)\n    else:\n        return dense.sensitivity.eigenvector_sensitivity(T, k, j, right=right)", "response": "r Sensitivity matrix of a selected eigenvector element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stationary_distribution_sensitivity(T, j):\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    if _issparse(T):\n        _showSparseConversionWarning()\n        stationary_distribution_sensitivity(T.todense(), j)\n    else:\n        return dense.sensitivity.stationary_distribution_sensitivity(T, j)", "response": "r Sensitivity matrix of a stationary distribution element."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef committor_sensitivity(T, A, B, i, forward=True):\n    # check inputs\n    T = _types.ensure_ndarray_or_sparse(T, ndim=2, uniform=True, kind='numeric')\n    A = _types.ensure_int_vector(A)\n    B = _types.ensure_int_vector(B)\n    if _issparse(T):\n        _showSparseConversionWarning()\n        committor_sensitivity(T.todense(), A, B, i, forward)\n    else:\n        if forward:\n            return dense.sensitivity.forward_committor_sensitivity(T, A, B, i)\n        else:\n            return dense.sensitivity.backward_committor_sensitivity(T, A, B, i)", "response": "r Compute the sensitivity matrix of a specified committor entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allclose_sparse(A, B, rtol=1e-5, atol=1e-8):\n    A = A.tocsr()\n    B = B.tocsr()\n\n    \"\"\"Shape\"\"\"\n    same_shape = (A.shape == B.shape)\n\n    \"\"\"Data\"\"\"\n    if same_shape:\n        diff = (A - B).data\n        same_data = np.allclose(diff, 0.0, rtol=rtol, atol=atol)\n\n        return same_data\n    else:\n        return False", "response": "Returns True if all the matrices A and B are equal in the same matter as numpy. allclose."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extensions():\n    USE_CYTHON = False\n    try:\n        from Cython.Build import cythonize\n        USE_CYTHON = True\n    except ImportError:\n        warnings.warn('Cython not found. Using pre cythonized files.')\n\n    from numpy import get_include as _np_inc\n    np_inc = _np_inc()\n\n    exts = []\n\n    mle_trev_given_pi_dense_module = \\\n        Extension('msmtools.estimation.dense.mle_trev_given_pi',\n                  sources=['msmtools/estimation/dense/mle_trev_given_pi.pyx',\n                           'msmtools/estimation/dense/_mle_trev_given_pi.c'],\n                  depends=['msmtools/util/sigint_handler.h'],\n                  include_dirs=['msmtools/estimation/dense', np_inc])\n\n    mle_trev_given_pi_sparse_module = \\\n        Extension('msmtools.estimation.sparse.mle_trev_given_pi',\n                  sources=['msmtools/estimation/sparse/mle_trev_given_pi.pyx',\n                           'msmtools/estimation/sparse/_mle_trev_given_pi.c'],\n                  depends=['msmtools/util/sigint_handler.h'],\n                  include_dirs=['msmtools/estimation/dense', np_inc])\n\n    mle_trev_dense_module = \\\n        Extension('msmtools.estimation.dense.mle_trev',\n                  sources=['msmtools/estimation/dense/mle_trev.pyx',\n                           'msmtools/estimation/dense/_mle_trev.c'],\n                  depends=['msmtools/util/sigint_handler.h'],\n                  include_dirs=[np_inc])\n\n    mle_trev_sparse_module = \\\n        Extension('msmtools.estimation.sparse.mle_trev',\n                  sources=['msmtools/estimation/sparse/mle_trev.pyx',\n                           'msmtools/estimation/sparse/_mle_trev.c'],\n                  depends=['msmtools/util/sigint_handler.h'],\n                  include_dirs=[np_inc,\n                                ])\n    rnglib_src = ['msmtools/estimation/dense/rnglib/rnglib.c',\n                  'msmtools/estimation/dense/rnglib/ranlib.c']\n\n    mle_trev_sparse_newton_module = \\\n        Extension('msmtools.estimation.sparse.newton.objective_sparse',\n                  sources=['msmtools/estimation/sparse/newton/objective_sparse.pyx'],\n                  libraries=['m'] if sys.platform != 'win32' else [],\n                  include_dirs=[np_inc,\n                                ]\n                  )\n\n    sampler_rev = \\\n        Extension('msmtools.estimation.dense.sampler_rev',\n                  sources=['msmtools/estimation/dense/sampler_rev.pyx',\n                           'msmtools/estimation/dense/sample_rev.c',\n                           ] + rnglib_src,\n                  include_dirs=[np_inc,\n                                ])\n\n    sampler_revpi = \\\n        Extension('msmtools.estimation.dense.sampler_revpi',\n                  sources=['msmtools/estimation/dense/sampler_revpi.pyx',\n                           'msmtools/estimation/dense/sample_revpi.c',\n                          ] + rnglib_src,\n                  include_dirs=[np_inc,\n                                ])\n\n    kahandot_module = \\\n        Extension('msmtools.util.kahandot',\n                  sources = ['msmtools/util/kahandot_src/kahandot.pyx',\n                             'msmtools/util/kahandot_src/_kahandot.c'],\n                  depends = ['msmtools/util/kahandot_src/_kahandot.h'],\n                  include_dirs=[np_inc,\n                                ])\n\n    exts += [mle_trev_given_pi_dense_module,\n             mle_trev_given_pi_sparse_module,\n             mle_trev_dense_module,\n             mle_trev_sparse_module,\n             mle_trev_sparse_newton_module,\n             sampler_rev,\n             sampler_revpi,\n             kahandot_module\n            ]\n\n    if USE_CYTHON: # if we have cython available now, cythonize module\n        exts = cythonize(exts)\n    else:\n        # replace pyx files by their pre generated c code.\n        for e in exts:\n            new_src = []\n            for s in e.sources:\n                new_src.append(s.replace('.pyx', '.c'))\n            e.sources = new_src\n\n    return exts", "response": "This function returns a list of all extensions that can be used to build the current version of the current version of the current version of the version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tmatrix_cov(C, row=None):\n\n    if row is None:\n        alpha = C + 1.0  # Dirichlet parameters\n        alpha0 = alpha.sum(axis=1)  # Sum of paramters (per row)\n\n        norm = alpha0 ** 2 * (alpha0 + 1.0)\n\n        \"\"\"Non-normalized covariance tensor\"\"\"\n        Z = -alpha[:, :, np.newaxis] * alpha[:, np.newaxis, :]\n\n        \"\"\"Correct-diagonal\"\"\"\n        ind = np.diag_indices(C.shape[0])\n        Z[:, ind[0], ind[1]] += alpha0[:, np.newaxis] * alpha\n\n        \"\"\"Covariance matrix\"\"\"\n        cov = Z / norm[:, np.newaxis, np.newaxis]\n\n        return cov\n\n    else:\n        alpha = C[row, :] + 1.0\n        return dirichlet_covariance(alpha)", "response": "r Returns the covariance matrix for the non - reversible transition matrix ensemble\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error_perturbation_single(C, S, R=None):\n    cov = tmatrix_cov(C)  # (M, M, M)\n    if R is None:\n        R = S\n    X = S[:, :, np.newaxis] * cov * R[:, np.newaxis, :]\n    return X.sum()", "response": "r Error - perturbation arising from a given sensitivity\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef error_perturbation_var(C, S):\n    K = S.shape[0]\n    cov = tmatrix_cov(C)\n    for i in range(K):\n        R = S[i, :, :]\n        X[i] = (R[:, :, np.newaxis] * cov * R[:, np.newaxis, :]).sum()", "response": "r Error - perturbation arising from a given sensitivity tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error_perturbation(C, S):\n    if len(S.shape) == 2:  # Scalar observable\n        return error_perturbation_single(C, S)\n    elif len(S.shape) == 3:  # Vector observable\n        return error_perturbation_cov(C, S)\n    else:\n        raise ValueError(\"Sensitivity matrix S has to be a 2d or 3d array\")", "response": "r Error perturbation for given sensitivity matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mfpt(T, target):\n    dim = T.shape[0]\n    A = eye(dim, dim) - T\n\n    \"\"\"Convert to DOK (dictionary of keys) matrix to enable\n    row-slicing and assignement\"\"\"\n    A = A.todok()\n    D = A.diagonal()\n    A[target, :] = 0.0\n    D[target] = 1.0\n    A.setdiag(D)\n    \"\"\"Convert back to CSR-format for fast sparse linear algebra\"\"\"\n    A = A.tocsr()\n    b = np.ones(dim)\n    b[target] = 0.0\n    m_t = spsolve(A, b)\n    return m_t", "response": "r Mean first passage times to a set of target states."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing mean - first - passage time between subsets of state space.", "response": "def mfpt_between_sets(T, target, origin, mu=None):\n    \"\"\"Compute mean-first-passage time between subsets of state space.\n\n    Parameters\n    ----------\n    T : scipy.sparse matrix\n        Transition matrix.\n    target : int or list of int\n        Set of target states.\n    origin : int or list of int\n        Set of starting states.\n    mu : (M,) ndarray (optional)\n        The stationary distribution of the transition matrix T.\n\n    Returns\n    -------\n    tXY : float\n        Mean first passage time between set X and Y.\n\n    Notes\n    -----\n    The mean first passage time :math:`\\mathbf{E}_X[T_Y]` is the expected\n    hitting time of one state :math:`y` in :math:`Y` when starting in a\n    state :math:`x` in :math:`X`:\n\n    .. math :: \\mathbb{E}_X[T_Y] = \\sum_{x \\in X}\n                \\frac{\\mu_x \\mathbb{E}_x[T_Y]}{\\sum_{z \\in X} \\mu_z}\n\n    \"\"\"\n    if mu is None:\n        mu = stationary_distribution(T)\n\n    \"\"\"Stationary distribution restriced on starting set X\"\"\"\n    nuX = mu[origin]\n    muX = nuX / np.sum(nuX)\n\n    \"\"\"Mean first-passage time to Y (for all possible starting states)\"\"\"\n    tY = mfpt(T, target)\n\n    \"\"\"Mean first-passage time from X to Y\"\"\"\n    tXY = np.dot(muX, tY[origin])\n    return tXY"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the full system of the inequality constraints.", "response": "def solve_full(z, Fval, DPhival, G, A):    \n    M, N=G.shape\n    P, N=A.shape\n\n    \"\"\"Total number of inequality constraints\"\"\"\n    m=M    \n\n    \"\"\"Primal variable\"\"\"\n    x=z[0:N]\n\n    \"\"\"Multiplier for equality constraints\"\"\"\n    nu=z[N:N+P]\n\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l=z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s=z[N+P+M:]\n\n    \"\"\"Dual infeasibility\"\"\"\n    rd = Fval[0:N]\n    \n    \"\"\"Primal infeasibility\"\"\"\n    rp1 = Fval[N:N+P]\n    rp2 = Fval[N+P:N+P+M]\n\n    \"\"\"Centrality\"\"\"\n    rc = Fval[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = np.diag(l/s)\n\n    \"\"\"Condensed system\"\"\"\n    if issparse(DPhival):\n        if not issparse(A):\n            A = csr_matrix(A)        \n        H = DPhival + mydot(G.T, mydot(SIG, G))\n        J = bmat([[H, A.T], [A, None]])\n    else:\n        if issparse(A):\n            A = A.toarray()\n        J = np.zeros((N+P, N+P))\n        J[0:N, 0:N] = DPhival + mydot(G.T, mydot(SIG, G))            \n        J[0:N, N:] = A.T\n        J[N:, 0:N] = A\n\n    b1 = -rd - mydot(G.T, mydot(SIG, rp2)) + mydot(G.T, rc/s)\n    b2 = -rp1\n    b = np.hstack((b1, b2))\n\n    \"\"\"Prepare iterative solve via MINRES\"\"\"\n    sign = np.zeros(N+P)\n    sign[0:N/2] = 1.0\n    sign[N/2:] = -1.0\n    S = diags(sign, 0)\n    J_new = mydot(S, csr_matrix(J))\n    b_new = mydot(S, b)\n\n    dJ_new = np.abs(J_new.diagonal())\n    dPc = np.ones(J_new.shape[0])\n    ind = (dJ_new > 0.0)\n    dPc[ind] = 1.0/dJ_new[ind]\n    Pc = diags(dPc, 0)    \n    dxnu, info = minres(J_new, b_new, tol=1e-8, M=Pc)\n    \n    # dxnu = solve(J, b)\n    dx = dxnu[0:N]\n    dnu = dxnu[N:]\n\n    \"\"\"Obtain search directions for l and s\"\"\"\n    ds = -rp2 - mydot(G, dx)\n    dl = -mydot(SIG, ds) - rc/s\n\n    dz = np.hstack((dx, dnu, dl, ds))\n    return dz"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naugment a set of inequality constraints with a set of inequality constraints.", "response": "def factor_aug(z, DPhival, G, A):    \n    M, N = G.shape\n    P, N = A.shape\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l = z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s = z[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n\n    \"\"\"Condensed system\"\"\"\n    if issparse(DPhival):\n        if not issparse(A):\n            A = csr_matrix(A)        \n        H = DPhival + mydot(G.T, mydot(SIG, G))\n        J = bmat([[H, A.T], [A, None]])\n    else:\n        if issparse(A):\n            A = A.toarray()\n        J = np.zeros((N+P, N+P))\n        J[0:N, 0:N] = DPhival + mydot(G.T, mydot(SIG, G))            \n        J[0:N, N:] = A.T\n        J[N:, 0:N] = A\n\n    LU = myfactor(J)    \n    return LU"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef solve_factorized_aug(z, Fval, LU, G, A):\n    M, N=G.shape\n    P, N=A.shape\n\n    \"\"\"Total number of inequality constraints\"\"\"\n    m = M    \n\n    \"\"\"Primal variable\"\"\"\n    x = z[0:N]\n\n    \"\"\"Multiplier for equality constraints\"\"\"\n    nu = z[N:N+P]\n\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l = z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s = z[N+P+M:]\n\n    \"\"\"Dual infeasibility\"\"\"\n    rd = Fval[0:N]\n    \n    \"\"\"Primal infeasibility\"\"\"\n    rp1 = Fval[N:N+P]\n    rp2 = Fval[N+P:N+P+M]\n\n    \"\"\"Centrality\"\"\"\n    rc = Fval[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n\n    \"\"\"RHS for condensed system\"\"\"\n    b1 = -rd - mydot(G.T, mydot(SIG, rp2)) + mydot(G.T, rc/s)\n    b2 = -rp1\n    b = np.hstack((b1, b2))\n    dxnu = mysolve(LU, b)\n    dx = dxnu[0:N]\n    dnu = dxnu[N:]\n\n    \"\"\"Obtain search directions for l and s\"\"\"\n    ds = -rp2 - mydot(G, dx)\n    dl = -mydot(SIG, ds) - rc/s\n\n    dz = np.hstack((dx, dnu, dl, ds))\n    return dz", "response": "Solve factorized augmented system with respect to inequality constraints."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef factor_schur(z, DPhival, G, A):\n    M, N = G.shape\n    P, N = A.shape\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l = z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s = z[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n\n    \"\"\"Augmented Jacobian\"\"\"\n    H = DPhival + mydot(G.T, mydot(SIG, G))\n\n    \"\"\"Factor H\"\"\"\n    LU_H = myfactor(H)\n\n    \"\"\"Compute H^{-1}A^{T}\"\"\"\n    HinvAt = mysolve(LU_H, A.T)\n\n    \"\"\"Compute Schur complement AH^{-1}A^{T}\"\"\"\n    S = mydot(A, HinvAt)\n\n    \"\"\"Factor Schur complement\"\"\"\n    LU_S = myfactor(S)\n\n    LU = (LU_S, LU_H)\n    return LU", "response": "Factor the Schur complement of a group of states."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef solve_factorized_schur(z, Fval, LU, G, A):\n    M, N=G.shape\n    P, N=A.shape\n\n    \"\"\"Total number of inequality constraints\"\"\"\n    m = M    \n\n    \"\"\"Primal variable\"\"\"\n    x = z[0:N]\n\n    \"\"\"Multiplier for equality constraints\"\"\"\n    nu = z[N:N+P]\n\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l = z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s = z[N+P+M:]\n\n    \"\"\"Dual infeasibility\"\"\"\n    rd = Fval[0:N]\n    \n    \"\"\"Primal infeasibility\"\"\"\n    rp1 = Fval[N:N+P]\n    rp2 = Fval[N+P:N+P+M]\n\n    \"\"\"Centrality\"\"\"\n    rc = Fval[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n\n    \"\"\"Assemble right hand side of augmented system\"\"\"\n    r1 = rd + mydot(G.T, mydot(SIG, rp2)) - mydot(G.T, rc/s)\n    r2 = rp1\n\n    \"\"\"Unpack LU-factors\"\"\"\n    LU_S, LU_H = LU\n\n    \"\"\"Assemble right hand side for normal equation\"\"\"\n    b = r2 - mydot(A, mysolve(LU_H, r1))  \n\n    \"\"\"Solve for dnu\"\"\"\n    dnu = mysolve(LU_S, b)\n       \n    \"\"\"Solve for dx\"\"\"\n    dx = mysolve(LU_H, -(r1 + mydot(A.T, dnu)))    \n    \n    \"\"\"Obtain search directions for l and s\"\"\"\n    ds = -rp2 - mydot(G, dx)\n    dl = -mydot(SIG, ds) - rc/s\n\n    dz = np.hstack((dx, dnu, dl, ds))\n    return dz", "response": "Solve factorized Schur problem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expected_counts_stationary(T, n, mu=None):\n    if (n <= 0):\n        EC = coo_matrix(T.shape, dtype=float)\n        return EC\n    else:\n        if mu is None:\n            mu = stationary_distribution(T)\n        D_mu = diags(mu, 0)\n        EC = n * D_mu.dot(T)\n        return EC", "response": "r Compute expected transition counts for Markov chain in equilibrium."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fingerprint_relaxation(P, p0, obs, tau=1, k=None, ncv=None):\n    one_vec = np.ones(P.shape[0])\n    return fingerprint(P, one_vec, obs2=obs, p0=p0, tau=tau, k=k, ncv=ncv)", "response": "r Computes the dynamical fingerprint of the relaxation experiment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef correlation_matvec(P, obs1, obs2=None, times=[1]):\n    if obs2 is None:\n        obs2 = obs1\n\n    \"\"\"Compute stationary vector\"\"\"\n    mu = statdist(P)\n    obs1mu = mu * obs1\n\n    times = np.asarray(times)\n    \"\"\"Sort in increasing order\"\"\"\n    ind = np.argsort(times)\n    times = times[ind]\n\n    if times[0] < 0:\n        raise ValueError(\"Times can not be negative\")\n    dt = times[1:] - times[0:-1]\n\n    nt = len(times)\n\n    correlations = np.zeros(nt)\n\n    \"\"\"Propagate obs2 to initial time\"\"\"\n    obs2_t = 1.0 * obs2\n    obs2_t = propagate(P, obs2_t, times[0])\n    correlations[0] = np.dot(obs1mu, obs2_t)\n    for i in range(nt - 1):\n        obs2_t = propagate(P, obs2_t, dt[i])\n        correlations[i + 1] = np.dot(obs1mu, obs2_t)\n\n    \"\"\"Cast back to original order of time points\"\"\"\n    correlations = correlations[ind]\n\n    return correlations", "response": "r Compute the time - correlation between two states in state space P and obs1 and obs2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relaxation_decomp(P, p0, obs, times=[1], k=None, ncv=None):\n    R, D, L = rdl_decomposition(P, k=k, ncv=ncv)\n    \"\"\"Extract eigenvalues\"\"\"\n    ev = np.diagonal(D)\n    \"\"\"Amplitudes\"\"\"\n    amplitudes = np.dot(p0, R) * np.dot(L, obs)\n    \"\"\"Propgate eigenvalues\"\"\"\n    times = np.asarray(times)\n    ev_t = ev[np.newaxis, :] ** times[:, np.newaxis]\n    \"\"\"Compute result\"\"\"\n    res = np.dot(ev_t, amplitudes)\n    \"\"\"Truncate imgainary part - is zero anyways\"\"\"\n    res = res.real\n    return res", "response": "rRelaxation. relaxation_decomposition The relaxation experiment describes the time - evolution of an expectation value starting at a non - equilibrium\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef propagate(A, x, N):\n    y = 1.0 * x\n    for i in range(N):\n        y = A.dot(y)\n    return y", "response": "r Use matrix A to propagate vector x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _maxlength(X):\n    return np.fromiter((map(lambda x: len(x), X)), dtype=int).max()", "response": "Returns the maximum length of signal trajectories X"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a discrete trajectory of the given transition matrix P with length N.", "response": "def generate_traj(P, N, start=None, stop=None, dt=1):\n    \"\"\"\n    Generates a realization of the Markov chain with transition matrix P.\n\n    Parameters\n    ----------\n    P : (n, n) ndarray\n        transition matrix\n    N : int\n        trajectory length\n    start : int, optional, default = None\n        starting state. If not given, will sample from the stationary distribution of P\n    stop : int or int-array-like, optional, default = None\n        stopping set. If given, the trajectory will be stopped before N steps\n        once a state of the stop set is reached\n    dt : int\n        trajectory will be saved every dt time steps.\n        Internally, the dt'th power of P is taken to ensure a more efficient simulation.\n\n    Returns\n    -------\n    traj_sliced : (N/dt, ) ndarray\n        A discrete trajectory with length N/dt\n\n    \"\"\"\n    sampler = MarkovChainSampler(P, dt=dt)\n    return sampler.trajectory(N, start=start, stop=stop)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate multiple realizations of the Markov chain with transition matrix P.", "response": "def generate_trajs(P, M, N, start=None, stop=None, dt=1):\n    \"\"\"\n    Generates multiple realizations of the Markov chain with transition matrix P.\n\n    Parameters\n    ----------\n    P : (n, n) ndarray\n        transition matrix\n    M : int\n        number of trajectories\n    N : int\n        trajectory length\n    start : int, optional, default = None\n        starting state. If not given, will sample from the stationary distribution of P\n    stop : int or int-array-like, optional, default = None\n        stopping set. If given, the trajectory will be stopped before N steps\n        once a state of the stop set is reached\n    dt : int\n        trajectory will be saved every dt time steps.\n        Internally, the dt'th power of P is taken to ensure a more efficient simulation.\n\n    Returns\n    -------\n    traj_sliced : (N/dt, ) ndarray\n        A discrete trajectory with length N/dt\n\n    \"\"\"\n    sampler = MarkovChainSampler(P, dt=dt)\n    return sampler.trajectories(M, N, start=start, stop=stop)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a trajectory realization of length N starting from state s .", "response": "def trajectory(self, N, start=None, stop=None):\n        \"\"\"\n        Generates a trajectory realization of length N, starting from state s\n\n        Parameters\n        ----------\n        N : int\n            trajectory length\n        start : int, optional, default = None\n            starting state. If not given, will sample from the stationary distribution of P\n        stop : int or int-array-like, optional, default = None\n            stopping set. If given, the trajectory will be stopped before N steps\n            once a state of the stop set is reached\n\n        \"\"\"\n        # check input\n        stop = types.ensure_int_vector_or_None(stop, require_order=False)\n\n        if start is None:\n            if self.mudist is None:\n                # compute mu, the stationary distribution of P\n                import msmtools.analysis as msmana\n                from scipy.stats import rv_discrete\n\n                mu = msmana.stationary_distribution(self.P)\n                self.mudist = rv_discrete(values=(np.arange(self.n), mu))\n            # sample starting point from mu\n            start = self.mudist.rvs()\n\n        # evaluate stopping set\n        stopat = np.ndarray((self.n), dtype=bool)\n        stopat[:] = False\n        if (stop is not None):\n            for s in np.array(stop):\n                stopat[s] = True\n\n        # result\n        traj = np.zeros(N, dtype=int)\n        traj[0] = start\n        # already at stopping state?\n        if stopat[traj[0]]:\n            return traj[:1]\n        # else run until end or stopping state\n        for t in range(1, N):\n            traj[t] = self.rgs[traj[t - 1]].rvs()\n            if stopat[traj[t]]:\n                return traj[:t+1]\n        # return\n        return traj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate M trajectories each of length N starting from state s starting from state s stopping from state s stopping from state s", "response": "def trajectories(self, M, N, start=None, stop=None):\n        \"\"\"\n        Generates M trajectories, each of length N, starting from state s\n\n        Parameters\n        ----------\n        M : int\n            number of trajectories\n        N : int\n            trajectory length\n        start : int, optional, default = None\n            starting state. If not given, will sample from the stationary distribution of P\n        stop : int or int-array-like, optional, default = None\n            stopping set. If given, the trajectory will be stopped before N steps\n            once a state of the stop set is reached\n\n        \"\"\"\n        trajs = [self.trajectory(N, start=start, stop=stop) for _ in range(M)]\n        return trajs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits the discrete trajectory into conditional sequences by starting state", "response": "def _split_sequences_singletraj(dtraj, nstates, lag):\n    \"\"\" splits the discrete trajectory into conditional sequences by starting state\n\n    Parameters\n    ----------\n    dtraj : int-iterable\n        discrete trajectory\n    nstates : int\n        total number of discrete states\n    lag : int\n        lag time\n\n    \"\"\"\n    sall = [[] for _ in range(nstates)]\n    res_states = []\n    res_seqs = []\n\n    for t in range(len(dtraj)-lag):\n        sall[dtraj[t]].append(dtraj[t+lag])\n\n    for i in range(nstates):\n        if len(sall[i]) > 0:\n            res_states.append(i)\n            res_seqs.append(np.array(sall[i]))\n    return res_states, res_seqs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting the discrete trajectories into conditional sequences by starting state", "response": "def _split_sequences_multitraj(dtrajs, lag):\n    \"\"\" splits the discrete trajectories into conditional sequences by starting state\n\n    Parameters\n    ----------\n    dtrajs : list of int-iterables\n        discrete trajectories\n    nstates : int\n        total number of discrete states\n    lag : int\n        lag time\n\n    \"\"\"\n    n = number_of_states(dtrajs)\n    res = []\n    for i in range(n):\n        res.append([])\n    for dtraj in dtrajs:\n        states, seqs = _split_sequences_singletraj(dtraj, n, lag)\n        for i in range(len(states)):\n            res[states[i]].append(seqs[i])\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of conditional sequences for transition i > j given all conditional sequences", "response": "def _indicator_multitraj(ss, i, j):\n    \"\"\" Returns conditional sequence for transition i -> j given all conditional sequences \"\"\"\n    iseqs = ss[i]\n    res = []\n    for iseq in iseqs:\n        x = np.zeros(len(iseq))\n        I = np.where(iseq == j)\n        x[I] = 1.0\n        res.append(x)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef statistical_inefficiencies(dtrajs, lag, C=None, truncate_acf=True, mact=2.0, n_jobs=1, callback=None):\n    # count matrix\n    if C is None:\n        C = count_matrix_coo2_mult(dtrajs, lag, sliding=True, sparse=True)\n    if callback is not None:\n        if not callable(callback):\n            raise ValueError('Provided callback is not callable')\n    # split sequences\n    splitseq = _split_sequences_multitraj(dtrajs, lag)\n    # compute inefficiencies\n    I, J = C.nonzero()\n    if n_jobs > 1:\n        from multiprocessing.pool import Pool, MapResult\n        from contextlib import closing\n        import tempfile\n\n        # to avoid pickling partial results, we store these in a numpy.memmap\n        ntf = tempfile.NamedTemporaryFile(delete=False)\n        arr = np.memmap(ntf.name, dtype=np.float64, mode='w+', shape=C.nnz)\n        #arr[:] = np.nan\n        gen = _arguments_generator(I, J, splitseq, truncate_acf=truncate_acf, mact=truncate_acf,\n                                   array=ntf.name, njobs=n_jobs)\n        if callback:\n            x = gen.n_blocks()\n            _callback = lambda _: callback(x)\n        else:\n            _callback = callback\n        with closing(Pool(n_jobs)) as pool:\n            result_async = [pool.apply_async(_wrapper, (args,), callback=_callback)\n                            for args in gen]\n\n            [t.get() for t in result_async]\n            data = np.array(arr[:])\n            #assert np.all(np.isfinite(data))\n        import os\n        os.unlink(ntf.name)\n    else:\n        data = np.empty(C.nnz)\n        for index, (i, j) in enumerate(zip(I, J)):\n            data[index] = statistical_inefficiency(_indicator_multitraj(splitseq, i, j),\n                                                   truncate_acf=truncate_acf, mact=mact)\n            if callback is not None:\n                callback(1)\n    res = csr_matrix((data, (I, J)), shape=C.shape)\n    return res", "response": "r Computes the statistical inefficiencies of sliding - window transition counts at given lag time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef effective_count_matrix(dtrajs, lag, average='row', truncate_acf=True, mact=1.0, n_jobs=1, callback=None):\n    # observed C\n    C = count_matrix_coo2_mult(dtrajs, lag, sliding=True, sparse=True)\n    # statistical inefficiencies\n    si = statistical_inefficiencies(dtrajs, lag, C=C, truncate_acf=truncate_acf, mact=mact,\n                                    n_jobs=n_jobs, callback=callback)\n    # effective element-wise counts\n    Ceff = C.multiply(si)\n    # averaging\n    if average.lower() == 'row':\n        # reduction factor by row\n        factor = np.array(Ceff.sum(axis=1) / np.maximum(1.0, C.sum(axis=1)))\n        # row-based effective counts\n        Ceff = scipy.sparse.csr_matrix(C.multiply(factor))\n    elif average.lower() == 'all':\n        # reduction factor by all\n        factor = Ceff.sum() / C.sum()\n        Ceff = scipy.sparse.csr_matrix(C.multiply(factor))\n    # else: by element, we're done.\n\n    return Ceff", "response": "r Computes the statistically effective transition count matrix for a given set of discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transition_matrix_non_reversible(C):\n    # multiply by 1.0 to make sure we're not doing integer division\n    rowsums = 1.0 * np.sum(C, axis=1)\n    if np.min(rowsums) <= 0:\n        raise ValueError(\n            \"Transition matrix has row sum of \" + str(np.min(rowsums)) + \". Must have strictly positive row sums.\")\n    return np.divide(C, rowsums[:, np.newaxis])", "response": "r Estimates a non - reversible transition matrix from count matrix C."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef backward_iteration(A, mu, x0, tol=1e-14, maxiter=100):\n    T = A - mu * np.eye(A.shape[0])\n    \"\"\"LU-factor of T\"\"\"\n    lupiv = lu_factor(T)\n    \"\"\"Starting iterate with ||y_0||=1\"\"\"\n    r0 = 1.0 / np.linalg.norm(x0)\n    y0 = x0 * r0\n    \"\"\"Local variables for inverse iteration\"\"\"\n    y = 1.0 * y0\n    r = 1.0 * r0\n    for i in range(maxiter):\n        x = lu_solve(lupiv, y)\n        r = 1.0 / np.linalg.norm(x)\n        y = x * r\n        if r <= tol:\n            return y\n    msg = \"Failed to converge after %d iterations, residuum is %e\" % (maxiter, r)\n    raise RuntimeError(msg)", "response": "r Finds the eigenvector to approximate eigenvalue for desired eigenvalue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef time_correlation_by_diagonalization(P, pi, obs1, obs2=None, time=1, rdl=None):\n    if rdl is None:\n        raise ValueError(\"no rdl decomposition\")\n    R, D, L = rdl\n\n    d_times = np.diag(D) ** time\n    diag_inds = np.diag_indices_from(D)\n    D_time = np.zeros(D.shape, dtype=d_times.dtype)\n    D_time[diag_inds] = d_times\n    P_time = np.dot(np.dot(R, D_time), L)\n\n    # multiply element-wise obs1 and pi. this is obs1' diag(pi)\n    l = np.multiply(obs1, pi)\n    m = np.dot(P_time, obs2)\n    result = np.dot(l, m)\n    return result", "response": "Calculates time correlation. Raises P to power times by diagonalization."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef time_correlations_direct(P, pi, obs1, obs2=None, times=[1]):\n    n_t = len(times)\n    times = np.sort(times)  # sort it to use caching of previously computed correlations\n    f = np.zeros(n_t)\n\n    # maximum time > number of rows?\n    if times[-1] > P.shape[0]:\n        use_diagonalization = True\n        R, D, L = rdl_decomposition(P)\n        # discard imaginary part, if all elements i=0\n        if not np.any(np.iscomplex(R)):\n            R = np.real(R)\n        if not np.any(np.iscomplex(D)):\n            D = np.real(D)\n        if not np.any(np.iscomplex(L)):\n            L = np.real(L)\n        rdl = (R, D, L)\n\n    if use_diagonalization:\n        for i in range(n_t):\n            f[i] = time_correlation_by_diagonalization(P, pi, obs1, obs2, times[i], rdl)\n    else:\n        start_values = None\n        for i in range(n_t):\n            f[i], start_values = \\\n                time_correlation_direct_by_mtx_vec_prod(P, pi, obs1, obs2,\n                                                        times[i], start_values, True)\n    return f", "response": "r Computes the time - cross - correlation of obs1 or time - cross - correlation with obs2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef time_relaxations_direct(P, p0, obs, times=[1]):\n    n_t = len(times)\n    times = np.sort(times)\n\n    # maximum time > number of rows?\n    if times[-1] > P.shape[0]:\n        use_diagonalization = True\n        R, D, L = rdl_decomposition(P)\n        # discard imaginary part, if all elements i=0\n        if not np.any(np.iscomplex(R)):\n            R = np.real(R)\n        if not np.any(np.iscomplex(D)):\n            D = np.real(D)\n        if not np.any(np.iscomplex(L)):\n            L = np.real(L)\n        rdl = (R, D, L)\n\n    f = np.empty(n_t, dtype=D.dtype)\n\n    if use_diagonalization:\n        for i in range(n_t):\n            f[i] = time_relaxation_direct_by_diagonalization(P, p0, obs, times[i], rdl)\n    else:\n        start_values = None\n        for i in range(n_t):\n            f[i], start_values = time_relaxation_direct_by_mtx_vec_prod(P, p0, obs, times[i], start_values, True)\n    return f", "response": "r Compute time - relaxations of obs with respect of given initial distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef factor_aug(z, DPhival, G, A):\n    M, N = G.shape\n    P, N = A.shape\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l = z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s = z[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n    # SIG = diags(l*s, 0)\n\n    \"\"\"Convert A\"\"\"\n    if not issparse(A):\n        A = csr_matrix(A)\n\n    \"\"\"Convert G\"\"\"\n    if not issparse(G):\n        G = csr_matrix(G)\n\n    \"\"\"Since we expect symmetric DPhival, we need to change A\"\"\"\n    sign = np.zeros(N)\n    sign[0:N//2] = 1.0\n    sign[N//2:] = -1.0\n    T = diags(sign, 0)\n\n    A_new = A.dot(T)\n    \n    W = AugmentedSystem(DPhival, G, SIG, A_new)\n    return W", "response": "r Augment the current iterate z with the current factor DPhival and G and A."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solve_factorized_aug(z, Fval, LU, G, A):\n    M, N=G.shape\n    P, N=A.shape\n\n    \"\"\"Total number of inequality constraints\"\"\"\n    m=M    \n\n    \"\"\"Primal variable\"\"\"\n    x=z[0:N]\n\n    \"\"\"Multiplier for equality constraints\"\"\"\n    nu=z[N:N+P]\n\n    \"\"\"Multiplier for inequality constraints\"\"\"\n    l=z[N+P:N+P+M]\n\n    \"\"\"Slacks\"\"\"\n    s=z[N+P+M:]\n\n    \"\"\"Dual infeasibility\"\"\"\n    rd = Fval[0:N]\n    \n    \"\"\"Primal infeasibility\"\"\"\n    rp1 = Fval[N:N+P]\n    rp2 = Fval[N+P:N+P+M]\n\n    \"\"\"Centrality\"\"\"\n    rc = Fval[N+P+M:]\n\n    \"\"\"Sigma matrix\"\"\"\n    SIG = diags(l/s, 0)\n\n    \"\"\"LU is actually the augmented system W\"\"\"\n    W = LU\n\n    b1 = -rd - mydot(G.T, mydot(SIG, rp2)) + mydot(G.T, rc/s)\n    b2 = -rp1\n    b = np.hstack((b1, b2))\n\n    \"\"\"Prepare iterative solve via MINRES\"\"\"\n    sign = np.zeros(N+P)\n    sign[0:N//2] = 1.0\n    sign[N//2:] = -1.0\n    T = diags(sign, 0)        \n   \n    \"\"\"Change rhs\"\"\"\n    b_new = mydot(T, b)\n\n    dW = np.abs(W.diagonal())\n    dPc = np.ones(W.shape[0])\n    ind = (dW > 0.0)\n    dPc[ind] = 1.0/dW[ind]\n    Pc = diags(dPc, 0)    \n    dxnu, info = minres(W, b_new, tol=1e-10, M=Pc)\n    \n    # dxnu = solve(J, b)\n    dx = dxnu[0:N]\n    dnu = dxnu[N:]\n\n    \"\"\"Obtain search directions for l and s\"\"\"\n    ds = -rp2 - mydot(G, dx)\n    # ds = s*ds\n    # SIG = np.diag(l/s)\n    dl = -mydot(SIG, ds) - rc/s\n\n    dz = np.hstack((dx, dnu, dl, ds))\n    return dz", "response": "Solve factorized augmented system with respect to the given Fval."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pathways(self, fraction=1.0, maxiter=1000):\n        return tptapi.pathways(self.net_flux, self.A, self.B,\n                               fraction=fraction, maxiter=maxiter)", "response": "Decompose flux network into dominant reaction paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pathways_to_flux(self, paths, pathfluxes, n=None):\n        if (n is None):\n            n = 0\n            for p in paths:\n                n = max(n, np.max(p))\n            n += 1\n\n        # initialize flux\n        F = np.zeros((n, n))\n        for i in range(len(paths)):\n            p = paths[i]\n            for t in range(len(p) - 1):\n                F[p[t], p[t + 1]] += pathfluxes[i]\n        return F", "response": "r Sums up the flux from the pathways given\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_coarse_sets(self, user_sets):\n        # set-ify everything\n        setA = set(self.A)\n        setB = set(self.B)\n        setI = set(self.I)\n        raw_sets = [set(user_set) for user_set in user_sets]\n\n        # anything missing? Compute all listed states\n        set_all = set(range(self.nstates))\n        set_all_user = []\n        for user_set in raw_sets:\n            set_all_user += user_set\n        set_all_user = set(set_all_user)\n        # ... and add all the unlisted states in a separate set\n        set_rest = set_all - set_all_user\n        if len(set_rest) > 0:\n            raw_sets.append(set_rest)\n\n        # split sets\n        Asets = []\n        Isets = []\n        Bsets = []\n        for raw_set in raw_sets:\n            s = raw_set.intersection(setA)\n            if len(s) > 0:\n                Asets.append(s)\n            s = raw_set.intersection(setI)\n            if len(s) > 0:\n                Isets.append(s)\n            s = raw_set.intersection(setB)\n            if len(s) > 0:\n                Bsets.append(s)\n        tpt_sets = Asets + Isets + Bsets\n        Aindexes = list(range(0, len(Asets)))\n        Bindexes = list(range(len(Asets) + len(Isets), len(tpt_sets)))\n\n        return (tpt_sets, Aindexes, Bindexes)", "response": "r This method computes the sets to coarse - grain the tpt flux to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coarse_grain(self, user_sets):\n        # coarse-grain sets\n        (tpt_sets, Aindexes, Bindexes) = self._compute_coarse_sets(user_sets)\n        nnew = len(tpt_sets)\n\n        # coarse-grain fluxHere we should branch between sparse and dense implementations, but currently there is only a\n        F_coarse = tptapi.coarsegrain(self._gross_flux, tpt_sets)\n        Fnet_coarse = tptapi.to_netflux(F_coarse)\n\n        # coarse-grain stationary probability and committors - this can be done all dense\n        pstat_coarse = np.zeros((nnew))\n        forward_committor_coarse = np.zeros((nnew))\n        backward_committor_coarse = np.zeros((nnew))\n        for i in range(0, nnew):\n            I = list(tpt_sets[i])\n            muI = self._mu[I]\n            pstat_coarse[i] = np.sum(muI)\n            partialI = muI / pstat_coarse[i]  # normalized stationary probability over I\n            forward_committor_coarse[i] = np.dot(partialI, self._qplus[I])\n            backward_committor_coarse[i] = np.dot(partialI, self._qminus[I])\n\n        res = ReactiveFlux(Aindexes, Bindexes, Fnet_coarse, mu=pstat_coarse,\n                           qminus=backward_committor_coarse, qplus=forward_committor_coarse, gross_flux=F_coarse)\n        return (tpt_sets, res)", "response": "r Coarse - grain the flux onto the user - defined sets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transition_matrix(self):\n        P0 = np.diag(self.r, k=0)\n        P1 = np.diag(self.p[0:-1], k=1)\n        P_1 = np.diag(self.q[1:], k=-1)\n        return P0 + P1 + P_1", "response": "Tridiagonal transition matrix for the given base - level birth and death chain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforwarding committor for birth - and - death - chain.", "response": "def committor_forward(self, a, b):\n        r\"\"\"Forward committor for birth-and-death-chain.\n\n        The forward committor is the probability to hit\n        state b before hitting state a starting in state x,\n\n            u_x=P_x(T_b<T_a)\n\n        T_i is the first arrival time of the chain to state i,\n\n            T_i = inf( t>0 | X_t=i )\n\n        Parameters\n        ----------\n        a : int\n            State index\n        b : int\n            State index\n\n        Returns\n        -------\n        u : (M,) ndarray\n            Vector of committor probabilities.\n\n        \"\"\"\n        u = np.zeros(self.dim)\n        g = np.zeros(self.dim - 1)\n        g[0] = 1.0\n        g[1:] = np.cumprod(self.q[1:-1] / self.p[1:-1])\n\n        \"\"\"If a and b are equal the event T_b<T_a is impossible\n           for any starting state x so that the committor is\n           zero everywhere\"\"\"\n        if a == b:\n            return u\n        elif a < b:\n            \"\"\"Birth-death chain has to hit a before it can hit b\"\"\"\n            u[0:a + 1] = 0.0\n            \"\"\"Birth-death chain has to hit b before it can hit a\"\"\"\n            u[b:] = 1.0\n            \"\"\"Intermediate states are given in terms of sums of g\"\"\"\n            u[a + 1:b] = np.cumsum(g[a:b])[0:-1] / np.sum(g[a:b])\n            return u\n        else:\n            u[0:b + 1] = 1.0\n            u[a:] = 0.0\n            u[b + 1:a] = (np.cumsum(g[b:a])[0:-1] / np.sum(g[b:a]))[::-1]\n            return u"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef totalflux(self, a, b):\n        flux = self.flux(a, b)\n        A = list(range(a + 1))\n        notA = list(range(a + 1, flux.shape[0]))\n        F = flux[A, :][:, notA].sum()\n        return F", "response": "r The tiotal flux for the reaction\n            A = 0... a = 1... b = 1... M = 2... M = 2... M = 3..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnormalize a transition matrix T to be used with the reversible estimators to fix an almost coverged transition matrix.", "response": "def correct_transition_matrix(T, reversible=None):\n    r\"\"\"Normalize transition matrix\n\n    Fixes a the row normalization of a transition matrix.\n    To be used with the reversible estimators to fix an almost coverged\n    transition matrix.\n\n    Parameters\n    ----------\n    T : (M, M) ndarray\n        matrix to correct\n    reversible : boolean\n        for future use\n\n    Returns\n    -------\n    (M, M) ndarray\n        corrected transition matrix\n    \"\"\"\n    row_sums = T.sum(axis=1).A1\n    max_sum = np.max(row_sums)\n    if max_sum == 0.0:\n         max_sum = 1.0\n    return (T + scipy.sparse.diags(-row_sums+max_sum, 0)) / max_sum"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transition_matrix_reversible_pisym(C, return_statdist=False, **kwargs):\n    # nonreversible estimate\n    T_nonrev = transition_matrix_non_reversible(C)\n    from msmtools.analysis import stationary_distribution\n    pi = stationary_distribution(T_nonrev)\n    # correlation matrix\n    X = scipy.sparse.diags(pi).dot(T_nonrev)\n    X = X.T + X\n    # result\n    pi_rev = np.array(X.sum(axis=1)).squeeze()\n    T_rev = scipy.sparse.diags(1.0/pi_rev).dot(X)\n    if return_statdist:\n        #np.testing.assert_allclose(pi, stationary_distribution(T_rev))\n        #np.testing.assert_allclose(T_rev.T.dot(pi), pi)\n        return T_rev, pi\n    return T_rev", "response": "r Estimates reversible transition matrix estimate for a given count matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_likelihood(C, T):\n    C = C.tocsr()\n    T = T.tocsr()\n    ind = scipy.nonzero(C)\n    relT = np.array(T[ind])[0, :]\n    relT = np.log(relT)\n    relC = np.array(C[ind])[0, :]\n\n    return relT.dot(relC)", "response": "get\n    implementation of likelihood of C given T\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forward_committor(T, A, B):\n    X = set(range(T.shape[0]))\n    A = set(A)\n    B = set(B)\n    AB = A.intersection(B)\n    notAB = X.difference(A).difference(B)\n    if len(AB) > 0:\n        raise ValueError(\"Sets A and B have to be disjoint\")\n    L = T - eye(T.shape[0], T.shape[0])\n\n    \"\"\"Assemble left hand-side W for linear system\"\"\"\n    \"\"\"Equation (I)\"\"\"\n    W = 1.0 * L\n\n    \"\"\"Equation (II)\"\"\"\n    W = W.todok()\n    W[list(A), :] = 0.0\n    W.tocsr()\n    W = W + coo_matrix((np.ones(len(A)), (list(A), list(A))), shape=W.shape).tocsr()\n\n    \"\"\"Equation (III)\"\"\"\n    W = W.todok()\n    W[list(B), :] = 0.0\n    W.tocsr()\n    W = W + coo_matrix((np.ones(len(B)), (list(B), list(B))), shape=W.shape).tocsr()\n\n    \"\"\"Assemble right hand side r for linear system\"\"\"\n    \"\"\"Equation (I+II)\"\"\"\n    r = np.zeros(T.shape[0])\n    \"\"\"Equation (III)\"\"\"\n    r[list(B)] = 1.0\n\n    u = spsolve(W, r)\n    return u", "response": "r Forward committor between given sets A and B."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef backward_committor(T, A, B):\n    X = set(range(T.shape[0]))\n    A = set(A)\n    B = set(B)\n    AB = A.intersection(B)\n    notAB = X.difference(A).difference(B)\n    if len(AB) > 0:\n        raise ValueError(\"Sets A and B have to be disjoint\")\n    pi = stationary_distribution(T)\n    L = T - eye(T.shape[0], T.shape[0])\n    D = diags([pi, ], [0, ])\n    K = (D.dot(L)).T\n\n    \"\"\"Assemble left-hand side W for linear system\"\"\"\n    \"\"\"Equation (I)\"\"\"\n    W = 1.0 * K\n\n    \"\"\"Equation (II)\"\"\"\n    W = W.todok()\n    W[list(A), :] = 0.0\n    W.tocsr()\n    W = W + coo_matrix((np.ones(len(A)), (list(A), list(A))), shape=W.shape).tocsr()\n\n    \"\"\"Equation (III)\"\"\"\n    W = W.todok()\n    W[list(B), :] = 0.0\n    W.tocsr()\n    W = W + coo_matrix((np.ones(len(B)), (list(B), list(B))), shape=W.shape).tocsr()\n\n    \"\"\"Assemble right-hand side r for linear system\"\"\"\n    \"\"\"Equation (I)+(III)\"\"\"\n    r = np.zeros(T.shape[0])\n    \"\"\"Equation (II)\"\"\"\n    r[list(A)] = 1.0\n\n    u = spsolve(W, r)\n\n    return u", "response": "r Returns the backward committor between given sets A and B."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload_file(token, channel_name, file_name):\n\n    slack = Slacker(token)\n\n    slack.files.upload(file_name, channels=channel_name)", "response": "Upload a file to a channel"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef args_priority(args, environ):\n    '''\n        priority of token\n        1) as argumment: -t\n        2) as environ variable\n\n        priority of as_user\n        1) as argument: -a\n        2) as environ variable\n    '''\n\n    arg_token = args.token\n    arg_as_user = args.as_user\n\n    slack_token_var_name = 'SLACK_TOKEN'\n    if slack_token_var_name in environ.keys():\n        token = environ[slack_token_var_name]\n    else:\n        token = None\n\n    if arg_token:\n        token = arg_token\n\n    # slack as_user\n    slack_as_user_var_name = 'SLACK_AS_USER'\n    as_user = bool(environ.get(slack_as_user_var_name))\n\n    if arg_as_user:\n        as_user = True\n\n    return token, as_user, args.channel", "response": "Returns the priority of the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up the V8 machinery.", "response": "def set_up():\n    \"\"\"\n    Set ups the V8 machinery:\\\n    platform, VM and context.\n\n    This function is not thread-safe,\\\n    it must be called from a place\\\n    where is guaranteed it will be\\\n    called once and only once.\\\n    Probably within the main-thread\\\n    at import time.\n    \"\"\"\n    global _context\n\n    if _context is not None:\n        raise AssertionError(\n            'This function must only be called '\n            'once in an application lifetime')\n\n    platform.set_up()\n    vm = platform.create_vm()\n    vm.set_up()\n    _context = vm.create_context()\n    _context.set_up()\n    atexit.register(_tear_down)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the minimization. Returns ------- K : (N,N) ndarray the optimal rate matrix", "response": "def run(self):\n        \"\"\"Run the minimization.\n\n        Returns\n        -------\n        K : (N,N) ndarray\n            the optimal rate matrix\n        \"\"\"\n        if self.verbose:\n            self.selftest()\n        self.count = 0\n        if self.verbose:\n            logging.info('initial value of the objective function is %f'\n                         % self.function(self.initial))\n        theta0 = self.initial\n        theta, f, d = fmin_l_bfgs_b(self.function_and_gradient, theta0, fprime=None, args=(),\n                                    approx_grad=False, bounds=self.bounds, factr=self.tol,\n                                    pgtol=1.0E-11, disp=0, maxiter=self.maxiter, maxfun=self.maxiter, maxls=100)\n        if self.verbose:\n            logging.info('l_bfgs_b says: '+str(d))\n            logging.info('objective function value reached: %f' % f)\n        if d['warnflag'] != 0:\n            raise_or_warn(str(d), on_error=self.on_error, warning=NotConvergedWarning, exception=NotConvergedError)\n\n        K = np.zeros((self.N, self.N))\n        K[self.I, self.J] = theta / self.pi[self.I]\n        K[self.J, self.I] = theta / self.pi[self.J]\n        np.fill_diagonal(K, -np.sum(K, axis=1))\n        self.K = K\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_forwarders(resolv=\"resolv.conf\"):\n    ns = []\n    if os.path.exists(resolv):\n        for l in open(resolv):\n            if l.startswith(\"nameserver\"):\n                address = l.strip().split(\" \", 2)[1]\n                # forwarding to ourselves would be bad\n                if not address.startswith(\"127\"):\n                    ns.append(address)\n    if not ns:\n        ns = ['8.8.8.8', '8.8.4.4']\n    return ns", "response": "Find the forwarders in the given resolv. conf file default to 8. 8 and 8. 4. 4"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning source code with quotes unified.", "response": "def format_code(source, preferred_quote=\"'\"):\n    \"\"\"Return source code with quotes unified.\"\"\"\n    try:\n        return _format_code(source, preferred_quote)\n    except (tokenize.TokenError, IndentationError):\n        return source"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning source code with quotes unified.", "response": "def _format_code(source, preferred_quote):\n    \"\"\"Return source code with quotes unified.\"\"\"\n    if not source:\n        return source\n\n    modified_tokens = []\n\n    sio = io.StringIO(source)\n    for (token_type,\n         token_string,\n         start,\n         end,\n         line) in tokenize.generate_tokens(sio.readline):\n\n        if token_type == tokenize.STRING:\n            token_string = unify_quotes(token_string,\n                                        preferred_quote=preferred_quote)\n\n        modified_tokens.append(\n            (token_type, token_string, start, end, line))\n\n    return untokenize.untokenize(modified_tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unify_quotes(token_string, preferred_quote):\n    bad_quote = {'\"': \"'\",\n                 \"'\": '\"'}[preferred_quote]\n\n    allowed_starts = {\n        '': bad_quote,\n        'f': 'f' + bad_quote,\n        'b': 'b' + bad_quote\n    }\n\n    if not any(token_string.startswith(start)\n               for start in allowed_starts.values()):\n        return token_string\n\n    if token_string.count(bad_quote) != 2:\n        return token_string\n\n    if preferred_quote in token_string:\n        return token_string\n\n    assert token_string.endswith(bad_quote)\n    assert len(token_string) >= 2\n    for prefix, start in allowed_starts.items():\n        if token_string.startswith(start):\n            chars_to_strip_from_front = len(start)\n            return '{prefix}{preferred_quote}{token}{preferred_quote}'.format(\n                prefix=prefix,\n                preferred_quote=preferred_quote,\n                token=token_string[chars_to_strip_from_front:-1]\n            )", "response": "Return string with quotes changed to preferred_quote if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_with_encoding(filename, encoding, mode='r'):\n    return io.open(filename, mode=mode, encoding=encoding,\n                   newline='')", "response": "Return opened file with a specific encoding."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetecting encoding of the file.", "response": "def detect_encoding(filename):\n    \"\"\"Return file encoding.\"\"\"\n    try:\n        with open(filename, 'rb') as input_file:\n            from lib2to3.pgen2 import tokenize as lib2to3_tokenize\n            encoding = lib2to3_tokenize.detect_encoding(input_file.readline)[0]\n\n            # Check for correctness of encoding.\n            with open_with_encoding(filename, encoding) as input_file:\n                input_file.read()\n\n        return encoding\n    except (SyntaxError, LookupError, UnicodeDecodeError):\n        return 'latin-1'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning format_code on a file.", "response": "def format_file(filename, args, standard_out):\n    \"\"\"Run format_code() on a file.\n\n    Returns `True` if any changes are needed and they are not being done\n    in-place.\n\n    \"\"\"\n    encoding = detect_encoding(filename)\n    with open_with_encoding(filename, encoding=encoding) as input_file:\n        source = input_file.read()\n        formatted_source = format_code(\n            source,\n            preferred_quote=args.quote)\n\n    if source != formatted_source:\n        if args.in_place:\n            with open_with_encoding(filename, mode='w',\n                                    encoding=encoding) as output_file:\n                output_file.write(formatted_source)\n        else:\n            import difflib\n            diff = difflib.unified_diff(\n                source.splitlines(),\n                formatted_source.splitlines(),\n                'before/' + filename,\n                'after/' + filename,\n                lineterm='')\n            standard_out.write('\\n'.join(list(diff) + ['']))\n\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _main(argv, standard_out, standard_error):\n    import argparse\n    parser = argparse.ArgumentParser(description=__doc__, prog='unify')\n    parser.add_argument('-i', '--in-place', action='store_true',\n                        help='make changes to files instead of printing diffs')\n    parser.add_argument('-c', '--check-only', action='store_true',\n                        help='exit with a status code of 1 if any changes are'\n                             ' still needed')\n    parser.add_argument('-r', '--recursive', action='store_true',\n                        help='drill down directories recursively')\n    parser.add_argument('--quote', help='preferred quote', choices=[\"'\", '\"'],\n                        default=\"'\")\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s ' + __version__)\n    parser.add_argument('files', nargs='+',\n                        help='files to format')\n\n    args = parser.parse_args(argv[1:])\n\n    filenames = list(set(args.files))\n    changes_needed = False\n    failure = False\n    while filenames:\n        name = filenames.pop(0)\n        if args.recursive and os.path.isdir(name):\n            for root, directories, children in os.walk(unicode(name)):\n                filenames += [os.path.join(root, f) for f in children\n                              if f.endswith('.py') and\n                              not f.startswith('.')]\n                directories[:] = [d for d in directories\n                                  if not d.startswith('.')]\n        else:\n            try:\n                if format_file(name, args=args, standard_out=standard_out):\n                    changes_needed = True\n            except IOError as exception:\n                print(unicode(exception), file=standard_error)\n                failure = True\n\n    if failure or (args.check_only and changes_needed):\n        return 1", "response": "Main function for unifying the files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef effective_count_matrix(dtrajs, lag, average='row', mact=1.0, n_jobs=1, callback=None):\n\n    dtrajs = _ensure_dtraj_list(dtrajs)\n    import os\n    # enforce one job on windows.\n    if os.name == 'nt':\n        n_jobs = 1\n    return sparse.effective_counts.effective_count_matrix(dtrajs, lag, average=average, mact=mact, n_jobs=n_jobs, callback=callback)", "response": "r Computes the statistically effective transition count matrix for a given set of discrete trajectories and lag time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connected_sets(C, directed=True):\n    if isdense(C):\n        return sparse.connectivity.connected_sets(csr_matrix(C), directed=directed)\n    else:\n        return sparse.connectivity.connected_sets(C, directed=directed)", "response": "r Compute the connected sets of microstates given by the count matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks connectivity of the given count matrix.", "response": "def is_connected(C, directed=True):\n    \"\"\"Check connectivity of the given matrix.\n\n    Parameters\n    ----------\n    C : scipy.sparse matrix\n        Count matrix specifying edge weights.\n    directed : bool, optional\n       Whether to compute connected components for a directed or\n       undirected graph. Default is True.\n\n    Returns\n    -------\n    is_connected: bool\n        True if C is connected, False otherwise.\n\n    See also\n    --------\n    largest_connected_submatrix\n\n    Notes\n    -----\n    A count matrix is connected if the graph having the count matrix\n    as adjacency matrix has a single connected component. Connectivity\n    of a graph can be efficiently checked using Tarjan's algorithm.\n\n    References\n    ----------\n    .. [1] Tarjan, R E. 1972. Depth-first search and linear graph\n        algorithms. SIAM Journal on Computing 1 (2): 146-160.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from msmtools.estimation import is_connected\n\n    >>> C = np.array([[10, 1, 0], [2, 0, 3], [0, 0, 4]])\n    >>> is_connected(C)\n    False\n\n    >>> is_connected(C, directed=False)\n    True\n\n    \"\"\"\n    if isdense(C):\n        return sparse.connectivity.is_connected(csr_matrix(C), directed=directed)\n    else:\n        return sparse.connectivity.is_connected(C, directed=directed)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prior_const(C, alpha=0.001):\n    if isdense(C):\n        return sparse.prior.prior_const(C, alpha=alpha)\n    else:\n        warnings.warn(\"Prior will be a dense matrix for sparse input\")\n        return sparse.prior.prior_const(C, alpha=alpha)", "response": "r Constant prior for given count matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prior_rev(C, alpha=-1.0):\n    if isdense(C):\n        return sparse.prior.prior_rev(C, alpha=alpha)\n    else:\n        warnings.warn(\"Prior will be a dense matrix for sparse input\")\n        return sparse.prior.prior_rev(C, alpha=alpha)", "response": "r Returns the prior counts for sampling of reversible transition matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_likelihood(C, T):\n    if issparse(C) and issparse(T):\n        return sparse.likelihood.log_likelihood(C, T)\n    else:\n        # use the dense likelihood calculator for all other cases\n        # if a mix of dense/sparse C/T matrices is used, then both\n        # will be converted to ndarrays.\n        if not isinstance(C, np.ndarray):\n            C = np.array(C)\n        if not isinstance(T, np.ndarray):\n            T = np.array(T)\n        # computation is still efficient, because we only use terms\n        # for nonzero elements of T\n        nz = np.nonzero(T)\n        return np.dot(C[nz], np.log(T[nz]))", "response": "r Log - likelihood of the count matrix given a transition matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tmatrix_cov(C, k=None):\n    if issparse(C):\n        warnings.warn(\"Covariance matrix will be dense for sparse input\")\n        C = C.toarray()\n    return dense.covariance.tmatrix_cov(C, row=k)", "response": "r Returns the covariance matrix for the non - reversible transition matrix posterior."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample_tmatrix(C, nsample=1, nsteps=None, reversible=False, mu=None, T0=None, return_statdist=False):\n    if issparse(C):\n        _showSparseConversionWarning()\n        C = C.toarray()\n\n    sampler = tmatrix_sampler(C, reversible=reversible, mu=mu, T0=T0, nsteps=nsteps)\n    return sampler.sample(nsamples=nsample, return_statdist=return_statdist)", "response": "r samples transition matrices from the posterior distribution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the hitting probability of all states to the target states.", "response": "def hitting_probability(P, target):\n    \"\"\"\n    Computes the hitting probabilities for all states to the target states.\n\n    The hitting probability of state i to set A is defined as the minimal,\n    non-negative solution of:\n\n    .. math::\n        h_i^A &= 1                    \\:\\:\\:\\:  i\\in A \\\\\n        h_i^A &= \\sum_j p_{ij} h_i^A  \\:\\:\\:\\:  i \\notin A\n\n    Returns\n    =======\n    h : ndarray(n)\n        a vector with hitting probabilities\n    \"\"\"\n    if hasattr(target, \"__len__\"):\n        target = np.array(target)\n    else:\n        target = np.array([target])\n    # target size\n    n = np.shape(P)[0]\n    # nontarget\n    nontarget = np.array(list(set(range(n)) - set(target)), dtype=int)\n    # stable states\n    stable = np.where(np.isclose(np.diag(P), 1) == True)[0]\n    # everything else\n    origin = np.array(list(set(nontarget) - set(stable)), dtype=int)\n    # solve hitting probability problem (P-I)x = -b\n    A = P[origin, :][:, origin] - np.eye((len(origin)))\n    b = np.sum(-P[origin, :][:, target], axis=1)\n    x = np.linalg.solve(A, b)\n    # fill up full solution with 0's for stable states and 1's for target\n    xfull = np.ones((n))\n    xfull[origin] = x\n    xfull[target] = 1\n    xfull[stable] = 0\n\n    return xfull"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_negative_entries(A):\n    A = A.tocoo()\n\n    data = A.data\n    row = A.row\n    col = A.col\n\n    \"\"\"Positive entries\"\"\"\n    pos = data > 0.0\n\n    datap = data[pos]\n    rowp = row[pos]\n    colp = col[pos]\n\n    Aplus = coo_matrix((datap, (rowp, colp)), shape=A.shape)\n    return Aplus", "response": "r Removes all negative entries from sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flux_matrix(T, pi, qminus, qplus, netflux=True):\n    D1 = diags((pi * qminus,), (0,))\n    D2 = diags((qplus,), (0,))\n\n    flux = D1.dot(T.dot(D2))\n\n    \"\"\"Remove self-fluxes\"\"\"\n    flux = flux - diags(flux.diagonal(), 0)\n\n    \"\"\"Return net or gross flux\"\"\"\n    if netflux:\n        return to_netflux(flux)\n    else:\n        return flux", "response": "r Compute the flux matrix of a set of states."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef coarsegrain(F, sets):\n    nnew = len(sets)\n    Fin = F.tocsr()\n    Fc = csr_matrix((nnew, nnew))\n    for i in range(0, nnew - 1):\n        for j in range(i, nnew):\n            I = list(sets[i])\n            J = list(sets[j])\n            Fc[i, j] = (Fin[I, :][:, J]).sum()\n            Fc[j, i] = (Fin[J, :][:, I]).sum()\n    return Fc", "response": "r Coarse - grain the flux to the given sets of states."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef total_flux(flux, A):\n    X = set(np.arange(flux.shape[0]))  # total state space\n    A = set(A)\n    notA = X.difference(A)\n\n    \"\"\"Extract rows corresponding to A\"\"\"\n    W = flux.tocsr()\n    W = W[list(A), :]\n    \"\"\"Extract columns corresonding to X\\A\"\"\"\n    W = W.tocsc()\n    W = W[:, list(notA)]\n\n    F = W.sum()\n    return F", "response": "r Compute the total flux between reactant and product."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the sensitivity matrix for the forward committor from A to B given transition matrix T.", "response": "def forward_committor_sensitivity(T, A, B, index):\n    \"\"\"\n    calculate the sensitivity matrix for index of the forward committor from A to B given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    A : array like\n        List of integer state labels for set A\n    B : array like\n        List of integer state labels for set B\n    index : entry of the committor for which the sensitivity is to be computed\n\n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    \"\"\"\n\n    n = len(T)\n    set_X = numpy.arange(n)  # set(range(n))\n    set_A = numpy.unique(A)  # set(A)\n    set_B = numpy.unique(B)  # set(B)\n    set_AB = numpy.union1d(set_A, set_B)  # set_A | set_B\n    notAB = numpy.setdiff1d(set_X, set_AB, True)  # list(set_X - set_AB)\n    m = len(notAB)\n\n    K = T - numpy.diag(numpy.ones(n))\n\n    U = K[numpy.ix_(notAB.tolist(), notAB.tolist())]\n\n    v = numpy.zeros(m)\n\n    # for i in xrange(0, m):\n    #   for k in xrange(0, len(set_B)):\n    #       v[i] = v[i] - K[notAB[i], B[k]]\n    v[:] = v[:] - K[notAB[:], B[:]]\n\n    qI = numpy.linalg.solve(U, v)\n\n    q_forward = numpy.zeros(n)\n    #q_forward[set_A] = 0 # double assignment.\n    q_forward[set_B] = 1\n    #for i in range(len(notAB)):\n    q_forward[notAB[:]] = qI[:]\n\n    target = numpy.eye(1, n, index)\n    target = target[0, notAB]\n\n    UinvVec = numpy.linalg.solve(U.T, target)\n    Siab = numpy.zeros((n, n))\n\n    for i in range(m):\n        Siab[notAB[i]] = - UinvVec[i] * q_forward\n\n    return Siab"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the sensitivity matrix for index of the backward committor from A to B given transition matrix T. Parameters ---------- T : numpy.ndarray shape = (n, n) Transition matrix A : array like List of integer state labels for set A B : array like List of integer state labels for set B index : entry of the committor for which the sensitivity is to be computed Returns ------- x : ndarray, shape=(n, n) Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.", "response": "def backward_committor_sensitivity(T, A, B, index):\n    \"\"\"\n    calculate the sensitivity matrix for index of the backward committor from A to B given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    A : array like\n        List of integer state labels for set A\n    B : array like\n        List of integer state labels for set B\n    index : entry of the committor for which the sensitivity is to be computed\n\n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    \"\"\"\n\n    # This is really ugly to compute. The problem is, that changes in T induce changes in\n    # the stationary distribution and so we need to add this influence, too\n    # I implemented something which is correct, but don't ask me about the derivation\n\n    n = len(T)\n\n    trT = numpy.transpose(T)\n\n    one = numpy.ones(n)\n    eq = stationary_distribution(T)\n\n    mEQ = numpy.diag(eq)\n    mIEQ = numpy.diag(1.0 / eq)\n    mSEQ = numpy.diag(1.0 / eq / eq)\n\n    backT = numpy.dot(mIEQ, numpy.dot(trT, mEQ))\n\n    qMat = forward_committor_sensitivity(backT, A, B, index)\n\n    matA = trT - numpy.identity(n)\n    matA = numpy.concatenate((matA, [one]))\n\n    phiM = numpy.linalg.pinv(matA)\n\n    phiM = phiM[:, 0:n]\n\n    trQMat = numpy.transpose(qMat)\n\n    d1 = numpy.dot(mSEQ, numpy.diagonal(numpy.dot(numpy.dot(trT, mEQ), trQMat), 0))\n    d2 = numpy.diagonal(numpy.dot(numpy.dot(trQMat, mIEQ), trT), 0)\n\n    psi1 = numpy.dot(d1, phiM)\n    psi2 = numpy.dot(-d2, phiM)\n\n    v1 = psi1 - one * numpy.dot(psi1, eq)\n    v3 = psi2 - one * numpy.dot(psi2, eq)\n\n    part1 = numpy.outer(eq, v1)\n    part2 = numpy.dot(numpy.dot(mEQ, trQMat), mIEQ)\n    part3 = numpy.outer(eq, v3)\n\n    sensitivity = part1 + part2 + part3\n\n    return sensitivity"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigenvalue_sensitivity(T, k):\n\n    eValues, rightEigenvectors = numpy.linalg.eig(T)\n    leftEigenvectors = numpy.linalg.inv(rightEigenvectors)\n\n    perm = numpy.argsort(eValues)[::-1]\n\n    rightEigenvectors = rightEigenvectors[:, perm]\n    leftEigenvectors = leftEigenvectors[perm]\n\n    sensitivity = numpy.outer(leftEigenvectors[k], rightEigenvectors[:, k])\n\n    return sensitivity", "response": "Calculates the sensitivity matrix for the entry index k given transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timescale_sensitivity(T, k):\n\n    eValues, rightEigenvectors = numpy.linalg.eig(T)\n    leftEigenvectors = numpy.linalg.inv(rightEigenvectors)\n\n    perm = numpy.argsort(eValues)[::-1]\n\n    eValues = eValues[perm]\n    rightEigenvectors = rightEigenvectors[:, perm]\n    leftEigenvectors = leftEigenvectors[perm]\n\n    eVal = eValues[k]\n\n    sensitivity = numpy.outer(leftEigenvectors[k], rightEigenvectors[:, k])\n\n    if eVal < 1.0:\n        factor = 1.0 / (numpy.log(eVal) ** 2) / eVal\n    else:\n        factor = 0.0\n\n    sensitivity *= factor\n\n    return sensitivity", "response": "Calculates the sensitivity matrix for timescale k given transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the sensitivity matrix for the given entry j of left or right eigenvector k given transition matrix T.", "response": "def eigenvector_sensitivity(T, k, j, right=True):\n    \"\"\"\n    calculate the sensitivity matrix for entry j of left or right eigenvector k given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    k : int\n        eigenvector index ordered with descending eigenvalues\n    j : int\n        entry of eigenvector k for which the sensitivity is to be computed\n    right : boolean (default: True)\n        If set to True (default) the right eigenvectors are considered, otherwise the left ones\n\n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n\n    Remarks\n    -------\n    Eigenvectors can naturally be scaled and so will their sensitivity depend on their size.\n    For that reason we need to agree on a normalization for which the sensitivity is computed.\n    Here we use the natural norm(vector) = 1 condition which is different from the results, e.g.\n    the rdl_decomposition returns.\n    This is especially important for the stationary distribution, which is the first left eigenvector.\n    For this reason this function return a different sensitivity for the first left eigenvector\n    than the function stationary_distribution_sensitivity and this function should not be used in this\n    case!\n    \"\"\"\n\n    n = len(T)\n\n    if not right:\n        T = numpy.transpose(T)\n\n    eValues, rightEigenvectors = numpy.linalg.eig(T)\n    leftEigenvectors = numpy.linalg.inv(rightEigenvectors)\n    perm = numpy.argsort(eValues)[::-1]\n\n    eValues = eValues[perm]\n    rightEigenvectors = rightEigenvectors[:, perm]\n    leftEigenvectors = leftEigenvectors[perm]\n\n    rEV = rightEigenvectors[:, k]\n    lEV = leftEigenvectors[k]\n    eVal = eValues[k]\n\n    vecA = numpy.zeros(n)\n    vecA[j] = 1.0\n\n    matA = T - eVal * numpy.identity(n)\n    # Use here rEV as additional condition, means that we assume the vector to be\n    # orthogonal to rEV\n    matA = numpy.concatenate((matA, [rEV]))\n\n    phi = numpy.linalg.lstsq(numpy.transpose(matA), vecA, rcond=-1)\n\n    phi = numpy.delete(phi[0], -1)\n\n    sensitivity = -numpy.outer(phi, rEV) + numpy.dot(phi, rEV) * numpy.outer(lEV, rEV)\n\n    if not right:\n        sensitivity = numpy.transpose(sensitivity)\n\n    return sensitivity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mfpt_sensitivity(T, target, j):\n\n    n = len(T)\n\n    matA = T - numpy.diag(numpy.ones((n)))\n    matA[target] *= 0\n    matA[target, target] = 1.0\n\n    tVec = -1. * numpy.ones(n)\n    tVec[target] = 0\n\n    mfpt = numpy.linalg.solve(matA, tVec)\n    aVec = numpy.zeros(n)\n    aVec[j] = 1.0\n\n    phiVec = numpy.linalg.solve(numpy.transpose(matA), aVec)\n\n    # TODO: Check sign of sensitivity!\n\n    sensitivity = -1.0 * numpy.outer(phiVec, mfpt)\n    sensitivity[target] *= 0\n\n    return sensitivity", "response": "Calculates the sensitivity matrix for the entry j of the mean first passage time given transition matrix T."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef geometric_series(q, n):\n    q = np.asarray(q)\n    if n < 0:\n        raise ValueError('Finite geometric series is only defined for n>=0.')\n    else:\n        \"\"\"q is scalar\"\"\"\n        if q.ndim == 0:\n            if q == 1:\n                s = (n + 1) * 1.0\n                return s\n            else:\n                s = (1.0 - q ** (n + 1)) / (1.0 - q)\n                return s\n        \"\"\"q is ndarray\"\"\"\n        s = np.zeros(np.shape(q), dtype=q.dtype)\n        \"\"\"All elements with value q=1\"\"\"\n        ind = (q == 1.0)\n        \"\"\"For q=1 the sum has the value s=n+1\"\"\"\n        s[ind] = (n + 1) * 1.0\n        \"\"\"All elements with value q\\neq 1\"\"\"\n        not_ind = np.logical_not(ind)\n        s[not_ind] = (1.0 - q[not_ind] ** (n + 1)) / (1.0 - q[not_ind])\n        return s", "response": "Compute finite geometric series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ec_matrix_vector(p0, T, n):\n    if (n <= 0):\n        EC = np.zeros(T.shape)\n        return EC\n    else:\n        \"\"\"Probability vector after (k=0) propagations\"\"\"\n        p_k = 1.0 * p0\n        \"\"\"Sum of vectors after (k=0) propagations\"\"\"\n        p_sum = 1.0 * p_k\n        for k in range(n - 1):\n            \"\"\"Propagate one step p_{k} -> p_{k+1}\"\"\"\n            p_k = np.dot(p_k, T)\n            \"\"\"Update sum\"\"\"\n            p_sum += p_k\n        \"\"\"Expected counts\"\"\"\n        EC = p_sum[:, np.newaxis] * T\n        return EC", "response": "r Compute expected transition counts for Markov chain after n\n    steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef primal_dual_solve(func, x0, Dfunc, A, b, G, h, args=(), tol=1e-10,\n                      maxiter=100, show_progress=True, full_output=False):\n    \"\"\"Wrap calls to function and Jacobian\"\"\"\n    fcalls, func = wrap_function(func, args)\n    Dfcalls, Dfunc = wrap_function(Dfunc, args)\n\n    M, N = G.shape\n    P, N = A.shape\n\n    \"\"\"Total number of inequality constraints\"\"\"\n    m = M\n\n    def gap(z):\n        r\"\"\"Gap-function\"\"\"\n        l = z[N+P:N+P+M]\n        s = z[N+P+M:]\n        return mydot(l, s)/m\n\n    def centrality(z):\n        r\"\"\"Centrality function\"\"\"\n        l = z[N+P:N+P+M]\n        s = z[N+P+M:]\n        return np.min(l*s)\n\n    def KKT(z, sigma=0.0):\n        r\"\"\"KKT system (possible perturbed).\"\"\"\n\n        \"\"\"Primal variable\"\"\"\n        x = z[0:N]\n\n        \"\"\"Multiplier for equality constraints\"\"\"\n        nu = z[N:N+P]\n\n        \"\"\"Multiplier for inequality constraints\"\"\"\n        l = z[N+P:N+P+M]\n\n        \"\"\"Slacks\"\"\"\n        s = z[N+P+M:]\n\n        \"\"\"Evaluate objective function\"\"\"\n        F = func(x)\n\n        \"\"\"Dual infeasibility\"\"\"\n        rdual = F+mydot(A.transpose(), nu)+mydot(G.transpose(), l)\n\n        \"\"\"Primal infeasibilities\"\"\"\n        rprim1 = mydot(A, x)-b\n        rprim2 = mydot(G, x)-h+s\n\n        \"\"\"Complementary slackness (perturbed)\"\"\"\n        mu = gap(z)\n        rslack = l*s-sigma*mu\n\n        return np.hstack((rdual, rprim1, rprim2, rslack))\n\n    def step_fast(z, KKTval, LU, G, A, mu, beta, gamma, alpha0):\n        r\"\"\"Affine scaling step.\"\"\"\n        dz = solve_factorized(z, KKTval, LU, G, A)\n\n        \"\"\"Reduce step length until slacks s and multipliers l are positive\"\"\"\n        alpha = 1.0*alpha_0\n        k = 0\n        for k in range(10):\n            z_new = z + alpha*dz\n            if np.all( z_new[N+P:] > 0.0 ):\n                break\n            alpha *= 0.5\n            k += 1\n        if k == 10 - 1:\n            raise RuntimeError(\"Maximum steplength reduction reached\")\n\n        \"\"\"Reduce step length until iterates lie in correct neighborhood\"\"\"\n        for k in range(10):\n            z_new = z + alpha*dz\n            KKTval_new = KKT(z_new)\n            dual = mynorm(KKTval_new[0:N])\n            prim = mynorm(KKTval_new[N:N+P+M])\n            mu_new = gap(z_new)\n            cent_new = centrality(z_new)\n\n            if (dual <= beta*mu_new and prim <= beta*mu_new and\n                cent_new >= gamma*mu_new):\n                break\n            alpha *= 0.5\n            # alpha *= 0.95\n        if k == 10 - 1:\n            raise RuntimeError(\"Maximum steplength reduction reached\")\n        return z_new, mu_new\n\n    def step_safe(z, KKTval, LU, G, A, mu, beta, gamma):\n        r\"\"\"Centering step.\"\"\"\n        dz = solve_factorized(z, KKTval, LU, G, A)\n\n        \"\"\"Reduce step length until slacks s and multipliers l are positive\"\"\"\n        alpha = 1.0\n        k = 0\n        for k in range(10):\n            z_new = z + alpha*dz\n            if np.all( z_new[N+P:] > 0.0 ):\n                break\n            alpha *= 0.5\n            k += 1\n        if k == 10 - 1:\n            raise RuntimeError(\"Maximum steplength reduction (pos.) reached\")\n\n        \"\"\"Reduce step length until iterates lie in correct neighborhood\n        and mu fulfills Armijo condition\"\"\"\n        k = 0\n        for k in range(10):\n            z_new = z+alpha*dz\n            KKTval_new = KKT(z_new, sigma=SIGMA)\n            dual = mynorm(KKTval_new[0:N])\n            prim = mynorm(KKTval_new[N:N+P+M])\n            mu_new = gap(z_new)\n            cent_new = centrality(z_new)\n\n            if (dual <= beta*mu_new and prim <= beta*mu_new and\n                cent_new >= gamma*mu_new and\n                mu_new<=(1.0-KAPPA*alpha*(1.0-SIGMA))*mu):\n                break\n            alpha *= 0.5\n        if k == 10 - 1:\n            raise RuntimeError(\"Maximum steplength reduction reached\")\n        return z_new, mu_new\n\n    \"\"\"INITIALIZATION\"\"\"\n\n    \"\"\"Initial Slacks for inequality constraints\"\"\"\n    s0 = -1.0*(mydot(G, x0)-h)\n\n    \"\"\"Initial multipliers for inequality constraints\"\"\"\n    l0 = 1.0*np.ones(M)\n\n    \"\"\"Initial multipliers for equality constraints\"\"\"\n    nu0 = np.zeros(P)\n\n    \"\"\"Initial point\"\"\"\n    z0 = np.hstack((x0, nu0, l0, s0))\n\n    \"\"\"Initial KKT-values\"\"\"\n    KKTval0 = KKT(z0, sigma=0.0)\n    mu0 = gap(z0)\n    dual0 = mynorm(KKTval0[0:N])\n    prim0 = mynorm(KKTval0[N:N+P+M])\n\n    \"\"\"Initial neighborhood\"\"\"\n    beta = BETA * np.sqrt(dual0**2 + prim0**2)/mu0\n    gamma = 1.0 * GAMMA_MAX\n\n    \"\"\"Number of fast steps\"\"\"\n    t = 0\n\n    \"\"\"Number of iterations\"\"\"\n    n = 0\n\n    \"\"\"Dummy variable for step type\"\"\"\n    step_type = \" \"\n\n    if show_progress:\n        print(\"%s %s %s %s %s %s %s\" %(\"iter\", \"gap\", \"dual\", \"primal\",\n                                       \"min\", \"max\", \"step\"))\n    \"\"\"MAIN LOOP\"\"\"\n    z = z0\n    x = z0[0:N]\n    KKTval = KKTval0\n    dual = dual0\n    prim = prim0\n    mu = mu0\n    Dfunc_val = Dfunc(x)\n    LU = factor(z, Dfunc_val, G, A)\n\n    if full_output:\n        info = {'z': []}\n        info['z'].append(z)\n\n    for n in range(maxiter):\n        if show_progress:\n            l=z[N+P:N+P+M]\n            s=z[N+P+M:]\n            print(\"%i %.6e %.6e %.6e %.6e %.6e %s\" %(n+1, mu, dual, prim,\n                                                     np.min(l*s), np.max(l*s),\n                                                     step_type))\n        \"\"\"Attempt fast step\"\"\"\n        beta_new = (1.0 + GAMMA_BAR**(t+1)) * beta\n        gamma_new = GAMMA_MIN + GAMMA_BAR**(t+1)*(GAMMA_MAX - GAMMA_MIN)\n        alpha_0 = 1.0 - np.sqrt(mu)/GAMMA_BAR**t\n\n        if alpha_0 > 0.0:\n            z_new, mu_new = step_fast(z, KKTval, LU, G, A, mu,\n                                      beta_new, gamma_new, alpha_0)\n            if mu_new < RHO * mu:\n                \"\"\"Fast successful\"\"\"\n                z = z_new\n                mu = mu_new\n                beta = beta_new\n                gamma = gamma_new\n                t += 1\n                step_type = \"f\"\n            else:\n                \"\"\"Perturbed right-had side\"\"\"\n                KKTval_pert = 1.0*KKTval\n                KKTval_pert[N+P+M:] -= SIGMA * mu\n                z, mu = step_safe(z, KKTval_pert, LU, G, A, mu, beta, gamma)\n                step_type = \"s\"\n        else:\n            \"\"\"Perturbed right-hand side\"\"\"\n            KKTval_pert = 1.0*KKTval\n            KKTval_pert[N+P+M:] -= SIGMA * mu\n            z, mu = step_safe(z, KKTval_pert, LU, G, A, mu, beta, gamma)\n            step_type = \"s\"\n\n        \"\"\"Compute new iterates\"\"\"\n        KKTval = KKT(z, sigma=0.0)\n        dual = mynorm(KKTval[0:N])\n        prim = mynorm(KKTval[N:N+P+M])\n        x = z[0:N]\n        Dfunc_val = Dfunc(x)\n        LU = factor(z, Dfunc_val, G, A)\n        if full_output:\n            info['z'].append(z)\n        if mu < tol and dual < tol and prim < tol:\n            break\n    if n == maxiter - 1:\n        raise RuntimeError(\"Maximum number of iterations reached\")\n\n    if show_progress:\n        l=z[N+P:N+P+M]\n        s=z[N+P+M:]\n        print(\"%i %.6e %.6e %.6e %.6e %.6e %s\" %(n+1, mu, dual, prim,\n                                                 np.min(l*s), np.max(l*s),\n                                                 step_type))\n    if full_output:\n        return z[0:N], info\n    else:\n        return z[0:N]", "response": "Evaluate a primal dual objective function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving the MLE - Revolution problem with a given matrix.", "response": "def solve_mle_rev(C, tol=1e-10, maxiter=100, show_progress=False, full_output=False,\n                  return_statdist=True, **kwargs):\n    \"\"\"Number of states\"\"\"\n    M = C.shape[0]\n\n    \"\"\"Initial guess for primal-point\"\"\"\n    z0 = np.zeros(2*M)\n    z0[0:M] = 1.0\n\n    \"\"\"Inequality constraints\"\"\"\n    # G = np.zeros((M, 2*M))\n    # G[np.arange(M), np.arange(M)] = -1.0\n    G = -1.0*scipy.sparse.eye(M, n=2*M, k=0)\n    h = np.zeros(M)\n\n    \"\"\"Equality constraints\"\"\"\n    A = np.zeros((1, 2*M))\n    A[0, M] = 1.0\n    b = np.array([0.0])\n\n    \"\"\"Scaling\"\"\"\n    c0 = C.max()\n    C = C/c0\n\n    \"\"\"Symmetric part\"\"\"\n    Cs = C + C.T\n\n    \"\"\"Column sum\"\"\"\n    c = C.sum(axis=0)\n\n    if scipy.sparse.issparse(C):\n        Cs = Cs.tocsr()\n        c = c.A1\n        A = scipy.sparse.csr_matrix(A)\n        F = objective_sparse.F\n        DF = objective_sparse.DFsym\n        convert_solution = objective_sparse.convert_solution\n    else:\n        F = objective_dense.F\n        DF = objective_dense.DF\n        convert_solution = objective_dense.convert_solution\n\n    \"\"\"PDIP iteration\"\"\"\n    res = primal_dual_solve(F, z0, DF, A, b, G, h,\n                            args=(Cs, c),\n                            maxiter=maxiter, tol=tol,\n                            show_progress=show_progress,\n                            full_output=full_output)\n    if full_output:\n        z, info = res\n    else:\n        z = res\n    pi, P = convert_solution(z, Cs)\n\n    result = [P]\n    if return_statdist:\n        result.append(pi)\n    if full_output:\n        result.append(info)\n\n    return tuple(result) if len(result) > 1 else result[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef blueprint_name_to_url(name):\n        if name[-1:] == \".\":\n            name = name[:-1]\n        name = str(name).replace(\".\", \"/\")\n        return name", "response": "converts a blueprint name to a url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Don't use \"from appname.models import ModelName\". \n        # Use orm.ModelName to refer to models in this application,\n        # and orm['appname.ModelName'] for models in other applications.\n        from account_keeping.models import Invoice\n        for invoice in Invoice.objects.all():\n            invoice.save()", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_client(provider, token=''):\n    \"Return the API client for the given provider.\"\n    cls = OAuth2Client\n    if provider.request_token_url:\n        cls = OAuthClient\n    return cls(provider, token)", "response": "Return the API client for the given provider."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching user profile information.", "response": "def get_profile_info(self, raw_token, profile_info_params={}):\n        \"Fetch user profile information.\"\n        try:\n            response = self.request('get', self.provider.profile_url, token=raw_token, params=profile_info_params)\n            response.raise_for_status()\n        except RequestException as e:\n            logger.error('Unable to fetch user profile: {0}'.format(e))\n            return None\n        else:\n            return response.json() or response.text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_access_token(self, request, callback=None):\n        \"Fetch access token from callback request.\"\n        raw_token = request.session.get(self.session_key, None)\n        verifier = request.GET.get('oauth_verifier', None)\n        if raw_token is not None and verifier is not None:\n            data = {'oauth_verifier': verifier}\n            callback = request.build_absolute_uri(callback or request.path)\n            callback = force_text(callback)\n            try:\n                response = self.request('post', self.provider.access_token_url,\n                                        token=raw_token, data=data, oauth_callback=callback)\n                response.raise_for_status()\n            except RequestException as e:\n                logger.error('Unable to fetch access token: {0}'.format(e))\n                return None\n            else:\n                return response.text\n        return None", "response": "Fetch access token from callback request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget request parameters for redirect url.", "response": "def get_redirect_args(self, request, callback):\n        \"Get request parameters for redirect url.\"\n        callback = force_text(request.build_absolute_uri(callback))\n        raw_token = self.get_request_token(request, callback)\n        token, secret = self.parse_raw_token(raw_token)\n        if token is not None and secret is not None:\n            request.session[self.session_key] = raw_token\n        return {\n            'oauth_token': token,\n            'oauth_callback': callback,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_raw_token(self, raw_token):\n        \"Parse token and secret from raw token response.\"\n        if raw_token is None:\n            return (None, None)\n        qs = parse_qs(raw_token)\n        token = qs.get('oauth_token', [None])[0]\n        secret = qs.get('oauth_token_secret', [None])[0]\n        return (token, secret)", "response": "Parse token and secret from raw token response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, method, url, **kwargs):\n        \"Build remote url request. Constructs necessary auth.\"\n        user_token = kwargs.pop('token', self.token)\n        token, secret = self.parse_raw_token(user_token)\n        callback = kwargs.pop('oauth_callback', None)\n        verifier = kwargs.get('data', {}).pop('oauth_verifier', None)\n        oauth = OAuth1(\n            resource_owner_key=token,\n            resource_owner_secret=secret,\n            client_key=self.provider.consumer_key,\n            client_secret=self.provider.consumer_secret,\n            verifier=verifier,\n            callback_uri=callback,\n        )\n        kwargs['auth'] = oauth\n        return super(OAuthClient, self).request(method, url, **kwargs)", "response": "Build remote url request. Constructs necessary auth."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_application_state(self, request, callback):\n        \"Check optional state parameter.\"\n        stored = request.session.get(self.session_key, None)\n        returned = request.GET.get('state', None)\n        check = False\n        if stored is not None:\n            if returned is not None:\n                check = constant_time_compare(stored, returned)\n            else:\n                logger.error('No state parameter returned by the provider.')\n        else:\n            logger.error('No state stored in the sesssion.')\n        return check", "response": "Check optional state parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting request parameters for redirect url.", "response": "def get_redirect_args(self, request, callback):\n        \"Get request parameters for redirect url.\"\n        callback = request.build_absolute_uri(callback)\n        args = {\n            'client_id': self.provider.consumer_key,\n            'redirect_uri': callback,\n            'response_type': 'code',\n        }\n        state = self.get_application_state(request, callback)\n        if state is not None:\n            args['state'] = state\n            request.session[self.session_key] = state\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses token and secret from raw token response.", "response": "def parse_raw_token(self, raw_token):\n        \"Parse token and secret from raw token response.\"\n        if raw_token is None:\n            return (None, None)\n        # Load as json first then parse as query string\n        try:\n            token_data = json.loads(raw_token)\n        except ValueError:\n            qs = parse_qs(raw_token)\n            token = qs.get('access_token', [None])[0]\n        else:\n            token = token_data.get('access_token', None)\n        return (token, None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(self, method, url, **kwargs):\n        \"Build remote url request. Constructs necessary auth.\"\n        user_token = kwargs.pop('token', self.token)\n        token, _ = self.parse_raw_token(user_token)\n        if token is not None:\n            params = kwargs.get('params', {})\n            params['access_token'] = token\n            kwargs['params'] = params\n        return super(OAuth2Client, self).request(method, url, **kwargs)", "response": "Build remote url request. Constructs necessary auth."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _stripped_codes(codes):\n    return tuple([\n        code.strip() for code in codes.split(',')\n        if code.strip()\n    ])", "response": "Return a tuple of stripped codes split by ','."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(self, filename, line, codes):\n        if self.regex_match_any(line, codes):\n            if self._vary_codes:\n                self.codes = tuple([codes[-1]])\n            return True", "response": "Match rule and set attribute codes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches user for a given provider by id.", "response": "def authenticate(self, provider=None, identifier=None):\n        \"Fetch user for a given provider by id.\"\n        provider_q = Q(provider__name=provider)\n        if isinstance(provider, Provider):\n            provider_q = Q(provider=provider)\n        try:\n            access = AccountAccess.objects.filter(\n                provider_q, identifier=identifier\n            ).select_related('user')[0]\n        except IndexError:\n            return None\n        else:\n            return access.user"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __extract_modules(self, loader, name, is_pkg):\n\n        \"\"\" if module found load module and save all attributes in the module found \"\"\"\n        mod = loader.find_module(name).load_module(name)\n\n        \"\"\" find the attribute method on each module \"\"\"\n        if hasattr(mod, '__method__'):\n\n            \"\"\" register to the blueprint if method attribute found \"\"\"\n            module_router = ModuleRouter(mod,\n                                         ignore_names=self.__serialize_module_paths()\n                                         ).register_route(app=self.application, name=name)\n\n            self.__routers.extend(module_router.routers)\n            self.__modules.append(mod)\n\n        else:\n            \"\"\" prompt not found notification \"\"\"\n            # print('{} has no module attribute method'.format(mod))\n            pass", "response": "extracts all modules from the given module and adds them to the list of modules that are registered to the blueprint"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_client(self, provider):\n        \"Get instance of the OAuth client for this provider.\"\n        if self.client_class is not None:\n            return self.client_class(provider)\n        return get_client(provider)", "response": "Get instance of the OAuth client for this provider."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_redirect_url(self, **kwargs):\n        \"Build redirect url for a given provider.\"\n        name = kwargs.get('provider', '')\n        try:\n            provider = Provider.objects.get(name=name)\n        except Provider.DoesNotExist:\n            raise Http404('Unknown OAuth provider.')\n        else:\n            if not provider.enabled():\n                raise Http404('Provider %s is not enabled.' % name)\n            client = self.get_client(provider)\n            callback = self.get_callback_url(provider)\n            params = self.get_additional_parameters(provider)\n            return client.get_redirect_url(self.request, callback=callback, parameters=params)", "response": "Build redirect url for a given provider."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a shell auth. User.", "response": "def get_or_create_user(self, provider, access, info):\n        \"Create a shell auth.User.\"\n        digest = hashlib.sha1(smart_bytes(access)).digest()\n        # Base 64 encode to get below 30 characters\n        # Removed padding characters\n        username = force_text(base64.urlsafe_b64encode(digest)).replace('=', '')\n        User = get_user_model()\n        kwargs = {\n            User.USERNAME_FIELD: username,\n            'email': '',\n            'password': None\n        }\n        return User.objects.create_user(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_user_id(self, provider, info):\n        \"Return unique identifier from the profile info.\"\n        id_key = self.provider_id or 'id'\n        result = info\n        try:\n            for key in id_key.split('.'):\n                result = result[key]\n            return result\n        except KeyError:\n            return None", "response": "Return unique identifier from the profile info."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_existing_user(self, provider, user, access, info):\n        \"Login user and redirect.\"\n        login(self.request, user)\n        return redirect(self.get_login_redirect(provider, user, access))", "response": "Login user and redirect."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a shell auth. User and redirect.", "response": "def handle_new_user(self, provider, access, info):\n        \"Create a shell auth.User and redirect.\"\n        user = self.get_or_create_user(provider, access, info)\n        access.user = user\n        AccountAccess.objects.filter(pk=access.pk).update(user=user)\n        user = authenticate(provider=access.provider, identifier=access.identifier)\n        login(self.request, user)\n        return redirect(self.get_login_redirect(provider, user, access, True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndiscovers bridges via NUPNP.", "response": "async def discover_nupnp(websession):\n    \"\"\"Discover bridges via NUPNP.\"\"\"\n    async with websession.get(URL_NUPNP) as res:\n        return [Bridge(item['internalipaddress'], websession=websession)\n                for item in (await res.json())]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the number of months that have already passed in the given year.", "response": "def get_months_of_year(year):\n    \"\"\"\n    Returns the number of months that have already passed in the given year.\n\n    This is useful for calculating averages on the year view. For past years,\n    we should divide by 12, but for the current year, we should divide by\n    the current month.\n\n    \"\"\"\n    current_year = now().year\n    if year == current_year:\n        return now().month\n    if year > current_year:\n        return 1\n    if year < current_year:\n        return 12"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_totals_by_payee(self, account, start_date=None, end_date=None):\n        qs = Transaction.objects.filter(account=account, parent__isnull=True)\n        qs = qs.values('payee').annotate(models.Sum('value_gross'))\n        qs = qs.order_by('payee__name')\n        return qs", "response": "Returns a queryset of all transaction totals grouped by Payee."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_without_invoice(self):\n        qs = Transaction.objects.filter(\n            children__isnull=True, invoice__isnull=True)\n        return qs", "response": "Returns transactions that don t have an invoice."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps function for filtering enabled providers.", "response": "def _get_enabled():\n    \"\"\"Wrapped function for filtering enabled providers.\"\"\"\n    providers = Provider.objects.all()\n    return [p for p in providers if p.enabled()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the list of enabled providers to the context.", "response": "def available_providers(request):\n    \"Adds the list of enabled providers to the context.\"\n    if APPENGINE:\n        # Note: AppEngine inequality queries are limited to one property.\n        # See https://developers.google.com/appengine/docs/python/datastore/queries#Python_Restrictions_on_queries\n        # Users have also noted that the exclusion queries don't work\n        # See https://github.com/mlavin/django-all-access/pull/46\n        # So this is lazily-filtered in Python\n        qs = SimpleLazyObject(lambda: _get_enabled())\n    else:\n        qs = Provider.objects.filter(consumer_secret__isnull=False, consumer_key__isnull=False)\n    return {'allaccess_providers': qs}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(command, **kw):\n    # Windows low-level subprocess API wants str for current working\n    # directory.\n    if sys.platform == 'win32':\n        _cwd = kw.get('cwd', None)\n        if _cwd is not None:\n            kw['cwd'] = _cwd.decode()\n    try:\n        # In Python 3, iterating over bytes yield integers, so we call\n        # `splitlines()` to force Python 3 to give us lines instead.\n        return check_output(command, **kw).splitlines()\n    except CalledProcessError:\n        return ()\n    except FileNotFoundError:\n        print(\"The {} binary was not found. Skipping directory {}.\\n\"\n              .format(command[0], kw['cwd'].decode(\"UTF-8\")))\n        return ()", "response": "Run command catch any exception and return lines of output."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_repositories_with_locate(path):\n    command = [b'locate', b'-0']\n    for dotdir in DOTDIRS:\n        # Escaping the slash (using '\\/' rather than '/') is an\n        # important signal to locate(1) that these glob patterns are\n        # supposed to match the full path, so that things like\n        # '.hgignore' files do not show up in the result.\n        command.append(br'%s\\/%s' % (escape(path), escape(dotdir)))\n        command.append(br'%s\\/*/%s' % (escape(path), escape(dotdir)))\n    try:\n        paths = check_output(command).strip(b'\\0').split(b'\\0')\n    except CalledProcessError:\n        return []\n    return [os.path.split(p) for p in paths\n            if not os.path.islink(p) and os.path.isdir(p)]", "response": "Use locate to find repositories with locate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_repositories_by_walking_without_following_symlinks(path):\n    repos = []\n    for dirpath, dirnames, filenames in os.walk(path, followlinks=False):\n        for dotdir in set(dirnames) & DOTDIRS:\n            repos.append((dirpath, dotdir))\n    return repos", "response": "Walk a tree and return a sequence of ( directory dotdir ) pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_repositories_by_walking_and_following_symlinks(path):\n    repos = []\n\n    # This is for detecting symlink loops and escaping them. This is similar to\n    # http://stackoverflow.com/questions/36977259/avoiding-infinite-recursion-with-os-walk/36977656#36977656\n    def inode(path):\n        stats = os.stat(path)\n        return stats.st_dev, stats.st_ino\n    seen_inodes = {inode(path)}\n\n    for dirpath, dirnames, filenames in os.walk(path, followlinks=True):\n        inodes = [inode(os.path.join(dirpath, p)) for p in dirnames]\n        dirnames[:] = [p for p, i in zip(dirnames, inodes)\n                       if not i in seen_inodes]\n        seen_inodes.update(inodes)\n\n        for dotdir in set(dirnames) & DOTDIRS:\n            repos.append((dirpath, dotdir))\n    return repos", "response": "Walk a tree and return a sequence of ( directory dotdir ) pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns hg status. Returns a 2-element tuple: * Text lines describing the status of the repository. * Empty sequence of subrepos, since hg does not support them.", "response": "def status_mercurial(path, ignore_set, options):\n    \"\"\"Run hg status.\n\n    Returns a 2-element tuple:\n    * Text lines describing the status of the repository.\n    * Empty sequence of subrepos, since hg does not support them.\n    \"\"\"\n    lines = run(['hg', '--config', 'extensions.color=!', 'st'], cwd=path)\n    subrepos = ()\n    return [b' ' + l for l in lines if not l.startswith(b'?')], subrepos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status_git(path, ignore_set, options):\n    # Check whether current branch is dirty:\n    lines = [l for l in run(('git', 'status', '-s', '-b'), cwd=path)\n             if (options.untracked or not l.startswith(b'?'))\n             and not l.startswith(b'##')]\n\n    # Check all branches for unpushed commits:\n    lines += [l for l in run(('git', 'branch', '-v'), cwd=path)\n              if (b' [ahead ' in l)]\n\n    # Check for non-tracking branches:\n    if options.non_tracking:\n        lines += [l for l in run(('git', 'for-each-ref',\n                                  '--format=[%(refname:short)]%(upstream)',\n                                   'refs/heads'), cwd=path)\n                  if l.endswith(b']')]\n\n    if options.stash:\n        lines += [l for l in run(('git', 'stash', 'list'), cwd=path)]\n\n    discovered_submodules = []\n    for l in run(('git', 'submodule', 'status'), cwd=path):\n        match = git_submodule.search(l)\n        if match:\n            discovered_submodules.append(match.group(1))\n\n    return lines, discovered_submodules", "response": "Run git status.\n\n    Returns a 2-element tuple:\n    * Text lines describing the status of the repository.\n    * List of subrepository paths, relative to the repository itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef status_subversion(path, ignore_set, options):\n    subrepos = ()\n    if path in ignore_set:\n        return None, subrepos\n    keepers = []\n    for line in run(['svn', 'st', '-v'], cwd=path):\n        if not line.strip():\n            continue\n        if line.startswith(b'Performing') or line[0] in b'X?':\n            continue\n        status = line[:8]\n        ignored_states = options.ignore_svn_states\n        if ignored_states and status.strip() in ignored_states:\n            continue\n        filename = line[8:].split(None, 3)[-1]\n        ignore_set.add(os.path.join(path, filename))\n        if status.strip():\n            keepers.append(b' ' + status + filename)\n    return keepers, subrepos", "response": "Run svn status.\n\n    Returns a 2-element tuple:\n    * Text lines describing the status of the repository.\n    * Empty sequence of subrepos, since svn does not support them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of repositories and a list of patterns scan each of them.", "response": "def scan(repos, options):\n    \"\"\"Given a repository list [(path, vcsname), ...], scan each of them.\"\"\"\n    ignore_set = set()\n    repos = repos[::-1]  # Create a queue we can push and pop from\n    while repos:\n        directory, dotdir = repos.pop()\n        ignore_this = any(pat in directory for pat in options.ignore_patterns)\n        if ignore_this:\n            if options.verbose:\n                output(b'Ignoring repo: %s' % directory)\n                output(b'')\n            continue\n\n        vcsname, get_status = SYSTEMS[dotdir]\n        lines, subrepos = get_status(directory, ignore_set, options)\n\n        # We want to tackle subrepos immediately after their repository,\n        # so we put them at the front of the queue.\n        subrepos = [(os.path.join(directory, r), dotdir) for r in subrepos]\n        repos.extend(reversed(subrepos))\n\n        if lines is None:  # signal that we should ignore this one\n            continue\n        if lines or options.verbose:\n            output(b'%s - %s' % (directory, vcsname))\n            for line in lines:\n                output(line)\n            output(b'')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget pep8 reporter state from stack.", "response": "def get_reporter_state():\n    \"\"\"Get pep8 reporter state from stack.\"\"\"\n    # Stack\n    # 1. get_reporter_state (i.e. this function)\n    # 2. putty_ignore_code\n    # 3. QueueReport.error or pep8.StandardReport.error for flake8 -j 1\n    # 4. pep8.Checker.check_ast or check_physical or check_logical\n    #    locals contains `tree` (ast) for check_ast\n    frame = sys._getframe(3)\n    reporter = frame.f_locals['self']\n    line_number = frame.f_locals['line_number']\n    offset = frame.f_locals['offset']\n    text = frame.f_locals['text']\n    check = frame.f_locals['check']\n    return reporter, line_number, offset, text, check"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements pep8 ignore_code hook.", "response": "def putty_ignore_code(options, code):\n    \"\"\"Implement pep8 'ignore_code' hook.\"\"\"\n    reporter, line_number, offset, text, check = get_reporter_state()\n    try:\n        line = reporter.lines[line_number - 1]\n    except IndexError:\n        line = ''\n\n    options.ignore = options._orig_ignore\n    options.select = options._orig_select\n\n    for rule in options.putty_ignore:\n        if rule.match(reporter.filename, line, list(reporter.counters) + [code]):\n            if rule._append_codes:\n                options.ignore = options.ignore + rule.codes\n            else:\n                options.ignore = rule.codes\n\n    for rule in options.putty_select:\n        if rule.match(reporter.filename, line, list(reporter.counters) + [code]):\n            if rule._append_codes:\n                options.select = options.select + rule.codes\n            else:\n                options.select = rule.codes\n\n    return ignore_code(options, code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds options for command line and config file.", "response": "def add_options(cls, parser):\n        \"\"\"Add options for command line and config file.\"\"\"\n        parser.add_option(\n            '--putty-select', metavar='errors', default='',\n            help='putty select list',\n        )\n        parser.add_option(\n            '--putty-ignore', metavar='errors', default='',\n            help='putty ignore list',\n        )\n        parser.add_option(\n            '--putty-no-auto-ignore', action='store_false',\n            dest='putty_auto_ignore', default=False,\n            help=(' (default) do not auto ignore lines matching '\n                  '# flake8: disable=<code>,<code>'),\n        )\n        parser.add_option(\n            '--putty-auto-ignore', action='store_true',\n            dest='putty_auto_ignore', default=False,\n            help=('auto ignore lines matching '\n                  '# flake8: disable=<code>,<code>'),\n        )\n        parser.config_options.append('putty-select')\n        parser.config_options.append('putty-ignore')\n        parser.config_options.append('putty-auto-ignore')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing options and activate ignore_code handler.", "response": "def parse_options(cls, options):\n        \"\"\"Parse options and activate `ignore_code` handler.\"\"\"\n        if (not options.putty_select and not options.putty_ignore and\n                not options.putty_auto_ignore):\n            return\n\n        options._orig_select = options.select\n        options._orig_ignore = options.ignore\n\n        options.putty_select = Parser(options.putty_select)._rules\n        options.putty_ignore = Parser(options.putty_ignore)._rules\n\n        if options.putty_auto_ignore:\n            options.putty_ignore.append(AutoLineDisableRule())\n\n        options.ignore_code = functools.partial(\n            putty_ignore_code,\n            options,\n        )\n\n        options.report._ignore_code = options.ignore_code"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck response for error message.", "response": "def _raise_on_error(data):\n    \"\"\"Check response for error message.\"\"\"\n    if isinstance(data, list):\n        data = data[0]\n\n    if isinstance(data, dict) and 'error' in data:\n        raise_error(data['error'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_user(self, device_type):\n        result = await self.request('post', '', {\n            'devicetype': device_type\n        }, auth=False)\n        self.username = result[0]['success']['username']\n        return self.username", "response": "Create a user.\n\n        https://developers.meethue.com/documentation/configuration-api#71_create_user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a request to the API.", "response": "async def request(self, method, path, json=None, auth=True):\n        \"\"\"Make a request to the API.\"\"\"\n        url = 'http://{}/api/'.format(self.host)\n        if auth:\n            url += '{}/'.format(self.username)\n        url += path\n\n        try:\n            async with self.websession.request(method, url, json=json) as res:\n                if res.content_type != 'application/json':\n                    raise ResponseError(\n                        'Invalid content type: {}'.format(res.content_type))\n                data = await res.json()\n                _raise_on_error(data)\n                return data\n        except client_exceptions.ClientError as err:\n            raise RequestError(\n                'Error requesting data from {}: {}'.format(self.host, err)\n            ) from None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange config of a Daylight sensor.", "response": "async def set_config(self, on=None, long=None, lat=None,\n                         sunriseoffset=None, sunsetoffset=None):\n        \"\"\"Change config of a Daylight sensor.\"\"\"\n        data = {\n            key: value for key, value in {\n                'on': on,\n                'long': long,\n                'lat': lat,\n                'sunriseoffset': sunriseoffset,\n                'sunsetoffset': sunsetoffset,\n            }.items() if value is not None\n        }\n\n        await self._request('put', 'sensors/{}/config'.format(self.id),\n                            json=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges config of a CLIP LightLevel sensor.", "response": "async def set_config(self, on=None, tholddark=None, tholdoffset=None):\n        \"\"\"Change config of a CLIP LightLevel sensor.\"\"\"\n        data = {\n            key: value for key, value in {\n                'on': on,\n                'tholddark': tholddark,\n                'tholdoffset': tholdoffset,\n            }.items() if value is not None\n        }\n\n        await self._request('put', 'sensors/{}/config'.format(self.id),\n                            json=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all unpaid invoices on freckle but with transactions.", "response": "def get_unpaid_invoices_with_transactions(branch=None):\n    \"\"\"\n    Returns all invoices that are unpaid on freckle but have transactions.\n\n    This means, that the invoice is either partially paid and can be left as\n    unpaid in freckle, or the invoice has been fully paid and should be set to\n    paid in freckle as well.\n\n    \"\"\"\n    if not client:  # pragma: nocover\n        return None\n    result = {}\n    try:\n        unpaid_invoices = client.fetch_json(\n            'invoices', query_params={'state': 'unpaid'})\n    except (ConnectionError, HTTPError):  # pragma: nocover\n        result.update({'error': _('Wasn\\'t able to connect to Freckle.')})\n    else:\n        invoices = []\n        for invoice in unpaid_invoices:\n            invoice_with_transactions = models.Invoice.objects.filter(\n                invoice_number=invoice['reference'],\n                transactions__isnull=False)\n            if branch:\n                invoice_with_transactions = invoice_with_transactions.filter(\n                    branch=branch)\n            if invoice_with_transactions:\n                invoices.append(invoice)\n        result.update({'invoices': invoices})\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the user - specific configuration file and set the default values.", "response": "def load_cfg(self, src_dir):\r\n        \"\"\"\r\n        Load defaults from configuration file:\r\n         - from the source/directory/.dirsync file (prioritary)\r\n         - and/or a %HOME%/.dirsync user config file\r\n        \"\"\"\r\n\r\n        # last files override previous ones\r\n        cfg_files = [os.path.expanduser(USER_CFG_FILE),\r\n                     os.path.abspath(os.path.join(src_dir, '.dirsync'))]\r\n\r\n        cfg = ConfigParser()\r\n        cfg.read(cfg_files)\r\n\r\n        if not cfg.has_section('defaults'):\r\n            return\r\n\r\n        defaults = {}\r\n        for name, val in cfg.items('defaults'):\r\n            try:\r\n                opt = OPTIONS[name][1]\r\n            except KeyError:\r\n                if name == 'action':\r\n                    if val in OPTIONS:\r\n                        defaults['action'] = val\r\n                    else:\r\n                        raise ValueError('Invalid value for \"action\" option '\r\n                                         'in configuration file: %s' % val)\r\n                continue\r\n\r\n            curdef = opt.get('default', '')\r\n            if isinstance(curdef, bool):\r\n                newdef = val not in ('0', 'False', 'false')\r\n            elif isinstance(curdef, string_types):\r\n                newdef = val\r\n            else:\r\n                newdef = val.strip('\\n').split('\\n')\r\n\r\n            defaults[name] = newdef\r\n\r\n        self.set_defaults(**defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_fields(src, to):\n    args = tuple(getattr(src, field.attname) for field in src._meta.fields)\n    return to(*args)", "response": "Returns a new instance of to_cls with fields fetched from src."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the status code of the active workers.", "response": "def active_status(self, request):\n        \"\"\"\n        This will only work if you have django-celery installed (for now).\n        In case you only need to work with status codes to find out if the\n        workers are up or not.\n        This will only work if we assume our db only contains \"active workers\".\n        To use this feature, you must ensure you use only named workers,\n        For example: \"-n worker1@localhost:8000\".\n        http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\n        \"\"\"\n\n        app_installed = \"djcelery\" in settings.INSTALLED_APPS\n        if not app_installed:\n            return Response(status=status.HTTP_501_NOT_IMPLEMENTED)\n\n        from djcelery.models import WorkerState\n\n        count_workers = WorkerState.objects.all().count()\n        result = self.inspect.active()\n\n        if result is not None and count_workers == len(result):\n            return Response(status=status.HTTP_200_OK)\n\n        return Response(status=status.HTTP_404_NOT_FOUND)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare contents of two directories.", "response": "def _compare(self, dir1, dir2):\n        \"\"\" Compare contents of two directories \"\"\"\n\n        left = set()\n        right = set()\n\n        self._numdirs += 1\n\n        excl_patterns = set(self._exclude).union(self._ignore)\n\n        for cwd, dirs, files in os.walk(dir1):\n            self._numdirs += len(dirs)\n            for f in dirs + files:\n                path = os.path.relpath(os.path.join(cwd, f), dir1)\n                re_path = path.replace('\\\\', '/')\n                if self._only:\n                    for pattern in self._only:\n                        if re.match(pattern, re_path):\n                            # go to exclude and ignore filtering\n                            break\n                    else:\n                        # next item, this one does not match any pattern\n                        # in the _only list\n                        continue\n\n                add_path = False\n                for pattern in self._include:\n                    if re.match(pattern, re_path):\n                        add_path = True\n                        break\n                else:\n                    # path was not in includes\n                    # test if it is in excludes\n                    for pattern in excl_patterns:\n                        if re.match(pattern, re_path):\n                            # path is in excludes, do not add it\n                            break\n                    else:\n                        # path was not in excludes\n                        # it should be added\n                        add_path = True\n\n                if add_path:\n                    left.add(path)\n                    anc_dirs = re_path[:-1].split('/')\n                    for i in range(1, len(anc_dirs)):\n                        left.add('/'.join(anc_dirs[:i]))\n\n        for cwd, dirs, files in os.walk(dir2):\n            for f in dirs + files:\n                path = os.path.relpath(os.path.join(cwd, f), dir2)\n                re_path = path.replace('\\\\', '/')\n                for pattern in self._ignore:\n                    if re.match(pattern, re_path):\n                        if f in dirs:\n                            dirs.remove(f)\n                        break\n                else:\n                    right.add(path)\n                    # no need to add the parent dirs here,\n                    # as there is no _only pattern detection\n                    if f in dirs and path not in left:\n                        self._numdirs += 1\n\n        common = left.intersection(right)\n        left.difference_update(common)\n        right.difference_update(common)\n\n        return DCMP(left, right, common)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cmptimestamps(self, filest1, filest2):\n\n        mtime_cmp = int((filest1.st_mtime - filest2.st_mtime) * 1000) > 0\n        if self._use_ctime:\n            return mtime_cmp or \\\n                   int((filest1.st_ctime - filest2.st_mtime) * 1000) > 0\n        else:\n            return mtime_cmp", "response": "Compare time stamps of two files and return True if they are newer than the file1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dirdiffandupdate(self, dir1, dir2):\n        self._dowork(dir1, dir2, None, self._update)", "response": "Private function which does directory diff & update"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sync(self):\n\n        self._copyfiles = True\n        self._updatefiles = True\n        self._creatdirs = True\n        self._copydirection = 0\n\n        if self._verbose:\n            self.log('Synchronizing directory %s with %s\\n' %\n                     (self._dir2, self._dir1))\n        self._dirdiffcopyandupdate(self._dir1, self._dir2)", "response": "Synchronize two directories w. r. t\n        each other s contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self):\n\n        self._copyfiles = False\n        self._updatefiles = True\n        self._purge = False\n        self._creatdirs = False\n\n        if self._verbose:\n            self.log('Updating directory %s with %s\\n' %\n                     (self._dir2, self._dir1))\n        self._dirdiffandupdate(self._dir1, self._dir2)", "response": "Update the target directory with the common version of the source directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef report(self):\n\n        # We need only the first 4 significant digits\n        tt = (str(self._endtime - self._starttime))[:4]\n\n        self.log('\\n%s finished in %s seconds.' % (__pkg_name__, tt))\n        self.log('%d directories parsed, %d files copied' %\n                 (self._numdirs, self._numfiles))\n        if self._numdelfiles:\n            self.log('%d files were purged.' % self._numdelfiles)\n        if self._numdeldirs:\n            self.log('%d directories were purged.' % self._numdeldirs)\n        if self._numnewdirs:\n            self.log('%d directories were created.' % self._numnewdirs)\n        if self._numupdates:\n            self.log('%d files were updated by timestamp.' % self._numupdates)\n\n        # Failure stats\n        self.log('')\n        if self._numcopyfld:\n            self.log('there were errors in copying %d files.'\n                     % self._numcopyfld)\n        if self._numdirsfld:\n            self.log('there were errors in creating %d directories.'\n                     % self._numdirsfld)\n        if self._numupdsfld:\n            self.log('there were errors in updating %d files.'\n                     % self._numupdsfld)\n        if self._numdeldfld:\n            self.log('there were errors in purging %d directories.'\n                     % self._numdeldfld)\n        if self._numdelffld:\n            self.log('there were errors in purging %d files.'\n                     % self._numdelffld)", "response": "Print the report of work at the end of the iteration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def set_action(self, on=None, bri=None, hue=None, sat=None, xy=None,\n                         ct=None, alert=None, effect=None, transitiontime=None,\n                         bri_inc=None, sat_inc=None, hue_inc=None, ct_inc=None,\n                         xy_inc=None, scene=None):\n        \"\"\"Change action of a group.\"\"\"\n        data = {\n            key: value for key, value in {\n                'on': on,\n                'bri': bri,\n                'hue': hue,\n                'sat': sat,\n                'xy': xy,\n                'ct': ct,\n                'alert': alert,\n                'effect': effect,\n                'transitiontime': transitiontime,\n                'bri_inc': bri_inc,\n                'sat_inc': sat_inc,\n                'hue_inc': hue_inc,\n                'ct_inc': ct_inc,\n                'xy_inc': xy_inc,\n                'scene': scene,\n            }.items() if value is not None\n        }\n\n        await self._request('put', 'groups/{}/action'.format(self.id),\n                            json=data)", "response": "Change action of a group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling pre - tasks.", "response": "def _pre_tasks(self):\n        \"\"\"\n        Pre-tasks handler.\n        \"\"\"\n        if self.flushdb:\n            management.call_command('flush', verbosity=0, interactive=False)\n            logger.info('Flushed database')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield successive n - sized chunks from l.", "response": "def chunks(l, n):\n    \"\"\"\n    Yields successive n-sized chunks from l.\n    \"\"\"\n    for i in _range(0, len(l), n):\n        yield l[i:i + n]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog recipe instance SQL queries (actually, only time).", "response": "def log_queries(recipe):\n    \"\"\"\n    Logs recipe instance SQL queries (actually, only time).\n    \"\"\"\n    logger.debug(\n        '\u2690 Badge %s: SQL queries time %.2f second(s)',\n        recipe.slug,\n        sum([float(q['time']) for q in connection.queries]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a new recipe class.", "response": "def register(self, recipe):\n        \"\"\"\n        Registers a new recipe class.\n        \"\"\"\n\n        if not isinstance(recipe, (list, tuple)):\n            recipe = [recipe, ]\n\n        for item in recipe:\n            recipe = self.get_recipe_instance_from_class(item)\n            self._registry[recipe.slug] = recipe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unregister(self, recipe):\n        recipe = self.get_recipe_instance_from_class(recipe)\n        if recipe.slug in self._registry:\n            del self._registry[recipe.slug]", "response": "Unregisters a given recipe class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the recipe instance for the given badge slug.", "response": "def get_recipe_instance(self, badge):\n        \"\"\"\n        Returns the recipe instance for the given badge slug.\n        If badge has not been registered, raises ``exceptions.BadgeNotFound``.\n        \"\"\"\n        from .exceptions import BadgeNotFound\n        if badge in self._registry:\n            return self.recipes[badge]\n        raise BadgeNotFound()"}
