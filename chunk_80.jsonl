{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread at most size bytes from the file.", "response": "def read(self, size=-1):\n        \"\"\"Read at most `size` bytes from the file (less if there\n        isn't enough data).\n\n        The bytes are returned as an instance of :class:`str`. If\n        `size` is negative or omitted all data is read.\n\n        :Parameters:\n          - `size` (optional): the number of bytes to read\n        \"\"\"\n        if not size:\n            return defer.succeed(None)\n\n        remainder = int(self.length) - self.__position\n        if size < 0 or size > remainder:\n            size = remainder\n\n        class State(object): pass\n        state = State()\n        state.data = self.__buffer\n        state.chunk_number = (len(state.data) + self.__position) / self.chunk_size\n\n        def iterate(_=None):\n            if len(state.data) < size:\n                return self.__chunks.find_one({\"files_id\": self._id, \"n\": state.chunk_number})\\\n                    .addCallback(process).addCallback(iterate)\n            return defer.succeed(None)\n\n        def process(chunk):\n            if not chunk:\n                raise CorruptGridFile(\"TxMongo: no chunk #{0}\".format(state.chunk_number))\n\n            if not state.data:\n                state.data += chunk[\"data\"][self.__position % self.chunk_size:]\n            else:\n                state.data += chunk[\"data\"]\n\n            state.chunk_number += 1\n\n        def done(_):\n            self.__position += size\n            to_return = state.data[:size]\n            self.__buffer = state.data[size:]\n            return to_return\n\n        return iterate().addCallback(done)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seek(self, pos, whence=os.SEEK_SET):\n        if whence == os.SEEK_SET:\n            new_pos = pos\n        elif whence == os.SEEK_CUR:\n            new_pos = self.__position + pos\n        elif whence == os.SEEK_END:\n            new_pos = int(self.length) + pos\n        else:\n            raise IOError(22, \"TxMongo: invalid value for `whence`\")\n\n        if new_pos < 0:\n            raise IOError(22, \"TxMongo: invalid value for `pos` - must be positive\")\n\n        self.__position = new_pos", "response": "Set the current position of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all the new requests that were found in this request.", "response": "def get_new_requests(self):\n        \"\"\"Retrieve all the new request that were found in this request.\n\n        Returns:\n            list(:class:`nyawc.http.Request`): A list of request objects.\n\n        \"\"\"\n\n        content_type = self.__queue_item.response.headers.get('content-type')\n        scrapers = self.__get_all_scrapers()\n        new_requests = []\n\n        for scraper in scrapers:\n            instance = scraper(self.__options, self.__queue_item)\n            if self.__content_type_matches(content_type, instance.content_types):\n                new_requests.extend(instance.get_requests())\n\n        return new_requests"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a request with the given data.", "response": "def __make_request(self, url, method, data, auth, cookies, headers, proxies, timeout, verify):\n        \"\"\"Execute a request with the given data.\n\n        Args:\n            url (str): The URL to call.\n            method (str): The method (e.g. `get` or `post`).\n            data (str): The data to call the URL with.\n            auth (obj): The authentication class.\n            cookies (obj): The cookie dict.\n            headers (obj): The header dict.\n            proxies (obj): The proxies dict.\n            timeout (int): The request timeout in seconds.\n            verify (mixed): SSL verification.\n\n        Returns:\n            obj: The response object.\n\n        \"\"\"\n\n        request_by_method = getattr(requests, method)\n        return request_by_method(\n            url=url,\n            data=data,\n            auth=auth,\n            cookies=cookies,\n            headers=headers,\n            proxies=proxies,\n            timeout=timeout,\n            verify=verify,\n            allow_redirects=True,\n            stream=False\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind all available scraper references.", "response": "def __get_all_scrapers(self):\n        \"\"\"Find all available scraper references.\n\n        Returns:\n            list(obj): The scraper references.\n\n        \"\"\"\n\n        modules_strings = self.__get_all_scrapers_modules()\n        modules = []\n\n        for module_string in modules_strings:\n            module = importlib.import_module(\"nyawc.scrapers.\" + module_string)\n            modules.append(getattr(module, module_string))\n\n        return modules"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding all available scraper modules.", "response": "def __get_all_scrapers_modules(self):\n        \"\"\"Find all available scraper modules.\n\n        Returns:\n            list(obj): The scraper modules.\n\n        \"\"\"\n\n        modules = []\n\n        file = os.path.realpath(__file__)\n        folder = os.path.dirname(file)\n\n        for filename in os.listdir(folder + \"/../scrapers\"):\n            if filename.endswith(\"Scraper.py\") and not filename.startswith(\"Base\"):\n                modules.append(filename[:-3])\n\n        return modules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the given content type matches one of the available content types.", "response": "def __content_type_matches(self, content_type, available_content_types):\n        \"\"\"Check if the given content type matches one of the available content types.\n\n        Args:\n            content_type (str): The given content type.\n            available_content_types list(str): All the available content types.\n\n        Returns:\n            bool: True if a match was found, False otherwise.\n\n        \"\"\"\n\n        if content_type is None:\n            return False\n\n        if content_type in available_content_types:\n            return True\n\n        for available_content_type in available_content_types:\n            if available_content_type in content_type:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all the new requests that were found in the response.", "response": "def derived_get_requests(self):\n        \"\"\"Get all the new requests that were found in the response.\n\n        Returns:\n            list(:class:`nyawc.http.Request`): A list of new requests that were found.\n\n        \"\"\"\n\n        host = self.queue_item.response.url\n        content = self.queue_item.response.text\n\n        found_requests = []\n\n        for expression in self.__expressions:\n            matches = re.findall(expression[\"raw\"], content)\n\n            for match in matches:\n                found_url = match[expression[\"group\"]]\n                absolute_url = URLHelper.make_absolute(host, found_url)\n                found_requests.append(Request(absolute_url))\n\n        return found_requests"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef increase_route_count(self, crawled_request):\n\n        for route in self.__routing_options.routes:\n            if re.compile(route).match(crawled_request.url):\n                count_key = str(route) + crawled_request.method\n                \n                if count_key in self.__routing_count.keys():\n                    self.__routing_count[count_key] += 1\n                else:\n                    self.__routing_count[count_key] = 1\n\n                break", "response": "Increase the count that determines how many times a URL of a certain route has been crawled."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_treshold_reached(self, scraped_request):\n\n        for route in self.__routing_options.routes:\n            if re.compile(route).match(scraped_request.url):\n                count_key = str(route) + scraped_request.method\n\n                if count_key in self.__routing_count.keys():\n                    return self.__routing_count[count_key] >= self.__routing_options.minimum_threshold\n                \n        return False", "response": "Checks if the minimum treshold of the given requests is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a request to the queue.", "response": "def add_request(self, request):\n        \"\"\"Add a request to the queue.\n\n        Args:\n            request (:class:`nyawc.http.Request`): The request to add.\n\n        Returns:\n            :class:`nyawc.QueueItem`: The created queue item.\n\n        \"\"\"\n\n        queue_item = QueueItem(request, Response(request.url))\n        self.add(queue_item)\n        return queue_item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the given request already exists in the queue.", "response": "def has_request(self, request):\n        \"\"\"Check if the given request already exists in the queue.\n\n        Args:\n            request (:class:`nyawc.http.Request`): The request to check.\n\n        Returns:\n            bool: True if already exists, False otherwise.\n\n        \"\"\"\n\n        queue_item = QueueItem(request, Response(request.url))\n        key = queue_item.get_hash()\n\n        for status in QueueItem.STATUSES:\n            if key in self.__get_var(\"items_\" + status).keys():\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, queue_item):\n\n        hash_key = queue_item.get_hash()\n        items = self.__get_var(\"items_\" + queue_item.status)\n\n        if hash_key in items.keys():\n            return\n\n        items[queue_item.get_hash()] = queue_item\n\n        self.count_total += 1", "response": "Add a request or response pair to the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, queue_item, status):\n\n        items = self.__get_var(\"items_\" + queue_item.status)\n\n        del items[queue_item.get_hash()]\n        self.count_total -= 1\n\n        queue_item.status = status\n        self.add(queue_item)", "response": "Move a request or response pair to another status."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves a bulk of request / response pairs to another status", "response": "def move_bulk(self, from_statuses, to_status):\n        \"\"\"Move a bulk of request/response pairs to another status\n\n        Args:\n            from_statuses list(str): The statuses to move from\n            to_status (str): The status to move to\n\n        \"\"\"\n\n        for status in from_statuses:\n            from_status_items = self.__get_var(\"items_\" + status)\n            self.__set_var(\"items_\" + status, OrderedDict())\n\n            to_status_items = self.__get_var(\"items_\" + to_status)\n            to_status_items.update(from_status_items)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the first item in the queue that has the given status.", "response": "def get_first(self, status):\n        \"\"\"Get the first item in the queue that has the given status.\n\n        Args:\n            status (str): return the first item with this status.\n\n        Returns:\n            :class:`nyawc.QueueItem`: The first queue item with the given status.\n\n        \"\"\"\n\n        items = self.get_all(status)\n\n        if items:\n            return list(items.items())[0][1]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the progress of the queue in percentage ( float.", "response": "def get_progress(self):\n        \"\"\"Get the progress of the queue in percentage (float).\n\n        Returns:\n            float: The 'finished' progress in percentage.\n\n        \"\"\"\n\n        count_remaining = len(self.items_queued) + len(self.items_in_progress)\n        percentage_remaining = 100 / self.count_total * count_remaining\n\n        return 100 - percentage_remaining"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_for_type(input_type=\"text\"):\n\n        if input_type in RandomInputHelper.cache:\n            return RandomInputHelper.cache[input_type]\n\n        types = {\n            \"text\": RandomInputHelper.get_random_value,\n            \"hidden\": RandomInputHelper.get_random_value,\n            \"search\": RandomInputHelper.get_random_value,\n            \"color\": RandomInputHelper.get_random_color,\n            \"week\": {\"function\": RandomInputHelper.get_random_value, \"params\": [2, [\"1234\"]]},\n            \"password\": RandomInputHelper.get_random_password,\n            \"number\": RandomInputHelper.get_random_number,\n            \"tel\": RandomInputHelper.get_random_telephonenumber,\n            \"url\": RandomInputHelper.get_random_url,\n            \"textarea\": RandomInputHelper.get_random_text,\n            \"email\": RandomInputHelper.get_random_email\n        }\n\n        if types.get(input_type) is None:\n            return \"\"\n\n        if type(types.get(input_type)) is dict:\n            generator = types.get(input_type)\n            value = generator.get(\"function\")(*generator.get(\"params\"))\n        else:\n            value = types.get(input_type)()\n\n        RandomInputHelper.cache[input_type] = value\n\n        return value", "response": "Get a random string for the given html input type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a random string with the given length.", "response": "def get_random_value(length=10, character_sets=[string.ascii_uppercase, string.ascii_lowercase]):\n        \"\"\"Get a random string with the given length.\n\n        Args:\n            length (int): The length of the string to return.\n            character_sets list(str): The caracter sets to use.\n\n        Returns:\n            str: The random string.\n\n        \"\"\"\n\n        return \"\".join(random.choice(\"\".join(character_sets)) for i in range(length))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a random email address with the given ltd.", "response": "def get_random_email(ltd=\"com\"):\n        \"\"\"Get a random email address with the given ltd.\n\n        Args:\n            ltd (str): The ltd to use (e.g. com).\n\n        Returns:\n            str: The random email.\n\n        \"\"\"\n\n        email = [\n            RandomInputHelper.get_random_value(6, [string.ascii_lowercase]),\n            \"@\",\n            RandomInputHelper.get_random_value(6, [string.ascii_lowercase]),\n            \".\",\n            ltd\n        ]\n\n        return \"\".join(email)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a random password that complies with most of the requirements.", "response": "def get_random_password():\n        \"\"\"Get a random password that complies with most of the requirements.\n\n        Note:\n            This random password is not strong and not \"really\" random, and should only be\n            used for testing purposes.\n\n        Returns:\n            str: The random password.\n\n        \"\"\"\n\n        password = []\n\n        password.append(RandomInputHelper.get_random_value(4, [string.ascii_lowercase]))\n        password.append(RandomInputHelper.get_random_value(2, [string.digits]))\n        password.append(RandomInputHelper.get_random_value(2, [\"$&*@!\"]))\n        password.append(RandomInputHelper.get_random_value(4, [string.ascii_uppercase]))\n\n        return \"\".join(password)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_random_url(ltd=\"com\"):\n\n        url = [\n            \"https://\",\n            RandomInputHelper.get_random_value(8, [string.ascii_lowercase]),\n            \".\",\n            ltd\n        ]\n\n        return \"\".join(url)", "response": "Get a random url with the given ltd."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a random 10 digit phone number that complies with most of the requirements.", "response": "def get_random_telephonenumber():\n        \"\"\"Get a random 10 digit phone number that complies with most of the requirements.\n\n        Returns:\n            str: The random telephone number.\n\n        \"\"\"\n\n        phone = [\n            RandomInputHelper.get_random_value(3, \"123456789\"),\n            RandomInputHelper.get_random_value(3, \"12345678\"),\n            \"\".join(map(str, random.sample(range(10), 4)))\n        ]\n\n        return \"-\".join(phone)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_with_options(request, options, parent_queue_item=None):\n\n        request.auth = copy.deepcopy(options.identity.auth)\n        request.cookies = copy.deepcopy(options.identity.cookies)\n        request.headers = copy.deepcopy(options.identity.headers)\n        request.proxies = copy.deepcopy(options.identity.proxies)\n        request.timeout = copy.copy(options.performance.request_timeout)\n\n        if parent_queue_item != None:\n            for cookie in parent_queue_item.request.cookies:\n                request.cookies.set(cookie.name, cookie.value, domain=cookie.domain, path=cookie.path)\n\n            for cookie in parent_queue_item.response.cookies:\n                request.cookies.set(cookie.name, cookie.value, domain=cookie.domain, path=cookie.path)\n\n        if options.misc.verify_ssl_certificates and options.misc.trusted_certificates:\n            request.verify = options.misc.trusted_certificates\n        else:\n            request.verify = options.misc.verify_ssl_certificates", "response": "Patch the given request with the given options."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the new request complies with the crawling scope.", "response": "def complies_with_scope(queue_item, new_request, scope):\n        \"\"\"Check if the new request complies with the crawling scope.\n\n        Args:\n            queue_item (:class:`nyawc.QueueItem`): The parent queue item of the new request.\n            new_request (:class:`nyawc.http.Request`): The request to check.\n            scope (:class:`nyawc.Options.OptionsScope`): The scope to check.\n\n        Returns:\n            bool: True if it complies, False otherwise.\n\n        \"\"\"\n\n        if not URLHelper.is_parsable(queue_item.request.url):\n            return False\n\n        if not URLHelper.is_parsable(new_request.url):\n            return False\n\n        if scope.request_methods:\n            if not queue_item.request.method in scope.request_methods:\n                return False\n\n        if scope.protocol_must_match:\n            if URLHelper.get_protocol(queue_item.request.url) != URLHelper.get_protocol(new_request.url):\n                return False\n\n        if scope.subdomain_must_match:\n            current_subdomain = URLHelper.get_subdomain(queue_item.request.url)\n            new_subdomain = URLHelper.get_subdomain(new_request.url)\n\n            www_matches = False\n\n            if current_subdomain == \"www\" and new_subdomain == \"\":\n                www_matches = True\n\n            if new_subdomain == \"www\" and current_subdomain == \"\":\n                www_matches = True\n\n            if not www_matches and current_subdomain != new_subdomain:\n                return False\n\n        if scope.hostname_must_match:\n            if URLHelper.get_hostname(queue_item.request.url) != URLHelper.get_hostname(new_request.url):\n                return False\n\n        if scope.tld_must_match:\n            if URLHelper.get_tld(queue_item.request.url) != URLHelper.get_tld(new_request.url):\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cookie_header(queue_item):\n\n        header = []\n        path = URLHelper.get_path(queue_item.request.url)\n\n        for cookie in queue_item.request.cookies:\n            root_path = cookie.path == \"\" or cookie.path == \"/\"\n            if path.startswith(cookie.path) or root_path:\n                header.append(cookie.name + \"=\" + cookie.value)\n\n        return \"&\".join(header)", "response": "Convert a requests cookie jar to a HTTP request cookie header value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_soup_response(self):\n\n        if self.response is not None:\n            if self.__response_soup is None:\n                result = BeautifulSoup(self.response.text, \"lxml\")\n\n                if self.decomposed:\n                    return result\n                else:\n                    self.__response_soup = BeautifulSoup(self.response.text, \"lxml\")\n\n        return self.__response_soup", "response": "Get the response as a cached BeautifulSoup container."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate and return the dict index hash of the given queue item.", "response": "def get_hash(self):\n        \"\"\"Generate and return the dict index hash of the given queue item.\n\n        Note:\n            Cookies should not be included in the hash calculation because\n            otherwise requests are crawled multiple times with e.g. different\n            session keys, causing infinite crawling recursion.\n\n        Note:\n            At this moment the keys do not actually get hashed since it works perfectly without and\n            since hashing the keys requires us to built hash collision management.\n\n        Returns:\n            str: The hash of the given queue item.\n\n        \"\"\"\n\n        if self.__index_hash:\n            return self.__index_hash\n\n        key = self.request.method\n\n        key += URLHelper.get_protocol(self.request.url)\n        key += URLHelper.get_subdomain(self.request.url)\n        key += URLHelper.get_hostname(self.request.url)\n        key += URLHelper.get_tld(self.request.url)\n        key += URLHelper.get_path(self.request.url)\n\n        key += str(URLHelper.get_ordered_params(self.request.url))\n\n        if self.request.data is not None:\n            key += str(self.request.data.keys())\n\n        self.__index_hash = key\n        return self.__index_hash"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef derived_get_requests(self):\n\n        host = self.queue_item.response.url\n        soup = self.queue_item.get_soup_response()\n\n        found_requests = []\n\n        for form in soup.find_all(\"form\"):\n            found_requests.append(self.__get_request(host, form))\n\n        return found_requests", "response": "Get all the new requests that were found in the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a request from the given soup form.", "response": "def __get_request(self, host, soup):\n        \"\"\"Build a request from the given soup form.\n\n        Args:\n            host str: The URL of the current queue item.\n            soup (obj): The BeautifulSoup form.\n\n        Returns:\n            :class:`nyawc.http.Request`: The new Request.\n\n        \"\"\"\n\n        url = URLHelper.make_absolute(host, self.__trim_grave_accent(soup[\"action\"])) if soup.has_attr(\"action\") else host\n        method_original = soup[\"method\"] if soup.has_attr(\"method\") else \"get\"\n        method = \"post\" if method_original.lower() == \"post\" else \"get\"\n        data = self.__get_form_data(soup)\n\n        return Request(url, method, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrimming grave accents manually.", "response": "def __trim_grave_accent(self, href):\n        \"\"\"Trim grave accents manually (because BeautifulSoup doesn\"t support it).\n\n        Args:\n            href (str): The BeautifulSoup href value.\n\n        Returns:\n            str: The BeautifulSoup href value without grave accents.\n\n        \"\"\"\n\n        if href.startswith(\"`\"):\n            href = href[1:]\n\n        if href.endswith(\"`\"):\n            href = href[:-1]\n\n        return href"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a form data dict from the given BeautifulSoup object.", "response": "def __get_form_data(self, soup):\n        \"\"\"Build a form data dict from the given form.\n\n        Args:\n            soup (obj): The BeautifulSoup form.\n\n        Returns:\n            obj: The form data (key/value).\n\n        \"\"\"\n\n        elements = self.__get_valid_form_data_elements(soup)\n        form_data = self.__get_default_form_data_input(elements)\n        callback = self.options.callbacks.form_before_autofill\n        action = callback(self.queue_item, elements, form_data)\n\n        if action == CrawlerActions.DO_AUTOFILL_FORM:\n            self.__autofill_form_data(form_data, elements)\n\n        return form_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all valid form data elements.", "response": "def __get_valid_form_data_elements(self, soup):\n        \"\"\"Get all valid form input elements.\n\n        Note:\n            An element is valid when the value can be updated client-side\n            and the element has a name attribute.\n\n        Args:\n            soup (obj): The BeautifulSoup form.\n\n        Returns:\n            list(obj): Soup elements.\n\n        \"\"\"\n\n        elements = []\n\n        for element in soup.find_all([\"input\", \"button\", \"textarea\", \"select\"]):\n            if element.has_attr(\"name\"):\n                elements.append(element)\n\n        return elements"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_default_form_data_input(self, elements):\n\n        form_data = OrderedDict()\n\n        for element in elements:\n            default_value = self.__get_default_value_from_element(element)\n\n            if default_value is False:\n                continue\n\n            form_data[element[\"name\"]] = default_value\n\n        return form_data", "response": "Get the default form data for the given elements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the default value of a form element.", "response": "def __get_default_value_from_element(self, element):\n        \"\"\"Get the default value of a form element\n\n        Args:\n            elements (obj): The soup element.\n\n        Returns:\n            str: The default value\n\n        \"\"\"\n\n        if element.name == \"select\":\n            options = element.find_all(\"option\")\n            is_multiple = element.has_attr(\"multiple\")\n\n            selected_options = [\n                option for option in options\n                if option.has_attr(\"selected\")\n            ]\n\n            if not selected_options and options:\n                selected_options = [options[0]]\n\n            selected_values = []\n\n            if is_multiple:\n                for option in selected_options:\n                    value = option[\"value\"] if option.has_attr(\"value\") else option.string\n                    selected_values.append(value)\n\n                return selected_values\n            elif len(selected_options) >= 1:\n                if selected_options[0].has_attr(\"value\"):\n                    return selected_options[0][\"value\"]\n                else:\n                    return selected_options[0].string\n\n            return \"\"\n\n        if element.name == \"textarea\":\n            return element.string if element.string is not None else \"\"\n\n        if element.name == \"input\" and element.has_attr(\"type\"):\n            if element[\"type\"] in (\"checkbox\", \"radio\"):\n                if not element.has_attr(\"checked\"):\n                    return False\n\n                if element.has_attr(\"value\"):\n                    return element[\"value\"]\n                else:\n                    return \"on\"\n\n        if element.has_attr(\"value\"):\n            return element[\"value\"]\n\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_absolute(base, relative):\n\n        # Python 3.4 and lower do not remove folder traversal strings.\n        # This was fixed in 3.5 (https://docs.python.org/3/whatsnew/3.5.html#urllib)\n        while relative.startswith('/../') or relative.startswith('../'):\n            relative = relative[3:]\n\n            base_parsed = urlparse(base)\n            new_path = base_parsed.path.rsplit('/', 1)[0]\n            base_parsed = base_parsed._replace(path=new_path)\n            base = base_parsed.geturl()\n\n        return urljoin(base, relative)", "response": "Make the given ( relative ) URL absolute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending the given URL with the given data OrderedDict.", "response": "def append_with_data(url, data):\n        \"\"\"Append the given URL with the given data OrderedDict.\n\n        Args:\n            url (str): The URL to append.\n            data (obj): The key value OrderedDict to append to the URL.\n\n        Returns:\n            str: The new URL.\n\n        \"\"\"\n\n        if data is None:\n            return url\n\n        url_parts = list(urlparse(url))\n\n        query = OrderedDict(parse_qsl(url_parts[4], keep_blank_values=True))\n        query.update(data)\n\n        url_parts[4] = URLHelper.query_dict_to_string(query)\n\n        return urlunparse(url_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the given URL is parsable.", "response": "def is_parsable(url):\n        \"\"\"Check if the given URL is parsable (make sure it's a valid URL). If it is parsable, also cache it.\n\n        Args:\n            url (str): The URL to check.\n\n        Returns:\n            bool: True if parsable, False otherwise.\n\n        \"\"\"\n\n        try:\n            parsed = urlparse(url)\n            URLHelper.__cache[url] = parsed\n            return True\n        except:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the protocol of the given URL.", "response": "def get_protocol(url):\n        \"\"\"Get the protocol (e.g. http, https or ftp) of the given URL.\n\n        Args:\n            url (str): The URL to get the protocol from.\n\n        Returns:\n            str: The URL protocol\n\n        \"\"\"\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        return URLHelper.__cache[url].scheme"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_subdomain(url):\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        return \".\".join(URLHelper.__cache[url].netloc.split(\".\")[:-2])", "response": "Get the subdomain of the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the hostname of the given URL.", "response": "def get_hostname(url):\n        \"\"\"Get the hostname of the given URL.\n\n        Args:\n            url (str): The URL to get the hostname from.\n\n        Returns:\n            str: The hostname\n\n        \"\"\"\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        parts = URLHelper.__cache[url].netloc.split(\".\")\n\n        if len(parts) == 1:\n            return parts[0]\n        else:\n            return \".\".join(parts[-2:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tld(url):\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        parts = URLHelper.__cache[url].netloc.split(\".\")\n\n        if len(parts) == 1:\n            return \"\"\n        else:\n            return parts[-1]", "response": "Get the tld of the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the path of the given URL.", "response": "def get_path(url):\n        \"\"\"Get the path (e.g /page/23) of the given URL.\n\n        Args:\n            url (str): The URL to get the path from.\n\n        Returns:\n            str: The path\n\n        \"\"\"\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        return URLHelper.__cache[url].path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the query parameters of the given URL in alphabetical order.", "response": "def get_ordered_params(url):\n        \"\"\"Get the query parameters of the given URL in alphabetical order.\n\n        Args:\n            url (str): The URL to get the query parameters from.\n\n        Returns:\n            str: The query parameters\n\n        \"\"\"\n\n        if url not in URLHelper.__cache:\n            URLHelper.__cache[url] = urlparse(url)\n\n        params = URLHelper.query_string_to_dict(URLHelper.__cache[url].query)\n\n        return OrderedDict(sorted(params.items()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an OrderedDict to a query string.", "response": "def query_dict_to_string(query):\n        \"\"\"Convert an OrderedDict to a query string.\n\n        Args:\n            query (obj): The key value object with query params.\n\n        Returns:\n            str: The query string.\n\n        Note:\n            This method does the same as urllib.parse.urlencode except\n            that it doesn't actually encode the values.\n\n        \"\"\"\n\n        query_params = []\n\n        for key, value in query.items():\n            query_params.append(key + \"=\" + value)\n\n        return \"&\".join(query_params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a query string to a query dict.", "response": "def query_string_to_dict(query):\n        \"\"\"Convert a string to a query dict.\n\n        Args:\n            query (str): The query string.\n\n        Returns:\n            obj: The key value object with query params.\n\n        Note:\n            This method does the same as urllib.parse.parse_qsl except\n            that it doesn't actually decode the values.\n\n        \"\"\"\n\n        query_params = {}\n\n        for key_value in query.split(\"&\"):\n            key_value_pair = key_value.split(\"=\", 1)\n\n            key = key_value_pair[0] if len(key_value_pair) >= 1 else \"\"\n            value = key_value_pair[1] if len(key_value_pair) == 2 else \"\"\n\n            query_params[key] = value\n\n        return query_params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_version():\n\n        if PackageHelper.__version:\n            return PackageHelper.__version\n\n        PackageHelper.__version = \"Unknown\"\n\n        # If this is a GIT clone without install, use the ``.semver`` file.\n        file = os.path.realpath(__file__)\n        folder = os.path.dirname(file)\n\n        try:\n            semver = open(folder + \"/../../.semver\", \"r\")\n            PackageHelper.__version = semver.read().rstrip()\n            semver.close()\n            return PackageHelper.__version\n        except:\n            pass\n\n        # If the package was installed, get the version number via Python's distribution details.\n        try:\n            distribution = pkg_resources.get_distribution(PackageHelper.get_alias())\n            if distribution.version:\n                PackageHelper.__version = distribution.version\n            return PackageHelper.__version\n        except:\n            pass\n\n        return PackageHelper.__version", "response": "Get the version number of this package."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the given GitHub RST contents to PyPi RST contents.", "response": "def rst_to_pypi(contents):\n        \"\"\"Convert the given GitHub RST contents to PyPi RST contents (since some RST directives are not available in PyPi).\n\n        Args:\n            contents (str): The GitHub compatible RST contents.\n\n        Returns:\n            str: The PyPi compatible RST contents.\n\n        \"\"\"\n\n        # The PyPi description does not support the SVG file type.\n        contents = contents.replace(\".svg?pypi=png.from.svg\", \".png\")\n\n        # Convert ``<br class=\"title\">`` to a H1 title\n        asterisks_length = len(PackageHelper.get_name())\n        asterisks = \"*\" * asterisks_length\n        title = asterisks + \"\\n\" + PackageHelper.get_name() + \"\\n\" + asterisks;\n\n        contents = re.sub(r\"(\\.\\. raw\\:\\: html\\n)(\\n {2,4})(\\<br class=\\\"title\\\"\\>)\", title, contents)\n\n        # The PyPi description does not support raw HTML\n        contents = re.sub(r\"(\\.\\. raw\\:\\: html\\n)((\\n {2,4})([A-Za-z0-9<>\\ =\\\"\\/])*)*\", \"\", contents)\n\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef derived_get_requests(self):\n\n        attributes = {\n            \"src\": True,\n            \"href\": True,\n            \"link\": True,\n            \"script\": True,\n            \"url\": True\n        }\n\n        host = self.queue_item.response.url\n        soup = self.queue_item.get_soup_response()\n        base_element = soup.find(\"base\", href=True)\n        elements = soup.select(\"[{}]\".format(\"],[\".join(attributes.keys())))\n\n        # Always use the URL from the base element if it exists.\n        # https://www.w3schools.com/tags/tag_base.asp\n        if base_element:\n            host = URLHelper.make_absolute(host, base_element[\"href\"])\n\n        found_requests = []\n\n        for element in elements:\n            for attribute in attributes.keys():\n                if not element.has_attr(attribute):\n                    continue\n\n                found_url = self.__trim_grave_accent(element[attribute])\n\n                if URLHelper.is_mailto(found_url):\n                    continue\n\n                absolute_url = URLHelper.make_absolute(host, found_url)\n                found_requests.append(Request(absolute_url))\n\n        return found_requests", "response": "Get all the new requests that were found in the response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n\n        try:\n            self.__options.callbacks.request_in_thread_before_start(self.__queue_item)\n        except Exception as e:\n            print(e)\n\n        new_requests = []\n        failed = False\n\n        try:\n            handler = Handler(self.__options, self.__queue_item)\n            new_requests = handler.get_new_requests()\n\n            try:\n                self.__queue_item.response.raise_for_status()\n            except Exception:\n                if self.__queue_item.request.parent_raised_error:\n                    failed = True\n                else:\n                    for new_request in new_requests:\n                        new_request.parent_raised_error = True\n\n        except Exception as e:\n            failed = True\n\n            error_message = \"Setting status of '{}' to '{}' because of an HTTP error.\".format(\n                self.__queue_item.request.url,\n                QueueItem.STATUS_ERRORED\n            )\n\n            DebugHelper.output(self.__options, error_message)\n            DebugHelper.output(self.__options, e)\n\n            try:\n                self.__options.callbacks.request_on_error(self.__queue_item, str(e))\n            except Exception as e:\n                print(e)\n\n        for new_request in new_requests:\n            new_request.parent_url = self.__queue_item.request.url\n\n        try:\n            self.__options.callbacks.request_in_thread_after_finish(self.__queue_item)\n        except Exception as e:\n            print(e)\n\n        with self.__callback_lock:\n            self.__callback(self.__queue_item, new_requests, failed)", "response": "Executes the HTTP call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the crawler using the given request.", "response": "def start_with(self, request):\n        \"\"\"Start the crawler using the given request.\n\n        Args:\n            request (:class:`nyawc.http.Request`): The startpoint for the crawler.\n\n        \"\"\"\n\n        HTTPRequestHelper.patch_with_options(request, self.__options)\n        self.queue.add_request(request)\n\n        self.__crawler_start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __spawn_new_requests(self):\n\n        self.__should_spawn_new_requests = False\n\n        in_progress_count = len(self.queue.get_all(QueueItem.STATUS_IN_PROGRESS))\n\n        while in_progress_count < self.__options.performance.max_threads:\n            if self.__spawn_new_request():\n                in_progress_count += 1\n            else:\n                break\n\n        if in_progress_count == 0:\n            self.__crawler_stop()", "response": "Spawn new requests until the max threads option value is reached."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __spawn_new_request(self):\n\n        first_in_line = self.queue.get_first(QueueItem.STATUS_QUEUED)\n        \n        if first_in_line is None:\n            return False\n\n        while self.routing.is_treshold_reached(first_in_line.request):\n            self.queue.move(first_in_line, QueueItem.STATUS_CANCELLED)\n\n            first_in_line = self.queue.get_first(QueueItem.STATUS_QUEUED)\n            if first_in_line is None:\n                return False\n\n        self.__request_start(first_in_line)\n        return True", "response": "Spawn the first queued request if there is one available."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __crawler_start(self):\n\n        try:\n            self.__options.callbacks.crawler_before_start()\n        except Exception as e:\n            print(e)\n            print(traceback.format_exc())\n\n        self.__spawn_new_requests()\n\n        while not self.__stopped:\n            if self.__should_stop:\n                self.__crawler_stop()\n\n            if self.__should_spawn_new_requests:\n                self.__spawn_new_requests()\n\n            time.sleep(0.1)", "response": "Spawn new requests in the main thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __crawler_stop(self):\n\n        if self.__stopping:\n            return\n\n        self.__stopping = True\n        self.__wait_for_current_threads()\n\n        self.queue.move_bulk([\n            QueueItem.STATUS_QUEUED,\n            QueueItem.STATUS_IN_PROGRESS\n        ], QueueItem.STATUS_CANCELLED)\n\n        self.__crawler_finish()\n        self.__stopped = True", "response": "Mark the crawler as stopped."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when the crawler is finished.", "response": "def __crawler_finish(self):\n        \"\"\"Called when the crawler is finished because there are no queued requests left or it was stopped.\"\"\"\n\n        try:\n            self.__options.callbacks.crawler_after_finish(self.queue)\n        except Exception as e:\n            print(e)\n            print(traceback.format_exc())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __request_start(self, queue_item):\n\n        try:\n            action = self.__options.callbacks.request_before_start(self.queue, queue_item)\n        except Exception as e:\n            action = None\n            print(e)\n            print(traceback.format_exc())\n\n        if action == CrawlerActions.DO_STOP_CRAWLING:\n            self.__should_stop = True\n\n        if action == CrawlerActions.DO_SKIP_TO_NEXT:\n            self.queue.move(queue_item, QueueItem.STATUS_FINISHED)\n            self.__should_spawn_new_requests = True\n\n        if action == CrawlerActions.DO_CONTINUE_CRAWLING or action is None:\n            self.queue.move(queue_item, QueueItem.STATUS_IN_PROGRESS)\n\n            thread = CrawlerThread(self.__request_finish, self.__lock, self.__options, queue_item)\n            self.__threads[queue_item.get_hash()] = thread\n            thread.daemon = True\n            thread.start()", "response": "Execute the request in given queue item."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __request_finish(self, queue_item, new_requests, request_failed=False):\n\n        if self.__stopping:\n            return\n\n        del self.__threads[queue_item.get_hash()]\n\n        if request_failed:\n            new_queue_items = []\n            self.queue.move(queue_item, QueueItem.STATUS_ERRORED)\n        else:\n            self.routing.increase_route_count(queue_item.request)\n            new_queue_items = self.__add_scraped_requests_to_queue(queue_item, new_requests)\n            self.queue.move(queue_item, QueueItem.STATUS_FINISHED)\n\n        try:\n            action = self.__options.callbacks.request_after_finish(self.queue, queue_item, new_queue_items)\n        except Exception as e:\n            action = None\n            print(e)\n            print(traceback.format_exc())\n        \n        queue_item.decompose()\n\n        if action == CrawlerActions.DO_STOP_CRAWLING:\n            self.__should_stop = True\n\n        if action == CrawlerActions.DO_CONTINUE_CRAWLING or action is None:\n            self.__should_spawn_new_requests = True", "response": "Called when the crawler finished the given queue item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the scraped requests to queue items and also add them to the queue.", "response": "def __add_scraped_requests_to_queue(self, queue_item, scraped_requests):\n        \"\"\"Convert the scraped requests to queue items, return them and also add them to the queue.\n\n        Args:\n            queue_item (:class:`nyawc.QueueItem`): The request/response pair that finished.\n            new_requests list(:class:`nyawc.http.Request`): All the requests that were found during this request.\n\n        Returns:\n            list(:class:`nyawc.QueueItem`): The new queue items.\n\n        \"\"\"\n\n        new_queue_items = []\n\n        for scraped_request in scraped_requests:\n            HTTPRequestHelper.patch_with_options(scraped_request, self.__options, queue_item)\n\n            if not HTTPRequestHelper.complies_with_scope(queue_item, scraped_request, self.__options.scope):\n                continue\n\n            if self.queue.has_request(scraped_request):\n                continue\n\n            scraped_request.depth = queue_item.request.depth + 1\n            if self.__options.scope.max_depth is not None:\n                if scraped_request.depth > self.__options.scope.max_depth:\n                    continue\n\n            new_queue_item = self.queue.add_request(scraped_request)\n            new_queue_items.append(new_queue_item)\n\n        return new_queue_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all the new requests that were found in the response.", "response": "def get_requests(self):\n        \"\"\"Get all the new requests that were found in the response.\n\n        Returns:\n            list(:class:`nyawc.http.Request`): A list of new requests that were found.\n\n        \"\"\"\n\n        requests = self.derived_get_requests()\n\n        for request in requests:\n            request.url = URLHelper.remove_hash(request.url)\n\n        return requests"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes debug in third party libraries correctly.", "response": "def setup(options):\n        \"\"\"Initialize debug/logging in third party libraries correctly.\n\n        Args:\n            options (:class:`nyawc.Options`): The options to use for the current crawling runtime.\n\n        \"\"\"\n\n        if not options.misc.debug:\n            requests.packages.urllib3.disable_warnings(\n                requests.packages.urllib3.exceptions.InsecureRequestWarning\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef T(t, step=30):\n    if hasattr(t, 'timestamp'):\n        timestamp = t.timestamp()\n    else:\n        # python 2\n        if t.tzinfo is None:\n            timestamp = time.mktime(t.timetuple())\n        else:\n            utc_naive  = t.replace(tzinfo=None) - t.utcoffset()\n            timestamp = (utc_naive - datetime.datetime(1970, 1, 1)).total_seconds()\n\n    return int(timestamp) // step", "response": "The TOTP T value (number of time steps since the epoch)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, train_set, test_set):\n        with tf.Graph().as_default(), tf.Session() as self.tf_session:\n            self.build_model()\n            tf.global_variables_initializer().run()\n            third = self.num_epochs // 3\n\n            for i in range(self.num_epochs):\n                lr_decay = self.lr_decay ** max(i - third, 0.0)\n                self.tf_session.run(\n                    tf.assign(self.lr_var, tf.multiply(self.learning_rate, lr_decay)))\n\n                train_perplexity = self._run_train_step(train_set, 'train')\n                print(\"Epoch: %d Train Perplexity: %.3f\"\n                      % (i + 1, train_perplexity))\n\n            test_perplexity = self._run_train_step(test_set, 'test')\n            print(\"Test Perplexity: %.3f\" % test_perplexity)", "response": "Fit the model to the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a single training step.", "response": "def _run_train_step(self, data, mode='train'):\n        \"\"\"Run a single training step.\n\n        :param data: input data\n        :param mode: 'train' or 'test'.\n        \"\"\"\n        epoch_size = ((len(data) // self.batch_size) - 1) // self.num_steps\n        costs = 0.0\n        iters = 0\n        step = 0\n        state = self._init_state.eval()\n        op = self._train_op if mode == 'train' else tf.no_op()\n\n        for step, (x, y) in enumerate(\n            utilities.seq_data_iterator(\n                data, self.batch_size, self.num_steps)):\n            cost, state, _ = self.tf_session.run(\n                [self.cost, self.final_state, op],\n                {self.input_data: x,\n                 self.input_labels: y,\n                 self._init_state: state})\n\n            costs += cost\n            iters += self.num_steps\n\n        if step % (epoch_size // 10) == 10:\n            print(\"%.3f perplexity\" % (step * 1.0 / epoch_size))\n\n        return np.exp(costs / iters)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the model s computational graph.", "response": "def build_model(self):\n        \"\"\"Build the model's computational graph.\"\"\"\n        with tf.variable_scope(\n                \"model\", reuse=None, initializer=self.initializer):\n            self._create_placeholders()\n            self._create_rnn_cells()\n            self._create_initstate_and_embeddings()\n            self._create_rnn_architecture()\n            self._create_optimizer_node()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_placeholders(self):\n        self.input_data = tf.placeholder(\n            tf.int32, [self.batch_size, self.num_steps])\n        self.input_labels = tf.placeholder(\n            tf.int32, [self.batch_size, self.num_steps])", "response": "Create the computational graph s placeholders."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_rnn_cells(self):\n        lstm_cell = tf.nn.rnn_cell.LSTMCell(\n            self.num_hidden, forget_bias=0.0)\n        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n            lstm_cell, output_keep_prob=self.dropout)\n        self.cell = tf.nn.rnn_cell.MultiRNNCell(\n            [lstm_cell] * self.num_layers)", "response": "Create the LSTM cells."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the initial state for the cell and the data embeddings.", "response": "def _create_initstate_and_embeddings(self):\n        \"\"\"Create the initial state for the cell and the data embeddings.\"\"\"\n        self._init_state = self.cell.zero_state(self.batch_size, tf.float32)\n        embedding = tf.get_variable(\n            \"embedding\", [self.vocab_size, self.num_hidden])\n        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n        self.inputs = tf.nn.dropout(inputs, self.dropout)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_rnn_architecture(self):\n        self.inputs = [tf.squeeze(i, [1]) for i in tf.split(\n            axis=1, num_or_size_splits=self.num_steps, value=self.inputs)]\n        outputs, state = tf.nn.rnn(\n            self.cell, self.inputs, initial_state=self._init_state)\n\n        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.num_hidden])\n        softmax_w = tf.get_variable(\n            \"softmax_w\", [self.num_hidden, self.vocab_size])\n        softmax_b = tf.get_variable(\"softmax_b\", [self.vocab_size])\n        logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n        loss = tf.nn.seq2seq.sequence_loss_by_example(\n            [logits],\n            [tf.reshape(self.input_labels, [-1])],\n            [tf.ones([self.batch_size * self.num_steps])])\n\n        self.cost = tf.div(tf.reduce_sum(loss), self.batch_size)\n        self.final_state = state", "response": "Create the training architecture and the last layer of the LSTM."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the optimizer node of the graph.", "response": "def _create_optimizer_node(self):\n        \"\"\"Create the optimizer node of the graph.\"\"\"\n        self.lr_var = tf.Variable(0.0, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n                                          self.max_grad_norm)\n        optimizer = tf.train.GradientDescentOptimizer(self.lr_var)\n        self._train_op = optimizer.apply_gradients(zip(grads, tvars))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_placeholders(self, n_features, n_classes):\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name='x-input')\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name='y-input')\n        self.keep_prob = tf.placeholder(\n            tf.float32, name='keep-probs')", "response": "Create the TensorFlow placeholders for the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_variables(self, n_features):\n        if self.do_pretrain:\n            self._create_variables_pretrain()\n        else:\n            self._create_variables_no_pretrain(n_features)", "response": "Create the TensorFlow variables for the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate model variables (no previous unsupervised pretraining.", "response": "def _create_variables_no_pretrain(self, n_features):\n        \"\"\"Create model variables (no previous unsupervised pretraining).\n\n        :param n_features: number of features\n        :return: self\n        \"\"\"\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            if l == 0:\n                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n                    shape=[n_features, self.layers[l]], stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n                    [self.layers[l]], stddev=0.1)))\n            else:\n                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n                    shape=[self.layers[l - 1], self.layers[l]], stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n                    [self.layers[l]], stddev=0.1)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_variables_pretrain(self):\n        for l, layer in enumerate(self.layers):\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name='enc-w-{}'.format(l))\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name='enc-b-{}'.format(l))", "response": "Create model variables (previous unsupervised pretraining."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the encoding layers for supervised finetuning.", "response": "def _create_encoding_layers(self):\n        \"\"\"Create the encoding layers for supervised finetuning.\n\n        :return: output of the final encoding layer.\n        \"\"\"\n        next_train = self.input_data\n        self.layer_nodes = []\n\n        for l, layer in enumerate(self.layers):\n\n            with tf.name_scope(\"encode-{}\".format(l)):\n\n                y_act = tf.add(\n                    tf.matmul(next_train, self.encoding_w_[l]),\n                    self.encoding_b_[l]\n                )\n\n                if self.finetune_enc_act_func[l] is not None:\n                    layer_y = self.finetune_enc_act_func[l](y_act)\n\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_train)\n\n        self.encode = next_train"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_decoding_layers(self):\n        next_decode = self.encode\n\n        for l, layer in reversed(list(enumerate(self.layers))):\n\n            with tf.name_scope(\"decode-{}\".format(l)):\n\n                # Create decoding variables\n                if self.tied_weights:\n                    dec_w = tf.transpose(self.encoding_w_[l])\n                else:\n                    dec_w = tf.Variable(tf.transpose(\n                        self.encoding_w_[l].initialized_value()))\n\n                dec_b = tf.Variable(tf.constant(\n                    0.1, shape=[dec_w.get_shape().dims[1].value]))\n                self.decoding_w.append(dec_w)\n                self.decoding_b.append(dec_b)\n\n                y_act = tf.add(\n                    tf.matmul(next_decode, dec_w),\n                    dec_b\n                )\n\n                if self.finetune_dec_act_func[l] is not None:\n                    layer_y = self.finetune_dec_act_func[l](y_act)\n\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_decode = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_decode)\n\n        self.reconstruction = next_decode", "response": "Create the decoding layers for reconstruction finetuning."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_mnist_dataset(mode='supervised', one_hot=True):\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=one_hot)\n\n    # Training set\n    trX = mnist.train.images\n    trY = mnist.train.labels\n\n    # Validation set\n    vlX = mnist.validation.images\n    vlY = mnist.validation.labels\n\n    # Test set\n    teX = mnist.test.images\n    teY = mnist.test.labels\n\n    if mode == 'supervised':\n        return trX, trY, vlX, vlY, teX, teY\n\n    elif mode == 'unsupervised':\n        return trX, vlX, teX", "response": "Load the MNIST handwritten digits dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_cifar10_dataset(cifar_dir, mode='supervised'):\n    # Training set\n    trX = None\n    trY = np.array([])\n\n    # Test set\n    teX = np.array([])\n    teY = np.array([])\n\n    for fn in os.listdir(cifar_dir):\n\n        if not fn.startswith('batches') and not fn.startswith('readme'):\n            fo = open(os.path.join(cifar_dir, fn), 'rb')\n            data_batch = pickle.load(fo)\n            fo.close()\n\n            if fn.startswith('data'):\n\n                if trX is None:\n                    trX = data_batch['data']\n                    trY = data_batch['labels']\n                else:\n                    trX = np.concatenate((trX, data_batch['data']), axis=0)\n                    trY = np.concatenate((trY, data_batch['labels']), axis=0)\n\n            if fn.startswith('test'):\n                teX = data_batch['data']\n                teY = data_batch['labels']\n\n    trX = trX.astype(np.float32) / 255.\n    teX = teX.astype(np.float32) / 255.\n\n    if mode == 'supervised':\n        return trX, trY, teX, teY\n\n    elif mode == 'unsupervised':\n        return trX, teX", "response": "Load the cifar10 dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linear(prev_layer, out_dim, name=\"linear\"):\n        with tf.name_scope(name):\n            in_dim = prev_layer.get_shape()[1].value\n            W = tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=0.1))\n            b = tf.Variable(tf.constant(0.1, shape=[out_dim]))\n            out = tf.add(tf.matmul(prev_layer, W), b)\n            return (out, W, b)", "response": "Create a linear fully - connected layer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the regularization tensor.", "response": "def regularization(variables, regtype, regcoef, name=\"regularization\"):\n        \"\"\"Compute the regularization tensor.\n\n        Parameters\n        ----------\n\n        variables : list of tf.Variable\n            List of model variables.\n\n        regtype : str\n            Type of regularization. Can be [\"none\", \"l1\", \"l2\"]\n\n        regcoef : float,\n            Regularization coefficient.\n\n        name : str, optional (default = \"regularization\")\n            Name for the regularization op.\n\n        Returns\n        -------\n\n        tf.Tensor : Regularization tensor.\n        \"\"\"\n        with tf.name_scope(name):\n            if regtype != 'none':\n                regs = tf.constant(0.0)\n                for v in variables:\n                    if regtype == 'l2':\n                        regs = tf.add(regs, tf.nn.l2_loss(v))\n                    elif regtype == 'l1':\n                        regs = tf.add(regs, tf.reduce_sum(tf.abs(v)))\n\n                return tf.multiply(regcoef, regs)\n            else:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accuracy(mod_y, ref_y, summary=True, name=\"accuracy\"):\n        with tf.name_scope(name):\n            mod_pred = tf.argmax(mod_y, 1)\n            correct_pred = tf.equal(mod_pred, tf.argmax(ref_y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n            if summary:\n                tf.summary.scalar('accuracy', accuracy)\n            return accuracy", "response": "Accuracy computation op.\n\n        Parameters\n        ----------\n\n        mod_y : tf.Tensor\n            Model output tensor.\n\n        ref_y : tf.Tensor\n            Reference input tensor.\n\n        summary : bool, optional (default = True)\n            Whether to save tf summary for the op.\n\n        Returns\n        -------\n\n        tf.Tensor : accuracy op. tensor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_model(self, n_features, n_classes):\n        self._create_placeholders(n_features, n_classes)\n        self._create_variables(n_features, n_classes)\n\n        self.mod_y = tf.nn.softmax(\n            tf.add(tf.matmul(self.input_data, self.W_), self.b_))\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = tf.train.GradientDescentOptimizer(\n            self.learning_rate).minimize(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)", "response": "Create the computational graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the TensorFlow variables for the model.", "response": "def _create_variables(self, n_features, n_classes):\n        \"\"\"Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :param n_classes: number of classes\n        :return: self\n        \"\"\"\n        self.W_ = tf.Variable(\n            tf.zeros([n_features, n_classes]), name='weights')\n        self.b_ = tf.Variable(\n            tf.zeros([n_classes]), name='biases')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pretrain_procedure(self, layer_objs, layer_graphs, set_params_func,\n                           train_set, validation_set=None):\n        \"\"\"Perform unsupervised pretraining of the model.\n\n        :param layer_objs: list of model objects (autoencoders or rbms)\n        :param layer_graphs: list of model tf.Graph objects\n        :param set_params_func: function used to set the parameters after\n            pretraining\n        :param train_set: training set\n        :param validation_set: validation set\n        :return: return data encoded by the last layer\n        \"\"\"\n        next_train = train_set\n        next_valid = validation_set\n\n        for l, layer_obj in enumerate(layer_objs):\n            print('Training layer {}...'.format(l + 1))\n            next_train, next_valid = self._pretrain_layer_and_gen_feed(\n                layer_obj, set_params_func, next_train, next_valid,\n                layer_graphs[l])\n\n        return next_train, next_valid", "response": "Perform unsupervised pretraining of the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pretrain_layer_and_gen_feed(self, layer_obj, set_params_func,\n                                     train_set, validation_set, graph):\n        \"\"\"Pretrain a single autoencoder and encode the data for the next layer.\n\n        :param layer_obj: layer model\n        :param set_params_func: function used to set the parameters after\n            pretraining\n        :param train_set: training set\n        :param validation_set: validation set\n        :param graph: tf object for the rbm\n        :return: encoded train data, encoded validation data\n        \"\"\"\n        layer_obj.fit(train_set, train_set,\n                      validation_set, validation_set, graph=graph)\n\n        with graph.as_default():\n            set_params_func(layer_obj, graph)\n\n            next_train = layer_obj.transform(train_set, graph=graph)\n            if validation_set is not None:\n                next_valid = layer_obj.transform(validation_set, graph=graph)\n            else:\n                next_valid = None\n\n        return next_train, next_valid", "response": "Pretrain a single autoencoder and encode the data for the next layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_layers_output(self, dataset):\n        layers_out = []\n\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                for l in self.layer_nodes:\n                    layers_out.append(l.eval({self.input_data: dataset,\n                                              self.keep_prob: 1}))\n\n        if layers_out == []:\n            raise Exception(\"This method is not implemented for this model\")\n        else:\n            return layers_out", "response": "Get output from each layer of the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the parameters of the model.", "response": "def get_parameters(self, params, graph=None):\n        \"\"\"Get the parameters of the model.\n\n        :param params: dictionary of keys (str names) and values (tensors).\n        :return: evaluated tensors in params\n        \"\"\"\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                out = {}\n                for par in params:\n                    if type(params[par]) == list:\n                        for i, p in enumerate(params[par]):\n                            out[par + '-' + str(i+1)] = p.eval()\n                    else:\n                        out[par] = params[par].eval()\n                return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit(self, train_X, train_Y, val_X=None, val_Y=None, graph=None):\n        if len(train_Y.shape) != 1:\n            num_classes = train_Y.shape[1]\n        else:\n            raise Exception(\"Please convert the labels with one-hot encoding.\")\n\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            # Build model\n            self.build_model(train_X.shape[1], num_classes)\n            with tf.Session() as self.tf_session:\n                # Initialize tf stuff\n                summary_objs = tf_utils.init_tf_ops(self.tf_session)\n                self.tf_merged_summaries = summary_objs[0]\n                self.tf_summary_writer = summary_objs[1]\n                self.tf_saver = summary_objs[2]\n                # Train model\n                self._train_model(train_X, train_Y, val_X, val_Y)\n                # Save model\n                self.tf_saver.save(self.tf_session, self.model_path)", "response": "Fit the model to the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, test_X):\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: test_X,\n                    self.keep_prob: 1\n                }\n                return self.mod_y.eval(feed)", "response": "Predict the labels for the test set."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the mean accuracy over the test set.", "response": "def score(self, test_X, test_Y):\n        \"\"\"Compute the mean accuracy over the test set.\n\n        Parameters\n        ----------\n\n        test_X : array_like, shape (n_samples, n_features)\n            Test data.\n\n        test_Y : array_like, shape (n_samples, n_features)\n            Test labels.\n\n        Returns\n        -------\n\n        float : mean accuracy over the test set\n        \"\"\"\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: test_X,\n                    self.input_labels: test_Y,\n                    self.keep_prob: 1\n                }\n                return self.accuracy.eval(feed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pretrain(self, train_set, validation_set=None):\n        self.do_pretrain = True\n\n        def set_params_func(autoenc, autoencgraph):\n            params = autoenc.get_parameters(graph=autoencgraph)\n            self.encoding_w_.append(params['enc_w'])\n            self.encoding_b_.append(params['enc_b'])\n\n        return SupervisedModel.pretrain_procedure(\n            self, self.autoencoders, self.autoencoder_graphs,\n            set_params_func=set_params_func, train_set=train_set,\n            validation_set=validation_set)", "response": "Perform Unsupervised pretraining of the autoencoder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the computational graph. This method is called by the training code.", "response": "def build_model(self, n_features, n_classes):\n        \"\"\"Create the computational graph.\n\n        This graph is intented to be created for finetuning,\n        i.e. after unsupervised pretraining.\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        \"\"\"\n        self._create_placeholders(n_features, n_classes)\n        self._create_variables(n_features)\n\n        next_train = self._create_encoding_layers()\n        self.mod_y, _, _ = Layers.linear(next_train, n_classes)\n        self.layer_nodes.append(self.mod_y)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_variables_no_pretrain(self, n_features):\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            w_name = 'enc-w-{}'.format(l)\n            b_name = 'enc-b-{}'.format(l)\n\n            if l == 0:\n                w_shape = [n_features, self.layers[l]]\n            else:\n                w_shape = [self.layers[l - 1], self.layers[l]]\n\n            w_init = tf.truncated_normal(shape=w_shape, stddev=0.1)\n            W = tf.Variable(w_init, name=w_name)\n            tf.summary.histogram(w_name, W)\n            self.encoding_w_.append(W)\n\n            b_init = tf.constant(0.1, shape=[self.layers[l]])\n            b = tf.Variable(b_init, name=b_name)\n            tf.summary.histogram(b_name, b)\n            self.encoding_b_.append(b)", "response": "Create model variables for no previous unsupervised pretraining."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing TensorFlow operations. This function initialize the following tensorflow ops: * init variables ops * summary ops * create model saver Parameters ---------- sess : object Tensorflow `Session` object Returns ------- tuple : (summary_merged, summary_writer) * tf merged summaries object * tf summary writer object * tf saver object", "response": "def init_tf_ops(sess):\n    \"\"\"Initialize TensorFlow operations.\n\n    This function initialize the following tensorflow ops:\n        * init variables ops\n        * summary ops\n        * create model saver\n\n    Parameters\n    ----------\n\n    sess : object\n        Tensorflow `Session` object\n\n    Returns\n    -------\n\n    tuple : (summary_merged, summary_writer)\n        * tf merged summaries object\n        * tf summary writer object\n        * tf saver object\n    \"\"\"\n    summary_merged = tf.summary.merge_all()\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n\n    sess.run(init_op)\n\n    # Retrieve run identifier\n    run_id = 0\n    for e in os.listdir(Config().logs_dir):\n        if e[:3] == 'run':\n            r = int(e[3:])\n            if r > run_id:\n                run_id = r\n    run_id += 1\n    run_dir = os.path.join(Config().logs_dir, 'run' + str(run_id))\n    print('Tensorboard logs dir for this run is %s' % (run_dir))\n\n    summary_writer = tf.summary.FileWriter(run_dir, sess.graph)\n\n    return (summary_merged, summary_writer, saver)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the summaries and error computation on the validation set.", "response": "def run_summaries(\n        sess, merged_summaries, summary_writer, epoch, feed, tens):\n    \"\"\"Run the summaries and error computation on the validation set.\n\n    Parameters\n    ----------\n\n    sess : tf.Session\n        Tensorflow session object.\n\n    merged_summaries : tf obj\n        Tensorflow merged summaries obj.\n\n    summary_writer : tf.summary.FileWriter\n        Tensorflow summary writer obj.\n\n    epoch : int\n        Current training epoch.\n\n    feed : dict\n        Validation feed dict.\n\n    tens : tf.Tensor\n        Tensor to display and evaluate during training.\n        Can be self.accuracy for SupervisedModel or self.cost for\n        UnsupervisedModel.\n\n    Returns\n    -------\n\n    err : float, mean error over the validation set.\n    \"\"\"\n    try:\n        result = sess.run([merged_summaries, tens], feed_dict=feed)\n        summary_str = result[0]\n        out = result[1]\n        summary_writer.add_summary(summary_str, epoch)\n    except tf.errors.InvalidArgumentError:\n        out = sess.run(tens, feed_dict=feed)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms Unsupervised pretraining of the DBN.", "response": "def pretrain(self, train_set, validation_set=None):\n        \"\"\"Perform Unsupervised pretraining of the DBN.\"\"\"\n        self.do_pretrain = True\n\n        def set_params_func(rbmmachine, rbmgraph):\n            params = rbmmachine.get_parameters(graph=rbmgraph)\n            self.encoding_w_.append(params['W'])\n            self.encoding_b_.append(params['bh_'])\n\n        return SupervisedModel.pretrain_procedure(\n            self, self.rbms, self.rbm_graphs, set_params_func=set_params_func,\n            train_set=train_set, validation_set=validation_set)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_variables_pretrain(self):\n        for l, layer in enumerate(self.layers):\n\n            w_name = 'enc-w-{}'.format(l)\n            b_name = 'enc-b-{}'.format(l)\n\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name=w_name)\n            tf.summary.histogram(w_name, self.encoding_w_[l])\n\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name=b_name)\n            tf.summary.histogram(b_name, self.encoding_w_[l])", "response": "Create model variables (previous unsupervised pretraining."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraining the model. Parameters ---------- train_X : array_like Training data, shape (num_samples, num_features). train_Y : array_like, optional (default = None) Reference training data, shape (num_samples, num_features). val_X : array_like, optional, default None Validation data, shape (num_val_samples, num_features). val_Y : array_like, optional, default None Reference validation data, shape (num_val_samples, num_features). Returns ------- self : trained model instance", "response": "def _train_model(self, train_X, train_Y=None, val_X=None, val_Y=None):\n        \"\"\"Train the model.\n\n        Parameters\n        ----------\n\n        train_X : array_like\n            Training data, shape (num_samples, num_features).\n\n        train_Y : array_like, optional (default = None)\n            Reference training data, shape (num_samples, num_features).\n\n        val_X : array_like, optional, default None\n            Validation data, shape (num_val_samples, num_features).\n\n        val_Y : array_like, optional, default None\n            Reference validation data, shape (num_val_samples, num_features).\n\n        Returns\n        -------\n\n        self : trained model instance\n        \"\"\"\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n            self._run_train_step(train_X)\n            if val_X is not None:\n                feed = {self.input_data_orig: val_X,\n                        self.input_data: val_X}\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(\"Reconstruction loss: %s\" % (err))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_train_step(self, train_X):\n        x_corrupted = utilities.corrupt_input(\n            train_X, self.tf_session, self.corr_type, self.corr_frac)\n\n        shuff = list(zip(train_X, x_corrupted))\n        np.random.shuffle(shuff)\n\n        batches = [_ for _ in utilities.gen_batches(shuff, self.batch_size)]\n\n        for batch in batches:\n            x_batch, x_corr_batch = zip(*batch)\n            tr_feed = {self.input_data_orig: x_batch,\n                       self.input_data: x_corr_batch}\n            self.tf_session.run(self.train_step, feed_dict=tr_feed)", "response": "Runs a training step."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_model(self, n_features, W_=None, bh_=None, bv_=None):\n        self._create_placeholders(n_features)\n        self._create_variables(n_features, W_, bh_, bv_)\n\n        self._create_encode_layer()\n        self._create_decode_layer()\n\n        variables = [self.W_, self.bh_, self.bv_]\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(\n            self.reconstruction, self.input_data_orig, regterm=regterm)\n        self.train_step = self.trainer.compile(self.cost)", "response": "Create the computational graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the TensorFlow variables for the class.", "response": "def _create_variables(self, n_features, W_=None, bh_=None, bv_=None):\n        \"\"\"Create the TensorFlow variables for the model.\n\n        :return: self\n        \"\"\"\n        if W_:\n            self.W_ = tf.Variable(W_, name='enc-w')\n        else:\n            self.W_ = tf.Variable(\n                tf.truncated_normal(\n                    shape=[n_features, self.n_components], stddev=0.1),\n                name='enc-w')\n\n        if bh_:\n            self.bh_ = tf.Variable(bh_, name='hidden-bias')\n        else:\n            self.bh_ = tf.Variable(tf.constant(\n                0.1, shape=[self.n_components]), name='hidden-bias')\n\n        if bv_:\n            self.bv_ = tf.Variable(bv_, name='visible-bias')\n        else:\n            self.bv_ = tf.Variable(tf.constant(\n                0.1, shape=[n_features]), name='visible-bias')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the encoding layer of the network.", "response": "def _create_encode_layer(self):\n        \"\"\"Create the encoding layer of the network.\n\n        Returns\n        -------\n\n        self\n        \"\"\"\n        with tf.name_scope(\"encoder\"):\n\n            activation = tf.add(\n                tf.matmul(self.input_data, self.W_),\n                self.bh_\n            )\n\n            if self.enc_act_func:\n                self.encode = self.enc_act_func(activation)\n            else:\n                self.encode = activation\n\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the decoding layer of the network.", "response": "def _create_decode_layer(self):\n        \"\"\"Create the decoding layer of the network.\n\n        Returns\n        -------\n\n        self\n        \"\"\"\n        with tf.name_scope(\"decoder\"):\n\n            activation = tf.add(\n                tf.matmul(self.encode, tf.transpose(self.W_)),\n                self.bv_\n            )\n\n            if self.dec_act_func:\n                self.reconstruction = self.dec_act_func(activation)\n            else:\n                self.reconstruction = activation\n\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget samples from a tensor of probabilities.", "response": "def sample_prob(probs, rand):\n    \"\"\"Get samples from a tensor of probabilities.\n\n    :param probs: tensor of probabilities\n    :param rand: tensor (of the same shape as probs) of random values\n    :return: binary sample of probabilities\n    \"\"\"\n    return tf.nn.relu(tf.sign(probs - rand))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncorrupt a fraction of data according to the chosen noise method.", "response": "def corrupt_input(data, sess, corrtype, corrfrac):\n    \"\"\"Corrupt a fraction of data according to the chosen noise method.\n\n    :return: corrupted data\n    \"\"\"\n    corruption_ratio = np.round(corrfrac * data.shape[1]).astype(np.int)\n\n    if corrtype == 'none':\n        return np.copy(data)\n\n    if corrfrac > 0.0:\n        if corrtype == 'masking':\n            return masking_noise(data, sess, corrfrac)\n\n        elif corrtype == 'salt_and_pepper':\n            return salt_and_pepper_noise(data, corruption_ratio)\n    else:\n        return np.copy(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xavier_init(fan_in, fan_out, const=1):\n    low = -const * np.sqrt(6.0 / (fan_in + fan_out))\n    high = const * np.sqrt(6.0 / (fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high)", "response": "Xavier initialization of network weights."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndivide input data into batches.", "response": "def gen_batches(data, batch_size):\n    \"\"\"Divide input data into batches.\n\n    :param data: input data\n    :param batch_size: size of each batch\n    :return: data divided into batches\n    \"\"\"\n    data = np.array(data)\n\n    for i in range(0, data.shape[0], batch_size):\n        yield data[i:i + batch_size]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the vector of labels dataY into one - hot encoded labels", "response": "def to_one_hot(dataY):\n    \"\"\"Convert the vector of labels dataY into one-hot encoding.\n\n    :param dataY: vector of labels\n    :return: one-hot encoded labels\n    \"\"\"\n    nc = 1 + np.max(dataY)\n    onehot = [np.zeros(nc, dtype=np.int8) for _ in dataY]\n    for i, j in enumerate(dataY):\n        onehot[i][j] = 1\n    return onehot"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a matrix of probabilities into binary values.", "response": "def conv2bin(data):\n    \"\"\"Convert a matrix of probabilities into binary values.\n\n    If the matrix has values <= 0 or >= 1, the values are\n    normalized to be in [0, 1].\n\n    :type data: numpy array\n    :param data: input matrix\n    :return: converted binary matrix\n    \"\"\"\n    if data.min() < 0 or data.max() > 1:\n        data = normalize(data)\n\n    out_data = data.copy()\n\n    for i, sample in enumerate(out_data):\n\n        for j, val in enumerate(sample):\n\n            if np.random.random() <= val:\n                out_data[i][j] = 1\n            else:\n                out_data[i][j] = 0\n\n    return out_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize(data):\n    out_data = data.copy()\n\n    for i, sample in enumerate(out_data):\n        out_data[i] /= sum(out_data[i])\n\n    return out_data", "response": "Normalize the data to be in the [ 0 1 ) range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef masking_noise(data, sess, v):\n    data_noise = data.copy()\n    rand = tf.random_uniform(data.shape)\n    data_noise[sess.run(tf.nn.relu(tf.sign(v - rand))).astype(np.bool)] = 0\n\n    return data_noise", "response": "Apply masking noise to data in X."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply salt and pepper noise to data in a .", "response": "def salt_and_pepper_noise(X, v):\n    \"\"\"Apply salt and pepper noise to data in X.\n\n    In other words a fraction v of elements of X\n    (chosen at random) is set to its maximum or minimum value according to a\n    fair coin flip.\n    If minimum or maximum are not given, the min (max) value in X is taken.\n    :param X: array_like, Input data\n    :param v: int, fraction of elements to distort\n    :return: transformed data\n    \"\"\"\n    X_noise = X.copy()\n    n_features = X.shape[1]\n\n    mn = X.min()\n    mx = X.max()\n\n    for i, sample in enumerate(X):\n        mask = np.random.randint(0, n_features, v)\n\n        for m in mask:\n\n            if np.random.random() < 0.5:\n                X_noise[i][m] = mn\n            else:\n                X_noise[i][m] = mx\n\n    return X_noise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexpand the given lists into the length of the layers.", "response": "def expand_args(**args_to_expand):\n    \"\"\"Expand the given lists into the length of the layers.\n\n    This is used as a convenience so that the user does not need to specify the\n    complete list of parameters for model initialization.\n    IE the user can just specify one parameter and this function will expand it\n    \"\"\"\n    layers = args_to_expand['layers']\n    try:\n        items = args_to_expand.iteritems()\n    except AttributeError:\n        items = args_to_expand.items()\n\n    for key, val in items:\n        if isinstance(val, list) and len(val) != len(layers):\n            args_to_expand[key] = [val[0] for _ in layers]\n\n    return args_to_expand"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flag_to_list(flagval, flagtype):\n    if flagtype == 'int':\n        return [int(_) for _ in flagval.split(',') if _]\n\n    elif flagtype == 'float':\n        return [float(_) for _ in flagval.split(',') if _]\n\n    elif flagtype == 'str':\n        return [_ for _ in flagval.split(',') if _]\n\n    else:\n        raise Exception(\"incorrect type\")", "response": "Convert a string of comma - separated tf flags to a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts activation function name to tf function.", "response": "def str2actfunc(act_func):\n    \"\"\"Convert activation function name to tf function.\"\"\"\n    if act_func == 'sigmoid':\n        return tf.nn.sigmoid\n\n    elif act_func == 'tanh':\n        return tf.nn.tanh\n\n    elif act_func == 'relu':\n        return tf.nn.relu"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef random_seed_np_tf(seed):\n    if seed >= 0:\n        np.random.seed(seed)\n        tf.set_random_seed(seed)\n        return True\n    else:\n        return False", "response": "Seed numpy and tensorflow random number generators."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave an image with the given parameters.", "response": "def gen_image(img, width, height, outfile, img_type='grey'):\n    \"\"\"Save an image with the given parameters.\"\"\"\n    assert len(img) == width * height or len(img) == width * height * 3\n\n    if img_type == 'grey':\n        misc.imsave(outfile, img.reshape(width, height))\n\n    elif img_type == 'color':\n        misc.imsave(outfile, img.reshape(3, width, height))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_weights_as_images(weights_npy, width, height, outdir='img/',\n                          n_images=10, img_type='grey'):\n    \"\"\"Create and save the weights of the hidden units as images.\n    :param weights_npy: path to the weights .npy file\n    :param width: width of the images\n    :param height: height of the images\n    :param outdir: output directory\n    :param n_images: number of images to generate\n    :param img_type: 'grey' or 'color' (RGB)\n    \"\"\"\n    weights = np.load(weights_npy)\n    perm = np.random.permutation(weights.shape[1])[:n_images]\n\n    for p in perm:\n        w = np.array([i[p] for i in weights])\n        image_path = outdir + 'w_{}.png'.format(p)\n        gen_image(w, width, height, image_path, img_type)", "response": "Create and save the weights of the hidden units as images."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrain the model. :param train_set: training set :param train_labels: training labels :param validation_set: validation set :param validation_labels: validation labels :return: self", "response": "def _train_model(self, train_set, train_labels,\n                     validation_set, validation_labels):\n        \"\"\"Train the model.\n\n        :param train_set: training set\n        :param train_labels: training labels\n        :param validation_set: validation set\n        :param validation_labels: validation labels\n        :return: self\n        \"\"\"\n        shuff = list(zip(train_set, train_labels))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(list(shuff))\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch,\n                               self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_labels,\n                        self.keep_prob: 1}\n                acc = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.accuracy)\n                pbar.set_description(\"Accuracy: %s\" % (acc))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the computational graph of the model.", "response": "def build_model(self, n_features, n_classes):\n        \"\"\"Create the computational graph of the model.\n\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        \"\"\"\n        self._create_placeholders(n_features, n_classes)\n        self._create_layers(n_classes)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the layers of the model from self. layers.", "response": "def _create_layers(self, n_classes):\n        \"\"\"Create the layers of the model from self.layers.\n\n        :param n_classes: number of classes\n        :return: self\n        \"\"\"\n        next_layer_feed = tf.reshape(self.input_data,\n                                     [-1, self.original_shape[0],\n                                      self.original_shape[1],\n                                      self.original_shape[2]])\n        prev_output_dim = self.original_shape[2]\n        # this flags indicates whether we are building the first dense layer\n        first_full = True\n\n        self.W_vars = []\n        self.B_vars = []\n\n        for i, l in enumerate(self.layers.split(',')):\n\n            node = l.split('-')\n            node_type = node[0]\n\n            if node_type == 'conv2d':\n\n                # ################### #\n                # Convolutional Layer #\n                # ################### #\n\n                # fx, fy = shape of the convolutional filter\n                # feature_maps = number of output dimensions\n                fx, fy, feature_maps, stride = int(node[1]),\\\n                     int(node[2]), int(node[3]), int(node[4])\n\n                print('Building Convolutional layer with %d input channels\\\n                      and %d %dx%d filters with stride %d' %\n                      (prev_output_dim, feature_maps, fx, fy, stride))\n\n                # Create weights and biases\n                W_conv = self.weight_variable(\n                    [fx, fy, prev_output_dim, feature_maps])\n                b_conv = self.bias_variable([feature_maps])\n                self.W_vars.append(W_conv)\n                self.B_vars.append(b_conv)\n\n                # Convolution and Activation function\n                h_conv = tf.nn.relu(\n                    self.conv2d(next_layer_feed, W_conv, stride) + b_conv)\n\n                # keep track of the number of output dims of the previous layer\n                prev_output_dim = feature_maps\n                # output node of the last layer\n                next_layer_feed = h_conv\n\n            elif node_type == 'maxpool':\n\n                # ################# #\n                # Max Pooling Layer #\n                # ################# #\n\n                ksize = int(node[1])\n\n                print('Building Max Pooling layer with size %d' % ksize)\n\n                next_layer_feed = self.max_pool(next_layer_feed, ksize)\n\n            elif node_type == 'full':\n\n                # ####################### #\n                # Densely Connected Layer #\n                # ####################### #\n\n                if first_full:  # first fully connected layer\n\n                    dim = int(node[1])\n                    shp = next_layer_feed.get_shape()\n                    tmpx = shp[1].value\n                    tmpy = shp[2].value\n                    fanin = tmpx * tmpy * prev_output_dim\n\n                    print('Building fully connected layer with %d in units\\\n                          and %d out units' % (fanin, dim))\n\n                    W_fc = self.weight_variable([fanin, dim])\n                    b_fc = self.bias_variable([dim])\n                    self.W_vars.append(W_fc)\n                    self.B_vars.append(b_fc)\n\n                    h_pool_flat = tf.reshape(next_layer_feed, [-1, fanin])\n                    h_fc = tf.nn.relu(tf.add(\n                        tf.matmul(h_pool_flat, W_fc),\n                        b_fc))\n                    h_fc_drop = tf.nn.dropout(h_fc, self.keep_prob)\n\n                    prev_output_dim = dim\n                    next_layer_feed = h_fc_drop\n\n                    first_full = False\n\n                else:  # not first fully connected layer\n\n                    dim = int(node[1])\n                    W_fc = self.weight_variable([prev_output_dim, dim])\n                    b_fc = self.bias_variable([dim])\n                    self.W_vars.append(W_fc)\n                    self.B_vars.append(b_fc)\n\n                    h_fc = tf.nn.relu(tf.add(\n                        tf.matmul(next_layer_feed, W_fc), b_fc))\n                    h_fc_drop = tf.nn.dropout(h_fc, self.keep_prob)\n\n                    prev_output_dim = dim\n                    next_layer_feed = h_fc_drop\n\n            elif node_type == 'softmax':\n\n                # ############# #\n                # Softmax Layer #\n                # ############# #\n\n                print('Building softmax layer with %d in units and\\\n                      %d out units' % (prev_output_dim, n_classes))\n\n                W_sm = self.weight_variable([prev_output_dim, n_classes])\n                b_sm = self.bias_variable([n_classes])\n                self.W_vars.append(W_sm)\n                self.B_vars.append(b_sm)\n\n                self.mod_y = tf.add(tf.matmul(next_layer_feed, W_sm), b_sm)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reconstruct(self, data, graph=None):\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {self.input_data: data, self.keep_prob: 1}\n                return self.reconstruction.eval(feed)", "response": "Reconstruct data according to the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the reconstruction loss over the test set.", "response": "def score(self, data, data_ref, graph=None):\n        \"\"\"Compute the reconstruction loss over the test set.\n\n        Parameters\n        ----------\n\n        data : array_like\n            Data to reconstruct.\n\n        data_ref : array_like\n            Reference data.\n\n        Returns\n        -------\n\n        float: Mean error.\n        \"\"\"\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: data,\n                    self.input_labels: data_ref,\n                    self.keep_prob: 1\n                }\n                return self.cost.eval(feed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles the optimizer with the given training parameters.", "response": "def compile(self, cost, name_scope=\"train\"):\n        \"\"\"Compile the optimizer with the given training parameters.\n\n        Parameters\n        ----------\n        cost : Tensor\n            A Tensor containing the value to minimize.\n        name_scope : str , optional (default=\"train\")\n            Optional name scope for the optimizer graph ops.\n        \"\"\"\n        with tf.name_scope(name_scope):\n            return self.opt_.minimize(cost)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile(self, mod_y, ref_y, regterm=None):\n        with tf.name_scope(self.name):\n            if self.lfunc == 'cross_entropy':\n                clip_inf = tf.clip_by_value(mod_y, 1e-10, float('inf'))\n                clip_sup = tf.clip_by_value(1 - mod_y, 1e-10, float('inf'))\n\n                cost = - tf.reduce_mean(tf.add(\n                        tf.multiply(ref_y, tf.log(clip_inf)),\n                        tf.multiply(tf.subtract(1.0, ref_y), tf.log(clip_sup))))\n\n            elif self.lfunc == 'softmax_cross_entropy':\n                cost = tf.losses.softmax_cross_entropy(ref_y, mod_y)\n\n            elif self.lfunc == 'mse':\n                cost = tf.sqrt(tf.reduce_mean(\n                    tf.square(tf.subtract(ref_y, mod_y))))\n\n            else:\n                cost = None\n\n        if cost is not None:\n            cost = cost + regterm if regterm is not None else cost\n            tf.summary.scalar(self.lfunc, cost)\n        else:\n            cost = None\n\n        return cost", "response": "Compute the loss function tensor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_model(self, n_features, encoding_w=None, encoding_b=None):\n        self._create_placeholders(n_features, n_features)\n\n        if encoding_w and encoding_b:\n            self.encoding_w_ = encoding_w\n            self.encoding_b_ = encoding_b\n        else:\n            self._create_variables(n_features)\n\n        self._create_encoding_layers()\n        self._create_decoding_layers()\n\n        variables = []\n        variables.extend(self.encoding_w_)\n        variables.extend(self.encoding_b_)\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(\n            self.reconstruction, self.input_labels, regterm=regterm)\n        self.train_step = self.trainer.compile(self.cost)", "response": "Create the computational graph for the reconstruction task."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrain the model. :param train_set: training set :param validation_set: validation set. optional, default None :return: self", "response": "def _train_model(self, train_set, train_ref=None, validation_set=None,\n                      Validation_ref=None):\n        \"\"\"Train the model.\n\n        :param train_set: training set\n        :param validation_set: validation set. optional, default None\n        :return: self\n        \"\"\"\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n            self._run_train_step(train_set)\n\n            if validation_set is not None:\n                feed = self._create_feed_dict(validation_set)\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(\"Reconstruction loss: %s\" % (err))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_train_step(self, train_set):\n        np.random.shuffle(train_set)\n\n        batches = [_ for _ in utilities.gen_batches(train_set,\n                                                    self.batch_size)]\n        updates = [self.w_upd8, self.bh_upd8, self.bv_upd8]\n\n        for batch in batches:\n            self.tf_session.run(updates,\n                                feed_dict=self._create_feed_dict(batch))", "response": "Runs a training step."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the dictionary of data to feed to tf session during training.", "response": "def _create_feed_dict(self, data):\n        \"\"\"Create the dictionary of data to feed to tf session during training.\n\n        :param data: training/validation set batch\n        :return: dictionary(self.input_data: data, self.hrand: random_uniform,\n                            self.vrand: random_uniform)\n        \"\"\"\n        return {\n            self.input_data: data,\n            self.hrand: np.random.rand(data.shape[0], self.num_hidden),\n            self.vrand: np.random.rand(data.shape[0], data.shape[1])\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the Restricted Boltzmann Machine model in TensorFlow.", "response": "def build_model(self, n_features, regtype='none'):\n        \"\"\"Build the Restricted Boltzmann Machine model in TensorFlow.\n\n        :param n_features: number of features\n        :param regtype: regularization type\n        :return: self\n        \"\"\"\n        self._create_placeholders(n_features)\n        self._create_variables(n_features)\n        self.encode = self.sample_hidden_from_visible(self.input_data)[0]\n        self.reconstruction = self.sample_visible_from_hidden(\n            self.encode, n_features)\n\n        hprob0, hstate0, vprob, hprob1, hstate1 = self.gibbs_sampling_step(\n            self.input_data, n_features)\n        positive = self.compute_positive_association(self.input_data,\n                                                     hprob0, hstate0)\n\n        nn_input = vprob\n\n        for step in range(self.gibbs_sampling_steps - 1):\n            hprob, hstate, vprob, hprob1, hstate1 = self.gibbs_sampling_step(\n                nn_input, n_features)\n            nn_input = vprob\n\n        negative = tf.matmul(tf.transpose(vprob), hprob1)\n\n        self.w_upd8 = self.W.assign_add(\n            self.learning_rate * (positive - negative) / self.batch_size)\n\n        self.bh_upd8 = self.bh_.assign_add(tf.multiply(self.learning_rate, tf.reduce_mean(\n            tf.subtract(hprob0, hprob1), 0)))\n\n        self.bv_upd8 = self.bv_.assign_add(tf.multiply(self.learning_rate, tf.reduce_mean(\n            tf.subtract(self.input_data, vprob), 0)))\n\n        variables = [self.W, self.bh_, self.bv_]\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(vprob, self.input_data, regterm=regterm)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_placeholders(self, n_features):\n        self.input_data = tf.placeholder(tf.float32, [None, n_features],\n                                         name='x-input')\n        self.hrand = tf.placeholder(tf.float32, [None, self.num_hidden],\n                                    name='hrand')\n        self.vrand = tf.placeholder(tf.float32, [None, n_features],\n                                    name='vrand')\n        # not used in this model, created just to comply with\n        # unsupervised_model.py\n        self.input_labels = tf.placeholder(tf.float32)\n        self.keep_prob = tf.placeholder(tf.float32, name='keep-probs')", "response": "Create the TensorFlow placeholders for the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the TensorFlow variables for the class.", "response": "def _create_variables(self, n_features):\n        \"\"\"Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        \"\"\"\n        w_name = 'weights'\n        self.W = tf.Variable(tf.truncated_normal(\n            shape=[n_features, self.num_hidden], stddev=0.1), name=w_name)\n        tf.summary.histogram(w_name, self.W)\n\n        bh_name = 'hidden-bias'\n        self.bh_ = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]),\n                               name=bh_name)\n        tf.summary.histogram(bh_name, self.bh_)\n\n        bv_name = 'visible-bias'\n        self.bv_ = tf.Variable(tf.constant(0.1, shape=[n_features]),\n                               name=bv_name)\n        tf.summary.histogram(bv_name, self.bv_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gibbs_sampling_step(self, visible, n_features):\n        hprobs, hstates = self.sample_hidden_from_visible(visible)\n        vprobs = self.sample_visible_from_hidden(hprobs, n_features)\n        hprobs1, hstates1 = self.sample_hidden_from_visible(vprobs)\n\n        return hprobs, hstates, vprobs, hprobs1, hstates1", "response": "Perform one step of gibbs sampling."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_hidden_from_visible(self, visible):\n        hprobs = tf.nn.sigmoid(tf.add(tf.matmul(visible, self.W), self.bh_))\n        hstates = utilities.sample_prob(hprobs, self.hrand)\n\n        return hprobs, hstates", "response": "Sample the hidden units from the visible units."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample the visible units from the hidden units.", "response": "def sample_visible_from_hidden(self, hidden, n_features):\n        \"\"\"Sample the visible units from the hidden units.\n\n        This is the Negative phase of the Contrastive Divergence algorithm.\n        :param hidden: activations of the hidden units\n        :param n_features: number of features\n        :return: visible probabilities\n        \"\"\"\n        visible_activation = tf.add(\n            tf.matmul(hidden, tf.transpose(self.W)),\n            self.bv_\n        )\n\n        if self.visible_unit_type == 'bin':\n            vprobs = tf.nn.sigmoid(visible_activation)\n\n        elif self.visible_unit_type == 'gauss':\n            vprobs = tf.truncated_normal(\n                (1, n_features), mean=visible_activation, stddev=self.stddev)\n\n        else:\n            vprobs = None\n\n        return vprobs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes positive associations between visible and hidden units.", "response": "def compute_positive_association(self, visible,\n                                     hidden_probs, hidden_states):\n        \"\"\"Compute positive associations between visible and hidden units.\n\n        :param visible: visible units\n        :param hidden_probs: hidden units probabilities\n        :param hidden_states: hidden units states\n        :return: positive association = dot(visible.T, hidden)\n        \"\"\"\n        if self.visible_unit_type == 'bin':\n            positive = tf.matmul(tf.transpose(visible), hidden_states)\n\n        elif self.visible_unit_type == 'gauss':\n            positive = tf.matmul(tf.transpose(visible), hidden_probs)\n\n        else:\n            positive = None\n\n        return positive"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a trained model from disk.", "response": "def load_model(self, shape, gibbs_sampling_steps, model_path):\n        \"\"\"Load a trained model from disk.\n\n        The shape of the model (num_visible, num_hidden) and the number\n        of gibbs sampling steps must be known in order to restore the model.\n        :param shape: tuple(num_visible, num_hidden)\n        :param gibbs_sampling_steps:\n        :param model_path:\n        :return: self\n        \"\"\"\n        n_features, self.num_hidden = shape[0], shape[1]\n        self.gibbs_sampling_steps = gibbs_sampling_steps\n\n        self.build_model(n_features)\n\n        init_op = tf.global_variables_initializer()\n        self.tf_saver = tf.train.Saver()\n\n        with tf.Session() as self.tf_session:\n\n            self.tf_session.run(init_op)\n            self.tf_saver.restore(self.tf_session, model_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the model parameters in the form of numpy arrays.", "response": "def get_parameters(self, graph=None):\n        \"\"\"Return the model parameters in the form of numpy arrays.\n\n        :param graph: tf graph object\n        :return: model parameters\n        \"\"\"\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n\n                return {\n                    'W': self.W.eval(),\n                    'bh_': self.bh_.eval(),\n                    'bv_': self.bv_.eval()\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lowest_cost_action(ic, dc, sc, im, dm, sm, cost):\n    best_action = None\n    best_match_count = -1\n    min_cost = min(ic, dc, sc)\n    if min_cost == sc and cost == 0:\n        best_action = EQUAL\n        best_match_count = sm\n    elif min_cost == sc and cost == 1:\n        best_action = REPLACE\n        best_match_count = sm\n    elif min_cost == ic and im > best_match_count:\n        best_action = INSERT\n        best_match_count = im\n    elif min_cost == dc and dm > best_match_count:\n        best_action = DELETE\n        best_match_count = dm\n    return best_action", "response": "Choose the action that results in the lowest cost."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef highest_match_action(ic, dc, sc, im, dm, sm, cost):\n    # pylint: disable=unused-argument\n    best_action = None\n    lowest_cost = float(\"inf\")\n    max_match = max(im, dm, sm)\n    if max_match == sm and cost == 0:\n        best_action = EQUAL\n        lowest_cost = sm\n    elif max_match == sm and cost == 1:\n        best_action = REPLACE\n        lowest_cost = sm\n    elif max_match == im and ic < lowest_cost:\n        best_action = INSERT\n        lowest_cost = ic\n    elif max_match == dm and dc < lowest_cost:\n        best_action = DELETE\n        lowest_cost = dc\n    return best_action", "response": "Choose the action that results in the highest match score."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the edit distance between two sequences.", "response": "def edit_distance(seq1, seq2, action_function=lowest_cost_action, test=operator.eq):\n    \"\"\"Computes the edit distance between the two given sequences.\n    This uses the relatively fast method that only constructs\n    two columns of the 2d array for edits.  This function actually uses four columns\n    because we track the number of matches too.\n    \"\"\"\n    m = len(seq1)\n    n = len(seq2)\n    # Special, easy cases:\n    if seq1 == seq2:\n        return 0, n\n    if m == 0:\n        return n, 0\n    if n == 0:\n        return m, 0\n    v0 = [0] * (n + 1)     # The two 'error' columns\n    v1 = [0] * (n + 1)\n    m0 = [0] * (n + 1)     # The two 'match' columns\n    m1 = [0] * (n + 1)\n    for i in range(1, n + 1):\n        v0[i] = i\n    for i in range(1, m + 1):\n        v1[0] = i\n        for j in range(1, n + 1):\n            cost = 0 if test(seq1[i - 1], seq2[j - 1]) else 1\n            # The costs\n            ins_cost = v1[j - 1] + 1\n            del_cost = v0[j] + 1\n            sub_cost = v0[j - 1] + cost\n            # Match counts\n            ins_match = m1[j - 1]\n            del_match = m0[j]\n            sub_match = m0[j - 1] + int(not cost)\n\n            action = action_function(ins_cost, del_cost, sub_cost, ins_match,\n                                     del_match, sub_match, cost)\n\n            if action in [EQUAL, REPLACE]:\n                v1[j] = sub_cost\n                m1[j] = sub_match\n            elif action == INSERT:\n                v1[j] = ins_cost\n                m1[j] = ins_match\n            elif action == DELETE:\n                v1[j] = del_cost\n                m1[j] = del_match\n            else:\n                raise Exception('Invalid dynamic programming option returned!')\n                # Copy the columns over\n        for i in range(0, n + 1):\n            v0[i] = v1[i]\n            m0[i] = m1[i]\n    return v1[n], m1[n]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edit_distance_backpointer(seq1, seq2, action_function=lowest_cost_action, test=operator.eq):\n    matches = 0\n    # Create a 2d distance array\n    m = len(seq1)\n    n = len(seq2)\n    # distances array:\n    d = [[0 for x in range(n + 1)] for y in range(m + 1)]\n    # backpointer array:\n    bp = [[None for x in range(n + 1)] for y in range(m + 1)]\n    # matches array:\n    matches = [[0 for x in range(n + 1)] for y in range(m + 1)]\n    # source prefixes can be transformed into empty string by\n    # dropping all characters\n    for i in range(1, m + 1):\n        d[i][0] = i\n        bp[i][0] = [DELETE, i - 1, i, 0, 0]\n    # target prefixes can be reached from empty source prefix by inserting\n    # every characters\n    for j in range(1, n + 1):\n        d[0][j] = j\n        bp[0][j] = [INSERT, 0, 0, j - 1, j]\n    # compute the edit distance...\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n\n            cost = 0 if test(seq1[i - 1], seq2[j - 1]) else 1\n            # The costs of each action...\n            ins_cost = d[i][j - 1] + 1       # insertion\n            del_cost = d[i - 1][j] + 1       # deletion\n            sub_cost = d[i - 1][j - 1] + cost  # substitution/match\n\n            # The match scores of each action\n            ins_match = matches[i][j - 1]\n            del_match = matches[i - 1][j]\n            sub_match = matches[i - 1][j - 1] + int(not cost)\n\n            action = action_function(ins_cost, del_cost, sub_cost, ins_match,\n                                     del_match, sub_match, cost)\n            if action == EQUAL:\n                d[i][j] = sub_cost\n                matches[i][j] = sub_match\n                bp[i][j] = [EQUAL, i - 1, i, j - 1, j]\n            elif action == REPLACE:\n                d[i][j] = sub_cost\n                matches[i][j] = sub_match\n                bp[i][j] = [REPLACE, i - 1, i, j - 1, j]\n            elif action == INSERT:\n                d[i][j] = ins_cost\n                matches[i][j] = ins_match\n                bp[i][j] = [INSERT, i - 1, i - 1, j - 1, j]\n            elif action == DELETE:\n                d[i][j] = del_cost\n                matches[i][j] = del_match\n                bp[i][j] = [DELETE, i - 1, i, j - 1, j - 1]\n            else:\n                raise Exception('Invalid dynamic programming action returned!')\n\n    opcodes = get_opcodes_from_bp_table(bp)\n    return d[m][n], matches[m][n], opcodes", "response": "This function returns the edit distance between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_opcodes_from_bp_table(bp):\n    x = len(bp) - 1\n    y = len(bp[0]) - 1\n    opcodes = []\n    while x != 0 or y != 0:\n        this_bp = bp[x][y]\n        opcodes.append(this_bp)\n        if this_bp[0] == EQUAL or this_bp[0] == REPLACE:\n            x = x - 1\n            y = y - 1\n        elif this_bp[0] == INSERT:\n            y = y - 1\n        elif this_bp[0] == DELETE:\n            x = x - 1\n    opcodes.reverse()\n    return opcodes", "response": "Given a 2d list structure collect the opcodes from the best path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n\n    if len(sys.argv) != 3:\n        print('Usage: {} <file1> <file2>'.format(sys.argv[0]))\n        exit(-1)\n    file1 = sys.argv[1]\n    file2 = sys.argv[2]\n\n    with open(file1) as f1, open(file2) as f2:\n        for line1, line2 in zip(f1, f2):\n            print(\"Line 1: {}\".format(line1.strip()))\n            print(\"Line 2: {}\".format(line2.strip()))\n            dist, _, _ = edit_distance_backpointer(line1.split(), line2.split())\n            print('Distance: {}'.format(dist))\n            print('=' * 80)", "response": "Read two files line - by - line and print edit distances between each pair of lines. Will terminate at the end of the shorter of the two files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_seqs(self, a, b):\n        self.set_seq1(a)\n        self.set_seq2(b)\n        self._reset_object()", "response": "Specify two alternative sequences -- reset any cached values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of Opcodes.", "response": "def get_opcodes(self):\n        \"\"\"Returns a list of opcodes.  Opcodes are the same as defined by\n        :py:mod:`difflib`.\"\"\"\n        if not self.opcodes:\n            d, m, opcodes = edit_distance_backpointer(self.seq1, self.seq2,\n                                                      action_function=self.action_function,\n                                                      test=self.test)\n            if self.dist:\n                assert d == self.dist\n            if self._matches:\n                assert m == self._matches\n            self.dist = d\n            self._matches = m\n            self.opcodes = opcodes\n        return self.opcodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nratioing of matches to the average sequence length.", "response": "def ratio(self):\n        \"\"\"Ratio of matches to the average sequence length.\"\"\"\n        return 2.0 * self.matches() / (len(self.seq1) + len(self.seq2))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_distance_fast(self):\n        d, m = edit_distance(self.seq1, self.seq2,\n                             action_function=self.action_function,\n                             test=self.test)\n        if self.dist:\n            assert d == self.dist\n        if self._matches:\n            assert m == self._matches\n        self.dist = d\n        self._matches = m", "response": "Calls edit_distance and asserts that if we already have values for\n        matches and distance that they match."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef async_request(self, reqs:list)->list:\n        result = (response.result() for response in reqs)\n        ret = [r.json() if r.status_code == 200 else None for r in result]\n        return ret", "response": "\u5f02\u6b65\u5e76\u53d1\u8bf7\u6c42\n        :param reqs: \u8bf7\u6c42\u5217\u8868\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_kline(self, symbol, period, size=150, _async=False):\n        params = {'symbol': symbol, 'period': period, 'size': size}\n\n        url = u.MARKET_URL + '/market/history/kline'\n        return http_get_request(url, params, _async=_async)", "response": "Get KLine from KLine"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets last depth of a symbol", "response": "def get_last_depth(self, symbol, _type, _async=False):\n        \"\"\"\n         \u83b7\u53d6marketdepth\n        :param symbol\n        :param type: \u53ef\u9009\u503c\uff1a{ percent10, step0, step1, step2, step3, step4, step5 }\n        :return:\n        \"\"\"\n        params = {'symbol': symbol, 'type': _type}\n\n        url = u.MARKET_URL + '/market/depth'\n        return http_get_request(url, params, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the last ticker for a given symbol", "response": "def get_last_ticker(self, symbol, _async=False):\n        \"\"\"\n        \u83b7\u53d6tradedetail\n        :param symbol\n        :return:\n        \"\"\"\n        params = {'symbol': symbol}\n\n        url = u.MARKET_URL + '/market/trade'\n        return http_get_request(url, params, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the last ticker for a symbol", "response": "def get_ticker(self, symbol, size=1, _async=False):\n        \"\"\"\n        \u83b7\u53d6\u5386\u53f2ticker\n        :param symbol:\n        :param size: \u53ef\u9009[1,2000]\n        :return:\n        \"\"\"\n        params = {'symbol': symbol, 'size': size}\n\n        url = u.MARKET_URL + '/market/history/trade'\n        return http_get_request(url, params, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all 24h kline", "response": "def get_all_last_24h_kline(self, _async=False):\n        \"\"\"\n        \u83b7\u53d6\u6240\u670924\u5c0f\u65f6\u7684\u6982\u51b5\n        :param _async:\n        :return:\n        \"\"\"\n        params = {}\n        url = u.MARKET_URL + '/market/tickers'\n        return http_get_request(url, params, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_symbols(self, site='Pro', _async=False):\n        assert site in ['Pro', 'HADAX']\n        params = {}\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}common/symbols'\n        return api_key_get(params, path, _async=_async)", "response": "Get all the symbols for a given site"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_accounts(self, _async=False):\n        path = '/v1/account/accounts'\n        params = {}\n        return api_key_get(params, path, _async=_async)", "response": "Get all the accounts in the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_balance(self, acc_id=None, site='Pro', _async=False):\n        acc_id = self.acc_id if acc_id is None else acc_id\n        assert site in ['Pro', 'HADAX']\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}account/accounts/{acc_id}/balance'\n        # params = {'account-id': self.acct_id}\n        params = {}\n        return api_key_get(params, path, _async=_async)", "response": "Get the balance of the current account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending order to an account", "response": "def send_order(self, acc_id, amount, symbol, _type, price=0, site='Pro', _async=False):\n        \"\"\"\n        \u521b\u5efa\u5e76\u6267\u884c\u8ba2\u5355\n        :param amount:\n        :param source: \u5982\u679c\u4f7f\u7528\u501f\u8d37\u8d44\u4ea7\u4ea4\u6613\uff0c\u8bf7\u5728\u4e0b\u5355\u63a5\u53e3,\u8bf7\u6c42\u53c2\u6570source\u4e2d\u586b\u5199'margin-api'\n        :param symbol:\n        :param _type: \u53ef\u9009\u503c {buy-market\uff1a\u5e02\u4ef7\u4e70, sell-market\uff1a\u5e02\u4ef7\u5356, buy-limit\uff1a\u9650\u4ef7\u4e70, sell-limit\uff1a\u9650\u4ef7\u5356, buy-ioc\uff1aIOC\u4e70\u5355, sell-ioc\uff1aIOC\u5356\u5355}\n        :param price:\n        :return:\n        \"\"\"\n        assert site in ['Pro', 'HADAX']\n        assert _type in u.ORDER_TYPE\n        params = {\n            'account-id': acc_id,\n            'amount': amount,\n            'symbol': symbol,\n            'type': _type,\n            'source': 'api'\n        }\n        if price:\n            params['price'] = price\n\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}order/orders/place'\n        return api_key_post(params, path, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef batchcancel_orders(self, order_ids: list, _async=False):\n        assert isinstance(order_ids, list)\n        params = {'order-ids': order_ids}\n        path = f'/v1/order/orders/batchcancel'\n        return api_key_post(params, path, _async=_async)", "response": "Cancel all orders in the order_ids list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef batchcancel_openOrders(self, acc_id, symbol=None, side=None, size=None, _async=False):\n\n        params = {}\n        path = '/v1/order/batchCancelOpenOrders'\n        params['account-id'] = acc_id\n        if symbol:\n            params['symbol'] = symbol\n        if side:\n            assert side in ['buy', 'sell']\n            params['side'] = side\n        if size:\n            params['size'] = size\n\n        return api_key_get(params, path, _async=_async)", "response": "Cancel open orders for an account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_openOrders(self, acc_id=None, symbol=None, side=None, size=None, _async=False):\n        params = {}\n        path = '/v1/order/openOrders'\n        if all([acc_id, symbol]):\n            params['account-id'] = acc_id\n            params['symbol'] = symbol\n        if side:\n            assert side in ['buy', 'sell']\n            params['side'] = side\n        if size:\n            params['size'] = size\n\n        return api_key_get(params, path, _async=_async)", "response": "\u67e5\u8be2\u672a\u6210\u4ea4\u8ba2\u5355\n        :param acc_id: \u5e10\u53f7ID\n        :param symbol: \u4ea4\u6613\u5bf9ID\n        :param side: \u4ea4\u6613\u65b9\u5411\uff0c'buy'\u6216\u8005'sell'\n        :param size: \u8bb0\u5f55\u6761\u6570\uff0c\u6700\u5927500\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_orders_info(self,\n                        symbol,\n                        states:list,\n                        types:list=None,\n                        start_date=None,\n                        end_date=None,\n                        _from=None,\n                        direct=None,\n                        size=None,\n                        _async=False):\n        \"\"\"\n        \u67e5\u8be2\u5f53\u524d\u59d4\u6258\u3001\u5386\u53f2\u59d4\u6258\n        :param symbol:\n        :param states: \u53ef\u9009\u503c {pre-submitted \u51c6\u5907\u63d0\u4ea4, submitted \u5df2\u63d0\u4ea4, partial-filled \u90e8\u5206\u6210\u4ea4, partial-canceled \u90e8\u5206\u6210\u4ea4\u64a4\u9500, filled \u5b8c\u5168\u6210\u4ea4, canceled \u5df2\u64a4\u9500}\n        :param types: \u53ef\u9009\u503c {buy-market\uff1a\u5e02\u4ef7\u4e70, sell-market\uff1a\u5e02\u4ef7\u5356, buy-limit\uff1a\u9650\u4ef7\u4e70, sell-limit\uff1a\u9650\u4ef7\u5356}\n        :param start_date:\n        :param end_date:\n        :param _from:\n        :param direct: \u53ef\u9009\u503c{prev \u5411\u524d\uff0cnext \u5411\u540e}\n        :param size:\n        :return:\n        \"\"\"\n        states = ','.join(states)\n        params = {'symbol': symbol, 'states': states}\n\n        if types:\n            params['types'] = ','.join(types)\n        if start_date:\n            sd = parser.parse(start_date).date()\n            params['start-date'] = str(sd)\n        if end_date:\n            ed = parser.parse(end_date).date()\n            params['end-date'] = str(ed)\n        if _from:\n            params['from'] = _from\n        if direct:\n            assert direct in ['prev', 'next']\n            params['direct'] = direct\n        if size:\n            params['size'] = size\n        path = '/v1/order/orders'\n        return api_key_get(params, path, _async=_async)", "response": "Get order info for a given order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef req_withdraw(self, address, amount, currency, fee=0, addr_tag=\"\", _async=False):\n        params = {\n            'address': address,\n            'amount': amount,\n            'currency': currency,\n            'fee': fee,\n            'addr-tag': addr_tag\n        }\n        path = '/v1/dw/withdraw/api/create'\n\n        return api_key_post(params, path, _async=_async)", "response": "Request withdraw a new key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a margin order to the specified account.", "response": "def send_margin_order(self, acc_id, amount, symbol, _type, price=0, _async=False):\n        \"\"\"\n        \u521b\u5efa\u5e76\u6267\u884c\u501f\u8d37\u8ba2\u5355\n        :param amount:\n        :param symbol:\n        :param _type: \u53ef\u9009\u503c {buy-market\uff1a\u5e02\u4ef7\u4e70, sell-market\uff1a\u5e02\u4ef7\u5356, buy-limit\uff1a\u9650\u4ef7\u4e70, sell-limit\uff1a\u9650\u4ef7\u5356}\n        :param price:\n        :return:\n        \"\"\"\n\n        params = {\n            'account-id': acc_id,\n            'amount': amount,\n            'symbol': symbol,\n            'type': _type,\n            'source': 'margin-api'\n        }\n        if price:\n            params['price'] = price\n\n        path = '/v1/order/orders/place'\n        return api_key_post(params, path, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exchange_to_margin(self, symbol, currency, amount, _async=False):\n        params = {'symbol': symbol, 'currency': currency, 'amount': amount}\n\n        path = '/v1/dw/transfer-in/margin'\n        return api_key_post(params, path, _async=_async)", "response": "\u73b0\u8d27\u8d26\u6237\u5212\u5165\u81f3\u501f\u8d37\u8d26\u6237\n        :param amount:\n        :param currency:\n        :param symbol:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_margin_balance(self, symbol=None, _async=False):\n        params = {}\n        path = '/v1/margin/accounts/balance'\n        if symbol:\n            params['symbol'] = symbol\n\n        return api_key_get(params, path, _async=_async)", "response": "Get the margin balance for a symbol"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the current config of an etf", "response": "def get_etf_config(self, etf_name, _async=False):\n        \"\"\"\n        \u67e5\u8be2etf\u7684\u57fa\u672c\u4fe1\u606f\n        :param etf_name:  etf\u57fa\u91d1\u540d\u79f0\n        :param _async:\n        :return:\n        \"\"\"\n        params = {}\n        path = '/etf/swap/config'\n        params['etf_name'] = etf_name\n\n        return api_key_get(params, path,  _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef etf_swap_in(self, etf_name, amount, _async=False):\n\n        params = {}\n        path = '/etf/swap/in'\n        params['etf_name'] = etf_name\n        params['amount'] = amount\n\n        return api_key_post(params, path,  _async=_async)", "response": "\u6362\u5165etf\n        :param etf_name: etf\u57fa\u91d1\u540d\u79f0\n        :param amount:   \u6570\u91cf\n        :param _async:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_etf_records(self, etf_name, offset, limit, _async=False):\n        params = {}\n        path = '/etf/list'\n        params['etf_name'] = etf_name\n        params['offset'] = offset\n        params['limit'] = limit\n\n        return api_key_get(params, path, _async=_async)", "response": "Get all the etags records"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_quotation_kline(self, symbol, period, limit=None, _async=False):\n        params = {}\n        path = '/quotation/market/history/kline'\n        params['symbol'] = symbol\n        params['period'] = period\n        if limit:\n            params['limit'] = limit\n\n        return api_key_get(params, path, _async=_async)", "response": "Get the kline for a given symbol and period"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransfer a user s key", "response": "def transfer(self, sub_uid, currency, amount, transfer_type, _async=False):\n        \"\"\"\n        \u6bcd\u8d26\u6237\u6267\u884c\u5b50\u8d26\u6237\u5212\u8f6c\n        :param sub_uid: \u5b50\u8d26\u6237id\n        :param currency: \u5e01\u79cd\n        :param amount: \u5212\u8f6c\u91d1\u989d\n        :param transfer_type: \u5212\u8f6c\u7c7b\u578b\uff0cmaster-transfer-in\uff08\u5b50\u8d26\u6237\u5212\u8f6c\u7ed9\u6bcd\u8d26\u6237\u865a\u62df\u5e01\uff09/ master-transfer-out \uff08\u6bcd\u8d26\u6237\u5212\u8f6c\u7ed9\u5b50\u8d26\u6237\u865a\u62df\u5e01\uff09/master-point-transfer-in \uff08\u5b50\u8d26\u6237\u5212\u8f6c\u7ed9\u6bcd\u8d26\u6237\u70b9\u5361\uff09/master-point-transfer-out\uff08\u6bcd\u8d26\u6237\u5212\u8f6c\u7ed9\u5b50\u8d26\u6237\u70b9\u5361\uff09\n        :param _async: \u662f\u5426\u5f02\u6b65\u6267\u884c\n        :return:\n        \"\"\"\n        params = {}\n        path = '/v1/subuser/transfer'\n        params['sub-uid'] = sub_uid\n        params['currency'] = currency\n        params['amount'] = amount\n        params['type'] = transfer_type\n\n        return api_key_post(params, path, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_aggregate_balance(self, _async=False):\n        params = {}\n        path = '/v1/subuser/aggregate-balance'\n        return  api_key_get(params, path, _async=_async)", "response": "\u67e5\u8be2\u6240\u6709\u5b50\u8d26\u6237\u6c47\u603b\n        :param _async: \u662f\u5426\u5f02\u6b65\u6267\u884c\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sub_balance(self, sub_id, _async=False):\n\n        params = {}\n        params['sub-uid'] = sub_id\n        path = '/v1/account/accounts/{sub-uid}'\n        return api_key_get(params, path, _async=_async)", "response": "Get the balance of a sub account"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_last_24h_kline(self):\n        params = {}\n        url = u.MARKET_URL + '/market/tickers'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(http_get_request(url, params))\n\n            return handle\n\n        return _wrapper", "response": "\u83b7\u53d6\u6240\u6709ticker\n        :param _async:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_symbols(self, site='Pro'):\n        assert site in ['Pro', 'HADAX']\n        params = {}\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}common/symbols'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_get(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Decorator for get_symbols that returns a dict of all available symbols."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_balance(self, acc_id, site='Pro'):\n        assert site in ['Pro', 'HADAX']\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}account/accounts/{acc_id}/balance'\n        # params = {'account-id': self.acct_id}\n        params = {}\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_get(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Decorator for getting the balance of an account"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend order to user", "response": "def send_order(self, acc_id, amount, symbol, _type, price=0, site='Pro'):\n        \"\"\"\n        \u521b\u5efa\u5e76\u6267\u884c\u8ba2\u5355\n        :param amount:\n        :param source: \u5982\u679c\u4f7f\u7528\u501f\u8d37\u8d44\u4ea7\u4ea4\u6613\uff0c\u8bf7\u5728\u4e0b\u5355\u63a5\u53e3,\u8bf7\u6c42\u53c2\u6570source\u4e2d\u586b\u5199'margin-api'\n        :param symbol:\n        :param _type: \u53ef\u9009\u503c {buy-market\uff1a\u5e02\u4ef7\u4e70, sell-market\uff1a\u5e02\u4ef7\u5356, buy-limit\uff1a\u9650\u4ef7\u4e70, sell-limit\uff1a\u9650\u4ef7\u5356, buy-ioc\uff1aIOC\u4e70\u5355, sell-ioc\uff1aIOC\u5356\u5355}\n        :param price:\n        :return:\n        \"\"\"\n        assert site in ['Pro', 'HADAX']\n        assert _type in u.ORDER_TYPE\n        params = {\n            'account-id': acc_id,\n            'amount': amount,\n            'symbol': symbol,\n            'type': _type,\n            'source': 'api'\n        }\n        if price:\n            params['price'] = price\n\n        path = f'/v1{\"/\" if site == \"Pro\" else \"/hadax/\"}order/orders/place'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_post(params, path))\n\n            return handle\n\n        return _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef batchcancel_order(self, order_ids: list):\n        assert isinstance(order_ids, list)\n        params = {'order-ids': order_ids}\n        path = f'/v1/order/orders/batchcancel'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_post(params, path))\n\n            return handle\n\n        return _wrapper", "response": "A decorator to cancel a batch of order items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_order_matchresults(self, order_id):\n        params = {}\n        path = f'/v1/order/orders/{order_id}/matchresults'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_get(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Decorator for getting order matchresults."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef req_withdraw(self, address, amount, currency, fee=0, addr_tag=\"\"):\n        params = {\n            'address': address,\n            'amount': amount,\n            'currency': currency,\n            'fee': fee,\n            'addr-tag': addr_tag\n        }\n        path = '/v1/dw/withdraw/api/create'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_post(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Wrapper for the API key withdraw"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps for cancel_withdraw API.", "response": "def cancel_withdraw(self, address_id):\n        \"\"\"\n        \u7533\u8bf7\u53d6\u6d88\u63d0\u73b0\u865a\u62df\u5e01\n        :param address_id:\n        :return: {\n                  \"status\": \"ok\",\n                  \"data\": 700\n                }\n        \"\"\"\n        params = {}\n        path = f'/v1/dw/withdraw-virtual/{address_id}/cancel'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_post(params, path))\n\n            return handle\n\n        return _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_margin_order(self, acc_id, amount, symbol, _type, price=0):\n\n        params = {\n            'account-id': acc_id,\n            'amount': amount,\n            'symbol': symbol,\n            'type': _type,\n            'source': 'margin-api'\n        }\n        if price:\n            params['price'] = price\n\n        path = '/v1/order/orders/place'\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_post(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Send a margin order to the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_margin_balance(self, symbol):\n        params = {}\n        path = '/v1/margin/accounts/balance'\n        if symbol:\n            params['symbol'] = symbol\n\n        def _wrapper(_func):\n            @wraps(_func)\n            def handle():\n                _func(api_key_get(params, path))\n\n            return handle\n\n        return _wrapper", "response": "Decorator for getting margin balance for a symbol"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_onRsp(self, req):\n        def wrapper(_callback):\n            callbackList = self._req_callbacks.setdefault(req, [])\n            callbackList.append(_callback)\n            return _callback\n        return wrapper", "response": "\u6dfb\u52a0\u56de\u8c03\u5904\u7406\u51fd\u6570\u7684\u88c5\u9970\u5668\n        :param req: \u5177\u4f53\u7684topic\uff0c\u5982\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unregister_handle_func(self, _handle_func_name, topic):\n        handler_list = self._handle_funcs.get(topic, [])\n        for i, h in enumerate(handler_list):\n            if h is _handle_func_name or h.__name__ == _handle_func_name:\n                handler_list.pop(i)\n\n        if self._handle_funcs.get(topic) == []:\n            self._handle_funcs.pop(topic)", "response": "Unregister a handler function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating signature from params", "response": "def createSign(pParams, method, host_url, request_path, secret_key):\n    \"\"\"\n    from \u706b\u5e01demo, \u6784\u9020\u7b7e\u540d\n    :param pParams:\n    :param method:\n    :param host_url:\n    :param request_path:\n    :param secret_key:\n    :return:\n    \"\"\"\n    sorted_params = sorted(pParams.items(), key=lambda d: d[0], reverse=False)\n    encode_params = urllib.parse.urlencode(sorted_params)\n    payload = [method, host_url, request_path, encode_params]\n    payload = '\\n'.join(payload)\n    payload = payload.encode(encoding='UTF8')\n    secret_key = secret_key.encode(encoding='UTF8')\n\n    digest = hmac.new(secret_key, payload, digestmod=hashlib.sha256).digest()\n    signature = base64.b64encode(digest)\n    signature = signature.decode()\n    return signature"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef http_get_request(url, params, add_to_headers=None, _async=False):\n    headers = {\n        'Content-type':\n        'application/x-www-form-urlencoded',\n        'User-Agent':\n        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',\n    }\n    if add_to_headers:\n        headers.update(add_to_headers)\n    postdata = urllib.parse.urlencode(params)\n    if _async:\n        response = async_session.get(url, params=postdata, headers=headers, timeout=5)\n        return response\n    else:\n        response = requests.get(url, postdata, headers=headers, timeout=5)\n        try:\n            if response.status_code == 200:\n                return response.json()\n            else:\n                logger.debug(\n                    f'<GET>error_code:{response.status_code}  reason:{response.reason} detail:{response.text}')\n                return\n        except BaseException as e:\n            logger.exception(f'<GET>httpGet failed, detail is:{response.text},{e}')\n            return", "response": "from \u706b\u5e01demo, get\u65b9\u6cd5\n    :param url:\n    :param params:\n    :param add_to_headers:\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef api_key_get(params, request_path, _async=False):\n    method = 'GET'\n    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n    params.update({\n        'AccessKeyId': ACCESS_KEY,\n        'SignatureMethod': 'HmacSHA256',\n        'SignatureVersion': '2',\n        'Timestamp': timestamp\n    })\n\n    host_url = TRADE_URL\n    host_name = urllib.parse.urlparse(host_url).hostname\n    host_name = host_name.lower()\n    secret_sign = createSign(params, method, host_name, request_path,\n                                     SECRET_KEY)\n    params['Signature'] = secret_sign\n    if PRIVATE_KEY:\n        params['PrivateSignature'] = createPrivateSign(secret_sign, PRIVATE_KEY)\n    url = host_url + request_path\n    return http_get_request(url, params, _async=_async)", "response": "from \u706b\u5e01demo, \u6784\u9020get\u8bf7\u6c42\u5e76\u8c03\u7528get\u65b9\u6cd5\n    :param params:\n    :param request_path:\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nposts \u8bf7\u6c42\u5e76\u8c03\u7528 post \u65b9\u6cd5", "response": "def api_key_post(params, request_path, _async=False):\n    \"\"\"\n    from \u706b\u5e01demo, \u6784\u9020post\u8bf7\u6c42\u5e76\u8c03\u7528post\u65b9\u6cd5\n    :param params:\n    :param request_path:\n    :return:\n    \"\"\"\n    method = 'POST'\n    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n    params_to_sign = {\n        'AccessKeyId': ACCESS_KEY,\n        'SignatureMethod': 'HmacSHA256',\n        'SignatureVersion': '2',\n        'Timestamp': timestamp\n    }\n\n    host_url = TRADE_URL\n    host_name = urllib.parse.urlparse(host_url).hostname\n    host_name = host_name.lower()\n    secret_sign = createSign(params_to_sign, method, host_name,\n                                             request_path, SECRET_KEY)\n    params_to_sign['Signature'] = secret_sign\n    if PRIVATE_KEY:\n        params_to_sign['PrivateSignature'] = createPrivateSign(secret_sign, PRIVATE_KEY)\n    url = host_url + request_path + '?' + urllib.parse.urlencode(params_to_sign)\n    return http_post_request(url, params, _async=_async)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handler_profiler(filename=None):\n    if filename == None:\n        f = sys.stdout\n    else:\n        f = open(filename, 'w')\n    def _callfunc(handle):\n        @wraps(handle)\n        def func(self, topic, msg):\n            t0 = time.time()\n            handle(self, topic, msg)\n            t1 = time.time()\n            print(f'{self.name}-handle\u8fd0\u884c\u65f6\u95f4:{t1 - t0}s', file=f)\n\n        return func\n    return _callfunc", "response": "A helper function that provides a profiler for the handling of a sequence of messages."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse arguments and return a dict of the current state of the current instance of the class.", "response": "def parse_args(self, *args, **kwargs):\n        \"\"\" Parse arguments \"\"\"\n        args = super(SatUtilsParser, self).parse_args(*args, **kwargs)\n        args = vars(args)\n        args = {k: v for k, v in args.items() if v is not None}\n\n        if args.get('command', None) is None:\n            self.print_help()\n            sys.exit(0)\n\n        # set logging level\n        if 'verbosity' in args:\n            logging.basicConfig(stream=sys.stdout, level=(50-args.pop('verbosity') * 10))\n\n        # set global configuration options\n        if 'url' in args:\n            config.API_URL = args.pop('url')\n        if 'datadir' in args:\n            config.DATADIR = args.pop('datadir')\n        if 'filename' in args:\n            config.FILENAME = args.pop('filename')\n\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a newbie class with all the skills needed.", "response": "def newbie(cls, *args, **kwargs):\n        \"\"\" Create a newbie class, with all the skills needed \"\"\"\n        parser = cls(*args, **kwargs)\n        subparser = parser.add_subparsers(dest='command')\n        parents = [parser.pparser, parser.output_parser]\n\n        sparser = subparser.add_parser('search', help='Perform new search of items', parents=parents)\n        \"\"\" Adds search arguments to a parser \"\"\"\n        parser.search_group = sparser.add_argument_group('search options')\n        parser.search_group.add_argument('-c', '--collection', help='Name of collection', default=None)\n        h = 'One or more scene IDs from provided collection (ignores other parameters)'\n        parser.search_group.add_argument('--ids', help=h, nargs='*', default=None)\n        parser.search_group.add_argument('--bbox', help='Bounding box (min lon, min lat, max lon, max lat)', nargs=4)\n        parser.search_group.add_argument('--intersects', help='GeoJSON Feature (file or string)')\n        parser.search_group.add_argument('--datetime', help='Single date/time or begin and end date/time (e.g., 2017-01-01/2017-02-15)')\n        parser.search_group.add_argument('-p', '--property', nargs='*', help='Properties of form KEY=VALUE (<, >, <=, >=, = supported)')\n        parser.search_group.add_argument('--sort', help='Sort by fields', nargs='*')\n        h = 'Only output how many Items found'\n        parser.search_group.add_argument('--found', help=h, action='store_true', default=False)\n        parser.search_group.add_argument('--url', help='URL of the API', default=config.API_URL)\n\n        parents.append(parser.download_parser)\n        lparser = subparser.add_parser('load', help='Load items from previous search', parents=parents)\n        lparser.add_argument('items', help='GeoJSON file of Items')\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(items=None, printmd=None, printcal=False, found=False,\n         save=None, download=None, requestor_pays=False, **kwargs):\n    \"\"\" Main function for performing a search \"\"\"\n    \n    if items is None:\n        ## if there are no items then perform a search\n        search = Search.search(**kwargs)\n        if found:\n            num = search.found()\n            print('%s items found' % num)\n            return num\n        items = search.items()\n    else:\n        # otherwise, load a search from a file\n        items = Items.load(items)\n\n    print('%s items found' % len(items))\n\n    # print metadata\n    if printmd is not None:\n        print(items.summary(printmd))\n\n    # print calendar\n    if printcal:\n        print(items.calendar())\n\n    # save all metadata in JSON file\n    if save is not None:\n        items.save(filename=save)\n\n    # download files given `download` keys\n    if download is not None:\n        if 'ALL' in download:\n            # get complete set of assets\n            download = set([k for i in items for k in i.assets])\n        for key in download:\n            items.download(key=key, path=config.DATADIR, filename=config.FILENAME, requestor_pays=requestor_pays)\n\n    return items", "response": "Main function for performing a search"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef found(self):\n        if 'ids' in self.kwargs:\n            cid = self.kwargs['query']['collection']['eq']\n            return len(self.items_by_id(self.kwargs['ids'], cid))\n        kwargs = {\n            'page': 1,\n            'limit': 0\n        }\n        kwargs.update(self.kwargs)\n        results = self.query(**kwargs)\n        return results['meta']['found']", "response": "Small query to determine total number of hits"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(cls, url=urljoin(config.API_URL, 'stac/search'), **kwargs):\n        logger.debug('Query URL: %s, Body: %s' % (url, json.dumps(kwargs)))\n        response = requests.post(url, data=json.dumps(kwargs))\n        # API error\n        if response.status_code != 200:\n            raise SatSearchError(response.text)\n        return response.json()", "response": "Query the SatSearch API for the current user s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a Collection record", "response": "def collection(cls, cid):\n        \"\"\" Get a Collection record \"\"\"\n        url = urljoin(config.API_URL, 'collections/%s' % cid)\n        return Collection(cls.query(url=url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns Items from collection with matching ids", "response": "def items_by_id(cls, ids, collection):\n        \"\"\" Return Items from collection with matching ids \"\"\"\n        col = cls.collection(collection)\n        items = []\n        base_url = urljoin(config.API_URL, 'collections/%s/items' % collection)\n        for id in ids:\n            try:\n                items.append(Item(cls.query(urljoin(base_url, id))))\n            except SatSearchError as err:\n                pass\n        return Items(items, collections=[col])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all of the Items and Collections for this search", "response": "def items(self, limit=10000):\n        \"\"\" Return all of the Items and Collections for this search \"\"\"\n        _limit = 500\n        if 'ids' in self.kwargs:\n            col = self.kwargs.get('query', {}).get('collection', {}).get('eq', None)\n            if col is None:\n                raise SatSearchError('Collection required when searching by id')\n            return self.items_by_id(self.kwargs['ids'], col)\n\n        items = []\n        found = self.found()\n        if found > limit:\n            logger.warning('There are more items found (%s) than the limit (%s) provided.' % (found, limit))\n        maxitems = min(found, limit)\n        kwargs = {\n            'page': 1,\n            'limit': min(_limit, maxitems)\n        }\n        kwargs.update(self.kwargs)\n        while len(items) < maxitems:\n            items += [Item(i) for i in self.query(**kwargs)['features']]\n            kwargs['page'] += 1\n\n        # retrieve collections\n        collections = []\n        for c in set([item.properties['collection'] for item in items if 'collection' in item.properties]):\n            collections.append(self.collection(c))\n            #del collections[c]['links']\n\n        # merge collections into items\n        #_items = []\n        #for item in items:\n        #    import pdb; pdb.set_trace()\n        #    if 'collection' in item['properties']:\n        #        item = dict_merge(item, collections[item['properties']['collection']])\n        #    _items.append(Item(item))\n\n        return Items(items, collections=collections, search=self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking shutil. copyfileobj but only copy a range of the streams.", "response": "def copy_byte_range(infile, outfile, start=None, stop=None, bufsize=16*1024):\n    '''Like shutil.copyfileobj, but only copy a range of the streams.\n\n    Both start and stop are inclusive.\n    '''\n    if start is not None: infile.seek(start)\n    while 1:\n        to_read = min(bufsize, stop + 1 - infile.tell() if stop else bufsize)\n        buf = infile.read(to_read)\n        if not buf:\n            break\n        outfile.write(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_byte_range(byte_range):\n    '''Returns the two numbers in 'bytes=123-456' or throws ValueError.\n\n    The last number or both numbers may be None.\n    '''\n    if byte_range.strip() == '':\n        return None, None\n\n    m = BYTE_RANGE_RE.match(byte_range)\n    if not m:\n        raise ValueError('Invalid byte range %s' % byte_range)\n\n    first, last = [x and int(x) for x in m.groups()]\n    if last and last < first:\n        raise ValueError('Invalid byte range %s' % byte_range)\n    return first, last", "response": "Parses a byte range into two numbers in bytes = 123 - 456 and returns the first and last numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_author(package):\n    init_py = readfile(os.path.join(package, '__init__.py'))\n    author = re.search(\"__author__ = u?['\\\"]([^'\\\"]+)['\\\"]\", init_py).group(1)\n    return UltraMagicString(author)", "response": "Return package version as listed in __version__ in init. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_version(package):\n    init_py = readfile(os.path.join(package, '__init__.py'))\n    return re.search(\"__version__ = ['\\\"]([^'\\\"]+)['\\\"]\", init_py).group(1)", "response": "Get package version as listed in __version__ in init. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache_page(*args, **kwargs):\n    '''\n    Same as django's ``cache_page`` decorator, but wraps the view into\n    additional decorators before and after that. Makes it possible to serve multiple\n    flavours without getting into trouble with django's caching that doesn't\n    know about flavours.\n    '''\n    decorator = _django_cache_page(*args, **kwargs)\n    def flavoured_decorator(func):\n        return vary_on_flavour_fetch(decorator(vary_on_flavour_update(func)))\n    return flavoured_decorator", "response": "A decorator that returns a view that caches the flavours of the current page."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefaults RC for the current user.", "response": "def DefaultRC():\n    \"\"\"\n    Check XDG_CONFIG_DIR before ~/.cmakelintrc\n    \"\"\"\n    xdg = os.path.join(os.path.expanduser('~'), '.config')\n    if 'XDG_CONFIG_DIR' in os.environ:\n        xdg = os.environ['XDG_CONFIG_DIR']\n    xdgfile = os.path.join(xdg, 'cmakelintrc')\n    if os.path.exists(xdgfile):\n        return xdgfile\n    return os.path.join(os.path.expanduser('~'), '.cmakelintrc')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclean comments in a line.", "response": "def CleanComments(line, quote=False):\n    \"\"\"\n    quote means 'was in a quote starting this line' so that\n    quoted lines can be eaten/removed.\n    \"\"\"\n    if line.find('#') == -1 and line.find('\"') == -1:\n        if quote:\n            return '', quote\n        else:\n            return line, quote\n    # else have to check for comment\n    prior = []\n    prev = ''\n    for char in line:\n        try:\n            if char == '\"':\n                if prev != '\\\\':\n                    quote = not quote\n                    prior.append(char)\n                continue\n            elif char == '#' and not quote:\n                break\n            if not quote:\n                prior.append(char)\n        finally:\n            prev = char\n\n    # rstrip removes trailing space between end of command and the comment # start\n\n    return ''.join(prior).rstrip(), quote"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CheckLineLength(filename, linenumber, clean_lines, errors):\n    line = clean_lines.raw_lines[linenumber]\n    if len(line) > _lint_state.linelength:\n        return errors(\n                filename,\n                linenumber,\n                'linelength',\n                'Lines should be <= %d characters long' %\n                    (_lint_state.linelength))", "response": "Check if the line is longer than the recommended length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CheckUpperLowerCase(filename, linenumber, clean_lines, errors):\n    line = clean_lines.lines[linenumber]\n    if ContainsCommand(line):\n        command = GetCommand(line)\n        if IsCommandMixedCase(command):\n            return errors(\n                    filename,\n                    linenumber,\n                    'readability/wonkycase',\n                    'Do not use mixed case commands')\n        if clean_lines.have_seen_uppercase is None:\n            clean_lines.have_seen_uppercase = IsCommandUpperCase(command)\n        else:\n            is_upper = IsCommandUpperCase(command)\n            if is_upper != clean_lines.have_seen_uppercase:\n                return errors(\n                        filename,\n                        linenumber,\n                        'readability/mixedcase',\n                        'Do not mix upper and lower case commands')", "response": "Checks that the command is either upper case or lower case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CheckCommandSpaces(filename, linenumber, clean_lines, errors):\n    line = clean_lines.lines[linenumber]\n    match = ContainsCommand(line)\n    if match and len(match.group(2)):\n        errors(filename, linenumber, 'whitespace/extra',\n                \"Extra spaces between '%s' and its ()\"%(match.group(1)))\n    if match:\n        spaces_after_open = len(_RE_COMMAND_START_SPACES.match(line).group(1))\n        initial_spaces = GetInitialSpaces(line)\n        initial_linenumber = linenumber\n        end = None\n        while True:\n            line = clean_lines.lines[linenumber]\n            end = _RE_COMMAND_END_SPACES.search(line)\n            if end:\n                break\n            linenumber += 1\n            if linenumber >= len(clean_lines.lines):\n                break\n        if linenumber == len(clean_lines.lines) and not end:\n            errors(filename, initial_linenumber, 'syntax',\n                    'Unable to find the end of this command')\n        if end:\n            spaces_before_end = len(end.group(1))\n            initial_spaces = GetInitialSpaces(line)\n            if initial_linenumber != linenumber and spaces_before_end >= initial_spaces:\n                spaces_before_end -= initial_spaces\n\n            if spaces_after_open != spaces_before_end:\n                errors(filename, initial_linenumber, 'whitespace/mismatch',\n                        'Mismatching spaces inside () after command')", "response": "Checks if the command spaces are present in the clean_lines."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck for logic inside else endif etc.", "response": "def CheckRepeatLogic(filename, linenumber, clean_lines, errors):\n    \"\"\"\n    Check for logic inside else, endif etc\n    \"\"\"\n    line = clean_lines.lines[linenumber]\n    for cmd in _logic_commands:\n        if re.search(r'\\b%s\\b'%cmd, line.lower()):\n            m = _RE_LOGIC_CHECK.search(line)\n            if m:\n                errors(filename, linenumber, 'readability/logic',\n                        'Expression repeated inside %s; '\n                        'better to use only %s()'%(cmd, m.group(1)))\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the style of a single line.", "response": "def CheckStyle(filename, linenumber, clean_lines, errors):\n    \"\"\"\n    Check style issues. These are:\n    No extra spaces between command and parenthesis\n    Matching spaces between parenthesis and arguments\n    No repeated logic in else(), endif(), endmacro()\n    \"\"\"\n    CheckIndent(filename, linenumber, clean_lines, errors)\n    CheckCommandSpaces(filename, linenumber, clean_lines, errors)\n    line = clean_lines.raw_lines[linenumber]\n    if line.find('\\t') != -1:\n        errors(filename, linenumber, 'whitespace/tabs', 'Tab found; please use spaces')\n\n    if line and line[-1].isspace():\n        errors(filename, linenumber, 'whitespace/eol', 'Line ends in whitespace')\n\n    CheckRepeatLogic(filename, linenumber, clean_lines, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ProcessLine(filename, linenumber, clean_lines, errors):\n    CheckLintPragma(filename, linenumber, clean_lines.raw_lines[linenumber], errors)\n    CheckLineLength(filename, linenumber, clean_lines, errors)\n    CheckUpperLowerCase(filename, linenumber, clean_lines, errors)\n    CheckStyle(filename, linenumber, clean_lines, errors)\n    if IsFindPackage(filename):\n        CheckFindPackage(filename, linenumber, clean_lines, errors)", "response": "Processes a single line of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weron_2002_figure2(n = 10000):\n  # local import to avoid dependency for non-debug use\n  import matplotlib.pyplot as plt\n  # note: these values are calculated by measurements in inkscape of the plot\n  # from the paper\n  reported = [6.708, 13.103, 20.240, 21.924, 22.256, 24.112, 24.054, 26.299, \n              26.897]\n  reported_raw = [160.599, 141.663, 128.454, 115.617, 103.651, 95.481, 86.810,\n                  81.799, 76.270]\n  def height_to_h(height):\n    return 0.49 + height / 29.894 * 0.01\n  reported = height_to_h(np.array(reported))\n  reported_raw = height_to_h(np.array(reported_raw))\n  data = []\n  for e in range(8,17):\n    l = 2**e\n    nvals = 2**np.arange(6,e)\n    rsn = np.mean([\n      nolds.hurst_rs(np.random.normal(size=l), fit=\"poly\")\n      for _ in range(n)\n    ])\n    rs50 = np.mean([\n      nolds.hurst_rs(np.random.normal(size=l), fit=\"poly\", nvals=nvals)\n      for _ in range(n)\n    ])\n    rs50_raw = np.mean([\n      nolds.hurst_rs(np.random.normal(size=l), fit=\"poly\", nvals=nvals, corrected=False)\n      for _ in range(n)\n    ])\n    data.append((rsn, rs50, rs50_raw))\n  lines = plt.plot(np.arange(8,17), data)\n  r = plt.plot(np.arange(8,17), reported)\n  rr = plt.plot(np.arange(8,17), reported_raw)\n  plt.legend(r + rr + lines, (\"weron\", \"weron_raw\", \"rsn\", \"rs50\", \"rs50_raw\"))\n  plt.xticks(np.arange(8,17),2**np.arange(8,17))\n  plt.xlabel(\"sequence length\")\n  plt.ylabel(\"estimated hurst exponent\")\n  plt.show()", "response": "This function creates a figure 2 of [ w ]_ comparing the reported values by Weron and the values obtained by Weron."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_hurst_hist():\n  # local import to avoid dependency for non-debug use\n  import matplotlib.pyplot as plt\n  hs = [nolds.hurst_rs(np.random.random(size=10000), corrected=True) for _ in range(100)]\n  plt.hist(hs, bins=20)\n  plt.xlabel(\"esimated value of hurst exponent\")\n  plt.ylabel(\"number of experiments\")\n  plt.show()", "response": "Plots a histogram of values obtained for the hurst exponent of uniformly\n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a bifurcation plot of the given map and superimposes the true \u00fcllcation plot of the tent .", "response": "def plot_lyap(maptype=\"logistic\"):\n  \"\"\"\n  Plots a bifurcation plot of the given map and superimposes the true\n  lyapunov exponent as well as the estimates of the largest lyapunov exponent\n  obtained by ``lyap_r`` and ``lyap_e``. The idea for this plot is taken from [ll]_.\n\n  This function requires the package ``matplotlib``.\n\n  References:\n\n  .. [ll] Manfred F\u00fcllsack, \"Lyapunov exponent\",\n     url: http://systems-sciences.uni-graz.at/etextbook/sw2/lyapunov.html\n\n  Kwargs:\n    maptype (str):\n      can be either ``\"logistic\"`` for the logistic map or ``\"tent\"`` for the tent\n      map.\n  \"\"\"\n  # local import to avoid dependency for non-debug use\n  import matplotlib.pyplot as plt\n\n  x_start = 0.1\n  n = 140\n  nbifur = 40\n  if maptype == \"logistic\":\n    param_name = \"r\"\n    param_range = np.arange(2, 4, 0.01)\n    full_data = np.array([\n      np.fromiter(datasets.logistic_map(x_start, n, r),dtype=\"float32\")\n      for r in param_range\n    ])\n    # It can be proven that the lyapunov exponent of the logistic map\n    # (or any map that is an iterative application of a function) can be\n    # calculated as the mean of the logarithm of the absolute of the\n    # derivative at the individual data points.\n    # For a proof see for example: \n    # https://blog.abhranil.net/2015/05/15/lyapunov-exponent-of-the-logistic-map-mathematica-code/\n    # Derivative of logistic map: f(x) = r * x * (1 - x) = r * x - r * x\u00b2\n    # => f'(x) = r - 2 * r * x\n    lambdas = [\n      np.mean(np.log(abs(r - 2 * r * x[np.where(x != 0.5)])))\n      for x,r in zip(full_data, param_range)\n    ]\n  elif maptype == \"tent\":\n    param_name = \"$\\mu$\"\n    param_range = np.arange(0, 2, 0.01)\n    full_data = np.array([\n      np.fromiter(datasets.tent_map(x_start, n, mu),dtype=\"float32\")\n      for mu in param_range\n    ])\n    # for the tent map the lyapunov exponent is much easier to calculate\n    # since the values are multiplied by mu in each step, two trajectories\n    # starting in x and x + delta will have a distance of delta * mu^n after n\n    # steps. Therefore the lyapunov exponent should be log(mu).\n    lambdas = np.log(param_range, where=param_range > 0)\n    lambdas[np.where(param_range <= 0)] = np.nan\n  else:\n    raise Error(\"maptype %s not recognized\" % maptype)\n\n  kwargs_e = { \"emb_dim\": 6, \"matrix_dim\": 2 }\n  kwargs_r = { \"emb_dim\": 6, \"lag\": 2, \"min_tsep\": 20, \"trajectory_len\": 20}\n  lambdas_e = [max(nolds.lyap_e(d, **kwargs_e)) for d in full_data]\n  lambdas_r = [nolds.lyap_r(d, **kwargs_r) for d in full_data]\n  bifur_x = np.repeat(param_range, nbifur)\n  bifur = np.reshape(full_data[:,-nbifur:], nbifur * param_range.shape[0])\n\n  plt.title(\"Lyapunov exponent of the %s map\" % maptype)\n  plt.plot(param_range, lambdas, \"b-\", label=\"true lyap. exponent\")\n  elab = \"estimation using lyap_e\"\n  rlab = \"estimation using lyap_r\"\n  plt.plot(param_range, lambdas_e, color=\"#00AAAA\", label=elab)\n  plt.plot(param_range, lambdas_r, color=\"#AA00AA\", label=rlab)\n  plt.plot(param_range, np.zeros(len(param_range)), \"g--\")\n  plt.plot(bifur_x, bifur, \"ro\", alpha=0.1, label=\"bifurcation plot\")\n  plt.ylim((-2, 2))\n  plt.xlabel(param_name)\n  plt.ylabel(\"lyap. exp / %s(x, %s)\" % (maptype, param_name))\n  plt.legend(loc=\"best\")\n  plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef profiling():\n  import cProfile\n  n = 10000\n  data = np.cumsum(np.random.random(n) - 0.5)\n  cProfile.runctx('lyap_e(data)', {'lyap_e': nolds.lyap_e}, {'data': data})", "response": "Runs a profiling test for the function lyap_e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hurst_compare_nvals(data, nvals=None):\n  import matplotlib.pyplot as plt\n  data = np.asarray(data)\n  n_all = np.arange(2,len(data)+1)\n  dd_all = nolds.hurst_rs(data, nvals=n_all, debug_data=True, fit=\"poly\")\n  dd_def = nolds.hurst_rs(data, debug_data=True, fit=\"poly\")\n  n_def = np.round(np.exp(dd_def[1][0])).astype(\"int32\")\n  n_div = n_all[np.where(len(data) % n_all[:-1] == 0)]\n  dd_div = nolds.hurst_rs(data, nvals=n_div, debug_data=True, fit=\"poly\")\n  def corr(nvals):\n    return [np.log(nolds.expected_rs(n)) for n in nvals]\n\n\n  l_all = plt.plot(dd_all[1][0], dd_all[1][1] - corr(n_all), \"o\")\n  l_def = plt.plot(dd_def[1][0], dd_def[1][1] - corr(n_def), \"o\")\n  l_div = plt.plot(dd_div[1][0], dd_div[1][1] - corr(n_div), \"o\")\n  l_cst = []\n  t_cst = []\n\n  if nvals is not None:\n    dd_cst = nolds.hurst_rs(data, nvals=nvals, debug_data=True, fit=\"poly\")\n    l_cst = plt.plot(dd_cst[1][0], dd_cst[1][1] - corr(nvals), \"o\")\n    l_cst = l_cst\n    t_cst = [\"custom\"]\n  plt.xlabel(\"log(n)\")\n  plt.ylabel(\"log((R/S)_n - E[(R/S)_n])\")\n  plt.legend(l_all + l_def + l_div + l_cst, [\"all\", \"default\", \"divisors\"] + t_cst)\n  labeled_data = zip([dd_all[0], dd_def[0], dd_div[0]], [\"all\", \"def\", \"div\"])\n  for data, label in labeled_data:\n    print(\"%s: %.3f\" % (label, data))\n  if nvals is not None:\n    print(\"custom: %.3f\" % dd_cst[0])\n  plt.show()", "response": "Function hurst_rs. Returns a plot that compares the results of different choices for nvals\n."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to generate fractional brownian motions of desired length.", "response": "def fbm(n, H=0.75):\n  \"\"\"\n  Generates fractional brownian motions of desired length.\n\n  Author:\n    Christian Thomae\n\n  References:\n    .. [fbm_1] https://en.wikipedia.org/wiki/Fractional_Brownian_motion#Method_1_of_simulation\n\n  Args:\n    n (int):\n      length of sequence to generate\n  Kwargs:\n    H (float):\n      hurst parameter\n\n  Returns:\n    array of float:\n      simulated fractional brownian motion\n  \"\"\"\n  # TODO more detailed description of fbm\n  assert H > 0 and H < 1\n\n  def R(t, s):\n    twoH = 2 * H\n    return 0.5 * (s**twoH + t**twoH - np.abs(t - s)**twoH)\n  # form the matrix tau\n  gamma = R(*np.mgrid[0:n, 0:n])  # apply R to every element in matrix\n  w, P = np.linalg.eigh(gamma)\n  L = np.diag(w)\n  sigma = np.dot(np.dot(P, np.sqrt(L)), np.linalg.inv(P))\n  v = np.random.randn(n)\n  return np.dot(sigma, v)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qrandom(n):\n  import quantumrandom\n  return np.concatenate([\n    quantumrandom.get_data(data_type='uint16', array_length=1024)\n    for i in range(int(np.ceil(n/1024.0)))\n  ])[:n]", "response": "This function creates an array of n true random numbers obtained from the quantum random\n number generator at qrng. anu. edu. au."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_qrandom():\n  fname = \"datasets/qrandom.npy\"\n  with pkg_resources.resource_stream(__name__, fname) as f:\n    return np.load(f)", "response": "Load a set of 10000 random numbers generated by qrandom."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the dataset brown72 with a prescribed Hurst exponent of 0. 72", "response": "def load_brown72():\n  \"\"\"\n  Loads the dataset brown72 with a prescribed Hurst exponent of 0.72\n\n  Source: http://www.bearcave.com/misl/misl_tech/wavelets/hurst/\n\n  Returns:\n    float array:\n      the dataset\n  \"\"\"\n  fname = \"datasets/brown72.npy\"\n  with pkg_resources.resource_stream(__name__, fname) as f:\n    return np.load(f)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tent_map(x, steps, mu=2):\n  for _ in range(steps):\n    x = mu * x if x < 0.5 else mu * (1 - x)\n    yield x", "response": "Generates a time series of the tent map."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logistic_map(x, steps, r=4):\n  for _ in range(steps):\n    x = r * x * (1 - x)\n    yield x", "response": "r Generates a logistic map of the given number of steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a time - delay embedding of a time series.", "response": "def delay_embedding(data, emb_dim, lag=1):\n  \"\"\"\n  Perform a time-delay embedding of a time series\n\n  Args:\n    data (array-like):\n      the data that should be embedded\n    emb_dim (int):\n      the embedding dimension\n  Kwargs:\n    lag (int):\n      the lag between elements in the embedded vectors\n\n  Returns:\n    emb_dim x m array:\n      matrix of embedded vectors of the form\n      [data[i], data[i+lag], data[i+2*lag], ... data[i+(emb_dim-1)*lag]]\n      for i in 0 to m-1 (m = len(data)-(emb_dim-1)*lag)\n  \"\"\"\n  data = np.asarray(data)\n  min_len = (emb_dim - 1) * lag + 1\n  if len(data) < min_len:\n    msg = \"cannot embed data of length {} with embedding dimension {} \" \\\n        + \"and lag {}, minimum required length is {}\"\n    raise ValueError(msg.format(len(data), emb_dim, lag, min_len))\n  m = len(data) - min_len + 1\n  indices = np.repeat([np.arange(emb_dim) * lag], m, axis=0)\n  indices += np.arange(m).reshape((m, 1))\n  return data[indices]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nestimate the Lyapunov exponent of a given set of data.", "response": "def lyap_r(data, emb_dim=10, lag=None, min_tsep=None, tau=1, min_neighbors=20,\n           trajectory_len=20, fit=\"RANSAC\", debug_plot=False, debug_data=False,\n           plot_file=None, fit_offset=0):\n  \"\"\"\n  Estimates the largest Lyapunov exponent using the algorithm of Rosenstein\n  et al. [lr_1]_.\n\n  Explanation of Lyapunov exponents:\n    See lyap_e.\n\n  Explanation of the algorithm:\n    The algorithm of Rosenstein et al. is only able to recover the largest\n    Lyapunov exponent, but behaves rather robust to parameter choices.\n\n    The idea for the algorithm relates closely to the definition of Lyapunov\n    exponents. First, the dynamics of the data are reconstructed using a delay\n    embedding method with a lag, such that each value x_i of the data is mapped\n    to the vector\n\n    X_i = [x_i, x_(i+lag), x_(i+2*lag), ..., x_(i+(emb_dim-1) * lag)]\n\n    For each such vector X_i, we find the closest neighbor X_j using the\n    euclidean distance. We know that as we follow the trajectories from X_i and\n    X_j in time in a chaotic system the distances between X_(i+k) and X_(j+k)\n    denoted as d_i(k) will increase according to a power law\n    d_i(k) = c * e^(lambda * k) where lambda is a good approximation of the\n    highest Lyapunov exponent, because the exponential expansion along the axis\n    associated with this exponent will quickly dominate the expansion or\n    contraction along other axes.\n\n    To calculate lambda, we look at the logarithm of the distance trajectory,\n    because log(d_i(k)) = log(c) + lambda * k. This gives a set of lines\n    (one for each index i) whose slope is an approximation of lambda. We\n    therefore extract the mean log trajectory d'(k) by taking the mean of\n    log(d_i(k)) over all orbit vectors X_i. We then fit a straight line to\n    the plot of d'(k) versus k. The slope of the line gives the desired\n    parameter lambda.\n\n  Method for choosing min_tsep:\n    Usually we want to find neighbors between points that are close in phase\n    space but not too close in time, because we want to avoid spurious\n    correlations between the obtained trajectories that originate from temporal\n    dependencies rather than the dynamic properties of the system. Therefore it\n    is critical to find a good value for min_tsep. One rather plausible\n    estimate for this value is to set min_tsep to the mean period of the\n    signal, which can be obtained by calculating the mean frequency using the\n    fast fourier transform. This procedure is used by default if the user sets\n    min_tsep = None.\n\n  Method for choosing lag:\n    Another parameter that can be hard to choose by instinct alone is the lag\n    between individual values in a vector of the embedded orbit. Here,\n    Rosenstein et al. suggest to set the lag to the distance where the\n    autocorrelation function drops below 1 - 1/e times its original (maximal)\n    value. This procedure is used by default if the user sets lag = None.\n\n  References:\n    .. [lr_1] M. T. Rosenstein, J. J. Collins, and C. J. De Luca,\n       \u201cA practical method for calculating largest Lyapunov exponents from\n       small data sets,\u201d Physica D: Nonlinear Phenomena, vol. 65, no. 1,\n       pp. 117\u2013134, 1993.\n\n  Reference Code:\n    .. [lr_a] mirwais, \"Largest Lyapunov Exponent with Rosenstein's Algorithm\",\n       url: http://www.mathworks.com/matlabcentral/fileexchange/38424-largest-lyapunov-exponent-with-rosenstein-s-algorithm\n    .. [lr_b] Shapour Mohammadi, \"LYAPROSEN: MATLAB function to calculate\n       Lyapunov exponent\",\n       url: https://ideas.repec.org/c/boc/bocode/t741502.html\n\n  Args:\n    data (iterable of float):\n      (one-dimensional) time series\n  Kwargs:\n    emb_dim (int):\n      embedding dimension for delay embedding\n    lag (float):\n      lag for delay embedding\n    min_tsep (float):\n      minimal temporal separation between two \"neighbors\" (default:\n      find a suitable value by calculating the mean period of the data)\n    tau (float):\n      step size between data points in the time series in seconds\n      (normalization scaling factor for exponents)\n    min_neighbors (int):\n      if lag=None, the search for a suitable lag will be stopped when the\n      number of potential neighbors for a vector drops below min_neighbors\n    trajectory_len (int):\n      the time (in number of data points) to follow the distance\n      trajectories between two neighboring points\n    fit (str):\n      the fitting method to use for the line fit, either 'poly' for normal\n      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which\n      is more robust to outliers\n    debug_plot (boolean):\n      if True, a simple plot of the final line-fitting step will\n      be shown\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n    fit_offset (int):\n      neglect the first fit_offset steps when fitting\n\n  Returns:\n    float:\n      an estimate of the largest Lyapunov exponent (a positive exponent is\n      a strong indicator for chaos)\n    (1d-vector, 1d-vector, list):\n      only present if debug_data is True: debug data of the form\n      ``(ks, div_traj, poly)`` where ``ks`` are the x-values of the line fit, \n      ``div_traj`` are the y-values and ``poly`` are the line coefficients\n      (``[slope, intercept]``).\n\n  \"\"\"\n  # convert data to float to avoid overflow errors in rowwise_euclidean\n  data = np.asarray(data, dtype=\"float32\")\n  n = len(data)\n  max_tsep_factor = 0.25\n  if lag is None or min_tsep is None:\n    # both the algorithm for lag and min_tsep need the fft\n    f = np.fft.rfft(data, n * 2 - 1)\n  if min_tsep is None:\n    # calculate min_tsep as mean period (= 1 / mean frequency)\n    mf = np.fft.rfftfreq(n * 2 - 1) * np.abs(f)\n    mf = np.mean(mf[1:]) / np.sum(np.abs(f[1:]))\n    min_tsep = int(np.ceil(1.0 / mf))\n    if min_tsep > max_tsep_factor * n:\n      min_tsep = int(max_tsep_factor * n)\n      msg = \"signal has very low mean frequency, setting min_tsep = {:d}\"\n      warnings.warn(msg.format(min_tsep), RuntimeWarning)\n  if lag is None:\n    # calculate the lag as point where the autocorrelation drops to (1 - 1/e)\n    # times its maximum value\n    # note: the Wiener\u2013Khinchin theorem states that the spectral\n    # decomposition of the autocorrelation function of a process is the power\n    # spectrum of that process\n    # => we can use fft to calculate the autocorrelation\n    acorr = np.fft.irfft(f * np.conj(f))\n    acorr = np.roll(acorr, n - 1)\n    eps = acorr[n - 1] * (1 - 1.0 / np.e)\n    lag = 1\n    # small helper function to calculate resulting number of vectors for a\n    # given lag value\n    def nb_neighbors(lag_value):\n      min_len = lyap_r_len(\n        emb_dim=emb_dim, lag=i, trajectory_len=trajectory_len,\n        min_tsep=min_tsep\n      )\n      return max(0, n - min_len)\n    # find lag\n    for i in range(1,n):\n      lag = i\n      if acorr[n - 1 + i] < eps or acorr[n - 1 - i] < eps:\n        break\n      if nb_neighbors(i) < min_neighbors:\n        msg = \"autocorrelation declined too slowly to find suitable lag\" \\\n          + \", setting lag to {}\"\n        warnings.warn(msg.format(lag), RuntimeWarning)\n        break\n  min_len = lyap_r_len(\n    emb_dim=emb_dim, lag=lag, trajectory_len=trajectory_len,\n    min_tsep=min_tsep\n  )\n  if len(data) < min_len:\n    msg = \"for emb_dim = {}, lag = {}, min_tsep = {} and trajectory_len = {}\" \\\n      + \" you need at least {} datapoints in your time series\"\n    warnings.warn(\n      msg.format(emb_dim, lag, min_tsep, trajectory_len, min_len),\n      RuntimeWarning\n    )\n  # delay embedding\n  orbit = delay_embedding(data, emb_dim, lag)\n  m = len(orbit)\n  # construct matrix with pairwise distances between vectors in orbit\n  dists = np.array([rowwise_euclidean(orbit, orbit[i]) for i in range(m)])\n  # we do not want to consider vectors as neighbor that are less than min_tsep\n  # time steps together => mask the distances min_tsep to the right and left of\n  # each index by setting them to infinity (will never be considered as nearest\n  # neighbors)\n  for i in range(m):\n    dists[i, max(0, i - min_tsep):i + min_tsep + 1] = float(\"inf\")\n  # check that we have enough data points to continue\n  ntraj = m - trajectory_len + 1\n  min_traj = min_tsep * 2 + 2 # in each row min_tsep + 1 disances are inf\n  if ntraj <= 0:\n    msg = \"Not enough data points. Need {} additional data points to follow \" \\\n        + \"a complete trajectory.\"\n    raise ValueError(msg.format(-ntraj+1))\n  if ntraj < min_traj:\n    # not enough data points => there are rows where all values are inf\n    assert np.any(np.all(np.isinf(dists[:ntraj, :ntraj]), axis=1))\n    msg = \"Not enough data points. At least {} trajectories are required \" \\\n        + \"to find a valid neighbor for each orbit vector with min_tsep={} \" \\\n        + \"but only {} could be created.\"\n    raise ValueError(msg.format(min_traj, min_tsep, ntraj))\n  assert np.all(np.any(np.isfinite(dists[:ntraj, :ntraj]), axis=1))\n  # find nearest neighbors (exclude last columns, because these vectors cannot\n  # be followed in time for trajectory_len steps)\n  nb_idx = np.argmin(dists[:ntraj, :ntraj], axis=1)\n  \n  # build divergence trajectory by averaging distances along the trajectory\n  # over all neighbor pairs\n  div_traj = np.zeros(trajectory_len, dtype=float)\n  for k in range(trajectory_len):\n    # calculate mean trajectory distance at step k\n    indices = (np.arange(ntraj) + k, nb_idx + k)\n    div_traj_k = dists[indices]\n    # filter entries where distance is zero (would lead to -inf after log)\n    nonzero = np.where(div_traj_k != 0)\n    if len(nonzero[0]) == 0:\n      # if all entries where zero, we have to use -inf\n      div_traj[k] = -np.inf\n    else:\n      div_traj[k] = np.mean(np.log(div_traj_k[nonzero]))\n  # filter -inf entries from mean trajectory\n  ks = np.arange(trajectory_len)\n  finite = np.where(np.isfinite(div_traj))\n  ks = ks[finite]\n  div_traj = div_traj[finite]\n  if len(ks) < 1:\n    # if all points or all but one point in the trajectory is -inf, we cannot\n    # fit a line through the remaining points => return -inf as exponent\n    poly = [-np.inf, 0]\n  else:\n    # normal line fitting\n    poly = poly_fit(ks[fit_offset:], div_traj[fit_offset:], 1, fit=fit)\n  if debug_plot:\n    plot_reg(ks[fit_offset:], div_traj[fit_offset:], poly, \"k\", \"log(d(k))\", fname=plot_file)\n  le = poly[0] / tau\n  if debug_data:\n    return (le, (ks, div_traj, poly))\n  else:\n    return le"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lyap_e(data, emb_dim=10, matrix_dim=4, min_nb=None, min_tsep=0, tau=1,\n           debug_plot=False, debug_data=False, plot_file=None):\n  \"\"\"\n  Estimates the Lyapunov exponents for the given data using the algorithm of\n  Eckmann et al. [le_1]_.\n\n  Recommendations for parameter settings by Eckmann et al.:\n    * long recording time improves accuracy, small tau does not\n    * use large values for emb_dim\n    * matrix_dim should be 'somewhat larger than the expected number of\n      positive Lyapunov exponents'\n    * min_nb = min(2 * matrix_dim, matrix_dim + 4)\n\n  Explanation of Lyapunov exponents:\n    The Lyapunov exponent describes the rate of separation of two\n    infinitesimally close trajectories of a dynamical system in phase space.\n    In a chaotic system, these trajectories diverge exponentially following\n    the equation:\n\n    \\|X(t, X_0) - X(t, X_0 + eps)| = e^(lambda * t) * \\|eps|\n\n    In this equation X(t, X_0) is the trajectory of the system X starting at\n    the point X_0 in phase space at time t. eps is the (infinitesimal)\n    difference vector and lambda is called the Lyapunov exponent. If the\n    system has more than one free variable, the phase space is\n    multidimensional and each dimension has its own Lyapunov exponent. The\n    existence of at least one positive Lyapunov exponent is generally seen as\n    a strong indicator for chaos.\n\n  Explanation of the Algorithm:\n    To calculate the Lyapunov exponents analytically, the Jacobian of the\n    system is required. The algorithm of Eckmann et al. therefore tries to\n    estimate this Jacobian by reconstructing the dynamics of the system from\n    which the time series was obtained. For this, several steps are required:\n\n    * Embed the time series [x_1, x_2, ..., x_(N-1)] in an orbit of emb_dim\n      dimensions (map each point x_i of the time series to a vector\n      [x_i, x_(i+1), x_(i+2), ... x_(i+emb_dim-1)]).\n    * For each vector X_i in this orbit find a radius r_i so that at least\n      min_nb other vectors lie within (chebyshev-)distance r_i around X_i.\n      These vectors will be called \"neighbors\" of X_i.\n    * Find the Matrix T_i that sends points from the neighborhood of X_i to\n      the neighborhood of X_(i+1). To avoid undetermined values in T_i, we\n      construct T_i not with size (emb_dim x emb_dim) but with size\n      (matrix_dim x matrix_dim), so that we have a larger \"step size\" m in the\n      X_i, which are now defined as X'_i = [x_i, x_(i+m), x_(i+2m),\n      ... x_(i+(matrix_dim-1)*m)]. This means that emb_dim-1 must be divisible\n      by matrix_dim-1. The T_i are then found by a linear least squares fit,\n      assuring that T_i (X_j - X_i) ~= X_(j+m) - X_(i+m) for any X_j in the\n      neighborhood of X_i.\n    * Starting with i = 1 and Q_0 = identity successively decompose the matrix\n      T_i * Q_(i-1) into the matrices Q_i and R_i by a QR-decomposition.\n    * Calculate the Lyapunov exponents from the mean of the logarithm of the\n      diagonal elements of the matrices R_i. To normalize the Lyapunov\n      exponents, they have to be divided by m and by the step size tau of the\n      original time series.\n\n  References:\n    .. [le_1] J. P. Eckmann, S. O. Kamphorst, D. Ruelle, and S. Ciliberto,\n       \u201cLiapunov exponents from time series,\u201d Physical Review A,\n       vol. 34, no. 6, pp. 4971\u20134979, 1986.\n\n  Reference code:\n    .. [le_a] Manfred F\u00fcllsack, \"Lyapunov exponent\",\n       url: http://systems-sciences.uni-graz.at/etextbook/sw2/lyapunov.html\n    .. [le_b] Steve SIU, Lyapunov Exponents Toolbox (LET),\n       url: http://www.mathworks.com/matlabcentral/fileexchange/233-let/content/LET/findlyap.m\n    .. [le_c] Rainer Hegger, Holger Kantz, and Thomas Schreiber, TISEAN,\n       url: http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/index.html\n\n  Args:\n    data (array-like of float):\n      (scalar) data points\n\n  Kwargs:\n    emb_dim (int):\n      embedding dimension\n    matrix_dim (int):\n      matrix dimension (emb_dim - 1 must be divisible by matrix_dim - 1)\n    min_nb (int):\n      minimal number of neighbors\n      (default: min(2 * matrix_dim, matrix_dim + 4))\n    min_tsep (int):\n      minimal temporal separation between two \"neighbors\"\n    tau (float):\n      step size of the data in seconds\n      (normalization scaling factor for exponents)\n    debug_plot (boolean):\n      if True, a histogram matrix of the individual estimates will be shown\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n\n  Returns:\n    float array:\n      array of matrix_dim Lyapunov exponents (positive exponents are indicators\n      for chaos)\n    2d-array of floats:\n      only present if debug_data is True: all estimates for the matrix_dim\n      Lyapunov exponents from the x iterations of R_i. The shape of this debug\n      data is (x, matrix_dim).\n  \"\"\"\n  data = np.asarray(data)\n  n = len(data)\n  if (emb_dim - 1) % (matrix_dim - 1) != 0:\n    raise ValueError(\"emb_dim - 1 must be divisible by matrix_dim - 1!\")\n  m = (emb_dim - 1) // (matrix_dim - 1)\n  if min_nb is None:\n    # minimal number of neighbors as suggested by Eckmann et al.\n    min_nb = min(2 * matrix_dim, matrix_dim + 4)\n\n  min_len = lyap_e_len(\n    emb_dim=emb_dim, matrix_dim=matrix_dim, min_nb=min_nb, min_tsep=min_tsep\n  )\n  if n < min_len:\n    msg = \"{} data points are not enough! For emb_dim = {}, matrix_dim = {}, \" \\\n      + \"min_tsep = {} and min_nb = {} you need at least {} data points \" \\\n      + \"in your time series\"\n    warnings.warn(\n      msg.format(n, emb_dim, matrix_dim, min_tsep, min_nb, min_len),\n      RuntimeWarning\n    )\n\n  # construct orbit as matrix (e = emb_dim)\n  # x0 x1 x2 ... xe-1\n  # x1 x2 x3 ... xe\n  # x2 x3 x4 ... xe+1\n  # ...\n\n  # note: we need to be able to step m points further for the beta vector\n  #       => maximum start index is n - emb_dim - m\n  orbit = delay_embedding(data[:-m], emb_dim, lag=1)\n  if len(orbit) < min_nb:\n    assert len(data) < min_len\n    msg = \"Not enough data points. Need at least {} additional data points \" \\\n        + \"to have min_nb = {} neighbor candidates\"\n    raise ValueError(msg.format(min_nb-len(orbit), min_nb))\n  old_Q = np.identity(matrix_dim)\n  lexp = np.zeros(matrix_dim, dtype=\"float32\")\n  lexp_counts = np.zeros(lexp.shape)\n  debug_values = []\n  # TODO reduce number of points to visit?\n  # TODO performance test!\n  for i in range(len(orbit)):\n    # find neighbors for each vector in the orbit using the chebyshev distance\n    diffs = rowwise_chebyshev(orbit, orbit[i])\n    # ensure that we do not count the difference of the vector to itself\n    diffs[i] = float('inf')\n    # mask all neighbors that are too close in time to the vector itself\n    mask_from = max(0, i - min_tsep)\n    mask_to = min(len(diffs), i + min_tsep + 1)\n    diffs[mask_from:mask_to] = np.inf\n    indices = np.argsort(diffs)\n    idx = indices[min_nb - 1]  # index of the min_nb-nearest neighbor\n    r = diffs[idx]  # corresponding distance\n    if np.isinf(r):\n      assert len(data) < min_len\n      msg = \"Not enough data points. Orbit vector {} has less than min_nb = \" \\\n          + \"{} valid neighbors that are at least min_tsep = {} time steps \" \\\n          + \"away. Input must have at least length {}.\"\n      raise ValueError(msg.format(i, min_nb, min_tsep, min_len))\n    # there may be more than min_nb vectors at distance r (if multiple vectors\n    # have a distance of exactly r)\n    # => update index accordingly\n    indices = np.where(diffs <= r)[0]\n\n    # find the matrix T_i that satisifies\n    # T_i (orbit'[j] - orbit'[i]) = (orbit'[j+m] - orbit'[i+m])\n    # for all neighbors j where orbit'[i] = [x[i], x[i+m],\n    # ... x[i + (matrix_dim-1)*m]]\n\n    # note that T_i has the following form:\n    # 0  1  0  ... 0\n    # 0  0  1  ... 0\n    # ...\n    # a0 a1 a2 ... a(matrix_dim-1)\n\n    # This is because for all rows except the last one the aforementioned\n    # equation has a clear solution since orbit'[j+m] - orbit'[i+m] =\n    # [x[j+m]-x[i+m], x[j+2*m]-x[i+2*m], ... x[j+d_M*m]-x[i+d_M*m]]\n    # and\n    # orbit'[j] - orbit'[i] =\n    # [x[j]-x[i], x[j+m]-x[i+m], ... x[j+(d_M-1)*m]-x[i+(d_M-1)*m]]\n    # therefore x[j+k*m] - x[i+k*m] is already contained in\n    # orbit'[j] - orbit'[x] for all k from 1 to matrix_dim-1. Only for\n    # k = matrix_dim there is an actual problem to solve.\n\n    # We can therefore find a = [a0, a1, a2, ... a(matrix_dim-1)] by\n    # formulating a linear least squares problem (mat_X * a = vec_beta)\n    # as follows.\n\n    # build matrix X for linear least squares (d_M = matrix_dim)\n    # x_j1 - x_i   x_j1+m - x_i+m   ...   x_j1+(d_M-1)m - x_i+(d_M-1)m\n    # x_j2 - x_i   x_j2+m - x_i+m   ...   x_j2+(d_M-1)m - x_i+(d_M-1)m\n    # ...\n\n    # note: emb_dim = (d_M - 1) * m + 1\n    mat_X = np.array([data[j:j + emb_dim:m] for j in indices])\n    mat_X -= data[i:i + emb_dim:m]\n\n    # build vector beta for linear least squares\n    # x_j1+(d_M)m - x_i+(d_M)m\n    # x_j2+(d_M)m - x_i+(d_M)m\n    # ...\n    if max(np.max(indices),i) + matrix_dim * m >= len(data):\n      assert len(data) < min_len\n      msg = \"Not enough data points. Cannot follow orbit vector {} for \" \\\n          + \"{} (matrix_dim * m) time steps. Input must have at least length \" \\\n          + \"{}.\"\n      raise ValueError(msg.format(i, matrix_dim * m, min_len))\n    vec_beta = data[indices + matrix_dim * m] - data[i + matrix_dim * m]\n\n    # perform linear least squares\n    a, _, _, _ = np.linalg.lstsq(mat_X, vec_beta, rcond=-1)\n    # build matrix T\n    # 0  1  0  ... 0\n    # 0  0  1  ... 0\n    # ...\n    # 0  0  0  ... 1\n    # a1 a2 a3 ... a_(d_M)\n    mat_T = np.zeros((matrix_dim, matrix_dim))\n    mat_T[:-1, 1:] = np.identity(matrix_dim - 1)\n    mat_T[-1] = a\n\n    # QR-decomposition of T * old_Q\n    mat_Q, mat_R = np.linalg.qr(np.dot(mat_T, old_Q))\n    # force diagonal of R to be positive\n    # (if QR = A then also QLL'R = A with L' = L^-1)\n    sign_diag = np.sign(np.diag(mat_R))\n    sign_diag[np.where(sign_diag == 0)] = 1\n    sign_diag = np.diag(sign_diag)\n    mat_Q = np.dot(mat_Q, sign_diag)\n    mat_R = np.dot(sign_diag, mat_R)\n\n    old_Q = mat_Q\n    # successively build sum for Lyapunov exponents\n    diag_R = np.diag(mat_R)\n    # filter zeros in mat_R (would lead to -infs)\n    idx = np.where(diag_R > 0)\n    lexp_i = np.zeros(diag_R.shape, dtype=\"float32\")\n    lexp_i[idx] = np.log(diag_R[idx])\n    lexp_i[np.where(diag_R == 0)] = np.inf\n    if debug_plot or debug_data:\n      debug_values.append(lexp_i / tau / m)\n    lexp[idx] += lexp_i[idx]\n    lexp_counts[idx] += 1\n  # end of loop over orbit vectors\n  # it may happen that all R-matrices contained zeros => exponent really has\n  # to be -inf\n  if debug_plot:\n    plot_histogram_matrix(np.array(debug_values), \"layp_e\", fname=plot_file)\n  # normalize exponents over number of individual mat_Rs\n  idx = np.where(lexp_counts > 0)\n  lexp[idx] /= lexp_counts[idx]\n  lexp[np.where(lexp_counts == 0)] = np.inf\n  # normalize with respect to tau\n  lexp /= tau\n  # take m into account\n  lexp /= m\n  if debug_data:\n    return (lexp, np.array(debug_values))\n  return lexp", "response": "Estimate the Lyapunov exponents for a given data set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the sample entropy of the given data. Explanation of the sample entropy: The sample entropy of a time series is defined as the negative natural logarithm of the conditional probability that two sequences similar for emb_dim points remain similar at the next point, excluding self-matches. A lower value for the sample entropy therefore corresponds to a higher probability indicating more self-similarity. Explanation of the algorithm: The algorithm constructs all subsequences of length emb_dim [s_1, s_2, s_3, ...] and then counts each pair (s_i, s_j) with i != j where dist(s_i, s_j) < tolerance. The same process is repeated for all subsequences of length emb_dim + 1. The sum of similar sequence pairs with length emb_dim + 1 is divided by the sum of similar sequence pairs with length emb_dim. The result of the algorithm is the negative logarithm of this ratio/probability. References: .. [se_1] J. S. Richman and J. R. Moorman, \u201cPhysiological time-series analysis using approximate entropy and sample entropy,\u201d American Journal of Physiology-Heart and Circulatory Physiology, vol. 278, no. 6, pp. H2039\u2013H2049, 2000. Reference code: .. [se_a] \"sample_entropy\" function in R-package \"pracma\", url: https://cran.r-project.org/web/packages/pracma/pracma.pdf Args: data (array-like of float): input data Kwargs: emb_dim (int): the embedding dimension (length of vectors to compare) tolerance (float): distance threshold for two template vectors to be considered equal (default: 0.2 * std(data)) dist (function (2d-array, 1d-array) -> 1d-array): distance function used to calculate the distance between template vectors. Sampen is defined using ``rowwise_chebyshev``. You should only use something else, if you are sure that you need it. debug_plot (boolean): if True, a histogram of the individual distances for m and m+1 debug_data (boolean): if True, debugging data will be returned alongside the result plot_file (str): if debug_plot is True and plot_file is not None, the plot will be saved under the given file name instead of directly showing it through ``plt.show()`` Returns: float: the sample entropy of the data (negative logarithm of ratio between similar template vectors of length emb_dim + 1 and emb_dim) [float list, float list]: Lists of lists of the form ``[dists_m, dists_m1]`` containing the distances between template vectors for m (dists_m) and for m + 1 (dists_m1).", "response": "def sampen(data, emb_dim=2, tolerance=None, dist=rowwise_chebyshev,\n           debug_plot=False, debug_data=False, plot_file=None):\n  \"\"\"\n  Computes the sample entropy of the given data.\n\n  Explanation of the sample entropy:\n    The sample entropy of a time series is defined as the negative natural\n    logarithm of the conditional probability that two sequences similar for\n    emb_dim points remain similar at the next point, excluding self-matches.\n\n    A lower value for the sample entropy therefore corresponds to a higher\n    probability indicating more self-similarity.\n\n  Explanation of the algorithm:\n    The algorithm constructs all subsequences of length emb_dim\n    [s_1, s_2, s_3, ...] and then counts each pair (s_i, s_j) with i != j\n    where dist(s_i, s_j) < tolerance. The same process is repeated for all\n    subsequences of length emb_dim + 1. The sum of similar sequence pairs\n    with length emb_dim + 1 is divided by the sum of similar sequence pairs\n    with length emb_dim. The result of the algorithm is the negative logarithm\n    of this ratio/probability.\n\n  References:\n    .. [se_1] J. S. Richman and J. R. Moorman, \u201cPhysiological time-series\n       analysis using approximate entropy and sample entropy,\u201d\n       American Journal of Physiology-Heart and Circulatory Physiology,\n       vol. 278, no. 6, pp. H2039\u2013H2049, 2000.\n\n  Reference code:\n    .. [se_a] \"sample_entropy\" function in R-package \"pracma\",\n        url: https://cran.r-project.org/web/packages/pracma/pracma.pdf\n\n  Args:\n    data (array-like of float):\n      input data\n\n  Kwargs:\n    emb_dim (int):\n      the embedding dimension (length of vectors to compare)\n    tolerance (float):\n      distance threshold for two template vectors to be considered equal\n      (default: 0.2 * std(data))\n    dist (function (2d-array, 1d-array) -> 1d-array):\n      distance function used to calculate the distance between template\n      vectors. Sampen is defined using ``rowwise_chebyshev``. You should only use\n      something else, if you are sure that you need it.\n    debug_plot (boolean):\n      if True, a histogram of the individual distances for m and m+1\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n\n  Returns:\n    float:\n      the sample entropy of the data (negative logarithm of ratio between\n      similar template vectors of length emb_dim + 1 and emb_dim)\n    [float list, float list]:\n      Lists of lists of the form ``[dists_m, dists_m1]`` containing the distances\n      between template vectors for m (dists_m) and for m + 1 (dists_m1).\n  \"\"\"\n  data = np.asarray(data)\n    \n  if tolerance is None:\n    tolerance = 0.2 * np.std(data)\n  n = len(data)\n\n  # build matrix of \"template vectors\"\n  # (all consecutive subsequences of length m)\n  # x0 x1 x2 x3 ... xm-1\n  # x1 x2 x3 x4 ... xm\n  # x2 x3 x4 x5 ... xm+1\n  # ...\n  # x_n-m-1     ... xn-1\n\n  # since we need two of these matrices for m = emb_dim and m = emb_dim +1,\n  # we build one that is large enough => shape (emb_dim+1, n-emb_dim)\n\n  # note that we ignore the last possible template vector with length emb_dim,\n  # because this vector has no corresponding vector of length m+1 and thus does\n  # not count towards the conditional probability\n  # (otherwise first dimension would be n-emb_dim+1 and not n-emb_dim)\n  tVecs = delay_embedding(np.asarray(data), emb_dim+1, lag=1)\n  plot_data = []\n  counts = []\n  for m in [emb_dim, emb_dim + 1]:\n    counts.append(0)\n    plot_data.append([])\n    # get the matrix that we need for the current m\n    tVecsM = tVecs[:n - m + 1, :m]\n    # successively calculate distances between each pair of template vectors\n    for i in range(len(tVecsM) - 1):\n      dsts = dist(tVecsM[i + 1:], tVecsM[i])\n      if debug_plot:\n        plot_data[-1].extend(dsts)\n      # count how many distances are smaller than the tolerance\n      counts[-1] += np.sum(dsts < tolerance)\n  if counts[1] == 0:\n    # log would be infinite => cannot determine saen\n    saen = np.inf\n  else:\n    saen = -np.log(1.0 * counts[1] / counts[0])\n  if debug_plot:\n    plot_dists(plot_data, tolerance, m, title=\"sampEn = {:.3f}\".format(saen),\n               fname=plot_file)\n  if debug_data:\n    return (saen, plot_data)\n  else:\n    return saen"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of integers by successively halving the total length total_N and then rounding down to nearest integer.", "response": "def binary_n(total_N, min_n=50):\n  \"\"\"\n  Creates a list of values by successively halving the total length total_N\n  until the resulting value is less than min_n.\n\n  Non-integer results are rounded down.\n\n  Args:\n    total_N (int):\n      total length\n  Kwargs:\n    min_n (int):\n      minimal length after division\n\n  Returns:\n    list of integers:\n      total_N/2, total_N/4, total_N/8, ... until total_N/2^i < min_n\n  \"\"\"\n  max_exp = np.log2(1.0 * total_N / min_n)\n  max_exp = int(np.floor(max_exp))\n  return [int(np.floor(1.0 * total_N / (2**i))) for i in range(1, max_exp + 1)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a list of values by successively multiplying a minimum value min_n by a factor > 1 until a maximum value max_n is reached. Non-integer results are rounded down. Args: min_n (float): minimum value (must be < max_n) max_n (float): maximum value (must be > min_n) factor (float): factor used to increase min_n (must be > 1) Returns: list of integers: min_n, min_n * factor, min_n * factor^2, ... min_n * factor^i < max_n without duplicates", "response": "def logarithmic_n(min_n, max_n, factor):\n  \"\"\"\n  Creates a list of values by successively multiplying a minimum value min_n by\n  a factor > 1 until a maximum value max_n is reached.\n\n  Non-integer results are rounded down.\n\n  Args:\n    min_n (float):\n      minimum value (must be < max_n)\n    max_n (float):\n      maximum value (must be > min_n)\n    factor (float):\n      factor used to increase min_n (must be > 1)\n\n  Returns:\n    list of integers:\n      min_n, min_n * factor, min_n * factor^2, ... min_n * factor^i < max_n\n      without duplicates\n  \"\"\"\n  assert max_n > min_n\n  assert factor > 1\n  # stop condition: min * f^x = max\n  # => f^x = max/min\n  # => x = log(max/min) / log(f)\n  max_i = int(np.floor(np.log(1.0 * max_n / min_n) / np.log(factor)))\n  ns = [min_n]\n  for i in range(max_i + 1):\n    n = int(np.floor(min_n * (factor ** i)))\n    if n > ns[-1]:\n      ns.append(n)\n  return ns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logmid_n(max_n, ratio=1/4.0, nsteps=15):\n  l = np.log(max_n)\n  span = l * ratio\n  start = l * (1 - ratio) * 0.5\n  midrange = start + 1.0*np.arange(nsteps)/nsteps*span\n  nvals = np.round(np.exp(midrange)).astype(\"int32\")\n  return np.unique(nvals)", "response": "This function creates an array of integers that lie evenly spaced in the middle of the logarithmic interval relative to log ( max_n )."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logarithmic_r(min_n, max_n, factor):\n  assert max_n > min_n\n  assert factor > 1\n  max_i = int(np.floor(np.log(1.0 * max_n / min_n) / np.log(factor)))\n  return [min_n * (factor ** i) for i in range(max_i + 1)]", "response": "Logarithmic R implementation of the logarithmic R function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the expected R^n for a given n.", "response": "def expected_rs(n):\n  \"\"\"\n  Calculates the expected (R/S)_n for white noise for a given n.\n\n  This is used as a correction factor in the function hurst_rs. It uses the\n  formula of Anis-Lloyd-Peters (see [h_3]_).\n\n  Args:\n    n (int):\n      the value of n for which the expected (R/S)_n should be calculated\n\n  Returns:\n    float:\n      expected (R/S)_n for white noise\n  \"\"\"\n  front = (n - 0.5) / n\n  i = np.arange(1,n)\n  back = np.sum(np.sqrt((n - i) / i))\n  if n <= 340:\n    middle = math.gamma((n-1) * 0.5) / math.sqrt(math.pi) / math.gamma(n * 0.5)\n  else:\n    middle = 1.0 / math.sqrt(n * math.pi * 0.5)\n  return front * middle * back"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expected_h(nvals, fit=\"RANSAC\"):\n  rsvals = [expected_rs(n) for n in nvals]\n  poly = poly_fit(np.log(nvals), np.log(rsvals), 1, fit=fit)\n  return poly[0]", "response": "Uses expected_rs to calculate the expected value for the Hurst exponent h\n based on the values of n used for the calculation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate an individual R / S value in the rescaled range approach for a given time series.", "response": "def rs(data, n, unbiased=True):\n  \"\"\"\n  Calculates an individual R/S value in the rescaled range approach for\n  a given n.\n\n  Note: This is just a helper function for hurst_rs and should not be called\n  directly.\n\n  Args:\n    data (array-like of float):\n      time series\n    n (float):\n      size of the subseries in which data should be split\n\n  Kwargs:\n    unbiased (boolean):\n      if True, the standard deviation based on the unbiased variance\n      (1/(N-1) instead of 1/N) will be used. This should be the default choice,\n      since the true mean of the sequences is not known. This parameter should\n      only be changed to recreate results of other implementations.\n\n  Returns:\n    float:\n      (R/S)_n\n  \"\"\"\n  data = np.asarray(data)\n  total_N = len(data)\n  m = total_N // n # number of sequences\n  # cut values at the end of data to make the array divisible by n\n  data = data[:total_N - (total_N % n)]\n  # split remaining data into subsequences of length n\n  seqs = np.reshape(data, (m, n))\n  # calculate means of subsequences\n  means = np.mean(seqs, axis=1)\n  # normalize subsequences by substracting mean\n  y = seqs - means.reshape((m, 1))\n  # build cumulative sum of subsequences\n  y = np.cumsum(y, axis=1)\n  # find ranges\n  r = np.max(y, axis=1) - np.min(y, axis=1)\n  # find standard deviation\n  # we should use the unbiased estimator, since we do not know the true mean\n  s = np.std(seqs, axis=1, ddof=1 if unbiased else 0)\n  # some ranges may be zero and have to be excluded from the analysis\n  idx = np.where(r != 0)\n  r = r[idx]\n  s = s[idx]\n  # it may happen that all ranges are zero (if all values in data are equal)\n  if len(r) == 0:\n    return np.nan\n  else:\n    # return mean of r/s along subsequence index\n    return np.mean(r / s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the trend lines of the window with the given x - values and y - values.", "response": "def plot_reg(xvals, yvals, poly, x_label=\"x\", y_label=\"y\", data_label=\"data\",\n             reg_label=\"regression line\", fname=None):\n  \"\"\"\n  Helper function to plot trend lines for line-fitting approaches. This\n  function will show a plot through ``plt.show()`` and close it after the window\n  has been closed by the user.\n\n  Args:\n    xvals (list/array of float):\n      list of x-values\n    yvals (list/array of float):\n      list of y-values\n    poly (list/array of float):\n      polynomial parameters as accepted by ``np.polyval``\n  Kwargs:\n    x_label (str):\n      label of the x-axis\n    y_label (str):\n      label of the y-axis\n    data_label (str):\n      label of the data\n    reg_label(str):\n      label of the regression line\n    fname (str):\n      file name (if not None, the plot will be saved to disc instead of\n      showing it though ``plt.show()``)\n  \"\"\"\n  # local import to avoid dependency for non-debug use\n  import matplotlib.pyplot as plt\n  plt.plot(xvals, yvals, \"bo\", label=data_label)\n  if not (poly is None):\n    plt.plot(xvals, np.polyval(poly, xvals), \"r-\", label=reg_label)\n  plt.xlabel(x_label)\n  plt.ylabel(y_label)\n  plt.legend(loc=\"best\")\n  if fname is None:\n    plt.show()\n  else:\n    plt.savefig(fname)\n  plt.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hurst_rs(data, nvals=None, fit=\"RANSAC\", debug_plot=False,\n             debug_data=False, plot_file=None, corrected=True, unbiased=True):\n  \"\"\"\n  Calculates the Hurst exponent by a standard rescaled range (R/S) approach.\n\n  Explanation of Hurst exponent:\n    The Hurst exponent is a measure for the \"long-term memory\" of a\n    time series, meaning the long statistical dependencies in the data that do\n    not originate from cycles.\n\n    It originates from H.E. Hursts observations of the problem of long-term\n    storage in water reservoirs. If x_i is the discharge of a river in year i\n    and we observe this discharge for N years, we can calculate the storage\n    capacity that would be required to keep the discharge steady at its mean\n    value.\n\n    To do so, we first substract the mean over all x_i from the individual\n    x_i to obtain the departures x'_i from the mean for each year i. As the\n    excess or deficit in discharge always carrys over from year i to year i+1,\n    we need to examine the cumulative sum of x'_i, denoted by y_i. This\n    cumulative sum represents the filling of our hypothetical storage. If the\n    sum is above 0, we are storing excess discharge from the river, if it is\n    below zero we have compensated a deficit in discharge by releasing\n    water from the storage. The range (maximum - minimum) R of y_i therefore\n    represents the total capacity required for the storage.\n\n    Hurst showed that this value follows a steady trend for varying N if it\n    is normalized by the standard deviation sigma over the x_i. Namely he\n    obtained the following formula:\n\n    R/sigma = (N/2)^K\n\n    In this equation, K is called the Hurst exponent. Its value is 0.5 for\n    white noise, but becomes greater for time series that exhibit some positive\n    dependency on previous values. For negative dependencies it becomes less\n    than 0.5.\n\n  Explanation of the algorithm:\n    The rescaled range (R/S) approach is directly derived from Hurst's\n    definition. The time series of length N is split into non-overlapping\n    subseries of length n. Then, R and S (S = sigma) are calculated for each\n    subseries and the mean is taken over all subseries yielding (R/S)_n. This\n    process is repeated for several lengths n. Finally, the exponent K is\n    obtained by fitting a straight line to the plot of log((R/S)_n) vs log(n).\n\n    There seems to be no consensus how to chose the subseries lenghts n.\n    This function therefore leaves the choice to the user. The module provides\n    some utility functions for \"typical\" values:\n\n      * binary_n: N/2, N/4, N/8, ...\n      * logarithmic_n: min_n, min_n * f, min_n * f^2, ...\n\n  References:\n    .. [h_1] H. E. Hurst, \u201cThe problem of long-term storage in reservoirs,\u201d\n       International Association of Scientific Hydrology. Bulletin, vol. 1,\n       no. 3, pp. 13\u201327, 1956.\n    .. [h_2] H. E. Hurst, \u201cA suggested statistical model of some time series\n       which occur in nature,\u201d Nature, vol. 180, p. 494, 1957.\n    .. [h_3] R. Weron, \u201cEstimating long-range dependence: finite sample\n       properties and confidence intervals,\u201d Physica A: Statistical Mechanics\n       and its Applications, vol. 312, no. 1, pp. 285\u2013299, 2002.\n\n  Reference Code:\n    .. [h_a] \"hurst\" function in R-package \"pracma\",\n             url: https://cran.r-project.org/web/packages/pracma/pracma.pdf\n\n             Note: Pracma yields several estimates of the Hurst exponent, which\n             are listed below. Unless otherwise stated they use the divisors\n             of the length of the sequence as n. The length is reduced by at\n             most 1% to find the value that has the most divisors.\n\n             * The \"Simple R/S\" estimate is just log((R/S)_n) / log(n) for \n               n = N.\n             * The \"theoretical Hurst exponent\" is the value that would be\n               expected of an uncorrected rescaled range approach for random\n               noise of the size of the input data.\n             * The \"empirical Hurst exponent\" is the uncorrected Hurst exponent\n               obtained by the rescaled range approach.\n             * The \"corrected empirical Hurst exponent\" is the Anis-Lloyd-Peters\n               corrected Hurst exponent, but with sqrt(1/2 * pi * n) added to\n               the (R/S)_n before the log.\n             * The \"corrected R over S Hurst exponent\" uses the R-function \"lm\"\n               instead of pracmas own \"polyfit\" and uses n = N/2, N/4, N/8, ...\n               by successively halving the subsequences (which means that some\n               subsequences may be one element longer than others). In contrast\n               to its name it does not use the Anis-Lloyd-Peters correction\n               factor.\n\n             If you want to compare the output of pracma to the output of\n             nolds, the \"empirical hurst exponent\" is the only measure that\n             exactly corresponds to the Hurst measure implemented in nolds\n             (by choosing corrected=False, fit=\"poly\" and employing the same\n             strategy for choosing n as the divisors of the (reduced)\n             sequence length).\n    .. [h_b] Rafael Weron, \"HURST: MATLAB function to compute the Hurst\n             exponent using R/S Analysis\",\n             url: https://ideas.repec.org/c/wuu/hscode/m11003.html\n\n             Note: When the same values for nvals are used and fit is set to\n             \"poly\", nolds yields exactly the same results as this\n             implementation.\n    .. [h_c] Bill Davidson, \"Hurst exponent\",\n             url: http://www.mathworks.com/matlabcentral/fileexchange/9842-hurst-exponent\n    .. [h_d] Tomaso Aste, \"Generalized Hurst exponent\",\n             url: http://de.mathworks.com/matlabcentral/fileexchange/30076-generalized-hurst-exponent\n\n  Args:\n    data (array-like of float):\n      time series\n  Kwargs:\n    nvals (iterable of int):\n      sizes of subseries to use\n      (default: logmid_n(total_N, ratio=1/4.0, nsteps=15) , that is 15\n      logarithmically spaced values in the medium 25% of the logarithmic range)\n\n      Generally, the choice for n is a trade-off between the length and the\n      number of the subsequences that are used for the calculation of the\n      (R/S)_n. Very low values of n lead to high variance in the ``r`` and ``s``\n      while very high values may leave too few subsequences that the mean along\n      them is still meaningful. Logarithmic spacing makes sense, because it \n      translates to even spacing in the log-log-plot.\n    fit (str):\n      the fitting method to use for the line fit, either 'poly' for normal\n      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which\n      is more robust to outliers\n    debug_plot (boolean):\n      if True, a simple plot of the final line-fitting step will be shown\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n    corrected (boolean):\n      if True, the Anis-Lloyd-Peters correction factor will be applied to the\n      output according to the expected value for the individual (R/S)_n\n      (see [h_3]_)\n    unbiased (boolean):\n      if True, the standard deviation based on the unbiased variance\n      (1/(N-1) instead of 1/N) will be used. This should be the default choice,\n      since the true mean of the sequences is not known. This parameter should\n      only be changed to recreate results of other implementations.\n\n  Returns:\n    float:\n      estimated Hurst exponent K using a rescaled range approach (if K = 0.5\n      there are no long-range correlations in the data, if K < 0.5 there are\n      negative long-range correlations, if K > 0.5 there are positive\n      long-range correlations)\n    (1d-vector, 1d-vector, list):\n      only present if debug_data is True: debug data of the form\n      ``(nvals, rsvals, poly)`` where ``nvals`` are the values used for log(n), \n      ``rsvals`` are the corresponding log((R/S)_n) and ``poly`` are the line \n      coefficients (``[slope, intercept]``)\n  \"\"\"\n  data = np.asarray(data)\n  total_N = len(data)\n  if nvals is None:\n    # chooses a default value for nvals that will give 15 logarithmically\n    # spaced datapoints leaning towards the middle of the logarithmic range\n    # (since both too small and too large n introduce too much variance)\n    nvals = logmid_n(total_N, ratio=1/4.0, nsteps=15)\n  # get individual values for (R/S)_n\n  rsvals = np.array([rs(data, n, unbiased=unbiased) for n in nvals])\n  # filter NaNs (zeros should not be possible, because if R is 0 then\n  # S is also zero)\n  not_nan = np.logical_not(np.isnan(rsvals))\n  rsvals = rsvals[not_nan]\n  nvals = np.asarray(nvals)[not_nan]\n  # it may happen that no rsvals are left (if all values of data are the same)\n  if len(rsvals) == 0:\n    poly = [np.nan, np.nan]\n    if debug_plot:\n      warnings.warn(\"Cannot display debug plot, all (R/S)_n are NaN\")\n  else:\n    # fit a line to the logarithm of the obtained (R/S)_n\n    xvals = np.log(nvals)\n    yvals = np.log(rsvals)\n    if corrected:\n      yvals -= np.log([expected_rs(n) for n in nvals])\n    poly = poly_fit(xvals, yvals, 1, fit=fit)\n    if debug_plot:\n      plot_reg(xvals, yvals, poly, \"log(n)\", \"log((R/S)_n)\",\n               fname=plot_file)\n  # account for correction if necessary\n  h = poly[0] + 0.5 if corrected else poly[0]\n  # return line slope (+ correction) as hurst exponent\n  if debug_data:\n    return (h, (np.log(nvals), np.log(rsvals), poly))\n  else:\n    return h", "response": "Calculates the Hurst exponent of a time series using a standard rescaled range approach."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef corr_dim(data, emb_dim, rvals=None, dist=rowwise_euclidean,\n             fit=\"RANSAC\", debug_plot=False, debug_data=False, plot_file=None):\n  \"\"\"\n  Calculates the correlation dimension with the Grassberger-Procaccia algorithm\n\n  Explanation of correlation dimension:\n    The correlation dimension is a characteristic measure that can be used\n    to describe the geometry of chaotic attractors. It is defined using the\n    correlation sum C(r) which is the fraction of pairs of points X_i in the\n    phase space whose distance is smaller than r.\n\n    If the relation between C(r) and r can be described by the power law\n\n    C(r) ~ r^D\n\n    then D is called the correlation dimension of the system.\n\n    In a d-dimensional system, the maximum value for D is d. This value is\n    obtained for systems that expand uniformly in each dimension with time.\n    The lowest possible value is 0 for a system with constant C(r) (i.e. a\n    system that visits just one point in the phase space). Generally if D is\n    lower than d and the system has an attractor, this attractor is called\n    \"strange\" and D is a measure of this \"strangeness\".\n\n  Explanation of the algorithm:\n    The Grassberger-Procaccia algorithm calculates C(r) for a range of\n    different r and then fits a straight line into the plot of log(C(r))\n    versus log(r).\n\n    This version of the algorithm is created for one-dimensional (scalar) time\n    series. Therefore, before calculating C(r), a delay embedding of the time\n    series is performed to yield emb_dim dimensional vectors\n    Y_i = [X_i, X_(i+1), X_(i+2), ... X_(i+embd_dim-1)]. Choosing a higher\n    value for emb_dim allows to reconstruct higher dimensional dynamics and\n    avoids \"systematic errors due to corrections to scaling\".\n\n  References:\n    .. [cd_1] P. Grassberger and I. Procaccia, \u201cCharacterization of strange\n              attractors,\u201d Physical review letters, vol. 50, no. 5, p. 346,\n              1983.\n    .. [cd_2] P. Grassberger and I. Procaccia, \u201cMeasuring the strangeness of\n              strange attractors,\u201d Physica D: Nonlinear Phenomena, vol. 9,\n              no. 1, pp. 189\u2013208, 1983.\n    .. [cd_3] P. Grassberger, \u201cGrassberger-Procaccia algorithm,\u201d\n              Scholarpedia, vol. 2, no. 5, p. 3043.\n              urL: http://www.scholarpedia.org/article/Grassberger-Procaccia_algorithm\n\n  Reference Code:\n    .. [cd_a] \"corrDim\" function in R package \"fractal\",\n              url: https://cran.r-project.org/web/packages/fractal/fractal.pdf\n    .. [cd_b] Peng Yuehua, \"Correlation dimension\",\n              url: http://de.mathworks.com/matlabcentral/fileexchange/24089-correlation-dimension\n\n  Args:\n    data (array-like of float):\n      time series of data points\n    emb_dim (int):\n      embedding dimension\n  Kwargs:\n    rvals (iterable of float):\n      list of values for to use for r\n      (default: logarithmic_r(0.1 * std, 0.5 * std, 1.03))\n    dist (function (2d-array, 1d-array) -> 1d-array):\n      row-wise difference function\n    fit (str):\n      the fitting method to use for the line fit, either 'poly' for normal\n      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which\n      is more robust to outliers\n    debug_plot (boolean):\n      if True, a simple plot of the final line-fitting step will be shown\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n\n  Returns:\n    float:\n      correlation dimension as slope of the line fitted to log(r) vs log(C(r))\n    (1d-vector, 1d-vector, list):\n      only present if debug_data is True: debug data of the form\n      ``(rvals, csums, poly)`` where ``rvals`` are the values used for log(r), \n      ``csums`` are the corresponding log(C(r)) and ``poly`` are the line \n      coefficients (``[slope, intercept]``)\n  \"\"\"\n  data = np.asarray(data)\n\n  # TODO what are good values for r?\n  # TODO do this for multiple values of emb_dim?\n  if rvals is None:\n    sd = np.std(data)\n    rvals = logarithmic_r(0.1 * sd, 0.5 * sd, 1.03)\n  n = len(data)\n  orbit = delay_embedding(data, emb_dim, lag=1)\n  dists = np.array([dist(orbit, orbit[i]) for i in range(len(orbit))])\n  csums = []\n  for r in rvals:\n    s = 1.0 / (n * (n - 1)) * np.sum(dists < r)\n    csums.append(s)\n  csums = np.array(csums)\n  # filter zeros from csums\n  nonzero = np.where(csums != 0)\n  rvals = np.array(rvals)[nonzero]\n  csums = csums[nonzero]\n  if len(csums) == 0:\n    # all sums are zero => we cannot fit a line\n    poly = [np.nan, np.nan]\n  else:\n    poly = poly_fit(np.log(rvals), np.log(csums), 1)\n  if debug_plot:\n    plot_reg(np.log(rvals), np.log(csums), poly, \"log(r)\", \"log(C(r))\",\n             fname=plot_file)\n  if debug_data:\n    return (poly[0], (np.log(rvals), np.log(csums), poly))\n  else:\n    return poly[0]", "response": "Calculates the correlation dimension of a time series with Grassberger - Procaccia algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a detrended fluctuation analysis (DFA) on the given data Recommendations for parameter settings by Hardstone et al.: * nvals should be equally spaced on a logarithmic scale so that each window scale hase the same weight * min(nvals) < 4 does not make much sense as fitting a polynomial (even if it is only of order 1) to 3 or less data points is very prone. * max(nvals) > len(data) / 10 does not make much sense as we will then have less than 10 windows to calculate the average fluctuation * use overlap=True to obtain more windows and therefore better statistics (at an increased computational cost) Explanation of DFA: Detrended fluctuation analysis, much like the Hurst exponent, is used to find long-term statistical dependencies in time series. The idea behind DFA originates from the definition of self-affine processes. A process X is said to be self-affine if the standard deviation of the values within a window of length n changes with the window length factor L in a power law: std(X,L * n) = L^H * std(X, n) where std(X, k) is the standard deviation of the process X calculated over windows of size k. In this equation, H is called the Hurst parameter, which behaves indeed very similar to the Hurst exponent. Like the Hurst exponent, H can be obtained from a time series by calculating std(X,n) for different n and fitting a straight line to the plot of log(std(X,n)) versus log(n). To calculate a single std(X,n), the time series is split into windows of equal length n, so that the ith window of this size has the form W_(n,i) = [x_i, x_(i+1), x_(i+2), ... x_(i+n-1)] The value std(X,n) is then obtained by calculating std(W_(n,i)) for each i and averaging the obtained values over i. The aforementioned definition of self-affinity, however, assumes that the process is non-stationary (i.e. that the standard deviation changes over time) and it is highly influenced by local and global trends of the time series. To overcome these problems, an estimate alpha of H is calculated by using a \"walk\" or \"signal profile\" instead of the raw time series. This walk is obtained by substracting the mean and then taking the cumulative sum of the original time series. The local trends are removed for each window separately by fitting a polynomial p_(n,i) to the window W_(n,i) and then calculating W'_(n,i) = W_(n,i) - p_(n,i) (element-wise substraction). We then calculate std(X,n) as before only using the \"detrended\" window W'_(n,i) instead of W_(n,i). Instead of H we obtain the parameter alpha from the line fitting. For alpha < 1 the underlying process is stationary and can be modelled as fractional Gaussian noise with H = alpha. This means for alpha = 0.5 we have no correlation or \"memory\", for 0.5 < alpha < 1 we have a memory with positive correlation and for alpha < 0.5 the correlation is negative. For alpha > 1 the underlying process is non-stationary and can be modeled as fractional Brownian motion with H = alpha - 1. References: .. [dfa_1] C.-K. Peng, S. V. Buldyrev, S. Havlin, M. Simons, H. E. Stanley, and A. L. Goldberger, \u201cMosaic organization of DNA nucleotides,\u201d Physical Review E, vol. 49, no. 2, 1994. .. [dfa_2] R. Hardstone, S.-S. Poil, G. Schiavone, R. Jansen, V. V. Nikulin, H. D. Mansvelder, and K. Linkenkaer-Hansen, \u201cDetrended fluctuation analysis: A scale-free view on neuronal oscillations,\u201d Frontiers in Physiology, vol. 30, 2012. Reference code: .. [dfa_a] Peter Jurica, \"Introduction to MDFA in Python\", url: http://bsp.brain.riken.jp/~juricap/mdfa/mdfaintro.html .. [dfa_b] JE Mietus, \"dfa\", url: https://www.physionet.org/physiotools/dfa/dfa-1.htm .. [dfa_c] \"DFA\" function in R package \"fractal\" Args: data (array-like of float): time series Kwargs: nvals (iterable of int): subseries sizes at which to calculate fluctuation (default: logarithmic_n(4, 0.1*len(data), 1.2)) overlap (boolean): if True, the windows W_(n,i) will have a 50% overlap, otherwise non-overlapping windows will be used order (int): (polynomial) order of trend to remove fit_trend (str): the fitting method to use for fitting the trends, either 'poly' for normal least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which is more robust to outliers but also tends to lead to unstable results fit_exp (str): the fitting method to use for the line fit, either 'poly' for normal least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which is more robust to outliers debug_plot (boolean): if True, a simple plot of the final line-fitting step will be shown debug_data (boolean): if True, debugging data will be returned alongside the result plot_file (str): if debug_plot is True and plot_file is not None, the plot will be saved under the given file name instead of directly showing it through ``plt.show()`` Returns: float: the estimate alpha for the Hurst parameter (alpha < 1: stationary process similar to fractional Gaussian noise with H = alpha, alpha > 1: non-stationary process similar to fractional Brownian motion with H = alpha - 1) (1d-vector, 1d-vector, list): only present if debug_data is True: debug data of the form ``(nvals, fluctuations, poly)`` where ``nvals`` are the values used for log(n), ``fluctuations`` are the corresponding log(std(X,n)) and ``poly`` are the line coefficients (``[slope, intercept]``)", "response": "def dfa(data, nvals=None, overlap=True, order=1, fit_trend=\"poly\",\n        fit_exp=\"RANSAC\", debug_plot=False, debug_data=False, plot_file=None):\n  \"\"\"\n  Performs a detrended fluctuation analysis (DFA) on the given data\n\n  Recommendations for parameter settings by Hardstone et al.:\n    * nvals should be equally spaced on a logarithmic scale so that each window\n      scale hase the same weight\n    * min(nvals) < 4 does not make much sense as fitting a polynomial (even if\n      it is only of order 1) to 3 or less data points is very prone.\n    * max(nvals) > len(data) / 10 does not make much sense as we will then have\n      less than 10 windows to calculate the average fluctuation\n    * use overlap=True to obtain more windows and therefore better statistics\n      (at an increased computational cost)\n\n  Explanation of DFA:\n    Detrended fluctuation analysis, much like the Hurst exponent, is used to\n    find long-term statistical dependencies in time series.\n\n    The idea behind DFA originates from the definition of self-affine\n    processes. A process X is said to be self-affine if the standard deviation\n    of the values within a window of length n changes with the window length\n    factor L in a power law:\n\n    std(X,L * n) = L^H * std(X, n)\n\n    where std(X, k) is the standard deviation of the process X calculated over\n    windows of size k. In this equation, H is called the Hurst parameter, which\n    behaves indeed very similar to the Hurst exponent.\n\n    Like the Hurst exponent, H can be obtained from a time series by\n    calculating std(X,n) for different n and fitting a straight line to the\n    plot of log(std(X,n)) versus log(n).\n\n    To calculate a single std(X,n), the time series is split into windows of\n    equal length n, so that the ith window of this size has the form\n\n    W_(n,i) = [x_i, x_(i+1), x_(i+2), ... x_(i+n-1)]\n\n    The value std(X,n) is then obtained by calculating std(W_(n,i)) for each i\n    and averaging the obtained values over i.\n\n    The aforementioned definition of self-affinity, however, assumes that the\n    process is  non-stationary (i.e. that the standard deviation changes over\n    time) and it is highly influenced by local and global trends of the time\n    series.\n\n    To overcome these problems, an estimate alpha of H is calculated by using a\n    \"walk\" or \"signal profile\" instead of the raw time series. This walk is\n    obtained by substracting the mean and then taking the cumulative sum of the\n    original time series. The local trends are removed for each window\n    separately by fitting a polynomial p_(n,i) to the window W_(n,i) and then\n    calculating W'_(n,i) = W_(n,i) - p_(n,i) (element-wise substraction).\n\n    We then calculate std(X,n) as before only using the \"detrended\" window\n    W'_(n,i) instead of W_(n,i). Instead of H we obtain the parameter alpha\n    from the line fitting.\n\n    For alpha < 1 the underlying process is stationary and can be modelled as\n    fractional Gaussian noise with H = alpha. This means for alpha = 0.5 we\n    have no correlation or \"memory\", for 0.5 < alpha < 1 we have a memory with\n    positive correlation and for alpha < 0.5 the correlation is negative.\n\n    For alpha > 1 the underlying process is non-stationary and can be modeled\n    as fractional Brownian motion with H = alpha - 1.\n\n  References:\n    .. [dfa_1] C.-K. Peng, S. V. Buldyrev, S. Havlin, M. Simons,\n               H. E. Stanley, and A. L. Goldberger, \u201cMosaic organization of\n               DNA nucleotides,\u201d Physical Review E, vol. 49, no. 2, 1994.\n    .. [dfa_2] R. Hardstone, S.-S. Poil, G. Schiavone, R. Jansen,\n               V. V. Nikulin, H. D. Mansvelder, and K. Linkenkaer-Hansen,\n               \u201cDetrended fluctuation analysis: A scale-free view on neuronal\n               oscillations,\u201d Frontiers in Physiology, vol. 30, 2012.\n\n  Reference code:\n    .. [dfa_a] Peter Jurica, \"Introduction to MDFA in Python\",\n       url: http://bsp.brain.riken.jp/~juricap/mdfa/mdfaintro.html\n    .. [dfa_b] JE Mietus, \"dfa\",\n       url: https://www.physionet.org/physiotools/dfa/dfa-1.htm\n    .. [dfa_c] \"DFA\" function in R package \"fractal\"\n\n  Args:\n    data (array-like of float):\n      time series\n  Kwargs:\n    nvals (iterable of int):\n      subseries sizes at which to calculate fluctuation\n      (default: logarithmic_n(4, 0.1*len(data), 1.2))\n    overlap (boolean):\n      if True, the windows W_(n,i) will have a 50% overlap,\n      otherwise non-overlapping windows will be used\n    order (int):\n      (polynomial) order of trend to remove\n    fit_trend (str):\n      the fitting method to use for fitting the trends, either 'poly'\n      for normal least squares polynomial fitting or 'RANSAC' for\n      RANSAC-fitting which is more robust to outliers but also tends to\n      lead to unstable results\n    fit_exp (str):\n      the fitting method to use for the line fit, either 'poly' for normal\n      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which\n      is more robust to outliers\n    debug_plot (boolean):\n      if True, a simple plot of the final line-fitting step will be shown\n    debug_data (boolean):\n      if True, debugging data will be returned alongside the result\n    plot_file (str):\n      if debug_plot is True and plot_file is not None, the plot will be saved\n      under the given file name instead of directly showing it through\n      ``plt.show()``\n  Returns:\n    float:\n      the estimate alpha for the Hurst parameter (alpha < 1: stationary\n      process similar to fractional Gaussian noise with H = alpha,\n      alpha > 1: non-stationary process similar to fractional Brownian\n      motion with H = alpha - 1)\n    (1d-vector, 1d-vector, list):\n      only present if debug_data is True: debug data of the form\n      ``(nvals, fluctuations, poly)`` where ``nvals`` are the values used for\n      log(n), ``fluctuations`` are the corresponding log(std(X,n)) and ``poly``\n      are the line coefficients (``[slope, intercept]``)\n  \"\"\"\n  data = np.asarray(data)\n  total_N = len(data)\n  if nvals is None:\n    if total_N > 70:\n      nvals = logarithmic_n(4, 0.1 * total_N, 1.2)\n    elif total_N > 10:\n      nvals = [4, 5, 6, 7, 8, 9]\n    else:\n      nvals = [total_N-2, total_N-1]\n      msg = \"choosing nvals = {} , DFA with less than ten data points is \" \\\n          + \"extremely unreliable\"\n      warnings.warn(msg.format(nvals),RuntimeWarning)\n  if len(nvals) < 2:\n    raise ValueError(\"at least two nvals are needed\")\n  if np.min(nvals) < 2:\n    raise ValueError(\"nvals must be at least two\")\n  if np.max(nvals) >= total_N:\n    raise ValueError(\"nvals cannot be larger than the input size\")\n  # create the signal profile\n  # (cumulative sum of deviations from the mean => \"walk\")\n  walk = np.cumsum(data - np.mean(data))\n  fluctuations = []\n  for n in nvals:\n    assert n >= 2\n    # subdivide data into chunks of size n\n    if overlap:\n      # step size n/2 instead of n\n      d = np.array([walk[i:i + n] for i in range(0, len(walk) - n, n // 2)])\n    else:\n      # non-overlapping windows => we can simply do a reshape\n      d = walk[:total_N - (total_N % n)]\n      d = d.reshape((total_N // n, n))\n    # calculate local trends as polynomes\n    x = np.arange(n)\n    tpoly = [poly_fit(x, d[i], order, fit=fit_trend)\n             for i in range(len(d))]\n    tpoly = np.array(tpoly)\n    trend = np.array([np.polyval(tpoly[i], x) for i in range(len(d))])\n    # calculate standard deviation (\"fluctuation\") of walks in d around trend\n    flucs = np.sqrt(np.sum((d - trend) ** 2, axis=1) / n)\n    # calculate mean fluctuation over all subsequences\n    f_n = np.sum(flucs) / len(flucs)\n    fluctuations.append(f_n)\n  fluctuations = np.array(fluctuations)\n  # filter zeros from fluctuations\n  nonzero = np.where(fluctuations != 0)\n  nvals = np.array(nvals)[nonzero]\n  fluctuations = fluctuations[nonzero]\n  if len(fluctuations) == 0:\n    # all fluctuations are zero => we cannot fit a line\n    poly = [np.nan, np.nan]\n  else:\n    poly = poly_fit(np.log(nvals), np.log(fluctuations), 1,\n                    fit=fit_exp)\n  if debug_plot:\n    plot_reg(np.log(nvals), np.log(fluctuations), poly, \"log(n)\", \"std(X,n)\",\n             fname=plot_file)\n  if debug_data:\n    return (poly[0], (np.log(nvals), np.log(fluctuations), poly))\n  else:\n    return poly[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck HTTP response status.", "response": "def check_status(status, expected, path, headers=None,\n                 resp_headers=None, body=None, extras=None):\n  \"\"\"Check HTTP response status is expected.\n\n  Args:\n    status: HTTP response status. int.\n    expected: a list of expected statuses. A list of ints.\n    path: filename or a path prefix.\n    headers: HTTP request headers.\n    resp_headers: HTTP response headers.\n    body: HTTP response body.\n    extras: extra info to be logged verbatim if error occurs.\n\n  Raises:\n    AuthorizationError: if authorization failed.\n    NotFoundError: if an object that's expected to exist doesn't.\n    TimeoutError: if HTTP request timed out.\n    ServerError: if server experienced some errors.\n    FatalError: if any other unexpected errors occurred.\n  \"\"\"\n  if status in expected:\n    return\n\n  msg = ('Expect status %r from Google Storage. But got status %d.\\n'\n         'Path: %r.\\n'\n         'Request headers: %r.\\n'\n         'Response headers: %r.\\n'\n         'Body: %r.\\n'\n         'Extra info: %r.\\n' %\n         (expected, status, path, headers, resp_headers, body, extras))\n\n  if status == httplib.UNAUTHORIZED:\n    raise AuthorizationError(msg)\n  elif status == httplib.FORBIDDEN:\n    raise ForbiddenError(msg)\n  elif status == httplib.NOT_FOUND:\n    raise NotFoundError(msg)\n  elif status == httplib.REQUEST_TIMEOUT:\n    raise TimeoutError(msg)\n  elif status == httplib.REQUESTED_RANGE_NOT_SATISFIABLE:\n    raise InvalidRange(msg)\n  elif (status == httplib.OK and 308 in expected and\n        httplib.OK not in expected):\n    raise FileClosedError(msg)\n  elif status >= 500:\n    raise ServerError(msg)\n  else:\n    raise FatalError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget default RetryParams for current request and current thread.", "response": "def _get_default_retry_params():\n  \"\"\"Get default RetryParams for current request and current thread.\n\n  Returns:\n    A new instance of the default RetryParams.\n  \"\"\"\n  default = getattr(_thread_local_settings, 'default_retry_params', None)\n  if default is None or not default.belong_to_current_request():\n    return RetryParams()\n  else:\n    return copy.copy(default)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _should_retry(resp):\n  return (resp.status_code == httplib.REQUEST_TIMEOUT or\n          (resp.status_code >= 500 and\n           resp.status_code < 600))", "response": "Given a urlfetch response decide whether to retry that request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _eager_tasklet(tasklet):\n\n  @utils.wrapping(tasklet)\n  def eager_wrapper(*args, **kwds):\n    fut = tasklet(*args, **kwds)\n    _run_until_rpc()\n    return fut\n\n  return eager_wrapper", "response": "Decorator to turn tasklet to run eagerly."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, tasklet, **kwds):\n    start_time = time.time()\n    n = 1\n\n    while True:\n      e = None\n      result = None\n      got_result = False\n\n      try:\n        result = yield tasklet(**kwds)\n        got_result = True\n        if not self.should_retry(result):\n          raise ndb.Return(result)\n      except runtime.DeadlineExceededError:\n        logging.debug(\n            'Tasklet has exceeded request deadline after %s seconds total',\n            time.time() - start_time)\n        raise\n      except self.retriable_exceptions as e:\n        pass\n\n      if n == 1:\n        logging.debug('Tasklet is %r', tasklet)\n\n      delay = self.retry_params.delay(n, start_time)\n\n      if delay <= 0:\n        logging.debug(\n            'Tasklet failed after %s attempts and %s seconds in total',\n            n, time.time() - start_time)\n        if got_result:\n          raise ndb.Return(result)\n        elif e is not None:\n          raise e\n        else:\n          assert False, 'Should never reach here.'\n\n      if got_result:\n        logging.debug(\n            'Got result %r from tasklet.', result)\n      else:\n        logging.debug(\n            'Got exception \"%r\" from tasklet.', e)\n      logging.debug('Retry in %s seconds.', delay)\n      n += 1\n      yield tasklets.sleep(delay)", "response": "A generator that yields a tasklet with retry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the init arguments.", "response": "def _check(cls, name, val, can_be_zero=False, val_type=float):\n    \"\"\"Check init arguments.\n\n    Args:\n      name: name of the argument. For logging purpose.\n      val: value. Value has to be non negative number.\n      can_be_zero: whether value can be zero.\n      val_type: Python type of the value.\n\n    Returns:\n      The value.\n\n    Raises:\n      ValueError: when invalid value is passed in.\n      TypeError: when invalid value type is passed in.\n    \"\"\"\n    valid_types = [val_type]\n    if val_type is float:\n      valid_types.append(int)\n\n    if type(val) not in valid_types:\n      raise TypeError(\n          'Expect type %s for parameter %s' % (val_type.__name__, name))\n    if val < 0:\n      raise ValueError(\n          'Value for parameter %s has to be greater than 0' % name)\n    if not can_be_zero and val == 0:\n      raise ValueError(\n          'Value for parameter %s can not be 0' % name)\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating delay before the next retry.", "response": "def delay(self, n, start_time):\n    \"\"\"Calculate delay before the next retry.\n\n    Args:\n      n: the number of current attempt. The first attempt should be 1.\n      start_time: the time when retry started in unix time.\n\n    Returns:\n      Number of seconds to wait before next retry. -1 if retry should give up.\n    \"\"\"\n    if (n > self.max_retries or\n        (n > self.min_retries and\n         time.time() - start_time > self.max_retry_period)):\n      return -1\n    return min(\n        math.pow(self.backoff_factor, n-1) * self.initial_delay,\n        self.max_delay)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nopens a Google Cloud Storage file and returns it as a File - like object.", "response": "def open(filename,\n         mode='r',\n         content_type=None,\n         options=None,\n         read_buffer_size=storage_api.ReadBuffer.DEFAULT_BUFFER_SIZE,\n         retry_params=None,\n         _account_id=None,\n         offset=0):\n  \"\"\"Opens a Google Cloud Storage file and returns it as a File-like object.\n\n  Args:\n    filename: A Google Cloud Storage filename of form '/bucket/filename'.\n    mode: 'r' for reading mode. 'w' for writing mode.\n      In reading mode, the file must exist. In writing mode, a file will\n      be created or be overrode.\n    content_type: The MIME type of the file. str. Only valid in writing mode.\n    options: A str->basestring dict to specify additional headers to pass to\n      GCS e.g. {'x-goog-acl': 'private', 'x-goog-meta-foo': 'foo'}.\n      Supported options are x-goog-acl, x-goog-meta-, cache-control,\n      content-disposition, and content-encoding.\n      Only valid in writing mode.\n      See https://developers.google.com/storage/docs/reference-headers\n      for details.\n    read_buffer_size: The buffer size for read. Read keeps a buffer\n      and prefetches another one. To minimize blocking for large files,\n      always read by buffer size. To minimize number of RPC requests for\n      small files, set a large buffer size. Max is 30MB.\n    retry_params: An instance of api_utils.RetryParams for subsequent calls\n      to GCS from this file handle. If None, the default one is used.\n    _account_id: Internal-use only.\n    offset: Number of bytes to skip at the start of the file. If None, 0 is\n      used.\n\n  Returns:\n    A reading or writing buffer that supports File-like interface. Buffer\n    must be closed after operations are done.\n\n  Raises:\n    errors.AuthorizationError: if authorization failed.\n    errors.NotFoundError: if an object that's expected to exist doesn't.\n    ValueError: invalid open mode or if content_type or options are specified\n      in reading mode.\n  \"\"\"\n  common.validate_file_path(filename)\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n  filename = api_utils._quote_filename(filename)\n\n  if mode == 'w':\n    common.validate_options(options)\n    return storage_api.StreamingBuffer(api, filename, content_type, options)\n  elif mode == 'r':\n    if content_type or options:\n      raise ValueError('Options and content_type can only be specified '\n                       'for writing mode.')\n    return storage_api.ReadBuffer(api,\n                                  filename,\n                                  buffer_size=read_buffer_size,\n                                  offset=offset)\n  else:\n    raise ValueError('Invalid mode %s.' % mode)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(filename, retry_params=None, _account_id=None):\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n  common.validate_file_path(filename)\n  filename = api_utils._quote_filename(filename)\n  status, resp_headers, content = api.delete_object(filename)\n  errors.check_status(status, [204], filename, resp_headers=resp_headers,\n                      body=content)", "response": "Delete a Google Cloud Storage file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the location for the given bucket.", "response": "def get_location(bucket, retry_params=None, _account_id=None):\n  \"\"\"Returns the location for the given bucket.\n\n  https://cloud.google.com/storage/docs/bucket-locations\n\n  Args:\n    bucket: A Google Cloud Storage bucket of form '/bucket'.\n    retry_params: An api_utils.RetryParams for this call to GCS. If None,\n      the default one is used.\n    _account_id: Internal-use only.\n\n  Returns:\n    The location as a string.\n\n  Raises:\n    errors.AuthorizationError: if authorization failed.\n    errors.NotFoundError: if the bucket does not exist.\n  \"\"\"\n\n  return _get_bucket_attribute(bucket,\n                               'location',\n                               'LocationConstraint',\n                               retry_params=retry_params,\n                               _account_id=_account_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_storage_class(bucket, retry_params=None, _account_id=None):\n\n  return _get_bucket_attribute(bucket,\n                               'storageClass',\n                               'StorageClass',\n                               retry_params=retry_params,\n                               _account_id=_account_id)", "response": "Returns the storage class for the given bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_bucket_attribute(bucket,\n                          query_param,\n                          xml_response_tag,\n                          retry_params=None,\n                          _account_id=None):\n  \"\"\"Helper method to request a bucket parameter and parse the response.\n\n  Args:\n    bucket: A Google Cloud Storage bucket of form '/bucket'.\n    query_param: The query parameter to include in the get bucket request.\n    xml_response_tag: The expected tag in the xml response.\n    retry_params: An api_utils.RetryParams for this call to GCS. If None,\n      the default one is used.\n    _account_id: Internal-use only.\n\n  Returns:\n    The xml value as a string.  None if the returned xml does not match expected\n    format.\n\n  Raises:\n    errors.AuthorizationError: if authorization failed.\n    errors.NotFoundError: if the bucket does not exist.\n  \"\"\"\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n  common.validate_bucket_path(bucket)\n  status, headers, content = api.get_bucket('%s?%s' % (bucket, query_param))\n\n  errors.check_status(status, [200], bucket, resp_headers=headers, body=content)\n\n  root = ET.fromstring(content)\n  if root.tag == xml_response_tag and root.text:\n    return root.text\n  return None", "response": "Helper method to request a bucket parameter and parse the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stat(filename, retry_params=None, _account_id=None):\n  common.validate_file_path(filename)\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n  status, headers, content = api.head_object(\n      api_utils._quote_filename(filename))\n  errors.check_status(status, [200], filename, resp_headers=headers,\n                      body=content)\n  file_stat = common.GCSFileStat(\n      filename=filename,\n      st_size=common.get_stored_content_length(headers),\n      st_ctime=common.http_time_to_posix(headers.get('last-modified')),\n      etag=headers.get('etag'),\n      content_type=headers.get('content-type'),\n      metadata=common.get_metadata(headers))\n\n  return file_stat", "response": "Get GCSFileStat of a Google Cloud Storage file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying the file content from src to dst.", "response": "def copy2(src, dst, metadata=None, retry_params=None):\n  \"\"\"Copy the file content from src to dst.\n\n  Args:\n    src: /bucket/filename\n    dst: /bucket/filename\n    metadata: a dict of metadata for this copy. If None, old metadata is copied.\n      For example, {'x-goog-meta-foo': 'bar'}.\n    retry_params: An api_utils.RetryParams for this call to GCS. If None,\n      the default one is used.\n\n  Raises:\n    errors.AuthorizationError: if authorization failed.\n    errors.NotFoundError: if an object that's expected to exist doesn't.\n  \"\"\"\n  common.validate_file_path(src)\n  common.validate_file_path(dst)\n\n  if metadata is None:\n    metadata = {}\n    copy_meta = 'COPY'\n  else:\n    copy_meta = 'REPLACE'\n  metadata.update({'x-goog-copy-source': src,\n                   'x-goog-metadata-directive': copy_meta})\n\n  api = storage_api._get_storage_api(retry_params=retry_params)\n  status, resp_headers, content = api.put_object(\n      api_utils._quote_filename(dst), headers=metadata)\n  errors.check_status(status, [200], src, metadata, resp_headers, body=content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listbucket(path_prefix, marker=None, prefix=None, max_keys=None,\n               delimiter=None, retry_params=None, _account_id=None):\n  \"\"\"Returns a GCSFileStat iterator over a bucket.\n\n  Optional arguments can limit the result to a subset of files under bucket.\n\n  This function has two modes:\n  1. List bucket mode: Lists all files in the bucket without any concept of\n     hierarchy. GCS doesn't have real directory hierarchies.\n  2. Directory emulation mode: If you specify the 'delimiter' argument,\n     it is used as a path separator to emulate a hierarchy of directories.\n     In this mode, the \"path_prefix\" argument should end in the delimiter\n     specified (thus designates a logical directory). The logical directory's\n     contents, both files and subdirectories, are listed. The names of\n     subdirectories returned will end with the delimiter. So listbucket\n     can be called with the subdirectory name to list the subdirectory's\n     contents.\n\n  Args:\n    path_prefix: A Google Cloud Storage path of format \"/bucket\" or\n      \"/bucket/prefix\". Only objects whose fullpath starts with the\n      path_prefix will be returned.\n    marker: Another path prefix. Only objects whose fullpath starts\n      lexicographically after marker will be returned (exclusive).\n    prefix: Deprecated. Use path_prefix.\n    max_keys: The limit on the number of objects to return. int.\n      For best performance, specify max_keys only if you know how many objects\n      you want. Otherwise, this method requests large batches and handles\n      pagination for you.\n    delimiter: Use to turn on directory mode. str of one or multiple chars\n      that your bucket uses as its directory separator.\n    retry_params: An api_utils.RetryParams for this call to GCS. If None,\n      the default one is used.\n    _account_id: Internal-use only.\n\n  Examples:\n    For files \"/bucket/a\",\n              \"/bucket/bar/1\"\n              \"/bucket/foo\",\n              \"/bucket/foo/1\", \"/bucket/foo/2/1\", \"/bucket/foo/3/1\",\n\n    Regular mode:\n    listbucket(\"/bucket/f\", marker=\"/bucket/foo/1\")\n    will match \"/bucket/foo/2/1\", \"/bucket/foo/3/1\".\n\n    Directory mode:\n    listbucket(\"/bucket/\", delimiter=\"/\")\n    will match \"/bucket/a, \"/bucket/bar/\" \"/bucket/foo\", \"/bucket/foo/\".\n    listbucket(\"/bucket/foo/\", delimiter=\"/\")\n    will match \"/bucket/foo/1\", \"/bucket/foo/2/\", \"/bucket/foo/3/\"\n\n  Returns:\n    Regular mode:\n    A GCSFileStat iterator over matched files ordered by filename.\n    The iterator returns GCSFileStat objects. filename, etag, st_size,\n    st_ctime, and is_dir are set.\n\n    Directory emulation mode:\n    A GCSFileStat iterator over matched files and directories ordered by\n    name. The iterator returns GCSFileStat objects. For directories,\n    only the filename and is_dir fields are set.\n\n    The last name yielded can be used as next call's marker.\n  \"\"\"\n  if prefix:\n    common.validate_bucket_path(path_prefix)\n    bucket = path_prefix\n  else:\n    bucket, prefix = common._process_path_prefix(path_prefix)\n\n  if marker and marker.startswith(bucket):\n    marker = marker[len(bucket) + 1:]\n\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n  options = {}\n  if marker:\n    options['marker'] = marker\n  if max_keys:\n    options['max-keys'] = max_keys\n  if prefix:\n    options['prefix'] = prefix\n  if delimiter:\n    options['delimiter'] = delimiter\n\n  return _Bucket(api, bucket, options)", "response": "A simple method to list all files in a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the GCS Compose on the given files. Merges between 2 and 32 files into one file. Composite files may even be built from other existing composites, provided that the total component count does not exceed 1024. See here for details: https://cloud.google.com/storage/docs/composite-objects Args: list_of_files: List of file name strings with no leading slashes or bucket. destination_file: Path to the output file. Must have the bucket in the path. files_metadata: Optional, file metadata, order must match list_of_files, see link for available options: https://cloud.google.com/storage/docs/composite-objects#_Xml content_type: Optional, used to specify content-header of the output file. retry_params: Optional, an api_utils.RetryParams for this call to GCS. If None,the default one is used. _account_id: Internal-use only. Raises: ValueError: If the number of files is outside the range of 2-32.", "response": "def compose(list_of_files, destination_file, files_metadata=None,\n            content_type=None, retry_params=None, _account_id=None):\n  \"\"\"Runs the GCS Compose on the given files.\n\n  Merges between 2 and 32 files into one file. Composite files may even\n  be built from other existing composites, provided that the total\n  component count does not exceed 1024. See here for details:\n  https://cloud.google.com/storage/docs/composite-objects\n\n  Args:\n    list_of_files: List of file name strings with no leading slashes or bucket.\n    destination_file: Path to the output file. Must have the bucket in the path.\n    files_metadata: Optional, file metadata, order must match list_of_files,\n      see link for available options:\n      https://cloud.google.com/storage/docs/composite-objects#_Xml\n    content_type: Optional, used to specify content-header of the output file.\n    retry_params: Optional, an api_utils.RetryParams for this call to GCS.\n      If None,the default one is used.\n    _account_id: Internal-use only.\n\n  Raises:\n    ValueError: If the number of files is outside the range of 2-32.\n  \"\"\"\n  api = storage_api._get_storage_api(retry_params=retry_params,\n                                     account_id=_account_id)\n\n\n  if os.getenv('SERVER_SOFTWARE').startswith('Dev'):\n    def _temp_func(file_list, destination_file, content_type):\n      bucket = '/' + destination_file.split('/')[1] + '/'\n      with open(destination_file, 'w', content_type=content_type) as gcs_merge:\n        for source_file in file_list:\n          with open(bucket + source_file['Name'], 'r') as gcs_source:\n            gcs_merge.write(gcs_source.read())\n\n    compose_object = _temp_func\n  else:\n    compose_object = api.compose_object\n  file_list, _ = _validate_compose_list(destination_file,\n                                        list_of_files,\n                                        files_metadata, 32)\n  compose_object(file_list, destination_file, content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_compose_list(destination_file, file_list,\n                           files_metadata=None, number_of_files=32):\n  \"\"\"Validates the file_list and merges the file_list, files_metadata.\n\n  Args:\n    destination: Path to the file (ie. /destination_bucket/destination_file).\n    file_list: List of files to compose, see compose for details.\n    files_metadata: Meta details for each file in the file_list.\n    number_of_files: Maximum number of files allowed in the list.\n\n  Returns:\n    A tuple (list_of_files, bucket):\n      list_of_files: Ready to use dict version of the list.\n      bucket: bucket name extracted from the file paths.\n  \"\"\"\n  common.validate_file_path(destination_file)\n  bucket = destination_file[0:(destination_file.index('/', 1) + 1)]\n  try:\n    if isinstance(file_list, types.StringTypes):\n      raise TypeError\n    list_len = len(file_list)\n  except TypeError:\n    raise TypeError('file_list must be a list')\n\n  if list_len > number_of_files:\n    raise ValueError(\n          'Compose attempted to create composite with too many'\n           '(%i) components; limit is (%i).' % (list_len, number_of_files))\n  if list_len <= 0:\n    raise ValueError('Compose operation requires at'\n                     ' least one component; 0 provided.')\n\n  if files_metadata is None:\n    files_metadata = []\n  elif len(files_metadata) > list_len:\n    raise ValueError('files_metadata contains more entries(%i)'\n                     ' than file_list(%i)'\n                     % (len(files_metadata), list_len))\n  list_of_files = []\n  for source_file, meta_data in itertools.izip_longest(file_list,\n                                                       files_metadata):\n    if not isinstance(source_file, str):\n      raise TypeError('Each item of file_list must be a string')\n    if source_file.startswith('/'):\n      logging.warn('Detected a \"/\" at the start of the file, '\n                   'Unless the file name contains a \"/\" it '\n                   ' may cause files to be misread')\n    if source_file.startswith(bucket):\n      logging.warn('Detected bucket name at the start of the file, '\n                   'must not specify the bucket when listing file_names.'\n                   ' May cause files to be misread')\n    common.validate_file_path(bucket + source_file)\n\n    list_entry = {}\n\n    if meta_data is not None:\n      list_entry.update(meta_data)\n    list_entry['Name'] = source_file\n    list_of_files.append(list_entry)\n\n  return list_of_files, bucket", "response": "Validates the file_list and merges the file_list with the files_metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the request should be made to another GET bucket call.", "response": "def _should_get_another_batch(self, content):\n    \"\"\"Whether to issue another GET bucket call.\n\n    Args:\n      content: response XML.\n\n    Returns:\n      True if should, also update self._options for the next request.\n      False otherwise.\n    \"\"\"\n    if ('max-keys' in self._options and\n        self._options['max-keys'] <= common._MAX_GET_BUCKET_RESULT):\n      return False\n\n    elements = self._find_elements(\n        content, set([common._T_IS_TRUNCATED,\n                      common._T_NEXT_MARKER]))\n    if elements.get(common._T_IS_TRUNCATED, 'false').lower() != 'true':\n      return False\n\n    next_marker = elements.get(common._T_NEXT_MARKER)\n    if next_marker is None:\n      self._options.pop('marker', None)\n      return False\n    self._options['marker'] = next_marker\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_elements(self, result, elements):\n    element_mapping = {}\n    result = StringIO.StringIO(result)\n    for _, e in ET.iterparse(result, events=('end',)):\n      if not elements:\n        break\n      if e.tag in elements:\n        element_mapping[e.tag] = e.text\n        elements.remove(e.tag)\n    return element_mapping", "response": "Find interesting elements from XML."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_file(self, filename):\n    self.response.write('Creating file %s\\n' % filename)\n\n    write_retry_params = gcs.RetryParams(backoff_factor=1.1)\n    gcs_file = gcs.open(filename,\n                        'w',\n                        content_type='text/plain',\n                        options={'x-goog-meta-foo': 'foo',\n                                 'x-goog-meta-bar': 'bar'},\n                        retry_params=write_retry_params)\n    gcs_file.write('abcde\\n')\n    gcs_file.write('f'*1024*4 + '\\n')\n    gcs_file.close()\n    self.tmp_filenames_to_clean_up.append(filename)", "response": "Create a file.\n\n    The retry_params specified in the open call will override the default\n    retry params for this particular file handle.\n\n    Args:\n      filename: filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_bucket(self, bucket):\n    self.response.write('Listbucket result:\\n')\n\n    page_size = 1\n    stats = gcs.listbucket(bucket + '/foo', max_keys=page_size)\n    while True:\n      count = 0\n      for stat in stats:\n        count += 1\n        self.response.write(repr(stat))\n        self.response.write('\\n')\n\n      if count != page_size or count == 0:\n        break\n      stats = gcs.listbucket(bucket + '/foo', max_keys=page_size,\n                             marker=stat.filename)", "response": "Create several files and paginate through them."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateFile(filename):\n  with gcs.open(filename, 'w') as f:\n    f.write('abcde\\n')\n\n  blobstore_filename = '/gs' + filename\n  return blobstore.create_gs_key(blobstore_filename)", "response": "Create a GCS file with GCS client lib."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a fresh authentication token. Args: scopes: A list of scopes. service_account_id: Internal-use only. Raises: An ndb.Return with a tuple (token, expiration_time) where expiration_time is seconds since the epoch.", "response": "def _make_token_async(scopes, service_account_id):\n  \"\"\"Get a fresh authentication token.\n\n  Args:\n    scopes: A list of scopes.\n    service_account_id: Internal-use only.\n\n  Raises:\n    An ndb.Return with a tuple (token, expiration_time) where expiration_time is\n    seconds since the epoch.\n  \"\"\"\n  rpc = app_identity.create_rpc()\n  app_identity.make_get_access_token_call(rpc, scopes, service_account_id)\n  token, expires_at = yield rpc\n  raise ndb.Return((token, expires_at))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a function that returns a Future that will be returned when the synchronous method is called.", "response": "def _make_sync_method(name):\n  \"\"\"Helper to synthesize a synchronous method from an async method name.\n\n  Used by the @add_sync_methods class decorator below.\n\n  Args:\n    name: The name of the synchronous method.\n\n  Returns:\n    A method (with first argument 'self') that retrieves and calls\n    self.<name>, passing its own arguments, expects it to return a\n    Future, and then waits for and returns that Future's result.\n  \"\"\"\n\n  def sync_wrapper(self, *args, **kwds):\n    method = getattr(self, name)\n    future = method(*args, **kwds)\n    return future.get_result()\n\n  return sync_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclasses decorator to add synchronous methods corresponding to async methods.", "response": "def add_sync_methods(cls):\n  \"\"\"Class decorator to add synchronous methods corresponding to async methods.\n\n  This modifies the class in place, adding additional methods to it.\n  If a synchronous method of a given name already exists it is not\n  replaced.\n\n  Args:\n    cls: A class.\n\n  Returns:\n    The same class, modified in place.\n  \"\"\"\n  for name in cls.__dict__.keys():\n    if name.endswith('_async'):\n      sync_name = name[:-6]\n      if not hasattr(cls, sync_name):\n        setattr(cls, sync_name, _make_sync_method(name))\n  return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_request_async(self, url, method='GET', headers=None, payload=None,\n                       deadline=None, callback=None):\n    \"\"\"Issue one HTTP request.\n\n    It performs async retries using tasklets.\n\n    Args:\n      url: the url to fetch.\n      method: the method in which to fetch.\n      headers: the http headers.\n      payload: the data to submit in the fetch.\n      deadline: the deadline in which to make the call.\n      callback: the call to make once completed.\n\n    Yields:\n      The async fetch of the url.\n    \"\"\"\n    retry_wrapper = api_utils._RetryWrapper(\n        self.retry_params,\n        retriable_exceptions=api_utils._RETRIABLE_EXCEPTIONS,\n        should_retry=api_utils._should_retry)\n    resp = yield retry_wrapper.run(\n        self.urlfetch_async,\n        url=url,\n        method=method,\n        headers=headers,\n        payload=payload,\n        deadline=deadline,\n        callback=callback,\n        follow_redirects=False)\n    raise ndb.Return((resp.status_code, resp.headers, resp.content))", "response": "Issue one HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget an authentication token.", "response": "def get_token_async(self, refresh=False):\n    \"\"\"Get an authentication token.\n\n    The token is cached in memcache, keyed by the scopes argument.\n    Uses a random token expiration headroom value generated in the constructor\n    to eliminate a burst of GET_ACCESS_TOKEN API requests.\n\n    Args:\n      refresh: If True, ignore a cached token; default False.\n\n    Yields:\n      An authentication token. This token is guaranteed to be non-expired.\n    \"\"\"\n    key = '%s,%s' % (self.service_account_id, ','.join(self.scopes))\n    ts = yield _AE_TokenStorage_.get_by_id_async(\n        key,\n        use_cache=True,\n        use_memcache=self.retry_params.memcache_access_token,\n        use_datastore=self.retry_params.save_access_token)\n    if refresh or ts is None or ts.expires < (\n        time.time() + self.expiration_headroom):\n      token, expires_at = yield self.make_token_async(\n          self.scopes, self.service_account_id)\n      timeout = int(expires_at - time.time())\n      ts = _AE_TokenStorage_(id=key, token=token, expires=expires_at)\n      if timeout > 0:\n        yield ts.put_async(memcache_timeout=timeout,\n                           use_datastore=self.retry_params.save_access_token,\n                           force_writes=True,\n                           use_cache=True,\n                           use_memcache=self.retry_params.memcache_access_token)\n    raise ndb.Return(ts.token)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef urlfetch_async(self, url, method='GET', headers=None,\n                     payload=None, deadline=None, callback=None,\n                     follow_redirects=False):\n    \"\"\"Make an async urlfetch() call.\n\n    This is an async wrapper around urlfetch(). It adds an authentication\n    header.\n\n    Args:\n      url: the url to fetch.\n      method: the method in which to fetch.\n      headers: the http headers.\n      payload: the data to submit in the fetch.\n      deadline: the deadline in which to make the call.\n      callback: the call to make once completed.\n      follow_redirects: whether or not to follow redirects.\n\n    Yields:\n      This returns a Future despite not being decorated with @ndb.tasklet!\n    \"\"\"\n    headers = {} if headers is None else dict(headers)\n    headers.update(self.user_agent)\n    try:\n      self.token = yield self.get_token_async()\n    except app_identity.InternalError, e:\n      if os.environ.get('DATACENTER', '').endswith('sandman'):\n        self.token = None\n        logging.warning('Could not fetch an authentication token in sandman '\n                     'based Appengine devel setup; proceeding without one.')\n      else:\n        raise e\n    if self.token:\n      headers['authorization'] = 'OAuth ' + self.token\n\n    deadline = deadline or self.retry_params.urlfetch_timeout\n\n    ctx = ndb.get_context()\n    resp = yield ctx.urlfetch(\n        url, payload=payload, method=method,\n        headers=headers, follow_redirects=follow_redirects,\n        deadline=deadline, callback=callback)\n    raise ndb.Return(resp)", "response": "A wrapper around urlfetch that adds an authentication token to the headers and returns a Future that will be completed when the request is complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stored_content_length(headers):\n  length = headers.get('x-goog-stored-content-length')\n  if length is None:\n    length = headers.get('content-length')\n  return length", "response": "Returns the content length of the object as stored in GCS."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting user defined options from HTTP response headers.", "response": "def get_metadata(headers):\n  \"\"\"Get user defined options from HTTP response headers.\"\"\"\n  return dict((k, v) for k, v in headers.iteritems()\n              if any(k.lower().startswith(valid) for valid in _GCS_METADATA))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate and process a Google Cloud Stoarge path prefix.", "response": "def _process_path_prefix(path_prefix):\n  \"\"\"Validate and process a Google Cloud Stoarge path prefix.\n\n  Args:\n    path_prefix: a Google Cloud Storage path prefix of format '/bucket/prefix'\n      or '/bucket/' or '/bucket'.\n\n  Raises:\n    ValueError: if path is invalid.\n\n  Returns:\n    a tuple of /bucket and prefix. prefix can be None.\n  \"\"\"\n  _validate_path(path_prefix)\n  if not _GCS_PATH_PREFIX_REGEX.match(path_prefix):\n    raise ValueError('Path prefix should have format /bucket, /bucket/, '\n                     'or /bucket/prefix but got %s.' % path_prefix)\n  bucket_name_end = path_prefix.find('/', 1)\n  bucket = path_prefix\n  prefix = None\n  if bucket_name_end != -1:\n    bucket = path_prefix[:bucket_name_end]\n    prefix = path_prefix[bucket_name_end + 1:] or None\n  return bucket, prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_path(path):\n  if not path:\n    raise ValueError('Path is empty')\n  if not isinstance(path, basestring):\n    raise TypeError('Path should be a string but is %s (%s).' %\n                    (path.__class__, path))", "response": "Basic validation of Google Storage paths."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_options(options):\n  if not options:\n    return\n\n  for k, v in options.iteritems():\n    if not isinstance(k, str):\n      raise TypeError('option %r should be a str.' % k)\n    if not any(k.lower().startswith(valid) for valid in _GCS_OPTIONS):\n      raise ValueError('option %s is not supported.' % k)\n    if not isinstance(v, basestring):\n      raise TypeError('value %r for option %s should be of type basestring.' %\n                      (v, k))", "response": "Validate Google Cloud Storage options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dt_str_to_posix(dt_str):\n  parsable, _ = dt_str.split('.')\n  dt = datetime.datetime.strptime(parsable, _DT_FORMAT)\n  return calendar.timegm(dt.utctimetuple())", "response": "format str to posix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef posix_to_dt_str(posix):\n  dt = datetime.datetime.utcfromtimestamp(posix)\n  dt_str = dt.strftime(_DT_FORMAT)\n  return dt_str + '.000Z'", "response": "Reverse of str_to_datetime. posix_to_dt_str."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef local_run():\n  server_software = os.environ.get('SERVER_SOFTWARE')\n  if server_software is None:\n    return True\n  if 'remote_api' in server_software:\n    return False\n  if server_software.startswith(('Development', 'testutil')):\n    return True\n  return False", "response": "Whether we should hit GCS dev appserver stub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef memory_usage(method):\n  def wrapper(*args, **kwargs):\n    logging.info('Memory before method %s is %s.',\n                 method.__name__, runtime.memory_usage().current())\n    result = method(*args, **kwargs)\n    logging.info('Memory after method %s is %s',\n                 method.__name__, runtime.memory_usage().current())\n    return result\n  return wrapper", "response": "Log memory usage before and after a method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a storage_api instance for API methods.", "response": "def _get_storage_api(retry_params, account_id=None):\n  \"\"\"Returns storage_api instance for API methods.\n\n  Args:\n    retry_params: An instance of api_utils.RetryParams. If none,\n     thread's default will be used.\n    account_id: Internal-use only.\n\n  Returns:\n    A storage_api instance to handle urlfetch work to GCS.\n    On dev appserver, this instance will talk to a local stub by default.\n    However, if you pass the arguments --appidentity_email_address and\n    --appidentity_private_key_path to dev_appserver.py it will attempt to use\n    the real GCS with these credentials.  Alternatively, you can set a specific\n    access token with common.set_access_token.  You can also pass\n    --default_gcs_bucket_name to set the default bucket.\n  \"\"\"\n\n\n  api = _StorageApi(_StorageApi.full_control_scope,\n                    service_account_id=account_id,\n                    retry_params=retry_params)\n\n  # when running local unit tests, the service account is test@localhost\n  # from google.appengine.api.app_identity.app_identity_stub.APP_SERVICE_ACCOUNT_NAME\n  service_account = app_identity.get_service_account_name()\n  if (common.local_run() and not common.get_access_token()\n      and (not service_account or service_account.endswith('@localhost'))):\n    api.api_url = common.local_api_url()\n  if common.get_access_token():\n    api.token = common.get_access_token()\n  return api"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninherits docs. This method translates urlfetch exceptions to more service specific ones.", "response": "def do_request_async(self, url, method='GET', headers=None, payload=None,\n                       deadline=None, callback=None):\n    \"\"\"Inherit docs.\n\n    This method translates urlfetch exceptions to more service specific ones.\n    \"\"\"\n    if headers is None:\n      headers = {}\n    if 'x-goog-api-version' not in headers:\n      headers['x-goog-api-version'] = '2'\n    headers['accept-encoding'] = 'gzip, *'\n    try:\n      resp_tuple = yield super(_StorageApi, self).do_request_async(\n          url, method=method, headers=headers, payload=payload,\n          deadline=deadline, callback=callback)\n    except urlfetch.DownloadError as e:\n      raise errors.TimeoutError(\n          'Request to Google Cloud Storage timed out.', e)\n\n    raise ndb.Return(resp_tuple)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_object_async(self, path, **kwds):\n    return self.do_request_async(self.api_url + path, 'POST', **kwds)", "response": "POST to an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_object_async(self, path, **kwds):\n    return self.do_request_async(self.api_url + path, 'GET', **kwds)", "response": "GET an object.\n\n    Note: No payload argument is supported."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete an object. Note: No payload argument is supported.", "response": "def delete_object_async(self, path, **kwds):\n    \"\"\"DELETE an object.\n\n    Note: No payload argument is supported.\n    \"\"\"\n    return self.do_request_async(self.api_url + path, 'DELETE', **kwds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef head_object_async(self, path, **kwds):\n    return self.do_request_async(self.api_url + path, 'HEAD', **kwds)", "response": "HEAD an object.\n\n    Depending on request headers, HEAD returns various object properties,\n    e.g. Content-Length, Last-Modified, and ETag.\n\n    Note: No payload argument is supported."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomposes multiple objects together. Using the given list of files, calls the put object with the compose flag. This call merges all the files into the destination file. Args: file_list: list of dicts with the file name. destination_file: Path to the destination file. content_type: Content type for the destination file.", "response": "def compose_object(self, file_list, destination_file, content_type):\n    \"\"\"COMPOSE multiple objects together.\n\n    Using the given list of files, calls the put object with the compose flag.\n    This call merges all the files into the destination file.\n\n    Args:\n      file_list: list of dicts with the file name.\n      destination_file: Path to the destination file.\n      content_type: Content type for the destination file.\n    \"\"\"\n\n    xml_setting_list = ['<ComposeRequest>']\n\n    for meta_data in file_list:\n      xml_setting_list.append('<Component>')\n      for key, val in meta_data.iteritems():\n        xml_setting_list.append('<%s>%s</%s>' % (key, val, key))\n      xml_setting_list.append('</Component>')\n    xml_setting_list.append('</ComposeRequest>')\n    xml = ''.join(xml_setting_list)\n\n    if content_type is not None:\n      headers = {'Content-Type': content_type}\n    else:\n      headers = None\n    status, resp_headers, content = self.put_object(\n        api_utils._quote_filename(destination_file) + '?compose',\n        payload=xml,\n        headers=headers)\n    errors.check_status(status, [200], destination_file, resp_headers,\n                        body=content)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading one line delimited by '\\n' from the file.", "response": "def readline(self, size=-1):\n    \"\"\"Read one line delimited by '\\n' from the file.\n\n    A trailing newline character is kept in the string. It may be absent when a\n    file ends with an incomplete line. If the size argument is non-negative,\n    it specifies the maximum string size (counting the newline) to return.\n    A negative size is the same as unspecified. Empty string is returned\n    only when EOF is encountered immediately.\n\n    Args:\n      size: Maximum number of bytes to read. If not specified, readline stops\n        only on '\\n' or EOF.\n\n    Returns:\n      The data read as a string.\n\n    Raises:\n      IOError: When this buffer is closed.\n    \"\"\"\n    self._check_open()\n    if size == 0 or not self._remaining():\n      return ''\n\n    data_list = []\n    newline_offset = self._buffer.find_newline(size)\n    while newline_offset < 0:\n      data = self._buffer.read(size)\n      size -= len(data)\n      self._offset += len(data)\n      data_list.append(data)\n      if size == 0 or not self._remaining():\n        return ''.join(data_list)\n      self._buffer.reset(self._buffer_future.get_result())\n      self._request_next_buffer()\n      newline_offset = self._buffer.find_newline(size)\n\n    data = self._buffer.read_to_offset(newline_offset + 1)\n    self._offset += len(data)\n    data_list.append(data)\n\n    return ''.join(data_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from the RAW file.", "response": "def read(self, size=-1):\n    \"\"\"Read data from RAW file.\n\n    Args:\n      size: Number of bytes to read as integer. Actual number of bytes\n        read is always equal to size unless EOF is reached. If size is\n        negative or unspecified, read the entire file.\n\n    Returns:\n      data read as str.\n\n    Raises:\n      IOError: When this buffer is closed.\n    \"\"\"\n    self._check_open()\n    if not self._remaining():\n      return ''\n\n    data_list = []\n    while True:\n      remaining = self._buffer.remaining()\n      if size >= 0 and size < remaining:\n        data_list.append(self._buffer.read(size))\n        self._offset += size\n        break\n      else:\n        size -= remaining\n        self._offset += remaining\n        data_list.append(self._buffer.read())\n\n        if self._buffer_future is None:\n          if size < 0 or size >= self._remaining():\n            needs = self._remaining()\n          else:\n            needs = size\n          data_list.extend(self._get_segments(self._offset, needs))\n          self._offset += needs\n          break\n\n        if self._buffer_future:\n          self._buffer.reset(self._buffer_future.get_result())\n          self._buffer_future = None\n\n    if self._buffer_future is None:\n      self._request_next_buffer()\n    return ''.join(data_list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _request_next_buffer(self):\n    self._buffer_future = None\n    next_offset = self._offset + self._buffer.remaining()\n    if next_offset != self._file_size:\n      self._buffer_future = self._get_segment(next_offset,\n                                              self._buffer_size)", "response": "Request next buffer. Requires self. _offset and self. _buffer are consistent state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_segments(self, start, request_size):\n    if not request_size:\n      return []\n\n    end = start + request_size\n    futures = []\n\n    while request_size > self._max_request_size:\n      futures.append(self._get_segment(start, self._max_request_size))\n      request_size -= self._max_request_size\n      start += self._max_request_size\n    if start < end:\n      futures.append(self._get_segment(start, end - start))\n    return [fut.get_result() for fut in futures]", "response": "Get segments of the file from Google Storage as a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a segment of the file from Google Storage.", "response": "def _get_segment(self, start, request_size, check_response=True):\n    \"\"\"Get a segment of the file from Google Storage.\n\n    Args:\n      start: start offset of the segment. Inclusive. Have to be within the\n        range of the file.\n      request_size: number of bytes to request. Have to be small enough\n        for a single urlfetch request. May go over the logical range of the\n        file.\n      check_response: True to check the validity of GCS response automatically\n        before the future returns. False otherwise. See Yields section.\n\n    Yields:\n      If check_response is True, the segment [start, start + request_size)\n      of the file.\n      Otherwise, a tuple. The first element is the unverified file segment.\n      The second element is a closure that checks response. Caller should\n      first invoke the closure before consuing the file segment.\n\n    Raises:\n      ValueError: if the file has changed while reading.\n    \"\"\"\n    end = start + request_size - 1\n    content_range = '%d-%d' % (start, end)\n    headers = {'Range': 'bytes=' + content_range}\n    status, resp_headers, content = yield self._api.get_object_async(\n        self._path, headers=headers)\n    def _checker():\n      errors.check_status(status, [200, 206], self._path, headers,\n                          resp_headers, body=content)\n      self._check_etag(resp_headers.get('etag'))\n    if check_response:\n      _checker()\n      raise ndb.Return(content)\n    raise ndb.Return(content, _checker)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if etag is the same across requests to GCS.", "response": "def _check_etag(self, etag):\n    \"\"\"Check if etag is the same across requests to GCS.\n\n    If self._etag is None, set it. If etag is set, check that the new\n    etag equals the old one.\n\n    In the __init__ method, we fire one HEAD and one GET request using\n    ndb tasklet. One of them would return first and set the first value.\n\n    Args:\n      etag: etag from a GCS HTTP response. None if etag is not part of the\n        response header. It could be None for example in the case of GCS\n        composite file.\n\n    Raises:\n      ValueError: if two etags are not equal.\n    \"\"\"\n    if etag is None:\n      return\n    elif self._etag is None:\n      self._etag = etag\n    elif self._etag != etag:\n      raise ValueError('File on GCS has changed while reading.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seek(self, offset, whence=os.SEEK_SET):\n    self._check_open()\n\n    self._buffer.reset()\n    self._buffer_future = None\n\n    if whence == os.SEEK_SET:\n      self._offset = offset\n    elif whence == os.SEEK_CUR:\n      self._offset += offset\n    elif whence == os.SEEK_END:\n      self._offset = self._file_size + offset\n    else:\n      raise ValueError('Whence mode %s is invalid.' % str(whence))\n\n    self._offset = min(self._offset, self._file_size)\n    self._offset = max(self._offset, 0)\n    if self._remaining():\n      self._request_next_buffer()", "response": "Sets the file s current offset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread size bytes from the internal buffer and updates related offsets.", "response": "def read(self, size=-1):\n    \"\"\"Returns bytes from self._buffer and update related offsets.\n\n    Args:\n      size: number of bytes to read starting from current offset.\n        Read the entire buffer if negative.\n\n    Returns:\n      Requested bytes from buffer.\n    \"\"\"\n    if size < 0:\n      offset = len(self._buffer)\n    else:\n      offset = self._offset + size\n    return self.read_to_offset(offset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading bytes from the buffer and updates related offsets.", "response": "def read_to_offset(self, offset):\n    \"\"\"Returns bytes from self._buffer and update related offsets.\n\n    Args:\n      offset: read from current offset to this offset, exclusive.\n\n    Returns:\n      Requested bytes from buffer.\n    \"\"\"\n    assert offset >= self._offset\n    result = self._buffer[self._offset: offset]\n    self._offset += len(result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for newline char in buffer starting from current offset.", "response": "def find_newline(self, size=-1):\n    \"\"\"Search for newline char in buffer starting from current offset.\n\n    Args:\n      size: number of bytes to search. -1 means all.\n\n    Returns:\n      offset of newline char in buffer. -1 if doesn't exist.\n    \"\"\"\n    if size < 0:\n      return self._buffer.find('\\n', self._offset)\n    return self._buffer.find('\\n', self._offset, self._offset + size)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting some bytes to the internal buffer.", "response": "def write(self, data):\n    \"\"\"Write some bytes.\n\n    Args:\n      data: data to write. str.\n\n    Raises:\n      TypeError: if data is not of type str.\n    \"\"\"\n    self._check_open()\n    if not isinstance(data, str):\n      raise TypeError('Expected str but got %s.' % type(data))\n    if not data:\n      return\n    self._buffer.append(data)\n    self._buffered += len(data)\n    self._offset += len(data)\n    if self._buffered >= self._flushsize:\n      self._flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n    if not self.closed:\n      self.closed = True\n      self._flush(finish=True)\n      self._buffer = None", "response": "Flush the buffer and finalize the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _flush(self, finish=False):\n    while ((finish and self._buffered >= 0) or\n           (not finish and self._buffered >= self._blocksize)):\n      tmp_buffer = []\n      tmp_buffer_len = 0\n\n      excess = 0\n      while self._buffer:\n        buf = self._buffer.popleft()\n        size = len(buf)\n        self._buffered -= size\n        tmp_buffer.append(buf)\n        tmp_buffer_len += size\n        if tmp_buffer_len >= self._maxrequestsize:\n          excess = tmp_buffer_len - self._maxrequestsize\n          break\n        if not finish and (\n            tmp_buffer_len % self._blocksize + self._buffered <\n            self._blocksize):\n          excess = tmp_buffer_len % self._blocksize\n          break\n\n      if excess:\n        over = tmp_buffer.pop()\n        size = len(over)\n        assert size >= excess\n        tmp_buffer_len -= size\n        head, tail = over[:-excess], over[-excess:]\n        self._buffer.appendleft(tail)\n        self._buffered += len(tail)\n        if head:\n          tmp_buffer.append(head)\n          tmp_buffer_len += len(head)\n\n      data = ''.join(tmp_buffer)\n      file_len = '*'\n      if finish and not self._buffered:\n        file_len = self._written + len(data)\n      self._send_data(data, self._written, file_len)\n      self._written += len(data)\n      if file_len != '*':\n        break", "response": "Internal method to flush the buffered data to GCS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _send_data(self, data, start_offset, file_len):\n    headers = {}\n    end_offset = start_offset + len(data) - 1\n\n    if data:\n      headers['content-range'] = ('bytes %d-%d/%s' %\n                                  (start_offset, end_offset, file_len))\n    else:\n      headers['content-range'] = ('bytes */%s' % file_len)\n\n    status, response_headers, content = self._api.put_object(\n        self._path_with_token, payload=data, headers=headers)\n    if file_len == '*':\n      expected = 308\n    else:\n      expected = 200\n    errors.check_status(status, [expected], self._path, headers,\n                        response_headers, content,\n                        {'upload_path': self._path_with_token})", "response": "Send the data to the storage service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the last offset that has been written to GCS.", "response": "def _get_offset_from_gcs(self):\n    \"\"\"Get the last offset that has been written to GCS.\n\n    This is a utility method that does not modify self.\n\n    Returns:\n      an int of the last offset written to GCS by this upload, inclusive.\n      -1 means nothing has been written.\n    \"\"\"\n    headers = {'content-range': 'bytes */*'}\n    status, response_headers, content = self._api.put_object(\n        self._path_with_token, headers=headers)\n    errors.check_status(status, [308], self._path, headers,\n                        response_headers, content,\n                        {'upload_path': self._path_with_token})\n    val = response_headers.get('range')\n    if val is None:\n      return -1\n    _, offset = val.rsplit('-', 1)\n    return int(offset)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _force_close(self, file_length=None):\n    if file_length is None:\n      file_length = self._get_offset_from_gcs() + 1\n    self._send_data('', 0, file_length)", "response": "Force close this buffer on file_length."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extractall(self, path=\".\", members=None):\n    import copy\n    import operator\n    from tarfile import ExtractError\n    directories = []\n\n    if members is None:\n        members = self\n\n    for tarinfo in members:\n        if tarinfo.isdir():\n            directories.append(tarinfo)\n            tarinfo = copy.copy(tarinfo)\n            tarinfo.mode = 448\n        self.extract(tarinfo, path)\n\n    if sys.version_info < (2, 4):\n        def sorter(dir1, dir2):\n            return cmp(dir1.name, dir2.name)\n        directories.sort(sorter)\n        directories.reverse()\n    else:\n        directories.sort(key=operator.attrgetter('name'), reverse=True)\n\n    for tarinfo in directories:\n        dirpath = os.path.join(path, tarinfo.name)\n        try:\n            self.chown(tarinfo, dirpath)\n            self.utime(tarinfo, dirpath)\n            self.chmod(tarinfo, dirpath)\n        except ExtractError:\n            e = sys.exc_info()[1]\n            if self.errorlevel > 1:\n                raise\n            else:\n                self._dbg(1, \"tarfile: %s\" % e)", "response": "Extract all members from the archive to the current working directory and set owner modification time and permissions on the directories on the working directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_next_cycle(self):\n        old_states = dict(self._cstate.states)\n        self._cstate.start_next_cycle()\n        self._request_method = None\n        # self.their_http_version gets left alone, since it presumably lasts\n        # beyond a single request/response cycle\n        assert not self.client_is_waiting_for_100_continue\n        self._respond_to_state_changes(old_states)", "response": "Attempt to reset our connection state for a new request or response cycle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd data to our internal recieve buffer.", "response": "def receive_data(self, data):\n        \"\"\"Add data to our internal recieve buffer.\n\n        This does not actually do any processing on the data, just stores\n        it. To trigger processing, you have to call :meth:`next_event`.\n\n        Args:\n            data (:term:`bytes-like object`):\n                The new data that was just received.\n\n                Special case: If *data* is an empty byte-string like ``b\"\"``,\n                then this indicates that the remote side has closed the\n                connection (end of file). Normally this is convenient, because\n                standard Python APIs like :meth:`file.read` or\n                :meth:`socket.recv` use ``b\"\"`` to indicate end-of-file, while\n                other failures to read are indicated using other mechanisms\n                like raising :exc:`TimeoutError`. When using such an API you\n                can just blindly pass through whatever you get from ``read``\n                to :meth:`receive_data`, and everything will work.\n\n                But, if you have an API where reading an empty string is a\n                valid non-EOF condition, then you need to be aware of this and\n                make sure to check for such strings and avoid passing them to\n                :meth:`receive_data`.\n\n        Returns:\n            Nothing, but after calling this you should call :meth:`next_event`\n            to parse the newly received data.\n\n        Raises:\n            RuntimeError:\n                Raised if you pass an empty *data*, indicating EOF, and then\n                pass a non-empty *data*, indicating more data that somehow\n                arrived after the EOF.\n\n                (Calling ``receive_data(b\"\")`` multiple times is fine,\n                and equivalent to calling it once.)\n\n        \"\"\"\n        if data:\n            if self._receive_buffer_closed:\n                raise RuntimeError(\n                    \"received close, then received more data?\")\n            self._receive_buffer += data\n        else:\n            self._receive_buffer_closed = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the next event out of our receive buffer and update our internal internal", "response": "def next_event(self):\n        \"\"\"Parse the next event out of our receive buffer, update our internal\n        state, and return it.\n\n        This is a mutating operation -- think of it like calling :func:`next`\n        on an iterator.\n\n        Returns:\n            : One of three things:\n\n            1) An event object -- see :ref:`events`.\n\n            2) The special constant :data:`NEED_DATA`, which indicates that\n               you need to read more data from your socket and pass it to\n               :meth:`receive_data` before this method will be able to return\n               any more events.\n\n            3) The special constant :data:`PAUSED`, which indicates that we\n               are not in a state where we can process incoming data (usually\n               because the peer has finished their part of the current\n               request/response cycle, and you have not yet called\n               :meth:`start_next_cycle`). See :ref:`flow-control` for details.\n\n        Raises:\n            RemoteProtocolError:\n                The peer has misbehaved. You should close the connection\n                (possibly after sending some kind of 4xx response).\n\n        Once this method returns :class:`ConnectionClosed` once, then all\n        subsequent calls will also return :class:`ConnectionClosed`.\n\n        If this method raises any exception besides :exc:`RemoteProtocolError`\n        then that's a bug -- if it happens please file a bug report!\n\n        If this method raises any exception then it also sets\n        :attr:`Connection.their_state` to :data:`ERROR` -- see\n        :ref:`error-handling` for discussion.\n\n        \"\"\"\n\n        if self.their_state is ERROR:\n            raise RemoteProtocolError(\n                \"Can't receive data when peer state is ERROR\")\n        try:\n            event = self._extract_next_receive_event()\n            if event not in [NEED_DATA, PAUSED]:\n                self._process_event(self.their_role, event)\n                self._receive_buffer.compress()\n            if event is NEED_DATA:\n                if len(self._receive_buffer) > self._max_incomplete_event_size:\n                    # 431 is \"Request header fields too large\" which is pretty\n                    # much the only situation where we can get here\n                    raise RemoteProtocolError(\"Receive buffer too long\",\n                                              error_status_hint=431)\n                if self._receive_buffer_closed:\n                    # We're still trying to complete some event, but that's\n                    # never going to happen because no more data is coming\n                    raise RemoteProtocolError(\n                        \"peer unexpectedly closed connection\")\n            return event\n        except BaseException as exc:\n            self._process_error(self.their_role)\n            if isinstance(exc, LocalProtocolError):\n                exc._reraise_as_remote_protocol_error()\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send(self, event):\n        data_list = self.send_with_data_passthrough(event)\n        if data_list is None:\n            return None\n        else:\n            return b\"\".join(data_list)", "response": "Convert a high - level event into bytes that can be sent to the peer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_with_data_passthrough(self, event):\n        if self.our_state is ERROR:\n            raise LocalProtocolError(\n                \"Can't send data when our state is ERROR\")\n        try:\n            if type(event) is Response:\n                self._clean_up_response_headers_for_sending(event)\n            # We want to call _process_event before calling the writer,\n            # because if someone tries to do something invalid then this will\n            # give a sensible error message, while our writers all just assume\n            # they will only receive valid events. But, _process_event might\n            # change self._writer. So we have to do a little dance:\n            writer = self._writer\n            self._process_event(self.our_role, event)\n            if type(event) is ConnectionClosed:\n                return None\n            else:\n                # In any situation where writer is None, process_event should\n                # have raised ProtocolError\n                assert writer is not None\n                data_list = []\n                writer(event, data_list.append)\n                return data_list\n        except:\n            self._process_error(self.our_role)\n            raise", "response": "This method sends an event to the local protocol with a list of data as a list of bytes - like objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adam7_generate(width, height):\n\n    for xstart, ystart, xstep, ystep in adam7:\n        if xstart >= width:\n            continue\n        yield ((xstart, y, xstep) for y in range(ystart, height, ystep))", "response": "Generate the coordinates for the reduced scanlines\n    of size width by height pixels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck a palette argument for validity.", "response": "def check_palette(palette):\n    \"\"\"\n    Check a palette argument (to the :class:`Writer` class) for validity.\n    Returns the palette as a list if okay;\n    raises an exception otherwise.\n    \"\"\"\n\n    # None is the default and is allowed.\n    if palette is None:\n        return None\n\n    p = list(palette)\n    if not (0 < len(p) <= 256):\n        raise ProtocolError(\n            \"a palette must have between 1 and 256 entries,\"\n            \" see https://www.w3.org/TR/PNG/#11PLTE\")\n    seen_triple = False\n    for i, t in enumerate(p):\n        if len(t) not in (3, 4):\n            raise ProtocolError(\n                \"palette entry %d: entries must be 3- or 4-tuples.\" % i)\n        if len(t) == 3:\n            seen_triple = True\n        if seen_triple and len(t) == 4:\n            raise ProtocolError(\n                \"palette entry %d: all 4-tuples must precede all 3-tuples\" % i)\n        for x in t:\n            if int(x) != x or not(0 <= x <= 255):\n                raise ProtocolError(\n                    \"palette entry %d: \"\n                    \"values must be integer: 0 <= x <= 255\" % i)\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that these arguments are consistent.", "response": "def check_sizes(size, width, height):\n    \"\"\"\n    Check that these arguments, if supplied, are consistent.\n    Return a (width, height) pair.\n    \"\"\"\n\n    if not size:\n        return width, height\n\n    if len(size) != 2:\n        raise ProtocolError(\n            \"size argument should be a pair (width, height)\")\n    if width is not None and width != size[0]:\n        raise ProtocolError(\n            \"size[0] (%r) and width (%r) should match when both are used.\"\n            % (size[0], width))\n    if height is not None and height != size[1]:\n        raise ProtocolError(\n            \"size[1] (%r) and height (%r) should match when both are used.\"\n            % (size[1], height))\n    return size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_color(c, greyscale, which):\n\n    if c is None:\n        return c\n    if greyscale:\n        try:\n            len(c)\n        except TypeError:\n            c = (c,)\n        if len(c) != 1:\n            raise ProtocolError(\"%s for greyscale must be 1-tuple\" % which)\n        if not is_natural(c[0]):\n            raise ProtocolError(\n                \"%s colour for greyscale must be integer\" % which)\n    else:\n        if not (len(c) == 3 and\n                is_natural(c[0]) and\n                is_natural(c[1]) and\n                is_natural(c[2])):\n            raise ProtocolError(\n                \"%s colour must be a triple of integers\" % which)\n    return c", "response": "Checks that a colour argument for transparent or background options\n    is the right form."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_chunk(outfile, tag, data=b''):\n\n    data = bytes(data)\n    # http://www.w3.org/TR/PNG/#5Chunk-layout\n    outfile.write(struct.pack(\"!I\", len(data)))\n    outfile.write(tag)\n    outfile.write(data)\n    checksum = zlib.crc32(tag)\n    checksum = zlib.crc32(data, checksum)\n    checksum &= 2 ** 32 - 1\n    outfile.write(struct.pack(\"!I\", checksum))", "response": "Write a PNG chunk to the output file including the length and checksum."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_chunks(out, chunks):\n\n    out.write(signature)\n    for chunk in chunks:\n        write_chunk(out, *chunk)", "response": "Write out the PNG file with the given chunks."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rescale_rows(rows, rescale):\n\n    # One factor for each channel\n    fs = [float(2 ** s[1] - 1)/float(2 ** s[0] - 1)\n          for s in rescale]\n\n    # Assume all target_bitdepths are the same\n    target_bitdepths = set(s[1] for s in rescale)\n    assert len(target_bitdepths) == 1\n    (target_bitdepth, ) = target_bitdepths\n    typecode = 'BH'[target_bitdepth > 8]\n\n    # Number of channels\n    n_chans = len(rescale)\n\n    for row in rows:\n        rescaled_row = array(typecode, iter(row))\n        for i in range(n_chans):\n            channel = array(\n                typecode,\n                (int(round(fs[i] * x)) for x in row[i::n_chans]))\n            rescaled_row[i::n_chans] = channel\n        yield rescaled_row", "response": "Rescales the rows in the order of source_bitdepth to target_bitdepth."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpack_rows(rows):\n    for row in rows:\n        fmt = '!%dH' % len(row)\n        yield bytearray(struct.pack(fmt, *row))", "response": "Unpack each row from being 16 - bits per value to being a sequence of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_palette_chunks(palette):\n\n    p = bytearray()\n    t = bytearray()\n\n    for x in palette:\n        p.extend(x[0:3])\n        if len(x) > 3:\n            t.append(x[3])\n    if t:\n        return p, t\n    return p, None", "response": "Create the byte sequences for a PLY and a tRNS chunk."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if bitdepth rescale is required.", "response": "def check_bitdepth_rescale(\n        palette, bitdepth, transparent, alpha, greyscale):\n    \"\"\"\n    Returns (bitdepth, rescale) pair.\n    \"\"\"\n\n    if palette:\n        if len(bitdepth) != 1:\n            raise ProtocolError(\n                \"with palette, only a single bitdepth may be used\")\n        (bitdepth, ) = bitdepth\n        if bitdepth not in (1, 2, 4, 8):\n            raise ProtocolError(\n                \"with palette, bitdepth must be 1, 2, 4, or 8\")\n        if transparent is not None:\n            raise ProtocolError(\"transparent and palette not compatible\")\n        if alpha:\n            raise ProtocolError(\"alpha and palette not compatible\")\n        if greyscale:\n            raise ProtocolError(\"greyscale and palette not compatible\")\n        return bitdepth, None\n\n    # No palette, check for sBIT chunk generation.\n\n    if greyscale and not alpha:\n        # Single channel, L.\n        (bitdepth,) = bitdepth\n        if bitdepth in (1, 2, 4, 8, 16):\n            return bitdepth, None\n        if bitdepth > 8:\n            targetbitdepth = 16\n        elif bitdepth == 3:\n            targetbitdepth = 4\n        else:\n            assert bitdepth in (5, 6, 7)\n            targetbitdepth = 8\n        return targetbitdepth, [(bitdepth, targetbitdepth)]\n\n    assert alpha or not greyscale\n\n    depth_set = tuple(set(bitdepth))\n    if depth_set in [(8,), (16,)]:\n        # No sBIT required.\n        (bitdepth, ) = depth_set\n        return bitdepth, None\n\n    targetbitdepth = (8, 16)[max(bitdepth) > 8]\n    return targetbitdepth, [(b, targetbitdepth) for b in bitdepth]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_array(a, mode=None, info={}):\n\n    # We abuse the *info* parameter by modifying it.  Take a copy here.\n    # (Also typechecks *info* to some extent).\n    info = dict(info)\n\n    # Syntax check mode string.\n    match = RegexModeDecode.match(mode)\n    if not match:\n        raise Error(\"mode string should be 'RGB' or 'L;16' or similar.\")\n\n    mode, bitdepth = match.groups()\n    if bitdepth:\n        bitdepth = int(bitdepth)\n\n    # Colour format.\n    if 'greyscale' in info:\n        if bool(info['greyscale']) != ('L' in mode):\n            raise ProtocolError(\"info['greyscale'] should match mode.\")\n    info['greyscale'] = 'L' in mode\n\n    alpha = 'A' in mode\n    if 'alpha' in info:\n        if bool(info['alpha']) != alpha:\n            raise ProtocolError(\"info['alpha'] should match mode.\")\n    info['alpha'] = alpha\n\n    # Get bitdepth from *mode* if possible.\n    if bitdepth:\n        if info.get(\"bitdepth\") and bitdepth != info['bitdepth']:\n            raise ProtocolError(\n                \"bitdepth (%d) should match bitdepth of info (%d).\" %\n                (bitdepth, info['bitdepth']))\n        info['bitdepth'] = bitdepth\n\n    # Fill in and/or check entries in *info*.\n    # Dimensions.\n    width, height = check_sizes(\n        info.get(\"size\"),\n        info.get(\"width\"),\n        info.get(\"height\"))\n    if width:\n        info[\"width\"] = width\n    if height:\n        info[\"height\"] = height\n\n    if \"height\" not in info:\n        try:\n            info['height'] = len(a)\n        except TypeError:\n            raise ProtocolError(\n                \"len(a) does not work, supply info['height'] instead.\")\n\n    planes = len(mode)\n    if 'planes' in info:\n        if info['planes'] != planes:\n            raise Error(\"info['planes'] should match mode.\")\n\n    # In order to work out whether we the array is 2D or 3D we need its\n    # first row, which requires that we take a copy of its iterator.\n    # We may also need the first row to derive width and bitdepth.\n    a, t = itertools.tee(a)\n    row = next(t)\n    del t\n\n    testelement = row\n    if 'width' not in info:\n        width = len(row) // planes\n        info['width'] = width\n\n    if 'bitdepth' not in info:\n        try:\n            dtype = testelement.dtype\n            # goto the \"else:\" clause.  Sorry.\n        except AttributeError:\n            try:\n                # Try a Python array.array.\n                bitdepth = 8 * testelement.itemsize\n            except AttributeError:\n                # We can't determine it from the array element's datatype,\n                # use a default of 8.\n                bitdepth = 8\n        else:\n            # If we got here without exception,\n            # we now assume that the array is a numpy array.\n            if dtype.kind == 'b':\n                bitdepth = 1\n            else:\n                bitdepth = 8 * dtype.itemsize\n        info['bitdepth'] = bitdepth\n\n    for thing in [\"width\", \"height\", \"bitdepth\", \"greyscale\", \"alpha\"]:\n        assert thing in info\n\n    return Image(a, info)", "response": "Create a PNG Image object from a 2 - dimensional array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decompress(data_blocks):\n\n    # Currently, with no max_length parameter to decompress,\n    # this routine will do one yield per IDAT chunk: Not very\n    # incremental.\n    d = zlib.decompressobj()\n    # Each IDAT chunk is passed to the decompressor, then any\n    # remaining state is decompressed out.\n    for data in data_blocks:\n        # :todo: add a max_length argument here to limit output size.\n        yield bytearray(d.decompress(data))\n    yield bytearray(d.flush())", "response": "Decompresses the given data_blocks and yields the decompressed byte strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the bitdepth and colortype are well - formed.", "response": "def check_bitdepth_colortype(bitdepth, colortype):\n    \"\"\"\n    Check that `bitdepth` and `colortype` are both valid,\n    and specified in a valid combination.\n    Returns (None) if valid, raise an Exception if not valid.\n    \"\"\"\n\n    if bitdepth not in (1, 2, 4, 8, 16):\n        raise FormatError(\"invalid bit depth %d\" % bitdepth)\n    if colortype not in (0, 2, 3, 4, 6):\n        raise FormatError(\"invalid colour type %d\" % colortype)\n    # Check indexed (palettized) images have 8 or fewer bits\n    # per pixel; check only indexed or greyscale images have\n    # fewer than 8 bits per pixel.\n    if colortype & 1 and bitdepth > 8:\n        raise FormatError(\n            \"Indexed images (colour type %d) cannot\"\n            \" have bitdepth > 8 (bit depth %d).\"\n            \" See http://www.w3.org/TR/2003/REC-PNG-20031110/#table111 .\"\n            % (bitdepth, colortype))\n    if bitdepth < 8 and colortype not in (0, 3):\n        raise FormatError(\n            \"Illegal combination of bit depth (%d)\"\n            \" and colour type (%d).\"\n            \" See http://www.w3.org/TR/2003/REC-PNG-20031110/#table111 .\"\n            % (bitdepth, colortype))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, outfile, rows):\n\n        # Values per row\n        vpr = self.width * self.planes\n\n        def check_rows(rows):\n            \"\"\"\n            Yield each row in rows,\n            but check each row first (for correct width).\n            \"\"\"\n            for i, row in enumerate(rows):\n                try:\n                    wrong_length = len(row) != vpr\n                except TypeError:\n                    # When using an itertools.ichain object or\n                    # other generator not supporting __len__,\n                    # we set this to False to skip the check.\n                    wrong_length = False\n                if wrong_length:\n                    # Note: row numbers start at 0.\n                    raise ProtocolError(\n                        \"Expected %d values but got %d value, in row %d\" %\n                        (vpr, len(row), i))\n                yield row\n\n        if self.interlace:\n            fmt = 'BH'[self.bitdepth > 8]\n            a = array(fmt, itertools.chain(*check_rows(rows)))\n            return self.write_array(outfile, a)\n\n        nrows = self.write_passes(outfile, check_rows(rows))\n        if nrows != self.height:\n            raise ProtocolError(\n                \"rows supplied (%d) does not match height (%d)\" %\n                (nrows, self.height))", "response": "Write a PNG image to the output file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a PNG image to the output file.", "response": "def write_passes(self, outfile, rows):\n        \"\"\"\n        Write a PNG image to the output file.\n\n        Most users are expected to find the :meth:`write` or\n        :meth:`write_array` method more convenient.\n\n        The rows should be given to this method in the order that\n        they appear in the output file.\n        For straightlaced images, this is the usual top to bottom ordering.\n        For interlaced images the rows should have been interlaced before\n        passing them to this function.\n\n        `rows` should be an iterable that yields each row\n        (each row being a sequence of values).\n        \"\"\"\n\n        # Ensure rows are scaled (to 4-/8-/16-bit),\n        # and packed into bytes.\n\n        if self.rescale:\n            rows = rescale_rows(rows, self.rescale)\n\n        if self.bitdepth < 8:\n            rows = pack_rows(rows, self.bitdepth)\n        elif self.bitdepth == 16:\n            rows = unpack_rows(rows)\n\n        return self.write_packed(outfile, rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_packed(self, outfile, rows):\n\n        self.write_preamble(outfile)\n\n        # http://www.w3.org/TR/PNG/#11IDAT\n        if self.compression is not None:\n            compressor = zlib.compressobj(self.compression)\n        else:\n            compressor = zlib.compressobj()\n\n        # data accumulates bytes to be compressed for the IDAT chunk;\n        # it's compressed when sufficiently large.\n        data = bytearray()\n\n        for i, row in enumerate(rows):\n            # Add \"None\" filter type.\n            # Currently, it's essential that this filter type be used\n            # for every scanline as\n            # we do not mark the first row of a reduced pass image;\n            # that means we could accidentally compute\n            # the wrong filtered scanline if we used\n            # \"up\", \"average\", or \"paeth\" on such a line.\n            data.append(0)\n            data.extend(row)\n            if len(data) > self.chunk_limit:\n                # :todo: bytes() only necessary in Python 2\n                compressed = compressor.compress(bytes(data))\n                if len(compressed):\n                    write_chunk(outfile, b'IDAT', compressed)\n                data = bytearray()\n\n        compressed = compressor.compress(bytes(data))\n        flushed = compressor.flush()\n        if len(compressed) or len(flushed):\n            write_chunk(outfile, b'IDAT', compressed + flushed)\n        # http://www.w3.org/TR/PNG/#11IEND\n        write_chunk(outfile, b'IEND')\n        return i + 1", "response": "Write PNG file to outfile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_array(self, outfile, pixels):\n\n        if self.interlace:\n            if type(pixels) != array:\n                # Coerce to array type\n                fmt = 'BH'[self.bitdepth > 8]\n                pixels = array(fmt, pixels)\n            self.write_passes(outfile, self.array_scanlines_interlace(pixels))\n        else:\n            self.write_passes(outfile, self.array_scanlines(pixels))", "response": "Write an array that holds all the image values\n            as a PNG file on the output file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef array_scanlines(self, pixels):\n\n        # Values per row\n        vpr = self.width * self.planes\n        stop = 0\n        for y in range(self.height):\n            start = stop\n            stop = start + vpr\n            yield pixels[start:stop]", "response": "Yields the scanlines from the given array of pixels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, file):\n\n        w = Writer(**self.info)\n\n        with open(file, 'wb') as fd:\n            w.write(fd, self.rows)", "response": "Save the image to the named file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the image to the open file object.", "response": "def write(self, file):\n        \"\"\"Write the image to the open file object.\n\n        See `.save()` if you have a filename.\n\n        In general, you can only call this method once;\n        after it has been called the first time the PNG image is written,\n        the source data will have been streamed, and\n        cannot be streamed again.\n        \"\"\"\n\n        w = Writer(**self.info)\n        w.write(file, self.rows)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chunk(self, lenient=False):\n\n        self.validate_signature()\n\n        # http://www.w3.org/TR/PNG/#5Chunk-layout\n        if not self.atchunk:\n            self.atchunk = self._chunk_len_type()\n        if not self.atchunk:\n            raise ChunkError(\"No more chunks.\")\n        length, type = self.atchunk\n        self.atchunk = None\n\n        data = self.file.read(length)\n        if len(data) != length:\n            raise ChunkError(\n                'Chunk %s too short for required %i octets.'\n                % (type, length))\n        checksum = self.file.read(4)\n        if len(checksum) != 4:\n            raise ChunkError('Chunk %s too short for checksum.' % type)\n        verify = zlib.crc32(type)\n        verify = zlib.crc32(data, verify)\n        # Whether the output from zlib.crc32 is signed or not varies\n        # according to hideous implementation details, see\n        # http://bugs.python.org/issue1202 .\n        # We coerce it to be positive here (in a way which works on\n        # Python 2.3 and older).\n        verify &= 2**32 - 1\n        verify = struct.pack('!I', verify)\n        if checksum != verify:\n            (a, ) = struct.unpack('!I', checksum)\n            (b, ) = struct.unpack('!I', verify)\n            message = (\"Checksum error in %s chunk: 0x%08X != 0x%08X.\"\n                       % (type.decode('ascii'), a, b))\n            if lenient:\n                warnings.warn(message, RuntimeWarning)\n            else:\n                raise ChunkError(message)\n        return type, data", "response": "Read the next PNG chunk from the input file and return a tuple of type and data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chunks(self):\n\n        while True:\n            t, v = self.chunk()\n            yield t, v\n            if t == b'IEND':\n                break", "response": "Return an iterator that will yield each chunk type and content as a\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef undo_filter(self, filter_type, scanline, previous):\n\n        # :todo: Would it be better to update scanline in place?\n        result = scanline\n\n        if filter_type == 0:\n            return result\n\n        if filter_type not in (1, 2, 3, 4):\n            raise FormatError(\n                'Invalid PNG Filter Type.  '\n                'See http://www.w3.org/TR/2003/REC-PNG-20031110/#9Filters .')\n\n        # Filter unit.  The stride from one pixel to the corresponding\n        # byte from the previous pixel.  Normally this is the pixel\n        # size in bytes, but when this is smaller than 1, the previous\n        # byte is used instead.\n        fu = max(1, self.psize)\n\n        # For the first line of a pass, synthesize a dummy previous\n        # line.  An alternative approach would be to observe that on the\n        # first line 'up' is the same as 'null', 'paeth' is the same\n        # as 'sub', with only 'average' requiring any special case.\n        if not previous:\n            previous = bytearray([0] * len(scanline))\n\n        # Call appropriate filter algorithm.  Note that 0 has already\n        # been dealt with.\n        fn = (None,\n              undo_filter_sub,\n              undo_filter_up,\n              undo_filter_average,\n              undo_filter_paeth)[filter_type]\n        fn(fu, scanline, previous, result)\n        return result", "response": "Undo a PNG filter for a scanline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _deinterlace(self, raw):\n\n        # Values per row (of the target image)\n        vpr = self.width * self.planes\n\n        # Values per image\n        vpi = vpr * self.height\n        # Interleaving writes to the output array randomly\n        # (well, not quite), so the entire output array must be in memory.\n        # Make a result array, and make it big enough.\n        if self.bitdepth > 8:\n            a = array('H', [0] * vpi)\n        else:\n            a = bytearray([0] * vpi)\n        source_offset = 0\n\n        for lines in adam7_generate(self.width, self.height):\n            # The previous (reconstructed) scanline.\n            # `None` at the beginning of a pass\n            # to indicate that there is no previous line.\n            recon = None\n            for x, y, xstep in lines:\n                # Pixels per row (reduced pass image)\n                ppr = int(math.ceil((self.width - x) / float(xstep)))\n                # Row size in bytes for this pass.\n                row_size = int(math.ceil(self.psize * ppr))\n\n                filter_type = raw[source_offset]\n                source_offset += 1\n                scanline = raw[source_offset: source_offset + row_size]\n                source_offset += row_size\n                recon = self.undo_filter(filter_type, scanline, recon)\n                # Convert so that there is one element per pixel value\n                flat = self._bytes_to_values(recon, width=ppr)\n                if xstep == 1:\n                    assert x == 0\n                    offset = y * vpr\n                    a[offset: offset + vpr] = flat\n                else:\n                    offset = y * vpr + x * self.planes\n                    end_offset = (y + 1) * vpr\n                    skip = self.planes * xstep\n                    for i in range(self.planes):\n                        a[offset + i: end_offset: skip] = \\\n                            flat[i:: self.planes]\n\n        return a", "response": "Read raw pixel data undo filters and flatten."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bytes_to_values(self, bs, width=None):\n\n        if self.bitdepth == 8:\n            return bytearray(bs)\n        if self.bitdepth == 16:\n            return array('H',\n                         struct.unpack('!%dH' % (len(bs) // 2), bs))\n\n        assert self.bitdepth < 8\n        if width is None:\n            width = self.width\n        # Samples per byte\n        spb = 8 // self.bitdepth\n        out = bytearray()\n        mask = 2**self.bitdepth - 1\n        shifts = [self.bitdepth * i\n                  for i in reversed(list(range(spb)))]\n        for o in bs:\n            out.extend([mask & (o >> i) for i in shifts])\n        return out[:width]", "response": "Convert a packed row of bytes into a row of values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating through the packed IDAT files.", "response": "def _iter_straight_packed(self, byte_blocks):\n        \"\"\"Iterator that undoes the effect of filtering;\n        yields each row as a sequence of packed bytes.\n        Assumes input is straightlaced.\n        `byte_blocks` should be an iterable that yields the raw bytes\n        in blocks of arbitrary size.\n        \"\"\"\n\n        # length of row, in bytes\n        rb = self.row_bytes\n        a = bytearray()\n        # The previous (reconstructed) scanline.\n        # None indicates first line of image.\n        recon = None\n        for some_bytes in byte_blocks:\n            a.extend(some_bytes)\n            while len(a) >= rb + 1:\n                filter_type = a[0]\n                scanline = a[1: rb + 1]\n                del a[: rb + 1]\n                recon = self.undo_filter(filter_type, scanline, recon)\n                yield recon\n        if len(a) != 0:\n            # :file:format We get here with a file format error:\n            # when the available bytes (after decompressing) do not\n            # pack into exact rows.\n            raise FormatError('Wrong size for decompressed IDAT chunk.')\n        assert len(a) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the image metadata by reading the initial part of the PNG file up to and including the IDAT chunk.", "response": "def preamble(self, lenient=False):\n        \"\"\"\n        Extract the image metadata by reading\n        the initial part of the PNG file up to\n        the start of the ``IDAT`` chunk.\n        All the chunks that precede the ``IDAT`` chunk are\n        read and either processed for metadata or discarded.\n\n        If the optional `lenient` argument evaluates to `True`,\n        checksum failures will raise warnings rather than exceptions.\n        \"\"\"\n\n        self.validate_signature()\n\n        while True:\n            if not self.atchunk:\n                self.atchunk = self._chunk_len_type()\n                if self.atchunk is None:\n                    raise FormatError('This PNG file has no IDAT chunks.')\n            if self.atchunk[1] == b'IDAT':\n                return\n            self.process_chunk(lenient=lenient)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread just enough of the input to acquire the next chunk s length and type ; return a ( length type*) pair where *type* is a byte sequence.", "response": "def _chunk_len_type(self):\n        \"\"\"\n        Reads just enough of the input to\n        determine the next chunk's length and type;\n        return a (*length*, *type*) pair where *type* is a byte sequence.\n        If there are no more chunks, ``None`` is returned.\n        \"\"\"\n\n        x = self.file.read(8)\n        if not x:\n            return None\n        if len(x) != 8:\n            raise FormatError(\n                'End of file whilst reading chunk length and type.')\n        length, type = struct.unpack('!I4s', x)\n        if length > 2 ** 31 - 1:\n            raise FormatError('Chunk %s is too large: %d.' % (type, length))\n        # Check that all bytes are in valid ASCII range.\n        # https://www.w3.org/TR/2003/REC-PNG-20031110/#5Chunk-layout\n        type_bytes = set(bytearray(type))\n        if not(type_bytes <= set(range(65, 91)) | set(range(97, 123))):\n            raise FormatError(\n                'Chunk %r has invalid Chunk Type.'\n                % list(type))\n        return length, type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_chunk(self, lenient=False):\n\n        type, data = self.chunk(lenient=lenient)\n        method = '_process_' + type.decode('ascii')\n        m = getattr(self, method, None)\n        if m:\n            m(data)", "response": "Process the next chunk and its data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, lenient=False):\n\n        def iteridat():\n            \"\"\"Iterator that yields all the ``IDAT`` chunks as strings.\"\"\"\n            while True:\n                type, data = self.chunk(lenient=lenient)\n                if type == b'IEND':\n                    # http://www.w3.org/TR/PNG/#11IEND\n                    break\n                if type != b'IDAT':\n                    continue\n                # type == b'IDAT'\n                # http://www.w3.org/TR/PNG/#11IDAT\n                if self.colormap and not self.plte:\n                    warnings.warn(\"PLTE chunk is required before IDAT chunk\")\n                yield data\n\n        self.preamble(lenient=lenient)\n        raw = decompress(iteridat())\n\n        if self.interlace:\n            def rows_from_interlace():\n                \"\"\"Yield each row from an interlaced PNG.\"\"\"\n                # It's important that this iterator doesn't read\n                # IDAT chunks until it yields the first row.\n                bs = bytearray(itertools.chain(*raw))\n                arraycode = 'BH'[self.bitdepth > 8]\n                # Like :meth:`group` but\n                # producing an array.array object for each row.\n                values = self._deinterlace(bs)\n                vpr = self.width * self.planes\n                for i in range(0, len(values), vpr):\n                    row = array(arraycode, values[i:i+vpr])\n                    yield row\n            rows = rows_from_interlace()\n        else:\n            rows = self._iter_bytes_to_values(self._iter_straight_packed(raw))\n        info = dict()\n        for attr in 'greyscale alpha planes bitdepth interlace'.split():\n            info[attr] = getattr(self, attr)\n        info['size'] = (self.width, self.height)\n        for attr in 'gamma transparent background'.split():\n            a = getattr(self, attr, None)\n            if a is not None:\n                info[attr] = a\n        if getattr(self, 'x_pixels_per_unit', None):\n            info['physical'] = Resolution(self.x_pixels_per_unit,\n                                          self.y_pixels_per_unit,\n                                          self.unit_is_meter)\n        if self.plte:\n            info['palette'] = self.palette()\n        return self.width, self.height, rows, info", "response": "Read the PNG file and decode it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns image as RGB pixels.", "response": "def asRGB(self):\n        \"\"\"\n        Return image as RGB pixels.\n        RGB colour images are passed through unchanged;\n        greyscales are expanded into RGB triplets\n        (there is a small speed overhead for doing this).\n\n        An alpha channel in the source image will raise an exception.\n\n        The return values are as for the :meth:`read` method except that\n        the *info* reflect the returned pixels, not the source image.\n        In particular,\n        for this method ``info['greyscale']`` will be ``False``.\n        \"\"\"\n\n        width, height, pixels, info = self.asDirect()\n        if info['alpha']:\n            raise Error(\"will not convert image with alpha channel to RGB\")\n        if not info['greyscale']:\n            return width, height, pixels, info\n        info['greyscale'] = False\n        info['planes'] = 3\n\n        if info['bitdepth'] > 8:\n            def newarray():\n                return array('H', [0])\n        else:\n            def newarray():\n                return bytearray([0])\n\n        def iterrgb():\n            for row in pixels:\n                a = newarray() * 3 * width\n                for i in range(3):\n                    a[i::3] = row\n                yield a\n        return width, height, iterrgb(), info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef asRGBA(self):\n\n        width, height, pixels, info = self.asDirect()\n        if info['alpha'] and not info['greyscale']:\n            return width, height, pixels, info\n        typecode = 'BH'[info['bitdepth'] > 8]\n        maxval = 2**info['bitdepth'] - 1\n        maxbuffer = struct.pack('=' + typecode, maxval) * 4 * width\n\n        if info['bitdepth'] > 8:\n            def newarray():\n                return array('H', maxbuffer)\n        else:\n            def newarray():\n                return bytearray(maxbuffer)\n\n        if info['alpha'] and info['greyscale']:\n            # LA to RGBA\n            def convert():\n                for row in pixels:\n                    # Create a fresh target row, then copy L channel\n                    # into first three target channels, and A channel\n                    # into fourth channel.\n                    a = newarray()\n                    convert_la_to_rgba(row, a)\n                    yield a\n        elif info['greyscale']:\n            # L to RGBA\n            def convert():\n                for row in pixels:\n                    a = newarray()\n                    convert_l_to_rgba(row, a)\n                    yield a\n        else:\n            assert not info['alpha'] and not info['greyscale']\n            # RGB to RGBA\n\n            def convert():\n                for row in pixels:\n                    a = newarray()\n                    convert_rgb_to_rgba(row, a)\n                    yield a\n        info['alpha'] = True\n        info['greyscale'] = False\n        info['planes'] = 4\n        return width, height, convert(), info", "response": "Return image as RGBA pixels."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a function that maps all values from 0. 0 to 1. 0 and maps the range [ 0. 0 m to 1. 0.", "response": "def black(m):\n    \"\"\"Return a function that maps all values from [0.0,m] to 0, and maps\n    the range [m,1.0] into [0.0, 1.0] linearly.\n    \"\"\"\n\n    m = float(m)\n\n    def f(x):\n        if x <= m:\n            return 0.0\n        return (x - m) / (1.0 - m)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rows_to_png(out, rows, size):\n\n    # Write out PNG signature.\n    out.write(bytearray([137, 80, 78, 71, 13, 10, 26, 10]))\n    # Write out PNG header chunk.\n    header = struct.pack(\">2LBBBBB\", size[0], size[1], 8, 0, 0, 0, 0)\n    write_chunk(out, b\"IHDR\", header)\n\n    bs = bytearray()\n    for row in rows:\n        bs.append(0)\n        bs.extend(row)\n    write_chunk(out, b\"IDAT\", zlib.compress(bs))\n\n    write_chunk(out, b\"IEND\", bytearray())", "response": "Write to the binary file out a single channel 8 - bit PNG. rows should yield each row in turn ; size should be the tuple of width height in pixels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef char(i):\n\n    i = ord(i)\n    if i not in font:\n        return [(0,)] * 8\n    return [(ord(row),) for row in font[i].decode('hex')]", "response": "Get image data for the character i."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef texttoraster(m):\n\n    lines = m.split('\\n')\n    maxlen = max(len(line) for line in lines)\n    justified = [line.ljust(maxlen) for line in lines]\n    rasters = [linetoraster(line) for line in justified]\n    x, = set(r[0] for r in rasters)\n    y = sum(r[1] for r in rasters)\n    raster = itertools.chain(*(r[2] for r in rasters))\n    return x, y, raster", "response": "Convert the string m to a raster image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a single line of text m to a raster image.", "response": "def linetoraster(m):\n    \"\"\"Convert a single line of text *m* to a raster image,\n    by rendering it using the font in *font*.\n\n    A triple of (*width*, *height*, *pixels*) is returned;\n    *pixels* is in boxed row packed pixel format.\n    \"\"\"\n\n    # Assumes monospaced font.\n    x = 8 * len(m)\n    y = 8\n    return x, y, [itertools.chain(*row) for row in zip(*map(char, m))]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encodefuns():\n\n    def desc(ascii):\n        \"\"\"Return textDescription type [ICC 2001] 6.5.17.  The ASCII part is\n        filled in with the string `ascii`, the Unicode and ScriptCode parts\n        are empty.\"\"\"\n\n        ascii += '\\x00'\n        n = len(ascii)\n\n        return struct.pack('>L%ds2LHB67s' % n,\n                           n, ascii, 0, 0, 0, 0, '')\n\n    def text(ascii):\n        \"\"\"Return textType [ICC 2001] 6.5.18.\"\"\"\n\n        return ascii + '\\x00'\n\n    def curv(f=None, n=256):\n        \"\"\"Return a curveType, [ICC 2001] 6.5.3.  If no arguments are\n        supplied then a TRC for a linear response is generated (no entries).\n        If an argument is supplied and it is a number (for *f* to be a\n        number it  means that ``float(f)==f``) then a TRC for that\n        gamma value is generated.\n        Otherwise `f` is assumed to be a function that maps [0.0, 1.0] to\n        [0.0, 1.0]; an `n` element table is generated for it.\n        \"\"\"\n\n        if f is None:\n            return struct.pack('>L', 0)\n        try:\n            if float(f) == f:\n                return struct.pack('>LH', 1, int(round(f * 2 ** 8)))\n        except (TypeError, ValueError):\n            pass\n        assert n >= 2\n        table = []\n        M = float(n - 1)\n        for i in range(n):\n            x = i / M\n            table.append(int(round(f(x) * 65535)))\n        return struct.pack('>L%dH' % n, n, *table)\n\n    def XYZ(*l):\n        return struct.pack('>3l', *map(fs15f16, l))\n\n    return locals()", "response": "Returns a dictionary mapping ICC type signature sig to encoding ICC type content."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes a Python value as an ICC type.", "response": "def encode(tsig, *l):\n    \"\"\"Encode a Python value as an ICC type.  `tsig` is the type\n    signature to (the first 4 bytes of the encoded value, see [ICC 2004]\n    section 10.\n    \"\"\"\n\n    fun = encodefuns()\n    if tsig not in fun:\n        raise \"No encoder for type %r.\" % tsig\n    v = fun[tsig](*l)\n    # Padd tsig out with spaces.\n    tsig = (tsig + '   ')[: 4]\n    return tsig + ('\\x00' * 4) + v"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting profile from PNG file.", "response": "def profileFromPNG(inp):\n    \"\"\"\n    Extract profile from PNG file.  Return (*profile*, *name*)\n    pair.\n    \"\"\"\n    r = png.Reader(file=inp)\n    _, chunk = r.chunk('iCCP')\n    i = chunk.index(b'\\x00')\n    name = chunk[: i]\n    compression = chunk[i + 1]\n    assert compression == 0\n    profile = zlib.decompress(chunk[i + 2:])\n    return profile, name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef writeICCdatetime(t=None):\n\n    import time\n    if t is None:\n        t = time.gmtime()\n    return struct.pack('>6H', *t[:6])", "response": "Write an ICC datetime in a 12 byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts sequence of ICC s15Fixed16 to list of float.", "response": "def s15f16l(s):\n    \"\"\"Convert sequence of ICC s15Fixed16 to list of float.\"\"\"\n    # Note: As long as float has at least 32 bits of mantissa, all\n    # values are preserved.\n    n = len(s) // 4\n    t = struct.unpack('>%dl' % n, s)\n    return map((2**-16).__mul__, t)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ICCdecode(s):\n\n    sig = s[0:4].strip()\n    f = dict(text=RDtext,\n             XYZ=RDXYZ,\n             curv=RDcurv,\n             vcgt=RDvcgt,\n             sf32=RDsf32,\n             )\n    if sig not in f:\n        return None\n    return (sig, f[sig](s))", "response": "Take an ICC encoded tag and dispatch on its type signature\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef RDmluc(s):\n    # See [ICC 2004] 10.13\n    assert s[0:4] == 'mluc'\n    n, sz = struct.unpack('>2L', s[8:16])\n    assert sz == 12\n    record = []\n    for i in range(n):\n        lc, l, o = struct.unpack('4s2L', s[16 + 12 * n: 28 + 12 * n])\n        record.append(lc, s[o: o + l])\n    # How are strings encoded?\n    return record", "response": "Convert ICC multiLocalizedUnicodeType to RDmluc format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef RDcurv(s):\n    # See [ICC 2001] 6.5.3\n    assert s[0:4] == 'curv'\n    count, = struct.unpack('>L', s[8:12])\n    if count == 0:\n        return dict(gamma=1)\n    table = struct.unpack('>%dH' % count, s[12:])\n    if count == 1:\n        return dict(gamma=table[0] * 2 ** -8)\n    return table", "response": "Convert ICC curveType to RDcurv format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting Apple CMVideoCardGammaType to CMVideoCardGammaType.", "response": "def RDvcgt(s):\n    \"\"\"Convert Apple CMVideoCardGammaType.\"\"\"\n    # See\n    # http://developer.apple.com/documentation/GraphicsImaging/Reference/ColorSync_Manager/Reference/reference.html#//apple_ref/c/tdef/CMVideoCardGammaType\n    assert s[0:4] == 'vcgt'\n    tagtype, = struct.unpack('>L', s[8:12])\n    if tagtype != 0:\n        return s[8:]\n    if tagtype == 0:\n        # Table.\n        channels, count, size = struct.unpack('>3H', s[12:18])\n        if size == 1:\n            fmt = 'B'\n        elif size == 2:\n            fmt = 'H'\n        else:\n            return s[8:]\n        n = len(s[18:]) // size\n        t = struct.unpack('>%d%s' % (n, fmt), s[18:])\n        t = group(t, count)\n        return size, t\n    return s[8:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef greyInput(self):\n\n        self.d.update(dict(profileclass='scnr',\n                           colourspace='GRAY', pcs='XYZ '))\n        return self", "response": "Adjust self. d dictionary for greyscale input device."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites ICC Profile to the file.", "response": "def write(self, out):\n        \"\"\"Write ICC Profile to the file.\"\"\"\n\n        if not self.rawtagtable:\n            self.rawtagtable = self.rawtagdict.items()\n        tags = tagblock(self.rawtagtable)\n        self.writeHeader(out, 128 + len(tags))\n        out.write(tags)\n        out.flush()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef writeHeader(self, out, size=999):\n\n        def defaultkey(d, key, value):\n            \"\"\"Add ``[key]==value`` to the dictionary `d`, but only if\n            it does not have that key already.\n            \"\"\"\n\n            if key in d:\n                return\n            d[key] = value\n\n        z = '\\x00' * 4\n        defaults = dict(preferredCMM=z,\n                        version='02000000',\n                        profileclass=z,\n                        colourspace=z,\n                        pcs='XYZ ',\n                        created=writeICCdatetime(),\n                        acsp='acsp',\n                        platform=z,\n                        flag=0,\n                        manufacturer=z,\n                        model=0,\n                        deviceattributes=0,\n                        intent=0,\n                        pcsilluminant=encodefuns()['XYZ'](*D50()),\n                        creator=z,\n                        )\n        for k, v in defaults.items():\n            defaultkey(self.d, k, v)\n\n        hl = map(self.d.__getitem__,\n                 ['preferredCMM', 'version', 'profileclass', 'colourspace',\n                  'pcs', 'created', 'acsp', 'platform', 'flag',\n                  'manufacturer', 'model', 'deviceattributes', 'intent',\n                  'pcsilluminant', 'creator'])\n        # Convert to struct.pack input\n        hl[1] = int(hl[1], 16)\n\n        out.write(struct.pack('>L4sL4s4s4s12s4s4sL4sLQL12s4s', size, *hl))\n        out.write('\\x00' * 44)\n        return self", "response": "Write the header of the current instance to the file stream out."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts Plan 9 file to PNG format. Works with either uncompressed or compressed files.", "response": "def convert(f, output=sys.stdout):\n    \"\"\"Convert Plan 9 file to PNG format.  Works with either uncompressed\n    or compressed files.\n    \"\"\"\n\n    r = f.read(11)\n    if r == 'compressed\\n':\n        png(output, *decompress(f))\n    else:\n        png(output, *glue(f, r))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef glue(f, r):\n\n    r = r + f.read(60 - len(r))\n    return (r, f)", "response": "Return ( metadata stream f ) pair where r is the initial portion of\n    the metadata that has already been read from the stream f."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts 60 character string r to a 5 - tuple.", "response": "def meta(r):\n    \"\"\"Convert 60 character string `r`, the metadata from an image file.\n    Returns a 5-tuple (*chan*,*minx*,*miny*,*limx*,*limy*).  5-tuples may\n    settle into lists in transit.\n\n    As per http://plan9.bell-labs.com/magic/man2html/6/image the metadata\n    comprises 5 words separated by blanks.  As it happens each word starts\n    at an index that is a multiple of 12, but this routine does not care\n    about that.\"\"\"\n\n    r = r.split()\n    # :todo: raise FormatError\n    assert len(r) == 5\n    r = [r[0]] + map(int, r[1:])\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bitdepthof(pixel):\n\n    maxd = 0\n    for c in re.findall(r'[a-z]\\d*', pixel):\n        if c[0] != 'x':\n            maxd = max(maxd, int(c[1:]))\n    return maxd", "response": "Return the bitdepth for a Plan9 pixel format string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an uncompressed Plan 9 image file to pair of metadata and pixels.", "response": "def pixmeta(metadata, f):\n    \"\"\"Convert (uncompressed) Plan 9 image file to pair of (*metadata*,\n    *pixels*).  This is intended to be used by PyPNG format.  *metadata*\n    is the metadata returned in a dictionary, *pixels* is an iterator that\n    yields each row in boxed row flat pixel format.\n\n    `f`, the input file, should be cued up to the start of the image data.\n    \"\"\"\n\n    chan, minx, miny, limx, limy = metadata\n    rows = limy - miny\n    width = limx - minx\n    nchans = len(re.findall('[a-wyz]', chan))\n    alpha = 'a' in chan\n    # Iverson's convention for the win!\n    ncolour = nchans - alpha\n    greyscale = ncolour == 1\n    bitdepth = bitdepthof(chan)\n    maxval = 2**bitdepth - 1\n    # PNG style metadata\n    meta = dict(size=(width, rows), bitdepth=bitdepthof(chan),\n                greyscale=greyscale, alpha=alpha, planes=nchans)\n\n    return itertools.imap(\n        lambda x: itertools.chain(*x),\n        block(unpack(f, rows, width, chan, maxval), width)), meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert to PNG format.", "response": "def png(out, metadata, f):\n    \"\"\"Convert to PNG format.\n    `metadata` should be a Plan9 5-tuple;\n    `f` the input file (see :meth:`pixmeta`).\n    \"\"\"\n\n    import png\n\n    pixels, meta = pixmeta(metadata, f)\n    p = png.Writer(**meta)\n    p.write(out, pixels)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack(f, rows, width, pixel, maxval):\n\n    def mask(w):\n        \"\"\"An integer, to be used as a mask, with bottom `w` bits set to 1.\"\"\"\n\n        return (1 << w) - 1\n\n    def deblock(f, depth, width):\n        \"\"\"A \"packer\" used to convert multiple bytes into single pixels.\n        `depth` is the pixel depth in bits (>= 8), `width` is the row width in\n        pixels.\n        \"\"\"\n\n        w = depth // 8\n        i = 0\n        for block in f:\n            for i in range(len(block) // w):\n                p = block[w * i: w * (i + 1)]\n                i += w\n                # Convert p to little-endian integer, x\n                x = 0\n                s = 1   # scale\n                for j in p:\n                    x += s * ord(j)\n                    s <<= 8\n                yield x\n\n    def bitfunge(f, depth, width):\n        \"\"\"A \"packer\" used to convert single bytes into multiple pixels.\n        Depth is the pixel depth (< 8), width is the row width in pixels.\n        \"\"\"\n\n        for block in f:\n            col = 0\n            for i in block:\n                x = ord(i)\n                for j in range(8 / depth):\n                    yield x >> (8 - depth)\n                    col += 1\n                    if col == width:\n                        # A row-end forces a new byte even if\n                        # we haven't consumed all of the current byte.\n                        # Effectively rows are bit-padded to make\n                        # a whole number of bytes.\n                        col = 0\n                        break\n                    x <<= depth\n\n    # number of bits in each channel\n    chan = map(int, re.findall(r'\\d+', pixel))\n    # type of each channel\n    type = re.findall('[a-z]', pixel)\n\n    depth = sum(chan)\n\n    # According to the value of depth pick a \"packer\" that either gathers\n    # multiple bytes into a single pixel (for depth >= 8) or split bytes\n    # into several pixels (for depth < 8)\n    if depth >= 8:\n        assert depth % 8 == 0\n        packer = deblock\n    else:\n        assert 8 % depth == 0\n        packer = bitfunge\n\n    for x in packer(f, depth, width):\n        # x is the pixel as an unsigned integer\n        o = []\n        # This is a bit yucky.  Extract each channel from the _most_\n        # significant part of x.\n        for j in range(len(chan)):\n            v = (x >> (depth - chan[j])) & mask(chan[j])\n            x <<= chan[j]\n            if type[j] != 'x':\n                # scale to maxval\n                v = v * float(maxval) / mask(chan[j])\n                v = int(v + 0.5)\n                o.append(v)\n        yield o", "response": "Unpacks a single file into a sequence of n - tuples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decompress(f):\n\n    r = meta(f.read(60))\n    return r, decomprest(f, r[4])", "response": "Decompress a Plan 9 image file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decomprest(f, rows):\n\n    row = 0\n    while row < rows:\n        row, o = deblock(f)\n        yield o", "response": "Iterator that decompresses the rest of a file once the metadata\n    have been consumed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecompress a single block from a compressed Plan 9 image file.", "response": "def deblock(f):\n    \"\"\"Decompress a single block from a compressed Plan 9 image file.\n    Each block starts with 2 decimal strings of 12 bytes each.  Yields a\n    sequence of (row, data) pairs where row is the total number of rows\n    processed according to the file format and data is the decompressed\n    data for a set of rows.\"\"\"\n\n    row = int(f.read(12))\n    size = int(f.read(12))\n    if not (0 <= size <= 6000):\n        raise Error('block has invalid size; not a Plan 9 image file?')\n\n    # Since each block is at most 6000 bytes we may as well read it all in\n    # one go.\n    d = f.read(size)\n    i = 0\n    o = []\n\n    while i < size:\n        x = ord(d[i])\n        i += 1\n        if x & 0x80:\n            x = (x & 0x7f) + 1\n            lit = d[i: i + x]\n            i += x\n            o.extend(lit)\n            continue\n        # x's high-order bit is 0\n        length = (x >> 2) + 3\n        # Offset is made from bottom 2 bits of x and all 8 bits of next\n        # byte.  http://plan9.bell-labs.com/magic/man2html/6/image doesn't\n        # say whether x's 2 bits are most significant or least significant.\n        # But it is clear from inspecting a random file,\n        # http://plan9.bell-labs.com/sources/plan9/sys/games/lib/sokoban/images/cargo.bit\n        # that x's 2 bits are most significant.\n        offset = (x & 3) << 8\n        offset |= ord(d[i])\n        i += 1\n        # Note: complement operator neatly maps (0 to 1023) to (-1 to\n        # -1024).  Adding len(o) gives a (non-negative) offset into o from\n        # which to start indexing.\n        offset = ~offset + len(o)\n        if offset < 0:\n            raise Error('byte offset indexes off the begininning of '\n                        'the output buffer; not a Plan 9 image file?')\n        for j in range(length):\n            o.append(o[offset + j])\n    return row, ''.join(o)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the local and remote resolver list file.", "response": "def updateList(self):\n        \"\"\"\n        Check to see if the X{resolver list} needs updating\n\n        Get the filemtime on the local list, if it's older than the hosted list\n        download the new one\n        \"\"\"\n\n        logging.debug(\"Checking local and remote resolver list for update\")\n\n        # If the local resolver file does not exist, or it has expired\n        if not os.path.isfile(self.listLocal) or \\\n                os.path.getmtime(self.listLocal) < \\\n                time.time() - self.updateListEvery:\n            logging.info(\"Updating resolver list file\")\n            r = requests.get(\n                self.listLocation,\n                headers={\n                    'User-Agent': \"dnsyo/{0}\".format(\n                        pkg_resources.get_distribution(\"dnsyo\").version\n                    )\n                }\n            )\n\n            if r.status_code != 200:\n                # If status code response is not 200 and we don't\n                # already have a resolvers file, raise an exception\n                # Otherwise keep going with the old file\n                if not os.path.isfile(self.listLocal):\n                    # File does not exist locally, we can't continue\n                    raise EnvironmentError(\n                        \"List location returned HTTP status {0} and we \"\n                        \"don't have a local copy of resolvers to fall \"\n                        \"back on. Can't continue\".format(\n                            r.status_code\n                        )\n                    )\n            else:\n                # Save the file\n                with open(self.listLocal, 'w') as lf:\n                    lf.write(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload and filter the server list for only the servers we care about", "response": "def prepareList(self, listFile=False, noSample=False):\n        \"\"\"\n        Load and filter the server list for only the servers we care about\n        \"\"\"\n\n        logging.debug(\"Loading resolver file\")\n\n        listFileLocation = self.listLocal if not listFile else listFile\n\n        # Resolve the user part of the path\n        listLocal = os.path.expanduser(listFileLocation)\n\n        # Check local file location exists and is writable\n        assert os.path.isdir(os.path.dirname(listLocal)),\\\n            \"{0} is not a directory!\".format(os.path.dirname(listLocal))\n        assert os.access(os.path.dirname(listLocal), os.W_OK),\\\n            \"{0} is not writable!\".format(os.path.dirname(listLocal))\n\n        # Open and yaml parse the resolver list\n        with open(listLocal) as ll:\n            raw = ll.read()\n            # Use safe_load, just to be safe.\n            serverList = yaml.safe_load(raw)\n\n        # Remove all but the specified countries from the server list\n        if self.country is not None:\n            logging.debug(\"Filtering serverList for country {0}\"\n                          .format(self.country))\n\n            serverList = [d for d in serverList\n                          if d['country'] == self.country]\n\n            if len(serverList) == 0:\n                raise ValueError(\"There are no servers avaliable \"\n                                 \"with the country code {0}\"\n                                 .format(self.country))\n\n        # Get selected number of servers\n        if self.maxServers == 'ALL' or noSample:\n            # Set servers to the number of servers we have\n            self.maxServers = len(serverList)\n        elif self.maxServers > len(serverList):\n            # We were asked for more servers than exist in the list\n            logging.warning(\n                \"You asked me to query {0} servers, but I only have \"\n                \"{1} servers in my serverlist\".format(\n                    self.maxServers,\n                    len(serverList)\n                )\n            )\n\n            # Fallback to setting it to all\n            self.maxServers = len(serverList)\n\n        # Get a random selection of the specified number\n        # of servers from the list\n        self.serverList = random.sample(serverList, self.maxServers)\n\n        return self.serverList"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query(self, domain, recordType, progress=True):\n\n        # Ignore domain validation, if someone wants to lookup an invalid\n        # domain let them, just ensure it's a string\n        assert type(domain) == str, \"Domain must be a string\"\n\n        # Ensure record type is valid, and in our list of allowed records\n        recordType = recordType.upper()\n        assert recordType in self.lookupRecordTypes, \\\n            \"Record type is not in valid list of record types {0}\". \\\n            format(', '.join(self.lookupRecordTypes))\n\n        self.domain = domain\n        self.recordType = recordType\n        self.resultsColated = []\n        self.results = []\n\n        if len(self.serverList) == 0:\n            logging.warning(\"Server list is empty. Attempting \"\n                            \"to populate with prepareList\")\n            self.prepareList()\n\n        logging.debug(\"Starting query against {0} servers\".format(\n            len(self.serverList)))\n\n        workers = []\n        startTime = datetime.utcnow()\n        serverCounter = 0\n\n        # Run continuously while waiting for results\n        while len(self.results) < len(self.serverList):\n\n            # Count the workers still running\n            runningWorkers = len([w for w in workers if w.result is None])\n\n            # Get the results of any finished workers\n            for i, w in enumerate(workers):\n                if w.result:\n                    # Add the results and get rid of the worker from the\n                    # worker list\n                    self.results.append(w.result)\n                    workers.pop(i)\n\n            # Output progress\n            if progress:\n                # Output progress on one line that updates if terminal\n                # supports it\n                sys.stdout.write(\n                    \"\\r\\x1b[KStatus: Queried {0} of {1} servers, duration: {2}\"\n                    .format(len(self.results), len(self.serverList),\n                            (datetime.utcnow() - startTime))\n                )\n                # Make sure the stdout updates\n                sys.stdout.flush()\n\n            # Start more workers if needed\n            if runningWorkers < self.maxWorkers:\n                logging.debug(\"Starting {0} workers\".format(\n                    self.maxWorkers - runningWorkers))\n\n                # Start however many workers we need\n                # based on max workers - running workers\n                for i in range(0, self.maxWorkers - runningWorkers):\n                    if serverCounter < len(self.serverList):\n\n                        # Create a new thread with all the details\n                        wt = QueryWorker()\n                        wt.server = self.serverList[serverCounter]\n                        wt.domain = domain\n                        wt.recType = recordType\n                        wt.daemon = True\n\n                        # Add it to the worker tracker\n                        workers.append(wt)\n\n                        # Start it\n                        wt.start()\n\n                        serverCounter += 1\n\n            # Pause a little bit\n            time.sleep(0.1)\n\n        # Now colate the results\n        # Group by number of servers with the same response\n        for r in self.results:\n            # Result already in collation\n            if r['results'] in [rs['results'] for rs in self.resultsColated]:\n                cid = [\n                    i for i, rs in enumerate(self.resultsColated)\n                    if r['results'] == rs['results']\n                ][0]\n\n                self.resultsColated[cid]['servers'].append(r['server'])\n            else:\n                self.resultsColated.append(\n                    {\n                        'servers': [\n                            r['server']\n                        ],\n                        'results': r['results'],\n                        'success': r['success']\n                    }\n                )\n\n        if progress:\n            sys.stdout.write(\"\\n\\n\")\n\n        logging.debug(\"There are {0} unique results\".format(\n            len(self.resultsColated)))", "response": "Query the server list for a specific record type and record type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef outputStandard(self, extended=False):\n\n        successfulResponses = len(\n            [\n                True for rsp in self.results if rsp['success']\n            ]\n        )\n\n        sys.stdout.write(\"\"\" - RESULTS\n\nI asked {num_servers} servers for {rec_type} records related to {domain},\n{success_responses} responded with records and {error_responses} gave errors\nHere are the results;\\n\\n\\n\"\"\".format(\n            num_servers=len(self.serverList),\n            rec_type=self.recordType,\n            domain=self.domain,\n            success_responses=successfulResponses,\n            error_responses=len(self.serverList) - successfulResponses\n        ))\n\n        errors = []\n\n        for rsp in self.resultsColated:\n\n            out = []\n\n            if extended:\n                out.append(\"The following servers\\n\")\n                out.append(\"\\n\".join([\n                    \" - {0} ({1} - {2})\".\n                    format(s['ip'], s['provider'], s['country'])\n                    for s in rsp['servers']]))\n\n                out.append(\"\\nresponded with;\\n\")\n            else:\n                out.append(\"{num_servers} servers responded with;\\n\".format(\n                    num_servers=len(rsp['servers']))\n                )\n\n            out.append(\n                \"\\n\".join(rsp['results'])\n            )\n\n            out.append(\"\\n\\n\")\n\n            if rsp['success']:\n                sys.stdout.write(\"\".join(out))\n            else:\n                errors.append(\"\".join(out))\n\n        sys.stdout.write(\"\\n\\nAnd here are the errors;\\n\\n\\n\")\n\n        sys.stdout.write(\"\".join(errors))", "response": "Output the standard output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n\n        logging.debug(\"Querying server {0}\".format(self.server['ip']))\n\n        try:\n            # Create a DNS resolver query\n            rsvr = dns.resolver.Resolver()\n            rsvr.nameservers = [self.server['ip']]\n            rsvr.lifetime = 5\n            rsvr.timeout = 5\n\n            qry = rsvr.query(self.domain, self.recType)\n\n            # Get the results, sort for consistancy\n            results = sorted([r.to_text() for r in qry])\n            success = True\n        # Handle all the various exceptions\n        except dns.resolver.NXDOMAIN:\n            success = False\n            results = ['NXDOMAIN']\n        except dns.resolver.NoNameservers:\n            success = False\n            results = ['No Nameservers']\n        except dns.resolver.NoAnswer:\n            success = False\n            results = ['No Answer']\n        except dns.resolver.Timeout:\n            success = False\n            results = ['Server Timeout']\n\n        # Save the results\n        self.result = {\n            'server': self.server,\n            'results': results,\n            'success': success\n        }", "response": "Do a single DNS query against a server and save the result in self. result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses arguments and pass them into the main class This is invoked from the `dnsyo` script", "response": "def run():\n    \"\"\"\n    Parse arguments and pass them into the main class\n\n    This is invoked from the `dnsyo` script\n    \"\"\"\n\n    # List all the possible options, defaults and help\n    options = [\n        ['resolverlist', 'store',\n         'Location of the yaml resolvers list to download (http/https)',\n         'https://www.codesam.co.uk/files/dnsyo/list/resolver-list.yml'],\n        ['resolverfile', 'store',\n         'Location of the local yaml resolvers file',\n         '~/.dnsyo-resolvers-list.yaml'],\n        ['verbose:v', 'store_true', 'Extended debug info'],\n        ['simple:s', 'store_true',\n         'Simple output mode (good for UNIX parsing)'],\n        ['extended:x', 'store_true',\n         'Extended output mode including server addresses'],\n        ['threads:t', 'store', 'Number of worker threads to use', 100],\n        ['servers:q', 'store',\n         'Maximum number of servers to query (or ALL)', 500],\n        ['country:c', 'store',\n         'Query servers by two letter country code'],\n        ['update', 'store_true',\n         'Check the list for working servers'],\n        ['updateSummary', 'store',\n         'Location for the summary status of the update'],\n        ['updateDestination', 'store',\n         'Destination resolver list for update']\n    ]\n\n    # Create an argparse\n    p = ArgumentParser(\n        usage=\"%(prog)s [options] domain [type]\",\n        description=\"Query lots of DNS servers and colate the results\",\n        epilog=\"https://github.com/samarudge/dnsyo\"\n    )\n\n    # Load the options\n    for opt in options:\n        # Split them into name and short flag\n        if \":\" in opt[0]:\n            name, flag = opt[0].split(':')\n        else:\n            name, flag = opt[0], None\n\n        # Set a default\n        default = opt[3] if len(opt) > 3 else None\n\n        # Add it to the parser object\n        arguments = ['--{0}'.format(name)]\n        if flag:\n            arguments.append('-{0}'.format(flag))\n\n        p.add_argument(\n            *arguments,\n            dest=name,\n            action=opt[1],\n            help=opt[2],\n            default=default\n        )\n\n    # Add the default positional arguments\n    p.add_argument('domain', action=\"store\",\n                   help=\"Domain to query\", default=None,\n                   nargs=\"?\")\n    p.add_argument('type', action=\"store\",\n                   help='Record type (A, CNAME, MX, etc.)',\n                   default=\"A\", nargs=\"?\")\n\n    opts = p.parse_args()\n\n    # Dirty hack to get around --update not needing domain or record\n    if not opts.update and not opts.domain:\n        p.error(\"You must provide a domain!\")\n        sys.exit(3)\n\n    # Setup logging\n    if len(logging.root.handlers) == 0:  # Only if there aren't any loggers\n        if opts.verbose:\n            # If the verbose option is passed, set debug output\n            logging.basicConfig(\n                level=logging.DEBUG\n            )\n        elif opts.simple:\n            # If the simple option is passed only output warnings and errors\n            logging.basicConfig(\n                level=logging.WARNING\n            )\n        else:\n            # Otherwise just info\n            logging.basicConfig(\n                level=logging.INFO\n            )\n\n        logging.debug(\"Debug logging enabled\")\n\n    # Prepare the lookup request\n    try:\n        lookup = dnsyo(\n            listLocation=opts.resolverlist,\n            listLocal=opts.resolverfile,\n            maxWorkers=opts.threads,\n            maxServers=opts.servers,\n            country=opts.country\n        )\n    except AssertionError as e:\n        # Some arguments were not valid, show the error and exit\n        p.error(e)\n        sys.exit(3)\n\n    if opts.update:\n        # Do a list update\n        if not opts.updateSummary or not opts.updateDestination:\n            p.error(\"Must supply updateSummary and updateDestination!\")\n            sys.exit(3)\n\n        u = updater.update(lookup, opts.updateSummary, opts.updateDestination)\n    else:\n        # Do a lookup\n\n        # Update the nameserver list, if needed\n        lookup.updateList()\n\n        # Filter the list to only the servers we want\n        lookup.prepareList()\n\n        try:\n            # Query the servers, display progress if not simple output\n            lookup.query(\n                domain=opts.domain,\n                recordType=opts.type,\n                progress=not opts.simple\n            )\n        except ValueError as e:\n            p.error(e)\n            sys.exit(3)\n\n        # Output the relevant result format\n        if opts.simple:\n            lookup.outputSimple()\n        else:\n            lookup.outputStandard(opts.extended)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a AWS Key ID into a Unix ID", "response": "def aws_to_unix_id(aws_key_id):\n    \"\"\"Converts a AWS Key ID into a UID\"\"\"\n    uid_bytes = hashlib.sha256(aws_key_id.encode()).digest()[-2:]\n    if USING_PYTHON2:\n        return 2000 + int(from_bytes(uid_bytes) // 2)\n    else:\n        return 2000 + (int.from_bytes(uid_bytes, byteorder=sys.byteorder) // 2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gorg(cls):\n    assert isinstance(cls, GenericMeta)\n    if hasattr(cls, '_gorg'):\n        return cls._gorg\n    while cls.__origin__ is not None:\n        cls = cls.__origin__\n    return cls", "response": "This function exists for compatibility with old typing versions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest if the type is a generic callable type including subclasses excluding non - generic types and callables.", "response": "def is_callable_type(tp):\n    \"\"\"Test if the type is a generic callable type, including subclasses\n    excluding non-generic types and callables.\n    Examples::\n\n        is_callable_type(int) == False\n        is_callable_type(type) == False\n        is_callable_type(Callable) == True\n        is_callable_type(Callable[..., int]) == True\n        is_callable_type(Callable[[int, int], Iterable[str]]) == True\n        class MyClass(Callable[[int], int]):\n            ...\n        is_callable_type(MyClass) == True\n\n    For more general tests use callable(), for more precise test\n    (excluding subclasses) use::\n\n        get_origin(tp) is collections.abc.Callable  # Callable prior to Python 3.7\n    \"\"\"\n    if NEW_TYPING:\n        return (tp is Callable or isinstance(tp, _GenericAlias) and\n                tp.__origin__ is collections.abc.Callable or\n                isinstance(tp, type) and issubclass(tp, Generic) and\n                issubclass(tp, collections.abc.Callable))\n    return type(tp) is CallableMeta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_tuple_type(tp):\n    if NEW_TYPING:\n        return (tp is Tuple or isinstance(tp, _GenericAlias) and\n                tp.__origin__ is tuple or\n                isinstance(tp, type) and issubclass(tp, Generic) and\n                issubclass(tp, tuple))\n    return type(tp) is TupleMeta", "response": "Test if the type is a generic tuple type including subclasses excluding non - generic classes excluding non - generic classes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_optional_type(tp):\n\n    if tp is type(None):\n        return True\n    elif is_union_type(tp):\n        return any(is_optional_type(tt) for tt in get_args(tp, evaluate=True))\n    else:\n        return False", "response": "Returns True if the type is optional."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_union_type(tp):\n    if NEW_TYPING:\n        return (tp is Union or\n                isinstance(tp, _GenericAlias) and tp.__origin__ is Union)\n    return type(tp) is _Union", "response": "Test if the type is a union type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_last_origin(tp):\n    if NEW_TYPING:\n        raise ValueError('This function is only supported in Python 3.6,'\n                         ' use get_origin instead')\n    sentinel = object()\n    origin = getattr(tp, '__origin__', sentinel)\n    if origin is sentinel:\n        return None\n    if origin is None:\n        return tp\n    return origin", "response": "Get the last base of subscripted type. Supports generic types Union Callable and Tuple. Returns None for unsupported types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_origin(tp):\n    if NEW_TYPING:\n        if isinstance(tp, _GenericAlias):\n            return tp.__origin__ if tp.__origin__ is not ClassVar else None\n        if tp is Generic:\n            return Generic\n        return None\n    if isinstance(tp, GenericMeta):\n        return _gorg(tp)\n    if is_union_type(tp):\n        return Union\n\n    return None", "response": "Returns the unsubscripted version of a type. Supports generic types Union Callable and Tuple. Returns None for unsupported types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning type parameters of a parameterizable type as a tuple in lexicographic order.", "response": "def get_parameters(tp):\n    \"\"\"Return type parameters of a parameterizable type as a tuple\n    in lexicographic order. Parameterizable types are generic types,\n    unions, tuple types and callable types. Examples::\n\n        get_parameters(int) == ()\n        get_parameters(Generic) == ()\n        get_parameters(Union) == ()\n        get_parameters(List[int]) == ()\n\n        get_parameters(Generic[T]) == (T,)\n        get_parameters(Tuple[List[T], List[S_co]]) == (T, S_co)\n        get_parameters(Union[S_co, Tuple[T, T]][int, U]) == (U,)\n        get_parameters(Mapping[T, Tuple[S_co, T]]) == (T, S_co)\n    \"\"\"\n    if NEW_TYPING:\n        if (isinstance(tp, _GenericAlias) or\n            isinstance(tp, type) and issubclass(tp, Generic) and\n            tp is not Generic):\n            return tp.__parameters__\n        return ()\n    if (\n        is_generic_type(tp) or is_union_type(tp) or\n        is_callable_type(tp) or is_tuple_type(tp)\n    ):\n        return tp.__parameters__ if tp.__parameters__ is not None else ()\n    return ()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_last_args(tp):\n    if NEW_TYPING:\n        raise ValueError('This function is only supported in Python 3.6,'\n                         ' use get_args instead')\n    if is_classvar(tp):\n        return (tp.__type__,) if tp.__type__ is not None else ()\n    if (\n        is_generic_type(tp) or is_union_type(tp) or\n        is_callable_type(tp) or is_tuple_type(tp)\n    ):\n        return tp.__args__ if tp.__args__ is not None else ()\n    return ()", "response": "Get the last arguments of a type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eval_args(args):\n    res = []\n    for arg in args:\n        if not isinstance(arg, tuple):\n            res.append(arg)\n        elif is_callable_type(arg[0]):\n            callable_args = _eval_args(arg[1:])\n            if len(arg) == 2:\n                res.append(Callable[[], callable_args[0]])\n            elif arg[1] is Ellipsis:\n                res.append(Callable[..., callable_args[1]])\n            else:\n                res.append(Callable[list(callable_args[:-1]), callable_args[-1]])\n        else:\n            res.append(type(arg[0]).__getitem__(arg[0], _eval_args(arg[1:])))\n    return tuple(res)", "response": "Internal helper for get_args."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget type arguments with all substitutions performed.", "response": "def get_args(tp, evaluate=None):\n    \"\"\"Get type arguments with all substitutions performed. For unions,\n    basic simplifications used by Union constructor are performed.\n    On versions prior to 3.7 if `evaluate` is False (default),\n    report result as nested tuple, this matches\n    the internal representation of types. If `evaluate` is True\n    (or if Python version is 3.7 or greater), then all\n    type parameters are applied (this could be time and memory expensive).\n    Examples::\n\n        get_args(int) == ()\n        get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        get_args(Union[int, Tuple[T, int]][str]) == (int, (Tuple, str, int))\n\n        get_args(Union[int, Tuple[T, int]][str], evaluate=True) == \\\n                 (int, Tuple[str, int])\n        get_args(Dict[int, Tuple[T, T]][Optional[int]], evaluate=True) == \\\n                 (int, Tuple[Optional[int], Optional[int]])\n        get_args(Callable[[], T][int], evaluate=True) == ([], int,)\n    \"\"\"\n    if NEW_TYPING:\n        if evaluate is not None and not evaluate:\n            raise ValueError('evaluate can only be True in Python 3.7')\n        if isinstance(tp, _GenericAlias):\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n    if is_classvar(tp):\n        return (tp.__type__,)\n    if (\n        is_generic_type(tp) or is_union_type(tp) or\n        is_callable_type(tp) or is_tuple_type(tp)\n    ):\n        tree = tp._subs_tree()\n        if isinstance(tree, tuple) and len(tree) > 1:\n            if not evaluate:\n                return tree[1:]\n            res = _eval_args(tree[1:])\n            if get_origin(tp) is Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n    return ()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow the browser instance.", "response": "def show(self):\n        \"\"\"\n        Create an igv.js \"Browser\" instance on the front end.\n        \"\"\"\n        display(HTML(\"\"\"<div id=\"%s\"></div>\"\"\" % (self.igv_id)))\n        # DON'T check status before showing browser,\n        msg = json.dumps({\n            \"id\": self.igv_id,\n            \"command\": \"create\",\n            \"options\": self.config\n        })\n        self.comm.send(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_track(self, track):\n\n        # Check for minimal igv.js requirements (the only required field for all tracks is url, which must be a string)\n        if isinstance(track, dict) == False:\n            raise Exception(\"track parameter must be a dictionary\")\n\n        return self._send({\n            \"id\": self.igv_id,\n            \"command\": \"loadTrack\",\n            \"track\": track\n        })", "response": "Corresponds to the igv. js Browser function loadTrack"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubscribe to an igv. js event.", "response": "def on(self, eventName, cb):\n        \"\"\"\n        Subscribe to an igv.js event.\n\n        :param Name of the event.  Currently only \"locuschange\" is supported.\n        :type str\n        :param cb - callback function taking a single argument.  For the locuschange event this argument will contain\n                a dictionary of the form  {chr, start, end}\n        :type function\n        \"\"\"\n        self.eventHandlers[eventName] = cb\n        return self._send({\n            \"id\": self.igv_id,\n            \"command\": \"on\",\n            \"eventName\": eventName\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_response(response: str, trim_log_values: bool = False, **kwargs: Any) -> None:\n    return log_(response, response_logger, logging.INFO, trim=trim_log_values, **kwargs)", "response": "Log a response to the log_logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps jsonschema. validate returning the same object passed in.", "response": "def validate(request: Union[Dict, List], schema: dict) -> Union[Dict, List]:\n    \"\"\"\n    Wraps jsonschema.validate, returning the same object passed in.\n\n    Args:\n        request: The deserialized-from-json request.\n        schema: The jsonschema schema to validate against.\n\n    Raises:\n        jsonschema.ValidationError\n    \"\"\"\n    jsonschema_validate(request, schema)\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef call(method: Method, *args: Any, **kwargs: Any) -> Any:\n    return validate_args(method, *args, **kwargs)(*args, **kwargs)", "response": "Calls the method with the arguments provided and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_call(request: Request, methods: Methods, *, debug: bool) -> Response:\n    with handle_exceptions(request, debug) as handler:\n        result = call(methods.items[request.method], *request.args, **request.kwargs)\n        handler.response = SuccessResponse(result=result, id=request.id)\n    return handler.response", "response": "Call a request catching exceptions to ensure we always return a Response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_requests(\n    requests: Union[Request, Iterable[Request]], methods: Methods, debug: bool\n) -> Response:\n    \"\"\"\n    Takes a request or list of Requests and calls them.\n\n    Args:\n        requests: Request object, or a collection of them.\n        methods: The list of methods that can be called.\n        debug: Include more information in error responses.\n    \"\"\"\n    if isinstance(requests, collections.Iterable):\n        return BatchResponse(safe_call(r, methods, debug=debug) for r in requests)\n    return safe_call(requests, methods, debug=debug)", "response": "Takes a request or a list of Requests and calls them."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_requests(\n    requests: Union[Dict, List], *, context: Any = NOCONTEXT, convert_camel_case: bool\n) -> Union[Request, Set[Request]]:\n    \"\"\"\n    Create a Request object from a dictionary (or list of them).\n\n    Args:\n        requests: Request object, or a collection of them.\n        methods: The list of methods that can be called.\n        context: If specified, will be the first positional argument in all requests.\n        convert_camel_case: Will convert the method name/any named params to snake case.\n\n    Returns:\n        A Request object, or a collection of them.\n    \"\"\"\n    if isinstance(requests, list):\n        return {\n            Request(context=context, convert_camel_case=convert_camel_case, **request)\n            for request in requests\n        }\n    return Request(context=context, convert_camel_case=convert_camel_case, **requests)", "response": "Create a Request object from a dictionary or list of requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dispatch_pure(\n    request: str,\n    methods: Methods,\n    *,\n    context: Any,\n    convert_camel_case: bool,\n    debug: bool,\n) -> Response:\n    \"\"\"\n    Pure version of dispatch - no logging, no optional parameters.\n\n    Does two things:\n        1. Deserializes and validates the string.\n        2. Calls each request.\n\n    Args:\n        request: The incoming request string.\n        methods: Collection of methods that can be called.\n        context: If specified, will be the first positional argument in all requests.\n        convert_camel_case: Will convert the method name/any named params to snake case.\n        debug: Include more information in error responses.\n    Returns:\n        A Response.\n    \"\"\"\n    try:\n        deserialized = validate(deserialize(request), schema)\n    except JSONDecodeError as exc:\n        return InvalidJSONResponse(data=str(exc), debug=debug)\n    except ValidationError as exc:\n        return InvalidJSONRPCResponse(data=None, debug=debug)\n    return call_requests(\n        create_requests(\n            deserialized, context=context, convert_camel_case=convert_camel_case\n        ),\n        methods,\n        debug=debug,\n    )", "response": "This function is a pure version of dispatch that handles all the requests and returns a Response object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serve(name: str = \"\", port: int = 5000) -> None:\n    logging.info(\" * Listening on port %s\", port)\n    httpd = HTTPServer((name, port), RequestHandler)\n    httpd.serve_forever()", "response": "A basic way to serve the methods."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_camel_case_string(name: str) -> str:\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", string).lower()", "response": "Convert camel case string to snake case string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert all keys of a dict from camel case to snake case recursively", "response": "def convert_camel_case_keys(original_dict: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Converts all keys of a dict from camel case to snake case, recursively\"\"\"\n    new_dict = dict()\n    for key, val in original_dict.items():\n        if isinstance(val, dict):\n            # Recurse\n            new_dict[convert_camel_case_string(key)] = convert_camel_case_keys(val)\n        else:\n            new_dict[convert_camel_case_string(key)] = val\n    return new_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_arguments(\n    params: Union[List, Dict, object] = NOPARAMS, context: Any = NOCONTEXT\n) -> Tuple[List, Dict]:\n    \"\"\"\n    Get the positional and keyword arguments from a request.\n\n    Takes the 'params' part of a JSON-RPC request and converts it to either positional\n    or named arguments usable in a Python function call. Note that a JSON-RPC request\n    can only have positional _or_ named arguments, but not both. See\n    http://www.jsonrpc.org/specification#parameter_structures\n\n    Args:\n        params: The 'params' part of the JSON-RPC request (should be a list or dict).\n            The 'params' value can be a JSON array (Python list), object (Python dict),\n            or None.\n        context: Optionally include some context data, which will be included as the\n            first positional arguments passed to the method.\n\n    Returns:\n        A two-tuple containing the positional (in a list, or None) and named (in a dict,\n        or None) arguments, extracted from the 'params' part of the request.\n    \"\"\"\n    positionals, nameds = [], {}  # type: list, dict\n    if params is not NOPARAMS:\n        assert isinstance(params, (list, dict))\n        if isinstance(params, list):\n            positionals, nameds = (params, {})\n        elif isinstance(params, dict):\n            positionals, nameds = ([], params)\n\n    # If context data was passed, include it as the first positional argument.\n    if context is not NOCONTEXT:\n        positionals = [context] + positionals\n\n    return (positionals, nameds)", "response": "Returns the positional and keyword arguments from a JSON - RPC request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging a request or response string to the given logger.", "response": "def log_(\n    message: str,\n    logger: logging.Logger,\n    level: int = logging.INFO,\n    extra: Optional[Dict] = None,\n    trim: bool = False,\n) -> None:\n    \"\"\"\n    Log a request or response\n\n    Args:\n        message: JSON-RPC request or response string.\n        logger: \n        level: Log level.\n        extra: More details to include in the log entry.\n        trim: Abbreviate log messages.\n    \"\"\"\n    if extra is None:\n        extra = {}\n    # Clean up the message for logging\n    if message:\n        message = message.replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"{ \", \"{\")\n    if trim:\n        message = _trim_message(message)\n    # Log.\n    logger.log(level, message, extra=extra)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_args(func: Method, *args: Any, **kwargs: Any) -> Method:\n    signature(func).bind(*args, **kwargs)\n    return func", "response": "Validate the arguments passed to a function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a function to add a new entry to the list.", "response": "def add(self, *args: Any, **kwargs: Any) -> Optional[Callable]:\n        \"\"\"\n        Register a function to the list.\n\n        Args:\n            *args: Set/Sequence of positional arguments.\n            **kwargs: Mapping of named arguments.\n\n        Raises:\n            AttributeError: Raised if the method being added has no name. (i.e. it has\n                no `__name__` property, and no `name` argument was given.)\n\n        Examples:\n            methods = Methods()\n            @methods.add\n            def subtract(minuend, subtrahend):\n                return minuend - subtrahend\n        \"\"\"\n        self.items = {\n            **self.items,\n            # Methods passed as positional args need a __name__ attribute, raises\n            # AttributeError otherwise.\n            **{m.__name__: validate(m) for m in args},\n            **{k: validate(v) for k, v in kwargs.items()},\n        }\n        if len(args):\n            return args[0]  # for the decorator to work\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate to_language from_language returns the translation using google translate", "response": "def translate(to_translate, to_language=\"auto\", from_language=\"auto\"):\n    \"\"\"Returns the translation using google translate\n    you must shortcut the language you define\n    (French = fr, English = en, Spanish = es, etc...)\n    if not defined it will detect it or use english by default\n\n    Example:\n    print(translate(\"salut tu vas bien?\", \"en\"))\n    hello you alright?\n    \"\"\"\n    base_link = \"http://translate.google.com/m?hl=%s&sl=%s&q=%s\"\n    if (sys.version_info[0] < 3):\n        to_translate = urllib.quote_plus(to_translate)\n        link = base_link % (to_language, from_language, to_translate)\n        request = urllib2.Request(link, headers=agent)\n        raw_data = urllib2.urlopen(request).read()\n    else:\n        to_translate = urllib.parse.quote(to_translate)\n        link = base_link % (to_language, from_language, to_translate)\n        request = urllib.request.Request(link, headers=agent)\n        raw_data = urllib.request.urlopen(request).read()\n    data = raw_data.decode(\"utf-8\")\n    expr = r'class=\"t0\">(.*?)<'\n    re_result = re.findall(expr, data)\n    if (len(re_result) == 0):\n        result = \"\"\n    else:\n        result = unescape(re_result[0])\n    return (result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_hyphen_commands(raw_cli_arguments):\n    for i in ['gen-sample']:\n        raw_cli_arguments[i.replace('-', '_')] = raw_cli_arguments[i]\n        raw_cli_arguments.pop(i)\n    return raw_cli_arguments", "response": "Update options to match their module names with underscores."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides main CLI entrypoint.", "response": "def main():\n    \"\"\"Provide main CLI entrypoint.\"\"\"\n    if os.environ.get('DEBUG'):\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n        # botocore info is spammy\n        logging.getLogger('botocore').setLevel(logging.ERROR)\n\n    cli_arguments = fix_hyphen_commands(docopt(__doc__, version=version))\n\n    # at least one of these must be enabled, i.e. the value is 'True'... but unfortunately\n    #  `docopts` doesn't give you the hierarchy... so given 'gen-sample cfn', there are\n    #  TWO enabled items in the list, 'gen-sample' and 'cfn'\n    possible_commands = [command for command, enabled in cli_arguments.items() if enabled]\n\n    command_class = find_command_class(possible_commands)\n    if command_class:\n        command_class(cli_arguments).execute()\n    else:\n        LOGGER.error(\"class not found for command '%s'\", possible_commands)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the hash of all of the given files at the given root.", "response": "def calculate_hash_of_files(files, root):\n    \"\"\"Return a hash of all of the given files at the given root.\n\n    Adapted from stacker.hooks.aws_lambda; used according to its license:\n    https://github.com/cloudtools/stacker/blob/1.4.0/LICENSE\n\n    Args:\n        files (list[str]): file names to include in the hash calculation,\n            relative to ``root``.\n        root (str): base directory to analyze files in.\n    Returns:\n        str: A hash of the hashes of the given files.\n\n    \"\"\"\n    file_hash = hashlib.md5()\n    for fname in sorted(files):\n        fileobj = os.path.join(root, fname)\n        file_hash.update((fname + \"\\0\").encode())\n        with open(fileobj, \"rb\") as filedes:\n            for chunk in iter(lambda: filedes.read(4096), \"\"):  # noqa pylint: disable=cell-var-from-loop\n                if not chunk:\n                    break\n                file_hash.update(chunk)\n            file_hash.update(\"\\0\".encode())\n\n    return file_hash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_hash_of_files(root_path, directories=None):\n    if not directories:\n        directories = [{'path': './'}]\n\n    files_to_hash = []\n    for i in directories:\n        ignorer = get_ignorer(os.path.join(root_path, i['path']),\n                              i.get('exclusions'))\n\n        with change_dir(root_path):\n            for root, dirs, files in os.walk(i['path'], topdown=True):\n                if (root != './') and ignorer.is_ignored(root, True):\n                    dirs[:] = []\n                    files[:] = []\n                else:\n                    for filename in files:\n                        filepath = os.path.join(root, filename)\n                        if not ignorer.is_ignored(filepath):\n                            files_to_hash.append(\n                                filepath[2:] if filepath.startswith('./') else filepath  # noqa\n                            )\n\n    return calculate_hash_of_files(files_to_hash, root_path)", "response": "Generate md5 hash of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates ignorer with directory gitignore file.", "response": "def get_ignorer(path, additional_exclusions=None):\n    \"\"\"Create ignorer with directory gitignore file.\"\"\"\n    ignorefile = zgitignore.ZgitIgnore()\n    gitignore_file = os.path.join(path, '.gitignore')\n    if os.path.isfile(gitignore_file):\n        with open(gitignore_file, 'r') as fileobj:\n            ignorefile.add_patterns(fileobj.read().splitlines())\n\n    if additional_exclusions is not None:\n        ignorefile.add_patterns(additional_exclusions)\n\n    return ignorefile"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads Terraform archive and return path to it.", "response": "def download_tf_release(version, versions_dir, command_suffix,\n                        tf_platform=None, arch=None):\n    \"\"\"Download Terraform archive and return path to it.\"\"\"\n    version_dir = os.path.join(versions_dir, version)\n\n    if arch is None:\n        arch = (\n            os.environ.get('TFENV_ARCH') if os.environ.get('TFENV_ARCH')\n            else 'amd64')\n\n    if tf_platform:\n        tfver_os = tf_platform + '_' + arch\n    else:\n        if platform.system().startswith('Darwin'):\n            tfver_os = \"darwin_%s\" % arch\n        elif platform.system().startswith('MINGW64') or (\n                platform.system().startswith('MSYS_NT') or (\n                    platform.system().startswith('CYGWIN_NT'))):\n            tfver_os = \"windows_%s\" % arch\n        else:\n            tfver_os = \"linux_%s\" % arch\n\n    download_dir = tempfile.mkdtemp()\n    filename = \"terraform_%s_%s.zip\" % (version, tfver_os)\n    shasums_name = \"terraform_%s_SHA256SUMS\" % version\n    tf_url = \"https://releases.hashicorp.com/terraform/\" + version\n\n    for i in [filename, shasums_name]:\n        urlretrieve(tf_url + '/' + i,\n                    os.path.join(download_dir, i))\n\n    tf_hash = get_hash_for_filename(filename, os.path.join(download_dir,\n                                                           shasums_name))\n    if tf_hash != sha256sum(os.path.join(download_dir, filename)):\n        LOGGER.error(\"Downloaded Terraform %s does not match sha256 %s\",\n                     filename, tf_hash)\n        sys.exit(1)\n\n    tf_zipfile = zipfile.ZipFile(os.path.join(download_dir, filename))\n    os.mkdir(version_dir)\n    tf_zipfile.extractall(version_dir)\n    tf_zipfile.close()\n    shutil.rmtree(download_dir)\n    os.chmod(  # ensure it is executable\n        os.path.join(version_dir,\n                     'terraform' + command_suffix),\n        os.stat(os.path.join(version_dir,\n                             'terraform' + command_suffix)).st_mode | 0o0111\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_available_tf_versions(include_prerelease=False):\n    tf_releases = json.loads(\n        requests.get('https://releases.hashicorp.com/index.json').text\n    )['terraform']\n    tf_versions = sorted([k  # descending\n                          for k, _v in tf_releases['versions'].items()],\n                         key=LooseVersion,\n                         reverse=True)\n    if include_prerelease:\n        return tf_versions\n    return [i for i in tf_versions if '-' not in i]", "response": "Return available Terraform versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninspects terraform files and find minimum version.", "response": "def find_min_required(path):\n    \"\"\"Inspect terraform files and find minimum version.\"\"\"\n    found_min_required = ''\n    for filename in glob.glob(os.path.join(path, '*.tf')):\n        with open(filename, 'r') as stream:\n            tf_config = hcl.load(stream)\n            if tf_config.get('terraform', {}).get('required_version'):\n                found_min_required = tf_config.get('terraform',\n                                                   {}).get('required_version')\n                break\n\n    if found_min_required:\n        if re.match(r'^!=.+', found_min_required):\n            LOGGER.error('Min required Terraform version is a negation (%s) '\n                         '- unable to determine required version',\n                         found_min_required)\n            sys.exit(1)\n        else:\n            found_min_required = re.search(r'[0-9]*\\.[0-9]*(?:\\.[0-9]*)?',\n                                           found_min_required).group(0)\n            LOGGER.debug(\"Detected minimum terraform version is %s\",\n                         found_min_required)\n            return found_min_required\n    LOGGER.error('Terraform version specified as min-required, but unable to '\n                 'find a specified version requirement in this module\\'s tf '\n                 'files')\n    sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version_requested(path):\n    tf_version_path = os.path.join(path,\n                                   TF_VERSION_FILENAME)\n    if not os.path.isfile(tf_version_path):\n        LOGGER.error(\"Terraform install attempted and no %s file present to \"\n                     \"dictate the version. Please create it (e.g.  write \"\n                     \"\\\"0.11.13\\\" (without quotes) to the file and try again\",\n                     TF_VERSION_FILENAME)\n        sys.exit(1)\n    with open(tf_version_path, 'r') as stream:\n        ver = stream.read().rstrip()\n    return ver", "response": "Return string listing requested Terraform version."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures versions directory is available.", "response": "def ensure_versions_dir_exists(tfenv_path):\n    \"\"\"Ensure versions directory is available.\"\"\"\n    versions_dir = os.path.join(tfenv_path, 'versions')\n    if not os.path.isdir(tfenv_path):\n        os.mkdir(tfenv_path)\n    if not os.path.isdir(versions_dir):\n        os.mkdir(versions_dir)\n    return versions_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall the Terraform version requested.", "response": "def install(self, version_requested=None):\n        \"\"\"Ensure terraform is available.\"\"\"\n        command_suffix = '.exe' if platform.system() == 'Windows' else ''\n        versions_dir = ensure_versions_dir_exists(self.tfenv_dir)\n\n        if not version_requested:\n            version_requested = get_version_requested(self.path)\n\n        if re.match(r'^min-required$', version_requested):\n            LOGGER.debug('tfenv: detecting minimal required version')\n            version_requested = find_min_required(self.path)\n\n        if re.match(r'^latest:.*$', version_requested):\n            regex = re.search(r'latest:(.*)', version_requested).group(1)\n            include_prerelease_versions = False\n        elif re.match(r'^latest$', version_requested):\n            regex = r'^[0-9]+\\.[0-9]+\\.[0-9]+$'\n            include_prerelease_versions = False\n        else:\n            regex = \"^%s$\" % version_requested\n            include_prerelease_versions = True\n            # Return early (i.e before reaching out to the internet) if the\n            # matching version is already installed\n            if os.path.isdir(os.path.join(versions_dir,\n                                          version_requested)):\n                LOGGER.info(\"Terraform version %s already installed; using \"\n                            \"it...\", version_requested)\n                return os.path.join(versions_dir,\n                                    version_requested,\n                                    'terraform') + command_suffix\n\n        try:\n            version = next(i\n                           for i in get_available_tf_versions(\n                               include_prerelease_versions)\n                           if re.match(regex, i))\n        except StopIteration:\n            LOGGER.error(\"Unable to find a Terraform version matching regex: %s\",\n                         regex)\n            sys.exit(1)\n\n        # Now that a version has been selected, skip downloading if it's\n        # already been downloaded\n        if os.path.isdir(os.path.join(versions_dir,\n                                      version)):\n            LOGGER.info(\"Terraform version %s already installed; using it...\",\n                        version)\n            return os.path.join(versions_dir,\n                                version,\n                                'terraform') + command_suffix\n\n        LOGGER.info(\"Downloading and using Terraform version %s ...\",\n                    version)\n        download_tf_release(version, versions_dir, command_suffix)\n        LOGGER.info(\"Downloaded Terraform %s successfully\", version)\n        return os.path.join(versions_dir, version, 'terraform') + command_suffix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_existing_iam_env_vars(self):\n        for i in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY',\n                  'AWS_SESSION_TOKEN']:\n            if i in self.env_vars:\n                self.env_vars['OLD_' + i] = self.env_vars[i]", "response": "Backup IAM environment variables for later restoration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore_existing_iam_env_vars(self):\n        for i in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY',\n                  'AWS_SESSION_TOKEN']:\n            if 'OLD_' + i in self.env_vars:\n                self.env_vars[i] = self.env_vars['OLD_' + i]\n            elif i in self.env_vars:\n                self.env_vars.pop(i)", "response": "Restore IAM environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hello(event, context):  # pylint: disable=unused-argument\n    body = {\n        \"message\": \"Go Serverless v1.0! Your function executed successfully!\",\n        \"input\": event\n    }\n\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(body)\n    }\n\n    return response", "response": "Return Serverless Hello World."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_env_dirs(self):\n        repo_dirs = next(os.walk(self.env_root))[1]\n        if '.git' in repo_dirs:\n            repo_dirs.remove('.git')  # not relevant for any repo operations\n        return repo_dirs", "response": "Return list of directories in env_root."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list of yaml files in env_root.", "response": "def get_yaml_files_at_env_root(self):\n        \"\"\"Return list of yaml files in env_root.\"\"\"\n        yaml_files = glob.glob(\n            os.path.join(self.env_root, '*.yaml')\n        )\n        yml_files = glob.glob(\n            os.path.join(self.env_root, '*.yml')\n        )\n        return yaml_files + yml_files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn boolean on whether a path only contains directories.", "response": "def path_only_contains_dirs(self, path):\n        \"\"\"Return boolean on whether a path only contains directories.\"\"\"\n        pathlistdir = os.listdir(path)\n        if pathlistdir == []:\n            return True\n        if any(os.path.isfile(os.path.join(path, i)) for i in pathlistdir):\n            return False\n        return all(self.path_only_contains_dirs(os.path.join(path, i)) for i in pathlistdir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of empty directories in path.", "response": "def get_empty_dirs(self, path):\n        \"\"\"Return a list of empty directories in path.\"\"\"\n        empty_dirs = []\n        for i in os.listdir(path):\n            child_path = os.path.join(path, i)\n            if i == '.git' or os.path.isfile(child_path) or os.path.islink(child_path):  # noqa\n                continue\n            if self.path_only_contains_dirs(child_path):\n                empty_dirs.append(i)\n        return empty_dirs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_runway_config(self):\n        if not os.path.isfile(self.runway_config_path):\n            LOGGER.error(\"Runway config file was not found (looking for \"\n                         \"%s)\",\n                         self.runway_config_path)\n            sys.exit(1)\n        with open(self.runway_config_path) as data_file:\n            return yaml.safe_load(data_file)", "response": "Read and parse runway. yml."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runway_config(self):\n        if not self._runway_config:\n            self._runway_config = self.parse_runway_config()\n        return self._runway_config", "response": "Return parsed runway. yml."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes objects in bucket.", "response": "def purge_bucket(context, provider, **kwargs):\n    \"\"\"Delete objects in bucket.\"\"\"\n    session = get_session(provider.region)\n\n    if kwargs.get('bucket_name'):\n        bucket_name = kwargs['bucket_name']\n    else:\n        if kwargs.get('bucket_output_lookup'):\n            value = kwargs['bucket_output_lookup']\n            handler = OutputLookup.handle\n        elif kwargs.get('bucket_rxref_lookup'):\n            value = kwargs['bucket_rxref_lookup']\n            handler = RxrefLookup.handle\n        elif kwargs.get('bucket_xref_lookup'):\n            value = kwargs['bucket_xref_lookup']\n            handler = XrefLookup.handle\n        else:\n            LOGGER.fatal('No bucket name/source provided.')\n            return False\n\n        try:  # Exit early if the bucket's stack is already deleted\n            session.client('cloudformation').describe_stacks(\n                StackName=context.get_fqn(value.split('::')[0])\n            )\n        except ClientError as exc:\n            if 'does not exist' in exc.response['Error']['Message']:\n                LOGGER.info('S3 bucket stack appears to have already been '\n                            'deleted...')\n                return True\n            raise\n\n        bucket_name = handler(\n            value,\n            provider=provider,\n            context=context\n        )\n\n    s3_resource = session.resource('s3')\n    try:\n        s3_resource.meta.client.head_bucket(Bucket=bucket_name)\n    except ClientError as exc:\n        if exc.response['Error']['Code'] == '404':\n            LOGGER.info(\"%s S3 bucket appears to have already been deleted...\",\n                        bucket_name)\n            return True\n        raise\n\n    bucket = s3_resource.Bucket(bucket_name)\n    bucket.object_versions.delete()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the template for the current state of the current object.", "response": "def create_template(self):\n        \"\"\"Create template (main function called by Stacker).\"\"\"\n        template = self.template\n        variables = self.get_variables()\n        self.template.add_version('2010-09-09')\n        self.template.add_description('Terraform State Resources')\n\n        # Conditions\n        for i in ['BucketName', 'TableName']:\n            template.add_condition(\n                \"%sOmitted\" % i,\n                Or(Equals(variables[i].ref, ''),\n                   Equals(variables[i].ref, 'undefined'))\n            )\n\n        # Resources\n        terraformlocktable = template.add_resource(\n            dynamodb.Table(\n                'TerraformStateTable',\n                AttributeDefinitions=[\n                    dynamodb.AttributeDefinition(\n                        AttributeName='LockID',\n                        AttributeType='S'\n                    )\n                ],\n                KeySchema=[\n                    dynamodb.KeySchema(\n                        AttributeName='LockID',\n                        KeyType='HASH'\n                    )\n                ],\n                ProvisionedThroughput=dynamodb.ProvisionedThroughput(\n                    ReadCapacityUnits=2,\n                    WriteCapacityUnits=2\n                ),\n                TableName=If(\n                    'TableNameOmitted',\n                    NoValue,\n                    variables['TableName'].ref\n                )\n            )\n        )\n        template.add_output(Output(\n            '%sName' % terraformlocktable.title,\n            Description='Name of DynamoDB table for Terraform state',\n            Value=terraformlocktable.ref()\n        ))\n\n        terraformstatebucket = template.add_resource(\n            s3.Bucket(\n                'TerraformStateBucket',\n                AccessControl=s3.Private,\n                BucketName=If(\n                    'BucketNameOmitted',\n                    NoValue,\n                    variables['BucketName'].ref\n                ),\n                LifecycleConfiguration=s3.LifecycleConfiguration(\n                    Rules=[\n                        s3.LifecycleRule(\n                            NoncurrentVersionExpirationInDays=90,\n                            Status='Enabled'\n                        )\n                    ]\n                ),\n                VersioningConfiguration=s3.VersioningConfiguration(\n                    Status='Enabled'\n                )\n            )\n        )\n        template.add_output(Output(\n            '%sName' % terraformstatebucket.title,\n            Description='Name of bucket storing Terraform state',\n            Value=terraformstatebucket.ref()\n        ))\n        template.add_output(Output(\n            '%sArn' % terraformstatebucket.title,\n            Description='Arn of bucket storing Terraform state',\n            Value=terraformstatebucket.get_att('Arn')\n        ))\n\n        managementpolicy = template.add_resource(\n            iam.ManagedPolicy(\n                'ManagementPolicy',\n                Description='Managed policy for Terraform state management.',\n                Path='/',\n                PolicyDocument=PolicyDocument(\n                    Version='2012-10-17',\n                    Statement=[\n                        # https://www.terraform.io/docs/backends/types/s3.html#s3-bucket-permissions\n                        Statement(\n                            Action=[awacs.s3.ListBucket],\n                            Effect=Allow,\n                            Resource=[terraformstatebucket.get_att('Arn')]\n                        ),\n                        Statement(\n                            Action=[awacs.s3.GetObject,\n                                    awacs.s3.PutObject],\n                            Effect=Allow,\n                            Resource=[\n                                Join('', [terraformstatebucket.get_att('Arn'),\n                                          '/*'])\n                            ]\n                        ),\n                        Statement(\n                            Action=[awacs.dynamodb.GetItem,\n                                    awacs.dynamodb.PutItem,\n                                    awacs.dynamodb.DeleteItem],\n                            Effect=Allow,\n                            Resource=[terraformlocktable.get_att('Arn')]\n                        )\n                    ]\n                )\n            )\n        )\n        template.add_output(\n            Output(\n                'PolicyArn',\n                Description='Managed policy Arn',\n                Value=managementpolicy.ref()\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_api_endpoint():\n    environment = subprocess.check_output(['pipenv',\n                                           'run',\n                                           'runway',\n                                           'whichenv']).decode().strip()\n    environment_file = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)),\n        'src',\n        'environments',\n        'environment.prod.ts' if environment == 'prod' else 'environment.ts'\n    )\n    cloudformation = boto3.resource('cloudformation')\n    stack = cloudformation.Stack(STACK_PREFIX + environment)\n    endpoint = [i['OutputValue'] for i in stack.outputs\n                if i['OutputKey'] == 'ServiceEndpoint'][0]\n\n    with open(environment_file, 'r') as stream:\n        content = stream.read()\n    content = re.sub(r'api_url: \\'.*\\'$',\n                     \"api_url: '%s/api'\" % endpoint,\n                     content,\n                     flags=re.M)\n    with open(environment_file, 'w') as stream:\n        stream.write(content)", "response": "Update app environment file with backend endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_dir(newdir):\n    prevdir = os.getcwd()\n    os.chdir(os.path.expanduser(newdir))\n    try:\n        yield\n    finally:\n        os.chdir(prevdir)", "response": "Change directory.\n\n    Adapted from http://stackoverflow.com/a/24176022"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_file_is_executable(path):\n    if platform.system() != 'Windows' and (\n            not stat.S_IXUSR & os.stat(path)[stat.ST_MODE]):\n        print(\"Error: File %s is not executable\" % path)\n        sys.exit(1)", "response": "Exit if file is not executable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge dict2 into dict1.", "response": "def merge_dicts(dict1, dict2, deep_merge=True):\n    \"\"\"Merge dict2 into dict1.\"\"\"\n    if deep_merge:\n        if isinstance(dict1, list) and isinstance(dict2, list):\n            return dict1 + dict2\n\n        if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n            return dict2\n\n        for key in dict2:\n            dict1[key] = merge_dicts(dict1[key], dict2[key]) if key in dict1 else dict2[key]  # noqa pylint: disable=line-too-long\n        return dict1\n    dict3 = dict1.copy()\n    dict3.update(dict2)\n    return dict3"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn boto3 client args dict with environment creds.", "response": "def extract_boto_args_from_env(env_vars):\n    \"\"\"Return boto3 client args dict with environment creds.\"\"\"\n    boto_args = {}\n    for i in ['aws_access_key_id', 'aws_secret_access_key',\n              'aws_session_token']:\n        if env_vars.get(i.upper()):\n            boto_args[i] = env_vars[i.upper()]\n    return boto_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flatten_path_lists(env_dict, env_root=None):\n    for (key, val) in env_dict.items():\n        # Lists are presumed to be path components and will be turned back\n        # to strings\n        if isinstance(val, list):\n            env_dict[key] = os.path.join(env_root, os.path.join(*val)) if (env_root and not os.path.isabs(os.path.join(*val))) else os.path.join(*val)  # noqa pylint: disable=line-too-long\n    return env_dict", "response": "Join paths in environment dict down to strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn single - level dictionary from dictionary of dictionaries.", "response": "def merge_nested_environment_dicts(env_dicts, env_name=None, env_root=None):\n    \"\"\"Return single-level dictionary from dictionary of dictionaries.\"\"\"\n    # If the provided dictionary is just a single \"level\" (no nested\n    # environments), it applies to all environments\n    if all(isinstance(val, (six.string_types, list))\n           for (_key, val) in env_dicts.items()):\n        return flatten_path_lists(env_dicts, env_root)\n\n    if env_name is None:\n        if env_dicts.get('*'):\n            return flatten_path_lists(env_dicts.get('*'), env_root)\n        raise AttributeError(\"Provided config key:val pairs %s aren't usable with no environment provided\" % env_dicts)  # noqa pylint: disable=line-too-long\n\n    if not env_dicts.get('*') and not env_dicts.get(env_name):\n        raise AttributeError(\"Provided config key:val pairs %s aren't usable with environment %s\" % (env_dicts, env_name))  # noqa pylint: disable=line-too-long\n\n    combined_dicts = merge_dicts(env_dicts.get('*', {}),\n                                 env_dicts.get(env_name, {}))\n    return flatten_path_lists(combined_dicts, env_root)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning path of embedded libraries.", "response": "def get_embedded_lib_path():\n    \"\"\"Return path of embedded libraries.\"\"\"\n    return os.path.join(\n        os.path.dirname(os.path.abspath(__file__)),\n        'embedded'\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hash_for_filename(filename, hashfile_path):\n    filehash = ''\n    with open(hashfile_path, 'r') as stream:\n        for _cnt, line in enumerate(stream):\n            if line.rstrip().endswith(filename):\n                filehash = re.match(r'^[A-Za-z0-9]*', line).group(0)\n                break\n    if filehash:\n        return filehash\n    raise AttributeError(\"Filename %s not found in hash file\" % filename)", "response": "Return hash for filename in the hashfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns command list with working Windows commands.", "response": "def fix_windows_command_list(commands):\n    # type: (List[str]) -> List[str]\n    \"\"\"Return command list with working Windows commands.\n\n    npm on windows is npm.cmd, which will blow up\n    subprocess.check_call(['npm', '...'])\n\n    Similar issues arise when calling python apps like pipenv that will have\n    a windows-only suffix applied to them\n    \"\"\"\n    fully_qualified_cmd_path = which(commands[0])\n    if fully_qualified_cmd_path and (\n            not which(commands[0], add_win_suffixes=False)):\n        commands[0] = os.path.basename(fully_qualified_cmd_path)\n    return commands"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a list of commands.", "response": "def run_commands(commands,  # type: List[Union[str, List[str], Dict[str, Union[str, List[str]]]]]\n                 directory,  # type: str\n                 env=None  # type: Optional[Dict[str, Union[str, int]]]\n                ):  # noqa\n    # type: (...) -> None\n    \"\"\"Run list of commands.\"\"\"\n    if env is None:\n        env = os.environ.copy()\n    for step in commands:\n        if isinstance(step, (list, six.string_types)):\n            execution_dir = directory\n            raw_command = step\n        elif step.get('command'):  # dictionary\n            execution_dir = os.path.join(directory,\n                                         step.get('cwd')) if step.get('cwd') else directory  # noqa pylint: disable=line-too-long\n            raw_command = step['command']\n        else:\n            raise AttributeError(\"Invalid command step: %s\" % step)\n        command_list = raw_command.split(' ') if isinstance(raw_command, six.string_types) else raw_command  # noqa pylint: disable=line-too-long\n        if platform.system().lower() == 'windows':\n            command_list = fix_windows_command_list(command_list)\n\n        with change_dir(execution_dir):\n            check_call(command_list, env=env)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning SHA256 hash of file.", "response": "def sha256sum(filename):\n    \"\"\"Return SHA256 hash of file.\"\"\"\n    sha256 = hashlib.sha256()\n    mem_view = memoryview(bytearray(128*1024))\n    with open(filename, 'rb', buffering=0) as stream:\n        for i in iter(lambda: stream.readinto(mem_view), 0):\n            sha256.update(mem_view[:i])\n    return sha256.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef use_embedded_pkgs(embedded_lib_path=None):\n    if embedded_lib_path is None:\n        embedded_lib_path = get_embedded_lib_path()\n\n    old_sys_path = list(sys.path)\n    sys.path.insert(\n        1,  # https://stackoverflow.com/a/10097543\n        embedded_lib_path\n    )\n    try:\n        yield\n    finally:\n        sys.path = old_sys_path", "response": "Temporarily prepend embedded packages to sys. path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmimic which command behavior.", "response": "def which(program, add_win_suffixes=True):\n    \"\"\"Mimic 'which' command behavior.\n\n    Adapted from https://stackoverflow.com/a/377028\n    \"\"\"\n    def is_exe(fpath):\n        \"\"\"Determine if program exists and is executable.\"\"\"\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if add_win_suffixes and platform.system().lower() == 'windows' and not (\n            fname.endswith('.exe') or fname.endswith('.cmd')):\n        fnames = [fname + '.exe', fname + '.cmd']\n    else:\n        fnames = [fname]\n\n    for i in fnames:\n        if fpath:\n            exe_file = os.path.join(fpath, i)\n            if is_exe(exe_file):\n                return exe_file\n        else:\n            for path in os.environ['PATH'].split(os.pathsep):\n                exe_file = os.path.join(path, i)\n                if is_exe(exe_file):\n                    return exe_file\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_config_backend_options(module_opts, env_name, env_vars):\n    backend_opts = {}\n\n    if module_opts.get('terraform_backend_config'):\n        backend_opts['config'] = merge_nested_environment_dicts(\n            module_opts.get('terraform_backend_config'),\n            env_name\n        )\n    if module_opts.get('terraform_backend_cfn_outputs'):\n        if not backend_opts.get('config'):\n            backend_opts['config'] = {}\n        if not backend_opts['config'].get('region'):\n            backend_opts['config']['region'] = env_vars['AWS_DEFAULT_REGION']\n\n        boto_args = extract_boto_args_from_env(env_vars)\n        cfn_client = boto3.client(\n            'cloudformation',\n            region_name=backend_opts['config']['region'],\n            **boto_args\n        )\n        for (key, val) in merge_nested_environment_dicts(module_opts.get('terraform_backend_cfn_outputs'),  # noqa pylint: disable=line-too-long\n                                                         env_name).items():\n            backend_opts['config'][key] = find_cfn_output(\n                val.split('::')[1],\n                cfn_client.describe_stacks(\n                    StackName=val.split('::')[0]\n                )['Stacks'][0]['Outputs']\n            )\n\n    return backend_opts", "response": "Create config backend options from module options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_backend_init_list(backend_vals):\n    cmd_list = []\n    for (key, val) in backend_vals.items():\n        cmd_list.append('-backend-config')\n        cmd_list.append(key + '=' + val)\n    return cmd_list", "response": "Turn backend config dict into command line items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_backend_tfvars_file(path, environment, region):\n    backend_filenames = gen_backend_tfvars_files(environment, region)\n    for name in backend_filenames:\n        if os.path.isfile(os.path.join(path, name)):\n            return name\n    return backend_filenames[-1]", "response": "Determine Terraform backend file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn version of Terraform requested in module options.", "response": "def get_module_defined_tf_var(terraform_version_opts, env_name):\n    \"\"\"Return version of Terraform requested in module options.\"\"\"\n    if isinstance(terraform_version_opts, six.string_types):\n        return terraform_version_opts\n    if terraform_version_opts.get(env_name):\n        return terraform_version_opts.get(env_name)\n    if terraform_version_opts.get('*'):\n        return terraform_version_opts.get('*')\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine Terraform workspace - specific tfvars file name.", "response": "def get_workspace_tfvars_file(path, environment, region):\n    \"\"\"Determine Terraform workspace-specific tfvars file name.\"\"\"\n    for name in gen_workspace_tfvars_files(environment, region):\n        if os.path.isfile(os.path.join(path, name)):\n            return name\n    return \"%s.tfvars\" % environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning terraform directory and run init if necessary. If deploying a TF module to multiple regions (or any scenario requiring multiple backend configs), switching the backend will cause TF to compare the old and new backends. This will frequently cause an access error as the creds/role for the new backend won't always have access to the old one. This method compares the defined & initialized backend configs and trashes the terraform directory & re-inits if they're out of sync.", "response": "def reinit_on_backend_changes(tf_bin,  # pylint: disable=too-many-arguments\n                              module_path, backend_options, env_name,\n                              env_region, env_vars):\n    \"\"\"Clean terraform directory and run init if necessary.\n\n    If deploying a TF module to multiple regions (or any scenario requiring\n    multiple backend configs), switching the backend will cause TF to\n    compare the old and new backends. This will frequently cause an access\n    error as the creds/role for the new backend won't always have access to\n    the old one.\n\n    This method compares the defined & initialized backend configs and\n    trashes the terraform directory & re-inits if they're out of sync.\n    \"\"\"\n    terraform_dir = os.path.join(module_path, '.terraform')\n    local_tfstate_path = os.path.join(terraform_dir, 'terraform.tfstate')\n    current_backend_config = {}\n    desired_backend_config = {}\n\n    LOGGER.debug('Comparing previous & desired Terraform backend configs')\n    if os.path.isfile(local_tfstate_path):\n        with open(local_tfstate_path, 'r') as stream:\n            current_backend_config = hcl.load(stream).get('backend',\n                                                          {}).get('config',\n                                                                  {})\n\n    if backend_options.get('config'):\n        desired_backend_config = backend_options.get('config')\n    elif os.path.isfile(os.path.join(module_path,\n                                     backend_options.get('filename'))):\n        with open(os.path.join(module_path,\n                               backend_options.get('filename')),\n                  'r') as stream:\n            desired_backend_config = hcl.load(stream)\n\n    # Can't solely rely on the backend info defined in runway options or\n    # backend files; merge in the values defined in main.tf\n    # (or whatever tf file)\n    for filename in ['main.tf'] + glob.glob(os.path.join(module_path, '*.tf')):\n        if os.path.isfile(filename):\n            with open(filename, 'r') as stream:\n                tf_config = hcl.load(stream)\n                if tf_config.get('terraform', {}).get('backend'):\n                    [(_s3key, tffile_backend_config)] = tf_config['terraform']['backend'].items()  # noqa pylint: disable=line-too-long\n                    desired_backend_config = merge_dicts(\n                        desired_backend_config,\n                        tffile_backend_config\n                    )\n                    break\n\n    if current_backend_config != desired_backend_config:\n        LOGGER.info(\"Desired and previously initialized TF backend config is \"\n                    \"out of sync; trashing local TF state directory %s\",\n                    terraform_dir)\n        send2trash(terraform_dir)\n        run_terraform_init(\n            tf_bin=tf_bin,\n            module_path=module_path,\n            backend_options=backend_options,\n            env_name=env_name,\n            env_region=env_region,\n            env_vars=env_vars\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_terraform(self, command='plan'):  # noqa pylint: disable=too-many-branches,too-many-statements\n        response = {'skipped_configs': False}\n        tf_cmd = [command]\n\n        if command == 'destroy':\n            tf_cmd.append('-force')\n        elif command == 'apply':\n            if 'CI' in self.context.env_vars:\n                tf_cmd.append('-auto-approve=true')\n            else:\n                tf_cmd.append('-auto-approve=false')\n\n        workspace_tfvars_file = get_workspace_tfvars_file(self.path,\n                                                          self.context.env_name,  # noqa\n                                                          self.context.env_region)  # noqa\n        backend_options = create_config_backend_options(self.options.get('options', {}),  # noqa\n                                                        self.context.env_name,\n                                                        self.context.env_vars)\n        # This filename will only be used if it exists\n        backend_options['filename'] = get_backend_tfvars_file(\n            self.path,\n            self.context.env_name,\n            self.context.env_region\n        )\n        workspace_tfvar_present = os.path.isfile(\n            os.path.join(self.path, workspace_tfvars_file)\n        )\n        if workspace_tfvar_present:\n            tf_cmd.append(\"-var-file=%s\" % workspace_tfvars_file)\n        if isinstance(self.options.get('environments',\n                                       {}).get(self.context.env_name),\n                      dict):\n            for (key, val) in self.options['environments'][self.context.env_name].items():  # noqa\n                tf_cmd.extend(['-var', \"%s=%s\" % (key, val)])\n\n        if self.options.get('environments', {}).get(self.context.env_name) or (\n                workspace_tfvar_present):\n            LOGGER.info(\"Preparing to run terraform %s on %s...\",\n                        command,\n                        os.path.basename(self.path))\n            module_defined_tf_var = get_module_defined_tf_var(\n                self.options.get('options', {}).get('terraform_version', {}),\n                self.context.env_name\n            )\n            if module_defined_tf_var:\n                tf_bin = TFEnv(self.path).install(module_defined_tf_var)\n            elif os.path.isfile(os.path.join(self.path,\n                                             '.terraform-version')):\n                tf_bin = TFEnv(self.path).install()\n            else:\n                if not which('terraform'):\n                    LOGGER.error('Terraform not available (a '\n                                 '\".terraform-version\" file is not present '\n                                 'and \"terraform\" not found in path). Fix '\n                                 'this by writing a desired Terraform version '\n                                 'to your module\\'s .terraform-version file '\n                                 'or installing Terraform.')\n                    sys.exit(1)\n                tf_bin = 'terraform'\n            tf_cmd.insert(0, tf_bin)\n            with change_dir(self.path):\n                if not os.path.isdir(os.path.join(self.path, '.terraform')) or (  # noqa\n                        os.path.isfile(os.path.join(self.path,\n                                                    '.terraform',\n                                                    FAILED_INIT_FILENAME))):\n                    if os.path.isfile(os.path.join(self.path,\n                                                   '.terraform',\n                                                   FAILED_INIT_FILENAME)):\n                        LOGGER.info('Previous init failed; trashing '\n                                    '.terraform directory and running it '\n                                    'again...')\n                        send2trash(os.path.join(self.path, '.terraform'))\n                    else:\n                        LOGGER.info('.terraform directory missing; running '\n                                    '\"terraform init\"...')\n                    run_terraform_init(\n                        tf_bin=tf_bin,\n                        module_path=self.path,\n                        backend_options=backend_options,\n                        env_name=self.context.env_name,\n                        env_region=self.context.env_region,\n                        env_vars=self.context.env_vars\n                    )\n                else:\n                    reinit_on_backend_changes(\n                        tf_bin=tf_bin,\n                        module_path=self.path,\n                        backend_options=backend_options,\n                        env_name=self.context.env_name,\n                        env_region=self.context.env_region,\n                        env_vars=self.context.env_vars\n                    )\n                LOGGER.debug('Checking current Terraform workspace...')\n                current_tf_workspace = subprocess.check_output(\n                    [tf_bin,\n                     'workspace',\n                     'show'],\n                    env=self.context.env_vars\n                ).strip().decode()\n                if current_tf_workspace != self.context.env_name:\n                    LOGGER.info(\"Terraform workspace currently set to %s; \"\n                                \"switching to %s...\",\n                                current_tf_workspace,\n                                self.context.env_name)\n                    LOGGER.debug('Checking available Terraform '\n                                 'workspaces...')\n                    available_tf_envs = subprocess.check_output(\n                        [tf_bin, 'workspace', 'list'],\n                        env=self.context.env_vars\n                    ).decode()\n                    if re.compile(\"^[*\\\\s]\\\\s%s$\" % self.context.env_name,\n                                  re.M).search(available_tf_envs):\n                        run_module_command(\n                            cmd_list=[tf_bin, 'workspace', 'select',\n                                      self.context.env_name],\n                            env_vars=self.context.env_vars\n                        )\n                    else:\n                        LOGGER.info(\"Terraform workspace %s not found; \"\n                                    \"creating it...\",\n                                    self.context.env_name)\n                        run_module_command(\n                            cmd_list=[tf_bin, 'workspace', 'new',\n                                      self.context.env_name],\n                            env_vars=self.context.env_vars\n                        )\n                    # Previously, another tf init was run here after every\n                    # workspace switch/creation. That does not appear to be\n                    # necessary now (this note can be removed in the future,\n                    # i.e. after 1.0)\n                if 'SKIP_TF_GET' not in self.context.env_vars:\n                    LOGGER.info('Executing \"terraform get\" to update remote '\n                                'modules')\n                    run_module_command(\n                        cmd_list=[tf_bin, 'get', '-update=true'],\n                        env_vars=self.context.env_vars\n                    )\n                else:\n                    LOGGER.info('Skipping \"terraform get\" due to '\n                                '\"SKIP_TF_GET\" environment variable...')\n                LOGGER.info(\"Running Terraform %s on %s (\\\"%s\\\")\",\n                            command,\n                            os.path.basename(self.path),\n                            \" \".join(tf_cmd))\n                run_module_command(cmd_list=tf_cmd,\n                                   env_vars=self.context.env_vars)\n        else:\n            response['skipped_configs'] = True\n            LOGGER.info(\"Skipping Terraform %s of %s\",\n                        command,\n                        os.path.basename(self.path))\n            LOGGER.info(\n                \"(no tfvars file for this environment/region found -- looking \"\n                \"for one of \\\"%s\\\")\",\n                ', '.join(gen_workspace_tfvars_files(\n                    self.context.env_name,\n                    self.context.env_region)))\n        return response", "response": "Runs Terraform on the current context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_command_class(possible_command_names):\n    for command_name in possible_command_names:\n        if hasattr(ALL_COMMANDS_MODULE, command_name):\n            command_module = getattr(ALL_COMMANDS_MODULE, command_name)\n            command_class_hierarchy = getmembers(command_module, isclass)\n            command_class_tuple = list(filter(_not_base_class, command_class_hierarchy))[0]\n            return command_class_tuple[1]\n    return None", "response": "Try to find a class for one of the given command names."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates skeleton sample module.", "response": "def generate_sample_module(module_dir):\n    \"\"\"Generate skeleton sample module.\"\"\"\n    if os.path.isdir(module_dir):\n        LOGGER.error(\"Error generating sample module -- directory %s \"\n                     \"already exists!\",\n                     module_dir)\n        sys.exit(1)\n    os.mkdir(module_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_sample_sls_module(env_root, module_dir=None):\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.sls')\n    generate_sample_module(module_dir)\n    for i in ['config-dev-us-east-1.json', 'handler.py', 'package.json',\n              'serverless.yml']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'serverless',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    LOGGER.info(\"Sample Serverless module created at %s\",\n                module_dir)", "response": "Generate skeleton Serverless sample module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_sample_sls_tsc_module(env_root, module_dir=None):\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.sls')\n    generate_sample_module(module_dir)\n    for i in ['package.json', 'serverless.yml', 'tsconfig.json',\n              'webpack.config.js']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'sls-tsc',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    os.mkdir(os.path.join(module_dir, 'src'))\n    for i in ['handler.spec.ts', 'handler.ts']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'sls-tsc',\n                         'src',\n                         i),\n            os.path.join(module_dir, 'src', i),\n        )\n    LOGGER.info(\"Sample Serverless TypeScript module created at %s\",\n                module_dir)", "response": "Generate skeleton Serverless TypeScript sample module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate skeleton CDK TS sample module.", "response": "def generate_sample_cdk_tsc_module(env_root, module_dir=None):\n    \"\"\"Generate skeleton CDK TS sample module.\"\"\"\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.cdk')\n    generate_sample_module(module_dir)\n    for i in ['.npmignore', 'cdk.json', 'package.json', 'runway.module.yml',\n              'tsconfig.json', 'README.md']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cdk-tsc',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    for i in [['bin', 'sample.ts'], ['lib', 'sample-stack.ts']]:\n        os.mkdir(os.path.join(module_dir, i[0]))\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cdk-tsc',\n                         i[0],\n                         i[1]),\n            os.path.join(module_dir, i[0], i[1]),\n        )\n    with open(os.path.join(module_dir, '.gitignore'), 'w') as stream:\n        stream.write('*.js\\n')\n        stream.write('*.d.ts\\n')\n        stream.write('node_modules\\n')\n    LOGGER.info(\"Sample CDK module created at %s\", module_dir)\n    LOGGER.info('To finish its setup, change to the %s directory and execute '\n                '\"npm install\" to generate its lockfile.', module_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate skeleton CDK C# sample module.", "response": "def generate_sample_cdk_cs_module(env_root, module_dir=None):\n    \"\"\"Generate skeleton CDK C# sample module.\"\"\"\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.cdk')\n    generate_sample_module(module_dir)\n    for i in ['add-project.hook.d.ts', 'cdk.json', 'package.json',\n              'runway.module.yml', 'README.md']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cdk-csharp',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    shutil.copyfile(\n        os.path.join(ROOT,\n                     'templates',\n                     'cdk-csharp',\n                     'dot_gitignore'),\n        os.path.join(module_dir, '.gitignore'),\n    )\n    os.mkdir(os.path.join(module_dir, 'src'))\n    shutil.copyfile(\n        os.path.join(ROOT,\n                     'templates',\n                     'cdk-csharp',\n                     'src',\n                     'HelloCdk.sln'),\n        os.path.join(module_dir, 'src', 'HelloCdk.sln'),\n    )\n    os.mkdir(os.path.join(module_dir, 'src', 'HelloCdk'))\n    for i in ['HelloCdk.csproj', 'HelloConstruct.cs', 'HelloStack.cs',\n              'Program.cs']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cdk-csharp',\n                         'src',\n                         'HelloCdk',\n                         i),\n            os.path.join(module_dir, 'src', 'HelloCdk', i),\n        )\n    LOGGER.info(\"Sample C# CDK module created at %s\", module_dir)\n    LOGGER.info('To finish its setup, change to the %s directory and execute '\n                '\"npm install\" to generate its lockfile.', module_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_sample_cdk_py_module(env_root, module_dir=None):\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.cdk')\n    generate_sample_module(module_dir)\n    for i in ['app.py', 'cdk.json', 'lambda-index.py', 'package.json',\n              'runway.module.yml', 'Pipfile']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cdk-py',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    with open(os.path.join(module_dir, '.gitignore'), 'w') as stream:\n        stream.write('node_modules')\n    LOGGER.info(\"Sample CDK module created at %s\", module_dir)\n    LOGGER.info('To finish its setup, change to the %s directory and execute '\n                '\"npm install\" and \"pipenv update -d --three\" to generate its '\n                'lockfiles.', module_dir)", "response": "Generate skeleton CDK python sample module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates skeleton CloudFormation sample module.", "response": "def generate_sample_cfn_module(env_root, module_dir=None):\n    \"\"\"Generate skeleton CloudFormation sample module.\"\"\"\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.cfn')\n    generate_sample_module(module_dir)\n    for i in ['stacks.yaml', 'dev-us-east-1.env']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'cfn',\n                         i),\n            os.path.join(module_dir, i)\n        )\n    os.mkdir(os.path.join(module_dir, 'templates'))\n    with open(os.path.join(module_dir,\n                           'templates',\n                           'tf_state.yml'), 'w') as stream:\n        stream.write(\n            cfn_flip.flip(\n                check_output(\n                    [sys.executable,\n                     os.path.join(ROOT,\n                                  'templates',\n                                  'stacker',\n                                  'tfstate_blueprints',\n                                  'tf_state.py')]\n                )\n\n            )\n        )\n    LOGGER.info(\"Sample CloudFormation module created at %s\",\n                module_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_sample_stacker_module(env_root, module_dir=None):\n    if module_dir is None:\n        module_dir = os.path.join(env_root,\n                                  'runway-sample-tfstate.cfn')\n    generate_sample_module(module_dir)\n    for i in ['stacks.yaml', 'dev-us-east-1.env']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'stacker',\n                         i),\n            os.path.join(module_dir, i)\n        )\n    os.mkdir(os.path.join(module_dir, 'tfstate_blueprints'))\n    for i in ['__init__.py', 'tf_state.py']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'stacker',\n                         'tfstate_blueprints',\n                         i),\n            os.path.join(module_dir, 'tfstate_blueprints', i)\n        )\n    os.chmod(  # make blueprint executable\n        os.path.join(module_dir, 'tfstate_blueprints', 'tf_state.py'),\n        os.stat(os.path.join(module_dir,\n                             'tfstate_blueprints',\n                             'tf_state.py')).st_mode | 0o0111\n    )\n    LOGGER.info(\"Sample Stacker module created at %s\",\n                module_dir)", "response": "Generate skeleton Stacker sample module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_sample_tf_module(env_root, module_dir=None):\n    if module_dir is None:\n        module_dir = os.path.join(env_root, 'sampleapp.tf')\n    generate_sample_module(module_dir)\n    for i in ['backend-us-east-1.tfvars', 'dev-us-east-1.tfvars', 'main.tf']:\n        shutil.copyfile(\n            os.path.join(ROOT,\n                         'templates',\n                         'terraform',\n                         i),\n            os.path.join(module_dir, i),\n        )\n    tf_ver_template = os.path.join(ROOT,\n                                   'templates',\n                                   'terraform',\n                                   '.terraform-version')\n    if os.path.isfile(tf_ver_template):\n        shutil.copyfile(\n            tf_ver_template,\n            os.path.join(module_dir, '.terraform-version'),\n        )\n    else:  # running directly from git\n        latest_tf_ver = get_latest_tf_version()\n        with open(os.path.join(module_dir,\n                               '.terraform-version'), 'w') as stream:\n            stream.write(latest_tf_ver)\n\n    LOGGER.info(\"Sample Terraform app created at %s\",\n                module_dir)", "response": "Generate skeleton Terraform sample module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self):\n        if self._cli_arguments['cfn']:\n            generate_sample_cfn_module(self.env_root)\n        elif self._cli_arguments['sls']:\n            generate_sample_sls_module(self.env_root)\n        elif self._cli_arguments['sls-tsc']:\n            generate_sample_sls_tsc_module(self.env_root)\n        elif self._cli_arguments['stacker']:\n            generate_sample_stacker_module(self.env_root)\n        elif self._cli_arguments['tf']:\n            generate_sample_tf_module(self.env_root)\n        elif self._cli_arguments['cdk-tsc']:\n            generate_sample_cdk_tsc_module(self.env_root)\n        elif self._cli_arguments['cdk-py']:\n            generate_sample_cdk_py_module(self.env_root)\n        elif self._cli_arguments['cdk-csharp']:\n            generate_sample_cdk_cs_module(self.env_root)", "response": "Execute the selected module generator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self):  # pylint: disable=no-self-use\n        if os.path.isfile('runway.yml'):\n            print('Runway config already present')\n            sys.exit(1)\n        with open('runway.yml', 'w') as stream:\n            stream.write(\"\"\"---\ndeployments:\n  - modules:\n      - nameofmyfirstmodulefolder\n      - nameofmysecondmodulefolder\n      # - etc...\n    regions:\n      - us-east-1\n\"\"\")\n        print('runway.yml generated')\n        print('See additional getting started information at '\n              'https://docs.onica.com/projects/runway/en/latest/how_to_use.html')", "response": "Execute the main entry point for the runway."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_template(self):\n        template = self.template\n        # variables = self.get_variables()\n        template.add_version('2010-09-09')\n        template.add_description('Static Website - Dependencies')\n\n        # Resources\n        awslogbucket = template.add_resource(\n            s3.Bucket(\n                'AWSLogBucket',\n                AccessControl=s3.Private,\n                VersioningConfiguration=s3.VersioningConfiguration(\n                    Status='Enabled'\n                )\n            )\n        )\n        template.add_output(Output(\n            'AWSLogBucketName',\n            Description='Name of bucket storing AWS logs',\n            Value=awslogbucket.ref()\n        ))\n\n        template.add_resource(\n            s3.BucketPolicy(\n                'AllowAWSLogWriting',\n                Bucket=awslogbucket.ref(),\n                PolicyDocument=Policy(\n                    Version='2012-10-17',\n                    Statement=[\n                        Statement(\n                            Action=[awacs.s3.PutObject],\n                            Effect=Allow,\n                            Principal=AWSPrincipal(Join(':',\n                                                        ['arn:aws:iam:',\n                                                         AccountId,\n                                                         'root'])),\n                            Resource=[\n                                Join('', ['arn:aws:s3:::',\n                                          awslogbucket.ref(),\n                                          '/*'])\n                            ]\n                        )\n                    ]\n                )\n            )\n        )\n        artifacts = template.add_resource(\n            s3.Bucket(\n                'Artifacts',\n                AccessControl=s3.Private,\n                LifecycleConfiguration=s3.LifecycleConfiguration(\n                    Rules=[\n                        s3.LifecycleRule(\n                            NoncurrentVersionExpirationInDays=90,\n                            Status='Enabled'\n                        )\n                    ]\n                ),\n                VersioningConfiguration=s3.VersioningConfiguration(\n                    Status='Enabled'\n                )\n            )\n        )\n        template.add_output(Output(\n            'ArtifactsBucketName',\n            Description='Name of bucket storing artifacts',\n            Value=artifacts.ref()\n        ))", "response": "Create the template for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef does_s3_object_exist(bucket_name, key, session=None):\n    if session:\n        s3_resource = session.resource('s3')\n    else:\n        s3_resource = boto3.resource('s3')\n\n    try:\n        s3_resource.Object(bucket_name, key).load()\n    except ClientError as exc:\n        if exc.response['Error']['Code'] == '404':\n            return False\n        raise\n    return True", "response": "Determine if object exists on s3."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_and_extract_to_mkdtemp(bucket, key, session=None):\n    if session:\n        s3_client = session.client('s3')\n    else:\n        s3_client = boto3.client('s3')\n    transfer = S3Transfer(s3_client)\n\n    filedes, temp_file = tempfile.mkstemp()\n    os.close(filedes)\n    transfer.download_file(bucket, key, temp_file)\n\n    output_dir = tempfile.mkdtemp()\n    zip_ref = zipfile.ZipFile(temp_file, 'r')\n    zip_ref.extractall(output_dir)\n    zip_ref.close()\n    os.remove(temp_file)\n    return output_dir", "response": "Download zip archive and extract it to temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nzipping built static site and upload to S3.", "response": "def zip_and_upload(app_dir, bucket, key, session=None):\n    \"\"\"Zip built static site and upload to S3.\"\"\"\n    if session:\n        s3_client = session.client('s3')\n    else:\n        s3_client = boto3.client('s3')\n    transfer = S3Transfer(s3_client)\n\n    filedes, temp_file = tempfile.mkstemp()\n    os.close(filedes)\n    LOGGER.info(\"staticsite: archiving app at %s to s3://%s/%s\",\n                app_dir, bucket, key)\n    with zipfile.ZipFile(temp_file, 'w', zipfile.ZIP_DEFLATED) as filehandle:\n        with change_dir(app_dir):\n            for dirname, _subdirs, files in os.walk('./'):\n                if dirname != './':\n                    filehandle.write(dirname)\n                for filename in files:\n                    filehandle.write(os.path.join(dirname, filename))\n    transfer.upload_file(temp_file, bucket, key)\n    os.remove(temp_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(context, provider, **kwargs):  # pylint: disable=unused-argument\n    session = get_session(provider.region)\n    options = kwargs.get('options', {})\n    context_dict = {}\n    context_dict['artifact_key_prefix'] = \"%s-%s-\" % (options['namespace'], options['name'])  # noqa\n    default_param_name = \"%shash\" % context_dict['artifact_key_prefix']\n\n    if options.get('build_output'):\n        build_output = os.path.join(\n            options['path'],\n            options['build_output']\n        )\n    else:\n        build_output = options['path']\n\n    context_dict['artifact_bucket_name'] = RxrefLookup.handle(\n        kwargs.get('artifact_bucket_rxref_lookup'),\n        provider=provider,\n        context=context\n    )\n\n    if options.get('pre_build_steps'):\n        run_commands(options['pre_build_steps'], options['path'])\n\n    context_dict['hash'] = get_hash_of_files(\n        root_path=options['path'],\n        directories=options.get('source_hashing', {}).get('directories')\n    )\n\n    # Now determine if the current staticsite has already been deployed\n    if options.get('source_hashing', {}).get('enabled', True):\n        context_dict['hash_tracking_parameter'] = options.get(\n            'source_hashing', {}).get('parameter', default_param_name)\n\n        ssm_client = session.client('ssm')\n\n        try:\n            old_parameter_value = ssm_client.get_parameter(\n                Name=context_dict['hash_tracking_parameter']\n            )['Parameter']['Value']\n        except ssm_client.exceptions.ParameterNotFound:\n            old_parameter_value = None\n    else:\n        context_dict['hash_tracking_disabled'] = True\n        old_parameter_value = None\n\n    context_dict['current_archive_filename'] = (\n        context_dict['artifact_key_prefix'] + context_dict['hash'] + '.zip'\n    )\n    if old_parameter_value:\n        context_dict['old_archive_filename'] = (\n            context_dict['artifact_key_prefix'] + old_parameter_value + '.zip'\n        )\n\n    if old_parameter_value == context_dict['hash']:\n        LOGGER.info(\"staticsite: skipping build; app hash %s already deployed \"\n                    \"in this environment\",\n                    context_dict['hash'])\n        context_dict['deploy_is_current'] = True\n        return context_dict\n\n    if does_s3_object_exist(context_dict['artifact_bucket_name'],\n                            context_dict['current_archive_filename'],\n                            session):\n        context_dict['app_directory'] = download_and_extract_to_mkdtemp(\n            context_dict['artifact_bucket_name'],\n            context_dict['current_archive_filename'], session\n        )\n    else:\n        if options.get('build_steps'):\n            LOGGER.info('staticsite: executing build commands')\n            run_commands(options['build_steps'], options['path'])\n        zip_and_upload(build_output, context_dict['artifact_bucket_name'],\n                       context_dict['current_archive_filename'], session)\n        context_dict['app_directory'] = build_output\n\n    context_dict['deploy_is_current'] = False\n    return context_dict", "response": "Builds a static site."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexiting if config is invalid.", "response": "def ensure_valid_environment_config(module_name, config):\n    \"\"\"Exit if config is invalid.\"\"\"\n    if not config.get('namespace'):\n        LOGGER.fatal(\"staticsite: module %s's environment configuration is \"\n                     \"missing a namespace definition!\",\n                     module_name)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_website_module(self, command):\n        name = self.options.get('name') if self.options.get('name') else self.options.get('path')  # noqa pylint: disable=line-too-long\n        ensure_valid_environment_config(\n            name,\n            self.options.get('environments',\n                             {}).get(self.context.env_name,\n                                     {}))\n        module_dir = tempfile.mkdtemp()\n        LOGGER.info(\"staticsite: Generating CloudFormation configuration for \"\n                    \"module %s in %s\",\n                    name,\n                    module_dir)\n\n        # Default parameter name matches build_staticsite hook\n        hash_param = self.options.get('options', {}).get('source_hashing', {}).get('parameter') if self.options.get('options', {}).get('source_hashing', {}).get('parameter') else \"${namespace}-%s-hash\" % name # noqa pylint: disable=line-too-long\n        build_staticsite_args = self.options.copy()\n        if not build_staticsite_args.get('options'):\n            build_staticsite_args['options'] = {}\n        build_staticsite_args['artifact_bucket_rxref_lookup'] = \"%s-dependencies::ArtifactsBucketName\" % name  # noqa pylint: disable=line-too-long\n        build_staticsite_args['options']['namespace'] = '${namespace}'\n        build_staticsite_args['options']['name'] = name\n        build_staticsite_args['options']['path'] = os.path.join(\n            os.path.realpath(self.context.env_root),\n            self.path\n        )\n\n        with open(os.path.join(module_dir, '01-dependencies.yaml'), 'w') as output_stream:  # noqa\n            yaml.dump(\n                {'namespace': '${namespace}',\n                 'stacker_bucket': '',\n                 'stacks': {\n                     \"%s-dependencies\" % name: {\n                         'class_path': 'runway.blueprints.staticsite.dependencies.Dependencies'}},  # noqa pylint: disable=line-too-long\n                 'pre_destroy': [\n                     {'path': 'runway.hooks.cleanup_s3.purge_bucket',\n                      'required': True,\n                      'args': {\n                          'bucket_rxref_lookup': \"%s-dependencies::%s\" % (name, i)  # noqa\n                      }} for i in ['AWSLogBucketName', 'ArtifactsBucketName']\n                 ]},\n                output_stream,\n                default_flow_style=False\n            )\n        site_stack_variables = {\n            'Aliases': '${default staticsite_aliases::undefined}',\n            'RewriteDirectoryIndex': '${default staticsite_rewrite_directory_index::undefined}',  # noqa pylint: disable=line-too-long\n            'WAFWebACL': '${default staticsite_web_acl::undefined}'\n        }\n        if self.options.get('environments',\n                            {}).get(self.context.env_name,\n                                    {}).get('staticsite_enable_cf_logging',\n                                            True):\n            site_stack_variables['LogBucketName'] = \"${rxref %s-dependencies::AWSLogBucketName}\" % name  # noqa pylint: disable=line-too-long\n        if self.options.get('environments',\n                            {}).get(self.context.env_name,\n                                    {}).get('staticsite_acmcert_ssm_param'):\n            site_stack_variables['AcmCertificateArn'] = '${ssmstore ${staticsite_acmcert_ssm_param}}'  # noqa pylint: disable=line-too-long\n        else:\n            site_stack_variables['AcmCertificateArn'] = '${default staticsite_acmcert_arn::undefined}'  # noqa pylint: disable=line-too-long\n        # If staticsite_lambda_function_associations defined, add to stack\n        # config\n        if self.options.get('environments',\n                            {}).get(self.context.env_name,\n                                    {}).get('staticsite_lambda_function_associations'):  # noqa\n            site_stack_variables['lambda_function_associations'] = self.options.get(  # noqa\n                'environments',\n                {}\n            ).get(self.context.env_name,\n                  {}).get('staticsite_lambda_function_associations')\n            self.options.get('environments',\n                             {}).get(self.context.env_name,\n                                     {}).pop('staticsite_lambda_function_associations')  # noqa\n        with open(os.path.join(module_dir, '02-staticsite.yaml'), 'w') as output_stream:  # noqa\n            yaml.dump(\n                {'namespace': '${namespace}',\n                 'stacker_bucket': '',\n                 'pre_build': [\n                     {'path': 'runway.hooks.staticsite.build_staticsite.build',\n                      'required': True,\n                      'data_key': 'staticsite',\n                      'args': build_staticsite_args}\n                 ],\n                 'stacks': {\n                     name: {\n                         'class_path': 'runway.blueprints.staticsite.staticsite.StaticSite',  # noqa\n                         'variables': site_stack_variables}},\n                 'post_build': [\n                     {'path': 'runway.hooks.staticsite.upload_staticsite.sync',\n                      'required': True,\n                      'args': {\n                          'bucket_output_lookup': '%s::BucketName' % name,\n                          'distributionid_output_lookup': '%s::CFDistributionId' % name,  # noqa\n                          'distributiondomain_output_lookup': '%s::CFDistributionDomainName' % name}}  # noqa pylint: disable=line-too-long\n                 ],\n                 'pre_destroy': [\n                     {'path': 'runway.hooks.cleanup_s3.purge_bucket',\n                      'required': True,\n                      'args': {\n                          'bucket_rxref_lookup': \"%s::BucketName\" % name\n                      }}\n                 ],\n                 'post_destroy': [\n                     {'path': 'runway.hooks.cleanup_ssm.delete_param',\n                      'args': {\n                          'parameter_name': hash_param\n                      }}\n                 ]},\n                output_stream,\n                default_flow_style=False\n            )\n\n        cfn = CloudFormation(\n            self.context,\n            module_dir,\n            {i: self.options[i] for i in self.options if i != 'class_path'}\n        )\n        getattr(cfn, command)()", "response": "Create stacker configuration for website module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plan(self):\n        if self.options.get('environments', {}).get(self.context.env_name):\n            self.setup_website_module(command='plan')\n        else:\n            LOGGER.info(\"Skipping staticsite plan of %s; no environment \"\n                        \"config found for this environment/region\",\n                        self.options['path'])", "response": "Create website CFN module and run stacker diff."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert npm command list to string for display to user.", "response": "def format_npm_command_for_logging(command):\n    \"\"\"Convert npm command list to string for display to user.\"\"\"\n    if platform.system().lower() == 'windows':\n        if command[0] == 'npx.cmd' and command[1] == '-c':\n            return \"npx.cmd -c \\\"%s\\\"\" % \" \".join(command[2:])\n        return \" \".join(command)\n    # Strip out redundant npx quotes not needed when executing the command\n    # directly\n    return \" \".join(command).replace('\\'\\'', '\\'')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_node_command(command, command_opts, path):\n    if which(NPX_BIN):\n        # Use npx if available (npm v5.2+)\n        LOGGER.debug(\"Using npx to invoke %s.\", command)\n        if platform.system().lower() == 'windows':\n            cmd_list = [NPX_BIN,\n                        '-c',\n                        \"%s %s\" % (command, ' '.join(command_opts))]\n        else:\n            # The nested app-through-npx-via-subprocess command invocation\n            # requires this redundant quoting\n            cmd_list = [NPX_BIN,\n                        '-c',\n                        \"''%s %s''\" % (command, ' '.join(command_opts))]\n    else:\n        LOGGER.debug('npx not found; falling back invoking %s shell script '\n                     'directly.', command)\n        cmd_list = [\n            os.path.join(path,\n                         'node_modules',\n                         '.bin',\n                         command)\n        ] + command_opts\n    return cmd_list", "response": "Generate node bin command list for subprocess execution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nshelling out to provisioner command.", "response": "def run_module_command(cmd_list, env_vars, exit_on_error=True):\n    \"\"\"Shell out to provisioner command.\"\"\"\n    if exit_on_error:\n        try:\n            subprocess.check_call(cmd_list, env=env_vars)\n        except subprocess.CalledProcessError as shelloutexc:\n            sys.exit(shelloutexc.returncode)\n    else:\n        subprocess.check_call(cmd_list, env=env_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if npm ci should be used in lieu of npm install.", "response": "def use_npm_ci(path):\n    \"\"\"Return true if npm ci should be used in lieu of npm install.\"\"\"\n    # https://docs.npmjs.com/cli/ci#description\n    with open(os.devnull, 'w') as fnull:\n        if ((os.path.isfile(os.path.join(path,\n                                         'package-lock.json')) or\n             os.path.isfile(os.path.join(path,\n                                         'npm-shrinkwrap.json'))) and\n                subprocess.call(\n                    [NPM_BIN, 'ci', '-h'],\n                    stdout=fnull,\n                    stderr=subprocess.STDOUT\n                ) == 0):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn bool on whether cdk command should continue in current env.", "response": "def cdk_module_matches_env(env_name, env_config, env_vars):\n    \"\"\"Return bool on whether cdk command should continue in current env.\"\"\"\n    if env_config.get(env_name):\n        current_env_config = env_config[env_name]\n        if isinstance(current_env_config, type(True)) and current_env_config:\n            return True\n        if isinstance(current_env_config, six.string_types):\n            (account_id, region) = current_env_config.split('/')\n            if region == env_vars['AWS_DEFAULT_REGION']:\n                boto_args = extract_boto_args_from_env(env_vars)\n                sts_client = boto3.client(\n                    'sts',\n                    region_name=env_vars['AWS_DEFAULT_REGION'],\n                    **boto_args\n                )\n                if sts_client.get_caller_identity()['Account'] == account_id:\n                    return True\n        if isinstance(current_env_config, dict):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list of CDK stacks.", "response": "def get_cdk_stacks(module_path, env_vars, context_opts):\n    \"\"\"Return list of CDK stacks.\"\"\"\n    LOGGER.debug('Listing stacks in the CDK app prior to '\n                 'diff')\n    return subprocess.check_output(\n        generate_node_command(\n            command='cdk',\n            command_opts=['list'] + context_opts,\n            path=module_path),\n        env=env_vars\n    ).strip().split('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining type of module and return deployment module class.", "response": "def determine_module_class(path, class_path):\n    \"\"\"Determine type of module and return deployment module class.\"\"\"\n    if not class_path:\n        # First check directory name for type-indicating suffix\n        basename = os.path.basename(path)\n        if basename.endswith('.sls'):\n            class_path = 'runway.module.serverless.Serverless'\n        elif basename.endswith('.tf'):\n            class_path = 'runway.module.terraform.Terraform'\n        elif basename.endswith('.cdk'):\n            class_path = 'runway.module.cdk.CloudDevelopmentKit'\n        elif basename.endswith('.cfn'):\n            class_path = 'runway.module.cloudformation.CloudFormation'\n\n    if not class_path:\n        # Fallback to autodetection\n        if os.path.isfile(os.path.join(path, 'serverless.yml')):\n            class_path = 'runway.module.serverless.Serverless'\n        elif glob.glob(os.path.join(path, '*.tf')):\n            class_path = 'runway.module.terraform.Terraform'\n        elif os.path.isfile(os.path.join(path, 'cdk.json')) \\\n                and os.path.isfile(os.path.join(path, 'package.json')):\n            class_path = 'runway.module.cdk.CloudDevelopmentKit'\n        elif glob.glob(os.path.join(path, '*.env')) or (\n                glob.glob(os.path.join(path, '*.yaml'))) or (\n                    glob.glob(os.path.join(path, '*.yml'))):\n            class_path = 'runway.module.cloudformation.CloudFormation'\n\n    if not class_path:\n        LOGGER.error('No module class found for %s', os.path.basename(path))\n        sys.exit(1)\n\n    return load_object_from_string(class_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_module_opts_from_file(path, module_options):\n    module_options_file = os.path.join(path,\n                                       'runway.module.yml')\n    if os.path.isfile(module_options_file):\n        with open(module_options_file, 'r') as stream:\n            module_options = merge_dicts(module_options,\n                                         yaml.safe_load(stream))\n    return module_options", "response": "Load any options defined in module. yml file in path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreverting to previous credentials if necessary.", "response": "def post_deploy_assume_role(assume_role_config, context):\n    \"\"\"Revert to previous credentials, if necessary.\"\"\"\n    if isinstance(assume_role_config, dict):\n        if assume_role_config.get('post_deploy_env_revert'):\n            context.restore_existing_iam_env_vars()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_deploy_assume_role(assume_role_config, context):\n    if isinstance(assume_role_config, dict):\n        assume_role_arn = ''\n        if assume_role_config.get('post_deploy_env_revert'):\n            context.save_existing_iam_env_vars()\n        if assume_role_config.get('arn'):\n            assume_role_arn = assume_role_config['arn']\n            assume_role_duration = assume_role_config.get('duration')\n        elif assume_role_config.get(context.env_name):\n            if isinstance(assume_role_config[context.env_name], dict):\n                assume_role_arn = assume_role_config[context.env_name]['arn']  # noqa\n                assume_role_duration = assume_role_config[context.env_name].get('duration')  # noqa pylint: disable=line-too-long\n            else:\n                assume_role_arn = assume_role_config[context.env_name]\n                assume_role_duration = None\n        else:\n            LOGGER.info('Skipping assume-role; no role found for '\n                        'environment %s...',\n                        context.env_name)\n\n        if assume_role_arn:\n            context.env_vars = merge_dicts(\n                context.env_vars,\n                assume_role(\n                    role_arn=assume_role_arn,\n                    session_name=assume_role_config.get('session_name', None),\n                    duration_seconds=assume_role_duration,\n                    region=context.env_region,\n                    env_vars=context.env_vars\n                )\n            )\n    else:\n        context.env_vars = merge_dicts(\n            context.env_vars,\n            assume_role(role_arn=assume_role_config,\n                        region=context.env_region,\n                        env_vars=context.env_vars)\n        )", "response": "Pre - deploy assume role."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexit if list_account_aliases doesn t include account_alias.", "response": "def validate_account_alias(iam_client, account_alias):\n    \"\"\"Exit if list_account_aliases doesn't include account_alias.\"\"\"\n    # Super overkill here using pagination when an account can only\n    # have a single alias, but at least this implementation should be\n    # future-proof\n    current_account_aliases = []\n    paginator = iam_client.get_paginator('list_account_aliases')\n    response_iterator = paginator.paginate()\n    for page in response_iterator:\n        current_account_aliases.extend(page.get('AccountAliases', []))\n    if account_alias in current_account_aliases:\n        LOGGER.info('Verified current AWS account alias matches required '\n                    'alias %s.',\n                    account_alias)\n    else:\n        LOGGER.error('Current AWS account aliases \"%s\" do not match '\n                     'required account alias %s in Runway config.',\n                     ','.join(current_account_aliases),\n                     account_alias)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_account_id(sts_client, account_id):\n    resp = sts_client.get_caller_identity()\n    if 'Account' in resp:\n        if resp['Account'] == account_id:\n            LOGGER.info('Verified current AWS account matches required '\n                        'account id %s.',\n                        account_id)\n        else:\n            LOGGER.error('Current AWS account %s does not match '\n                         'required account %s in Runway config.',\n                         resp['Account'],\n                         account_id)\n            sys.exit(1)\n    else:\n        LOGGER.error('Error checking current account ID')\n        sys.exit(1)", "response": "Exit if get_caller_identity doesn t match account_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_account_credentials(deployment, context):\n    boto_args = {'region_name': context.env_vars['AWS_DEFAULT_REGION']}\n    for i in ['aws_access_key_id', 'aws_secret_access_key',\n              'aws_session_token']:\n        if context.env_vars.get(i.upper()):\n            boto_args[i] = context.env_vars[i.upper()]\n    if isinstance(deployment.get('account-id'), (int, six.string_types)):\n        account_id = str(deployment['account-id'])\n    elif deployment.get('account-id', {}).get(context.env_name):\n        account_id = str(deployment['account-id'][context.env_name])\n    else:\n        account_id = None\n    if account_id:\n        validate_account_id(boto3.client('sts', **boto_args), account_id)\n    if isinstance(deployment.get('account-alias'), six.string_types):\n        account_alias = deployment['account-alias']\n    elif deployment.get('account-alias', {}).get(context.env_name):\n        account_alias = deployment['account-alias'][context.env_name]\n    else:\n        account_alias = None\n    if account_alias:\n        validate_account_alias(boto3.client('iam', **boto_args),\n                               account_alias)", "response": "Exit if requested deployment account doesn t match credentials."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a helper note about how the environment was determined.", "response": "def echo_detected_environment(env_name, env_vars):\n    \"\"\"Print a helper note about how the environment was determined.\"\"\"\n    env_override_name = 'DEPLOY_ENVIRONMENT'\n    LOGGER.info(\"\")\n    if env_override_name in env_vars:\n        LOGGER.info(\"Environment \\\"%s\\\" was determined from the %s environment variable.\",\n                    env_name,\n                    env_override_name)\n        LOGGER.info(\"If this is not correct, update \"\n                    \"the value (or unset it to fall back to the name of \"\n                    \"the current git branch or parent directory).\")\n    else:\n        LOGGER.info(\"Environment \\\"%s\\\" was determined from the current \"\n                    \"git branch or parent directory.\",\n                    env_name)\n        LOGGER.info(\"If this is not the environment name, update the branch/folder name or \"\n                    \"set an override value via the %s environment variable\",\n                    env_override_name)\n    LOGGER.info(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _module_menu_entry(module, environment_name):\n    name = _module_name_for_display(module)\n    if isinstance(module, dict):\n        environment_config = module.get('environments', {}).get(environment_name)\n        if environment_config:\n            return \"%s (%s)\" % (name, environment_config)\n    return \"%s\" % (name)", "response": "Build a string to display in the select module menu."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a string to display in the select deployment menu.", "response": "def _deployment_menu_entry(deployment):\n    \"\"\"Build a string to display in the 'select deployment' menu.\"\"\"\n    paths = \", \".join([_module_name_for_display(module) for module in deployment['modules']])\n    regions = \", \".join(deployment.get('regions', []))\n    return \"%s - %s (%s)\" % (deployment.get('name'), paths, regions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute apps code command.", "response": "def run(self, deployments=None, command='plan'):  # noqa pylint: disable=too-many-branches,too-many-statements\n        \"\"\"Execute apps/code command.\"\"\"\n        if deployments is None:\n            deployments = self.runway_config['deployments']\n        context = Context(env_name=get_env(self.env_root,\n                                           self.runway_config.get('ignore_git_branch', False)),\n                          env_region=None,\n                          env_root=self.env_root,\n                          env_vars=os.environ.copy())\n        echo_detected_environment(context.env_name, context.env_vars)\n\n        # set default names if needed\n        for i, deployment in enumerate(deployments):\n            if not deployment.get('name'):\n                deployment['name'] = 'deployment_' + str(i+1)\n\n        if command == 'destroy':\n            LOGGER.info('WARNING!')\n            LOGGER.info('Runway is running in DESTROY mode.')\n\n        if context.env_vars.get('CI', None):\n            if command == 'destroy':\n                deployments_to_run = self.reverse_deployments(deployments)\n            else:\n                deployments_to_run = deployments\n        else:\n            if command == 'destroy':\n                LOGGER.info('Any/all deployment(s) selected will be '\n                            'irrecoverably DESTROYED.')\n                deployments_to_run = self.reverse_deployments(\n                    self.select_deployment_to_run(\n                        context.env_name,\n                        deployments,\n                        command=command\n                    )\n                )\n            else:\n                deployments_to_run = self.select_deployment_to_run(\n                    context.env_name,\n                    deployments\n                )\n\n        LOGGER.info(\"Found %d deployment(s)\", len(deployments_to_run))\n        for i, deployment in enumerate(deployments_to_run):\n            LOGGER.info(\"\")\n            LOGGER.info(\"\")\n            LOGGER.info(\"======= Processing deployment '%s' ===========================\",\n                        deployment.get('name'))\n\n            if deployment.get('regions'):\n                if deployment.get('env_vars'):\n                    deployment_env_vars = merge_nested_environment_dicts(\n                        deployment.get('env_vars'), env_name=context.env_name,\n                        env_root=self.env_root\n                    )\n                    if deployment_env_vars:\n                        LOGGER.info(\"OS environment variable overrides being \"\n                                    \"applied this deployment: %s\",\n                                    str(deployment_env_vars))\n                    context.env_vars = merge_dicts(context.env_vars, deployment_env_vars)\n\n                LOGGER.info(\"\")\n                LOGGER.info(\"Attempting to deploy '%s' to region(s): %s\",\n                            context.env_name,\n                            \", \".join(deployment['regions']))\n\n                for region in deployment['regions']:\n                    LOGGER.info(\"\")\n                    LOGGER.info(\"======= Processing region %s ================\"\n                                \"===========\", region)\n\n                    context.env_region = region\n                    context.env_vars = merge_dicts(\n                        context.env_vars,\n                        {'AWS_DEFAULT_REGION': context.env_region,\n                         'AWS_REGION': context.env_region}\n                    )\n                    if deployment.get('assume-role'):\n                        pre_deploy_assume_role(deployment['assume-role'], context)\n                    if deployment.get('account-id') or (deployment.get('account-alias')):\n                        validate_account_credentials(deployment, context)\n\n                    modules = deployment.get('modules', [])\n                    if deployment.get('current_dir'):\n                        modules.append('.' + os.sep)\n                    for module in modules:\n                        self._deploy_module(module, deployment, context, command)\n\n                if deployment.get('assume-role'):\n                    post_deploy_assume_role(deployment['assume-role'], context)\n            else:\n                LOGGER.error('No region configured for any deployment')\n                sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reverse_deployments(deployments=None):\n        if deployments is None:\n            deployments = []\n\n        reversed_deployments = []\n        for i in deployments[::-1]:\n            deployment = copy.deepcopy(i)\n            for config in ['modules', 'regions']:\n                if deployment.get(config):\n                    deployment[config] = deployment[config][::-1]\n            reversed_deployments.append(deployment)\n        return reversed_deployments", "response": "Reverse deployments and the modules and regions in them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_deployment_to_run(env_name, deployments=None, command='build'):  # noqa pylint: disable=too-many-branches,too-many-statements,too-many-locals\n        if deployments is None or not deployments:\n            return []\n        deployments_to_run = []\n\n        num_deployments = len(deployments)\n\n        if num_deployments == 1:\n            selected_deployment_index = 1\n        else:\n            print('')\n            print('Configured deployments:')\n            for i, deployment in enumerate(deployments):\n                print(\" %d: %s\" % (i+1, _deployment_menu_entry(deployment)))\n            print('')\n            print('')\n            if command == 'destroy':\n                print('(Operating in destroy mode -- \"all\" will destroy all '\n                      'deployments in reverse order)')\n            selected_deployment_index = input('Enter number of deployment to run (or \"all\"): ')\n\n        if selected_deployment_index == 'all':\n            return deployments\n        if selected_deployment_index == '':\n            LOGGER.error('Please select a valid number (or \"all\")')\n            sys.exit(1)\n\n        selected_deployment = deployments[int(selected_deployment_index) - 1]\n        if selected_deployment.get('current_dir', False):\n            deployments_to_run.append(selected_deployment)\n        elif not selected_deployment.get('modules', []):\n            LOGGER.error('No modules configured in selected deployment')\n            sys.exit(1)\n        elif len(selected_deployment['modules']) == 1:\n            # No need to select a module in the deployment - there's only one\n            if command == 'destroy':\n                LOGGER.info('(only one deployment detected; all modules '\n                            'automatically selected for termination)')\n                if not strtobool(input('Proceed?: ')):\n                    sys.exit(0)\n            deployments_to_run.append(selected_deployment)\n        else:\n            modules = selected_deployment['modules']\n            print('')\n            print('Configured modules in deployment \\'%s\\':' % selected_deployment.get('name'))\n            for i, module in enumerate(modules):\n                print(\" %s: %s\" % (i+1, _module_menu_entry(module, env_name)))\n            print('')\n            print('')\n            if command == 'destroy':\n                print('(Operating in destroy mode -- \"all\" will destroy all '\n                      'deployments in reverse order)')\n            selected_module_index = input('Enter number of module to run (or \"all\"): ')\n            if selected_module_index == 'all':\n                deployments_to_run.append(selected_deployment)\n            elif selected_module_index == '' or (\n                    not selected_module_index.isdigit() or (\n                        not 0 < int(selected_module_index) <= len(modules))):\n                LOGGER.error('Please select a valid number (or \"all\")')\n                sys.exit(1)\n            else:\n                selected_deployment['modules'] = [modules[int(selected_module_index) - 1]]\n                deployments_to_run.append(selected_deployment)\n\n        LOGGER.debug('Selected deployment is %s...', deployments_to_run)\n        return deployments_to_run", "response": "Query user for deployments to run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self):\n        clean_cmd = ['git', 'clean', '-X', '-d']\n        if 'CI' not in os.environ:\n            LOGGER.info('The following files/directories will be deleted:')\n            LOGGER.info('')\n            LOGGER.info(check_output(clean_cmd + ['-n']).decode())\n            if not strtobool(input('Proceed?: ')):\n                return False\n        check_call(clean_cmd + ['-f'])\n        empty_dirs = self.get_empty_dirs(self.env_root)\n        if empty_dirs != []:\n            LOGGER.info('Now removing empty directories:')\n        for directory in empty_dirs:\n            LOGGER.info(\"Removing %s/\", directory)\n            shutil.rmtree(os.path.join(self.env_root, directory))\n        return True", "response": "Execute git clean to remove untracked build files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate possible SLS config files names.", "response": "def gen_sls_config_files(stage, region):\n    \"\"\"Generate possible SLS config files names.\"\"\"\n    names = []\n    for ext in ['yml', 'json']:\n        # Give preference to explicit stage-region files\n        names.append(\n            os.path.join('env',\n                         \"%s-%s.%s\" % (stage, region, ext))\n        )\n        names.append(\"config-%s-%s.%s\" % (stage, region, ext))\n        # Fallback to stage name only\n        names.append(\n            os.path.join('env',\n                         \"%s.%s\" % (stage, ext))\n        )\n        names.append(\"config-%s.%s\" % (stage, ext))\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sls_config_file(path, stage, region):\n    for name in gen_sls_config_files(stage, region):\n        if os.path.isfile(os.path.join(path, name)):\n            return name\n    return \"config-%s.json\" % stage", "response": "Determine Serverless config file name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns sls remove command.", "response": "def run_sls_remove(sls_cmd, env_vars):\n    \"\"\"Run sls remove command.\"\"\"\n    sls_process = subprocess.Popen(sls_cmd,\n                                   stdout=subprocess.PIPE,\n                                   env=env_vars)\n    stdoutdata, _stderrdata = sls_process.communicate()\n    sls_return = sls_process.wait()\n    print(stdoutdata)\n    if sls_return != 0 and (sls_return == 1 and not (\n            re.search(r\"Stack '.*' does not exist\", stdoutdata))):\n        sys.exit(sls_return)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling pre and post hooks.", "response": "def handle_hooks(stage, hooks, provider, context, dump, outline):\n    \"\"\"Handle pre/post hooks.\n\n    Args:\n        stage (str): The name of the hook stage - pre_build/post_build.\n        hooks (list): A list of dictionaries containing the hooks to execute.\n        provider (:class:`stacker.provider.base.BaseProvider`): The provider\n            the current stack is using.\n        context (:class:`stacker.context.Context`): The current stacker\n            context.\n        dump (bool): Whether running with dump set or not.\n        outline (bool): Whether running with outline set or not.\n\n    \"\"\"\n    if not outline and not dump and hooks:\n        util.handle_hooks(\n            stage=stage,\n            hooks=hooks,\n            provider=provider,\n            context=context\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_archives_to_prune(archives, hook_data):\n    files_to_skip = []\n    for i in ['current_archive_filename', 'old_archive_filename']:\n        if hook_data.get(i):\n            files_to_skip.append(hook_data[i])\n    archives.sort(key=itemgetter('LastModified'),\n                  reverse=False)  # sort from oldest to newest\n    # Drop all but last 15 files\n    return [i['Key'] for i in archives[:-15] if i['Key'] not in files_to_skip]", "response": "Return list of keys to delete."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures config file can be loaded by Stacker.", "response": "def ensure_stacker_compat_config(config_filename):\n    \"\"\"Ensure config file can be loaded by Stacker.\"\"\"\n    try:\n        with open(config_filename, 'r') as stream:\n            yaml.safe_load(stream)\n    except yaml.constructor.ConstructorError as yaml_error:\n        if yaml_error.problem.startswith(\n                'could not determine a constructor for the tag \\'!'):\n            LOGGER.error('\"%s\" appears to be a CloudFormation template, '\n                         'but is located in the top level of a module '\n                         'alongside the CloudFormation config files (i.e. '\n                         'the file or files indicating the stack names & '\n                         'parameters). Please move the template to a '\n                         'subdirectory.',\n                         config_filename)\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stacker_env_file(path, environment, region):\n    for name in gen_stacker_env_files(environment, region):\n        if os.path.isfile(os.path.join(path, name)):\n            return name\n    return \"%s-%s.env\" % (environment, region)", "response": "Determine Stacker environment file name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_stacker_cmd_string(args, lib_path):\n    if platform.system().lower() == 'windows':\n        # Because this will be run via subprocess, the backslashes on Windows\n        # will cause command errors\n        lib_path = lib_path.replace('\\\\', '/')\n    return (\"import sys;\"\n            \"sys.argv = ['stacker'] + {args};\"\n            \"sys.path.insert(1, '{lib_path}');\"\n            \"from stacker.logger import setup_logging;\"\n            \"from stacker.commands import Stacker;\"\n            \"stacker = Stacker(setup_logging=setup_logging);\"\n            \"args = stacker.parse_args({args});\"\n            \"stacker.configure(args);args.run(args)\".format(args=str(args),\n                                                            lib_path=lib_path))", "response": "Generate a stacker invocation script from command line arg list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_stacker(self, command='diff'):  # pylint: disable=too-many-branches,too-many-locals\n        response = {'skipped_configs': False}\n        stacker_cmd = [command, \"--region=%s\" % self.context.env_region]\n\n        if command == 'destroy':\n            stacker_cmd.append('--force')\n        elif command == 'build':\n            if 'CI' in self.context.env_vars:\n                stacker_cmd.append('--recreate-failed')\n            else:\n                stacker_cmd.append('--interactive')\n\n        if 'DEBUG' in self.context.env_vars:\n            stacker_cmd.append('--verbose')  # Increase logging if requested\n\n        stacker_env_file = get_stacker_env_file(self.path,\n                                                self.context.env_name,\n                                                self.context.env_region)\n        stacker_env_file_present = os.path.isfile(\n            os.path.join(self.path, stacker_env_file)\n        )\n        if isinstance(self.options.get('environments',\n                                       {}).get(self.context.env_name),\n                      dict):\n            for (key, val) in self.options['environments'][self.context.env_name].items():  # noqa\n                stacker_cmd.extend(['-e', \"%s=%s\" % (key, val)])\n        if stacker_env_file_present:\n            stacker_cmd.append(stacker_env_file)\n\n        if not (stacker_env_file_present or self.options.get(\n                'environments',\n                {}).get(self.context.env_name)):\n            response['skipped_configs'] = True\n            LOGGER.info(\n                \"Skipping stacker %s; no environment \"\n                \"file found for this environment/region \"\n                \"(looking for one of \\\"%s\\\")\",\n                command,\n                ', '.join(\n                    gen_stacker_env_files(self.context.env_name,  # noqa\n                                          self.context.env_region))  # noqa\n            )\n        else:\n            with change_dir(self.path):\n                # Iterate through any stacker yaml configs to deploy them in order\n                # or destroy them in reverse order\n                for _root, _dirs, files in os.walk(self.path):\n                    sorted_files = sorted(files)\n                    if command == 'destroy':\n                        sorted_files = reversed(sorted_files)\n                    for name in sorted_files:\n                        if re.match(r\"runway(\\..*)?\\.yml\", name) or (\n                                name.startswith('.')):\n                            # Hidden files (e.g. .gitlab-ci.yml) or runway configs\n                            # definitely aren't stacker config files\n                            continue\n                        if os.path.splitext(name)[1] in ['.yaml', '.yml']:\n                            ensure_stacker_compat_config(\n                                os.path.join(self.path, name)\n                            )\n                            LOGGER.info(\"Running stacker %s on %s in region %s\",\n                                        command,\n                                        name,\n                                        self.context.env_region)\n                            stacker_cmd_str = make_stacker_cmd_string(\n                                stacker_cmd + [name],\n                                get_embedded_lib_path()\n                            )\n                            stacker_cmd_list = [sys.executable, '-c']\n                            LOGGER.debug(\n                                \"Stacker command being executed: %s \\\"%s\\\"\",\n                                ' '.join(stacker_cmd_list),\n                                stacker_cmd_str\n                            )\n                            run_module_command(\n                                cmd_list=stacker_cmd_list + [stacker_cmd_str],\n                                env_vars=self.context.env_vars\n                            )\n                    break  # only need top level files\n        return response", "response": "Runs the specified command on the stacker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cf_distribution_class():\n    if LooseVersion(troposphere.__version__) == LooseVersion('2.4.0'):\n        cf_dist = cloudfront.Distribution\n        cf_dist.props['DistributionConfig'] = (DistributionConfig, True)\n        return cf_dist\n    return cloudfront.Distribution", "response": "Return the correct troposphere CF distribution class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_s3_origin_conf_class():\n    if LooseVersion(troposphere.__version__) > LooseVersion('2.4.0'):\n        return cloudfront.S3OriginConfig\n    if LooseVersion(troposphere.__version__) == LooseVersion('2.4.0'):\n        return S3OriginConfig\n    return cloudfront.S3Origin", "response": "Return the correct S3 Origin Config class for troposphere."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_template(self):\n        template = self.template\n        variables = self.get_variables()\n        template.add_version('2010-09-09')\n        template.add_description('Static Website - Bucket and Distribution')\n\n        # Conditions\n        template.add_condition(\n            'AcmCertSpecified',\n            And(Not(Equals(variables['AcmCertificateArn'].ref, '')),\n                Not(Equals(variables['AcmCertificateArn'].ref, 'undefined')))\n        )\n        template.add_condition(\n            'AliasesSpecified',\n            And(Not(Equals(Select(0, variables['Aliases'].ref), '')),\n                Not(Equals(Select(0, variables['Aliases'].ref), 'undefined')))\n        )\n        template.add_condition(\n            'CFLoggingEnabled',\n            And(Not(Equals(variables['LogBucketName'].ref, '')),\n                Not(Equals(variables['LogBucketName'].ref, 'undefined')))\n        )\n        template.add_condition(\n            'DirectoryIndexSpecified',\n            And(Not(Equals(variables['RewriteDirectoryIndex'].ref, '')),\n                Not(Equals(variables['RewriteDirectoryIndex'].ref, 'undefined')))  # noqa\n        )\n        template.add_condition(\n            'WAFNameSpecified',\n            And(Not(Equals(variables['WAFWebACL'].ref, '')),\n                Not(Equals(variables['WAFWebACL'].ref, 'undefined')))\n        )\n\n        # Resources\n        oai = template.add_resource(\n            cloudfront.CloudFrontOriginAccessIdentity(\n                'OAI',\n                CloudFrontOriginAccessIdentityConfig=cloudfront.CloudFrontOriginAccessIdentityConfig(  # noqa pylint: disable=line-too-long\n                    Comment='CF access to website'\n                )\n            )\n        )\n\n        bucket = template.add_resource(\n            s3.Bucket(\n                'Bucket',\n                AccessControl=s3.Private,\n                LifecycleConfiguration=s3.LifecycleConfiguration(\n                    Rules=[\n                        s3.LifecycleRule(\n                            NoncurrentVersionExpirationInDays=90,\n                            Status='Enabled'\n                        )\n                    ]\n                ),\n                VersioningConfiguration=s3.VersioningConfiguration(\n                    Status='Enabled'\n                ),\n                WebsiteConfiguration=s3.WebsiteConfiguration(\n                    IndexDocument='index.html',\n                    ErrorDocument='error.html'\n                )\n            )\n        )\n        template.add_output(Output(\n            'BucketName',\n            Description='Name of website bucket',\n            Value=bucket.ref()\n        ))\n\n        allowcfaccess = template.add_resource(\n            s3.BucketPolicy(\n                'AllowCFAccess',\n                Bucket=bucket.ref(),\n                PolicyDocument=PolicyDocument(\n                    Version='2012-10-17',\n                    Statement=[\n                        Statement(\n                            Action=[awacs.s3.GetObject],\n                            Effect=Allow,\n                            Principal=Principal(\n                                'CanonicalUser',\n                                oai.get_att('S3CanonicalUserId')\n                            ),\n                            Resource=[\n                                Join('', [bucket.get_att('Arn'),\n                                          '/*'])\n                            ]\n                        )\n                    ]\n                )\n            )\n        )\n\n        cfdirectoryindexrewriterole = template.add_resource(\n            iam.Role(\n                'CFDirectoryIndexRewriteRole',\n                Condition='DirectoryIndexSpecified',\n                AssumeRolePolicyDocument=PolicyDocument(\n                    Version='2012-10-17',\n                    Statement=[\n                        Statement(\n                            Effect=Allow,\n                            Action=[awacs.sts.AssumeRole],\n                            Principal=Principal('Service',\n                                                ['lambda.amazonaws.com',\n                                                 'edgelambda.amazonaws.com'])\n                        )\n                    ]\n                ),\n                ManagedPolicyArns=[\n                    IAM_ARN_PREFIX + 'AWSLambdaBasicExecutionRole'\n                ]\n            )\n        )\n\n        cfdirectoryindexrewrite = template.add_resource(\n            awslambda.Function(\n                'CFDirectoryIndexRewrite',\n                Condition='DirectoryIndexSpecified',\n                Code=awslambda.Code(\n                    ZipFile=Join(\n                        '',\n                        [\"'use strict';\\n\",\n                         \"exports.handler = (event, context, callback) => {\\n\",\n                         \"\\n\",\n                         \"    // Extract the request from the CloudFront event that is sent to Lambda@Edge\\n\",  # noqa pylint: disable=line-too-long\n                         \"    var request = event.Records[0].cf.request;\\n\",\n                         \"    // Extract the URI from the request\\n\",\n                         \"    var olduri = request.uri;\\n\",\n                         \"    // Match any '/' that occurs at the end of a URI. Replace it with a default index\\n\",  # noqa pylint: disable=line-too-long\n                         \"    var newuri = olduri.replace(/\\\\/$/, '\\\\/\",\n                         variables['RewriteDirectoryIndex'].ref,\n                         \"');\\n\",  # noqa\n                         \"    // Log the URI as received by CloudFront and the new URI to be used to fetch from origin\\n\",  # noqa pylint: disable=line-too-long\n                         \"    console.log(\\\"Old URI: \\\" + olduri);\\n\",\n                         \"    console.log(\\\"New URI: \\\" + newuri);\\n\",\n                         \"    // Replace the received URI with the URI that includes the index page\\n\",  # noqa pylint: disable=line-too-long\n                         \"    request.uri = newuri;\\n\",\n                         \"    // Return to CloudFront\\n\",\n                         \"    return callback(null, request);\\n\",\n                         \"\\n\",\n                         \"};\\n\"]\n                    )\n                ),\n                Description='Rewrites CF directory HTTP requests to default page',  # noqa\n                Handler='index.handler',\n                Role=cfdirectoryindexrewriterole.get_att('Arn'),\n                Runtime='nodejs8.10'\n            )\n        )\n\n        # Generating a unique resource name here for the Lambda version, so it\n        # updates automatically if the lambda code changes\n        code_hash = hashlib.md5(\n            str(cfdirectoryindexrewrite.properties['Code'].properties['ZipFile'].to_dict()).encode()  # noqa pylint: disable=line-too-long\n        ).hexdigest()\n\n        cfdirectoryindexrewritever = template.add_resource(\n            awslambda.Version(\n                'CFDirectoryIndexRewriteVer' + code_hash,\n                Condition='DirectoryIndexSpecified',\n                FunctionName=cfdirectoryindexrewrite.ref()\n            )\n        )\n\n        # If custom associations defined, use them\n        if variables['lambda_function_associations']:\n            lambda_function_associations = [\n                cloudfront.LambdaFunctionAssociation(\n                    EventType=x['type'],\n                    LambdaFunctionARN=x['arn']\n                ) for x in variables['lambda_function_associations']\n            ]\n        else:  # otherwise fallback to pure CFN condition\n            lambda_function_associations = If(\n                'DirectoryIndexSpecified',\n                [cloudfront.LambdaFunctionAssociation(\n                    EventType='origin-request',\n                    LambdaFunctionARN=cfdirectoryindexrewritever.ref()\n                )],\n                NoValue\n            )\n\n        cfdistribution = template.add_resource(\n            get_cf_distribution_class()(\n                'CFDistribution',\n                DependsOn=allowcfaccess.title,\n                DistributionConfig=get_cf_distro_conf_class()(\n                    Aliases=If(\n                        'AliasesSpecified',\n                        variables['Aliases'].ref,\n                        NoValue\n                    ),\n                    Origins=[\n                        get_cf_origin_class()(\n                            DomainName=Join(\n                                '.',\n                                [bucket.ref(),\n                                 's3.amazonaws.com']),\n                            S3OriginConfig=get_s3_origin_conf_class()(\n                                OriginAccessIdentity=Join(\n                                    '',\n                                    ['origin-access-identity/cloudfront/',\n                                     oai.ref()])\n                            ),\n                            Id='S3Origin'\n                        )\n                    ],\n                    DefaultCacheBehavior=cloudfront.DefaultCacheBehavior(\n                        AllowedMethods=['GET', 'HEAD'],\n                        Compress=False,\n                        DefaultTTL='86400',\n                        ForwardedValues=cloudfront.ForwardedValues(\n                            Cookies=cloudfront.Cookies(Forward='none'),\n                            QueryString=False,\n                        ),\n                        LambdaFunctionAssociations=lambda_function_associations,  # noqa\n                        TargetOriginId='S3Origin',\n                        ViewerProtocolPolicy='redirect-to-https'\n                    ),\n                    DefaultRootObject='index.html',\n                    Logging=If(\n                        'CFLoggingEnabled',\n                        cloudfront.Logging(\n                            Bucket=Join('.',\n                                        [variables['LogBucketName'].ref,\n                                         's3.amazonaws.com'])\n                        ),\n                        NoValue\n                    ),\n                    PriceClass=variables['PriceClass'].ref,\n                    Enabled=True,\n                    WebACLId=If(\n                        'WAFNameSpecified',\n                        variables['WAFWebACL'].ref,\n                        NoValue\n                    ),\n                    ViewerCertificate=If(\n                        'AcmCertSpecified',\n                        cloudfront.ViewerCertificate(\n                            AcmCertificateArn=variables['AcmCertificateArn'].ref,  # noqa\n                            SslSupportMethod='sni-only'\n                        ),\n                        NoValue\n                    )\n                )\n            )\n        )\n        template.add_output(Output(\n            'CFDistributionId',\n            Description='CloudFront distribution ID',\n            Value=cfdistribution.ref()\n        ))\n        template.add_output(\n            Output(\n                'CFDistributionDomainName',\n                Description='CloudFront distribution domain name',\n                Value=cfdistribution.get_att('DomainName')\n            )\n        )", "response": "Create the template for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_available(self, show=None):\n        show = self.state.show if show is None else show\n        self.set_presence(PresenceState(available=True, show=show))", "response": "Sets the agent availability to True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_unavailable(self):\n        show = PresenceShow.NONE\n        self.set_presence(PresenceState(available=False, show=show))", "response": "Sets the agent availability to False."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_presence(self, state=None, status=None, priority=None):\n        state = state if state is not None else self.state\n        status = status if status is not None else self.status\n        priority = priority if priority is not None else self.priority\n        self.presenceserver.set_presence(state, status, priority)", "response": "Change the presence of a resource in the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of contacts", "response": "def get_contacts(self):\n        \"\"\"\n        Returns list of contacts\n\n        Returns:\n          dict: the roster of contacts\n\n        \"\"\"\n        for jid, item in self.roster.items.items():\n            try:\n                self._contacts[jid.bare()].update(item.export_as_json())\n            except KeyError:\n                self._contacts[jid.bare()] = item.export_as_json()\n\n        return self._contacts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_contact(self, jid):\n        try:\n            return self.get_contacts()[jid.bare()]\n        except KeyError:\n            raise ContactNotFound\n        except AttributeError:\n            raise AttributeError(\"jid must be an aioxmpp.JID object\")", "response": "Returns a dict containing the roster of a specific jid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subscribe(self, peer_jid):\n        self.roster.subscribe(aioxmpp.JID.fromstr(peer_jid).bare())", "response": "Subscribe to a specific JID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unsubscribe(self, peer_jid):\n        self.roster.unsubscribe(aioxmpp.JID.fromstr(peer_jid).bare())", "response": "Asks for unsubscription of a user from the roster"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef approve(self, peer_jid):\n        self.roster.approve(aioxmpp.JID.fromstr(peer_jid).bare())", "response": "Approve a subscription request from jid\n         "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append(self, event, category=None):\n        date = datetime.datetime.now()\n        self.store.insert(0, (date, event, category))\n        if len(self.store) > self.size:\n            del self.store[-1]", "response": "Adds a new event to the trace store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all the events that have been received in the store until a limit is defined.", "response": "def received(self, limit=None):\n        \"\"\"\n        Returns all the events that have been received (excluding sent events), until a limit if defined\n\n        Args:\n          limit (int, optional): the max length of the events to return (Default value = None)\n\n        Returns:\n          list: a list of received events\n\n        \"\"\"\n        return list(itertools.islice((itertools.filterfalse(lambda x: x[1].sent, self.store)), limit))[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, limit=None, to=None, category=None):\n        if category and not to:\n            msg_slice = itertools.islice((x for x in self.store if x[2] == category), limit)\n        elif to and not category:\n            to = JID.fromstr(to)\n            msg_slice = itertools.islice((x for x in self.store if _agent_in_msg(to, x[1])), limit)\n        elif to and category:\n            to = JID.fromstr(to)\n            msg_slice = itertools.islice((x for x in self.store if _agent_in_msg(to, x[1]) and x[2] == category), limit)\n        else:\n            msg_slice = self.all(limit=limit)\n            return msg_slice\n\n        return list(msg_slice)[::-1]", "response": "Returns the events that match the filters\n ArcGIS"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntells the container to start this agent.", "response": "def start(self, auto_register=True):\n        \"\"\"\n        Tells the container to start this agent.\n        It returns a coroutine or a future depending on whether it is called from a coroutine or a synchronous method.\n\n        Args:\n            auto_register (bool): register the agent in the server (Default value = True)\n        \"\"\"\n        return self.container.start_agent(agent=self, auto_register=auto_register)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the agent from a coroutine. This is the main method that starts the agent.", "response": "async def _async_start(self, auto_register=True):\n        \"\"\"\n        Starts the agent from a coroutine. This fires some actions:\n\n            * if auto_register: register the agent in the server\n            * runs the event loop\n            * connects the agent to the server\n            * runs the registered behaviours\n\n        Args:\n          auto_register (bool, optional): register the agent in the server (Default value = True)\n\n        \"\"\"\n\n        if auto_register:\n            await self._async_register()\n        self.client = aioxmpp.PresenceManagedClient(self.jid,\n                                                    aioxmpp.make_security_layer(self.password,\n                                                                                no_verify=not self.verify_security),\n                                                    loop=self.loop,\n                                                    logger=logging.getLogger(self.jid.localpart))\n\n        # obtain an instance of the service\n        self.message_dispatcher = self.client.summon(SimpleMessageDispatcher)\n\n        # Presence service\n        self.presence = PresenceManager(self)\n\n        await self._async_connect()\n\n        # register a message callback here\n        self.message_dispatcher.register_callback(\n            aioxmpp.MessageType.CHAT,\n            None,\n            self._message_received,\n        )\n        await self.setup()\n        self._alive.set()\n        for behaviour in self.behaviours:\n            if not behaviour.is_running:\n                behaviour.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _async_connect(self):  # pragma: no cover\n        try:\n            self.conn_coro = self.client.connected()\n            aenter = type(self.conn_coro).__aenter__(self.conn_coro)\n            self.stream = await aenter\n            logger.info(f\"Agent {str(self.jid)} connected and authenticated.\")\n        except aiosasl.AuthenticationFailure:\n            raise AuthenticationFailure(\n                \"Could not authenticate the agent. Check user and password or use auto_register=True\")", "response": "Connect and authenticate to the XMPP server. Async mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _async_register(self):  # pragma: no cover\n        metadata = aioxmpp.make_security_layer(None, no_verify=not self.verify_security)\n        query = ibr.Query(self.jid.localpart, self.password)\n        _, stream, features = await aioxmpp.node.connect_xmlstream(self.jid, metadata, loop=self.loop)\n        await ibr.register(stream, query)", "response": "Register the agent in the XMPP server from a coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_behaviour(self, behaviour, template=None):\n        behaviour.set_agent(self)\n        if issubclass(type(behaviour), FSMBehaviour):\n            for _, state in behaviour.get_states().items():\n                state.set_agent(self)\n        behaviour.set_template(template)\n        self.behaviours.append(behaviour)\n        if self.is_alive():\n            behaviour.start()", "response": "Adds and starts a behaviour to the agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_behaviour(self, behaviour):\n        if not self.has_behaviour(behaviour):\n            raise ValueError(\"This behaviour is not registered\")\n        index = self.behaviours.index(behaviour)\n        self.behaviours[index].kill()\n        self.behaviours.pop(index)", "response": "Removes a behaviour from the agent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping an agent and kills all its behaviours.", "response": "async def _async_stop(self):\n        \"\"\" Stops an agent and kills all its behaviours. \"\"\"\n        if self.presence:\n            self.presence.set_unavailable()\n        for behav in self.behaviours:\n            behav.kill()\n        if self.web.is_started():\n            await self.web.runner.cleanup()\n\n        \"\"\" Discconnect from XMPP server. \"\"\"\n        if self.is_alive():\n            # Disconnect from XMPP server\n            self.client.stop()\n            aexit = self.conn_coro.__aexit__(*sys.exc_info())\n            await aexit\n            logger.info(\"Client disconnected.\")\n\n        self._alive.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _message_received(self, msg):\n\n        msg = Message.from_node(msg)\n        return self.dispatch(msg)", "response": "This callback is called when a message is received."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch(self, msg):\n        logger.debug(f\"Got message: {msg}\")\n        futures = []\n        matched = False\n        for behaviour in (x for x in self.behaviours if x.match(msg)):\n            futures.append(self.submit(behaviour.enqueue(msg)))\n            logger.debug(f\"Message enqueued to behaviour: {behaviour}\")\n            self.traces.append(msg, category=str(behaviour))\n            matched = True\n        if not matched:\n            logger.warning(f\"No behaviour matched for message: {msg}\")\n            self.traces.append(msg)\n        return futures", "response": "Dispatch the message to every behaviour that matches the message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_node(cls, node):\n        if not isinstance(node, aioxmpp.stanza.Message):\n            raise AttributeError(\"node must be a aioxmpp.stanza.Message instance\")\n        msg = cls()\n        msg._to = node.to\n        msg._sender = node.from_\n        if None in node.body:\n            msg.body = node.body[None]\n        else:\n            for key in node.body.keys():\n                msg.body = node.body[key]\n                break\n\n        for data in node.xep0004_data:\n            if data.title == SPADE_X_METADATA:\n                for field in data.fields:\n                    if field.var != \"_thread_node\":\n                        msg.set_metadata(field.var, field.values[0])\n                    else:\n                        msg.thread = field.values[0]\n\n        return msg", "response": "Creates a new Message object from an aioxmpp. Message instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the to jid of the receiver.", "response": "def to(self, jid: str):\n        \"\"\"\n        Set jid of the receiver.\n\n        Args:\n          jid (str): the jid of the receiver.\n\n        \"\"\"\n        if jid is not None and not isinstance(jid, str):\n            raise TypeError(\"'to' MUST be a string\")\n        self._to = aioxmpp.JID.fromstr(jid) if jid is not None else None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sender(self, jid: str):\n        if jid is not None and not isinstance(jid, str):\n            raise TypeError(\"'sender' MUST be a string\")\n        self._sender = aioxmpp.JID.fromstr(jid) if jid is not None else None", "response": "Set the jid of the sender"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef body(self, body: str):\n        if body is not None and not isinstance(body, str):\n            raise TypeError(\"'body' MUST be a string\")\n        self._body = body", "response": "Sets the body of the message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the thread id of the message", "response": "def thread(self, value: str):\n        \"\"\"\n        Set thread id of the message\n\n        Args:\n            value (str): the thread id\n\n        \"\"\"\n        if value is not None and not isinstance(value, str):\n            raise TypeError(\"'thread' MUST be a string\")\n        self._thread = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_metadata(self, key: str, value: str):\n        if not isinstance(key, str) or not isinstance(value, str):\n            raise TypeError(\"'key' and 'value' of metadata MUST be strings\")\n        self.metadata[key] = value", "response": "Add a new metadata to the message\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_metadata(self, key) -> str:\n        return self.metadata[key] if key in self.metadata else None", "response": "Get the value of a metadata key. Returns None if metadata does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(self, message) -> bool:\n        if self.to and message.to != self.to:\n            return False\n\n        if self.sender and message.sender != self.sender:\n            return False\n\n        if self.body and message.body != self.body:\n            return False\n\n        if self.thread and message.thread != self.thread:\n            return False\n\n        for key, value in self.metadata.items():\n            if message.get_metadata(key) != value:\n                return False\n\n        logger.debug(f\"message matched {self} == {message}\")\n        return True", "response": "Returns True if the message matches with this message or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_reply(self):\n        return Message(\n            to=str(self.sender),\n            sender=str(self.to),\n            body=self.body,\n            thread=self.thread,\n            metadata=self.metadata\n        )", "response": "Creates a copy of the message with exchanged sender and receiver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the aioxmpp. message. Message object for sending.", "response": "def prepare(self):\n        \"\"\"\n        Returns an aioxmpp.stanza.Message built from the Message and prepared to be sent.\n\n        Returns:\n          aioxmpp.stanza.Message: the message prepared to be sent\n\n        \"\"\"\n\n        msg = aioxmpp.stanza.Message(\n            to=self.to,\n            from_=self.sender,\n            type_=aioxmpp.MessageType.CHAT,\n        )\n\n        msg.body[None] = self.body\n\n        # Send metadata using xep-0004: Data Forms (https://xmpp.org/extensions/xep-0004.html)\n        if len(self.metadata):\n            data = forms_xso.Data(type_=forms_xso.DataType.FORM)\n\n            for name, value in self.metadata.items():\n                data.fields.append(\n                    forms_xso.Field(\n                        var=name,\n                        type_=forms_xso.FieldType.TEXT_SINGLE,\n                        values=[value],\n                    )\n                )\n\n            if self.thread:\n                data.fields.append(forms_xso.Field(var=\"_thread_node\",\n                                                   type_=forms_xso.FieldType.TEXT_SINGLE,\n                                                   values=[self.thread]))\n\n            data.title = SPADE_X_METADATA\n            msg.xep0004_data = [data]\n\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unused_port(hostname):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((hostname, 0))\n        return s.getsockname()[1]", "response": "Return a port that is unused on the current host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def start_server_in_loop(runner, hostname, port, agent):\n    await runner.setup()\n    agent.web.server = aioweb.TCPSite(runner, hostname, port)\n    await agent.web.server.start()\n    logger.info(f\"Serving on http://{hostname}:{port}/\")", "response": "Starts the web server in a loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(self, hostname=None, port=None, templates_path=None):\n        self.hostname = hostname if hostname else \"localhost\"\n        if port:\n            self.port = port\n        elif not self.port:\n            self.port = unused_port(self.hostname)\n        if templates_path:\n            self.loaders.insert(0, jinja2.FileSystemLoader(templates_path))\n            self._set_loaders()\n        self.setup_routes()\n        self.runner = aioweb.AppRunner(self.app)\n        return self.agent.submit(start_server_in_loop(self.runner, self.hostname, self.port, self.agent))", "response": "Starts the web interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_controller(self, controller, template):\n        if template:\n            fn = aiohttp_jinja2.template(template_name=template)(controller)\n        else:\n            fn = self._parse_json_response(controller)\n        return fn", "response": "Wraps the controller with a jinja template or returns a json response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_agent(self, agent):\n        self.agent = agent\n        self.queue = asyncio.Queue(loop=self.agent.loop)\n        self.presence = agent.presence\n        self.web = agent.web", "response": "Sets the agent object for this behaviour."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmatch a message with the behaviour s template", "response": "def match(self, message: Message) -> bool:\n        \"\"\"\n        Matches a message with the behaviour's template\n\n        Args:\n          message(spade.message.Message): the message to match with\n\n        Returns:\n          bool: wheter the messaged matches or not\n\n        \"\"\"\n        if self.template:\n            return self.template.match(message)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring a knowledge item in the agent knowledge base.", "response": "def set(self, name: str, value: Any) -> None:\n        \"\"\"\n        Stores a knowledge item in the agent knowledge base.\n\n        Args:\n          name (str): name of the item\n          value (Any): value of the item\n\n        \"\"\"\n        self.agent.set(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        self.agent.submit(self._start())\n        self.is_running = True", "response": "starts behaviour in the event loop"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart coroutine. runs on_start coroutine and then_step coroutine.", "response": "async def _start(self):\n        \"\"\"\n        Start coroutine. runs on_start coroutine and then\n        runs the _step coroutine where the body of the behaviour\n        is called.\n        \"\"\"\n        self.agent._alive.wait()\n        try:\n            await self.on_start()\n        except Exception as e:\n            logger.error(\"Exception running on_start in behaviour {}: {}\".format(self, e))\n            self.kill(exit_code=e)\n        await self._step()\n        self._is_done.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kill(self, exit_code: Any = None):\n        self._force_kill.set()\n        if exit_code is not None:\n            self._exit_code = exit_code\n        logger.info(\"Killing behavior {0} with exit code: {1}\".format(self, exit_code))", "response": "Stops the behaviour and sets the exit code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exit_code(self) -> Any:\n        if self._done() or self.is_killed():\n            return self._exit_code\n        else:\n            raise BehaviourNotFinishedException", "response": "Returns the exit code of the behaviour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _step(self):\n        while not self._done() and not self.is_killed():\n            try:\n                await self._run()\n                await asyncio.sleep(0)  # relinquish cpu\n            except Exception as e:\n                logger.error(\"Exception running behaviour {}: {}\".format(self, e))\n                self.kill(exit_code=e)\n        try:\n            await self.on_end()\n        except Exception as e:\n            logger.error(\"Exception running on_end in behaviour {}: {}\".format(self, e))\n            self.kill(exit_code=e)", "response": "Main loop of the behaviour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def send(self, msg: Message):\n        if not msg.sender:\n            msg.sender = str(self.agent.jid)\n            logger.debug(f\"Adding agent's jid as sender to message: {msg}\")\n        await self.agent.container.send(msg, self)\n        msg.sent = True\n        self.agent.traces.append(msg, category=str(self))", "response": "Sends a message to the agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def receive(self, timeout: float = None) -> Union[Message, None]:\n        if timeout:\n            coro = self.queue.get()\n            try:\n                msg = await asyncio.wait_for(coro, timeout=timeout)\n            except asyncio.TimeoutError:\n                msg = None\n        else:\n            try:\n                msg = self.queue.get_nowait()\n            except asyncio.QueueEmpty:\n                msg = None\n        return msg", "response": "Receives a message from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef period(self, value: float):\n        if value < 0:\n            raise ValueError(\"Period must be greater or equal than zero.\")\n        self._period = timedelta(seconds=value)", "response": "Sets the period of the event."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_state(self, name: str, state: State, initial: bool = False):\n        if not issubclass(state.__class__, State):\n            raise AttributeError(\"state must be subclass of spade.behaviour.State\")\n        self._states[name] = state\n        if initial:\n            self.current_state = name", "response": "Adds a new state to the FSM."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_transition(self, source: str, dest: str):\n        self._transitions[source].append(dest)", "response": "Adds a transition from one state to another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a transition is valid", "response": "def is_valid_transition(self, source: str, dest: str) -> bool:\n        \"\"\"\n        Checks if a transitions is registered in the FSM\n\n        Args:\n          source (str): the source state name\n          dest (str): the destination state name\n\n        Returns:\n          bool: wether the transition is valid or not\n\n        \"\"\"\n        if dest not in self._states or source not in self._states:\n            raise NotValidState\n        elif dest not in self._transitions[source]:\n            raise NotValidTransition\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_graphviz(self) -> str:\n        graph = \"digraph finite_state_machine { rankdir=LR; node [fixedsize=true];\"\n        for origin, dest in self._transitions.items():\n            origin = origin.replace(\" \", \"_\")\n            for d in dest:\n                d = d.replace(\" \", \"_\")\n                graph += \"{0} -> {1};\".format(origin, d)\n        graph += \"}\"\n        return graph", "response": "Converts the FSM behaviour structure to Graphviz syntax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resample(x, sr_orig, sr_new, axis=-1, filter='kaiser_best', **kwargs):\n    '''Resample a signal x from sr_orig to sr_new along a given axis.\n\n    Parameters\n    ----------\n    x : np.ndarray, dtype=np.float*\n        The input signal(s) to resample.\n\n    sr_orig : int > 0\n        The sampling rate of x\n\n    sr_new : int > 0\n        The target sampling rate of the output signal(s)\n\n    axis : int\n        The target axis along which to resample `x`\n\n    filter : optional, str or callable\n        The resampling filter to use.\n\n        By default, uses the `kaiser_best` (pre-computed filter).\n\n    kwargs\n        additional keyword arguments provided to the specified filter\n\n    Returns\n    -------\n    y : np.ndarray\n        `x` resampled to `sr_new`\n\n    Raises\n    ------\n    ValueError\n        if `sr_orig` or `sr_new` is not positive\n\n    TypeError\n        if the input signal `x` has an unsupported data type.\n\n    Examples\n    --------\n    >>> # Generate a sine wave at 440 Hz for 5 seconds\n    >>> sr_orig = 44100.0\n    >>> x = np.sin(2 * np.pi * 440.0 / sr_orig * np.arange(5 * sr_orig))\n    >>> x\n    array([ 0.   ,  0.063, ..., -0.125, -0.063])\n    >>> # Resample to 22050 with default parameters\n    >>> resampy.resample(x, sr_orig, 22050)\n    array([ 0.011,  0.123, ..., -0.193, -0.103])\n    >>> # Resample using the fast (low-quality) filter\n    >>> resampy.resample(x, sr_orig, 22050, filter='kaiser_fast')\n    array([ 0.013,  0.121, ..., -0.189, -0.102])\n    >>> # Resample using a high-quality filter\n    >>> resampy.resample(x, sr_orig, 22050, filter='kaiser_best')\n    array([ 0.011,  0.123, ..., -0.193, -0.103])\n    >>> # Resample using a Hann-windowed sinc filter\n    >>> resampy.resample(x, sr_orig, 22050, filter='sinc_window',\n    ...                  window=scipy.signal.hann)\n    array([ 0.011,  0.123, ..., -0.193, -0.103])\n\n    >>> # Generate stereo data\n    >>> x_right = np.sin(2 * np.pi * 880.0 / sr_orig * np.arange(len(x)))])\n    >>> x_stereo = np.stack([x, x_right])\n    >>> x_stereo.shape\n    (2, 220500)\n    >>> # Resample along the time axis (1)\n    >>> y_stereo = resampy.resample(x, sr_orig, 22050, axis=1)\n    >>> y_stereo.shape\n    (2, 110250)\n    '''\n\n    if sr_orig <= 0:\n        raise ValueError('Invalid sample rate: sr_orig={}'.format(sr_orig))\n\n    if sr_new <= 0:\n        raise ValueError('Invalid sample rate: sr_new={}'.format(sr_new))\n\n    sample_ratio = float(sr_new) / sr_orig\n\n    # Set up the output shape\n    shape = list(x.shape)\n    shape[axis] = int(shape[axis] * sample_ratio)\n\n    if shape[axis] < 1:\n        raise ValueError('Input signal length={} is too small to '\n                         'resample from {}->{}'.format(x.shape[axis], sr_orig, sr_new))\n\n    y = np.zeros(shape, dtype=x.dtype)\n\n    interp_win, precision, _ = get_filter(filter, **kwargs)\n\n    if sample_ratio < 1:\n        interp_win *= sample_ratio\n\n    interp_delta = np.zeros_like(interp_win)\n    interp_delta[:-1] = np.diff(interp_win)\n\n    # Construct 2d views of the data with the resampling axis on the first dimension\n    x_2d = x.swapaxes(0, axis).reshape((x.shape[axis], -1))\n    y_2d = y.swapaxes(0, axis).reshape((y.shape[axis], -1))\n    resample_f(x_2d, y_2d, sample_ratio, interp_win, interp_delta, precision)\n\n    return y", "response": "Resample a signal x from sr_orig to sr_new along a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a windowed sinc interpolation filter for a given set of zero crossings.", "response": "def sinc_window(num_zeros=64, precision=9, window=None, rolloff=0.945):\n    '''Construct a windowed sinc interpolation filter\n\n    Parameters\n    ----------\n    num_zeros : int > 0\n        The number of zero-crossings to retain in the sinc filter\n\n    precision : int > 0\n        The number of filter coefficients to retain for each zero-crossing\n\n    window : callable\n        The window function.  By default, uses Blackman-Harris.\n\n    rolloff : float > 0\n        The roll-off frequency (as a fraction of nyquist)\n\n    Returns\n    -------\n    interp_window: np.ndarray [shape=(num_zeros * num_table + 1)]\n        The interpolation window (right-hand side)\n\n    num_bits: int\n        The number of bits of precision to use in the filter table\n\n    rolloff : float > 0\n        The roll-off frequency of the filter, as a fraction of Nyquist\n\n    Raises\n    ------\n    TypeError\n        if `window` is not callable or `None`\n    ValueError\n        if `num_zeros < 1`, `precision < 1`,\n        or `rolloff` is outside the range `(0, 1]`.\n\n    Examples\n    --------\n    >>> # A filter with 10 zero-crossings, 32 samples per crossing, and a\n    ... # Hann window for tapering.\n    >>> halfwin, prec, rolloff = resampy.filters.sinc_window(num_zeros=10, precision=5,\n    ...                                                      window=scipy.signal.hann)\n    >>> halfwin\n    array([  9.450e-01,   9.436e-01, ...,  -7.455e-07,  -0.000e+00])\n    >>> prec\n    32\n    >>> rolloff\n    0.945\n\n    >>> # Or using sinc-window filter construction directly in resample\n    >>> y = resampy.resample(x, sr_orig, sr_new, filter='sinc_window',\n    ...                      num_zeros=10, precision=5,\n    ...                      window=scipy.signal.hann)\n    '''\n\n    if window is None:\n        window = scipy.signal.blackmanharris\n    elif not six.callable(window):\n        raise TypeError('window must be callable, not type(window)={}'.format(type(window)))\n\n    if not 0 < rolloff <= 1:\n        raise ValueError('Invalid roll-off: rolloff={}'.format(rolloff))\n\n    if num_zeros < 1:\n        raise ValueError('Invalid num_zeros: num_zeros={}'.format(num_zeros))\n\n    if precision < 0:\n        raise ValueError('Invalid precision: precision={}'.format(precision))\n\n    # Generate the right-wing of the sinc\n    num_bits = 2**precision\n    n = num_bits * num_zeros\n    sinc_win = rolloff * np.sinc(rolloff * np.linspace(0, num_zeros, num=n + 1,\n                                                       endpoint=True))\n\n    # Build the window function and cut off the left half\n    taper = window(2 * n + 1)[n:]\n\n    interp_win = (taper * sinc_win)\n\n    return interp_win, num_bits, rolloff"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a window given its name or function handle.", "response": "def get_filter(name_or_function, **kwargs):\n    '''Retrieve a window given its name or function handle.\n\n    Parameters\n    ----------\n    name_or_function : str or callable\n        If a function, returns `name_or_function(**kwargs)`.\n\n        If a string, and it matches the name of one of the defined\n        filter functions, the corresponding function is called with `**kwargs`.\n\n        If a string, and it matches the name of a pre-computed filter,\n        the corresponding filter is retrieved, and kwargs is ignored.\n\n        Valid pre-computed filter names are:\n            - 'kaiser_fast'\n            - 'kaiser_best'\n\n    Returns\n    -------\n    half_window : np.ndarray\n        The right wing of the interpolation filter\n\n    precision : int > 0\n        The number of samples between zero-crossings of the filter\n\n    rolloff : float > 0\n        The roll-off frequency of the filter as a fraction of Nyquist\n\n    Raises\n    ------\n    NotImplementedError\n        If `name_or_function` cannot be found as a filter.\n    '''\n    if name_or_function in FILTER_FUNCTIONS:\n        return getattr(sys.modules[__name__], name_or_function)(**kwargs)\n    elif six.callable(name_or_function):\n        return name_or_function(**kwargs)\n    else:\n        try:\n            return load_filter(name_or_function)\n        except (IOError, ValueError):\n            raise NotImplementedError('Cannot load filter definition for '\n                                      '{}'.format(name_or_function))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a pre - computed filter.", "response": "def load_filter(filter_name):\n    '''Retrieve a pre-computed filter.\n\n    Parameters\n    ----------\n    filter_name : str\n        The key of the filter, e.g., 'kaiser_fast'\n\n    Returns\n    -------\n    half_window : np.ndarray\n        The right wing of the interpolation filter\n\n    precision : int > 0\n        The number of samples between zero-crossings of the fitler\n\n    rolloff : float > 0\n        The roll-off frequency of the filter, as a fraction of Nyquist\n    '''\n\n    fname = os.path.join('data',\n                         os.path.extsep.join([filter_name, 'npz']))\n\n    data = np.load(pkg_resources.resource_filename(__name__, fname))\n\n    return data['half_window'], data['precision'], data['rolloff']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_values_of(func):\n\n    signature = inspect.signature(func)\n    return [k\n            for k, v in signature.parameters.items()\n            if v.default is not inspect.Parameter.empty or\n            v.kind != inspect.Parameter.POSITIONAL_OR_KEYWORD]", "response": "Return the defaults of the function func."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all arguments of a function that do not have a default value.", "response": "def required_arguments(func):\n    \"\"\"Return all arguments of a function that do not have a default value.\"\"\"\n    defaults = default_values_of(func)\n    args = arguments_of(func)\n\n    if defaults:\n        args = args[:-len(defaults)]\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprompts the user to enter a free text message.", "response": "def text(message: Text,\n         default: Text = \"\",\n         validate: Union[Type[Validator],\n                         Callable[[Text], bool],\n                         None] = None,  # noqa\n         qmark: Text = DEFAULT_QUESTION_PREFIX,\n         style: Optional[Style] = None,\n         **kwargs: Any) -> Question:\n    \"\"\"Prompt the user to enter a free text message.\n\n       This question type can be used to prompt the user for some text input.\n\n       Args:\n           message: Question text\n\n           default: Default value will be returned if the user just hits\n                    enter.\n\n           validate: Require the entered value to pass a validation. The\n                     value can not be submited until the validator accepts\n                     it (e.g. to check minimum password length).\n\n                     This can either be a function accepting the input and\n                     returning a boolean, or an class reference to a\n                     subclass of the prompt toolkit Validator class.\n\n           qmark: Question prefix displayed in front of the question.\n                  By default this is a `?`\n\n           style: A custom color and style for the question parts. You can\n                  configure colors as well as font types for different elements.\n\n       Returns:\n           Question: Question instance, ready to be prompted (using `.ask()`).\n    \"\"\"\n\n    merged_style = merge_styles([DEFAULT_STYLE, style])\n\n    validator = build_validator(validate)\n\n    def get_prompt_tokens():\n        return [(\"class:qmark\", qmark),\n                (\"class:question\", ' {} '.format(message))]\n\n    p = PromptSession(get_prompt_tokens,\n                      style=merged_style,\n                      validator=validator,\n                      **kwargs)\n    p.default_buffer.reset(Document(default))\n\n    return Question(p.app)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def ask_async(self,\n                        patch_stdout: bool = False,\n                        kbi_msg: str = DEFAULT_KBI_MESSAGE) -> Any:\n        \"\"\"Ask the question using asyncio and return user response.\"\"\"\n\n        if self.should_skip_question:\n            return self.default\n\n        try:\n            sys.stdout.flush()\n            return await self.unsafe_ask_async(patch_stdout)\n        except KeyboardInterrupt:\n            print(\"\\n{}\\n\".format(kbi_msg))\n            return None", "response": "Ask the user for a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ask(self,\n            patch_stdout: bool = False,\n            kbi_msg: str = DEFAULT_KBI_MESSAGE) -> Any:\n        \"\"\"Ask the question synchronously and return user response.\"\"\"\n\n        if self.should_skip_question:\n            return self.default\n\n        try:\n            return self.unsafe_ask(patch_stdout)\n        except KeyboardInterrupt:\n            print(\"\\n{}\\n\".format(kbi_msg))\n            return None", "response": "Ask the question synchronously and return user response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nasking the question synchronously and return user response.", "response": "def unsafe_ask(self, patch_stdout: bool = False) -> Any:\n        \"\"\"Ask the question synchronously and return user response.\n\n        Does not catch keyboard interrupts.\"\"\"\n\n        if patch_stdout:\n            with prompt_toolkit.patch_stdout.patch_stdout():\n                return self.application.run()\n        else:\n            return self.application.run()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nskipping the question if flag is set and return the default instead.", "response": "def skip_if(self, condition: bool, default: Any = None) -> 'Question':\n        \"\"\"Skip the question if flag is set and return the default instead.\"\"\"\n\n        self.should_skip_question = condition\n        self.default = default\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def unsafe_ask_async(self, patch_stdout: bool = False) -> Any:\n\n        if not utils.ACTIVATED_ASYNC_MODE:\n            await utils.activate_prompt_toolkit_async_mode()\n\n        if patch_stdout:\n            # with prompt_toolkit.patch_stdout.patch_stdout():\n            return await self.application.run_async().to_asyncio_future()\n        else:\n            return await self.application.run_async().to_asyncio_future()", "response": "Ask the question using asyncio and return user response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_inquirer_layout(\n        ic: InquirerControl,\n        get_prompt_tokens: Callable[[], List[Tuple[Text, Text]]],\n        **kwargs) -> Layout:\n    \"\"\"Create a layout combining question and inquirer selection.\"\"\"\n\n    ps = PromptSession(get_prompt_tokens, reserve_space_for_menu=0, **kwargs)\n\n    _fix_unecessary_blank_lines(ps)\n\n    return Layout(HSplit([\n        ps.layout.container,\n        ConditionalContainer(\n            Window(ic),\n            filter=~IsDone()\n        )\n    ]))", "response": "Create a layout combining question and inquirer selection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(c: Union[Text, 'Choice', Dict[Text, Any]]) -> 'Choice':\n\n        if isinstance(c, Choice):\n            return c\n        elif isinstance(c, str):\n            return Choice(c, c)\n        else:\n            return Choice(c.get('name'),\n                          c.get('value'),\n                          c.get('disabled', None),\n                          c.get('checked'),\n                          c.get('key'))", "response": "Create a choice object from different representations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef password(message: Text,\n             default: Text = \"\",\n             validate: Union[Type[Validator],\n                             Callable[[Text], bool],\n                             None] = None,  # noqa\n             qmark: Text = DEFAULT_QUESTION_PREFIX,\n             style: Optional[Style] = None,\n             **kwargs: Any) -> Question:\n    \"\"\"Question the user to enter a secret text not displayed in the prompt.\n\n       This question type can be used to prompt the user for information\n       that should not be shown in the command line. The typed text will be\n       replaced with `*`.\n\n       Args:\n           message: Question text\n\n           default: Default value will be returned if the user just hits\n                    enter.\n\n           validate: Require the entered value to pass a validation. The\n                     value can not be submited until the validator accepts\n                     it (e.g. to check minimum password length).\n\n                     This can either be a function accepting the input and\n                     returning a boolean, or an class reference to a\n                     subclass of the prompt toolkit Validator class.\n\n           qmark: Question prefix displayed in front of the question.\n                  By default this is a `?`\n\n           style: A custom color and style for the question parts. You can\n                  configure colors as well as font types for different elements.\n\n       Returns:\n           Question: Question instance, ready to be prompted (using `.ask()`).\n    \"\"\"\n\n    return text.text(message, default, validate, qmark, style,\n                     is_password=True, **kwargs)", "response": "Prompt the user to enter a password."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(message: Text,\n           choices: List[Union[Text, Choice, Dict[Text, Any]]],\n           default: Optional[Text] = None,\n           qmark: Text = DEFAULT_QUESTION_PREFIX,\n           style: Optional[Style] = None,\n           use_shortcuts: bool = False,\n           use_indicator: bool = False,\n           **kwargs: Any) -> Question:\n    \"\"\"Prompt the user to select one item from the list of choices.\n\n    The user can only select one option.\n\n    Args:\n        message: Question text\n\n        choices: Items shown in the selection, this can contain `Choice` or\n                 or `Separator` objects or simple items as strings. Passing\n                 `Choice` objects, allows you to configure the item more\n                 (e.g. preselecting it or disabeling it).\n\n        default: Default return value (single value).\n\n        qmark: Question prefix displayed in front of the question.\n               By default this is a `?`\n\n        style: A custom color and style for the question parts. You can\n               configure colors as well as font types for different elements.\n\n        use_indicator: Flag to enable the small indicator in front of the\n                       list highlighting the current location of the selection\n                       cursor.\n\n        use_shortcuts: Allow the user to select items from the list using\n                       shortcuts. The shortcuts will be displayed in front of\n                       the list items.\n    Returns:\n        Question: Question instance, ready to be prompted (using `.ask()`).\n    \"\"\"\n    if choices is None or len(choices) == 0:\n        raise ValueError('A list of choices needs to be provided.')\n\n    if use_shortcuts and len(choices) > len(InquirerControl.SHORTCUT_KEYS):\n        raise ValueError('A list with shortcuts supports a maximum of {} '\n                         'choices as this is the maximum number '\n                         'of keyboard shortcuts that are available. You'\n                         'provided {} choices!'\n                         ''.format(len(InquirerControl.SHORTCUT_KEYS),\n                                   len(choices)))\n\n    merged_style = merge_styles([DEFAULT_STYLE, style])\n\n    ic = InquirerControl(choices, default,\n                         use_indicator=use_indicator,\n                         use_shortcuts=use_shortcuts)\n\n    def get_prompt_tokens():\n        # noinspection PyListCreation\n        tokens = [(\"class:qmark\", qmark),\n                  (\"class:question\", ' {} '.format(message))]\n\n        if ic.is_answered:\n            tokens.append((\"class:answer\", ' ' + ic.get_pointed_at().title))\n        else:\n            if use_shortcuts:\n                tokens.append((\"class:instruction\", ' (Use shortcuts)'))\n            else:\n                tokens.append((\"class:instruction\", ' (Use arrow keys)'))\n\n        return tokens\n\n    layout = common.create_inquirer_layout(ic, get_prompt_tokens, **kwargs)\n\n    bindings = KeyBindings()\n\n    @bindings.add(Keys.ControlQ, eager=True)\n    @bindings.add(Keys.ControlC, eager=True)\n    def _(event):\n        event.app.exit(exception=KeyboardInterrupt, style='class:aborting')\n\n    if use_shortcuts:\n        # add key bindings for choices\n        for i, c in enumerate(ic.choices):\n            if isinstance(c, Separator):\n                continue\n\n            # noinspection PyShadowingNames\n            def _reg_binding(i, keys):\n                # trick out late evaluation with a \"function factory\":\n                # https://stackoverflow.com/a/3431699\n                @bindings.add(keys, eager=True)\n                def select_choice(event):\n                    ic.pointed_at = i\n\n            _reg_binding(i, c.shortcut_key)\n    else:\n        @bindings.add(Keys.Down, eager=True)\n        @bindings.add(\"j\", eager=True)\n        def move_cursor_down(event):\n            ic.select_next()\n            while not ic.is_selection_valid():\n                ic.select_next()\n\n        @bindings.add(Keys.Up, eager=True)\n        @bindings.add(\"k\", eager=True)\n        def move_cursor_up(event):\n            ic.select_previous()\n            while not ic.is_selection_valid():\n                ic.select_previous()\n\n    @bindings.add(Keys.ControlM, eager=True)\n    def set_answer(event):\n        ic.is_answered = True\n        event.app.exit(result=ic.get_pointed_at().value)\n\n    @bindings.add(Keys.Any)\n    def other(event):\n        \"\"\"Disallow inserting other text. \"\"\"\n        pass\n\n    return Question(Application(\n        layout=layout,\n        key_bindings=bindings,\n        style=merged_style,\n        **kwargs\n    ))", "response": "Prompt the user to select one item from the list of choices."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef form(**kwargs: Question):\n    return Form(*(FormField(k, q) for k, q in kwargs.items()))", "response": "Create a form with multiple questions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prompt(questions: List[Dict[Text, Any]],\n           answers: Optional[Dict[Text, Any]] = None,\n           patch_stdout: bool = False,\n           true_color: bool = False,\n           kbi_msg: Text = DEFAULT_KBI_MESSAGE,\n           **kwargs):\n    \"\"\"Prompt the user for input on all the questions.\"\"\"\n\n    if isinstance(questions, dict):\n        questions = [questions]\n\n    answers = answers or {}\n\n    for question_config in questions:\n        # import the question\n        if 'type' not in question_config:\n            raise PromptParameterException('type')\n        if 'name' not in question_config:\n            raise PromptParameterException('name')\n\n        choices = question_config.get('choices')\n        if choices is not None and callable(choices):\n            question_config['choices'] = choices(answers)\n\n        _kwargs = kwargs.copy()\n        _kwargs.update(question_config)\n\n        _type = _kwargs.pop('type')\n        _filter = _kwargs.pop('filter', None)\n        name = _kwargs.pop('name')\n        when = _kwargs.pop('when', None)\n\n        if true_color:\n            _kwargs[\"color_depth\"] = ColorDepth.TRUE_COLOR\n\n        try:\n            if when:\n                # at least a little sanity check!\n                if callable(question_config['when']):\n                    try:\n                        if not question_config['when'](answers):\n                            continue\n                    except Exception as e:\n                        raise ValueError(\"Problem in 'when' check of {} \"\n                                         \"question: {}\".format(name, e))\n                else:\n                    raise ValueError(\"'when' needs to be function that \"\n                                     \"accepts a dict argument\")\n            if _filter:\n                # at least a little sanity check!\n                if not callable(_filter):\n                    raise ValueError(\"'filter' needs to be function that \"\n                                     \"accepts an argument\")\n\n            if callable(question_config.get('default')):\n                _kwargs['default'] = question_config['default'](answers)\n\n            create_question_func = prompt_by_name(_type)\n\n            if not create_question_func:\n                raise ValueError(\"No question type '{}' found. \"\n                                 \"Known question types are {}.\"\n                                 \"\".format(_type, \", \".join(AVAILABLE_PROMPTS)))\n\n            missing_args = list(utils.missing_arguments(create_question_func,\n                                                        _kwargs))\n            if missing_args:\n                raise PromptParameterException(missing_args[0])\n\n            question = create_question_func(**_kwargs)\n\n            answer = question.unsafe_ask(patch_stdout)\n\n            if answer is not None:\n                if _filter:\n                    try:\n                        answer = _filter(answer)\n                    except Exception as e:\n                        raise ValueError(\"Problem processing 'filter' of {} \"\n                                         \"question: {}\".format(name, e))\n                answers[name] = answer\n        except KeyboardInterrupt:\n            print('')\n            print(kbi_msg)\n            print('')\n            return {}\n    return answers", "response": "Prompt the user for input on all the questions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprompt the user to confirm or reject a question.", "response": "def confirm(message: Text,\n            default: bool = True,\n            qmark: Text = DEFAULT_QUESTION_PREFIX,\n            style: Optional[Style] = None,\n            **kwargs: Any) -> Question:\n    \"\"\"Prompt the user to confirm or reject.\n\n       This question type can be used to prompt the user for a confirmation\n       of a yes-or-no question. If the user just hits enter, the default\n       value will be returned.\n\n       Args:\n           message: Question text\n\n           default: Default value will be returned if the user just hits\n                    enter.\n\n           qmark: Question prefix displayed in front of the question.\n                  By default this is a `?`\n\n           style: A custom color and style for the question parts. You can\n                  configure colors as well as font types for different elements.\n\n       Returns:\n           Question: Question instance, ready to be prompted (using `.ask()`).\n       \"\"\"\n\n    merged_style = merge_styles([DEFAULT_STYLE, style])\n\n    status = {'answer': None}\n\n    def get_prompt_tokens():\n        tokens = []\n\n        tokens.append((\"class:qmark\", qmark))\n        tokens.append((\"class:question\", ' {} '.format(message)))\n\n        if status['answer'] is not None:\n            answer = ' {}'.format(YES if status['answer'] else NO)\n            tokens.append((\"class:answer\", answer))\n        else:\n            instruction = ' {}'.format(YES_OR_NO if default else NO_OR_YES)\n            tokens.append((\"class:instruction\", instruction))\n\n        return to_formatted_text(tokens)\n\n    bindings = KeyBindings()\n\n    @bindings.add(Keys.ControlQ, eager=True)\n    @bindings.add(Keys.ControlC, eager=True)\n    def _(event):\n        event.app.exit(exception=KeyboardInterrupt, style='class:aborting')\n\n    @bindings.add('n')\n    @bindings.add('N')\n    def key_n(event):\n        status['answer'] = False\n        event.app.exit(result=False)\n\n    @bindings.add('y')\n    @bindings.add('Y')\n    def key_y(event):\n        status['answer'] = True\n        event.app.exit(result=True)\n\n    @bindings.add(Keys.ControlM, eager=True)\n    def set_answer(event):\n        status['answer'] = default\n        event.app.exit(result=default)\n\n    @bindings.add(Keys.Any)\n    def other(event):\n        \"\"\"Disallow inserting other text.\"\"\"\n        pass\n\n    return Question(PromptSession(get_prompt_tokens,\n                                  key_bindings=bindings,\n                                  style=merged_style,\n                                  **kwargs).app)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rawselect(message: Text,\n              choices: List[Union[Text, Choice, Dict[Text, Any]]],\n              default: Optional[Text] = None,\n              qmark: Text = DEFAULT_QUESTION_PREFIX,\n              style: Optional[Style] = None,\n              **kwargs: Any) -> Question:\n    \"\"\"Ask the user to select one item from a list of choices using shortcuts.\n\n       The user can only select one option.\n\n       Args:\n           message: Question text\n\n           choices: Items shown in the selection, this can contain `Choice` or\n                    or `Separator` objects or simple items as strings. Passing\n                    `Choice` objects, allows you to configure the item more\n                    (e.g. preselecting it or disabeling it).\n\n           default: Default return value (single value).\n\n           qmark: Question prefix displayed in front of the question.\n                  By default this is a `?`\n\n           style: A custom color and style for the question parts. You can\n                  configure colors as well as font types for different elements.\n\n       Returns:\n           Question: Question instance, ready to be prompted (using `.ask()`).\n       \"\"\"\n    return select.select(message, choices, default, qmark, style,\n                         use_shortcuts=True,\n                         **kwargs)", "response": "Ask the user to select one item from a list of choices using shortcuts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nasking the user to select from a list of items.", "response": "def checkbox(message: Text,\n             choices: List[Union[Text, Choice, Dict[Text, Any]]],\n             default: Optional[Text] = None,\n             qmark: Text = DEFAULT_QUESTION_PREFIX,\n             style: Optional[Style] = None,\n             **kwargs: Any) -> Question:\n    \"\"\"Ask the user to select from a list of items.\n\n    This is a multiselect, the user can choose one, none or many of the\n    items.\n\n    Args:\n        message: Question text\n\n        choices: Items shown in the selection, this can contain `Choice` or\n                 or `Separator` objects or simple items as strings. Passing\n                 `Choice` objects, allows you to configure the item more\n                 (e.g. preselecting it or disabeling it).\n\n        default: Default return value (single value). If you want to preselect\n                 multiple items, use `Choice(\"foo\", checked=True)` instead.\n\n        qmark: Question prefix displayed in front of the question.\n               By default this is a `?`\n\n        style: A custom color and style for the question parts. You can\n               configure colors as well as font types for different elements.\n\n    Returns:\n        Question: Question instance, ready to be prompted (using `.ask()`).\n    \"\"\"\n\n    merged_style = merge_styles([DEFAULT_STYLE, style])\n\n    ic = InquirerControl(choices, default)\n\n    def get_prompt_tokens():\n        tokens = []\n\n        tokens.append((\"class:qmark\", qmark))\n        tokens.append((\"class:question\", ' {} '.format(message)))\n        if ic.is_answered:\n            nbr_selected = len(ic.selected_options)\n            if nbr_selected == 0:\n                tokens.append((\"class:answer\", ' done'))\n            elif nbr_selected == 1:\n                tokens.append((\"class:answer\",\n                               ' [{}]'.format(\n                                   ic.get_selected_values()[0].title)))\n            else:\n                tokens.append((\"class:answer\",\n                               ' done ({} selections)'.format(\n                                   nbr_selected)))\n        else:\n            tokens.append((\"class:instruction\",\n                           ' (Use arrow keys to move, '\n                           '<space> to select, '\n                           '<a> to toggle, '\n                           '<i> to invert)'))\n        return tokens\n\n    layout = common.create_inquirer_layout(ic, get_prompt_tokens, **kwargs)\n\n    bindings = KeyBindings()\n\n    @bindings.add(Keys.ControlQ, eager=True)\n    @bindings.add(Keys.ControlC, eager=True)\n    def _(event):\n        event.app.exit(exception=KeyboardInterrupt, style='class:aborting')\n\n    @bindings.add(' ', eager=True)\n    def toggle(event):\n        pointed_choice = ic.get_pointed_at().value\n        if pointed_choice in ic.selected_options:\n            ic.selected_options.remove(pointed_choice)\n        else:\n            ic.selected_options.append(pointed_choice)\n\n    @bindings.add('i', eager=True)\n    def invert(event):\n        inverted_selection = [c.value for c in ic.choices if\n                              not isinstance(c, Separator) and\n                              c.value not in ic.selected_options and\n                              not c.disabled]\n        ic.selected_options = inverted_selection\n\n    @bindings.add('a', eager=True)\n    def all(event):\n        all_selected = True  # all choices have been selected\n        for c in ic.choices:\n            if (not isinstance(c, Separator) and\n                    c.value not in ic.selected_options and not c.disabled):\n                # add missing ones\n                ic.selected_options.append(c.value)\n                all_selected = False\n        if all_selected:\n            ic.selected_options = []\n\n    @bindings.add(Keys.Down, eager=True)\n    @bindings.add(\"j\", eager=True)\n    def move_cursor_down(event):\n        ic.select_next()\n        while not ic.is_selection_valid():\n            ic.select_next()\n\n    @bindings.add(Keys.Up, eager=True)\n    @bindings.add(\"k\", eager=True)\n    def move_cursor_up(event):\n        ic.select_previous()\n        while not ic.is_selection_valid():\n            ic.select_previous()\n\n    @bindings.add(Keys.ControlM, eager=True)\n    def set_answer(event):\n        ic.is_answered = True\n        event.app.exit(result=[c.value for c in ic.get_selected_values()])\n\n    @bindings.add(Keys.Any)\n    def other(event):\n        \"\"\"Disallow inserting other text. \"\"\"\n        pass\n\n    return Question(Application(\n        layout=layout,\n        key_bindings=bindings,\n        style=merged_style,\n        **kwargs\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict containing a list of cassandra. cassandraengine. Model classes containing a list of cassandra. cassandraengine. Model classes containing cassandra. cassandraengine. Model classes within installed App.", "response": "def _discover_models(self):\n        \"\"\"\n        Return a dict containing a list of cassandra.cqlengine.Model classes\n        within installed App.\n        \"\"\"\n\n        apps = get_installed_apps()\n        connection = self.connection.connection.alias\n        keyspace = self.connection.connection.keyspace\n\n        for app in apps:\n            self._cql_models[app.__name__] = get_cql_models(\n                app, connection=connection, keyspace=keyspace)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef django_table_names(self, only_existing=False, **kwargs):\n\n        all_models = list(chain.from_iterable(self.cql_models.values()))\n        tables = [model.column_family_name(include_keyspace=False)\n                  for model in all_models]\n\n        return tables", "response": "Returns a list of all table names that have associated cqlengine models\n        and are present in settings. INSTALLED_APPS."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all table names in current keyspace", "response": "def table_names(self, cursor=None, **kwargs):\n        \"\"\"\n        Returns all table names in current keyspace\n        \"\"\"\n        # Avoid migration code being executed\n        if cursor:\n            return []\n\n        connection = self.connection.connection\n        keyspace_name = connection.keyspace\n        if not connection.cluster.schema_metadata_enabled and \\\n                keyspace_name not in connection.cluster.metadata.keyspaces:\n            connection.cluster.refresh_schema_metadata()\n\n        keyspace = connection.cluster.metadata.keyspaces[keyspace_name]\n        return keyspace.tables"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_models_keyspace(self, keyspace):\n\n        for models in self.connection.introspection.cql_models.values():\n            for model in models:\n                model.__keyspace__ = keyspace", "response": "Set keyspace for all connection models"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_model_instance(self, data):\n        return self.model(\n            session_key=self._get_or_create_session_key(),\n            session_data=self.encode(data),\n            expire_date=self.get_expiry_date(),\n        )", "response": "Create a new instance of the session model object which represents the current session state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the current session data to the database.", "response": "def save(self, must_create=False):\n        \"\"\"\n        Saves the current session data to the database. If 'must_create' is\n        True, a database error will be raised if the saving operation doesn't\n        create a *new* entry (as opposed to possibly updating an existing\n        entry).\n\n        :param must_create:\n        \"\"\"\n        if self.session_key is None:\n            return self.create()\n        data = self._get_session(no_load=must_create)\n        obj = self.create_model_instance(data)\n        obj.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_field_kwargs(self, field_name, model_field):\n        kwargs = {}\n        validator_kwarg = list(model_field.validators)\n\n        # The following will only be used by ModelField classes.\n        # Gets removed for everything else.\n        kwargs['model_field'] = model_field\n\n        if model_field.verbose_name and needs_label(model_field, field_name):\n            kwargs['label'] = capfirst(model_field.verbose_name)\n\n        if model_field.help_text:\n            kwargs['help_text'] = model_field.help_text\n\n        max_digits = getattr(model_field, 'max_digits', None)\n        if max_digits is not None:\n            kwargs['max_digits'] = max_digits\n\n        decimal_places = getattr(model_field, 'decimal_places', None)\n        if decimal_places is not None:\n            kwargs['decimal_places'] = decimal_places\n\n        if isinstance(model_field, models.TextField):\n            kwargs['style'] = {'base_template': 'textarea.html'}\n\n        if isinstance(model_field, models.AutoField) \\\n                or not model_field.editable:\n            # If this field is read-only, then return early.\n            # Further keyword arguments are not valid.\n            kwargs['read_only'] = True\n            return kwargs\n\n        if model_field.has_default or model_field.blank or model_field.null:\n            kwargs['required'] = False\n\n        if model_field.null and not isinstance(model_field,\n                                               models.NullBooleanField):\n            kwargs['allow_null'] = True\n\n        if model_field.blank and (\n            isinstance(model_field, models.CharField) or\n            isinstance(model_field, models.TextField) or\n            isinstance(model_field, columns.Text)\n        ):\n            kwargs['allow_blank'] = True\n\n        if isinstance(model_field, models.FilePathField):\n            kwargs['path'] = model_field.path\n\n            if model_field.match is not None:\n                kwargs['match'] = model_field.match\n\n            if model_field.recursive is not False:\n                kwargs['recursive'] = model_field.recursive\n\n            if model_field.allow_files is not True:\n                kwargs['allow_files'] = model_field.allow_files\n\n            if model_field.allow_folders is not False:\n                kwargs['allow_folders'] = model_field.allow_folders\n\n        if model_field.choices:\n            # If this model field contains choices, then return early.\n            # Further keyword arguments are not valid.\n            kwargs['choices'] = model_field.choices\n            return kwargs\n\n        # Our decimal validation is handled in the field code,\n        # not validator code.\n        # (In Django 1.9+ this differs from previous style)\n        if isinstance(model_field, models.DecimalField) and DecimalValidator:\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, DecimalValidator)\n            ]\n\n        # Ensure that max_length is passed explicitly as a keyword arg,\n        # rather than as a validator.\n        max_length = getattr(model_field, 'max_length', None)\n        if max_length is not None and (\n                    isinstance(model_field, models.CharField) or\n                    isinstance(model_field, models.TextField)):\n            kwargs['max_length'] = max_length\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, validators.MaxLengthValidator)\n            ]\n\n        # Ensure that min_length is passed explicitly as a keyword arg,\n        # rather than as a validator.\n        min_length = next((\n            validator.limit_value for validator in validator_kwarg\n            if isinstance(validator, validators.MinLengthValidator)\n        ), None)\n        if min_length is not None and isinstance(model_field,\n                                                 models.CharField):\n            kwargs['min_length'] = min_length\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, validators.MinLengthValidator)\n            ]\n\n        # Ensure that max_value is passed explicitly as a keyword arg,\n        # rather than as a validator.\n        max_value = next((\n            validator.limit_value for validator in validator_kwarg\n            if isinstance(validator, validators.MaxValueValidator)\n        ), None)\n        if max_value is not None and isinstance(model_field,\n                                                NUMERIC_FIELD_TYPES):\n            kwargs['max_value'] = max_value\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, validators.MaxValueValidator)\n            ]\n\n        # Ensure that max_value is passed explicitly as a keyword arg,\n        # rather than as a validator.\n        min_value = next((\n            validator.limit_value for validator in validator_kwarg\n            if isinstance(validator, validators.MinValueValidator)\n        ), None)\n        if min_value is not None and isinstance(model_field,\n                                                NUMERIC_FIELD_TYPES):\n            kwargs['min_value'] = min_value\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, validators.MinValueValidator)\n            ]\n\n        # URLField does not need to include the URLValidator argument,\n        # as it is explicitly added in.\n        if isinstance(model_field, models.URLField):\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if not isinstance(validator, validators.URLValidator)\n            ]\n\n        # EmailField does not need to include the validate_email argument,\n        # as it is explicitly added in.\n        if isinstance(model_field, models.EmailField):\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if validator is not validators.validate_email\n            ]\n\n        # SlugField do not need to include the 'validate_slug' argument,\n        if isinstance(model_field, models.SlugField):\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if validator is not validators.validate_slug\n            ]\n\n        # IPAddressField do not need to include the 'validate_ipv46_address'\n        # argument,\n        if isinstance(model_field, models.GenericIPAddressField):\n            validator_kwarg = [\n                validator for validator in validator_kwarg\n                if validator is not validators.validate_ipv46_address\n            ]\n\n        if getattr(model_field, 'unique', False):\n            warnings.warn(\n                'UniqueValidator is currently not supported '\n                'in DjangoCassandraSerializer'\n            )\n\n        if validator_kwarg:\n            kwargs['validators'] = validator_kwarg\n\n        return kwargs", "response": "Returns a dictionary of keyword arguments for a basic non - relational field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport the management module within each installed app and register the dispatcher events.", "response": "def _import_management():\n        \"\"\"\n        Import the 'management' module within each installed app, to register\n        dispatcher events.\n        \"\"\"\n\n        from importlib import import_module\n\n        for app_name in settings.INSTALLED_APPS:\n            try:\n                import_module('.management', app_name)\n            except SystemError:\n                # We get SystemError if INSTALLED_APPS contains the\n                # name of a class rather than a module\n                pass\n            except ImportError as exc:\n                # This is slightly hackish. We want to ignore ImportErrors\n                # if the \"management\" module itself is missing -- but we don't\n                # want to ignore the exception if the management module exists\n                # but raises an ImportError for some reason. The only way we\n                # can do this is to check the text of the exception. Note that\n                # we're a bit broad in how we check the text, because different\n                # Python implementations may not use the same text.\n                # CPython uses the text \"No module named management\"\n                # PyPy uses \"No module named myproject.myapp.management\"\n                msg = exc.args[0]\n                if not msg.startswith('No module named') \\\n                        or 'management' not in msg:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_installed_apps():\n    if django.VERSION >= (1, 7):\n        from django.apps import apps\n        return [a.models_module for a in apps.get_app_configs()\n                if a.models_module is not None]\n    else:\n        from django.db import models\n        return models.get_apps()", "response": "Return list of all installed apps"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of all cassandra. cqlengine. Model objects within app that should be synced to keyspace", "response": "def get_cql_models(app, connection=None, keyspace=None):\n    \"\"\"\n    :param app: django models module\n    :param connection: connection name\n    :param keyspace: keyspace\n    :return: list of all cassandra.cqlengine.Model within app that should be\n    synced to keyspace.\n    \"\"\"\n    from .models import DjangoCassandraModel\n    models = []\n    single_cassandra_connection = len(list(get_cassandra_connections())) == 1\n    is_default_connection = connection == DEFAULT_DB_ALIAS or \\\n        single_cassandra_connection\n\n    for name, obj in inspect.getmembers(app):\n        cql_model_types = (\n            cqlengine.models.Model,\n            DjangoCassandraModel\n        )\n        if (\n            inspect.isclass(obj) and issubclass(obj, cql_model_types) and\n            not obj.__abstract__\n        ):\n            if obj.__connection__ == connection or \\\n                    (obj.__connection__ is None and is_default_connection) or \\\n                    obj.__connection__ is None and obj.__keyspace__ is not None and obj.__keyspace__ == keyspace:\n                models.append(obj)\n\n    return models"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cassandra_connections():\n\n    from django.db import connections\n    for alias in connections:\n        engine = connections[alias].settings_dict.get('ENGINE', '')\n        if engine == 'django_cassandra_engine':\n            yield alias, connections[alias]", "response": "Returns a list of tuples for all cassandra_connection_names in DATABASES dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_cassandra_connection():\n    for alias, conn in get_cassandra_connections():\n        if conn.connection.default:\n            return alias, conn\n\n    return list(get_cassandra_connections())[0]", "response": "Return first default cassandra connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cassandra_connection(alias=None, name=None):\n\n    for _alias, connection in get_cassandra_connections():\n        if alias is not None:\n            if alias == _alias:\n                return connection\n        elif name is not None:\n            if name == connection.settings_dict['NAME']:\n                return connection\n        else:\n            return connection", "response": "Returns cassandra connection with given alias or just first found."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides handle to change django_cassandra_engine to be dummy database backend Pretend django_cassandra_engine to be dummy database backend with no support for migrations.", "response": "def handle(self, *args, **options):\n        \"\"\"\n        Pretend django_cassandra_engine to be dummy database backend\n        with no support for migrations.\n        \"\"\"\n        self._change_cassandra_engine_name('django.db.backends.dummy')\n        try:\n            super(Command, self).handle(*args, **options)\n        finally:\n            self._change_cassandra_engine_name('django_cassandra_engine')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sql_flush(self, style, tables, sequences, allow_cascade=False):\n\n        for table in tables:\n            qs = \"TRUNCATE {}\".format(table)\n            self.connection.connection.execute(qs)\n\n        return []", "response": "Truncate all existing tables in current keyspace."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts field names including pk to real field names.", "response": "def convert_pk_field_names_to_real(model, field_names):\n    \"\"\"\n    Convert field names including 'pk' to the real field names:\n\n    >>> convert_pk_field_names_to_real(['pk', 'another_field'])\n    ['real_pk_field', 'another_field']\n    \"\"\"\n    pk_field_names = tuple(f.name for f in model._get_primary_key_columns())\n\n    def append_field(field_name):\n        if field_name not in real_field_names:\n            real_field_names.append(field_name)\n\n    real_field_names = []\n    for name in field_names:\n        if name == 'pk':\n            for real_pk_field_name in pk_field_names:\n                append_field(real_pk_field_name)\n        elif name == '-pk':\n            for real_pk_field_name in pk_field_names:\n                append_field('-' + real_pk_field_name)\n        else:\n            append_field(name)\n    return real_field_names"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd each field as a private field.", "response": "def add_field(self, field, **kwargs):\n        \"\"\"Add each field as a private field.\"\"\"\n        getattr(self, self._private_fields_name).append(field)\n        self._expire_cache(reverse=True)\n        self._expire_cache(reverse=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding Django Field attributes to each cqlengine. Column instance.", "response": "def _give_columns_django_field_attributes(self):\n        \"\"\"\n        Add Django Field attributes to each cqlengine.Column instance.\n\n        So that the Django Options class may interact with it as if it were\n        a Django Field.\n        \"\"\"\n        methods_to_add = (\n            django_field_methods.value_from_object,\n            django_field_methods.value_to_string,\n            django_field_methods.get_attname,\n            django_field_methods.get_cache_name,\n            django_field_methods.pre_save,\n            django_field_methods.get_prep_value,\n            django_field_methods.get_choices,\n            django_field_methods.get_choices_default,\n            django_field_methods.save_form_data,\n            django_field_methods.formfield,\n            django_field_methods.get_db_prep_value,\n            django_field_methods.get_db_prep_save,\n            django_field_methods.db_type_suffix,\n            django_field_methods.select_format,\n            django_field_methods.get_internal_type,\n            django_field_methods.get_attname_column,\n            django_field_methods.check,\n            django_field_methods._check_field_name,\n            django_field_methods._check_db_index,\n            django_field_methods.deconstruct,\n            django_field_methods.run_validators,\n            django_field_methods.clean,\n            django_field_methods.get_db_converters,\n            django_field_methods.get_prep_lookup,\n            django_field_methods.get_db_prep_lookup,\n            django_field_methods.get_filter_kwargs_for_object,\n            django_field_methods.set_attributes_from_name,\n            django_field_methods.db_parameters,\n            django_field_methods.get_pk_value_on_save,\n            django_field_methods.get_col,\n        )\n        for name, cql_column in six.iteritems(self._defined_columns):\n            self._set_column_django_attributes(cql_column=cql_column, name=name)\n            for method in methods_to_add:\n                try:\n                    method_name = method.func_name\n                except AttributeError:\n                    # python 3\n                    method_name = method.__name__\n\n                new_method = six.create_bound_method(method, cql_column)\n                setattr(cql_column, method_name, new_method)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbases on cqlengine. models. BaseModel. _get_column but to work with pk", "response": "def _get_column(cls, name):\n        \"\"\"\n        Based on cqlengine.models.BaseModel._get_column.\n\n        But to work with 'pk'\n        \"\"\"\n        if name == 'pk':\n            return cls._meta.get_field(cls._meta.pk.name)\n        return cls._columns[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_next(request):\n    next = request.POST.get('next', request.GET.get('next',\n                            request.META.get('HTTP_REFERER', None)))\n    if not next:\n        next = request.path\n    return next", "response": "This function returns the next URL for the current page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transpose_image(self, image):\n        EXIF_ORIENTATION_STEPS = {\n            1: [],\n            2: ['FLIP_LEFT_RIGHT'],\n            3: ['ROTATE_180'],\n            4: ['FLIP_TOP_BOTTOM'],\n            5: ['ROTATE_270', 'FLIP_LEFT_RIGHT'],\n            6: ['ROTATE_270'],\n            7: ['ROTATE_90', 'FLIP_LEFT_RIGHT'],\n            8: ['ROTATE_90'],\n        }\n        try:\n            orientation = image._getexif()[0x0112]\n            ops = EXIF_ORIENTATION_STEPS[orientation]\n        except:\n            ops = []\n        for method in ops:\n            image = image.transpose(getattr(Image, method))\n        return image", "response": "Transposes an image by using the EXIF information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a cache key consisten of a username and image size.", "response": "def get_cache_key(user_or_username, size, prefix):\n    \"\"\"\n    Returns a cache key consisten of a username and image size.\n    \"\"\"\n    if isinstance(user_or_username, get_user_model()):\n        user_or_username = get_username(user_or_username)\n    key = six.u('%s_%s_%s') % (prefix, user_or_username, size)\n    return six.u('%s_%s') % (slugify(key)[:100],\n                             hashlib.md5(force_bytes(key)).hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef invalidate_cache(user, size=None):\n    sizes = set(settings.AVATAR_AUTO_GENERATE_SIZES)\n    if size is not None:\n        sizes.add(size)\n    for prefix in cached_funcs:\n        for size in sizes:\n            cache.delete(get_cache_key(user, size, prefix))", "response": "Function to be called when saving or changing an user s avatars."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a public token for sandbox testing.", "response": "def create(self,\n               institution_id,\n               initial_products,\n               _options=None,\n               webhook=None,\n               transactions__start_date=None,\n               transactions__end_date=None,\n               ):\n        '''\n        Generate a public token for sandbox testing.\n\n        :param  str     institution_id:\n\n        :param  [str]   initial_products:\n\n        :param  str     webhook:\n        '''\n        options = _options or {}\n\n        if webhook is not None:\n            options['webhook'] = webhook\n\n        transaction_options = {}\n        transaction_options.update(options.get('transactions', {}))\n        if transactions__start_date is not None:\n            transaction_options['start_date'] = transactions__start_date\n        if transactions__end_date is not None:\n            transaction_options['end_date'] = transactions__end_date\n        if transaction_options:\n            options['transactions'] = transaction_options\n\n        return self.client.post_public_key('/sandbox/public_token/create', {\n            'institution_id': institution_id,\n            'initial_products': initial_products,\n            'options': options,\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self,\n            access_token,\n            start_date,\n            end_date,\n            _options=None,\n            account_ids=None,\n            count=None,\n            offset=None,\n            ):\n        '''\n        Return accounts and transactions for an item.\n        (`HTTP docs <https://plaid.com/docs/api/#transactions>`__)\n\n        The transactions in the response are paginated -- compare the number of\n        transactions received so far against response['total_transactions'] to\n        determine whether to fetch another page.\n\n        :param  str     access_token:\n        :param  str     start_date:     The earliest date for transactions.\n        :param  str     end_date:       The latest date for transactions.\n        :param  [str]   account_ids:    A list of account_ids to retrieve for\n                                        the item. Optional.\n        :param  int     count:          The number of transactions to fetch.\n                                        Optional.\n        :param  int     offset:         The number of transactions to skip from\n                                        the beginning of the fetch. Optional.\n\n        All date should be formatted as ``YYYY-MM-DD``.\n        '''\n        options = _options or {}\n\n        if account_ids is not None:\n            options['account_ids'] = account_ids\n        if count is not None:\n            options['count'] = count\n        if offset is not None:\n            options['offset'] = offset\n\n        return self.client.post('/transactions/get', {\n            'access_token': access_token,\n            'start_date': start_date,\n            'end_date': end_date,\n            'options': options,\n        })", "response": "Get accounts and transactions for an item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_response(response):\n        '''\n        Create an error of the right class from an API response.\n\n        :param   response    dict        Response JSON\n        '''\n        cls = PLAID_ERROR_TYPE_MAP.get(response['error_type'], PlaidError)\n        return cls(response['error_message'],\n                   response['error_type'],\n                   response['error_code'],\n                   response['display_message'],\n                   response['request_id'],\n                   response.get('causes'))", "response": "Create an error of the right class from an API response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self,\n               access_tokens,\n               days_requested,\n               options=None):\n        '''\n        Create an asset report.\n\n        :param  [str]   access_tokens:  A list of access tokens, one token for\n                                        each Item to be included in the Asset\n                                        Report.\n        :param  int     days_requested: Days of transaction history requested\n                                        to be included in the Asset Report.\n        :param  dict    options:        An optional dictionary. For more\n                                        information on the options object, see\n                                        the documentation site listed above.\n        '''\n        options = options or {}\n\n        return self.client.post('/asset_report/create', {\n            'access_tokens': access_tokens,\n            'days_requested': days_requested,\n            'options': options,\n        })", "response": "Creates an asset report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refresh(self,\n                asset_report_token,\n                days_requested,\n                options=None):\n        '''\n        Create a new, refreshed asset report based on an existing asset report.\n\n        :param  str   asset_report_token:   The existing Asset Report's asset\n                                            report token.\n        :param  int   days_requested:       Days of transaction history\n                                            requested to be included in the\n                                            Asset Report.\n        :param  dict  options:              An optional dictionary. This is the\n                                            same object used in `create`.\n        '''\n        options = options or {}\n\n        return self.client.post('/asset_report/refresh', {\n            'asset_report_token': asset_report_token,\n            'days_requested': days_requested,\n            'options': options,\n        })", "response": "Refresh an existing asset report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self,\n            asset_report_token,\n            include_insights=False):\n        '''\n        Retrieves an asset report.\n\n        :param  str   asset_report_token:   The asset report token for the\n                                            asset report you created.\n        :param  bool  include_insights:     An optional boolean specifying\n                                            whether we should retrieve the\n                                            report as an Asset Report with\n                                            Insights. For more, see\n                                            https://plaid.com/docs/#retrieve-json-report-request.\n        '''\n\n        return self.client.post('/asset_report/get', {\n            'asset_report_token': asset_report_token,\n            'include_insights': include_insights,\n        })", "response": "Retrieves an asset report."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post(self, path, data, is_json=True):\n        '''Make a post request with client_id and secret key.'''\n        post_data = {\n            'client_id': self.client_id,\n            'secret': self.secret,\n        }\n        post_data.update(data)\n        return self._post(path, post_data, is_json)", "response": "Make a post request with client_id and secret key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post_public(self, path, data, is_json=True):\n        '''Make a post request requiring no auth.'''\n        return self._post(path, data, is_json)", "response": "Make a post request requiring no auth."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a post request using a public key.", "response": "def post_public_key(self, path, data, is_json=True):\n        '''Make a post request using a public key.'''\n        post_data = {\n            'public_key': self.public_key\n        }\n        post_data.update(data)\n        return self._post(path, post_data, is_json)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, count, offset=0, _options=None):\n        '''\n        Fetch all Plaid institutions, using /institutions/all.\n\n        :param  int     count:      Number of institutions to fetch.\n        :param  int     offset:     Number of institutions to skip.\n        '''\n\n        options = _options or {}\n\n        return self.client.post('/institutions/get', {\n            'count': count,\n            'offset': offset,\n            'options': options,\n        })", "response": "Fetch all Plaid institutions using / institutions / all."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_by_id(self, institution_id, _options=None):\n        '''\n        Fetch a single institution by id.\n\n        :param  str     institution_id:\n        '''\n        options = _options or {}\n\n        return self.client.post_public_key('/institutions/get_by_id', {\n            'institution_id': institution_id,\n            'options': options,\n        })", "response": "Fetch a single institution by id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(self, query, _options={}, products=None):\n        '''\n        Search all institutions by name.\n\n        :param  str      query:     Query against the full list of\n                                    institutions.\n        :param  [str]    products:  Filter FIs by available products. Optional.\n        '''\n        options = _options or {}\n\n        return self.client.post_public_key('/institutions/search', {\n            'query': query,\n            'products': products,\n            'options': options,\n        })", "response": "Search all institutions by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve account and routing numbers for checking and savings accounts.", "response": "def get(self,\n            access_token,\n            _options=None,\n            account_ids=None):\n        '''\n        Retrieve account and routing numbers for checking and savings accounts.\n        (`HTTP docs <https://plaid.com/docs/api/#auth>`__)\n\n        :param  str     access_token:\n        :param  [str]   account_ids:    A list of account_ids to retrieve for\n                                        the item. Optional.\n        '''\n        options = _options or {}\n        if account_ids is not None:\n            options['account_ids'] = account_ids\n\n        return self.client.post('/auth/get', {\n            'access_token': access_token,\n            'options': options,\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_avro(file_path_or_buffer, schema=None, **kwargs):\n    if isinstance(file_path_or_buffer, six.string_types):\n        with open(file_path_or_buffer, 'rb') as f:\n            return __file_to_dataframe(f, schema, **kwargs)\n    else:\n        return __file_to_dataframe(file_path_or_buffer, schema, **kwargs)", "response": "Reads an Avro file into a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites an Avro file from a DataFrame.", "response": "def to_avro(file_path_or_buffer, df, schema=None, codec='null', append=False):\n    \"\"\"\n    Avro file writer.\n\n    Args:\n        file_path_or_buffer:\n            Output file path or file-like object.\n        df: pd.DataFrame.\n        schema: Dict of Avro schema.\n            If it's set None, inferring schema.\n        append: Boolean to control if will append to existing file\n        codec: A string indicating the compression codec to use.\n            Default is no compression (\"null\"), other acceptable values are\n            \"snappy\" and \"deflate\". You must have python-snappy installed to use\n            the snappy codec.\n\n    \"\"\"\n    if schema is None:\n        schema = __schema_infer(df)\n\n    open_mode = 'wb' if not append else 'a+b'\n\n    if isinstance(file_path_or_buffer, six.string_types):\n        with open(file_path_or_buffer, open_mode) as f:\n            fastavro.writer(f, schema=schema,\n                            records=df.to_dict('records'), codec=codec)\n    else:\n        fastavro.writer(file_path_or_buffer, schema=schema,\n                        records=df.to_dict('records'), codec=codec)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, x, w=1):\n        self.n += w\n\n        if len(self) == 0:\n            self._add_centroid(Centroid(x, w))\n            return\n\n        S = self._find_closest_centroids(x)\n\n        while len(S) != 0 and w > 0:\n            j = choice(list(range(len(S))))\n            c_j = S[j]\n\n            q = self._compute_centroid_quantile(c_j)\n\n            # This filters the out centroids that do not satisfy the second part\n            # of the definition of S. See original paper by Dunning.\n            if c_j.count + w > self._threshold(q):\n                S.pop(j)\n                continue\n\n            delta_w = min(self._threshold(q) - c_j.count, w)\n            self._update_centroid(c_j, x, delta_w)\n            w -= delta_w\n            S.pop(j)\n\n        if w > 0:\n            self._add_centroid(Centroid(x, w))\n\n        if len(self) > self.K / self.delta:\n            self.compress()\n\n        return", "response": "Update the t - digest with value x and weight w."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the t - digest with an iterable of values.", "response": "def batch_update(self, values, w=1):\n        \"\"\"\n        Update the t-digest with an iterable of values. This assumes all points have the\n        same weight.\n        \"\"\"\n        for x in values:\n            self.update(x, w)\n        self.compress()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef percentile(self, p):\n\n        if not (0 <= p <= 100):\n            raise ValueError(\"p must be between 0 and 100, inclusive.\")\n\n        p = float(p)/100.\n        p *= self.n\n        c_i = None\n        t = 0\n\n        if p == 0:\n            return self.C.min_item()[1].mean\n\n        for i, key in enumerate(self.C.keys()):\n            c_i_plus_one = self.C[key]\n            if i == 0:\n                k = c_i_plus_one.count / 2\n\n            else:\n                k = (c_i_plus_one.count + c_i.count) / 2.\n                if p < t + k:\n                    z1 = p - t\n                    z2 = t + k - p\n                    return (c_i.mean * z2 + c_i_plus_one.mean * z1) / (z1 + z2)\n            c_i = c_i_plus_one\n            t += k\n\n        return self.C.max_item()[1].mean", "response": "Computes the percentile of a specific value in [ 0 100 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cdf(self, x):\n        t = 0\n        N = float(self.n)\n\n        if len(self) == 1:  # only one centroid\n            return int(x >= self.C.min_key())\n\n        for i, key in enumerate(self.C.keys()):\n            c_i = self.C[key]\n            if i == len(self) - 1:\n                delta = (c_i.mean - self.C.prev_item(key)[1].mean) / 2.\n            else:\n                delta = (self.C.succ_item(key)[1].mean - c_i.mean) / 2.\n            z = max(-1, (x - c_i.mean) / delta)\n\n            if z < 1:\n                return t / N + c_i.count / N * (z + 1) / 2\n\n            t += c_i.count\n        return 1", "response": "Computes the cdf of a specific value ie. computes F ( x ) where F denotes\n            the CDF of the distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trimmed_mean(self, p1, p2):\n        if not (p1 < p2):\n            raise ValueError(\"p1 must be between 0 and 100 and less than p2.\")\n\n        min_count = p1 / 100. * self.n\n        max_count = p2 / 100. * self.n\n\n        trimmed_sum = trimmed_count = curr_count = 0\n        for i, c in enumerate(self.C.values()):\n            next_count = curr_count + c.count\n            if next_count <= min_count:\n                curr_count = next_count\n                continue\n\n            count = c.count\n            if curr_count < min_count:\n                count = next_count - min_count\n            if next_count > max_count:\n                count -= next_count - max_count\n\n            trimmed_sum += count * c.mean\n            trimmed_count += count\n\n            if next_count >= max_count:\n                break\n            curr_count = next_count\n\n        if trimmed_count == 0:\n            return 0\n        return trimmed_sum / trimmed_count", "response": "Computes the mean of the distribution between the two percentiles p1 and p2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef centroids_to_list(self):\n        centroids = []\n        for key in self.C.keys():\n            tree_values = self.C.get_value(key)\n            centroids.append({'m':tree_values.mean, 'c':tree_values.count})\n        return centroids", "response": "Returns a Python list of the TDigest object s Centroid values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Python dictionary of the TDigest and internal Centroid values.", "response": "def to_dict(self):\n        \"\"\"\n        Returns a Python dictionary of the TDigest and internal Centroid values.\n        Or use centroids_to_list() for a list of only the Centroid values.\n\n        \"\"\"\n        return {'n':self.n, 'delta':self.delta, 'K':self.K, 'centroids':self.centroids_to_list()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the TDigest object with dictionary values.", "response": "def update_from_dict(self, dict_values):\n        \"\"\"\n        Updates TDigest object with dictionary values.\n\n        The digest delta and K values are optional if you would like to update them,\n        but the n value is not required because it is computed from the centroid weights.\n\n        For example, you can initalize a new TDigest:\n            digest = TDigest()\n        Then load dictionary values into the digest:\n            digest.update_from_dict({'K': 25, 'delta': 0.01, 'centroids': [{'c': 1.0, 'm': 1.0}, {'c': 1.0, 'm': 2.0}, {'c': 1.0, 'm': 3.0}]})\n\n        Or update an existing digest where the centroids will be appropriately merged:\n            digest = TDigest()\n            digest.update(1)\n            digest.update(2)\n            digest.update(3)\n            digest.update_from_dict({'K': 25, 'delta': 0.01, 'centroids': [{'c': 1.0, 'm': 1.0}, {'c': 1.0, 'm': 2.0}, {'c': 1.0, 'm': 3.0}]})\n\n        Resulting in the digest having merged similar centroids by increasing their weight:\n            {'K': 25, 'delta': 0.01, 'centroids': [{'c': 2.0, 'm': 1.0}, {'c': 2.0, 'm': 2.0}, {'c': 2.0, 'm': 3.0}], 'n': 6.0}\n\n        Alternative you can provide only a list of centroid values with update_centroids_from_list()\n\n        \"\"\"\n        self.delta = dict_values.get('delta', self.delta)\n        self.K = dict_values.get('K', self.K)\n        self.update_centroids_from_list(dict_values['centroids'])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds or update Centroids from a Python list.", "response": "def update_centroids_from_list(self, list_values):\n        \"\"\"\n        Add or update Centroids from a Python list.\n        Any existing centroids in the digest object are appropriately updated.\n\n        Example:\n            digest.update_centroids([{'c': 1.0, 'm': 1.0}, {'c': 1.0, 'm': 2.0}, {'c': 1.0, 'm': 3.0}])\n\n        \"\"\"\n        [self.update(value['m'], value['c']) for value in list_values]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if the matrix reprensents a directed graph .", "response": "def is_undirected(matrix):\n    \"\"\"\n    Determine if the matrix reprensents a directed graph\n\n    :param matrix: The matrix to tested\n    :returns: boolean\n    \"\"\"\n    if isspmatrix(matrix):\n        return sparse_allclose(matrix, matrix.transpose())\n    \n    return np.allclose(matrix, matrix.T)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_to_adjacency_matrix(matrix):\n    for i in range(matrix.shape[0]):\n        \n        if isspmatrix(matrix):\n            col = find(matrix[:,i])[2]\n        else:\n            col = matrix[:,i].T.tolist()[0]\n\n        coeff = max( Fraction(c).limit_denominator().denominator for c in col )\n        matrix[:,i] *= coeff\n\n    return matrix", "response": "Converts transition matrix into adjacency matrix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delta_matrix(matrix, clusters):\n    if isspmatrix(matrix):\n        delta = dok_matrix(matrix.shape)\n    else:\n        delta = np.zeros(matrix.shape)\n\n    for i in clusters :\n        for j in permutations(i, 2):\n            delta[j] = 1\n\n    return delta", "response": "Compute delta matrix where delta[i j ] = 1 for i in clusters and delta[j i ] = 1 for i in clusters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modularity(matrix, clusters):\n    matrix = convert_to_adjacency_matrix(matrix)\n    m = matrix.sum()\n\n    if isspmatrix(matrix):\n        matrix_2 = matrix.tocsr(copy=True)\n    else :\n        matrix_2 = matrix\n\n    if is_undirected(matrix):\n        expected = lambda i,j : (( matrix_2[i,:].sum() + matrix[:,i].sum() )*\n                                 ( matrix[:,j].sum() + matrix_2[j,:].sum() ))\n    else:\n        expected = lambda i,j : ( matrix_2[i,:].sum()*matrix[:,j].sum() )\n    \n    delta   = delta_matrix(matrix, clusters)\n    indices = np.array(delta.nonzero())\n    \n    Q = sum( matrix[i, j] - expected(i, j)/m for i, j in indices.T )/m\n    \n    return Q", "response": "Compute the modularity of the node in the cluster list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_graph(matrix, clusters, **kwargs):\n    # make a networkx graph from the adjacency matrix\n    graph = nx.Graph(matrix)\n    \n    # map node to cluster id for colors\n    cluster_map = {node: i for i, cluster in enumerate(clusters) for node in cluster}\n    colors = [cluster_map[i] for i in range(len(graph.nodes()))]\n    \n    # if colormap not specified in kwargs, use a default\n    if not kwargs.get(\"cmap\", False):\n        kwargs[\"cmap\"] = cm.tab20\n    \n    # draw\n    nx.draw_networkx(graph, node_color=colors, **kwargs)\n    axis(\"off\")\n    show(block=False)", "response": "Visualize the clustering\n    \n    :param matrix: The unprocessed adjacency matrix\n    :param clusters: list of tuples containing clusters as returned\n                     by 'get_clusters'\n    :param kwargs: Additional keyword arguments to be passed to\n                   networkx.draw_networkx"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nversion of np. allclose for use with sparse matrices", "response": "def sparse_allclose(a, b, rtol=1e-5, atol=1e-8):\n    \"\"\"\n    Version of np.allclose for use with sparse matrices\n    \"\"\"\n    c = np.abs(a - b) - rtol * np.abs(b)\n    # noinspection PyUnresolvedReferences\n    return c.max() <= atol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninflating a given matrix by raising the n elements to the given power.", "response": "def inflate(matrix, power):\n    \"\"\"\n    Apply cluster inflation to the given matrix by raising\n    each element to the given power.\n    \n    :param matrix: The matrix to be inflated\n    :param power: Cluster inflation parameter\n    :returns: The inflated matrix\n    \"\"\"\n    if isspmatrix(matrix):\n        return normalize(matrix.power(power))\n\n    return normalize(np.power(matrix, power))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying cluster expansion to the given matrix by raising the matrix to the given power.", "response": "def expand(matrix, power):\n    \"\"\"\n    Apply cluster expansion to the given matrix by raising\n    the matrix to the given power.\n    \n    :param matrix: The matrix to be expanded\n    :param power: Cluster expansion parameter\n    :returns: The expanded matrix\n    \"\"\"\n    if isspmatrix(matrix):\n        return matrix ** power\n\n    return np.linalg.matrix_power(matrix, power)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding self - loops to the matrix by setting the diagonal to loop_value", "response": "def add_self_loops(matrix, loop_value):\n    \"\"\"\n    Add self-loops to the matrix by setting the diagonal\n    to loop_value\n    \n    :param matrix: The matrix to add loops to\n    :param loop_value: Value to use for self-loops\n    :returns: The matrix with self-loops\n    \"\"\"\n    shape = matrix.shape\n    assert shape[0] == shape[1], \"Error, matrix is not square\"\n\n    if isspmatrix(matrix):\n        new_matrix = matrix.todok()\n    else:\n        new_matrix = matrix.copy()\n\n    for i in range(shape[0]):\n        new_matrix[i, i] = loop_value\n\n    if isspmatrix(matrix):\n        return new_matrix.tocsc()\n\n    return new_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prune(matrix, threshold):\n    if isspmatrix(matrix):\n        pruned = dok_matrix(matrix.shape)\n        pruned[matrix >= threshold] = matrix[matrix >= threshold]\n        pruned = pruned.tocsc()\n    else:\n        pruned = matrix.copy()\n        pruned[pruned < threshold] = 0\n\n    # keep max value in each column. same behaviour for dense/sparse\n    num_cols = matrix.shape[1]\n    row_indices = matrix.argmax(axis=0).reshape((num_cols,))\n    col_indices = np.arange(num_cols)\n    pruned[row_indices, col_indices] = matrix[row_indices, col_indices]\n\n    return pruned", "response": "Prune the matrix so that very small edges are removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the two matrices are approximately equal.", "response": "def converged(matrix1, matrix2):\n    \"\"\"\n    Check for convergence by determining if \n    matrix1 and matrix2 are approximately equal.\n    \n    :param matrix1: The matrix to compare with matrix2\n    :param matrix2: The matrix to compare with matrix1\n    :returns: True if matrix1 and matrix2 approximately equal\n    \"\"\"\n    if isspmatrix(matrix1) or isspmatrix(matrix2):\n        return sparse_allclose(matrix1, matrix2)\n\n    return np.allclose(matrix1, matrix2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a single iteration (expansion + inflation) of the mcl algorithm :param matrix: The matrix to perform the iteration on :param expansion: Cluster expansion factor :param inflation: Cluster inflation factor", "response": "def iterate(matrix, expansion, inflation):\n    \"\"\"\n    Run a single iteration (expansion + inflation) of the mcl algorithm\n    \n    :param matrix: The matrix to perform the iteration on\n    :param expansion: Cluster expansion factor\n    :param inflation: Cluster inflation factor\n    \"\"\"\n    # Expansion\n    matrix = expand(matrix, expansion)\n\n    # Inflation\n    matrix = inflate(matrix, inflation)\n\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_clusters(matrix):\n    if not isspmatrix(matrix):\n        # cast to sparse so that we don't need to handle different \n        # matrix types\n        matrix = csc_matrix(matrix)\n\n    # get the attractors - non-zero elements of the matrix diagonal\n    attractors = matrix.diagonal().nonzero()[0]\n\n    # somewhere to put the clusters\n    clusters = set()\n\n    # the nodes in the same row as each attractor form a cluster\n    for attractor in attractors:\n        cluster = tuple(matrix.getrow(attractor).nonzero()[1].tolist())\n        clusters.add(cluster)\n\n    return sorted(list(clusters))", "response": "Retrieve the clusters from the matrix produced by the MCL algorithm\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_mcl(matrix, expansion=2, inflation=2, loop_value=1,\n            iterations=100, pruning_threshold=0.001, pruning_frequency=1,\n            convergence_check_frequency=1, verbose=False):\n    \"\"\"\n    Perform MCL on the given similarity matrix\n    \n    :param matrix: The similarity matrix to cluster\n    :param expansion: The cluster expansion factor\n    :param inflation: The cluster inflation factor\n    :param loop_value: Initialization value for self-loops\n    :param iterations: Maximum number of iterations\n           (actual number of iterations will be less if convergence is reached)\n    :param pruning_threshold: Threshold below which matrix elements will be set\n           set to 0\n    :param pruning_frequency: Perform pruning every 'pruning_frequency'\n           iterations. \n    :param convergence_check_frequency: Perform the check for convergence\n           every convergence_check_frequency iterations\n    :param verbose: Print extra information to the console\n    :returns: The final matrix\n    \"\"\"\n    assert expansion > 1, \"Invalid expansion parameter\"\n    assert inflation > 1, \"Invalid inflation parameter\"\n    assert loop_value >= 0, \"Invalid loop_value\"\n    assert iterations > 0, \"Invalid number of iterations\"\n    assert pruning_threshold >= 0, \"Invalid pruning_threshold\"\n    assert pruning_frequency > 0, \"Invalid pruning_frequency\"\n    assert convergence_check_frequency > 0, \"Invalid convergence_check_frequency\"\n\n    printer = MessagePrinter(verbose)\n\n    printer.print(\"-\" * 50)\n    printer.print(\"MCL Parameters\")\n    printer.print(\"Expansion: {}\".format(expansion))\n    printer.print(\"Inflation: {}\".format(inflation))\n    if pruning_threshold > 0:\n        printer.print(\"Pruning threshold: {}, frequency: {} iteration{}\".format(\n            pruning_threshold, pruning_frequency, \"s\" if pruning_frequency > 1 else \"\"))\n    else:\n        printer.print(\"No pruning\")\n    printer.print(\"Convergence check: {} iteration{}\".format(\n        convergence_check_frequency, \"s\" if convergence_check_frequency > 1 else \"\"))\n    printer.print(\"Maximum iterations: {}\".format(iterations))\n    printer.print(\"{} matrix mode\".format(\"Sparse\" if isspmatrix(matrix) else \"Dense\"))\n    printer.print(\"-\" * 50)\n\n    # Initialize self-loops\n    if loop_value > 0:\n        matrix = add_self_loops(matrix, loop_value)\n\n    # Normalize\n    matrix = normalize(matrix)\n\n    # iterations\n    for i in range(iterations):\n        printer.print(\"Iteration {}\".format(i + 1))\n\n        # store current matrix for convergence checking\n        last_mat = matrix.copy()\n\n        # perform MCL expansion and inflation\n        matrix = iterate(matrix, expansion, inflation)\n\n        # prune\n        if pruning_threshold > 0 and i % pruning_frequency == pruning_frequency - 1:\n            printer.print(\"Pruning\")\n            matrix = prune(matrix, pruning_threshold)\n\n        # Check for convergence\n        if i % convergence_check_frequency == convergence_check_frequency - 1:\n            printer.print(\"Checking for convergence\")\n            if converged(matrix, last_mat):\n                printer.print(\"Converged after {} iteration{}\".format(i + 1, \"s\" if i > 0 else \"\"))\n                break\n\n    printer.print(\"-\" * 50)\n\n    return matrix", "response": "This function performs MCL on the given similarity matrix and returns the final matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nguesses concat function from given data", "response": "def _guess_concat(data):\n    \"\"\"\n    Guess concat function from given data\n    \"\"\"\n    return {\n        type(u''): u''.join,\n        type(b''): concat_bytes,\n    }.get(type(data), list)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_code_table(self, out=sys.stdout):\n        out.write(u'bits  code       (value)  symbol\\n')\n        for symbol, (bitsize, value) in sorted(self._table.items()):\n            out.write(u'{b:4d}  {c:10} ({v:5d})  {s!r}\\n'.format(\n                b=bitsize, v=value, s=symbol, c=bin(value)[2:].rjust(bitsize, '0')\n            ))", "response": "Print code table overview"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes given data in streaming fashion.", "response": "def encode_streaming(self, data):\n        \"\"\"\n        Encode given data in streaming fashion.\n\n        :param data: sequence of symbols (e.g. byte string, unicode string, list, iterator)\n        :return: generator of bytes (single character strings in Python2, ints in Python 3)\n        \"\"\"\n        # Buffer value and size\n        buffer = 0\n        size = 0\n        for s in data:\n            b, v = self._table[s]\n            # Shift new bits in the buffer\n            buffer = (buffer << b) + v\n            size += b\n            while size >= 8:\n                byte = buffer >> (size - 8)\n                yield to_byte(byte)\n                buffer = buffer - (byte << (size - 8))\n                size -= 8\n\n        # Handling of the final sub-byte chunk.\n        # The end of the encoded bit stream does not align necessarily with byte boundaries,\n        # so we need an \"end of file\" indicator symbol (_EOF) to guard against decoding\n        # the non-data trailing bits of the last byte.\n        # As an optimization however, while encoding _EOF, it is only necessary to encode up to\n        # the end of the current byte and cut off there.\n        # No new byte has to be started for the remainder, saving us one (or more) output bytes.\n        if size > 0:\n            b, v = self._table[_EOF]\n            buffer = (buffer << b) + v\n            size += b\n            if size >= 8:\n                byte = buffer >> (size - 8)\n            else:\n                byte = buffer << (8 - size)\n            yield to_byte(byte)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode given data. :param data: sequence of bytes (string, list or generator of bytes) :param as_list: whether to return as a list instead of :return:", "response": "def decode(self, data, as_list=False):\n        \"\"\"\n        Decode given data.\n\n        :param data: sequence of bytes (string, list or generator of bytes)\n        :param as_list: whether to return as a list instead of\n        :return:\n        \"\"\"\n        return self._concat(self.decode_streaming(data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes given data in streaming fashion", "response": "def decode_streaming(self, data):\n        \"\"\"\n        Decode given data in streaming fashion\n\n        :param data: sequence of bytes (string, list or generator of bytes)\n        :return: generator of symbols\n        \"\"\"\n        # Reverse lookup table: map (bitsize, value) to symbols\n        lookup = dict(((b, v), s) for (s, (b, v)) in self._table.items())\n\n        buffer = 0\n        size = 0\n        for byte in data:\n            for m in [128, 64, 32, 16, 8, 4, 2, 1]:\n                buffer = (buffer << 1) + bool(from_byte(byte) & m)\n                size += 1\n                if (size, buffer) in lookup:\n                    symbol = lookup[size, buffer]\n                    if symbol == _EOF:\n                        return\n                    yield symbol\n                    buffer = 0\n                    size = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_frequencies(cls, frequencies, concat=None):\n        concat = concat or _guess_concat(next(iter(frequencies)))\n\n        # Heap consists of tuples: (frequency, [list of tuples: (symbol, (bitsize, value))])\n        heap = [(f, [(s, (0, 0))]) for s, f in frequencies.items()]\n        # Add EOF symbol.\n        # TODO: argument to set frequency of EOF?\n        heap.append((1, [(_EOF, (0, 0))]))\n\n        # Use heapq approach to build the encodings of the huffman tree leaves.\n        heapify(heap)\n        while len(heap) > 1:\n            # Pop the 2 smallest items from heap\n            a = heappop(heap)\n            b = heappop(heap)\n            # Merge nodes (update codes for each symbol appropriately)\n            merged = (\n                a[0] + b[0],\n                [(s, (n + 1, v)) for (s, (n, v)) in a[1]]\n                + [(s, (n + 1, (1 << n) + v)) for (s, (n, v)) in b[1]]\n            )\n            heappush(heap, merged)\n\n        # Code table is dictionary mapping symbol to (bitsize, value)\n        table = dict(heappop(heap)[1])\n\n        return cls(table, concat=concat, check=False)", "response": "Build Huffman code table from a dictionary mapping symbol to frequency mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_data(cls, data):\n        frequencies = collections.Counter(data)\n        return cls.from_frequencies(frequencies, concat=_guess_concat(data))", "response": "Build HuffmanCoder\n        from sequence of symbols."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset state about the previous query in preparation for running another query", "response": "def _reset_state(self):\n        \"\"\"Reset state about the previous query in preparation for running another query\"\"\"\n        self._uuid = None\n        self._columns = None\n        self._rownumber = 0\n        # Internal helper state\n        self._state = self._STATE_NONE\n        self._data = None\n        self._columns = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, operation, parameters=None, is_response=True):\n        if parameters is None or not parameters:\n            sql = operation\n        else:\n            sql = operation % _escaper.escape_args(parameters)\n\n        self._reset_state()\n\n        self._state = self._STATE_RUNNING\n        self._uuid = uuid.uuid1()\n\n        if is_response:\n            response = self._db.select(sql, settings={'query_id': self._uuid})\n            self._process_response(response)\n        else:\n            self._db.raw(sql)", "response": "Prepare and execute a database operation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef executemany(self, operation, seq_of_parameters):\n        values_list = []\n        RE_INSERT_VALUES = re.compile(\n            r\"\\s*((?:INSERT|REPLACE)\\s.+\\sVALUES?\\s*)\" +\n            r\"(\\(\\s*(?:%s|%\\(.+\\)s)\\s*(?:,\\s*(?:%s|%\\(.+\\)s)\\s*)*\\))\" +\n            r\"(\\s*(?:ON DUPLICATE.*)?);?\\s*\\Z\",\n            re.IGNORECASE | re.DOTALL)\n\n        m = RE_INSERT_VALUES.match(operation)\n        if m:\n            q_prefix = m.group(1) % ()\n            q_values = m.group(2).rstrip()\n\n            for parameters in seq_of_parameters[:-1]:\n                values_list.append(q_values % _escaper.escape_args(parameters))\n            query = '{} {};'.format(q_prefix, ','.join(values_list))\n            return self._db.raw(query)\n        for parameters in seq_of_parameters[:-1]:\n            self.execute(operation, parameters, is_response=False)", "response": "Prepare a database operation and then execute it against all parameter\n            sequences or mappings found in the sequence seq_of_parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetchone(self):\n        if self._state == self._STATE_NONE:\n            raise Exception(\"No query yet\")\n        if not self._data:\n            return None\n        else:\n            self._rownumber += 1\n            return self._data.pop(0)", "response": "Fetch the next row of a query result set returning a single sequence or None when no more data is available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetchmany(self, size=None):\n        if self._state == self._STATE_NONE:\n            raise Exception(\"No query yet\")\n\n        if size is None:\n            size = 1\n\n        if not self._data:\n            return []\n        else:\n            if len(self._data) > size:\n                result, self._data = self._data[:size], self._data[size:]\n            else:\n                result, self._data = self._data, []\n            self._rownumber += len(result)\n            return result", "response": "Fetch the next set of rows of a query result returning a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetchall(self):\n        if self._state == self._STATE_NONE:\n            raise Exception(\"No query yet\")\n\n        if not self._data:\n            return []\n        else:\n            result, self._data = self._data, []\n            self._rownumber += len(result)\n            return result", "response": "Fetch all rows of a query result returning them as a sequence of sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the internal state with the data from the response", "response": "def _process_response(self, response):\n        \"\"\" Update the internal state with the data from the response \"\"\"\n        assert self._state == self._STATE_RUNNING, \"Should be running if processing response\"\n        cols = None\n        data = []\n        for r in response:\n            if not cols:\n                cols = [(f, r._fields[f].db_type) for f in r._fields]\n            data.append([getattr(r, f) for f in r._fields])\n        self._data = data\n        self._columns = cols\n        self._state = self._STATE_FINISHED"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _frequency_order_transform(sets):\n    logging.debug(\"Applying frequency order transform on tokens...\")\n    counts = reversed(Counter(token for s in sets for token in s).most_common())\n    order = dict((token, i) for i, (token, _) in enumerate(counts))\n    sets = [np.sort([order[token] for token in s]) for s in sets]\n    logging.debug(\"Done applying frequency order.\")\n    return sets, order", "response": "Transform tokens to integers according to global frequency order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_pairs(sets, similarity_func_name=\"jaccard\",\n        similarity_threshold=0.5):\n    \"\"\"Find all pairs of sets with similarity greater than a threshold.\n    This is an implementation of the All-Pair-Binary algorithm in the paper\n    \"Scaling Up All Pairs Similarity Search\" by Bayardo et al., with\n    position filter enhancement.\n\n    Args:\n        sets (list): a list of sets, each entry is an iterable representing a\n            set.\n        similarity_func_name (str): the name of the similarity function used;\n            this function currently supports `\"jaccard\"` and `\"cosine\"`.\n        similarity_threshold (float): the threshold used, must be a float\n            between 0 and 1.0.\n\n    Returns:\n        pairs (Iterator[tuple]): an iterator of tuples `(x, y, similarity)`\n            where `x` and `y` are the indices of sets in the input list `sets`.\n    \"\"\"\n    if not isinstance(sets, list) or len(sets) == 0:\n        raise ValueError(\"Input parameter sets must be a non-empty list.\")\n    if similarity_func_name not in _similarity_funcs:\n        raise ValueError(\"Similarity function {} is not supported.\".format(\n            similarity_func_name))\n    if similarity_threshold < 0 or similarity_threshold > 1.0:\n        raise ValueError(\"Similarity threshold must be in the range [0, 1].\")\n    if similarity_func_name not in _symmetric_similarity_funcs:\n        raise ValueError(\"The similarity function must be symmetric \"\n        \"({})\".format(\", \".join(_symmetric_similarity_funcs)))\n    similarity_func = _similarity_funcs[similarity_func_name]\n    overlap_threshold_func = _overlap_threshold_funcs[similarity_func_name]\n    position_filter_func = _position_filter_funcs[similarity_func_name]\n    sets, _ = _frequency_order_transform(sets)\n    index = defaultdict(list)\n    logging.debug(\"Find all pairs with similarities >= {}...\".format(\n        similarity_threshold))\n    count = 0\n    for x1 in np.argsort([len(s) for s in sets]):\n        s1 = sets[x1]\n        t = overlap_threshold_func(len(s1), similarity_threshold)\n        prefix_size = len(s1) - t + 1\n        prefix = s1[:prefix_size]\n        # Find candidates using tokens in the prefix.\n        candidates = set([x2 for p1, token in enumerate(prefix)\n                for x2, p2 in index[token]\n                if position_filter_func(s1, sets[x2], p1, p2,\n                    similarity_threshold)])\n        for x2 in candidates:\n            s2 = sets[x2]\n            sim = similarity_func(s1, s2)\n            if sim < similarity_threshold:\n                continue\n            # Output reverse-ordered set index pair (larger first).\n            yield tuple(sorted([x1, x2], reverse=True) + [sim,])\n            count += 1\n        # Insert this prefix into index.\n        for j, token in enumerate(prefix):\n            index[token].append((x1, j))\n    logging.debug(\"{} pairs found.\".format(count))", "response": "This function returns all pairs of sets with similarity greater than a threshold."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery the search index for sets similar to the query set.", "response": "def query(self, s):\n        \"\"\"Query the search index for sets similar to the query set.\n\n        Args:\n            s (Iterable): the query set.\n\n        Returns (list): a list of tuples `(index, similarity)` where the index\n            is the index of the matching sets in the original list of sets.\n        \"\"\"\n        s1 = np.sort([self.order[token] for token in s if token in self.order])\n        logging.debug(\"{} original tokens and {} tokens after applying \"\n            \"frequency order.\".format(len(s), len(s1)))\n        prefix = self._get_prefix(s1)\n        candidates = set([i for p1, token in enumerate(prefix)\n                for i, p2 in self.index[token]\n                if self.position_filter_func(s1, self.sets[i], p1, p2,\n                    self.similarity_threshold)])\n        logging.debug(\"{} candidates found.\".format(len(candidates)))\n        results = deque([])\n        for i in candidates:\n            s2 = self.sets[i]\n            sim = self.similarity_func(s1, s2)\n            if sim < self.similarity_threshold:\n                continue\n            results.append((i, sim))\n        logging.debug(\"{} verified sets found.\".format(len(results)))\n        return list(results)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, bqm, num_reads=10):\n        values = tuple(bqm.vartype.value)\n\n        def _itersample():\n            for __ in range(num_reads):\n                sample = {v: choice(values) for v in bqm.linear}\n                energy = bqm.energy(sample)\n\n                yield sample, energy\n\n        samples, energies = zip(*_itersample())\n\n        return SampleSet.from_samples(samples, bqm.vartype, energies)", "response": "Gives random samples for a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combinations(n, k, strength=1, vartype=BINARY):\n    if isinstance(n, abc.Sized) and isinstance(n, abc.Iterable):\n        # what we actually want is abc.Collection but that doesn't exist in\n        # python2\n        variables = n\n    else:\n        try:\n            variables = range(n)\n        except TypeError:\n            raise TypeError('n should be a collection or an integer')\n\n    if k > len(variables) or k < 0:\n        raise ValueError(\"cannot select k={} from {} variables\".format(k, len(variables)))\n\n    # (\\sum_i x_i - k)^2\n    #     = \\sum_i x_i \\sum_j x_j - 2k\\sum_i x_i + k^2\n    #     = \\sum_i,j x_ix_j + (1 - 2k)\\sim_i x_i + k^2\n    lbias = float(strength*(1 - 2*k))\n    qbias = float(2*strength)\n\n    bqm = BinaryQuadraticModel.empty(vartype)\n    bqm.add_variables_from(((v, lbias) for v in variables), vartype=BINARY)\n    bqm.add_interactions_from(((u, v, qbias)\n                               for u, v in itertools.combinations(variables, 2)),\n                              vartype=BINARY)\n    bqm.add_offset(strength*(k**2))\n\n    return bqm", "response": "r Generate a binary quadratic model that minimizes when k of n variables are selected."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random binary quadratic model from a graph.", "response": "def uniform(graph, vartype, low=0.0, high=1.0, cls=BinaryQuadraticModel,\n            seed=None):\n    \"\"\"Generate a bqm with random biases and offset.\n\n    Biases and offset are drawn from a uniform distribution range (low, high).\n\n    Args:\n        graph (int/tuple[nodes, edges]/:obj:`~networkx.Graph`):\n            The graph to build the bqm loops on. Either an integer n, interpreted as a\n            complete graph of size n, or a nodes/edges pair, or a NetworkX graph.\n\n        vartype (:class:`.Vartype`/str/set):\n            Variable type for the binary quadratic model. Accepted input values:\n\n            * :class:`.Vartype.SPIN`, ``'SPIN'``, ``{-1, 1}``\n            * :class:`.Vartype.BINARY`, ``'BINARY'``, ``{0, 1}``\n\n        low (float, optional, default=0.0):\n            The low end of the range for the random biases.\n\n        high (float, optional, default=1.0):\n            The high end of the range for the random biases.\n\n        cls (:class:`.BinaryQuadraticModel`):\n            Binary quadratic model class to build from.\n\n        seed (int, optional, default=None):\n            Random seed.\n\n    Returns:\n        :obj:`.BinaryQuadraticModel`\n\n    \"\"\"\n    if seed is None:\n        seed = numpy.random.randint(2**32, dtype=np.uint32)\n    r = numpy.random.RandomState(seed)\n\n    variables, edges = graph\n\n    index = {v: idx for idx, v in enumerate(variables)}\n\n    if edges:\n        irow, icol = zip(*((index[u], index[v]) for u, v in edges))\n    else:\n        irow = icol = tuple()\n\n    ldata = r.uniform(low, high, size=len(variables))\n    qdata = r.uniform(low, high, size=len(irow))\n    offset = r.uniform(low, high)\n\n    return cls.from_numpy_vectors(ldata, (irow, icol, qdata), offset, vartype,\n                                  variable_order=variables)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ran_r(r, graph, cls=BinaryQuadraticModel, seed=None):\n    if not isinstance(r, int):\n        raise TypeError(\"r should be a positive integer\")\n    if r < 1:\n        raise ValueError(\"r should be a positive integer\")\n\n    if seed is None:\n        seed = numpy.random.randint(2**32, dtype=np.uint32)\n    rnd = numpy.random.RandomState(seed)\n\n    variables, edges = graph\n\n    index = {v: idx for idx, v in enumerate(variables)}\n\n    if edges:\n        irow, icol = zip(*((index[u], index[v]) for u, v in edges))\n    else:\n        irow = icol = tuple()\n\n    ldata = np.zeros(len(variables))\n\n    rvals = np.empty(2*r)\n    rvals[0:r] = range(-r, 0)\n    rvals[r:] = range(1, r+1)\n    qdata = rnd.choice(rvals, size=len(irow))\n\n    offset = 0\n\n    return cls.from_numpy_vectors(ldata, (irow, icol, qdata), offset, vartype='SPIN',\n                                  variable_order=variables)", "response": "Generate an Ising model for a RANr problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a decorator which ensures that all other bqm variable labeling and all other bqm sample - like inputs are index labeled and consistent.", "response": "def bqm_index_labelled_input(var_labels_arg_name, samples_arg_names):\n    \"\"\"Returns a decorator which ensures bqm variable labeling and all other\n    specified sample-like inputs are index labeled and consistent.\n\n    Args:\n        var_labels_arg_name (str):\n            The name of the argument that the user should use to pass in an\n            index labeling for the bqm.\n\n        samples_arg_names (list[str]):\n            The names of the expected sample-like inputs which should be\n            indexed according to the labels passed to the argument\n            `var_labels_arg_name`.\n\n    Returns:\n        Function decorator.\n    \"\"\"\n\n    def index_label_decorator(f):\n        @wraps(f)\n        def _index_label(sampler, bqm, **kwargs):\n            if not hasattr(bqm, 'linear'):\n                raise TypeError('expected input to be a BinaryQuadraticModel')\n            linear = bqm.linear\n\n            var_labels = kwargs.get(var_labels_arg_name, None)\n            has_samples_input = any(kwargs.get(arg_name, None) is not None\n                                    for arg_name in samples_arg_names)\n\n            if var_labels is None:\n                # if already index-labelled, just continue\n                if all(v in linear for v in range(len(bqm))):\n                    return f(sampler, bqm, **kwargs)\n\n                if has_samples_input:\n                    err_str = (\"Argument `{}` must be provided if any of the\"\n                               \" samples arguments {} are provided and the \"\n                               \"bqm is not already index-labelled\".format(\n                                   var_labels_arg_name,\n                                   samples_arg_names))\n                    raise ValueError(err_str)\n\n                try:\n                    inverse_mapping = dict(enumerate(sorted(linear)))\n                except TypeError:\n                    # in python3 unlike types cannot be sorted\n                    inverse_mapping = dict(enumerate(linear))\n                var_labels = {v: i for i, v in iteritems(inverse_mapping)}\n\n            else:\n                inverse_mapping = {i: v for v, i in iteritems(var_labels)}\n\n            response = f(sampler,\n                         bqm.relabel_variables(var_labels, inplace=False),\n                         **kwargs)\n\n            # unapply the relabeling\n            return response.relabel_variables(inverse_mapping, inplace=True)\n\n        return _index_label\n\n    return index_label_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bqm_structured(f):\n\n    @wraps(f)\n    def new_f(sampler, bqm, **kwargs):\n        try:\n            structure = sampler.structure\n            adjacency = structure.adjacency\n        except AttributeError:\n            if isinstance(sampler, Structured):\n                raise RuntimeError(\"something is wrong with the structured sampler\")\n            else:\n                raise TypeError(\"sampler does not have a structure property\")\n\n        if not all(v in adjacency for v in bqm.linear):\n            # todo: better error message\n            raise BinaryQuadraticModelStructureError(\"given bqm does not match the sampler's structure\")\n        if not all(u in adjacency[v] for u, v in bqm.quadratic):\n            # todo: better error message\n            raise BinaryQuadraticModelStructureError(\"given bqm does not match the sampler's structure\")\n\n        return f(sampler, bqm, **kwargs)\n\n    return new_f", "response": "Decorator to raise an error if the given bqm does not match the sampler s structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vartype_argument(*arg_names):\n    # by default, constrain only one argument, the 'vartype`\n    if not arg_names:\n        arg_names = ['vartype']\n\n    def _vartype_arg(f):\n        argspec = getargspec(f)\n\n        def _enforce_single_arg(name, args, kwargs):\n            try:\n                vartype = kwargs[name]\n            except KeyError:\n                raise TypeError('vartype argument missing')\n\n            kwargs[name] = as_vartype(vartype)\n\n        @wraps(f)\n        def new_f(*args, **kwargs):\n            # bound actual f arguments (including defaults) to f argument names\n            # (note: if call arguments don't match actual function signature,\n            # we'll fail here with the standard `TypeError`)\n            bound_args = inspect.getcallargs(f, *args, **kwargs)\n\n            # `getcallargs` doesn't merge additional positional/keyword arguments,\n            # so do it manually\n            final_args = list(bound_args.pop(argspec.varargs, ()))\n            final_kwargs = bound_args.pop(argspec.keywords, {})\n\n            final_kwargs.update(bound_args)\n            for name in arg_names:\n                _enforce_single_arg(name, final_args, final_kwargs)\n\n            return f(*final_args, **final_kwargs)\n\n        return new_f\n\n    return _vartype_arg", "response": "Ensures the wrapped function receives valid vartype argument."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_argument(*arg_names, **options):\n\n    # by default, constrain only one argument, the 'G`\n    if not arg_names:\n        arg_names = ['G']\n\n    # we only allow one option allow_None\n    allow_None = options.pop(\"allow_None\", False)\n    if options:\n        # to keep it consistent with python3\n        # behaviour like graph_argument(*arg_names, allow_None=False)\n        key, _ = options.popitem()\n        msg = \"graph_argument() for an unexpected keyword argument '{}'\".format(key)\n        raise TypeError(msg)\n\n    def _graph_arg(f):\n        argspec = getargspec(f)\n\n        def _enforce_single_arg(name, args, kwargs):\n            try:\n                G = kwargs[name]\n            except KeyError:\n                raise TypeError('Graph argument missing')\n\n            if hasattr(G, 'edges') and hasattr(G, 'nodes'):\n                # networkx or perhaps a named tuple\n                kwargs[name] = (list(G.nodes), list(G.edges))\n\n            elif _is_integer(G):\n                # an integer, cast to a complete graph\n                kwargs[name] = (list(range(G)), list(itertools.combinations(range(G), 2)))\n\n            elif isinstance(G, abc.Sequence) and len(G) == 2:\n                # is a pair nodes/edges\n                if isinstance(G[0], integer_types):\n                    # if nodes is an int\n                    kwargs[name] = (list(range(G[0])), G[1])\n\n            elif allow_None and G is None:\n                # allow None to be passed through\n                return G\n\n            else:\n                raise ValueError('Unexpected graph input form')\n\n            return\n\n        @wraps(f)\n        def new_f(*args, **kwargs):\n            # bound actual f arguments (including defaults) to f argument names\n            # (note: if call arguments don't match actual function signature,\n            # we'll fail here with the standard `TypeError`)\n            bound_args = inspect.getcallargs(f, *args, **kwargs)\n\n            # `getcallargs` doesn't merge additional positional/keyword arguments,\n            # so do it manually\n            final_args = list(bound_args.pop(argspec.varargs, ()))\n            final_kwargs = bound_args.pop(argspec.keywords, {})\n\n            final_kwargs.update(bound_args)\n            for name in arg_names:\n                _enforce_single_arg(name, final_args, final_kwargs)\n\n            return f(*final_args, **final_kwargs)\n\n        return new_f\n\n    return _graph_arg", "response": "Decorator to coerce given graph arguments into a consistent form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array2bytes(arr, bytes_type=bytes):\n    bio = io.BytesIO()\n    np.save(bio, arr, allow_pickle=False)\n    return bytes_type(bio.getvalue())", "response": "Wraps NumPy s save function to return bytes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, bqm, **kwargs):\n        tkw = self._truncate_kwargs\n        if self._aggregate:\n            return self.child.sample(bqm, **kwargs).aggregate().truncate(**tkw)\n        else:\n            return self.child.sample(bqm, **kwargs).truncate(**tkw)", "response": "Sample from the problem provided by bqm and truncate output."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample(self, bqm):\n        M = bqm.binary.to_numpy_matrix()\n        off = bqm.binary.offset\n\n        if M.shape == (0, 0):\n            return SampleSet.from_samples([], bqm.vartype, energy=[])\n\n        sample = np.zeros((len(bqm),), dtype=bool)\n\n        # now we iterate, flipping one bit at a time until we have\n        # traversed all samples. This is a Gray code.\n        # https://en.wikipedia.org/wiki/Gray_code\n        def iter_samples():\n            sample = np.zeros((len(bqm)), dtype=bool)\n            energy = 0.0\n\n            yield sample.copy(), energy + off\n\n            for i in range(1, 1 << len(bqm)):\n                v = _ffs(i)\n\n                # flip the bit in the sample\n                sample[v] = not sample[v]\n\n                # for now just calculate the energy, but there is a more clever way by calculating\n                # the energy delta for the single bit flip, don't have time, pull requests\n                # appreciated!\n                energy = sample.dot(M).dot(sample.transpose())\n\n                yield sample.copy(), float(energy) + off\n\n        samples, energies = zip(*iter_samples())\n\n        response = SampleSet.from_samples(np.array(samples, dtype='int8'), Vartype.BINARY, energies)\n\n        # make sure the response matches the given vartype, in-place.\n        response.change_vartype(bqm.vartype, inplace=True)\n\n        return response", "response": "Sample from a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples from the provided binary quadratic model.", "response": "def sample(self, bqm, sampling_mode=True, **parameters):\n        \"\"\"Sample from the provided binary quadratic model.\n\n        Uses the :func:`~dimod.roof_duality.fix_variables` function to determine\n        which variables to fix.\n\n        Args:\n            bqm (:obj:`dimod.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            sampling_mode (bool, optional, default=True):\n                In sampling mode, only roof-duality is used. When\n                `sampling_mode` is false, strongly connected components are used\n                to fix more variables, but in some optimal solutions these\n                variables may take different values.\n\n            **parameters:\n                Parameters for the child sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n        # use roof-duality to decide which variables to fix\n        parameters['fixed_variables'] = fix_variables(bqm, sampling_mode=sampling_mode)\n        return super(RoofDualityComposite, self).sample(bqm, **parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncast various inputs to a valid vartype object.", "response": "def as_vartype(vartype):\n    \"\"\"Cast various inputs to a valid vartype object.\n\n    Args:\n        vartype (:class:`.Vartype`/str/set):\n            Variable type. Accepted input values:\n\n            * :class:`.Vartype.SPIN`, ``'SPIN'``, ``{-1, 1}``\n            * :class:`.Vartype.BINARY`, ``'BINARY'``, ``{0, 1}``\n\n    Returns:\n        :class:`.Vartype`: Either :class:`.Vartype.SPIN` or\n        :class:`.Vartype.BINARY`.\n\n    See also:\n        :func:`~dimod.decorators.vartype_argument`\n\n    \"\"\"\n    if isinstance(vartype, Vartype):\n        return vartype\n\n    try:\n        if isinstance(vartype, str):\n            vartype = Vartype[vartype]\n        elif isinstance(vartype, frozenset):\n            vartype = Vartype(vartype)\n        else:\n            vartype = Vartype(frozenset(vartype))\n\n    except (ValueError, KeyError):\n        raise TypeError((\"expected input vartype to be one of: \"\n                         \"Vartype.SPIN, 'SPIN', {-1, 1}, \"\n                         \"Vartype.BINARY, 'BINARY', or {0, 1}.\"))\n\n    return vartype"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the energy of the given sample.", "response": "def energy(self, sample_like, dtype=np.float):\n        \"\"\"The energy of the given sample.\n\n        Args:\n            sample_like (samples_like):\n                A raw sample. `sample_like` is an extension of\n                NumPy's array_like structure. See :func:`.as_samples`.\n\n            dtype (:class:`numpy.dtype`, optional):\n                The data type of the returned energies. Defaults to float.\n\n        Returns:\n            The energy.\n\n        \"\"\"\n        energy, = self.energies(sample_like, dtype=dtype)\n        return energy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef energies(self, samples_like, dtype=np.float):\n        samples, labels = as_samples(samples_like)\n        if labels:\n            idx, label = zip(*enumerate(labels))\n            labeldict = dict(zip(label, idx))\n        else:\n            labeldict = {}\n\n        num_samples = samples.shape[0]\n\n        energies = np.zeros(num_samples, dtype=dtype)\n        for term, bias in self.items():\n            if len(term) == 0:\n                energies += bias\n            else:\n                energies += np.prod([samples[:, labeldict[v]] for v in term], axis=0) * bias\n\n        return energies", "response": "Returns the energy of the given samples."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrelabeling the variables of a binary polynomial with the given mapping.", "response": "def relabel_variables(self, mapping, inplace=True):\n        \"\"\"Relabel variables of a binary polynomial as specified by mapping.\n\n        Args:\n            mapping (dict):\n                Dict mapping current variable labels to new ones. If an\n                incomplete mapping is provided, unmapped variables retain their\n                current labels.\n\n            inplace (bool, optional, default=True):\n                If True, the binary polynomial is updated in-place; otherwise, a\n                new binary polynomial is returned.\n\n        Returns:\n            :class:`.BinaryPolynomial`: A binary polynomial with the variables\n            relabeled. If `inplace` is set to True, returns itself.\n\n        \"\"\"\n        if not inplace:\n            return self.copy().relabel_variables(mapping, inplace=True)\n\n        try:\n            old_labels = set(mapping)\n            new_labels = set(mapping.values())\n        except TypeError:\n            raise ValueError(\"mapping targets must be hashable objects\")\n\n        variables = self.variables\n        for v in new_labels:\n            if v in variables and v not in old_labels:\n                raise ValueError(('A variable cannot be relabeled \"{}\" without also relabeling '\n                                  \"the existing variable of the same name\").format(v))\n\n        shared = old_labels & new_labels\n        if shared:\n            old_to_intermediate, intermediate_to_new = resolve_label_conflict(mapping, old_labels, new_labels)\n\n            self.relabel_variables(old_to_intermediate, inplace=True)\n            self.relabel_variables(intermediate_to_new, inplace=True)\n            return self\n\n        for oldterm, bias in list(self.items()):\n            newterm = frozenset((mapping.get(v, v) for v in oldterm))\n\n            if newterm != oldterm:\n                self[newterm] = bias\n                del self[oldterm]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize(self, bias_range=1, poly_range=None, ignored_terms=None):\n\n        def parse_range(r):\n            if isinstance(r, Number):\n                return -abs(r), abs(r)\n            return r\n\n        if ignored_terms is None:\n            ignored_terms = set()\n        else:\n            ignored_terms = {asfrozenset(term) for term in ignored_terms}\n\n        if poly_range is None:\n            linear_range, poly_range = bias_range, bias_range\n        else:\n            linear_range = bias_range\n\n        lin_range, poly_range = map(parse_range, (linear_range, poly_range))\n\n        # determine the current ranges for linear, higherorder\n        lmin = lmax = 0\n        pmin = pmax = 0\n        for term, bias in self.items():\n\n            if term in ignored_terms:\n                # we don't use the ignored terms to calculate the scaling\n                continue\n\n            if len(term) == 1:\n                lmin = min(bias, lmin)\n                lmax = max(bias, lmax)\n            elif len(term) > 1:\n                pmin = min(bias, pmin)\n                pmax = max(bias, pmax)\n\n        inv_scalar = max(lmin / lin_range[0], lmax / lin_range[1],\n                         pmin / poly_range[0], pmax / poly_range[1])\n\n        if inv_scalar != 0:\n            self.scale(1 / inv_scalar, ignored_terms=ignored_terms)", "response": "Normalizes the biases of the binary polynomial such that they fall in the provided range."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scale(self, scalar, ignored_terms=None):\n\n        if ignored_terms is None:\n            ignored_terms = set()\n        else:\n            ignored_terms = {asfrozenset(term) for term in ignored_terms}\n\n        for term in self:\n            if term not in ignored_terms:\n                self[term] *= scalar", "response": "Multiply the polynomial by the given scalar."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a binary polynomial from a higher - order Ising problem.", "response": "def from_hising(cls, h, J, offset=None):\n        \"\"\"Construct a binary polynomial from a higher-order Ising problem.\n\n        Args:\n            h (dict):\n                The linear biases.\n\n            J (dict):\n                The higher-order biases.\n\n            offset (optional, default=0.0):\n                Constant offset applied to the model.\n\n        Returns:\n            :obj:`.BinaryPolynomial`\n\n        Examples:\n            >>> poly = dimod.BinaryPolynomial.from_hising({'a': 2}, {'ab': -1}, 0)\n\n        \"\"\"\n        poly = {(k,): v for k, v in h.items()}\n        poly.update(J)\n        if offset is not None:\n            poly[frozenset([])] = offset\n        return cls(poly, Vartype.SPIN)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a higher - order Ising problem from a binary polynomial.", "response": "def to_hising(self):\n        \"\"\"Construct a higher-order Ising problem from a binary polynomial.\n\n        Returns:\n            tuple: A 3-tuple of the form (`h`, `J`, `offset`) where `h` includes\n            the linear biases, `J` has the higher-order biases and `offset` is\n            the linear offset.\n\n        Examples:\n            >>> poly = dimod.BinaryPolynomial({'a': -1, 'ab': 1, 'abc': -1}, dimod.SPIN)\n            >>> h, J, off = poly.to_hising()\n            >>> h\n            {'a': -1}\n\n        \"\"\"\n        if self.vartype is Vartype.BINARY:\n            return self.to_spin().to_hising()\n\n        h = {}\n        J = {}\n        offset = 0\n        for term, bias in self.items():\n            if len(term) == 0:\n                offset += bias\n            elif len(term) == 1:\n                v, = term\n                h[v] = bias\n            else:\n                J[tuple(term)] = bias\n\n        return h, J, offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a binary polynomial from a higher - order unconstrained binary optimization problem.", "response": "def from_hubo(cls, H, offset=None):\n        \"\"\"Construct a binary polynomial from a higher-order unconstrained\n        binary optimization (HUBO) problem.\n\n        Args:\n            H (dict):\n                Coefficients of a higher-order unconstrained binary optimization\n                (HUBO) model.\n\n        Returns:\n            :obj:`.BinaryPolynomial`\n\n        Examples:\n            >>> poly = dimod.BinaryPolynomial.from_hubo({('a', 'b', 'c'): -1})\n\n        \"\"\"\n        poly = cls(H, Vartype.BINARY)\n        if offset is not None:\n            poly[()] = poly.get((), 0) + offset\n        return poly"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_hubo(self):\n        if self.vartype is Vartype.SPIN:\n            return self.to_binary().to_hubo()\n\n        H = {tuple(term): bias for term, bias in self.items() if term}\n        offset = self[tuple()] if tuple() in self else 0\n        return H, offset", "response": "Construct a higher - order unconstrained binary optimization problem from a binary polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_binary(self, copy=False):\n        if self.vartype is Vartype.BINARY:\n            if copy:\n                return self.copy()\n            else:\n                return self\n\n        new = BinaryPolynomial({}, Vartype.BINARY)\n\n        # s = 2x - 1\n        for term, bias in self.items():\n            for t in map(frozenset, powerset(term)):\n                newbias = bias * 2**len(t) * (-1)**(len(term) - len(t))\n\n                if t in new:\n                    new[t] += newbias\n                else:\n                    new[t] = newbias\n\n        return new", "response": "Return a binary polynomial over the set of variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ising_simulated_annealing(h, J, beta_range=None, num_sweeps=1000):\n\n    if beta_range is None:\n        beta_init = .1\n\n        sigmas = {v: abs(h[v]) for v in h}\n        for u, v in J:\n            sigmas[u] += abs(J[(u, v)])\n            sigmas[v] += abs(J[(u, v)])\n\n        if sigmas:\n            beta_final = 2. * max(itervalues(sigmas))\n        else:\n            beta_final = 0.0\n\n    else:\n        if not isinstance(beta_range, (tuple, list)):\n            raise TypeError(\"'beta_range' should be a tuple of length 2\")\n        if any(not isinstance(b, (int, float)) for b in beta_range):\n            raise TypeError(\"values in 'beta_range' should be numeric\")\n        if any(b <= 0 for b in beta_range):\n            raise ValueError(\"beta values in 'beta_range' should be positive\")\n        if len(beta_range) != 2:\n            raise ValueError(\"'beta_range' should be a tuple of length 2\")\n        beta_init, beta_final = beta_range\n    if not isinstance(num_sweeps, int):\n        raise TypeError(\"'sweeps' should be a positive int\")\n    if num_sweeps <= 0:\n        raise ValueError(\"'sweeps' should be a positive int\")\n\n    # We want the schedule to be linear in beta (inverse temperature)\n    betas = [beta_init + i * (beta_final - beta_init) / (num_sweeps - 1.)\n             for i in range(num_sweeps)]\n\n    # set up the adjacency matrix. We can rely on every node in J already being in h\n    adj = {n: set() for n in h}\n    for n0, n1 in J:\n        adj[n0].add(n1)\n        adj[n1].add(n0)\n\n    # we will use a vertex coloring for the graph and update the nodes by color. A quick\n    # greedy coloring will be sufficient.\n    __, colors = greedy_coloring(adj)\n\n    # let's make our initial guess (randomly)\n    spins = {v: random.choice((-1, 1)) for v in h}\n\n    # there are exactly as many betas as sweeps\n    for beta in betas:\n\n        # we want to know the gain in energy for flipping each of the spins.\n        # We can calculate all of the linear terms simultaneously\n        energy_diff_h = {v: -2 * spins[v] * h[v] for v in h}\n\n        # for each color, do updates\n        for color in colors:\n\n            nodes = colors[color]\n\n            # we now want to know the energy change for flipping the spins within\n            # the color class\n            energy_diff_J = {}\n            for v0 in nodes:\n                ediff = 0\n                for v1 in adj[v0]:\n                    if (v0, v1) in J:\n                        ediff += spins[v0] * spins[v1] * J[(v0, v1)]\n                    if (v1, v0) in J:\n                        ediff += spins[v0] * spins[v1] * J[(v1, v0)]\n\n                energy_diff_J[v0] = -2. * ediff\n\n            # now decide whether to flip spins according to the\n            # following scheme:\n            #   p ~ Uniform(0, 1)\n            #   log(p) < -beta * (energy_diff)\n            for v in nodes:\n                logp = math.log(random.uniform(0, 1))\n                if logp < -1. * beta * (energy_diff_h[v] + energy_diff_J[v]):\n                    # flip the variable in the spins\n                    spins[v] *= -1\n\n    return spins, ising_energy(spins, h, J)", "response": "Tries to find the spins that minimize the given Ising problem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining a vertex coloring.", "response": "def greedy_coloring(adj):\n    \"\"\"Determines a vertex coloring.\n\n    Args:\n        adj (dict): The edge structure of the graph to be colored.\n            `adj` should be of the form {node: neighbors, ...} where\n            neighbors is a set.\n\n    Returns:\n        dict: the coloring {node: color, ...}\n        dict: the colors {color: [node, ...], ...}\n\n    Note:\n        This is a greedy heuristic: the resulting coloring is not\n        necessarily minimal.\n\n    \"\"\"\n\n    # now let's start coloring\n    coloring = {}\n    colors = {}\n    possible_colors = {n: set(range(len(adj))) for n in adj}\n    while possible_colors:\n\n        # get the n with the fewest possible colors\n        n = min(possible_colors, key=lambda n: len(possible_colors[n]))\n\n        # assign that node the lowest color it can still have\n        color = min(possible_colors[n])\n        coloring[n] = color\n        if color not in colors:\n            colors[color] = {n}\n        else:\n            colors[color].add(n)\n\n        # also remove color from the possible colors for n's neighbors\n        for neighbor in adj[n]:\n            if neighbor in possible_colors and color in possible_colors[neighbor]:\n                possible_colors[neighbor].remove(color)\n\n        # finally remove n from nodes\n        del possible_colors[n]\n\n    return coloring, colors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsampling from an annealing and return a SampleSet object.", "response": "def sample(self, bqm, beta_range=None, num_reads=10, num_sweeps=1000):\n        \"\"\"Sample from low-energy spin states using simulated annealing.\n\n        Args:\n            bqm (:obj:`.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            beta_range (tuple, optional): Beginning and end of the beta schedule\n                (beta is the inverse temperature) as a 2-tuple. The schedule is applied\n                linearly in beta. Default is chosen based on the total bias associated\n                with each node.\n\n            num_reads (int, optional, default=10):\n                Number of reads. Each sample is the result of a single run of\n                the simulated annealing algorithm.\n\n            num_sweeps (int, optional, default=1000):\n                Number of sweeps or steps.\n\n        Returns:\n            :obj:`.SampleSet`\n\n        Note:\n            This is a reference implementation, not optimized for speed\n            and therefore not an appropriate sampler for benchmarking.\n\n        \"\"\"\n\n        # input checking\n        # h, J are handled by the @ising decorator\n        # beta_range, sweeps are handled by ising_simulated_annealing\n        if not isinstance(num_reads, int):\n            raise TypeError(\"'samples' should be a positive integer\")\n        if num_reads < 1:\n            raise ValueError(\"'samples' should be a positive integer\")\n\n        h, J, offset = bqm.to_ising()\n\n        # run the simulated annealing algorithm\n        samples = []\n        energies = []\n        for __ in range(num_reads):\n            sample, energy = ising_simulated_annealing(h, J, beta_range, num_sweeps)\n            samples.append(sample)\n            energies.append(energy)\n\n        response = SampleSet.from_samples(samples, Vartype.SPIN, energies)\n        response.change_vartype(bqm.vartype, offset, inplace=True)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscaling and samples from the provided binary quadratic model.", "response": "def sample(self, bqm, scalar=None, bias_range=1, quadratic_range=None,\n               ignored_variables=None, ignored_interactions=None,\n               ignore_offset=False, **parameters):\n        \"\"\" Scale and sample from the provided binary quadratic model.\n\n        if scalar is not given, problem is scaled based on bias and quadratic\n        ranges. See :meth:`.BinaryQuadraticModel.scale` and\n        :meth:`.BinaryQuadraticModel.normalize`\n\n        Args:\n            bqm (:obj:`dimod.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            scalar (number):\n                Value by which to scale the energy range of the binary quadratic model.\n\n            bias_range (number/pair):\n                Value/range by which to normalize the all the biases, or if\n                `quadratic_range` is provided, just the linear biases.\n\n            quadratic_range (number/pair):\n                Value/range by which to normalize the quadratic biases.\n\n            ignored_variables (iterable, optional):\n                Biases associated with these variables are not scaled.\n\n            ignored_interactions (iterable[tuple], optional):\n                As an iterable of 2-tuples. Biases associated with these interactions are not scaled.\n\n            ignore_offset (bool, default=False):\n                If True, the offset is not scaled.\n\n            **parameters:\n                Parameters for the sampling method, specified by the child sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n        ignored_variables, ignored_interactions = _check_params(\n            ignored_variables, ignored_interactions)\n\n        child = self.child\n        bqm_copy = _scaled_bqm(bqm, scalar, bias_range, quadratic_range,\n                               ignored_variables, ignored_interactions,\n                               ignore_offset)\n        response = child.sample(bqm_copy, **parameters)\n\n        return _scale_back_response(bqm, response, bqm_copy.info['scalar'],\n                                    ignored_variables, ignored_interactions,\n                                    ignore_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample_ising(self, h, J, offset=0, scalar=None,\n                     bias_range=1, quadratic_range=None,\n                     ignored_variables=None, ignored_interactions=None,\n                     ignore_offset=False, **parameters):\n        \"\"\" Scale and sample from the problem provided by h, J, offset\n\n        if scalar is not given, problem is scaled based on bias and quadratic\n        ranges.\n\n        Args:\n            h (dict): linear biases\n\n            J (dict): quadratic or higher order biases\n\n            offset (float, optional): constant energy offset\n\n            scalar (number):\n                Value by which to scale the energy range of the binary quadratic model.\n\n            bias_range (number/pair):\n                Value/range by which to normalize the all the biases, or if\n                `quadratic_range` is provided, just the linear biases.\n\n            quadratic_range (number/pair):\n                Value/range by which to normalize the quadratic biases.\n\n            ignored_variables (iterable, optional):\n                Biases associated with these variables are not scaled.\n\n            ignored_interactions (iterable[tuple], optional):\n                As an iterable of 2-tuples. Biases associated with these interactions are not scaled.\n\n            ignore_offset (bool, default=False):\n                If True, the offset is not scaled.\n\n            **parameters:\n                Parameters for the sampling method, specified by the child sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n\n        if any(len(inter) > 2 for inter in J):\n            # handle HUBO\n            import warnings\n            msg = (\"Support for higher order Ising models in ScaleComposite is \"\n                   \"deprecated and will be removed in dimod 0.9.0. Please use \"\n                   \"PolyScaleComposite.sample_hising instead.\")\n            warnings.warn(msg, DeprecationWarning)\n\n            from dimod.reference.composites.higherordercomposites import PolyScaleComposite\n            from dimod.higherorder.polynomial import BinaryPolynomial\n\n            poly = BinaryPolynomial.from_hising(h, J, offset=offset)\n\n            ignored_terms = set()\n            if ignored_variables is not None:\n                ignored_terms.update(frozenset(v) for v in ignored_variables)\n            if ignored_interactions is not None:\n                ignored_terms.update(frozenset(inter) for inter in ignored_interactions)\n            if ignore_offset:\n                ignored_terms.add(frozenset())\n\n            return PolyScaleComposite(self.child).sample_poly(poly, scalar=scalar,\n                                                              bias_range=bias_range,\n                                                              poly_range=quadratic_range,\n                                                              ignored_terms=ignored_terms,\n                                                              **parameters)\n\n        bqm = BinaryQuadraticModel.from_ising(h, J, offset=offset)\n        return self.sample(bqm, scalar=scalar,\n                           bias_range=bias_range,\n                           quadratic_range=quadratic_range,\n                           ignored_variables=ignored_variables,\n                           ignored_interactions=ignored_interactions,\n                           ignore_offset=ignore_offset, **parameters)", "response": "Scale and sample from the problem provided by h J and offset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an anticluster problem on a Chimera lattice.", "response": "def chimera_anticluster(m, n=None, t=4, multiplier=3.0,\n                        cls=BinaryQuadraticModel, subgraph=None, seed=None):\n    \"\"\"Generate an anticluster problem on a Chimera lattice.\n\n    An anticluster problem has weak interactions within a tile and strong\n    interactions between tiles.\n\n    Args:\n        m (int):\n            Number of rows in the Chimera lattice.\n\n        n (int, optional, default=m):\n            Number of columns in the Chimera lattice.\n\n        t (int, optional, default=t):\n            Size of the shore within each Chimera tile.\n\n        multiplier (number, optional, default=3.0):\n            Strength of the intertile edges.\n\n        cls (class, optional, default=:class:`.BinaryQuadraticModel`):\n            Binary quadratic model class to build from.\n\n        subgraph (int/tuple[nodes, edges]/:obj:`~networkx.Graph`):\n            A subgraph of a Chimera(m, n, t) graph to build the anticluster\n            problem on.\n\n        seed (int, optional, default=None):\n            Random seed.\n\n    Returns:\n        :obj:`.BinaryQuadraticModel`: spin-valued binary quadratic model.\n\n    \"\"\"\n    if seed is None:\n        seed = numpy.random.randint(2**32, dtype=np.uint32)\n    r = numpy.random.RandomState(seed)\n\n    m = int(m)\n    if n is None:\n        n = m\n    else:\n        n = int(n)\n    t = int(t)\n\n    ldata = np.zeros(m*n*t*2)  # number of nodes\n\n    if m and n and t:\n        inrow, incol = zip(*_iter_chimera_tile_edges(m, n, t))\n\n        if m > 1 or n > 1:\n            outrow, outcol = zip(*_iter_chimera_intertile_edges(m, n, t))\n        else:\n            outrow = outcol = tuple()\n\n        qdata = r.choice((-1., 1.), size=len(inrow)+len(outrow))\n\n        qdata[len(inrow):] *= multiplier\n\n        irow = inrow + outrow\n        icol = incol + outcol\n\n    else:\n        irow = icol = qdata = tuple()\n\n    bqm = cls.from_numpy_vectors(ldata, (irow, icol, qdata), 0.0, SPIN)\n\n    if subgraph is not None:\n        nodes, edges = subgraph\n\n        subbqm = cls.empty(SPIN)\n\n        try:\n            subbqm.add_variables_from((v, bqm.linear[v]) for v in nodes)\n\n        except KeyError:\n            msg = \"given 'subgraph' contains nodes not in Chimera({}, {}, {})\".format(m, n, t)\n            raise ValueError(msg)\n\n        try:\n            subbqm.add_interactions_from((u, v, bqm.adj[u][v]) for u, v in edges)\n        except KeyError:\n            msg = \"given 'subgraph' contains edges not in Chimera({}, {}, {})\".format(m, n, t)\n            raise ValueError(msg)\n\n        bqm = subbqm\n\n    return bqm"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndumps a binary quadratic model to a string in COOrdinate format.", "response": "def dump(bqm, fp, vartype_header=False):\n    \"\"\"Dump a binary quadratic model to a string in COOrdinate format.\"\"\"\n    for triplet in _iter_triplets(bqm, vartype_header):\n        fp.write('%s\\n' % triplet)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loads(s, cls=BinaryQuadraticModel, vartype=None):\n    return load(s.split('\\n'), cls=cls, vartype=vartype)", "response": "Load a COOrdinate formatted binary quadratic model from a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a COOrdinate formatted binary quadratic model from a file.", "response": "def load(fp, cls=BinaryQuadraticModel, vartype=None):\n    \"\"\"Load a COOrdinate formatted binary quadratic model from a file.\"\"\"\n    pattern = re.compile(_LINE_REGEX)\n    vartype_pattern = re.compile(_VARTYPE_HEADER_REGEX)\n\n    triplets = []\n    for line in fp:\n        triplets.extend(pattern.findall(line))\n\n        vt = vartype_pattern.findall(line)\n        if vt:\n            if vartype is None:\n                vartype = vt[0]\n            else:\n                if isinstance(vartype, str):\n                    vartype = Vartype[vartype]\n                else:\n                    vartype = Vartype(vartype)\n                if Vartype[vt[0]] != vartype:\n                    raise ValueError(\"vartypes from headers and/or inputs do not match\")\n\n    if vartype is None:\n        raise ValueError(\"vartype must be provided either as a header or as an argument\")\n\n    bqm = cls.empty(vartype)\n\n    for u, v, bias in triplets:\n        if u == v:\n            bqm.add_variable(int(u), float(bias))\n        else:\n            bqm.add_interaction(int(u), int(v), float(bias))\n\n    return bqm"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spin(self):\n        # NB: The existence of the _spin property implies that it is up to date, methods that\n        # invalidate it will erase the property\n        try:\n            spin = self._spin\n            if spin is not None:\n                return spin\n        except AttributeError:\n            pass\n\n        if self.vartype is Vartype.SPIN:\n            self._spin = spin = self\n        else:\n            self._counterpart = self._spin = spin = self.change_vartype(Vartype.SPIN, inplace=False)\n\n            # we also want to go ahead and set spin.binary to refer back to self\n            spin._binary = self\n\n        return spin", "response": "Return an instance of the Ising model class corresponding to the current object s spin property."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef binary(self):\n        # NB: The existence of the _binary property implies that it is up to date, methods that\n        # invalidate it will erase the property\n        try:\n            binary = self._binary\n            if binary is not None:\n                return binary\n        except AttributeError:\n            pass\n\n        if self.vartype is Vartype.BINARY:\n            self._binary = binary = self\n        else:\n            self._counterpart = self._binary = binary = self.change_vartype(Vartype.BINARY, inplace=False)\n\n            # we also want to go ahead and set binary.spin to refer back to self\n            binary._spin = self\n\n        return binary", "response": "A property that is used to instantiate the binary quadratic model for the given base model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_variable(self, v, bias, vartype=None):\n\n        # handle the case that a different vartype is provided\n        if vartype is not None and vartype is not self.vartype:\n            if self.vartype is Vartype.SPIN and vartype is Vartype.BINARY:\n                # convert from binary to spin\n                bias /= 2\n                self.offset += bias\n            elif self.vartype is Vartype.BINARY and vartype is Vartype.SPIN:\n                # convert from spin to binary\n                self.offset -= bias\n                bias *= 2\n            else:\n                raise ValueError(\"unknown vartype\")\n\n        # we used to do this using self.linear but working directly with _adj\n        # is much faster\n        _adj = self._adj\n        if v in _adj:\n            if v in _adj[v]:\n                _adj[v][v] += bias\n            else:\n                _adj[v][v] = bias\n        else:\n            _adj[v] = {v: bias}\n\n        try:\n            self._counterpart.add_variable(v, bias, vartype=self.vartype)\n        except AttributeError:\n            pass", "response": "Add a variable v and its bias to a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds variables and or biases to the binary quadratic model.", "response": "def add_variables_from(self, linear, vartype=None):\n        \"\"\"Add variables and/or linear biases to a binary quadratic model.\n\n        Args:\n            linear (dict[variable, bias]/iterable[(variable, bias)]):\n                A collection of variables and their linear biases to add to the model.\n                If a dict, keys are variables in the binary quadratic model and\n                values are biases. Alternatively, an iterable of (variable, bias) pairs.\n                Variables can be any python object that is a valid dict key.\n                Many methods and functions expect the biases\n                to be numbers but this is not explicitly checked.\n                If any variable already exists in the model, its bias is added to\n                the variable's current linear bias.\n\n            vartype (:class:`.Vartype`, optional, default=None):\n                Vartype of the given bias. If None, the vartype of the binary\n                quadratic model is used. Valid values are :class:`.Vartype.SPIN` or\n                :class:`.Vartype.BINARY`.\n\n        Examples:\n            This example creates creates an empty Ising model, adds two variables,\n            and subsequently adds to the bias of the one while adding a new, third,\n            variable.\n\n            >>> import dimod\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({}, {}, 0.0, dimod.SPIN)\n            >>> len(bqm.linear)\n            0\n            >>> bqm.add_variables_from({'a': .5, 'b': -1.})\n            >>> 'b' in bqm\n            True\n            >>> bqm.add_variables_from({'b': -1., 'c': 2.0})\n            >>> bqm.linear['b']\n            -2.0\n\n        \"\"\"\n        if isinstance(linear, abc.Mapping):\n            for v, bias in iteritems(linear):\n                self.add_variable(v, bias, vartype=vartype)\n        else:\n            try:\n                for v, bias in linear:\n                    self.add_variable(v, bias, vartype=vartype)\n            except TypeError:\n                raise TypeError(\"expected 'linear' to be a dict or an iterable of 2-tuples.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_interaction(self, u, v, bias, vartype=None):\n        if u == v:\n            raise ValueError(\"no self-loops allowed, therefore ({}, {}) is not an allowed interaction\".format(u, v))\n\n        _adj = self._adj\n\n        if vartype is not None and vartype is not self.vartype:\n            if self.vartype is Vartype.SPIN and vartype is Vartype.BINARY:\n                # convert from binary to spin\n                bias /= 4\n\n                self.add_offset(bias)\n                self.add_variable(u, bias)\n                self.add_variable(v, bias)\n\n            elif self.vartype is Vartype.BINARY and vartype is Vartype.SPIN:\n                # convert from spin to binary\n\n                self.add_offset(bias)\n                self.add_variable(u, -2 * bias)\n                self.add_variable(v, -2 * bias)\n\n                bias *= 4\n            else:\n                raise ValueError(\"unknown vartype\")\n        else:\n            # so that they exist.\n            if u not in self:\n                _adj[u] = {}\n            if v not in self:\n                _adj[v] = {}\n\n        if u in _adj[v]:\n            _adj[u][v] = _adj[v][u] = _adj[u][v] + bias\n        else:\n            _adj[u][v] = _adj[v][u] = bias\n\n        try:\n            self._counterpart.add_interaction(u, v, bias, vartype=self.vartype)\n        except AttributeError:\n            pass", "response": "Add an interaction and or quadratic bias to a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_interactions_from(self, quadratic, vartype=None):\n        if isinstance(quadratic, abc.Mapping):\n            for (u, v), bias in iteritems(quadratic):\n                self.add_interaction(u, v, bias, vartype=vartype)\n        else:\n            try:\n                for u, v, bias in quadratic:\n                    self.add_interaction(u, v, bias, vartype=vartype)\n            except TypeError:\n                raise TypeError(\"expected 'quadratic' to be a dict or an iterable of 3-tuples.\")", "response": "Add interaction and / or quadratic biases to a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_variable(self, v):\n        if v not in self:\n            return\n\n        adj = self.adj\n\n        # first remove all the interactions associated with v\n        while adj[v]:\n            self.remove_interaction(v, next(iter(adj[v])))\n\n        # remove the variable\n        del self.linear[v]\n\n        try:\n            # invalidates counterpart\n            del self._counterpart\n            if self.vartype is not Vartype.BINARY and hasattr(self, '_binary'):\n                del self._binary\n            elif self.vartype is not Vartype.SPIN and hasattr(self, '_spin'):\n                del self._spin\n        except AttributeError:\n            pass", "response": "Removes a variable v and all its interactions from a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an interaction of variables u v from a binary quadratic model.", "response": "def remove_interaction(self, u, v):\n        \"\"\"Remove interaction of variables u, v from a binary quadratic model.\n\n        Args:\n            u (variable):\n                One of the pair of variables in the binary quadratic model that\n                has an interaction.\n\n            v (variable):\n                One of the pair of variables in the binary quadratic model that\n                has an interaction.\n\n        Notes:\n            Any interaction not in the binary quadratic model is ignored.\n\n        Examples:\n            This example creates an Ising model with three variables that has interactions\n            between two, and then removes an interaction.\n\n            >>> import dimod\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({}, {('a', 'b'): -1.0, ('b', 'c'): 1.0}, 0.0, dimod.SPIN)\n            >>> bqm.remove_interaction('b', 'c')\n            >>> ('b', 'c') in bqm.quadratic\n            False\n            >>> bqm.remove_interaction('a', 'c')  # not an interaction, so ignored\n            >>> len(bqm.quadratic)\n            1\n\n        \"\"\"\n\n        try:\n            del self.quadratic[(u, v)]\n        except KeyError:\n            return  # no interaction with that name\n\n        try:\n            # invalidates counterpart\n            del self._counterpart\n            if self.vartype is not Vartype.BINARY and hasattr(self, '_binary'):\n                del self._binary\n            elif self.vartype is not Vartype.SPIN and hasattr(self, '_spin'):\n                del self._spin\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all specified interactions from the binary quadratic model.", "response": "def remove_interactions_from(self, interactions):\n        \"\"\"Remove all specified interactions from the binary quadratic model.\n\n        Args:\n            interactions (iterable[[variable, variable]]):\n                A collection of interactions. Each interaction should be a 2-tuple of variables\n                in the binary quadratic model.\n\n        Notes:\n            Any interaction not in the binary quadratic model is ignored.\n\n        Examples:\n            This example creates an Ising model with three variables that has interactions\n            between two, and then removes an interaction.\n\n            >>> import dimod\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({}, {('a', 'b'): -1.0, ('b', 'c'): 1.0}, 0.0, dimod.SPIN)\n            >>> bqm.remove_interactions_from([('b', 'c'), ('a', 'c')])  # ('a', 'c') is not an interaction, so ignored\n            >>> len(bqm.quadratic)\n            1\n\n        \"\"\"\n        for u, v in interactions:\n            self.remove_interaction(u, v)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_offset(self, offset):\n        self.offset += offset\n\n        try:\n            self._counterpart.add_offset(offset)\n        except AttributeError:\n            pass", "response": "Add specified value to the constant energy offset of a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmultiplies the energy range of a binary quadratic model.", "response": "def scale(self, scalar, ignored_variables=None, ignored_interactions=None,\n              ignore_offset=False):\n        \"\"\"Multiply by the specified scalar all the biases and offset of a binary quadratic model.\n\n        Args:\n            scalar (number):\n                Value by which to scale the energy range of the binary quadratic model.\n\n            ignored_variables (iterable, optional):\n                Biases associated with these variables are not scaled.\n\n            ignored_interactions (iterable[tuple], optional):\n                As an iterable of 2-tuples. Biases associated with these interactions are not scaled.\n\n            ignore_offset (bool, default=False):\n                If True, the offset is not scaled.\n\n        Examples:\n\n            This example creates a binary quadratic model and then scales it to half\n            the original energy range.\n\n            >>> import dimod\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({'a': -2.0, 'b': 2.0}, {('a', 'b'): -1.0}, 1.0, dimod.SPIN)\n            >>> bqm.scale(0.5)\n            >>> bqm.linear['a']\n            -1.0\n            >>> bqm.quadratic[('a', 'b')]\n            -0.5\n            >>> bqm.offset\n            0.5\n\n        \"\"\"\n\n        if ignored_variables is None:\n            ignored_variables = set()\n        elif not isinstance(ignored_variables, abc.Container):\n            ignored_variables = set(ignored_variables)\n\n        if ignored_interactions is None:\n            ignored_interactions = set()\n        elif not isinstance(ignored_interactions, abc.Container):\n            ignored_interactions = set(ignored_interactions)\n\n        linear = self.linear\n        for v in linear:\n            if v in ignored_variables:\n                continue\n            linear[v] *= scalar\n\n        quadratic = self.quadratic\n        for u, v in quadratic:\n            if (u, v) in ignored_interactions or (v, u) in ignored_interactions:\n                continue\n            quadratic[(u, v)] *= scalar\n\n        if not ignore_offset:\n            self.offset *= scalar\n\n        try:\n            self._counterpart.scale(scalar, ignored_variables=ignored_variables,\n                                    ignored_interactions=ignored_interactions)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes the biases of the binary quadratic model such that they fall in the provided range and adjusts the offset appropriately.", "response": "def normalize(self, bias_range=1, quadratic_range=None,\n                  ignored_variables=None, ignored_interactions=None,\n                  ignore_offset=False):\n        \"\"\"Normalizes the biases of the binary quadratic model such that they\n        fall in the provided range(s), and adjusts the offset appropriately.\n\n        If `quadratic_range` is provided, then `bias_range` will be treated as\n        the range for the linear biases and `quadratic_range` will be used for\n        the range of the quadratic biases.\n\n        Args:\n            bias_range (number/pair):\n                Value/range by which to normalize the all the biases, or if\n                `quadratic_range` is provided, just the linear biases.\n\n            quadratic_range (number/pair):\n                Value/range by which to normalize the quadratic biases.\n\n            ignored_variables (iterable, optional):\n                Biases associated with these variables are not scaled.\n\n            ignored_interactions (iterable[tuple], optional):\n                As an iterable of 2-tuples. Biases associated with these interactions are not scaled.\n\n            ignore_offset (bool, default=False):\n                If True, the offset is not scaled.\n\n        Examples:\n\n            >>> bqm = dimod.BinaryQuadraticModel({'a': -2.0, 'b': 1.5},\n            ...                                  {('a', 'b'): -1.0},\n            ...                                  1.0, dimod.SPIN)\n            >>> max(abs(bias) for bias in bqm.linear.values())\n            2.0\n            >>> max(abs(bias) for bias in bqm.quadratic.values())\n            1.0\n            >>> bqm.normalize([-1.0, 1.0])\n            >>> max(abs(bias) for bias in bqm.linear.values())\n            1.0\n            >>> max(abs(bias) for bias in bqm.quadratic.values())\n            0.5\n\n        \"\"\"\n\n        def parse_range(r):\n            if isinstance(r, Number):\n                return -abs(r), abs(r)\n            return r\n\n        def min_and_max(iterable):\n            if not iterable:\n                return 0, 0\n            return min(iterable), max(iterable)\n\n        if ignored_variables is None:\n            ignored_variables = set()\n        elif not isinstance(ignored_variables, abc.Container):\n            ignored_variables = set(ignored_variables)\n\n        if ignored_interactions is None:\n            ignored_interactions = set()\n        elif not isinstance(ignored_interactions, abc.Container):\n            ignored_interactions = set(ignored_interactions)\n\n        if quadratic_range is None:\n            linear_range, quadratic_range = bias_range, bias_range\n        else:\n            linear_range = bias_range\n\n        lin_range, quad_range = map(parse_range, (linear_range,\n                                                  quadratic_range))\n\n        lin_min, lin_max = min_and_max([v for k, v in self.linear.items()\n                                        if k not in ignored_variables])\n        quad_min, quad_max = min_and_max([v for (a, b), v in self.quadratic.items()\n                                          if ((a, b) not in ignored_interactions\n                                              and (b, a) not in\n                                              ignored_interactions)])\n\n        inv_scalar = max(lin_min / lin_range[0], lin_max / lin_range[1],\n                         quad_min / quad_range[0], quad_max / quad_range[1])\n\n        if inv_scalar != 0:\n            self.scale(1 / inv_scalar, ignored_variables=ignored_variables,\n                       ignored_interactions=ignored_interactions,\n                       ignore_offset=ignore_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fix_variable(self, v, value):\n        adj = self.adj\n        linear = self.linear\n\n        if value not in self.vartype.value:\n            raise ValueError(\"expected value to be in {}, received {} instead\".format(self.vartype.value, value))\n\n        removed_interactions = []\n        for u in adj[v]:\n            self.add_variable(u, value * adj[v][u])\n            removed_interactions.append((u, v))\n        self.remove_interactions_from(removed_interactions)\n\n        self.add_offset(value * linear[v])\n        self.remove_variable(v)", "response": "Fix the value of a variable and remove it from a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fix_variables(self, fixed):\n        for v, val in fixed.items():\n            self.fix_variable(v, val)", "response": "Fix the value of the variables and remove it from a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nflipping a variable in a binary quadratic model.", "response": "def flip_variable(self, v):\n        \"\"\"Flip variable v in a binary quadratic model.\n\n        Args:\n            v (variable):\n                Variable in the binary quadratic model. If v is not in the binary\n                quadratic model, it is ignored.\n\n        Examples:\n            This example creates a binary quadratic model with two variables and inverts\n            the value of one.\n\n            >>> import dimod\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({1: 1, 2: 2}, {(1, 2): 0.5}, 0.5, dimod.SPIN)\n            >>> bqm.flip_variable(1)\n            >>> bqm.linear[1], bqm.linear[2], bqm.quadratic[(1, 2)]\n            (-1.0, 2, -0.5)\n\n        \"\"\"\n        adj = self.adj\n        linear = self.linear\n        quadratic = self.quadratic\n\n        if v not in adj:\n            return\n\n        if self.vartype is Vartype.SPIN:\n            # in this case we just multiply by -1\n            linear[v] *= -1.\n            for u in adj[v]:\n                adj[v][u] *= -1.\n                adj[u][v] *= -1.\n\n                if (u, v) in quadratic:\n                    quadratic[(u, v)] *= -1.\n                elif (v, u) in quadratic:\n                    quadratic[(v, u)] *= -1.\n                else:\n                    raise RuntimeError(\"quadratic is missing an interaction\")\n\n        elif self.vartype is Vartype.BINARY:\n            self.offset += linear[v]\n            linear[v] *= -1\n\n            for u in adj[v]:\n                bias = adj[v][u]\n\n                adj[v][u] *= -1.\n                adj[u][v] *= -1.\n\n                linear[u] += bias\n\n                if (u, v) in quadratic:\n                    quadratic[(u, v)] *= -1.\n                elif (v, u) in quadratic:\n                    quadratic[(v, u)] *= -1.\n                else:\n                    raise RuntimeError(\"quadratic is missing an interaction\")\n\n        else:\n            raise RuntimeError(\"Unexpected vartype\")\n\n        try:\n            self._counterpart.flip_variable(v)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, bqm, ignore_info=True):\n        self.add_variables_from(bqm.linear, vartype=bqm.vartype)\n        self.add_interactions_from(bqm.quadratic, vartype=bqm.vartype)\n        self.add_offset(bqm.offset)\n\n        if not ignore_info:\n            self.info.update(bqm.info)", "response": "Update one binary quadratic model from another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenforce u v being the same variable in a binary quadratic model.", "response": "def contract_variables(self, u, v):\n        \"\"\"Enforce u, v being the same variable in a binary quadratic model.\n\n        The resulting variable is labeled 'u'. Values of interactions between `v` and\n        variables that `u` interacts with are added to the corresponding interactions\n        of `u`.\n\n        Args:\n            u (variable):\n                Variable in the binary quadratic model.\n\n            v (variable):\n                Variable in the binary quadratic model.\n\n        Examples:\n           This example creates a binary quadratic model representing the K4 complete graph\n           and contracts node (variable) 3 into node 2. The interactions between\n           3 and its neighbors 1 and 4 are added to the corresponding interactions\n           between 2 and those same neighbors.\n\n           >>> import dimod\n           ...\n           >>> linear = {1: 1, 2: 2, 3: 3, 4: 4}\n           >>> quadratic = {(1, 2): 12, (1, 3): 13, (1, 4): 14,\n           ...              (2, 3): 23, (2, 4): 24,\n           ...              (3, 4): 34}\n           >>> bqm = dimod.BinaryQuadraticModel(linear, quadratic, 0.5, dimod.SPIN)\n           >>> bqm.contract_variables(2, 3)\n           >>> 3 in bqm.linear\n           False\n           >>> bqm.quadratic[(1, 2)]\n           25\n\n        \"\"\"\n        adj = self.adj\n\n        if u not in adj:\n            raise ValueError(\"{} is not a variable in the binary quadratic model\".format(u))\n        if v not in adj:\n            raise ValueError(\"{} is not a variable in the binary quadratic model\".format(v))\n\n        # if there is an interaction between u, v it becomes linear for u\n        if v in adj[u]:\n            if self.vartype is Vartype.BINARY:\n                self.add_variable(u, adj[u][v])\n            elif self.vartype is Vartype.SPIN:\n                self.add_offset(adj[u][v])\n            else:\n                raise RuntimeError(\"unexpected vartype\")\n            self.remove_interaction(u, v)\n\n        # all of the interactions that v has become interactions for u\n        neighbors = list(adj[v])\n        for w in neighbors:\n            self.add_interaction(u, w, adj[v][w])\n            self.remove_interaction(v, w)\n\n        # finally remove v\n        self.remove_variable(v)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrelabel the variables of a binary quadratic model as specified by mapping.", "response": "def relabel_variables(self, mapping, inplace=True):\n        \"\"\"Relabel variables of a binary quadratic model as specified by mapping.\n\n        Args:\n            mapping (dict):\n                Dict mapping current variable labels to new ones. If an incomplete mapping is\n                provided, unmapped variables retain their current labels.\n\n            inplace (bool, optional, default=True):\n                If True, the binary quadratic model is updated in-place; otherwise, a new binary\n                quadratic model is returned.\n\n        Returns:\n            :class:`.BinaryQuadraticModel`: A binary quadratic model\n            with the variables relabeled. If `inplace` is set to True, returns\n            itself.\n\n        Examples:\n            This example creates a binary quadratic model with two variables and relables one.\n\n            >>> import dimod\n            ...\n            >>> model = dimod.BinaryQuadraticModel({0: 0., 1: 1.}, {(0, 1): -1}, 0.0, vartype=dimod.SPIN)\n            >>> model.relabel_variables({0: 'a'})   # doctest: +SKIP\n            BinaryQuadraticModel({1: 1.0, 'a': 0.0}, {('a', 1): -1}, 0.0, Vartype.SPIN)\n\n            This example creates a binary quadratic model with two variables and returns a new\n            model with relabled variables.\n\n            >>> import dimod\n            ...\n            >>> model = dimod.BinaryQuadraticModel({0: 0., 1: 1.}, {(0, 1): -1}, 0.0, vartype=dimod.SPIN)\n            >>> new_model = model.relabel_variables({0: 'a', 1: 'b'}, inplace=False)  # doctest: +SKIP\n            >>> new_model.quadratic       # doctest: +SKIP\n            {('a', 'b'): -1}\n\n        \"\"\"\n        try:\n            old_labels = set(mapping)\n            new_labels = set(itervalues(mapping))\n        except TypeError:\n            raise ValueError(\"mapping targets must be hashable objects\")\n\n        for v in new_labels:\n            if v in self.linear and v not in old_labels:\n                raise ValueError(('A variable cannot be relabeled \"{}\" without also relabeling '\n                                  \"the existing variable of the same name\").format(v))\n\n        if inplace:\n            shared = old_labels & new_labels\n            if shared:\n                old_to_intermediate, intermediate_to_new = resolve_label_conflict(mapping, old_labels, new_labels)\n\n                self.relabel_variables(old_to_intermediate, inplace=True)\n                self.relabel_variables(intermediate_to_new, inplace=True)\n                return self\n\n            linear = self.linear\n            quadratic = self.quadratic\n            adj = self.adj\n\n            # rebuild linear and adj with the new labels\n            for old in list(linear):\n                if old not in mapping:\n                    continue\n\n                new = mapping[old]\n\n                # get the new interactions that need to be added\n                new_interactions = [(new, v, adj[old][v]) for v in adj[old]]\n\n                self.add_variable(new, linear[old])\n                self.add_interactions_from(new_interactions)\n                self.remove_variable(old)\n\n            return self\n        else:\n            return BinaryQuadraticModel({mapping.get(v, v): bias for v, bias in iteritems(self.linear)},\n                                        {(mapping.get(u, u), mapping.get(v, v)): bias\n                                         for (u, v), bias in iteritems(self.quadratic)},\n                                        self.offset, self.vartype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a binary quadratic model with the specified vartype.", "response": "def change_vartype(self, vartype, inplace=True):\n        \"\"\"Create a binary quadratic model with the specified vartype.\n\n        Args:\n            vartype (:class:`.Vartype`/str/set, optional):\n                Variable type for the changed model. Accepted input values:\n\n                * :class:`.Vartype.SPIN`, ``'SPIN'``, ``{-1, 1}``\n                * :class:`.Vartype.BINARY`, ``'BINARY'``, ``{0, 1}``\n\n            inplace (bool, optional, default=True):\n                If True, the binary quadratic model is updated in-place; otherwise, a new binary\n                quadratic model is returned.\n\n        Returns:\n            :class:`.BinaryQuadraticModel`. A new binary quadratic model with\n            vartype matching input 'vartype'.\n\n        Examples:\n            This example creates an Ising model and then creates a QUBO from it.\n\n            >>> import dimod\n            ...\n            >>> bqm_spin = dimod.BinaryQuadraticModel({1: 1, 2: 2}, {(1, 2): 0.5}, 0.5, dimod.SPIN)\n            >>> bqm_qubo = bqm_spin.change_vartype('BINARY', inplace=False)\n            >>> bqm_spin.offset, bqm_spin.vartype\n            (0.5, <Vartype.SPIN: frozenset({1, -1})>)\n            >>> bqm_qubo.offset, bqm_qubo.vartype\n            (-2.0, <Vartype.BINARY: frozenset({0, 1})>)\n\n        \"\"\"\n\n        if not inplace:\n            # create a new model of the appropriate type, then add self's biases to it\n            new_model = BinaryQuadraticModel({}, {}, 0.0, vartype)\n\n            new_model.add_variables_from(self.linear, vartype=self.vartype)\n            new_model.add_interactions_from(self.quadratic, vartype=self.vartype)\n            new_model.add_offset(self.offset)\n\n            return new_model\n\n        # in this case we are doing things in-place, if the desired vartype matches self.vartype,\n        # then we don't need to do anything\n        if vartype is self.vartype:\n            return self\n\n        if self.vartype is Vartype.SPIN and vartype is Vartype.BINARY:\n            linear, quadratic, offset = self.spin_to_binary(self.linear, self.quadratic, self.offset)\n        elif self.vartype is Vartype.BINARY and vartype is Vartype.SPIN:\n            linear, quadratic, offset = self.binary_to_spin(self.linear, self.quadratic, self.offset)\n        else:\n            raise RuntimeError(\"something has gone wrong. unknown vartype conversion.\")\n\n        # drop everything\n        for v in linear:\n            self.remove_variable(v)\n        self.add_offset(-self.offset)\n\n        self.vartype = vartype\n        self.add_variables_from(linear)\n        self.add_interactions_from(quadratic)\n        self.add_offset(offset)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting linear quadratic and offset from spin to binary.", "response": "def spin_to_binary(linear, quadratic, offset):\n        \"\"\"convert linear, quadratic, and offset from spin to binary.\n        Does no checking of vartype. Copies all of the values into new objects.\n        \"\"\"\n\n        # the linear biases are the easiest\n        new_linear = {v: 2. * bias for v, bias in iteritems(linear)}\n\n        # next the quadratic biases\n        new_quadratic = {}\n        for (u, v), bias in iteritems(quadratic):\n            new_quadratic[(u, v)] = 4. * bias\n            new_linear[u] -= 2. * bias\n            new_linear[v] -= 2. * bias\n\n        # finally calculate the offset\n        offset += sum(itervalues(quadratic)) - sum(itervalues(linear))\n\n        return new_linear, new_quadratic, offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert linear quadratic and offset from binary to spin.", "response": "def binary_to_spin(linear, quadratic, offset):\n        \"\"\"convert linear, quadratic and offset from binary to spin.\n        Does no checking of vartype. Copies all of the values into new objects.\n        \"\"\"\n        h = {}\n        J = {}\n        linear_offset = 0.0\n        quadratic_offset = 0.0\n\n        for u, bias in iteritems(linear):\n            h[u] = .5 * bias\n            linear_offset += bias\n\n        for (u, v), bias in iteritems(quadratic):\n\n            J[(u, v)] = .25 * bias\n\n            h[u] += .25 * bias\n            h[v] += .25 * bias\n\n            quadratic_offset += bias\n\n        offset += .5 * linear_offset + .25 * quadratic_offset\n\n        return h, J, offset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self):\n        # new objects are constructed for each, so we just need to pass them in\n        return BinaryQuadraticModel(self.linear, self.quadratic, self.offset, self.vartype, **self.info)", "response": "Create a copy of a BinaryQuadraticModel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef energy(self, sample):\n        linear = self.linear\n        quadratic = self.quadratic\n\n        if isinstance(sample, SampleView):\n            # because the SampleView object simply reads from an underlying matrix, each read\n            # is relatively expensive.\n            # However, sample.items() is ~10x faster than {sample[v] for v in sample}, therefore\n            # it is much more efficient to dump sample into a dictionary for repeated reads\n            sample = dict(sample)\n\n        en = self.offset\n        en += sum(linear[v] * sample[v] for v in linear)\n        en += sum(sample[u] * sample[v] * quadratic[(u, v)] for u, v in quadratic)\n        return en", "response": "Calculates the energy of a specified sample of a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the energies of the given samples.", "response": "def energies(self, samples_like, dtype=np.float):\n        \"\"\"Determine the energies of the given samples.\n\n        Args:\n            samples_like (samples_like):\n                A collection of raw samples. `samples_like` is an extension of NumPy's array_like\n                structure. See :func:`.as_samples`.\n\n            dtype (:class:`numpy.dtype`):\n                The data type of the returned energies.\n\n        Returns:\n            :obj:`numpy.ndarray`: The energies.\n\n        \"\"\"\n        samples, labels = as_samples(samples_like)\n\n        if all(v == idx for idx, v in enumerate(labels)):\n            ldata, (irow, icol, qdata), offset = self.to_numpy_vectors(dtype=dtype)\n        else:\n            ldata, (irow, icol, qdata), offset = self.to_numpy_vectors(variable_order=labels, dtype=dtype)\n\n        energies = samples.dot(ldata) + (samples[:, irow]*samples[:, icol]).dot(qdata) + offset\n        return np.asarray(energies, dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_coo(self, fp=None, vartype_header=False):\n        import dimod.serialization.coo as coo\n\n        if fp is None:\n            return coo.dumps(self, vartype_header)\n        else:\n            coo.dump(self, fp, vartype_header)", "response": "Serialize the binary quadratic model to a COOrdinate - format string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the binary quadratic model to a serializable object.", "response": "def to_serializable(self, use_bytes=False, bias_dtype=np.float32,\n                        bytes_type=bytes):\n        \"\"\"Convert the binary quadratic model to a serializable object.\n\n        Args:\n            use_bytes (bool, optional, default=False):\n                If True, a compact representation representing the biases as bytes is used.\n\n            bias_dtype (numpy.dtype, optional, default=numpy.float32):\n                If `use_bytes` is True, this numpy dtype will be used to\n                represent the bias values in the serialized format.\n\n            bytes_type (class, optional, default=bytes):\n                This class will be used to wrap the bytes objects in the\n                serialization if `use_bytes` is true. Useful for when using\n                Python 2 and using BSON encoding, which will not accept the raw\n                `bytes` type, so `bson.Binary` can be used instead.\n\n        Returns:\n            dict: An object that can be serialized.\n\n        Examples:\n\n            Encode using JSON\n\n            >>> import dimod\n            >>> import json\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({'a': -1.0, 'b': 1.0}, {('a', 'b'): -1.0}, 0.0, dimod.SPIN)\n            >>> s = json.dumps(bqm.to_serializable())\n\n            Encode using BSON_ in python 3.5+\n\n            >>> import dimod\n            >>> import bson\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({'a': -1.0, 'b': 1.0}, {('a', 'b'): -1.0}, 0.0, dimod.SPIN)\n            >>> doc = bqm.to_serializable(use_bytes=True)\n            >>> b = bson.BSON.encode(doc)  # doctest: +SKIP\n\n            Encode using BSON in python 2.7. Because :class:`bytes` is an alias for :class:`str`,\n            we need to signal to the encoder that it should encode the biases and labels as binary\n            data.\n\n            >>> import dimod\n            >>> import bson\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel({'a': -1.0, 'b': 1.0}, {('a', 'b'): -1.0}, 0.0, dimod.SPIN)\n            >>> doc = bqm.to_serializable(use_bytes=True, bytes_type=bson.Binary)\n            >>> b = bson.BSON.encode(doc)  # doctest: +SKIP\n\n        See also:\n            :meth:`~.BinaryQuadraticModel.from_serializable`\n\n            :func:`json.dumps`, :func:`json.dump` JSON encoding functions\n\n            :meth:`bson.BSON.encode` BSON encoding method\n\n        .. _BSON: http://bsonspec.org/\n\n        \"\"\"\n        from dimod.package_info import __version__\n        schema_version = \"2.0.0\"\n\n        try:\n            variables = sorted(self.variables)\n        except TypeError:\n            # sorting unlike types in py3\n            variables = list(self.variables)\n\n        num_variables = len(variables)\n\n        # when doing byte encoding we can use less space depending on the\n        # total number of variables\n        index_dtype = np.uint16 if num_variables <= 2**16 else np.uint32\n\n        ldata, (irow, icol, qdata), offset = self.to_numpy_vectors(\n            dtype=bias_dtype,\n            index_dtype=index_dtype,\n            sort_indices=True,\n            variable_order=variables)\n\n        doc = {\"basetype\": \"BinaryQuadraticModel\",\n               \"type\": type(self).__name__,\n               \"version\": {\"dimod\": __version__,\n                           \"bqm_schema\": schema_version},\n               \"variable_labels\": variables,\n               \"variable_type\": self.vartype.name,\n               \"info\": self.info,\n               \"offset\": float(offset),\n               \"use_bytes\": bool(use_bytes)\n               }\n\n        if use_bytes:\n            doc.update({'linear_biases': array2bytes(ldata, bytes_type=bytes_type),\n                        'quadratic_biases': array2bytes(qdata, bytes_type=bytes_type),\n                        'quadratic_head': array2bytes(irow, bytes_type=bytes_type),\n                        'quadratic_tail': array2bytes(icol, bytes_type=bytes_type)})\n        else:\n            doc.update({'linear_biases': ldata.tolist(),\n                        'quadratic_biases': qdata.tolist(),\n                        'quadratic_head': irow.tolist(),\n                        'quadratic_tail': icol.tolist()})\n\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_serializable(cls, obj):\n        if obj.get(\"version\", {\"bqm_schema\": \"1.0.0\"})[\"bqm_schema\"] != \"2.0.0\":\n            return cls._from_serializable_v1(obj)\n\n        variables = [tuple(v) if isinstance(v, list) else v\n                     for v in obj[\"variable_labels\"]]\n\n        if obj[\"use_bytes\"]:\n            ldata = bytes2array(obj[\"linear_biases\"])\n            qdata = bytes2array(obj[\"quadratic_biases\"])\n            irow = bytes2array(obj[\"quadratic_head\"])\n            icol = bytes2array(obj[\"quadratic_tail\"])\n        else:\n            ldata = obj[\"linear_biases\"]\n            qdata = obj[\"quadratic_biases\"]\n            irow = obj[\"quadratic_head\"]\n            icol = obj[\"quadratic_tail\"]\n\n        offset = obj[\"offset\"]\n        vartype = obj[\"variable_type\"]\n\n        bqm = cls.from_numpy_vectors(ldata,\n                                     (irow, icol, qdata),\n                                     offset,\n                                     str(vartype),  # handle unicode for py2\n                                     variable_order=variables)\n\n        bqm.info.update(obj[\"info\"])\n        return bqm", "response": "Deserialize a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a binary quadratic model to NetworkX graph format.", "response": "def to_networkx_graph(self, node_attribute_name='bias', edge_attribute_name='bias'):\n        \"\"\"Convert a binary quadratic model to NetworkX graph format.\n\n        Args:\n            node_attribute_name (hashable, optional, default='bias'):\n                Attribute name for linear biases.\n\n            edge_attribute_name (hashable, optional, default='bias'):\n                Attribute name for quadratic biases.\n\n        Returns:\n            :class:`networkx.Graph`: A NetworkX graph with biases stored as\n            node/edge attributes.\n\n        Examples:\n            This example converts a binary quadratic model to a NetworkX graph, using first\n            the default attribute name for quadratic biases then \"weight\".\n\n            >>> import networkx as nx\n            >>> bqm = dimod.BinaryQuadraticModel({0: 1, 1: -1, 2: .5},\n            ...                                  {(0, 1): .5, (1, 2): 1.5},\n            ...                                  1.4,\n            ...                                  dimod.SPIN)\n            >>> BQM = bqm.to_networkx_graph()\n            >>> BQM[0][1]['bias']\n            0.5\n            >>> BQM.node[0]['bias']\n            1\n            >>> BQM_w = bqm.to_networkx_graph(edge_attribute_name='weight')\n            >>> BQM_w[0][1]['weight']\n            0.5\n\n        \"\"\"\n        import networkx as nx\n\n        BQM = nx.Graph()\n\n        # add the linear biases\n        BQM.add_nodes_from(((v, {node_attribute_name: bias, 'vartype': self.vartype})\n                            for v, bias in iteritems(self.linear)))\n\n        # add the quadratic biases\n        BQM.add_edges_from(((u, v, {edge_attribute_name: bias}) for (u, v), bias in iteritems(self.quadratic)))\n\n        # set the offset and vartype properties for the graph\n        BQM.offset = self.offset\n        BQM.vartype = self.vartype\n\n        return BQM"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a binary quadratic model from a NetworkX graph.", "response": "def from_networkx_graph(cls, G, vartype=None, node_attribute_name='bias',\n                            edge_attribute_name='bias'):\n        \"\"\"Create a binary quadratic model from a NetworkX graph.\n\n        Args:\n            G (:obj:`networkx.Graph`):\n                A NetworkX graph with biases stored as node/edge attributes.\n\n            vartype (:class:`.Vartype`/str/set, optional):\n                Variable type for the binary quadratic model. Accepted input\n                values:\n\n                * :class:`.Vartype.SPIN`, ``'SPIN'``, ``{-1, 1}``\n                * :class:`.Vartype.BINARY`, ``'BINARY'``, ``{0, 1}``\n\n                If not provided, the `G` should have a vartype attribute. If\n                `vartype` is provided and `G.vartype` exists then the argument\n                overrides the property.\n\n            node_attribute_name (hashable, optional, default='bias'):\n                Attribute name for linear biases. If the node does not have a\n                matching attribute then the bias defaults to 0.\n\n            edge_attribute_name (hashable, optional, default='bias'):\n                Attribute name for quadratic biases. If the edge does not have a\n                matching attribute then the bias defaults to 0.\n\n        Returns:\n            :obj:`.BinaryQuadraticModel`\n\n        Examples:\n\n            >>> import networkx as nx\n            ...\n            >>> G = nx.Graph()\n            >>> G.add_node('a', bias=.5)\n            >>> G.add_edge('a', 'b', bias=-1)\n            >>> bqm = dimod.BinaryQuadraticModel.from_networkx_graph(G, 'SPIN')\n            >>> bqm.adj['a']['b']\n            -1\n\n        \"\"\"\n        if vartype is None:\n            if not hasattr(G, 'vartype'):\n                msg = (\"either 'vartype' argument must be provided or \"\n                       \"the given graph should have a vartype attribute.\")\n                raise ValueError(msg)\n            vartype = G.vartype\n\n        linear = G.nodes(data=node_attribute_name, default=0)\n        quadratic = G.edges(data=edge_attribute_name, default=0)\n        offset = getattr(G, 'offset', 0)\n\n        return cls(linear, quadratic, offset, vartype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_ising(self):\n        # cast to a dict\n        return dict(self.spin.linear), dict(self.spin.quadratic), self.spin.offset", "response": "Converts a binary quadratic model to Ising format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a binary quadratic model from an Ising problem.", "response": "def from_ising(cls, h, J, offset=0.0):\n        \"\"\"Create a binary quadratic model from an Ising problem.\n\n\n        Args:\n            h (dict/list):\n                Linear biases of the Ising problem. If a dict, should be of the\n                form `{v: bias, ...}` where v is a spin-valued variable and `bias`\n                is its associated bias. If a list, it is treated as a list of\n                biases where the indices are the variable labels.\n\n            J (dict[(variable, variable), bias]):\n                Quadratic biases of the Ising problem.\n\n            offset (optional, default=0.0):\n                Constant offset applied to the model.\n\n        Returns:\n            :class:`.BinaryQuadraticModel`: Binary quadratic model with vartype set to\n            :class:`.Vartype.SPIN`.\n\n        Examples:\n            This example creates a binary quadratic model from an Ising problem.\n\n            >>> import dimod\n            >>> h = {1: 1, 2: 2, 3: 3, 4: 4}\n            >>> J = {(1, 2): 12, (1, 3): 13, (1, 4): 14,\n            ...      (2, 3): 23, (2, 4): 24,\n            ...      (3, 4): 34}\n            >>> model = dimod.BinaryQuadraticModel.from_ising(h, J, offset = 0.0)\n            >>> model      # doctest: +SKIP\n            BinaryQuadraticModel({1: 1, 2: 2, 3: 3, 4: 4}, {(1, 2): 12, (1, 3): 13, (1, 4): 14, (2, 3): 23, (3, 4): 34, (2, 4): 24}, 0.0, Vartype.SPIN)\n\n        \"\"\"\n        if isinstance(h, abc.Sequence):\n            h = dict(enumerate(h))\n\n        return cls(h, J, offset, Vartype.SPIN)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_qubo(self):\n        qubo = dict(self.binary.quadratic)\n        qubo.update(((v, v), bias) for v, bias in iteritems(self.binary.linear))\n        return qubo, self.binary.offset", "response": "Convert a binary quadratic model to QUBO format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_qubo(cls, Q, offset=0.0):\n        linear = {}\n        quadratic = {}\n        for (u, v), bias in iteritems(Q):\n            if u == v:\n                linear[u] = bias\n            else:\n                quadratic[(u, v)] = bias\n\n        return cls(linear, quadratic, offset, Vartype.BINARY)", "response": "Create a binary quadratic model from a QUBO model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_numpy_matrix(self, variable_order=None):\n        import numpy as np\n\n        if variable_order is None:\n            # just use the existing variable labels, assuming that they are [0, N)\n            num_variables = len(self)\n            mat = np.zeros((num_variables, num_variables), dtype=float)\n\n            try:\n                for v, bias in iteritems(self.binary.linear):\n                    mat[v, v] = bias\n            except IndexError:\n                raise ValueError((\"if 'variable_order' is not provided, binary quadratic model must be \"\n                                  \"index labeled [0, ..., N-1]\"))\n\n            for (u, v), bias in iteritems(self.binary.quadratic):\n                if u < v:\n                    mat[u, v] = bias\n                else:\n                    mat[v, u] = bias\n\n        else:\n            num_variables = len(variable_order)\n            idx = {v: i for i, v in enumerate(variable_order)}\n\n            mat = np.zeros((num_variables, num_variables), dtype=float)\n\n            try:\n                for v, bias in iteritems(self.binary.linear):\n                    mat[idx[v], idx[v]] = bias\n            except KeyError as e:\n                raise ValueError((\"variable {} is missing from variable_order\".format(e)))\n\n            for (u, v), bias in iteritems(self.binary.quadratic):\n                iu, iv = idx[u], idx[v]\n                if iu < iv:\n                    mat[iu, iv] = bias\n                else:\n                    mat[iv, iu] = bias\n\n        return mat", "response": "Convert a binary quadratic model to a NumPy 2D array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a binary quadratic model from a NumPy array.", "response": "def from_numpy_matrix(cls, mat, variable_order=None, offset=0.0, interactions=None):\n        \"\"\"Create a binary quadratic model from a NumPy array.\n\n        Args:\n            mat (:class:`numpy.ndarray`):\n                Coefficients of a quadratic unconstrained binary optimization (QUBO)\n                model formatted as a square NumPy 2D array.\n\n            variable_order (list, optional):\n                If provided, labels the QUBO variables; otherwise, row/column indices are used.\n                If `variable_order` is longer than the array, extra values are ignored.\n\n            offset (optional, default=0.0):\n                Constant offset for the binary quadratic model.\n\n            interactions (iterable, optional, default=[]):\n                Any additional 0.0-bias interactions to be added to the binary quadratic model.\n\n        Returns:\n            :class:`.BinaryQuadraticModel`: Binary quadratic model with vartype set to\n            :class:`.Vartype.BINARY`.\n\n\n        Examples:\n            This example creates a binary quadratic model from a QUBO in NumPy format while\n            adding an interaction with a new variable ('f'), ignoring an extra variable\n            ('g'), and setting an offset.\n\n            >>> import dimod\n            >>> import numpy as np\n            >>> Q = np.array([[1, 0, 0, 10, 11],\n            ...               [0, 2, 0, 12, 13],\n            ...               [0, 0, 3, 14, 15],\n            ...               [0, 0, 0, 4, 0],\n            ...               [0, 0, 0, 0, 5]]).astype(np.float32)\n            >>> model = dimod.BinaryQuadraticModel.from_numpy_matrix(Q,\n            ...         variable_order = ['a', 'b', 'c', 'd', 'e', 'f', 'g'],\n            ...         offset = 2.5,\n            ...         interactions = {('a', 'f')})\n            >>> model.linear   # doctest: +SKIP\n            {'a': 1.0, 'b': 2.0, 'c': 3.0, 'd': 4.0, 'e': 5.0, 'f': 0.0}\n            >>> model.quadratic[('a', 'd')]\n            10.0\n            >>> model.quadratic[('a', 'f')]\n            0.0\n            >>> model.offset\n            2.5\n\n        \"\"\"\n        import numpy as np\n\n        if mat.ndim != 2:\n            raise ValueError(\"expected input mat to be a square 2D numpy array\")\n\n        num_row, num_col = mat.shape\n        if num_col != num_row:\n            raise ValueError(\"expected input mat to be a square 2D numpy array\")\n\n        if variable_order is None:\n            variable_order = list(range(num_row))\n\n        if interactions is None:\n            interactions = []\n\n        bqm = cls({}, {}, offset, Vartype.BINARY)\n\n        for (row, col), bias in np.ndenumerate(mat):\n            if row == col:\n                bqm.add_variable(variable_order[row], bias)\n            elif bias:\n                bqm.add_interaction(variable_order[row], variable_order[col], bias)\n\n        for u, v in interactions:\n            bqm.add_interaction(u, v, 0.0)\n\n        return bqm"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a binary quadratic model to numpy arrays.", "response": "def to_numpy_vectors(self, variable_order=None, dtype=np.float, index_dtype=np.int64, sort_indices=False):\n        \"\"\"Convert a binary quadratic model to numpy arrays.\n\n        Args:\n            variable_order (iterable, optional):\n                If provided, labels the variables; otherwise, row/column indices are used.\n\n            dtype (:class:`numpy.dtype`, optional):\n                Data-type of the biases. By default, the data-type is inferred from the biases.\n\n            index_dtype (:class:`numpy.dtype`, optional):\n                Data-type of the indices. By default, the data-type is inferred from the labels.\n\n            sort_indices (bool, optional, default=False):\n                If True, the indices are sorted, first by row then by column. Otherwise they\n                match :attr:`~.BinaryQuadraticModel.quadratic`.\n\n        Returns:\n            :obj:`~numpy.ndarray`: A numpy array of the linear biases.\n\n            tuple: The quadratic biases in COOrdinate format.\n\n                :obj:`~numpy.ndarray`: A numpy array of the row indices of the quadratic matrix\n                entries\n\n                :obj:`~numpy.ndarray`: A numpy array of the column indices of the quadratic matrix\n                entries\n\n                :obj:`~numpy.ndarray`: A numpy array of the values of the quadratic matrix\n                entries\n\n            The offset\n\n        Examples:\n            >>> bqm = dimod.BinaryQuadraticModel({}, {(0, 1): .5, (3, 2): -1, (0, 3): 1.5}, 0.0, dimod.SPIN)\n            >>> lin, (i, j, vals), off = bqm.to_numpy_vectors(sort_indices=True)\n            >>> lin\n            array([0., 0., 0., 0.])\n            >>> i\n            array([0, 0, 2])\n            >>> j\n            array([1, 3, 3])\n            >>> vals\n            array([ 0.5,  1.5, -1. ])\n\n        \"\"\"\n        linear = self.linear\n        quadratic = self.quadratic\n\n        num_variables = len(linear)\n        num_interactions = len(quadratic)\n\n        irow = np.empty(num_interactions, dtype=index_dtype)\n        icol = np.empty(num_interactions, dtype=index_dtype)\n        qdata = np.empty(num_interactions, dtype=dtype)\n\n        if variable_order is None:\n            try:\n                ldata = np.fromiter((linear[v] for v in range(num_variables)), count=num_variables, dtype=dtype)\n            except KeyError:\n                raise ValueError((\"if 'variable_order' is not provided, binary quadratic model must be \"\n                                  \"index labeled [0, ..., N-1]\"))\n\n            # we could speed this up a lot with cython\n            for idx, ((u, v), bias) in enumerate(quadratic.items()):\n                irow[idx] = u\n                icol[idx] = v\n                qdata[idx] = bias\n\n        else:\n            try:\n                ldata = np.fromiter((linear[v] for v in variable_order), count=num_variables, dtype=dtype)\n            except KeyError:\n                raise ValueError(\"provided 'variable_order' does not match binary quadratic model\")\n\n            label_to_idx = {v: idx for idx, v in enumerate(variable_order)}\n\n            # we could speed this up a lot with cython\n            for idx, ((u, v), bias) in enumerate(quadratic.items()):\n                irow[idx] = label_to_idx[u]\n                icol[idx] = label_to_idx[v]\n                qdata[idx] = bias\n\n        if sort_indices:\n            # row index should be less than col index, this handles upper-triangular vs lower-triangular\n            swaps = irow > icol\n            if swaps.any():\n                # in-place\n                irow[swaps], icol[swaps] = icol[swaps], irow[swaps]\n\n            # sort lexigraphically\n            order = np.lexsort((irow, icol))\n            if not (order == range(len(order))).all():\n                # copy\n                irow = irow[order]\n                icol = icol[order]\n                qdata = qdata[order]\n\n        return ldata, (irow, icol, qdata), ldata.dtype.type(self.offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_numpy_vectors(cls, linear, quadratic, offset, vartype, variable_order=None):\n\n        try:\n            heads, tails, values = quadratic\n        except ValueError:\n            raise ValueError(\"quadratic should be a 3-tuple\")\n\n        if not len(heads) == len(tails) == len(values):\n            raise ValueError(\"row, col, and bias should be of equal length\")\n\n        if variable_order is None:\n            variable_order = list(range(len(linear)))\n\n        linear = {v: float(bias) for v, bias in zip(variable_order, linear)}\n        quadratic = {(variable_order[u], variable_order[v]): float(bias)\n                     for u, v, bias in zip(heads, tails, values)}\n\n        return cls(linear, quadratic, offset, vartype)", "response": "Create a binary quadratic model from a 1D array - like iterable of linear biases quadratic and offset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a binary quadratic model to pandas DataFrame format.", "response": "def to_pandas_dataframe(self):\n        \"\"\"Convert a binary quadratic model to pandas DataFrame format.\n\n        Returns:\n            :class:`pandas.DataFrame`: The binary quadratic model as a DataFrame. The DataFrame has\n            binary vartype. The rows and columns are labeled by the variables in the binary quadratic\n            model.\n\n        Notes:\n            The DataFrame representation of a binary quadratic model only makes sense for binary models.\n            For a binary sample x, the energy of the model is given by:\n\n            .. math::\n\n                E(x) = x^T Q x\n\n\n            The offset is dropped when converting to a pandas DataFrame.\n\n        Examples:\n            This example converts a binary quadratic model to pandas DataFrame format.\n\n            >>> import dimod\n            >>> model = dimod.BinaryQuadraticModel({'a': 1.1, 'b': -1., 'c': .5},\n            ...                                    {('a', 'b'): .5, ('b', 'c'): 1.5},\n            ...                                    1.4,\n            ...                                    dimod.BINARY)\n            >>> model.to_pandas_dataframe()  # doctest: +SKIP\n                 a    b    c\n            a  1.1  0.5  0.0\n            b  0.0 -1.0  1.5\n            c  0.0  0.0  0.5\n\n        \"\"\"\n        import pandas as pd\n\n        try:\n            variable_order = sorted(self.linear)\n        except TypeError:\n            variable_order = list(self.linear)\n\n        return pd.DataFrame(self.to_numpy_matrix(variable_order=variable_order),\n                            index=variable_order,\n                            columns=variable_order)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_pandas_dataframe(cls, bqm_df, offset=0.0, interactions=None):\n        if interactions is None:\n            interactions = []\n\n        bqm = cls({}, {}, offset, Vartype.BINARY)\n\n        for u, row in bqm_df.iterrows():\n            for v, bias in row.iteritems():\n                if u == v:\n                    bqm.add_variable(u, bias)\n                elif bias:\n                    bqm.add_interaction(u, v, bias)\n\n        for u, v in interactions:\n            bqm.add_interaction(u, v, 0.0)\n\n        return bqm", "response": "Create a binary quadratic model from a pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsample from the provided binary quadratic model.", "response": "def sample(self, bqm, fixed_variables=None, **parameters):\n        \"\"\"Sample from the provided binary quadratic model.\n\n        Args:\n            bqm (:obj:`dimod.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            fixed_variables (dict):\n                A dictionary of variable assignments.\n\n            **parameters:\n                Parameters for the sampling method, specified by the child sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n\n        # solve the problem on the child system\n        child = self.child\n        bqm_copy = bqm.copy()\n        if fixed_variables is None:\n            fixed_variables = {}\n\n        bqm_copy.fix_variables(fixed_variables)\n        sampleset = child.sample(bqm_copy, **parameters)\n\n        if len(sampleset):\n            return sampleset.append_variables(fixed_variables)\n        elif fixed_variables:\n            return type(sampleset).from_samples_bqm(fixed_variables, bqm=bqm)\n        else:\n            # no fixed variables and sampleset is empty\n            return sampleset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the induced energy of a given sample for an Ising model.", "response": "def ising_energy(sample, h, J, offset=0.0):\n    \"\"\"Calculate the energy for the specified sample of an Ising model.\n\n    Energy of a sample for a binary quadratic model is defined as a sum, offset\n    by the constant energy offset associated with the model, of\n    the sample multipled by the linear bias of the variable and\n    all its interactions. For an Ising model,\n\n    .. math::\n\n        E(\\mathbf{s}) = \\sum_v h_v s_v + \\sum_{u,v} J_{u,v} s_u s_v + c\n\n    where :math:`s_v` is the sample, :math:`h_v` is the linear bias, :math:`J_{u,v}`\n    the quadratic bias (interactions), and :math:`c` the energy offset.\n\n    Args:\n        sample (dict[variable, spin]):\n            Sample for a binary quadratic model as a dict of form {v: spin, ...},\n            where keys are variables of the model and values are spins (either -1 or 1).\n        h (dict[variable, bias]):\n            Linear biases as a dict of the form {v: bias, ...}, where keys are variables of\n            the model and values are biases.\n        J (dict[(variable, variable), bias]):\n           Quadratic biases as a dict of the form {(u, v): bias, ...}, where keys\n           are 2-tuples of variables of the model and values are quadratic biases\n           associated with the pair of variables (the interaction).\n        offset (numeric, optional, default=0):\n            Constant offset to be applied to the energy. Default 0.\n\n    Returns:\n        float: The induced energy.\n\n    Notes:\n        No input checking is performed.\n\n    Examples:\n        This example calculates the energy of a sample representing two down spins for\n        an Ising model of two variables that have positive biases of value 1 and\n        are positively coupled with an interaction of value 1.\n\n        >>> import dimod\n        >>> sample = {1: -1, 2: -1}\n        >>> h = {1: 1, 2: 1}\n        >>> J = {(1, 2): 1}\n        >>> dimod.ising_energy(sample, h, J, 0.5)\n        -0.5\n\n    References\n    ----------\n\n    `Ising model on Wikipedia <https://en.wikipedia.org/wiki/Ising_model>`_\n\n    \"\"\"\n    # add the contribution from the linear biases\n    for v in h:\n        offset += h[v] * sample[v]\n\n    # add the contribution from the quadratic biases\n    for v0, v1 in J:\n        offset += J[(v0, v1)] * sample[v0] * sample[v1]\n\n    return offset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the energy of a specified sample of a binary quadratic model.", "response": "def qubo_energy(sample, Q, offset=0.0):\n    \"\"\"Calculate the energy for the specified sample of a QUBO model.\n\n    Energy of a sample for a binary quadratic model is defined as a sum, offset\n    by the constant energy offset associated with the model, of\n    the sample multipled by the linear bias of the variable and\n    all its interactions. For a quadratic unconstrained binary optimization (QUBO)\n    model,\n\n    .. math::\n\n        E(\\mathbf{x}) = \\sum_{u,v} Q_{u,v} x_u x_v + c\n\n    where :math:`x_v` is the sample, :math:`Q_{u,v}`\n    a matrix of biases, and :math:`c` the energy offset.\n\n    Args:\n        sample (dict[variable, spin]):\n            Sample for a binary quadratic model as a dict of form {v: bin, ...},\n            where keys are variables of the model and values are binary (either 0 or 1).\n        Q (dict[(variable, variable), coefficient]):\n            QUBO coefficients in a dict of form {(u, v): coefficient, ...}, where keys\n            are 2-tuples of variables of the model and values are biases\n            associated with the pair of variables. Tuples (u, v) represent interactions\n            and (v, v) linear biases.\n        offset (numeric, optional, default=0):\n            Constant offset to be applied to the energy. Default 0.\n\n    Returns:\n        float: The induced energy.\n\n    Notes:\n        No input checking is performed.\n\n    Examples:\n        This example calculates the energy of a sample representing two zeros for\n        a QUBO model of two variables that have positive biases of value 1 and\n        are positively coupled with an interaction of value 1.\n\n        >>> import dimod\n        >>> sample = {1: 0, 2: 0}\n        >>> Q = {(1, 1): 1, (2, 2): 1, (1, 2): 1}\n        >>> dimod.qubo_energy(sample, Q, 0.5)\n        0.5\n\n    References\n    ----------\n\n    `QUBO model on Wikipedia <https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization>`_\n\n    \"\"\"\n    for v0, v1 in Q:\n        offset += sample[v0] * sample[v1] * Q[(v0, v1)]\n\n    return offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an Ising problem to a QUBO problem.", "response": "def ising_to_qubo(h, J, offset=0.0):\n    \"\"\"Convert an Ising problem to a QUBO problem.\n\n    Map an Ising model defined on spins (variables with {-1, +1} values) to quadratic\n    unconstrained binary optimization (QUBO) formulation :math:`x'  Q  x` defined over\n    binary variables (0 or 1 values), where the linear term is contained along the diagonal of Q.\n    Return matrix Q that defines the model as well as the offset in energy between the two\n    problem formulations:\n\n    .. math::\n\n         s'  J  s + h'  s = offset + x'  Q  x\n\n    See :meth:`~dimod.utilities.qubo_to_ising` for the inverse function.\n\n    Args:\n        h (dict[variable, bias]):\n            Linear biases as a dict of the form {v: bias, ...}, where keys are variables of\n            the model and values are biases.\n        J (dict[(variable, variable), bias]):\n           Quadratic biases as a dict of the form {(u, v): bias, ...}, where keys\n           are 2-tuples of variables of the model and values are quadratic biases\n           associated with the pair of variables (the interaction).\n        offset (numeric, optional, default=0):\n            Constant offset to be applied to the energy. Default 0.\n\n    Returns:\n        (dict, float): A 2-tuple containing:\n\n            dict: QUBO coefficients.\n\n            float: New energy offset.\n\n    Examples:\n        This example converts an Ising problem of two variables that have positive\n        biases of value 1 and are positively coupled with an interaction of value 1\n        to a QUBO problem.\n\n        >>> import dimod\n        >>> h = {1: 1, 2: 1}\n        >>> J = {(1, 2): 1}\n        >>> dimod.ising_to_qubo(h, J, 0.5)  # doctest: +SKIP\n        ({(1, 1): 0.0, (1, 2): 4.0, (2, 2): 0.0}, -0.5)\n\n    \"\"\"\n    # the linear biases are the easiest\n    q = {(v, v): 2. * bias for v, bias in iteritems(h)}\n\n    # next the quadratic biases\n    for (u, v), bias in iteritems(J):\n        if bias == 0.0:\n            continue\n        q[(u, v)] = 4. * bias\n        q[(u, u)] -= 2. * bias\n        q[(v, v)] -= 2. * bias\n\n    # finally calculate the offset\n    offset += sum(itervalues(J)) - sum(itervalues(h))\n\n    return q, offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a QUBO problem to an Ising problem.", "response": "def qubo_to_ising(Q, offset=0.0):\n    \"\"\"Convert a QUBO problem to an Ising problem.\n\n    Map a quadratic unconstrained binary optimization (QUBO) problem :math:`x'  Q  x`\n    defined over binary variables (0 or 1 values), where the linear term is contained along\n    the diagonal of Q, to an Ising model defined on spins (variables with {-1, +1} values).\n    Return h and J that define the Ising model as well as the offset in energy\n    between the two problem formulations:\n\n    .. math::\n\n         x'  Q  x  = offset + s'  J  s + h'  s\n\n    See :meth:`~dimod.utilities.ising_to_qubo` for the inverse function.\n\n    Args:\n        Q (dict[(variable, variable), coefficient]):\n            QUBO coefficients in a dict of form {(u, v): coefficient, ...}, where keys\n            are 2-tuples of variables of the model and values are biases\n            associated with the pair of variables. Tuples (u, v) represent interactions\n            and (v, v) linear biases.\n        offset (numeric, optional, default=0):\n            Constant offset to be applied to the energy. Default 0.\n\n    Returns:\n        (dict, dict, float): A 3-tuple containing:\n\n            dict: Linear coefficients of the Ising problem.\n\n            dict: Quadratic coefficients of the Ising problem.\n\n            float: New energy offset.\n\n    Examples:\n        This example converts a QUBO problem of two variables that have positive\n        biases of value 1 and are positively coupled with an interaction of value 1\n        to an Ising problem.\n\n        >>> import dimod\n        >>> Q = {(1, 1): 1, (2, 2): 1, (1, 2): 1}\n        >>> dimod.qubo_to_ising(Q, 0.5)    # doctest: +SKIP\n        ({1: 0.75, 2: 0.75}, {(1, 2): 0.25}, 1.75)\n\n    \"\"\"\n    h = {}\n    J = {}\n    linear_offset = 0.0\n    quadratic_offset = 0.0\n\n    for (u, v), bias in iteritems(Q):\n        if u == v:\n            if u in h:\n                h[u] += .5 * bias\n            else:\n                h[u] = .5 * bias\n            linear_offset += bias\n\n        else:\n            if bias != 0.0:\n                J[(u, v)] = .25 * bias\n\n            if u in h:\n                h[u] += .25 * bias\n            else:\n                h[u] = .25 * bias\n\n            if v in h:\n                h[v] += .25 * bias\n            else:\n                h[v] = .25 * bias\n\n            quadratic_offset += bias\n\n    offset += .5 * linear_offset + .25 * quadratic_offset\n\n    return h, J, offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve a self - labeling conflict by creating an intermediate labeling.", "response": "def resolve_label_conflict(mapping, old_labels=None, new_labels=None):\n    \"\"\"Resolve a self-labeling conflict by creating an intermediate labeling.\n\n    Args:\n        mapping (dict):\n            A dict mapping the current variable labels to new ones.\n\n        old_labels (set, optional, default=None):\n            The keys of mapping. Can be passed in for performance reasons. These are not checked.\n\n        new_labels (set, optional, default=None):\n            The values of mapping. Can be passed in for performance reasons. These are not checked.\n\n    Returns:\n        tuple: A 2-tuple containing:\n\n            dict: A map from the keys of mapping to an intermediate labeling\n\n            dict: A map from the intermediate labeling to the values of mapping.\n\n    \"\"\"\n\n    if old_labels is None:\n        old_labels = set(mapping)\n    if new_labels is None:\n        new_labels = set(itervalues(mapping))\n\n    # counter will be used to generate the intermediate labels, as an easy optimization\n    # we start the counter with a high number because often variables are labeled by\n    # integers starting from 0\n    counter = itertools.count(2 * len(mapping))\n\n    old_to_intermediate = {}\n    intermediate_to_new = {}\n\n    for old, new in iteritems(mapping):\n        if old == new:\n            # we can remove self-labels\n            continue\n\n        if old in new_labels or new in old_labels:\n\n            # try to get a new unique label\n            lbl = next(counter)\n            while lbl in new_labels or lbl in old_labels:\n                lbl = next(counter)\n\n            # add it to the mapping\n            old_to_intermediate[old] = lbl\n            intermediate_to_new[lbl] = new\n\n        else:\n            old_to_intermediate[old] = new\n            # don't need to add it to intermediate_to_new because it is a self-label\n\n    return old_to_intermediate, intermediate_to_new"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_variables(bqm, sampling_mode=True):\n    try:\n        from dimod.roof_duality._fix_variables import fix_variables_wrapper\n    except ImportError:\n        raise ImportError(\"c++ extension roof_duality is not built\")\n\n    if sampling_mode:\n        method = 2  # roof-duality only\n    else:\n        method = 1  # roof-duality and strongly connected components\n\n    linear = bqm.linear\n    if all(v in linear for v in range(len(bqm))):\n        # we can work with the binary form of the bqm directly\n        fixed = fix_variables_wrapper(bqm.binary, method)\n    else:\n        try:\n            inverse_mapping = dict(enumerate(sorted(linear)))\n        except TypeError:\n            # in python3 unlike types cannot be sorted\n            inverse_mapping = dict(enumerate(linear))\n        mapping = {v: i for i, v in inverse_mapping.items()}\n\n        fixed = fix_variables_wrapper(bqm.relabel_variables(mapping, inplace=False).binary, method)\n        fixed = {inverse_mapping[v]: val for v, val in fixed.items()}\n\n    if bqm.vartype is Vartype.SPIN:\n        return {v: 2*val - 1 for v, val in fixed.items()}\n    else:\n        return fixed", "response": "This function fixes the variables of a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dimod_object_hook(obj):\n    if _is_sampleset_v2(obj):\n        # in the future we could handle subtypes but right now we just have the\n        # one\n        return SampleSet.from_serializable(obj)\n    elif _is_bqm_v2(obj):\n        # in the future we could handle subtypes but right now we just have the\n        # one\n        return BinaryQuadraticModel.from_serializable(obj)\n    return obj", "response": "JSON - decoding for dimod objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a list label into a tuple. Works recursively on nested lists.", "response": "def _decode_label(label):\n    \"\"\"Convert a list label into a tuple. Works recursively on nested lists.\"\"\"\n    if isinstance(label, list):\n        return tuple(_decode_label(v) for v in label)\n    return label"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a tuple label into a list. Works recursively on nested tuples.", "response": "def _encode_label(label):\n    \"\"\"Convert a tuple label into a list. Works recursively on nested tuples.\"\"\"\n    if isinstance(label, tuple):\n        return [_encode_label(v) for v in label]\n    return label"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a bqm with a gap of 2 that represents the product of two variables.", "response": "def _spin_product(variables):\n    \"\"\"Create a bqm with a gap of 2 that represents the product of two variables.\n\n    Note that spin-product requires an auxiliary variable.\n\n    Args:\n        variables (list):\n            multiplier, multiplicand, product, aux\n\n    Returns:\n        :obj:`.BinaryQuadraticModel`\n\n    \"\"\"\n    multiplier, multiplicand, product, aux = variables\n\n    return BinaryQuadraticModel({multiplier: -.5,\n                                 multiplicand: -.5,\n                                 product: -.5,\n                                 aux: -1.},\n                                {(multiplier, multiplicand): .5,\n                                 (multiplier, product): .5,\n                                 (multiplier, aux): 1.,\n                                 (multiplicand, product): .5,\n                                 (multiplicand, aux): 1.,\n                                 (product, aux): 1.},\n                                2.,\n                                Vartype.SPIN)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a binary quadratic model that represents the product of two variables.", "response": "def _binary_product(variables):\n    \"\"\"Create a bqm with a gap of 2 that represents the product of two variables.\n\n    Args:\n        variables (list):\n            multiplier, multiplicand, product\n\n    Returns:\n        :obj:`.BinaryQuadraticModel`\n\n    \"\"\"\n    multiplier, multiplicand, product = variables\n\n    return BinaryQuadraticModel({multiplier: 0.0,\n                                 multiplicand: 0.0,\n                                 product: 3.0},\n                                {(multiplier, multiplicand): 1.0,\n                                 (multiplier, product): -2.0,\n                                 (multiplicand, product): -2.0},\n                                0.0,\n                                Vartype.BINARY)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_quadratic(poly, strength, vartype=None, bqm=None):\n\n    if bqm is None:\n        if vartype is None:\n            raise ValueError(\"one of vartype and bqm must be provided\")\n        bqm = BinaryQuadraticModel.empty(vartype)\n    else:\n        if not isinstance(bqm, BinaryQuadraticModel):\n            raise TypeError('create_using must be a BinaryQuadraticModel')\n        if vartype is not None and vartype is not bqm.vartype:\n            raise ValueError(\"one of vartype and create_using must be provided\")\n    bqm.info['reduction'] = {}\n\n    new_poly = {}\n    for term, bias in iteritems(poly):\n        if len(term) == 0:\n            bqm.add_offset(bias)\n        elif len(term) == 1:\n            v, = term\n            bqm.add_variable(v, bias)\n        else:\n            new_poly[term] = bias\n\n    return _reduce_degree(bqm, new_poly, vartype, strength)", "response": "Create a binary quadratic model from a higher order polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reduce_degree(bqm, poly, vartype, scale):\n\n    if all(len(term) <= 2 for term in poly):\n        # termination criteria, we are already quadratic\n        bqm.add_interactions_from(poly)\n        return bqm\n\n    # determine which pair of variables appear most often\n    paircounter = Counter()\n    for term in poly:\n        if len(term) > 2:\n            for u, v in itertools.combinations(term, 2):\n                pair = frozenset((u, v))\n                paircounter[pair] += 1\n\n    pair, __ = paircounter.most_common(1)[0]\n    u, v = pair\n\n    # make a new product variable and aux variable and add constraint that u*v == p\n    p = '{}*{}'.format(u, v)\n\n    while p in bqm.linear:\n        p = '_' + p\n\n    if vartype is Vartype.BINARY:\n        constraint = _binary_product([u, v, p])\n\n        bqm.info['reduction'][(u, v)] = {'product': p}\n    else:\n        aux = 'aux{},{}'.format(u, v)\n        while aux in bqm.linear:\n            aux = '_' + aux\n        constraint = _spin_product([u, v, p, aux])\n\n        bqm.info['reduction'][(u, v)] = {'product': p, 'auxiliary': aux}\n\n    constraint.scale(scale)\n    bqm.update(constraint)\n\n    new_poly = {}\n    for interaction, bias in poly.items():\n        if u in interaction and v in interaction:\n\n            if len(interaction) == 2:\n                # in this case we are reducing a quadratic bias, so it becomes linear and can\n                # be removed\n                assert len(interaction) >= 2\n                bqm.add_variable(p, bias)\n                continue\n\n            interaction = tuple(s for s in interaction if s not in pair)\n            interaction += (p,)\n\n        if interaction in new_poly:\n            new_poly[interaction] += bias\n        else:\n            new_poly[interaction] = bias\n\n    return _reduce_degree(bqm, new_poly, vartype, scale)", "response": "reduce degree of a binary quadratic model by adding a constraint that is a linear quadratic."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poly_energy(sample_like, poly):\n\n    msg = (\"poly_energy is deprecated and will be removed in dimod 0.9.0.\"\n           \"In the future, use BinaryPolynomial.energy\")\n    warnings.warn(msg, DeprecationWarning)\n    # dev note the vartype is not used in the energy calculation and this will\n    # be deprecated in the future\n    return BinaryPolynomial(poly, 'SPIN').energy(sample_like)", "response": "Calculates the energy of a sample from a higher order polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the energy of samples from a higher order polynomial.", "response": "def poly_energies(samples_like, poly):\n    \"\"\"Calculates energy of samples from a higher order polynomial.\n\n    Args:\n        sample (samples_like):\n            A collection of raw samples. `samples_like` is an extension of\n            NumPy's array_like structure. See :func:`.as_samples`.\n\n        poly (dict):\n            Polynomial as a dict of form {term: bias, ...}, where `term` is a\n            tuple of variables and `bias` the associated bias. Variable\n            labeling/indexing of terms in poly dict must match that of the\n            sample(s).\n\n    Returns:\n        list/:obj:`numpy.ndarray`: The energy of the sample(s).\n\n    \"\"\"\n    msg = (\"poly_energies is deprecated and will be removed in dimod 0.9.0.\"\n           \"In the future, use BinaryPolynomial.energies\")\n    warnings.warn(msg, DeprecationWarning)\n    # dev note the vartype is not used in the energy calculation and this will\n    # be deprecated in the future\n    return BinaryPolynomial(poly, 'SPIN').energies(samples_like)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frustrated_loop(graph, num_cycles, R=float('inf'), cycle_predicates=tuple(),\n                    max_failed_cycles=100, seed=None):\n    \"\"\"Generate a frustrated loop problem.\n\n    A (generic) frustrated loop (FL) problem is a sum of Hamiltonians, each generated from a single\n    \"good\" loop.\n    1. Generate a loop by random walking on the support graph.\n    2. If the cycle is \"good\" (according to provided predicates), continue, else go to 1.\n    3. Choose one edge of the loop to be anti-ferromagnetic; all other edges are ferromagnetic.\n    4. Add the loop's coupler values to the FL problem.\n    If at any time the magnitude of a coupler in the FL problem exceeds a given precision `R`,\n    remove that coupler from consideration in the loop generation procedure.\n\n    This is a generic generator of FL problems that encompasses both the original FL problem\n    definition from [#HJARTL]_ and the limited FL problem definition from [#KLH]_\n\n    Args:\n        graph (int/tuple[nodes, edges]/:obj:`~networkx.Graph`):\n            The graph to build the frustrated loops on. Either an integer n, interpreted as a\n            complete graph of size n, or a nodes/edges pair, or a NetworkX graph.\n\n        num_cyles (int):\n            Desired number of frustrated cycles.\n\n        R (int, optional, default=inf):\n            Maximum interaction weight.\n\n        cycle_predicates (tuple[function], optional):\n            An iterable of functions, which should accept a cycle and return a bool.\n\n        max_failed_cycles (int, optional, default=100):\n            Maximum number of failures to find a cycle before terminating.\n\n        seed (int, optional, default=None):\n            Random seed.\n\n    .. [#HJARTL] Hen, I., J. Job, T. Albash, T.F. R\u00f8nnow, M. Troyer, D. Lidar. Probing for quantum\n        speedup in spin glass problems with planted solutions. https://arxiv.org/abs/1502.01663v2\n\n    .. [#KLH] King, A.D., T. Lanting, R. Harris. Performance of a quantum annealer on range-limited\n        constraint satisfaction problems. https://arxiv.org/abs/1502.02098\n\n    \"\"\"\n    nodes, edges = graph\n    if num_cycles <= 0:\n        raise ValueError(\"num_cycles should be a positive integer\")\n    if R <= 0:\n        raise ValueError(\"R should be a positive integer\")\n    if max_failed_cycles <= 0:\n        raise ValueError(\"max_failed_cycles should be a positive integer\")\n\n    if seed is None:\n        seed = numpy.random.randint(2**32, dtype=np.uint32)\n    r = numpy.random.RandomState(seed)\n\n    # G = nx.Graph(edges)\n    # J = collections.defaultdict(int)\n    adj = {v: set() for v in nodes}\n    for u, v in edges:\n        if u in adj:\n            adj[u].add(v)\n        else:\n            adj[u] = {v}\n        if v in adj:\n            adj[v].add(u)\n        else:\n            adj[v] = {u}\n    bqm = BinaryQuadraticModel({v: 0.0 for v in nodes}, {edge: 0.0 for edge in edges}, 0.0, SPIN)\n\n    failed_cycles = 0\n    good_cycles = 0\n    while good_cycles < num_cycles and failed_cycles < max_failed_cycles:\n\n        cycle = _random_cycle(adj, r)\n\n        # if the cycle failed or it is otherwise invalid, mark as failed and continue\n        if cycle is None or not all(pred(cycle) for pred in cycle_predicates):\n            failed_cycles += 1\n            continue\n\n        # If its a good cycle, modify J with it.\n        good_cycles += 1\n\n        cycle_J = {(cycle[i - 1], cycle[i]): -1. for i in range(len(cycle))}\n\n        # randomly select an edge and flip it\n        idx = r.randint(len(cycle))\n        cycle_J[(cycle[idx - 1], cycle[idx])] *= -1.\n\n        # update the bqm\n        bqm.add_interactions_from(cycle_J)\n\n        for u, v in cycle_J:\n            if abs(bqm.adj[u][v]) >= R:\n                adj[u].remove(v)\n                adj[v].remove(u)\n\n    if good_cycles < num_cycles:\n        raise RuntimeError\n\n    return bqm", "response": "Generate a frustrated FL problem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind a random cycle using a random graph walk.", "response": "def _random_cycle(adj, random_state):\n    \"\"\"Find a cycle using a random graph walk.\"\"\"\n\n    # step through idx values in adj to pick a random one, random.choice does not work on dicts\n    n = random_state.randint(len(adj))\n    for idx, v in enumerate(adj):\n        if idx == n:\n            break\n    start = v\n\n    walk = [start]\n    visited = {start: 0}\n\n    while True:\n        if len(walk) > 1:\n            # as long as we don't step back one we won't have any repeated edges\n            previous = walk[-2]\n            neighbors = [u for u in adj[walk[-1]] if u != previous]\n        else:\n            neighbors = list(adj[walk[-1]])\n\n        if not neighbors:\n            # we've walked into a dead end\n            return None\n\n        # get a random neighbor\n        u = random_state.choice(neighbors)\n        if u in visited:\n            # if we've seen this neighbour, then we have a cycle starting from it\n            return walk[visited[u]:]\n        else:\n            # add to walk and keep moving\n            walk.append(u)\n            visited[u] = len(visited)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsample from the binary quadratic model.", "response": "def sample(self, bqm, num_spin_reversal_transforms=2, spin_reversal_variables=None, **kwargs):\n        \"\"\"Sample from the binary quadratic model.\n\n        Args:\n            bqm (:obj:`~dimod.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            num_spin_reversal_transforms (integer, optional, default=2):\n                Number of spin reversal transform runs.\n\n            spin_reversal_variables (list/dict, optional):\n                Deprecated and no longer functional.\n\n        Returns:\n            :obj:`.SampleSet`\n\n        Examples:\n            This example runs 100 spin reversals applied to one variable of a QUBO problem.\n\n            >>> import dimod\n            ...\n            >>> base_sampler = dimod.ExactSolver()\n            >>> composed_sampler = dimod.SpinReversalTransformComposite(base_sampler)\n            >>> Q = {('a', 'a'): -1, ('b', 'b'): -1, ('a', 'b'): 2}\n            >>> response = composed_sampler.sample_qubo(Q,\n            ...               num_spin_reversal_transforms=100,\n            ...               spin_reversal_variables={'a'})\n            >>> len(response)\n            400\n            >>> print(next(response.data()))           # doctest: +SKIP\n            Sample(sample={'a': 0, 'b': 1}, energy=-1.0)\n\n        \"\"\"\n\n        if spin_reversal_variables is not None:\n            # this kwarg does not actually make sense for multiple SRTs. To\n            # get the same functionality a user should apply them by hand\n            # to their BQM before submitting.\n            import warnings\n            warnings.warn(\"'spin_reversal_variables' kwarg is deprecated and no longer functions.\",\n                          DeprecationWarning)\n\n        # make a main response\n        responses = []\n\n        flipped_bqm = bqm.copy()\n        transform = {v: False for v in bqm.variables}\n\n        for ii in range(num_spin_reversal_transforms):\n            # flip each variable with a 50% chance\n            for v in bqm:\n                if random() > .5:\n                    transform[v] = not transform[v]\n                    flipped_bqm.flip_variable(v)\n\n            flipped_response = self.child.sample(flipped_bqm, **kwargs)\n\n            tf_idxs = [flipped_response.variables.index(v)\n                       for v, flip in transform.items() if flip]\n\n            if bqm.vartype is Vartype.SPIN:\n                flipped_response.record.sample[:, tf_idxs] = -1 * flipped_response.record.sample[:, tf_idxs]\n            else:\n                flipped_response.record.sample[:, tf_idxs] = 1 - flipped_response.record.sample[:, tf_idxs]\n\n            responses.append(flipped_response)\n\n        return concatenate(responses)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a column to the table.", "response": "def append(self, header, f, _left=False):\n        \"\"\"Add a column to the table.\n\n        Args:\n            header (str):\n                Column header\n\n            f (function(datum)->str):\n                Makes the row string from the datum. Str returned by f should\n                have the same width as header.\n\n        \"\"\"\n        self.items_length += len(header)\n        if _left:\n            self.deque.appendleft((header, f))\n        else:\n            self.deque.append((header, f))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an index column. Left justified width is determined by the space needed to print the largest index.", "response": "def append_index(self, num_rows):\n        \"\"\"Add an index column.\n\n        Left justified, width is determined by the space needed to print the\n        largest index.\n        \"\"\"\n        width = len(str(num_rows - 1))\n\n        def f(datum):\n            return str(datum.idx).ljust(width)\n        header = ' '*width\n\n        self.append(header, f)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_sample(self, v, vartype, _left=False):\n        vstr = str(v).rjust(2)  # the variable will be len 0, or 1\n        length = len(vstr)\n\n        if vartype is dimod.SPIN:\n            def f(datum):\n                return _spinstr(datum.sample[v], rjust=length)\n        else:\n            def f(datum):\n                return _binarystr(datum.sample[v], rjust=length)\n\n        self.append(vstr, f, _left=_left)", "response": "Add a sample column to the table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a data vector column.", "response": "def append_vector(self, name, vector, _left=False):\n        \"\"\"Add a data vectors column.\"\"\"\n        if np.issubdtype(vector.dtype, np.integer):\n            # determine the length we need\n            largest = str(max(vector.max(), vector.min(), key=abs))\n            length = max(len(largest), min(7, len(name)))  # how many spaces we need to represent\n\n            if len(name) > length:\n                header = name[:length-1] + '.'\n            else:\n                header = name.rjust(length)\n\n            def f(datum):\n                return str(getattr(datum, name)).rjust(length)\n        elif np.issubdtype(vector.dtype, np.floating):\n            largest = np.format_float_positional(max(vector.max(), vector.min(), key=abs),\n                                                 precision=6, trim='0')\n            length = max(len(largest), min(7, len(name)))  # how many spaces we need to represent\n            if len(name) > length:\n                header = name[:length-1] + '.'\n            else:\n                header = name.rjust(length)\n\n            def f(datum):\n                return np.format_float_positional(getattr(datum, name),\n                                                  precision=6, trim='0',\n                                                  ).rjust(length)\n        else:\n            length = 7\n            if len(name) > length:\n                header = name[:length-1] + '.'\n            else:\n                header = name.rjust(length)\n\n            def f(datum):\n                r = repr(getattr(datum, name))\n                if len(r) > length:\n                    r = r[:length-3] + '...'\n                return r.rjust(length)\n\n        self.append(header, f, _left=_left)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format(self, obj, **kwargs):\n        sio = StringIO()\n        self.fprint(obj, stream=sio, **kwargs)\n        return sio.getvalue()", "response": "Return the formatted representation of the object as a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the formatted representation of the object on stream", "response": "def fprint(self, obj, stream=None, **kwargs):\n        \"\"\"Prints the formatted representation of the object on stream\"\"\"\n        if stream is None:\n            stream = sys.stdout\n\n        options = self.options\n        options.update(kwargs)\n\n        if isinstance(obj, dimod.SampleSet):\n            self._print_sampleset(obj, stream, **options)\n            return\n\n        raise TypeError(\"cannot format type {}\".format(type(obj)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a samples_like object to a NumPy array and list of labels.", "response": "def as_samples(samples_like, dtype=None, copy=False, order='C'):\n    \"\"\"Convert a samples_like object to a NumPy array and list of labels.\n\n    Args:\n        samples_like (samples_like):\n            A collection of raw samples. `samples_like` is an extension of\n            NumPy's array_like_ structure. See examples below.\n\n        dtype (data-type, optional):\n            dtype for the returned samples array. If not provided, it is either\n            derived from `samples_like`, if that object has a dtype, or set to\n            :class:`numpy.int8`.\n\n        copy (bool, optional, default=False):\n            If true, then samples_like is guaranteed to be copied, otherwise\n            it is only copied if necessary.\n\n        order ({'K', 'A', 'C', 'F'}, optional, default='C'):\n            Specify the memory layout of the array. See :func:`numpy.array`.\n\n    Returns:\n        tuple: A 2-tuple containing:\n\n            :obj:`numpy.ndarray`: Samples.\n\n            list: Variable labels\n\n    Examples:\n        The following examples convert a variety of samples_like objects:\n\n        NumPy arrays\n\n        >>> import numpy as np\n        ...\n        >>> dimod.as_samples(np.ones(5, dtype='int8'))\n        (array([[1, 1, 1, 1, 1]], dtype=int8), [0, 1, 2, 3, 4])\n        >>> dimod.as_samples(np.zeros((5, 2), dtype='int8'))\n        (array([[0, 0],\n               [0, 0],\n               [0, 0],\n               [0, 0],\n               [0, 0]], dtype=int8), [0, 1])\n\n        Lists\n\n        >>> dimod.as_samples([-1, +1, -1])\n        (array([[-1,  1, -1]], dtype=int8), [0, 1, 2])\n        >>> dimod.as_samples([[-1], [+1], [-1]])\n        (array([[-1],\n               [ 1],\n               [-1]], dtype=int8), [0])\n\n        Dicts\n\n        >>> dimod.as_samples({'a': 0, 'b': 1, 'c': 0}) # doctest: +SKIP\n        (array([[0, 1, 0]], dtype=int8), ['a', 'b', 'c'])\n        >>> dimod.as_samples([{'a': -1, 'b': +1}, {'a': 1, 'b': 1}]) # doctest: +SKIP\n        (array([[-1,  1],\n                [ 1,  1]], dtype=int8), ['a', 'b'])\n\n        A 2-tuple containing an array_like object and a list of labels\n\n        >>> dimod.as_samples(([-1, +1, -1], ['a', 'b', 'c']))\n        (array([[-1,  1, -1]], dtype=int8), ['a', 'b', 'c'])\n        >>> dimod.as_samples((np.zeros((5, 2), dtype='int8'), ['in', 'out']))\n        (array([[0, 0],\n               [0, 0],\n               [0, 0],\n               [0, 0],\n               [0, 0]], dtype=int8), ['in', 'out'])\n\n    .. _array_like: https://docs.scipy.org/doc/numpy/user/basics.creation.html\n\n    \"\"\"\n    if isinstance(samples_like, SampleSet):\n        # we implicitely support this by handling an iterable of mapping but\n        # it is much faster to just do this here.\n        return samples_like.record.sample, list(samples_like.variables)\n\n    if isinstance(samples_like, tuple) and len(samples_like) == 2:\n        samples_like, labels = samples_like\n\n        if not isinstance(labels, list) and labels is not None:\n            labels = list(labels)\n    else:\n        labels = None\n\n    if isinstance(samples_like, abc.Iterator):\n        # if we don't check this case we can get unexpected behaviour where an\n        # iterator can be depleted\n        raise TypeError('samples_like cannot be an iterator')\n\n    if isinstance(samples_like, abc.Mapping):\n        return as_samples(([samples_like], labels), dtype=dtype)\n\n    if (isinstance(samples_like, list) and samples_like and\n            isinstance(samples_like[0], numbers.Number)):\n        # this is not actually necessary but it speeds up the\n        # samples_like = [1, 0, 1,...] case significantly\n        return as_samples(([samples_like], labels), dtype=dtype)\n\n    if not isinstance(samples_like, np.ndarray):\n        if any(isinstance(sample, abc.Mapping) for sample in samples_like):\n            # go through samples-like, turning the dicts into lists\n            samples_like, old = list(samples_like), samples_like\n\n            if labels is None:\n                first = samples_like[0]\n                if isinstance(first, abc.Mapping):\n                    labels = list(first)\n                else:\n                    labels = list(range(len(first)))\n\n            for idx, sample in enumerate(old):\n                if isinstance(sample, abc.Mapping):\n                    try:\n                        samples_like[idx] = [sample[v] for v in labels]\n                    except KeyError:\n                        raise ValueError(\"samples_like and labels do not match\")\n\n    if dtype is None and not hasattr(samples_like, 'dtype'):\n        dtype = np.int8\n\n    # samples-like should now be array-like\n    arr = np.array(samples_like, dtype=dtype, copy=copy, order=order)\n\n    if arr.ndim > 2:\n        raise ValueError(\"expected samples_like to be <= 2 dimensions\")\n    if arr.ndim < 2:\n        if arr.size:\n            arr = np.atleast_2d(arr)\n        elif labels:  # is not None and len > 0\n            arr = arr.reshape((0, len(labels)))\n        else:\n            arr = arr.reshape((0, 0))\n\n    # ok we're basically done, just need to check against the labels\n    if labels is None:\n        return arr, list(range(arr.shape[1]))\n    elif len(labels) != arr.shape[1]:\n        raise ValueError(\"samples_like and labels dimensions do not match\")\n    else:\n        return arr, labels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine a list of samplesets into a single sample set.", "response": "def concatenate(samplesets, defaults=None):\n    \"\"\"Combine SampleSets.\n\n    Args:\n        samplesets (iterable[:obj:`.SampleSet`):\n            An iterable of sample sets.\n\n        defaults (dict, optional):\n            Dictionary mapping data vector names to the corresponding default values.\n\n    Returns:\n        :obj:`.SampleSet`: A sample set with the same vartype and variable order as the first\n        given in `samplesets`.\n\n    Examples:\n        >>> a = dimod.SampleSet.from_samples(([-1, +1], 'ab'), dimod.SPIN, energy=-1)\n        >>> b = dimod.SampleSet.from_samples(([-1, +1], 'ba'), dimod.SPIN, energy=-1)\n        >>> ab = dimod.concatenate((a, b))\n        >>> ab.record.sample\n        array([[-1,  1],\n               [ 1, -1]], dtype=int8)\n\n    \"\"\"\n\n    itertup = iter(samplesets)\n\n    try:\n        first = next(itertup)\n    except StopIteration:\n        raise ValueError(\"samplesets must contain at least one SampleSet\")\n\n    vartype = first.vartype\n    variables = first.variables\n\n    records = [first.record]\n    records.extend(_iter_records(itertup, vartype, variables))\n\n    # dev note: I was able to get ~2x performance boost when trying to\n    # implement the same functionality here by hand (I didn't know that\n    # this function existed then). However I think it is better to use\n    # numpy's function and rely on their testing etc. If however this becomes\n    # a performance bottleneck in the future, it might be worth changing.\n    record = recfunctions.stack_arrays(records, defaults=defaults,\n                                       asrecarray=True, usemask=False)\n\n    return SampleSet(record, variables, {}, vartype)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_samples(cls, samples_like, vartype, energy, info=None,\n                     num_occurrences=None, aggregate_samples=False,\n                     sort_labels=True, **vectors):\n        \"\"\"Build a :class:`SampleSet` from raw samples.\n\n        Args:\n            samples_like:\n                A collection of raw samples. 'samples_like' is an extension of NumPy's array_like_.\n                See :func:`.as_samples`.\n\n            vartype (:class:`.Vartype`/str/set):\n                Variable type for the :class:`SampleSet`. Accepted input values:\n\n                * :class:`.Vartype.SPIN`, ``'SPIN'``, ``{-1, 1}``\n                * :class:`.Vartype.BINARY`, ``'BINARY'``, ``{0, 1}``\n\n            energy (array_like):\n                Vector of energies.\n\n            info (dict, optional):\n                Information about the :class:`SampleSet` as a whole formatted as a dict.\n\n            num_occurrences (array_like, optional):\n                Number of occurrences for each sample. If not provided, defaults to a vector of 1s.\n\n            aggregate_samples (bool, optional, default=False):\n                If true, returned :obj:`.SampleSet` will have all unique samples.\n\n            sort_labels (bool, optional, default=True):\n                If true, :attr:`.SampleSet.variables` will be in sorted-order.\n                Note that mixed types are not sortable in which case the given\n                order will be maintained.\n\n            **vectors (array_like):\n                Other per-sample data.\n\n        Returns:\n            :obj:`.SampleSet`\n\n        Examples:\n            This example creates a SampleSet out of a samples_like object (a dict).\n\n            >>> import dimod\n            >>> import numpy as np\n            ...\n            >>> dimod.SampleSet.from_samples(dimod.as_samples({'a': 0, 'b': 1, 'c': 0}),\n            ...                              'BINARY', 0)   # doctest: +SKIP\n            SampleSet(rec.array([([0, 1, 0], 0, 1)],\n            ...       dtype=[('sample', 'i1', (3,)), ('energy', '<i4'), ('num_occurrences', '<i4')]),\n            ...       ['a', 'b', 'c'], {}, 'BINARY')\n\n        .. _array_like:  https://docs.scipy.org/doc/numpy/user/basics.creation.html#converting-python-array-like-objects-to-numpy-arrays\n        \"\"\"\n        if aggregate_samples:\n            return cls.from_samples(samples_like, vartype, energy,\n                                    info=info, num_occurrences=num_occurrences,\n                                    aggregate_samples=False,\n                                    **vectors).aggregate()\n\n        # get the samples, variable labels\n        samples, variables = as_samples(samples_like)\n\n        if sort_labels and variables:  # need something to sort\n            try:\n                reindex, new_variables = zip(*sorted(enumerate(variables),\n                                                     key=lambda tup: tup[1]))\n            except TypeError:\n                # unlike types are not sortable in python3, so we do nothing\n                pass\n            else:\n                if new_variables != variables:\n                    # avoid the copy if possible\n                    samples = samples[:, reindex]\n                    variables = new_variables\n\n        num_samples, num_variables = samples.shape\n\n        energy = np.asarray(energy)\n\n        # num_occurrences\n        if num_occurrences is None:\n            num_occurrences = np.ones(num_samples, dtype=int)\n        else:\n            num_occurrences = np.asarray(num_occurrences)\n\n        # now construct the record\n        datatypes = [('sample', samples.dtype, (num_variables,)),\n                     ('energy', energy.dtype),\n                     ('num_occurrences', num_occurrences.dtype)]\n        for key, vector in vectors.items():\n            vectors[key] = vector = np.asarray(vector)\n            datatypes.append((key, vector.dtype, vector.shape[1:]))\n\n        record = np.rec.array(np.zeros(num_samples, dtype=datatypes))\n        record['sample'] = samples\n        record['energy'] = energy\n        record['num_occurrences'] = num_occurrences\n        for key, vector in vectors.items():\n            record[key] = vector\n\n        if info is None:\n            info = {}\n\n        return cls(record, variables, info, vartype)", "response": "Build a new object of class SampleSet from a list of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_samples_bqm(cls, samples_like, bqm, **kwargs):\n        # more performant to do this once, here rather than again in bqm.energies\n        # and in cls.from_samples\n        samples_like = as_samples(samples_like)\n\n        energies = bqm.energies(samples_like)\n\n        return cls.from_samples(samples_like, energy=energies, vartype=bqm.vartype, **kwargs)", "response": "Build a SampleSet from a collection of raw samples using a binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a new instance of the class cls from a concurrent. futures. Future.", "response": "def from_future(cls, future, result_hook=None):\n        \"\"\"Construct a :class:`SampleSet` referencing the result of a future computation.\n\n        Args:\n            future (object):\n                Object that contains or will contain the information needed to construct a\n                :class:`SampleSet`. If `future` has a :meth:`~concurrent.futures.Future.done` method,\n                this determines the value returned by :meth:`.SampleSet.done`.\n\n            result_hook (callable, optional):\n                A function that is called to resolve the future. Must accept the future and return\n                a :obj:`.SampleSet`. If not provided, set to\n\n                .. code-block:: python\n\n                    def result_hook(future):\n                        return future.result()\n\n        Returns:\n            :obj:`.SampleSet`\n\n        Notes:\n            The future is resolved on the first read of any of the :class:`SampleSet` properties.\n\n        Examples:\n            Run a dimod sampler on a single thread and load the returned future into :class:`SampleSet`.\n\n            >>> import dimod\n            >>> from concurrent.futures import ThreadPoolExecutor\n            ...\n            >>> bqm = dimod.BinaryQuadraticModel.from_ising({}, {('a', 'b'): -1})\n            >>> with ThreadPoolExecutor(max_workers=1) as executor:\n            ...     future = executor.submit(dimod.ExactSolver().sample, bqm)\n            ...     sampleset = dimod.SampleSet.from_future(future)\n            >>> sampleset.record\n            rec.array([([-1, -1], -1., 1), ([ 1, -1],  1., 1), ([ 1,  1], -1., 1),\n                       ([-1,  1],  1., 1)],\n                      dtype=[('sample', 'i1', (2,)), ('energy', '<f8'), ('num_occurrences', '<i8')])\n\n        \"\"\"\n        obj = cls.__new__(cls)\n        obj._future = future\n\n        if result_hook is None:\n            def result_hook(future):\n                return future.result()\n        elif not callable(result_hook):\n            raise TypeError(\"expected result_hook to be callable\")\n\n        obj._result_hook = result_hook\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef first(self):\n        try:\n            return next(self.data(sorted_by='energy', name='Sample'))\n        except StopIteration:\n            raise ValueError('{} is empty'.format(self.__class__.__name__))", "response": "Return the first sample in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if a pending computation is done.", "response": "def done(self):\n        \"\"\"Return True if a pending computation is done.\n\n        Used when a :class:`SampleSet` is constructed with :meth:`SampleSet.from_future`.\n\n        Examples:\n            This example uses a :class:`~concurrent.futures.Future` object directly. Typically\n            a :class:`~concurrent.futures.Executor` sets the result of the future\n            (see documentation for :mod:`concurrent.futures`).\n\n            >>> import dimod\n            >>> from concurrent.futures import Future\n            ...\n            >>> future = Future()\n            >>> sampleset = dimod.SampleSet.from_future(future)\n            >>> future.done()\n            False\n            >>> future.set_result(dimod.ExactSolver().sample_ising({0: -1}, {}))\n            >>> future.done()\n            True\n            >>> sampleset.record.sample\n            array([[-1],\n                   [ 1]], dtype=int8)\n\n        \"\"\"\n        return (not hasattr(self, '_future')) or (not hasattr(self._future, 'done')) or self._future.done()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef samples(self, n=None, sorted_by='energy'):\n        if n is not None:\n            return self.samples(sorted_by=sorted_by)[:n]\n\n        if sorted_by is None:\n            samples = self.record.sample\n        else:\n            order = np.argsort(self.record[sorted_by])\n            samples = self.record.sample[order]\n\n        return SamplesArray(samples, self.variables)", "response": "Return an iterable over the samples of the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data(self, fields=None, sorted_by='energy', name='Sample', reverse=False,\n             sample_dict_cast=True, index=False):\n        \"\"\"Iterate over the data in the :class:`SampleSet`.\n\n        Args:\n            fields (list, optional, default=None):\n                If specified, only these fields are included in the yielded tuples.\n                The special field name 'sample' can be used to view the samples.\n\n            sorted_by (str/None, optional, default='energy'):\n                Selects the record field used to sort the samples. If None, the samples are yielded\n                in record order.\n\n            name (str/None, optional, default='Sample'):\n                Name of the yielded namedtuples or None to yield regular tuples.\n\n            reverse (bool, optional, default=False):\n                If True, yield in reverse order.\n\n            sample_dict_cast (bool, optional, default=False):\n                If True, samples are returned as dicts rather than\n                :class:`.SampleView`. Note that this can lead to very heavy memory\n                usage.\n\n            index (bool, optional, default=False):\n                If True, `datum.idx` gives the corresponding index of the\n                :attr:`.SampleSet.record`.\n\n        Yields:\n            namedtuple/tuple: The data in the :class:`SampleSet`, in the order specified by the input\n            `fields`.\n\n        Examples:\n\n            >>> import dimod\n            ...\n            >>> sampleset = dimod.ExactSolver().sample_ising({'a': -0.5, 'b': 1.0}, {('a', 'b'): -1})\n            >>> for datum in sampleset.data(fields=['sample', 'energy']):   # doctest: +SKIP\n            ...     print(datum)\n            Sample(sample={'a': -1, 'b': -1}, energy=-1.5)\n            Sample(sample={'a': 1, 'b': -1}, energy=-0.5)\n            Sample(sample={'a': 1, 'b': 1}, energy=-0.5)\n            Sample(sample={'a': -1, 'b': 1}, energy=2.5)\n            >>> for energy, in sampleset.data(fields=['energy'], sorted_by='energy'):\n            ...     print(energy)\n            ...\n            -1.5\n            -0.5\n            -0.5\n            2.5\n            >>> print(next(sampleset.data(fields=['energy'], name='ExactSolverSample')))\n            ExactSolverSample(energy=-1.5)\n\n        \"\"\"\n        record = self.record\n\n        if fields is None:\n            # make sure that sample, energy is first\n            fields = self._REQUIRED_FIELDS + [field for field in record.dtype.fields\n                                              if field not in self._REQUIRED_FIELDS]\n            if index:\n                fields.append('idx')\n\n        if sorted_by is None:\n            order = np.arange(len(self))\n        elif index:\n            # we want a stable sort but it can be slower\n            order = np.argsort(record[sorted_by], kind='stable')\n        else:\n            order = np.argsort(record[sorted_by])\n\n        if reverse:\n            order = np.flip(order)\n\n        if name is None:\n            # yielding a tuple\n            def _pack(values):\n                return tuple(values)\n        else:\n            # yielding a named tuple\n            SampleTuple = namedtuple(name, fields)\n\n            def _pack(values):\n                return SampleTuple(*values)\n\n        def _values(idx):\n            for field in fields:\n                if field == 'sample':\n                    sample = SampleView(record.sample[idx, :], self.variables)\n                    if sample_dict_cast:\n                        sample = dict(sample)\n                    yield sample\n                elif field == 'idx':\n                    yield idx\n                else:\n                    yield record[field][idx]\n\n        for idx in order:\n            yield _pack(_values(idx))", "response": "Iterate over the data in the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a shallow copy of the object.", "response": "def copy(self):\n        \"\"\"Create a shallow copy.\"\"\"\n        return self.__class__(self.record.copy(),\n                              self.variables,  # a new one is made in all cases\n                              self.info.copy(),\n                              self.vartype)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_vartype(self, vartype, energy_offset=0.0, inplace=True):\n        if not inplace:\n            return self.copy().change_vartype(vartype, energy_offset, inplace=True)\n\n        if energy_offset:\n            self.record.energy = self.record.energy + energy_offset\n\n        if vartype is self.vartype:\n            return self  # we're done!\n\n        if vartype is Vartype.SPIN and self.vartype is Vartype.BINARY:\n            self.record.sample = 2 * self.record.sample - 1\n            self._vartype = vartype\n        elif vartype is Vartype.BINARY and self.vartype is Vartype.SPIN:\n            self.record.sample = (self.record.sample + 1) // 2\n            self._vartype = vartype\n        else:\n            raise ValueError(\"Cannot convert from {} to {}\".format(self.vartype, vartype))\n\n        return self", "response": "Return a new : class :. SampleSet with the given vartype and the given energy offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef relabel_variables(self, mapping, inplace=True):\n        if not inplace:\n            return self.copy().relabel_variables(mapping, inplace=True)\n\n        self.variables.relabel(mapping)\n        return self", "response": "Relabel the variables of a : class : SampleSet according to the specified mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef aggregate(self):\n        _, indices, inverse = np.unique(self.record.sample, axis=0,\n                                        return_index=True, return_inverse=True)\n\n        # unique also sorts the array which we don't want, so we undo the sort\n        order = np.argsort(indices)\n        indices = indices[order]\n\n        record = self.record[indices]\n\n        # fix the number of occurrences\n        record.num_occurrences = 0\n        for old_idx, new_idx in enumerate(inverse):\n            new_idx = order[new_idx]\n            record[new_idx].num_occurrences += self.record[old_idx].num_occurrences\n\n        # dev note: we don't check the energies as they should be the same\n        # for individual samples\n\n        return type(self)(record, self.variables, copy.deepcopy(self.info),\n                          self.vartype)", "response": "Create a new SampleSet with repeated samples aggregated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new sample set with the given variables and values added.", "response": "def append_variables(self, samples_like, sort_labels=True):\n        \"\"\"Create a new sampleset with the given variables with values added.\n\n        Not defined for empty sample sets. Note that when `sample_like` is\n        a :obj:`.SampleSet`, the data vectors and info are ignored.\n\n        Args:\n            samples_like:\n                Samples to add to the sample set. Should either be a single\n                sample or should match the length of the sample set. See\n                :func:`.as_samples` for what is allowed to be `samples_like`.\n\n            sort_labels (bool, optional, default=True):\n                If true, returned :attr:`.SampleSet.variables` will be in\n                sorted-order. Note that mixed types are not sortable in which\n                case the given order will be maintained.\n\n        Returns:\n            :obj:`.SampleSet`: A new sample set with the variables/values added.\n\n        Examples:\n\n            >>> sampleset = dimod.SampleSet.from_samples([{'a': -1, 'b': +1},\n            ...                                           {'a': +1, 'b': +1}],\n            ...                                          dimod.SPIN,\n            ...                                          energy=[-1.0, 1.0])\n            >>> new = sampleset.append_variables({'c': -1})\n            >>> print(new)\n               a  b  c energy num_oc.\n            0 -1 +1 -1   -1.0       1\n            1 +1 +1 -1    1.0       1\n            ['SPIN', 2 rows, 2 samples, 3 variables]\n\n            Add variables from another sampleset to the original above. Note\n            that the energies do not change.\n\n            >>> another = dimod.SampleSet.from_samples([{'c': -1, 'd': +1},\n            ...                                         {'c': +1, 'd': +1}],\n            ...                                        dimod.SPIN,\n            ...                                        energy=[-2.0, 1.0])\n            >>> new = sampleset.append_variables(another)\n            >>> print(new)\n               a  b  c  d energy num_oc.\n            0 -1 +1 -1 +1   -1.0       1\n            1 +1 +1 +1 +1    1.0       1\n            ['SPIN', 2 rows, 2 samples, 4 variables]\n\n        \"\"\"\n        samples, labels = as_samples(samples_like)\n\n        num_samples = len(self)\n\n        # we don't handle multiple values\n        if samples.shape[0] == num_samples:\n            # we don't need to do anything, it's already the correct shape\n            pass\n        elif samples.shape[0] == 1 and num_samples:\n            samples = np.repeat(samples, num_samples, axis=0)\n        else:\n            msg = (\"mismatched shape. The samples to append should either be \"\n                   \"a single sample or should match the length of the sample \"\n                   \"set. Empty sample sets cannot be appended to.\")\n            raise ValueError(msg)\n\n        # append requires the new variables to be unique\n        variables = self.variables\n        if any(v in variables for v in labels):\n            msg = \"Appended samples cannot contain variables in sample set\"\n            raise ValueError(msg)\n\n        new_variables = list(variables) + labels\n        new_samples = np.hstack((self.record.sample, samples))\n\n        return type(self).from_samples((new_samples, new_variables),\n                                       self.vartype,\n                                       info=copy.deepcopy(self.info),  # make a copy\n                                       sort_labels=sort_labels,\n                                       **self.data_vectors)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new sample set containing the lowest energy possible in the current sample set.", "response": "def lowest(self, rtol=1.e-5, atol=1.e-8):\n        \"\"\"Return a sample set containing the lowest-energy samples.\n\n        A sample is included if its energy is within tolerance of the lowest\n        energy in the sample set. The following equation is used to determine\n        if two values are equivalent:\n\n        absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n        See :func:`numpy.isclose` for additional details and caveats.\n\n        Args:\n            rtol (float, optional, default=1.e-5):\n                The relative tolerance (see above).\n\n            atol (float, optional, default=1.e-8):\n                The absolute tolerance (see above).\n\n        Returns:\n            :obj:`.SampleSet`: A new sample set containing the lowest energy\n            samples as delimited by configured tolerances from the lowest energy\n            sample in the current sample set.\n\n        Examples:\n            >>> sampleset = dimod.ExactSolver().sample_ising({'a': .001},\n            ...                                              {('a', 'b'): -1})\n            >>> print(sampleset.lowest())\n               a  b energy num_oc.\n            0 -1 -1 -1.001       1\n            ['SPIN', 1 rows, 1 samples, 2 variables]\n            >>> print(sampleset.lowest(atol=.1))\n               a  b energy num_oc.\n            0 -1 -1 -1.001       1\n            1 +1 +1 -0.999       1\n            ['SPIN', 2 rows, 2 samples, 2 variables]\n\n        Note:\n            \"Lowest energy\" is the lowest energy in the sample set. This is not\n            always the \"ground energy\" which is the lowest energy possible\n            for a binary quadratic model.\n\n        \"\"\"\n\n        if len(self) == 0:\n            # empty so all are lowest\n            return self.copy()\n\n        record = self.record\n\n        # want all the rows within tolerance of the minimal energy\n        close = np.isclose(record.energy,\n                           np.min(record.energy),\n                           rtol=rtol, atol=atol)\n        record = record[close]\n\n        return type(self)(record, self.variables, copy.deepcopy(self.info),\n                          self.vartype)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_serializable(self, use_bytes=False, bytes_type=bytes):\n        schema_version = \"2.0.0\"\n\n        record = {name: array2bytes(vector)\n                  for name, vector in self.data_vectors.items()}\n        record['sample'] = array2bytes(np.packbits(self.record.sample > 0))\n\n        if not use_bytes:\n            for name in record:\n                record[name] = base64.b64encode(record[name]).decode(\"UTF-8\")\n\n        return {\"basetype\": \"SampleSet\",\n                \"type\": type(self).__name__,\n                \"record\": record,\n                \"sample_dtype\": str(self.record.sample.dtype),  # need this to unpack\n                \"sample_shape\": self.record.sample.shape,  # need this to unpack\n                \"variable_type\": self.vartype.name,\n                \"info\": self.info,\n                \"version\": {\"dimod\": __version__,\n                            \"sampleset_schema\": schema_version},\n                \"variable_labels\": list(self.variables),\n                \"use_bytes\": bool(use_bytes)}", "response": "Convert a : class : SampleSet to a serializable object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_pandas_dataframe(self, sample_column=False):\n        import pandas as pd\n\n        if sample_column:\n            df = pd.DataFrame(self.data(sorted_by=None, sample_dict_cast=True))\n\n        else:\n            # work directly with the record, it's much faster\n            df = pd.DataFrame(self.record.sample, columns=self.variables)\n\n            for field in sorted(self.record.dtype.fields):  # sort for consistency\n                if field == 'sample':\n                    continue\n\n                df.loc[:, field] = self.record[field]\n\n        return df", "response": "Convert a SampleSet to a Pandas DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a penalty satisfaction vector for the given sampleSet and a binary quadratic model object.", "response": "def penalty_satisfaction(response, bqm):\n    \"\"\" Creates a penalty satisfaction list\n\n    Given a sampleSet and a bqm object, will create a binary list informing\n    whether the penalties introduced during degree reduction are satisfied for\n    each sample in sampleSet\n\n    Args:\n        response (:obj:`.SampleSet`): Samples corresponding to provided bqm\n\n        bqm (:obj:`.BinaryQuadraticModel`): a bqm object that contains\n            its reduction info.\n\n    Returns:\n        :obj:`numpy.ndarray`: a binary array of penalty satisfaction information\n\n    \"\"\"\n    record = response.record\n    label_dict = response.variables.index\n\n    if len(bqm.info['reduction']) == 0:\n        return np.array([1] * len(record.sample))\n\n    penalty_vector = np.prod([record.sample[:, label_dict[qi]] *\n                              record.sample[:, label_dict[qj]]\n                              == record.sample[:,\n                                 label_dict[valdict['product']]]\n                              for (qi, qj), valdict in\n                              bqm.info['reduction'].items()], axis=0)\n    return penalty_vector"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef polymorph_response(response, poly, bqm,\n                       penalty_strength=None,\n                       keep_penalty_variables=True,\n                       discard_unsatisfied=False):\n    \"\"\" Transforms the sampleset for the higher order problem.\n\n    Given a response of a penalized HUBO, this function creates a new sampleset\n    object, taking into account penalty information and calculates the\n    energies of samples for the higherorder problem.\n\n    Args:\n        response (:obj:`.SampleSet`): response for a penalized hubo.\n\n        poly (:obj:`.BinaryPolynomial`):\n            A binary polynomial.\n\n        bqm (:obj:`dimod.BinaryQuadraticModel`): Binary quadratic model of the\n            reduced problem.\n\n        penalty_strength (float, optional): default is None, if provided,\n            will be added to the info field of the returned sampleSet object.\n\n        keep_penalty_variables (bool, optional): default is True. if False\n            will remove the variables used for penalty from the samples\n\n        discard_unsatisfied (bool, optional): default is False. If True\n            will discard samples that do not satisfy the penalty conditions.\n\n    Returns:\n        (:obj:`.SampleSet'): A sampleSet object that has additional penalty\n            information. The energies of samples are calculated for the HUBO\n            ignoring the penalty variables.\n\n    \"\"\"\n    record = response.record\n    penalty_vector = penalty_satisfaction(response, bqm)\n    original_variables = bqm.variables\n\n    if discard_unsatisfied:\n        samples_to_keep = list(map(bool, list(penalty_vector)))\n        penalty_vector = np.array([True] * np.sum(samples_to_keep))\n    else:\n        samples_to_keep = list(map(bool, [1] * len(record.sample)))\n\n    samples = record.sample[samples_to_keep]\n    energy_vector = poly.energies((samples, response.variables))\n\n    if not keep_penalty_variables:\n        original_variables = poly.variables\n        idxs = [response.variables.index[v] for v in original_variables]\n        samples = np.asarray(samples[:, idxs])\n\n    num_samples, num_variables = np.shape(samples)\n\n    datatypes = [('sample', np.dtype(np.int8), (num_variables,)),\n                 ('energy', energy_vector.dtype),\n                 ('penalty_satisfaction',\n                  penalty_vector.dtype)]\n    datatypes.extend((name, record[name].dtype, record[name].shape[1:])\n                     for name in record.dtype.names if\n                     name not in {'sample',\n                                  'energy'})\n\n    data = np.rec.array(np.empty(num_samples, dtype=datatypes))\n    data.sample = samples\n    data.energy = energy_vector\n    for name in record.dtype.names:\n        if name not in {'sample', 'energy'}:\n            data[name] = record[name][samples_to_keep]\n\n    data['penalty_satisfaction'] = penalty_vector\n    response.info['reduction'] = bqm.info['reduction']\n    if penalty_strength is not None:\n        response.info['penalty_strength'] = penalty_strength\n    return SampleSet(data, original_variables, response.info,\n                     response.vartype)", "response": "This function takes a response from a Penn Treebank problem and returns a new sampleset object that has the same energy vector as the original sampleset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample_poly(self, poly, penalty_strength=1.0,\n                    keep_penalty_variables=False,\n                    discard_unsatisfied=False, **parameters):\n        \"\"\"Sample from the given binary polynomial.\n\n        Takes the given binary polynomial, introduces penalties, reduces the\n        higher-order problem into a quadratic problem and sends it to its child\n        sampler.\n\n        Args:\n            poly (:obj:`.BinaryPolynomial`):\n                A binary polynomial.\n\n            penalty_strength (float, optional): Strength of the reduction constraint.\n                Insufficient strength can result in the binary quadratic model\n                not having the same minimization as the polynomial.\n\n            keep_penalty_variables (bool, optional): default is True. if False\n                will remove the variables used for penalty from the samples\n\n            discard_unsatisfied (bool, optional): default is False. If True\n                will discard samples that do not satisfy the penalty conditions.\n\n            **parameters: Parameters for the sampling method, specified by\n            the child sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n\n        bqm = make_quadratic(poly, penalty_strength, vartype=poly.vartype)\n        response = self.child.sample(bqm, **parameters)\n\n        return polymorph_response(response, poly, bqm,\n                                  penalty_strength=penalty_strength,\n                                  keep_penalty_variables=keep_penalty_variables,\n                                  discard_unsatisfied=discard_unsatisfied)", "response": "Sample from the given binary polynomial and returns a SampleSet object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_poly(self, poly, scalar=None, bias_range=1, poly_range=None,\n                    ignored_terms=None, **parameters):\n        \"\"\"Scale and sample from the given binary polynomial.\n\n        If scalar is not given, problem is scaled based on bias and polynomial\n        ranges. See :meth:`.BinaryPolynomial.scale` and\n        :meth:`.BinaryPolynomial.normalize`\n\n        Args:\n            poly (obj:`.BinaryPolynomial`): A binary polynomial.\n\n            scalar (number, optional):\n                Value by which to scale the energy range of the binary polynomial.\n\n            bias_range (number/pair, optional, default=1):\n                Value/range by which to normalize the all the biases, or if\n                `poly_range` is provided, just the linear biases.\n\n            poly_range (number/pair, optional):\n                Value/range by which to normalize the higher order biases.\n\n            ignored_terms (iterable, optional):\n                Biases associated with these terms are not scaled.\n\n            **parameters:\n                Other parameters for the sampling method, specified by\n                the child sampler.\n\n        \"\"\"\n\n        if ignored_terms is None:\n            ignored_terms = set()\n        else:\n            ignored_terms = {frozenset(term) for term in ignored_terms}\n\n        # scale and normalize happen in-place so we need to make a copy\n        original, poly = poly, poly.copy()\n\n        if scalar is not None:\n            poly.scale(scalar, ignored_terms=ignored_terms)\n        else:\n            poly.normalize(bias_range=bias_range, poly_range=poly_range,\n                           ignored_terms=ignored_terms)\n\n            # we need to know how much we scaled by, which we can do by looking\n            # at the biases\n            try:\n                v = next(v for v, bias in original.items()\n                         if bias and v not in ignored_terms)\n            except StopIteration:\n                # nothing to scale\n                scalar = 1\n            else:\n                scalar = poly[v] / original[v]\n\n        sampleset = self.child.sample_poly(poly, **parameters)\n\n        if ignored_terms:\n            # we need to recalculate the energy\n            sampleset.record.energy = original.energies((sampleset.record.sample,\n                                                         sampleset.variables))\n        else:\n            sampleset.record.energy /= scalar\n\n        return sampleset", "response": "Scales and samples from a binary polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling from the binary polynomial and truncate output.", "response": "def sample_poly(self, poly, **kwargs):\n        \"\"\"Sample from the binary polynomial and truncate output.\n\n        Args:\n            poly (obj:`.BinaryPolynomial`): A binary polynomial.\n\n            **kwargs:\n                Parameters for the sampling method, specified by the child\n                sampler.\n\n        Returns:\n            :obj:`dimod.SampleSet`\n\n        \"\"\"\n        tkw = self._truncate_kwargs\n        if self._aggregate:\n            return self.child.sample_poly(poly, **kwargs).aggregate().truncate(**tkw)\n        else:\n            return self.child.sample_poly(poly, **kwargs).truncate(**tkw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an iterable of samples where each sample is a dict to a numpy 2d array. Also determines the labels are None.", "response": "def _samples_dicts_to_array(samples_dicts, labels):\n    \"\"\"Convert an iterable of samples where each sample is a dict to a numpy 2d array. Also\n    determines the labels is they are None.\n    \"\"\"\n    itersamples = iter(samples_dicts)\n\n    first_sample = next(itersamples)\n\n    if labels is None:\n        labels = list(first_sample)\n\n    num_variables = len(labels)\n\n    def _iter_samples():\n        yield np.fromiter((first_sample[v] for v in labels),\n                          count=num_variables, dtype=np.int8)\n\n        try:\n            for sample in itersamples:\n                yield np.fromiter((sample[v] for v in labels),\n                                  count=num_variables, dtype=np.int8)\n        except KeyError:\n            msg = (\"Each dict in 'samples' must have the same keys.\")\n            raise ValueError(msg)\n\n    return np.stack(list(_iter_samples())), labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncombines samples and per - sample data into a numpy structured array.", "response": "def data_struct_array(sample, **vectors):  # data_struct_array(sample, *, energy, **vectors):\n    \"\"\"Combine samples and per-sample data into a numpy structured array.\n\n    Args:\n        sample (array_like):\n            Samples, in any form that can be converted into a numpy array.\n\n        energy (array_like, required):\n            Required keyword argument. Energies, in any form that can be converted into a numpy\n            1-dimensional array.\n\n        **kwargs (array_like):\n            Other per-sample data, in any form that can be converted into a numpy array.\n\n    Returns:\n        :obj:`~numpy.ndarray`: A numpy structured array. Has fields ['sample', 'energy', 'num_occurrences', **kwargs]\n\n    \"\"\"\n    if not len(sample):\n        # if samples are empty\n        sample = np.zeros((0, 0), dtype=np.int8)\n    else:\n        sample = np.asarray(sample, dtype=np.int8)\n\n        if sample.ndim < 2:\n            sample = np.expand_dims(sample, 0)\n\n    num_samples, num_variables = sample.shape\n\n    if 'num_occurrences' not in vectors:\n        vectors['num_occurrences'] = [1] * num_samples\n\n    datavectors = {}\n    datatypes = [('sample', np.dtype(np.int8), (num_variables,))]\n\n    for kwarg, vector in vectors.items():\n        dtype = float if kwarg == 'energy' else None\n        datavectors[kwarg] = vector = np.asarray(vector, dtype)\n\n        if len(vector.shape) < 1 or vector.shape[0] != num_samples:\n            msg = ('{} and sample have a mismatched shape {}, {}. They must have the same size '\n                   'in the first axis.').format(kwarg, vector.shape, sample.shape)\n            raise ValueError(msg)\n\n        datatypes.append((kwarg, vector.dtype, vector.shape[1:]))\n\n    if 'energy' not in datavectors:\n        # consistent error with the one thrown in python3\n        raise TypeError('data_struct_array() needs keyword-only argument energy')\n    elif datavectors['energy'].shape != (num_samples,):\n        raise ValueError('energy should be a vector of length {}'.format(num_samples))\n\n    data = np.rec.array(np.zeros(num_samples, dtype=datatypes))\n\n    data['sample'] = sample\n\n    for kwarg, vector in datavectors.items():\n        data[kwarg] = vector\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_samples(cls, samples_like, vectors, info, vartype, variable_labels=None):\n\n        # there is no np.is_array_like so we use a try-except block\n        try:\n            # trying to cast it to int8 rules out list of dictionaries. If we didn't try to cast\n            # then it would just create a vector of np.object\n            samples = np.asarray(samples_like, dtype=np.int8)\n        except TypeError:\n            # if labels are None, they are set here\n            samples, variable_labels = _samples_dicts_to_array(samples_like, variable_labels)\n\n        assert samples.dtype == np.int8, 'sanity check'\n\n        record = data_struct_array(samples, **vectors)\n\n        # if labels are still None, set them here. We could do this in an else in the try-except\n        # block, but the samples-array might not have the correct shape\n        if variable_labels is None:\n            __, num_variables = record.sample.shape\n            variable_labels = list(range(num_variables))\n\n        return cls(record, variable_labels, info, vartype)", "response": "Build a response from a collection of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _valid_config(config, required):\n    '''\n    .. todo:: add documentation of this method\n\n    ``config``: doxygen input we're looking for\n    ``required``: if ``True``, must be present.  if ``False``, NOT ALLOWED to be present\n    '''\n    re_template = r\"\\s*{config}\\s*=.*\".format(config=config)\n    found = re.search(re_template, configs.exhaleDoxygenStdin)\n    if required:\n        return found is not None\n    else:\n        return found is None", "response": "Check if a doxygen config is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the unique identifier for a given object.", "response": "def breathe_identifier(self):\n        \"\"\"\n        The unique identifier for breathe directives.\n\n        .. note::\n\n            This method is currently assumed to only be called for nodes that are\n            in :data:`exhale.utils.LEAF_LIKE_KINDS` (see also\n            :func:`exhale.graph.ExhaleRoot.generateSingleNodeRST` where it is used).\n\n        **Return**\n\n            :class:`python:str`\n                Usually, this will just be ``self.name``.  However, for functions in\n                particular the signature must be included to distinguish overloads.\n        \"\"\"\n        if self.kind == \"function\":\n            # TODO: breathe bug with templates and overloads, don't know what to do...\n            return \"{name}({parameters})\".format(\n                name=self.name,\n                parameters=\", \".join(self.parameters)\n            )\n\n        return self.name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef full_signature(self):\n        if self.kind == \"function\":\n            return \"{template}{return_type} {name}({parameters})\".format(\n                template=\"template <{0}> \".format(\", \".join(self.template)) if self.template else \"\",\n                return_type=self.return_type,\n                name=self.name,\n                parameters=\", \".join(self.parameters)\n            )\n        raise RuntimeError(\n            \"full_signature may only be called for a 'function', but {name} is a '{kind}' node.\".format(\n                name=self.name, kind=self.kind\n            )\n        )", "response": "Returns the full signature of a function node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef templateParametersStringAsRestList(self, nodeByRefid):\n        '''\n        .. todo::\n\n           document this, create another method for creating this without the need for\n           generating links, to be used in making the node titles and labels\n        '''\n        if not self.template_params:\n            return None\n        else:\n            param_stream = StringIO()\n            for param_t, decl_n, def_n in self.template_params:\n                refid, typeid = param_t\n                # Say you wanted a custom link text 'custom', and somewhere\n                # else you had an internal link '.. _some_link:'.  Then you do\n                #     `custom <some_link_>`_\n                # LOL. RST is confusing\n                if refid:\n                    # Easy case: the refid is something Exhale is explicitly documenting\n                    if refid in nodeByRefid:\n                        link = \"{0}_\".format(nodeByRefid[refid].link_name)\n                    else:\n                        # It's going to get generated by Breathe down the line, we need\n                        # to reference the page the directive will appear on.\n                        parent_refid = \"\"\n                        for key in nodeByRefid:\n                            if len(key) > len(parent_refid) and key in refid:\n                                parent_refid = key\n                        parent = nodeByRefid[parent_refid]\n                        parent_page = os.path.basename(parent.file_name.replace(\".rst\", \".html\"))\n                        link = \"{page}#{refid}\".format(page=parent_page, refid=refid)\n                    param_stream.write(\n                        \"#. `{typeid} <{link}>`_\".format(\n                            typeid=typeid,\n                            # Not necessarily an ExhaleNode link, should be a link by\n                            # the time Breathe is finished?\n                            link=link\n                        )\n                    )\n                    close_please = False\n                else:\n                    param_stream.write(\"#. ``{typeid}\".format(typeid=typeid))\n                    close_please = True\n\n                # The type is in there, but when parsed it may have given something like\n                # `class X` for the typeid (meaning nothing else to write).  For others,\n                # the decl_n is the declared name of the template parameter.  E.g. it\n                # was parsed as `typeid <- class` and `decl_n <- X`.\n                if decl_n:\n                    param_stream.write(\" \")\n                    if not close_please:\n                        param_stream.write(\"``\")\n                    param_stream.write(\"{decl_n}\".format(decl_n=decl_n))\n                    close_please = True\n\n                # When templates provide a default value, `def_n` is it.  When parsed,\n                # if the `decl_n` and `def_n` are the same, `def_n` is explicitly set\n                # to be None.\n                if def_n:\n                    param_stream.write(\" \")\n                    if not close_please:\n                        param_stream.write(\"``\")\n                    param_stream.write(\"= {def_n}``\".format(def_n=def_n))\n                    close_please = True\n\n                if close_please:\n                    param_stream.write(\"``\")\n\n                param_stream.write(\"\\n\")\n\n            param_stream.write(\"\\n\")\n            param_value = param_stream.getvalue()\n            param_stream.close()\n            return param_value", "response": "This method generates a string of the template parameters for the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef baseOrDerivedListString(self, lst, nodeByRefid):\n        '''\n        .. todo:: long time from now: intersphinx should be possible here\n        '''\n        # lst should either be self.base_compounds or self.derived_compounds\n        if not lst:\n            return None\n\n        bod_stream = StringIO()\n        for prot, refid, string in lst:\n            bod_stream.write(\"- \")\n\n            # Include the prototype\n            if prot:\n                bod_stream.write(\"``{0}\".format(prot))\n                please_close = True\n            else:\n                please_close = False\n\n            # Create the link, if possible\n            # TODO: how to do intersphinx links here?\n            if refid:\n                # TODO: why are these links not working????????????????????????????????\n                ###########flake8breaks :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/ :/\n                # if please_close:\n                #     bod_stream.write(\"`` \")  # close prototype\n                # bod_stream.write(\"`{name} <{link}_>`_\".format(\n                #     # name=string.replace(\"<\", \"&gt;\").replace(\">\", \"&lt;\"),\n                #     name=string.replace(\"<\", \"\").replace(\">\", \"\"),\n                #     link=nodeByRefid[refid].link_name\n                # ))\n                if not please_close:\n                    bod_stream.write(\"``\")\n                else:\n                    bod_stream.write(\" \")\n                bod_stream.write(\"{string}`` (:ref:`{link}`)\".format(\n                    string=string,\n                    link=nodeByRefid[refid].link_name\n                ))\n            else:\n                if not please_close:\n                    bod_stream.write(\"``\")\n                else:\n                    bod_stream.write(\" \")\n                bod_stream.write(\"{0}``\".format(string))\n            bod_stream.write(\"\\n\")\n\n        bod_value = bod_stream.getvalue()\n        bod_stream.close()\n        return bod_value", "response": "Returns a string representation of the list of base or derived components."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findNestedNamespaces(self, lst):\n        '''\n        Recursive helper function for finding nested namespaces.  If this node is a\n        namespace node, it is appended to ``lst``.  Each node also calls each of its\n        child ``findNestedNamespaces`` with the same list.\n\n        :Parameters:\n            ``lst`` (list)\n                The list each namespace node is to be appended to.\n        '''\n        if self.kind == \"namespace\":\n            lst.append(self)\n        for c in self.children:\n            c.findNestedNamespaces(lst)", "response": "Recursive helper function for finding nested namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findNestedDirectories(self, lst):\n        '''\n        Recursive helper function for finding nested directories.  If this node is a\n        directory node, it is appended to ``lst``.  Each node also calls each of its\n        child ``findNestedDirectories`` with the same list.\n\n        :Parameters:\n            ``lst`` (list)\n                The list each directory node is to be appended to.\n        '''\n        if self.kind == \"dir\":\n            lst.append(self)\n        for c in self.children:\n            c.findNestedDirectories(lst)", "response": "Recursive helper function for finding nested directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findNestedClassLike(self, lst):\n        '''\n        Recursive helper function for finding nested classes and structs.  If this node\n        is a class or struct, it is appended to ``lst``.  Each node also calls each of\n        its child ``findNestedClassLike`` with the same list.\n\n        :Parameters:\n            ``lst`` (list)\n                The list each class or struct node is to be appended to.\n        '''\n        if self.kind == \"class\" or self.kind == \"struct\":\n            lst.append(self)\n        for c in self.children:\n            c.findNestedClassLike(lst)", "response": "Recursive helper function for finding nested classes and structs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toConsole(self, level, fmt_spec, printChildren=True):\n        '''\n        Debugging tool for printing hierarchies / ownership to the console.  Recursively\n        calls children ``toConsole`` if this node is not a directory or a file, and\n        ``printChildren == True``.\n\n        .. todo:: fmt_spec docs needed. keys are ``kind`` and values are color spec\n\n        :Parameters:\n            ``level`` (int)\n                The indentation level to be used, should be greater than or equal to 0.\n\n            ``printChildren`` (bool)\n                Whether or not the ``toConsole`` method for the children found in\n                ``self.children`` should be called with ``level+1``.  Default is True,\n                set to False for directories and files.\n        '''\n        indent = \"  \" * level\n        utils.verbose_log(\"{indent}- [{kind}]: {name}\".format(\n            indent=indent,\n            kind=utils._use_color(self.kind, fmt_spec[self.kind], sys.stderr),\n            name=self.name\n        ))\n        # files are children of directories, the file section will print those children\n        if self.kind == \"dir\":\n            for c in self.children:\n                c.toConsole(level + 1, fmt_spec, printChildren=False)\n        elif printChildren:\n            if self.kind == \"file\":\n                next_indent = \"  \" * (level + 1)\n                utils.verbose_log(\"{next_indent}[[[ location=\\\"{loc}\\\" ]]]\".format(\n                    next_indent=next_indent,\n                    loc=self.location\n                ))\n                for incl in self.includes:\n                    utils.verbose_log(\"{next_indent}- #include <{incl}>\".format(\n                        next_indent=next_indent,\n                        incl=incl\n                    ))\n                for ref, name in self.included_by:\n                    utils.verbose_log(\"{next_indent}- included by: [{name}]\".format(\n                        next_indent=next_indent,\n                        name=name\n                    ))\n                for n in self.namespaces_used:\n                    n.toConsole(level + 1, fmt_spec, printChildren=False)\n                for c in self.children:\n                    c.toConsole(level + 1, fmt_spec)\n            elif self.kind == \"class\" or self.kind == \"struct\":\n                relevant_children = []\n                for c in self.children:\n                    if c.kind == \"class\" or c.kind == \"struct\" or \\\n                       c.kind == \"enum\"  or c.kind == \"union\":\n                        relevant_children.append(c)\n\n                for rc in sorted(relevant_children):\n                    rc.toConsole(level + 1, fmt_spec)\n            elif self.kind != \"union\":\n                for c in self.children:\n                    c.toConsole(level + 1, fmt_spec)", "response": "This method prints the contents of this node to the console."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsorts the children of this object in place and has each child sort its own children.", "response": "def typeSort(self):\n        '''\n        Sorts ``self.children`` in place, and has each child sort its own children.\n        Refer to :func:`~exhale.graph.ExhaleRoot.deepSortList` for more information on\n        when this is necessary.\n        '''\n        self.children.sort()\n        for c in self.children:\n            c.typeSort()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if this node is included in the class view hierarchy.", "response": "def inClassHierarchy(self):\n        '''\n        Whether or not this node should be included in the class view hierarchy.  Helper\n        method for :func:`~exhale.graph.ExhaleNode.toHierarchy`.  Sets the member\n        variable ``self.in_class_hierarchy`` to True if appropriate.\n\n        :Return (bool):\n            True if this node should be included in the class view --- either it is a\n            node of kind ``struct``, ``class``, ``enum``, ``union``, or it is a\n            ``namespace`` that one or more if its descendants was one of the previous\n            four kinds.  Returns False otherwise.\n        '''\n        if self.kind == \"namespace\":\n            for c in self.children:\n                if c.inClassHierarchy():\n                    return True\n            return False\n        else:\n            # flag that this node is already in the class view so we can find the\n            # missing top level nodes at the end\n            self.in_class_hierarchy = True\n\n            # Skip children whose names were requested to be explicitly ignored.\n            for exclude in configs._compiled_listing_exclude:\n                if exclude.match(self.name):\n                    return False\n\n            return self.kind in {\"struct\", \"class\", \"enum\", \"union\"}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if this node should be included in the file view hierarchy.", "response": "def inFileHierarchy(self):\n        '''\n        Whether or not this node should be included in the file view hierarchy.  Helper\n        method for :func:`~exhale.graph.ExhaleNode.toHierarchy`.  Sets the member\n        variable ``self.in_file_hierarchy`` to True if appropriate.\n\n        :Return (bool):\n            True if this node should be included in the file view --- either it is a\n            node of kind ``file``, or it is a ``dir`` that one or more if its\n            descendants was a ``file``.  Returns False otherwise.\n        '''\n        if self.kind == \"file\":\n            # flag that this file is already in the directory view so that potential\n            # missing files can be found later.\n            self.in_file_hierarchy = True\n            return True\n        elif self.kind == \"dir\":\n            for c in self.children:\n                if c.inFileHierarchy():\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toHierarchy(self, classView, level, stream, lastChild=False):\n        '''\n        **Parameters**\n            ``classView`` (bool)\n                ``True`` if generating the Class Hierarchy, ``False`` for File Hierarchy.\n\n            ``level`` (int)\n                Recursion level used to determine indentation.\n\n            ``stream`` (StringIO)\n                The stream to write the contents to.\n\n            ``lastChild`` (bool)\n                When :data:`~exhale.configs.createTreeView` is ``True`` and\n                :data:`~exhale.configs.treeViewIsBootstrap` is ``False``, the generated\n                HTML ``li`` elements need to add a ``class=\"lastChild\"`` to use the\n                appropriate styling.\n\n        .. todo:: add thorough documentation of this\n        '''\n        if self.inHierarchy(classView):\n            # For the Tree Views, we need to know if there are nested children before\n            # writing anything.  If there are, we need to open a new list\n            nested_children = self.hierarchySortedDirectDescendants(classView)\n\n            ############################################################################\n            # Write out this node.                                                     #\n            ############################################################################\n            # Easy case: just write another bullet point\n            if not configs.createTreeView:\n                stream.write(\"{indent}- :ref:`{link}`\\n\".format(\n                    indent='    ' * level,\n                    link=self.link_name\n                ))\n            # Otherwise, we're generating some raw HTML and/or JavaScript depending on\n            # whether we are using bootstrap or not\n            else:\n                # Declare the relevant links needed for the Tree Views\n                indent = \"  \" * (level * 2)\n                next_indent = \"  {0}\".format(indent)\n\n                # turn double underscores into underscores, then underscores into hyphens\n                html_link = self.link_name.replace(\"__\", \"_\").replace(\"_\", \"-\")\n                href = \"{file}.html#{anchor}\".format(\n                    file=self.file_name.rsplit(\".rst\", 1)[0],\n                    anchor=html_link\n                )\n\n                # should always have at least two parts (templates will have more)\n                title_as_link_parts = self.title.split(\" \")\n                if self.template_params:\n                    # E.g. 'Template Class Foo'\n                    q_start = 0\n                    q_end   = 2\n                else:\n                    # E.g. 'Class Foo'\n                    q_start = 0\n                    q_end   = 1\n                # the qualifier will not be part of the hyperlink (for clarity of\n                # navigation), the link_title will be\n                qualifier   = \" \".join(title_as_link_parts[q_start:q_end])\n                link_title  = \" \".join(title_as_link_parts[q_end:])\n                link_title  = link_title.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n                # the actual text / link inside of the list item\n                li_text     = '{qualifier} <a href=\"{href}\">{link_title}</a>'.format(\n                    qualifier=qualifier,\n                    href=href,\n                    link_title=link_title\n                )\n\n                if configs.treeViewIsBootstrap:\n                    text = \"text: \\\"<span class=\\\\\\\"{span_cls}\\\\\\\">{qualifier}</span> {link_title}\\\"\".format(\n                        span_cls=configs.treeViewBootstrapTextSpanClass,\n                        qualifier=qualifier,\n                        link_title=link_title\n                    )\n                    link = \"href: \\\"{href}\\\"\".format(href=href)\n                    # write some json data, something like\n                    #     {\n                    #         text: \"<span class=\\\\\\\"text-muted\\\\\\\"> some text\",\n                    #         href: \"link to actual item\",\n                    #         selectable: false,\n                    stream.write(\"{indent}{{\\n{next_indent}{text},\\n\".format(\n                        indent=indent,\n                        next_indent=next_indent,\n                        text=text\n                    ))\n                    stream.write(\"{next_indent}{link},\\n{next_indent}selectable: false,\\n\".format(\n                        next_indent=next_indent,\n                        link=link\n                    ))\n                    # if requested, add the badge indicating how many children there are\n                    # only add this if there are children\n                    if configs.treeViewBootstrapUseBadgeTags and nested_children:\n                        stream.write(\"{next_indent}tags: ['{num_children}'],\\n\".format(\n                            next_indent=next_indent,\n                            num_children=len(nested_children)\n                        ))\n\n                    if nested_children:\n                        # If there are children then `nodes: [ ... ]` will be next\n                        stream.write(\"\\n{next_indent}nodes: [\\n\".format(next_indent=next_indent))\n                    else:\n                        # Otherwise, this element is ending.  JavaScript doesn't care\n                        # about trailing commas :)\n                        stream.write(\"{indent}}},\\n\".format(indent=indent))\n                else:\n                    if lastChild:\n                        opening_li = '<li class=\"lastChild\">'\n                    else:\n                        opening_li = \"<li>\"\n\n                    if nested_children:\n                        # write this list element and begin the next list\n                        # writes something like\n                        #     <li>\n                        #         some text with an href\n                        #         <ul>\n                        #\n                        # the <ul> started here gets closed below\n                        stream.write(\"{indent}{li}\\n{next_indent}{li_text}\\n{next_indent}<ul>\\n\".format(\n                            indent=indent,\n                            li=opening_li,\n                            next_indent=next_indent,\n                            li_text=li_text\n                        ))\n                    else:\n                        # write this list element and end it now (since no children)\n                        # writes something like\n                        #    <li>\n                        #        some text with an href\n                        #    </li>\n                        stream.write(\"{indent}{li}{li_text}</li>\\n\".format(\n                            indent=indent,\n                            li=opening_li,\n                            li_text=li_text\n                        ))\n\n            ############################################################################\n            # Write out all of the children (if there are any).                        #\n            ############################################################################\n            last_child_index = len(nested_children) - 1\n            child_idx        = 0\n            for child in nested_children:\n                child.toHierarchy(classView, level + 1, stream, child_idx == last_child_index)\n                child_idx += 1\n\n            ############################################################################\n            # If there were children, close the lists we started above.                #\n            ############################################################################\n            if configs.createTreeView and nested_children:\n                if configs.treeViewIsBootstrap:\n                    # close the `nodes: [ ... ]` and final } for element\n                    # the final comma IS necessary, and extra commas don't matter in javascript\n                    stream.write(\"{next_indent}]\\n{indent}}},\\n\".format(\n                        next_indent=next_indent,\n                        indent=indent\n                    ))\n                else:\n                    stream.write(\"{next_indent}</ul>\\n{indent}</li>\\n\".format(\n                        next_indent=next_indent,\n                        indent=indent\n                    ))", "response": "Writes out the contents of this node to the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trackNodeIfUnseen(self, node):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.discoverAllNodes`.  If the node\n        is not in self.all_nodes yet, add it to both self.all_nodes as well as the\n        corresponding ``self.<breathe_kind>`` list.\n\n        :Parameters:\n            ``node`` (ExhaleNode)\n                The node to begin tracking if not already present.\n        '''\n        if node not in self.all_nodes:\n            self.all_nodes.append(node)\n            self.node_by_refid[node.refid] = node\n            if node.kind == \"class\" or node.kind == \"struct\":\n                self.class_like.append(node)\n            elif node.kind == \"namespace\":\n                self.namespaces.append(node)\n            elif node.kind == \"enum\":\n                self.enums.append(node)\n            elif node.kind == \"enumvalue\":\n                self.enum_values.append(node)\n            elif node.kind == \"define\":\n                self.defines.append(node)\n            elif node.kind == \"file\":\n                self.files.append(node)\n            elif node.kind == \"dir\":\n                self.dirs.append(node)\n            elif node.kind == \"function\":\n                self.functions.append(node)\n            elif node.kind == \"variable\":\n                self.variables.append(node)\n            elif node.kind == \"group\":\n                self.groups.append(node)\n            elif node.kind == \"typedef\":\n                self.typedefs.append(node)\n            elif node.kind == \"union\":\n                self.unions.append(node)", "response": "Adds the given node to the internal list of all nodes if it is not already in the internal list of all breathe types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reparentAll(self):\n        '''\n        Fixes some of the parental relationships lost in parsing the Breathe graph.\n        File relationships are recovered in\n        :func:`~exhale.graph.ExhaleRoot.fileRefDiscovery`.  This method simply calls in\n        this order:\n\n        1. :func:`~exhale.graph.ExhaleRoot.reparentUnions`\n        2. :func:`~exhale.graph.ExhaleRoot.reparentClassLike`\n        3. :func:`~exhale.graph.ExhaleRoot.reparentDirectories`\n        4. :func:`~exhale.graph.ExhaleRoot.renameToNamespaceScopes`\n        5. :func:`~exhale.graph.ExhaleRoot.reparentNamespaces`\n        '''\n        self.reparentUnions()\n        self.reparentClassLike()\n        self.reparentDirectories()\n        self.renameToNamespaceScopes()\n        self.reparentNamespaces()\n\n        # make sure all children lists are unique (no duplicate children)\n        for node in self.all_nodes:\n            node.children = list(set(node.children))", "response": "Reparent all parental relationships in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reparentUnions(self):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.reparentAll`.  Namespaces and\n        classes should have the unions defined in them to be in the child list of itself\n        rather than floating around.  Union nodes that are reparented (e.g. a union\n        defined in a class) will be removed from the list ``self.unions`` since the\n        Breathe directive for its parent (e.g. the class) will include the documentation\n        for the union.  The consequence of this is that a union defined in a class will\n        **not** appear in the full api listing of Unions.\n        '''\n        # unions declared in a class will not link to the individual union page, so\n        # we will instead elect to remove these from the list of unions\n        removals = []\n        for u in self.unions:\n            parts = u.name.split(\"::\")\n            if len(parts) >= 2:\n                # TODO: nested unions are not supported right now...\n                parent_name = \"::\".join(p for p in parts[:-1])\n                reparented  = False\n                # see if the name matches any potential parents\n                for node in itertools.chain(self.class_like, self.namespaces):\n                    if node.name == parent_name:\n                        node.children.append(u)\n                        u.parent = node\n                        reparented = True\n                        break\n                # if not reparented, try the namespaces\n                if reparented:\n                    removals.append(u)\n                else:\n                    # << verboseBuild\n                    utils.verbose_log(\n                        \"The union {0} has '::' in its name, but no parent was found!\".format(u.name),\n                        utils.AnsiColors.BOLD_RED\n                    )\n\n        # remove the unions from self.unions that were declared in class_like objects\n        for rm in removals:\n            self.unions.remove(rm)", "response": "Reparent all unions of a class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrename all the children of this node to the corresponding namespace scopes.", "response": "def renameToNamespaceScopes(self):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.reparentAll`. Some compounds in\n        Breathe such as functions and variables do not have the namespace name they are\n        declared in before the name of the actual compound.  This method prepends the\n        appropriate (nested) namespace name before the name of any child that does not\n        already have it.\n\n        For example, the variable ``MAX_DEPTH`` declared in namespace ``external`` would\n        have its ExhaleNode's ``name`` attribute changed from ``MAX_DEPTH`` to\n        ``external::MAX_DEPTH``.\n        '''\n        for n in self.namespaces:\n            namespace_name = \"{0}::\".format(n.name)\n            for child in n.children:\n                if namespace_name not in child.name:\n                    child.name = \"{0}{1}\".format(namespace_name, child.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fileRefDiscovery(self):\n        '''\n        Finds the missing components for file nodes by parsing the Doxygen xml (which is\n        just the ``doxygen_output_dir/node.refid``).  Additional items parsed include\n        adding items whose ``refid`` tag are used in this file, the <programlisting> for\n        the file, what it includes and what includes it, as well as the location of the\n        file (with respsect to the *Doxygen* root).\n\n        Care must be taken to only include a refid found with specific tags.  The\n        parsing of the xml file was done by just looking at some example outputs.  It\n        seems to be working correctly, but there may be some subtle use cases that break\n        it.\n\n        .. warning::\n            Some enums, classes, variables, etc declared in the file will not have their\n            associated refid in the declaration of the file, but will be present in the\n            <programlisting>.  These are added to the files' list of children when they\n            are found, but this parental relationship cannot be formed if you set\n            ``XML_PROGRAMLISTING = NO`` with Doxygen.  An example of such an enum would\n            be an enum declared inside of a namespace within this file.\n        '''\n        if not os.path.isdir(configs._doxygen_xml_output_directory):\n            utils.fancyError(\"The doxygen xml output directory [{0}] is not valid!\".format(\n                configs._doxygen_xml_output_directory\n            ))\n\n        # parse the doxygen xml file and extract all refid's put in it\n        # keys: file object, values: list of refid's\n        doxygen_xml_file_ownerships = {}\n        # innerclass, innernamespace, etc\n        ref_regex    = re.compile(r'.*<inner.*refid=\"(\\w+)\".*')\n        # what files this file includes\n        inc_regex    = re.compile(r'.*<includes.*>(.+)</includes>')\n        # what files include this file\n        inc_by_regex = re.compile(r'.*<includedby refid=\"(\\w+)\".*>(.*)</includedby>')\n        # the actual location of the file\n        loc_regex    = re.compile(r'.*<location file=\"(.*)\"/>')\n\n        for f in self.files:\n            doxygen_xml_file_ownerships[f] = []\n            try:\n                doxy_xml_path = os.path.join(configs._doxygen_xml_output_directory, \"{0}.xml\".format(f.refid))\n                with codecs.open(doxy_xml_path, \"r\", \"utf-8\") as doxy_file:\n                    processing_code_listing = False  # shows up at bottom of xml\n                    for line in doxy_file:\n                        # see if this line represents the location tag\n                        match = loc_regex.match(line)\n                        if match is not None:\n                            f.location = os.path.normpath(match.groups()[0])\n                            continue\n\n                        if not processing_code_listing:\n                            # gather included by references\n                            match = inc_by_regex.match(line)\n                            if match is not None:\n                                ref, name = match.groups()\n                                f.included_by.append((ref, name))\n                                continue\n                            # gather includes lines\n                            match = inc_regex.match(line)\n                            if match is not None:\n                                inc = match.groups()[0]\n                                f.includes.append(inc)\n                                continue\n                            # gather any classes, namespaces, etc declared in the file\n                            match = ref_regex.match(line)\n                            if match is not None:\n                                match_refid = match.groups()[0]\n                                if match_refid in self.node_by_refid:\n                                    doxygen_xml_file_ownerships[f].append(match_refid)\n                                continue\n                            # lastly, see if we are starting the code listing\n                            if \"<programlisting>\" in line:\n                                processing_code_listing = True\n                        elif processing_code_listing:\n                            if \"</programlisting>\" in line:\n                                processing_code_listing = False\n                            else:\n                                f.program_listing.append(line)\n            except:\n                utils.fancyError(\n                    \"Unable to process doxygen xml for file [{0}].\\n\".format(f.name)\n                )\n\n        #\n        # IMPORTANT: do not set the parent field of anything being added as a child to the file\n        #\n\n        # hack to make things work right on RTD\n        # TODO: do this at construction rather than as a post process!\n        if configs.doxygenStripFromPath is not None:\n            for node in itertools.chain(self.files, self.dirs):\n                if node.kind == \"file\":\n                    manip = node.location\n                else:  # node.kind == \"dir\"\n                    manip = node.name\n\n                abs_strip_path = os.path.normpath(os.path.abspath(\n                    configs.doxygenStripFromPath\n                ))\n                if manip.startswith(abs_strip_path):\n                    manip = os.path.relpath(manip, abs_strip_path)\n\n                manip = os.path.normpath(manip)\n                if node.kind == \"file\":\n                    node.location = manip\n                else:  # node.kind == \"dir\"\n                    node.name = manip\n\n        # now that we have parsed all the listed refid's in the doxygen xml, reparent\n        # the nodes that we care about\n        allowable_child_kinds = [\"struct\", \"class\", \"function\", \"typedef\", \"define\", \"enum\", \"union\"]\n        for f in self.files:\n            for match_refid in doxygen_xml_file_ownerships[f]:\n                child = self.node_by_refid[match_refid]\n                if child.kind in allowable_child_kinds:\n                    if child not in f.children:\n                        f.children.append(child)\n                elif child.kind == \"namespace\":\n                    if child not in f.namespaces_used:\n                        f.namespaces_used.append(child)\n\n        # last but not least, some different kinds declared in the file that are scoped\n        # in a namespace they will show up in the programlisting, but not at the toplevel.\n        for f in self.files:\n            potential_orphans = []\n            for n in f.namespaces_used:\n                for child in n.children:\n                    if child.kind == \"enum\"     or child.kind == \"variable\" or \\\n                       child.kind == \"function\" or child.kind == \"typedef\"  or \\\n                       child.kind == \"union\":\n                        potential_orphans.append(child)\n\n            # now that we have a list of potential orphans, see if this doxygen xml had\n            # the refid of a given child present.\n            for orphan in potential_orphans:\n                unresolved_name = orphan.name.split(\"::\")[-1]\n                if f.refid in orphan.refid and any(unresolved_name in line for line in f.program_listing):\n                    if orphan not in f.children:\n                        f.children.append(orphan)\n\n        # Last but not least, make sure all children know where they were defined.\n        for f in self.files:\n            for child in f.children:\n                if child.def_in_file is None:\n                    child.def_in_file = f\n                elif child.def_in_file != f:\n                    # << verboseBuild\n                    utils.verbose_log(\n                        \"Conflicting file definition for [{0}]: both [{1}] and [{2}] found.\".format(\n                            child.name, child.def_in_file.name, f.name\n                        ),\n                        utils.AnsiColors.BOLD_RED\n                    )", "response": "This method is used to find the missing components for file nodes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filePostProcess(self):\n        '''\n        The real name of this method should be ``reparentFiles``, but to avoid confusion\n        with what stage this must happen at it is called this instead.  After the\n        :func:`~exhale.graph.ExhaleRoot.fileRefDiscovery` method has been called, each\n        file will have its location parsed.  This method reparents files to directories\n        accordingly, so the file view hierarchy can be complete.\n        '''\n        # directories are already reparented, traverse the children and get a flattened\n        # list of all directories. previously, all directories should have had their\n        # names adjusted to remove a potentially leading path separator\n        nodes_remaining = [d for d in self.dirs]\n        all_directories = []\n        while len(nodes_remaining) > 0:\n            d = nodes_remaining.pop()\n            all_directories.append(d)\n            for child in d.children:\n                if child.kind == \"dir\":\n                    nodes_remaining.append(child)\n\n        all_directories.sort()\n\n        for f in self.files:\n            if not f.location:\n                sys.stderr.write(utils.critical(\n                    \"Cannot reparent file [{0}] because it's location was not discovered.\\n\".format(\n                        f.name\n                    )\n                ))\n                continue\n            elif os.sep not in f.location:\n                # top-level file, cannot parent do a directory\n                utils.verbose_log(\n                    \"### File [{0}] with location [{1}] was identified as being at the top level\".format(\n                        f.name, f.location\n                    ),\n                    utils.AnsiColors.BOLD_YELLOW\n                )\n                continue\n\n            dirname = os.path.dirname(f.location)\n            found = False\n            for d in all_directories:\n                if dirname == d.name:\n                    d.children.append(f)\n                    f.parent = d\n                    found = True\n                    break\n\n            if not found:\n                sys.stderr.write(utils.critical(\n                    \"Could not find directory parent of file [{0}] with location [{1}].\\n\".format(\n                        f.name, f.location\n                    )\n                ))", "response": "This method is called when the file is parsed and the file view hierarchy is complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches file and namespace node XML contents for function signatures.", "response": "def parseFunctionSignatures(self):\n        \"\"\"Search file and namespace node XML contents for function signatures.\"\"\"\n        # Keys: string refid of either namespace or file nodes\n        # Values: list of function objects that should be defined there\n        parent_to_func = {}\n        for func in self.functions:\n            # Case 1: it is a function inside a namespace, the function information\n            # is in the namespace's XML file.\n            if func.parent:\n                parent_refid = None\n                if func.parent.kind == \"namespace\":\n                    parent_refid = func.parent.refid\n                else:\n                    raise RuntimeError(textwrap.dedent('''\n                        Function [{0}] with refid=[{1}] had a parent of kind '{2}':\n                        Parent name=[{3}], refid=[{4}].\n\n                        Functions may only have namespace parents.  Please report this\n                        issue online, Exhale has a parsing error.\n                    '''.format(func.name, func.refid, func.parent.name, func.parent.refid)))\n            # Case 2: top-level function, it's information is in the file node's XML.\n            elif func.def_in_file:\n                parent_refid = func.def_in_file.refid\n            else:\n                utils.verbose_log(utils.critical(\n                    \"Cannot parse function [{0}] signature, refid=[{2}], no parent/def_in_file found!\".format(\n                        func.name, func.refid\n                    )\n                ))\n\n            # If we found a suitable parent refid, gather in parent_to_func.\n            if parent_refid:\n                if parent_refid not in parent_to_func:\n                    parent_to_func[parent_refid] = []\n                parent_to_func[parent_refid].append(func)\n\n        # Now we have a mapping of all defining elements to where the function\n        # signatures _should_ live.\n        # TODO: setwise comparison / report when children vs parent_to_func[refid] differ?\n        for refid in parent_to_func:\n            parent = self.node_by_refid[refid]\n            parent_contents = utils.nodeCompoundXMLContents(parent)\n            if not parent_contents:\n                continue  ############flake8efphase: TODO: error, log?\n\n            try:\n                parent_soup = BeautifulSoup(parent_contents, \"lxml-xml\")\n            except:\n                continue\n\n            cdef = parent_soup.doxygen.compounddef\n            func_section = None\n            for section in cdef.find_all(\"sectiondef\", recursive=False):\n                if \"kind\" in section.attrs and section.attrs[\"kind\"] == \"func\":\n                    func_section = section\n                    break\n\n            if not func_section:\n                continue############flake8efphase: TODO: error, log?\n\n            functions = parent_to_func[refid]\n            for memberdef in func_section.find_all(\"memberdef\", recursive=False):\n                if \"kind\" not in memberdef.attrs or memberdef.attrs[\"kind\"] != \"function\":\n                    continue\n\n                func_refid = memberdef.attrs[\"id\"]\n                func = None\n                for candidate in functions:\n                    if candidate.refid == func_refid:\n                        func = candidate\n                        break\n\n                if not func:\n                    continue ############flake8efphase: TODO: error, log?\n                functions.remove(func)\n\n                # At last, we can actually parse the function signature\n                # 1. The function return type.\n                func.return_type = utils.sanitize(\n                    memberdef.find(\"type\", recursive=False).text\n                )\n                # 2. The function parameter list.\n                parameters = []\n                for param in memberdef.find_all(\"param\", recursive=False):\n                    parameters.append(param.type.text)\n                func.parameters = utils.sanitize_all(parameters)\n                # 3. The template parameter list.\n                templateparamlist = memberdef.templateparamlist\n                if templateparamlist:\n                    template = []\n                    for param in templateparamlist.find_all(\"param\", recursive=False):\n                        template.append(param.type.text)\n                    func.template = utils.sanitize_all(template)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort all internal lists.", "response": "def sortInternals(self):\n        '''\n        Sort all internal lists (``class_like``, ``namespaces``, ``variables``, etc)\n        mostly how doxygen would, alphabetical but also hierarchical (e.g. structs\n        appear before classes in listings).  Some internal lists are just sorted, and\n        some are deep sorted (:func:`~exhale.graph.ExhaleRoot.deepSortList`).\n        '''\n        # some of the lists only need to be sorted, some of them need to be sorted and\n        # have each node sort its children\n        # leaf-like lists: no child sort\n        self.defines.sort()\n        self.enums.sort()\n        self.enum_values.sort()\n        self.functions.sort()\n        self.groups.sort()\n        self.typedefs.sort()\n        self.variables.sort()\n\n        # hierarchical lists: sort children\n        self.deepSortList(self.class_like)\n        self.deepSortList(self.namespaces)\n        self.deepSortList(self.unions)\n        self.deepSortList(self.files)\n        self.deepSortList(self.dirs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generateAPIRootHeader(self):\n        '''\n        This method creates the root library api file that will include all of the\n        different hierarchy views and full api listing.  If ``self.root_directory`` is\n        not a current directory, it is created first.  Afterward, the root API file is\n        created and its title is written, as well as the value of\n        ``configs.afterTitleDescription``.\n        '''\n        try:\n            if not os.path.isdir(self.root_directory):\n                os.mkdir(self.root_directory)\n        except:\n            utils.fancyError(\n                \"Cannot create the directory: {0}\".format(self.root_directory)\n            )\n        try:\n            with codecs.open(self.full_root_file_path, \"w\", \"utf-8\") as generated_index:\n                # Add the metadata if they requested it\n                if configs.pageLevelConfigMeta:\n                    generated_index.write(\"{0}\\n\\n\".format(configs.pageLevelConfigMeta))\n\n                generated_index.write(textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=configs.rootFileTitle,\n                    heading_mark=utils.heading_mark(\n                        configs.rootFileTitle,\n                        configs.SECTION_HEADING_CHAR\n                    )\n                )))\n\n                if configs.afterTitleDescription:\n                    generated_index.write(\"\\n{0}\\n\\n\".format(configs.afterTitleDescription))\n        except:\n            utils.fancyError(\n                \"Unable to create the root api file / header: {0}\".format(self.full_root_file_path)\n            )", "response": "This method creates the root library api file that will include all of the hierarchy views and full api listing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating all of the reStructuredText documents related to types parsed by the base class.", "response": "def generateNodeDocuments(self):\n        '''\n        Creates all of the reStructuredText documents related to types parsed by\n        Doxygen.  This includes all leaf-like documents (``class``, ``struct``,\n        ``enum``, ``typedef``, ``union``, ``variable``, and ``define``), as well as\n        namespace, file, and directory pages.\n\n        During the reparenting phase of the parsing process, nested items were added as\n        a child to their actual parent.  For classes, structs, enums, and unions, if\n        it was reparented to a ``namespace`` it will *remain* in its respective\n        ``self.<breathe_kind>`` list.  However, if it was an internally declared child\n        of a class or struct (nested classes, structs, enums, and unions), this node\n        will be removed from its ``self.<breathe_kind>`` list to avoid duplication in\n        the class hierarchy generation.\n\n        When generating the full API, though, we will want to include all of these and\n        therefore must call :func:`~exhale.graph.ExhaleRoot.generateSingleNodeRST` with\n        all of the nested items.  For nested classes and structs, this is done by just\n        calling ``node.findNestedClassLike`` for every node in ``self.class_like``.  The\n        resulting list then has all of ``self.class_like``, as well as any nested\n        classes and structs found.  With ``enum`` and ``union``, these would have been\n        reparented to a **class** or **struct** if it was removed from the relevant\n        ``self.<breathe_kind>`` list.  Meaning we must make sure that we genererate the\n        single node RST documents for everything by finding the nested enums and unions\n        from ``self.class_like``, as well as everything in ``self.enums`` and\n        ``self.unions``.\n        '''\n        # initialize all of the nodes first\n        for node in self.all_nodes:\n            self.initializeNodeFilenameAndLink(node)\n\n        self.adjustFunctionTitles()\n\n        # now that all potential ``node.link_name`` members are initialized, generate\n        # the leaf-like documents\n        for node in self.all_nodes:\n            if node.kind in utils.LEAF_LIKE_KINDS:\n                self.generateSingleNodeRST(node)\n\n        # generate the remaining parent-like documents\n        self.generateNamespaceNodeDocuments()\n        self.generateFileNodeDocuments()\n        self.generateDirectoryNodeDocuments()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initializeNodeFilenameAndLink(self, node):\n        '''\n        Sets the ``file_name`` and ``link_name`` for the specified node.  If the kind\n        of this node is \"file\", then this method will also set the ``program_file``\n        as well as the ``program_link_name`` fields.\n\n        Since we are operating inside of a ``containmentFolder``, this method **will**\n        include ``self.root_directory`` in this path so that you can just use::\n\n            with codecs.open(node.file_name, \"w\", \"utf-8\") as gen_file:\n                # ... write the file ...\n\n        Having the ``containmentFolder`` is important for when we want to generate the\n        file, but when we want to use it with ``include`` or ``toctree`` this will\n        need to change.  Refer to\n        :func:`~exhale.graph.ExhaleRoot.gerrymanderNodeFilenames`.\n\n        This method also sets the value of ``node.title``, which will be used in both\n        the reStructuredText document of the node as well as the links generated in the\n        class view hierarchy (<a href=\"...\"> for the ``createTreeView = True`` option).\n\n        :type:  exhale.graph.ExhaleNode\n        :param: node\n            The node that we are setting the above information for.\n        '''\n        # Flag for title special treatment at end.\n        template_special = False\n\n        # Special cases: directories and files do not have an equivalent C++ domain\n        # construct in Sphinx, as well as Exhale does not use the corresponding Breathe\n        # directive for these compounds.  Similarly, Exhale does not use the Breathe\n        # namespace directive.  As such, where possible, the filename should be as\n        # human-friendly has possible so that users can conveniently link to the\n        # internal Exhal ref's using e.g. :ref:`file_dir_subdir_filename.h`.\n        SPECIAL_CASES = [\"dir\", \"file\", \"namespace\"]\n        if node.kind in SPECIAL_CASES:\n            if node.kind == \"file\":\n                unique_id = node.location\n            else:\n                unique_id = node.name\n\n            unique_id = unique_id.replace(\":\", \"_\").replace(os.sep, \"_\").replace(\" \", \"_\")\n            if node.kind == \"namespace\":\n                title = node.name.split(\"::\")[-1]\n            else:\n                title = os.path.basename(unique_id.replace(\"_\", os.sep))\n        else:\n            unique_id = node.refid\n\n            # special treatment for templates\n            first_lt = node.name.find(\"<\")\n            last_gt  = node.name.rfind(\">\")\n            # dealing with a template when this is true\n            if first_lt > -1 and last_gt > -1:\n                # NOTE: this has to happen for partial / full template specializations\n                #       When specializations occur, the \"<param1, param2>\" etc show up\n                #       in `node.name`.\n                template_special = True\n                #flake8failhere\n                # TODO: when specializations occur, can you find a way to link to them\n                # in the title?  Issue: nested templates prevent splitting on ','\n                title = \"{cls}{templates}\".format(\n                    cls=node.name[:first_lt].split(\"::\")[-1],  # remove namespaces\n                    templates=node.name[first_lt:last_gt + 1]  # template params\n                )\n            else:\n                title = node.name.split(\"::\")[-1]\n\n            # additionally, I feel that nested classes should have their fully qualified\n            # name without namespaces for clarity\n            prepend_parent = False\n            if node.kind in [\"class\", \"struct\", \"enum\", \"union\"]:\n                if node.parent is not None and node.parent.kind in [\"class\", \"struct\"]:\n                    prepend_parent = True\n            if prepend_parent:\n                title = \"{parent}::{child}\".format(\n                    parent=node.parent.name.split(\"::\")[-1],\n                    child=title\n                )\n\n        # `unique_id` and `title` should be set approriately for all nodes by this point\n        if node.kind in SPECIAL_CASES:\n            node.link_name = \"{kind}_{id}\".format(kind=node.kind, id=unique_id)\n            node.file_name = \"{link_name}.rst\".format(link_name=node.link_name)\n        else:\n            # The node.link_name is the internal reference for exhale to link to in the\n            # library API listing.  We cannot use unique_id in \"non-special-cases\"\n            # because that will be the Doxygen refid, which Breathe will use as the\n            # actual documentation ref.  This link_name is an anchor point to the top\n            # of the page, but it cannot be a duplicate.\n            #\n            # Lastly, the Doxygen refid _may_ have the kind in it (e.g., a class or\n            # struct), but also may _not_ (e.g., a function is a hash appended to the\n            # file that defined it).  So a little bit of trickery is used to make sure\n            # that the generated filename is at least _somewhat_ understandable for a\n            # human to know what it is documenting (or at least its kind...).\n            node.link_name = \"exhale_{kind}_{id}\".format(kind=node.kind, id=unique_id)\n            if unique_id.startswith(node.kind):\n                node.file_name = \"{id}.rst\".format(id=unique_id)\n            else:\n                node.file_name = \"{kind}_{id}.rst\".format(kind=node.kind, id=unique_id)\n\n        # Make sure this file can always be generated.  We do not need to change the\n        # node.link_name, just make sure the file being written to is OK.\n        if len(node.file_name) >= configs.MAXIMUM_FILENAME_LENGTH:\n            # hashlib.sha1 will produce a length 40 string.\n            node.file_name = \"{kind}_{sha1}.rst\".format(\n                kind=node.kind, sha1=hashlib.sha1(node.link_name.encode()).hexdigest()\n            )\n\n        # Create the program listing internal link and filename.\n        if node.kind == \"file\":\n            node.program_link_name = \"program_listing_{link_name}\".format(link_name=node.link_name)\n            node.program_file = \"{pl_link_name}.rst\".format(pl_link_name=node.program_link_name)\n\n            # Adding a 'program_listing_' prefix may have made this too long.  If so,\n            # change both node.file_name and node.program_file for consistency.\n            if len(node.program_file) >= configs.MAXIMUM_FILENAME_LENGTH:\n                sha1 = hashlib.sha1(node.link_name.encode()).hexdigest()\n                node.file_name = \"{kind}_{sha1}.rst\".format(kind=node.kind, sha1=sha1)\n                node.program_file = \"program_listing_{file_name}\".format(file_name=node.file_name)\n\n        # Now force everything in the containment folder\n        for attr in [\"file_name\", \"program_file\"]:\n            if hasattr(node, attr):\n                full_path = os.path.join(self.root_directory, getattr(node, attr))\n                if platform.system() == \"Windows\" and len(full_path) >= configs.MAXIMUM_WINDOWS_PATH_LENGTH:\n                    # NOTE: self.root_directory is *ALREADY* an absolute path, this\n                    #       prefix requires absolute paths!  See documentation for\n                    #       configs.MAXIMUM_WINDOWS_PATH_LENGTH.\n                    full_path = \"{magic}{full_path}\".format(\n                        magic=\"{slash}{slash}?{slash}\".format(slash=\"\\\\\"),  # \\\\?\\ I HATE YOU WINDOWS\n                        full_path=full_path\n                    )\n                setattr(node, attr, full_path)\n\n        #flake8failhereplease: add a test with decltype!\n        # account for decltype(&T::var) etc, could be in name or template params\n        # html_safe_name = html_safe_name.replace(\"&\", \"_AMP_\").replace(\"*\", \"_STAR_\")\n        # html_safe_name = html_safe_name.replace(\"(\", \"_LPAREN_\").replace(\")\", \"_RPAREN_\")\n        # html_safe_name = html_safe_name.replace(\"<\", \"_LT_\").replace(\">\", \"_GT_\").replace(\",\", \"_COMMA_\")\n\n        # breathe does not prepend the namespace for variables and typedefs, so\n        # I choose to leave the fully qualified name in the title for added clarity\n        if node.kind in [\"variable\", \"typedef\"]:\n            title = node.name\n\n        # Last but not least, set the title for the page to be generated.\n        node.title = \"{kind} {title}\".format(\n            kind=utils.qualifyKind(node.kind),\n            title=title\n        )\n        if node.template_params or template_special:\n            node.title = \"Template {title}\".format(title=node.title)", "response": "Initialize the file name and link name for the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a single node in the archive.", "response": "def generateSingleNodeRST(self, node):\n        '''\n        Creates the reStructuredText document for the leaf like node object.\n\n        It is **assumed** that the specified ``node.kind`` is in\n        :data:`~exhale.utils.LEAF_LIKE_KINDS`.  File, directory, and namespace nodes are\n        treated separately.\n\n        :Parameters:\n            ``node`` (ExhaleNode)\n                The leaf like node being generated by this method.\n        '''\n        try:\n            with codecs.open(node.file_name, \"w\", \"utf-8\") as gen_file:\n                ########################################################################\n                # Page header / linking.                                               #\n                ########################################################################\n                # generate a link label for every generated file\n                link_declaration = \".. _{0}:\".format(node.link_name)\n\n                # acquire the file this was originally defined in\n                if node.def_in_file:\n                    defined_in = \"- Defined in :ref:`{where}`\".format(where=node.def_in_file.link_name)\n                else:\n                    defined_in = \".. did not find file this was defined in\"\n                    sys.stderr.write(utils.critical(\n                        \"Did not locate file that defined {0} [{1}]; no link generated.\\n\".format(node.kind,\n                                                                                                  node.name)\n                    ))\n\n                # Add the metadata if they requested it\n                if configs.pageLevelConfigMeta:\n                    gen_file.write(\"{0}\\n\\n\".format(configs.pageLevelConfigMeta))\n\n                gen_file.write(textwrap.dedent('''\\\n                    {link}\n\n                    {heading}\n                    {heading_mark}\n\n                    {defined_in}\n\n                '''.format(\n                    link=link_declaration,\n                    heading=node.title,\n                    heading_mark=utils.heading_mark(\n                        node.title,\n                        configs.SECTION_HEADING_CHAR\n                    ),\n                    defined_in=defined_in\n                )))\n\n                contents = utils.contentsDirectiveOrNone(node.kind)\n                if contents:\n                    gen_file.write(contents)\n\n                ########################################################################\n                # Nested relationships.                                                #\n                ########################################################################\n                # link to outer types if this node is a nested type\n                if node.parent and (node.parent.kind == \"struct\" or node.parent.kind == \"class\"):\n                    nested_type_of = \"This {kind} is a nested type of :ref:`{parent}`.\".format(\n                        kind=node.kind,\n                        parent=node.parent.link_name\n                    )\n                else:\n                    nested_type_of = None\n\n                # if this has nested types, link to them\n                nested_defs = None\n                if node.kind == \"class\" or node.kind == \"struct\":\n                    nested_children = []\n                    for c in node.children:\n                        c.findNestedEnums(nested_children)\n                        c.findNestedUnions(nested_children)\n                        c.findNestedClassLike(nested_children)\n\n                    if nested_children:\n                        # build up a list of links, custom sort function will force\n                        # double nested and beyond to appear after their parent by\n                        # sorting on their name\n                        nested_children.sort(key=lambda x: x.name)\n                        nested_child_stream = StringIO()\n                        for nc in nested_children:\n                            nested_child_stream.write(\"- :ref:`{0}`\\n\".format(nc.link_name))\n\n                        # extract the list of links and add them as a subsection in the header\n                        nested_child_string = nested_child_stream.getvalue()\n                        nested_child_stream.close()\n                        heading = \"Nested Types\"\n                        nested_defs = textwrap.dedent('''\n                            {heading}\n                            {heading_mark}\n\n                        '''.format(\n                            heading=heading,\n                            heading_mark=utils.heading_mark(\n                                heading,\n                                configs.SUB_SUB_SECTION_HEADING_CHAR\n                            )\n                        ))\n                        nested_defs = \"{0}{1}\\n\".format(nested_defs, nested_child_string)\n\n                if nested_type_of or nested_defs:\n                    heading = \"Nested Relationships\"\n                    gen_file.write(textwrap.dedent('''\n                        {heading}\n                        {heading_mark}\n\n                    '''.format(\n                        heading=heading,\n                        heading_mark=utils.heading_mark(\n                            heading,\n                            configs.SUB_SECTION_HEADING_CHAR\n                        )\n                    )))\n                    if nested_type_of:\n                        gen_file.write(\"{0}\\n\\n\".format(nested_type_of))\n                    if nested_defs:\n                        gen_file.write(nested_defs)\n\n                ########################################################################\n                # Inheritance relationships.                                           #\n                ########################################################################\n                ##### remove this duplicated nonsense someday\n                if node.base_compounds or node.derived_compounds:\n                    heading = \"Inheritance Relationships\"\n                    gen_file.write(textwrap.dedent('''\n                        {heading}\n                        {heading_mark}\n                    '''.format(\n                        heading=heading,\n                        heading_mark=utils.heading_mark(\n                            heading,\n                            configs.SUB_SECTION_HEADING_CHAR\n                        )\n                    )))\n                    if node.base_compounds:\n                        if len(node.base_compounds) == 1:\n                            title = \"Base Type\"\n                        else:\n                            title = \"Base Types\"\n\n                        gen_file.write(textwrap.dedent('''\n                            {heading}\n                            {heading_mark}\n\n                        '''.format(\n                            heading=title,\n                            heading_mark=utils.heading_mark(\n                                title,\n                                configs.SUB_SUB_SECTION_HEADING_CHAR\n                            )\n                        )))\n                        gen_file.write(\"{0}\\n\".format(node.baseOrDerivedListString(\n                            node.base_compounds, self.node_by_refid\n                        )))\n                    if node.derived_compounds:\n                        if len(node.derived_compounds) == 1:\n                            title = \"Derived Type\"\n                        else:\n                            title = \"Derived Types\"\n                        gen_file.write(textwrap.dedent('''\n                            {heading}\n                            {heading_mark}\n\n                        '''.format(\n                            heading=title,\n                            heading_mark=utils.heading_mark(\n                                title,\n                                configs.SUB_SUB_SECTION_HEADING_CHAR\n                            )\n                        )))\n                        gen_file.write(\"{0}\\n\".format(node.baseOrDerivedListString(\n                            node.derived_compounds, self.node_by_refid\n                        )))\n\n                ########################################################################\n                # Template parameter listing.                                          #\n                ########################################################################\n                if configs.includeTemplateParamOrderList:\n                    template = node.templateParametersStringAsRestList(self.node_by_refid)\n                    if template:\n                        heading = \"Template Parameter Order\"\n                        gen_file.write(textwrap.dedent('''\n                            {heading}\n                            {heading_mark}\n\n                        '''.format(\n                            heading=heading,\n                            heading_mark=utils.heading_mark(\n                                heading,\n                                configs.SUB_SECTION_HEADING_CHAR\n                            )\n                        )))\n\n                        gen_file.write(\"{template_params}\\n\\n\".format(template_params=template))\n\n                        # << verboseBuild\n                        utils.verbose_log(\n                            \"+++ {kind} {name} has usable template parameters:\\n{params}\".format(\n                                kind=node.kind,\n                                name=node.name,\n                                params=utils.prefix(\"    \", template)\n                            ),\n                            utils.AnsiColors.BOLD_CYAN\n                        )\n\n                ########################################################################\n                # The Breathe directive!!!                                             #\n                ########################################################################\n                heading = \"{kind} Documentation\".format(kind=utils.qualifyKind(node.kind))\n                gen_file.write(textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=heading,\n                    heading_mark=utils.heading_mark(\n                        heading,\n                        configs.SUB_SECTION_HEADING_CHAR\n                    )\n                )))\n                # inject the appropriate doxygen directive and name of this node\n                directive = \".. {directive}:: {breathe_identifier}\".format(\n                    directive=utils.kindAsBreatheDirective(node.kind),\n                    breathe_identifier=node.breathe_identifier()\n                )\n                gen_file.write(\"\\n{directive}\\n\".format(directive=directive))\n                # include any specific directives for this doxygen directive\n                specifications = utils.prefix(\n                    \"   \",\n                    \"\\n\".join(spec for spec in utils.specificationsForKind(node.kind))\n                )\n                gen_file.write(specifications)\n        except:\n            utils.fancyError(\n                \"Critical error while generating the file for [{0}].\".format(node.file_name)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the reStructuredText document for every namespace including nested namespaces that were removed from self. namespaces and added as children namespaces that were added as children .", "response": "def generateNamespaceNodeDocuments(self):\n        '''\n        Generates the reStructuredText document for every namespace, including nested\n        namespaces that were removed from ``self.namespaces`` (but added as children\n        to one of the namespaces in ``self.namespaces``).\n\n        The documents generated do not use the Breathe namespace directive, but instead\n        link to the relevant documents associated with this namespace.\n        '''\n        # go through all of the top level namespaces\n        for n in self.namespaces:\n            # find any nested namespaces\n            nested_namespaces = []\n            for child in n.children:\n                child.findNestedNamespaces(nested_namespaces)\n            # generate the children first\n            for nested in reversed(sorted(nested_namespaces)):\n                self.generateSingleNamespace(nested)\n            # generate this top level namespace\n            self.generateSingleNamespace(n)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a reStructuredText document for a single namespace node.", "response": "def generateSingleNamespace(self, nspace):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.generateNamespaceNodeDocuments`.\n        Writes the reStructuredText file for the given namespace.\n\n        :Parameters:\n            ``nspace`` (ExhaleNode)\n                The namespace node to create the reStructuredText document for.\n        '''\n        try:\n            with codecs.open(nspace.file_name, \"w\", \"utf-8\") as gen_file:\n                # Add the metadata if they requested it\n                if configs.pageLevelConfigMeta:\n                    gen_file.write(\"{0}\\n\\n\".format(configs.pageLevelConfigMeta))\n\n                nspace.title = \"{0} {1}\".format(utils.qualifyKind(nspace.kind), nspace.name)\n\n                # generate a link label for every generated file\n                gen_file.write(textwrap.dedent('''\n                    .. _{link}:\n\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    link=nspace.link_name,\n                    heading=nspace.title,\n                    heading_mark=utils.heading_mark(nspace.title, configs.SECTION_HEADING_CHAR)\n                )))\n\n                brief, detailed = parse.getBriefAndDetailedRST(self, nspace)\n                if brief:\n                    gen_file.write(\"{0}\\n\\n\".format(brief))\n\n                # include the contents directive if requested\n                contents = utils.contentsDirectiveOrNone(nspace.kind)\n                if contents:\n                    gen_file.write(\"{0}\\n\\n\".format(contents))\n\n                if detailed:\n                    gen_file.write(\"{0}\\n\\n\".format(detailed))\n\n                # generate the headings and links for the children\n                children_string = self.generateNamespaceChildrenString(nspace)\n                gen_file.write(children_string)\n        except:\n            utils.fancyError(\n                \"Critical error while generating the file for [{0}]\".format(nspace.file_name)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generateNamespaceChildrenString(self, nspace):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.generateSingleNamespace`, and\n        :func:`~exhale.graph.ExhaleRoot.generateFileNodeDocuments`.  Builds the\n        body text for the namespace node document that links to all of the child\n        namespaces, structs, classes, functions, typedefs, unions, and variables\n        associated with this namespace.\n\n        :Parameters:\n            ``nspace`` (ExhaleNode)\n                The namespace node we are generating the body text for.\n\n        :Return (str):\n            The string to be written to the namespace node's reStructuredText document.\n        '''\n        # sort the children\n        nsp_namespaces        = []\n        nsp_nested_class_like = []\n        nsp_enums             = []\n        nsp_functions         = []\n        nsp_typedefs          = []\n        nsp_unions            = []\n        nsp_variables         = []\n        for child in nspace.children:\n            # Skip children whose names were requested to be explicitly ignored.\n            should_exclude = False\n            for exclude in configs._compiled_listing_exclude:\n                if exclude.match(child.name):\n                    should_exclude = True\n            if should_exclude:\n                continue\n\n            if child.kind == \"namespace\":\n                nsp_namespaces.append(child)\n            elif child.kind == \"struct\" or child.kind == \"class\":\n                child.findNestedClassLike(nsp_nested_class_like)\n                child.findNestedEnums(nsp_enums)\n                child.findNestedUnions(nsp_unions)\n            elif child.kind == \"enum\":\n                nsp_enums.append(child)\n            elif child.kind == \"function\":\n                nsp_functions.append(child)\n            elif child.kind == \"typedef\":\n                nsp_typedefs.append(child)\n            elif child.kind == \"union\":\n                nsp_unions.append(child)\n            elif child.kind == \"variable\":\n                nsp_variables.append(child)\n\n        # generate their headings if they exist (no Defines...that's not a C++ thing...)\n        children_stream = StringIO()\n        self.generateSortedChildListString(children_stream, \"Namespaces\", nsp_namespaces)\n        self.generateSortedChildListString(children_stream, \"Classes\", nsp_nested_class_like)\n        self.generateSortedChildListString(children_stream, \"Enums\", nsp_enums)\n        self.generateSortedChildListString(children_stream, \"Functions\", nsp_functions)\n        self.generateSortedChildListString(children_stream, \"Typedefs\", nsp_typedefs)\n        self.generateSortedChildListString(children_stream, \"Unions\", nsp_unions)\n        self.generateSortedChildListString(children_stream, \"Variables\", nsp_variables)\n        # read out the buffer contents, close it and return the desired string\n        children_string = children_stream.getvalue()\n        children_stream.close()\n        return children_string", "response": "Generates the string that contains all of the children of the namespace node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the reStructuredText documents for all files and the file s node identifiers.", "response": "def generateFileNodeDocuments(self):\n        '''\n        Generates the reStructuredText documents for files as well as the file's\n        program listing reStructuredText document if applicable.  Refer to\n        :ref:`usage_customizing_file_pages` for changing the output of this method.\n        The remainder of the file lists all nodes that have been discovered to be\n        defined (e.g. classes) or referred to (e.g. included files or files that include\n        this file).\n\n        .. todo::\n\n           writing the actual file should be set in one method so that things for files,\n           namespaces, and leaflike nodes don't keep getting out of sync\n        '''\n        for f in self.files:\n            # if the programlisting was included, length will be at least 1 line\n            if len(f.program_listing) > 0:\n                include_program_listing = True\n                lexer = utils.doxygenLanguageToPygmentsLexer(f.location, f.language)\n                full_program_listing = '.. code-block:: {0}\\n\\n'.format(lexer)\n\n                # need to reformat each line to remove xml tags / put <>& back in\n                for pgf_line in f.program_listing:\n                    fixed_whitespace = re.sub(r'<sp/>', ' ', pgf_line)\n                    # for our purposes, this is good enough:\n                    #     http://stackoverflow.com/a/4869782/3814202\n                    no_xml_tags  = re.sub(r'<[^<]+?>', '', fixed_whitespace)\n                    revive_lt    = re.sub(r'&lt;', '<', no_xml_tags)\n                    revive_gt    = re.sub(r'&gt;', '>', revive_lt)\n                    revive_quote = re.sub(r'&quot;', '\"', revive_gt)\n                    revive_apos  = re.sub(r'&apos;', \"'\", revive_quote)\n                    revive_amp   = re.sub(r'&amp;', '&', revive_apos)\n                    full_program_listing = \"{}   {}\".format(full_program_listing, revive_amp)\n\n                # create the programlisting file\n                try:\n                    with codecs.open(f.program_file, \"w\", \"utf-8\") as gen_file:\n                        # Add the metadata if they requested it\n                        if configs.pageLevelConfigMeta:\n                            gen_file.write(\"{0}\\n\\n\".format(configs.pageLevelConfigMeta))\n\n                        # generate a link label for every generated file\n                        link_declaration = \".. _{}:\".format(f.program_link_name)\n                        # every generated file must have a header for sphinx to be happy\n                        prog_title = \"Program Listing for {} {}\".format(utils.qualifyKind(f.kind), f.name)\n                        gen_file.write(textwrap.dedent('''\n                            {link}\n\n                            {heading}\n                            {heading_mark}\n\n                            |exhale_lsh| :ref:`Return to documentation for file <{file}>` (``{location}``)\n\n                            .. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS\n\n                        '''.format(\n                            link=link_declaration,\n                            heading=prog_title,\n                            heading_mark=utils.heading_mark(\n                                prog_title,\n                                configs.SECTION_HEADING_CHAR\n                            ),\n                            file=f.link_name,\n                            location=f.location\n                        )))\n                        gen_file.write(full_program_listing)\n                except:\n                    utils.fancyError(\n                        \"Critical error while generating the file for [{0}]\".format(f.file_name)\n                    )\n            else:\n                include_program_listing = False\n\n        for f in self.files:\n            if len(f.location) > 0:\n                heading = \"Definition (``{where}``)\".format(where=f.location)\n                file_definition = textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=heading,\n                    heading_mark=utils.heading_mark(\n                        heading,\n                        configs.SUB_SECTION_HEADING_CHAR\n                    )\n                ))\n            else:\n                file_definition = \"\"\n\n            if include_program_listing and file_definition != \"\":\n                prog_file_definition = textwrap.dedent('''\n                    .. toctree::\n                       :maxdepth: 1\n\n                       {prog_link}\n                '''.format(prog_link=os.path.basename(f.program_file)))\n                file_definition = \"{}{}\".format(file_definition, prog_file_definition)\n\n            if len(f.includes) > 0:\n                file_includes_stream = StringIO()\n                heading = \"Includes\"\n                file_includes_stream.write(textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=heading,\n                    heading_mark=utils.heading_mark(\n                        heading,\n                        configs.SUB_SECTION_HEADING_CHAR\n                    )\n                )))\n                for incl in sorted(f.includes):\n                    local_file = None\n                    for incl_file in self.files:\n                        if incl in incl_file.location:\n                            local_file = incl_file\n                            break\n                    if local_file is not None:\n                        file_includes_stream.write(textwrap.dedent('''\n                            - ``{include}`` (:ref:`{link}`)\n                        '''.format(include=incl, link=local_file.link_name)))\n                    else:\n                        file_includes_stream.write(textwrap.dedent('''\n                            - ``{include}``\n                        '''.format(include=incl)))\n\n                file_includes = file_includes_stream.getvalue()\n                file_includes_stream.close()\n            else:\n                file_includes = \"\"\n\n            if len(f.included_by) > 0:\n                file_included_by_stream = StringIO()\n                heading = \"Included By\"\n                file_included_by_stream.write(textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=heading,\n                    heading_mark=utils.heading_mark(\n                        heading,\n                        configs.SUB_SECTION_HEADING_CHAR\n                    )\n                )))\n                for incl_ref, incl_name in f.included_by:\n                    for incl_file in self.files:\n                        if incl_ref == incl_file.refid:\n                            file_included_by_stream.write(textwrap.dedent('''\n                                - :ref:`{link}`\n                            '''.format(link=incl_file.link_name)))\n                            break\n                file_included_by = file_included_by_stream.getvalue()\n                file_included_by_stream.close()\n            else:\n                file_included_by = \"\"\n\n            # generate their headings if they exist --- DO NOT USE findNested*, these are included recursively\n            file_structs    = []\n            file_classes    = []\n            file_enums      = []\n            file_functions  = []\n            file_typedefs   = []\n            file_unions     = []\n            file_variables  = []\n            file_defines    = []\n            for child in f.children:\n                if child.kind == \"struct\":\n                    file_structs.append(child)\n                elif child.kind == \"class\":\n                    file_classes.append(child)\n                elif child.kind == \"enum\":\n                    file_enums.append(child)\n                elif child.kind == \"function\":\n                    file_functions.append(child)\n                elif child.kind == \"typedef\":\n                    file_typedefs.append(child)\n                elif child.kind == \"union\":\n                    file_unions.append(child)\n                elif child.kind == \"variable\":\n                    file_variables.append(child)\n                elif child.kind == \"define\":\n                    file_defines.append(child)\n\n            # generate the listing of children referenced to from this file\n            children_stream = StringIO()\n            self.generateSortedChildListString(children_stream, \"Namespaces\", f.namespaces_used)\n            self.generateSortedChildListString(children_stream, \"Classes\", file_structs + file_classes)\n            self.generateSortedChildListString(children_stream, \"Enums\", file_enums)\n            self.generateSortedChildListString(children_stream, \"Functions\", file_functions)\n            self.generateSortedChildListString(children_stream, \"Defines\", file_defines)\n            self.generateSortedChildListString(children_stream, \"Typedefs\", file_typedefs)\n            self.generateSortedChildListString(children_stream, \"Unions\", file_unions)\n            self.generateSortedChildListString(children_stream, \"Variables\", file_variables)\n\n            children_string = children_stream.getvalue()\n            children_stream.close()\n\n            try:\n                with codecs.open(f.file_name, \"w\", \"utf-8\") as gen_file:\n                    # Add the metadata if they requested it\n                    if configs.pageLevelConfigMeta:\n                        gen_file.write(\"{0}\\n\\n\".format(configs.pageLevelConfigMeta))\n\n                    # generate a link label for every generated file\n                    link_declaration = \".. _{0}:\".format(f.link_name)\n                    # every generated file must have a header for sphinx to be happy\n                    f.title = \"{0} {1}\".format(utils.qualifyKind(f.kind), f.name)\n                    gen_file.write(textwrap.dedent('''\n                        {link}\n\n                        {heading}\n                        {heading_mark}\n                    '''.format(\n                        link=link_declaration,\n                        heading=f.title,\n                        heading_mark=utils.heading_mark(\n                            f.title,\n                            configs.SECTION_HEADING_CHAR\n                        )\n                    )))\n\n                    if f.parent and f.parent.kind == \"dir\":\n                        gen_file.write(textwrap.dedent('''\n                            |exhale_lsh| :ref:`Parent directory <{parent_link}>` (``{parent_name}``)\n\n                            .. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS\n                        '''.format(\n                            parent_link=f.parent.link_name, parent_name=f.parent.name\n                        )))\n\n                    brief, detailed = parse.getBriefAndDetailedRST(self, f)\n                    if brief:\n                        gen_file.write(\"\\n{brief}\\n\".format(brief=brief))\n\n                    # include the contents directive if requested\n                    contents = utils.contentsDirectiveOrNone(f.kind)\n                    if contents:\n                        gen_file.write(contents)\n\n                    gen_file.write(textwrap.dedent('''\n                        {definition}\n\n                        {detailed}\n\n                        {includes}\n\n                        {includeby}\n\n                        {children}\n                    '''.format(\n                        definition=file_definition,\n                        detailed=detailed,\n                        includes=file_includes,\n                        includeby=file_included_by,\n                        children=children_string\n                    )).lstrip())\n            except:\n                utils.fancyError(\n                    \"Critical error while generating the file for [{0}]\".format(f.file_name)\n                )\n\n            if configs.generateBreatheFileDirectives:\n                try:\n                    with codecs.open(f.file_name, \"a\", \"utf-8\") as gen_file:\n                        heading        = \"Full File Listing\"\n                        heading_mark   = utils.heading_mark(\n                            heading, configs.SUB_SECTION_HEADING_CHAR\n                        )\n                        directive      = utils.kindAsBreatheDirective(f.kind)\n                        node           = f.location\n                        specifications = \"\\n   \".join(\n                            spec for spec in utils.specificationsForKind(f.kind)\n                        )\n\n                        gen_file.write(textwrap.dedent('''\n                            {heading}\n                            {heading_mark}\n\n                            .. {directive}:: {node}\n                               {specifications}\n                        '''.format(\n                            heading=heading,\n                            heading_mark=heading_mark,\n                            directive=directive,\n                            node=node,\n                            specifications=specifications\n                        )))\n                except:\n                    utils.fancyError(\n                        \"Critical error while generating the breathe directive for [{0}]\".format(f.file_name)\n                    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generateDirectoryNodeDocuments(self):\n        '''\n        Generates all of the directory reStructuredText documents.\n        '''\n        all_dirs = []\n        for d in self.dirs:\n            d.findNestedDirectories(all_dirs)\n\n        for d in all_dirs:\n            self.generateDirectoryNodeRST(d)", "response": "Generates all of the directory reStructuredText documents."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the root library api file s body text.", "response": "def generateAPIRootBody(self):\n        '''\n        Generates the root library api file's body text.  The method calls\n        :func:`~exhale.graph.ExhaleRoot.gerrymanderNodeFilenames` first to enable proper\n        internal linkage between reStructuredText documents.  Afterward, it calls\n        :func:`~exhale.graph.ExhaleRoot.generateViewHierarchies` followed by\n        :func:`~exhale.graph.ExhaleRoot.generateUnabridgedAPI` to generate both\n        hierarchies as well as the full API listing.  As a result, three files will now\n        be ready:\n\n        1. ``self.class_hierarchy_file``\n        2. ``self.file_hierarchy_file``\n        3. ``self.unabridged_api_file``\n\n        These three files are then *included* into the root library file.  The\n        consequence of using an ``include`` directive is that Sphinx will complain about\n        these three files never being included in any ``toctree`` directive.  These\n        warnings are expected, and preferred to using a ``toctree`` because otherwise\n        the user would have to click on the class view link from the ``toctree`` in\n        order to see it.  This behavior has been acceptable for me so far, but if it\n        is causing you problems please raise an issue on GitHub and I may be able to\n        conditionally use a ``toctree`` if you really need it.\n        '''\n        try:\n            self.gerrymanderNodeFilenames()\n            self.generateViewHierarchies()\n            self.generateUnabridgedAPI()\n            with codecs.open(self.full_root_file_path, \"a\", \"utf-8\") as generated_index:\n                # Include the class and file hierarchies\n                generated_index.write(\".. include:: {0}\\n\\n\".format(\n                    os.path.basename(self.class_hierarchy_file)\n                ))\n                generated_index.write(\".. include:: {0}\\n\\n\".format(\n                    os.path.basename(self.file_hierarchy_file)\n                ))\n\n                # Add the afterHierarchyDescription if provided\n                if configs.afterHierarchyDescription:\n                    generated_index.write(\n                        \"\\n{0}\\n\\n\".format(configs.afterHierarchyDescription)\n                    )\n\n                # Include the unabridged API\n                generated_index.write(\".. include:: {0}\\n\\n\".format(\n                    os.path.basename(self.unabridged_api_file)\n                ))\n\n                # Add the afterBodySummary if provided\n                if configs.afterBodySummary:\n                    generated_index.write(\n                        \"\\n{0}\\n\\n\".format(configs.afterBodySummary)\n                    )\n\n                # The following should only be applied to the page library root page\n                # Applying it to other pages will result in an error\n                if self.use_tree_view and configs.treeViewIsBootstrap:\n                    generated_index.write(textwrap.dedent('''\n\n                        .. raw:: html\n\n                           <script type=\"text/javascript\">\n                               /* NOTE: if you are reading this, Exhale generated this directly. */\n                               $(document).ready(function() {{\n                                   /* Inspired by very informative answer to get color of links:\n                                      https://stackoverflow.com/a/2707837/3814202 */\n                                   var $fake_link = $('<a href=\"#\"></a>').hide().appendTo(\"body\");\n                                   var linkColor = $fake_link.css(\"color\");\n                                   $fake_link.remove();\n\n                                   var $fake_p = $('<p class=\"{icon_mimic}\"></p>').hide().appendTo(\"body\");\n                                   var iconColor = $fake_p.css(\"color\");\n                                   $fake_p.remove();\n\n                                   /* After much deliberation, using JavaScript directly to enforce that the\n                                    * link and glyphicon receive different colors is fruitless, because the\n                                    * bootstrap treeview library will overwrite the style every time.  Instead,\n                                    * leaning on the library code itself to append some styling to the head,\n                                    * I choose to mix a couple of things:\n                                    *\n                                    * 1. Set the `color` property of bootstrap treeview globally, this would\n                                    *    normally affect the color of both the link text and the icon.\n                                    * 2. Apply custom forced styling of the glyphicon itself in order to make\n                                    *    it a little more clear to the user (via different colors) that the\n                                    *    act of clicking the icon and the act of clicking the link text perform\n                                    *    different actions.  The icon expands, the text navigates to the page.\n                                    */\n                                    // Part 1: use linkColor as a parameter to bootstrap treeview\n\n                                    // apply the class view hierarchy\n                                    $(\"#{class_idx}\").treeview({{\n                                        data: {class_func_name}(),\n                                        enableLinks: true,\n                                        color: linkColor,\n                                        showTags: {show_tags},\n                                        collapseIcon: \"{collapse_icon}\",\n                                        expandIcon: \"{expand_icon}\",\n                                        levels: {levels},\n                                        onhoverColor: \"{onhover_color}\"\n                                    }});\n\n                                    // apply the file view hierarchy\n                                    $(\"#{file_idx}\").treeview({{\n                                        data: {file_func_name}(),\n                                        enableLinks: true,\n                                        color: linkColor,\n                                        showTags: {show_tags},\n                                        collapseIcon: \"{collapse_icon}\",\n                                        expandIcon: \"{expand_icon}\",\n                                        levels: {levels},\n                                        onhoverColor: \"{onhover_color}\"\n                                    }});\n\n                                    // Part 2: override the style of the glyphicons by injecting some CSS\n                                    $('<style type=\"text/css\" id=\"exhaleTreeviewOverride\">' +\n                                      '    .treeview span[class~=icon] {{ '                 +\n                                      '        color: ' + iconColor + ' ! important;'       +\n                                      '    }}'                                              +\n                                      '</style>').appendTo('head');\n                               }});\n                           </script>\n                    '''.format(\n                        icon_mimic=configs.treeViewBootstrapIconMimicColor,\n                        class_idx=configs._class_hierarchy_id,\n                        class_func_name=configs._bstrap_class_hierarchy_fn_data_name,\n                        file_idx=configs._file_hierarchy_id,\n                        file_func_name=configs._bstrap_file_hierarchy_fn_data_name,\n                        show_tags=\"true\" if configs.treeViewBootstrapUseBadgeTags else \"false\",\n                        collapse_icon=configs.treeViewBootstrapCollapseIcon,\n                        expand_icon=configs.treeViewBootstrapExpandIcon,\n                        levels=configs.treeViewBootstrapLevels,\n                        onhover_color=configs.treeViewBootstrapOnhoverColor\n                    )))\n        except:\n            utils.fancyError(\n                \"Unable to create the root api body: [{0}]\".format(self.full_root_file_path)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping method to create the view hierarchies.", "response": "def generateViewHierarchies(self):\n        '''\n        Wrapper method to create the view hierarchies.  Currently it just calls\n        :func:`~exhale.graph.ExhaleRoot.generateClassView` and\n        :func:`~exhale.graph.ExhaleRoot.generateDirectoryView` --- if you want to implement\n        additional hierarchies, implement the additionaly hierarchy method and call it\n        from here.  Then make sure to ``include`` it in\n        :func:`~exhale.graph.ExhaleRoot.generateAPIRootBody`.\n        '''\n        # gather the class hierarchy data and write it out\n        class_view_data = self.generateClassView()\n        self.writeOutHierarchy(True, class_view_data)\n        # gather the file hierarchy data and write it out\n        file_view_data = self.generateDirectoryView()\n        self.writeOutHierarchy(False, file_view_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generateClassView(self):\n        '''\n        Generates the class view hierarchy, writing it to ``self.class_hierarchy_file``.\n        '''\n        class_view_stream = StringIO()\n\n        for n in self.namespaces:\n            n.toHierarchy(True, 0, class_view_stream)\n\n        # Add everything that was not nested in a namespace.\n        missing = []\n        # class-like objects (structs and classes)\n        for cl in sorted(self.class_like):\n            if not cl.in_class_hierarchy:\n                missing.append(cl)\n        # enums\n        for e in sorted(self.enums):\n            if not e.in_class_hierarchy:\n                missing.append(e)\n        # unions\n        for u in sorted(self.unions):\n            if not u.in_class_hierarchy:\n                missing.append(u)\n\n        if len(missing) > 0:\n            idx = 0\n            last_missing_child = len(missing) - 1\n            for m in missing:\n                m.toHierarchy(True, 0, class_view_stream, idx == last_missing_child)\n                idx += 1\n        elif configs.createTreeView:\n            # need to restart since there were no missing children found, otherwise the\n            # last namespace will not correctly have a lastChild\n            class_view_stream.close()\n            class_view_stream = StringIO()\n\n            last_nspace_index = len(self.namespaces) - 1\n            for idx in range(last_nspace_index + 1):\n                nspace = self.namespaces[idx]\n                nspace.toHierarchy(True, 0, class_view_stream, idx == last_nspace_index)\n\n        # extract the value from the stream and close it down\n        class_view_string = class_view_stream.getvalue()\n        class_view_stream.close()\n        return class_view_string", "response": "Generates the class view hierarchy for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the file view hierarchy writing it to self. file_hierarchy_file.", "response": "def generateDirectoryView(self):\n        '''\n        Generates the file view hierarchy, writing it to ``self.file_hierarchy_file``.\n        '''\n        file_view_stream = StringIO()\n\n        for d in self.dirs:\n            d.toHierarchy(False, 0, file_view_stream)\n\n        # add potential missing files (not sure if this is possible though)\n        missing = []\n        for f in sorted(self.files):\n            if not f.in_file_hierarchy:\n                missing.append(f)\n\n        found_missing = len(missing) > 0\n        if found_missing:\n            idx = 0\n            last_missing_child = len(missing) - 1\n            for m in missing:\n                m.toHierarchy(False, 0, file_view_stream, idx == last_missing_child)\n                idx += 1\n        elif configs.createTreeView:\n            # need to restart since there were no missing children found, otherwise the\n            # last directory will not correctly have a lastChild\n            file_view_stream.close()\n            file_view_stream = StringIO()\n\n            last_dir_index = len(self.dirs) - 1\n            for idx in range(last_dir_index + 1):\n                curr_d = self.dirs[idx]\n                curr_d.toHierarchy(False, 0, file_view_stream, idx == last_dir_index)\n\n        # extract the value from the stream and close it down\n        file_view_string = file_view_stream.getvalue()\n        file_view_stream.close()\n        return file_view_string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the unabridged API listing for the given item.", "response": "def generateUnabridgedAPI(self):\n        '''\n        Generates the unabridged (full) API listing into ``self.unabridged_api_file``.\n        This is necessary as some items may not show up in either hierarchy view,\n        depending on:\n\n        1. The item.  For example, if a namespace has only one member which is a\n           variable, then neither the namespace nor the variable will be declared in the\n           class view hierarchy.  It will be present in the file page it was declared in\n           but not on the main library page.\n\n        2. The configurations of Doxygen.  For example, see the warning in\n           :func:`~exhale.graph.ExhaleRoot.fileRefDiscovery`.  Items whose parents cannot\n           be rediscovered withouth the programlisting will still be documented, their\n           link appearing in the unabridged API listing.\n\n        Currently, the API is generated in the following (somewhat arbitrary) order:\n\n        - Namespaces\n        - Classes and Structs\n        - Enums\n        - Unions\n        - Functions\n        - Variables\n        - Defines\n        - Typedefs\n        - Directories\n        - Files\n\n        If you want to change the ordering, just change the order of the calls to\n        :func:`~exhale.graph.ExhaleRoot.enumerateAll` in this method.\n        '''\n        ####flake8fail\n        # TODO: I've reverted my decision, the full API should include everything,\n        # including nested types.  the code below invalidates certain portions of the\n        # docs and probably gets rid of a need for the recursive find methods\n        all_namespaces = []\n        all_class_like = []\n        all_enums      = []\n        all_unions     = []\n        all_functions  = []\n        all_variables  = []\n        all_defines    = []\n        all_typedefs   = []\n        all_dirs       = []\n        all_files      = []\n\n        for node in self.all_nodes:\n            if node.kind == \"namespace\":\n                all_namespaces.append(node)\n            elif node.kind == \"class\" or node.kind == \"struct\":\n                all_class_like.append(node)\n            elif node.kind == \"enum\":\n                all_enums.append(node)\n            elif node.kind == \"union\":\n                all_unions.append(node)\n            elif node.kind == \"function\":\n                all_functions.append(node)\n            elif node.kind == \"variable\":\n                all_variables.append(node)\n            elif node.kind == \"define\":\n                all_defines.append(node)\n            elif node.kind == \"typedef\":\n                all_typedefs.append(node)\n            elif node.kind == \"dir\":\n                all_dirs.append(node)\n            elif node.kind == \"file\":\n                all_files.append(node)\n\n        try:\n            with codecs.open(self.unabridged_api_file, \"w\", \"utf-8\") as full_api_file:\n                # write the header\n                full_api_file.write(textwrap.dedent('''\n                    {heading}\n                    {heading_mark}\n\n                '''.format(\n                    heading=configs.fullApiSubSectionTitle,\n                    heading_mark=utils.heading_mark(\n                        configs.fullApiSubSectionTitle,\n                        configs.SUB_SECTION_HEADING_CHAR\n                    )\n                )))\n\n                # write everything to file: reorder these lines for different outcomes\n                self.enumerateAll(         \"Namespaces\", all_namespaces, full_api_file)\n                self.enumerateAll(\"Classes and Structs\", all_class_like, full_api_file)\n                self.enumerateAll(              \"Enums\",      all_enums, full_api_file)\n                self.enumerateAll(             \"Unions\",     all_unions, full_api_file)\n                self.enumerateAll(          \"Functions\",  all_functions, full_api_file)\n                self.enumerateAll(          \"Variables\",  all_variables, full_api_file)\n                self.enumerateAll(            \"Defines\",    all_defines, full_api_file)\n                self.enumerateAll(           \"Typedefs\",   all_typedefs, full_api_file)\n                self.enumerateAll(        \"Directories\",       all_dirs, full_api_file)\n                self.enumerateAll(              \"Files\",      all_files, full_api_file)\n        except:\n            utils.fancyError(\"Error writing the unabridged API.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef consoleFormat(self, sectionTitle, lst, fmt_spec):\n        '''\n        Helper method for :func:`~exhale.graph.ExhaleRoot.toConsole`.  Prints the given\n        ``sectionTitle`` and calls :func:`~exhale.graph.ExhaleNode.toConsole` with ``0``\n        as the level for every ExhaleNode in ``lst``.\n\n        **Parameters**\n            ``sectionTitle`` (str)\n                The title that will be printed with some visual separators around it.\n\n            ``lst`` (list)\n                The list of ExhaleNodes to print to the console.\n        '''\n        if not configs.verboseBuild:\n            return\n\n        utils.verbose_log(textwrap.dedent('''\n            ###########################################################\n            ## {0}\n            ###########################################################'''.format(sectionTitle)))\n        for l in lst:\n            l.toConsole(0, fmt_spec)", "response": "Helper method for printing the given sectionTitle and list of ExhaleNodes to the console."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a string that can be used to generate the contents directive for the given kind.", "response": "def contentsDirectiveOrNone(kind):\n    '''\n    Generates a string ``.. contents::`` directives according to the rules outlined in\n    the :ref:`using_contents_directives` section.\n\n    **Parameters**\n        ``kind`` (str)\n            The ``kind`` of the compound (one of :data:`~exhale.utils.AVAILABLE_KINDS`).\n\n    **Return**\n        ``str`` or ``None``\n            If this ``kind`` should have a ``.. contents::`` directive, it returns the\n            string that can be written to file.  Otherwise, ``None`` is returned.\n    '''\n    if configs.contentsDirectives and kind in configs.kindsWithContentsDirectives:\n        ret = \"\\n.. contents:: {contentsTitle}\".format(\n            contentsTitle=configs.contentsTitle\n        )\n        if configs.contentsSpecifiers:\n            specs = \"\\n\".join(s for s in configs.contentsSpecifiers)\n            ret   = \"{directive}\\n{specs}\".format(\n                directive=ret,\n                specs=prefix(\"   \", specs)\n            )\n        return \"{full_directive}\\n\\n\".format(full_directive=ret)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeCustomSpecificationsMapping(func):\n    '''\n    Creates the \"pickleable\" dictionary that will be used with\n    :data:`~exhale.configs.customSpecificationsMapping` supplied to ``exhale_args`` in\n    your ``conf.py``.\n\n    **Parameters**\n        ``func`` (types.FunctionType)\n            A callable function that takes as input a string from\n            :data:`~exhale.utils.AVAILABLE_KINDS` and returns a ``list`` of strings.\n\n            The empty list ``[]`` indicates to use the Breathe defaults.\n\n    **Return**\n        ``dict``\n            A dictionary where the keys are every value in\n            :data:`~exhale.utils.AVAILABLE_KINDS`, and the values are the ``list``\n            returns of the input ``func``.\n\n    .. note::\n\n       To help ensure the dictionary has everything it needs for the rest of Exhale to\n       function, a \"secret\" key-value pair is inserted to the returned dictionary.\n    '''\n    # Make sure they gave us a function\n    if not isinstance(func, types.FunctionType):\n        raise ValueError(\n            \"The input to exhale.util.makeCustomSpecificationsMapping was *NOT* a function: {0}\".format(\n                type(func)\n            )\n        )\n\n    # Stamp the return to ensure exhale created this function.\n    ret = {configs._closure_map_sanity_check: configs._closure_map_sanity_check}\n    try:\n        # Because we cannot pickle a fully-fledged function object, we are going to go\n        # through every kind and store its return value.\n        for kind in AVAILABLE_KINDS:\n            specs = func(kind)\n            bad   = type(specs) is not list\n            for s in specs:\n                if not isinstance(s, six.string_types):\n                    bad = True\n                    break\n            if bad:\n                raise RuntimeError(textwrap.dedent('''\n                    The specifications function did not return a valid list for input\n\n                        `{kind}`\n\n                    1. Make sure that every entry in the returned list is a string.\n                    2. If you want to use the breathe defaults, you must return the\n                       empty list `[]`.\n                '''.format(kind=kind)))\n            ret[kind] = specs\n    except Exception as e:\n        raise RuntimeError(\"Unable to create custom specifications:\\n{0}\".format(e))\n\n    # Everything went according to plan, send it back to `conf.py` :)\n    return ret", "response": "Creates a pickleable dictionary that will be used with the customSpecificationsMapping function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsanitize the input name for use with breathe directives.", "response": "def sanitize(name):\n    \"\"\"\n    Sanitize the specified ``name`` for use with breathe directives.\n\n    **Parameters**\n\n    ``name`` (:class:`python:str`)\n        The name to be sanitized.\n\n    **Return**\n\n    :class:`python:str`\n        The input ``name`` sanitized to use with breathe directives (primarily for use\n        with ``.. doxygenfunction::``).  Replacements such as ``\"&lt;\" -> \"<\"`` are\n        performed, as well as removing spaces ``\"< \" -> \"<\"`` must be done.  Breathe is\n        particularly sensitive with respect to whitespace.\n    \"\"\"\n    return name.replace(\n        \"&lt;\", \"<\"\n    ).replace(\n        \"&gt;\", \">\"\n    ).replace(\n        \"&amp;\", \"&\"\n    ).replace(\n        \"< \", \"<\"\n    ).replace(\n        \" >\", \">\"\n    ).replace(\n        \" &\", \"&\"\n    ).replace(\n        \"& \", \"&\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef doxygenLanguageToPygmentsLexer(location, language):\n    '''\n    Given an input location and language specification, acquire the Pygments lexer to\n    use for this file.\n\n    1. If :data:`configs.lexerMapping <exhale.configs.lexerMapping>` has been specified,\n       then :data:`configs._compiled_lexer_mapping <exhale.configs._compiled_lexer_mapping>`\n       will be queried first using the ``location`` parameter.\n    2. If no matching was found, then the appropriate lexer defined in\n       :data:`LANG_TO_LEX <exhale.utils.LANG_TO_LEX>` is used.\n    3. If no matching language is found, ``\"none\"`` is returned (indicating to Pygments\n       that no syntax highlighting should occur).\n    '''\n    if configs._compiled_lexer_mapping:\n        for regex in configs._compiled_lexer_mapping:\n            if regex.match(location):\n                return configs._compiled_lexer_mapping[regex]\n\n    if language in LANG_TO_LEX:\n        return LANG_TO_LEX[language]\n\n    return \"none\"", "response": "Returns a Pygments lexer to use for a given language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _use_color(msg, ansi_fmt, output_stream):\n    '''\n    Based on :data:`~exhale.configs.alwaysColorize`, returns the colorized or\n    non-colorized output when ``output_stream`` is not a TTY (e.g. redirecting\n    to a file).\n\n    **Parameters**\n        ``msg`` (str)\n            The message that is going to be printed by the caller of this method.\n\n        ``ansi_fmt`` (str)\n            The ANSI color format to use when coloring is supposed to happen.\n\n        ``output_stream`` (file)\n            Assumed to be either ``sys.stdout`` or ``sys.stderr``.\n\n    **Return**\n        ``str``\n            The message ``msg`` in color, or not, depending on both\n            :data:`~exhale.configs.alwaysColorize` and whether or not the\n            ``output_stream`` is a TTY.\n    '''\n    if configs._on_rtd or (not configs.alwaysColorize and not output_stream.isatty()):\n        log = msg\n    else:\n        log = colorize(msg, ansi_fmt)\n    return log", "response": "Returns the message in color depending on the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convertDescriptionToRST(textRoot, node, soupTag, heading):\n    '''\n    Parses the ``node`` XML document and returns a reStructuredText formatted\n    string.  Helper method for :func:`~exhale.parse.getBriefAndDetailedRST`.\n\n    .. todo:: actually document this\n    '''\n    if soupTag.para:\n        children = soupTag.findChildren(recursive=False)\n        for child in children:\n            walk(textRoot, child, 0, None, \"\\n\")\n        contents = soupTag.get_text()\n\n        if not heading:\n            return contents\n\n        start = textwrap.dedent('''\n            {heading}\n            {heading_mark}\n        '''.format(\n            heading=heading,\n            heading_mark=utils.heading_mark(\n                heading,\n                configs.SUB_SECTION_HEADING_CHAR\n            )\n        ))\n        return \"{0}{1}\".format(start, contents)\n    else:\n        return \"\"", "response": "Converts the description node to RST."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an input node return a tuple of strings where the first element of the tuple is the brief description and the second is the detailed description.", "response": "def getBriefAndDetailedRST(textRoot, node):\n    '''\n    Given an input ``node``, return a tuple of strings where the first element of\n    the return is the ``brief`` description and the second is the ``detailed``\n    description.\n\n    .. todo:: actually document this\n    '''\n    node_xml_contents = utils.nodeCompoundXMLContents(node)\n    if not node_xml_contents:\n        return \"\", \"\"\n\n    try:\n        node_soup = BeautifulSoup(node_xml_contents, \"lxml-xml\")\n    except:\n        utils.fancyError(\"Unable to parse [{0}] xml using BeautifulSoup\".format(node.name))\n\n    try:\n        # In the file xml definitions, things such as enums or defines are listed inside\n        # of <sectiondef> tags, which may have some nested <briefdescription> or\n        # <detaileddescription> tags.  So as long as we make sure not to search\n        # recursively, then the following will extract the file descriptions only\n        # process the brief description if provided\n        brief      = node_soup.doxygen.compounddef.find_all(\"briefdescription\", recursive=False)\n        brief_desc = \"\"\n        if len(brief) == 1:\n            brief = brief[0]\n            # Empty descriptions will usually get parsed as a single newline, which we\n            # want to ignore ;)\n            if not brief.get_text().isspace():\n                brief_desc = convertDescriptionToRST(textRoot, node, brief, None)\n\n        # process the detailed description if provided\n        detailed      = node_soup.doxygen.compounddef.find_all(\"detaileddescription\", recursive=False)\n        detailed_desc = \"\"\n        if len(detailed) == 1:\n            detailed = detailed[0]\n            if not detailed.get_text().isspace():\n                detailed_desc = convertDescriptionToRST(textRoot, node, detailed, \"Detailed Description\")\n\n        return brief_desc, detailed_desc\n    except:\n        utils.fancyError(\n            \"Could not acquire soup.doxygen.compounddef; likely not a doxygen xml file.\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the absolute URL using the target and the desired endpoint.", "response": "def _build_url(self, endpoint):\n        \"\"\"\n        Builds the absolute URL using the target and desired endpoint.\n        \"\"\"\n        try:\n            path = self.endpoints[endpoint]\n        except KeyError:\n            msg = 'Unknown endpoint `{0}`'\n            raise ValueError(msg.format(endpoint))\n        absolute_url = urljoin(self.target, path)\n        return absolute_url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new project egg to the Scrapyd service.", "response": "def add_version(self, project, version, egg):\n        \"\"\"\n        Adds a new project egg to the Scrapyd service. First class, maps to\n        Scrapyd's add version endpoint.\n        \"\"\"\n        url = self._build_url(constants.ADD_VERSION_ENDPOINT)\n        data = {\n            'project': project,\n            'version': version\n        }\n        files = {\n            'egg': egg\n        }\n        json = self.client.post(url, data=data, files=files,\n                                timeout=self.timeout)\n        return json['spiders']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels a job from a specific project.", "response": "def cancel(self, project, job, signal=None):\n        \"\"\"\n        Cancels a job from a specific project. First class, maps to\n        Scrapyd's cancel job endpoint.\n        \"\"\"\n        url = self._build_url(constants.CANCEL_ENDPOINT)\n        data = {\n            'project': project,\n            'job': job,\n        }\n        if signal is not None:\n            data['signal'] = signal\n        json = self.client.post(url, data=data, timeout=self.timeout)\n        return json['prevstate']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes all versions of a project.", "response": "def delete_project(self, project):\n        \"\"\"\n        Deletes all versions of a project. First class, maps to Scrapyd's\n        delete project endpoint.\n        \"\"\"\n        url = self._build_url(constants.DELETE_PROJECT_ENDPOINT)\n        data = {\n            'project': project,\n        }\n        self.client.post(url, data=data, timeout=self.timeout)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a specific version of a project.", "response": "def delete_version(self, project, version):\n        \"\"\"\n        Deletes a specific version of a project. First class, maps to\n        Scrapyd's delete version endpoint.\n        \"\"\"\n        url = self._build_url(constants.DELETE_VERSION_ENDPOINT)\n        data = {\n            'project': project,\n            'version': version\n        }\n        self.client.post(url, data=data, timeout=self.timeout)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the status of a specific job.", "response": "def job_status(self, project, job_id):\n        \"\"\"\n        Retrieves the 'status' of a specific job specified by its id. Derived,\n        utilises Scrapyd's list jobs endpoint to provide the answer.\n        \"\"\"\n        all_jobs = self.list_jobs(project)\n        for state in constants.JOB_STATES:\n            job_ids = [job['id'] for job in all_jobs[state]]\n            if job_id in job_ids:\n                return state\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all known jobs for a project.", "response": "def list_jobs(self, project):\n        \"\"\"\n        Lists all known jobs for a project. First class, maps to Scrapyd's\n        list jobs endpoint.\n        \"\"\"\n        url = self._build_url(constants.LIST_JOBS_ENDPOINT)\n        params = {'project': project}\n        jobs = self.client.get(url, params=params, timeout=self.timeout)\n        return jobs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all deployed projects. First class maps to Scrapyd s list projects endpoint.", "response": "def list_projects(self):\n        \"\"\"\n        Lists all deployed projects. First class, maps to Scrapyd's\n        list projects endpoint.\n        \"\"\"\n        url = self._build_url(constants.LIST_PROJECTS_ENDPOINT)\n        json = self.client.get(url, timeout=self.timeout)\n        return json['projects']"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all known spiders for a specific project.", "response": "def list_spiders(self, project):\n        \"\"\"\n        Lists all known spiders for a specific project. First class, maps\n        to Scrapyd's list spiders endpoint.\n        \"\"\"\n        url = self._build_url(constants.LIST_SPIDERS_ENDPOINT)\n        params = {'project': project}\n        json = self.client.get(url, params=params, timeout=self.timeout)\n        return json['spiders']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all deployed versions of a specific project.", "response": "def list_versions(self, project):\n        \"\"\"\n        Lists all deployed versions of a specific project. First class, maps\n        to Scrapyd's list versions endpoint.\n        \"\"\"\n        url = self._build_url(constants.LIST_VERSIONS_ENDPOINT)\n        params = {'project': project}\n        json = self.client.get(url, params=params, timeout=self.timeout)\n        return json['versions']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nschedules a spider from a specific project to run.", "response": "def schedule(self, project, spider, settings=None, **kwargs):\n        \"\"\"\n        Schedules a spider from a specific project to run. First class, maps\n        to Scrapyd's scheduling endpoint.\n        \"\"\"\n\n        url = self._build_url(constants.SCHEDULE_ENDPOINT)\n        data = {\n            'project': project,\n            'spider': spider\n        }\n        data.update(kwargs)\n        if settings:\n            setting_params = []\n            for setting_name, value in iteritems(settings):\n                setting_params.append('{0}={1}'.format(setting_name, value))\n            data['setting'] = setting_params\n        json = self.client.post(url, data=data, timeout=self.timeout)\n        return json['jobid']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays the load status of a service.", "response": "def daemon_status(self):\n        \"\"\"\n        Displays the load status of a service.\n        :rtype: dict\n        \"\"\"\n        url = self._build_url(constants.DAEMON_STATUS_ENDPOINT)\n        json = self.client.get(url, timeout=self.timeout)\n        return json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the response from Scrapyd and returns the object", "response": "def _handle_response(self, response):\n        \"\"\"\n        Handles the response received from Scrapyd.\n        \"\"\"\n        if not response.ok:\n            raise ScrapydResponseError(\n                \"Scrapyd returned a {0} error: {1}\".format(\n                    response.status_code,\n                    response.text))\n\n        try:\n            json = response.json()\n        except ValueError:\n            raise ScrapydResponseError(\"Scrapyd returned an invalid JSON \"\n                                       \"response: {0}\".format(response.text))\n        if json['status'] == 'ok':\n            json.pop('status')\n            return json\n        elif json['status'] == 'error':\n            raise ScrapydResponseError(json['message'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields all content in this node regardless of whitespace or not.", "response": "def all(self):\n        r\"\"\"Returns all content in this node, regardless of whitespace or\n        not. This includes all LaTeX needed to reconstruct the original source.\n\n        >>> from TexSoup import TexSoup\n        >>> soup = TexSoup(r'''\n        ... \\newcommand{reverseconcat}[3]{#3#2#1}\n        ... ''')\n        >>> list(soup.all)\n        ['\\n', \\newcommand{reverseconcat}[3]{#3#2#1}, '\\n']\n        \"\"\"\n        for child in self.expr.all:\n            if isinstance(child, TexExpr):\n                node = TexNode(child)\n                node.parent = self\n                yield node\n            else:\n                yield child"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef children(self):\n        for child in self.expr.children:\n            node = TexNode(child)\n            node.parent = self\n            yield node", "response": "Iterate over all children of this TeX element."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef text(self):\n        for descendant in self.contents:\n            if isinstance(descendant, TokenWithPosition):\n                yield descendant\n            elif hasattr(descendant, 'text'):\n                yield from descendant.text", "response": "All text in descendant nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert(self, i, *nodes):\n        assert isinstance(i, int), (\n                'Provided index \"{}\" is not an integer! Did you switch your '\n                'arguments? The first argument to `insert` is the '\n                'index.'.format(i))\n        self.expr.insert(i, *nodes)", "response": "r Inserts nodes at the given position i in the list of children of this node s list of children."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count(self, name=None, **attrs):\n        return len(list(self.find_all(name, **attrs)))", "response": "Count the number of descendants matching criteria."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete this node from the parse tree.", "response": "def delete(self):\n        r\"\"\"Delete this node from the parse tree.\n\n        Where applicable, this will remove all descendants of this node from\n        the parse tree.\n\n        >>> from TexSoup import TexSoup\n        >>> soup = TexSoup(r'''\\textit{\\color{blue}{Silly}}\\textit{keep me!}''')\n        >>> soup.textit.color.delete()\n        >>> soup\n        \\textit{}\\textit{keep me!}\n        >>> soup.textit.delete()\n        >>> soup\n        \\textit{keep me!}\n        \"\"\"\n\n        # TODO: needs better abstraction for supports contents\n        parent = self.parent\n        if parent.expr._supports_contents():\n            parent.remove(self)\n            return\n\n        # TODO: needs abstraction for removing from arg\n        for arg in parent.args:\n            if self.expr in arg.contents:\n                arg.contents.remove(self.expr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find(self, name=None, **attrs):\n        try:\n            return next(self.find_all(name, **attrs))\n        except StopIteration:\n            return None", "response": "r First descendant node matching criteria."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all descendant nodes matching criteria.", "response": "def find_all(self, name=None, **attrs):\n        r\"\"\"Return all descendant nodes matching criteria.\n\n        :param Union[None,str] name: name of LaTeX expression\n        :param attrs: LaTeX expression attributes, such as item text.\n        :return: All descendant nodes matching criteria\n        :rtype: Iterator[TexNode]\n\n        >>> from TexSoup import TexSoup\n        >>> soup = TexSoup(r'''\n        ... \\section{Ooo}\n        ... \\textit{eee}\n        ... \\textit{ooo}''')\n        >>> gen = soup.find_all('textit')\n        >>> next(gen)\n        \\textit{eee}\n        >>> next(gen)\n        \\textit{ooo}\n        >>> next(soup.find_all('textbf'))\n        Traceback (most recent call last):\n        ...\n        StopIteration\n        \"\"\"\n        for descendant in self.__descendants():\n            if hasattr(descendant, '__match__') and \\\n                    descendant.__match__(name, attrs):\n                yield descendant"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace(self, child, *nodes):\n        self.expr.insert(\n            self.expr.remove(child.expr),\n            *nodes)", "response": "Replace provided node with nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __descendants(self):\n        return itertools.chain(self.contents,\n                               *[c.descendants for c in self.children])", "response": "Implementation for descendants, hacky workaround for __getattr__\n        issues."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield all content in this expression regardless of whitespace or not.", "response": "def all(self):\n        r\"\"\"Returns all content in this expression, regardless of whitespace or\n        not. This includes all LaTeX needed to reconstruct the original source.\n\n        >>> expr1 = TexExpr('textbf', ('\\n', 'hi'))\n        >>> expr2 = TexExpr('textbf', ('\\n', 'hi'), preserve_whitespace=True)\n        >>> list(expr1.all) == list(expr2.all)\n        True\n        \"\"\"\n        for arg in self.args:\n            for expr in arg:\n                yield expr\n        for content in self._contents:\n            yield content"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contents(self):\n        for content in self.all:\n            is_whitespace = isinstance(content, str) and content.isspace()\n            if not is_whitespace or self.preserve_whitespace:\n                yield content", "response": "Returns all contents in this expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokens(self):\n        for content in self.contents:\n            if isinstance(content, TokenWithPosition):\n                for word in content.split():\n                    yield word\n            else:\n                yield content", "response": "Yield all tokens for a particular expression into a list of words and other expressions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert content at specified position into expression.", "response": "def insert(self, i, *exprs):\n        \"\"\"Insert content at specified position into expression.\n\n        :param int i: Position to add content to\n        :param Union[TexExpr,str] exprs: List of contents to add\n\n        >>> expr = TexExpr('textbf', ('hello',))\n        >>> expr\n        TexExpr('textbf', ['hello'])\n        >>> expr.insert(0, 'world')\n        >>> expr\n        TexExpr('textbf', ['world', 'hello'])\n        \"\"\"\n        self._assert_supports_contents()\n        for j, expr in enumerate(exprs):\n            self._contents.insert(i + j, expr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a provided expression from its list of contents.", "response": "def remove(self, expr):\n        \"\"\"Remove a provided expression from its list of contents.\n\n        :param Union[TexExpr,str] expr: Content to add\n        :return: index of the expression removed\n        :rtype: int\n\n        >>> expr = TexExpr('textbf', ('hello',))\n        >>> expr.remove('hello')\n        0\n        >>> expr\n        TexExpr('textbf', [])\n        \"\"\"\n        self._assert_supports_contents()\n        index = self._contents.index(expr)\n        self._contents.remove(expr)\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(s):\n        if isinstance(s, arg_type):\n            return s\n        if isinstance(s, (list, tuple)):\n            for arg in arg_type:\n                if [s[0], s[-1]] == arg.delims():\n                    return arg(*s[1:-1])\n            raise TypeError('Malformed argument. First and last elements must '\n                            'match a valid argument format. In this case, TexSoup'\n                            ' could not find matching punctuation for: %s.\\n'\n                            'Common issues include: Unescaped special characters,'\n                            ' mistyped closing punctuation, misalignment.' % (str(s)))\n        for arg in arg_type:\n            if arg.__is__(s):\n                return arg(arg.__strip__(s))\n        raise TypeError('Malformed argument. Must be an Arg or a string in '\n                        'either brackets or curly braces.')", "response": "Parse a string or list and return an Argument object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert(self, i, arg):\n        arg = self.__coerce(arg)\n\n        if isinstance(arg, Arg):\n            super().insert(i, arg)\n\n        if len(self) <= 1:\n            self.all.append(arg)\n        else:\n            if i > len(self):\n                i = len(self) - 1\n\n            before = self[i - 1]\n            index_before = self.all.index(before)\n            self.all.insert(index_before + 1, arg)", "response": "r Insert whitespace an unparsed argument string or an argument\n        object into the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove either an unparsed argument string or an argument object.", "response": "def remove(self, item):\n        \"\"\"Remove either an unparsed argument string or an argument object.\n\n        :param Union[str,Arg] item: Item to remove\n\n        >>> arguments = TexArgs([RArg('arg0'), '[arg2]', '{arg3}'])\n        >>> arguments.remove('{arg0}')\n        >>> len(arguments)\n        2\n        >>> arguments[0]\n        OArg('arg2')\n        \"\"\"\n        item = self.__coerce(item)\n        self.all.remove(item)\n        super().remove(item)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopping argument object at provided index.", "response": "def pop(self, i):\n        \"\"\"Pop argument object at provided index.\n\n        :param int i: Index to pop from the list\n\n        >>> arguments = TexArgs([RArg('arg0'), '[arg2]', '{arg3}'])\n        >>> arguments.pop(1)\n        OArg('arg2')\n        >>> len(arguments)\n        2\n        >>> arguments[0]\n        RArg('arg0')\n        \"\"\"\n        item = super().pop(i)\n        j = self.all.index(item)\n        return self.all.pop(j)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving forward by j steps.", "response": "def forward(self, j=1):\n        \"\"\"Move forward by j steps.\n\n        >>> b = Buffer('abcdef')\n        >>> b.forward(3)\n        'abc'\n        >>> b.forward(-2)\n        'bc'\n        \"\"\"\n        if j < 0:\n            return self.backward(-j)\n        self.__i += j\n        return self[self.__i-j:self.__i]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nforward until one of the provided matches is found.", "response": "def num_forward_until(self, condition):\n        \"\"\"Forward until one of the provided matches is found.\n\n        :param condition: set of valid strings\n        \"\"\"\n        i, c = 0, ''\n        while self.hasNext() and not condition(self.peek()):\n            c += self.forward(1)\n            i += 1\n        assert self.backward(i) == c\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forward_until(self, condition):\n        c = TokenWithPosition('', self.peek().position)\n        while self.hasNext() and not condition(self.peek()):\n            c += self.forward(1)\n        return c", "response": "Forward until one of the provided matches is found."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef peek(self, j=(0, 1)):\n        try:\n            if isinstance(j, int):\n                return self[self.__i+j]\n            return self[self.__i + j[0]:self.__i + j[1]]\n        except IndexError:\n            return None", "response": "Return the next value of the next value in the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(tex):\n    if isinstance(tex, str):\n        tex = tex\n    else:\n        tex = ''.join(itertools.chain(*tex))\n    buf, children = Buffer(tokenize(tex)), []\n    while buf.hasNext():\n        content = read_tex(buf)\n        if content is not None:\n            children.append(content)\n    return TexEnv('[tex]', children), tex", "response": "Read and parse all LaTeX source into a TexEnv object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve(tex):\n\n    # soupify\n    soup = TexSoup(tex)\n\n    # resolve subimports\n    for subimport in soup.find_all('subimport'):\n        path = subimport.args[0] + subimport.args[1]\n        subimport.replace_with(*resolve(open(path)).contents)\n\n    # resolve imports\n    for _import in soup.find_all('import'):\n        _import.replace_with(*resolve(open(_import.args[0])).contents)\n\n    # resolve includes\n    for include in soup.find_all('include'):\n        include.replace_with(*resolve(open(include.args[0])).contents)\n\n    return soup", "response": "Resolve all imports and update the parse tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sollen(tex, command):\n    return sum(len(a.string) for a in TexSoup(tex).find_all(command))", "response": "r Measure solution length by finding all the strings in the LaTeX source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef TexSoup(tex_code):\n    parsed, src = read(tex_code)\n    return TexNode(parsed, src=src)", "response": "A high - level function that parses a Tex into a searchable structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts all labels then count the number of times each is referenced in the provided file. Does not follow \\ includes.", "response": "def count(tex):\n    \"\"\"Extract all labels, then count the number of times each is referenced in\n    the provided file. Does not follow \\includes.\n    \"\"\"\n\n    # soupify\n    soup = TexSoup(tex)\n\n    # extract all unique labels\n    labels = set(label.string for label in soup.find_all('label'))\n\n    # create dictionary mapping label to number of references\n    return dict((label, soup.find_all('\\ref{%s}' % label)) for label in labels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next_token(text):\n    while text.hasNext():\n        for name, f in tokenizers:\n            current_token = f(text)\n            if current_token is not None:\n                return current_token", "response": "r Returns the next possible token in LaTeX."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tokenize(text):\n    current_token = next_token(text)\n    while current_token is not None:\n        yield current_token\n        current_token = next_token(text)", "response": "Generator for LaTeX tokens on text ignoring comments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess command that augments or modifies punctuation. This is important to the tokenization of a string, as opening or closing punctuation is not supposed to match. :param Buffer text: iterator over text, with current position", "response": "def tokenize_punctuation_command(text):\n    \"\"\"Process command that augments or modifies punctuation.\n\n    This is important to the tokenization of a string, as opening or closing\n    punctuation is not supposed to match.\n\n    :param Buffer text: iterator over text, with current position\n    \"\"\"\n    if text.peek() == '\\\\':\n        for point in PUNCTUATION_COMMANDS:\n            if text.peek((1, len(point) + 1)) == point:\n                return text.forward(len(point) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess command but ignore line breaks.", "response": "def tokenize_command(text):\n    \"\"\"Process command, but ignore line breaks. (double backslash)\n\n    :param Buffer text: iterator over line, with current position\n    \"\"\"\n    if text.peek() == '\\\\':\n        c = text.forward(1)\n        tokens = set(string.punctuation + string.whitespace) - {'*'}\n        while text.hasNext() and (c == '\\\\' or text.peek() not in tokens) and c not in MATH_TOKENS:\n            c += text.forward(1)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess both optional and required arguments.", "response": "def tokenize_argument(text):\n    \"\"\"Process both optional and required arguments.\n\n    :param Buffer text: iterator over line, with current position\n    \"\"\"\n    for delim in ARG_TOKENS:\n        if text.startswith(delim):\n            return text.forward(len(delim))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tokenize_math(text):\n    if text.startswith('$') and (\n       text.position == 0 or text.peek(-1) != '\\\\' or text.endswith(r'\\\\')):\n        starter = '$$' if text.startswith('$$') else '$'\n        return TokenWithPosition(text.forward(len(starter)), text.position)", "response": "r Tokenize math in text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tokenize_string(text, delimiters=None):\n    if delimiters is None:\n        delimiters = ALL_TOKENS\n    result = TokenWithPosition('', text.position)\n    for c in text:\n        if c == '\\\\' and str(text.peek()) in delimiters and str(c + text.peek()) not in delimiters:\n            c += next(text)\n        elif str(c) in delimiters:  # assumes all tokens are single characters\n            text.backward(1)\n            return result\n        result += c\n        if text.peek((0, 2)) == '\\\\\\\\':\n            result += text.forward(2)\n        if text.peek((0, 2)) == '\\n\\n':\n            result += text.forward(2)\n            return result\n    return result", "response": "r Process a string of text into a sequence of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_tex(src):\n    c = next(src)\n    if c.startswith('%'):\n        return c\n    elif c.startswith('$'):\n        name = '$$' if c.startswith('$$') else '$'\n        expr = TexEnv(name, [], nobegin=True)\n        return read_math_env(src, expr)\n    elif c.startswith('\\[') or c.startswith(\"\\(\"):\n        if c.startswith('\\['):\n            name = 'displaymath'\n            begin = '\\['\n            end = '\\]'\n        else:\n            name = \"math\"\n            begin = \"\\(\"\n            end = \"\\)\"\n\n        expr = TexEnv(name, [], nobegin=True, begin=begin, end=end)\n        return read_math_env(src, expr)\n    elif c.startswith('\\\\'):\n        command = TokenWithPosition(c[1:], src.position)\n        if command == 'item':\n            contents, arg = read_item(src)\n            mode, expr = 'command', TexCmd(command, contents, arg)\n        elif command == 'begin':\n            mode, expr, _ = 'begin', TexEnv(src.peek(1)), src.forward(3)\n        else:\n            mode, expr = 'command', TexCmd(command)\n\n        expr.args = read_args(src, expr.args)\n\n        if mode == 'begin':\n            read_env(src, expr)\n        return expr\n    if c in ARG_START_TOKENS:\n        return read_arg(src, c)\n    return c", "response": "r Read next expression from buffer src."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_arg(src, c):\n    content = [c]\n    while src.hasNext():\n        if src.peek() in ARG_END_TOKENS:\n            content.append(next(src))\n            break\n        else:\n            content.append(read_tex(src))\n    return Arg.parse(content)", "response": "Read the argument from buffer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pareto_front(self, *args, **kwargs):\n        if self._pareto_front is None:\n            self._pareto_front = self._calc_pareto_front(*args, **kwargs)\n\n        return self._pareto_front", "response": "Returns the Pareto front of a given problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the pareto set for a problem.", "response": "def pareto_set(self, *args, **kwargs):\n        \"\"\"\n        Returns\n        -------\n        S : np.array\n            Returns the pareto set for a problem. Points in the X space to be known to be optimal!\n        \"\"\"\n        if self._pareto_set is None:\n            self._pareto_set = self._calc_pareto_set(*args, **kwargs)\n\n        return self._pareto_set"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the given problem and return the values of the given object.", "response": "def evaluate(self,\n                 X,\n                 *args,\n                 return_values_of=\"auto\",\n                 return_as_dictionary=False,\n                 **kwargs):\n\n        \"\"\"\n        Evaluate the given problem.\n\n        The function values set as defined in the function.\n        The constraint values are meant to be positive if infeasible. A higher positive values means \"more\" infeasible\".\n        If they are 0 or negative, they will be considered as feasible what ever their value is.\n\n        Parameters\n        ----------\n\n        X : np.array\n            A two dimensional matrix where each row is a point to evaluate and each column a variable.\n\n        return_as_dictionary : bool\n            If this is true than only one object, a dictionary, is returned. This contains all the results\n            that are defined by return_values_of. Otherwise, by default a tuple as defined is returned.\n\n        return_values_of : list of strings\n            You can provide a list of strings which defines the values that are returned. By default it is set to\n            \"auto\" which means depending on the problem the function values or additional the constraint violation (if\n            the problem has constraints) are returned. Otherwise, you can provide a list of values to be returned.\n\n            Allowed is [\"F\", \"CV\", \"G\", \"dF\", \"dG\", \"dCV\", \"hF\", \"hG\", \"hCV\", \"feasible\"] where the d stands for\n            derivative and h stands for hessian matrix.\n\n\n        Returns\n        -------\n\n            A dictionary, if return_as_dictionary enabled, or a list of values as defined in return_values_of.\n\n        \"\"\"\n\n        # make the array at least 2-d - even if only one row should be evaluated\n        only_single_value = len(np.shape(X)) == 1\n        X = np.atleast_2d(X)\n\n        # check the dimensionality of the problem and the given input\n        if X.shape[1] != self.n_var:\n            raise Exception('Input dimension %s are not equal to n_var %s!' % (X.shape[1], self.n_var))\n\n        # automatic return the function values and CV if it has constraints if not defined otherwise\n        if type(return_values_of) == str and return_values_of == \"auto\":\n            return_values_of = [\"F\"]\n            if self.n_constr > 0:\n                return_values_of.append(\"CV\")\n\n        # create the output dictionary for _evaluate to be filled\n        out = {}\n        for val in return_values_of:\n            out[val] = None\n\n        # all values that are set in the evaluation function\n        values_not_set = [val for val in return_values_of if val not in self.evaluation_of]\n\n        # have a look if gradients are not set and try to use autograd and calculate grading if implemented using it\n        gradients_not_set = [val for val in values_not_set if val.startswith(\"d\")]\n\n        # if no autograd is necessary for evaluation just traditionally use the evaluation method\n        if len(gradients_not_set) == 0:\n            self._evaluate(X, out, *args, **kwargs)\n            at_least2d(out)\n\n        # otherwise try to use autograd to calculate the gradient for this problem\n        else:\n\n            # calculate the function value by tracing all the calculations\n            root, _ = run_and_trace(self._evaluate, X, *[out])\n            at_least2d(out)\n\n            # the dictionary where the values are stored\n            deriv = {}\n\n            # if the result is calculated to be derivable\n            for key, val in out.items():\n\n                # if yes it is already a derivative\n                if key.startswith(\"d\"):\n                    continue\n\n                name = \"d\" + key\n                is_derivable = (type(val) == autograd.numpy.numpy_boxes.ArrayBox)\n\n                # if should be returned AND was not calculated yet AND is derivable using autograd\n                if name in return_values_of and out.get(name) is None and is_derivable:\n\n                    # calculate the jacobian matrix and set it - (ignore warnings of autograd here)\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"ignore\")\n\n                        if \"h\" + key not in out:\n                            jac = calc_jacobian(root, val)\n                        else:\n\n                            def calc_gradient(X):\n                                _out = {}\n                                _root, _ = run_and_trace(self._evaluate, X, *[_out])\n                                at_least2d(_out)\n                                jac = calc_jacobian(_root, _out[key])\n                                return jac\n\n                            _root, jac = run_and_trace(calc_gradient, X)\n\n                            hessian = []\n                            for k in range(jac.shape[1]):\n                                _hessian = calc_jacobian(_root, jac[:, k])\n                                hessian.append(_hessian[:, None, ...])\n                            hessian = np.concatenate(hessian, axis=1)\n                            deriv[\"h\" + key] = hessian\n\n                        deriv[name] = jac\n\n            # merge to the output\n            out = {**out, **deriv}\n\n            # convert back to conventional numpy arrays - no array box as return type\n            for key in out.keys():\n                if type(out[key]) == autograd.numpy.numpy_boxes.ArrayBox:\n                    out[key] = out[key]._value\n\n        # if constraint violation should be returned as well\n        if self.n_constr == 0:\n            CV = np.zeros([X.shape[0], 1])\n        else:\n            CV = Problem.calc_constraint_violation(out[\"G\"])\n\n        if \"CV\" in return_values_of:\n            out[\"CV\"] = CV\n\n        # if an additional boolean flag for feasibility should be returned\n        if \"feasible\" in return_values_of:\n            out[\"feasible\"] = (CV <= 0)\n\n        # remove the first dimension of the output - in case input was a 1d- vector\n        if only_single_value:\n            for key in out.keys():\n                if out[key] is not None:\n                    out[key] = out[key][0, :]\n\n        if return_as_dictionary:\n            return out\n        else:\n\n            # if just a single value do not return a tuple\n            if len(return_values_of) == 1:\n                return out[return_values_of[0]]\n            else:\n                return tuple([out[val] for val in return_values_of])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fraction_visited(source, sink, waypoint, msm):\n\n    for_committors = committors([source], [sink], msm)\n    cond_committors = conditional_committors(source, sink, waypoint, msm)\n\n    if hasattr(msm, 'all_transmats_'):\n        frac_visited = np.zeros((msm.n_states,))\n        for i, tprob in enumerate(msm.all_transmats_):\n            frac_visited[i] = _fraction_visited(source, sink, waypoint,\n                                                msm.transmat_, for_committors,\n                                                cond_committors)\n        return np.median(frac_visited, axis=0)\n\n    return _fraction_visited(source, sink, waypoint, msm.transmat_,\n                             for_committors, cond_committors)", "response": "Returns the fraction of visits between source and sink states."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the hub score for one or more waypoints.", "response": "def hub_scores(msm, waypoints=None):\n    \"\"\"\n    Calculate the hub score for one or more waypoints\n\n    The \"hub score\" is a measure of how well traveled a certain state or\n    set of states is in a network. Specifically, it is the fraction of\n    times that a walker visits a state en route from some state A to another\n    state B, averaged over all combinations of A and B.\n\n\n    Parameters\n    ----------\n    msm : msmbuilder.MarkovStateModel\n        MSM to analyze\n    waypoints : array_like, int, optional\n        The index of the intermediate state (or more than one).\n        If None, then all waypoints will be used\n\n    Returns\n    -------\n    hub_score : float\n        The hub score for the waypoint\n\n    References\n    ----------\n    .. [1] Dickson & Brooks (2012), J. Chem. Theory Comput., 8, 3044-3052.\n    \"\"\"\n\n    n_states = msm.n_states_\n    if isinstance(waypoints, int):\n        waypoints = [waypoints]\n    elif waypoints is None:\n        waypoints = xrange(n_states)\n    elif not (isinstance(waypoints, list) or\n              isinstance(waypoints, np.ndarray)):\n        raise ValueError(\"waypoints (%s) must be an int, a list, or None\" %\n                         str(waypoints))\n\n    hub_scores = []\n    for waypoint in waypoints:\n        other_states = (i for i in xrange(n_states) if i != waypoint)\n\n        # calculate the hub score for this waypoint\n        hub_score = 0.0\n        for (source, sink) in itertools.permutations(other_states, 2):\n            hub_score += fraction_visited(source, sink, waypoint, msm)\n\n        hub_score /= float((n_states - 1) * (n_states - 2))\n        hub_scores.append(hub_score)\n\n    return np.array(hub_scores)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the fraction of visiting a source state to a sink state.", "response": "def _fraction_visited(source, sink, waypoint, tprob, for_committors,\n                      cond_committors):\n    \"\"\"\n    Calculate the fraction of times a walker on `tprob` going from `sources`\n    to `sinks` will travel through the set of states `waypoints` en route.\n\n    Computes the conditional committors q^{ABC^+} and uses them to find the\n    fraction of paths mentioned above.\n\n    Note that in the notation of Dickson et. al. this computes h_c(A,B), with\n        sources   = A\n        sinks     = B\n        waypoint  = C\n\n    Parameters\n    ----------\n    source : int\n        The index of the source state\n    sink : int\n        The index of the sink state\n    waypoint : int\n        The index of the intermediate state\n    tprob : np.ndarray\n        Transition matrix\n    for_committors : np.ndarray\n        The forward committors for the reaction sources -> sinks\n    cond_committors : np.ndarray\n        Conditional committors, i.e. the probability of visiting\n        a waypoint when on a path between source and sink.\n\n    Returns\n    -------\n    fraction_visited : float\n        The fraction of times a walker going from `sources` -> `sinks` stops\n        by `waypoints` on its way.\n\n    See Also\n    --------\n    msmbuilder.tpt.conditional_committors\n        Calculate the probability of visiting a waypoint while on a path\n        between a source and sink.\n    msmbuilder.tpt.hub_scores : function\n        Compute the 'hub score', the weighted fraction of visits for an\n        entire network.\n\n    References\n    ----------\n    .. [1] Dickson & Brooks (2012), J. Chem. Theory Comput., 8, 3044-3052.\n    \"\"\"\n\n    fraction_visited = (np.float(tprob[source, :].dot(cond_committors)) /\n                        np.float(tprob[source, :].dot(for_committors)))\n\n    return fraction_visited"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, sequences, y=None):\n        super(BACE, self).fit(sequences, y=y)\n        if self.n_macrostates is not None:\n            self._do_lumping()\n        else:\n            raise RuntimeError('n_macrostates must not be None to fit')\n\n        return self", "response": "Fit a BACE lumping model using a sequence of cluster assignments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _do_lumping(self):\n        c = copy.deepcopy(self.countsmat_)\n        if self.sliding_window:\n            c *= self.lag_time\n\n        c, macro_map, statesKeep = self._filterFunc(c)\n\n        w = np.array(c.sum(axis=1)).flatten()\n        w[statesKeep] += 1\n\n        unmerged = np.zeros(w.shape[0], dtype=np.int8)\n        unmerged[statesKeep] = 1\n\n        # get nonzero indices in upper triangle\n        indRecalc = self._getInds(c, statesKeep)\n        dMat = np.zeros(c.shape, dtype=np.float32)\n\n        i = 0\n        nCurrentStates = statesKeep.shape[0]\n\n        self.bayesFactors = {}\n\n        dMat, minX, minY = self._calcDMat(c, w, indRecalc, dMat,\n                                         statesKeep, unmerged)\n\n        while nCurrentStates > self.n_macrostates:\n            c, w, indRecalc, dMat, macro_map, statesKeep, unmerged, minX, minY = self._mergeTwoClosestStates(\n                c, w, indRecalc, dMat, macro_map,\n                statesKeep, minX, minY, unmerged)\n\n            nCurrentStates -= 1\n\n            if self.save_all_maps:\n                saved_map = copy.deepcopy(macro_map)\n                self.map_dict[nCurrentStates] = saved_map\n\n            if nCurrentStates - 1 == self.n_macrostates:\n                self.microstate_mapping_ = macro_map", "response": "Do the BACE lumping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and fit a lumped version of the given MSM object.", "response": "def from_msm(cls, msm, n_macrostates, filter=1.1, save_all_maps=True,\n                 n_proc=1, chunk_size=100):\n        \"\"\"Create and fit lumped model from pre-existing MSM.\n\n        Parameters\n        ----------\n        msm : MarkovStateModel\n            The input microstate msm to use.\n        n_macrostates : int\n            The number of macrostates\n\n        Returns\n        -------\n        lumper : cls\n            The fit MVCA object.\n        \"\"\"\n        params = msm.get_params()\n        lumper = cls(n_macrostates, filter, save_all_maps, n_proc,\n                     chunk_size, **params)\n\n        lumper.transmat_ = msm.transmat_\n        lumper.populations_ = msm.populations_\n        lumper.mapping_ = msm.mapping_\n        lumper.countsmat_ = msm.countsmat_\n        lumper.n_states_ = msm.n_states_\n\n        if n_macrostates is not None:\n            lumper._do_lumping()\n\n        return lumper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef percentage(self):\n        if self.currval >= self.maxval:\n            return 100.0\n        return self.currval * 100.0 / self.maxval", "response": "Returns the progress as a percentage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef doublewell_eigs(n_grid, lag_time=1):\n    return _brownian_eigs(n_grid, lag_time, DOUBLEWELL_GRAD_POTENTIAL,\n                          -np.pi, np.pi, reflect_bc=True)", "response": "Analytic eigenvalues and eigenenvectors for the doublwell system"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _brownian_eigs(n_grid, lag_time, grad_potential, xmin, xmax, reflect_bc):\n    transmat = _brownian_transmat(n_grid, lag_time, grad_potential, xmin, xmax, reflect_bc)\n    u, lv, rv = _solve_msm_eigensystem(transmat, k=len(transmat) - 1)\n    return u, rv", "response": "Analytic eigenvalues and eigenenvectors for 1D Brownian dynamics\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_packages():\n    packages = ['mdtraj.scripts']\n    for dir,subdirs,files in os.walk('MDTraj'):\n        package = dir.replace(os.path.sep, '.')\n        if '__init__.py' not in files:\n            # not a package\n            continue\n        packages.append(package.replace('MDTraj', 'mdtraj'))\n    return packages", "response": "Find all of mdtraj s python packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _detect_sse3(self):\n        \"Does this compiler support SSE3 intrinsics?\"\n        self._print_support_start('SSE3')\n        result = self.hasfunction('__m128 v; _mm_hadd_ps(v,v)',\n                           include='<pmmintrin.h>',\n                           extra_postargs=['-msse3'])\n        self._print_support_end('SSE3', result)\n        return result", "response": "Does this compiler support SSE3 intrinsics?"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo this compiler support SSE4. 1 intrinsics?", "response": "def _detect_sse41(self):\n        \"Does this compiler support SSE4.1 intrinsics?\"\n        self._print_support_start('SSE4.1')\n        result = self.hasfunction( '__m128 v; _mm_round_ps(v,0x00)',\n                           include='<smmintrin.h>',\n                           extra_postargs=['-msse4'])\n        self._print_support_end('SSE4.1', result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uncertainty_K(self):\n        if self.information_ is None:\n            self._build_information()\n\n        sigma_K = _ratematrix.sigma_K(\n            self.information_, theta=self.theta_, n=self.n_states_)\n        return sigma_K", "response": "Estimate of the element - wise asymptotic standard deviation\n        in the rate matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestimates of the element - wise asymptotic standard deviation in the stationary distribution.", "response": "def uncertainty_pi(self):\n        \"\"\"Estimate of the element-wise asymptotic standard deviation\n        in the stationary distribution.\n        \"\"\"\n        if self.information_ is None:\n            self._build_information()\n\n        sigma_pi = _ratematrix.sigma_pi(\n            self.information_, theta=self.theta_, n=self.n_states_)\n        return sigma_pi"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uncertainty_eigenvalues(self):\n        if self.information_ is None:\n            self._build_information()\n\n        sigma_eigenvalues = _ratematrix.sigma_eigenvalues(\n            self.information_, theta=self.theta_, n=self.n_states_)\n\n        if self.n_timescales is None:\n            return sigma_eigenvalues\n        return np.nan_to_num(sigma_eigenvalues[:self.n_timescales+1])", "response": "Estimate of the element - wise asymptotic standard deviation\n            in the model eigenvalues\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uncertainty_timescales(self):\n        if self.information_ is None:\n            self._build_information()\n\n        sigma_timescales = _ratematrix.sigma_timescales(\n            self.information_, theta=self.theta_, n=self.n_states_)\n\n        if self.n_timescales is None:\n            return sigma_timescales\n        return sigma_timescales[:self.n_timescales]", "response": "Estimate of the element - wise asymptotic standard deviation\n        in the model relaxation timescales."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initial_guess(self, countsmat):\n\n        if self.theta_ is not None:\n            return self.theta_\n\n        if self.guess == 'log':\n            transmat, pi = _transmat_mle_prinz(countsmat)\n            K = np.real(scipy.linalg.logm(transmat)) / self.lag_time\n\n        elif self.guess == 'pseudo':\n            transmat, pi = _transmat_mle_prinz(countsmat)\n            K = (transmat - np.eye(self.n_states_)) / self.lag_time\n\n        elif isinstance(self.guess, np.ndarray):\n            pi = _solve_ratemat_eigensystem(self.guess)[1][:, 0]\n            K = self.guess\n\n        S = np.multiply(np.sqrt(np.outer(pi, 1/pi)), K)\n        sflat = np.maximum(S[np.triu_indices_from(countsmat, k=1)], 0)\n        theta0 = np.concatenate((sflat, np.log(pi)))\n        return theta0", "response": "Generate an initial guess for the countsmat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the inverse of hessian of the log likelihood at theta_", "response": "def _build_information(self):\n        \"\"\"Build the inverse of hessian of the log likelihood at theta_\n        \"\"\"\n        lag_time = float(self.lag_time)\n\n        # only the \"active set\" of variables not at the bounds of the\n        # feasible set.\n        inds = np.where(self.theta_ != 0)[0]\n\n        hessian = _ratematrix.hessian(\n            self.theta_, self.countsmat_, t=lag_time, inds=inds)\n\n        self.information_ = np.zeros((len(self.theta_), len(self.theta_)))\n        self.information_[np.ix_(inds, inds)] = scipy.linalg.pinv(-hessian)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef score(self, sequences, y=None):\n        # eigenvectors from the model we're scoring, `self`\n        V = self.right_eigenvectors_\n\n        m2 = self.__class__(**self.get_params())\n        m2.fit(sequences)\n\n        if self.mapping_ != m2.mapping_:\n            V = self._map_eigenvectors(V, m2.mapping_)\n\n        S = np.diag(m2.populations_)\n        C = S.dot(m2.transmat_)\n\n        try:\n            trace = np.trace(V.T.dot(C.dot(V)).dot(np.linalg.inv(V.T.dot(S.dot(V)))))\n        except np.linalg.LinAlgError:\n            trace = np.nan\n\n        return trace", "response": "Score the model on new data using the generalized matrix Rayleigh."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog of the posterior probability and gradient of the posterior.", "response": "def _log_posterior(theta, counts, alpha, beta, n):\n    \"\"\"Log of the posterior probability and gradient\n\n    Parameters\n    ----------\n    theta : ndarray, shape=(n_params,)\n        The free parameters of the reversible rate matrix\n    counts : ndarray, shape=(n, n)\n        The count matrix (sufficient statistics for the likielihood)\n    alpha : ndarray, shape=(n,)\n        Dirichlet concentration parameters\n    beta : ndarray, shape=(n_params-n,)\n        Scale parameter for the exponential prior on the symmetric rate\n        matrix.\n    \"\"\"\n    # likelihood + grad\n    logp1, grad = loglikelihood(theta, counts)\n    # exponential prior on s_{ij}\n    logp2 = lexponential(theta[:-n], beta, grad=grad[:-n])\n    # dirichlet prior on \\pi\n    logp3 = ldirichlet_softmax(theta[-n:], alpha=alpha, grad=grad[-n:])\n    logp = logp1 + logp2 + logp3\n    return logp, grad"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_timescales_(self):\n\n        us, lvs, rvs = self._get_eigensystem()\n        # make sure to leave off equilibrium distribution\n        timescales = -1 / us[:,1:]\n        return timescales", "response": "Returns the time for each sample in the ensemble."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn random integer from a categorical distribution.", "response": "def categorical(pvals, size=None, random_state=None):\n    \"\"\"Return random integer from a categorical distribution\n\n    Parameters\n    ----------\n    pvals : sequence of floats, length p\n        Probabilities of each of the ``p`` different outcomes.  These\n        should sum to 1.\n    size : int or tuple of ints, optional\n        Defines the shape of the returned array of random integers. If None\n        (the default), returns a single float.\n    random_state: RandomState or an int seed, optional\n        A random number generator instance.\n    \"\"\"\n    cumsum = np.cumsum(pvals)\n    if size is None:\n        size = (1,)\n        axis = 0\n    elif isinstance(size, tuple):\n        size = size + (1,)\n        axis = len(size) - 1\n    else:\n        raise TypeError('size must be an int or tuple of ints')\n\n    random_state = check_random_state(random_state)\n    return np.sum(cumsum < random_state.random_sample(size), axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the forward committors of the reaction sources and sinks.", "response": "def committors(sources, sinks, msm):\n    \"\"\"\n    Get the forward committors of the reaction sources -> sinks.\n\n    Parameters\n    ----------\n    sources : array_like, int\n        The set of unfolded/reactant states.\n    sinks : array_like, int\n        The set of folded/product states.\n    msm : msmbuilder.MarkovStateModel\n        MSM fit to the data.\n\n    Returns\n    -------\n    forward_committors : np.ndarray\n        The forward committors for the reaction sources -> sinks\n\n    References\n    ----------\n    .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of\n           transition paths. J. Stat. Phys. 123, 503-523 (2006).\n    .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E.\n           Transition path theory for Markov jump processes.\n           Multiscale Model. Simul. 7, 1192-1219 (2009).\n    .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive\n           flux and folding pathways in network models of\n           coarse-grained protein dynamics. J. Chem. Phys.\n           130, 205102 (2009).\n    .. [4] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding\n           pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009):\n           19011-19016.\n    \"\"\"\n\n    if hasattr(msm, 'all_transmats_'):\n        commits = np.zeros(msm.all_transmats_.shape[:2])\n        for i, tprob in enumerate(msm.all_transmats_):\n            commits[i, :] = _committors(sources, sinks, tprob)\n        return np.median(commits, axis=0)\n\n    return _committors(sources, sinks, msm.transmat_)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef conditional_committors(source, sink, waypoint, msm):\n\n    # typecheck\n    for data in [source, sink, waypoint]:\n        if not isinstance(data, int):\n            raise ValueError(\"source, sink, and waypoint must be integers.\")\n\n    if (source == waypoint) or (sink == waypoint) or (sink == source):\n        raise ValueError('source, sink, waypoint must all be disjoint!')\n\n    if hasattr(msm, 'all_transmats_'):\n        cond_committors = np.zeros(msm.all_transmats_.shape[:2])\n        for i, tprob in enumerate(msm.all_transmats_):\n            cond_committors[i, :] = _conditional_committors(source, sink,\n                                                            waypoint, tprob)\n        return np.median(cond_committors, axis=0)\n\n    return _conditional_committors(source, sink, waypoint, msm.transmat_)", "response": "Returns the conditional committors for a given source and sink."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _conditional_committors(source, sink, waypoint, tprob):\n\n    n_states = np.shape(tprob)[0]\n\n    forward_committors = _committors([source], [sink], tprob)\n\n    # permute the transition matrix into cannonical form - send waypoint the the\n    # last row, and source + sink to the end after that\n    Bsink_indices = [source, sink, waypoint]\n    perm = np.array([i for i in xrange(n_states) if i not in Bsink_indices],\n                    dtype=int)\n    perm = np.concatenate([perm, Bsink_indices])\n    permuted_tprob = tprob[perm, :][:, perm]\n\n    # extract P, R\n    n = n_states - len(Bsink_indices)\n    P = permuted_tprob[:n, :n]\n    R = permuted_tprob[:n, n:]\n\n    # calculate the conditional committors ( B = N*R ), B[i,j] is the prob\n    # state i ends in j, where j runs over the source + sink + waypoint\n    # (waypoint is position -1)\n    B = np.dot(np.linalg.inv(np.eye(n) - P), R)\n\n    # add probs for the sinks, waypoint / b[i] is P( i --> {C & not A, B} )\n    b = np.append(B[:, -1].flatten(), [0.0] * (len(Bsink_indices) - 1) + [1.0])\n    cond_committors = b * forward_committors[waypoint]\n\n    # get the original order\n    cond_committors = cond_committors[np.argsort(perm)]\n\n    return cond_committors", "response": "Computes the conditional committors for a given source and sink state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the forward committors of the reaction sources and sinks.", "response": "def _committors(sources, sinks, tprob):\n    \"\"\"\n    Get the forward committors of the reaction sources -> sinks.\n\n    Parameters\n    ----------\n    sources : array_like, int\n        The set of unfolded/reactant states.\n    sinks : array_like, int\n        The set of folded/product states.\n    tprob : np.ndarray\n        Transition matrix\n\n    Returns\n    -------\n    forward_committors : np.ndarray\n        The forward committors for the reaction sources -> sinks\n\n    References\n    ----------\n    .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of\n           transition paths. J. Stat. Phys. 123, 503-523 (2006).\n    .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E.\n           Transition path theory for Markov jump processes.\n           Multiscale Model. Simul. 7, 1192-1219 (2009).\n    .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive\n           flux and folding pathways in network models of\n           coarse-grained protein dynamics. J. Chem. Phys.\n           130, 205102 (2009).\n    .. [4] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding\n           pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009):\n           19011-19016.\n    \"\"\"\n    n_states = np.shape(tprob)[0]\n\n    sources = np.array(sources, dtype=int).reshape((-1, 1))\n    sinks = np.array(sinks, dtype=int).reshape((-1, 1))\n\n    # construct the committor problem\n    lhs = np.eye(n_states) - tprob\n\n    for a in sources:\n        lhs[a, :] = 0.0  # np.zeros(n)\n        lhs[:, a] = 0.0\n        lhs[a, a] = 1.0\n\n    for b in sinks:\n        lhs[b, :] = 0.0  # np.zeros(n)\n        lhs[:, b] = 0.0\n        lhs[b, b] = 1.0\n\n    ident_sinks = np.zeros(n_states)\n    ident_sinks[sinks] = 1.0\n\n    rhs = np.dot(tprob, ident_sinks)\n    rhs[sources] = 0.0\n    rhs[sinks] = 1.0\n\n    forward_committors = np.linalg.solve(lhs, rhs)\n\n    return forward_committors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate figure 4 from Metzner s paper [ 1 ].", "response": "def _metzner_figure_4():\n    \"\"\"Generate figure 4 from Metzner's paper [1].\n    \n    This can be used as a rough test of the sampler\n    \"\"\"\n    import matplotlib.pyplot as pp\n    def _scatter(Ts, xi, xj, yi, yj):\n        pp.grid(False)\n        pp.hexbin(Ts[:, xi, xj], Ts[:, yi, yj], cmap='hot_r', vmin=0, vmax=100)\n        pp.xlabel('T_{%d,%d}' % (xi+1, xj+1))\n        pp.ylabel('T_{%d,%d}' % (yi+1, yj+1))\n        pp.plot([0,1], [1,0], c='k')\n        pp.ylim(0, 1)\n        pp.xlim(0, 1)\n    \n    C = np.array([[1, 10, 2], [2, 26, 3], [15, 20, 20]])\n    Ts = np.array(list(metzner_mcmc_slow(C, 100000)))\n\n    pp.figure(figsize=(6, 6)); pp.subplot(axisbg=(0,0,0,0))\n    _scatter(Ts, 0, 1, 0, 2)\n\n    pp.figure(figsize=(6, 6)); pp.subplot(axisbg=(0,0,0,0))\n    _scatter(Ts, 1, 0, 1, 2)\n    pp.figure(figsize=(6, 6)); pp.subplot(axisbg=(0,0,0,0))\n    _scatter(Ts, 2, 0, 2, 1)\n    pp.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_perturb_params(countsmat, transmat=None):\n    '''\n    Computes transition probabilities and standard errors of the transition probabilities due to \n    finite sampling using the MSM counts matrix. First, the transition probabilities are computed \n    by dividing the each element c_ij by the row-sumemd counts of row i. THe standard errors are then\n    computed by first computing the standard deviation of the transition probability, treating each count \n    as a Bernoulli process with p = t_ij (std = (t_ij - t_ij ^2)^0.5). This is then divided by the \n    square root of the row-summed counts of row i to obtain the standard error.\n    \n    Parameters:\n    ----------\n    countsmat: np.ndarray\n        The msm counts matrix\n    transmat: np.ndarray\n        If you have a transition matrix you want to use (e.g. MLE symmetrized), you can supply that here. This\n        function will use the transition probabilities from this matrix to calculate the Bernoulli standard deviations, \n        which will be divided by the row-summed counts in the original supplied counts matrix.\n\n    Returns:\n    -----------\n    transmat, np.ndarray:\n        The MSM transition matrix\n    scale, np.ndarray:\n        The matrix of standard errors for each transition probability\n    '''\n    norm = np.sum(countsmat, axis=1)\n    if not transmat:\n        transmat = (countsmat.transpose() / norm).transpose()\n    counts = (np.ones((len(transmat), len(transmat))) * norm).transpose()\n    scale = ((transmat - transmat ** 2) ** 0.5 / counts ** 0.5) + 10 ** -15\n    return transmat, scale", "response": "This function creates the perturbation parameters for a count \n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a sampled MSM transition matrix by treating each nonzero entry in the MSM transition matrix by treating it as a Gaussian random variable and the mean t_ij and standard deviation equal to the standard error computed using create_perturb_params.", "response": "def perturb_tmat(transmat, scale):\n    '''\n    Perturbs each nonzero entry in the MSM transition matrix by treating it as a Gaussian random variable\n    with mean t_ij and standard deviation equal to the standard error computed using \"create_perturb_params\".\n    Returns a sampled transition matrix that takes into consideration errors due to finite sampling\n    (useful for boostrapping, etc.)\n\n    Parameters:\n    ----------\n    transmat: np.ndarray:\n        The transition matrix, whose elements serve as the means of the Gaussian random variables\n    scale: np.ndarray:\n        The matrix of standard errors. For transition probability t_ij, this is assumed to be the standard \n        error of the mean of a binomial distribution with p = transition probability and number of observations \n        equal to the summed counts in row i.\n\n    '''\n    output = np.vectorize(np.random.normal)(transmat, scale)\n    output[np.where(output < 0)] = 0\n    return (output.transpose() / np.sum(output, axis=1)).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_layout():\n    tica_msm = TemplateDir(\n        'tica',\n        [\n            'tica/tica.py',\n            'tica/tica-plot.py',\n            'tica/tica-sample-coordinate.py',\n            'tica/tica-sample-coordinate-plot.py',\n        ],\n        [\n            TemplateDir(\n                'cluster',\n                [\n                    'cluster/cluster.py',\n                    'cluster/cluster-plot.py',\n                    'cluster/sample-clusters.py',\n                    'cluster/sample-clusters-plot.py',\n                ],\n                [\n                    TemplateDir(\n                        'msm',\n                        [\n                            'msm/timescales.py',\n                            'msm/timescales-plot.py',\n                            'msm/microstate.py',\n                            'msm/microstate-plot.py',\n                            'msm/microstate-traj.py',\n                        ],\n                        [],\n                    )\n                ]\n            )\n        ]\n    )\n    layout = TemplateDir(\n        '',\n        [\n            '0-test-install.py',\n            '1-get-example-data.py',\n            'README.md',\n        ],\n        [\n            TemplateDir(\n                'analysis',\n                [\n                    'analysis/gather-metadata.py',\n                    'analysis/gather-metadata-plot.py',\n                ],\n                [\n                    TemplateDir(\n                        'rmsd',\n                        [\n                            'rmsd/rmsd.py',\n                            'rmsd/rmsd-plot.py',\n                        ],\n                        [],\n                    ),\n                    TemplateDir(\n                        'landmarks',\n                        [\n                            'landmarks/find-landmarks.py',\n                            'landmarks/featurize.py',\n                            'landmarks/featurize-plot.py',\n                        ],\n                        [tica_msm],\n                    ),\n                    TemplateDir(\n                        'dihedrals',\n                        [\n                            'dihedrals/featurize.py',\n                            'dihedrals/featurize-plot.py',\n                        ],\n                        [tica_msm],\n                    )\n                ]\n            )\n        ]\n    )\n    return layout", "response": "Specify a hierarchy of our templates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the named TemplateDir in the hierarchy", "response": "def find(self, name, limit=None):\n        \"\"\"Find the named TemplateDir in the hierarchy\"\"\"\n        if name == self.name:\n            if limit is not None:\n                assert limit == 1\n                self.subdirs = []\n            return self\n        for subdir in self.subdirs:\n            res = subdir.find(name, limit)\n            if res is not None:\n                return res\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the implied timescales for a given set of state - items.", "response": "def implied_timescales(sequences, lag_times, n_timescales=10,\n                       msm=None, n_jobs=1, verbose=0):\n    \"\"\"\n    Calculate the implied timescales for a given MSM.\n\n    Parameters\n    ----------\n    sequences : list of array-like\n        List of sequences, or a single sequence. Each \n        sequence should be a 1D iterable of state\n        labels. Labels can be integers, strings, or\n        other orderable objects.\n    lag_times : array-like\n        Lag times to calculate implied timescales at.\n    n_timescales : int, optional\n        Number of timescales to calculate.\n    msm : msmbuilder.msm.MarkovStateModel, optional\n        Instance of an MSM to specify parameters other\n        than the lag time. If None, then the default\n        parameters (as implemented by msmbuilder.msm.MarkovStateModel)\n        will be used.\n    n_jobs : int, optional\n        Number of jobs to run in parallel\n\n    Returns\n    -------\n    timescales : np.ndarray, shape = [n_models, n_timescales]\n        The slowest timescales (in units of lag times) for each\n        model.\n    \"\"\"\n\n    if msm is None:\n        msm = MarkovStateModel()\n\n    param_grid = {'lag_time' : lag_times}\n    models = param_sweep(msm, sequences, param_grid, n_jobs=n_jobs,\n                         verbose=verbose)\n    timescales = [m.timescales_ for m in models]\n    n_timescales = min(n_timescales, min(len(ts) for ts in timescales))\n    timescales = np.array([ts[:n_timescales] for ts in timescales])\n    return timescales"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _replace_labels(doc):\n    lines = doc.splitlines()\n    labelstart, labelend = None, None\n    foundattributes = False\n    for i, line in enumerate(lines):\n        stripped = line.strip()\n        if stripped == 'Attributes':\n            foundattributes = True\n        if foundattributes and not labelstart and stripped.startswith('labels_'):\n            labelstart = len('\\n'.join(lines[:i])) + 1\n        if labelstart and not labelend and stripped == '':\n            labelend = len('\\n'.join(lines[:i + 1]))\n\n    if labelstart is None or labelend is None:\n        return doc\n\n    replace = '\\n'.join([\n        '    labels_ : list of arrays, each of shape [sequence_length, ]',\n        '        The label of each point is an integer in [0, n_clusters).',\n        '',\n    ])\n    return doc[:labelstart] + replace + doc[labelend:]", "response": "Really hacky find - and - replace method that replaces the labels_ in the sklearn base class with the labels_ in the sklearn base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npredicts the closest cluster each sample in X belongs to.", "response": "def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            New data to predict.\n\n        Returns\n        -------\n        Y : array, shape [n_samples,]\n            Index of the closest center each sample belongs to.\n        \"\"\"\n        if isinstance(X, np.ndarray):\n            if not (X.dtype == 'float32' or X.dtype == 'float64'):\n                X = X.astype('float64')\n        labels, inertia = libdistance.assign_nearest(\n            X, self.cluster_centers_, metric=self.metric)\n        return labels"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, sequences, y=None):\n        MultiSequenceClusterMixin.fit(self, sequences)\n        self.cluster_ids_ = self._split_indices(self.cluster_ids_)\n        return self", "response": "Fit the kcenters clustering on the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave an arbitrary python object to disk using pickle.", "response": "def dump(value, filename, compress=None, cache_size=None):\n    \"\"\"Save an arbitrary python object using pickle.\n\n    Parameters\n    -----------\n    value : any Python object\n        The object to store to disk using pickle.\n    filename : string\n        The name of the file in which it is to be stored\n    compress : None\n        No longer used\n    cache_size : positive number, optional\n        No longer used\n\n    See Also\n    --------\n    load : corresponding loader\n    \"\"\"\n    if compress is not None or cache_size is not None:\n        warnings.warn(\"compress and cache_size are no longer valid options\")\n\n    with open(filename, 'wb') as f:\n        pickle.dump(value, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads an object that has been saved with dump.", "response": "def load(filename):\n    \"\"\"Load an object that has been saved with dump.\n\n    We try to open it using the pickle protocol. As a fallback, we\n    use joblib.load. Joblib was the default prior to msmbuilder v3.2\n\n    Parameters\n    ----------\n    filename : string\n        The name of the file to load.\n    \"\"\"\n    try:\n        with open(filename, 'rb') as f:\n            return pickle.load(f)\n    except Exception as e1:\n        try:\n            return jl_load(filename)\n        except Exception as e2:\n            raise IOError(\n                \"Unable to load {} using the pickle or joblib protocol.\\n\"\n                \"Pickle: {}\\n\"\n                \"Joblib: {}\".format(filename, e1, e2)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping value to file fn", "response": "def verbosedump(value, fn, compress=None):\n    \"\"\"Verbose wrapper around dump\"\"\"\n    print('Saving \"%s\"... (%s)' % (fn, type(value)))\n    dump(value, fn, compress=compress)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the transition path theory flux matrix for a set of sources and sinks.", "response": "def fluxes(sources, sinks, msm, for_committors=None):\n    \"\"\"\n    Compute the transition path theory flux matrix.\n\n    Parameters\n    ----------\n    sources : array_like, int\n        The set of unfolded/reactant states.\n    sinks : array_like, int\n        The set of folded/product states.\n    msm : msmbuilder.MarkovStateModel\n        MSM that has been fit to data.\n    for_committors : np.ndarray, optional\n        The forward committors associated with `sources`, `sinks`, and `tprob`.\n        If not provided, is calculated from scratch. If provided, `sources`\n        and `sinks` are ignored.\n\n    Returns\n    -------\n    flux_matrix : np.ndarray\n        The flux matrix\n\n    See Also\n    --------\n    net_fluxes\n\n    References\n    ----------\n    .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of\n           transition paths. J. Stat. Phys. 123, 503-523 (2006).\n    .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E.\n           Transition path theory for Markov jump processes.\n           Multiscale Model. Simul. 7, 1192-1219 (2009).\n    .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive\n           flux and folding pathways in network models of\n           coarse-grained protein dynamics. J. Chem. Phys.\n           130, 205102 (2009).\n    .. [4] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding\n           pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009):\n           19011-19016.\n    \"\"\"\n\n    if hasattr(msm, 'all_transmats_'):\n        fluxes = np.zeros_like(msm.all_transmats_)\n\n        for i, el in enumerate(zip(msm.all_transmats_, msm.all_populations_)):\n            tprob = el[0]\n            populations = el[1]\n            fluxes[i, :, :] = _fluxes(sources, sinks, tprob,\n                                      populations, for_committors)\n        return np.median(fluxes, axis=0)\n\n    return _fluxes(sources, sinks, msm.transmat_, msm.populations_,\n                   for_committors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef net_fluxes(sources, sinks, msm, for_committors=None):\n\n    flux_matrix = fluxes(sources, sinks, msm, for_committors=for_committors)\n\n    net_flux = flux_matrix - flux_matrix.T\n    net_flux[np.where(net_flux < 0)] = 0.0\n\n    return net_flux", "response": "Computes the net flux matrix for a set of sources and sinks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the transition path theory flux matrix for a set of sources sinks and tprob.", "response": "def _fluxes(sources, sinks, tprob, populations, for_committors=None):\n    \"\"\"\n    Compute the transition path theory flux matrix.\n\n    Parameters\n    ----------\n    sources : array_like, int\n        The set of unfolded/reactant states.\n    sinks : array_like, int\n        The set of folded/product states.\n    tprob : np.ndarray\n        Transition matrix\n    populations : np.ndarray, (n_states,)\n        MSM populations\n    for_committors : np.ndarray, optional\n        The forward committors associated with `sources`, `sinks`, and `tprob`.\n        If not provided, is calculated from scratch. If provided, `sources`\n        and `sinks` are ignored.\n\n    Returns\n    -------\n    flux_matrix : np.ndarray\n        The flux matrix\n\n    See Also\n    --------\n    net_fluxes\n\n    References\n    ----------\n    .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of\n           transition paths. J. Stat. Phys. 123, 503-523 (2006).\n    .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E.\n           Transition path theory for Markov jump processes.\n           Multiscale Model. Simul. 7, 1192-1219 (2009).\n    .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive\n           flux and folding pathways in network models of\n           coarse-grained protein dynamics. J. Chem. Phys.\n           130, 205102 (2009).\n    .. [4] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding\n           pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009):\n           19011-19016.\n    \"\"\"\n    n_states = np.shape(populations)[0]\n\n    # check if we got the committors\n    if for_committors is None:\n        for_committors = _committors(sources, sinks, tprob)\n    else:\n        for_committors = np.array(for_committors)\n        if for_committors.shape != (n_states,):\n            raise ValueError(\"Shape of committors %s should be %s\" %\n                             (str(for_committors.shape), str((n_states,))))\n\n    sources = np.array(sources).reshape((-1,))\n    sinks = np.array(sinks).reshape((-1,))\n\n    X = np.zeros((n_states, n_states))\n    X[(np.arange(n_states), np.arange(n_states))] = (populations *\n                                                     (1.0 - for_committors))\n\n    Y = np.zeros((n_states, n_states))\n    Y[(np.arange(n_states), np.arange(n_states))] = for_committors\n\n    fluxes = np.dot(np.dot(X, tprob), Y)\n    fluxes[(np.arange(n_states), np.arange(n_states))] = np.zeros(n_states)\n\n    return fluxes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef featurize_all(filenames, featurizer, topology, chunk=1000, stride=1):\n    data = []\n    indices = []\n    fns = []\n\n    for file in filenames:\n        kwargs = {} if file.endswith('.h5') else {'top': topology}\n        count = 0\n        for t in md.iterload(file, chunk=chunk, stride=stride, **kwargs):\n            x = featurizer.partial_transform(t)\n            n_frames = len(x)\n\n            data.append(x)\n            indices.append(count + (stride * np.arange(n_frames)))\n            fns.extend([file] * n_frames)\n            count += (stride * n_frames)\n    if len(data) == 0:\n        raise ValueError(\"None!\")\n\n    return np.concatenate(data), np.concatenate(indices), np.array(fns)", "response": "Load and featurize many MD trajectories with the same topology."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef describe_features(self, traj):\n        n_f = self.partial_transform(traj).shape[1]\n        zippy=zip(itertools.repeat(\"N/A\", n_f),\n                  itertools.repeat(\"N/A\", n_f),\n                  itertools.repeat(\"N/A\", n_f),\n                  itertools.repeat((\"N/A\",\"N/A\",\"N/A\",\"N/A\"), n_f))\n\n        return dict_maker(zippy)", "response": "Generic method for describing features in the notion of the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partial_transform(self, traj):\n        traj.superpose(self.reference_traj,\n                       atom_indices=self.superpose_atom_indices)\n        diff2 = (traj.xyz[:, self.atom_indices] -\n                 self.reference_traj.xyz[0, self.atom_indices]) ** 2\n        x = np.sqrt(np.sum(diff2, axis=2))\n        return x", "response": "Featurize an MD trajectory into a vector space via distance\n clf to the original mdtraj. TrajectoryTrajectory\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of dictionaries describing the LandmarkRMSD features.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the LandmarkRMSD features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each feature\n                - resnames: unique names of residues\n                - atominds: the four atom indicies\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: Alpha Angle\n                - featuregroup: the type of dihedral angle and whether sin or\n                  cos has been applied.\n        \"\"\"\n        feature_descs = []\n        # fill in the atom indices using just the first frame\n        self.partial_transform(traj[0])\n        top = traj.topology\n\n        aind_tuples = [self.atom_indices for _ in range(self.sliced_reference_traj.n_frames)]\n        zippy = zippy_maker(aind_tuples, top)\n\n        zippy = itertools.product([\"LandMarkFeaturizer\"], [\"RMSD\"], [self.sigma], zippy)\n\n        feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef partial_transform(self, traj):\n        d = md.geometry.compute_distances(traj, self.pair_indices,\n                                          periodic=self.periodic)\n        return d ** self.exponent", "response": "Featurize an MD trajectory into a vector space via pairwise\n        atom - atom distances"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of dictionaries describing the atom pair features.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the atom pair features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each dihedral\n                - resnames: unique names of residues\n                - atominds: the two atom inds\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: AtomPairsFeaturizer\n                - featuregroup: Distance.\n                - other info : Value of the exponent\n        \"\"\"\n        feature_descs = []\n\n        top = traj.topology\n        residue_indices = [[top.atom(i[0]).residue.index, top.atom(i[1]).residue.index] \\\n                           for i in self.atom_indices]\n\n        aind = []\n        resseqs = []\n        resnames = []\n        for ind,resid_ids in enumerate(residue_indices):\n            aind += [[i for i in self.atom_indices[ind]]]\n            resseqs += [[top.residue(ri).resSeq for ri in resid_ids]]\n            resnames += [[top.residue(ri).name for ri in resid_ids]]\n\n        zippy = itertools.product([\"AtomPairs\"], [\"Distance\"],\n                                  [\"Exponent {}\".format(self.exponent)],\n                                  zip(aind, resseqs, residue_indices, resnames))\n\n        feature_descs.extend(dict_maker(zippy))\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of dictionaries describing the dihderal features for the given trajectory.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the dihderal features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each dihedral\n                - resnames: unique names of residues\n                - atominds: the four atom indicies\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: Dihedral\n                - featuregroup: the type of dihedral angle and whether sin or\n                  cos has been applied.\n        \"\"\"\n\n        feature_descs = []\n        for dihed_type in self.types:\n            # TODO: Don't recompute dihedrals, just get the indices\n            func = getattr(md, 'compute_%s' % dihed_type)\n            # ainds is a list of four-tuples of atoms participating\n            # in each dihedral\n            aind_tuples, _ = func(traj)\n            top = traj.topology\n            zippy = zippy_maker(aind_tuples, top)\n\n            if self.sincos:\n                zippy = itertools.product(['Dihedral'],[dihed_type], ['sin', 'cos'], zippy)\n            else:\n                zippy = itertools.product(['Dihedral'],[dihed_type], ['nosincos'], zippy)\n\n            feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef partial_transform(self, traj):\n        x = []\n        for a in self.types:\n            func = getattr(md, 'compute_%s' % a)\n            _, y = func(traj)\n\n            if self.sincos:\n                x.extend([np.sin(y), np.cos(y)])\n            else:\n                x.append(y)\n\n        return np.hstack(x)", "response": "Featurize an MD trajectory into a vector space via calculation\n            of dihedral angles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of dictionaries describing the dihderal features of the ligand.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the dihderal features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each dihedral\n                - resnames: unique names of residues\n                - atominds: the four atom indicies\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: Dihedral\n                - featuregroup: The bin index(0..nbins-1)\n                and dihedral type(phi/psi/chi1 etc )\n        \"\"\"\n        feature_descs = []\n        for dihed_type in self.types:\n            # TODO: Don't recompute dihedrals, just get the indices\n            func = getattr(md, 'compute_%s' % dihed_type)\n            # ainds is a list of four-tuples of atoms participating\n            # in each dihedral\n            aind_tuples, _ = func(traj)\n\n            top = traj.topology\n            bin_info =[]\n            resseqs = []\n            resids = []\n            resnames = []\n            all_aind = []\n            #its bin0---all phis bin1--all_phis\n            for bin_index in range(self.n_bins):\n                for ainds in aind_tuples:\n                    resid = set(top.atom(ai).residue.index for ai in ainds)\n                    all_aind.append(ainds)\n                    bin_info += [\"bin-%d\"%bin_index]\n                    resids += [list(resid)]\n                    reseq = set(top.atom(ai).residue.resSeq for ai in ainds)\n                    resseqs += [list(reseq)]\n                    resname = set(top.atom(ai).residue.name for ai in ainds)\n                    resnames += [list(resname)]\n\n            zippy = zip(all_aind, resseqs, resids, resnames)\n            #fast check to make sure we have the right number of features\n            assert len(bin_info) == len(aind_tuples) * self.n_bins\n\n            zippy = zip([\"VonMises\"]*len(bin_info),\n                        [dihed_type]*len(bin_info),\n                        bin_info,\n                        zippy)\n\n            feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef partial_transform(self, traj):\n        x = []\n        for a in self.types:\n            func = getattr(md, 'compute_%s' % a)\n            _, y = func(traj)\n            res = vm.pdf(y[..., np.newaxis],\n                         loc=self.loc, kappa=self.kappa)\n            #we reshape the results using a  Fortran-like index order,\n            #so that it goes over the columns first. This should put the results\n            #phi dihedrals(all bin0 then all bin1), psi dihedrals(all_bin1)\n            x.extend(np.reshape(res, (1, -1, self.n_bins*y.shape[1]), order='F'))\n        return np.hstack(x)", "response": "Featurize an MD trajectory into a vector space via calculation\n            of soft - bins over dihdral angle space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef describe_features(self, traj):\n        feature_descs = []\n        # fill in the atom indices using just the first frame\n        self.partial_transform(traj[0])\n        top = traj.topology\n        if self.atom_indices is None:\n            raise ValueError(\"Cannot describe features for \"\n                             \"trajectories with \"\n                              \"fewer than 4 alpha carbon\"\n                              \"using AlphaAngleFeaturizer.\")\n\n        aind_tuples = self.atom_indices\n\n        zippy = zippy_maker(aind_tuples, top)\n\n        if self.sincos:\n            zippy = itertools.product([\"AlphaAngle\"], [\"N/A\"], ['cos', 'sin'], zippy)\n        else:\n            zippy = itertools.product([\"AlphaAngle\"], [\"N/A\"], ['nosincos'], zippy)\n\n        feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs", "response": "Return a list of dictionaries describing the dihderal features for the given trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of dictionaries describing the SASA features.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the SASA features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each SASA feature\n                - resnames: names of residues\n                - atominds: atom index or atom indices in mode=\"residue\"\n                - resseqs: residue ids (not necessarily 0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: SASA\n                - featuregroup: atom or residue\n        \"\"\"\n\n        feature_descs = []\n        _, mapping = md.geometry.sasa.shrake_rupley(traj, mode=self.mode, get_mapping=True)\n        top = traj.topology\n\n        if self.mode == \"residue\":\n            resids = np.unique(mapping)\n            resseqs = [top.residue(ri).resSeq for ri in resids]\n            resnames = [top.residue(ri).name for ri in resids]\n            atoms_in_res = [res.atoms for res in top.residues]\n            aind_tuples = []\n            # For each resdiue...\n            for i,x in enumerate(atoms_in_res):\n                # For each atom in the residues, append it's index\n                aind_tuples.append([atom.index for atom in x])\n            zippy = itertools.product(['SASA'],['N/A'],[self.mode], zip(aind_tuples, resseqs, resids, resnames))\n        else:\n            resids = [top.atom(ai).residue.index for ai in mapping]\n            resseqs = [top.atom(ai).residue.resSeq for ai in mapping]\n            resnames = [top.atom(ai).residue.name for ai in mapping]\n            zippy = itertools.product(['SASA'],['N/A'],[self.mode], zip(mapping, resseqs, resids, resnames))\n\n        feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of dictionaries describing the contacts features.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the contacts features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each dihedral\n                - resnames: unique names of residues\n                - atominds: atom indices(returns CA if scheme is ca_inds,otherwise\n                            returns all atom_inds)\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: Contact\n                - featuregroup: ca, heavy etc.\n        \"\"\"\n        feature_descs = []\n        # fill in the atom indices using just the first frame\n        if self.soft_min:\n            distances, residue_indices = md.compute_contacts(traj[0], self.contacts,\n                                                         self.scheme,\n                                                         self.ignore_nonprotein,\n                                                         soft_min=self.soft_min,\n                                                         soft_min_beta=self.soft_min_beta,\n                                                         periodic=self.periodic)\n        else:\n            distances, residue_indices = md.compute_contacts(traj[0], self.contacts,\n                                                         self.scheme,\n                                                         self.ignore_nonprotein,\n                                                         periodic=self.periodic)\n        top = traj.topology\n\n        aind = []\n        resseqs = []\n        resnames = []\n        if self.scheme=='ca':\n            atom_ind_list = [[j.index for j in i.atoms if j.name=='CA']\n                             for i in top.residues]\n        elif self.scheme=='closest-heavy':\n            atom_ind_list = [[j.index for j in i.atoms if j.element.name!=\"hydrogen\"]\n                             for i in top.residues]\n        elif self.scheme=='closest':\n            atom_ind_list = [[j.index for j in i.atoms] for i in top.residues]\n        else:\n            atom_ind_list = [[\"N/A\"] for i in top.residues]\n\n        for resid_ids in residue_indices:\n            aind += [[atom_ind_list[ri] for ri in resid_ids]]\n            resseqs += [[top.residue(ri).resSeq for ri in resid_ids]]\n            resnames += [[top.residue(ri).name for ri in resid_ids]]\n        zippy = itertools.product([\"Contact\"], [self.scheme],\n                                  [\"{}\".format(self.soft_min_beta)],\n                                  zip(aind, resseqs, residue_indices, resnames))\n\n        feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partial_transform(self, traj):\n        if self.index is not None:\n            return traj[:, self.index]\n        else:\n            return traj[:, :self.first]", "response": "Slice a single input array along to select a subset of features."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, sequences, y=None):\n        MultiSequenceClusterMixin.fit(self, sequences)\n        self.distances_ = self._split(self.distances_)\n        return self", "response": "Fit the kcenters clustering on the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef param_sweep(model, sequences, param_grid, n_jobs=1, verbose=0):\n\n    if isinstance(param_grid, dict):\n        param_grid = ParameterGrid(param_grid)\n    elif not isinstance(param_grid, ParameterGrid):\n        raise ValueError(\"param_grid must be a dict or ParamaterGrid instance\")\n\n    # iterable with (model, sequence) as items\n    iter_args = ((clone(model).set_params(**params), sequences)\n                 for params in param_grid)\n\n    models = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        delayed(_param_sweep_helper)(args) for args in iter_args)\n\n    return models", "response": "Fit a series of models over a range of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a dataset object for storing lists of sequences on disk.", "response": "def dataset(path, mode='r', fmt=None, verbose=False, **kwargs):\n    \"\"\"Open a dataset object\n\n    MSMBuilder supports several dataset 'formats' for storing\n    lists of sequences on disk.\n\n    This function can also be used as a context manager.\n\n    Parameters\n    ----------\n    path : str\n        The path to the dataset on the filesystem\n    mode : {'r', 'w', 'a'}\n        Open a dataset for reading, writing, or appending. Note that\n        some formats only support a subset of these modes.\n    fmt : {'dir-npy', 'hdf5', 'mdtraj'}\n        The format of the data on disk\n\n        ``dir-npy``\n            A directory of binary numpy files, one file per sequence\n\n        ``hdf5``\n            A single hdf5 file with each sequence as an array node\n\n        ``mdtraj``\n            A read-only set of trajectory files that can be loaded\n            with mdtraj\n\n    verbose : bool\n        Whether to print information about the dataset\n\n    \"\"\"\n\n    if mode == 'r' and fmt is None:\n        fmt = _guess_format(path)\n    elif mode in 'wa' and fmt is None:\n        raise ValueError('mode=\"%s\", but no fmt. fmt=%s' % (mode, fmt))\n\n    if fmt == 'dir-npy':\n        return NumpyDirDataset(path, mode=mode, verbose=verbose)\n    elif fmt == 'mdtraj':\n        return MDTrajDataset(path, mode=mode, verbose=verbose, **kwargs)\n    elif fmt == 'hdf5':\n        return HDF5Dataset(path, mode=mode, verbose=verbose)\n    elif fmt.endswith(\"-union\"):\n        raise ValueError(\"union datasets have been removed. \"\n                         \"Please use msmbuilder.featurizer.FeatureUnion\")\n    else:\n        raise NotImplementedError(\"Unknown format fmt='%s'\" % fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nguessing the format of a dataset based on its filename.", "response": "def _guess_format(path):\n    \"\"\"Guess the format of a dataset based on its filename / filenames.\n    \"\"\"\n    if os.path.isdir(path):\n        return 'dir-npy'\n\n    if path.endswith('.h5') or path.endswith('.hdf5'):\n        # TODO: Check for mdtraj .h5 file\n        return 'hdf5'\n\n    # TODO: What about a list of trajectories, e.g. from command line nargs='+'\n    return 'mdtraj'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _keynat(string):\n    r = []\n    for c in string:\n        if c.isdigit():\n            if r and isinstance(r[-1], int):\n                r[-1] = r[-1] * 10 + int(c)\n            else:\n                r.append(int(c))\n        else:\n            r.append(9 + ord(c))\n    return r", "response": "A natural sort helper function for sort and sorted() without using regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the partial_transform method of the estimator on this dataset Parameters ---------- estimator : object with ``partial_fit`` method This object will be used to transform this dataset into a new dataset. The estimator should be fitted prior to calling this method. out_ds : str or Dataset This dataset will be transformed and saved into out_ds. If out_ds is a path, a new dataset will be created at that path. fmt : str The type of dataset to create if out_ds is a string. Returns ------- out_ds : Dataset The tranformed dataset.", "response": "def transform_with(self, estimator, out_ds, fmt=None):\n        \"\"\"Call the partial_transform method of the estimator on this dataset\n\n        Parameters\n        ----------\n        estimator : object with ``partial_fit`` method\n            This object will be used to transform this dataset into a new\n            dataset. The estimator should be fitted prior to calling\n            this method.\n        out_ds : str or Dataset\n            This dataset will be transformed and saved into out_ds. If\n            out_ds is a path, a new dataset will be created at that path.\n        fmt : str\n            The type of dataset to create if out_ds is a string.\n\n        Returns\n        -------\n        out_ds : Dataset\n            The tranformed dataset.\n        \"\"\"\n        if isinstance(out_ds, str):\n            out_ds = self.create_derived(out_ds, fmt=fmt)\n        elif isinstance(out_ds, _BaseDataset):\n            err = \"Dataset must be opened in write mode.\"\n            assert out_ds.mode in ('w', 'a'), err\n        else:\n            err = \"Please specify a dataset path or an existing dataset.\"\n            raise ValueError(err)\n        for key in self.keys():\n            out_ds[key] = estimator.partial_transform(self[key])\n        return out_ds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new dataset with the given estimator and save it into out_ds.", "response": "def fit_transform_with(self, estimator, out_ds, fmt=None):\n        \"\"\"Create a new dataset with the given estimator.\n\n        The estimator will be fit by this dataset, and then each trajectory\n        will be transformed by the estimator.\n\n        Parameters\n        ----------\n        estimator : BaseEstimator\n            This object will be fit and used to transform this dataset\n            into a new dataset.\n        out_ds : str or Dataset\n            This dataset will be transformed and saved into out_ds. If\n            out_ds is a path, a new dataset will be created at that path.\n        fmt : str\n            The type of dataset to create if out_ds is a string.\n\n        Returns\n        -------\n        out_ds : Dataset\n            The transformed dataset.\n\n        Examples\n        --------\n        diheds = dataset(\"diheds\")\n        tica = diheds.fit_transform_with(tICA(), 'tica')\n        kmeans = tica.fit_transform_with(KMeans(), 'kmeans')\n        msm = kmeans.fit_with(MarkovStateModel())\n        \"\"\"\n        self.fit_with(estimator)\n        return self.transform_with(estimator, out_ds, fmt=fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a discrete approximation to a multivariate normal distribution.", "response": "def discrete_approx_mvn(X, means, covars, match_variances=True):\n    \"\"\"Find a discrete approximation to a multivariate normal distribution.\n\n    The method employs find the discrete distribution with support only at the\n    supplied points X with minimal K-L divergence to a target multivariate\n    normal distribution under the constraints that the mean and variance\n    of the discrete distribution match the normal distribution exactly.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape=(n_points, n_features)\n        The allowable points\n    means : np.ndarray, shape=(n_features)\n        The mean vector of the MVN\n    covars : np.ndarray, shape=(n_features, n_features) or shape=(n_features,)\n        If covars is 2D, it's interpreted as the covariance matrix for\n        the model. If 1D, we assume a diagonal covariance matrix with the\n        specified diagonal entries.\n    match_variances : bool, optimal\n        When True, both the means and the variances of the discrete distribution\n        are constrained. Under some circumstances, this is not satisfiable (e.g.\n        if there aren't enough samples\n\n    Returns\n    -------\n    weights : np.ndarray, shape=(n_samples,)\n        The weight for each of the points in X in the resulting\n        discrete probability distribution\n\n    Notes\n    -----\n    The discrete distribution is one that has mass only at the specified\n    points. It can therefore be parameterized by a set of weights on each\n    point. If :math:`\\{X_i\\}` is the set of allowable points, and\n    :math:`\\{w_i\\}` are the weights, then our discrete distribution has\n    the form\n\n    .. math::\n\n        p(y; w) = w_i \\sum \\delta(y - X_i).\n\n    We chose the :math:`w_i` by minimizing the K-L divergence from the our\n    discrete distribution to the desired multivariate normal subject to a\n    constraint that the first moments of the discrete distribution match\n    the mean of the multivariate normal exactly, and that the variances\n    also match. Let :math:`q(x)` be the target distribution. The optimal\n    weights are then\n\n    .. math::\n\n        min_{\\{w_i\\}} \\sum_i p(X_i; w) \\log \\frac{p(X_i; w)}{q(X_i)}\n\n    subject to\n\n    .. math::\n\n        \\sum_i (X_i)       p(X_i; w) = \\int_\\Omega (x)    q(x) = \\mu,\n        \\sum_i (X_i-mu)**2 p(X_i; w) = \\int_\\Omega (x-mu) q(x).\n\n    References\n    ----------\n    .. [1] Tanaka, Ken'ichiro, and Alexis Akira Toda. \"Discrete approximations\n    of continuous distributions by maximum entropy.\" Economics Letters 118.3\n    (2013): 445-450.\n    \"\"\"\n    X = ensure_type(np.asarray(X), dtype=np.float32, ndim=2, name='X', warn_on_cast=False)\n    means = ensure_type(np.asarray(means), np.float64, ndim=1, name='means', warn_on_cast=False)\n    covars = np.asarray(covars)\n\n    # Get the un-normalized probability of each point X_i in the MVN\n    # `prob` are the q(X_i) in the mathematics\n    # `moments` are the \\bar{T} that we want to match.\n    if covars.ndim == 1:\n        # diagonal covariance case\n        if not len(covars) == len(means):\n            raise ValueError('Shape Error: covars and means musth have the same length')\n        prob = np.exp(-0.5 * np.sum(1. / np.sqrt(covars) * (X - means) ** 2, axis=1))\n        moments = np.concatenate((means, covars)) if match_variances else means\n\n    elif covars.ndim == 2:\n        if not (covars.shape[0] == len(means) and covars.shape[1] == len(means)):\n            raise ValueError('Shape Error: covars must be square, with size = len(means)')\n\n        # full 2d covariance matrix\n        cv_chol = scipy.linalg.cholesky(covars, lower=True)\n        cv_sol = scipy.linalg.solve_triangular(cv_chol, (X - means).T, lower=True).T\n        prob = np.exp(-0.5 * (np.sum(cv_sol ** 2, axis=1)))\n        moments = np.concatenate((means, np.diag(covars))) if match_variances else means\n    else:\n        raise ValueError('covars must be 1D or 2D')\n\n    # this is T(x_i) for each X_i\n    moment_contributions = np.hstack((X, (X - means) ** 2)) if match_variances else X\n\n    def objective_and_grad(l):\n        dot = np.dot(moment_contributions, l)\n        lse = scipy.misc.logsumexp(dot, b=prob)\n        # value of the objective function\n        obj_value = lse - np.dot(l, moments)\n\n        # gradient of objective function\n        dot_max = dot.max(axis=0)\n\n        exp_term = np.sum(moment_contributions * (prob * np.exp(dot - dot_max)).reshape(-1, 1), axis=0)\n        log_numerator = np.log(exp_term) + dot_max\n        grad_value = np.exp(log_numerator - lse) - moments\n\n        return obj_value, grad_value\n\n    result = scipy.optimize.minimize(\n        objective_and_grad, jac=True, x0=np.ones_like(moments), method='BFGS')\n    if not result['success']:\n        raise NotSatisfiableError()\n\n    dot = np.dot(moment_contributions, result['x'])\n    log_denominator = scipy.misc.logsumexp(dot, b=prob)\n    weights = prob * np.exp(dot - log_denominator)\n    if not np.all(np.isfinite(weights)):\n        raise NotSatisfiableError()\n    weights = weights / np.sum(weights)\n    return weights"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing agglomerative clustering. Parameters ---------- X : array-like, shape=(n_samples, n_features) Returns ------- self", "response": "def fit(self, X, y=None):\n        \"\"\"\n        Compute agglomerative clustering.\n\n        Parameters\n        ----------\n        X : array-like, shape=(n_samples, n_features)\n\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.max_landmarks is not None:\n            if self.n_clusters > self.n_landmarks:\n                self.n_landmarks = self.max_landmarks\n        \n        if self.n_landmarks is None:\n            distances = pdist(X, self.metric)\n            tree = linkage(distances, method=self.linkage)\n            self.landmark_labels_ = fcluster(tree, criterion='maxclust',\n                                             t=self.n_clusters) - 1\n            self.cardinality_ = np.bincount(self.landmark_labels_)\n            self.squared_distances_within_cluster_ = np.zeros(self.n_clusters)\n\n            n = len(X)\n            for k in range(len(distances)):\n                i = int(n - 2 - np.floor(np.sqrt(-8*k + 4*n*(n-1)-7)/2.0 - 0.5))\n                j = int(k + i + 1 - n*(n-1)/2 + (n-i)*((n-i)-1)/2)\n                if self.landmark_labels_[i] == self.landmark_labels_[j]:\n                    self.squared_distances_within_cluster_[\n                        self.landmark_labels_[i]] += distances[k] ** 2\n\n            self.landmarks_ = X\n\n        else:\n            if self.landmark_strategy == 'random':\n                land_indices = check_random_state(self.random_state).randint(\n                    len(X), size=self.n_landmarks)\n            else:\n                land_indices = np.arange(len(X))[::(len(X) //\n                                        self.n_landmarks)][:self.n_landmarks]\n\n            distances = pdist(X[land_indices], self.metric)\n            tree = linkage(distances, method=self.linkage)\n            self.landmark_labels_ = fcluster(tree, criterion='maxclust',\n                                             t=self.n_clusters) - 1\n            self.cardinality_ = np.bincount(self.landmark_labels_)\n            self.squared_distances_within_cluster_ = np.zeros(self.n_clusters)\n\n            n = len(X[land_indices])\n            for k in range(len(distances)):\n                i = int(n - 2 - np.floor(np.sqrt(-8*k + 4*n*(n-1)-7)/2.0 - 0.5))\n                j = int(k + i + 1 - n*(n-1)/2 + (n-i)*((n-i)-1)/2)\n                if self.landmark_labels_[i] == self.landmark_labels_[j]:\n                    self.squared_distances_within_cluster_[\n                        self.landmark_labels_[i]] += distances[k] ** 2\n\n            self.landmarks_ = X[land_indices]\n\n        if self.metric != 'rmsd':\n            cluster_centers_ = []\n            for i in range(self.n_clusters):\n                temp = list(np.mean(self.landmarks_[self.landmark_labels_==i], axis=0))\n                cluster_centers_.append(temp)\n            self.cluster_centers_ = np.array(cluster_centers_)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, X):\n\n        dists = cdist(X, self.landmarks_, self.metric)\n        pfunc_name = self.ward_predictor if self.linkage == 'ward' else self.linkage\n\n        try:\n            pooling_func = POOLING_FUNCTIONS[pfunc_name]\n        except KeyError:\n                raise ValueError(\"linkage {} is not supported\".format(pfunc_name))\n\n        pooled_distances = np.empty(len(X))\n        pooled_distances.fill(np.infty)\n        labels = np.zeros(len(X), dtype=int)\n\n        for i in range(self.n_clusters):\n            if np.any(self.landmark_labels_ == i):\n                d = pooling_func(dists[:, self.landmark_labels_ == i],\n                                 self.cardinality_[i],\n                                 self.squared_distances_within_cluster_[i])\n                if np.any(d < 0):\n                    warnings.warn(\"Distance shouldn't be negative.\")\n                mask = (d < pooled_distances)\n                pooled_distances[mask] = d[mask]\n                labels[mask] = i\n            else:\n                print(\"No data points were assigned to cluster {}\".format(i))\n\n        return labels", "response": "Predict the closest cluster each sample in X belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef top_path(sources, sinks, net_flux):\n    sources = np.array(sources, dtype=np.int).reshape((-1,))\n    sinks = np.array(sinks, dtype=np.int).reshape((-1,))\n\n    n_states = net_flux.shape[0]\n\n    queue = list(sources)\n    # nodes to check (the \"queue\")\n    # going to use list.pop method so I can't keep it as an array\n\n    visited = np.zeros(n_states).astype(np.bool)\n    # have we already checked this node?\n\n    previous_node = np.ones(n_states).astype(np.int) * -1\n    # what node was found before finding this one\n\n    min_fluxes = np.ones(n_states) * -1 * np.inf\n    # what is the flux of the highest flux path\n    # from this node to the source set.\n\n    min_fluxes[sources] = np.inf\n    # source states are connected to the source\n    # so this distance is zero which means the flux is infinite\n\n    while len(queue) > 0: # iterate until there's nothing to check anymore\n\n        test_node = queue.pop(min_fluxes[queue].argmax())\n        # find the node in the queue that has the\n        # highest flux path to it from the source set\n\n        visited[test_node] = True\n\n        if np.all(visited[sinks]):\n            # if we've visited all of the sink states, then we just have to choose\n            # the path that goes to the sink state that is closest to the source\n            break\n\n        # if test_node in sinks: # I *think* we want to break ... or are there paths we still\n        # need to check?\n        # continue\n        # I think if sinks is more than one state we have to check everything\n\n        # now update the distances for each neighbor of the test_node:\n        neighbors = np.where(net_flux[test_node, :] > 0)[0]\n        if len(neighbors) == 0:\n            continue\n\n        new_fluxes = net_flux[test_node, neighbors].flatten()\n        # flux from test_node to each neighbor\n\n        new_fluxes[np.where(new_fluxes > min_fluxes[test_node])] = min_fluxes[test_node]\n        # previous step to get to test_node was lower flux, so that is still the path flux\n\n        ind = np.where((1 - visited[neighbors]) & (new_fluxes > min_fluxes[neighbors]))\n        min_fluxes[neighbors[ind]] = new_fluxes[ind]\n\n        previous_node[neighbors[ind]] = test_node\n        # each of these neighbors came from this test_node\n        # we don't want to update the nodes that have already been visited\n\n        queue.extend(neighbors[ind])\n\n    top_path = []\n    # populate the path in reverse\n    top_path.append(int(sinks[min_fluxes[sinks].argmax()]))\n    # find the closest sink state\n\n    while previous_node[top_path[-1]] != -1:\n        top_path.append(previous_node[top_path[-1]])\n\n    return np.array(top_path[::-1]), min_fluxes[top_path[0]]", "response": "This function returns the array corresponding to the top path between two sets of source states and sink states."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the top N paths by iteratively performing Dijkstra's algorithm. Parameters ---------- sources : array_like, int One-dimensional list of nodes to define the source states. sinks : array_like, int One-dimensional list of nodes to define the sink states. net_flux : np.ndarray Net flux of the MSM remove_path : str or callable, optional Function for removing a path from the net flux matrix. (if str, one of {'subtract', 'bottleneck'}) See note below for more details. num_paths : int, optional Number of paths to find flux_cutoff : float, optional Quit looking for paths once the explained flux is greater than this cutoff (as a percentage of the total). Returns ------- paths : list List of paths. Each item is an array of nodes visited in the path. fluxes : np.ndarray, shape = [n_paths,] Flux of each path returned. Notes ----- The Dijkstra algorithm only allows for computing the *single* top flux pathway through the net flux matrix. If we want many paths, there are many ways of finding the *second* highest flux pathway. The algorithm proceeds as follows: 1. Using the Djikstra algorithm, find the highest flux pathway from the sources to the sink states 2. Remove that pathway from the net flux matrix by some criterion 3. Repeat (1) with the modified net flux matrix Currently, there are two schemes for step (2): - 'subtract' : Remove the path by subtracting the flux of the path from every edge in the path. This was suggested by Metzner, Schutte, and Vanden-Eijnden. Transition Path Theory for Markov Jump Processes. Multiscale Model. Simul. 7, 1192-1219 (2009). - 'bottleneck' : Remove the path by only removing the edge that corresponds to the bottleneck of the path. If a new scheme is desired, the user may pass a function that takes the net_flux and the path to remove and returns the new net flux matrix. See Also -------- msmbuilder.tpt.top_path : function for computing the single highest flux pathway through a network. References ---------- .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of transition paths. J. Stat. Phys. 123, 503-523 (2006). .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E. Transition path theory for Markov jump processes. Multiscale Model. Simul. 7, 1192-1219 (2009). .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive flux and folding pathways in network models of coarse-grained protein dynamics. J. Chem. Phys. 130, 205102 (2009). .. [4] Dijkstra, E. W. A Note on Two Problems in Connexion with Graphs. Numeriche Mathematik 1, 269-271 (1959). .. [5] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009): 19011-19016.", "response": "def paths(sources, sinks, net_flux, remove_path='subtract',\n          num_paths=np.inf, flux_cutoff=(1-1E-10)):\n    \"\"\"\n    Get the top N paths by iteratively performing Dijkstra's\n    algorithm.\n\n    Parameters\n    ----------\n    sources : array_like, int\n        One-dimensional list of nodes to define the source states.\n    sinks : array_like, int\n        One-dimensional list of nodes to define the sink states.\n    net_flux : np.ndarray\n        Net flux of the MSM\n    remove_path : str or callable, optional\n        Function for removing a path from the net flux matrix.\n        (if str, one of {'subtract', 'bottleneck'})\n        See note below for more details.\n    num_paths : int, optional\n        Number of paths to find\n    flux_cutoff : float, optional\n        Quit looking for paths once the explained flux is greater\n        than this cutoff (as a percentage of the total).\n\n    Returns\n    -------\n    paths : list\n        List of paths. Each item is an array of nodes visited\n        in the path.\n    fluxes : np.ndarray, shape = [n_paths,]\n        Flux of each path returned.\n\n    Notes\n    -----\n    The Dijkstra algorithm only allows for computing the\n    *single* top flux pathway through the net flux matrix. If\n    we want many paths, there are many ways of finding the\n    *second* highest flux pathway.\n\n    The algorithm proceeds as follows:\n\n    1. Using the Djikstra algorithm, find the highest flux\n       pathway from the sources to the sink states\n    2. Remove that pathway from the net flux matrix by\n       some criterion\n    3. Repeat (1) with the modified net flux matrix\n\n    Currently, there are two schemes for step (2):\n\n    - 'subtract' : Remove the path by subtracting the flux\n      of the path from every edge in the path. This was\n      suggested by Metzner, Schutte, and Vanden-Eijnden.\n      Transition Path Theory for Markov Jump Processes.\n      Multiscale Model. Simul. 7, 1192-1219 (2009).\n    - 'bottleneck' : Remove the path by only removing\n      the edge that corresponds to the bottleneck of the\n      path.\n\n    If a new scheme is desired, the user may pass a function\n    that takes the net_flux and the path to remove and returns\n    the new net flux matrix.\n\n    See Also\n    --------\n    msmbuilder.tpt.top_path : function for computing the single\n        highest flux pathway through a network.\n\n    References\n    ----------\n    .. [1] Weinan, E. and Vanden-Eijnden, E. Towards a theory of\n           transition paths. J. Stat. Phys. 123, 503-523 (2006).\n    .. [2] Metzner, P., Schutte, C. & Vanden-Eijnden, E.\n           Transition path theory for Markov jump processes.\n           Multiscale Model. Simul. 7, 1192-1219 (2009).\n    .. [3] Berezhkovskii, A., Hummer, G. & Szabo, A. Reactive\n           flux and folding pathways in network models of\n           coarse-grained protein dynamics. J. Chem. Phys.\n           130, 205102 (2009).\n    .. [4] Dijkstra, E. W. A Note on Two Problems in Connexion with Graphs.\n           Numeriche Mathematik 1, 269-271 (1959).\n    .. [5] Noe, Frank, et al. \"Constructing the equilibrium ensemble of folding\n           pathways from short off-equilibrium simulations.\" PNAS 106.45 (2009):\n           19011-19016.\n    \"\"\"\n\n    if not callable(remove_path):\n        if remove_path == 'subtract':\n            remove_path = _subtract_path_flux\n        elif remove_path == 'bottleneck':\n            remove_path = _remove_bottleneck\n        else:\n            raise ValueError(\"remove_path_func (%s) must be a callable or one of ['subtract', 'bottleneck']\" % str(remove_path))\n\n    net_flux = copy.copy(net_flux)\n\n    paths = []\n    fluxes = []\n\n    total_flux = net_flux[sources, :].sum()\n    # total flux is the total flux coming from the sources (or going into the sinks)\n\n    not_done = True\n    counter = 0\n    expl_flux = 0.0\n    while not_done:\n        path, flux = top_path(sources, sinks, net_flux)\n        if np.isinf(flux):\n            break\n\n        paths.append(path)\n        fluxes.append(flux)\n\n        expl_flux += flux / total_flux\n        counter += 1\n\n        if counter >= num_paths or expl_flux >= flux_cutoff:\n            break\n\n        # modify the net_flux matrix\n        net_flux = remove_path(net_flux, path)\n\n    fluxes = np.array(fluxes)\n\n    return paths, fluxes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, sequences, y=None):\n        self._initialized = False\n        check_iter_of_sequences(sequences, max_iter=3)  # we might be lazy-loading\n        for X in sequences:\n            self._fit(X)\n\n        if self.n_sequences_ == 0:\n            raise ValueError('All sequences were shorter than '\n                             'the lag time, %d' % self.lag_time)\n\n        return self", "response": "Fit the model with a collection of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies dimensionality reduction on X.", "response": "def transform(self, sequences):\n        \"\"\"Apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        sequences: list of array-like, each of shape (n_samples_i, n_features)\n            Training data, where n_samples_i in the number of samples\n            in sequence i and n_features is the number of features.\n\n        Returns\n        -------\n        sequence_new : list of array-like, each of shape (n_samples_i, n_components)\n\n        \"\"\"\n        check_iter_of_sequences(sequences, max_iter=3)  # we might be lazy-loading\n        sequences_new = []\n\n        for X in sequences:\n            X = array2d(X)\n            if self.means_ is not None:\n                X = X - self.means_\n            X_transformed = np.dot(X, self.components_.T)\n\n            if self.kinetic_mapping:\n                X_transformed *= self.eigenvalues_\n\n            if self.commute_mapping:\n                # thanks to @maxentile and @jchodera for providing/directing to a\n                # reference implementation in pyemma\n                #(markovmodel/PyEMMA#963)\n                # dampening smaller timescales based recommendtion of  [7]\n                #\n                # some timescales are NaNs and regularized timescales will \n                # be negative when they are less than the lag time; all these\n                # are set to zero using nan_to_num before returning\n                regularized_timescales = 0.5 * self.timescales_ *\\\n                                         np.tanh( np.pi *((self.timescales_ - self.lag_time)\n                                                          /self.lag_time) + 1)\n                X_transformed *= np.sqrt(regularized_timescales / 2)\n                X_transformed = np.nan_to_num(X_transformed)\n            sequences_new.append(X_transformed)\n\n        return sequences_new"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, X_all, y=None):\n        if self._sliding_window:\n            return [X[k::self._lag_time] for k in range(self._lag_time) for X in X_all]\n        else:\n            return [X[::self._lag_time] for X in X_all]", "response": "Subsample several time series."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the time - step of relaxation timescales for each sample in the ensemble.", "response": "def all_timescales_(self):\n        \"\"\"Implied relaxation timescales each sample in the ensemble\n\n        Returns\n        -------\n        timescales : array-like, shape = (n_samples, n_timescales,)\n            The longest implied relaxation timescales of the each sample in\n            the ensemble of transition matrices, expressed in units of\n            time-step between indices in the source data supplied\n            to ``fit()``.\n\n        References\n        ----------\n        .. [1] Prinz, Jan-Hendrik, et al. \"Markov models of molecular kinetics:\n        Generation and validation.\" J. Chem. Phys. 134.17 (2011): 174105.\n        \"\"\"\n\n        us, lvs, rvs = self._get_eigensystem()\n        # make sure to leave off equilibrium distribution\n        timescales = - self.lag_time / np.log(us[:, 1:])\n        return timescales"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iterate_tracker(maxiter, max_nc, verbose=False):\n    last_hash = None\n    last_hash_count = 0\n    arr = yield\n\n    for i in xrange(maxiter):\n        arr = yield i\n        if arr is not None:    \n            hsh = hashlib.sha1(arr.view(np.uint8)).hexdigest()\n            if last_hash == hsh:\n                last_hash_count += 1\n            else:\n                last_hash = hsh\n                last_hash_count = 1\n\n            if last_hash_count >= max_nc:\n                if verbose:\n                    print('Termination. Over %d iterations without '\n                          'change.' % max_nc)\n                break", "response": "Generator that breaks after maxiter times after the same\n    array has been sent in more max_nc times in a row."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscores the model on new data using the generalized matrix Rayleigh quotient.", "response": "def score(self, sequences, y=None):\n        \"\"\"Score the model on new data using the generalized matrix Rayleigh quotient\n\n        Parameters\n        ----------\n        sequences : list of array, each of shape (n_samples_i, n_features)\n            Test data. A list of sequences in afeature space, each of which is a 2D\n            array of possibily different lengths, but the same number of features.\n\n        Returns\n        -------\n        gmrq : float\n            Generalized matrix Rayleigh quotient. This number indicates how\n            well the top ``n_timescales+1`` eigenvectors of this tICA model perform\n            as slowly decorrelating collective variables for the new data in\n            ``sequences``.\n\n        References\n        ----------\n        .. [1] McGibbon, R. T. and V. S. Pande, \"Variational cross-validation\n           of slow dynamical modes in molecular kinetics\" J. Chem. Phys. 142,\n           124105 (2015)\n        \"\"\"\n\n        assert self._initialized\n        V = self.eigenvectors_\n\n        # Note: How do we deal with regularization parameters like gamma\n        # here? I'm not sure. Should C and S be estimated using self's\n        # regularization parameters?\n        m2 = self.__class__(shrinkage=self.shrinkage, n_components=self.n_components, lag_time=self.lag_time,\n                            landmarks=self.landmarks, kernel_params=self.kernel_params)\n        m2.fit(sequences)\n        numerator = V.T.dot(m2.offset_correlation_).dot(V)\n        denominator = V.T.dot(m2.covariance_).dot(V)\n\n        try:\n            trace = np.trace(numerator.dot(np.linalg.inv(denominator)))\n        except np.linalg.LinAlgError:\n            trace = np.nan\n        return trace"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit a PCCA lumping model using a sequence of cluster assignments.", "response": "def fit(self, sequences, y=None):\n        \"\"\"Fit a PCCA lumping model using a sequence of cluster assignments.\n\n        Parameters\n        ----------\n        sequences : list(np.ndarray(dtype='int'))\n            List of arrays of cluster assignments\n        y : None\n            Unused, present for sklearn compatibility only.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        super(PCCA, self).fit(sequences, y=y)\n        self._do_lumping()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_lumping(self):\n\n        # Extract non-perron eigenvectors\n        right_eigenvectors = self.right_eigenvectors_[:, 1:]\n\n        assert self.n_states_ > 0\n        microstate_mapping = np.zeros(self.n_states_, dtype=int)\n\n        def spread(x):\n            return x.max() - x.min()\n\n        for i in range(self.n_macrostates - 1):\n            v = right_eigenvectors[:, i]\n            all_spreads = np.array([spread(v[microstate_mapping == k])\n                                    for k in range(i + 1)])\n            state_to_split = np.argmax(all_spreads)\n            inds = ((microstate_mapping == state_to_split) &\n                    (v >= self.pcca_tolerance))\n            microstate_mapping[inds] = i + 1\n\n        self.microstate_mapping_ = microstate_mapping", "response": "Do the PCCA lumping."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and fit a new MarkovStateModel object from a MarkovStateModel object.", "response": "def from_msm(cls, msm, n_macrostates, objective_function=None):\n        \"\"\"Create and fit lumped model from pre-existing MSM.\n\n        Parameters\n        ----------\n        msm : MarkovStateModel\n            The input microstate msm to use.\n        n_macrostates : int\n            The number of macrostates\n\n        Returns\n        -------\n        lumper : cls\n            The fit PCCA(+) object.\n        \"\"\"\n        params = msm.get_params()\n        lumper = cls(n_macrostates=n_macrostates,\n                     objective_function=objective_function, **params)\n\n        lumper.transmat_ = msm.transmat_\n        lumper.populations_ = msm.populations_\n        lumper.mapping_ = msm.mapping_\n        lumper.countsmat_ = msm.countsmat_\n        lumper.n_states_ = msm.n_states_\n\n        lumper._do_lumping()\n\n        return lumper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the metastability of the PCCA + objective function.", "response": "def metastability(alpha, T, right_eigenvectors, square_map, pi):\n    \"\"\"Return the metastability PCCA+ objective function.\n\n    Parameters\n    ----------\n    alpha : ndarray\n        Parameters of objective function (e.g. flattened A)\n    T : csr sparse matrix\n        Transition matrix\n    right_eigenvectors : ndarray\n        The right eigenvectors.\n    square_map : ndarray\n        Mapping from square indices (i,j) to flat indices (k).\n    pi : ndarray\n        Equilibrium Populations of transition matrix.\n\n    Returns\n    -------\n    obj : float\n        The objective function\n\n    Notes\n    -------\n    metastability: try to make metastable fuzzy state decomposition.\n    Defined in ref. [2].\n    \"\"\"\n\n    num_micro, num_eigen = right_eigenvectors.shape\n\n    A, chi, mapping = calculate_fuzzy_chi(alpha, square_map,\n                                          right_eigenvectors)\n\n    # If current point is infeasible or leads to degenerate lumping.\n    if (len(np.unique(mapping)) != right_eigenvectors.shape[1] or\n            has_constraint_violation(A, right_eigenvectors)):\n        return -1.0 * np.inf\n\n    obj = 0.0\n    # Calculate  metastabilty of the lumped model.  Eqn 4.20 in LAA.\n    for i in range(num_eigen):\n        obj += np.dot(T.dot(chi[:, i]), pi * chi[:, i]) / np.dot(chi[:, i], pi)\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the crispness of the PCCA + objective function.", "response": "def crispness(alpha, T, right_eigenvectors, square_map, pi):\n    \"\"\"Return the crispness PCCA+ objective function.\n\n    Parameters\n    ----------\n    alpha : ndarray\n        Parameters of objective function (e.g. flattened A)\n    T : csr sparse matrix\n        Transition matrix\n    right_eigenvectors : ndarray\n        The right eigenvectors.\n    square_map : ndarray\n        Mapping from square indices (i,j) to flat indices (k).\n    pi : ndarray\n        Equilibrium Populations of transition matrix.\n\n    Returns\n    -------\n    obj : float\n        The objective function\n\n    Notes\n    -------\n    Tries to make crisp state decompostion.  This function is\n    defined in [3].\n    \"\"\"\n\n    A, chi, mapping = calculate_fuzzy_chi(alpha, square_map,\n                                          right_eigenvectors)\n\n    # If current point is infeasible or leads to degenerate lumping.\n    if (len(np.unique(mapping)) != right_eigenvectors.shape[1] or\n            has_constraint_violation(A, right_eigenvectors)):\n        return -1.0 * np.inf\n\n    obj = tr(dot(diag(1. / A[0]), dot(A.transpose(), A)))\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets mappings from the square array A to the flat vector of parameters alpha.", "response": "def get_maps(A):\n    \"\"\"Get mappings from the square array A to the flat vector of parameters\n    alpha.\n\n    Helper function for PCCA+ optimization.\n\n    Parameters\n    ----------\n    A : ndarray\n        The transformation matrix A.\n\n    Returns\n    -------\n    flat_map : ndarray\n        Mapping from flat indices (k) to square (i,j) indices.\n    square map : ndarray\n        Mapping from square indices (i,j) to flat indices (k).\n    \"\"\"\n\n    N = A.shape[0]\n    flat_map = []\n    for i in range(1, N):\n        for j in range(1, N):\n            flat_map.append([i, j])\n\n    flat_map = np.array(flat_map)\n\n    square_map = np.zeros(A.shape, 'int')\n\n    for k in range((N - 1) ** 2):\n        i, j = flat_map[k]\n        square_map[i, j] = k\n\n    return flat_map, square_map"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_constraint_violation(A, right_eigenvectors, epsilon=1E-8):\n\n    lhs = 1 - A[0, 1:].sum()\n    rhs = dot(right_eigenvectors[:, 1:], A[1:, 0])\n    rhs = -1 * rhs.min()\n\n    if abs(lhs - rhs) > epsilon:\n        return True\n    else:\n        return False", "response": "Checks if a constraint violation exists in the transformation matrix A."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind simplex structure in eigenvectors to begin PCCA +.", "response": "def index_search(right_eigenvectors):\n    \"\"\"Find simplex structure in eigenvectors to begin PCCA+.\n\n\n    Parameters\n    ----------\n    right_eigenvectors :  ndarray\n        Right eigenvectors of transition matrix\n\n    Returns\n    -------\n    index : ndarray\n        Indices of simplex\n    \"\"\"\n\n    num_micro, num_eigen = right_eigenvectors.shape\n\n    index = np.zeros(num_eigen, 'int')\n\n    # first vertex: row with largest norm\n    index[0] = np.argmax(\n            [norm(right_eigenvectors[i]) for i in range(num_micro)])\n\n    ortho_sys = right_eigenvectors - np.outer(np.ones(num_micro),\n                                              right_eigenvectors[index[0]])\n\n    for j in range(1, num_eigen):\n        temp = ortho_sys[index[j - 1]].copy()\n        for l in range(num_micro):\n            ortho_sys[l] -= temp * dot(ortho_sys[l], temp)\n\n        dist_list = np.array([norm(ortho_sys[l]) for l in range(num_micro)])\n\n        index[j] = np.argmax(dist_list)\n\n        ortho_sys /= dist_list.max()\n\n    return index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfilling in feasible initial guess for transition matrix A.", "response": "def fill_A(A, right_eigenvectors):\n    \"\"\"Construct feasible initial guess for transformation matrix A.\n\n\n    Parameters\n    ----------\n    A : ndarray\n        Possibly non-feasible transformation matrix.\n    right_eigenvectors :  ndarray\n        Right eigenvectors of transition matrix\n\n    Returns\n    -------\n    A : ndarray\n        Feasible transformation matrix.\n    \"\"\"\n    num_micro, num_eigen = right_eigenvectors.shape\n\n    A = A.copy()\n\n    # compute 1st column of A by row sum condition\n    A[1:, 0] = -1 * A[1:, 1:].sum(1)\n\n    # compute 1st row of A by maximum condition\n    A[0] = -1 * dot(right_eigenvectors[:, 1:].real, A[1:]).min(0)\n\n    # rescale A to be in the feasible set\n    A /= A[0].sum()\n\n    return A"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the fuzzy membership matrix.", "response": "def calculate_fuzzy_chi(alpha, square_map, right_eigenvectors):\n    \"\"\"Calculate the membership matrix (chi) from parameters alpha.\n\n    Parameters\n    ----------\n    alpha : ndarray\n        Parameters of objective function (e.g. flattened A)\n    square_map : ndarray\n        Mapping from square indices (i,j) to flat indices (k).\n    right_eigenvectors : ndarray\n        The right eigenvectors.\n\n    Returns\n    -------\n    A : ndarray\n        The transformation matrix A\n    chi_fuzzy : ndarray\n        The (fuzzy) membership matrix.\n    mapping: ndarray\n        The mapping from microstates to macrostates.\n    \"\"\"\n    # Convert parameter vector into matrix A\n    A = to_square(alpha, square_map)\n    # Make A feasible.\n    A = fill_A(A, right_eigenvectors)\n    # Calculate the fuzzy membership matrix.\n    chi_fuzzy = np.dot(right_eigenvectors, A)\n    # Calculate the microstate mapping.\n    mapping = np.argmax(chi_fuzzy, 1)\n    return A, chi_fuzzy, mapping"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform PCCA + algorithm by optimizing the transformation matrix A.", "response": "def _do_lumping(self):\n        \"\"\"Perform PCCA+ algorithm by optimizing transformation matrix A.\n\n        Creates the following member variables:\n        -------\n        A : ndarray\n            The transformation matrix.\n        chi : ndarray\n            The membership matrix\n        microstate_mapping : ndarray\n            Mapping from microstates to macrostates.\n\n        \"\"\"\n        right_eigenvectors = self.right_eigenvectors_[:, :self.n_macrostates]\n        index = index_search(right_eigenvectors)\n\n        # compute transformation matrix A as initial guess for local\n        # optimization (maybe not feasible)\n        A = right_eigenvectors[index, :]\n\n        A = inv(A)\n        A = fill_A(A, right_eigenvectors)\n\n        if self.do_minimization:\n            A = self._optimize_A(A)\n\n        self.A_ = fill_A(A, right_eigenvectors)\n        self.chi_ = dot(right_eigenvectors, self.A_)\n        self.microstate_mapping_ = np.argmax(self.chi_, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds optimal transformation matrix A by minimization.", "response": "def _optimize_A(self, A):\n        \"\"\"Find optimal transformation matrix A by minimization.\n\n        Parameters\n        ----------\n        A : ndarray\n            The transformation matrix A.\n\n        Returns\n        -------\n        A : ndarray\n            The transformation matrix.\n        \"\"\"\n        right_eigenvectors = self.right_eigenvectors_[:, :self.n_macrostates]\n        flat_map, square_map = get_maps(A)\n        alpha = to_flat(1.0 * A, flat_map)\n\n        def obj(x):\n            return -1 * self._objective_function(\n                    x, self.transmat_, right_eigenvectors, square_map,\n                    self.populations_\n            )\n\n        alpha = scipy.optimize.basinhopping(\n                obj, alpha, niter_success=1000,\n        )['x']\n\n        alpha = scipy.optimize.fmin(\n                obj, alpha, full_output=True, xtol=1E-4, ftol=1E-4,\n                maxfun=5000, maxiter=100000\n        )[0]\n\n        if np.isneginf(obj(alpha)):\n            raise ValueError(\n                    \"Error: minimization has not located a feasible point.\")\n\n        A = to_square(alpha, square_map)\n        return A"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the rate matrix for the current set of state k and return the dominant eigenpairs of the current set of state k.", "response": "def _solve_ratemat_eigensystem(theta, k, n):\n    \"\"\"Find the dominant eigenpairs of a reversible rate matrix (master\n    equation)\n\n    Parameters\n    ----------\n    theta : ndarray, shape=(n_params,)\n        The free parameters of the rate matrix\n    k : int\n        The number of eigenpairs to find\n    n : int\n        The number of states\n\n    Notes\n    -----\n    Normalize the left (:math:`\\phi`) and right (:math:``\\psi``) eigenfunctions\n    according to the following criteria.\n      * The first left eigenvector, \\phi_1, _is_ the stationary\n        distribution, and thus should be normalized to sum to 1.\n      * The left-right eigenpairs should be biorthonormal:\n        <\\phi_i, \\psi_j> = \\delta_{ij}\n      * The left eigenvectors should satisfy\n        <\\phi_i, \\phi_i>_{\\mu^{-1}} = 1\n      * The right eigenvectors should satisfy <\\psi_i, \\psi_i>_{\\mu} = 1\n\n    Returns\n    -------\n    eigvals : np.ndarray, shape=(k,)\n        The largest `k` eigenvalues\n    lv : np.ndarray, shape=(n_states, k)\n        The normalized left eigenvectors (:math:`\\phi`) of the rate matrix.\n    rv :  np.ndarray, shape=(n_states, k)\n        The normalized right eigenvectors (:math:`\\psi`) of the rate matrix.\n    \"\"\"\n    S = np.zeros((n, n))\n    pi = np.exp(theta[-n:])\n    pi = pi / pi.sum()\n\n    _ratematrix.build_ratemat(theta, n, S, which='S')\n    u, lv, rv = map(np.asarray, _ratematrix.eig_K(S, n, pi, 'S'))\n    order = np.argsort(-u)\n    u = u[order[:k]]\n    lv = lv[:, order[:k]]\n    rv = rv[:, order[:k]]\n\n    return _normalize_eigensystem(u, lv, rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsolves the dominant eigenpairs of an MSM transition matrix.", "response": "def _solve_msm_eigensystem(transmat, k):\n    \"\"\"Find the dominant eigenpairs of an MSM transition matrix\n\n    Parameters\n    ----------\n    transmat : np.ndarray, shape=(n_states, n_states)\n        The transition matrix\n    k : int\n        The number of eigenpairs to find.\n\n    Notes\n    -----\n    Normalize the left (:math:`\\phi`) and right (:math:``\\psi``) eigenfunctions\n    according to the following criteria.\n      * The first left eigenvector, \\phi_1, _is_ the stationary\n        distribution, and thus should be normalized to sum to 1.\n      * The left-right eigenpairs should be biorthonormal:\n        <\\phi_i, \\psi_j> = \\delta_{ij}\n      * The left eigenvectors should satisfy\n        <\\phi_i, \\phi_i>_{\\mu^{-1}} = 1\n      * The right eigenvectors should satisfy <\\psi_i, \\psi_i>_{\\mu} = 1\n\n    Returns\n    -------\n    eigvals : np.ndarray, shape=(k,)\n        The largest `k` eigenvalues\n    lv : np.ndarray, shape=(n_states, k)\n        The normalized left eigenvectors (:math:`\\phi`) of ``transmat``\n    rv :  np.ndarray, shape=(n_states, k)\n        The normalized right eigenvectors (:math:`\\psi`) of ``transmat``\n    \"\"\"\n    u, lv, rv = scipy.linalg.eig(transmat, left=True, right=True)\n    order = np.argsort(-np.real(u))\n    u = np.real_if_close(u[order[:k]])\n    lv = np.real_if_close(lv[:, order[:k]])\n    rv = np.real_if_close(rv[:, order[:k]])\n    return _normalize_eigensystem(u, lv, rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_eigensystem(u, lv, rv):\n    # first normalize the stationary distribution separately\n    lv[:, 0] = lv[:, 0] / np.sum(lv[:, 0])\n\n    for i in range(1, lv.shape[1]):\n        # the remaining left eigenvectors to satisfy\n        # <\\phi_i, \\phi_i>_{\\mu^{-1}} = 1\n        lv[:, i] = lv[:, i] / np.sqrt(np.dot(lv[:, i], lv[:, i] / lv[:, 0]))\n\n    for i in range(rv.shape[1]):\n        # the right eigenvectors to satisfy <\\phi_i, \\psi_j> = \\delta_{ij}\n        rv[:, i] = rv[:, i] / np.dot(lv[:, i], rv[:, i])\n\n    return u, lv, rv", "response": "Normalize the eigenvectors of a reversible Markov state model according\n    to our preferred scheme."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrim a transition count matrix down to its maximal strongly connected subgraph.", "response": "def _strongly_connected_subgraph(counts, weight=1, verbose=True):\n    \"\"\"Trim a transition count matrix down to its maximal\n    strongly ergodic subgraph.\n\n    From the counts matrix, we define a graph where there exists\n    a directed edge between two nodes, `i` and `j` if\n    `counts[i][j] > weight`. We then find the nodes belonging to the largest\n    strongly connected subgraph of this graph, and return a new counts\n    matrix formed by these rows and columns of the input `counts` matrix.\n\n    Parameters\n    ----------\n    counts : np.array, shape=(n_states_in, n_states_in)\n        Input set of directed counts.\n    weight : float\n        Threshold by which ergodicity is judged in the input data. Greater or\n        equal to this many transition counts in both directions are required\n        to include an edge in the ergodic subgraph.\n    verbose : bool\n        Print a short statement\n\n    Returns\n    -------\n    counts_component :\n        \"Trimmed\" version of ``counts``, including only states in the\n        maximal strongly ergodic subgraph.\n    mapping : dict\n        Mapping from \"input\" states indices to \"output\" state indices\n        The semantics of ``mapping[i] = j`` is that state ``i`` from the\n        \"input space\" for the counts matrix is represented by the index\n        ``j`` in counts_component\n    \"\"\"\n    n_states_input = counts.shape[0]\n    n_components, component_assignments = csgraph.connected_components(\n            csr_matrix(counts >= weight), connection=\"strong\")\n    populations = np.array(counts.sum(0)).flatten()\n    component_pops = np.array([populations[component_assignments == i].sum() for\n                               i in range(n_components)])\n    which_component = component_pops.argmax()\n\n    def cpop(which):\n        csum = component_pops.sum()\n        return 100 * component_pops[which] / csum if csum != 0 else np.nan\n\n    percent_retained = cpop(which_component)\n    if verbose:\n        print(\"MSM contains %d strongly connected component%s \"\n              \"above weight=%.2f. Component %d selected, with \"\n              \"population %f%%\" % (\n              n_components, 's' if (n_components != 1) else '',\n              weight, which_component, percent_retained))\n\n    # keys are all of the \"input states\" which have a valid mapping to the output.\n    keys = np.arange(n_states_input)[component_assignments == which_component]\n\n    if n_components == n_states_input and counts[np.ix_(keys, keys)] == 0:\n        # if we have a completely disconnected graph with no self-transitions\n        return np.zeros((0, 0)), {}, percent_retained\n\n    # values are the \"output\" state that these guys are mapped to\n    values = np.arange(len(keys))\n    mapping = dict(zip(keys, values))\n    n_states_output = len(mapping)\n\n    trimmed_counts = np.zeros((n_states_output, n_states_output),\n                              dtype=counts.dtype)\n    trimmed_counts[np.ix_(values, values)] = counts[np.ix_(keys, keys)]\n    return trimmed_counts, mapping, percent_retained"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts the number of directed transitions in a collection of sequences.", "response": "def _transition_counts(sequences, lag_time=1, sliding_window=True):\n    \"\"\"Count the number of directed transitions in a collection of sequences\n    in a discrete space.\n\n    Parameters\n    ----------\n    sequences : list of array-like\n        List of sequences, or a single sequence. Each sequence should be a\n        1D iterable of state labels. Labels can be integers, strings, or\n        other orderable objects.\n    lag_time : int\n        The time (index) delay for the counts.\n    sliding_window : bool\n        When lag_time > 1, consider *all*\n        ``N = lag_time`` strided sequences starting from index\n         0, 1, 2, ..., ``lag_time - 1``. The total, raw counts will\n         be divided by ``N``. When this is False, only start from index 0.\n\n    Returns\n    -------\n    counts : array, shape=(n_states, n_states)\n        ``counts[i][j]`` counts the number of times a sequences was in state\n        `i` at time t, and state `j` at time `t+self.lag_time`, over the\n        full set of trajectories.\n    mapping : dict\n        Mapping from the items in the sequences to the indices in\n        ``(0, n_states-1)`` used for the count matrix.\n\n    Examples\n    --------\n    >>> sequence = [0, 0, 0, 1, 1]\n    >>> counts, mapping = _transition_counts([sequence])\n    >>> print counts\n    [[2, 1],\n     [0, 1]]\n    >>> print mapping\n    {0: 0, 1: 1}\n\n    >>> sequence = [100, 200, 300]\n    >>> counts, mapping = _transition_counts([sequence])\n    >>> print counts\n    [[ 0.  1.  0.]\n     [ 0.  0.  1.]\n     [ 0.  0.  0.]]\n    >>> print mapping\n    {100: 0, 200: 1, 300: 2}\n\n    Notes\n    -----\n    `None` and `NaN` are recognized immediately as invalid labels. Therefore,\n    transition counts from or to a sequence item which is NaN or None will not\n    be counted. The mapping return value will not include the NaN or None.\n    \"\"\"\n    if (not sliding_window) and lag_time > 1:\n        return _transition_counts([X[::lag_time] for X in sequences],\n                                  lag_time=1)\n\n    classes = np.unique(np.concatenate(sequences))\n    contains_nan = (classes.dtype.kind == 'f') and np.any(np.isnan(classes))\n    contains_none = any(c is None for c in classes)\n\n    if contains_nan:\n        classes = classes[~np.isnan(classes)]\n    if contains_none:\n        classes = [c for c in classes if c is not None]\n\n    n_states = len(classes)\n\n    mapping = dict(zip(classes, range(n_states)))\n    mapping_is_identity = (not contains_nan\n                           and not contains_none\n                           and classes.dtype.kind == 'i'\n                           and np.all(classes == np.arange(n_states)))\n    mapping_fn = np.vectorize(mapping.get, otypes=[np.int])\n    none_to_nan = np.vectorize(lambda x: np.nan if x is None else x,\n                               otypes=[np.float])\n\n    counts = np.zeros((n_states, n_states), dtype=float)\n    _transitions = []\n\n    for y in sequences:\n        y = np.asarray(y)\n        from_states = y[: -lag_time: 1]\n        to_states = y[lag_time::1]\n\n        if contains_none:\n            from_states = none_to_nan(from_states)\n            to_states = none_to_nan(to_states)\n\n        if contains_nan or contains_none:\n            # mask out nan in either from_states or to_states\n            mask = ~(np.isnan(from_states) + np.isnan(to_states))\n            from_states = from_states[mask]\n            to_states = to_states[mask]\n\n        if (not mapping_is_identity) and len(from_states) > 0 and len(\n                to_states) > 0:\n            from_states = mapping_fn(from_states)\n            to_states = mapping_fn(to_states)\n\n        _transitions.append(np.row_stack((from_states, to_states)))\n\n    transitions = np.hstack(_transitions)\n    C = coo_matrix((np.ones(transitions.shape[1], dtype=int), transitions),\n                   shape=(n_states, n_states))\n    counts = counts + np.asarray(C.todense())\n\n    # If sliding window is False, this function will be called recursively\n    # with strided trajectories and lag_time = 1, which gives the desired\n    # number of counts. If sliding window is True, the counts are divided\n    # by the \"number of windows\" (i.e. the lag_time). Count magnitudes\n    # will be comparable between sliding-window and non-sliding-window cases.\n    # If lag_time = 1, sliding_window makes no difference.\n    counts /= float(lag_time)\n\n    return counts, mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dict_compose(dict1, dict2):\n    return {k: dict2.get(v) for k, v in dict1.items() if v in dict2}", "response": "Returns a dict that is the same as dict_compose but with the keys that are not in dict2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partial_transform(self, sequence, mode='clip'):\n        if mode not in ['clip', 'fill']:\n            raise ValueError('mode must be one of [\"clip\", \"fill\"]: %s' % mode)\n        sequence = np.asarray(sequence)\n        if sequence.ndim != 1:\n            raise ValueError(\"Each sequence must be 1D\")\n\n        f = np.vectorize(lambda k: self.mapping_.get(k, np.nan),\n                         otypes=[np.float])\n\n        a = f(sequence)\n        if mode == 'fill':\n            if np.all(np.mod(a, 1) == 0):\n                result = a.astype(int)\n            else:\n                result = a\n        elif mode == 'clip':\n            result = [a[s].astype(int) for s in\n                      np.ma.clump_unmasked(np.ma.masked_invalid(a))]\n        else:\n            raise RuntimeError()\n\n        return result", "response": "Transform a sequence to internal indexing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, sequences, mode='clip'):\n        if mode not in ['clip', 'fill']:\n            raise ValueError('mode must be one of [\"clip\", \"fill\"]: %s' % mode)\n        sequences = list_of_1d(sequences)\n\n        result = []\n        for y in sequences:\n            if mode == 'fill':\n                result.append(self.partial_transform(y, mode))\n            elif mode == 'clip':\n                result.extend(self.partial_transform(y, mode))\n            else:\n                raise RuntimeError()\n\n        return result", "response": "Transform a list of sequences to internal indexing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_ergodic_cutoff(self):\n        ec_is_str = isinstance(self.ergodic_cutoff, str)\n        if ec_is_str and self.ergodic_cutoff.lower() == 'on':\n            if self.sliding_window:\n                return 1.0 / self.lag_time\n            else:\n                return 1.0\n        elif ec_is_str and self.ergodic_cutoff.lower() == 'off':\n            return 0.0\n        else:\n            return self.ergodic_cutoff", "response": "Parse the ergodic_cutoff input and return a numeric value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform a list of sequences from internal indexing into a list of labels", "response": "def inverse_transform(self, sequences):\n        \"\"\"Transform a list of sequences from internal indexing into\n        labels\n\n        Parameters\n        ----------\n        sequences : list\n            List of sequences, each of which is one-dimensional array of\n            integers in ``0, ..., n_states_ - 1``.\n\n        Returns\n        -------\n        sequences : list\n            List of sequences, each of which is one-dimensional array\n            of labels.\n        \"\"\"\n        sequences = list_of_1d(sequences)\n        inverse_mapping = {v: k for k, v in self.mapping_.items()}\n        f = np.vectorize(inverse_mapping.get)\n\n        result = []\n        for y in sequences:\n            uq = np.unique(y)\n            if not np.all(np.logical_and(0 <= uq, uq < self.n_states_)):\n                raise ValueError('sequence must be between 0 and n_states-1')\n\n            result.append(f(y))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample_discrete(self, state=None, n_steps=100, random_state=None):\n        random = check_random_state(random_state)\n        r = random.rand(1 + n_steps)\n\n        if state is None:\n            initial = np.sum(np.cumsum(self.populations_) < r[0])\n        elif hasattr(state, '__len__') and len(state) == self.n_states_:\n            initial = np.sum(np.cumsum(state) < r[0])\n        else:\n            initial = self.mapping_[state]\n\n        cstr = np.cumsum(self.transmat_, axis=1)\n\n        chain = [initial]\n        for i in range(1, n_steps):\n            chain.append(np.sum(cstr[chain[i - 1], :] < r[i]))\n\n        return self.inverse_transform([chain])[0]", "response": "r Generate a random sequence of states by propagating the model with discrete time steps given by the lagtime."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing samples from a list of sequences of states.", "response": "def draw_samples(self, sequences, n_samples, random_state=None):\n        \"\"\"Sample conformations for a sequences of states.\n\n        Parameters\n        ----------\n        sequences : list or list of lists\n            A sequence or list of sequences, in which each element corresponds\n            to a state label.\n        n_samples : int\n            How many samples to return for any given state.\n\n        Returns\n        -------\n        selected_pairs_by_state : np.array, dtype=int,\n            shape=(n_states, n_samples, 2) selected_pairs_by_state[state] gives\n            an array of randomly selected (trj, frame) pairs from the specified\n            state.\n\n        See Also\n        --------\n        utils.map_drawn_samples : Extract conformations from MD trajectories by\n        index.\n\n        \"\"\"\n        if not any([isinstance(seq, collections.Iterable)\n                    for seq in sequences]):\n            sequences = [sequences]\n\n        random = check_random_state(random_state)\n\n        selected_pairs_by_state = []\n        for state in range(self.n_states_):\n            all_frames = [np.where(a == state)[0] for a in sequences]\n            pairs = [(trj, frame) for (trj, frames) in enumerate(all_frames)\n                     for frame in frames]\n            if pairs:\n                selected_pairs_by_state.append(\n                        [pairs[random.choice(len(pairs))]\n                         for i in range(n_samples)])\n            else:\n                selected_pairs_by_state.append([])\n\n        return np.array(selected_pairs_by_state)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo the MVCA lumping.", "response": "def _do_lumping(self):\n        \"\"\"Do the MVCA lumping.\n        \"\"\"\n        model = LandmarkAgglomerative(linkage='ward',\n                                      n_clusters=self.n_macrostates,\n                                      metric=self.metric,\n                                      n_landmarks=self.n_landmarks,\n                                      landmark_strategy=self.landmark_strategy,\n                                      random_state=self.random_state)\n        model.fit([self.transmat_])\n\n        if self.fit_only:\n            microstate_mapping_ = model.landmark_labels_\n\n        else:\n            microstate_mapping_ = model.transform([self.transmat_])[0]\n\n        self.microstate_mapping_ = microstate_mapping_"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_msm(cls, msm, n_macrostates, metric=js_metric_array, \n                 n_landmarks=None, landmark_strategy='stride',\n                 random_state=None, get_linkage=False, fit_only=False):\n        \"\"\"Create and fit lumped model from pre-existing MSM.\n\n        Parameters\n        ----------\n        msm : MarkovStateModel\n            The input microstate msm to use.\n        n_macrostates : int\n            The number of macrostates\n        get_linkage : boolean, default=False\n            Whether to return linkage and elbow data objects. Warning:\n            This will compute n choose 2 pairwise distances\n\n        Returns\n        -------\n        lumper : cls\n            The fit MVCA object.\n        pairwise_dists : if get_linkage is True, np.array,\n                         [number of microstates choose 2]\n        linkage : if get_linkage is True, scipy linkage object\n        elbow_data : if get_linkage is True, np.array,\n                     [number of microstates - 1]. Change in updated Ward\n                     objective function, indexed by n_macrostates - 1\n\n        Example\n        -------\n        plt.figure()\n        scipy.cluster.hierarchy.dendrogram(mvca.linkage)\n\n        scatter(arange(1,n_microstates), mvca.elbow_data)\n        \"\"\"\n        params = msm.get_params()\n        lumper = cls(n_macrostates, metric=metric, fit_only=fit_only,\n                 n_landmarks=n_landmarks, landmark_strategy=landmark_strategy,\n                 random_state=random_state, **params)\n\n        lumper.transmat_ = msm.transmat_\n        lumper.populations_ = msm.populations_\n        lumper.mapping_ = msm.mapping_\n        lumper.countsmat_ = msm.countsmat_\n        lumper.n_states_ = msm.n_states_\n\n        if n_macrostates is not None:\n            lumper._do_lumping()\n\n        if get_linkage:\n            p = pdist(msm.transmat_, metric=metric)\n            l = scipy.cluster.hierarchy.linkage(p, 'ward')\n\n            lumper.pairwise_dists = p\n            lumper.linkage = l\n            lumper.elbow_data = l[:, 2][::-1]\n\n        else:\n            lumper.pairwise_dists = None\n            lumper.linkage = None\n            lumper.elbow_data = None\n\n        return lumper", "response": "Create and fit a lumped model from a MarkovStateModel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _truncate(self, x, k):\n        ''' given a vector x, leave its top-k absolute-value entries alone, and set the rest to 0 '''\n        not_F = np.argsort(np.abs(x))[:-k]\n        x[not_F] = 0\n        return x", "response": "given a vector x leave its top - k absolute - value entries alone and set the rest to 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _truncated_power_method(self, A, x0, k, max_iter=10000, thresh=1e-8):\n        '''\n        given a matrix A, an initial guess x0, and a maximum cardinality k,\n        find the best k-sparse approximation to its dominant eigenvector\n\n        References\n        ----------\n        [1] Yuan, X-T. and Zhang, T. \"Truncated Power Method for Sparse Eigenvalue Problems.\"\n        Journal of Machine Learning Research. Vol. 14. 2013.\n        http://www.jmlr.org/papers/volume14/yuan13a/yuan13a.pdf\n        '''\n\n        xts = [x0]\n        for t in range(max_iter):\n            xts.append(self._normalize(self._truncate(np.dot(A, xts[-1]), k)))\n            if np.linalg.norm(xts[-1] - xts[-2]) < thresh: break\n        return xts[-1]", "response": "Truncated power method for Sparse Eigenvalue Problems."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(self, X):\n        labels, inertia = libdistance.assign_nearest(\n            X, self.cluster_centers_, metric=self.metric)\n        return labels", "response": "Predict the closest cluster each sample in X belongs to."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the kcenters clustering on the data.", "response": "def fit(self, sequences, y=None):\n        \"\"\"Fit the kcenters clustering on the data\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries, or ``md.Trajectory``. Each\n            sequence may have a different length, but they all must have the\n            same number of features, or the same number of atoms if they are\n            ``md.Trajectory``s.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        MultiSequenceClusterMixin.fit(self, sequences)\n        self.cluster_center_indices_ = self._split_indices(self.cluster_center_indices_)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y=None):\n        # X = check_array(X)\n        t0 = time.time()\n        self.X = X\n        self._run()\n        t1 = time.time()\n#        print(\"APM clustering Time Cost:\", t1 - t0)\n        return self", "response": "Perform clustering. A cluster. A cluster is trained on the data X."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run(self):\n#        print(\"Doing APM Clustering...\")\n        # Start looping for maxIter times\n        n_macrostates = 1  # initialized as 1 because no macrostate exist in loop 0\n        metaQ = -1.0\n        prevQ = -1.0\n        global_maxQ = -1.0\n        local_maxQ = -1.0\n\n        for iter in range(self.max_iter):\n            self.__max_state = -1\n            self.__micro_stack = []\n            for k in range(n_macrostates):\n                self._do_split(micro_state=k, sub_clus=self.sub_clus)\n                self._do_time_clustering(macro_state=k)\n\n            # do Lumping\n            n_micro_states = np.amax(self.__temp_labels_) + 1\n            if n_micro_states > self.n_macrostates:\n#                print(\"PCCA Lumping...\", n_micro_states, \"microstates\")\n                self.__temp_MacroAssignments_ = self._do_lumping(\n                    n_macrostates=n_macrostates)\n                #self.__temp_labels_ = [copy.copy(element) for element in self.__temp_MacroAssignments_]\n\n                #Calculate Metastabilty\n                prevQ = metaQ\n                metaQ = self.__temp_transmat_.diagonal().sum()\n                metaQ /= len(self.__temp_transmat_)\n            else:\n                self.__temp_MacroAssignments_ = [\n                    copy.copy(element) for element in self.__temp_labels_\n                ]\n\n            # Optimization / Monte-Carlo\n            acceptedMove = False\n            MCacc = np.exp(metaQ * metaQ - prevQ * prevQ)\n\n            if MCacc > 1.0:\n                MCacc = 1.0\n            optLim = 0.95\n\n            if MCacc > optLim:\n                acceptedMove = True\n\n            if acceptedMove:\n                local_maxQ = metaQ\n                if metaQ > global_maxQ:\n                    global_maxQ = metaQ\n                    self.MacroAssignments_ = [\n                        copy.copy(element)\n                        for element in self.__temp_MacroAssignments_\n                    ]\n                    self.labels_ = [copy.copy(element)\n                                    for element in self.__temp_labels_]\n                    self.transmat_ = self.__temp_transmat_\n\n#            print(\"Loop:\", iter, \"AcceptedMove?\", acceptedMove, \"metaQ:\",\n#                  metaQ, \"prevQ:\", prevQ, \"global_maxQ:\", global_maxQ,\n#                  \"local_maxQ:\", local_maxQ, \"macroCount:\", n_macrostates)\n            #set n_macrostates\n            n_macrostates = self.n_macrostates\n            self.__temp_labels_ = [copy.copy(element)\n                                   for element in self.__temp_MacroAssignments_\n                                   ]", "response": "Do the APM clustering."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, pbar):\n        if pbar.currval == 0:\n            return 'ETA:  --:--:--'\n        elif pbar.finished:\n            return 'Time: %s' % self.format_time(pbar.seconds_elapsed)\n        else:\n            elapsed = pbar.seconds_elapsed\n            currval1, elapsed1 = self._update_samples(pbar.currval, elapsed)\n            eta = self._eta(pbar.maxval, pbar.currval, elapsed)\n            if pbar.currval > currval1:\n                etasamp = self._eta(pbar.maxval - currval1,\n                                    pbar.currval - currval1,\n                                    elapsed - elapsed1)\n                weight = (pbar.currval / float(pbar.maxval)) ** 0.5\n                eta = (1 - weight) * eta + weight * etasamp\n            return 'ETA:  %s' % self.format_time(eta)", "response": "Updates the widget to show the ETA or total time when finished."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, pbar):\n\n        if pbar.seconds_elapsed < 2e-6 or pbar.currval < 2e-6: # =~ 0\n            scaled = power = 0\n        else:\n            speed = pbar.currval / pbar.seconds_elapsed\n            power = int(math.log(speed, 1000))\n            scaled = speed / 1000.**power\n\n        return self.FORMAT % (scaled, self.PREFIXES[power], self.unit)", "response": "Updates the widget with the current SI prefixed speed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of acceptable atom pairs from a trajectory.", "response": "def get_atompair_indices(reference_traj, keep_atoms=None,\n                         exclude_atoms=None, reject_bonded=True):\n    \"\"\"Get a list of acceptable atom pairs.\n\n    Parameters\n    ----------\n    reference_traj : mdtraj.Trajectory\n        Trajectory to grab atom pairs from\n    keep_atoms : np.ndarray, dtype=string, optional\n        Select only these atom names. Defaults to N, CA, CB, C, O, H\n    exclude_atoms : np.ndarray, dtype=string, optional\n        Exclude these atom names\n    reject_bonded : bool, default=True\n        If True, exclude bonded atompairs.\n\n    Returns\n    -------\n    atom_indices : np.ndarray, dtype=int\n        The atom indices that pass your criteria\n    pair_indices : np.ndarray, dtype=int, shape=(N, 2)\n        Pairs of atom indices that pass your criteria.\n\n    Notes\n    -----\n    This function has been optimized for speed.  A naive implementation\n    can be slow (~minutes) for large proteins.\n    \"\"\"\n    if keep_atoms is None:\n        keep_atoms = ATOM_NAMES\n\n    top, bonds = reference_traj.top.to_dataframe()\n\n    if keep_atoms is not None:\n        atom_indices = top[top.name.isin(keep_atoms) == True].index.values\n\n    if exclude_atoms is not None:\n        atom_indices = top[top.name.isin(exclude_atoms) == False].index.values\n\n    pair_indices = np.array(list(itertools.combinations(atom_indices, 2)))\n\n    if reject_bonded:\n        a_list = bonds.min(1)\n        b_list = bonds.max(1)\n\n        n = atom_indices.max() + 1\n\n        bond_hashes = a_list + b_list * n\n        pair_hashes = pair_indices[:, 0] + pair_indices[:, 1] * n\n\n        not_bonds = ~np.in1d(pair_hashes, bond_hashes)\n\n        pair_indices = np.array([(a, b) for k, (a, b)\n                                 in enumerate(pair_indices)\n                                 if not_bonds[k]])\n\n    return atom_indices, pair_indices"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_dimension(trajs, dimension, n_frames, scheme=\"linear\"):\n    fixed_indices = list(trajs.keys())\n    trajs = [trajs[k][:, [dimension]] for k in fixed_indices]\n    txx = np.concatenate([traj[:,0] for traj in trajs])\n\n    if scheme == \"linear\":\n        spaced_points = np.linspace(np.min(txx), np.max(txx), n_frames)\n        spaced_points = spaced_points[:, np.newaxis]\n    elif scheme == \"random\":\n        spaced_points = np.sort(np.random.choice(txx, n_frames))\n        spaced_points = spaced_points[:, np.newaxis]\n    elif scheme == \"edge\":\n        _cut_point = n_frames // 2\n        txx = np.sort(txx)\n        spaced_points = np.hstack((txx[:_cut_point],\n                                   txx[-_cut_point:]))\n        spaced_points = np.reshape(spaced_points, newshape=(len(spaced_points), 1))\n    else:\n        raise ValueError(\"Scheme has be to one of linear, random or edge\")\n\n    tree = KDTree(trajs)\n    dists, inds = tree.query(spaced_points)\n    return [(fixed_indices[i], j) for i, j in inds]", "response": "Sample a dimension of the data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfitting the grid to the data points X.", "response": "def fit(self, X, y=None):\n        \"\"\"Fit the grid\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Data points\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = array2d(X)\n        self.n_features = X.shape[1]\n        self.n_bins = self.n_bins_per_feature ** self.n_features\n\n        if self.min is None:\n            min = np.min(X, axis=0)\n        elif isinstance(self.min, numbers.Number):\n            min = self.min * np.ones(self.n_features)\n        else:\n            min = np.asarray(self.min)\n            if not min.shape == (self.n_features,):\n                raise ValueError('min shape error')\n\n        if self.max is None:\n            max = np.max(X, axis=0)\n        elif isinstance(self.max, numbers.Number):\n            max = self.max * np.ones(self.n_features)\n        else:\n            max = np.asarray(self.max)\n            if not max.shape == (self.n_features,):\n                raise ValueError('max shape error')\n\n        self.grid = np.array(\n            [np.linspace(min[i] - EPS, max[i] + EPS, self.n_bins_per_feature + 1)\n             for i in range(self.n_features)])\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, X):\n        if np.any(X < self.grid[:, 0]) or np.any(X > self.grid[:, -1]):\n            raise ValueError('data out of min/max bounds')\n\n        binassign = np.zeros((self.n_features, len(X)), dtype=int)\n        for i in range(self.n_features):\n            binassign[i] = np.digitize(X[:, i], self.grid[i]) - 1\n\n        labels = np.dot(self.n_bins_per_feature ** np.arange(self.n_features), binassign)\n        assert np.max(labels) < self.n_bins\n        return labels", "response": "Predict the index of the grid cell containing each sample in X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partial_transform(self, traj_zip):\n        return np.concatenate([self._dim_match(traj) / norm\n                               for traj, norm in zip(traj_zip, self._norms)],\n                              axis=1)", "response": "Featurize an MD trajectory into a vector space."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the datasets are the same length", "response": "def _check_same_length(self, trajs_tuple):\n        \"\"\"Check that the datasets are the same length\"\"\"\n        lens = [len(trajs) for trajs in trajs_tuple]\n        if len(set(lens)) > 1:\n            err = \"Each dataset must be the same length. You gave: {}\"\n            err = err.format(lens)\n            raise ValueError(err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mfpts(msm, sinks=None, lag_time=1., errors=False, n_samples=100):\n\n    if hasattr(msm, 'all_transmats_'):\n        if errors:\n            output = []\n            for i in range(n_samples):\n                mfpts = np.zeros_like(msm.all_transmats_)\n                for i, el in enumerate(zip(msm.all_transmats_, msm.all_countsmats_)):\n                    loc, scale = create_perturb_params(el[1])\n                    tprob = perturb_tmat(loc, scale)\n                    populations = _solve_msm_eigensystem(tprob, 1)[1]\n                    mfpts[i, :, :] = _mfpts(tprob, populations, sinks, lag_time)\n                output.append(np.median(mfpts, axis=0))             \n            return np.array(output)\n        \n        mfpts = np.zeros_like(msm.all_transmats_)\n        for i, el in enumerate(zip(msm.all_transmats_, msm.all_populations_)):\n            tprob = el[0]\n            populations = el[1]\n            mfpts[i, :, :] = _mfpts(tprob, populations, sinks, lag_time)\n        return np.median(mfpts, axis=0)\n    \n    if errors:\n        loc, scale = create_perturb_params(msm.countsmat_)\n        output = []\n        for i in range(n_samples):\n            tprob = perturb_tmat(loc, scale)\n            populations = _solve_msm_eigensystem(tprob, 1)[1]\n            output.append(_mfpts(tprob, populations, sinks, lag_time))\n        return np.array(output)\n    return _mfpts(msm.transmat_, msm.populations_, sinks, lag_time)", "response": "Returns the mean first passage time for all states to a set of sinks."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the Mean First Passage Time (MFPT) for all states to a *set* of sinks. Parameters ---------- tprob : np.ndarray Transition matrix populations : np.ndarray, (n_states,) MSM populations sinks : array_like, int, optional Indices of the sink states. There are two use-cases: - None [default] : All MFPTs will be calculated, and the result is a matrix of the MFPT from state i to state j. This uses the fundamental matrix formalism. - list of ints or int : Only the MFPTs into these sink states will be computed. The result is a vector, with entry i corresponding to the average time it takes to first get to *any* sink state from state i lag_time : float, optional Lag time for the model. The MFPT will be reported in whatever units are given here. Default is (1) which is in units of the lag time of the MSM. Returns ------- mfpts : np.ndarray, float MFPT in time units of lag_time, which depends on the input value of sinks: - If sinks is None, then mfpts's shape is (n_states, n_states). Where mfpts[i, j] is the mean first passage time to state j from state i. - If sinks contains one or more states, then mfpts's shape is (n_states,). Where mfpts[i] is the mean first passage time from state i to any state in sinks. References ---------- .. [1] Grinstead, C. M. and Snell, J. L. Introduction to Probability. American Mathematical Soc., 1998. As of November 2014, this chapter was available for free online: http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf", "response": "def _mfpts(tprob, populations, sinks, lag_time):\n    \"\"\"\n    Gets the Mean First Passage Time (MFPT) for all states to a *set*\n    of sinks.\n\n    Parameters\n    ----------\n    tprob : np.ndarray\n        Transition matrix\n    populations : np.ndarray, (n_states,)\n        MSM populations\n    sinks : array_like, int, optional\n        Indices of the sink states. There are two use-cases:\n            - None [default] : All MFPTs will be calculated, and the\n                result is a matrix of the MFPT from state i to state j.\n                This uses the fundamental matrix formalism.\n            - list of ints or int : Only the MFPTs into these sink\n                states will be computed. The result is a vector, with\n                entry i corresponding to the average time it takes to\n                first get to *any* sink state from state i\n    lag_time : float, optional\n        Lag time for the model. The MFPT will be reported in whatever\n        units are given here. Default is (1) which is in units of the\n        lag time of the MSM.\n\n    Returns\n    -------\n    mfpts : np.ndarray, float\n        MFPT in time units of lag_time, which depends on the input\n        value of sinks:\n\n        - If sinks is None, then mfpts's shape is (n_states, n_states).\n            Where mfpts[i, j] is the mean first passage time to state j\n            from state i.\n\n        - If sinks contains one or more states, then mfpts's shape\n            is (n_states,). Where mfpts[i] is the mean first passage\n            time from state i to any state in sinks.\n\n    References\n    ----------\n    .. [1] Grinstead, C. M. and Snell, J. L. Introduction to\n           Probability. American Mathematical Soc., 1998.\n\n    As of November 2014, this chapter was available for free online:\n        http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter11.pdf\n    \"\"\"\n\n    n_states = np.shape(populations)[0]\n\n    if sinks is None:\n        # Use Thm 11.16 in [1]\n        limiting_matrix = np.vstack([populations] * n_states)\n\n        # Fundamental matrix\n        fund_matrix = scipy.linalg.inv(np.eye(n_states) - tprob +\n                                       limiting_matrix)\n\n        # mfpt[i,j] = (fund_matrix[j,j] - fund_matrix[i,j]) / populations[j]\n        mfpts = fund_matrix * -1\n        for j in xrange(n_states):\n            mfpts[:, j] += fund_matrix[j, j]\n            mfpts[:, j] /= populations[j]\n\n        mfpts *= lag_time\n\n    else:\n        # See section 11.5, and use Thm 11.5\n        # Turn our ergodic MSM into an absorbing one (all sink\n        # states are absorbing). Then calculate the mean time\n        # to absorption.\n        # Note: we are slightly modifying the description in\n        # 11.5 so that we also get the mfpts[sink] = 0.0\n        sinks = np.array(sinks, dtype=int).reshape((-1,))\n\n        absorb_tprob = copy.copy(tprob)\n\n        for state in sinks:\n            absorb_tprob[state, :] = 0.0\n            absorb_tprob[state, state] = 2.0\n            # note it has to be 2 because we subtract\n            # the identity below.\n\n        lhs = np.eye(n_states) - absorb_tprob\n\n        rhs = np.ones(n_states)\n        for state in sinks:\n            rhs[state] = 0.0\n\n        mfpts = lag_time * np.linalg.solve(lhs, rhs)\n\n    return mfpts"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntypes for use with argument that will force a specific suffix CTYPE", "response": "def exttype(suffix):\n    \"\"\"Type for use with argument(... type=) that will force a specific suffix\n    Especially for output files, so that we can enforce the use of appropriate\n    file-type specific suffixes\"\"\"\n    def inner(s):\n        if s == '':\n            return s\n        first, last = os.path.splitext(s)\n        return first + suffix\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap around inspect. getargspec that works around cython classes.", "response": "def get_init_argspec(klass):\n    \"\"\"Wrapper around inspect.getargspec(klass.__init__) which, for cython\n    classes uses an auxiliary '_init_argspec' method, since they don't play\n    nice with the inspect module.\n\n    By convention, a cython class should define the classmethod _init_argspec\n    that, when called, returns what ``inspect.getargspec`` would be expected\n    to return when called on that class's __init__ method.\n    \"\"\"\n    if hasattr(klass, '_init_argspec'):\n        return _shim_argspec(klass._init_argspec())\n    elif PY2:\n        return _shim_argspec(inspect.getargspec(klass.__init__))\n    else:\n        return inspect.signature(klass.__init__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef backup(fn):\n    if not os.path.exists(fn):\n        return\n\n    backnum = 1\n    backfmt = \"{fn}.bak.{backnum}\"\n    trial_fn = backfmt.format(fn=fn, backnum=backnum)\n    while os.path.exists(trial_fn):\n        backnum += 1\n        trial_fn = backfmt.format(fn=fn, backnum=backnum)\n\n    warnings.warn(\"{fn} exists. Moving it to {newfn}\"\n                  .format(fn=fn, newfn=trial_fn),\n                  BackupWarning)\n    shutil.move(fn, trial_fn)", "response": "This function will backup the file if it exists and issue a warning if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn an arbitrary python object into a filename", "response": "def default_key_to_path(key, dfmt=\"{}\", ffmt=\"{}.npy\"):\n    \"\"\"Turn an arbitrary python object into a filename\n\n    This uses string formatting, so make sure your keys map\n    to unique strings. If the key is a tuple, it will join each\n    element of the tuple with '/', resulting in a filesystem\n    hierarchy of files.\n    \"\"\"\n    if isinstance(key, tuple):\n        paths = [dfmt.format(k) for k in key[:-1]]\n        paths += [ffmt.format(key[-1])]\n        return os.path.join(*paths)\n    else:\n        return ffmt.format(key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preload_tops(meta):\n    top_fns = set(meta['top_fn'])\n    tops = {}\n    for tfn in top_fns:\n        tops[tfn] = md.load_topology(tfn)\n    return tops", "response": "Load all topology files into memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads one topology file into memory.", "response": "def preload_top(meta):\n    \"\"\"Load one topology file into memory.\n\n    This function checks to make sure there's only one topology file\n    in play. When sampling frames, you have to have all the same\n    topology to concatenate.\n\n    Parameters\n    ----------\n    meta : pd.DataFrame\n        The DataFrame of metadata with a column named 'top_fn'\n\n    Returns\n    -------\n    top : md.Topology\n        The one topology file that can be used for all trajectories.\n    \"\"\"\n    top_fns = set(meta['top_fn'])\n    if len(top_fns) != 1:\n        raise ValueError(\"More than one topology is used in this project!\")\n    return md.load_topology(top_fns.pop())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef itertrajs(meta, stride=1):\n\n    tops = preload_tops(meta)\n    for i, row in meta.iterrows():\n        yield i, md.join(md.iterload(row['traj_fn'],\n                                     top=tops[row['top_fn']],\n                                     stride=stride),\n                         discard_overlapping_frames=False,\n                         check_topology=False)", "response": "Load one mdtraj trajectory at a time and yield it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_meta(meta, fn=\"meta.pandas.html\",\n                title=\"Project Metadata - MSMBuilder\", pandas_kwargs=None):\n    \"\"\"Render a metadata dataframe as an html webpage for inspection.\n\n    Parameters\n    ----------\n    meta : pd.Dataframe\n        The DataFrame of metadata\n    fn : str\n        Output filename (should end in html)\n    title : str\n        Page title\n    pandas_kwargs : dict\n        Arguments to be passed to pandas\n\n    \"\"\"\n    if pandas_kwargs is None:\n        pandas_kwargs = {}\n\n    kwargs_with_defaults = {\n        'classes': ('table', 'table-condensed', 'table-hover'),\n    }\n    kwargs_with_defaults.update(**pandas_kwargs)\n\n    env = Environment(loader=PackageLoader('msmbuilder', 'io_templates'))\n    templ = env.get_template(\"twitter-bootstrap.html\")\n    rendered = templ.render(\n        title=title,\n        content=meta.to_html(**kwargs_with_defaults)\n    )\n\n    # Ugh, pandas hardcodes border=\"1\"\n    rendered = re.sub(r' border=\"1\"', '', rendered)\n\n    backup(fn)\n    with open(fn, 'w') as f:\n        f.write(rendered)", "response": "Render a dataframe as an html webpage for inspection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave a Python object to a file.", "response": "def save_generic(obj, fn):\n    \"\"\"Save Python objects, including msmbuilder Estimators.\n\n    This is a convenience wrapper around Python's ``pickle``\n    serialization scheme. This protocol is backwards-compatible\n    among Python versions, but may not be \"forwards-compatible\".\n    A file saved with Python 3 won't be able to be opened under Python 2.\n    Please read the pickle docs (specifically related to the ``protocol``\n    parameter) to specify broader compatibility.\n\n    If a file already exists at the given filename, it will be backed\n    up.\n\n    Parameters\n    ----------\n    obj : object\n        A Python object to serialize (save to disk)\n    fn : str\n        Filename to save the object. We recommend using the '.pickl'\n        extension, but don't do anything to enforce that convention.\n    \"\"\"\n    backup(fn)\n    with open(fn, 'wb') as f:\n        pickle.dump(obj, f)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a dictionary of trajectories in a directory containing the n - dimensional data.", "response": "def save_trajs(trajs, fn, meta, key_to_path=None):\n    \"\"\"Save trajectory-like data\n\n    Data is stored in individual numpy binary files in the\n    directory given by ``fn``.\n\n    This method will automatically back up existing files named ``fn``.\n\n    Parameters\n    ----------\n    trajs : dict of (key, np.ndarray)\n        Dictionary of trajectory-like ndarray's keyed on ``meta.index``\n        values.\n    fn : str\n        Where to save the data. This will be a directory containing\n        one file per trajectory\n    meta : pd.DataFrame\n        The DataFrame of metadata\n    \"\"\"\n    if key_to_path is None:\n        key_to_path = default_key_to_path\n\n    validate_keys(meta.index, key_to_path)\n    backup(fn)\n    os.mkdir(fn)\n    for k in meta.index:\n        v = trajs[k]\n        npy_fn = os.path.join(fn, key_to_path(k))\n        os.makedirs(os.path.dirname(npy_fn), exist_ok=True)\n        np.save(npy_fn, v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload trajectory - like data from disk into a dictionary of dicts.", "response": "def load_trajs(fn, meta='meta.pandas.pickl', key_to_path=None):\n    \"\"\"Load trajectory-like data\n\n    Data is expected to be stored as if saved by ``save_trajs``.\n\n    This method finds trajectories based on the ``meta`` dataframe.\n    If you remove a file (trajectory) from disk, be sure to remove\n    its row from the dataframe. If you remove a row from the dataframe,\n    be aware that that trajectory (file) will not be loaded, even if\n    it exists on disk.\n\n    Parameters\n    ----------\n    fn : str\n        Where the data is saved. This should be a directory containing\n        one file per trajectory.\n    meta : pd.DataFrame or str\n        The DataFrame of metadata. If this is a string, it is interpreted\n        as a filename and the dataframe is loaded from disk.\n\n    Returns\n    -------\n    meta : pd.DataFrame\n        The DataFrame of metadata. If you passed in a string (filename)\n        to the ``meta`` input, this will be the loaded DataFrame. If\n        you gave a DataFrame object, this will just be a reference back\n        to that object\n    trajs : dict\n        Dictionary of trajectory-like np.ndarray's keyed on the values\n        of ``meta.index``.\n    \"\"\"\n    if key_to_path is None:\n        key_to_path = default_key_to_path\n\n    if isinstance(meta, str):\n        meta = load_meta(meta_fn=meta)\n    trajs = {}\n    for k in meta.index:\n        trajs[k] = np.load(os.path.join(fn, key_to_path(k)))\n    return meta, trajs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the k - tree for nearest neighbors.", "response": "def query(self, x, k=1, p=2, distance_upper_bound=np.inf):\n        \"\"\"Query the kd-tree for nearest neighbors\n\n        Parameters\n        ----------\n        x : array_like, last dimension self.m\n            An array of points to query.\n        k : int, optional\n            The number of nearest neighbors to return.\n        eps : nonnegative float, optional\n            Return approximate nearest neighbors; the kth returned value\n            is guaranteed to be no further than (1+eps) times the\n            distance to the real kth nearest neighbor.\n        p : float, 1<=p<=infinity, optional\n            Which Minkowski p-norm to use.\n            1 is the sum-of-absolute-values \"Manhattan\" distance\n            2 is the usual Euclidean distance\n            infinity is the maximum-coordinate-difference distance\n        distance_upper_bound : nonnegative float, optional\n            Return only neighbors within this distance. This is used to prune\n            tree searches, so if you are doing a series of nearest-neighbor\n            queries, it may help to supply the distance to the nearest neighbor\n            of the most recent point.\n\n        Returns\n        -------\n        d : float or array of floats\n            The distances to the nearest neighbors.\n            If x has shape tuple+(self.m,), then d has shape tuple if\n            k is one, or tuple+(k,) if k is larger than one. Missing\n            neighbors (e.g. when k > n or distance_upper_bound is\n            given) are indicated with infinite distances.  If k is None,\n            then d is an object array of shape tuple, containing lists\n            of distances. In either case the hits are sorted by distance\n            (nearest first).\n        i : tuple(int, int) or array of tuple(int, int)\n            The locations of the neighbors in self.data. Locations are\n            given by tuples of (traj_i, frame_i)\n\n        Examples\n        --------\n        >>> from msmbuilder.utils import KDTree\n        >>> X1 = 0.3 * np.random.RandomState(0).randn(500, 2)\n        >>> X2 = 0.3 * np.random.RandomState(1).randn(1000, 2) + 10\n        >>> tree = KDTree([X1, X2])\n        >>> pts = np.array([[0, 0], [10, 10]])\n        >>> tree.query(pts)\n        (array([ 0.0034,  0.0102]), array([[  0, 410], [  1, 670]]))\n        >>> tree.query(pts[0])\n        (0.0034, array([  0, 410]))\n        \"\"\"\n        cdists, cinds = self._kdtree.query(x, k, p, distance_upper_bound)\n        return cdists, self._split_indices(cinds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake indices in concatenation space and return as pairs", "response": "def _split_indices(self, concat_inds):\n        \"\"\"Take indices in 'concatenated space' and return as pairs\n        of (traj_i, frame_i)\n        \"\"\"\n        clengths = np.append([0], np.cumsum(self.__lengths))\n        mapping = np.zeros((clengths[-1], 2), dtype=int)\n        for traj_i, (start, end) in enumerate(zip(clengths[:-1], clengths[1:])):\n            mapping[start:end, 0] = traj_i\n            mapping[start:end, 1] = np.arange(end - start)\n        return mapping[concat_inds]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying dimensionality reduction to sequences", "response": "def transform(self, sequences):\n        \"\"\"Apply dimensionality reduction to sequences\n\n        Parameters\n        ----------\n        sequences: list of array-like, each of shape (n_samples_i, n_features)\n            Sequence data to transform, where n_samples_i in the number of samples\n            in sequence i and n_features is the number of features.\n\n        Returns\n        -------\n        sequence_new : list of array-like, each of shape (n_samples_i, n_components)\n        \"\"\"\n        check_iter_of_sequences(sequences)\n        transforms = []\n        for X in sequences:\n            transforms.append(self.partial_transform(X))\n        return transforms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the model and apply dimensionality reduction", "response": "def fit_transform(self, sequences, y=None):\n        \"\"\"Fit the model and apply dimensionality reduction\n\n        Parameters\n        ----------\n        sequences: list of array-like, each of shape (n_samples_i, n_features)\n            Training data, where n_samples_i in the number of samples\n            in sequence i and n_features is the number of features.\n        y : None\n            Ignored\n\n        Returns\n        -------\n        sequence_new : list of array-like, each of shape (n_samples_i, n_components)\n        \"\"\"\n        self.fit(sequences)\n        transforms = self.transform(sequences)\n\n        return transforms"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits the clustering on the data.", "response": "def fit(self, sequences, y=None):\n        \"\"\"Fit the  clustering on the data\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries. Each sequence may have\n            a different length, but they all must have the same number\n            of features.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        check_iter_of_sequences(sequences, allow_trajectory=self._allow_trajectory)\n        super(MultiSequenceClusterMixin, self).fit(self._concat(sequences))\n\n        if hasattr(self, 'labels_'):\n            self.labels_ = self._split(self.labels_)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npredict the closest cluster each sample in each sequence in sequences belongs to.", "response": "def predict(self, sequences, y=None):\n        \"\"\"Predict the closest cluster each sample in each sequence in\n        sequences belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries. Each sequence may have\n            a different length, but they all must have the same number\n            of features.\n\n        Returns\n        -------\n        Y : list of arrays, each of shape [sequence_length,]\n            Index of the closest center each sample belongs to.\n        \"\"\"\n        predictions = []\n        check_iter_of_sequences(sequences, allow_trajectory=self._allow_trajectory)\n        for X in sequences:\n            predictions.append(self.partial_predict(X))\n        return predictions"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts the closest cluster each sample in X belongs to.", "response": "def partial_predict(self, X, y=None):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        In the vector quantization literature, `cluster_centers_` is called\n        the code book and each value returned by `predict` is the index of\n        the closest code in the code book.\n\n        Parameters\n        ----------\n        X : array-like shape=(n_samples, n_features)\n            A single timeseries.\n\n        Returns\n        -------\n        Y : array, shape=(n_samples,)\n            Index of the cluster that each sample belongs to\n        \"\"\"\n        if isinstance(X, md.Trajectory):\n            X.center_coordinates()\n        return super(MultiSequenceClusterMixin, self).predict(X)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform clustering on X and returns cluster labels.", "response": "def fit_predict(self, sequences, y=None):\n        \"\"\"Performs clustering on X and returns cluster labels.\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries. Each sequence may have\n            a different length, but they all must have the same number\n            of features.\n\n        Returns\n        -------\n        Y : list of ndarray, each of shape [sequence_length, ]\n            Cluster labels\n        \"\"\"\n        if hasattr(super(MultiSequenceClusterMixin, self), 'fit_predict'):\n            check_iter_of_sequences(sequences, allow_trajectory=self._allow_trajectory)\n            labels = super(MultiSequenceClusterMixin, self).fit_predict(sequences)\n        else:\n            self.fit(sequences)\n            labels = self.predict(sequences)\n\n        if not isinstance(labels, list):\n            labels = self._split(labels)\n        return labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_iter_of_sequences(sequences, allow_trajectory=False, ndim=2,\n                            max_iter=None):\n    \"\"\"Check that ``sequences`` is a iterable of trajectory-like sequences,\n    suitable as input to ``fit()`` for estimators following the MSMBuilder\n    API.\n\n    Parameters\n    ----------\n    sequences : object\n        The object to check\n    allow_trajectory : bool\n        Are ``md.Trajectory``s allowed?\n    ndim : int\n        The expected dimensionality of the sequences\n    max_iter : int, optional\n        Only check at maximum the first ``max_iter`` entries in ``sequences``.\n    \"\"\"\n    value = True\n    for i, X in enumerate(sequences):\n        if not isinstance(X, np.ndarray):\n            if (not allow_trajectory) and isinstance(X, md.Trajectory):\n                value = False\n                break\n        if not isinstance(X, md.Trajectory) and X.ndim != ndim:\n            value = False\n            break\n        if max_iter is not None and i >= max_iter:\n            break\n\n    if not value:\n        raise ValueError('sequences must be a list of sequences')", "response": "Checks that the given iterable of trajectory - like sequences is a valid sequence of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef array2d(X, dtype=None, order=None, copy=False, force_all_finite=True):\n    X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\n    if force_all_finite:\n        _assert_all_finite(X_2d)\n    if X is X_2d and copy:\n        X_2d = _safe_copy(X_2d)\n    return X_2d", "response": "Returns at least 2 - d array with data from X"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a glob and a parser object create a dataframe that contains the metadata rows for the trajectory files.", "response": "def gather_metadata(fn_glob, parser):\n    \"\"\"Given a glob and a parser object, create a metadata dataframe.\n\n    Parameters\n    ----------\n    fn_glob : str\n        Glob string to find trajectory files.\n    parser : descendant of _Parser\n        Object that handles conversion of filenames to metadata rows.\n    \"\"\"\n    meta = pd.DataFrame(parser.parse_fn(fn) for fn in glob.iglob(fn_glob))\n    return meta.set_index(parser.index).sort_index()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate model parameters. Parameters ---------- sequences : list of array-like List of sequences, or a single sequence. Each sequence should be a 1D iterable of state labels. Labels can be integers, strings, or other orderable objects. Returns ------- self Notes ----- `None` and `NaN` are recognized immediately as invalid labels. Therefore, transition counts from or to a sequence item which is NaN or None will not be counted. The mapping_ attribute will not include the NaN or None.", "response": "def fit(self, sequences, y=None):\n        \"\"\"Estimate model parameters.\n\n        Parameters\n        ----------\n        sequences : list of array-like\n            List of sequences, or a single sequence. Each sequence should be a\n            1D iterable of state labels. Labels can be integers, strings, or\n            other orderable objects.\n\n        Returns\n        -------\n        self\n\n        Notes\n        -----\n        `None` and `NaN` are recognized immediately as invalid labels.\n        Therefore, transition counts from or to a sequence item which is NaN or\n        None will not be counted. The mapping_ attribute will not include the\n        NaN or None.\n        \"\"\"\n        self._build_counts(sequences)\n\n        # use a dict like a switch statement: dispatch to different\n        # transition matrix estimators depending on the value of\n        # self.reversible_type\n        fit_method_map = {\n            'mle': self._fit_mle,\n            'transpose': self._fit_transpose,\n            'none': self._fit_asymetric}\n\n        try:\n            # pull out the appropriate method\n            fit_method = fit_method_map[str(self.reversible_type).lower()]\n            # step 3. estimate transition matrix\n            self.transmat_, self.populations_ = fit_method(self.countsmat_)\n        except KeyError:\n            raise ValueError('reversible_type must be one of %s: %s' % (\n                ', '.join(fit_method_map.keys()), self.reversible_type))\n\n        self._is_dirty = True\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eigtransform(self, sequences, right=True, mode='clip'):\n\n        result = []\n        for y in self.transform(sequences, mode=mode):\n            if right:\n                op = self.right_eigenvectors_[:, 1:]\n            else:\n                op = self.left_eigenvectors_[:, 1:]\n\n            is_finite = np.isfinite(y)\n            if not np.all(is_finite):\n                value = np.empty((y.shape[0], op.shape[1]))\n                value[is_finite, :] = np.take(op, y[is_finite].astype(np.int), axis=0)\n                value[~is_finite, :] = np.nan\n            else:\n                value = np.take(op, y, axis=0)\n            result.append(value)\n\n        return result", "response": "r Returns a list of 2D arrays where each element in the transformed list is a 2D array with the n_samples n_timescales dynamical eigenvectors and the transformed data is a 2D array with the n_samples n_timescales dynamical eigenvectors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning some diagnostic summary statistics about this Markov model.", "response": "def summarize(self):\n        \"\"\"Return some diagnostic summary statistics about this Markov model\n        \"\"\"\n\n        doc = '''Markov state model\n------------------\nLag time         : {lag_time}\nReversible type  : {reversible_type}\nErgodic cutoff   : {ergodic_cutoff}\nPrior counts     : {prior_counts}\n\nNumber of states : {n_states}\nNumber of nonzero entries in counts matrix : {counts_nz} ({percent_counts_nz}%)\nNonzero counts matrix entries:\n    Min.   : {cnz_min:.1f}\n    1st Qu.: {cnz_1st:.1f}\n    Median : {cnz_med:.1f}\n    Mean   : {cnz_mean:.1f}\n    3rd Qu.: {cnz_3rd:.1f}\n    Max.   : {cnz_max:.1f}\n\nTotal transition counts :\n    {cnz_sum} counts\nTotal transition counts / lag_time:\n    {cnz_sum_per_lag} units\nTimescales:\n    [{ts}]  units\n'''\n        counts_nz = np.count_nonzero(self.countsmat_)\n        cnz = self.countsmat_[np.nonzero(self.countsmat_)]\n\n        return doc.format(\n            lag_time=self.lag_time,\n            reversible_type=self.reversible_type,\n            ergodic_cutoff=self.ergodic_cutoff,\n            prior_counts=self.prior_counts,\n            n_states=self.n_states_,\n            counts_nz=counts_nz,\n            percent_counts_nz=(100 * counts_nz / self.countsmat_.size),\n            cnz_min=np.min(cnz),\n            cnz_1st=np.percentile(cnz, 25),\n            cnz_med=np.percentile(cnz, 50),\n            cnz_mean=np.mean(cnz),\n            cnz_3rd=np.percentile(cnz, 75),\n            cnz_max=np.max(cnz),\n            cnz_sum=np.sum(cnz),\n            cnz_sum_per_lag=np.sum(cnz)/self.lag_time,\n            ts=', '.join(['{:.2f}'.format(t) for t in self.timescales_]),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timescales_(self):\n        u, lv, rv = self._get_eigensystem()\n\n        # make sure to leave off equilibrium distribution\n        with np.errstate(invalid='ignore', divide='ignore'):\n            timescales = - self.lag_time / np.log(u[1:])\n        return timescales", "response": "Returns the implied relaxation timescales of the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nestimate of the element - wise asymptotic standard deviation in the model eigenvalues.", "response": "def uncertainty_eigenvalues(self):\n        \"\"\"Estimate of the element-wise asymptotic standard deviation\n        in the model eigenvalues.\n\n        Returns\n        -------\n        sigma_eigs : np.array, shape=(n_timescales+1,)\n            The estimated symptotic standard deviation in the eigenvalues.\n\n        References\n        ----------\n        .. [1] Hinrichs, Nina Singhal, and Vijay S. Pande. \"Calculation of\n           the distribution of eigenvalues and eigenvectors in Markovian state\n           models for molecular dynamics.\" J. Chem. Phys. 126.24 (2007): 244101.\n        \"\"\"\n        if self.reversible_type is None:\n            raise NotImplementedError('reversible_type must be \"mle\" or \"transpose\"')\n\n        n_timescales = min(self.n_timescales if self.n_timescales is not None\n                           else self.n_states_ - 1, self.n_states_ - 1)\n\n        u, lv, rv = self._get_eigensystem()\n\n        sigma2 = np.zeros(n_timescales + 1)\n        for k in range(n_timescales + 1):\n            dLambda_dT = np.outer(lv[:, k], rv[:, k])\n            for i in range(self.n_states_):\n                ui = self.countsmat_[:, i]\n                wi = np.sum(ui)\n                cov = wi*np.diag(ui) - np.outer(ui, ui)\n                quad_form = dLambda_dT[i].dot(cov).dot(dLambda_dT[i])\n                sigma2[k] += quad_form / (wi**2*(wi+1))\n        return np.sqrt(sigma2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating of the element - wise asymptotic standard deviation in the implied timescales.", "response": "def uncertainty_timescales(self):\n        \"\"\"Estimate of the element-wise asymptotic standard deviation\n        in the model implied timescales.\n\n        Returns\n        -------\n        sigma_timescales : np.array, shape=(n_timescales,)\n            The estimated symptotic standard deviation in the implied\n            timescales.\n\n        References\n        ----------\n        .. [1] Hinrichs, Nina Singhal, and Vijay S. Pande. \"Calculation of\n           the distribution of eigenvalues and eigenvectors in Markovian state\n           models for molecular dynamics.\" J. Chem. Phys. 126.24 (2007): 244101.\n        \"\"\"\n        # drop the first eigenvalue\n        u = self.eigenvalues_[1:]\n        sigma_eigs = self.uncertainty_eigenvalues()[1:]\n\n        sigma_ts = sigma_eigs / (u * np.log(u)**2)\n        return sigma_ts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef describe_features(self, traj):\n        all_res = []\n        for feat in self.which_feat:\n            all_res.extend(self.features[feat].describe_features(traj))\n        return all_res", "response": "Returns a list of dictionaries describing the features in the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sliced version of the feature descriptor for each feature in the trajectory.", "response": "def describe_features(self, traj):\n        \"\"\"\n        Returns a sliced version of the feature descriptor\n        Parameters\n        ----------\n        traj : MDtraj trajectory object\n\n        Returns\n        -------\n        list of sliced dictionaries describing each feature.\n        \"\"\"\n        features_list = self.feat.describe_features(traj)\n        return [features_list[i] for i in self.indices]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting Preprocessing to X.", "response": "def fit(self, sequences, y=None):\n        \"\"\"Fit Preprocessing to X.\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries. Each sequence may have\n            a different length, but they all must have the same number\n            of features.\n        y : None\n            Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        check_iter_of_sequences(sequences)\n        s = super(MultiSequencePreprocessingMixin, self)\n        s.fit(self._concat(sequences))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply preprocessing to single sequence", "response": "def partial_transform(self, sequence):\n        \"\"\"Apply preprocessing to single sequence\n\n        Parameters\n        ----------\n        sequence: array like, shape (n_samples, n_features)\n            A single sequence to transform\n\n        Returns\n        -------\n        out : array like, shape (n_samples, n_features)\n        \"\"\"\n        s = super(MultiSequencePreprocessingMixin, self)\n        return s.transform(sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits Preprocessing to X.", "response": "def partial_fit(self, sequence, y=None):\n        \"\"\"Fit Preprocessing to X.\n        Parameters\n        ----------\n        sequence : array-like, [sequence_length, n_features]\n            A multivariate timeseries.\n        y : None\n            Ignored\n        Returns\n        -------\n        self\n        \"\"\"\n        s = super(MultiSequencePreprocessingMixin, self)\n        if hasattr(s, 'fit'):\n            return s.fit(sequence)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting Preprocessing to X.", "response": "def fit(self, X, y=None):\n        \"\"\"Fit Preprocessing to X.\n\n        Parameters\n        ----------\n        sequence : array-like, [sequence_length, n_features]\n            A multivariate timeseries.\n        y : None\n            Ignored\n\n        Returns\n        -------\n        self\n        \"\"\"\n        return self.partial_fit(np.concatenate(X, axis=0))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit Preprocessing to X.", "response": "def fit(self, sequences, y=None):\n            \"\"\"Fit Preprocessing to X.\n\n            Parameters\n            ----------\n            sequences : list of array-like, each of shape [sequence_length, n_features]\n                A list of multivariate timeseries. Each sequence may have\n                a different length, but they all must have the same number\n                of features.\n            y : None\n                Ignored\n\n            Returns\n            -------\n            self\n            \"\"\"\n            check_iter_of_sequences(sequences)\n            for sequence in sequences:\n                s = super(MultiSequencePreprocessingMixin, self)\n                s.partial_fit(sequence)\n\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of dictionaries describing the contacts features.", "response": "def describe_features(self, traj):\n        \"\"\"Return a list of dictionaries describing the contacts features.\n\n        Parameters\n        ----------\n        traj : mdtraj.Trajectory\n            The trajectory to describe\n\n        Returns\n        -------\n        feature_descs : list of dict\n            Dictionary describing each feature with the following information\n            about the atoms participating in each dihedral\n                - resnames: unique names of residues\n                - atominds: the four atom indicies\n                - resseqs: unique residue sequence ids (not necessarily\n                  0-indexed)\n                - resids: unique residue ids (0-indexed)\n                - featurizer: Contact\n                - featuregroup: ca, heavy etc.\n        \"\"\"\n        feature_descs = []\n        # fill in the atom indices using just the first frame\n        distances, residue_indices = md.compute_contacts(traj[0],\n                                        self.contacts, self.scheme,\n                                        ignore_nonprotein=False,\n                                        periodic=self.periodic)\n        top = traj.topology\n\n        aind = []\n        resseqs = []\n        resnames = []\n        for resid_ids in residue_indices:\n            aind += [\"N/A\"]\n            resseqs += [[top.residue(ri).resSeq for ri in resid_ids]]\n            resnames += [[top.residue(ri).name for ri in resid_ids]]\n\n        zippy = itertools.product([\"Ligand Contact\"], [self.scheme],\n                                  [\"N/A\"],\n                                  zip(aind, resseqs, residue_indices, resnames))\n\n        feature_descs.extend(dict_maker(zippy))\n\n        return feature_descs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef partial_transform(self, traj):\n\n        # check to make sure topologies are consistent with the reference frame\n        try:\n            assert traj.top == self.reference_frame.top\n        except:\n            warnings.warn(\"The topology of the trajectory is not\" +\n                          \"the same as that of the reference frame,\" +\n                          \"which might give meaningless results.\")\n\n        X = np.zeros((traj.n_frames, self.n_features))\n\n        for f in range(self.n_features):\n            frame = self.reference_traj[f]\n            traj.superpose(frame, atom_indices=self.align_indices)\n            X[:,f] = self._naive_rmsd(traj, frame, self.calculate_indices)\n\n        return X", "response": "Featurize an MD trajectory into a vector space via distance\n            after superposition\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrying a function func for max_retries times.", "response": "def retry(max_retries=1):\n    \"\"\" Retry a function `max_retries` times. \"\"\"\n\n    def retry_func(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            num_retries = 0\n            while num_retries <= max_retries:\n                try:\n                    ret = func(*args, **kwargs)\n                    break\n                except HTTPError:\n                    if num_retries == max_retries:\n                        raise\n                    num_retries += 1\n                    time.sleep(5)\n            return ret\n\n        return wrapper\n\n    return retry_func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path of the msmbuilder data directory.", "response": "def get_data_home(data_home=None):\n    \"\"\"Return the path of the msmbuilder data dir.\n\n    As of msmbuilder v3.6, this function will prefer data downloaded via\n    the msmb_data conda package (and located within the python installation\n    directory). If this package exists, we will use its data directory as\n    the data home. Otherwise, we use the old logic:\n\n    This folder is used by some large dataset loaders to avoid\n    downloading the data several times.\n\n    By default the data dir is set to a folder named 'msmbuilder_data'\n    in the user's home folder.\n\n    Alternatively, it can be set by the 'MSMBUILDER_DATA' environment\n    variable or programmatically by giving an explicit folder path. The\n    '~' symbol is expanded to the user home folder.\n\n    If the folder does not already exist, it is automatically created.\n    \"\"\"\n    if data_home is not None:\n        return _expand_and_makedir(data_home)\n\n    msmb_data = has_msmb_data()\n    if msmb_data is not None:\n        return _expand_and_makedir(msmb_data)\n\n    data_home = environ.get('MSMBUILDER_DATA', join('~', 'msmbuilder_data'))\n    return _expand_and_makedir(data_home)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a description from the Notes section of the docstring.", "response": "def description(cls):\n        \"\"\"Get a description from the Notes section of the docstring.\"\"\"\n        lines = [s.strip() for s in cls.__doc__.splitlines()]\n        note_i = lines.index(\"Notes\")\n        return \"\\n\".join(lines[note_i + 2:])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting all the key value pairs that were not set manually in __init__.", "response": "def set_generic_keys(self, properties, exclude_list):\n        \"\"\"\n        Sets all the key value pairs that were not set manually in __init__.\n        \"\"\"\n\n        generic_keys = set(properties.keys()) - set(exclude_list)\n        for generic_key in generic_keys:\n            self.__setattr__(\n                self._convert_to_snake_case(generic_key),\n                properties[generic_key],\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds statements with a resource that is a * or has a * in it.", "response": "def star_resource_statements(self):\n        \"\"\"\n        Find statements with a resources that is a * or has a * in it.\n        \"\"\"\n\n        star_resources = []\n        for statement in self.statements:\n            if not statement.resource:\n                continue\n            if statement.resource == \"*\" or (isinstance(statement.resource, list) and \"*\" in statement.resource):\n                star_resources.append(statement)\n        return star_resources"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wildcard_allowed_actions(self, pattern=None):\n\n        wildcard_allowed = []\n\n        for statement in self.statements:\n            if statement.wildcard_actions(pattern) and statement.effect == \"Allow\":\n                wildcard_allowed.append(statement)\n\n        return wildcard_allowed", "response": "Return a list of statements which allow wildcard actions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wildcard_allowed_principals(self, pattern=None):\n\n        wildcard_allowed = []\n\n        for statement in self.statements:\n            if statement.wildcard_principals(pattern) and statement.effect == \"Allow\":\n                wildcard_allowed.append(statement)\n\n        return wildcard_allowed", "response": "Return a list of statements which allow wildcard principals."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nonwhitelisted_allowed_principals(self, whitelist=None):\n\n        if not whitelist:\n            return []\n\n        nonwhitelisted = []\n        for statement in self.statements:\n            if statement.non_whitelisted_principals(whitelist) and statement.effect == \"Allow\":\n                nonwhitelisted.append(statement)\n\n        return nonwhitelisted", "response": "Find non whitelisted allowed principals."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind allowed not - principals.", "response": "def allows_not_principal(self):\n        \"\"\"Find allowed not-principals.\"\"\"\n        not_principals = []\n        for statement in self.statements:\n            if statement.not_principal and statement.effect == \"Allow\":\n                not_principals.append(statement)\n\n        return not_principals"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse and sets the parameters in the model.", "response": "def parse_parameters(self, parameters):\n        \"\"\"Parses and sets parameters in the model.\"\"\"\n\n        self.parameters = []\n        for param_name, param_value in parameters.items():\n            p = Parameter(param_name, param_value)\n            if p:\n                self.parameters.append(p)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_resources(self, resources):\n\n        self.resources = {}\n        resource_factory = ResourceFactory()\n        for res_id, res_value in resources.items():\n            r = resource_factory.create_resource(res_id, res_value)\n            if r:\n                if r.resource_type in self.resources:\n                    self.resources[r.resource_type].append(r)\n                else:\n                    self.resources[r.resource_type] = [r]", "response": "Parses and sets the resources in the model using a factory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef session(self):\n        if self._session is None:\n            self._session = requests.Session()\n            self._session.headers.update(self._headers)\n        return self._session", "response": "Get a new requests. Session object to benefit from connection pooling."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a KeycloakClient object for the current user.", "response": "def client(self):\n        \"\"\"\n        :rtype: keycloak.client.KeycloakClient\n        \"\"\"\n        if self._client is None:\n            self._client = KeycloakClient(server_url=self._server_url,\n                                          headers=self._headers)\n        return self._client"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new OpenID Connect client.", "response": "def open_id_connect(self, client_id, client_secret):\n        \"\"\"\n        Get OpenID Connect client\n\n        :param str client_id:\n        :param str client_secret:\n        :rtype: keycloak.openid_connect.KeycloakOpenidConnect\n        \"\"\"\n        return KeycloakOpenidConnect(realm=self, client_id=client_id,\n                                     client_secret=client_secret)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a user in Keycloak", "response": "def create(self, username, **kwargs):\n        \"\"\"\n        Create a user in Keycloak\n\n        http://www.keycloak.org/docs-api/3.4/rest-api/index.html#_users_resource\n\n        :param str username:\n        :param object credentials: (optional)\n        :param str first_name: (optional)\n        :param str last_name: (optional)\n        :param str email: (optional)\n        :param boolean enabled: (optional)\n        \"\"\"\n        payload = OrderedDict(username=username)\n\n        for key in USER_KWARGS:\n            from keycloak.admin.clientroles import to_camel_case\n            if key in kwargs:\n                payload[to_camel_case(key)] = kwargs[key]\n\n        return self._client.post(\n            url=self._client.get_full_url(\n                self.get_path('collection', realm=self._realm_name)\n            ),\n            data=json.dumps(payload, sort_keys=True)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all registered users", "response": "def all(self):\n        \"\"\"\n        Return all registered users\n\n        http://www.keycloak.org/docs-api/3.4/rest-api/index.html#_users_resource\n        \"\"\"\n        return self._client.get(\n            url=self._client.get_full_url(\n                self.get_path('collection', realm=self._realm_name)\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the registered user with the given user id.", "response": "def get(self):\n        \"\"\"\n        Return registered user with the given user id.\n\n        http://www.keycloak.org/docs-api/3.4/rest-api/index.html#_users_resource\n        \"\"\"\n        self._user = self._client.get(\n            url=self._client.get_full_url(\n                self.get_path(\n                    'single', realm=self._realm_name, user_id=self._user_id\n                )\n            )\n        )\n        self._user_id = self.user[\"id\"]\n        return self._user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, **kwargs):\n        payload = {}\n        for k, v in self.user.items():\n            payload[k] = v\n        for key in USER_KWARGS:\n            from keycloak.admin.clientroles import to_camel_case\n            if key in kwargs:\n                payload[to_camel_case(key)] = kwargs[key]\n        result = self._client.put(\n            url=self._client.get_full_url(\n                self.get_path(\n                    'single', realm=self._realm_name, user_id=self._user_id\n                )\n            ),\n            data=json.dumps(payload, sort_keys=True)\n        )\n        self.get()\n        return result", "response": "Update existing user.\n\n        https://www.keycloak.org/docs-api/2.5/rest-api/index.html#_userrepresentation\n\n        :param str first_name: first_name for user\n        :param str last_name: last_name for user\n        :param str email: Email for user\n        :param bool email_verified: User email verified\n        :param Map attributes: Atributes in user\n        :param string array realm_roles: Realm Roles\n        :param Map client_roles: Client Roles\n        :param string array groups: Groups for user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a resource set.", "response": "def resource_set_create(self, token, name, **kwargs):\n        \"\"\"\n        Create a resource set.\n\n        https://docs.kantarainitiative.org/uma/rec-oauth-resource-reg-v1_0_1.html#rfc.section.2.2.1\n\n        :param str token: client access token\n        :param str id: Identifier of the resource set\n        :param str name:\n        :param str uri: (optional)\n        :param str type: (optional)\n        :param list scopes: (optional)\n        :param str icon_url: (optional)\n        :param str DisplayName: (optional)\n        :param boolean ownerManagedAccess: (optional)\n        :param str owner: (optional)\n        :rtype: str\n        \"\"\"\n        return self._realm.client.post(\n            self.well_known['resource_registration_endpoint'],\n            data=self._get_data(name=name, **kwargs),\n            headers=self.get_headers(token)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate a resource set.", "response": "def resource_set_update(self, token, id, name, **kwargs):\n        \"\"\"\n        Update a resource set.\n\n        https://docs.kantarainitiative.org/uma/rec-oauth-resource-reg-v1_0_1.html#update-resource-set\n\n        :param str token: client access token\n        :param str id: Identifier of the resource set\n        :param str name:\n        :param str uri: (optional)\n        :param str type: (optional)\n        :param list scopes: (optional)\n        :param str icon_url: (optional)\n        :rtype: str\n        \"\"\"\n        return self._realm.client.put(\n            '{}/{}'.format(\n                self.well_known['resource_registration_endpoint'], id),\n            data=self._get_data(name=name, **kwargs),\n            headers=self.get_headers(token)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resource_set_read(self, token, id):\n        return self._realm.client.get(\n            '{}/{}'.format(\n                self.well_known['resource_registration_endpoint'], id),\n            headers=self.get_headers(token)\n        )", "response": "Read a resource set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a ticket form permission to resource.", "response": "def resource_create_ticket(self, token, id, scopes, **kwargs):\n        \"\"\"\n        Create a ticket form permission to resource.\n\n        https://www.keycloak.org/docs/latest/authorization_services/index.html#_service_protection_permission_api_papi\n\n        :param str token: user access token\n        :param str id: resource id\n        :param list scopes: scopes access is wanted\n        :param dict claims: (optional)\n        :rtype: dict\n        \"\"\"\n        data = dict(resource_id=id, resource_scopes=scopes, **kwargs)\n        return self._realm.client.post(\n            self.well_known['permission_endpoint'],\n            data=self._dumps([data]),\n            headers=self.get_headers(token)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resource_associate_permission(self, token, id, name, scopes, **kwargs):\n        return self._realm.client.post(\n            '{}/{}'.format(self.well_known['policy_endpoint'], id),\n            data=self._get_data(name=name, scopes=scopes, **kwargs),\n            headers=self.get_headers(token)\n        )", "response": "Associate a permission with a Resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate an existing permission.", "response": "def permission_update(self, token, id, **kwargs):\n        \"\"\"\n        To update an existing permission.\n\n        https://www.keycloak.org/docs/latest/authorization_services/index.html#_service_authorization_uma_policy_api\n\n        :param str token: client access token\n        :param str id: permission id\n\n        :rtype: dict\n        \"\"\"\n        return self._realm.client.put(\n            '{}/{}'.format(self.well_known['policy_endpoint'], id),\n            data=self._dumps(kwargs),\n            headers=self.get_headers(token)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a Permission. https://www.keycloak.org/docs/latest/authorization_services/index.html#removing-a-permission :param str token: client access token :param str id: permission id :rtype: dict", "response": "def permission_delete(self, token, id):\n        \"\"\"\n        Removing a Permission.\n\n        https://www.keycloak.org/docs/latest/authorization_services/index.html#removing-a-permission\n\n        :param str token: client access token\n        :param str id: permission id\n\n        :rtype: dict\n        \"\"\"\n        return self._realm.client.delete(\n            '{}/{}'.format(self.well_known['policy_endpoint'], id),\n            headers=self.get_headers(token)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef permission_list(self, token, **kwargs):\n        return self._realm.client.get(\n            self.well_known['policy_endpoint'],\n            headers=self.get_headers(token),\n            **kwargs\n        )", "response": "Get a list of permissions for a given token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a special resource entitlement for a user.", "response": "def entitlement(self, token):\n        \"\"\"\n        Client applications can use a specific endpoint to obtain a special\n        security token called a requesting party token (RPT). This token\n        consists of all the entitlements (or permissions) for a user as a\n        result of the evaluation of the permissions and authorization policies\n        associated with the resources being requested. With an RPT, client\n        applications can gain access to protected resources at the resource\n        server.\n\n        http://www.keycloak.org/docs/latest/authorization_services/index\n        .html#_service_entitlement_api\n\n        :rtype: dict\n        \"\"\"\n        headers = {\"Authorization\": \"Bearer %s\" % token}\n        url = self._realm.client.get_full_url(\n            PATH_ENTITLEMENT.format(self._realm.realm_name, self._client_id)\n        )\n        return self._realm.client.get(url, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _decode_token(cls, token):\n        missing_padding = len(token) % 4\n        if missing_padding != 0:\n            token += '=' * (4 - missing_padding)\n        return json.loads(base64.b64decode(token).decode('utf-8'))", "response": "Decode a base64 encoded permission information token into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_permissions(self, token, resource_scopes_tuples=None,\n                        submit_request=False, ticket=None):\n        \"\"\"\n        Request permissions for user from keycloak server.\n\n        https://www.keycloak.org/docs/latest/authorization_services/index\n        .html#_service_protection_permission_api_papi\n\n        :param str token: client access token\n        :param Iterable[Tuple[str, str]] resource_scopes_tuples:\n            list of tuples (resource, scope)\n        :param boolean submit_request: submit request if not allowed to access?\n        :param str ticket: Permissions ticket\n        rtype: dict\n        \"\"\"\n        headers = {\n            \"Authorization\": \"Bearer %s\" % token,\n            'Content-type': 'application/x-www-form-urlencoded',\n        }\n\n        data = [\n            ('grant_type', 'urn:ietf:params:oauth:grant-type:uma-ticket'),\n            ('audience', self._client_id),\n            ('response_include_resource_name', True),\n        ]\n\n        if resource_scopes_tuples:\n            for atuple in resource_scopes_tuples:\n                data.append(('permission', '#'.join(atuple)))\n            data.append(('submit_request', submit_request))\n        elif ticket:\n            data.append(('ticket', ticket))\n\n        authz_info = {}\n\n        try:\n            response = self._realm.client.post(\n                self.well_known['token_endpoint'],\n                data=urlencode(data),\n                headers=headers,\n            )\n\n            error = response.get('error')\n            if error:\n                self.logger.warning(\n                    '%s: %s',\n                    error,\n                    response.get('error_description')\n                )\n            else:\n                token = response.get('refresh_token')\n                decoded_token = self._decode_token(token.split('.')[1])\n                authz_info = decoded_token.get('authorization', {})\n        except KeycloakClientError as error:\n            self.logger.warning(str(error))\n        return authz_info", "response": "Request permissions for user from Keycloak server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval_permission(self, token, resource, scope, submit_request=False):\n        return self.eval_permissions(\n            token=token,\n            resource_scopes_tuples=[(resource, scope)],\n            submit_request=submit_request\n        )", "response": "Evaluates if user has permission for scope on resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eval_permissions(self, token, resource_scopes_tuples=None,\n                         submit_request=False):\n        \"\"\"\n        Evaluates if user has permission for all the resource scope\n        combinations.\n\n        :param str token: client access token\n        :param Iterable[Tuple[str, str]] resource_scopes_tuples: resource to\n        access\n        :param boolean submit_request: submit request if not allowed to access?\n        rtype: boolean\n        \"\"\"\n        permissions = self.get_permissions(\n            token=token,\n            resource_scopes_tuples=resource_scopes_tuples,\n            submit_request=submit_request\n        )\n\n        res = []\n        for permission in permissions.get('permissions', []):\n            for scope in permission.get('scopes', []):\n                ptuple = (permission.get('rsname'), scope)\n                if ptuple in resource_scopes_tuples:\n                    res.append(ptuple)\n\n        return res == resource_scopes_tuples", "response": "Evaluates if user has permission for all the resource scope tuples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the roles to the user s keycloak.", "response": "def add(self, roles):\n        \"\"\"\n        :param roles: _rolerepresentation array keycloak api\n        \"\"\"\n        return self._client.post(\n            url=self._client.get_full_url(\n                self.get_path(\n                    'single', realm=self._realm_name, id=self._user_id\n                )\n            ),\n            data=json.dumps(roles, sort_keys=True)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes a JSON Web Token into a JSON - Web Token Set.", "response": "def decode_token(self, token, key, algorithms=None, **kwargs):\n        \"\"\"\n        A JSON Web Key (JWK) is a JavaScript Object Notation (JSON) data\n        structure that represents a cryptographic key.  This specification\n        also defines a JWK Set JSON data structure that represents a set of\n        JWKs.  Cryptographic algorithms and identifiers for use with this\n        specification are described in the separate JSON Web Algorithms (JWA)\n        specification and IANA registries established by that specification.\n\n        https://tools.ietf.org/html/rfc7517\n\n        :param str token: A signed JWS to be verified.\n        :param str key: A key to attempt to verify the payload with.\n        :param str,list algorithms: (optional) Valid algorithms that should be\n            used to verify the JWS. Defaults to `['RS256']`\n        :param str audience: (optional) The intended audience of the token. If\n            the \"aud\" claim is included in the claim set, then the audience\n            must be included and must equal the provided claim.\n        :param str,iterable issuer: (optional) Acceptable value(s) for the\n            issuer of the token. If the \"iss\" claim is included in the claim\n            set, then the issuer must be given and the claim in the token must\n            be among the acceptable values.\n        :param str subject: (optional) The subject of the token. If the \"sub\"\n            claim is included in the claim set, then the subject must be\n            included and must equal the provided claim.\n        :param str access_token: (optional) An access token returned alongside\n            the id_token during the authorization grant flow. If the \"at_hash\"\n            claim is included in the claim set, then the access_token must be\n            included, and it must match the \"at_hash\" claim.\n        :param dict options: (optional) A dictionary of options for skipping\n            validation steps.\n            defaults:\n\n             .. code-block:: python\n\n                 {\n                    'verify_signature': True,\n                    'verify_aud': True,\n                    'verify_iat': True,\n                    'verify_exp': True,\n                    'verify_nbf': True,\n                    'verify_iss': True,\n                    'verify_sub': True,\n                    'verify_jti': True,\n                    'leeway': 0,\n                }\n\n        :return: The dict representation of the claims set, assuming the\n            signature is valid and all requested data validation passes.\n        :rtype: dict\n        :raises jose.exceptions.JWTError: If the signature is invalid in any\n            way.\n        :raises jose.exceptions.ExpiredSignatureError: If the signature has\n            expired.\n        :raises jose.exceptions.JWTClaimsError: If any claim is invalid in any\n            way.\n        \"\"\"\n        return jwt.decode(\n            token, key,\n            audience=kwargs.pop('audience', None) or self._client_id,\n            algorithms=algorithms or ['RS256'], **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logout(self, refresh_token):\n        return self._realm.client.post(self.get_url('end_session_endpoint'),\n                                       data={\n                                           'refresh_token': refresh_token,\n                                           'client_id': self._client_id,\n                                           'client_secret': self._client_secret\n                                       })", "response": "Logs out the authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authorization_url(self, **kwargs):\n        payload = {'response_type': 'code', 'client_id': self._client_id}\n\n        for key in kwargs.keys():\n            # Add items in a sorted way for unittest purposes.\n            payload[key] = kwargs[key]\n        payload = sorted(payload.items(), key=lambda val: val[0])\n        params = urlencode(payload)\n        url = self.get_url('authorization_endpoint')\n\n        return '{}?{}'.format(url, params)", "response": "Get authorization URL to redirect the resource owner to."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authorization_code(self, code, redirect_uri):\n        return self._token_request(grant_type='authorization_code', code=code,\n                                   redirect_uri=redirect_uri)", "response": "Retrieve access token by authorization code grant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve an access token by password credentials grant.", "response": "def password_credentials(self, username, password, **kwargs):\n        \"\"\"\n        Retrieve access token by 'password credentials' grant.\n\n        https://tools.ietf.org/html/rfc6749#section-4.3\n\n        :param str username: The user name to obtain an access token for\n        :param str password: The user's password\n        :rtype: dict\n        :return: Access token response\n        \"\"\"\n        return self._token_request(grant_type='password',\n                                   username=username, password=password,\n                                   **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrefresh an access token", "response": "def refresh_token(self, refresh_token, **kwargs):\n        \"\"\"\n        Refresh an access token\n\n        https://tools.ietf.org/html/rfc6749#section-6\n\n        :param str refresh_token:\n        :param str scope: (optional) Space delimited list of strings.\n        :rtype: dict\n        :return: Access token response\n        \"\"\"\n        return self._token_request(grant_type='refresh_token',\n                                   refresh_token=refresh_token, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing the actual call to the token end - point.", "response": "def _token_request(self, grant_type, **kwargs):\n        \"\"\"\n        Do the actual call to the token end-point.\n\n        :param grant_type:\n        :param kwargs: See invoking methods.\n        :return:\n        \"\"\"\n        payload = {\n            'grant_type': grant_type,\n            'client_id': self._client_id,\n            'client_secret': self._client_secret\n        }\n\n        payload.update(**kwargs)\n\n        return self._realm.client.post(self.get_url('token_endpoint'),\n                                       data=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _handle_response(self, req_ctx) -> Any:\n        async with req_ctx as response:\n            try:\n                response.raise_for_status()\n            except aiohttp.client.ClientResponseError as cre:\n                text = await response.text(errors='replace')\n                self.logger.debug('{cre}; '\n                                  'Request info: {cre.request_info}; '\n                                  'Response headers: {cre.headers}; '\n                                  'Response status: {cre.status}; '\n                                  'Content: {text}'.format(cre=cre, text=text))\n                raise KeycloakClientError(original_exc=cre)\n\n            try:\n                result = await response.json(content_type=None)\n            except ValueError:\n                result = await response.read()\n\n        return result", "response": "Handle the response from the Keycloak API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, name, **kwargs):\n        payload = OrderedDict(name=name)\n\n        for key in ROLE_KWARGS:\n            if key in kwargs:\n                payload[to_camel_case(key)] = kwargs[key]\n\n        return self._client.post(\n            url=self._client.get_full_url(\n                self.get_path('collection',\n                              realm=self._realm_name,\n                              id=self._client_id)\n            ),\n            data=json.dumps(payload, sort_keys=True)\n        )", "response": "Create new role with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, name, **kwargs):\n        payload = OrderedDict(name=name)\n\n        for key in ROLE_KWARGS:\n            if key in kwargs:\n                payload[to_camel_case(key)] = kwargs[key]\n\n        return self._client.put(\n            url=self._client.get_full_url(\n                self.get_path('single',\n                              realm=self._realm_name,\n                              id=self._client_id,\n                              role_name=self._role_name)\n            ),\n            data=json.dumps(payload, sort_keys=True)\n        )", "response": "Update existing role.\n\n        http://www.keycloak.org/docs-api/3.4/rest-api/index.html#_roles_resource\n\n        :param str name: Name for the role\n        :param str description: (optional)\n        :param str id: (optional)\n        :param bool client_role: (optional)\n        :param bool composite: (optional)\n        :param object composites: (optional)\n        :param str container_id: (optional)\n        :param bool scope_param_required: (optional)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts a RPM archive.", "response": "def extract_rpm (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a RPM archive.\"\"\"\n    # also check cpio\n    cpio = util.find_program(\"cpio\")\n    if not cpio:\n        raise util.PatoolError(\"cpio(1) is required for rpm2cpio extraction; please install it\")\n    path = util.shell_quote(os.path.abspath(archive))\n    cmdlist = [util.shell_quote(cmd), path, \"|\", util.shell_quote(cpio),\n        '--extract', '--make-directories', '--preserve-modification-time',\n        '--no-absolute-filenames', '--force-local', '--nonmatching',\n        r'\"*\\.\\.*\"']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    return (cmdlist, {'cwd': outdir, 'shell': True})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_tar (archive, compression, cmd, verbosity, interactive):\n    try:\n        with tarfile.open(archive) as tfile:\n            tfile.list(verbose=verbosity>1)\n    except Exception as err:\n        msg = \"error listing %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "List a TAR archive with the tarfile Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_tar (archive, compression, cmd, verbosity, interactive, outdir):\n    try:\n        with tarfile.open(archive) as tfile:\n            tfile.extractall(path=outdir)\n    except Exception as err:\n        msg = \"error extracting %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Extract a TAR archive with the tarfile Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_tar (archive, compression, cmd, verbosity, interactive, filenames):\n    mode = get_tar_mode(compression)\n    try:\n        with tarfile.open(archive, mode) as tfile:\n            for filename in filenames:\n                tfile.add(filename)\n    except Exception as err:\n        msg = \"error creating %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Create a TAR archive with the tarfile Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tar_mode (compression):\n    if compression == 'gzip':\n        return 'w:gz'\n    if compression == 'bzip2':\n        return 'w:bz2'\n    if compression == 'lzma' and py_lzma:\n        return 'w:xz'\n    if compression:\n        msg = 'pytarfile does not support %s for tar compression'\n        raise util.PatoolError(msg % compression)\n    # no compression\n    return 'w'", "response": "Determine tarfile open mode according to the given compression."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a singlefile archive using the standard Python interpreter.", "response": "def create_singlefile_standard (archive, compression, cmd, verbosity, interactive, filenames):\n    \"\"\"Standard routine to create a singlefile archive (like gzip).\"\"\"\n    cmdlist = [util.shell_quote(cmd)]\n    if verbosity > 1:\n        cmdlist.append('-v')\n    cmdlist.extend(['-c', '--'])\n    cmdlist.extend([util.shell_quote(x) for x in filenames])\n    cmdlist.extend(['>', util.shell_quote(archive)])\n    return (cmdlist, {'shell': True})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_tar (archive, compression, cmd, verbosity, interactive):\n    cmdlist = [cmd, '-n']\n    add_star_opts(cmdlist, compression, verbosity)\n    cmdlist.append(\"file=%s\" % archive)\n    return cmdlist", "response": "List a TAR archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_tar (archive, compression, cmd, verbosity, interactive, filenames):\n    cmdlist = [cmd, '-c']\n    add_star_opts(cmdlist, compression, verbosity)\n    cmdlist.append(\"file=%s\" % archive)\n    cmdlist.extend(filenames)\n    return cmdlist", "response": "Create a TAR archive."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_arj (archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [cmd, 'x', '-r']\n    if not interactive:\n        cmdlist.append('-y')\n    cmdlist.extend([archive, outdir])\n    return cmdlist", "response": "Extract an ARJ archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist an ARJ archive.", "response": "def list_arj (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List an ARJ archive.\"\"\"\n    cmdlist = [cmd]\n    if verbosity > 1:\n        cmdlist.append('v')\n    else:\n        cmdlist.append('l')\n    if not interactive:\n        cmdlist.append('-y')\n    cmdlist.extend(['-r', archive])\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_arj (archive, compression, cmd, verbosity, interactive, filenames):\n    cmdlist = [cmd, 'a', '-r']\n    if not interactive:\n        cmdlist.append('-y')\n    cmdlist.append(archive)\n    cmdlist.extend(filenames)\n    return cmdlist", "response": "Create an ARJ archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a ZPAQ archive.", "response": "def create_zpaq(archive, compression, cmd, verbosity, interactive, filenames):\n    \"\"\"Create a ZPAQ archive.\"\"\"\n    cmdlist = [cmd, 'a', archive]\n    cmdlist.extend(filenames)\n    cmdlist.extend(['-method', '4'])\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_alzip (archive, compression, cmd, verbosity, interactive, outdir):\n    return [cmd, '-d', outdir, archive]", "response": "Extract an ALZIP archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract a TAR archive.", "response": "def extract_tar (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a TAR archive.\"\"\"\n    cmdlist = [cmd, '--extract']\n    add_tar_opts(cmdlist, compression, verbosity)\n    cmdlist.extend([\"--file\", archive, '--directory', outdir])\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_tar (archive, compression, cmd, verbosity, interactive):\n    cmdlist = [cmd, '--list']\n    add_tar_opts(cmdlist, compression, verbosity)\n    cmdlist.extend([\"--file\", archive])\n    return cmdlist", "response": "List a TAR archive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_tar (archive, compression, cmd, verbosity, interactive, filenames):\n    cmdlist = [cmd, '--create']\n    add_tar_opts(cmdlist, compression, verbosity)\n    cmdlist.extend([\"--file\", archive, '--'])\n    cmdlist.extend(filenames)\n    return cmdlist", "response": "Create a TAR archive."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding tar options to cmdlist.", "response": "def add_tar_opts (cmdlist, compression, verbosity):\n    \"\"\"Add tar options to cmdlist.\"\"\"\n    progname = os.path.basename(cmdlist[0])\n    if compression == 'gzip':\n        cmdlist.append('-z')\n    elif compression == 'compress':\n        cmdlist.append('-Z')\n    elif compression == 'bzip2':\n        cmdlist.append('-j')\n    elif compression in ('lzma', 'xz') and progname == 'bsdtar':\n        cmdlist.append('--%s' % compression)\n    elif compression in ('lzma', 'xz', 'lzip'):\n        # use the compression name as program name since\n        # tar is picky which programs it can use\n        program = compression\n        # set compression program\n        cmdlist.extend(['--use-compress-program', program])\n    if verbosity > 1:\n        cmdlist.append('--verbose')\n    if progname == 'tar':\n        cmdlist.append('--force-local')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract a CHM archive.", "response": "def extract_chm (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a CHM archive.\"\"\"\n    # archmage can only extract in non-existing directories\n    # so a nice dirname is created\n    name = util.get_single_outfile(\"\", archive)\n    outfile = os.path.join(outdir, name)\n    return [cmd, '-x', os.path.abspath(archive), outfile]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_zip(archive, compression, cmd, verbosity, interactive):\n    try:\n        with zipfile.ZipFile(archive, \"r\") as zfile:\n            for name in zfile.namelist():\n                if verbosity >= 0:\n                    print(name)\n    except Exception as err:\n        msg = \"error listing %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "List a ZIP archive with the zipfile Python module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract a ZIP archive with the zipfile Python module.", "response": "def extract_zip(archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a ZIP archive with the zipfile Python module.\"\"\"\n    try:\n        with zipfile.ZipFile(archive) as zfile:\n            zfile.extractall(outdir)\n    except Exception as err:\n        msg = \"error extracting %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_zip(archive, compression, cmd, verbosity, interactive, filenames):\n    try:\n        with zipfile.ZipFile(archive, 'w') as zfile:\n            for filename in filenames:\n                if os.path.isdir(filename):\n                    write_directory(zfile, filename)\n                else:\n                    zfile.write(filename)\n    except Exception as err:\n        msg = \"error creating %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Create a ZIP archive with the zipfile Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_directory (zfile, directory):\n    for dirpath, dirnames, filenames in os.walk(directory):\n        zfile.write(dirpath)\n        for filename in filenames:\n            zfile.write(os.path.join(dirpath, filename))", "response": "Write all files in a directory to a zipfile instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_shn (archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [util.shell_quote(cmd)]\n    outfile = util.get_single_outfile(outdir, archive, extension=\".wav\")\n    cmdlist.extend(['-x', '-', util.shell_quote(outfile), '<',\n        util.shell_quote(archive)])\n    return (cmdlist, {'shell': True})", "response": "Decompress a SHN archive to a WAV file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_shn (archive, compression, cmd, verbosity, interactive, filenames):\n    if len(filenames) > 1:\n        raise util.PatoolError(\"multiple filenames for shorten not supported\")\n    cmdlist = [util.shell_quote(cmd)]\n    cmdlist.extend(['-', util.shell_quote(archive), '<',\n        util.shell_quote(filenames[0])])\n    return (cmdlist, {'shell': True})", "response": "Create a SHN archive."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_dms (archive, compression, cmd, verbosity, interactive, outdir):\n    check_archive_ext(archive)\n    cmdlist = [cmd, '-d', outdir]\n    if verbosity > 1:\n        cmdlist.append('-v')\n    cmdlist.extend(['u', archive])\n    return cmdlist", "response": "Extract a DMS archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_dms (archive, compression, cmd, verbosity, interactive):\n    check_archive_ext(archive)\n    return [cmd, 'v', archive]", "response": "List a DMS archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_archive_ext (archive):\n    if not archive.lower().endswith(\".dms\"):\n        rest = archive[-4:]\n        msg = \"xdms(1) archive file must end with `.dms', not `%s'\" % rest\n        raise util.PatoolError(msg)", "response": "xdms ( 1 ) cannot handle files with extensions other than. dms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist a XZ archive.", "response": "def list_xz (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a XZ archive.\"\"\"\n    cmdlist = [cmd]\n    cmdlist.append('-l')\n    if verbosity > 1:\n        cmdlist.append('-v')\n    cmdlist.append(archive)\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_lzma(archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [util.shell_quote(cmd), '--format=lzma']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    outfile = util.get_single_outfile(outdir, archive)\n    cmdlist.extend(['-c', '-d', '--', util.shell_quote(archive), '>',\n        util.shell_quote(outfile)])\n    return (cmdlist, {'shell': True})", "response": "Extract an LZMA archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_ace (archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [cmd, 'x']\n    if not outdir.endswith('/'):\n        outdir += '/'\n    cmdlist.extend([archive, outdir])\n    return cmdlist", "response": "Extract an ACE archive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_ace (archive, compression, cmd, verbosity, interactive):\n    cmdlist = [cmd]\n    if verbosity > 1:\n        cmdlist.append('v')\n    else:\n        cmdlist.append('l')\n    cmdlist.append(archive)\n    return cmdlist", "response": "List an ACE archive."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_bzip2 (archive, compression, cmd, verbosity, interactive, outdir):\n    targetname = util.get_single_outfile(outdir, archive)\n    try:\n        with bz2.BZ2File(archive) as bz2file:\n            with open(targetname, 'wb') as targetfile:\n                data = bz2file.read(READ_SIZE_BYTES)\n                while data:\n                    targetfile.write(data)\n                    data = bz2file.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error extracting %s to %s: %s\" % (archive, targetname, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Extract a BZIP2 archive with the bz2 Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_bzip2 (archive, compression, cmd, verbosity, interactive, filenames):\n    if len(filenames) > 1:\n        raise util.PatoolError('multi-file compression not supported in Python bz2')\n    try:\n        with bz2.BZ2File(archive, 'wb') as bz2file:\n            filename = filenames[0]\n            with open(filename, 'rb') as srcfile:\n                data = srcfile.read(READ_SIZE_BYTES)\n                while data:\n                    bz2file.write(data)\n                    data = srcfile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error creating %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Create a BZIP2 archive with the bz2 Python module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts a CHM archive.", "response": "def extract_chm (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a CHM archive.\"\"\"\n    return [cmd, os.path.abspath(archive), outdir]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts a LZH archive.", "response": "def extract_lzh (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a LZH archive.\"\"\"\n    opts = 'x'\n    if verbosity > 1:\n        opts += 'v'\n    opts += \"w=%s\" % outdir\n    return [cmd, opts, archive]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting a LZH archive.", "response": "def list_lzh (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a LZH archive.\"\"\"\n    cmdlist = [cmd]\n    if verbosity > 1:\n        cmdlist.append('v')\n    else:\n        cmdlist.append('l')\n    cmdlist.append(archive)\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecompresses an APE archive to a WAV file.", "response": "def extract_ape (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Decompress an APE archive to a WAV file.\"\"\"\n    outfile = util.get_single_outfile(outdir, archive, extension=\".wav\")\n    return [cmd, archive, outfile, '-d']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_adf (archive, compression, cmd, verbosity, interactive, outdir):\n    return [cmd, archive, '-d', outdir]", "response": "Extract an ADF archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts a LRZIP archive.", "response": "def extract_lrzip (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a LRZIP archive.\"\"\"\n    cmdlist = [cmd, '-d']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    outfile = util.get_single_outfile(outdir, archive)\n    cmdlist.extend([\"-o\", outfile, os.path.abspath(archive)])\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists a BZIP2 archive.", "response": "def list_bzip2 (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a BZIP2 archive.\"\"\"\n    return stripext(cmd, archive, verbosity)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_ape (archive, compression, cmd, verbosity, interactive):\n    return stripext(cmd, archive, verbosity, extension=\".wav\")", "response": "List an APE archive."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the name without suffix.", "response": "def stripext (cmd, archive, verbosity, extension=\"\"):\n    \"\"\"Print the name without suffix.\"\"\"\n    if verbosity >= 0:\n        print(util.stripext(archive)+extension)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_ar (archive, compression, cmd, verbosity, interactive, outdir):\n    opts = 'x'\n    if verbosity > 1:\n        opts += 'v'\n    cmdlist = [cmd, opts, os.path.abspath(archive)]\n    return (cmdlist, {'cwd': outdir})", "response": "Extract an AR archive."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist an AR archive.", "response": "def list_ar (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a AR archive.\"\"\"\n    opts = 't'\n    if verbosity > 1:\n        opts += 'v'\n    return [cmd, opts, archive]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_ar (archive, compression, cmd, verbosity, interactive, filenames):\n    opts = 'rc'\n    if verbosity > 1:\n        opts += 'v'\n    cmdlist = [cmd, opts, archive]\n    cmdlist.extend(filenames)\n    return cmdlist", "response": "Create an AR archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting a CAB archive.", "response": "def extract_cab (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a CAB archive.\"\"\"\n    cmdlist = [cmd, '-d', outdir]\n    if verbosity > 0:\n        cmdlist.append('-v')\n    cmdlist.append(archive)\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist a CAB archive.", "response": "def list_cab (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a CAB archive.\"\"\"\n    cmdlist = [cmd, '-l']\n    if verbosity > 0:\n        cmdlist.append('-v')\n    cmdlist.append(archive)\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_rzip (archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [cmd, '-d', '-k']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    outfile = util.get_single_outfile(outdir, archive)\n    cmdlist.extend([\"-o\", outfile, archive])\n    return cmdlist", "response": "Extract an RZIP archive."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeciding if the given program supports the given compression format natively.", "response": "def program_supports_compression (program, compression):\n    \"\"\"Decide if the given program supports the compression natively.\n    @return: True iff the program supports the given compression format\n      natively, else False.\n    \"\"\"\n    if program in ('tar', ):\n        return compression in ('gzip', 'bzip2', 'xz', 'lzip', 'compress', 'lzma') + py_lzma\n    elif program in ('star', 'bsdtar', 'py_tarfile'):\n        return compression in ('gzip', 'bzip2') + py_lzma\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_archive_format (filename):\n    mime, compression = util.guess_mime(filename)\n    if not (mime or compression):\n        raise util.PatoolError(\"unknown archive format for file `%s'\" % filename)\n    if mime in ArchiveMimetypes:\n        format = ArchiveMimetypes[mime]\n    else:\n        raise util.PatoolError(\"unknown archive format for file `%s' (mime-type is `%s')\" % (filename, mime))\n    if format == compression:\n        # file cannot be in same format compressed\n        compression = None\n    return format, compression", "response": "Detect filename archive format and optional compression."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_archive_format (format, compression):\n    if format not in ArchiveFormats:\n        raise util.PatoolError(\"unknown archive format `%s'\" % format)\n    if compression is not None and compression not in ArchiveCompressions:\n        raise util.PatoolError(\"unkonwn archive compression `%s'\" % compression)", "response": "Make sure format and compression are known."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_archive_program (format, command, program=None):\n    commands = ArchivePrograms[format]\n    programs = []\n    if program is not None:\n        # try a specific program first\n        programs.append(program)\n    # first try the universal programs with key None\n    for key in (None, command):\n        if key in commands:\n            programs.extend(commands[key])\n    if not programs:\n        raise util.PatoolError(\"%s archive format `%s' is not supported\" % (command, format))\n    # return the first existing program\n    for program in programs:\n        if program.startswith('py_'):\n            # it's a Python module and therefore always supported\n            return program\n        exe = util.find_program(program)\n        if exe:\n            if program == '7z' and format == 'rar' and not util.p7zip_supports_rar():\n                continue\n            return exe\n    # no programs found\n    raise util.PatoolError(\"could not find an executable program to %s format %s; candidates are (%s),\" % (command, format, \",\".join(programs)))", "response": "Find a suitable archive program for given format and mode."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting information about available archive formats to stdout.", "response": "def list_formats ():\n    \"\"\"Print information about available archive formats to stdout.\"\"\"\n    print(\"Archive programs of\", App)\n    print(\"Archive programs are searched in the following directories:\")\n    print(util.system_search_path())\n    print()\n    for format in ArchiveFormats:\n        print(format, \"files:\")\n        for command in ArchiveCommands:\n            programs = ArchivePrograms[format]\n            if command not in programs and None not in programs:\n                print(\"   %8s: - (not supported)\" % command)\n                continue\n            try:\n                program = find_archive_program(format, command)\n                print(\"   %8s: %s\" % (command, program), end=' ')\n                if format == 'tar':\n                    encs = [x for x in ArchiveCompressions if util.find_program(x)]\n                    if encs:\n                        print(\"(supported compressions: %s)\" % \", \".join(encs), end=' ')\n                elif format == '7z':\n                    if util.p7zip_supports_rar():\n                        print(\"(rar archives supported)\", end=' ')\n                    else:\n                        print(\"(rar archives not supported)\", end=' ')\n                print()\n            except util.PatoolError:\n                # display information what programs can handle this archive format\n                handlers = programs.get(None, programs.get(command))\n                print(\"   %8s: - (no program found; install %s)\" %\n                      (command, util.strlist_with_or(handlers)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a program supports the given compression.", "response": "def check_program_compression(archive, command, program, compression):\n    \"\"\"Check if a program supports the given compression.\"\"\"\n    program = os.path.basename(program)\n    if compression:\n        # check if compression is supported\n        if not program_supports_compression(program, compression):\n            if command == 'create':\n                comp_command = command\n            else:\n                comp_command = 'extract'\n            comp_prog = find_archive_program(compression, comp_command)\n            if not comp_prog:\n                msg = \"cannot %s archive `%s': compression `%s' not supported\"\n                raise util.PatoolError(msg % (command, archive, compression))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving a single file or directory inside outdir a level up. Returns True if successful False if not.", "response": "def move_outdir_orphan (outdir):\n    \"\"\"Move a single file or directory inside outdir a level up.\n    Never overwrite files.\n    Return (True, outfile) if successful, (False, reason) if not.\"\"\"\n    entries = os.listdir(outdir)\n    if len(entries) == 1:\n        src = os.path.join(outdir, entries[0])\n        dst = os.path.join(os.path.dirname(outdir), entries[0])\n        if os.path.exists(dst) or os.path.islink(dst):\n            return (False, \"local file exists\")\n        shutil.move(src, dst)\n        os.rmdir(outdir)\n        return (True, entries[0])\n    return (False, \"multiple files in root\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_file_readable (filename):\n    if not os.path.islink(filename):\n        util.set_mode(filename, stat.S_IRUSR)", "response": "Make file user readable if it is not a link."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_user_readable (directory):\n    for root, dirs, files in os.walk(directory, onerror=util.log_error):\n        for filename in files:\n            make_file_readable(os.path.join(root, filename))\n        for dirname in dirs:\n            make_dir_readable(os.path.join(root, dirname))", "response": "Make all files in given directory user readable. Also recurse into\n    subdirectories."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cleanup_outdir (outdir, archive):\n    make_user_readable(outdir)\n    # move single directory or file in outdir\n    (success, msg) = move_outdir_orphan(outdir)\n    if success:\n        # msg is a single directory or filename\n        return msg, \"`%s'\" % msg\n    # outdir remains unchanged\n    # rename it to something more user-friendly (basically the archive\n    # name without extension)\n    outdir2 = util.get_single_outfile(\"\", archive)\n    os.rename(outdir, outdir2)\n    return outdir2, \"`%s' (%s)\" % (outdir2, msg)", "response": "Cleanup outdir after extraction and return target file name and result string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract an archive. @return: output directory if command is 'extract', else None", "response": "def _extract_archive(archive, verbosity=0, interactive=True, outdir=None,\n                     program=None, format=None, compression=None):\n    \"\"\"Extract an archive.\n    @return: output directory if command is 'extract', else None\n    \"\"\"\n    if format is None:\n        format, compression = get_archive_format(archive)\n    check_archive_format(format, compression)\n    program = find_archive_program(format, 'extract', program=program)\n    check_program_compression(archive, 'extract', program, compression)\n    get_archive_cmdlist = get_archive_cmdlist_func(program, 'extract', format)\n    if outdir is None:\n        outdir = util.tmpdir(dir=\".\")\n        do_cleanup_outdir = True\n    else:\n        do_cleanup_outdir = False\n    try:\n        cmdlist = get_archive_cmdlist(archive, compression, program, verbosity, interactive, outdir)\n        if cmdlist:\n            # an empty command list means the get_archive_cmdlist() function\n            # already handled the command (eg. when it's a builtin Python\n            # function)\n            run_archive_cmdlist(cmdlist, verbosity=verbosity)\n        if do_cleanup_outdir:\n            target, msg = cleanup_outdir(outdir, archive)\n        else:\n            target, msg = outdir, \"`%s'\" % outdir\n        if verbosity >= 0:\n            util.log_info(\"... %s extracted to %s.\" % (archive, msg))\n        return target\n    finally:\n        # try to remove an empty temporary output directory\n        if do_cleanup_outdir:\n            try:\n                os.rmdir(outdir)\n            except OSError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntesting and list archives.", "response": "def _handle_archive(archive, command, verbosity=0, interactive=True,\n                    program=None, format=None, compression=None):\n    \"\"\"Test and list archives.\"\"\"\n    if format is None:\n        format, compression = get_archive_format(archive)\n    check_archive_format(format, compression)\n    if command not in ('list', 'test'):\n        raise util.PatoolError(\"invalid archive command `%s'\" % command)\n    program = find_archive_program(format, command, program=program)\n    check_program_compression(archive, command, program, compression)\n    get_archive_cmdlist = get_archive_cmdlist_func(program, command, format)\n    # prepare keyword arguments for command list\n    cmdlist = get_archive_cmdlist(archive, compression, program, verbosity, interactive)\n    if cmdlist:\n        # an empty command list means the get_archive_cmdlist() function\n        # already handled the command (eg. when it's a builtin Python\n        # function)\n        run_archive_cmdlist(cmdlist, verbosity=verbosity)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_archive_cmdlist_func (program, command, format):\n    # get python module for given archive program\n    key = util.stripext(os.path.basename(program).lower())\n    modulename = \".programs.\" + ProgramModules.get(key, key)\n    # import the module\n    try:\n        module = importlib.import_module(modulename, __name__)\n    except ImportError as msg:\n        raise util.PatoolError(msg)\n    # get archive handler function (eg. patoolib.programs.star.extract_tar)\n    try:\n        return getattr(module, '%s_%s' % (command, format))\n    except AttributeError as msg:\n        raise util.PatoolError(msg)", "response": "Get the Python function that executes the given program."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _diff_archives (archive1, archive2, verbosity=0, interactive=True):\n    if util.is_same_file(archive1, archive2):\n        return 0\n    diff = util.find_program(\"diff\")\n    if not diff:\n        msg = \"The diff(1) program is required for showing archive differences, please install it.\"\n        raise util.PatoolError(msg)\n    tmpdir1 = util.tmpdir()\n    try:\n        path1 = _extract_archive(archive1, outdir=tmpdir1, verbosity=-1)\n        tmpdir2 = util.tmpdir()\n        try:\n            path2 = _extract_archive(archive2, outdir=tmpdir2, verbosity=-1)\n            return util.run_checked([diff, \"-urN\", path1, path2], verbosity=1, ret_ok=(0, 1))\n        finally:\n            shutil.rmtree(tmpdir2, onerror=rmtree_log_error)\n    finally:\n        shutil.rmtree(tmpdir1, onerror=rmtree_log_error)", "response": "Show differences between two archives."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for given pattern in an archive.", "response": "def _search_archive(pattern, archive, verbosity=0, interactive=True):\n    \"\"\"Search for given pattern in an archive.\"\"\"\n    grep = util.find_program(\"grep\")\n    if not grep:\n        msg = \"The grep(1) program is required for searching archive contents, please install it.\"\n        raise util.PatoolError(msg)\n    tmpdir = util.tmpdir()\n    try:\n        path = _extract_archive(archive, outdir=tmpdir, verbosity=-1)\n        return util.run_checked([grep, \"-r\", \"-e\", pattern, \".\"], ret_ok=(0, 1), verbosity=1, cwd=path)\n    finally:\n        shutil.rmtree(tmpdir, onerror=rmtree_log_error)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrepackaging an archive to a different format.", "response": "def _repack_archive (archive1, archive2, verbosity=0, interactive=True):\n    \"\"\"Repackage an archive to a different format.\"\"\"\n    format1, compression1 = get_archive_format(archive1)\n    format2, compression2 = get_archive_format(archive2)\n    if format1 == format2 and compression1 == compression2:\n        # same format and compression allows to copy the file\n        util.link_or_copy(archive1, archive2, verbosity=verbosity)\n        return\n    tmpdir = util.tmpdir()\n    try:\n        kwargs = dict(verbosity=verbosity, outdir=tmpdir)\n        same_format = (format1 == format2 and compression1 and compression2)\n        if same_format:\n            # only decompress since the format is the same\n            kwargs['format'] = compression1\n        path = _extract_archive(archive1, **kwargs)\n        archive = os.path.abspath(archive2)\n        files = tuple(os.listdir(path))\n        olddir = os.getcwd()\n        os.chdir(path)\n        try:\n            kwargs = dict(verbosity=verbosity, interactive=interactive)\n            if same_format:\n                # only compress since the format is the same\n                kwargs['format'] = compression2\n            _create_archive(archive, files, **kwargs)\n        finally:\n            os.chdir(olddir)\n    finally:\n        shutil.rmtree(tmpdir, onerror=rmtree_log_error)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _recompress_archive(archive, verbosity=0, interactive=True):\n    format, compression = get_archive_format(archive)\n    if compression:\n        # only recompress the compression itself (eg. for .tar.xz)\n        format = compression\n    tmpdir = util.tmpdir()\n    tmpdir2 = util.tmpdir()\n    base, ext = os.path.splitext(os.path.basename(archive))\n    archive2 = util.get_single_outfile(tmpdir2, base, extension=ext)\n    try:\n        # extract\n        kwargs = dict(verbosity=verbosity, format=format, outdir=tmpdir)\n        path = _extract_archive(archive, **kwargs)\n        # compress to new file\n        olddir = os.getcwd()\n        os.chdir(path)\n        try:\n            kwargs = dict(verbosity=verbosity, interactive=interactive, format=format)\n            files = tuple(os.listdir(path))\n            _create_archive(archive2, files, **kwargs)\n        finally:\n            os.chdir(olddir)\n        # check file sizes and replace if new file is smaller\n        filesize = util.get_filesize(archive)\n        filesize2 = util.get_filesize(archive2)\n        if filesize2 < filesize:\n            # replace file\n            os.remove(archive)\n            shutil.move(archive2, archive)\n            diffsize = filesize - filesize2\n            return \"... recompressed file is now %s smaller.\" % util.strsize(diffsize)\n    finally:\n        shutil.rmtree(tmpdir, onerror=rmtree_log_error)\n        shutil.rmtree(tmpdir2, onerror=rmtree_log_error)\n    return \"... recompressed file is not smaller, leaving archive as is.\"", "response": "Try to recompress an archive to smaller size."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates given archive with given files.", "response": "def create_archive(archive, filenames, verbosity=0, program=None, interactive=True):\n    \"\"\"Create given archive with given files.\"\"\"\n    util.check_new_filename(archive)\n    util.check_archive_filelist(filenames)\n    if verbosity >= 0:\n        util.log_info(\"Creating %s ...\" % archive)\n    res = _create_archive(archive, filenames, verbosity=verbosity,\n                          interactive=interactive, program=program)\n    if verbosity >= 0:\n        util.log_info(\"... %s created.\" % archive)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_archives(archive1, archive2, verbosity=0, interactive=True):\n    util.check_existing_filename(archive1)\n    util.check_existing_filename(archive2)\n    if verbosity >= 0:\n        util.log_info(\"Comparing %s with %s ...\" % (archive1, archive2))\n    res = _diff_archives(archive1, archive2, verbosity=verbosity, interactive=interactive)\n    if res == 0 and verbosity >= 0:\n        util.log_info(\"... no differences found.\")", "response": "Print differences between two archives."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching pattern in archive members.", "response": "def search_archive(pattern, archive, verbosity=0, interactive=True):\n    \"\"\"Search pattern in archive members.\"\"\"\n    if not pattern:\n        raise util.PatoolError(\"empty search pattern\")\n    util.check_existing_filename(archive)\n    if verbosity >= 0:\n        util.log_info(\"Searching %r in %s ...\" % (pattern, archive))\n    res = _search_archive(pattern, archive, verbosity=verbosity, interactive=interactive)\n    if res == 1 and verbosity >= 0:\n        util.log_info(\"... %r not found\" % pattern)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recompress_archive(archive, verbosity=0, interactive=True):\n    util.check_existing_filename(archive)\n    util.check_writable_filename(archive)\n    if verbosity >= 0:\n        util.log_info(\"Recompressing %s ...\" % (archive,))\n    res = _recompress_archive(archive, verbosity=verbosity, interactive=interactive)\n    if res and verbosity >= 0:\n        util.log_info(res)\n    return 0", "response": "Recompress an archive to hopefully smaller size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_mimedb():\n    global mimedb\n    try:\n        mimedb = mimetypes.MimeTypes(strict=False)\n    except Exception as msg:\n        log_error(\"could not initialize MIME database: %s\" % msg)\n        return\n    add_mimedb_data(mimedb)", "response": "Initialize the internal MIME database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding missing encodings and mimetypes to MIME database.", "response": "def add_mimedb_data(mimedb):\n    \"\"\"Add missing encodings and mimetypes to MIME database.\"\"\"\n    mimedb.encodings_map['.bz2'] = 'bzip2'\n    mimedb.encodings_map['.lzma'] = 'lzma'\n    mimedb.encodings_map['.xz'] = 'xz'\n    mimedb.encodings_map['.lz'] = 'lzip'\n    mimedb.suffix_map['.tbz2'] = '.tar.bz2'\n    add_mimetype(mimedb, 'application/x-lzop', '.lzo')\n    add_mimetype(mimedb, 'application/x-adf', '.adf')\n    add_mimetype(mimedb, 'application/x-arj', '.arj')\n    add_mimetype(mimedb, 'application/x-lzma', '.lzma')\n    add_mimetype(mimedb, 'application/x-xz', '.xz')\n    add_mimetype(mimedb, 'application/java-archive', '.jar')\n    add_mimetype(mimedb, 'application/x-rar', '.rar')\n    add_mimetype(mimedb, 'application/x-rar', '.cbr')\n    add_mimetype(mimedb, 'application/x-7z-compressed', '.7z')\n    add_mimetype(mimedb, 'application/x-7z-compressed', '.cb7')\n    add_mimetype(mimedb, 'application/x-cab', '.cab')\n    add_mimetype(mimedb, 'application/x-rpm', '.rpm')\n    add_mimetype(mimedb, 'application/x-debian-package', '.deb')\n    add_mimetype(mimedb, 'application/x-ace', '.ace')\n    add_mimetype(mimedb, 'application/x-ace', '.cba')\n    add_mimetype(mimedb, 'application/x-archive', '.a')\n    add_mimetype(mimedb, 'application/x-alzip', '.alz')\n    add_mimetype(mimedb, 'application/x-arc', '.arc')\n    add_mimetype(mimedb, 'application/x-lrzip', '.lrz')\n    add_mimetype(mimedb, 'application/x-lha', '.lha')\n    add_mimetype(mimedb, 'application/x-lzh', '.lzh')\n    add_mimetype(mimedb, 'application/x-rzip', '.rz')\n    add_mimetype(mimedb, 'application/x-zoo', '.zoo')\n    add_mimetype(mimedb, 'application/x-dms', '.dms')\n    add_mimetype(mimedb, 'application/x-zip-compressed', '.crx')\n    add_mimetype(mimedb, 'application/x-shar', '.shar')\n    add_mimetype(mimedb, 'application/x-tar', '.cbt')\n    add_mimetype(mimedb, 'application/x-vhd', '.vhd')\n    add_mimetype(mimedb, 'audio/x-ape', '.ape')\n    add_mimetype(mimedb, 'audio/x-shn', '.shn')\n    add_mimetype(mimedb, 'audio/flac', '.flac')\n    add_mimetype(mimedb, 'application/x-chm', '.chm')\n    add_mimetype(mimedb, 'application/x-iso9660-image', '.iso')\n    add_mimetype(mimedb, 'application/zip', '.cbz')\n    add_mimetype(mimedb, 'application/zip', '.epub')\n    add_mimetype(mimedb, 'application/zip', '.apk')\n    add_mimetype(mimedb, 'application/zpaq', '.zpaq')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backtick (cmd, encoding='utf-8'):\n    data = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0]\n    return data.decode(encoding)", "response": "Return decoded output from command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning command without error checking.", "response": "def run (cmd, verbosity=0, **kwargs):\n    \"\"\"Run command without error checking.\n    @return: command return code\"\"\"\n    # Note that shell_quote_nt() result is not suitable for copy-paste\n    # (especially on Unix systems), but it looks nicer than shell_quote().\n    if verbosity >= 0:\n        log_info(\"running %s\" % \" \".join(map(shell_quote_nt, cmd)))\n    if kwargs:\n        if verbosity >= 0:\n            log_info(\"    with %s\" % \", \".join(\"%s=%s\" % (k, shell_quote(str(v)))\\\n                                           for k, v in kwargs.items()))\n        if kwargs.get(\"shell\"):\n            # for shell calls the command must be a string\n            cmd = \" \".join(cmd)\n    if verbosity < 1:\n        # hide command output on stdout\n        with open(os.devnull, 'wb') as devnull:\n            kwargs['stdout'] = devnull\n            res = subprocess.call(cmd, **kwargs)\n    else:\n        res = subprocess.call(cmd, **kwargs)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns command and raise PatoolError on error.", "response": "def run_checked (cmd, ret_ok=(0,), **kwargs):\n    \"\"\"Run command and raise PatoolError on error.\"\"\"\n    retcode = run(cmd, **kwargs)\n    if retcode not in ret_ok:\n        msg = \"Command `%s' returned non-zero exit status %d\" % (cmd, retcode)\n        raise PatoolError(msg)\n    return retcode"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef guess_mime (filename):\n    mime, encoding = guess_mime_file(filename)\n    if mime is None:\n        mime, encoding = guess_mime_mimedb(filename)\n    assert mime is not None or encoding is None\n    return mime, encoding", "response": "Guess the MIME type of a given filename using mimedb."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nguessing MIME type from given filename.", "response": "def guess_mime_mimedb (filename):\n    \"\"\"Guess MIME type from given filename.\n    @return: tuple (mime, encoding)\n    \"\"\"\n    mime, encoding = None, None\n    if mimedb is not None:\n        mime, encoding = mimedb.guess_type(filename, strict=False)\n    if mime not in ArchiveMimetypes and encoding in ArchiveCompressions:\n        # Files like 't.txt.gz' are recognized with encoding as format, and\n        # an unsupported mime-type like 'text/plain'. Fix this.\n        mime = Encoding2Mime[encoding]\n        encoding = None\n    return mime, encoding"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef guess_mime_file (filename):\n    mime, encoding = None, None\n    base, ext = os.path.splitext(filename)\n    if ext.lower() in ('.alz',):\n        # let mimedb recognize these extensions\n        return mime, encoding\n    if os.path.isfile(filename):\n        file_prog = find_program(\"file\")\n        if file_prog:\n            mime, encoding = guess_mime_file_mime(file_prog, filename)\n            if mime is None:\n                mime = guess_mime_file_text(file_prog, filename)\n                encoding = None\n    if mime in Mime2Encoding:\n        # try to look inside compressed archives\n        cmd = [file_prog, \"--brief\", \"--mime\", \"--uncompress\", filename]\n        try:\n            outparts = backtick(cmd).strip().split(\";\")\n        except OSError:\n            # ignore errors, as file(1) is only a fallback\n            return mime, encoding\n        mime2 = outparts[0].split(\" \", 1)[0]\n        # Some file(1) implementations return an empty or unknown mime type\n        # when the uncompressor program is not installed, other\n        # implementation return the original file type.\n        # The following detects both cases.\n        if (mime2 in ('application/x-empty', 'application/octet-stream') or\n            mime2 in Mime2Encoding):\n            # The uncompressor program file(1) uses is not installed\n            # or is not able to uncompress.\n            # Try to get mime information from the file extension.\n            mime2, encoding2 = guess_mime_mimedb(filename)\n            if mime2 in ArchiveMimetypes:\n                mime = mime2\n                encoding = encoding2\n        elif mime2 in ArchiveMimetypes:\n            mime = mime2\n            encoding = get_file_mime_encoding(outparts)\n    # Only return mime and encoding if the given mime can natively support the encoding.\n    if program_supports_compression(ArchiveMimetypes.get(mime), encoding):\n        return mime, encoding\n    else:\n        # If encoding is None, default back to `mime`.\n        return Encoding2Mime.get(encoding, mime), None", "response": "Guess MIME type of filename with file ( 1 ) using file and look the result string\n    "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef guess_mime_file_mime (file_prog, filename):\n    mime, encoding = None, None\n    cmd = [file_prog, \"--brief\", \"--mime-type\", filename]\n    try:\n        mime = backtick(cmd).strip()\n    except OSError:\n        # ignore errors, as file(1) is only a fallback\n        pass\n    if mime not in ArchiveMimetypes:\n        mime, encoding = None, None\n    return mime, encoding", "response": "Guess MIME type of filename with file1 and -- mime option."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget encoding value from splitted output of file - mime - uncompress.", "response": "def get_file_mime_encoding (parts):\n    \"\"\"Get encoding value from splitted output of file --mime --uncompress.\"\"\"\n    for part in parts:\n        for subpart in part.split(\" \"):\n            if subpart.startswith(\"compressed-encoding=\"):\n                mime = subpart.split(\"=\")[1].strip()\n                return Mime2Encoding.get(mime)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guess_mime_file_text (file_prog, filename):\n    cmd = [file_prog, \"--brief\", filename]\n    try:\n        output = backtick(cmd).strip()\n    except OSError:\n        # ignore errors, as file(1) is only a fallback\n        return None\n    # match output against known strings\n    for matcher, mime in FileText2Mime.items():\n        if output.startswith(matcher) and mime in ArchiveMimetypes:\n            return mime\n    return None", "response": "Determine MIME type of filename with file ( 1 )."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_existing_filename (filename, onlyfiles=True):\n    if not os.path.exists(filename):\n        raise PatoolError(\"file `%s' was not found\" % filename)\n    if not os.access(filename, os.R_OK):\n        raise PatoolError(\"file `%s' is not readable\" % filename)\n    if onlyfiles and not os.path.isfile(filename):\n        raise PatoolError(\"`%s' is not a file\" % filename)", "response": "Ensure that given filename is a valid existing file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the file list contains only existing files.", "response": "def check_archive_filelist (filenames):\n    \"\"\"Check that file list is not empty and contains only existing files.\"\"\"\n    if not filenames:\n        raise PatoolError(\"cannot create archive with empty filelist\")\n    for filename in filenames:\n        check_existing_filename(filename, onlyfiles=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_mode (filename, flags):\n    try:\n        mode = os.lstat(filename).st_mode\n    except OSError:\n        # ignore\n        return\n    if not (mode & flags):\n        try:\n            os.chmod(filename, flags | mode)\n        except OSError as msg:\n            log_error(\"could not set mode flags for `%s': %s\" % (filename, msg))", "response": "Set mode flags for given filename if not already set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tmpfile (dir=None, prefix=\"temp\", suffix=None):\n    return tempfile.mkstemp(suffix=suffix, prefix=prefix, dir=dir)[1]", "response": "Return a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_single_outfile (directory, archive, extension=\"\"):\n    outfile = os.path.join(directory, stripext(archive))\n    if os.path.exists(outfile + extension):\n        # prevent overwriting existing files\n        i = 1\n        newfile = \"%s%d\" % (outfile, i)\n        while os.path.exists(newfile + extension):\n            newfile = \"%s%d\" % (outfile, i)\n            i += 1\n        outfile = newfile\n    return outfile + extension", "response": "Get output filename if archive is in a single file format like gzip."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint environment key = value to out.", "response": "def print_env_info(key, out=sys.stderr):\n    \"\"\"If given environment key is defined, print it out.\"\"\"\n    value = os.getenv(key)\n    if value is not None:\n        print(key, \"=\", repr(value), file=out)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_app_info(out=sys.stderr):\n    print(\"System info:\", file=out)\n    print(configuration.App, file=out)\n    print(\"Python %(version)s on %(platform)s\" %\n                    {\"version\": sys.version, \"platform\": sys.platform}, file=out)\n    stime = strtime(time.time())\n    print(\"Local time:\", stime, file=out)\n    print(\"sys.argv\", sys.argv, file=out)", "response": "Print system and application info."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if the RAR codec is installed for 7z program.", "response": "def p7zip_supports_rar():\n    \"\"\"Determine if the RAR codec is installed for 7z program.\"\"\"\n    if os.name == 'nt':\n        # Assume RAR support is compiled into the binary.\n        return True\n    # the subdirectory and codec name\n    codecname = 'p7zip/Codecs/Rar29.so'\n    # search canonical user library dirs\n    for libdir in ('/usr/lib', '/usr/local/lib', '/usr/lib64', '/usr/local/lib64', '/usr/lib/i386-linux-gnu', '/usr/lib/x86_64-linux-gnu'):\n        fname = os.path.join(libdir, codecname)\n        if os.path.exists(fname):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_program (program):\n    if os.name == 'nt':\n        # Add some well-known archiver programs to the search path\n        path = os.environ['PATH']\n        path = append_to_path(path, get_nt_7z_dir())\n        path = append_to_path(path, get_nt_mac_dir())\n        path = append_to_path(path, get_nt_winrar_dir())\n    else:\n        # use default path\n        path = None\n    return which(program, path=path)", "response": "Look for program in environment PATH variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a directory to the PATH environment variable if it is a valid directory.", "response": "def append_to_path (path, directory):\n    \"\"\"Add a directory to the PATH environment variable, if it is a valid\n    directory.\"\"\"\n    if not os.path.isdir(directory) or directory in path:\n        return path\n    if not path.endswith(os.pathsep):\n        path += os.pathsep\n    return path + directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn 7 - Zip directory from registry or an empty string.", "response": "def get_nt_7z_dir ():\n    \"\"\"Return 7-Zip directory from registry, or an empty string.\"\"\"\n    # Python 3.x renamed the _winreg module to winreg\n    try:\n        import _winreg as winreg\n    except ImportError:\n        import winreg\n    try:\n        key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r\"SOFTWARE\\7-Zip\")\n        try:\n            return winreg.QueryValueEx(key, \"Path\")[0]\n        finally:\n            winreg.CloseKey(key)\n    except WindowsError:\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if filename1 and filename2 point to the same file object.", "response": "def is_same_file (filename1, filename2):\n    \"\"\"Check if filename1 and filename2 point to the same file object.\n    There can be false negatives, ie. the result is False, but it is\n    the same file anyway. Reason is that network filesystems can create\n    different paths to the same physical file.\n    \"\"\"\n    if filename1 == filename2:\n        return True\n    if os.name == 'posix':\n        return os.path.samefile(filename1, filename2)\n    return is_same_filename(filename1, filename2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if filename1 and filename2 are the same filename.", "response": "def is_same_filename (filename1, filename2):\n    \"\"\"Check if filename1 and filename2 are the same filename.\"\"\"\n    return os.path.realpath(filename1) == os.path.realpath(filename2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to link src to dst and if that fails copy the file.", "response": "def link_or_copy(src, dst, verbosity=0):\n    \"\"\"Try to make a hard link from src to dst and if that fails\n    copy the file. Hard links save some disk space and linking\n    should fail fast since no copying is involved.\n    \"\"\"\n    if verbosity > 0:\n        log_info(\"Copying %s -> %s\" % (src, dst))\n    try:\n        os.link(src, dst)\n    except (AttributeError, OSError):\n        try:\n            shutil.copy(src, dst)\n        except OSError as msg:\n            raise PatoolError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the current directory and return the new directory.", "response": "def chdir(directory):\n    \"\"\"Remember and return current directory before calling os.chdir().\n    If the current directory could not be determined, return None.\n    \"\"\"\n    try:\n        olddir = os.getcwd()\n    except OSError:\n        olddir = None\n    os.chdir(directory)\n    return olddir"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecompresses a FLAC archive to a WAV file.", "response": "def extract_flac (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Decompress a FLAC archive to a WAV file.\"\"\"\n    outfile = util.get_single_outfile(outdir, archive, extension=\".wav\")\n    cmdlist = [cmd, '--decode', archive, '--output-name', outfile]\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_flac (archive, compression, cmd, verbosity, interactive, filenames):\n    cmdlist = [cmd, filenames[0], '--best', '--output-name', archive]\n    return cmdlist", "response": "Create a FLAC archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting a RAR archive.", "response": "def extract_rar (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a RAR archive.\"\"\"\n    cmdlist = [cmd, 'x']\n    if not interactive:\n        cmdlist.extend(['-p-', '-y'])\n    cmdlist.extend(['--', os.path.abspath(archive)])\n    return (cmdlist, {'cwd': outdir})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_cpio (archive, compression, cmd, verbosity, interactive, outdir):\n    cmdlist = [util.shell_quote(cmd), '--extract', '--make-directories',\n        '--preserve-modification-time']\n    if sys.platform.startswith('linux') and not cmd.endswith('bsdcpio'):\n        cmdlist.extend(['--no-absolute-filenames',\n        '--force-local', '--nonmatching', r'\"*\\.\\.*\"'])\n    if verbosity > 1:\n        cmdlist.append('-v')\n    cmdlist.extend(['<', util.shell_quote(os.path.abspath(archive))])\n    return (cmdlist, {'cwd': outdir, 'shell': True})", "response": "Extract a CPIO archive."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_cpio(archive, compression, cmd, verbosity, interactive, filenames):\n    cmdlist = [util.shell_quote(cmd), '--create']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    if len(filenames) != 0:\n        findcmd = ['find']\n        findcmd.extend([util.shell_quote(x) for x in filenames])\n        findcmd.extend(['-print0', '|'])\n        cmdlist[0:0] = findcmd\n        cmdlist.append('-0')\n    cmdlist.extend([\">\", util.shell_quote(archive)])\n    return (cmdlist, {'shell': True})", "response": "Create a CPIO archive."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_arc (archive, compression, cmd, verbosity, interactive, outdir):\n    # Since extracted files will be placed in the current directory,\n    # the cwd argument has to be the output directory.\n    cmdlist = [cmd, 'x', os.path.abspath(archive)]\n    return (cmdlist, {'cwd': outdir})", "response": "Extract an ARC archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists a ARC archive.", "response": "def list_arc (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a ARC archive.\"\"\"\n    cmdlist = [cmd]\n    if verbosity > 1:\n        cmdlist.append('v')\n    else:\n        cmdlist.append('l')\n    cmdlist.append(archive)\n    return cmdlist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract a GZIP archive with the gzip Python module.", "response": "def extract_gzip (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a GZIP archive with the gzip Python module.\"\"\"\n    targetname = util.get_single_outfile(outdir, archive)\n    try:\n        with gzip.GzipFile(archive) as gzipfile:\n            with open(targetname, 'wb') as targetfile:\n                data = gzipfile.read(READ_SIZE_BYTES)\n                while data:\n                    targetfile.write(data)\n                    data = gzipfile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error extracting %s to %s: %s\" % (archive, targetname, err)\n        raise util.PatoolError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a GZIP archive with the gzip Python module.", "response": "def create_gzip (archive, compression, cmd, verbosity, interactive, filenames):\n    \"\"\"Create a GZIP archive with the gzip Python module.\"\"\"\n    if len(filenames) > 1:\n        raise util.PatoolError('multi-file compression not supported in Python gzip')\n    try:\n        with gzip.GzipFile(archive, 'wb') as gzipfile:\n            filename = filenames[0]\n            with open(filename, 'rb') as srcfile:\n                data = srcfile.read(READ_SIZE_BYTES)\n                while data:\n                    gzipfile.write(data)\n                    data = srcfile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error creating %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract(archive, compression, cmd, format, verbosity, outdir):\n    targetname = util.get_single_outfile(outdir, archive)\n    try:\n        with lzma.LZMAFile(archive, **_get_lzma_options(format)) as lzmafile:\n            with open(targetname, 'wb') as targetfile:\n                data = lzmafile.read(READ_SIZE_BYTES)\n                while data:\n                    targetfile.write(data)\n                    data = lzmafile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error extracting %s to %s: %s\" % (archive, targetname, err)\n        raise util.PatoolError(msg)\n    return None", "response": "Extract an LZMA or XZ archive with the lzma Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_lzma(archive, compression, cmd, verbosity, interactive, outdir):\n    return _extract(archive, compression, cmd, 'alone', verbosity, outdir)", "response": "Extract an LZMA archive with the lzma Python module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract an XZ archive with the lzma Python module.", "response": "def extract_xz(archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract an XZ archive with the lzma Python module.\"\"\"\n    return _extract(archive, compression, cmd, 'xz', verbosity, outdir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an LZMA or XZ archive with the lzma Python module.", "response": "def _create(archive, compression, cmd, format, verbosity, filenames):\n    \"\"\"Create an LZMA or XZ archive with the lzma Python module.\"\"\"\n    if len(filenames) > 1:\n        raise util.PatoolError('multi-file compression not supported in Python lzma')\n    try:\n        with lzma.LZMAFile(archive, mode='wb', **_get_lzma_options(format, preset=9)) as lzmafile:\n            filename = filenames[0]\n            with open(filename, 'rb') as srcfile:\n                data = srcfile.read(READ_SIZE_BYTES)\n                while data:\n                    lzmafile.write(data)\n                    data = srcfile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error creating %s: %s\" % (archive, err)\n        raise util.PatoolError(msg)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_lzma(archive, compression, cmd, verbosity, interactive, filenames):\n    return _create(archive, compression, cmd, 'alone', verbosity, filenames)", "response": "Create an LZMA archive with the lzma Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_xz(archive, compression, cmd, verbosity, interactive, filenames):\n    return _create(archive, compression, cmd, 'xz', verbosity, filenames)", "response": "Create an XZ archive with the lzma Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ssl_required(allow_non_ssl=False):\n    def wrapper(view_func):\n        def _checkssl(request, *args, **kwargs):\n            # allow_non_ssl=True lets non-https requests to come\n            # through to this view (and hence not redirect)\n            if hasattr(settings, 'SSL_ENABLED') and settings.SSL_ENABLED \\\n                    and not request.is_secure() and not allow_non_ssl:\n                return HttpResponseRedirect(\n                    request.build_absolute_uri().replace('http://', 'https://'))\n            return view_func(request, *args, **kwargs)\n\n        return _checkssl\n    return wrapper", "response": "Decorator to allow SSL requests through the current page."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef anonymous_required(view, redirect_to=None):\n    if redirect_to is None:\n        redirect_to = settings.LOGIN_REDIRECT_URL\n\n    @wraps(view)\n    def wrapper(request, *a, **k):\n        if request.user and request.user.is_authenticated():\n            return HttpResponseRedirect(redirect_to)\n        return view(request, *a, **k)\n    return wrapper", "response": "Decorator for views that allows anonymous users to access the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a generic variable from the session.", "response": "def generic_var(self, key, value=None):\n        \"\"\"\n        Stores generic variables in the session prepending it with _GENERIC_VAR_KEY_PREFIX.\n        \"\"\"\n        return self._get_or_set('{0}{1}'.format(self._GENERIC_VAR_KEY_PREFIX, key), value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nauthenticates the user and return the user object.", "response": "def authenticate(self, username=None, password=None, **kwargs):\n        \"\"\"\n        \"username\" being passed is really email address and being compared to as such.\n        \"\"\"\n        try:\n            user = User.objects.get(email=username)\n            if user.check_password(password):\n                return user\n        except (User.DoesNotExist, User.MultipleObjectsReturned):\n            logging.warning('Unsuccessful login attempt using username/email: {0}'.format(username))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a form field.", "response": "def render_form_field(parser, token):\n    \"\"\"\n    Usage is {% render_form_field form.field_name optional_help_text optional_css_classes %}\n\n    - optional_help_text and optional_css_classes are strings\n    - if optional_help_text is not given, then it is taken from form field object\n    \"\"\"\n    try:\n        help_text = None\n        css_classes = None\n\n        token_split = token.split_contents()\n        if len(token_split) == 4:\n            tag_name, form_field, help_text, css_classes = token.split_contents()\n        elif len(token_split) == 3:\n            tag_name, form_field, help_text = token.split_contents()\n        else:\n            tag_name, form_field = token.split_contents()\n    except ValueError:\n        raise template.TemplateSyntaxError(\n            \"Unable to parse arguments for {0}\".format(repr(token.contents.split()[0])))\n\n    return FormFieldNode(form_field, help_text=help_text, css_classes=css_classes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefine optional arguments with default values", "response": "def add_arguments(self, parser):\n        \"\"\"\n        Define optional arguments with default values\n        \"\"\"\n        parser.add_argument('--length', default=self.length,\n                            type=int, help=_('SECRET_KEY length default=%d' % self.length))\n\n        parser.add_argument('--alphabet', default=self.allowed_chars,\n                            type=str, help=_('alphabet to use default=%s' % self.allowed_chars))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_view(self, request, form_url='', extra_context=None):\n        model = self.model\n        opts = model._meta\n\n        if not self.has_add_permission(request):\n            raise PermissionDenied\n\n        ModelForm = self.get_form(request)\n        formsets = []\n\n        if request.method == 'POST':\n            form = ModelForm(request.POST, request.FILES)\n\n            if form.is_valid():\n                new_object = self.save_form(request, form, change=False)\n                form_validated = True\n            else:\n                form_validated = False\n                new_object = self.model()\n\n            prefixes = {}\n\n            for FormSet, inline in zip(self.get_formsets(request),\n                                       self.get_inline_instances(request)):\n                prefix = FormSet.get_default_prefix()\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n\n                if prefixes[prefix] != 1:\n                    prefix = \"{0}-{1}\".format(prefix, prefixes[prefix])\n\n                formset = FormSet(data=request.POST, files=request.FILES,\n                                  instance=new_object,\n                                  save_as_new=\"_saveasnew\" in request.POST,\n                                  prefix=prefix, queryset=inline.queryset(request))\n\n                formsets.append(formset)\n\n                for inline in self.get_inline_instances(request):\n                    # If this is the inline that matches this formset, and\n                    # we have some nested inlines to deal with, then we need\n                    # to get the relevant formset for each of the forms in\n                    # the current formset.\n                    if inline.inlines and inline.model == formset.model:\n                        for nested in inline.inline_instances:\n                            for the_form in formset.forms:\n                                InlineFormSet = nested.get_formset(request, the_form.instance)\n                                prefix = \"{0}-{1}\".format(the_form.prefix,\n                                                          InlineFormSet.get_default_prefix())\n                                formsets.append(InlineFormSet(request.POST, request.FILES,\n                                                              instance=the_form.instance,\n                                                              prefix=prefix))\n            if all_valid(formsets) and form_validated:\n                self.save_model(request, new_object, form, change=False)\n                form.save_m2m()\n\n                for formset in formsets:\n                    self.save_formset(request, form, formset, change=False)\n\n                self.log_addition(request, new_object)\n\n                return self.response_add(request, new_object)\n        else:\n            # Prepare the dict of initial data from the request.\n            # We have to special-case M2Ms as a list of comma-separated PKs.\n            initial = dict(request.GET.items())\n\n            for k in initial:\n                try:\n                    f = opts.get_field(k)\n                except models.FieldDoesNotExist:\n                    continue\n\n                if isinstance(f, models.ManyToManyField):\n                    initial[k] = initial[k].split(\",\")\n\n            form = ModelForm(initial=initial)\n            prefixes = {}\n\n            for FormSet, inline in zip(self.get_formsets(request),\n                                       self.get_inline_instances(request)):\n                prefix = FormSet.get_default_prefix()\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n\n                if prefixes[prefix] != 1:\n                    prefix = \"{0}-{1}\".format(prefix, prefixes[prefix])\n\n                formset = FormSet(instance=self.model(), prefix=prefix,\n                                  queryset=inline.queryset(request))\n                formsets.append(formset)\n\n        adminForm = helpers.AdminForm(form, list(self.get_fieldsets(request)),\n                                      self.prepopulated_fields, self.get_readonly_fields(request),\n                                      model_admin=self)\n\n        media = self.media + adminForm.media\n        inline_admin_formsets = []\n\n        for inline, formset in zip(self.get_inline_instances(request), formsets):\n            fieldsets = list(inline.get_fieldsets(request))\n            readonly = list(inline.get_readonly_fields(request))\n            inline_admin_formset = helpers.InlineAdminFormSet(inline, formset,\n                                                              fieldsets, readonly,\n                                                              model_admin=self)\n            if inline.inlines:\n                for form in formset.forms:\n                    if form.instance.pk:\n                        instance = form.instance\n                    else:\n                        instance = None\n\n                    form.inlines = inline.get_inlines(request, instance, prefix=form.prefix)\n\n                inline_admin_formset.inlines = inline.get_inlines(request)\n\n            inline_admin_formsets.append(inline_admin_formset)\n            media = media + inline_admin_formset.media\n\n        context = {\n            'title': _('Add %s') % force_unicode(opts.verbose_name),\n            'adminform': adminForm,\n            'is_popup': \"_popup\" in request.REQUEST,\n            'show_delete': False,\n            'media': mark_safe(media),\n            'inline_admin_formsets': inline_admin_formsets,\n            'errors': helpers.AdminErrorList(form, formsets),\n            'app_label': opts.app_label,\n        }\n\n        context.update(extra_context or {})\n\n        return self.render_change_form(request, context, form_url=form_url, add=True)", "response": "The add admin view for this model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_view(self, request, object_id, extra_context=None, **kwargs):\n        \"The 'change' admin view for this model.\"\n        model = self.model\n        opts = model._meta\n        obj = self.get_object(request, unquote(object_id))\n\n        if not self.has_change_permission(request, obj):\n            raise PermissionDenied\n\n        if obj is None:\n            raise Http404(_('%(name)s object with primary key %(key)r does not exist.') %\n                          {'name': force_unicode(opts.verbose_name), 'key': escape(object_id)})\n\n        if request.method == 'POST' and \"_saveasnew\" in request.POST:\n            return self.add_view(request, form_url='../add/')\n\n        ModelForm = self.get_form(request, obj)\n        formsets = []\n\n        if request.method == 'POST':\n            form = ModelForm(request.POST, request.FILES, instance=obj)\n\n            if form.is_valid():\n                form_validated = True\n                new_object = self.save_form(request, form, change=True)\n            else:\n                form_validated = False\n                new_object = obj\n\n            prefixes = {}\n\n            for FormSet, inline in zip(self.get_formsets(request, new_object),\n                                       self.get_inline_instances(request)):\n                prefix = FormSet.get_default_prefix()\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n\n                if prefixes[prefix] != 1:\n                    prefix = \"{0}-{1}\".format(prefix, prefixes[prefix])\n                formset = FormSet(request.POST, request.FILES,\n                                  instance=new_object, prefix=prefix,\n                                  queryset=inline.queryset(request))\n\n                formsets.append(formset)\n\n                for inline in self.get_inline_instances(request):\n                    # If this is the inline that matches this formset, and\n                    # we have some nested inlines to deal with, then we need\n                    # to get the relevant formset for each of the forms in\n                    # the current formset.\n                    if inline.inlines and inline.model == formset.model:\n                        for nested in inline.inline_instances:\n                            for the_form in formset.forms:\n                                InlineFormSet = nested.get_formset(request, the_form.instance)\n                                prefix = \"{0}-{1}\".format(the_form.prefix,\n                                                          InlineFormSet.get_default_prefix())\n                                formsets.append(InlineFormSet(request.POST, request.FILES,\n                                                              instance=the_form.instance,\n                                                              prefix=prefix))\n            if all_valid(formsets) and form_validated:\n                self.save_model(request, new_object, form, change=True)\n                form.save_m2m()\n\n                for formset in formsets:\n                    self.save_formset(request, form, formset, change=True)\n\n                change_message = self.construct_change_message(request, form, formsets)\n                self.log_change(request, new_object, change_message)\n\n                return self.response_change(request, new_object)\n\n        else:\n            form = ModelForm(instance=obj)\n            prefixes = {}\n\n            for FormSet, inline in zip(self.get_formsets(request, obj),\n                                       self.get_inline_instances(request)):\n                prefix = FormSet.get_default_prefix()\n                prefixes[prefix] = prefixes.get(prefix, 0) + 1\n                if prefixes[prefix] != 1:\n                    prefix = \"{0}-{1}\".format(prefix, prefixes[prefix])\n                formset = FormSet(instance=obj, prefix=prefix,\n                                  queryset=inline.queryset(request))\n                formsets.append(formset)\n\n        adminForm = helpers.AdminForm(form, self.get_fieldsets(request, obj),\n                                      self.prepopulated_fields,\n                                      self.get_readonly_fields(request, obj),\n                                      model_admin=self)\n        media = self.media + adminForm.media\n        inline_admin_formsets = []\n\n        for inline, formset in zip(self.get_inline_instances(request), formsets):\n            fieldsets = list(inline.get_fieldsets(request, obj))\n            readonly = list(inline.get_readonly_fields(request, obj))\n            inline_admin_formset = helpers.InlineAdminFormSet(inline, formset, fieldsets,\n                                                              readonly, model_admin=self)\n            if inline.inlines:\n                for form in formset.forms:\n                    if form.instance.pk:\n                        instance = form.instance\n                    else:\n                        instance = None\n\n                    form.inlines = inline.get_inlines(request, instance, prefix=form.prefix)\n\n                inline_admin_formset.inlines = inline.get_inlines(request)\n\n            inline_admin_formsets.append(inline_admin_formset)\n            media = media + inline_admin_formset.media\n\n        context = {\n            'title': _('Change %s') % force_unicode(opts.verbose_name),\n            'adminform': adminForm,\n            'object_id': object_id,\n            'original': obj,\n            'is_popup': \"_popup\" in request.REQUEST,\n            'media': mark_safe(media),\n            'inline_admin_formsets': inline_admin_formsets,\n            'errors': helpers.AdminErrorList(form, formsets),\n            'app_label': opts.app_label,\n        }\n\n        context.update(extra_context or {})\n\n        return self.render_change_form(request, context, change=True, obj=obj)", "response": "The change admin view for this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a BaseInlineFormSet class for use in admin add or change views.", "response": "def get_formset(self, request, obj=None, **kwargs):\n        \"\"\"\n        Returns a BaseInlineFormSet class for use in admin add/change views.\n        \"\"\"\n        if self.declared_fieldsets:\n            fields = flatten_fieldsets(self.declared_fieldsets)\n        else:\n            fields = None\n        if self.exclude is None:\n            exclude = []\n        else:\n            exclude = list(self.exclude)\n\n        exclude.extend(kwargs.get(\"exclude\", []))\n        exclude.extend(self.get_readonly_fields(request, obj))\n\n        # if exclude is an empty list we use None, since that's the actual\n        # default\n        exclude = exclude or None\n        defaults = {\n            \"form\": self.form,\n            \"formset\": self.formset,\n            \"fk_name\": self.fk_name,\n            \"fields\": fields,\n            \"exclude\": exclude,\n            \"formfield_callback\": curry(self.formfield_for_dbfield, request=request),\n            \"extra\": self.extra,\n            \"max_num\": self.max_num,\n            \"can_delete\": self.can_delete,\n        }\n        defaults.update(kwargs)\n\n        return inlineformset_factory(self.parent_model, self.model, **defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts our JSON object to a string before we save", "response": "def get_prep_value(self, value):\n        \"\"\"Convert our JSON object to a string before we save\"\"\"\n\n        if value == \"\":\n            return None\n\n        if isinstance(value, dict):\n            value = json.dumps(value, cls=DjangoJSONEncoder)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_among(value, *possibilities):\n    for possibility in possibilities:\n        if value == possibility:\n            return True\n    raise Exception('A different request value was encountered than expected: {0}'.format(value))", "response": "Ensure that the request value is one of the possibilities."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_mail(subject, message, from_email, recipient_emails, files=None,\n              html=False, reply_to=None, bcc=None, cc=None, files_manually=None):\n    \"\"\"\n    Sends email with advanced optional parameters\n\n    To attach non-file content (e.g. content not saved on disk), use\n    files_manually parameter and provide list of 3 element tuples, e.g.\n    [('design.png', img_data, 'image/png'),] which will be passed to\n    email.attach().\n    \"\"\"\n    import django.core.mail\n    try:\n        logging.debug('Sending mail to: {0}'.format(', '.join(r for r in recipient_emails)))\n        logging.debug('Message: {0}'.format(message))\n        email = django.core.mail.EmailMessage(subject, message, from_email, recipient_emails,\n                                              bcc, cc=cc)\n        if html:\n            email.content_subtype = \"html\"\n        if files:\n            for file in files:\n                email.attach_file(file)\n        if files_manually:\n            for filename, content, mimetype in files_manually:\n                email.attach(filename, content, mimetype)\n        if reply_to:\n            email.extra_headers = {'Reply-To': reply_to}\n        email.send()\n    except Exception as e:\n        # TODO:  Raise error again so that more information is included in the logs?\n        logging.error('Error sending message [{0}] from {1} to {2} {3}'.format(\n            subject, from_email, recipient_emails, e))", "response": "Sends an email to the specified list of recipients."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a fuzzy time since. Will only return the largest time. EX : 20 days 14 min Returns a fuzzy time since. EX : 14 days 14 min Returns a fuzzy time since. EX : 14 min Returns a fuzzy time since.", "response": "def humanize_time_since(timestamp=None):\n    \"\"\"\n    Returns a fuzzy time since. Will only return the largest time. EX: 20 days, 14 min\n    \"\"\"\n    timeDiff = datetime.datetime.now() - timestamp\n    days = timeDiff.days\n    hours = timeDiff.seconds / 3600\n    minutes = timeDiff.seconds % 3600 / 60\n    seconds = timeDiff.seconds % 3600 % 60\n\n    str = \"\"\n    if days > 0:\n        if days == 1:\n            t_str = \"day\"\n        else:\n            t_str = \"days\"\n        str += \"{0} {1}\".format(days, t_str)\n        return str\n    elif hours > 0:\n        if hours == 1:\n            t_str = \"hour\"\n        else:\n            t_str = \"hours\"\n        str += \"{0} {1}\".format(hours, t_str)\n        return str\n    elif minutes > 0:\n        if minutes == 1:\n            t_str = \"min\"\n        else:\n            t_str = \"mins\"\n        str += \"{0} {1}\".format(minutes, t_str)\n        return str\n    elif seconds > 0:\n        if seconds == 1:\n            t_str = \"sec\"\n        else:\n            t_str = \"secs\"\n        str += \"{0} {1}\".format(seconds, t_str)\n        return str\n    else:\n        return str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef skip_redundant(iterable, skipset=None):\n    if skipset is None:\n        skipset = set()\n    for item in iterable:\n        if item not in skipset:\n            skipset.add(item)\n            yield item", "response": "Yields items that are repeated or in the original skipset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the first confict - solving metaclass that is not needed for the given bases.", "response": "def get_noconflict_metaclass(bases, left_metas, right_metas):\n    \"\"\"\n    Not intended to be used outside of this module, unless you know what you are doing.\n    \"\"\"\n    # make tuple of needed metaclasses in specified priority order\n    metas = left_metas + tuple(map(type, bases)) + right_metas\n    needed_metas = remove_redundant(metas)\n\n    # return existing confict-solving meta, if any\n    if needed_metas in memoized_metaclasses_map:\n        return memoized_metaclasses_map[needed_metas]\n    # nope: compute, memoize and return needed conflict-solving meta\n    elif not needed_metas:         # wee, a trivial case, happy us\n        meta = type\n    elif len(needed_metas) == 1:  # another trivial case\n        meta = needed_metas[0]\n    # check for recursion, can happen i.e. for Zope ExtensionClasses\n    elif needed_metas == bases:\n        raise TypeError(\"Incompatible root metatypes\", needed_metas)\n    else:  # gotta work ...\n        metaname = '_' + ''.join([m.__name__ for m in needed_metas])\n        meta = classmaker()(metaname, needed_metas, {})\n    memoized_metaclasses_map[needed_metas] = meta\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_api_error_14(self, request):\n        request.method_params['captcha_key'] = self.get_captcha_key(request)\n        request.method_params['captcha_sid'] = request.api_error.captcha_sid\n\n        return self.send(request)", "response": "Called when API error is needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_api_error_15(self, request):\n        logger.error('Authorization failed. Access token will be dropped')\n        self.access_token = self.get_access_token()\n        return self.send(request)", "response": "Handle API error 15."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nauthorizes the user to the specified domain.", "response": "def authorize(self, auth_session):\n        \"\"\"\n        OAuth2\n        \"\"\"\n        # Ask access\n        ask_access_response = auth_session.post(self.AUTHORIZE_URL, self.get_auth_params())\n        url_queries = self.get_response_url_queries(ask_access_response)\n\n        if 'access_token' not in url_queries:\n            # Grant access\n            grant_access_action = self.get_form_action(ask_access_response)\n            grant_access_response = auth_session.post(grant_access_action)\n            url_queries = self.get_response_url_queries(grant_access_response)\n\n        return self.process_auth_url_queries(url_queries)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef talk_back(self, message):\n        quote = self.get_quote()\n        if quote:\n            self.reply(\"Actually, she said things like this: \\n%s\" % quote)", "response": "Tells you what she said."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_key():\n    import uuid\n    import time\n    import random\n    import hashlib\n\n    node = uuid.getnode()\n\n    h = hashlib.md5()\n    h.update(str(\"%s\" % node).encode('utf-8'))\n    key1 = h.hexdigest()\n\n    time.sleep(random.uniform(0, 0.5))\n    node = uuid.getnode()\n\n    h = hashlib.md5()\n    h.update(str(\"%s\" % node).encode('utf-8'))\n    key2 = h.hexdigest()\n\n    time.sleep(random.uniform(0, 0.5))\n    node = uuid.getnode()\n\n    h = hashlib.md5()\n    h.update(str(\"%s\" % node).encode('utf-8'))\n    key3 = h.hexdigest()\n\n    if key1 == key2 and key2 == key3:\n        return key1\n\n    return False", "response": "This method attempts to auto - generate a cryptographic key based on the hardware ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport all settings from the environment and config. py file.", "response": "def import_settings(quiet=True):\n    \"\"\"This method takes care of importing settings from the environment, and config.py file.\n\n    Order of operations:\n    1. Imports all WILL_ settings from the environment, and strips off the WILL_\n    2. Imports settings from config.py\n    3. Sets defaults for any missing, required settings.\n\n    This method takes a quiet kwarg, that when False, prints helpful output. Called that way during bootstrapping.\n    \"\"\"\n\n    settings = {}\n\n    # Import from environment, handle environment-specific parsing.\n    for k, v in os.environ.items():\n        if k[:5] == \"WILL_\":\n            k = k[5:]\n            settings[k] = v\n    if \"HIPCHAT_ROOMS\" in settings and type(settings[\"HIPCHAT_ROOMS\"]) is type(\"tes\"):\n        settings[\"HIPCHAT_ROOMS\"] = settings[\"HIPCHAT_ROOMS\"].split(\";\")\n\n    if \"ROOMS\" in settings:\n        settings[\"ROOMS\"] = settings[\"ROOMS\"].split(\";\")\n\n    if \"PLUGINS\" in settings:\n        settings[\"PLUGINS\"] = settings[\"PLUGINS\"].split(\";\")\n\n    if 'PLUGIN_BLACKLIST' in settings:\n        settings[\"PLUGIN_BLACKLIST\"] = (settings[\"PLUGIN_BLACKLIST\"].split(\";\")\n                                        if settings[\"PLUGIN_BLACKLIST\"] else [])\n\n    # If HIPCHAT_SERVER is set, we need to change the USERNAME slightly\n    # for XMPP to work.\n    if \"HIPCHAT_SERVER\" in settings:\n        settings[\"USERNAME\"] = \"{user}@{host}\".\\\n            format(user=settings[\"USERNAME\"].split(\"@\")[0],\n                   host=settings[\"HIPCHAT_SERVER\"])\n    else:\n        settings[\"HIPCHAT_SERVER\"] = \"api.hipchat.com\"\n\n    # Import from config\n    if not quiet:\n        puts(\"Importing config.py... \")\n    with indent(2):\n        try:\n            had_warning = False\n            try:\n                import config\n            except ImportError:\n                # Missing config.py.  Check for config.py.dist\n                if os.path.isfile(\"config.py.dist\"):\n                    confirm = input(\n                        \"Hi, looks like you're just starting up!\\nI didn't find a config.py, but I do see config.py.dist here. Want me to use that? (y/n) \"\n                    ).lower()\n                    if confirm in [\"y\", \"yes\"]:\n                        print(\"Great! One moment.\\n\\n\")\n                        os.rename(\"config.py.dist\", \"config.py\")\n                        import config\n                    else:\n                        print(\"Ok.  I can't start without one though. Quitting now!\")\n                        sys.exit(1)\n                else:\n                    error(\"I'm missing my config.py file. Usually one comes with the installation - maybe it got lost?\")\n                    sys.exit(1)\n\n            for k, v in config.__dict__.items():\n                # Ignore private variables\n                if \"__\" not in k:\n                    if k in os.environ and v != os.environ[k] and not quiet:\n                        warn(\"%s is set in the environment as '%s', but overridden in\"\n                             \" config.py as '%s'.\" % (k, os.environ[k], v))\n                        had_warning = True\n                    settings[k] = v\n\n            if not had_warning and not quiet:\n                show_valid(\"Valid.\")\n        except:\n            # TODO: Check to see if there's a config.py.dist\n            if not quiet:\n                warn(\"no config.py found.  This might be ok, but more likely, \"\n                     \"you haven't copied config.py.dist over to config.py\")\n\n    if not quiet:\n        puts(\"Verifying settings... \")\n\n    with indent(2):\n        # Deprecation and backwards-compatibility for Will 1.x-> 2.x\n        DEPRECATED_BUT_MAPPED_SETTINGS = {\n            \"USERNAME\": \"HIPCHAT_USERNAME\",\n            \"PASSWORD\": \"HIPCHAT_PASSWORD\",\n            \"V1_TOKEN\": \"HIPCHAT_V1_TOKEN\",\n            \"V2_TOKEN\": \"HIPCHAT_V2_TOKEN\",\n            \"TOKEN\": \"HIPCHAT_V1_TOKEN\",\n            \"ROOMS\": \"HIPCHAT_ROOMS\",\n            \"NAME\": \"HIPCHAT_NAME\",\n            \"HANDLE\": \"HIPCHAT_HANDLE\",\n            \"DEFAULT_ROOM\": \"HIPCHAT_DEFAULT_ROOM\",\n            \"SLACK_DEFAULT_ROOM\": \"SLACK_DEFAULT_CHANNEL\",\n        }\n        deprecation_warn_shown = False\n        for k, v in DEPRECATED_BUT_MAPPED_SETTINGS.items():\n            if not v in settings and k in settings:\n                if not deprecation_warn_shown and not quiet:\n                    error(\"Deprecated settings. The following settings will stop working in Will 2.2:\")\n                    deprecation_warn_shown = True\n                if not quiet:\n                    warn(\"Please update %s to %s.  \" % (k, v))\n                settings[v] = settings[k]\n                del settings[k]\n\n        # Migrate from 1.x\n        if \"CHAT_BACKENDS\" in settings and \"IO_BACKENDS\" not in settings:\n            IO_BACKENDS = []\n            for c in settings[\"CHAT_BACKENDS\"]:\n                IO_BACKENDS.append(\"will.backends.io_adapters.%s\" % c)\n            settings[\"IO_BACKENDS\"] = IO_BACKENDS\n            if not quiet:\n                warn(\n                    \"Deprecated settings.  Please update your config.py from:\"\n                    \"\\n   CHAT_BACKENDS = %s\\n   to\\n   IO_BACKENDS = %s\" %\n                    (settings[\"CHAT_BACKENDS\"], IO_BACKENDS)\n                )\n        if \"CHAT_BACKENDS\" not in settings and \"IO_BACKENDS\" not in settings:\n            if not quiet:\n                warn(\"\"\"Deprecated settings.  No backend found, so we're defaulting to hipchat and shell only.\nPlease add this to your config.py:\nIO_BACKENDS = \"\n    \"will.backends.io_adapters.hipchat\",\n    \"will.backends.io_adapters.shell\",\n#   \"will.backends.io_adapters.slack\",\n#   \"will.backends.io_adapters.rocketchat\",\n]\n\"\"\")\n            settings[\"IO_BACKENDS\"] = [\n                \"will.backends.io_adapters.hipchat\",\n                \"will.backends.io_adapters.shell\",\n            ]\n\n        if \"ANALYZE_BACKENDS\" not in settings:\n            if not quiet:\n                note(\"No ANALYZE_BACKENDS specified.  Defaulting to history only.\")\n            settings[\"ANALYZE_BACKENDS\"] = [\n                \"will.backends.analysis.nothing\",\n                \"will.backends.analysis.history\",\n            ]\n\n        if \"GENERATION_BACKENDS\" not in settings:\n            if not quiet:\n                note(\"No GENERATION_BACKENDS specified.  Defaulting to fuzzy_all_matches and strict_regex.\")\n            settings[\"GENERATION_BACKENDS\"] = [\n                \"will.backends.generation.fuzzy_all_matches\",\n                \"will.backends.generation.strict_regex\",\n            ]\n\n        if \"EXECUTION_BACKENDS\" not in settings:\n            if not quiet:\n                note(\"No EXECUTION_BACKENDS specified.  Defaulting to best_score.\")\n            settings[\"EXECUTION_BACKENDS\"] = [\n                \"will.backends.execution.best_score\",\n            ]\n\n        # Set for hipchat\n        for b in settings[\"IO_BACKENDS\"]:\n            if \"hipchat\" in b:\n                if \"ALLOW_INSECURE_HIPCHAT_SERVER\" in settings \\\n                        and (\n                        settings[\"ALLOW_INSECURE_HIPCHAT_SERVER\"] is True\n                        or settings[\"ALLOW_INSECURE_HIPCHAT_SERVER\"].lower() == \"true\"\n                        ):\n                    warn(\"You are choosing to run will with SSL disabled. \"\n                         \"This is INSECURE and should NEVER be deployed outside a development environment.\")\n                    settings[\"ALLOW_INSECURE_HIPCHAT_SERVER\"] = True\n                    settings[\"REQUESTS_OPTIONS\"] = {\n                        \"verify\": False,\n                    }\n                else:\n                    settings[\"ALLOW_INSECURE_HIPCHAT_SERVER\"] = False\n\n                if \"HIPCHAT_ROOMS\" not in settings:\n                    if not quiet:\n                        warn(\"no HIPCHAT_ROOMS list found in the environment or config.  \"\n                             \"This is ok - Will will just join all available HIPCHAT_rooms.\")\n                        settings[\"HIPCHAT_ROOMS\"] = None\n\n                if (\n                    \"HIPCHAT_DEFAULT_ROOM\" not in settings and \"HIPCHAT_ROOMS\" in settings\n                    and settings[\"HIPCHAT_ROOMS\"] and len(settings[\"HIPCHAT_ROOMS\"]) > 0\n                ):\n                    if not quiet:\n                        warn(\"no HIPCHAT_DEFAULT_ROOM found in the environment or config.  \"\n                             \"Defaulting to '%s', the first one.\" % settings[\"HIPCHAT_ROOMS\"][0])\n                    settings[\"HIPCHAT_DEFAULT_ROOM\"] = settings[\"HIPCHAT_ROOMS\"][0]\n\n            if \"HIPCHAT_HANDLE\" in settings and \"HIPCHAT_HANDLE_NOTED\" not in settings:\n                if not quiet:\n                    note(\n                        \"\"\"HIPCHAT_HANDLE is no longer required (or used), as Will knows how to get\\n\n                                his current handle from the HipChat servers.\"\"\"\n                    )\n                    settings[\"HIPCHAT_HANDLE_NOTED\"] = True\n\n            if \"HIPCHAT_NAME\" in settings and \"HIPCHAT_NAME_NOTED\" not in settings:\n                if not quiet:\n                    note(\n                        \"\"\"HIPCHAT_NAME is no longer required (or used), as Will knows how to get\\n\n                                his current name from the HipChat servers.\"\"\"\n                    )\n                    settings[\"HIPCHAT_NAME_NOTED\"] = True\n\n        # Rocket.chat\n        for b in settings[\"IO_BACKENDS\"]:\n            if \"rocketchat\" in b:\n                if \"ROCKETCHAT_USERNAME\" in settings and \"ROCKETCHAT_EMAIL\" not in settings:\n                    settings[\"ROCKETCHAT_EMAIL\"] = settings[\"ROCKETCHAT_USERNAME\"]\n                if \"ROCKETCHAT_URL\" in settings:\n                    if settings[\"ROCKETCHAT_URL\"].endswith(\"/\"):\n                        settings[\"ROCKETCHAT_URL\"] = settings[\"ROCKETCHAT_URL\"][:-1]\n\n        if (\n            \"DEFAULT_BACKEND\" not in settings and \"IO_BACKENDS\" in settings\n            and settings[\"IO_BACKENDS\"] and len(settings[\"IO_BACKENDS\"]) > 0\n        ):\n            if not quiet:\n                note(\"no DEFAULT_BACKEND found in the environment or config.\\n  \"\n                     \"      Defaulting to '%s', the first one.\" % settings[\"IO_BACKENDS\"][0])\n            settings[\"DEFAULT_BACKEND\"] = settings[\"IO_BACKENDS\"][0]\n\n        for b in settings[\"IO_BACKENDS\"]:\n            if \"slack\" in b and \"SLACK_DEFAULT_CHANNEL\" not in settings and not quiet:\n                warn(\n                    \"No SLACK_DEFAULT_CHANNEL set - any messages sent without an explicit channel will go \"\n                    \"to a non-deterministic channel that will has access to \"\n                    \"- this is almost certainly not what you want.\"\n                )\n\n        if \"HTTPSERVER_PORT\" not in settings:\n            # For heroku\n            if \"PORT\" in os.environ:\n                settings[\"HTTPSERVER_PORT\"] = os.environ[\"PORT\"]\n            else:\n                if not quiet:\n                    warn(\"no HTTPSERVER_PORT found in the environment or config.  Defaulting to ':80'.\")\n                settings[\"HTTPSERVER_PORT\"] = \"80\"\n\n        if \"STORAGE_BACKEND\" not in settings:\n            if not quiet:\n                warn(\"No STORAGE_BACKEND specified.  Defaulting to redis.\")\n            settings[\"STORAGE_BACKEND\"] = \"redis\"\n\n        if \"PUBSUB_BACKEND\" not in settings:\n            if not quiet:\n                warn(\"No PUBSUB_BACKEND specified.  Defaulting to redis.\")\n            settings[\"PUBSUB_BACKEND\"] = \"redis\"\n\n        if settings[\"STORAGE_BACKEND\"] == \"redis\" or settings[\"PUBSUB_BACKEND\"] == \"redis\":\n            if \"REDIS_URL\" not in settings:\n                # For heroku\n                if \"REDIS_URL\" in os.environ:\n                    settings[\"REDIS_URL\"] = os.environ[\"REDIS_URL\"]\n                    if not quiet:\n                        note(\"WILL_REDIS_URL not set, but it appears you're using Heroku Redis or another standard REDIS_URL. If so, all good.\")\n                if \"REDISCLOUD_URL\" in os.environ:\n                    settings[\"REDIS_URL\"] = os.environ[\"REDISCLOUD_URL\"]\n                    if not quiet:\n                        note(\"WILL_REDIS_URL not set, but it appears you're using RedisCloud. If so, all good.\")\n                elif \"REDISTOGO_URL\" in os.environ:\n                    settings[\"REDIS_URL\"] = os.environ[\"REDISTOGO_URL\"]\n                    if not quiet:\n                        note(\"WILL_REDIS_URL not set, but it appears you're using RedisToGo. If so, all good.\")\n                elif \"OPENREDIS_URL\" in os.environ:\n                    settings[\"REDIS_URL\"] = os.environ[\"OPENREDIS_URL\"]\n                    if not quiet:\n                        note(\"WILL_REDIS_URL not set, but it appears you're using OpenRedis. If so, all good.\")\n                else:\n                    settings[\"REDIS_URL\"] = \"redis://localhost:6379/7\"\n                    if not quiet:\n                        note(\"WILL_REDIS_URL not set.  Defaulting to redis://localhost:6379/7.\")\n\n            if not settings[\"REDIS_URL\"].startswith(\"redis://\"):\n                settings[\"REDIS_URL\"] = \"redis://%s\" % settings[\"REDIS_URL\"]\n\n            if \"REDIS_MAX_CONNECTIONS\" not in settings or not settings[\"REDIS_MAX_CONNECTIONS\"]:\n                settings[\"REDIS_MAX_CONNECTIONS\"] = 4\n                if not quiet:\n                    note(\"REDIS_MAX_CONNECTIONS not set. Defaulting to 4.\")\n\n        if settings[\"STORAGE_BACKEND\"] == \"file\":\n            if \"FILE_DIR\" not in settings:\n                settings[\"FILE_DIR\"] = \"~/.will/\"\n                if not quiet:\n                    note(\"FILE_DIR not set.  Defaulting to ~/.will/\")\n\n        if settings[\"STORAGE_BACKEND\"] == \"couchbase\":\n            if \"COUCHBASE_URL\" not in settings:\n                settings[\"COUCHBASE_URL\"] = \"couchbase:///will\"\n                if not quiet:\n                    note(\"COUCHBASE_URL not set.  Defaulting to couchbase:///will\")\n\n        if \"PUBLIC_URL\" not in settings:\n            default_public = \"http://localhost:%s\" % settings[\"HTTPSERVER_PORT\"]\n            settings[\"PUBLIC_URL\"] = default_public\n            if not quiet:\n                note(\"no PUBLIC_URL found in the environment or config.\\n        Defaulting to '%s'.\" % default_public)\n\n        if not \"REQUESTS_OPTIONS\" in settings:\n            settings[\"REQUESTS_OPTIONS\"] = {}\n\n        if \"TEMPLATE_DIRS\" not in settings:\n            if \"WILL_TEMPLATE_DIRS_PICKLED\" in os.environ:\n                # All good\n                pass\n            else:\n                settings[\"TEMPLATE_DIRS\"] = []\n\n        if \"WILL_HANDLE\" not in settings:\n            if \"HANDLE\" in settings:\n                settings[\"WILL_HANDLE\"] = settings[\"HANDLE\"]\n            elif \"SLACK_HANDLE\" in settings:\n                settings[\"WILL_HANDLE\"] = settings[\"SLACK_HANDLE\"]\n            elif \"HIPCHAT_HANDLE\" in settings:\n                settings[\"WILL_HANDLE\"] = settings[\"HIPCHAT_HANDLE\"]\n            elif \"ROCKETCHAT_HANDLE\" in settings:\n                settings[\"WILL_HANDLE\"] = settings[\"ROCKETCHAT_HANDLE\"]\n            else:\n                settings[\"WILL_HANDLE\"] = \"will\"\n\n        if \"ADMINS\" not in settings:\n            settings[\"ADMINS\"] = \"*\"\n        else:\n            if \"WILL_ADMINS\" in os.environ:\n                settings[\"ADMINS\"] = [a.strip().lower() for a in settings.get('ADMINS', '').split(';') if a.strip()]\n\n        if \"ADMINS\" in settings and settings[\"ADMINS\"] != \"*\":\n            warn(\"ADMINS is now deprecated, and will be removed at the end of 2017.  Please use ACL instead. See below for details\")\n            note(\"Change your config.py to:\\n  ACL = {\\n     'admins': %s\\n  }\" % settings[\"ADMINS\"])\n\n        if \"DISABLE_ACL\" not in settings:\n            settings[\"DISABLE_ACL\"] = False\n\n        if \"PROXY_URL\" in settings:\n            parsed_proxy_url = parse.urlparse(settings[\"PROXY_URL\"])\n            settings[\"USE_PROXY\"] = True\n            settings[\"PROXY_HOSTNAME\"] = parsed_proxy_url.hostname\n            settings[\"PROXY_USERNAME\"] = parsed_proxy_url.username\n            settings[\"PROXY_PASSWORD\"] = parsed_proxy_url.password\n            settings[\"PROXY_PORT\"] = parsed_proxy_url.port\n        else:\n            settings[\"USE_PROXY\"] = False\n\n        if \"EVENT_LOOP_INTERVAL\" not in settings:\n            settings[\"EVENT_LOOP_INTERVAL\"] = 0.025\n\n        if \"LOGLEVEL\" not in settings:\n            settings[\"LOGLEVEL\"] = \"ERROR\"\n\n        if \"ENABLE_INTERNAL_ENCRYPTION\" not in settings:\n            settings[\"ENABLE_INTERNAL_ENCRYPTION\"] = True\n\n        if \"SECRET_KEY\" not in settings:\n            if not quiet:\n                if \"ENABLE_INTERNAL_ENCRYPTION\" in settings and settings[\"ENABLE_INTERNAL_ENCRYPTION\"]:\n                    key = auto_key()\n                    if key:\n                        warn(\n                            \"\"\"No SECRET_KEY specified and ENABLE_INTERNAL_ENCRYPTION is on.\\n\n                               Temporarily auto-generating a key specific to this computer:\\n    {}\\n\n                               Please set WILL_SECRET_KEY in the environment as soon as possible to ensure \\n\n                               Will is able to access information from previous runs.\"\"\".format(key)\n                        )\n                    else:\n                        error(\n                            \"\"\"ENABLE_INTERNAL_ENCRYPTION is turned on, but a SECRET_KEY has not been given.\\n\n                               We tried to automatically generate temporary SECRET_KEY, but this appears to be a \\n\"\n                               shared or virtualized environment.\\n Please set a unique secret key in the\n                               environment as WILL_SECRET_KEY to run will.\"\"\"\n                        )\n                        print(\"  Unable to start will without a SECRET_KEY while encryption is turned on. Shutting down.\")\n                        sys.exit(1)\n\n                    settings[\"SECRET_KEY\"] = key\n                    os.environ[\"WILL_SECRET_KEY\"] = settings[\"SECRET_KEY\"]\n                    os.environ[\"WILL_EPHEMERAL_SECRET_KEY\"] = \"True\"\n\n        if \"FUZZY_MINIMUM_MATCH_CONFIDENCE\" not in settings:\n            settings[\"FUZZY_MINIMUM_MATCH_CONFIDENCE\"] = 91\n        if \"FUZZY_REGEX_ALLOWABLE_ERRORS\" not in settings:\n            settings[\"FUZZY_REGEX_ALLOWABLE_ERRORS\"] = 3\n\n        # Set them in the module namespace\n        for k in sorted(settings, key=lambda x: x[0]):\n            if not quiet:\n                show_valid(k)\n            globals()[k] = settings[k]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the following structure: /plugins __init__.py hello.py /templates blank.html .gitignore run_will.py requirements.txt Procfile README.md", "response": "def main():\n    \"\"\"\n    Creates the following structure:\n    /plugins\n        __init__.py\n        hello.py\n    /templates\n        blank.html\n    .gitignore\n    run_will.py\n    requirements.txt\n    Procfile\n    README.md\n    \"\"\"\n\n    print_head()\n    puts(\"Welcome to the will project generator.\")\n    puts(\"\")\n\n    if args.config_dist_only:\n        print(\"Generating config.py.dist...\")\n\n    else:\n        print(\"\\nGenerating will scaffold...\")\n\n    current_dir = os.getcwd()\n    plugins_dir = os.path.join(current_dir, \"plugins\")\n    templates_dir = os.path.join(current_dir, \"templates\")\n\n    if not args.config_dist_only:\n        print(\"  /plugins\")\n        # Set up the directories\n        if not os.path.exists(plugins_dir):\n            os.makedirs(plugins_dir)\n\n        print(\"     __init__.py\")\n        # Create the plugins __init__.py\n        with open(os.path.join(plugins_dir, \"__init__.py\"), 'w+') as f:\n            pass\n\n        print(\"     morning.py\")\n        # Create the morning plugin\n        morning_file_path = os.path.join(plugins_dir, \"morning.py\")\n        if not os.path.exists(morning_file_path):\n            with open(morning_file_path, 'w+') as f:\n                f.write(\"\"\"from will.plugin import WillPlugin\nfrom will.decorators import respond_to, periodic, hear, randomly, route, rendered_template, require_settings\n\n\nclass MorningPlugin(WillPlugin):\n\n    @respond_to(\"^good morning\")\n    def good_morning(self, message):\n        self.reply(\"oh, g'morning!\")\n\"\"\")\n\n        print(\"  /templates\")\n        if not os.path.exists(templates_dir):\n            os.makedirs(templates_dir)\n\n        print(\"     blank.html\")\n        # Create the plugins __init__.py\n        with open(os.path.join(templates_dir, \"blank.html\"), 'w+') as f:\n            pass\n\n        print(\"  .gitignore\")\n        # Create .gitignore, or at least add shelf.db\n        gitignore_path = os.path.join(current_dir, \".gitignore\")\n        if not os.path.exists(gitignore_path):\n            with open(gitignore_path, 'w+') as f:\n                f.write(\"\"\"*.py[cod]\npip-log.txt\nshelf.db\n        \"\"\")\n        else:\n            append_ignore = False\n            with open(gitignore_path, \"r+\") as f:\n                if \"shelf.db\" not in f.read():\n                    append_ignore = True\n            if append_ignore:\n                with open(gitignore_path, \"a\") as f:\n                    f.write(\"\\nshelf.db\\n\")\n\n        # Create run_will.py\n        print(\"  run_will.py\")\n        run_will_path = os.path.join(current_dir, \"run_will.py\")\n        if not os.path.exists(run_will_path):\n            with open(run_will_path, 'w+') as f:\n                f.write(\"\"\"#!/usr/bin/env python\nfrom will.main import WillBot\n\nif __name__ == '__main__':\n    bot = WillBot()\n    bot.bootstrap()\n\"\"\")\n        # And make it executable\n        st = os.stat('run_will.py')\n        os.chmod(\"run_will.py\", st.st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n\n    # Create config.py\n        print(\"  config.py.dist\")\n\n    config_path = os.path.join(current_dir, \"config.py.dist\")\n    if not os.path.exists(config_path) or ask_user(\"! config.py.dist exists.  Overwrite it?\"):\n        with open(os.path.join(PROJECT_ROOT, \"config.py.dist\"), \"r\") as source_f:\n            source = source_f.read()\n            if args.backends:\n                for backend in SERVICE_BACKENDS:\n                    if backend in args.backends:\n                        _enable_service(backend, source)\n                    else:\n                        __disable_service(backend, source)\n            else:\n                # Ask user thru cmd line what backends to enable\n                print(\"\\nWill supports a few different service backends.  Let's set up the ones you want:\\n\")\n                source = enable_disable_service(\"Slack\", source)\n                source = enable_disable_service(\"HipChat\", source)\n                source = enable_disable_service(\"Rocket.Chat\", source)\n                source = enable_disable_service(\"Shell\", source)\n\n            with open(config_path, \"w+\") as f:\n                config = source\n                f.write(config)\n\n    if not args.config_dist_only:\n        print(\"  requirements.txt\")\n        # Create requirements.txt\n        requirements_path = os.path.join(current_dir, \"requirements.txt\")\n        if not os.path.exists(requirements_path) or ask_user(\"! requirements.txt exists.  Overwrite it?\"):\n            with open(requirements_path, 'w+') as f:\n                f.write(requirements_txt)\n\n        print(\"  Procfile\")\n        # Create Procfile\n        requirements_path = os.path.join(current_dir, \"Procfile\")\n        if not os.path.exists(requirements_path):\n            with open(requirements_path, 'w+') as f:\n                f.write(\"web: python run_will.py\")\n\n        print(\"  README.md\")\n        # Create the readme\n        readme_path = os.path.join(current_dir, \"README.md\")\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w+') as f:\n                f.write(\"\"\"\nThis is our bot, a [will](https://github.com/skoczen/will) bot.\n    \"\"\")\n\n        print(\"\\nDone.\")\n\n        print(\"\\n Your will is now ready to go. Run ./run_will.py to get started!\")\n    else:\n        print(\"\\nCreated a config.py.dist.  Open it up to see what's new!\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimaging me ___ : Search google images for ___, and post a random one.", "response": "def image_me(self, message, search_query):\n        \"\"\"image me ___ : Search google images for ___, and post a random one.\"\"\"\n\n        if (\n                getattr(settings, \"GOOGLE_API_KEY\", False)\n                and getattr(settings, \"GOOGLE_CUSTOM_SEARCH_ENGINE_ID\", False)\n        ):\n            self.say(\n                \"Sorry, I'm missing my GOOGLE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID.\"\n                \" Can someone give them to me?\", color=\"red\"\n            )\n            # https://developers.google.com/custom-search/json-api/v1/reference/cse/list?hl=en\n            data = {\n                \"q\": search_query,\n                \"key\": settings.GOOGLE_API_KEY,\n                \"cx\": settings.GOOGLE_CUSTOM_SEARCH_ENGINE_ID,\n                \"safe\": \"medium\",\n                \"num\": 8,\n                \"searchType\": \"image\",\n            }\n            r = requests.get(\"https://www.googleapis.com/customsearch/v1\", params=data)\n            r.raise_for_status()\n            try:\n                response = r.json()\n                results = [result[\"link\"] for result in response[\"items\"] if \"items\" in r.json()]\n            except TypeError:\n                results = []\n        else:\n            # Fall back to a really ugly hack.\n            logging.warn(\n                \"Hey, I'm using a pretty ugly hack to get those images, and it might break. \"\n                \"Please set my GOOGLE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID when you have a chance.\"\n            )\n            r = requests.get(\"https://www.google.com/search?tbm=isch&safe=active&q=%s\" % search_query)\n            results = []\n            content = r.content.decode(\"utf-8\")\n            index = content.find(\"<img\")\n            while index != -1:\n                src_start = content.find('src=', index)\n                src_end = content.find(\" \", src_start)\n                match = content[src_start+5: src_end-1]\n\n                index = content.find(\"<img\", src_end)\n                results.append(match)\n        if results:\n            url = random.choice(results)\n            self.say(\"%s\" % url, message=message)\n        else:\n            self.say(\"Couldn't find anything!\", message=message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef say_bitly_short_url(self, message, long_url=None):\n\n        try:\n            import bitly_api   # pip install bitly_api\n        except ImportError:\n            raise ImportError(\n                \"Can't load BitlyPlugin, since the bitly_api python module isn't installed.\\n\"\n                \"To install it, run:\\n\"\n                \"  pip install bitly_api\"\n            )\n\n        # use oauth2 endpoints\n        c = bitly_api.Connection(access_token=settings.BITLY_ACCESS_TOKEN)\n        response = c.shorten(uri=long_url)\n        short_url = response['url']\n        self.say(\"Shorten URL: %s\" % short_url, message=message)", "response": "Shorten the URL using bitly service."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a reminder for a thing at a time.", "response": "def remind_me_at(self, message, reminder_text=None, remind_time=None, to_string=\"\"):\n        \"\"\"remind me to ___ at ___: Set a reminder for a thing, at a time.\"\"\"\n        parsed_time = self.parse_natural_time(remind_time)\n        natural_datetime = self.to_natural_day_and_time(parsed_time)\n        if to_string:\n            formatted_to_string = to_string\n        else:\n            formatted_to_string = \"\"\n        formatted_reminder_text = \"%(mention_handle)s, you asked me to remind you%(to_string)s %(reminder_text)s\" % {\n            \"mention_handle\": message.sender.mention_handle,\n            \"from_handle\": message.sender.handle,\n            \"reminder_text\": reminder_text,\n            \"to_string\": formatted_to_string,\n        }\n        self.schedule_say(formatted_reminder_text, parsed_time, message=message, notify=True)\n        self.say(\"%(reminder_text)s %(natural_datetime)s. Got it.\" % locals(), message=message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publish(self, topic, obj, reference_message=None):\n        logging.debug(\"Publishing topic (%s): \\n%s\" % (topic, obj))\n        e = Event(\n            data=obj,\n            type=topic,\n        )\n        if hasattr(obj, \"sender\"):\n            e.sender = obj.sender\n\n        if reference_message:\n            original_incoming_event_hash = None\n            if hasattr(reference_message, \"original_incoming_event_hash\"):\n                original_incoming_event_hash = reference_message.original_incoming_event_hash\n            elif hasattr(reference_message, \"source\") and hasattr(reference_message.source, \"hash\"):\n                original_incoming_event_hash = reference_message.source.hash\n            elif hasattr(reference_message, \"source\") and hasattr(reference_message.source, \"original_incoming_event_hash\"):\n                original_incoming_event_hash = reference_message.source.original_incoming_event_hash\n            elif hasattr(reference_message, \"hash\"):\n                original_incoming_event_hash = reference_message.hash\n            if original_incoming_event_hash:\n                e.original_incoming_event_hash = original_incoming_event_hash\n\n        return self.publish_to_backend(\n            self._localize_topic(topic),\n            self.encrypt(e)\n        )", "response": "Publishes an object to the pubsub backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_message(self):\n        try:\n            m = self.get_from_backend()\n            if m and m[\"type\"] not in SKIP_TYPES:\n                return self.decrypt(m[\"data\"])\n\n        except AttributeError:\n            raise Exception(\"Tried to call get message without having subscribed first!\")\n        except (KeyboardInterrupt, SystemExit):\n            pass\n        except:\n            logging.critical(\"Error in watching pubsub get message: \\n%s\" % traceback.format_exc())\n        return None", "response": "Gets the latest message from the backend and handles unpickling\n        and validation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplay a word game : Play a game where you think of words that start with a letter and fit a topic.", "response": "def word_game_round(self, message):\n        \"play a word game: Play a game where you think of words that start with a letter and fit a topic.\"\n\n        letter = random.choice(string.ascii_uppercase)\n        topics = []\n\n        while len(topics) < 10:\n            new_topic = random.choice(WORD_GAME_TOPICS)\n            if new_topic not in topics:\n                topics.append({\n                    \"index\": len(topics) + 1,\n                    \"topic\": new_topic\n                })\n\n        context = {\n            \"letter\": letter,\n            \"topics\": topics\n        }\n        self.say(rendered_template(\"word_game.html\", context), message=message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_my_info(self, message, contact_info=\"\"):\n        contacts = self.load(\"contact_info\", {})\n        contacts[message.sender.handle] = {\n            \"info\": contact_info,\n            \"name\": message.sender.name,\n        }\n        self.save(\"contact_info\", contacts)\n        self.say(\"Got it.\", message=message)", "response": "set my contact info to ____"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef respond_to_contact_info(self, message):\n        contacts = self.load(\"contact_info\", {})\n        context = {\n            \"contacts\": contacts,\n        }\n        contact_html = rendered_template(\"contact_info.html\", context)\n        self.say(contact_html, message=message)", "response": "contact info: Show everyone s emergency contact info."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef give_us_somethin_to_talk_about(self, message):\n        r = requests.get(\"http://www.chatoms.com/chatom.json?Normal=1&Fun=2&Philosophy=3&Out+There=4\")\n        data = r.json()\n        self.set_topic(data[\"text\"], message=message)", "response": "new topic : set the room topic to a random conversation starter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef participants_in_room(self, message):\n        room = self.get_room_from_message(message)\n        context = {\"participants\": room.participants, }\n        self.say(rendered_template(\"participants.html\", context), message=message, html=True)", "response": "who is in this room? : List all the participants of this room."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef help(self, message, plugin=None):\n        # help_data = self.load(\"help_files\")\n        selected_modules = help_modules = self.load(\"help_modules\")\n\n        self.say(\"Sure thing, %s.\" % message.sender.handle)\n\n        help_text = \"Here's what I know how to do:\"\n        if plugin and plugin in help_modules:\n            help_text = \"Here's what I know how to do about %s:\" % plugin\n            selected_modules = dict()\n            selected_modules[plugin] = help_modules[plugin]\n\n        for k in sorted(selected_modules, key=lambda x: x[0]):\n            help_data = selected_modules[k]\n            if help_data:\n                help_text += \"<br/><br/><b>%s</b>:\" % k\n                for line in help_data:\n                    if line:\n                        if \":\" in line:\n                            line = \"&nbsp; <b>%s</b>%s\" % (line[:line.find(\":\")], line[line.find(\":\"):])\n                        help_text += \"<br/> %s\" % line\n\n        self.say(help_text, html=True)", "response": "help is the normal help you re reading."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef help(self, message):\n        all_regexes = self.load(\"all_listener_regexes\")\n        help_text = \"Here's everything I know how to listen to:\"\n        for r in all_regexes:\n            help_text += \"\\n%s\" % r\n\n        self.say(help_text, message=message)", "response": "Advanced programmer - y help"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a google poem about __", "response": "def google_poem(self, message, topic):\n        \"\"\"make a poem about __: show a google poem about __\"\"\"\n        r = requests.get(\"http://www.google.com/complete/search?output=toolbar&q=\" + topic + \"%20\")\n        xmldoc = minidom.parseString(r.text)\n        item_list = xmldoc.getElementsByTagName(\"suggestion\")\n        context = {\"topic\": topic, \"lines\": [x.attributes[\"data\"].value for x in item_list[:4]]}\n        self.say(rendered_template(\"gpoem.html\", context), message, html=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_edgerc(rcinput, section='default'):\n        from .edgerc import EdgeRc \n        if isinstance(rcinput, EdgeRc):\n            rc = rcinput\n        else:\n            rc = EdgeRc(rcinput)\n\n        return EdgeGridAuth(\n            client_token=rc.get(section, 'client_token'),\n            client_secret=rc.get(section, 'client_secret'),\n            access_token=rc.get(section, 'access_token'),\n            headers_to_sign=rc.getlist(section, 'headers_to_sign'),\n            max_body=rc.getint(section, 'max_body')\n        )", "response": "Returns an instance of the appropriate class based on the contents of the given edgerc file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getlist(self, section, option):\n        value = self.get(section, option)\n        if value:\n            return value.split(',')\n        else:\n            return None", "response": "returns the named option as a list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef covstr(strings):\n    try:\n        result = int(strings)\n    except ValueError:\n        result = float(strings)\n    return result", "response": "convert string to int or float."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef real(self):\n        try:\n            unch = sum([covstr(self.__raw[3]), covstr(self.__raw[4])]) / 2\n            result = {\n            'name': unicode(self.__raw[36].replace(' ', ''), 'cp950'),\n            'no': self.__raw[0],\n            'range': self.__raw[1],    # \u6f32\u8dcc\u50f9\n            'time': self.__raw[2],     # \u53d6\u5f97\u6642\u9593\n            'max': self.__raw[3],      # \u6f32\u505c\u50f9\n            'min': self.__raw[4],      # \u8dcc\u505c\u50f9\n            'unch': '%.2f' % unch,  # \u6628\u65e5\u6536\u76e4\u50f9\n            'pp': '%.2f' % ((covstr(self.__raw[8]) - unch) / unch * 100),\n                                       # \u6f32\u8dcc\u5e45 %\n            'o': self.__raw[5],        # \u958b\u76e4\u50f9\n            'h': self.__raw[6],        # \u7576\u65e5\u6700\u9ad8\u50f9\n            'l': self.__raw[7],        # \u7576\u65e5\u6700\u4f4e\u50f9\n            'c': self.__raw[8],        # \u6210\u4ea4\u50f9/\u6536\u76e4\u50f9\n            'value': self.__raw[9],    # \u7d2f\u8a08\u6210\u4ea4\u91cf\n            'pvalue': self.__raw[10],  # \u8a72\u76e4\u6210\u4ea4\u91cf\n            'top5buy': [\n                            (self.__raw[11], self.__raw[12]),\n                            (self.__raw[13], self.__raw[14]),\n                            (self.__raw[15], self.__raw[16]),\n                            (self.__raw[17], self.__raw[18]),\n                            (self.__raw[19], self.__raw[20])\n                            ],\n            'top5sell': [\n                            (self.__raw[21], self.__raw[22]),\n                            (self.__raw[23], self.__raw[24]),\n                            (self.__raw[25], self.__raw[26]),\n                            (self.__raw[27], self.__raw[28]),\n                            (self.__raw[29], self.__raw[30])\n                            ]\n            }\n\n            if '-' in self.__raw[1]:  # \u6f32\u8dcc\u5224\u65b7 True, False\n                result['ranges'] = False  # price down\n            else:\n                result['ranges'] = True  # price up\n\n            result['crosspic'] = (\"http://chart.apis.google.com/chart?\" +\n                \"chf=bg,s,ffffff&chs=20x50&cht=ls\" +\n                \"&chd=t1:0,0,0|0,%(h)s,0|0,%(c)s,0|0,%(o)s,0|0,%(l)s,0\" +\n                \"&chds=%(l)s,%(h)s&chm=F,,1,1:4,20\") % result\n\n            result['top5buy'].sort()\n            result['top5sell'].sort()\n\n            return result\n        except (IndexError, ValueError):\n            return False", "response": "Real time data\n\n            :rtype: dict\n            :returns:\n\n                :name:     \u80a1\u7968\u540d\u7a31 Unicode\n                :no:       \u80a1\u7968\u4ee3\u78bc\n                :range:    \u6f32\u8dcc\u50f9\n                :ranges:   \u6f32\u8dcc\u5224\u65b7 True, False\n                :time:     \u53d6\u5f97\u6642\u9593\n                :max:      \u6f32\u505c\u50f9\n                :min:      \u8dcc\u505c\u50f9\n                :unch:     \u6628\u65e5\u6536\u76e4\u50f9\n                :pp:       \u6f32\u8dcc\u5e45 %\n                :o:        \u958b\u76e4\u50f9\n                :h:        \u7576\u65e5\u6700\u9ad8\u50f9\n                :l:        \u7576\u65e5\u6700\u4f4e\u50f9\n                :c:        \u6210\u4ea4\u50f9/\u6536\u76e4\u50f9\n                :value:    \u7d2f\u8a08\u6210\u4ea4\u91cf\n                :pvalue:   \u8a72\u76e4\u6210\u4ea4\u91cf\n                :top5buy:  \u6700\u4f73\u4e94\u6a94\u8cb7\u9032\u50f9\u91cf\u8cc7\u8a0a\n                :top5sell: \u6700\u4f73\u4e94\u6a94\u8ce3\u51fa\u50f9\u91cf\u8cc7\u8a0a\n                :crosspic: K\u7dda\u5716 by Google Chart"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef real(self):\n        ''' Get realtime data\n\n            :rtype: dict\n            :returns: \u4ee3\u78bc\u53ef\u4ee5\u53c3\u8003\uff1ahttp://goristock.appspot.com/API#apiweight\n        '''\n        result = self.__raw['1'].copy()\n        result['c'] = self.__raw['1']['value']\n        result['value'] = self.__raw['200']['v2']\n        result['date'] = self.__raw['0']['time']\n        return result", "response": "Get realtime data\n\n            :rtype: dict\n            :returns: \u4ee3\u78bc\u53ef\u4ee5\u53c3\u8003\uff1ahttp://goristock.appspot.com/API#apiweight"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay Taiwan Time now \u986f\u793a\u53f0\u7063\u6b64\u523b\u6642\u9593", "response": "def now(self):\n        ''' Display Taiwan Time now\n            \u986f\u793a\u53f0\u7063\u6b64\u523b\u6642\u9593\n        '''\n        utcnow = datetime.utcnow()\n        return utcnow + timedelta(hours=self.time_zone)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay Taiwan date now \u986f\u793a\u53f0\u7063\u6b64\u523b\u65e5\u671f", "response": "def date(self):\n        ''' Display Taiwan date now\n            \u986f\u793a\u53f0\u7063\u6b64\u523b\u65e5\u671f\n        '''\n        utcnow = datetime.utcnow()\n        return (utcnow + timedelta(hours=self.time_zone)).date()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a datetime object with the next day of the catalina.", "response": "def nextday(self):\n        ''' nextday: \u4e0b\u4e00\u500b\u65e5\u671f\n\n            :rtype: datetime\n            :returns: \u4e0b\u4e00\u500b\u9810\u8a2d\u6642\u9593\u65e5\u671f\n        '''\n        nextday = self.__zero.date() + timedelta(days=1)\n        return datetime.combine(nextday, time())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exptime(self):\n        ''' exptime: \u4e0b\u4e00\u500b\u65e5\u671f\u6642\u9593\n\n            :returns: \u4e0b\u4e00\u500b\u9810\u8a2d\u6642\u9593\n        '''\n        return self.nextday + timedelta(hours=self.__hour - 8,\n                                        minutes=self.__minutes)", "response": "get the next date of the last time"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bias_ratio(self, positive_or_negative=False):\n        return self.data.check_moving_average_bias_ratio(\n                               self.data.moving_average_bias_ratio(3, 6)[0],\n                               positive_or_negative=positive_or_negative)[0]", "response": "Check if moving average bias ratio is correct."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the best buy 1 is available.", "response": "def best_buy_1(self):\n        \"\"\" \u91cf\u5927\u6536\u7d05\n\n            :rtype: bool\n        \"\"\"\n        result = self.data.value[-1] > self.data.value[-2] and \\\n                 self.data.price[-1] > self.data.openprice[-1]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef best_buy_2(self):\n        result = self.data.value[-1] < self.data.value[-2] and \\\n                 self.data.price[-1] > self.data.price[-2]\n        return result", "response": "Returns True if the best buy 2 is available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the best sell 1 is found.", "response": "def best_sell_1(self):\n        \"\"\" \u91cf\u5927\u6536\u9ed1\n\n            :rtype: bool\n        \"\"\"\n        result = self.data.value[-1] > self.data.value[-2] and \\\n                 self.data.price[-1] < self.data.openprice[-1]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the best sell 2 is available.", "response": "def best_sell_2(self):\n        \"\"\" \u91cf\u7e2e\u50f9\u8dcc\n\n            :rtype: bool\n        \"\"\"\n        result = self.data.value[-1] < self.data.value[-2] and \\\n                 self.data.price[-1] < self.data.price[-2]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef best_four_point_to_buy(self):\n        result = []\n        if self.check_mins_bias_ratio() and \\\n            (self.best_buy_1() or self.best_buy_2() or self.best_buy_3() or \\\n             self.best_buy_4()):\n            if self.best_buy_1():\n                result.append(self.best_buy_1.__doc__.strip().decode('utf-8'))\n            if self.best_buy_2():\n                result.append(self.best_buy_2.__doc__.strip().decode('utf-8'))\n            if self.best_buy_3():\n                result.append(self.best_buy_3.__doc__.strip().decode('utf-8'))\n            if self.best_buy_4():\n                result.append(self.best_buy_4.__doc__.strip().decode('utf-8'))\n            result = ', '.join(result)\n        else:\n            result = False\n        return result", "response": "returns a list of the most five point to buy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport data from csv", "response": "def importcsv(self):\n        ''' import data from csv '''\n        csv_path = os.path.join(os.path.dirname(__file__), self.stock_no_files)\n        with open(csv_path) as csv_file:\n            csv_data = csv.reader(csv_file)\n            result = {}\n            for i in csv_data:\n                try:\n                    result[i[0]] = str(i[1]).decode('utf-8')\n                except ValueError:\n                    if i[0] == 'UPDATE':\n                        self.last_update = str(i[1]).decode('utf-8')\n                    else:\n                        pass\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the industry_code file and returns a dictionary of the data.", "response": "def __industry_code(self):\n        ''' import industry_code '''\n        csv_path = os.path.join(os.path.dirname(__file__),\n                self.industry_code_files)\n        with open(csv_path) as csv_file:\n            csv_data = csv.reader(csv_file)\n            result = {}\n            for i in csv_data:\n                result[i[0]] = i[1].decode('utf-8')\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload all industry comps", "response": "def __loadindcomps(self):\n        ''' import industry comps '''\n        csv_path = os.path.join(os.path.dirname(__file__), self.stock_no_files)\n        with open(csv_path) as csv_file:\n            csv_data = csv.reader(csv_file)\n            result = {}\n            check_words = re.compile(r'^[\\d]{2,}[\\w]?')\n            for i in csv_data:\n                if check_words.match(i[2]):\n                    try:\n                        result[i[2]].append(i[0].decode('utf-8'))\n                    except (ValueError, KeyError):\n                        try:\n                            result[i[2]] = [i[0].decode('utf-8')]\n                        except KeyError:\n                            pass\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, name):\n        pattern = re.compile(name)\n        result = {}\n        for i in self.__allstockno:\n            query = re.search(pattern, self.__allstockno[i])\n            if query:\n                query.group()\n                result[i] = self.__allstockno[i]\n        return result", "response": "\u641c\u5c0b\u80a1\u7968\u540d\u7a31 by unicode\n\n            :param str name: \u6b32\u641c\u5c0b\u7684\u5b57\u4e32\n            :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict of stock component names and their associated data.", "response": "def get_stock_comps_list(self):\n        \"\"\" \u56de\u50b3\u65e5\u5e38\u4ea4\u6613\u7684\u985e\u5225\u4ee3\u78bc\u8207\u540d\u7a31\n\n            :rtype: dict\n\n            .. versionadded:: 0.5.6\n        \"\"\"\n        code_list = self.industry_code\n        stock_comps_list = {}\n\n        for i in code_list:\n            if len(i) == 2 and i.isdigit():\n                stock_comps_list.update({i: code_list[i]})\n\n        return stock_comps_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stock_list(self):\n        all_stock = self.all_stock\n        industry_comps = self.industry_comps\n        result = {}\n\n        for comps_no in self.get_stock_comps_list():\n            if comps_no in industry_comps:\n                for stock_no in industry_comps[comps_no]:\n                    result.update({stock_no: all_stock[stock_no]})\n        return result", "response": "Return a dict of all stocks and their associated attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads date from CSV file.", "response": "def __loaddate():\n        ''' \u8f09\u5165\u6a94\u6848\n            \u6a94\u6848\u4f9d\u64da http://www.twse.com.tw/ch/trading/trading_days.php\n        '''\n        csv_path = os.path.join(os.path.dirname(__file__), 'opendate.csv')\n        with open(csv_path) as csv_file:\n            csv_data = csv.reader(csv_file)\n            result = {}\n            result['close'] = []\n            result['open'] = []\n            for i in csv_data:\n                if i[1] == '0':  # 0 = \u4f11\u5e02\n                    result['close'].append(datetime.strptime(i[0],\n                                                             '%Y/%m/%d').date())\n                elif i[1] == '1':  # 1 = \u958b\u5e02\n                    result['open'].append(datetime.strptime(i[0],\n                                                            '%Y/%m/%d').date())\n                else:\n                    pass\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef caldata(self, time):\n        ''' Market open or not.\n\n            :param datetime time: \u6b32\u5224\u65b7\u7684\u65e5\u671f\n            :rtype: bool\n            :returns: True \u70ba\u958b\u5e02\u3001False \u70ba\u4f11\u5e02\n        '''\n        if time.date() in self.__ocdate['close']:  # \u5224\u5c0d\u662f\u5426\u70ba\u6cd5\u5b9a\u4f11\u5e02\n            return False\n        elif time.date() in self.__ocdate['open']:  # \u5224\u5c0d\u662f\u5426\u70ba\u6cd5\u5b9a\u958b\u5e02\n            return True\n        else:\n            if time.weekday() <= 4:  # \u5224\u65b7\u662f\u5426\u70ba\u5e73\u5e38\u65e5\u958b\u5e02\n                return True\n            else:\n                return False", "response": "Return True if the user is in the calendar data False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch data from the database", "response": "def serial_fetch(self, stock_no, month, twse=None):\n        \"\"\" \u4e32\u63a5\u6bcf\u6708\u8cc7\u6599 \u820a\u2192\u65b0\n\n            :param str stock_no: \u80a1\u7968\u4ee3\u78bc\n            :param int month: \u64f7\u53d6 n \u500b\u6708\u7684\u8cc7\u6599\n            :param bool twse: \u6307\u5b9a\u662f\u5426\u70ba\u4e0a\u5e02\u8cc7\u6599\n            :rtype: tuple\n        \"\"\"\n        result = ()\n        self.__get_mons = month\n        self.__get_no = stock_no\n        self._twse = twse\n        for i in range(month):\n            nowdatetime = datetime.today() - relativedelta(months=i)\n            tolist = self.to_list(self.fetch_data(stock_no, nowdatetime))\n            result = tolist + result\n        return tuple(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert csv file to list", "response": "def to_list(self, csv_file):\n        \"\"\" \u4e32\u63a5\u6bcf\u65e5\u8cc7\u6599 \u820a\u2192\u65b0\n\n            :param csv csv_file: csv files\n            :rtype: list\n        \"\"\"\n        tolist = []\n        for i in csv_file:\n            i = [value.strip().replace(',', '') for value in i]\n            try:\n                for value in (1, 2, 3, 4, 5, 6, 8):\n                    i[value] = float(i[value])\n            except (IndexError, ValueError):\n                pass\n            tolist.append(i)\n        if self._twse:\n            if tolist:\n                _stock_info = tolist[0][0].split(' ')[1].strip()\n                self.__info = (_stock_info[:4],\n                               _stock_info[4:].decode('utf-8'))\n                self.__raw_rows_name = tolist[1]\n                return tuple(tolist[2:])\n            return tuple([])\n        else:\n            if len(tolist) > 6:\n                self.__raw_rows_name = tolist[4]\n                self.__info = (self.__get_no, OTCNo().all_stock[self.__get_no])\n                if len(tolist[5:]) > 1:\n                    return tuple(tolist[5:-1])\n            return tuple([])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a tuple of all the n \u500b\u6708\u7684\u8cc7\u6599s for a given month", "response": "def plus_mons(self, month):\n        \"\"\" \u589e\u52a0 n \u500b\u6708\u7684\u8cc7\u6599\n\n            :param int month: \u589e\u52a0 n \u500b\u6708\u7684\u8cc7\u6599\n            :rtype: tuple\n        \"\"\"\n        result = []\n        exist_mons = self.__get_mons\n        oldraw = list(self.__raw_data)\n        for i in range(month):\n            nowdatetime = datetime.today() - relativedelta(months=exist_mons) -\\\n                          relativedelta(months=i)\n            tolist = self.to_list(self.fetch_data(self.__info[0], nowdatetime))\n            result = list(tolist) + result\n        result = result + oldraw\n        self.__get_mons = exist_mons + month\n        return tuple(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching data from gretai. org. tw", "response": "def fetch_data(self, stock_no, nowdatetime):\n        \"\"\" Fetch data from gretai.org.tw(OTC)\n            return list.\n            \u5f9e gretai.org.tw \u4e0b\u8f09\u8cc7\u6599\uff0c\u56de\u50b3\u683c\u5f0f\u70ba csv.reader\n\n            0. \u65e5\u671f\n            1. \u6210\u4ea4\u80a1\u6578\n            2. \u6210\u4ea4\u91d1\u984d\n            3. \u958b\u76e4\u50f9\n            4. \u6700\u9ad8\u50f9\uff08\u7e8c\uff09\n            5. \u6700\u4f4e\u50f9\n            6. \u6536\u76e4\u50f9\n            7. \u6f32\u8dcc\u50f9\u5dee\n            8. \u6210\u4ea4\u7b46\u6578\n\n            :param str stock_no: \u80a1\u7968\u4ee3\u78bc\n            :param datetime nowdatetime: \u6b64\u523b\u6642\u9593\n            :rtype: list\n        \"\"\"\n        url = (\n            '/ch/stock/aftertrading/' +\n            'daily_trading_info/st43_download.php?d=%(year)d/%(mon)02d&' +\n            'stkno=%(stock)s&r=%(rand)s') % {\n                    'year': nowdatetime.year - 1911,\n                    'mon': nowdatetime.month,\n                    'stock': stock_no,\n                    'rand': random.randrange(1, 1000000)}\n\n        logging.info(url)\n        result = GRETAI_CONNECTIONS.urlopen('GET', url)\n        csv_files = csv.reader(StringIO(result.data))\n        self.__url.append(GRETAI_HOST + url)\n        return csv_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch_data(self, stock_no, nowdatetime):\n        result = TWSE_CONNECTIONS.request('POST',\n                '/ch/trading/exchange/STOCK_DAY/STOCK_DAYMAIN.php',\n                fields={'download': 'csv',\n                        'query_year': nowdatetime.year,\n                        'query_month': nowdatetime.month,\n                        'CO_ID': stock_no})\n        _de = result.data.decode('cp950', 'ignore')\n        csv_files = csv.reader(StringIO(_de.encode('utf-8')))\n        return csv_files", "response": "Fetch data from twse. com. tw\n            return list. tw\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the serial price", "response": "def __serial_price(self, rows=6):\n        \"\"\" \u53d6\u51fa\u67d0\u4e00\u50f9\u683c\u5e8f\u5217 *(\u820a\u2192\u65b0)*\n\n            \u9810\u8a2d\u5e8f\u5217\u6536\u76e4\u50f9 *(self.__serial_price(6))*\n\n            :rtype: list\n            :returns: \u9810\u8a2d\u5e8f\u5217\u6536\u76e4\u50f9 *(self.__serial_price(6))*\n        \"\"\"\n        result = (float(i[rows]) for i in self.__raw_data)\n        return list(result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __calculate_moving_average(self, date, row):\n        cal_data = self.__serial_price(row)\n        result = []\n        for dummy in range(len(cal_data) - int(date) + 1):\n            result.append(round(sum(cal_data[-date:]) / date, 2))\n            cal_data.pop()\n        result.reverse()\n        cont = self.__cal_continue(result)\n        return result, cont", "response": "Calculate moving average for a given date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __cal_continue(cls, list_data):\n        diff_data = []\n        for i in range(1, len(list_data)):\n            if list_data[-i] > list_data[-i - 1]:\n                diff_data.append(1)\n            else:\n                diff_data.append(-1)\n        cont = 0\n        for value in diff_data:\n            if value == diff_data[0]:\n                cont += 1\n            else:\n                break\n        return cont * diff_data[0]", "response": "Return the number of times the user has selected a new entry in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates moving average value", "response": "def moving_average_value(self, date):\n        \"\"\" \u8a08\u7b97 n \u65e5\u6210\u4ea4\u80a1\u6578\u5747\u91cf\u8207\u6301\u7e8c\u5929\u6578\n\n            :param int date: n \u65e5\n            :rtype: tuple (\u5e8f\u5217 \u820a\u2192\u65b0, \u6301\u7e8c\u5929\u6578)\n        \"\"\"\n        val, conti = self.__calculate_moving_average(date, 1)\n        val = (round(i / 1000, 3) for i in val)\n        return list(val), conti"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef moving_average_bias_ratio(self, date1, date2):\n        data1 = self.moving_average(date1)[0]\n        data2 = self.moving_average(date2)[0]\n        cal_list = []\n        for i in range(1, min(len(data1), len(data2)) + 1):\n            cal_list.append(data1[-i] - data2[-i])\n        cal_list.reverse()\n        cont = self.__cal_continue(cal_list)\n        return cal_list, cont", "response": "\u8a08\u7b97\u4e56\u96e2\u7387\uff08\u5747\u50f9\uff09\n            date1 - date2\n            date2 - date1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef value(self):\n        val = (round(i / 1000, 3) for i in self.__serial_price(1))\n        return list(val)", "response": "get a list of all the key value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_moving_average_bias_ratio(cls, data, sample=5,\n                                        positive_or_negative=False):\n        \"\"\"\u5224\u65b7\u6b63\u8ca0\u4e56\u96e2\u8f49\u6298\u9ede\u4f4d\u7f6e\n\n           :param list data: \u8a08\u7b97\u8cc7\u6599\n           :param int sample: \u8a08\u7b97\u7684\u5340\u9593\u6a23\u672c\u6578\u91cf\n           :param bool positive_or_negative: \u6b63\u4e56\u96e2 \u70ba True\uff0c\u8ca0\u4e56\u96e2 \u70ba False\n           :rtype: tuple\n           :returns: (True or False, \u7b2c\u5e7e\u500b\u8f49\u6298\u65e5, \u8f49\u6298\u9ede\u503c)\n        \"\"\"\n        return cls.__cal_ma_bias_ratio_point(data, sample,\n                                              positive_or_negative)", "response": "Check moving average bias ratio."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef options(self, parser, env=os.environ):\n        # pylint:disable=dangerous-default-value\n        super(FlakyPlugin, self).options(parser, env=env)\n        self.add_report_option(parser.add_option)\n        group = OptionGroup(\n            parser, \"Force flaky\", \"Force all tests to be flaky.\")\n        self.add_force_flaky_options(group.add_option)\n        parser.add_option_group(group)", "response": "Override. options to add options to the nose argument parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stream(self, multiprocess=False):\n        if multiprocess:\n            from flaky.multiprocess_string_io import MultiprocessingStringIO\n            return MultiprocessingStringIO()\n        return self._stream", "response": "Get the stream used to store the flaky report."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the base class configure to create the object that will be used to create the flaky log entries.", "response": "def configure(self, options, conf):\n        \"\"\"Base class override.\"\"\"\n        super(FlakyPlugin, self).configure(options, conf)\n        if not self.enabled:\n            return\n        is_multiprocess = int(getattr(options, 'multiprocess_workers', 0)) > 0\n        self._stream = self._get_stream(is_multiprocess)\n        self._flaky_result = TextTestResult(self._stream, [], 0)\n        self._flaky_report = options.flaky_report\n        self._flaky_success_report = options.flaky_success_report\n        self._force_flaky = options.force_flaky\n        self._max_runs = options.max_runs\n        self._min_passes = options.min_passes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride to handle errors in the baseclass.", "response": "def handleError(self, test, err):\n        \"\"\"\n        Baseclass override. Called when a test raises an exception.\n\n        If the test isn't going to be rerun again, then report the error\n        to the nose test result.\n\n        :param test:\n            The test that has raised an error\n        :type test:\n            :class:`nose.case.Test`\n        :param err:\n            Information about the test failure (from sys.exc_info())\n        :type err:\n            `tuple` of `class`, :class:`Exception`, `traceback`\n        :return:\n            True, if the test will be rerun; False, if nose should handle it.\n        :rtype:\n            `bool`\n        \"\"\"\n        # pylint:disable=invalid-name\n        want_error = self._handle_test_error_or_failure(test, err)\n        if not want_error and id(test) in self._tests_that_reran:\n            self._nose_result.addError(test, err)\n        return want_error or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handleFailure(self, test, err):\n        # pylint:disable=invalid-name\n        want_failure = self._handle_test_error_or_failure(test, err)\n        if not want_failure and id(test) in self._tests_that_reran:\n            self._nose_result.addFailure(test, err)\n        return want_failure or None", "response": "Baseclass override. Called when a test fails."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride Called when a test succeeds.", "response": "def addSuccess(self, test):\n        \"\"\"\n        Baseclass override. Called when a test succeeds.\n\n        Count remaining retries and compare with number of required successes\n        that have not yet been achieved; retry if necessary.\n\n        Returning True from this method keeps the test runner from reporting\n        the test as a success; this way we can retry and only report as a\n        success if we have achieved the required number of successes.\n\n        :param test:\n            The test that has succeeded\n        :type test:\n            :class:`nose.case.Test`\n        :return:\n            True, if the test will be rerun; False, if nose should handle it.\n        :rtype:\n            `bool`\n        \"\"\"\n        # pylint:disable=invalid-name\n        will_handle = self._handle_test_success(test)\n        test_id = id(test)\n        # If this isn't a rerun, the builtin reporter is going to report it as a success\n        if will_handle and test_id not in self._tests_that_reran:\n            self._tests_that_have_been_reported.add(test_id)\n        # If this test hasn't already been reported as successful, then do it now\n        if not will_handle and test_id in self._tests_that_reran and test_id not in self._tests_that_have_been_reported:\n            self._nose_result.addSuccess(test)\n        return will_handle or None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_flaky_attributes(max_runs=None, min_passes=None, rerun_filter=None):\n    if max_runs is None:\n        max_runs = 2\n    if min_passes is None:\n        min_passes = 1\n    if min_passes <= 0:\n        raise ValueError('min_passes must be positive')\n    if max_runs < min_passes:\n        raise ValueError('min_passes cannot be greater than max_runs!')\n\n    return {\n        FlakyNames.MAX_RUNS: max_runs,\n        FlakyNames.MIN_PASSES: min_passes,\n        FlakyNames.CURRENT_RUNS: 0,\n        FlakyNames.CURRENT_PASSES: 0,\n        FlakyNames.RERUN_FILTER: FilterWrapper(rerun_filter or _true),\n    }", "response": "Returns the default flaky attributes for a flaky test."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_unicode_string(obj):\n    try:\n        return unicode_type(obj)\n    except UnicodeDecodeError:\n        if hasattr(obj, 'decode'):\n            return obj.decode('utf-8', 'replace')\n        return str(obj).decode('utf-8', 'replace')", "response": "Ensures that the given object is a unicode string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _report_final_failure(self, err, flaky, name):\n        min_passes = flaky[FlakyNames.MIN_PASSES]\n        current_passes = flaky[FlakyNames.CURRENT_PASSES]\n        message = self._failure_message.format(\n            current_passes,\n            min_passes,\n        )\n        self._log_test_failure(name, err, message)", "response": "Report that a test has failed too many times to pass at least min_passes times."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _log_intermediate_failure(self, err, flaky, name):\n        max_runs = flaky[FlakyNames.MAX_RUNS]\n        runs_left = max_runs - flaky[FlakyNames.CURRENT_RUNS]\n        message = self._retry_failure_message.format(\n            runs_left,\n            max_runs,\n        )\n        self._log_test_failure(name, err, message)", "response": "Report that the test has failed but still has reruns left."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_report_option(add_option):\n        add_option(\n            '--no-flaky-report',\n            action='store_false',\n            dest='flaky_report',\n            default=True,\n            help=\"Suppress the report at the end of the \"\n                 \"run detailing flaky test results.\",\n        )\n        add_option(\n            '--no-success-flaky-report',\n            action='store_false',\n            dest='flaky_success_report',\n            default=True,\n            help=\"Suppress reporting flaky test successes\"\n                 \"in the report at the end of the \"\n                 \"run detailing flaky test results.\",\n        )", "response": "Adds an option to the test runner to suppress the flaky report."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd options to the test runner that force all tests to be flaky.", "response": "def add_force_flaky_options(add_option):\n        \"\"\"\n        Add options to the test runner that force all tests to be flaky.\n\n        :param add_option:\n            A function that can add an option to the test runner.\n            Its argspec should equal that of argparse.add_option.\n        :type add_option:\n            `callable`\n        \"\"\"\n        add_option(\n            '--force-flaky',\n            action=\"store_true\",\n            dest=\"force_flaky\",\n            default=False,\n            help=\"If this option is specified, we will treat all tests as \"\n                 \"flaky.\"\n        )\n        add_option(\n            '--max-runs',\n            action=\"store\",\n            dest=\"max_runs\",\n            type=int,\n            default=2,\n            help=\"If --force-flaky is specified, we will run each test at \"\n                 \"most this many times (unless the test has its own flaky \"\n                 \"decorator).\"\n        )\n        add_option(\n            '--min-passes',\n            action=\"store\",\n            dest=\"min_passes\",\n            type=int,\n            default=1,\n            help=\"If --force-flaky is specified, we will run each test at \"\n                 \"least this many times (unless the test has its own flaky \"\n                 \"decorator).\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_flaky_report(self, stream):\n        value = self._stream.getvalue()\n\n        # If everything succeeded and --no-success-flaky-report is specified\n        # don't print anything.\n        if not self._flaky_success_report and not value:\n            return\n\n        stream.write('===Flaky Test Report===\\n\\n')\n\n        # Python 2 will write to the stderr stream as a byte string, whereas\n        # Python 3 will write to the stream as text. Only encode into a byte\n        # string if the write tries to encode it first and raises a\n        # UnicodeEncodeError.\n        try:\n            stream.write(value)\n        except UnicodeEncodeError:\n            stream.write(value.encode('utf-8', 'replace'))\n\n        stream.write('\\n===End Flaky Test Report===\\n')", "response": "Write details about flaky tests to the test report."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _copy_flaky_attributes(cls, test, test_class):\n        test_callable = cls._get_test_callable(test)\n        if test_callable is None:\n            return\n        for attr, value in cls._get_flaky_attributes(test_class).items():\n            already_set = hasattr(test, attr)\n            if already_set:\n                continue\n            attr_on_callable = getattr(test_callable, attr, None)\n            if attr_on_callable is not None:\n                cls._set_flaky_attribute(test, attr, attr_on_callable)\n            elif value is not None:\n                cls._set_flaky_attribute(test, attr, value)", "response": "Copy flaky attributes from the test callable or class to the test."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _increment_flaky_attribute(cls, test_item, flaky_attribute):\n        cls._set_flaky_attribute(test_item, flaky_attribute, cls._get_flaky_attribute(test_item, flaky_attribute) + 1)", "response": "Increments the value of an attribute on a flaky test."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _has_flaky_attributes(cls, test):\n        current_runs = cls._get_flaky_attribute(test, FlakyNames.CURRENT_RUNS)\n        return current_runs is not None", "response": "Returns True if the test callable in question is marked as flaky."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the flaky related attributes from the test.", "response": "def _get_flaky_attributes(cls, test_item):\n        \"\"\"\n        Get all the flaky related attributes from the test.\n\n        :param test_item:\n            The test callable from which to get the flaky related attributes.\n        :type test_item:\n            `callable` or :class:`nose.case.Test` or :class:`Function`\n        :return:\n        :rtype:\n            `dict` of `unicode` to varies\n        \"\"\"\n        return {\n            attr: cls._get_flaky_attribute(\n                test_item,\n                attr,\n            ) for attr in FlakyNames()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking web connection. Returns True if web connection is OK, False otherwise.", "response": "def connection_ok():\n    \"\"\"Check web connection.\n    Returns True if web connection is OK, False otherwise.\n    \"\"\"\n    try:\n        urlopen(Dataset.base_url, timeout=1)\n        # if an index page is ever added, this will pass through\n        return True\n    except HTTPError:\n        # There's no index for BASE_URL so Error 404 is expected\n        return True\n    except URLError:\n        # This is raised if there is no internet connection\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _download_datasets():\n    def filepath(*args):\n        return abspath(join(dirname(__file__), '..', 'vega_datasets', *args))\n    dataset_listing = {}\n    for name in DATASETS_TO_DOWNLOAD:\n        data = Dataset(name)\n        url = data.url\n        filename = filepath('_data', data.filename)\n        print(\"retrieving data {0} -> {1}\".format(url, filename))\n        urlretrieve(url, filename)\n        dataset_listing[name] = '_data/{0}'.format(data.filename)\n    with open(filepath('local_datasets.json'), 'w') as f:\n        json.dump(dataset_listing, f, indent=2, sort_keys=True)", "response": "Utility to download datasets into package source"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_dataset_info():\n    def load_json(path):\n        raw = pkgutil.get_data('vega_datasets', path)\n        return json.loads(bytes_decode(raw))\n\n    info = load_json('datasets.json')\n    descriptions = load_json('dataset_info.json')\n    local_datasets = load_json('local_datasets.json')\n\n    for name in info:\n        info[name]['is_local'] = (name in local_datasets)\n    for name in descriptions:\n        info[name].update(descriptions[name])\n\n    return info", "response": "This loads dataset info from three package files vega_datasets. json\n    vega_datasets. local_datasets. json\n    It returns a dictionary with dataset information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init(cls, name):\n        clsdict = {subcls.name: subcls for subcls in cls.__subclasses__()\n                   if hasattr(subcls, 'name')}\n        return clsdict.get(name, cls)(name)", "response": "Return an instance of this class or an appropriate subclass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the info dictionary for the given name", "response": "def _infodict(cls, name):\n        \"\"\"load the info dictionary for the given name\"\"\"\n        info = cls._dataset_info.get(name, None)\n        if info is None:\n            raise ValueError('No such dataset {0} exists, '\n                             'use list_datasets() to get a list '\n                             'of available datasets.'.format(name))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raw(self, use_local=True):\n        if use_local and self.is_local:\n            return pkgutil.get_data('vega_datasets', self.pkg_filename)\n        else:\n            return urlopen(self.url).read()", "response": "Load the raw dataset from the remote or local file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads unzips and installs chromedriver.", "response": "def run(self):\n        \"\"\"\n        Downloads, unzips and installs chromedriver.\n        If a chromedriver binary is found in PATH it will be copied, otherwise downloaded.\n        \"\"\"\n        chromedriver_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'chromedriver_binary')\n        chromedriver_filename = find_binary_in_path(get_chromedriver_filename())\n        if chromedriver_filename:\n            print(\"\\nChromedriver already installed at {}...\\n\".format(chromedriver_filename))\n            new_filename = os.path.join(chromedriver_dir, get_chromedriver_filename())\n            self.copy_file(chromedriver_filename, new_filename)\n        else:\n            chromedriver_bin = get_chromedriver_filename()\n            chromedriver_filename = os.path.join(chromedriver_dir, chromedriver_bin)\n            if not os.path.isfile(chromedriver_filename):\n                print(\"\\nDownloading Chromedriver...\\n\")\n                if not os.path.isdir(chromedriver_dir):\n                    os.mkdir(chromedriver_dir)\n                url = get_chromedriver_url()\n                try:\n                    response = urlopen(url)\n                    if response.getcode() != 200:\n                        raise URLError('Not Found')\n                except URLError:\n                    raise RuntimeError('Failed to download chromedriver archive: {}'.format(url))\n                archive = BytesIO(response.read())\n                with zipfile.ZipFile(archive) as zip_file:\n                    zip_file.extract(chromedriver_bin, chromedriver_dir)\n            else:\n                print(\"\\nChromedriver already installed at {}...\\n\".format(chromedriver_filename))\n            if not os.access(chromedriver_filename, os.X_OK):\n                os.chmod(chromedriver_filename, 0o744)\n        build_py.run(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the directory of the chromedriver binary file to the PATH.", "response": "def add_chromedriver_to_path():\n    \"\"\"\n    Appends the directory of the chromedriver binary file to PATH.\n    \"\"\"\n    chromedriver_dir = os.path.abspath(os.path.dirname(__file__))\n    if 'PATH' not in os.environ:\n        os.environ['PATH'] = chromedriver_dir\n    elif chromedriver_dir not in os.environ['PATH']:\n        os.environ['PATH'] += utils.get_variable_separator()+chromedriver_dir"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the download URL for the current platform architecture and version.", "response": "def get_chromedriver_url(version='74.0.3729.6'):\n    \"\"\"\n    Generates the download URL for current platform , architecture and the given version. Default version is 74.0.3729.6.\n    Supports Linux, MacOS and Windows.\n    :param version: chromedriver version string, default '74.0.3729.6'\n    :return: Download URL for chromedriver\n    \"\"\"\n    base_url = 'https://chromedriver.storage.googleapis.com/'\n    if sys.platform.startswith('linux') and sys.maxsize > 2 ** 32:\n        platform = 'linux'\n        architecture = '64'\n    elif sys.platform == 'darwin':\n        platform = 'mac'\n        architecture = '64'\n    elif sys.platform.startswith('win'):\n        platform = 'win'\n        architecture = '32'\n    else:\n        raise RuntimeError('Could not determine chromedriver download URL for this platform.')\n    return base_url + version + '/chromedriver_' + platform + architecture + '.zip'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a binary named filename in the current PATH.", "response": "def find_binary_in_path(filename):\n    \"\"\"\n    Searches for a binary named `filename` in the current PATH. If an executable is found, its absolute path is returned\n    else None.\n    :param filename: Filename of the binary\n    :return: Absolute path or None\n    \"\"\"\n    if 'PATH' not in os.environ:\n        return None\n    for directory in os.environ['PATH'].split(get_variable_separator()):\n        binary = os.path.abspath(os.path.join(directory, filename))\n        if os.path.isfile(binary) and os.access(binary, os.X_OK):\n            return binary\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose session from remote.", "response": "async def _remote_close(self, exc=None):\n        \"\"\"close session from remote.\"\"\"\n        if self.state in (STATE_CLOSING, STATE_CLOSED):\n            return\n\n        log.info(\"close session: %s\", self.id)\n        self.state = STATE_CLOSING\n        if exc is not None:\n            self.exception = exc\n            self.interrupted = True\n        try:\n            await self.handler(SockjsMessage(MSG_CLOSE, exc), self)\n        except Exception:\n            log.exception(\"Exception in close handler.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a message to the client.", "response": "def send(self, msg):\n        \"\"\"send message to client.\"\"\"\n        assert isinstance(msg, str), \"String is required\"\n\n        if self._debug:\n            log.info(\"outgoing message: %s, %s\", self.id, str(msg)[:200])\n\n        if self.state != STATE_OPEN:\n            return\n\n        self._feed(FRAME_MESSAGE, msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message frame to the client.", "response": "def send_frame(self, frm):\n        \"\"\"send message frame to client.\"\"\"\n        if self._debug:\n            log.info(\"outgoing message: %s, %s\", self.id, frm[:200])\n\n        if self.state != STATE_OPEN:\n            return\n\n        self._feed(FRAME_MESSAGE_BLOB, frm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(cls, alias, certs, key, key_format='pkcs8'):\n        timestamp = int(time.time()) * 1000\n\n        cert_chain = []\n        for cert in certs:\n            cert_chain.append(('X.509', cert))\n\n        pke = cls(timestamp = timestamp,\n                               # Alias must be lower case or it will corrupt the keystore for Java Keytool and Keytool Explorer\n                               alias = alias.lower(),\n                               cert_chain = cert_chain)\n\n        if key_format == 'pkcs8':\n            private_key_info = decoder.decode(key, asn1Spec=rfc5208.PrivateKeyInfo())[0]\n\n            pke._algorithm_oid = private_key_info['privateKeyAlgorithm']['algorithm'].asTuple()\n            pke.pkey = private_key_info['privateKey'].asOctets()\n            pke.pkey_pkcs8 = key\n\n        elif key_format == 'rsa_raw':\n            pke._algorithm_oid = RSA_ENCRYPTION_OID\n\n            # We must encode it to pkcs8\n            private_key_info = rfc5208.PrivateKeyInfo()\n            private_key_info.setComponentByName('version','v1')\n            a = AlgorithmIdentifier()\n            a.setComponentByName('algorithm', pke._algorithm_oid)\n            a.setComponentByName('parameters', '\\x05\\x00')\n            private_key_info.setComponentByName('privateKeyAlgorithm', a)\n            private_key_info.setComponentByName('privateKey', key)\n\n            pke.pkey_pkcs8 = encoder.encode(private_key_info, ifNotEmpty=True)\n            pke.pkey = key\n\n        else:\n            raise UnsupportedKeyFormatException(\"Key Format '%s' is not supported\" % key_format)\n\n        return pke", "response": "Create a new private key entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencrypt the private key and stores it in the internal _encrypted attribute.", "response": "def encrypt(self, key_password):\n        \"\"\"\n        Encrypts the private key, so that it can be saved to a keystore.\n\n        This will make it necessary to decrypt it again if it is going to be used later.\n        Has no effect if the entry is already encrypted.\n\n        :param str key_password: The password to encrypt the entry with.\n        \"\"\"\n        if not self.is_decrypted():\n            return\n\n        encrypted_private_key = sun_crypto.jks_pkey_encrypt(self.pkey_pkcs8, key_password)\n\n        a = AlgorithmIdentifier()\n        a.setComponentByName('algorithm', sun_crypto.SUN_JKS_ALGO_ID)\n        a.setComponentByName('parameters', '\\x05\\x00')\n        epki = rfc5208.EncryptedPrivateKeyInfo()\n        epki.setComponentByName('encryptionAlgorithm',a)\n        epki.setComponentByName('encryptedData', encrypted_private_key)\n\n        self._encrypted = encoder.encode(epki)\n        self._pkey = None\n        self._pkey_pkcs8 = None\n        self._algorithm_oid = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(cls, alias, sealed_obj, algorithm, key, key_size):\n        timestamp = int(time.time()) * 1000\n\n        raise NotImplementedError(\"Creating Secret Keys not implemented\")", "response": "Helper function to create a new SecretKeyEntry instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new(cls, store_type, store_entries):\n        if store_type not in ['jks', 'jceks']:\n            raise UnsupportedKeystoreTypeException(\"The Keystore Type '%s' is not supported\" % store_type)\n\n        entries = {}\n        for entry in store_entries:\n            if not isinstance(entry, AbstractKeystoreEntry):\n                raise UnsupportedKeystoreEntryTypeException(\"Entries must be a KeyStore Entry\")\n\n            if store_type != 'jceks' and isinstance(entry, SecretKeyEntry):\n                raise UnsupportedKeystoreEntryTypeException('Secret Key only allowed in JCEKS keystores')\n\n            alias = entry.alias\n\n            if alias in entries:\n                raise DuplicateAliasException(\"Found duplicate alias '%s'\" % alias)\n            entries[alias] = entry\n\n        return cls(store_type, entries)", "response": "Create a new keystore with the specified entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loads(cls, data, store_password, try_decrypt_keys=True):\n        store_type = \"\"\n        magic_number = data[:4]\n        if magic_number == MAGIC_NUMBER_JKS:\n            store_type = \"jks\"\n        elif magic_number == MAGIC_NUMBER_JCEKS:\n            store_type = \"jceks\"\n        else:\n            raise BadKeystoreFormatException('Not a JKS or JCEKS keystore'\n                                             ' (magic number wrong; expected'\n                                             ' FEEDFEED or CECECECE)')\n\n        try:\n            version = b4.unpack_from(data, 4)[0]\n            if version != 2:\n                tmpl = 'Unsupported keystore version; expected v2, found v%r'\n                raise UnsupportedKeystoreVersionException(tmpl % version)\n\n            entries = {}\n\n            entry_count = b4.unpack_from(data, 8)[0]\n            pos = 12\n            for i in range(entry_count):\n                tag = b4.unpack_from(data, pos)[0]; pos += 4\n                alias, pos = cls._read_utf(data, pos, kind=\"entry alias\")\n                timestamp = int(b8.unpack_from(data, pos)[0]); pos += 8 # milliseconds since UNIX epoch\n\n                if tag == 1:\n                    entry, pos = cls._read_private_key(data, pos, store_type)\n                elif tag == 2:\n                    entry, pos = cls._read_trusted_cert(data, pos, store_type)\n                elif tag == 3:\n                    if store_type != \"jceks\":\n                        raise BadKeystoreFormatException(\"Unexpected entry tag {0} encountered in JKS keystore; only supported in JCEKS keystores\".format(tag))\n                    entry, pos = cls._read_secret_key(data, pos, store_type)\n                else:\n                    raise BadKeystoreFormatException(\"Unexpected keystore entry tag %d\", tag)\n\n                entry.alias = alias\n                entry.timestamp = timestamp\n\n                if try_decrypt_keys:\n                    try:\n                        entry.decrypt(store_password)\n                    except DecryptionFailureException:\n                        pass # ok, let user call decrypt() manually\n\n                if alias in entries:\n                    raise DuplicateAliasException(\"Found duplicate alias '%s'\" % alias)\n                entries[alias] = entry\n\n        except struct.error as e:\n            raise BadKeystoreFormatException(e)\n\n        # check keystore integrity (uses UTF-16BE encoding of the password)\n        hash_fn = hashlib.sha1\n        hash_digest_size = hash_fn().digest_size\n\n        store_password_utf16 = store_password.encode('utf-16be')\n        expected_hash = hash_fn(store_password_utf16 + SIGNATURE_WHITENING + data[:pos]).digest()\n        found_hash = data[pos:pos+hash_digest_size]\n\n        if len(found_hash) != hash_digest_size:\n            tmpl = \"Bad signature size; found %d bytes, expected %d bytes\"\n            raise BadKeystoreFormatException(tmpl % (len(found_hash),\n                                                     hash_digest_size))\n        if expected_hash != found_hash:\n            raise KeystoreSignatureException(\"Hash mismatch; incorrect keystore password?\")\n\n        return cls(store_type, entries)", "response": "Loads the given keystore file using the supplied password and returns a keystore instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the keystore to the given password.", "response": "def saves(self, store_password):\n        \"\"\"\n        Saves the keystore so that it can be read by other applications.\n\n        If any of the private keys are unencrypted, they will be encrypted\n        with the same password as the keystore.\n\n        :param str store_password: Password for the created keystore\n          (and for any unencrypted keys)\n\n        :returns: A byte string representation of the keystore.\n\n        :raises UnsupportedKeystoreTypeException: If the keystore\n          is of an unsupported type\n        :raises UnsupportedKeystoreEntryTypeException: If the keystore\n          contains an unsupported entry type\n        \"\"\"\n\n        if self.store_type == 'jks':\n            keystore = MAGIC_NUMBER_JKS\n        elif self.store_type == 'jceks':\n            raise NotImplementedError(\"Saving of JCEKS keystores is not implemented\")\n        else:\n            raise UnsupportedKeystoreTypeException(\"Only JKS and JCEKS keystores are supported\")\n\n        keystore += b4.pack(2) # version 2\n        keystore += b4.pack(len(self.entries))\n\n        for alias, item in self.entries.items():\n            if isinstance(item, TrustedCertEntry):\n                keystore += self._write_trusted_cert(alias, item)\n            elif isinstance(item, PrivateKeyEntry):\n                keystore += self._write_private_key(alias, item, store_password)\n            elif isinstance(item, SecretKeyEntry):\n                if self.store_type != 'jceks':\n                    raise UnsupportedKeystoreEntryTypeException('Secret Key only allowed in JCEKS keystores')\n                raise NotImplementedError(\"Saving of Secret Keys not implemented\")\n            else:\n                raise UnsupportedKeystoreEntryTypeException(\"Unknown entry type in keystore\")\n\n        hash_fn = hashlib.sha1\n        store_password_utf16 = store_password.encode('utf-16be')\n        hash = hash_fn(store_password_utf16 + SIGNATURE_WHITENING + keystore).digest()\n        keystore += hash\n\n        return keystore"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef secret_keys(self):\n        return dict([(a, e) for a, e in self.entries.items()\n                     if isinstance(e, SecretKeyEntry)])", "response": "A subset of the entries dictionary filtered down to only\n        those entries of type SecretKeyEntry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _java_is_subclass(cls, obj, class_name):\n        clazz = obj.get_class()\n        while clazz:\n            if clazz.name == class_name:\n                return True\n            clazz = clazz.superclass\n        return False", "response": "Given a deserialized JavaObject as returned by the javaobj library class_name determine whether it s a subclass of the given class name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derive_key(hashfn, purpose_byte, password_str, salt, iteration_count, desired_key_size):\n    password_bytes = (password_str.encode('utf-16be') + b\"\\x00\\x00\") if len(password_str) > 0 else b\"\"\n    u = hashfn().digest_size # in bytes\n    v = hashfn().block_size  # in bytes\n\n    _salt = bytearray(salt)\n    _password_bytes = bytearray(password_bytes)\n\n    D = bytearray([purpose_byte])*v\n    S_len = ((len(_salt) + v -1)//v)*v\n    S = bytearray([_salt[n % len(_salt)] for n in range(S_len)])\n    P_len = ((len(_password_bytes) + v -1)//v)*v\n    P = bytearray([_password_bytes[n % len(_password_bytes)] for n in range(P_len)])\n\n    I = S + P\n    c = (desired_key_size + u - 1)//u\n    derived_key = b\"\"\n\n    for i in range(1,c+1):\n        A = hashfn(bytes(D + I)).digest()\n        for j in range(iteration_count - 1):\n            A = hashfn(A).digest()\n\n        A = bytearray(A)\n        B = bytearray([A[n % len(A)] for n in range(v)])\n\n        # Treating I as a concatenation I_0, I_1, ..., I_(k-1) of v-bit\n        # blocks, where k=ceiling(s/v)+ceiling(p/v), modify I by\n        # setting I_j=(I_j+B+1) mod 2^v for each j.\n        for j in range(len(I)//v):\n            _adjust(I, j*v, B)\n\n        derived_key += bytes(A)\n\n    # truncate derived_key to the desired size\n    derived_key = derived_key[:desired_key_size]\n    return derived_key", "response": "Derives a key from the password and salt."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _adjust(a, a_offset, b):\n    x = (b[-1] & 0xFF) + (a[a_offset + len(b) - 1] & 0xFF) + 1\n    a[a_offset + len(b) - 1] = ctypes.c_ubyte(x).value\n    x >>= 8\n\n    for i in range(len(b)-2, -1, -1):\n        x += (b[i] & 0xFF) + (a[a_offset + i] & 0xFF)\n        a[a_offset + i] = ctypes.c_ubyte(x).value\n        x >>= 8", "response": "Adjust the contents of a to include the contents of b."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt_PBEWithSHAAndTwofishCBC(encrypted_data, password, salt, iteration_count):\n    iv  = derive_key(hashlib.sha1, PURPOSE_IV_MATERIAL,  password, salt, iteration_count, 16)\n    key = derive_key(hashlib.sha1, PURPOSE_KEY_MATERIAL, password, salt, iteration_count, 256//8)\n\n    encrypted_data = bytearray(encrypted_data)\n    encrypted_data_len = len(encrypted_data)\n    if encrypted_data_len % 16 != 0:\n        raise BadDataLengthException(\"encrypted data length is not a multiple of 16 bytes\")\n\n    plaintext = bytearray()\n\n    # slow and dirty CBC decrypt\n    from twofish import Twofish\n    cipher = Twofish(key)\n\n    last_cipher_block = bytearray(iv)\n    for block_offset in range(0, encrypted_data_len, 16):\n        cipher_block = encrypted_data[block_offset:block_offset+16]\n        plaintext_block = xor_bytearrays(bytearray(cipher.decrypt(bytes(cipher_block))), last_cipher_block)\n        plaintext.extend(plaintext_block)\n        last_cipher_block = cipher_block\n\n    plaintext = strip_pkcs7_padding(plaintext, 16)\n    return bytes(plaintext)", "response": "Decrypts a PBE with SHA - 1 and Twofish CBC."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencrypts a value with PBEWithSHAAndTwofishCBC assuming PKCS#12 - generated PBE parameters.", "response": "def encrypt_PBEWithSHAAndTwofishCBC(plaintext_data, password, salt, iteration_count):\n    \"\"\"\n    Encrypts a value with PBEWithSHAAndTwofishCBC, assuming PKCS#12-generated PBE parameters.\n    (Not explicitly defined as an algorithm in RFC 7292, but defined here nevertheless because of the assumption of PKCS#12 parameters).\n    \"\"\"\n    iv  = derive_key(hashlib.sha1, PURPOSE_IV_MATERIAL,  password, salt, iteration_count, 16)\n    key = derive_key(hashlib.sha1, PURPOSE_KEY_MATERIAL, password, salt, iteration_count, 256//8)\n\n    plaintext_data = add_pkcs7_padding(plaintext_data, 16)\n    plaintext_data = bytearray(plaintext_data)\n    plaintext_len = len(plaintext_data)\n    assert plaintext_len % 16 == 0\n\n    ciphertext = bytearray()\n\n    from twofish import Twofish\n    cipher = Twofish(key)\n\n    last_cipher_block = bytearray(iv)\n    for block_offset in range(0, plaintext_len, 16):\n        plaintext_block = plaintext_data[block_offset:block_offset+16]\n        cipher_block = bytearray(cipher.encrypt(bytes(xor_bytearrays(plaintext_block, last_cipher_block))))\n        ciphertext.extend(cipher_block)\n        last_cipher_block = cipher_block\n\n    return bytes(ciphertext)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jks_pkey_encrypt(key, password_str):\n    password_bytes = password_str.encode('utf-16be') # Java chars are UTF-16BE code units\n    iv = os.urandom(20)\n\n    key = bytearray(key)\n    xoring = zip(key, _jks_keystream(iv, password_bytes))\n    data = bytearray([d^k for d,k in xoring])\n\n    check = hashlib.sha1(bytes(password_bytes + key)).digest()\n    return bytes(iv + data + check)", "response": "Encrypts the private key with password protection algorithm used by JKS keystores."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _jks_keystream(iv, password):\n    cur = iv\n    while 1:\n        xhash = hashlib.sha1(bytes(password + cur)) # hashlib.sha1 in python 2.6 does not accept a bytearray argument\n        cur = bytearray(xhash.digest()) # make sure we iterate over ints in both Py2 and Py3\n        for byte in cur:\n            yield byte", "response": "Helper keystream generator for _jks_pkey_decrypt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef jce_pbe_decrypt(data, password, salt, iteration_count):\n    key, iv = _jce_pbe_derive_key_and_iv(password, salt, iteration_count)\n\n    from Cryptodome.Cipher import DES3\n    des3 = DES3.new(key, DES3.MODE_CBC, IV=iv)\n    padded = des3.decrypt(data)\n\n    result = strip_pkcs5_padding(padded)\n    return result", "response": "Decrypts data using the PBE with MD5 and triple DES encryption scheme."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _jce_invert_salt_half(salt_half):\n    salt = bytearray(salt_half)\n    salt[2] = salt[1]\n    salt[1] = salt[0]\n    salt[0] = salt[3]\n    return bytes(salt)", "response": "Invert the first salt half of a JCE cipher."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bitstring_to_bytes(bitstr):\n    bitlist = list(bitstr)\n    bits_missing = (8 - len(bitlist) % 8) % 8\n    bitlist = [0]*bits_missing + bitlist # pad with 0 bits to a multiple of 8\n    result = bytearray()\n    for i in range(0, len(bitlist), 8):\n        byte = 0\n        for j in range(8):\n            byte = (byte << 1) | bitlist[i+j]\n        result.append(byte)\n    return bytes(result)", "response": "Converts a pyasn1 univ. BitString instance to byte sequence of type bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_pkcs7_padding(m, block_size):\n    if len(m) < block_size or len(m) % block_size != 0:\n        raise BadPaddingException(\"Unable to strip padding: invalid message length\")\n\n    m = bytearray(m) # py2/3 compatibility: always returns individual indexed elements as ints\n    last_byte = m[-1]\n    # the <last_byte> bytes of m must all have value <last_byte>, otherwise something's wrong\n    if (last_byte <= 0 or last_byte > block_size) or (m[-last_byte:] != bytearray([last_byte])*last_byte):\n        raise BadPaddingException(\"Unable to strip padding: invalid padding found\")\n\n    return bytes(m[:-last_byte])", "response": "Strip padding from a PKCS#5 message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the contents of the given file into a new object of the class cls.", "response": "def load(cls, filename, store_password, try_decrypt_keys=True):\n        \"\"\"\n        Convenience wrapper function; reads the contents of the given file\n        and passes it through to :func:`loads`. See :func:`loads`.\n        \"\"\"\n        with open(filename, 'rb') as file:\n            input_bytes = file.read()\n            ret = cls.loads(input_bytes,\n                            store_password,\n                            try_decrypt_keys=try_decrypt_keys)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename, store_password):\n        with open(filename, 'wb') as file:\n            keystore_bytes = self.saves(store_password)\n            file.write(keystore_bytes)", "response": "Saves the content of the object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a UTF - 8 string from the data stream at the given position.", "response": "def _read_utf(cls, data, pos, kind=None):\n        \"\"\"\n        :param kind: Optional; a human-friendly identifier for the kind of UTF-8 data we're loading (e.g. is it a keystore alias? an algorithm identifier? something else?).\n                     Used to construct more informative exception messages when a decoding error occurs.\n        \"\"\"\n        size = b2.unpack_from(data, pos)[0]\n        pos += 2\n        try:\n            return data[pos:pos+size].decode('utf-8'), pos+size\n        except (UnicodeEncodeError, UnicodeDecodeError) as e:\n            raise BadKeystoreFormatException((\"Failed to read %s, contains bad UTF-8 data: %s\" % (kind, str(e))) if kind else \\\n                                             (\"Encountered bad UTF-8 data: %s\" % str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sealed_keys(self):\n        return dict([(a, e) for a, e in self.entries.items()\n                     if isinstance(e, BksSealedKeyEntry)])", "response": "A subset of the entries dictionary filtered down to only\n        those entries of type BksSealedKeyEntry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plain_keys(self):\n        return dict([(a, e) for a, e in self.entries.items()\n                     if isinstance(e, BksKeyEntry)])", "response": "A subset of the entries dictionary filtered down to only\n        those entries of type BksKeyEntry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a BKS keystore from a byte string.", "response": "def loads(cls, data, store_password, try_decrypt_keys=True):\n        \"\"\"\n        See :meth:`jks.jks.KeyStore.loads`.\n\n        :param bytes data: Byte string representation of the keystore to be loaded.\n        :param str password: Keystore password string\n        :param bool try_decrypt_keys: Whether to automatically try to decrypt any encountered key entries using the same password\n                                      as the keystore password.\n\n        :returns: A loaded :class:`BksKeyStore` instance, if the keystore could be successfully parsed and the supplied store password is correct.\n\n                  If the ``try_decrypt_keys`` parameters was set to ``True``, any keys that could be successfully decrypted using the\n                  store password have already been decrypted; otherwise, no atttempt to decrypt any key entries is made.\n\n        :raises BadKeystoreFormatException: If the keystore is malformed in some way\n        :raises UnsupportedKeystoreVersionException: If the keystore contains an unknown format version number\n        :raises KeystoreSignatureException: If the keystore signature could not be verified using the supplied store password\n        :raises DuplicateAliasException: If the keystore contains duplicate aliases\n        \"\"\"\n        try:\n            pos = 0\n            version = b4.unpack_from(data, pos)[0]; pos += 4\n            if version not in [1,2]:\n                raise UnsupportedKeystoreVersionException(\"Unsupported BKS keystore version; only V1 and V2 supported, found v\"+repr(version))\n\n            salt, pos = cls._read_data(data, pos)\n            iteration_count = b4.unpack_from(data, pos)[0]; pos += 4\n\n            store_type = \"bks\"\n            entries, size = cls._load_bks_entries(data[pos:], store_type, store_password, try_decrypt_keys=try_decrypt_keys)\n\n            hmac_fn = hashlib.sha1\n            hmac_digest_size = hmac_fn().digest_size\n            hmac_key_size = hmac_digest_size*8 if version != 1 else hmac_digest_size\n            hmac_key = rfc7292.derive_key(hmac_fn, rfc7292.PURPOSE_MAC_MATERIAL, store_password, salt, iteration_count, hmac_key_size//8)\n\n            store_data = data[pos:pos+size]\n            store_hmac = data[pos+size:pos+size+hmac_digest_size]\n            if len(store_hmac) != hmac_digest_size:\n                raise BadKeystoreFormatException(\"Bad HMAC size; found %d bytes, expected %d bytes\" % (len(store_hmac), hmac_digest_size))\n\n            hmac = HMAC.new(hmac_key, digestmod=SHA)\n            hmac.update(store_data)\n\n            computed_hmac = hmac.digest()\n            if store_hmac != computed_hmac:\n                raise KeystoreSignatureException(\"Hash mismatch; incorrect keystore password?\")\n            return cls(store_type, entries, version=version)\n\n        except struct.error as e:\n            raise BadKeystoreFormatException(e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_bks_key(cls, data, pos, store_type):\n        key_type = b1.unpack_from(data, pos)[0]; pos += 1\n        key_format, pos = BksKeyStore._read_utf(data, pos, kind=\"key format\")\n        key_algorithm, pos = BksKeyStore._read_utf(data, pos, kind=\"key algorithm\")\n        key_enc, pos = BksKeyStore._read_data(data, pos)\n\n        entry = BksKeyEntry(key_type, key_format, key_algorithm, key_enc, store_type=store_type)\n        return entry, pos", "response": "Given a data stream attempt to parse a stored BKS key entry at the given position and return it as a BksKeyEntry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(cls, data, store_password, try_decrypt_keys=True):\n        # Uber keystores contain the same entry data as BKS keystores, except they wrap it differently:\n        #    BKS  = BKS_store || HMAC-SHA1(BKS_store)\n        #    UBER = PBEWithSHAAndTwofish-CBC(BKS_store || SHA1(BKS_store))\n        #\n        # where BKS_store represents the entry format shared by both keystore types.\n        #\n        # The Twofish key size is 256 bits, the PBE key derivation scheme is that as outlined by PKCS#12 (RFC 7292),\n        # and the padding scheme for the Twofish cipher is PKCS#7.\n        try:\n            pos = 0\n            version = b4.unpack_from(data, pos)[0]; pos += 4\n            if version != 1:\n                raise UnsupportedKeystoreVersionException('Unsupported UBER keystore version; only v1 supported, found v'+repr(version))\n\n            salt, pos = cls._read_data(data, pos)\n            iteration_count = b4.unpack_from(data, pos)[0]; pos += 4\n\n            encrypted_bks_store = data[pos:]\n            try:\n                decrypted = rfc7292.decrypt_PBEWithSHAAndTwofishCBC(encrypted_bks_store, store_password, salt, iteration_count)\n            except BadDataLengthException as e:\n                raise BadKeystoreFormatException(\"Bad UBER keystore format: %s\" % str(e))\n            except BadPaddingException as e:\n                raise DecryptionFailureException(\"Failed to decrypt UBER keystore: bad password?\")\n\n            # Note: we can assume that the hash must be present at the last 20 bytes of the decrypted data (i.e. without first\n            # parsing through to see where the entry data actually ends), because valid UBER keystores generators should not put\n            # any trailing bytes after the hash prior to encrypting.\n            hash_fn = hashlib.sha1\n            hash_digest_size = hash_fn().digest_size\n\n            bks_store = decrypted[:-hash_digest_size]\n            bks_hash  = decrypted[-hash_digest_size:]\n            if len(bks_hash) != hash_digest_size:\n                raise BadKeystoreFormatException(\"Insufficient signature bytes; found %d bytes, expected %d bytes\" % (len(bks_hash), hash_digest_size))\n            if hash_fn(bks_store).digest() != bks_hash:\n                raise KeystoreSignatureException(\"Hash mismatch; incorrect keystore password?\")\n\n            store_type = \"uber\"\n            entries, size = cls._load_bks_entries(bks_store, store_type, store_password, try_decrypt_keys=try_decrypt_keys)\n            return cls(store_type, entries, version=version)\n\n        except struct.error as e:\n            raise BadKeystoreFormatException(e)", "response": "Loads a keystore from a byte string representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the number of columns and rows required to divide an image into n parts.", "response": "def calc_columns_rows(n):\n    \"\"\"\n    Calculate the number of columns and rows required to divide an image\n    into ``n`` parts.\n\n    Return a tuple of integers in the format (num_columns, num_rows)\n    \"\"\"\n    num_columns = int(ceil(sqrt(n)))\n    num_rows = int(ceil(n / float(num_columns)))\n    return (num_columns, num_rows)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_combined_size(tiles):\n    # TODO: Refactor calculating layout to avoid repetition.\n    columns, rows = calc_columns_rows(len(tiles))\n    tile_size = tiles[0].image.size\n    return (tile_size[0] * columns, tile_size[1] * rows)", "response": "Calculate combined size of tiles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting an image into a specified number of tiles.", "response": "def slice(filename, number_tiles=None, col=None, row=None, save=True):\n    \"\"\"\n    Split an image into a specified number of tiles.\n\n    Args:\n       filename (str):  The filename of the image to split.\n       number_tiles (int):  The number of tiles required.\n\n    Kwargs:\n       save (bool): Whether or not to save tiles to disk.\n\n    Returns:\n        Tuple of :class:`Tile` instances.\n    \"\"\"\n    im = Image.open(filename)\n    im_w, im_h = im.size\n\n    columns = 0\n    rows = 0\n    if not number_tiles is None:\n        validate_image(im, number_tiles)\n        columns, rows = calc_columns_rows(number_tiles)\n        extras = (columns * rows) - number_tiles\n    else:\n        validate_image_col_row(im, col, row)\n        columns = col\n        rows = row\n        extras = (columns * rows) - number_tiles\n\n\n    tile_w, tile_h = int(floor(im_w / columns)), int(floor(im_h / rows))\n\n    tiles = []\n    number = 1\n    for pos_y in range(0, im_h - rows, tile_h): # -rows for rounding error.\n        for pos_x in range(0, im_w - columns, tile_w): # as above.\n            area = (pos_x, pos_y, pos_x + tile_w, pos_y + tile_h)\n            image = im.crop(area)\n            position = (int(floor(pos_x / tile_w)) + 1,\n                        int(floor(pos_y / tile_h)) + 1)\n            coords = (pos_x, pos_y)\n            tile = Tile(image, number, position, coords)\n            tiles.append(tile)\n            number += 1\n    if save:\n        save_tiles(tiles,\n                   prefix=get_basename(filename),\n                   directory=os.path.dirname(filename))\n    return tuple(tiles)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites image files to disk. Create specified folder if they don t exist.", "response": "def save_tiles(tiles, prefix='', directory=os.getcwd(), format='png'):\n    \"\"\"\n    Write image files to disk. Create specified folder(s) if they\n       don't exist. Return list of :class:`Tile` instance.\n\n    Args:\n       tiles (list):  List, tuple or set of :class:`Tile` objects to save.\n       prefix (str):  Filename prefix of saved tiles.\n\n    Kwargs:\n       directory (str):  Directory to save tiles. Created if non-existant.\n\n    Returns:\n        Tuple of :class:`Tile` instances.\n    \"\"\"\n#    Causes problems in CLI script.\n#    if not os.path.exists(directory):\n#        os.makedirs(directory)\n    for tile in tiles:\n        tile.save(filename=tile.generate_filename(prefix=prefix,\n                                                  directory=directory,\n                                                  format=format), \n                                                  format=format)\n    return tuple(tiles)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_filename(self, directory=os.getcwd(), prefix='tile',\n                          format='png', path=True):\n        \"\"\"Construct and return a filename for this tile.\"\"\"\n        filename = prefix + '_{col:02d}_{row:02d}.{ext}'.format(\n                      col=self.column, row=self.row, ext=format.lower().replace('jpeg', 'jpg'))\n        if not path:\n            return filename\n        return os.path.join(directory, filename)", "response": "Construct and return a filename for this tile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_columns_rows(filenames):\n    tiles = []\n    for filename in filenames:\n        row, column = os.path.splitext(filename)[0][-5:].split('_')\n        tiles.append((int(row), int(column)))\n    rows = [pos[0] for pos in tiles]; columns = [pos[1] for pos in tiles]\n    num_rows = max(rows); num_columns = max(columns)\n    return (num_columns, num_rows)", "response": "Derive number of columns and rows from filenames."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consolidate_reverse_exprs(initial_state):\n    initial_state.inspect.b('mem_read', when=angr.BP_AFTER, action=_read_consolidate)\n    initial_state.inspect.b('reg_read', when=angr.BP_AFTER, action=_read_consolidate)", "response": "Consolidates the expressions in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_signed_range(se, expr):\n    size = expr.size()\n    umin = umax = smin = smax = None\n    if not sat_zero(se, expr):\n        try: \n            umin = se.min(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 0])\n            umax = se.max(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 0])\n            return (umin, umax)\n        except:\n            pass\n        try: \n            smin = -(1 << size) + se.min(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 1])\n            smax = -(1 << size) + se.max(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 1])\n            return (smin, smax)\n        except:\n            pass\n        return None\n    else:\n        try: \n            umax = se.max(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 0])\n            smin = 0\n            try:\n                smin = -(1 << size) + se.min(expr, extra_constraints=[claripy.Extract(size-1,size-1,expr) == 1])\n            except:\n                pass\n            return (smin, umax)\n        except:\n            pass\n\n        return None", "response": "Calculate the range of the expression with signed boundaries"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fn_check_full(fn):\n    status = True \n    if not os.path.isfile(fn): \n        status = False\n    else:\n        try: \n            open(fn) \n        except IOError:\n            status = False\n    return status", "response": "Check if a file exists and is a valid entry point."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps around gdal. Open", "response": "def fn_getds(fn):\n    \"\"\"Wrapper around gdal.Open()\n    \"\"\"\n    ds = None\n    if fn_check(fn):\n        ds = gdal.Open(fn, gdal.GA_ReadOnly)\n    else:\n        print(\"Unable to find %s\" % fn)\n    return ds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget masked array from input filename", "response": "def fn_getma(fn, bnum=1, return_ds=False):\n    \"\"\"Get masked array from input filename\n\n    Parameters\n    ----------\n    fn : str\n        Input filename string\n    bnum : int, optional\n        Band number\n    \n    Returns\n    -------\n    np.ma.array    \n        Masked array containing raster values\n    \"\"\"\n    #Add check for filename existence\n    ds = fn_getds(fn)\n    out = ds_getma(ds, bnum=bnum)\n    if return_ds:\n        out = (out, ds)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets masked array from input GDAL Band linkbase", "response": "def b_getma(b):\n    \"\"\"Get masked array from input GDAL Band\n\n    Parameters\n    ----------\n    b : gdal.Band \n        Input GDAL Band \n    \n    Returns\n    -------\n    np.ma.array    \n        Masked array containing raster values\n    \"\"\"\n    b_ndv = get_ndv_b(b)\n    #bma = np.ma.masked_equal(b.ReadAsArray(), b_ndv)\n    #This is more appropriate for float, handles precision issues\n    bma = np.ma.masked_values(b.ReadAsArray(), b_ndv)\n    return bma"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sub_dim(src_ds, scale=None, maxdim=1024):\n    ns = src_ds.RasterXSize\n    nl = src_ds.RasterYSize\n    maxdim = float(maxdim)\n    if scale is None:\n        scale_ns = ns/maxdim\n        scale_nl = nl/maxdim\n        scale = max(scale_ns, scale_nl)\n    #Need to check to make sure scale is positive real \n    if scale > 1:\n        ns = int(round(ns/scale))\n        nl = int(round(nl/scale))\n    return ns, nl, scale", "response": "Compute the dimensions of a subsampled dataset \n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a subsampled array rather than full resolution.", "response": "def ds_getma_sub(src_ds, bnum=1, scale=None, maxdim=1024., return_ds=False):    \n    \"\"\"Load a subsampled array, rather than full resolution\n\n    This is useful when working with large rasters\n\n    Uses buf_xsize and buf_ysize options from GDAL ReadAsArray method.\n\n    Parameters\n    ----------\n    ds : gdal.Dataset \n        Input GDAL Datset\n    bnum : int, optional\n        Band number\n    scale : int, optional\n        Scaling factor\n    maxdim : int, optional \n        Maximum dimension along either axis, in pixels\n    \n    Returns\n    -------\n    np.ma.array    \n        Masked array containing raster values\n    \"\"\"\n    #print src_ds.GetFileList()[0]\n    b = src_ds.GetRasterBand(bnum)\n    b_ndv = get_ndv_b(b)\n    ns, nl, scale = get_sub_dim(src_ds, scale, maxdim)\n    #The buf_size parameters determine the final array dimensions\n    b_array = b.ReadAsArray(buf_xsize=ns, buf_ysize=nl)\n    bma = np.ma.masked_values(b_array, b_ndv)\n    out = bma\n    if return_ds:\n        dtype = src_ds.GetRasterBand(1).DataType\n        src_ds_sub = gdal.GetDriverByName('MEM').Create('', ns, nl, 1, dtype)\n        gt = np.array(src_ds.GetGeoTransform())\n        gt[[1,5]] = gt[[1,5]]*scale\n        src_ds_sub.SetGeoTransform(list(gt))\n        src_ds_sub.SetProjection(src_ds.GetProjection())\n        b = src_ds_sub.GetRasterBand(1)\n        b.WriteArray(bma)\n        b.SetNoDataValue(b_ndv)\n        out = (bma, src_ds_sub)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef writeGTiff(a, dst_fn, src_ds=None, bnum=1, ndv=None, gt=None, proj=None, create=False, sparse=False):\n    #If input is not np.ma, this creates a new ma, which has default filL_value of 1E20\n    #Must manually override with ndv\n    #Also consumes a lot of memory\n    #Should bypass if input is bool\n    from pygeotools.lib.malib import checkma \n    a = checkma(a, fix=False)\n    #Want to preserve fill_value if already specified\n    if ndv is not None:\n        a.set_fill_value(ndv)\n    driver = gtif_drv\n    #Currently only support writing singleband rasters\n    #if a.ndim > 2:\n    #   np_nbands = a.shape[2]\n    #   if src_ds.RasterCount np_nbands: \n    #      for bnum in np_nbands:\n    nbands = 1\n    np_dt = a.dtype.name\n    if src_ds is not None:\n        #If this is a fn, get a ds\n        #Note: this saves a lot of unnecessary iolib.fn_getds calls\n        if isinstance(src_ds, str):\n            src_ds = fn_getds(src_ds)\n        #if isinstance(src_ds, gdal.Dataset):\n        src_dt = gdal.GetDataTypeName(src_ds.GetRasterBand(bnum).DataType)\n        src_gt = src_ds.GetGeoTransform()\n        #This is WKT\n        src_proj = src_ds.GetProjection()\n        #src_srs = osr.SpatialReference()  \n        #src_srs.ImportFromWkt(src_ds.GetProjectionRef())\n\n    #Probably a cleaner way to handle this\n    if gt is None:\n        gt = src_gt\n    if proj is None:\n        proj = src_proj\n\n    #Need to create a new copy of the default options\n    opt = list(gdal_opt)\n    \n    #Note: packbits is better for sparse data\n    if sparse:\n        opt.remove('COMPRESS=LZW')\n        opt.append('COMPRESS=PACKBITS')\n        #Not sure if VW can handle sparse tif\n        #opt.append('SPARSE_OK=TRUE')\n\n    #Use predictor=3 for floating point data\n    if 'float' in np_dt.lower() and 'COMPRESS=LZW' in opt: \n        opt.append('PREDICTOR=3')\n\n    #If input ma is same as src_ds, write out array using CreateCopy from existing dataset\n    #if not create and (src_ds is not None) and ((a.shape[0] == src_ds.RasterYSize) and (a.shape[1] == src_ds.RasterXSize) and (np_dt.lower() == src_dt.lower())): \n    #Should compare srs.IsSame(src_srs)\n    if not create and (src_ds is not None) and ((a.shape[0] == src_ds.RasterYSize) and (a.shape[1] == src_ds.RasterXSize) and (np_dt.lower() == src_dt.lower())) and (src_gt == gt) and (src_proj == proj):\n        #Note: third option is strict flag, set to false\n        dst_ds = driver.CreateCopy(dst_fn, src_ds, 0, options=opt)\n    #Otherwise, use Create\n    else:\n        a_dtype = a.dtype\n        gdal_dtype = np2gdal_dtype(a_dtype)\n        if a_dtype.name == 'bool':\n            #Set ndv to 0\n            a.fill_value = False\n            opt.remove('COMPRESS=LZW')\n            opt.append('COMPRESS=DEFLATE')\n            #opt.append('NBITS=1')\n        #Create(fn, nx, ny, nbands, dtype, opt)\n        dst_ds = driver.Create(dst_fn, a.shape[1], a.shape[0], nbands, gdal_dtype, options=opt)\n        #Note: Need GeoMA here to make this work, or accept gt as argument\n        #Could also do ds creation in calling script\n        if gt is not None:\n            dst_ds.SetGeoTransform(gt)\n        if proj is not None:\n            dst_ds.SetProjection(proj)\n    \n    dst_ds.GetRasterBand(bnum).WriteArray(a.filled())\n    dst_ds.GetRasterBand(bnum).SetNoDataValue(float(a.fill_value))\n    dst_ds = None", "response": "Write array to disk as GeoTiff."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef writevrt(out_csv,srs='EPSG:4326',x='field_1',y='field_2'):\n    out_vrt = os.path.splitext(out_csv)[0]+'.vrt'\n    out_csv = os.path.split(out_csv)[-1]\n    f = open(out_vrt, 'w')\n    f.write('<OGRVRTDataSource>\\n')\n    f.write('   <OGRVRTLayer name=\"%s\">\\n' % os.path.splitext(out_csv)[0])\n    f.write('        <SrcDataSource>%s</SrcDataSource>\\n' % out_csv)\n    f.write('        <GeometryType>wkbPoint</GeometryType>\\n')\n    f.write('        <LayerSRS>%s</LayerSRS>\\n' % srs)\n    f.write('        <GeometryField encoding=\"PointFromColumns\" x=\"%s\" y=\"%s\"/>\\n' % (x, y))\n    f.write('    </OGRVRTLayer>\\n')\n    f.write('</OGRVRTDataSource>\\n')\n    f.close()", "response": "Write out a vrt to accompany a csv of points"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef np2gdal_dtype(d):\n    dt_dict = gdal_array.codes        \n    if isinstance(d, (np.ndarray, np.generic)):\n        d = d.dtype\n    #This creates dtype from another built-in type\n    #d = np.dtype(d)\n    if isinstance(d, np.dtype):\n        if d.name == 'int8':\n            gdal_dt = 1\n        elif d.name == 'bool':\n            #Write out as Byte\n            gdal_dt = 1 \n        else:\n            gdal_dt = list(dt_dict.keys())[list(dt_dict.values()).index(d)]\n    else:\n        print(\"Input must be NumPy array or NumPy dtype\")\n        gdal_dt = None\n    return gdal_dt", "response": "Get GDAL RasterBand datatype that corresponds with NumPy datatype"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets NumPy datatype that corresponds with GDAL RasterBand datatype Input can be filename GDAL Dataset GDAL RasterBand or GDAL integer dtype", "response": "def gdal2np_dtype(b):\n    \"\"\"\n    Get NumPy datatype that corresponds with GDAL RasterBand datatype\n    Input can be filename, GDAL Dataset, GDAL RasterBand, or GDAL integer dtype\n    \"\"\"\n    dt_dict = gdal_array.codes\n    if isinstance(b, str):\n        b = gdal.Open(b)\n    if isinstance(b, gdal.Dataset):\n        b = b.GetRasterBand(1)\n    if isinstance(b, gdal.Band):\n        b = b.DataType\n    if isinstance(b, int):\n        np_dtype = dt_dict[b]\n    else:\n        np_dtype = None\n        print(\"Input must be GDAL Dataset or RasterBand object\")\n    return np_dtype"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ndv_b(b):\n\n    b_ndv = b.GetNoDataValue()\n    if b_ndv is None:\n        #Check ul pixel for ndv\n        ns = b.XSize\n        nl = b.YSize\n        ul = float(b.ReadAsArray(0, 0, 1, 1))\n        #ur = float(b.ReadAsArray(ns-1, 0, 1, 1))\n        lr = float(b.ReadAsArray(ns-1, nl-1, 1, 1))\n        #ll = float(b.ReadAsArray(0, nl-1, 1, 1))\n        #Probably better to use 3/4 corner criterion\n        #if ul == ur == lr == ll:\n        if np.isnan(ul) or ul == lr:\n            b_ndv = ul\n        else:\n            #Assume ndv is 0\n            b_ndv = 0\n    elif np.isnan(b_ndv):\n        b_dt = gdal.GetDataTypeName(b.DataType)\n        if 'Float' in b_dt:\n            b_ndv = np.nan\n        else:\n            b_ndv = 0\n    return b_ndv", "response": "Get NoData value for GDALRasterBand object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn system CPU count", "response": "def cpu_count(logical=True):\n    \"\"\"Return system CPU count\n    \"\"\"\n    if logical:\n        from multiprocessing import cpu_count\n        ncpu=cpu_count()\n    else:\n        import psutil\n        ncpu=psutil.cpu_count(logical=False)\n    return ncpu"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getfile(url, outdir=None):\n    fn = os.path.split(url)[-1]\n    if outdir is not None:\n        fn = os.path.join(outdir, fn)\n    if not os.path.exists(fn):\n        #Find appropriate urlretrieve for Python 2 and 3\n        try:\n            from urllib.request import urlretrieve\n        except ImportError:\n            from urllib import urlretrieve \n        print(\"Retrieving: %s\" % url)\n        #Add progress bar\n        urlretrieve(url, fn)\n    return fn", "response": "Function to fetch a file using urllib\n    Works with ftp\n    Works with urllib\n    Works with urllib\n    Works with urllib\n    Works with ftp\n    Works with urllib\n    Works with urllib\n    Works with ftp\n    Works with urllib\n    Works with urllib\n    Works with ftp\n    Works with urllib\n    Works with ftp\n    Works with urllib\n    Works with ftp\n"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to fetch files using requests", "response": "def getfile2(url, auth=None, outdir=None):\n    \"\"\"Function to fetch files using requests\n\n    Works with https authentication\n\n    \"\"\"\n    import requests\n    print(\"Retrieving: %s\" % url)\n    fn = os.path.split(url)[-1]\n    if outdir is not None:\n        fn = os.path.join(outdir, fn)\n    if auth is not None:\n        r = requests.get(url, stream=True, auth=auth)\n    else:\n        r = requests.get(url, stream=True)\n    chunk_size = 1000000\n    with open(fn, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size):\n            fd.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_auth():\n    import getpass\n    from requests.auth import HTTPDigestAuth\n    #This binds raw_input to input for Python 2\n    input_func = input\n    try:\n        input_func = raw_input\n    except NameError:\n        pass\n    uname = input_func(\"MODSCAG Username:\")\n    pw = getpass.getpass(\"MODSCAG Password:\")\n    auth = HTTPDigestAuth(uname, pw)\n    #wget -A'h8v4*snow_fraction.tif' --user=uname --password=pw\n    return auth", "response": "Get authorization token for https\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readcsv(fn):\n    import csv\n    #Check first line for header\n    with open(fn, 'r') as f:\n        reader = csv.DictReader(f)\n        hdr = reader.fieldnames\n\n    #Assume there is a header on first line, check \n    skiprows = 1\n    if np.all(f.isdigit() for f in hdr):\n        hdr = None\n        skiprows = 0\n\n    #Check header for lat/lon/z or x/y/z tags\n\n    #Should probably do genfromtxt here if header exists and dtype of cols is variable\n    pts = np.loadtxt(fn, delimiter=',', skiprows=skiprows, dtype=None)\n    return pts", "response": "Wrapper to read arbitrary csv file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef range_fltr(dem, rangelim):\n    print('Excluding values outside of range: {0:f} to {1:f}'.format(*rangelim))\n    out = np.ma.masked_outside(dem, *rangelim)\n    out.set_fill_value(dem.fill_value)\n    return out", "response": "Range filter (helper function)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef perc_fltr(dem, perc=(1.0, 99.0)):\n    rangelim = malib.calcperc(dem, perc)\n    print('Excluding values outside of percentile range: {0:0.2f} to {1:0.2f}'.format(*perc))\n    out = range_fltr(dem, rangelim)\n    return out", "response": "Percentile filter for a single sample"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sigma_fltr(dem, n=3):\n    std = dem.std()\n    u = dem.mean()\n    print('Excluding values outside of range: {1:0.2f} +/- {0}*{2:0.2f}'.format(n, u, std))\n    rangelim = (u - n*std, u + n*std)\n    out = range_fltr(dem, rangelim)\n    return out", "response": "sigma_fltr - Returns the sigma of the input array"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mad_fltr(dem, n=3):\n    mad, med = malib.mad(dem, return_med=True)\n    print('Excluding values outside of range: {1:0.3f} +/- {0}*{2:0.3f}'.format(n, med, mad))\n    rangelim = (med - n*mad, med + n*mad)\n    out = range_fltr(dem, rangelim)\n    return out", "response": "Returns a list of MADs in a Malib MAD."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gauss_fltr_astropy(dem, size=None, sigma=None, origmask=False, fill_interior=False):\n\n    #import astropy.nddata\n    import astropy.convolution\n    dem = malib.checkma(dem)\n    #Generate 2D gaussian kernel for input sigma and size\n    #Default size is 8*sigma in x and y directions\n    #kernel = astropy.nddata.make_kernel([size, size], sigma, 'gaussian')\n    #Size must be odd\n    if size is not None:\n        size = int(np.floor(size/2)*2 + 1)\n        size = max(size, 3)\n    #Truncate the filter at this many standard deviations. Default is 4.0\n    truncate = 3.0\n    if size is not None and sigma is None:\n        sigma = (size - 1) / (2*truncate)\n    elif size is None and sigma is not None:\n        #Round up to nearest odd int\n        size = int(np.ceil((sigma * (2*truncate) + 1)/2)*2 - 1)\n    elif size is None and sigma is None:\n        #Use default parameters\n        sigma = 1\n        size = int(np.ceil((sigma * (2*truncate) + 1)/2)*2 - 1)\n    size = max(size, 3)\n    kernel = astropy.convolution.Gaussian2DKernel(sigma, x_size=size, y_size=size, mode='oversample')\n\n    print(\"Applying gaussian smoothing filter with size %i and sigma %0.3f (sum %0.3f)\" % \\\n            (size, sigma, kernel.array.sum()))\n\n    #This will fill holes\n    #np.nan is float\n    #dem_filt_gauss = astropy.nddata.convolve(dem.astype(float).filled(np.nan), kernel, boundary='fill', fill_value=np.nan)\n    #dem_filt_gauss = astropy.convolution.convolve(dem.astype(float).filled(np.nan), kernel, boundary='fill', fill_value=np.nan)\n    #Added normalization to ensure filtered values are not brightened/darkened if kernelsum != 1\n    dem_filt_gauss = astropy.convolution.convolve(dem.astype(float).filled(np.nan), kernel, boundary='fill', fill_value=np.nan, normalize_kernel=True)\n    #This will preserve original ndv pixels, applying original mask after filtering\n    if origmask:\n        print(\"Applying original mask\")\n        #Allow filling of interior holes, but use original outer edge\n        if fill_interior:\n            mask = malib.maskfill(dem)\n        else:\n            mask = dem.mask\n        dem_filt_gauss = np.ma.array(dem_filt_gauss, mask=mask, fill_value=dem.fill_value)\n    out = np.ma.fix_invalid(dem_filt_gauss, copy=False, fill_value=dem.fill_value)\n    out.set_fill_value(dem.fill_value.astype(dem.dtype))\n    return out.astype(dem.dtype)", "response": "Astropy gaussian filter properly handles convolution with NaNs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gauss_fltr_pyramid(dem, size=None, full=False, origmask=False):\n    dem = malib.checkma(dem)\n    levels = int(np.floor(np.log2(size)))\n    #print levels\n    dim = np.floor(np.array(dem.shape)/float(2**levels) + 1)*(2**levels)\n    #print dem.shape\n    #print dim\n    #Can do something with np.pad here\n    #np.pad(a_fp.filled(), 1, mode='constant', constant_values=(a_fp.fill_value,))\n    dem2 = np.full(dim, dem.fill_value)\n    offset = (dim - np.array(dem.shape))/2.0\n    #print offset\n    #dem2[0:dem.shape[0],0:dem.shape[1]] = dem.data \n    dem2[offset[0]:dem.shape[0]+offset[0],offset[1]:dem.shape[1]+offset[1]] = dem.data \n    dem2 = np.ma.masked_equal(dem2, dem.fill_value)\n    #dem2 = dem\n    for n in range(levels):\n        print(dem2.shape)\n        dim = (np.floor(np.array(dem2.shape)/2.0 + 1)*2).astype(int)\n        #dem2 = gauss_fltr_astropy(dem2, size=5, origmask=origmask)\n        #dem2 = gauss_fltr_astropy(dem2, size=5)\n        dem2 = gauss_fltr_astropy(dem2, size=5)\n        #Note: Should use zoom with same bilinear interpolation here for consistency\n        #However, this doesn't respect nan\n        #dem2 = zoom(dem2, 0.5, order=1, prefilter=False, cval=dem.fill_value)\n        dem2 = dem2[::2,::2]\n    if full:\n        print(\"Resizing to original input dimensions\")\n        from scipy.ndimage import zoom\n        for n in range(levels):\n            print(dem2.shape)\n            #Note: order 1 is bilinear\n            dem2 = zoom(dem2, 2, order=1, prefilter=False, cval=dem.fill_value)\n        #dem2 = zoom(dem2, 2**levels, order=1, prefilter=False, cval=dem2.fill_value)\n        print(dem2.shape)\n        #This was for power of 2 offset\n        #offset = (2**levels)/2\n        #print offset\n        #dem2 = dem2[offset:dem.shape[0]+offset,offset:dem.shape[1]+offset]\n        #Use original offset\n        dem2 = dem2[offset[0]:dem.shape[0]+offset[0],offset[1]:dem.shape[1]+offset[1]]\n        if origmask:\n            print(\"Applying original mask\")\n            #Allow filling of interior holes, but use original outer edge\n            maskfill = malib.maskfill(dem)\n            #dem2 = np.ma.array(dem2, mask=np.ma.getmaskarray(dem))\n            dem2 = np.ma.array(dem2, mask=maskfill, fill_value=dem.fill_value)\n    return dem2", "response": "Pyaramidal downsampling approach for large kernels very fast"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gauss_fltr_opencv(dem, size=3, sigma=1):\n    import cv2\n    dem = malib.checkma(dem)\n    dem_cv = cv2.GaussianBlur(dem.filled(np.nan), (size, size), sigma)\n    out = np.ma.fix_invalid(dem_cv)\n    out.set_fill_value(dem.fill_value)\n    return out", "response": "OpenCV Gaussian filter\n    Still propagates NaN values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gaussfill(dem, size=3, newmask=None):\n    smooth = gauss_fltr_astropy(dem, size=size)\n    smooth[~dem.mask] = dem[~dem.mask]\n    if newmask is not None:\n        smooth = np.ma.array(smooth, mask=newmask)\n    return smooth", "response": "Gaussian filter with filling\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a circular mask for an array Useful when sampling rasters for a shot shot.", "response": "def circular_mask(size):\n    \"\"\"Create a circular mask for an array\n    \n    Useful when sampling rasters for a laser shot\n    \"\"\"\n    r = size/2\n    c = (r,r)\n    y,x = np.ogrid[-c[0]:size-c[0], -c[1]:size-c[1]]\n    mask = ~(x*x + y*y <= r*r)\n    return mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef median_fltr_skimage(dem, radius=3, erode=1, origmask=False):\n    #Note, ndimage doesn't properly handle ma - convert to nan\n    dem = malib.checkma(dem)\n    dem = dem.astype(np.float64)\n    #Mask islands\n    if erode > 0:\n        print(\"Eroding islands smaller than %s pixels\" % (erode * 2)) \n        dem = malib.mask_islands(dem, iterations=erode)\n    print(\"Applying median filter with radius %s\" % radius) \n    #Note: this funcitonality was present in scikit-image 0.9.3\n    import skimage.filter\n    dem_filt_med = skimage.filter.median_filter(dem, radius, mask=~dem.mask)\n    #Starting in version 0.10.0, this is the new filter\n    #This is the new filter, but only supports uint8 or unit16\n    #import skimage.filters\n    #import skimage.morphology \n    #dem_filt_med = skimage.filters.rank.median(dem, disk(radius), mask=~dem.mask)\n    #dem_filt_med = skimage.filters.median(dem, skimage.morphology.disk(radius), mask=~dem.mask)\n    #Now mask all nans\n    #skimage assigns the minimum value as nodata\n    #CHECK THIS, seems pretty hacky\n    #Also, looks like some valid values are masked at this stage, even though they should be above min\n    ndv = np.min(dem_filt_med)\n    #ndv = dem_filt_med.min() + 0.001\n    out = np.ma.masked_less_equal(dem_filt_med, ndv)\n    #Should probably replace the ndv with original ndv\n    out.set_fill_value(dem.fill_value)\n    if origmask:\n        print(\"Applying original mask\")\n        #Allow filling of interior holes, but use original outer edge\n        #maskfill = malib.maskfill(dem, iterations=radius)\n        maskfill = malib.maskfill(dem)\n        #dem_filt_gauss = np.ma.array(dem_filt_gauss, mask=dem.mask, fill_value=dem.fill_value)\n        out = np.ma.array(out, mask=maskfill, fill_value=dem.fill_value)\n    return out", "response": "Returns a new filter that can be applied to a single skimage object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uniform_fltr(dem, fsize=7):\n    print(\"Applying uniform filter with size %s\" % fsize)\n    #Note, ndimage doesn't properly handle ma - convert to nan\n    from scipy.ndimage.filters import unifiform_filter\n    dem_filt_med = uniform_filter(dem.filled(np.nan), fsize)\n    #Now mask all nans\n    out = np.ma.fix_invalid(dem_filt_med, copy=False, fill_value=dem.fill_value)\n    out.set_fill_value(dem.fill_value)\n    return out", "response": "Uniform (mean) filter\n    \n    Note: suffers from significant ringing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nerode pixels near nodata", "response": "def erode_edge(dem, iterations=1):\n    \"\"\"Erode pixels near nodata\n    \"\"\"\n    import scipy.ndimage as ndimage \n    print('Eroding pixels near nodata: %i iterations' % iterations)\n    mask = np.ma.getmaskarray(dem)\n    mask_dilate = ndimage.morphology.binary_dilation(mask, iterations=iterations)\n    out = np.ma.array(dem, mask=mask_dilate)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns Gaussian smoothing on an astropy stack object", "response": "def stack_smooth(s_orig, size=7, save=False):\n    \"\"\"Run Gaussian smoothing filter on exising stack object\n    \"\"\"\n    from copy import deepcopy\n    from pygeotools.lib import filtlib\n    print(\"Copying original DEMStack\")\n    s = deepcopy(s_orig)\n    s.stack_fn = os.path.splitext(s_orig.stack_fn)[0]+'_smooth%ipx.npz' % size\n\n    #Loop through each array and smooth\n    print(\"Smoothing all arrays in stack with %i px gaussian filter\" % size)\n    for i in range(s.ma_stack.shape[0]):\n        print('%i of %i' % (i+1, s.ma_stack.shape[0]))\n        s.ma_stack[i] = filtlib.gauss_fltr_astropy(s.ma_stack[i], size=size)\n\n    if s.stats:\n        s.compute_stats()\n        if save:\n            s.write_stats()\n    #Update datestack\n    if s.datestack and s.date_list_o.count() > 1:\n        s.compute_dt_stats()\n        if save:\n            s.write_datestack()\n    #Update trend \n    if s.trend:\n        s.compute_trend()\n        if save:\n            s.write_trend()\n    if save:\n        s.savestack()\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stack_clip(s_orig, extent, out_stack_fn=None, copy=True, save=False):\n    #Should check for valid extent\n\n    #This is not memory efficient, but is much simpler\n    #To be safe, if we are saving out, create a copy to avoid overwriting\n    if copy or save:\n        from copy import deepcopy\n        print(\"Copying original DEMStack\")\n        s = deepcopy(s_orig)\n    else:\n        #Want to be very careful here, as we could overwrite the original file\n        s = s_orig\n\n    from pygeotools.lib import geolib\n    gt = s.gt\n    s_shape = s.ma_stack.shape[1:3]\n\n    #Compute pixel bounds for input extent\n    min_x_px, max_y_px = geolib.mapToPixel(extent[0], extent[1], gt)\n    max_x_px, min_y_px = geolib.mapToPixel(extent[2], extent[3], gt)\n\n    #Clip to stack extent and round to whole integers\n    min_x_px = int(max(0, min_x_px)+0.5)\n    max_x_px = int(min(s_shape[1], max_x_px)+0.5)\n    min_y_px = int(max(0, min_y_px)+0.5)\n    max_y_px = int(min(s_shape[0], max_y_px)+0.5)\n  \n    #Clip the stack\n    x_slice = slice(min_x_px,max_x_px)\n    y_slice = slice(min_y_px,max_y_px)\n    s.ma_stack = s.ma_stack[:, y_slice, x_slice]\n\n    #Now update geospatial info\n    #This returns the pixel center in map coordinates\n    #Want to remove 0.5 px offset for upper left corner in gt\n    out_ul = geolib.pixelToMap(min_x_px - 0.5, min_y_px - 0.5, gt)\n   \n    #Update stack geotransform\n    s.gt[0] = out_ul[0]\n    s.gt[3] = out_ul[1]\n    #Update new stack extent\n    s.get_extent()\n\n    #Check for and discard emtpy arrays \n    #Might be faster to reshape then np.ma.count(s.ma_stack, axis=1)\n    count_list = np.array([i.count() for i in s.ma_stack])\n    idx = count_list > 0 \n    \n    #Output subset with valid data in next extent\n    #fn_list, source, error, error_dict_list, date_list, date_list_o\n    #Note: no need to copy again\n    s_sub = get_stack_subset(s, idx, out_stack_fn=out_stack_fn, copy=False, save=False) \n\n    print(\"Orig filename:\", s_orig.stack_fn)\n    print(\"Orig extent:\", s_orig.extent)\n    print(\"Orig dimensions:\", s_orig.ma_stack.shape)\n    print(\"Input extent:\", extent)\n    print(\"New filename:\", s_sub.stack_fn)\n    print(\"New extent:\", s_sub.extent)\n    print(\"New dimensions:\", s_sub.ma_stack.shape)\n    \n    if save:\n        if os.path.abspath(s_orig.stack_fn) == os.path.abspath(s_sub.stack_fn):\n            print(\"Original stack would be overwritten!\")\n            print(\"Skipping save\")\n        else:\n            s_sub.save = True\n            s_sub.savestack()\n\n    #The following should be unchanged by clip - it is more efficient to clip thes, but easier to regenerate\n    #if s.stats:\n    #stack_count, stack_mean, stack_min, stack_max, stack_std\n    #s.stack_min = s.stack_min[y_slice, x_slice]\n    #if s.datestack:\n    #dt_ptp, dt_min, dt_max, dt_center\n    #if s.med:\n    #stack_med\n    #if s.trend:\n    #trend, intercept, detrended_std\n    #Recompute stats/etc\n\n    return s_sub", "response": "Clip the stack object with limited extent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_stack_subset(s_orig, idx, out_stack_fn=None, copy=True, save=False):\n    #This must be a numpy boolean array\n    idx = np.array(idx)\n    if np.any(idx):\n        #This is not memory efficient, but is much simpler\n        #To be safe, if we are saving out, create a copy to avoid overwriting\n        if copy or save:\n            from copy import deepcopy\n            print(\"Copying original DEMStack\")\n            s = deepcopy(s_orig)\n        else:\n            #Want to be very careful here, as we could overwrite the original file\n            s = s_orig\n        #Update fn_list\n        #Note: need to change fn_list to np.array - object array, allows longer strings\n        #s.fn_list = s.fn_list[idx]\n        print(\"Original stack: %i\" % len(s_orig.fn_list))\n        s.fn_list = (np.array(s.fn_list)[idx]).tolist()\n        print(\"Filtered stack: %i\" % len(s.fn_list))\n        #Update date_lists\n        s.date_list = s.date_list[idx]\n        s.date_list_o = s.date_list_o[idx]\n        #Update ma\n        s.ma_stack = s.ma_stack[idx]\n        #Update source/error\n        #s.source = s.source[idx]\n        s.source = (np.array(s.source)[idx]).tolist()\n        s.error = s.error[idx]\n        s.error_dict_list = np.array(s.error_dict_list)[idx]\n        #Update stack_fn\n        #out_stack_fn should be full path, with npz\n        if out_stack_fn is None:\n            s.stack_fn = None\n            s.get_stack_fn()\n        else:\n            s.stack_fn = out_stack_fn\n        #Check to make sure we are not going to overwrite\n        if os.path.abspath(s_orig.stack_fn) == os.path.abspath(s.stack_fn):\n            print(\"Warning: new stack has identical filename: %s\" % s.stack_fn)\n            print(\"As a precaution, new stack will not be saved\")\n            save = False\n        s.save = save\n        #Update stats\n        if s.stats:\n            s.compute_stats()\n            if save:\n                s.write_stats()\n        #Update datestack\n        if s.datestack and s.date_list_o.count() > 1:\n            s.compute_dt_stats()\n            if save:\n                s.write_datestack()\n        #Update trend \n        if s.trend:\n            s.compute_trend()\n            if save:\n                s.write_trend()\n        if save:\n            s.savestack()\n    else:\n        print(\"No valid entries for input index array\")\n        s = None\n    return s", "response": "Create a new DEMStack object with a subset of an existing stack object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge two stack objects into one.", "response": "def stack_merge(s1, s2, out_stack_fn=None, sort=True, save=False):\n    \"\"\"Merge two stack objects\n    \"\"\"\n    from pygeotools.lib import geolib\n    from copy import deepcopy\n    #Assumes input stacks have identical extent, resolution, and projection\n    if s1.ma_stack.shape[1:3] != s2.ma_stack.shape[1:3]:\n        print(s1.ma_stack.shape)\n        print(s2.ma_stack.shape)\n        sys.exit('Input stacks must have identical array dimensions')\n    if not geolib.extent_compare(s1.extent, s2.extent):\n        print(s1.extent)\n        print(s2.extent)\n        sys.exit('Input stacks must have identical extent')\n    if not geolib.res_compare(s1.res, s2.res):\n        print(s1.res)\n        print(s2.res)\n        sys.exit('Input stacks must have identical res')\n\n    print(\"\\nCombining fn_list and ma_stack\")\n    fn_list = np.array(s1.fn_list + s2.fn_list)\n\n    if sort:\n        #Sort based on filenames (should be datesort)\n        sort_idx = np.argsort([os.path.split(x)[-1] for x in fn_list])\n    else:\n        sort_idx = Ellipsis\n\n    #Now pull out final, sorted order\n    fn_list = fn_list[sort_idx]\n    ma_stack = np.ma.vstack((s1.ma_stack, s2.ma_stack))[sort_idx]\n    #date_list = np.ma.dstack(s1.date_list, s2.date_list)\n    #date_list_o = np.ma.dstack(s1.date_list_o, s2.date_list_o)\n    source = np.array(s1.source + s2.source)[sort_idx]\n    error = np.ma.concatenate([s1.error, s2.error])[sort_idx]\n    #These are object arrays\n    error_dict_list = np.concatenate([s1.error_dict_list, s2.error_dict_list])[sort_idx]\n\n    print(\"Creating copy for new stack\")\n    s = deepcopy(s1)\n    s.fn_list = list(fn_list)\n    s.ma_stack = ma_stack\n    s.source = list(source)\n    s.error = error\n    s.error_dict_list = error_dict_list\n    #This will use original stack outdir\n    if not out_stack_fn:\n        s.get_stack_fn()\n    else:\n        s.stack_fn = out_stack_fn\n    s.get_date_list()\n\n    #These will preserve trend from one stack if present in only one stack\n    #Useful when combining surface topo and bed topo\n    if s1.datestack and s2.datestack:\n        s.compute_dt_stats()\n    if save and s1.datestack:\n        s.write_datestack()\n\n    if s1.stats and s2.stats:\n        s.compute_stats()\n    if save and s1.stats:\n        s.write_stats()\n\n    if s1.trend and s2.trend:\n        s.compute_trend()\n    if save and s1.trend:\n        s.write_trend()\n\n    if save:\n        s.savestack()\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ma_linreg(ma_stack, dt_list, n_thresh=2, model='linear', dt_stack_ptp=None, min_dt_ptp=None, smooth=False, \\\n        rsq=False, conf_test=False, parallel=True, n_cpu=None, remove_outliers=False):\n    \"\"\"Compute per-pixel linear regression for stack object\n    \"\"\"\n    #Need to check type of input dt_list\n    #For now assume it is Python datetime objects \n    from pygeotools.lib import timelib\n    date_list_o = np.ma.array(timelib.dt2o(dt_list))\n    date_list_o.set_fill_value(0.0)\n\n    #ma_stack = ma_stack[:,398:399,372:373]\n    #dt_stack_ptp = dt_stack_ptp[398:399,372:373]\n\n    #Only compute trend where we have n_thresh unmasked values in time\n    #Create valid pixel count\n    count = np.ma.masked_equal(ma_stack.count(axis=0), 0).astype(np.uint16).data\n    print(\"Excluding pixels with count < %i\" % n_thresh)\n    valid_mask_2D = (count >= n_thresh)\n\n    #Only compute trend where the time spread (ptp is max - min) is large\n    if dt_stack_ptp is not None:\n        if min_dt_ptp is None:\n            #Calculate from datestack ptp\n            max_dt_ptp = calcperc(dt_stack_ptp, (4, 96))[1]\n            #Calculate from list of available dates\n            #max_dt_ptp = np.ptp(calcperc(date_list_o, (4, 96)))\n            min_dt_ptp = 0.10 * max_dt_ptp\n        print(\"Excluding pixels with dt range < %0.2f days\" % min_dt_ptp) \n        valid_mask_2D = valid_mask_2D & (dt_stack_ptp >= min_dt_ptp).filled(False)\n\n    #Extract 1D time series for all valid pixel locations\n    y_orig = ma_stack[:, valid_mask_2D]\n    #Extract mask and invert: True where data is available\n    valid_mask = ~(np.ma.getmaskarray(y_orig))\n    valid_sample_count = np.inf\n\n    if y_orig.count() == 0:\n        print(\"No valid samples remain after count and min_dt_ptp filters\")\n        slope = None\n        intercept = None\n        detrended_std = None\n    else:\n        #Create empty (masked) output grids with original dimensions\n        slope = np.ma.masked_all_like(ma_stack[0])\n        intercept = np.ma.masked_all_like(ma_stack[0])\n        detrended_std = np.ma.masked_all_like(ma_stack[0])\n\n        #While loop here is to iteratively remove outliers, if desired\n        #Maximum number of iterations\n        max_n = 3\n        n = 1\n        while(y_orig.count() < valid_sample_count and n <= max_n):\n            valid_pixel_count = np.sum(valid_mask_2D)\n            valid_sample_count = y_orig.count()\n            print(\"%i valid pixels with up to %i timestamps: %i total valid samples\" % \\\n                    (valid_pixel_count, ma_stack.shape[0], valid_sample_count))\n            if model == 'theilsen' or model == 'ransac':\n                #Create empty arrays for slope and intercept results \n                m = np.ma.masked_all(y_orig.shape[1])\n                b = np.ma.masked_all(y_orig.shape[1])\n                if parallel:\n                    import multiprocessing as mp\n                    if n_cpu is None:\n                        n_cpu = iolib.cpu_count(logical=True)\n                    n_cpu = int(n_cpu)\n                    print(\"Running in parallel with %i processes\" % n_cpu)\n                    pool = mp.Pool(processes=n_cpu)\n                    results = pool.map(do_robust_linreg, [(date_list_o, y_orig[:,n], model) for n in range(y_orig.shape[1])])\n                    results = np.array(results)\n                    m = results[:,0]\n                    b = results[:,1]\n                else:\n                    for n in range(y_orig.shape[1]):\n                        print('%i of %i px' % (n, y_orig.shape[1]))\n                        y = y_orig[:,n]\n                        m[n], b[n] = do_robust_linreg(date_list_o, y, model)\n            else:\n                #if model == 'linear':\n                #Remove masks, fills with fill_value\n                y = y_orig.data\n                #Independent variable is time ordinal\n                x = date_list_o\n                x_mean = x.mean()\n                x = x.data\n                #Prepare matrices\n                X = np.c_[x, np.ones_like(x)]\n                a = np.swapaxes(np.dot(X.T, (X[None, :, :] * valid_mask.T[:, :, None])), 0, 1)\n                b = np.dot(X.T, (valid_mask*y))\n                #Solve for slope/intercept\n                print(\"Solving for trend\")\n                r = np.linalg.solve(a, b.T)\n                #Reshape to original dimensions\n                m = r[:,0]\n                b = r[:,1]\n\n            print(\"Computing residuals\")\n            #Compute model fit values for each valid timestamp\n            y_fit = m*np.ma.array(date_list_o.data[:,None]*valid_mask, mask=y_orig.mask) + b \n            #Compute residuals\n            resid = y_orig - y_fit\n            #Compute detrended std\n            #resid_std = resid.std(axis=0)\n            resid_std = mad(resid, axis=0)\n\n            if remove_outliers and n < max_n:\n                print(\"Removing residual outliers > 3-sigma\")\n                outlier_sigma = 3.0\n                #Mask any residuals outside valid range\n                valid_mask = valid_mask & (np.abs(resid) < (resid_std * outlier_sigma)).filled(False)\n                #Extract new valid samples\n                y_orig = np.ma.array(y_orig, mask=~valid_mask)\n                #Update valid mask\n                valid_count = (y_orig.count(axis=0) >= n_thresh)\n                y_orig = y_orig[:, valid_count]\n                valid_mask_2D[valid_mask_2D] = valid_count\n                #Extract 1D time series for all valid pixel locations\n                #Extract mask and invert: True where data is available\n                valid_mask = ~(np.ma.getmaskarray(y_orig))\n                #remove_outliers = False\n            else:\n                break\n            n += 1\n\n        #Fill in the valid indices\n        slope[valid_mask_2D] = m \n        intercept[valid_mask_2D] = b\n        detrended_std[valid_mask_2D] = resid_std\n\n        #Smooth the result\n        if smooth:\n            size = 5\n            print(\"Smoothing output with %i px gaussian filter\" % size)\n            from pygeotools.lib import filtlib\n            #Gaussian filter\n            #slope = filtlib.gauss_fltr_astropy(slope, size=size)\n            #intercept = filtlib.gauss_fltr_astropy(intercept, size=size)\n            #Median filter\n            slope = filtlib.rolling_fltr(slope, size=size, circular=False)\n            intercept = filtlib.rolling_fltr(intercept, size=size, circular=False)\n\n        if rsq:\n            rsquared = np.ma.masked_all_like(ma_stack[0])\n            SStot = np.sum((y_orig - y_orig.mean(axis=0))**2, axis=0).data\n            SSres = np.sum(resid**2, axis=0).data\n            r2 = 1 - (SSres/SStot)\n            rsquared[valid_mask_2D] = r2\n\n        if conf_test:\n            count = y_orig.count(axis=0)\n            SE = np.sqrt(SSres/(count - 2)/np.sum((x - x_mean)**2, axis=0))\n            T0 = r[:,0]/SE\n            alpha = 0.05\n            ta = np.zeros_like(r2)\n            from scipy.stats import t\n            for c in np.unique(count):\n                t1 = abs(t.ppf(alpha/2.0,c-2))\n                ta[(count == c)] = t1\n            sig = np.logical_and((T0 > -ta), (T0 < ta))\n            sigmask = np.zeros_like(valid_mask_2D, dtype=bool)\n            sigmask[valid_mask_2D] = ~sig\n            #SSerr = SStot - SSres\n            #F0 = SSres/(SSerr/(count - 2))\n            #from scipy.stats import f\n            #    f.cdf(sig, 1, c-2)\n            slope = np.ma.array(slope, mask=~sigmask)\n            intercept = np.ma.array(intercept, mask=~sigmask)\n            detrended_std = np.ma.array(detrended_std, mask=~sigmask)\n            rsquared = np.ma.array(rsquared, mask=~sigmask)\n        \n        #slope is in units of m/day since x is ordinal date\n        slope *= 365.25\n\n    return slope, intercept, detrended_std", "response": "Compute per - pixel linear regression for stack object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfill masked areas with random noise", "response": "def randomfill(a):\n    \"\"\"Fill masked areas with random noise\n    \n    This is needed for any fft-based operations\n    \"\"\"\n    a = checkma(a)\n    #For data that have already been normalized,\n    #This provides a proper normal distribution with mean=0 and std=1\n    #a = (a - a.mean()) / a.std()\n    #noise = a.mask * (np.random.randn(*a.shape))\n    noise = a.mask * np.random.normal(a.mean(), a.std(), a.shape)\n    #Add the noise\n    b = a.filled(0) + noise\n    return b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling masked areas with np. nan", "response": "def nanfill(a, f_a, *args, **kwargs):\n    \"\"\"Fill masked areas with np.nan\n\n    Wrapper for functions that can't handle ma (e.g. scipy.ndimage)\n    \n    This will force filters to ignore nan, but causes adjacent pixels to be set to nan as well: http://projects.scipy.org/scipy/ticket/1155 \n    \"\"\"\n    a = checkma(a)\n    ndv = a.fill_value  \n    #Note: The following fails for arrays that are not float (np.nan is float)\n    b = f_a(a.filled(np.nan), *args, **kwargs)\n    #the fix_invalid fill_value parameter doesn't seem to work\n    out = np.ma.fix_invalid(b, copy=False)\n    out.set_fill_value(ndv)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fast_median(a):\n    a = checkma(a)\n    #return scoreatpercentile(a.compressed(), 50)\n    if a.count() > 0:\n        out = np.percentile(a.compressed(), 50)\n    else:\n        out = np.ma.masked\n    return out", "response": "Fast median operation for masked array using 50th - percentile\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mad(a, axis=None, c=1.4826, return_med=False):\n    a = checkma(a)\n    #return np.ma.median(np.fabs(a - np.ma.median(a))) / c\n    if a.count() > 0:\n        if axis is None:\n            med = fast_median(a)\n            out = fast_median(np.fabs(a - med)) * c\n        else:\n            med = np.ma.median(a, axis=axis)\n            #This is necessary for broadcasting\n            med = np.expand_dims(med, axis=axis)\n            out = np.ma.median(np.ma.fabs(a - med), axis=axis) * c\n    else:\n        out = np.ma.masked\n    if return_med:\n        out = (out, med)\n    return out", "response": "Compute normalized median absolute difference of a vector and returns array with the median absolute value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calcperc(b, perc=(0.1,99.9)):\n    b = checkma(b)\n    if b.count() > 0:\n        #low = scoreatpercentile(b.compressed(), perc[0])\n        #high = scoreatpercentile(b.compressed(), perc[1])\n        low = np.percentile(b.compressed(), perc[0])\n        high = np.percentile(b.compressed(), perc[1])\n    else:\n        low = 0\n        high = 0\n\n    #low = scipy.stats.mstats.scoreatpercentile(b, perc[0])\n    #high = scipy.stats.mstats.scoreatpercentile(b, perc[1])\n\n    #This approach can be used for unmasked array, but values less than 0 are problematic\n    #bma_low = b.min()\n    #bma_high = b.max()\n    #low = scipy.stats.scoreatpercentile(b.data.flatten(), perc[0], (bma_low, bma_high))\n    #high = scipy.stats.scoreatpercentile(b.data.flatten(), perc[1], (bma_low, bma_high))\n    return low, high", "response": "Calculate values at specified percentiles\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the symmetrical percentile values for a single sequence", "response": "def calcperc_sym(b, perc=(0.1,99.9)):\n    \"\"\"\n    Get symmetrical percentile values\n    Useful for determining clim centered on 0 for difference maps\n    \"\"\"\n    clim = np.max(np.abs(calcperc(b, perc)))\n    #clim = (-clim, clim)\n    return -clim, clim"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iqr(b, perc=(25, 75)):\n    b = checkma(b)\n    low, high = calcperc(b, perc)\n    return low, high, high - low", "response": "Inter - quartile range"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and print statistics for input array", "response": "def get_stats(a, full=False):\n    \"\"\"Compute and print statistics for input array\n\n    Needs to be cleaned up, return a stats object\n    \"\"\"\n    from scipy.stats.mstats import mode \n    a = checkma(a)\n    thresh = 4E6\n    if full or a.count() < thresh:\n        q = (iqr(a))\n        p16, p84, spread = robust_spread(a)\n        #There has to be a better way to compute the mode for a ma\n        #mstats.mode returns tuple of (array[mode], array[count])\n        a_mode = float(mode(a, axis=None)[0])\n        stats = (a.count(), a.min(), a.max(), a.mean(dtype='float64'), a.std(dtype='float64'), \\\n                fast_median(a), mad(a), q[0], q[1], q[2], a_mode, p16, p84, spread) \n    else:\n        ac = a.compressed()\n        stride = int(np.around(ac.size / thresh))\n        ac = np.ma.array(ac[::stride])\n        #idx = np.random.permutation(ac.size)\n        #Note: need the ma cast here b/c of a.count() below\n        #ac = np.ma.array(ac[idx[::stride]])\n        q = (iqr(ac))\n        p16, p84, spread = robust_spread(ac)\n        ac_mode = float(mode(ac, axis=None)[0])\n        stats = (a.count(), a.min(), a.max(), a.mean(dtype='float64'), a.std(dtype='float64'), \\\n                fast_median(ac), mad(ac), q[0], q[1], q[2], ac_mode, p16, p84, spread) \n    return stats"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and print statistics for input array", "response": "def get_stats_dict(a_in, full=True):\n    \"\"\"Compute and print statistics for input array\n    \"\"\"\n    d = {}\n    a = checkma(a_in)\n    d['count'] = a.count()\n    thresh = 4E6\n    if not full and d['count'] > thresh:\n        a = a.compressed()\n        stride = int(np.around(a.size / thresh))\n        #a = np.ma.array(a[::stride])\n        a = a[::stride]\n    d['min'] = a.min()\n    d['max'] = a.max()\n    d['ptp'] = d['max'] - d['min']\n    d['mean'] = a.mean(dtype='float64')\n    d['std'] = a.std(dtype='float64')\n    d['nmad'], d['med'] = mad(a, return_med=True)\n    d['median'] = d['med']\n    d['p16'], d['p84'], d['spread'] = robust_spread(a)\n    from scipy.stats.mstats import mode \n    d['mode'] = mode(a, axis=None)[0]\n    for i in d:\n        d[i] = float(d[i])\n    d['count'] = int(d['count'])\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iv(b, **kwargs):\n    import matplotlib.pyplot as plt\n    import imview.imviewer as imview \n    b = checkma(b)\n    #if hasattr(kwargs,'imshow_kwargs'):\n    #    kwargs['imshow_kwargs']['interpolation'] = 'bicubic'\n    #else:\n    #    kwargs['imshow_kwargs'] = {'interpolation': 'bicubic'}\n    #bma_fig(fig, bma, cmap='gist_rainbow_r', clim=None, bg=None, n_subplt=1, subplt=1, label=None, **imshow_kwargs)\n    fig = plt.figure()\n    imview.bma_fig(fig, b, **kwargs)\n    plt.show()\n    return fig", "response": "Quick access to imview for interactive sessions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a 2D block view to 2D array.", "response": "def block_view(A, block=(3, 3)):\n    from numpy.lib.stride_tricks import as_strided as ast\n    \"\"\"Provide a 2D block view to 2D array. No error checking made.\n    Therefore meaningful (as implemented) only for blocks strictly\n    compatible with the shape of A.\"\"\"\n    # simple shape and strides computations may seem at first strange\n    # unless one is able to recognize the 'tuple additions' involved ;-)\n    shape= (A.shape[0]/ block[0], A.shape[1]/ block[1])+ block\n    strides= (block[0]* A.strides[0], block[1]* A.strides[1])+ A.strides\n    return ast(A, shape= shape, strides= strides)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sliding window over a n - dimensional array.", "response": "def sliding_window(a, ws, ss=None, flatten=True):\n    from numpy.lib.stride_tricks import as_strided as ast\n    '''\n    Return a sliding window over a in any number of dimensions\n     \n    Parameters:\n        a  - an n-dimensional numpy array\n        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size\n             of each dimension of the window\n        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the\n             amount to slide the window in each dimension. If not specified, it\n             defaults to ws.\n        flatten - if True, all slices are flattened, otherwise, there is an\n                  extra dimension for each dimension of the input.\n     \n    Returns\n        an array containing each n-dimensional window from a\n    '''\n     \n    if None is ss:\n        # ss was not provided. the windows will not overlap in any direction.\n        ss = ws\n    ws = norm_shape(ws)\n    ss = norm_shape(ss)\n     \n    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every\n    # dimension at once.\n    ws = np.array(ws)\n    ss = np.array(ss)\n    shape = np.array(a.shape)\n     \n    # ensure that ws, ss, and a.shape all have the same number of dimensions\n    ls = [len(shape),len(ws),len(ss)]\n    if 1 != len(set(ls)):\n        raise ValueError(\\\n        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n     \n    # ensure that ws is smaller than a in every dimension\n    if np.any(ws > shape):\n        raise ValueError(\\\n        'ws cannot be larger than a in any dimension.\\\n        a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n     \n    # how many slices will there be in each dimension?\n    newshape = norm_shape(((shape - ws) // ss) + 1)\n    # the shape of the strided array will be the number of slices in each dimension\n    # plus the shape of the window (tuple addition)\n    newshape += norm_shape(ws)\n    # the strides tuple will be the array's strides multiplied by step size, plus\n    # the array's strides (tuple addition)\n    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n    strided = ast(a,shape = newshape,strides = newstrides)\n    if not flatten:\n        return strided\n     \n    # Collapse strided so that it has one more dimension than the window.  I.e.,\n    # the new array is a flat list of slices.\n    meat = len(ws) if ws.shape else 0\n    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n    dim = firstdim + (newshape[-meat:])\n    # remove any dimensions with size 1\n    dim = [i for i in dim if i != 1]\n    return strided.reshape(dim)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnormalize numpy array shapes so they re always expressed as a tuple and even for one - dimensional shapes.", "response": "def norm_shape(shape):\n    '''\n    Normalize numpy array shapes so they're always expressed as a tuple,\n    even for one-dimensional shapes.\n     \n    Parameters\n        shape - an int, or a tuple of ints\n     \n    Returns\n        a shape tuple\n    '''\n    try:\n        i = int(shape)\n        return (i,)\n    except TypeError:\n        # shape was not a number\n        pass\n \n    try:\n        t = tuple(shape)\n        return t\n    except TypeError:\n        # shape was not iterable\n        pass\n     \n    raise TypeError('shape must be an int, or a tuple of ints')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the stack from file and set attributes.", "response": "def loadstack(self):\n        print(\"Loading stack from: %s\" % self.stack_fn)\n        data = np.load(self.stack_fn, encoding='latin1')\n        #self.fn_list = list([i.decode(\"utf-8\") for i in data['fn_list']])\n        self.fn_list = data['fn_list']\n        #Load flags originally used for stack creation\n        #self.flags = data['flags']\n        #{'datestack':self.datestack, 'stats':self.stats, 'med':self.med, 'trend':self.trend, 'sort':self.sort, 'save':self.save} \n        if 'source' in data:\n            self.source = list(data['source'])\n        else:\n            self.source = ['None' for i in self.fn_list]\n        if 'error' in data:\n            self.error = np.ma.fix_invalid(data['error'], fill_value=-9999)\n        else:\n            self.error = np.ma.zeros(len(self.fn_list))\n        #if 'error_dict_list' in data:\n        #    self.error_dict_list = data['error_dict_list'][()]\n        #else:\n        self.error_dict_list = [None for i in self.fn_list]\n        #This is a shortcut, should load from the data['date_list'] arrays\n        if 'date_list_o' in data:\n            from pygeotools.lib import timelib\n            from datetime import datetime\n            self.date_list_o = np.ma.fix_invalid(data['date_list_o'], fill_value=1.0)\n            #This is a hack - need universal timelib time zone support or stripping\n            self.date_list = np.ma.masked_equal([i.replace(tzinfo=None) for i in timelib.o2dt(self.date_list_o)], datetime(1,1,1))\n        else:\n            self.get_date_list()\n        print(\"Loading ma stack\")\n        self.ma_stack = np.ma.fix_invalid(data['ma_stack_full']).astype(self.dtype)\n        #Note: the str is an intermediate fix - all new stacks should have str written\n        self.proj = str(data['proj'])\n        #If we don't have gt, we're in trouble - can't recompute res/extent\n        if 'gt' in data:\n            self.gt = data['gt']\n        else:\n            print(\"No geotransform found in stack\")\n            #Check if res and extent are defined - can reconstruct\n            #Should throw error\n        #Note: Once we have gt, could just run get_res() and get_extent() to avoid the following\n        #Or could check to make sure consistent\n        #Some stacks in Oct 2015 and Nov 2015 did not have res/extent saved properly\n        \"\"\"\n        if 'res' in data:\n            if data['res'] != 'None':\n                #self.res = float(data['res'])\n                self.res = float(np.atleast_1d(data['res'])[0])\n            else:\n                self.get_res()\n        else:\n            self.get_res()\n        if 'extent' in data:\n            if data['extent'] != 'None':\n                #self.extent = list(data['extent'])\n                #self.extent = list(np.atleast_1d(data['extent'])[0])\n                extent = np.atleast_1d(data['extent'])[0]\n                if isinstance(extent, str):\n                    self.extent = [float(x) for x in extent.split()]\n                else:\n                    self.extent = list(extent)\n            else:\n                self.get_extent()\n        else:\n            self.get_extent() \n        \"\"\"\n        #Just do this to be safe, if gt is bad, no point in proceeding\n        self.get_res()\n        self.get_extent()\n\n        saveflag=False\n        if self.datestack:\n            #statlist = ['dt_stack', 'dt_mean', 'dt_ptp', 'dt_min', 'dt_max', 'dt_center']\n            statlist = ['dt_ptp', 'dt_min', 'dt_max', 'dt_center']\n            if all([s in data for s in statlist]):\n                print(\"Loading datestack\")\n                #self.dt_stack = np.ma.fix_invalid(data['dt_stack']).astype(self.dtype)\n                #self.dt_stack_mean = np.ma.fix_invalid(data['dt_mean'], fill_value=-9999).astype(self.dtype)\n                self.dt_stack_ptp = np.ma.fix_invalid(data['dt_ptp'], fill_value=-9999).astype(self.dtype)\n                self.dt_stack_min = np.ma.fix_invalid(data['dt_min'], fill_value=-9999).astype(self.dtype)\n                self.dt_stack_max = np.ma.fix_invalid(data['dt_max'], fill_value=-9999).astype(self.dtype)\n                self.dt_stack_center = np.ma.fix_invalid(data['dt_center'], fill_value=-9999).astype(self.dtype)\n            else:\n                if self.date_list_o.count() > 1: \n                    #self.make_datestack()\n                    self.compute_dt_stats()\n                    self.write_datestack()\n                    saveflag=True\n        if self.stats:\n            #Could do this individually to save time\n            statlist = ['count', 'mean', 'std', 'min', 'max']\n            if self.med:\n                statlist.append('med')\n                statlist.append('nmad')\n            if all([s in data for s in statlist]):\n                print(\"Loading stats\")\n                self.stack_count = np.ma.masked_equal(data['count'], 0).astype(np.uint16)\n                self.stack_mean = np.ma.fix_invalid(data['mean'], fill_value=-9999).astype(self.dtype)\n                self.stack_std = np.ma.fix_invalid(data['std'], fill_value=-9999).astype(self.dtype)\n                self.stack_min = np.ma.fix_invalid(data['min'], fill_value=-9999).astype(self.dtype)\n                self.stack_max = np.ma.fix_invalid(data['max'], fill_value=-9999).astype(self.dtype)\n                if self.med:\n                    self.stack_med = np.ma.fix_invalid(data['med'], fill_value=-9999).astype(self.dtype)\n                    self.stack_nmad = np.ma.fix_invalid(data['nmad'], fill_value=-9999).astype(self.dtype)\n            else:\n                if self.ma_stack.shape[0] > 1:\n                    self.compute_stats()\n                    self.write_stats()\n                    saveflag=True\n        if self.trend:\n            if 'n_thresh' in data: \n                self.n_thresh = data['n_thresh']\n            if 'min_dt_ptp' in data:\n                self.min_dt_ptp = data['min_dt_ptp']\n            if 'robust' in data: \n                self.robust = data['robust']\n            #statlist = ['trend', 'intercept', 'detrended_std', 'rsquared']\n            statlist = ['trend', 'intercept', 'detrended_std']\n            if all([s in data for s in statlist]):\n                print(\"Loading trend\")\n                self.stack_trend = np.ma.fix_invalid(data['trend'], fill_value=-9999).astype(self.dtype)\n                self.stack_intercept = np.ma.fix_invalid(data['intercept'], fill_value=-9999).astype(self.dtype)\n                self.stack_detrended_std = np.ma.fix_invalid(data['detrended_std'], fill_value=-9999).astype(self.dtype)\n                #self.stack_rsquared = np.ma.fix_invalid(data['rsquared'], fill_value=-9999).astype(self.dtype)\n            else:\n                if self.ma_stack.shape[0] >= self.n_thresh:\n                    self.compute_trend()\n                    self.write_trend()\n                    saveflag=True\n        if saveflag: \n            self.savestack()\n        data.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating srs for local orthographic projection centered at lat lon", "response": "def localortho(lon, lat):\n    \"\"\"Create srs for local orthographic projection centered at lat, lon\n    \"\"\"\n    local_srs = osr.SpatialReference()\n    local_proj = '+proj=ortho +lat_0=%0.7f +lon_0=%0.7f +datum=WGS84 +units=m +no_defs ' % (lat, lon)\n    local_srs.ImportFromProj4(local_proj)\n    return local_srs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert existing geom to local orthographic projection Useful for local cartesian distance and area calculations", "response": "def geom2localortho(geom):\n    \"\"\"Convert existing geom to local orthographic projection\n\n    Useful for local cartesian distance/area calculations\n    \"\"\"\n    cx, cy = geom.Centroid().GetPoint_2D()\n    lon, lat, z = cT_helper(cx, cy, 0, geom.GetSpatialReference(), wgs_srs)\n    local_srs = localortho(lon,lat)\n    local_geom = geom_dup(geom)\n    geom_transform(local_geom, local_srs)\n    return local_geom"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scale_ps(lat):\n    lat = np.array(lat)\n    if np.any(lat > 0):\n        m70_t70 = 1.9332279 \n        #Hack to deal with pole\n        lat[lat>=90.0] = 89.999999999\n    else:\n        # for 71 deg, southern PS  -- checked BS 5/2012\n        m70_t70 = 1.93903005  \n        lat[lat<=-90.0] = -89.999999999\n\n    #for WGS84, a=6378137, 1/f = 298.257223563 -> 1-sqrt(1-e^2) = f\n    #-> 1-(1-f)^2 = e2 =    0.006694379990141\n    #e2 = 0.006693883\n    e2 = 0.006694379990141  # BS calculated from WGS84 parameters 5/2012\n    e = np.sqrt(e2)\n\n    lat = np.abs(np.deg2rad(lat))\n    slat = np.sin(lat)\n    clat = np.cos(lat)\n\n    m = clat/np.sqrt(1. - e2*slat**2)\n    t = np.tan(np.pi/4 - lat/2)/((1. - e*slat)/(1. + e*slat))**(e/2)\n    k = m70_t70*t/m\n\n    scale=(1./k)\n    return scale", "response": "This function calculates the scaling factor for a polar stereographic CM and I grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert longitude from 0 to 360", "response": "def lon360to180(lon):\n    \"\"\"Convert longitude from (0, 360) to (-180, 180)\n    \"\"\"\n    if np.any(lon > 360.0) or np.any(lon < 0.0):\n        print(\"Warning: lon outside expected range\")\n        lon = wraplon(lon)\n    #lon[lon > 180.0] -= 360.0\n    #lon180 = (lon+180) - np.floor((lon+180)/360)*360 - 180\n    lon = lon - (lon.astype(int)/180)*360.0\n    return lon"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting longitude from - 180 to 360", "response": "def lon180to360(lon):\n    \"\"\"Convert longitude from (-180, 180) to (0, 360)\n    \"\"\"\n    if np.any(lon > 180.0) or np.any(lon < -180.0):\n        print(\"Warning: lon outside expected range\")\n        lon = lon360to180(lon)\n    #lon[lon < 0.0] += 360.0\n    lon = (lon + 360.0) % 360.0\n    return lon"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert decimal degrees to degrees minutes seconds", "response": "def dd2dms(dd):\n    \"\"\"Convert decimal degrees to degrees, minutes, seconds\n    \"\"\"\n    n = dd < 0\n    dd = abs(dd)\n    m,s = divmod(dd*3600,60)\n    d,m = divmod(m,60)\n    if n:\n        d = -d\n    return d,m,s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dms2dd(d,m,s):\n    if d < 0:\n        sign = -1\n    else:\n        sign = 1\n    dd = sign * (int(abs(d)) + float(m) / 60 + float(s) / 3600)\n    return dd", "response": "Convert degrees minutes seconds to decimal degrees\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting decimal degrees decimal minutes to degrees decimal seconds", "response": "def dd2dm(dd):\n    \"\"\"Convert decimal to degrees, decimal minutes\n    \"\"\"\n    d,m,s = dd2dms(dd)\n    m = m + float(s)/3600\n    return d,m,s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting map coordinates to pixel coordinates based on geotransform", "response": "def mapToPixel(mX, mY, geoTransform):\n    \"\"\"Convert map coordinates to pixel coordinates based on geotransform\n    \n    Accepts float or NumPy arrays\n\n    GDAL model used here - upper left corner of upper left pixel for mX, mY (and in GeoTransform)\n    \"\"\"\n    mX = np.asarray(mX)\n    mY = np.asarray(mY)\n    if geoTransform[2] + geoTransform[4] == 0:\n        pX = ((mX - geoTransform[0]) / geoTransform[1]) - 0.5\n        pY = ((mY - geoTransform[3]) / geoTransform[5]) - 0.5\n    else:\n        pX, pY = applyGeoTransform(mX, mY, invertGeoTransform(geoTransform))\n    #return int(pX), int(pY)\n    return pX, pY"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pixelToMap(pX, pY, geoTransform):\n    pX = np.asarray(pX, dtype=float)\n    pY = np.asarray(pY, dtype=float)\n    pX += 0.5\n    pY += 0.5\n    mX, mY = applyGeoTransform(pX, pY, geoTransform)\n    return mX, mY", "response": "Convert pixel coordinates to map coordinates based on geotransform\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute points on a regular grid using specified statistic", "response": "def block_stats(x,y,z,ds,stat='median',bins=None):\n    \"\"\"Compute points on a regular grid (matching input GDAL Dataset) from scattered point data using specified statistic\n\n    Wrapper for  scipy.stats.binned_statistic_2d\n    \n    Note: this is very fast for mean, std, count, but bignificantly slower for median\n    \"\"\"\n    import scipy.stats as stats\n    extent = ds_extent(ds)\n    #[[xmin, xmax], [ymin, ymax]]\n    range = [[extent[0], extent[2]], [extent[1], extent[3]]]\n    if bins is None:\n        bins = (ds.RasterXSize, ds.RasterYSize)\n    if stat == 'max':\n        stat = np.max\n    elif stat == 'min':\n        stat = np.min\n    #block_count, xedges, yedges, bin = stats.binned_statistic_2d(x,y,z,'count',bins,range)\n    block_stat, xedges, yedges, bin = stats.binned_statistic_2d(x,y,z,stat,bins,range)\n    #Get valid blocks\n    #if (stat == 'median') or (stat == 'mean'):\n    if stat in ('median', 'mean', np.max, np.min):\n        idx = ~np.isnan(block_stat)\n    else:\n        idx = (block_stat != 0)\n    idx_idx = idx.nonzero()\n    #Cell centers\n    res = [(xedges[1] - xedges[0]), (yedges[1] - yedges[0])]\n    out_x = xedges[:-1]+res[0]/2.0\n    out_y = yedges[:-1]+res[1]/2.0\n    out_x = out_x[idx_idx[0]]\n    out_y = out_y[idx_idx[1]]\n    out_z = block_stat[idx]\n    return out_x, out_y, out_z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef block_stats_grid(x,y,z,ds,stat='median'):\n    mx, my, mz = block_stats(x,y,z,ds,stat)\n    gt = ds.GetGeoTransform()\n    pX, pY = mapToPixel(mx, my, gt)\n    shape = (ds.RasterYSize, ds.RasterXSize)\n    ndv = -9999.0\n    a = np.full(shape, ndv)\n    a[pY.astype('int'), pX.astype('int')] = mz\n    return np.ma.masked_equal(a, ndv)", "response": "Fill regular grid from scattered point data using specified statistic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a GDAL Dataset in memory", "response": "def mem_ds(res, extent, srs=None, dtype=gdal.GDT_Float32):\n    \"\"\"Create a new GDAL Dataset in memory\n\n    Useful for various applications that require a Dataset\n    \"\"\"\n    #These round down to int\n    #dst_ns = int((extent[2] - extent[0])/res)\n    #dst_nl = int((extent[3] - extent[1])/res)\n    #This should pad by 1 pixel, but not if extent and res were calculated together to give whole int\n    dst_ns = int((extent[2] - extent[0])/res + 0.99)\n    dst_nl = int((extent[3] - extent[1])/res + 0.99)\n    m_ds = gdal.GetDriverByName('MEM').Create('', dst_ns, dst_nl, 1, dtype)\n    m_gt = [extent[0], res, 0, extent[3], 0, -res]\n    m_ds.SetGeoTransform(m_gt)\n    if srs is not None:\n        m_ds.SetProjection(srs.ExportToWkt())\n    return m_ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy projection and geotransform from one raster file to another raster file", "response": "def copyproj(src_fn, dst_fn, gt=True):\n    \"\"\"Copy projection and geotransform from one raster file to another\n    \"\"\"\n    src_ds = gdal.Open(src_fn, gdal.GA_ReadOnly)\n    dst_ds = gdal.Open(dst_fn, gdal.GA_Update)\n    dst_ds.SetProjection(src_ds.GetProjection())\n    if gt:\n        src_gt = np.array(src_ds.GetGeoTransform())\n        src_dim = np.array([src_ds.RasterXSize, src_ds.RasterYSize])\n        dst_dim = np.array([dst_ds.RasterXSize, dst_ds.RasterYSize])\n        #This preserves dst_fn resolution\n        if np.any(src_dim != dst_dim):\n            res_factor = src_dim/dst_dim.astype(float)\n            src_gt[[1, 5]] *= max(res_factor)\n            #src_gt[[1, 5]] *= min(res_factor)\n            #src_gt[[1, 5]] *= res_factor\n        dst_ds.SetGeoTransform(src_gt)\n    src_ds = None\n    dst_ds = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geom_dup(geom):\n    g = ogr.CreateGeometryFromWkt(geom.ExportToWkt())\n    g.AssignSpatialReference(geom.GetSpatialReference())\n    return g", "response": "Create duplicate geometry from a geometry object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geom_transform(geom, t_srs):\n    s_srs = geom.GetSpatialReference()\n    if not s_srs.IsSame(t_srs):\n        ct = osr.CoordinateTransformation(s_srs, t_srs)\n        geom.Transform(ct)\n        geom.AssignSpatialReference(t_srs)", "response": "Transform a geometry in place"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shp_dict(shp_fn, fields=None, geom=True):\n    from pygeotools.lib import timelib\n    ds = ogr.Open(shp_fn)\n    lyr = ds.GetLayer()\n    nfeat = lyr.GetFeatureCount()\n    print('%i input features\\n' % nfeat)\n    if fields is None:\n        fields = shp_fieldnames(lyr)\n    d_list = []\n    for n,feat in enumerate(lyr):\n        d = {}\n        if geom:\n            geom = feat.GetGeometryRef()\n            d['geom'] = geom\n        for f_name in fields:\n            i = str(feat.GetField(f_name))\n            if 'date' in f_name:\n                # date_f = f_name\n                #If d is float, clear off decimal\n                i = i.rsplit('.')[0]\n                i = timelib.strptime_fuzzy(str(i))\n            d[f_name] = i\n        d_list.append(d)\n    #d_list_sort = sorted(d_list, key=lambda k: k[date_f])\n    return d_list", "response": "Get a dictionary for all features in a shapefile optionally specify fields\n    "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lyr_proj(lyr, t_srs, preserve_fields=True):\n    #Need to check t_srs\n    s_srs = lyr.GetSpatialRef()\n    cT = osr.CoordinateTransformation(s_srs, t_srs)\n\n    #Do everything in memory\n    drv = ogr.GetDriverByName('Memory')\n\n    #Might want to save clipped, warped shp to disk?\n    # create the output layer\n    #drv = ogr.GetDriverByName('ESRI Shapefile')\n    #out_fn = '/tmp/temp.shp'\n    #if os.path.exists(out_fn):\n    #    driver.DeleteDataSource(out_fn)\n    #out_ds = driver.CreateDataSource(out_fn)\n    \n    out_ds = drv.CreateDataSource('out')\n    outlyr = out_ds.CreateLayer('out', srs=t_srs, geom_type=lyr.GetGeomType())\n\n    if preserve_fields:\n        # add fields\n        inLayerDefn = lyr.GetLayerDefn()\n        for i in range(0, inLayerDefn.GetFieldCount()):\n            fieldDefn = inLayerDefn.GetFieldDefn(i)\n            outlyr.CreateField(fieldDefn)\n        # get the output layer's feature definition\n    outLayerDefn = outlyr.GetLayerDefn()\n\n    # loop through the input features\n    inFeature = lyr.GetNextFeature()\n    while inFeature:\n        # get the input geometry\n        geom = inFeature.GetGeometryRef()\n        # reproject the geometry\n        geom.Transform(cT)\n        # create a new feature\n        outFeature = ogr.Feature(outLayerDefn)\n        # set the geometry and attribute\n        outFeature.SetGeometry(geom)\n        if preserve_fields:\n            for i in range(0, outLayerDefn.GetFieldCount()):\n                outFeature.SetField(outLayerDefn.GetFieldDefn(i).GetNameRef(), inFeature.GetField(i))\n        # add the feature to the shapefile\n        outlyr.CreateFeature(outFeature)\n        # destroy the features and get the next input feature\n        inFeature = lyr.GetNextFeature()\n    #NOTE: have to operate on ds here rather than lyr, otherwise segfault\n    return out_ds", "response": "Reproject an OGR layer into a new shapefile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shp2array(shp_fn, r_ds=None, res=None, extent=None, t_srs=None):\n    shp_ds = ogr.Open(shp_fn)\n    lyr = shp_ds.GetLayer()\n    #This returns xmin, ymin, xmax, ymax\n    shp_extent = lyr_extent(lyr)\n    shp_srs = lyr.GetSpatialRef()\n    # dst_dt = gdal.GDT_Byte\n    ndv = 0\n    if r_ds is not None:\n        r_extent = ds_extent(r_ds)\n        res = get_res(r_ds, square=True)[0] \n        if extent is None:\n            extent = r_extent\n        r_srs = get_ds_srs(r_ds)\n        r_geom = ds_geom(r_ds)\n        # dst_ns = r_ds.RasterXSize\n        # dst_nl = r_ds.RasterYSize\n        #Convert raster extent to shp_srs\n        cT = osr.CoordinateTransformation(r_srs, shp_srs)\n        r_geom_reproj = geom_dup(r_geom)\n        r_geom_reproj.Transform(cT)\n        r_geom_reproj.AssignSpatialReference(t_srs)\n        lyr.SetSpatialFilter(r_geom_reproj)\n        #lyr.SetSpatialFilter(ogr.CreateGeometryFromWkt(wkt))\n    else:\n        #TODO: clean this up\n        if res is None:\n            sys.exit(\"Must specify input res\")\n        if extent is None:\n            print(\"Using input shp extent\")\n            extent = shp_extent\n    if t_srs is None:\n        t_srs = r_srs\n    if not shp_srs.IsSame(t_srs):\n        print(\"Input shp srs: %s\" % shp_srs.ExportToProj4())\n        print(\"Specified output srs: %s\" % t_srs.ExportToProj4())\n        out_ds = lyr_proj(lyr, t_srs)\n        outlyr = out_ds.GetLayer()\n    else:\n        outlyr = lyr\n    #outlyr.SetSpatialFilter(r_geom)\n    m_ds = mem_ds(res, extent, srs=t_srs, dtype=gdal.GDT_Byte)\n    b = m_ds.GetRasterBand(1)\n    b.SetNoDataValue(ndv)\n    gdal.RasterizeLayer(m_ds, [1], outlyr, burn_values=[1])\n    a = b.ReadAsArray()\n    a = ~(a.astype('Bool'))\n    return a", "response": "Convert a shapefile to an array of shapesets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raster_shpclip(r_fn, shp_fn, extent='raster', bbox=False, pad=None, invert=False, verbose=False):\n    from pygeotools.lib import iolib\n    from pygeotools.lib import warplib\n\n    r_ds = iolib.fn_getds(r_fn)\n    r_srs = get_ds_srs(r_ds)\n    r_extent = ds_extent(r_ds)\n    r_extent_geom = bbox2geom(r_extent)\n\n    #NOTE: want to add spatial filter here to avoid reprojeting global RGI polygons, for example\n    \n    shp_ds = ogr.Open(shp_fn)\n    lyr = shp_ds.GetLayer()\n    shp_srs = lyr.GetSpatialRef()\n    if not r_srs.IsSame(shp_srs):\n        shp_ds = lyr_proj(lyr, r_srs)\n        lyr = shp_ds.GetLayer()\n    #This returns xmin, ymin, xmax, ymax\n    shp_extent = lyr_extent(lyr)\n    shp_extent_geom = bbox2geom(shp_extent)\n\n    #Define the output - can set to either raster or shp\n    #Could accept as cl arg\n    out_srs = r_srs\n\n    if extent == 'raster':\n        out_extent = r_extent \n    elif extent == 'shp':\n        out_extent = shp_extent\n    elif extent == 'intersection':\n        out_extent = geom_intersection([r_extent_geom, shp_extent_geom])\n    elif extent == 'union':\n        out_extent = geom_union([r_extent_geom, shp_extent_geom])\n    else:\n        print(\"Unexpected extent specification, reverting to input raster extent\")\n        out_extent = 'raster'\n\n    #Add padding around shp_extent\n    #Should implement buffer here\n    if pad is not None:\n        out_extent = pad_extent(out_extent, width=pad)\n\n    print(\"Raster to clip: %s\\nShapefile used to clip: %s\" % (r_fn, shp_fn))\n    if verbose:\n        print(shp_extent) \n        print(r_extent)\n        print(out_extent)\n\n    r_ds = warplib.memwarp(r_ds, extent=out_extent, t_srs=out_srs, r='cubic')\n    r = iolib.ds_getma(r_ds)\n\n    #If bbox, return without clipping, otherwise, clip to polygons\n    if not bbox:\n        #Create binary mask from shp\n        mask = shp2array(shp_fn, r_ds)\n        if invert:\n            mask = ~(mask)\n        #Now apply the mask\n        r = np.ma.array(r, mask=mask)\n    #Return both the array and the dataset, needed for writing out\n    #Should probably just write r to r_ds and return r_ds \n    return r, r_ds", "response": "Clip an input raster by input polygon shapefile for given extent"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting geometries from input shapefile", "response": "def shp2geom(shp_fn):\n    \"\"\"Extract geometries from input shapefile\n    \n    Need to handle multi-part geom: http://osgeo-org.1560.x6.nabble.com/Multipart-to-singlepart-td3746767.html\n    \"\"\"\n    ds = ogr.Open(shp_fn)\n    lyr = ds.GetLayer()\n    srs = lyr.GetSpatialRef()\n    lyr.ResetReading()\n    geom_list = []\n    for feat in lyr:\n        geom = feat.GetGeometryRef()\n        geom.AssignSpatialReference(srs)\n        #Duplicate the geometry, or segfault\n        #See: http://trac.osgeo.org/gdal/wiki/PythonGotchas\n        #g = ogr.CreateGeometryFromWkt(geom.ExportToWkt())\n        #g.AssignSpatialReference(srs)\n        g = geom_dup(geom)\n        geom_list.append(g)\n    #geom = ogr.ForceToPolygon(' '.join(geom_list))    \n    #Dissolve should convert multipolygon to single polygon \n    #return geom_list[0]\n    ds = None\n    return geom_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite out a new shapefile for input geometry", "response": "def geom2shp(geom, out_fn, fields=False):\n    \"\"\"Write out a new shapefile for input geometry\n    \"\"\"\n    from pygeotools.lib import timelib\n    driverName = \"ESRI Shapefile\"\n    drv = ogr.GetDriverByName(driverName)\n    if os.path.exists(out_fn):\n        drv.DeleteDataSource(out_fn)\n    out_ds = drv.CreateDataSource(out_fn)\n    out_lyrname = os.path.splitext(os.path.split(out_fn)[1])[0]\n    geom_srs = geom.GetSpatialReference()\n    geom_type = geom.GetGeometryType()\n    out_lyr = out_ds.CreateLayer(out_lyrname, geom_srs, geom_type)\n    if fields:\n        field_defn = ogr.FieldDefn(\"name\", ogr.OFTString)\n        field_defn.SetWidth(128)\n        out_lyr.CreateField(field_defn)\n        field_defn = ogr.FieldDefn(\"path\", ogr.OFTString)\n        field_defn.SetWidth(254)\n        out_lyr.CreateField(field_defn)\n        #field_defn = ogr.FieldDefn(\"date\", ogr.OFTString)\n        #This allows sorting by date\n        field_defn = ogr.FieldDefn(\"date\", ogr.OFTInteger)\n        field_defn.SetWidth(32)\n        out_lyr.CreateField(field_defn)\n        field_defn = ogr.FieldDefn(\"decyear\", ogr.OFTReal)\n        field_defn.SetPrecision(8)\n        field_defn.SetWidth(64)\n        out_lyr.CreateField(field_defn)\n    out_feat = ogr.Feature(out_lyr.GetLayerDefn())\n    out_feat.SetGeometry(geom)\n    if fields:\n        #Hack to force output extesion to tif, since out_fn is shp\n        out_path = os.path.splitext(out_fn)[0] + '.tif'\n        out_feat.SetField(\"name\", os.path.split(out_path)[-1])\n        out_feat.SetField(\"path\", out_path)\n        #Try to extract a date from input raster fn\n        out_feat_date = timelib.fn_getdatetime(out_fn)\n        if out_feat_date is not None:\n            datestamp = int(out_feat_date.strftime('%Y%m%d'))\n            #out_feat_date = int(out_feat_date.strftime('%Y%m%d%H%M'))\n            out_feat.SetField(\"date\", datestamp)\n            decyear = timelib.dt2decyear(out_feat_date)\n            out_feat.SetField(\"decyear\", decyear)\n    out_lyr.CreateFeature(out_feat)\n    out_ds = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_outline(ds, t_srs=None, scale=1.0, simplify=False, convex=False):\n    gt = np.array(ds.GetGeoTransform())\n    from pygeotools.lib import iolib\n    a = iolib.ds_getma_sub(ds, scale=scale)\n    #Create empty geometry\n    geom = ogr.Geometry(ogr.wkbPolygon)\n    #Check to make sure we have unmasked data\n    if a.count() != 0:\n        #Scale the gt for reduced resolution\n        #The UL coords should remain the same, as any rounding will trim LR\n        if (scale != 1.0):\n            gt[1] *= scale\n            gt[5] *= scale\n        #Get srs\n        ds_srs = get_ds_srs(ds)\n        if t_srs is None:\n            t_srs = ds_srs\n        #Find the unmasked edges\n        #Note: using only axis=0 from notmasked_edges will miss undercuts - see malib.get_edgemask\n        #Better ways to do this - binary mask, sum (see numpy2stl)\n        #edges0, edges1, edges = malib.get_edges(a)\n        px = np.ma.notmasked_edges(a, axis=0)\n        # coord = []\n        #Combine edge arrays, reversing order and adding first point to complete polygon\n        x = np.concatenate((px[0][1][::1], px[1][1][::-1], [px[0][1][0]]))\n        #x = np.concatenate((edges[0][1][::1], edges[1][1][::-1], [edges[0][1][0]]))\n        y = np.concatenate((px[0][0][::1], px[1][0][::-1], [px[0][0][0]]))\n        #y = np.concatenate((edges[0][0][::1], edges[1][0][::-1], [edges[0][0][0]]))\n        #Use np arrays for computing mapped coords\n        mx, my = pixelToMap(x, y, gt)\n        #Create wkt string\n        geom_wkt = 'POLYGON(({0}))'.format(', '.join(['{0} {1}'.format(*a) for a in zip(mx,my)]))\n        geom = ogr.CreateGeometryFromWkt(geom_wkt)\n        if not ds_srs.IsSame(t_srs):\n            ct = osr.CoordinateTransformation(ds_srs, t_srs)\n            geom.Transform(ct)\n        #Make sure geometry has correct srs assigned\n        geom.AssignSpatialReference(t_srs)\n        if not geom.IsValid():\n            tol = gt[1] * 0.1\n            geom = geom.Simplify(tol)\n        #Need to get output units and extent for tolerance specification\n        if simplify:\n            #2 pixel tolerance\n            tol = gt[1] * 2\n            geom = geom.Simplify(tol)\n        if convex:\n            geom = geom.ConvexHull()\n    else:\n        print(\"No unmasked values found\")\n    return geom", "response": "This function generates an outline of unmasked values in a raster"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting input point coordinates to map coordinates that match input dataset", "response": "def ds_cT(ds, x, y, xy_srs=wgs_srs):\n    \"\"\"Convert input point coordinates to map coordinates that match input dataset\n    \"\"\"\n    #Convert lat/lon to projected srs\n    ds_srs = get_ds_srs(ds)\n    #If xy_srs is undefined, assume it is the same as ds_srs\n    mX = x\n    mY = y\n    if xy_srs is not None:\n        if not ds_srs.IsSame(xy_srs):\n            mX, mY, mZ = cT_helper(x, y, 0, xy_srs, ds_srs)\n    return mX, mY"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(ds, mX, mY, xy_srs=None, bn=1, pad=0, min_samp_perc=50, circ=False, count=False):\n    from pygeotools.lib import iolib, malib\n\n    #Should offer option to fit plane to points and then sample values with sub-pixel precision\n\n    shape = (ds.RasterYSize, ds.RasterXSize)\n    gt = ds.GetGeoTransform()\n    b = ds.GetRasterBand(bn)\n    b_ndv = iolib.get_ndv_b(b)\n    b_dtype = b.DataType\n    np_dtype = iolib.gdal2np_dtype(b)\n\n    #If necessary, convert input coordiantes to match ds srs\n    mX, mY = ds_cT(ds, mX, mY, xy_srs=xy_srs)\n\n    #This will sample an area corresponding to diameter of ICESat shot\n    if pad == 'glas':\n        spotsize = 70\n        pad = int(np.ceil(((spotsize/gt[1])-1)/2))\n\n    mX = np.atleast_1d(mX)\n    mY = np.atleast_1d(mY)\n    #Convert to pixel indices\n    pX, pY = mapToPixel(mX, mY, gt)\n    #Mask anything outside image dimensions\n    pX = np.ma.masked_outside(pX, 0, shape[1]-1)\n    pY = np.ma.masked_outside(pY, 0, shape[0]-1)\n    common_mask = (~(np.logical_or(np.ma.getmaskarray(pX), np.ma.getmaskarray(pY)))).nonzero()[0]\n\n    #Define x and y sample windows\n    xwin=pad*2+1\n    ywin=pad*2+1\n\n    #This sets the minimum number of valid pixels, default 50%\n    min_samp = int(np.ceil((min_samp_perc/100.)*xwin*ywin))\n\n    #Create circular mask to simulate spot\n    #This only makes sense for for xwin > 3 \n    if circ:\n        circ_mask = filtlib.circular_mask(xwin)\n        min_samp = int(np.ceil((min_samp_perc/100.)*circ_mask.nonzero()[0].size))\n\n    pX_int = pX[common_mask].data\n    pY_int = pY[common_mask].data\n    #Round to nearest integer indices\n    pX_int = np.around(pX_int).astype(int)\n    pY_int = np.around(pY_int).astype(int)\n    #print(\"Valid extent: %i\" % pX_int.size)\n\n    #Create empty array to hold output\n    #Added the valid pixel count quickly, should clean this up for more systematic stats return at each sample\n    if count:\n        stats = np.full((pX_int.size, 3), b_ndv, dtype=np_dtype)\n    else:\n        stats = np.full((pX_int.size, 2), b_ndv, dtype=np_dtype)\n\n    r = gdal.GRA_NearestNeighbour\n    #r = gdal.GRA_Cubic\n\n    for i in range(pX_int.size):\n        #Could have float offsets here with GDAL resampling\n        samp = np.ma.masked_equal(b.ReadAsArray(xoff=pX_int[i]-pad, yoff=pY_int[i]-pad, win_xsize=xwin, win_ysize=ywin, resample_alg=r), b_ndv)\n        if circ:\n            samp = np.ma.array(samp, circ_mask)\n        if samp.count() >= min_samp:\n            if min_samp > 1:\n                #Use mean and std\n                #samp_med = samp.mean()\n                #samp_mad = samp.std()\n                #Use median and nmad (robust)\n                samp_med = malib.fast_median(samp)\n                samp_mad = malib.mad(samp)\n                stats[i][0] = samp_med \n                stats[i][1] = samp_mad\n                if count:\n                    stats[i][2] = samp.count()\n            else:\n                stats[i][0] = samp[0]\n                stats[i][1] = 0\n                if count:\n                    stats[i][2] = 1\n\n            #vals, resid, coef = ma_fitplane(samp, gt=[0, gt[1], 0, 0, 0, gt[5]], perc=None)\n            #Compute slope and aspect from plane\n            #rmse = malib.rmse(resid)\n\n    stats = np.ma.masked_equal(stats, b_ndv)\n    #Create empty array with as input points\n    if count:\n        out = np.full((pX.size, 3), b_ndv, dtype=np_dtype)\n    else:\n        out = np.full((pX.size, 2), b_ndv, dtype=np_dtype)\n    #Populate with valid samples\n    out[common_mask, :] = stats\n    out = np.ma.masked_equal(out, b_ndv)\n    return out", "response": "Sample input dataset at map coordinates and return value of ICESat shot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line2pts(geom, dl=None):\n    #Extract list of (x,y) tuples at nodes\n    nodes = geom.GetPoints()\n    #print \"%i nodes\" % len(nodes)\n   \n    #Point spacing in map units\n    if dl is None:\n        nsteps=1000\n        dl = geom.Length()/nsteps\n\n    #This only works for equidistant projection!\n    #l = np.arange(0, geom.Length(), dl)\n\n    #Initialize empty lists\n    l = []\n    mX = []\n    mY = []\n\n    #Add first point to output lists\n    l += [0]\n    x = nodes[0][0]\n    y = nodes[0][1]\n    mX += [x]\n    mY += [y]\n\n    #Remainder\n    rem_l = 0\n    #Previous length (initially 0)\n    last_l = l[-1]\n    \n    #Loop through each line segment in the feature\n    for i in range(0,len(nodes)-1):\n        x1, y1 = nodes[i]\n        x2, y2 = nodes[i+1]\n      \n        #Total length of segment\n        tl = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n\n        #Number of dl steps we can fit in this segment\n        #This returns floor \n        steps = int((tl+rem_l)/dl)\n\n        if steps > 0:\n            dx = ((x2-x1)/tl)*dl\n            dy = ((y2-y1)/tl)*dl\n            rem_x = rem_l*(dx/dl)\n            rem_y = rem_l*(dy/dl)\n            \n            #Loop through each step and append to lists\n            for n in range(1, steps+1):\n                l += [last_l + (dl*n)]\n                #Remove the existing remainder\n                x = x1 + (dx*n) - rem_x\n                y = y1 + (dy*n) - rem_y\n                mX += [x]\n                mY += [y]\n\n            #Note: could just build up arrays of pX, pY for entire line, then do single z extraction\n            #Update the remainder\n            rem_l += tl - (steps * dl)\n            last_l = l[-1]\n        else:\n            rem_l += tl \n\n    return l, mX, mY", "response": "Given an input line geom generate points at fixed interval\n    Used for extracting profile data from raster\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_res_stats(ds_list, t_srs=None):\n    if t_srs is None:\n        t_srs = get_ds_srs(ds_list[0]) \n    res = np.array([get_res(ds, t_srs=t_srs) for ds in ds_list])\n    #Check that all projections are identical\n    #gt_array = np.array([ds.GetGeoTransform() for ds in args])\n    #xres = gt_array[:,1]\n    #yres = -gt_array[:,5]\n    #if xres == yres:\n    #res = np.concatenate((xres, yres))\n    min = np.min(res)\n    max = np.max(res)\n    mean = np.mean(res)\n    med = np.median(res)\n    return (min, max, mean, med)", "response": "Return resolution stats for an input dataset list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting GDAL Dataset raster resolution", "response": "def get_res(ds, t_srs=None, square=False):\n    \"\"\"Get GDAL Dataset raster resolution\n    \"\"\"\n    gt = ds.GetGeoTransform()\n    ds_srs = get_ds_srs(ds)\n    #This is Xres, Yres\n    res = [gt[1], np.abs(gt[5])]\n    if square:\n        res = [np.mean(res), np.mean(res)]\n    if t_srs is not None and not ds_srs.IsSame(t_srs):\n        if True:\n            #This diagonal approach is similar to the approach in gdaltransformer.cpp\n            #Bad news for large extents near the poles\n            #ullr = get_ullr(ds, t_srs)\n            #diag = np.sqrt((ullr[0]-ullr[2])**2 + (ullr[1]-ullr[3])**2)\n            extent = ds_extent(ds, t_srs)\n            diag = np.sqrt((extent[2]-extent[0])**2 + (extent[3]-extent[1])**2)\n            res = diag / np.sqrt(ds.RasterXSize**2 + ds.RasterYSize**2)\n            res = [res, res]\n        else:        \n            #Compute from center pixel\n            ct = osr.CoordinateTransformation(ds_srs, t_srs)\n            pt = get_center(ds)\n            #Transform center coordinates\n            pt_ct = ct.TransformPoint(*pt)\n            #Transform center + single pixel offset coordinates\n            pt_ct_plus = ct.TransformPoint(pt[0] + gt[1], pt[1] + gt[5])\n            #Compute resolution in new units \n            res = [pt_ct_plus[0] - pt_ct[0], np.abs(pt_ct_plus[1] - pt_ct[1])]\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting center coordinates of GDAL Dataset", "response": "def get_center(ds, t_srs=None):\n    \"\"\"Get center coordinates of GDAL Dataset\n    \"\"\"\n    gt = ds.GetGeoTransform()\n    ds_srs = get_ds_srs(ds)\n    #Note: this is center of center pixel, not ul corner of center pixel\n    center = [gt[0] + (gt[1] * ds.RasterXSize/2.0), gt[3] + (gt[5] * ds.RasterYSize/2.0)]\n    #include t_srs.Validate() and t_srs.Fixup()\n    if t_srs is not None and not ds_srs.IsSame(t_srs):\n        ct = osr.CoordinateTransformation(ds_srs, t_srs)\n        center = list(ct.TransformPoint(*center)[0:2])\n    return center"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting GDAL Datset object for GDAL Datset", "response": "def get_ds_srs(ds):\n    \"\"\"Get srs object for GDAL Datset\n    \"\"\"\n    ds_srs = osr.SpatialReference()\n    ds_srs.ImportFromWkt(ds.GetProjectionRef())\n    return ds_srs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks validitiy of Dataset srs Return True if srs is properly defined", "response": "def srs_check(ds):\n    \"\"\"Check validitiy of Dataset srs\n\n    Return True if srs is properly defined\n    \"\"\"\n    # ds_srs = get_ds_srs(ds)\n    gt = np.array(ds.GetGeoTransform())\n    gt_check = ~np.all(gt == np.array((0.0, 1.0, 0.0, 0.0, 0.0, 1.0)))\n    proj_check = (ds.GetProjection() != '')\n    #proj_check = ds_srs.IsProjected()\n    out = False\n    if gt_check and proj_check:  \n        out = True\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ds_IsEmpty(ds):\n    out = False\n    b = ds.GetRasterBand(1)\n    #Looks like this throws:\n    #ERROR 1: Failed to compute min/max, no valid pixels found in sampling.\n    #Should just catch this rater than bothering with logic below\n    try: \n        mm = b.ComputeRasterMinMax()\n        if (mm[0] == mm[1]): \n            ndv = b.GetNoDataValue()\n            if ndv is None:\n                out = True\n            else:\n                if (mm[0] == ndv):\n                    out = True\n    except Exception:\n        out = True \n    #Check for std of nan\n    #import math\n    #stats = b.ComputeStatistics(1)\n    #for x in stats:\n    #    if math.isnan(x):\n    #       out = True\n    #       break\n    return out", "response": "Check to see if dataset is empty after warp\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting corner coordinates based on input geotransform and raster dimensions", "response": "def gt_corners(gt, nx, ny):\n    \"\"\"Get corner coordinates based on input geotransform and raster dimensions\n   \"\"\"\n    ul = [gt[0], gt[3]]\n    ll = [gt[0], gt[3] + (gt[5] * ny)]\n    ur = [gt[0] + (gt[1] * nx), gt[3]]\n    lr = [gt[0] + (gt[1] * nx), gt[3] + (gt[5] * ny)]\n    return ul, ll, ur, lr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef corner_extent(ul, ll, ur, lr): \n    xmin = min(ul[0], ll[0], ur[0], lr[0])\n    xmax = max(ul[0], ll[0], ur[0], lr[0])\n    ymin = min(ul[1], ll[1], ur[1], lr[1])\n    ymax = max(ul[1], ll[1], ur[1], lr[1])\n    extent = [xmin, ymin, xmax, ymax]\n    return extent", "response": "Get min max extent based on corner coord\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ds_extent(ds, t_srs=None):\n    ul, ll, ur, lr = gt_corners(ds.GetGeoTransform(), ds.RasterXSize, ds.RasterYSize) \n    ds_srs = get_ds_srs(ds) \n    if t_srs is not None and not ds_srs.IsSame(t_srs):\n        ct = osr.CoordinateTransformation(ds_srs, t_srs)\n        #Check to see if ct creation failed\n        #if ct == NULL:\n        #Check to see if transform failed\n        #if not ct.TransformPoint(extent[0], extent[1]):\n        #Need to check that transformed coordinates fall within appropriate bounds\n        ul = ct.TransformPoint(*ul)\n        ll = ct.TransformPoint(*ll)\n        ur = ct.TransformPoint(*ur)\n        lr = ct.TransformPoint(*lr)\n    extent = corner_extent(ul, ll, ur, lr)\n    return extent", "response": "Return min max extent of dataset based on corner coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ds_geom(ds, t_srs=None):\n    gt = ds.GetGeoTransform()\n    ds_srs = get_ds_srs(ds)\n    if t_srs is None:\n        t_srs = ds_srs\n    ns = ds.RasterXSize\n    nl = ds.RasterYSize\n    x = np.array([0, ns, ns, 0, 0], dtype=float)\n    y = np.array([0, 0, nl, nl, 0], dtype=float)\n    #Note: pixelToMap adds 0.5 to input coords, need to account for this here\n    x -= 0.5\n    y -= 0.5\n    mx, my = pixelToMap(x, y, gt)\n    geom_wkt = 'POLYGON(({0}))'.format(', '.join(['{0} {1}'.format(*a) for a in zip(mx,my)]))\n    geom = ogr.CreateGeometryFromWkt(geom_wkt)\n    geom.AssignSpatialReference(ds_srs)\n    if not ds_srs.IsSame(t_srs):\n        geom_transform(geom, t_srs)\n    return geom", "response": "Return dataset bbox envelope as geom\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geom_wh(geom):\n    e = geom.GetEnvelope()\n    h = e[1] - e[0]\n    w = e[3] - e[2]\n    return w, h", "response": "Compute width and height of geometry in projected units\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gdaldem_mem_ma(ma, ds=None, res=None, extent=None, srs=None, processing='hillshade', returnma=False, computeEdges=False):\n    if ds is None:\n        ds = mem_ds(res, extent, srs=None, dtype=gdal.GDT_Float32)\n    else:\n        ds = mem_ds_copy(ds)\n    b = ds.GetRasterBand(1)\n    b.WriteArray(ma)\n    out = gdaldem_mem_ds(ds, processing=processing, returnma=returnma)\n    return out", "response": "Wrapper to allow gdaldem calculations for arbitrary NumPy masked array input\n    Untested, work in progress placeholder\n    Should only need to specify res, can caluclate local gt, cartesian srs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap for gdaldem functions gdaldem_mem_ds Uses GDALdem API requires GDAL v2. 1 +", "response": "def gdaldem_mem_ds(ds, processing='hillshade', returnma=False, computeEdges=False):\n    \"\"\"\n    Wrapper for gdaldem functions\n\n    Uses gdaldem API, requires GDAL v2.1+\n    \"\"\"\n    choices = [\"hillshade\", \"slope\", \"aspect\", \"color-relief\", \"TRI\", \"TPI\", \"Roughness\"]\n    out = None\n    scale=1.0\n    if not get_ds_srs(ds).IsProjected():\n        scale=111120\n    if processing in choices:\n        out = gdal.DEMProcessing('', ds, processing, format='MEM', computeEdges=computeEdges, scale=scale)\n    else:\n        print(\"Invalid processing choice\")\n        print(choices)\n    #This should be a separate function\n    if returnma:\n        from pygeotools.lib import iolib\n        out = iolib.ds_getma(out)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps for gdaldem functions Taxonomy", "response": "def gdaldem_wrapper(fn, product='hs', returnma=True, verbose=True):\n    \"\"\"Wrapper for gdaldem functions\n\n    Note: gdaldem is directly avaialable through API as of GDAL v2.1\n\n    https://trac.osgeo.org/gdal/wiki/rfc59.1_utilities_as_a_library\n\n    This function is no longer necessry, and will eventually be removed.\n    \"\"\"\n    #These gdaldem functions should be able to ingest masked array\n    #Just write out temporary file, or maybe mem vrt?\n    valid_opt = ['hillshade', 'hs', 'slope', 'aspect', 'color-relief', 'TRI', 'TPI', 'roughness']\n    try:\n        open(fn)\n    except IOError:\n        print(\"Unable to open %s\" %fn)\n\n    if product not in valid_opt: \n        print(\"Invalid gdaldem option specified\")\n\n    import subprocess\n    from pygeotools.lib import iolib\n    bma = None\n    opts = []\n    if product == 'hs' or product == 'hillshade':\n        product = 'hillshade'\n        #opts = ['-compute_edges',]\n        out_fn = os.path.splitext(fn)[0]+'_hs_az315.tif'\n    else:\n        out_fn = os.path.splitext(fn)[0]+'_%s.tif' % product\n    if not os.path.exists(out_fn):\n        cmd = ['gdaldem', product]\n        cmd.extend(opts)\n        cmd.extend(iolib.gdal_opt_co)\n        cmd.extend([fn, out_fn])\n        if verbose:\n            print(' '.join(cmd))\n            cmd_opt = {}\n        else:\n            fnull = open(os.devnull, 'w')\n            cmd_opt = {'stdout':fnull, 'stderr':subprocess.STDOUT}\n        subprocess.call(cmd, shell=False, **cmd_opt)\n\n    if returnma:\n        ds = gdal.Open(out_fn, gdal.GA_ReadOnly)\n        bma = iolib.ds_getma(ds, 1)\n        return bma \n    else:\n        return out_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns offset for center of ds", "response": "def get_geoid_offset(ds, geoid_srs=egm08_srs):\n    \"\"\"Return offset for center of ds\n    \n    Offset is added to input (presumably WGS84 HAE) to get to geoid\n    \n    Note: requires vertical offset grids in proj share dir - see earlier note\n    \"\"\"\n    ds_srs = get_ds_srs(ds)\n    c = get_center(ds)\n    x, y, z = cT_helper(c[0], c[1], 0.0, ds_srs, geoid_srs)\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_xy_ma(bma, gt, stride=1, origmask=True, newmask=None):\n    pX = np.arange(0, bma.shape[1], stride)\n    pY = np.arange(0, bma.shape[0], stride)\n    psamp = np.meshgrid(pX, pY)\n    #if origmask:\n    #    psamp = np.ma.array(psamp, mask=np.ma.getmaskarray(bma), fill_value=0)\n    mX, mY = pixelToMap(psamp[0], psamp[1], gt)\n    mask = None\n    if origmask:\n        mask = np.ma.getmaskarray(bma)[::stride]\n    if newmask is not None:\n        mask = newmask[::stride]\n    mX = np.ma.array(mX, mask=mask, fill_value=0)\n    mY = np.ma.array(mY, mask=mask, fill_value=0)\n    return mX, mY", "response": "Return arrays of x and y map coordinates for input array and geotransform\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xy_1D(ds, stride=1, getval=False):\n    gt = ds.GetGeoTransform()\n    #stride = stride_m/gt[1]\n    pX = np.arange(0, ds.RasterXSize, stride)\n    pY = np.arange(0, ds.RasterYSize, stride)\n    mX, dummy = pixelToMap(pX, pY[0], gt)\n    dummy, mY = pixelToMap(pX[0], pY, gt)\n    return mX, mY", "response": "Return 1D arrays of x and y map coordinates for input GDAL Dataset \n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_xy_grids(ds, stride=1, getval=False):\n    gt = ds.GetGeoTransform()\n    #stride = stride_m/gt[1]\n    pX = np.arange(0, ds.RasterXSize, stride)\n    pY = np.arange(0, ds.RasterYSize, stride)\n    psamp = np.meshgrid(pX, pY)\n    mX, mY = pixelToMap(psamp[0], psamp[1], gt)\n    return mX, mY", "response": "Return 2D arrays of x and y map coordinates for input GDAL Dataset \n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fitPlaneSVD(XYZ):\n    [rows,cols] = XYZ.shape\n    # Set up constraint equations of the form  AB = 0,\n    # where B is a column vector of the plane coefficients\n    # in the form b(1)*X + b(2)*Y +b(3)*Z + b(4) = 0.\n    p = (np.ones((rows,1)))\n    AB = np.hstack([XYZ,p])\n    [u, d, v] = np.linalg.svd(AB,0)        \n    # Solution is last column of v.\n    B = np.array(v[3,:])\n    coeff = -B[[0, 1, 3]]/B[2]\n    return coeff", "response": "Fit a plane to input point data using SVD"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fitPlaneLSQ(XYZ):\n    [rows,cols] = XYZ.shape\n    G = np.ones((rows,3))\n    G[:,0] = XYZ[:,0]  #X\n    G[:,1] = XYZ[:,1]  #Y\n    Z = XYZ[:,2]\n    coeff,resid,rank,s = np.linalg.lstsq(G,Z,rcond=None)\n    return coeff", "response": "Fit a plane to input point data using LSQ\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ma_fitpoly(bma, order=1, gt=None, perc=(2,98), origmask=True):\n    if gt is None:\n        gt = [0, 1, 0, 0, 0, -1] \n    #Filter, can be useful to remove outliers\n    if perc is not None:\n        from pygeotools.lib import filtlib\n        bma_f = filtlib.perc_fltr(bma, perc)\n    else:\n        bma_f = bma\n    #Get indices\n    x, y = get_xy_ma(bma_f, gt, origmask=origmask)\n    #Fit only where we have valid data\n    bma_mask = np.ma.getmaskarray(bma)\n    coeff = polyfit2d(x[~bma_mask].data, y[~bma_mask].data, bma[~bma_mask].data, order=order) \n    #For 1D, these are: c, y, x, xy\n    print(coeff)\n    #Compute values for all x and y, unless origmask=True\n    vals = polyval2d(x, y, coeff) \n    resid = bma - vals\n    return vals, resid, coeff", "response": "Fit a plane to values in input array bma"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ma_fitplane(bma, gt=None, perc=(2,98), origmask=True):\n    if gt is None:\n        gt = [0, 1, 0, 0, 0, -1] \n    #Filter, can be useful to remove outliers\n    if perc is not None:\n        from pygeotools.lib import filtlib\n        bma_f = filtlib.perc_fltr(bma, perc)\n    else:\n        bma_f = bma\n    #Get indices\n    x_f, y_f = get_xy_ma(bma_f, gt, origmask=origmask)\n    #Regardless of desired output (origmask True or False), for fit, need to limit to valid pixels only \n    bma_f_mask = np.ma.getmaskarray(bma_f)\n    #Create xyz stack, needed for SVD\n    xyz = np.vstack((np.ma.array(x_f, mask=bma_f_mask).compressed(), \\\n            np.ma.array(y_f, mask=bma_f_mask).compressed(), bma_f.compressed())).T\n    #coeff = fitPlaneSVD(xyz)\n    coeff = fitPlaneLSQ(xyz)\n    print(coeff)\n    vals = coeff[0]*x_f + coeff[1]*y_f + coeff[2]\n    resid = bma_f - vals\n    return vals, resid, coeff", "response": "Fit a plane to values in input array"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit a plane to values in GDAL Dataset", "response": "def ds_fitplane(ds):\n    \"\"\"Fit a plane to values in GDAL Dataset\n    \"\"\"\n    from pygeotools.lib import iolib\n    bma = iolib.ds_getma(ds)\n    gt = ds.GetGeoTransform()\n    return ma_fitplane(bma, gt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine UTM Zone for input geometry", "response": "def getUTMzone(geom):\n    \"\"\"Determine UTM Zone for input geometry\n    \"\"\"\n    #If geom has srs properly defined, can do this\n    #geom.TransformTo(wgs_srs)\n    #Get centroid lat/lon\n    lon, lat = geom.Centroid().GetPoint_2D()\n    #Make sure we're -180 to 180\n    lon180 = (lon+180) - np.floor((lon+180)/360)*360 - 180\n    zonenum = int(np.floor((lon180 + 180)/6) + 1)\n    #Determine N/S hemisphere\n    if lat >= 0:\n        zonehem = 'N'\n    else:\n        zonehem = 'S'\n    #Deal with special cases\n    if (lat >= 56.0 and lat < 64.0 and lon180 >= 3.0 and lon180 < 12.0):\n        zonenum = 32\n    if (lat >= 72.0 and lat < 84.0): \n        if (lon180 >= 0.0 and lon180 < 9.0): \n            zonenum = 31\n        elif (lon180 >= 9.0 and lon180 < 21.0):\n            zonenum = 33\n        elif (lon180 >= 21.0 and lon180 < 33.0):\n            zonenum = 35\n        elif (lon180 >= 33.0 and lon180 < 42.0):\n            zonenum = 37\n    return str(zonenum)+zonehem"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_proj(geom, proj_list=None):\n    out_srs = None\n    if proj_list is None:\n        proj_list = gen_proj_list()\n    #Go through user-defined projeciton list\n    for projbox in proj_list:\n        if projbox.geom.Intersects(geom):\n            out_srs = projbox.srs\n            break\n    #If geom doesn't fall in any of the user projection bbox, use UTM\n    if out_srs is None:\n        out_srs = getUTMsrs(geom)\n    return out_srs", "response": "Determine the best projection for a geometry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_proj_list():\n    #Eventually, just read this in from a text file\n    proj_list = []\n    #Alaska\n    #Note, this spans -180/180\n    proj_list.append(ProjBox([-180, -130, 51.35, 71.35], 3338))\n    #proj_list.append(ProjBox([-130, 172.4, 51.35, 71.35], 3338))\n    #Transantarctic Mountains\n    proj_list.append(ProjBox([150, 175, -80, -70], 3294))\n    #Greenland\n    proj_list.append(ProjBox([-180, 180, 58, 82], 3413))\n    #Antarctica\n    proj_list.append(ProjBox([-180, 180, -90, -58], 3031))\n    #Arctic\n    proj_list.append(ProjBox([-180, 180, 60, 90], 3413))\n    return proj_list", "response": "Create list of projections with cascading preference\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xy2geom(x, y, t_srs=None):\n    geom_wkt = 'POINT({0} {1})'.format(x, y)\n    geom = ogr.CreateGeometryFromWkt(geom_wkt)\n    if t_srs is not None and not wgs_srs.IsSame(t_srs):\n        ct = osr.CoordinateTransformation(t_srs, wgs_srs)\n        geom.Transform(ct)\n        geom.AssignSpatialReference(t_srs)\n    return geom", "response": "Convert x and y point coordinates to geometry"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate dem_mosaic command for a single - threaded mosaicing process.", "response": "def get_dem_mosaic_cmd(fn_list, o, fn_list_txt=None, tr=None, t_srs=None, t_projwin=None, georef_tile_size=None, threads=None, tile=None, stat=None):\n    \"\"\"\n    Create ASP dem_mosaic command \n    Useful for spawning many single-threaded mosaicing processes\n    \"\"\"\n    cmd = ['dem_mosaic',]\n    if o is None:\n        o = 'mos'\n    cmd.extend(['-o', o])\n    if threads is None:\n        from pygeotools.lib import iolib\n        threads = iolib.cpu_count()\n    cmd.extend(['--threads', threads])\n    if tr is not None:\n        cmd.extend(['--tr', tr])\n    if t_srs is not None:\n        #cmd.extend(['--t_srs', t_srs.ExportToProj4()])\n        cmd.extend(['--t_srs', '\"%s\"' % t_srs.ExportToProj4()])\n        #cmd.extend(['--t_srs', \"%s\" % t_srs.ExportToProj4()])\n    if t_projwin is not None:\n        cmd.append('--t_projwin')\n        cmd.extend(t_projwin)\n        cmd.append('--force-projwin')\n    if tile is not None:\n        #Not yet implemented\n        #cmd.extend(tile_list)\n        cmd.append('--tile-index')\n        cmd.append(tile)\n    if georef_tile_size is not None:\n        cmd.extend(['--georef-tile-size', georef_tile_size])\n    if stat is not None:\n        if stat == 'wmean':\n            stat = None\n        else:\n            cmd.append('--%s' % stat.replace('index',''))\n            if stat in ['lastindex', 'firstindex', 'medianindex']:\n                #This will write out the index map to -last.tif by default\n                cmd.append('--save-index-map')\n                #Make sure we don't have ndv that conflicts with 0-based DEM indices\n                cmd.extend(['--output-nodata-value','-9999'])\n    #else:\n    #    cmd.extend(['--save-dem-weight', o+'_weight'])\n    #If user provided a file containing list of DEMs to mosaic (useful to avoid long bash command issues)\n    if fn_list_txt is not None:\n        if os.path.exists(fn_list_txt):\n            cmd.append('-l')\n            cmd.append(fn_list_txt)\n        else: \n            print(\"Could not find input text file containing list of inputs\")\n    else:\n        cmd.extend(fn_list)\n    cmd = [str(i) for i in cmd]\n    #print(cmd)\n    #return subprocess.call(cmd)\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef warp(src_ds, res=None, extent=None, t_srs=None, r='cubic', driver=mem_drv, dst_fn=None, dst_ndv=None, verbose=True):\n    src_srs = geolib.get_ds_srs(src_ds)\n    \n    if t_srs is None:\n        t_srs = geolib.get_ds_srs(src_ds)\n    \n    src_gt = src_ds.GetGeoTransform()\n    #Note: get_res returns [x_res, y_res]\n    #Could just use gt here and average x_res and y_res\n    src_res = geolib.get_res(src_ds, t_srs=t_srs, square=True)[0]\n\n    if res is None:\n        res = src_res\n\n    if extent is None:\n        extent = geolib.ds_geom_extent(src_ds, t_srs=t_srs)\n    \n    #Note: GDAL Lanczos creates block artifacts\n    #Wait for gdalwarp to support gaussian resampling\n    #Want to use Lanczos for downsampling\n    #if src_res < res:\n    #    gra = gdal.GRA_Lanczos\n    #See http://blog.codinghorror.com/better-image-resizing/\n    # Suggests cubic for downsampling, bilinear for upsampling\n    #    gra = gdal.GRA_Cubic\n    #Cubic for upsampling\n    #elif src_res >= res:\n    #    gra = gdal.GRA_Bilinear\n\n    gra = parse_rs_alg(r)\n\n    #At this point, the resolution and extent values must be float\n    #Extent must be list\n    res = float(res)\n    extent = [float(i) for i in extent]\n\n    #Might want to move this to memwarp_multi, keep memwarp basic w/ gdal.GRA types\n\n    #Create progress function\n    prog_func = None\n    if verbose:\n        prog_func = gdal.TermProgress\n    \n    if dst_fn is None:\n        #This is a dummy fn if only in mem, but can be accessed later via GetFileList()\n        #Actually, no, doesn't look like the filename survivies\n        dst_fn = ''\n    \n    #Compute output image dimensions\n    dst_nl = int(round((extent[3] - extent[1])/res))\n    dst_ns = int(round((extent[2] - extent[0])/res))\n    #dst_nl = int(math.ceil((extent[3] - extent[1])/res))\n    #dst_ns = int(math.ceil((extent[2] - extent[0])/res))\n    #dst_nl = int(math.floor((extent[3] - extent[1])/res))\n    #dst_ns = int(math.floor((extent[2] - extent[0])/res))\n    if verbose:\n        print('nl: %i ns: %i res: %0.3f' % (dst_nl, dst_ns, res))\n    #Create output dataset\n    src_b = src_ds.GetRasterBand(1)\n    src_dt = src_b.DataType\n    src_nl = src_ds.RasterYSize\n    src_ns = src_ds.RasterXSize\n\n    dst_ds = driver.Create(dst_fn, dst_ns, dst_nl, src_ds.RasterCount, src_dt) \n\n    dst_ds.SetProjection(t_srs.ExportToWkt())\n    #Might be an issue to use src_gt rotation terms here with arbitrary extent/res\n    dst_gt = [extent[0], res, src_gt[2], extent[3], src_gt[4], -res]\n    dst_ds.SetGeoTransform(dst_gt)\n   \n    #This will smooth the input before downsampling to prevent aliasing, fill gaps\n    #Pretty inefficent, as we need to create another intermediate dataset\n    gauss = False \n\n    for n in range(1, src_ds.RasterCount+1):\n        if dst_ndv is None:\n            src_b = src_ds.GetRasterBand(n)\n            src_ndv = iolib.get_ndv_b(src_b)\n            dst_ndv = src_ndv\n        b = dst_ds.GetRasterBand(n)\n        b.SetNoDataValue(dst_ndv)\n        b.Fill(dst_ndv)\n\n        if gauss:\n            from pygeotools.lib import filtlib\n            #src_a = src_b.GetVirtualMemArray()\n            #Compute resampling ratio to determine filter window size\n            res_ratio = float(res)/src_res\n            if verbose:\n                print(\"Resampling factor: %0.3f\" % res_ratio)\n            #Might be more efficient to do iterative gauss filter with size 3, rather than larger windows\n            f_size = math.floor(res_ratio/2.)*2+1\n            #This is conservative to avoid filling holes with noise\n            #f_size = math.floor(res_ratio/2.)*2-1\n            if f_size <= 1:\n                continue\n\n            if verbose:\n                print(\"Smoothing window size: %i\" % f_size)\n            #Create temp dataset to store filtered array - avoid overwriting original\n            temp_ds = driver.Create('', src_ns, src_nl, src_ds.RasterCount, src_dt) \n            temp_ds.SetProjection(src_srs.ExportToWkt())\n            temp_ds.SetGeoTransform(src_gt)\n            temp_b = temp_ds.GetRasterBand(n)\n            temp_b.SetNoDataValue(dst_ndv)\n            temp_b.Fill(dst_ndv)\n\n            src_a = iolib.b_getma(src_b)\n            src_a = filtlib.gauss_fltr_astropy(src_a, size=f_size)\n            #Want to run with maskfill, so only fills gaps, without expanding isolated points\n            temp_b.WriteArray(src_a)\n            src_ds = temp_ds\n            \n            #In theory, NN should be fine since we already smoothed.  In practice, cubic still provides slightly better results\n            #gra = gdal.GRA_NearestNeighbour\n    \n    \"\"\"\n    if not verbose:\n        #Suppress GDAL progress bar\n        orig_stdout = sys.stdout\n        sys.stdout = open(os.devnull, 'w')\n    \"\"\"\n\n    #Note: default maxerror=0.0, second 0.0 argument\n    gdal.ReprojectImage(src_ds, dst_ds, src_srs.ExportToWkt(), t_srs.ExportToWkt(), gra, 0.0, 0.0, prog_func)\n\n    \"\"\"\n    if not verbose:\n        sys.stdout.close()\n        sys.stdout = orig_stdout\n    \"\"\"\n\n    #Note: this is now done in diskwarp\n    #Write out to disk\n    #if driver != mem_drv:\n    #    dst_ds.FlushCache()\n\n    #Return GDAL dataset object in memory\n    return dst_ds", "response": "Warp an input dataset with predetermined arguments specifying output res extent and srs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef memwarp(src_ds, res=None, extent=None, t_srs=None, r=None, oudir=None, dst_ndv=0, verbose=True):\n    driver = iolib.mem_drv\n    return warp(src_ds, res, extent, t_srs, r, driver=driver, dst_ndv=dst_ndv, verbose=verbose)", "response": "Wrapper function that calls warp for single input Dataset with output to memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse arbitrary input t_srs into a set of objects.", "response": "def parse_srs(t_srs, src_ds_list=None):\n    \"\"\"Parse arbitrary input t_srs\n\n    Parameters\n    ----------\n    t_srs : str or gdal.Dataset or filename\n        Arbitrary input t_srs \n    src_ds_list : list of gdal.Dataset objects, optional\n        Needed if specifying 'first' or 'last'\n\n    Returns\n    -------\n    t_srs : osr.SpatialReference() object\n        Output spatial reference system\n    \"\"\"\n    if t_srs is None and src_ds_list is None:\n        print(\"Input t_srs and src_ds_list are both None\")\n    else:\n        if t_srs is None:\n            t_srs = 'first'\n        if t_srs == 'first' and src_ds_list is not None:\n            t_srs = geolib.get_ds_srs(src_ds_list[0])\n        elif t_srs == 'last' and src_ds_list is not None:\n            t_srs = geolib.get_ds_srs(src_ds_list[-1])\n        #elif t_srs == 'source':\n        #    t_srs = None \n        elif isinstance(t_srs, osr.SpatialReference): \n            pass\n        elif isinstance(t_srs, gdal.Dataset):\n            t_srs = geolib.get_ds_srs(t_srs)\n        elif isinstance(t_srs, str) and os.path.exists(t_srs): \n            t_srs = geolib.get_ds_srs(gdal.Open(t_srs))\n        elif isinstance(t_srs, str):\n            temp = osr.SpatialReference()\n            if 'EPSG' in t_srs.upper():\n                epsgcode = int(t_srs.split(':')[-1])\n                temp.ImportFromEPSG(epsgcode)\n            elif 'proj' in t_srs:\n                temp.ImportFromProj4(t_srs)\n            else:\n                #Assume the user knows what they are doing\n                temp.ImportFromWkt(t_srs)\n            t_srs = temp\n        else:\n            t_srs = None\n    return t_srs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses arbitrary input res Taxonomy", "response": "def parse_res(res, src_ds_list=None, t_srs=None):\n    \"\"\"Parse arbitrary input res \n\n    Parameters\n    ----------\n    res : str or gdal.Dataset or filename or float\n        Arbitrary input res \n    src_ds_list : list of gdal.Dataset objects, optional\n        Needed if specifying 'first' or 'last'\n    t_srs : osr.SpatialReference() object \n        Projection for res calculations, optional\n\n    Returns\n    -------\n    res : float \n        Output resolution\n        None if source resolution should be preserved\n    \"\"\"\n    #Default to using first t_srs for res calculations\n    #Assumes src_ds_list is not None\n    t_srs = parse_srs(t_srs, src_ds_list)\n\n    #Valid options for res\n    res_str_list = ['first', 'last', 'min', 'max', 'mean', 'med', 'common_scale_factor']\n\n    #Compute output resolution in t_srs\n    if res in res_str_list and src_ds_list is not None:\n        #Returns min, max, mean, med\n        res_stats = geolib.get_res_stats(src_ds_list, t_srs=t_srs)\n        if res == 'first':\n            res = geolib.get_res(src_ds_list[0], t_srs=t_srs, square=True)[0]\n        elif res == 'last':\n            res = geolib.get_res(src_ds_list[-1], t_srs=t_srs, square=True)[0]\n        elif res == 'min':\n            res = res_stats[0]\n        elif res == 'max':\n            res = res_stats[1]\n        elif res == 'mean':\n            res = res_stats[2]\n        elif res == 'med':\n            res = res_stats[3]\n        elif res == 'common_scale_factor':\n            #Determine res to upsample min and downsample max by constant factor\n            res = np.sqrt(res_stats[1]/res_stats[0]) * res_stats[0]\n    elif res == 'source':\n        res = None\n    elif isinstance(res, gdal.Dataset):\n        res = geolib.get_res(res, t_srs=t_srs, square=True)[0]\n    elif isinstance(res, str) and os.path.exists(res): \n        res = geolib.get_res(gdal.Open(res), t_srs=t_srs, square=True)[0]\n    else:\n        res = float(res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_extent(extent, src_ds_list=None, t_srs=None):\n\n    #Default to using first t_srs for extent calculations\n    if t_srs is not None:\n        t_srs = parse_srs(t_srs, src_ds_list)\n\n    #Valid strings\n    extent_str_list = ['first', 'last', 'intersection', 'union']\n\n    if extent in extent_str_list and src_ds_list is not None:\n        if len(src_ds_list) == 1 and (extent == 'intersection' or extent == 'union'):\n            extent = None\n        elif extent == 'first':\n            extent = geolib.ds_geom_extent(src_ds_list[0], t_srs=t_srs)\n            #extent = geolib.ds_extent(src_ds_list[0], t_srs=t_srs)\n        elif extent == 'last':\n            extent = geolib.ds_geom_extent(src_ds_list[-1], t_srs=t_srs)\n            #extent = geolib.ds_extent(src_ds_list[-1], t_srs=t_srs)\n        elif extent == 'intersection':\n            #By default, compute_intersection takes ref_srs from ref_ds\n            extent = geolib.ds_geom_intersection_extent(src_ds_list, t_srs=t_srs)\n            if len(src_ds_list) > 1 and extent is None:\n                sys.exit(\"Input images do not intersect\")\n        elif extent == 'union':\n            #Need to clean up union t_srs handling\n            extent = geolib.ds_geom_union_extent(src_ds_list, t_srs=t_srs)\n    elif extent == 'source':\n        extent = None\n    elif isinstance(extent, gdal.Dataset):\n        extent = geolib.ds_geom_extent(extent, t_srs=t_srs)\n    elif isinstance(extent, str) and os.path.exists(extent): \n        extent = geolib.ds_geom_extent(gdal.Open(extent), t_srs=t_srs)\n    elif isinstance(extent, (list, tuple, np.ndarray)):\n        extent = list(extent)\n    else:\n        extent = [float(i) for i in extent.split(' ')]\n    return extent", "response": "Parse arbitrary input extent from source dataset list or source dataset list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef warp_multi(src_ds_list, res='first', extent='intersection', t_srs='first', r='cubic', warptype=memwarp, outdir=None, dst_ndv=None, verbose=True, debug=False):\n    #Type cast arguments as str for evaluation\n    #Avoid path errors\n    #res = str(res)\n    #extent = str(extent)\n    #t_srs = str(t_srs)\n\n    #Parse the input\n    t_srs = parse_srs(t_srs, src_ds_list)\n    res = parse_res(res, src_ds_list, t_srs)\n    extent = parse_extent(extent, src_ds_list, t_srs)\n\n    if verbose:\n        print(\"\\nWarping all inputs to the following:\")\n        print(\"Resolution: %s\" % res)\n        print(\"Extent: %s\" % str(extent))\n        print(\"Projection: '%s'\" % t_srs.ExportToProj4())\n        print(\"Resampling alg: %s\\n\" % r)  \n\n    out_ds_list = []\n    for i, ds in enumerate(src_ds_list):\n        fn_list = ds.GetFileList()\n        fn = '[memory]'\n        if fn_list is not None:\n            fn = fn_list[0]\n        if verbose:\n            print(\"%i of %i: %s\" % (i+1, len(src_ds_list), fn))\n\n        #If input srs are different, must warp\n        ds_t_srs = geolib.get_ds_srs(ds)\n        srscheck = bool(t_srs.IsSame(ds_t_srs))\n       \n        if debug:\n            print('\\n%s' % ds_t_srs.ExportToWkt())\n            print('%s\\n' % t_srs.ExportToWkt())\n            print('srscheck: %s\\n' % srscheck)\n\n        rescheck = False\n        extentcheck = False\n\n        #if srscheck:\n        #Extract info from ds to see if warp is necessary\n        ds_res = geolib.get_res(ds, square=True)[0]\n        ds_extent = geolib.ds_extent(ds)\n\n        #Note: these checks necessary to handle rounding and precision issues\n        #Round extent and res to nearest mm\n        precision = 1E-3\n        #Or if t_srs has units of degrees\n        if ds_t_srs.IsGeographic():\n            precision = 1E-8\n\n        rescheck = (res is None) or geolib.res_compare(res, ds_res, precision=precision)\n        extentcheck = (extent is None) or geolib.extent_compare(extent, ds_extent, precision=precision)\n\n        if debug:\n            print('\\n%s, %s\\n' % (ds_res, res)) \n            print('%s' % ds_extent)\n            print('%s\\n' % extent) \n            print('rescheck: %s' % rescheck)\n            print('extentcheck: %s\\n' % extentcheck)\n\n        #If the ds passes all three, it is identical to desired output, short circuit\n        if rescheck and extentcheck and srscheck:\n            out_ds_list.append(ds)\n        else:\n            dst_ds = warptype(ds, res, extent, t_srs, r, outdir, dst_ndv=dst_ndv, verbose=verbose)\n            out_ds_list.append(dst_ds)\n\n    return out_ds_list", "response": "This function will warp all input datasets into one single dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps function for memwarp of multiple GDAL Datasets", "response": "def memwarp_multi(src_ds_list, res='first', extent='intersection', t_srs='first', r='cubic', verbose=True, dst_ndv=0):\n    \"\"\"Helper function for memwarp of multiple input GDAL Datasets\n    \"\"\"\n    return warp_multi(src_ds_list, res, extent, t_srs, r, warptype=memwarp, verbose=verbose, dst_ndv=dst_ndv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef memwarp_multi_fn(src_fn_list, res='first', extent='intersection', t_srs='first', r='cubic', verbose=True, dst_ndv=0):\n    #Should implement proper error handling here\n    if not iolib.fn_list_check(src_fn_list):\n        sys.exit('Missing input file(s)')\n    src_ds_list = [gdal.Open(fn, gdal.GA_ReadOnly) for fn in src_fn_list]\n    return memwarp_multi(src_ds_list, res, extent, t_srs, r, verbose=verbose, dst_ndv=dst_ndv)", "response": "This function is used to memwarp a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef diskwarp_multi(src_ds_list, res='first', extent='intersection', t_srs='first', r='cubic', verbose=True, outdir=None, dst_ndv=None):\n    return warp_multi(src_ds_list, res, extent, t_srs, r, verbose=verbose, warptype=diskwarp, outdir=outdir, dst_ndv=dst_ndv)", "response": "This function will diskwarp a set of GDAL Datasets and return the GDAL Datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diskwarp_multi_fn(src_fn_list, res='first', extent='intersection', t_srs='first', r='cubic', verbose=True, outdir=None, dst_ndv=None):\n    #Should implement proper error handling here\n    if not iolib.fn_list_check(src_fn_list):\n        sys.exit('Missing input file(s)')\n    src_ds_list = [gdal.Open(fn, gdal.GA_ReadOnly) for fn in src_fn_list]\n    return diskwarp_multi(src_ds_list, res, extent, t_srs, r, verbose=verbose, outdir=outdir, dst_ndv=dst_ndv)", "response": "This function is used to diskwarp a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef writeout(ds, outfn):\n    print(\"Writing out %s\" % outfn) \n    #Use outfn extension to get driver\n    #This may have issues if outfn already exists and the mem ds has different dimensions/res\n    out_ds = iolib.gtif_drv.CreateCopy(outfn, ds, 0, options=iolib.gdal_opt)\n    out_ds = None", "response": "Write ds to outfn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getTimeZone(lat, lon):\n    #Need to fix for Python 2.x and 3.X support\n    import urllib.request, urllib.error, urllib.parse\n    import xml.etree.ElementTree as ET\n    #http://api.askgeo.com/v1/918/aa8292ec06199d1207ccc15be3180213c984832707f0cbf3d3859db279b4b324/query.xml?points=37.78%2C-122.42%3B40.71%2C-74.01&databases=Point%2CTimeZone%2CAstronomy%2CNaturalEarthCountry%2CUsState2010%2CUsCounty2010%2CUsCountySubdivision2010%2CUsTract2010%2CUsBlockGroup2010%2CUsPlace2010%2CUsZcta2010\n    req = \"http://api.askgeo.com/v1/918/aa8292ec06199d1207ccc15be3180213c984832707f0cbf3d3859db279b4b324/query.xml?points=\"+str(lat)+\"%2C\"+str(lon)+\"&databases=TimeZone\"\n    opener = urllib.request.build_opener()\n    f = opener.open(req)\n    tree = ET.parse(f)\n    root = tree.getroot()\n    #Check response\n    tzid = None\n    if root.attrib['code'] == '0':\n        tz = list(root.iter('TimeZone'))[0]\n        #shortname = tz.attrib['ShortName']\n        tzid = tz.attrib['TimeZoneId']\n    return tzid", "response": "Get the timezone for a given latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getLocalTime(utc_dt, tz):\n    import pytz\n    local_tz = pytz.timezone(tz)\n    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n    return local_dt", "response": "Return local time of the given utc_dt with the given timezone"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing local solar time for given ( lat lon )", "response": "def solarTime(utc_dt, lat, lon):\n    \"\"\"Compute local solar time for given (lat, lon)\n    \"\"\"\n    import ephem\n    o = ephem.Observer()\n    o.date = utc_dt\n    o.lat = str(lat)\n    o.lon = str(lon)\n    sun = ephem.Sun()\n    sun.compute(o)\n    hour_angle = o.sidereal_time() - sun.ra\n    rad = str(ephem.hours(hour_angle + ephem.hours('12:00')).norm)\n    t = datetime.strptime(rad, '%H:%M:%S.%f')\n    solar_dt = datetime.combine(utc_dt.date(), t.time()) \n    return solar_dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strptime_fuzzy(s):\n    import dateutil.parser\n    dt = dateutil.parser.parse(str(s), fuzzy=True) \n    return dt", "response": "Fuzzy date string parsing\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fn_getdatetime_list(fn):\n    #Want to split last component\n    fn = os.path.split(os.path.splitext(fn)[0])[-1]\n    import re\n    #WV01_12JUN152223255-P1BS_R1C1-102001001B3B9800__WV01_12JUN152224050-P1BS_R1C1-102001001C555C00-DEM_4x.tif\n    #Need to parse above with month name \n    #Note: made this more restrictive to avoid false matches:\n    #'20130304_1510_1030010020770600_1030010020CEAB00-DEM_4x'\n    #This is a problem, b/c 2015/17/00:\n    #WV02_20130315_10300100207D5600_1030010020151700\n    #This code should be obsolete before 2019 \n    #Assume new filenames\n    #fn = fn[0:13]\n    #Use cascading re find to pull out timestamps\n    #Note: Want to be less restrictive here - could have a mix of YYYYMMDD_HHMM, YYYYMMDD and YYYY in filename\n    #Should probably search for all possibilities, then prune  \n    #NOTE: these don't include seconds in the time\n    #NOTE: could have 20130304_1510__20130304__whatever in filename\n    #The current approach will only catch the first datetime \n    dstr = None\n    out = None\n    #20180101_1200 or 20180101T1200\n    dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9](?:0[1-9]|1[012])(?:0[1-9]|[12][0-9]|3[01])[_T](?:0[0-9]|1[0-9]|2[0-3])[0-5][0-9]', fn)\n    #201801011200\n    if not dstr:\n        dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9](?:0[1-9]|1[012])(?:0[1-9]|[12][0-9]|3[01])(?:0[0-9]|1[0-9]|2[0-3])[0-5][0-9]', fn)\n    #20180101\n    if not dstr:\n        dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9](?:0[1-9]|1[012])(?:0[1-9]|[12][0-9]|3[01])(?:$|_|-)', fn)\n        #This should pick up dates separated by a dash\n        #dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9](?:0[1-9]|1[012])(?:0[1-9]|[12][0-9]|3[01])', fn)\n    #2018.609990\n    if not dstr:\n        dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9]\\.[0-9][0-9][0-9]*(?:$|_|-)', fn)\n        dstr = [d.lstrip('_').rstrip('_') for d in dstr]\n        dstr = [d.lstrip('-').rstrip('-') for d in dstr]\n        out = [decyear2dt(float(s)) for s in dstr]\n        dstr = None\n    #2018\n    if not dstr:\n        dstr = re.findall(r'(?:^|_|-)(?:19|20)[0-9][0-9](?:$|_|-)', fn)\n    #This is for USGS archive filenames\n    if not dstr:\n        dstr = re.findall(r'[0-3][0-9][a-z][a-z][a-z][0-9][0-9]', fn)\n        #This is USGS archive format\n        if dstr:\n            out = [datetime.strptime(s, '%d%b%y') for s in dstr][0]\n            dstr = None\n    if dstr:\n        #This is a hack to remove peripheral underscores and dashes\n        dstr = [d.lstrip('_').rstrip('_') for d in dstr]\n        dstr = [d.lstrip('-').rstrip('-') for d in dstr]\n        #This returns an empty list of nothing is found\n        out = [strptime_fuzzy(s) for s in dstr]\n    return out", "response": "Extract all datetime strings from input filename"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_t_factor(t1, t2):\n    t_factor = None\n    if t1 is not None and t2 is not None and t1 != t2:  \n        dt = t2 - t1\n        year = timedelta(days=365.25)\n        t_factor = abs(dt.total_seconds() / year.total_seconds()) \n    return t_factor", "response": "Get the time difference between two datetimes expressed as decimal year \n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort input filename list by datetime", "response": "def sort_fn_list(fn_list):\n    \"\"\"Sort input filename list by datetime\n    \"\"\"\n    dt_list = get_dt_list(fn_list)\n    fn_list_sort = [fn for (dt,fn) in sorted(zip(dt_list,fn_list))]\n    return fn_list_sort"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding some small offset to remove duplicate times Needed for xarray interp which expects monotonically increasing times", "response": "def fix_repeat_dt(dt_list, offset_s=0.001):\n    \"\"\"Add some small offset to remove duplicate times\n    Needed for xarray interp, which expects monotonically increasing times\n    \"\"\"\n    idx = (np.diff(dt_list) == timedelta(0))\n    while np.any(idx):\n        dt_list[idx.nonzero()[0] + 1] += timedelta(seconds=offset_s)\n        idx = (np.diff(dt_list) == timedelta(0))\n    return dt_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting list of datetime objects extracted from a filename", "response": "def get_dt_list(fn_list):\n    \"\"\"Get list of datetime objects, extracted from a filename\n    \"\"\"\n    dt_list = np.array([fn_getdatetime(fn) for fn in fn_list])\n    return dt_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_closest_dt_idx(dt, dt_list):\n    from pygeotools.lib import malib\n    dt_list = malib.checkma(dt_list, fix=False)\n    dt_diff = np.abs(dt - dt_list)\n    return dt_diff.argmin()", "response": "Get indices of dt_list that is closest to input dt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_closest_dt_padded_idx(dt, dt_list, pad=timedelta(days=30)):\n    #If pad is in decimal days\n    if not isinstance(pad, timedelta):\n        pad = timedelta(days=pad)\n    from pygeotools.lib import malib\n    dt_list = malib.checkma(dt_list, fix=False)\n    dt_diff = np.abs(dt - dt_list)\n    valid_idx = (dt_diff.data < pad).nonzero()[0]\n    return valid_idx", "response": "Get indices of dt_list that is closest to input dt + pad days"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dt_filter_rel_annual_idx(dt_list, min_rel_dt=(1,1), max_rel_dt=(12,31)):\n    dt_list = np.array(dt_list)\n    years = get_unique_years(dt_list)\n    from collections import OrderedDict\n    out = OrderedDict() \n    for year in years:\n        #If within the same year\n        if min_rel_dt[0] < max_rel_dt[1]:\n            dt1 = datetime(year, min_rel_dt[0], min_rel_dt[1])\n            dt2 = datetime(year, max_rel_dt[0], max_rel_dt[1])\n        #Or if our relative values include Jan 1\n        else:\n            dt1 = datetime(year, min_rel_dt[0], min_rel_dt[1])\n            dt2 = datetime(year+1, max_rel_dt[0], max_rel_dt[1])\n        idx = np.logical_and((dt_list >= dt1), (dt_list <= dt2))\n        if np.any(idx):\n            out[year] = idx\n    return out", "response": "Return a dictionary containing indices of timestamps that fall within relative month / day bounds of each year."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mean_date(dt_list):\n    dt_list_sort = sorted(dt_list)\n    dt_list_sort_rel = [dt - dt_list_sort[0] for dt in dt_list_sort]\n    avg_timedelta = sum(dt_list_sort_rel, timedelta())/len(dt_list_sort_rel)\n    return dt_list_sort[0] + avg_timedelta", "response": "Calcuate mean datetime from datetime list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef median_date(dt_list):\n    #dt_list_sort = sorted(dt_list)\n    idx = len(dt_list)/2\n    if len(dt_list) % 2 == 0:\n        md = mean_date([dt_list[idx-1], dt_list[idx]])\n    else:\n        md = dt_list[idx]\n    return md", "response": "Calcuate median datetime from datetime list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dt_cluster(dt_list, dt_thresh=16.0):\n    if not isinstance(dt_list[0], float):\n        o_list = dt2o(dt_list)\n    else:\n        o_list = dt_list\n    o_list_sort = np.sort(o_list)\n    o_list_sort_idx = np.argsort(o_list)\n    d = np.diff(o_list_sort)\n    #These are indices of breaks\n    #Add one so each b starts a cluster\n    b = np.nonzero(d > dt_thresh)[0] + 1 \n    #Add one to shape so we include final index\n    b = np.hstack((0, b, d.shape[0] + 1))\n    f_list = []\n    for i in range(len(b)-1):\n        #Need to subtract 1 here to give cluster bounds\n        b_idx = [b[i], b[i+1]-1]\n        b_dt = o_list_sort[b_idx]\n        #These should be identical if input is already sorted\n        b_idx_orig = o_list_sort_idx[b_idx]\n        all_idx = np.arange(b_idx[0], b_idx[1])\n        all_sort = o_list_sort[all_idx]\n        #These should be identical if input is already sorted\n        all_idx_orig = o_list_sort_idx[all_idx] \n        dict = {}\n        dict['break_indices'] = b_idx_orig \n        dict['break_ts_o'] = b_dt \n        dict['break_ts_dt'] = o2dt(b_dt) \n        dict['all_indices'] = all_idx_orig\n        dict['all_ts_o'] = all_sort\n        dict['all_ts_dt'] = o2dt(all_sort)\n        f_list.append(dict)\n    return f_list", "response": "Find clusters of similar datetimes within datetime list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts datetime to decimal year", "response": "def dt2decyear(dt):\n    \"\"\"Convert datetime to decimal year\n    \"\"\"\n    year = dt.year\n    startOfThisYear = datetime(year=year, month=1, day=1)\n    startOfNextYear = datetime(year=year+1, month=1, day=1)\n    yearElapsed = sinceEpoch(dt) - sinceEpoch(startOfThisYear)\n    yearDuration = sinceEpoch(startOfNextYear) - sinceEpoch(startOfThisYear)\n    fraction = yearElapsed/yearDuration\n    return year + fraction"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decyear2dt(t):\n    year = int(t)\n    rem = t - year \n    base = datetime(year, 1, 1)\n    dt = base + timedelta(seconds=(base.replace(year=base.year+1) - base).total_seconds() * rem)\n    #This works for np array input\n    #year = t.astype(int)\n    #rem = t - year \n    #base = np.array([datetime(y, 1, 1) for y in year])\n    return dt", "response": "Convert decimal year to datetime"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dt2jd(dt):\n    a = (14 - dt.month)//12\n    y = dt.year + 4800 - a\n    m = dt.month + 12*a - 3\n    return dt.day + ((153*m + 2)//5) + 365*y + y//4 - y//100 + y//400 - 32045", "response": "Convert datetime to julian date"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef jd2dt(jd):\n    n = int(round(float(jd)))\n    a = n + 32044\n    b = (4*a + 3)//146097\n    c = a - (146097*b)//4\n    d = (4*c + 3)//1461\n    e = c - (1461*d)//4\n    m = (5*e + 2)//153\n    day = e + 1 - (153*m + 2)//5\n    month = m + 3 - 12*(m//10)\n    year = 100*b + d - 4800 + m/10\n    \n    tfrac = 0.5 + float(jd) - n\n    tfrac_s = 86400.0 * tfrac \n    minfrac, hours = np.modf(tfrac_s / 3600.)\n    secfrac, minutes = np.modf(minfrac * 60.)\n    microsec, seconds = np.modf(secfrac * 60.)\n\n    return datetime(year, month, day, int(hours), int(minutes), int(seconds), int(microsec*1E6))", "response": "Convert a Julian date to a datetime object"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts GPS week and ms to a datetime", "response": "def gps2dt(gps_week, gps_ms):\n    \"\"\"Convert GPS week and ms to a datetime\n    \"\"\"\n    gps_epoch = datetime(1980,1,6,0,0,0)\n    gps_week_s = timedelta(seconds=gps_week*7*24*60*60)\n    gps_ms_s = timedelta(milliseconds=gps_ms) \n    return gps_epoch + gps_week_s + gps_ms_s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a ndb. KeyProperty object into a list of conversion results.", "response": "def convert_ndb_key_propety(ndb_key_prop, registry=None):\n    \"\"\"\n    Two conventions for handling KeyProperties:\n    #1.\n        Given:\n            store_key = ndb.KeyProperty(...)\n\n        Result is 2 fields:\n            store_id  = graphene.String() -> resolves to store_key.urlsafe()\n            store     = NdbKeyField()     -> resolves to entity\n\n    #2.\n        Given:\n            store = ndb.KeyProperty(...)\n\n        Result is 2 fields:\n            store_id = graphene.String() -> resolves to store_key.urlsafe()\n            store     = NdbKeyField()    -> resolves to entity\n\n    \"\"\"\n    is_repeated = ndb_key_prop._repeated\n    name = ndb_key_prop._code_name\n\n    if name.endswith('_key') or name.endswith('_keys'):\n        # Case #1 - name is of form 'store_key' or 'store_keys'\n        string_prop_name = rreplace(name, '_key', '_id', 1)\n        resolved_prop_name = name[:-4] if name.endswith('_key') else p.plural(name[:-5])\n    else:\n        # Case #2 - name is of form 'store'\n        singular_name = p.singular_noun(name) if p.singular_noun(name) else name\n        string_prop_name = singular_name + '_ids' if is_repeated else singular_name + '_id'\n        resolved_prop_name = name\n\n    return [\n        ConversionResult(name=string_prop_name, field=DynamicNdbKeyStringField(ndb_key_prop, registry=registry)),\n        ConversionResult(name=resolved_prop_name, field=DynamicNdbKeyReferenceField(ndb_key_prop, registry=registry))\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connection_from_ndb_query(query, args=None, connection_type=None, edge_type=None, pageinfo_type=None,\n                              transform_edges=None, context=None, **kwargs):\n    '''\n    A simple function that accepts an ndb Query and used ndb QueryIterator object(https://cloud.google.com/appengine/docs/python/ndb/queries#iterators)\n    to returns a connection object for use in GraphQL.\n    It uses array offsets as pagination,\n    so pagination will only work if the array is static.\n    '''\n    args = args or {}\n    connection_type = connection_type or Connection\n    edge_type = edge_type or Edge\n    pageinfo_type = pageinfo_type or PageInfo\n\n    full_args = dict(args, **kwargs)\n    first = full_args.get('first')\n    after = full_args.get('after')\n    has_previous_page = bool(after)\n    keys_only = full_args.get('keys_only', False)\n    batch_size = full_args.get('batch_size', 20)\n    page_size = first if first else full_args.get('page_size', 20)\n    start_cursor = ndb.Cursor(urlsafe=after) if after else None\n\n    ndb_iter = query.iter(produce_cursors=True, start_cursor=start_cursor, batch_size=batch_size, keys_only=keys_only, projection=query.projection)\n\n    edges = []\n    while len(edges) < page_size:\n        missing_edges_count = page_size - len(edges)\n        edges_page = generate_edges_page(ndb_iter, missing_edges_count, keys_only, edge_type)\n\n        edges.extend(transform_edges(edges_page, args, context) if transform_edges else edges_page)\n\n        if len(edges_page) < missing_edges_count:\n            break\n\n    try:\n        end_cursor = ndb_iter.cursor_after().urlsafe()\n    except BadArgumentError:\n        end_cursor = None\n\n    # Construct the connection\n    return connection_type(\n        edges=edges,\n        page_info=pageinfo_type(\n            start_cursor=start_cursor.urlsafe() if start_cursor else '',\n            end_cursor=end_cursor,\n            has_previous_page=has_previous_page,\n            has_next_page=ndb_iter.has_next()\n        )\n    )", "response": "A simple function that accepts an ndb Query and returns a connection object for use in GraphQL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disco(bytecode_version, co, timestamp, out=sys.stdout,\n          is_pypy=False, magic_int=None, source_size=None,\n          header=True, asm_format=False, show_bytes=False,\n          dup_lines=False):\n    \"\"\"\n    diassembles and deparses a given code block 'co'\n    \"\"\"\n\n    assert iscode(co)\n\n    show_module_header(bytecode_version, co, timestamp, out,\n                       is_pypy, magic_int, source_size, header,\n                        show_filename=False)\n\n    # store final output stream for case of error\n    real_out = out or sys.stdout\n\n    if co.co_filename and not asm_format:\n        real_out.write(format_code_info(co, bytecode_version) + \"\\n\")\n        pass\n\n    opc = get_opcode(bytecode_version, is_pypy)\n\n    if asm_format:\n        disco_loop_asm_format(opc, bytecode_version, co, real_out,\n                              {}, set([]))\n    else:\n        queue = deque([co])\n        disco_loop(opc, bytecode_version, queue, real_out,\n                   show_bytes=show_bytes)", "response": "Disparses a given code block."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisassembles a queue of code objects.", "response": "def disco_loop(opc, version, queue, real_out, dup_lines=False,\n               show_bytes=False):\n    \"\"\"Disassembles a queue of code objects. If we discover\n    another code object which will be found in co_consts, we add\n    the new code to the list. Note that the order of code discovery\n    is in the order of first encountered which is not amenable for\n    the format used by a disassembler where code objects should\n    be defined before using them in other functions.\n    However this is not recursive and will overall lead to less\n    memory consumption at run time.\n    \"\"\"\n\n    while len(queue) > 0:\n        co = queue.popleft()\n        if co.co_name not in ('<module>', '?'):\n            real_out.write(\"\\n\" + format_code_info(co, version) + \"\\n\")\n\n        bytecode = Bytecode(co, opc, dup_lines=dup_lines)\n        real_out.write(bytecode.dis(show_bytes=show_bytes) + \"\\n\")\n\n        for c in co.co_consts:\n            if iscode(c):\n                queue.append(c)\n            pass\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disco_loop_asm_format(opc, version, co, real_out,\n                          fn_name_map, all_fns):\n    \"\"\"Produces disassembly in a format more conducive to\n    automatic assembly by producing inner modules before they are\n    used by outer ones. Since this is recusive, we'll\n    use more stack space at runtime.\n    \"\"\"\n\n    if version < 3.0:\n        co = code2compat(co)\n    else:\n        co = code3compat(co)\n\n    co_name = co.co_name\n    mapped_name = fn_name_map.get(co_name, co_name)\n\n    new_consts = []\n    for c in co.co_consts:\n        if iscode(c):\n            if version < 3.0:\n                c_compat = code2compat(c)\n            else:\n                c_compat = code3compat(c)\n            disco_loop_asm_format(opc, version, c_compat, real_out,\n                                  fn_name_map, all_fns)\n\n            m = re.match(\".* object <(.+)> at\", str(c))\n            if m:\n                basename = m.group(1)\n                if basename != 'module':\n                    mapped_name = code_uniquify(basename, c.co_code)\n                    c_compat.co_name = mapped_name\n            c_compat.freeze()\n            new_consts.append(c_compat)\n        else:\n            new_consts.append(c)\n        pass\n    co.co_consts = new_consts\n\n    m = re.match(\"^<(.+)>$\", co.co_name)\n    if m or co_name in all_fns:\n        if co_name in all_fns:\n            basename = co_name\n        else:\n            basename = m.group(1)\n        if basename != 'module':\n            mapped_name = code_uniquify(basename, co.co_code)\n            co_name = mapped_name\n            assert mapped_name not in fn_name_map\n        fn_name_map[mapped_name] = basename\n        co.co_name = mapped_name\n        pass\n    elif co_name in fn_name_map:\n        # FIXME: better would be a hash of the co_code\n        mapped_name = code_uniquify(co_name, co.co_code)\n        fn_name_map[mapped_name] = co_name\n        co.co_name = mapped_name\n        pass\n\n    co = co.freeze()\n    all_fns.add(co_name)\n    if co.co_name != '<module>' or co.co_filename:\n        real_out.write(\"\\n\" + format_code_info(co, version, mapped_name) + \"\\n\")\n\n    bytecode = Bytecode(co, opc, dup_lines=True)\n    real_out.write(bytecode.dis(asm_format=True) + \"\\n\")", "response": "Produces disassembly in a format more conducive to the inner modules before they are used by outer ones."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisassemble a Python file into a Python object.", "response": "def disassemble_file(filename, outstream=sys.stdout,\n                     asm_format=False, header=False, show_bytes=False):\n    \"\"\"\n    disassemble Python byte-code file (.pyc)\n\n    If given a Python source file (\".py\") file, we'll\n    try to find the corresponding compiled object.\n    \"\"\"\n    filename = check_object_path(filename)\n    version, timestamp, magic_int, co, is_pypy, source_size  = load_module(filename)\n\n    if header:\n        show_module_header(version, co, timestamp, outstream,\n                           is_pypy, magic_int, source_size, show_filename=True)\n\n    else:\n        disco(version, co, timestamp, outstream, is_pypy, magic_int, source_size,\n              asm_format=asm_format, show_bytes=show_bytes)\n    # print co.co_filename\n    return filename, co, version, timestamp, magic_int"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_file(filename, out=sys.stdout):\n    fp = open(filename, 'rb')\n    try:\n      source = fp.read()\n      try:\n          if PYTHON_VERSION < 2.6:\n              co = compile(source, filename, 'exec')\n          else:\n              co = compile(source, filename, 'exec', dont_inherit=True)\n      except SyntaxError:\n          out.write('>>Syntax error in %s\\n' % filename)\n          raise\n    finally:\n      fp.close()\n    return co", "response": "Load a Python source file and compile it to byte - code"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_module(filename, code_objects=None, fast_load=False,\n                get_code=True):\n    \"\"\"load a module without importing it.\n    load_module(filename: string): version, magic_int, code_object\n\n    filename:\tname of file containing Python byte-code object\n                (normally a .pyc)\n\n    code_object: code_object from this file\n    version: Python major/minor value e.g. 2.7. or 3.4\n    magic_int: more specific than version. The actual byte code version of the\n               code object\n\n    Parsing the code object takes a bit of parsing time, but\n    sometimes all you want is the module info, time string, code size,\n    python version, etc. For that, set get_code=False.\n    \"\"\"\n\n    # Some sanity checks\n    if not osp.exists(filename):\n        raise ImportError(\"File name: '%s' doesn't exist\" % filename)\n    elif not osp.isfile(filename):\n        raise ImportError(\"File name: '%s' isn't a file\" % filename)\n    elif osp.getsize(filename) < 50:\n        raise ImportError(\"File name: '%s (%d bytes)' is too short to be a valid pyc file\" % (filename, osp.getsize(filename)))\n\n    with open(filename, 'rb') as fp:\n        return load_module_from_file_object(fp, filename=filename, code_objects=code_objects,\n                                            fast_load=fast_load, get_code=get_code)", "response": "Load a module from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_module_from_file_object(fp, filename='<unknown>', code_objects=None, fast_load=False,\n                get_code=True):\n    \"\"\"load a module from a file object without importing it.\n\n    See :func:load_module for a list of return values.\n    \"\"\"\n\n    if code_objects is None:\n        code_objects = {}\n\n    timestamp = 0\n    try:\n        magic = fp.read(4)\n        magic_int = magics.magic2int(magic)\n\n        # For reasons I don't understand, PyPy 3.2 stores a magic\n        # of '0'...  The two values below are for Python 2.x and 3.x respectively\n        if magic[0:1] in ['0', b'0']:\n            magic = magics.int2magic(3180+7)\n\n        try:\n            # FIXME: use the internal routine below\n            float_version = float(magics.versions[magic][:3])\n            # float_version = magics.magic_int2float(magic_int)\n        except KeyError:\n            if magic_int in (2657, 22138):\n                raise ImportError(\"This smells like Pyston which is not supported.\")\n\n            if len(magic) >= 2:\n                raise ImportError(\"Unknown magic number %s in %s\" %\n                                (ord(magic[0:1])+256*ord(magic[1:2]), filename))\n            else:\n                raise ImportError(\"Bad magic number: '%s'\" % magic)\n\n        if magic_int in (3010, 3020, 3030, 3040, 3050, 3060, 3061, 3361, 3371):\n            raise ImportError(\"%s is interim Python %s (%d) bytecode which is \"\n                              \"not supported.\\nFinal released versions are \"\n                              \"supported.\" % (\n                                  filename, magics.versions[magic],\n                                  magics.magic2int(magic)))\n        elif magic_int == 62135:\n            fp.seek(0)\n            return fix_dropbox_pyc(fp)\n        elif magic_int == 62215:\n            raise ImportError(\"%s is a dropbox-hacked Python %s (bytecode %d).\\n\"\n                              \"See https://github.com/kholia/dedrop for how to \"\n                              \"decrypt.\" % (\n                                  filename, magics.versions[magic],\n                                  magics.magic2int(magic)))\n\n        try:\n            # print version\n            ts = fp.read(4)\n            my_magic_int = magics.magic2int(imp.get_magic())\n            magic_int = magics.magic2int(magic)\n\n            if magic_int == 3393:\n                timestamp = 0\n                _ = unpack(\"<I\", ts)[0]         # hash word 1\n                _ = unpack(\"<I\", fp.read(4))[0] # hash word 2\n            elif magic_int in (3394, 3401):\n                timestamp = 0\n                _ = unpack(\"<I\", fp.read(4))[0] # pep552_bits\n            else:\n                timestamp = unpack(\"<I\", ts)[0]\n\n            # Note: a higher magic number doesn't necessarily mean a later\n            # release.  At Python 3.0 the magic number decreased\n            # significantly. Hence the range below. Also note inclusion of\n            # the size info, occurred within a Python major/minor\n            # release. Hence the test on the magic value rather than\n            # PYTHON_VERSION, although PYTHON_VERSION would probably work.\n            if 3200 <= magic_int < 20121 and magic_int not in (5892, 11913, 39170, 39171):\n                source_size = unpack(\"<I\", fp.read(4))[0] # size mod 2**32\n            else:\n                source_size = None\n\n            if get_code:\n                if my_magic_int == magic_int:\n                    bytecode = fp.read()\n                    co = marshal.loads(bytecode)\n                elif fast_load:\n                    co = xdis.marsh.load(fp, magics.magicint2version[magic_int])\n                else:\n                    co = xdis.unmarshal.load_code(fp, magic_int, code_objects)\n                pass\n            else:\n                co = None\n        except:\n            kind, msg = sys.exc_info()[0:2]\n            import traceback\n            traceback.print_exc()\n            raise ImportError(\"Ill-formed bytecode file %s\\n%s; %s\"\n                              % (filename, kind, msg))\n\n    finally:\n      fp.close()\n\n    return float_version, timestamp, magic_int, co, is_pypy(magic_int), source_size", "response": "Load a module from a file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_bytecode_file(bytecode_path, code, magic_int, filesize=0):\n    fp = open(bytecode_path, 'wb')\n    try:\n        if PYTHON3:\n            fp.write(pack('<Hcc', magic_int, b'\\r', b'\\n'))\n        else:\n            fp.write(pack('<Hcc', magic_int, '\\r', '\\n'))\n        fp.write(pack('<I', int(time.time())))\n        if (3000 <= magic_int < 20121):\n            # In Python 3 you need to write out the size mod 2**32 here\n            fp.write(pack('<I', filesize))\n        fp.write(marshal.dumps(code))\n    finally:\n      fp.close()", "response": "Write bytecode file _bytecode_path_ with code for having PythonMagicInt magic_int."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(asm, show_bytes, header, files):\n    Usage_short = \"\"\"usage:\n   %s [--asm] -i FILE...\n   %s --version\nType -h for for full help.\"\"\" % (program, program)\n\n\n    if not (2.5 <= PYTHON_VERSION <= 3.8):\n        sys.stderr(print(\"This works on Python version 2.5..3.8; have %s\" % PYTHON_VERSION))\n\n    if not len(files):\n        sys.stderr.write(\"No file(s) given..\\n\")\n        print(Usage_short, file=sys.stderr)\n        sys.exit(1)\n\n    for path in files:\n        # Some sanity checks\n        if not osp.exists(path):\n            sys.stderr.write(\"File name: '%s' doesn't exist\\n\" % path)\n            continue\n        elif not osp.isfile(path):\n            sys.stderr.write(\"File name: '%s' isn't a file\\n\" % path)\n            continue\n        elif osp.getsize(path) < 50:\n            sys.stderr.write(\"File name: '%s (%d bytes)' is too short to be a valid pyc file\\n\" % (path, osp.getsize(path)))\n            continue\n\n        disassemble_file(path, sys.stdout, asm, header, show_bytes)\n    return", "response": "Disassemble a Python bytecode file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_compile(codeobject, filename, timestamp, magic):\n    # Atomically write the pyc/pyo file.  Issue #13146.\n    # id() is used to generate a pseudo-random filename.\n    path_tmp = '%s.%s' % (filename, id(filename))\n    fc = None\n    try:\n        fc = open(path_tmp, 'wb')\n        if PYTHON3:\n            fc.write(bytes([0, 0, 0, 0]))\n        else:\n            fc.write('\\0\\0\\0\\0')\n        wr_long(fc, timestamp)\n        marshal.dump(codeobject, fc)\n        fc.flush()\n        fc.seek(0, 0)\n        fc.write(magic)\n        fc.close()\n        os.rename(path_tmp, filename)\n    except OSError:\n        try:\n            os.unlink(path_tmp)\n        except OSError:\n            pass\n        raise\n    finally:\n        if fc: fc.close()", "response": "Writes a code object as a byte - compiled file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_file(real_source_filename, real_bytecode_filename):\n    tempdir = tempfile.gettempdir()\n    source_filename = os.path.join(tempdir, \"testing.py\")\n    if not os.path.exists(real_source_filename):\n        return\n    try:\n        f = open(real_source_filename, 'U')\n    except:\n        return\n\n    codestring = f.read()\n    f.close()\n\n    codeobject1 = compile(codestring, source_filename,'exec')\n\n    (version, timestamp, magic_int, codeobject2, is_pypy,\n     source_size) = load_module(real_bytecode_filename)\n\n    # A hack for PyPy 3.2\n    if magic_int == 3180+7:\n        magic_int = 48\n\n    assert MAGIC == magics.int2magic(magic_int), \\\n      (\"magic_int %d vs %d in %s/%s\" %\n           (magic_int, magics.magic2int(MAGIC), os.getcwd(), real_bytecode_filename))\n    bytecode_filename1 = os.path.join(tempdir, \"testing1.pyc\")\n    dump_compile(codeobject1, bytecode_filename1, timestamp, MAGIC)\n    (version, timestamp, magic_int, codeobject3, is_pypy,\n     source_size) = load_module(real_bytecode_filename, fast_load=not is_pypy)\n\n    # compare_code(codeobject1, codeobject2)\n    # compare_code(codeobject2, codeobject3)\n\n    bytecode_filename2 = os.path.join(tempdir, \"testing2.pyc\")\n    dump_compile(codeobject1, bytecode_filename2, timestamp, magics.int2magic(magic_int))\n\n    compare_bytecode_files(bytecode_filename1, bytecode_filename2)\n    return", "response": "Verify that the file is correct."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a magic int like 62211 compute the corresponding magic byte string b'\\x03 \\ xxf3 \\ r \\ n", "response": "def int2magic(magic_int):\n    \"\"\"Given a magic int like 62211, compute the corresponding magic byte string\n     b'\\x03\\xf3\\r\\n' using the conversion method that does this.\n\n    See also dictionary magic2nt2version which has precomputed these values\n    for knonwn magic_int's.\n    \"\"\"\n\n    if (sys.version_info >= (3, 0)):\n        return struct.pack('<Hcc', magic_int, bytes('\\r', 'utf-8'), bytes('\\n', 'utf-8'))\n    else:\n        return struct.pack('<Hcc', magic_int, '\\r', '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef py_str2float(version):\n    if version.endswith('pypy'):\n        version = version[:-len('pypy')]\n    if version in magics:\n        magic = magics[version]\n        for v, m in list(magics.items()):\n            if m == magic:\n                try:\n                    return float(canonic_python_version[v])\n                except:\n                    try:\n                        m = re.match(r'^(\\d\\.)(\\d+)\\.(\\d+)$', v)\n                        if m:\n                            return float(m.group(1)+m.group(2))\n                    except:\n                        pass\n                    pass\n                pass\n            pass\n    raise RuntimeError(\"Can't find a valid Python version for version %s\"\n                       % version)\n    return", "response": "Convert a Python version into a two - digit canonic floating - point number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a sys. versions_info - compatible list into a canonic float number which can then be used to look up a magic number.", "response": "def sysinfo2float(version_info=sys.version_info):\n    \"\"\"Convert a sys.versions_info-compatible list into a 'canonic'\n    floating-point number which that can then be used to look up a\n    magic number.  Note that this can only be used for released version\n    of C Python, not interim development versions, since we can't\n    represent that as a floating-point number.\n\n    For handling Pypy, pyston, jython, etc. and interim versions of\n    C Python, use sysinfo2magic.\n    \"\"\"\n    vers_str = '.'.join([str(v) for v in version_info[0:3]])\n    if version_info[3] != 'final':\n        vers_str += '.' + ''.join([str(i) for i in version_info[3:]])\n\n    if IS_PYPY:\n        vers_str += 'pypy'\n    else:\n        try:\n            import platform\n            platform = platform.python_implementation()\n            if platform in ('Jython', 'Pyston'):\n                vers_str += platform\n                pass\n        except ImportError:\n            # Python may be too old, e.g. < 2.6 or implementation may\n            # just not have platform\n            pass\n        except AttributeError:\n            pass\n    return py_str2float(vers_str)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list sys. versions_info compatible list into a canonic version number which can then be used to look up a .", "response": "def sysinfo2magic(version_info=sys.version_info):\n    \"\"\"Convert a list sys.versions_info compatible list into a 'canonic'\n    floating-point number which that can then be used to look up a\n    magic number.  Note that this can raise an exception.\n    \"\"\"\n\n    # FIXME: DRY with sysinfo2float()\n    vers_str = '.'.join([str(v) for v in version_info[0:3]])\n    if version_info[3] != 'final':\n        vers_str += ''.join([str(v) for v in version_info[3:]])\n\n    if IS_PYPY:\n        vers_str += 'pypy'\n    else:\n        try:\n            import platform\n            platform = platform.python_implementation()\n            if platform in ('Jython', 'Pyston'):\n                vers_str += platform\n                pass\n        except ImportError:\n            # Python may be too old, e.g. < 2.6 or implementation may\n            # just not have platform\n            pass\n\n    return magics[vers_str]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the local dictionary l with the contents of the opcode. py module l.", "response": "def init_opdata(l, from_mod, version=None, is_pypy=False):\n    \"\"\"Sets up a number of the structures found in Python's\n    opcode.py. Python opcode.py routines assign attributes to modules.\n    In order to do this in a modular way here, the local dictionary\n    for the module is passed.\n    \"\"\"\n\n    if version:\n        l['python_version'] = version\n    l['is_pypy'] = is_pypy\n    l['cmp_op'] = cmp_op\n    l['HAVE_ARGUMENT'] = HAVE_ARGUMENT\n    if version <= 3.5:\n        l['findlinestarts'] = findlinestarts\n        l['findlabels']     = findlabels\n        l['get_jump_targets'] = get_jump_targets\n        l['get_jump_target_maps']  = get_jump_target_maps\n    else:\n        l['findlinestarts'] = wordcode.findlinestarts\n        l['findlabels']     = wordcode.findlabels\n        l['get_jump_targets'] = wordcode.get_jump_targets\n        l['get_jump_target_maps']  = wordcode.get_jump_target_maps\n\n    l['opmap'] = deepcopy(from_mod.opmap)\n    l['opname'] = deepcopy(from_mod.opname)\n\n    for field in fields2copy:\n        l[field] = list(getattr(from_mod, field))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving an opcode from the list l.", "response": "def rm_op(l, name, op):\n    \"\"\"Remove an opcode. This is used when basing a new Python release off\n    of another one, and there is an opcode that is in the old release\n    that was removed in the new release.\n    We are pretty aggressive about removing traces of the op.\n    \"\"\"\n\n    # opname is an array, so we need to keep the position in there.\n    l['opname'][op] = '<%s>' % op\n\n    if op in l['hasconst']:\n       l['hasconst'].remove(op)\n    if op in l['hascompare']:\n       l['hascompare'].remove(op)\n    if op in l['hascondition']:\n       l['hascondition'].remove(op)\n    if op in l['hasfree']:\n       l['hasfree'].remove(op)\n    if op in l['hasjabs']:\n       l['hasjabs'].remove(op)\n    if op in l['hasname']:\n       l['hasname'].remove(op)\n    if op in l['hasjrel']:\n       l['hasjrel'].remove(op)\n    if op in l['haslocal']:\n       l['haslocal'].remove(op)\n    if op in l['hasname']:\n       l['hasname'].remove(op)\n    if op in l['hasnargs']:\n       l['hasnargs'].remove(op)\n    if op in l['hasvargs']:\n       l['hasvargs'].remove(op)\n    if op in l['nofollow']:\n       l['nofollow'].remove(op)\n\n    assert l['opmap'][name] == op\n    del l['opmap'][name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfix opcode names in Python 3. 6.", "response": "def fix_opcode_names(opmap):\n    \"\"\"\n    Python stupidly named some OPCODES with a + which prevents using opcode name\n    directly as an attribute, e.g. SLICE+3. So we turn that into SLICE_3 so we\n    can then use opcode_23.SLICE_3.  Later Python's fix this.\n    \"\"\"\n    return dict([(k.replace('+', '_'), v)\n                  for (k, v) in opmap.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_opcodes(opmap):\n    op2name = {}\n    for k in opmap.keys():\n        op2name[opmap[k]] = k\n    for i in sorted(op2name.keys()):\n        print(\"%-3s %s\" % (str(i), op2name[i]))", "response": "Utility for dumping opcodes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_code(fp, magic_int, code_objects={}):\n    global internStrings, internObjects\n    internStrings = []\n    internObjects = []\n    seek_pos = fp.tell()\n    # Do a sanity check. Is this a code type?\n    b =  ord(fp.read(1))\n\n    if (b & 0x80):\n        b = b & 0x7f\n\n    c = chr(b)\n    if c != 'c':\n        raise TypeError(\"File %s doesn't smell like Python bytecode:\\n\"\n                        \"expecting code indicator 'c'; got '%s'\"\n                        % (fp.name, c))\n\n    fp.seek(seek_pos)\n    return load_code_internal(fp, magic_int, code_objects=code_objects)", "response": "Load a Python bytecode from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_flags(flags):\n    names = []\n    result = \"0x%08x\" % flags\n    for i in range(32):\n        flag = 1 << i\n        if flags & flag:\n            names.append(COMPILER_FLAG_NAMES.get(flag, hex(flag)))\n            flags ^= flag\n            if not flags:\n                break\n    else:\n        names.append(hex(flags))\n    names.reverse()\n    return \"%s (%s)\" % (result, \" | \".join(names))", "response": "Return pretty representation of code flags."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to compile the given source and returns a valid C object.", "response": "def _try_compile(source, name):\n    \"\"\"Attempts to compile the given source, first as an expression and\n       then as a statement if the first approach fails.\n\n       Utility function to accept strings in functions that otherwise\n       expect code objects\n    \"\"\"\n    try:\n        c = compile(source, name, 'eval')\n    except SyntaxError:\n        c = compile(source, name, 'exec')\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_code_object(x):\n    if hasattr(x, '__func__'): # Method\n        x = x.__func__\n    if hasattr(x, '__code__'): # Function\n        x = x.__code__\n    if hasattr(x, 'gi_code'):  # Generator\n        x = x.gi_code\n    if isinstance(x, str):     # Source code\n        x = _try_compile(x, \"<disassembly>\")\n    if hasattr(x, 'co_code'):  # Code object\n        return x\n    raise TypeError(\"don't know how to disassemble %s objects\" %\n                    type(x).__name__)", "response": "Helper to handle methods functions generators strings and raw code objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint details of methods functions or code to file.", "response": "def show_code(co, version, file=None):\n    \"\"\"Print details of methods, functions, or code to *file*.\n\n    If *file* is not provided, the output is printed on stdout.\n    \"\"\"\n    if file is None:\n        print(code_info(co, version))\n    else:\n        file.write(code_info(co, version) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the offsets in a byte code which are start of lines in the source.", "response": "def findlinestarts(code, dup_lines=False):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    if not isinstance(code.co_lnotab, str):\n        byte_increments = list(code.co_lnotab[0::2])\n        line_increments = list(code.co_lnotab[1::2])\n    else:\n        byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n        line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    offset = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if (lineno != lastlineno or\n                (dup_lines and 0 < byte_incr < 255)):\n                yield (offset, lineno)\n                lastlineno = lineno\n                pass\n            offset += byte_incr\n            pass\n        lineno += line_incr\n    if (lineno != lastlineno or\n        (dup_lines and 0 < byte_incr < 255)):\n        yield (offset, lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef offset2line(offset, linestarts):\n    if len(linestarts) == 0 or offset < linestarts[0][0]:\n        return 0\n    low = 0\n    high = len(linestarts) - 1\n    mid = (low + high + 1) // 2\n    while low <= high:\n        if linestarts[mid][0] > offset:\n            high = mid - 1\n        elif linestarts[mid][0] < offset:\n            low = mid + 1\n        else:\n            return linestarts[mid][1]\n        mid = (low + high + 1) // 2\n        pass\n    # Not found. Return closest position below\n    if mid >= len(linestarts):\n        return linestarts[len(linestarts)-1][1]\n    return linestarts[high][1]", "response": "Return the line number corresponding to the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_jump_targets(code, opc):\n    offsets = []\n    for offset, op, arg in unpack_opargs_bytecode(code, opc):\n        if arg is not None:\n            jump_offset = -1\n            if op in opc.JREL_OPS:\n                op_len = op_size(op, opc)\n                jump_offset = offset + op_len + arg\n            elif op in opc.JABS_OPS:\n                jump_offset = arg\n            if jump_offset >= 0:\n                if jump_offset not in offsets:\n                    offsets.append(jump_offset)\n    return offsets", "response": "Returns a list of instruction offsets in the supplied bytecode\n    which are the targets of some sort of jump instruction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_jump_target_maps(code, opc):\n    offset2prev = {}\n    prev_offset = -1\n    for offset, op, arg in unpack_opargs_bytecode(code, opc):\n        if prev_offset >= 0:\n            prev_list = offset2prev.get(offset, [])\n            prev_list.append(prev_offset)\n            offset2prev[offset] = prev_list\n        if op in opc.NOFOLLOW:\n            prev_offset = -1\n        else:\n            prev_offset = offset\n        if arg is not None:\n            jump_offset = -1\n            if op in opc.JREL_OPS:\n                op_len = op_size(op, opc)\n                jump_offset = offset + op_len + arg\n            elif op in opc.JABS_OPS:\n                jump_offset = arg\n            if jump_offset >= 0:\n                prev_list = offset2prev.get(jump_offset, [])\n                prev_list.append(offset)\n                offset2prev[jump_offset] = prev_list\n    return offset2prev", "response": "Returns a dictionary where the keys are an offset and the values are a list of instruction offsets which can get run before that ArcGIS instruction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_instructions_bytes(bytecode, opc, varnames=None, names=None, constants=None,\n                           cells=None, linestarts=None, line_offset=0):\n    \"\"\"Iterate over the instructions in a bytecode string.\n\n    Generates a sequence of Instruction namedtuples giving the details of each\n    opcode.  Additional information about the code's runtime environment\n    (e.g. variable names, constants) can be specified using optional\n    arguments.\n\n    \"\"\"\n    labels = opc.findlabels(bytecode, opc)\n    # label_maps = get_jump_target_maps(bytecode, opc)\n    extended_arg = 0\n\n    # FIXME: We really need to distinguish 3.6.0a1 from 3.6.a3.\n    # See below FIXME\n    python_36 = True if opc.python_version >= 3.6 else False\n\n    starts_line = None\n    # enumerate() is not an option, since we sometimes process\n    # multiple elements on a single pass through the loop\n    n = len(bytecode)\n    i = 0\n    extended_arg_count  = 0\n    extended_arg = 0\n    extended_arg_size = op_size(opc.EXTENDED_ARG, opc)\n    while i < n:\n        op = code2num(bytecode, i)\n\n        offset = i\n        if linestarts is not None:\n            starts_line = linestarts.get(i, None)\n            if starts_line is not None:\n                starts_line += line_offset\n        if i in labels:\n            #  come_from = label_maps[i]\n            if False: # come_from[0] > i:\n                is_jump_target = 'loop'\n                # print(\"XXX %s at %d\" % (opc.opname[op], i))\n                # from trepan.api import debug; debug()\n            else:\n                is_jump_target = True\n        else:\n            is_jump_target = False\n\n        i += 1\n        arg = None\n        argval = None\n        argrepr = ''\n        has_arg = op_has_argument(op, opc)\n        optype = None\n        if has_arg:\n            if python_36:\n                arg = code2num(bytecode, i) | extended_arg\n                extended_arg = (arg << 8) if op == opc.EXTENDED_ARG else 0\n                # FIXME: Python 3.6.0a1 is 2, for 3.6.a3 we have 1\n                i += 1\n            else:\n                arg = code2num(bytecode, i) + code2num(bytecode, i+1)*256 + extended_arg\n                i += 2\n                extended_arg = arg*65536 if op == opc.EXTENDED_ARG else 0\n\n            #  Set argval to the dereferenced value of the argument when\n            #  availabe, and argrepr to the string representation of argval.\n            #    disassemble_bytes needs the string repr of the\n            #    raw name index for LOAD_GLOBAL, LOAD_CONST, etc.\n            argval = arg\n            if op in opc.CONST_OPS:\n                argval, argrepr = _get_const_info(arg, constants)\n                optype = 'const'\n            elif op in opc.NAME_OPS:\n                argval, argrepr = _get_name_info(arg, names)\n                optype = 'name'\n            elif op in opc.JREL_OPS:\n                argval = i + arg\n                argrepr = \"to \" + repr(argval)\n                optype = 'jrel'\n            elif op in opc.JABS_OPS:\n                argval = arg\n                argrepr = \"to \" + repr(argval)\n                optype = 'jabs'\n            elif op in opc.LOCAL_OPS:\n                argval, argrepr = _get_name_info(arg, varnames)\n                optype = 'local'\n            elif op in opc.COMPARE_OPS:\n                argval = opc.cmp_op[arg]\n                argrepr = argval\n                optype = 'compare'\n            elif op in opc.FREE_OPS:\n                argval, argrepr = _get_name_info(arg, cells)\n                optype = 'free'\n            elif op in opc.NARGS_OPS:\n                optype = 'nargs'\n                if not python_36:\n                    argrepr = (\"%d positional, %d keyword pair\" %\n                               (code2num(bytecode, i-2), code2num(bytecode, i-1)))\n            # This has to come after hasnargs. Some are in both?\n            elif op in opc.VARGS_OPS:\n                optype = 'vargs'\n            if hasattr(opc, 'opcode_arg_fmt') and opc.opname[op] in opc.opcode_arg_fmt:\n                argrepr = opc.opcode_arg_fmt[opc.opname[op]](arg)\n        elif python_36:\n            i += 1\n\n        opname = opc.opname[op]\n        inst_size = op_size(op, opc) + (extended_arg_count * extended_arg_size)\n        yield Instruction(opname, op, optype, inst_size, arg, argval, argrepr,\n                          has_arg, offset, starts_line, is_jump_target,\n                          extended_arg_count != 0)\n        extended_arg_count = extended_arg_count + 1 if op == opc.EXTENDED_ARG else 0", "response": "Iterate over the instructions in a bytecode string and return a sequence of Instructions namedtuples giving the details of each instruction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert list of list of tuples to bytecode.", "response": "def list2bytecode(l, opc, varnames, consts):\n    \"\"\"Convert list/tuple of list/tuples to bytecode\n    _names_ contains a list of name objects\n    \"\"\"\n    bc = []\n    for i, opcodes in enumerate(l):\n        opname = opcodes[0]\n        operands = opcodes[1:]\n        if opname not in opc.opname:\n            raise TypeError(\n                \"error at item %d [%s, %s], opcode not valid\" %\n                (i, opname, operands))\n        opcode = opc.opmap[opname]\n        bc.append(opcode)\n        print(opname, operands)\n        gen = (j for j in operands if operands)\n        for j in gen:\n            k = (consts if opcode in opc.CONST_OPS else varnames).index(j)\n            if k == -1:\n                raise TypeError(\n                    \"operand %s [%s, %s], not found in names\" %\n                    (i, opname, operands))\n            else:\n                bc += num2code(k)\n                pass\n            pass\n        pass\n    if opc.python_version < 3.0:\n        return reduce(lambda a, b: a + chr(b), bc, '')\n    else:\n        if PYTHON3:\n            return bytes(bc)\n        else:\n            return bytes(bytearray(bc))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisassembling the instruction into a string.", "response": "def disassemble(self, lineno_width=3,\n                    mark_as_current=False,\n                    asm_format=False,\n                    show_bytes=False):\n        \"\"\"Format instruction details for inclusion in disassembly output\n\n        *lineno_width* sets the width of the line number field (0 omits it)\n        *mark_as_current* inserts a '-->' marker arrow as part of the line\n        \"\"\"\n        fields = []\n        if asm_format:\n            indexed_operand = set(['name', 'local', 'compare', 'free'])\n        # Column: Source code line number\n        if lineno_width:\n            if self.starts_line is not None:\n                if asm_format:\n                    lineno_fmt = \"%%%dd:\\n\" % lineno_width\n                    fields.append(lineno_fmt % self.starts_line)\n                    fields.append(' ' * (lineno_width))\n                    if self.is_jump_target:\n                        fields.append(' ' * (lineno_width-1))\n                else:\n                    lineno_fmt = \"%%%dd:\" % lineno_width\n                    fields.append(lineno_fmt % self.starts_line)\n            else:\n                fields.append(' ' * (lineno_width+1))\n        # Column: Current instruction indicator\n        if mark_as_current and not asm_format:\n            fields.append('-->')\n        else:\n            fields.append('   ')\n        # Column: Jump target marker\n        if self.is_jump_target:\n            if not asm_format:\n                fields.append('>>')\n            else:\n                fields = [\"L%d:\\n\" % self.offset] + fields\n                if not self.starts_line:\n                    fields.append(' ')\n        else:\n            fields.append('  ')\n        # Column: Instruction offset from start of code sequence\n        if not asm_format:\n            fields.append(repr(self.offset).rjust(4))\n\n        if show_bytes:\n            hex_bytecode = \"|%02x\" % self.opcode\n            if self.inst_size == 1:\n                # Not 3.6 or later\n                hex_bytecode += ' ' * (2*3)\n            if self.inst_size == 2:\n                # Must by Python 3.6 or later\n                if self.has_arg:\n                    hex_bytecode += \" %02x\" % (self.arg % 256)\n                else :\n                    hex_bytecode += ' 00'\n            elif self.inst_size == 3:\n                # Not 3.6 or later\n                hex_bytecode += \" %02x %02x\" % (\n                    (self.arg >> 8, self.arg % 256))\n\n            fields.append(hex_bytecode + '|')\n\n        # Column: Opcode name\n        fields.append(self.opname.ljust(20))\n\n        # Column: Opcode argument\n        if self.arg is not None:\n            argrepr = self.argrepr\n            if asm_format:\n                if self.optype == 'jabs':\n                    fields.append('L' + str(self.arg))\n                elif self.optype == 'jrel':\n                    argval = self.offset + self.arg + self.inst_size\n                    fields.append('L' + str(argval))\n                elif self.optype in indexed_operand:\n                    fields.append('(%s)' % argrepr)\n                    argrepr = None\n                elif (self.optype == 'const'\n                      and not re.search('\\s', argrepr)):\n                    fields.append('(%s)' % argrepr)\n                    argrepr = None\n                else:\n                    fields.append(repr(self.arg))\n            elif not (show_bytes and argrepr):\n                fields.append(repr(self.arg).rjust(6))\n            # Column: Opcode argument details\n            if argrepr:\n                fields.append('(%s)' % argrepr)\n                pass\n            pass\n        return ' '.join(fields).rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a Bytecode from a traceback.", "response": "def from_traceback(cls, tb):\n        \"\"\" Construct a Bytecode from the given traceback \"\"\"\n        while tb.tb_next:\n            tb = tb.tb_next\n        return cls(tb.tb_frame.f_code, current_offset=tb.tb_lasti)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dis(self, asm_format=False, show_bytes=False):\n        co = self.codeobj\n        if self.current_offset is not None:\n            offset = self.current_offset\n        else:\n            offset = -1\n        output = StringIO()\n        self.disassemble_bytes(co.co_code, varnames=co.co_varnames,\n                               names=co.co_names, constants=co.co_consts,\n                               cells=self._cell_names,\n                               linestarts=self._linestarts,\n                               line_offset=self._line_offset,\n                               file=output,\n                               lasti=offset,\n                               asm_format=asm_format,\n                               show_bytes=show_bytes)\n        return output.getvalue()", "response": "Return a formatted view of the bytecode operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_instructions(self, x, first_line=None):\n        co = get_code_object(x)\n        cell_names = co.co_cellvars + co.co_freevars\n        linestarts = dict(self.opc.findlinestarts(co))\n        if first_line is not None:\n            line_offset = first_line - co.co_firstlineno\n        else:\n            line_offset = 0\n        return get_instructions_bytes(co.co_code, self.opc, co.co_varnames,\n                                      co.co_names, co.co_consts, cell_names, linestarts,\n                                      line_offset)", "response": "Returns a series of Instruction named tuples giving the details of each operation in the code object x."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef findlinestarts(code, dup_lines=False):\n    if PYTHON3:\n        byte_increments = code.co_lnotab[0::2]\n        line_increments = code.co_lnotab[1::2]\n    else:\n        byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n        line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if (lineno != lastlineno or\n                (not dup_lines and 0 < byte_incr < 255)):\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        if line_incr >= 0x80:\n            # line_increments is an array of 8-bit signed integers\n            line_incr -= 0x100\n        lineno += line_incr\n    if (lineno != lastlineno or\n        (not dup_lines and 0 < byte_incr < 255)):\n        yield (addr, lineno)", "response": "Find the offsets in a byte code which are start of lines in the source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_jump_targets(code, opc):\n    offsets = []\n    for offset, op, arg in unpack_opargs_wordcode(code, opc):\n        if arg is not None:\n            if op in opc.JREL_OPS:\n                jump_offset = offset + 2 + arg\n            elif op in opc.JABS_OPS:\n                jump_offset = arg\n            else:\n                continue\n            if jump_offset not in offsets:\n                offsets.append(jump_offset)\n    return offsets", "response": "Returns a list of instruction offsets in the supplied bytecode\n    which are the targets of some sort of jump instruction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary where the keys are an offset and the values are a list of instruction offsets which can get run before that ArcGIS instruction.", "response": "def get_jump_target_maps(code, opc):\n    \"\"\"Returns a dictionary where the key is an offset and the values are\n    a list of instruction offsets which can get run before that\n    instruction. This includes jump instructions as well as non-jump\n    instructions. Therefore, the keys of the dictionary are reachible\n    instructions. The values of the dictionary may be useful in control-flow\n    analysis.\n    \"\"\"\n    offset2prev = {}\n    prev_offset = -1\n    for offset, op, arg in unpack_opargs_wordcode(code, opc):\n        if prev_offset >= 0:\n            prev_list = offset2prev.get(offset, [])\n            prev_list.append(prev_offset)\n            offset2prev[offset] = prev_list\n        prev_offset = offset\n        if op in opc.NOFOLLOW:\n            prev_offset = -1\n        if arg is not None:\n            jump_offset = -1\n            if op in opc.JREL_OPS:\n                jump_offset = offset + 2 + arg\n            elif op in opc.JABS_OPS:\n                jump_offset = arg\n            if jump_offset >= 0:\n                prev_list = offset2prev.get(jump_offset, [])\n                prev_list.append(offset)\n                offset2prev[jump_offset] = prev_list\n    return offset2prev"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_std_api(python_version=sys.version_info, variant=VARIANT):\n    if isinstance(python_version, float):\n        major = int(python_version)\n        minor = int(((python_version - major) + 0.05) * 10)\n        python_version = (major, minor)\n    return _StdApi(python_version, variant)", "response": "Generate a standard dismodule object which can be used in the same way as the standard dismodule."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting details of methods functions or code to file.", "response": "def show_code(self, x, file=None):\n        \"\"\"Print details of methods, functions, or code to *file*.\n\n        If *file* is not provided, the output is printed on stdout.\n        \"\"\"\n        return _show_code(x, self.opc.version, file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dis(self, x=None, file=None):\n        self._print(self.Bytecode(x).dis(), file)", "response": "Disassemble classes methods generators or code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef distb(self, tb=None, file=None):\n        if tb is None:\n            try:\n                tb = sys.last_traceback\n            except AttributeError:\n                raise RuntimeError(\"no last traceback to disassemble\")\n            while tb.tb_next: tb = tb.tb_next\n        self.disassemble(tb.tb_frame.f_code, tb.tb_lasti, file=file)", "response": "Disassemble a traceback (default: last traceback)."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disassemble(self, code, lasti=-1, file=None):\n        return self.disco(code, lasti, file)", "response": "Disassemble a code object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisassembling a code object.", "response": "def disco(self, code, lasti=-1, file=None):\n        \"\"\"Disassemble a code object.\"\"\"\n        return _disco(self.python_version, code, timestamp=0,\n                      out=file, is_pypy=self.is_pypy, header=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a code object from the buffer and return it as a Python code object.", "response": "def load_code(self):\n    \"\"\"\n    Returns a Python code object like xdis.unmarshal.load_code(),\n    but in we decrypt the data in self.bufstr.\n\n    That is:\n      * calculate the TEA key,\n      * decrypt self.bufstr\n      * create and return a Python code-object\n    \"\"\"\n    a = self.load_int()\n    b = self.load_int()\n    key = get_keys(a, b)\n    padsize = (b + 15) & ~0xf\n    intsize = padsize/4\n    data = self.bufstr[self.bufpos:self.bufpos+padsize]\n    # print(\"%d: %d (%d=%d)\" % (self.bufpos, b, padsize, len(data)))\n    data = list(struct.unpack('<%dL' % intsize, data))\n    tea_decipher(data, key)\n    self.bufpos += padsize\n    obj = xmarshal._FastUnmarshaller(struct.pack('<%dL' % intsize, *data))\n    code = obj.load_code()\n    co_code = patch(code.co_code)\n    if PYTHON3:\n        return Code2Compat(code.co_argcount, code.co_nlocals, code.co_stacksize,\n                           code.co_flags,\n                           co_code, code.co_consts, code.co_names, code.co_varnames,\n                           code.co_filename, code.co_name, code.co_firstlineno,\n                           code.co_lnotab, code.co_freevars, code.co_cellvars)\n    else:\n        return types.CodeType(code.co_argcount, code.co_nlocals, code.co_stacksize, code.co_flags,\n                              co_code, code.co_consts, code.co_names, code.co_varnames,\n                              code.co_filename, code.co_name, code.co_firstlineno,\n                              code.co_lnotab, code.co_freevars, code.co_cellvars)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(s):\n    um = xmarshal._FastUnmarshaller(s)\n    um.dispatch[xmarshal.TYPE_CODE] = load_code\n    return um.load()", "response": "loads a byte string into a sequence of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the protocol class for the given name.", "response": "def get_protocol_from_name(name):\n    \"\"\"\n    Returns the protocol class for the protocol with the given name.\n\n    :type  name: str\n    :param name: The name of the protocol.\n    :rtype:  Protocol\n    :return: The protocol class.\n    \"\"\"\n    cls = protocol_map.get(name)\n    if not cls:\n        raise ValueError('Unsupported protocol \"%s\".' % name)\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_protocol(name, **kwargs):\n    cls = protocol_map.get(name)\n    if not cls:\n        raise ValueError('Unsupported protocol \"%s\".' % name)\n    return cls(**kwargs)", "response": "Creates an instance of the given protocol with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new protocol instance from a given host.", "response": "def prepare(host, default_protocol='telnet', **kwargs):\n    \"\"\"\n    Creates an instance of the protocol by either parsing the given\n    URL-formatted hostname using :class:`Exscript.util.url`, or according to\n    the options of the given :class:`Exscript.Host`.\n\n    :type  host: str or Host\n    :param host: A URL-formatted hostname or a :class:`Exscript.Host` instance.\n    :type  default_protocol: str\n    :param default_protocol: Protocol that is used if the URL specifies none.\n    :type  kwargs: dict\n    :param kwargs: Passed to the protocol constructor.\n    :rtype:  Protocol\n    :return: An instance of the protocol.\n    \"\"\"\n    host = to_host(host, default_protocol=default_protocol)\n    protocol = host.get_protocol()\n    conn = create_protocol(protocol, **kwargs)\n    if protocol == 'pseudo':\n        filename = host.get_address()\n        conn.device.add_commands_from_file(filename)\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new protocol that connects to the given host.", "response": "def connect(host, default_protocol='telnet', **kwargs):\n    \"\"\"\n    Like :class:`prepare()`, but also connects to the host by calling\n    :class:`Protocol.connect()`. If the URL or host contain any login info, this\n    function also logs into the host using :class:`Protocol.login()`.\n\n    :type  host: str or Host\n    :param host: A URL-formatted hostname or a :class:`Exscript.Host` object.\n    :type  default_protocol: str\n    :param default_protocol: Protocol that is used if the URL specifies none.\n    :type  kwargs: dict\n    :param kwargs: Passed to the protocol constructor.\n    :rtype:  Protocol\n    :return: An instance of the protocol.\n    \"\"\"\n    host = to_host(host)\n    conn = prepare(host, default_protocol, **kwargs)\n    account = host.get_account()\n    conn.connect(host.get_address(), host.get_tcp_port())\n    if account is not None:\n        conn.login(account)\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfolds to a long a digest supplied as a string.", "response": "def _long_from_raw(thehash):\n    \"\"\"Fold to a long, a digest supplied as a string.\"\"\"\n    hashnum = 0\n    for h in thehash:\n        hashnum <<= 8\n        hashnum |= ord(bytes([h]))\n    return hashnum"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef otp(password, seed, sequence):\n    if len(password) not in list(range(4, 64)):\n        raise ValueError('passphrase length')\n    if len(seed) not in list(range(1, 17)):\n        raise ValueError('seed length')\n    for x in seed:\n        if not x in _VALIDSEEDCHARACTERS:\n            raise ValueError('seed composition')\n    if sequence < 0:\n        raise ValueError('sequence')\n\n    # Pycryptodome only supports byte strings.\n    seed = seed.encode('utf-8')\n    password = password.encode('utf-8')\n\n    # Discard the first <sequence> keys\n    thehash = MD4.new(seed + password).digest()\n    thehash = _fold_md4_or_md5(thehash)\n    for i in range(0, sequence):\n        thehash = _fold_md4_or_md5(MD4.new(thehash).digest())\n\n    # Generate the result\n    return _sixword_from_raw(thehash)", "response": "Calculates a one - time password hash using the given password seed and sequence number and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the first match of the given string against the given regular expression.", "response": "def first_match(string, regex, flags=re.M):\n    \"\"\"\n    Matches the given string against the given regex.\n\n      - If no match is found and the regular expression has zero or one\n        groups, this function returns None.\n\n      - If no match is found and the regular expression has more than one\n        group, this function returns a tuple of None. The number of elements\n        in the tuple equals the number of groups in the regular expression.\n\n      - If a match is found and the regular expression has no groups,\n        the entire string is returned.\n\n      - If a match is found and the regular expression has one group,\n        the matching string from the group is returned.\n\n      - If a match is found and the regular expression has multiple groups,\n        a tuple containing the matching strings from the groups is returned.\n\n    This behavior ensures that the following assignments can never fail::\n\n       foo   = 'my test'\n       match = first_match(foo, r'aaa')         # Returns None\n       match = first_match(foo, r'\\S+')         # Returns 'my test'\n       match = first_match(foo, r'(aaa)')       # Returns None\n       match = first_match(foo, r'(\\S+)')       # Returns 'my'\n       match = first_match(foo, r'(aaa) (\\S+)') # Returns (None, None)\n       match = first_match(foo, r'(\\S+) (\\S+)') # Returns ('my', 'foo')\n\n    :type  string: string|Exscript.protocols.Protocol\n    :param string: The string that is matched, or a Protocol object.\n    :type  regex: string\n    :param regex: A regular expression.\n    :type  flags: int\n    :param flags: The flags for compiling the regex; e.g. re.I\n    :rtype:  string|tuple\n    :return: A match, or a tuple of matches.\n    \"\"\"\n    if isinstance(string, Protocol):\n        string = string.response\n    return _first_match(string, re.compile(regex, flags))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of strings or tuples that match the given regular expression.", "response": "def any_match(string, regex, flags=re.M):\n    \"\"\"\n    Matches the given string against the given regex.\n\n      - If no match is found, this function returns an empty list.\n\n      - If a match is found and the regular expression has no groups,\n        a list of matching lines returned.\n\n      - If a match is found and the regular expression has one group,\n        a list of matching strings is returned.\n\n      - If a match is found and the regular expression has multiple groups,\n        a list containing tuples of matching strings is returned.\n\n    This behavior ensures that the following can never fail::\n\n        foo = '1 uno\\\\n2 due'\n        for m in any_match(foo, r'aaa'):         # Returns []\n            print(m)\n\n        for m in any_match(foo, r'\\S+'):         # Returns ['1 uno', '2 due']\n            print(m)\n\n        for m in any_match(foo, r'(aaa)'):       # Returns []\n            print(m)\n\n        for m in any_match(foo, r'(\\S+)'):       # Returns ['1', '2']\n            print(m)\n\n        for one, two in any_match(foo, r'(aaa) (\\S+)'): # Returns []\n            print(m)\n\n        for one, two in any_match(foo, r'(\\S+) (\\S+)'): # Returns [('1', 'uno'), ('2', 'due')]\n            print(m)\n\n    :type  string: string|Exscript.protocols.Protocol\n    :param string: The string that is matched, or a Protocol object.\n    :type  regex: string\n    :param regex: A regular expression.\n    :type  flags: int\n    :param flags: The flags for compiling the regex; e.g. re.I\n    :rtype:  list[string|tuple]\n    :return: A list of strings, or a list of tuples.\n    \"\"\"\n    if isinstance(string, Protocol):\n        string = string.response\n    compiled = re.compile(regex, flags)\n    results = []\n    if compiled.groups <= 1:\n        for line in string.split('\\n'):\n            match = _first_match(line, compiled)\n            if match is None:\n                continue\n            results.append(match)\n    else:\n        for line in string.split('\\n'):\n            match = _first_match(line, compiled)\n            if match[0] is None:\n                continue\n            results.append(match)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef daemonize():\n    sys.stdout.flush()\n    sys.stderr.flush()\n\n    # UNIX double-fork magic. We need to fork before any threads are\n    # created.\n    pid = os.fork()\n    if pid > 0:\n        # Exit first parent.\n        sys.exit(0)\n\n    # Decouple from parent environment.\n    os.chdir('/')\n    os.setsid()\n    os.umask(0)\n\n    # Now fork again.\n    pid = os.fork()\n    if pid > 0:\n        # Exit second parent.\n        sys.exit(0)\n\n    _redirect_output(os.devnull)", "response": "Forks and daemonizes the current process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aborted(self, exc_info):\n        self.exc_info = exc_info\n        self.did_end = True\n        self.write(format_exception(*self.exc_info))", "response": "Called by a logger to log an exception."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_file(filename, password='', keytype=None):\n        if keytype is None:\n            try:\n                key = RSAKey.from_private_key_file(filename)\n                keytype = 'rsa'\n            except SSHException as e:\n                try:\n                    key = DSSKey.from_private_key_file(filename)\n                    keytype = 'dss'\n                except SSHException as e:\n                    msg = 'not a recognized private key: ' + repr(filename)\n                    raise ValueError(msg)\n        key = PrivateKey(keytype)\n        key.filename = filename\n        key.password = password\n        return key", "response": "Returns a new PrivateKey instance with the given attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize the given IP address into a standard fixed - length form such as 1234. 0010102A. 00101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101A.", "response": "def normalize_ip(ip):\n    \"\"\"\n    Transform the address into a standard, fixed-length form, such as:\n\n        1234:0:01:02:: -> 1234:0000:0001:0002:0000:0000:0000:0000\n        1234::A -> 1234:0000:0000:0000:0000:0000:0000:000a\n\n    :type  ip: string\n    :param ip: An IP address.\n    :rtype:  string\n    :return: The normalized IP.\n    \"\"\"\n    theip = ip\n    if theip.startswith('::'):\n        theip = '0' + theip\n    if theip.endswith('::'):\n        theip += '0'\n    segments = theip.split(':')\n    if len(segments) == 1:\n        raise ValueError('no colons in ipv6 address: ' + repr(ip))\n    fill = 8 - len(segments)\n    if fill < 0:\n        raise ValueError('ipv6 address has too many segments: ' + repr(ip))\n    result = []\n    for segment in segments:\n        if segment == '':\n            if fill == 0:\n                raise ValueError('unexpected double colon: ' + repr(ip))\n            for n in range(fill + 1):\n                result.append('0000')\n            fill = 0\n        else:\n            try:\n                int(segment, 16)\n            except ValueError:\n                raise ValueError('invalid hex value in ' + repr(ip))\n            result.append(segment.rjust(4, '0'))\n    return ':'.join(result).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_ip(ip):\n    theip = normalize_ip(ip)\n    segments = ['%x' % int(s, 16) for s in theip.split(':')]\n\n    # Find the longest consecutive sequence of zeroes.\n    seq = {0: 0}\n    start = None\n    count = 0\n    for n, segment in enumerate(segments):\n        if segment != '0':\n            start = None\n            count = 0\n            continue\n        if start is None:\n            start = n\n        count += 1\n        seq[count] = start\n\n    # Replace those zeroes by a double colon.\n    count = max(seq)\n    start = seq[count]\n    result = []\n    for n, segment in enumerate(segments):\n        if n == start and count > 1:\n            if n == 0:\n                result.append('')\n            result.append('')\n            if n == 7:\n                result.append('')\n            continue\n        elif start < n < start + count:\n            if n == 7:\n                result.append('')\n            continue\n        result.append(segment)\n    return ':'.join(result)", "response": "Cleans up the ip address for the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_prefix(prefix, default_length=128):\n    if '/' in prefix:\n        network, pfxlen = prefix.split('/')\n    else:\n        network = prefix\n        pfxlen = default_length\n    return network, int(pfxlen)", "response": "Splits the given IP prefix into a network address and a prefix length."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_uri(self, uri):\n        try:\n            uri = Url.from_string(uri, self.protocol)\n        except ValueError:\n            raise ValueError('Hostname parse error: ' + repr(uri))\n        hostname = uri.hostname or ''\n        name = uri.path and hostname + uri.path or hostname\n        self.set_protocol(uri.protocol)\n        self.set_tcp_port(uri.port)\n        self.set_name(name)\n        self.set_address(name)\n\n        if uri.username is not None \\\n           or uri.password1 is not None \\\n           or uri.password2:\n            account = Account(uri.username, uri.password1, uri.password2)\n            self.set_account(account)\n\n        for key, val in list(uri.vars.items()):\n            self.set(key, val)", "response": "Sets the protocol hostname address TCP port number username and password from the given URL formatted hostname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a URI formatted representation of the host including all the attributes except for the name.", "response": "def get_uri(self):\n        \"\"\"\n        Returns a URI formatted representation of the host, including all\n        of it's attributes except for the name. Uses the\n        address, not the name of the host to build the URI.\n\n        :rtype:  str\n        :return: A URI.\n        \"\"\"\n        url = Url()\n        url.protocol = self.get_protocol()\n        url.hostname = self.get_address()\n        url.port = self.get_tcp_port()\n        url.vars = dict((k, to_list(v))\n                        for (k, v) in list(self.get_all().items())\n                        if isinstance(v, str) or isinstance(v, list))\n\n        if self.account:\n            url.username = self.account.get_name()\n            url.password1 = self.account.get_password()\n            url.password2 = self.account.authorization_password\n\n        return str(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary containing the host s attributes.", "response": "def get_dict(self):\n        \"\"\"\n        Returns a dict containing the host's attributes. The following\n        keys are contained:\n\n            - hostname\n            - address\n            - protocol\n            - port\n\n        :rtype:  dict\n        :return: The resulting dictionary.\n        \"\"\"\n        return {'hostname': self.get_name(),\n                'address':  self.get_address(),\n                'protocol': self.get_protocol(),\n                'port':     self.get_tcp_port()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_address(self, address):\n        if is_ip(address):\n            self.address = clean_ip(address)\n        else:\n            self.address = address", "response": "Set the address of the remote host that the is contacted without the related information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_option(self, name, value):\n        if name not in ('debug', 'verify_fingerprint', 'driver'):\n            raise TypeError('No such option: ' + repr(name))\n        if self.options is None:\n            self.options = {}\n        self.options[name] = value", "response": "Sets the value of an option for the host."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the value of the given option if it is defined returns the given default value otherwise.", "response": "def get_option(self, name, default=None):\n        \"\"\"\n        Returns the value of the given option if it is defined, returns\n        the given default value otherwise.\n\n        :type  name: str\n        :param name: The option name.\n        :type  default: object\n        :param default: A default value.\n        \"\"\"\n        if self.options is None:\n            return default\n        return self.options.get(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_tcp_port(self, tcp_port):\n        if tcp_port is None:\n            self.tcp_port = None\n            return\n        self.tcp_port = int(tcp_port)", "response": "This method sets the TCP port number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the given variable value in the object for later retrieval.", "response": "def set(self, name, value):\n        \"\"\"\n        Stores the given variable/value in the object for later retrieval.\n\n        :type  name: string\n        :param name: The name of the variable.\n        :type  value: object\n        :param value: The value of the variable.\n        \"\"\"\n        if self.vars is None:\n            self.vars = {}\n        self.vars[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending the given value to the given name.", "response": "def append(self, name, value):\n        \"\"\"\n        Appends the given value to the list variable with the given name.\n\n        :type  name: string\n        :param name: The name of the variable.\n        :type  value: object\n        :param value: The appended value.\n        \"\"\"\n        if self.vars is None:\n            self.vars = {}\n        if name in self.vars:\n            self.vars[name].append(value)\n        else:\n            self.vars[name] = [value]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking set but only sets the value if the variable is not already set.", "response": "def set_default(self, name, value):\n        \"\"\"\n        Like set(), but only sets the value if the variable is not already\n        defined.\n\n        :type  name: string\n        :param name: The name of the variable.\n        :type  value: object\n        :param value: The value of the variable.\n        \"\"\"\n        if self.vars is None:\n            self.vars = {}\n        if name not in self.vars:\n            self.vars[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the given variable or the given default value if the variable is not defined.", "response": "def get(self, name, default=None):\n        \"\"\"\n        Returns the value of the given variable, or the given default\n        value if the variable is not defined.\n\n        :type  name: string\n        :param name: The name of the variable.\n        :type  default: object\n        :param default: The default value.\n        :rtype:  object\n        :return: The value of the variable.\n        \"\"\"\n        if self.vars is None:\n            return default\n        return self.vars.get(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate a one - time password hash using the given password seed and sequence number and returns it.", "response": "def otp(scope, password, seed, seqs):\n    \"\"\"\n    Calculates a one-time password hash using the given password, seed, and\n    sequence number and returns it.\n    Uses the md4/sixword algorithm as supported by TACACS+ servers.\n\n    :type  password: string\n    :param password: A password.\n    :type  seed: string\n    :param seed: A username.\n    :type  seqs: int\n    :param seqs: A sequence number, or a list of sequence numbers.\n    :rtype:  string\n    :return: A hash, or a list of hashes.\n    \"\"\"\n    return [crypt.otp(password[0], seed[0], int(seq)) for seq in seqs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_label(obj, name, **kwargs):\n    labels = obj.__dict__.setdefault('_labels', dict())\n    labels[name] = kwargs\n    return obj", "response": "Adds a label to an object such that it can later be checked with the label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_label(obj, name):\n    labels = obj.__dict__.get('_labels')\n    if labels is None:\n        return None\n    return labels.get(name)", "response": "Checks whether an object has the given label attached and returns the associated options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy all labels of one object to another object.", "response": "def copy_labels(src, dst):\n    \"\"\"\n    Copies all labels of one object to another object.\n\n    :type  src: object\n    :param src: The object to check read the labels from.\n    :type  dst: object\n    :param dst: The object into which the labels are copied.\n    \"\"\"\n    labels = src.__dict__.get('_labels')\n    if labels is None:\n        return\n    dst.__dict__['_labels'] = labels.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serializeable_exc_info(thetype, ex, tb):\n    return thetype, ex, ''.join(traceback.format_exception(thetype, ex, tb))", "response": "Serialize exception info tuple into a nice format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representation of an exception.", "response": "def format_exception(thetype, ex, tb):\n    \"\"\"\n    This function is a drop-in replacement for Python's\n    traceback.format_exception().\n\n    Since traceback objects can not be pickled, Exscript is forced to\n    manipulate them before they are passed accross process boundaries.\n    This leads to the fact the Python's traceback.format_exception()\n    no longer works for those objects.\n\n    This function works with any traceback object, regardless of whether\n    or not Exscript manipulated it.\n    \"\"\"\n    if isinstance(tb, str):\n        return tb\n    return ''.join(traceback.format_exception(thetype, ex, tb))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deprecated(func):\n    def decorated(*args, **kwargs):\n        warnings.warn('Call to deprecated function %s.' % func.__name__,\n                      category=DeprecationWarning,\n                      stacklevel=2)\n        return func(*args, **kwargs)\n    decorated.__name__ = func.__name__\n    decorated.__doc__ = func.__doc__\n    decorated.__dict__.update(func.__dict__)\n    return decorated", "response": "A decorator for marking functions as deprecated. Results in\n    a printed warning message when the function is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef synchronized(func):\n    @wraps(func)\n    def wrapped(self, *args, **kwargs):\n        try:\n            rlock = self._sync_lock\n        except AttributeError:\n            from multiprocessing import RLock\n            rlock = self.__dict__.setdefault('_sync_lock', RLock())\n        with rlock:\n            return func(self, *args, **kwargs)\n    return wrapped", "response": "Decorator for synchronizing method access."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eval(conn, string, strip_command=True, **kwargs):\n    parser_args = {'strip_command': strip_command}\n    return _run(conn, None, string, parser_args, **kwargs)", "response": "Compiles a given string and executes it on the given connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the template from a file", "response": "def eval_file(conn, filename, strip_command=True, **kwargs):\n    \"\"\"\n    Convenience wrapper around eval() that reads the template from a file\n    instead.\n\n    :type  conn: Exscript.protocols.Protocol\n    :param conn: The connection on which to run the template.\n    :type  filename: string\n    :param filename: The name of the template file.\n    :type  strip_command: bool\n    :param strip_command: Whether to strip the command echo from the response.\n    :type  kwargs: dict\n    :param kwargs: Variables to define in the template.\n    \"\"\"\n    parser_args = {'strip_command': strip_command}\n    with open(filename, 'r') as fp:\n        return _run(conn, filename, fp.read(), parser_args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps around paste that reads a file from a file instead.", "response": "def paste_file(conn, filename, **kwargs):\n    \"\"\"\n    Convenience wrapper around paste() that reads the template from a file\n    instead.\n\n    :type  conn: Exscript.protocols.Protocol\n    :param conn: The connection on which to run the template.\n    :type  filename: string\n    :param filename: The name of the template file.\n    :type  kwargs: dict\n    :param kwargs: Variables to define in the template.\n    \"\"\"\n    with open(filename, 'r') as fp:\n        return _run(conn, None, fp.read(), {'no_prompt': True}, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _urlparse_qs(url):\n    # Extract the query part from the URL.\n    querystring = urlparse(url)[4]\n\n    # Split the query into name/value pairs.\n    pairs = [s2 for s1 in querystring.split('&') for s2 in s1.split(';')]\n\n    # Split the name/value pairs.\n    result = OrderedDefaultDict(list)\n    for name_value in pairs:\n        pair = name_value.split('=', 1)\n        if len(pair) != 2:\n            continue\n\n        if len(pair[1]) > 0:\n            name = _unquote(pair[0].replace('+', ' '))\n            value = _unquote(pair[1].replace('+', ' '))\n            result[name].append(value)\n\n    return result", "response": "Parse a URL query string and return the components as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a string into a Url object.", "response": "def from_string(url, default_protocol='telnet'):\n        \"\"\"\n        Parses the given URL and returns an URL object. There are some\n        differences to Python's built-in URL parser:\n\n        - It is less strict, many more inputs are accepted. This is\n          necessary to allow for passing a simple hostname as a URL.\n        - You may specify a default protocol that is used when the http://\n          portion is missing.\n        - The port number defaults to the well-known port of the given\n          protocol.\n        - The query variables are parsed into a dictionary (Url.vars).\n\n        :type  url: str\n        :param url: A URL.\n        :type  default_protocol: string\n        :param default_protocol: A protocol name.\n        :rtype:  Url\n        :return: The Url object contructed from the given URL.\n        \"\"\"\n        if url is None:\n            raise TypeError('Expected string but got' + type(url))\n\n        # Extract the protocol name from the URL.\n        result = Url()\n        match = re.match(r'(\\w+)://', url)\n        if match:\n            result.protocol = match.group(1)\n        else:\n            result.protocol = default_protocol\n\n        # Now remove the query from the url.\n        query = ''\n        if '?' in url:\n            url, query = url.split('?', 1)\n        result.vars = _urlparse_qs('http://dummy/?' + query)\n\n        # Substitute the protocol name by 'http', because Python's urlsplit\n        # fails on our protocol names otherwise.\n        prefix = result.protocol + '://'\n        if url.startswith(prefix):\n            url = url[len(prefix):]\n        url = 'http://' + url\n\n        # Parse the remaining url.\n        parsed = urlsplit(url, 'http', False)\n        netloc = parsed[1]\n\n        # Parse username and password.\n        auth = ''\n        if '@' in netloc:\n            auth, netloc = netloc.split('@')\n            auth = auth.split(':')\n            try:\n                result.username = _unquote(auth[0])\n                result.password1 = _unquote(auth[1])\n                result.password2 = _unquote(auth[2])\n            except IndexError:\n                pass\n\n        # Parse hostname and port number.\n        result.hostname = netloc + parsed.path\n        result.port = _WELL_KNOWN_PORTS.get(result.protocol)\n        if ':' in netloc:\n            result.hostname, port = netloc.split(':')\n            result.port = int(port)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the debug level.", "response": "def set_debug(self, debug=1):\n        \"\"\"\n        Set the debug level.\n\n        :type  debug: int\n        :param debug: The debug level.\n        \"\"\"\n        self._check_if_ready()\n        self.debug = debug\n        self.main_loop.debug = debug"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_max_threads(self, max_threads):\n        if max_threads is None:\n            raise TypeError('max_threads must not be None.')\n        self._check_if_ready()\n        self.collection.set_max_working(max_threads)", "response": "Sets the maximum number of concurrent threads."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enqueue(self, function, name=None, times=1, data=None):\n        self._check_if_ready()\n        return self.main_loop.enqueue(function, name, times, data)", "response": "Adds a function to the queue for execution."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enqueue_or_ignore(self, function, name=None, times=1, data=None):\n        self._check_if_ready()\n        return self.main_loop.enqueue_or_ignore(function, name, times, data)", "response": "Enqueue a function to the main loop and return the id of the new job."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef priority_enqueue(self,\n                         function,\n                         name=None,\n                         force_start=False,\n                         times=1,\n                         data=None):\n        \"\"\"\n        Like :class:`enqueue()`, but adds the given function at the top of the\n        queue.\n        If force_start is True, the function is immediately started even when\n        the maximum number of concurrent threads is already reached.\n\n        :type  function: callable\n        :param function: The function that is executed.\n        :type  name: str\n        :param name: Stored in Job.name.\n        :type  force_start: bool\n        :param force_start: Whether to start execution immediately.\n        :type  times: int\n        :param times: The maximum number of attempts.\n        :type  data: object\n        :param data: Optional data to store in Job.data.\n        :rtype:  int\n        :return: The id of the new job.\n        \"\"\"\n        self._check_if_ready()\n        return self.main_loop.priority_enqueue(function,\n                                               name,\n                                               force_start,\n                                               times,\n                                               data)", "response": "Enqueue a function at the top of the priority queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops execution of enqueued jobs and wait for all running jobs to complete.", "response": "def shutdown(self, restart=True):\n        \"\"\"\n        Stop the execution of enqueued jobs, and wait for all running\n        jobs to complete. This method is synchronous and returns as soon\n        as all jobs are terminated (i.e. all threads are stopped).\n\n        If restart is True, the workqueue is restarted and paused,\n        so you may fill it with new jobs.\n\n        If restart is False, the WorkQueue can no longer be used after calling\n        this method.\n\n        :type  restart: bool\n        :param restart: Whether to restart the queue after shutting down.\n        \"\"\"\n        self._check_if_ready()\n        self.collection.stop()\n        self.collection.wait()\n        self.main_loop.join()\n        self.main_loop = None\n        self.collection.clear()\n        if restart:\n            self.collection.start()\n            self._init()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndestroying the queue and all the jobs in the queue.", "response": "def destroy(self):\n        \"\"\"\n        Like shutdown(), but does not restart the queue and does not\n        wait for already started jobs to complete.\n        \"\"\"\n        self._check_if_ready()\n        self.collection.stop()\n        self.main_loop.join()\n        self.main_loop = None\n        self.collection.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticate_user(scope, user=[None], password=[None]):\n    conn = scope.get('__connection__')\n    user = user[0]\n    if user is None:\n        conn.app_authenticate()\n    else:\n        account = Account(user, password[0])\n        conn.app_authenticate(account)\n    return True", "response": "Authenticate a user and password."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authorize(scope, password=[None]):\n    conn = scope.get('__connection__')\n    password = password[0]\n    if password is None:\n        conn.app_authorize()\n    else:\n        account = Account('', password)\n        conn.app_authorize(account)\n    return True", "response": "This function is used to authorize a new user on the current connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a command on the remote host that causes an authorization procedure to be started, then authorizes using the given password in the same way in which authorize() works. Depending on the detected operating system of the remote host the following commands are started: - on IOS, the \"enable\" command is executed. - nothing on other operating systems yet. :type password: string :param password: A password.", "response": "def auto_authorize(scope, password=[None]):\n    \"\"\"\n    Executes a command on the remote host that causes an authorization\n    procedure to be started, then authorizes using the given password\n    in the same way in which authorize() works.\n    Depending on the detected operating system of the remote host the\n    following commands are started:\n\n      - on IOS, the \"enable\" command is executed.\n      - nothing on other operating systems yet.\n\n    :type  password: string\n    :param password: A password.\n    \"\"\"\n    conn = scope.get('__connection__')\n    password = password[0]\n    if password is None:\n        conn.auto_app_authorize()\n    else:\n        account = Account('', password)\n        conn.auto_app_authorize(account)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the connection with the remote host.", "response": "def close(scope):\n    \"\"\"\n    Closes the existing connection with the remote host. This function is\n    rarely used, as normally Exscript closes the connection automatically\n    when the script has completed.\n    \"\"\"\n    conn = scope.get('__connection__')\n    conn.close(1)\n    scope.define(__response__=conn.response)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exec_(scope, data):\n    conn = scope.get('__connection__')\n    response = []\n    for line in data:\n        conn.send(line)\n        conn.expect_prompt()\n        response += conn.response.split('\\n')[1:]\n    scope.define(__response__=response)\n    return True", "response": "Sends the given data to the remote host and waits until the remote host has responded with a prompt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execline(scope, data):\n    conn = scope.get('__connection__')\n    response = []\n    for line in data:\n        conn.execute(line)\n        response += conn.response.split('\\n')[1:]\n    scope.define(__response__=response)\n    return True", "response": "Like exec but appends a newline to the command in data before sending it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send(scope, data):\n    conn = scope.get('__connection__')\n    for line in data:\n        conn.send(line)\n    return True", "response": "Send data to the remote host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for(scope, prompt):\n    conn = scope.get('__connection__')\n    conn.expect(prompt)\n    scope.define(__response__=conn.response)\n    return True", "response": "Waits until the response of the remote host contains the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_prompt(scope, prompt=None):\n    conn = scope.get('__connection__')\n    conn.set_prompt(prompt)\n    return True", "response": "Sets the prompt pattern for the given scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the error pattern for the remote .", "response": "def set_error(scope, error_re=None):\n    \"\"\"\n    Defines a pattern that, whenever detected in the response of the remote\n    host, causes an error to be raised.\n\n    In other words, whenever Exscript waits for a prompt, it searches the\n    response of the host for the given pattern and raises an error if the\n    pattern is found.\n\n    :type  error_re: regex\n    :param error_re: The error pattern.\n    \"\"\"\n    conn = scope.get('__connection__')\n    conn.set_error_prompt(error_re)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_timeout(scope, timeout):\n    conn = scope.get('__connection__')\n    conn.set_timeout(int(timeout[0]))\n    return True", "response": "Sets the timeout for the a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a command to the list of commands that can be executed.", "response": "def add_command(self, command, handler, prompt=True):\n        \"\"\"\n        Registers a command.\n\n        The command may be either a string (which is then automatically\n        compiled into a regular expression), or a pre-compiled regular\n        expression object.\n\n        If the given response handler is a string, it is sent as the\n        response to any command that matches the given regular expression.\n        If the given response handler is a function, it is called\n        with the command passed as an argument.\n\n        :type  command: str|regex\n        :param command: A string or a compiled regular expression.\n        :type  handler: function|str\n        :param handler: A string, or a response handler.\n        :type  prompt: bool\n        :param prompt: Whether to show a prompt after completing the command.\n        \"\"\"\n        if prompt:\n            thehandler = self._create_autoprompt_handler(handler)\n        else:\n            thehandler = handler\n        self.commands.add(command, thehandler)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping around add_command_handler that reads the commands from the given file.", "response": "def add_commands_from_file(self, filename, autoprompt=True):\n        \"\"\"\n        Wrapper around add_command_handler that reads the handlers from the\n        file with the given name. The file is a Python script containing\n        a list named 'commands' of tuples that map command names to\n        handlers.\n\n        :type  filename: str\n        :param filename: The name of the file containing the tuples.\n        :type  autoprompt: bool\n        :param autoprompt: Whether to append a prompt to each response.\n        \"\"\"\n        if autoprompt:\n            deco = self._create_autoprompt_handler\n        else:\n            deco = None\n        self.commands.add_from_file(filename, deco)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize or reset the virtual device.", "response": "def init(self):\n        \"\"\"\n        Init or reset the virtual device.\n\n        :rtype:  str\n        :return: The initial response of the virtual device.\n        \"\"\"\n        self.logged_in = False\n\n        if self.login_type == self.LOGIN_TYPE_PASSWORDONLY:\n            self.prompt_stage = self.PROMPT_STAGE_PASSWORD\n        elif self.login_type == self.LOGIN_TYPE_NONE:\n            self.prompt_stage = self.PROMPT_STAGE_CUSTOM\n        else:\n            self.prompt_stage = self.PROMPT_STAGE_USERNAME\n\n        return self.banner + self._get_prompt()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do(self, command):\n        echo = self.echo and command or ''\n        if not self.logged_in:\n            return echo + '\\n' + self._get_prompt()\n\n        response = self.commands.eval(command)\n        if response is None:\n            return echo + '\\n' + self._get_prompt()\n        return echo + response", "response": "Executes the given command on the virtual device and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprompting the user for input.", "response": "def prompt(key,\n           message,\n           default=None,\n           doverh=True,\n           strip=True,\n           check=None,\n           history=None):\n    \"\"\"\n    Prompt the user for input. This function is similar to Python's built\n    in raw_input, with the following differences:\n\n        - You may specify a default value that is returned if the user\n          presses \"enter\" without entering anything.\n        - The user's input is recorded in a config file, and offered\n          as the default value the next time this function is used\n          (based on the key argument).\n\n    The config file is based around the :class:`InputHistory`. If a history object\n    is not passed in the history argument, a new one will be created.\n\n    The key argument specifies under which name the input is saved in the\n    config file.\n\n    The given default value is only used if a default was not found in the\n    history.\n\n    The strip argument specifies that the returned value should be stripped\n    of whitespace (default).\n\n    The check argument allows for validating the input; if the validation\n    fails, the user is prompted again before the value is stored in the\n    InputHistory. Example usage::\n\n        def validate(input):\n            if len(input) < 4:\n                return 'Please enter at least 4 characters!'\n        value = prompt('test', 'Enter a value', 'My Default', check = validate)\n        print('You entered:', value)\n\n    This leads to the following output::\n\n        Please enter a value [My Default]: abc\n        Please enter at least 4 characters!\n        Please enter a value [My Default]: Foobar\n        You entered: Foobar\n\n    The next time the same code is started, the input 'Foobar' is remembered::\n\n        Please enter a value [Foobar]:        (enters nothing)\n        You entered: Foobar\n\n    :type  key: str\n    :param key: The key under which to store the input in the :class:`InputHistory`.\n    :type  message: str\n    :param message: The user prompt.\n    :type  default: str|None\n    :param default: The offered default if none was found in the history.\n    :type  doverh: bool\n    :param doverh: Whether to prefer default value over history value.\n    :type  strip: bool\n    :param strip: Whether to remove whitespace from the input.\n    :type  check: callable\n    :param check: A function that is called for validating the input.\n    :type  history: :class:`InputHistory` or None\n    :param history: The history used for recording default values, or None.\n    \"\"\"\n    if history is None:\n        history = InputHistory()\n    if not doverh or default is None:\n        default = history.get(key, str(default))\n    while True:\n        if default is None:\n            value = input('%s: ' % message)\n        else:\n            value = input('%s [%s]: ' % (message, default)) or default\n        if strip and isinstance(value, str):\n            value = value.strip()\n        if not check:\n            break\n        errors = check(value)\n        if errors:\n            print('\\n'.join(to_list(errors)))\n        else:\n            break\n    history.set(key, value)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprompt the user to enter a filename.", "response": "def get_filename(key, message, default=None, history=None):\n    \"\"\"\n    Like :meth:`prompt`, but only accepts the name of an existing file\n    as an input.\n\n    :type  key: str\n    :param key: The key under which to store the input in the :class:`InputHistory`.\n    :type  message: str\n    :param message: The user prompt.\n    :type  default: str|None\n    :param default: The offered default if none was found in the history.\n    :type  history: :class:`InputHistory` or None\n    :param history: The history used for recording default values, or None.\n    \"\"\"\n    def _validate(string):\n        if not os.path.isfile(string):\n            return 'File not found. Please enter a filename.'\n    return prompt(key, message, default, True, _validate, history)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprompts the user for his login name defaulting to the USER environment variable. Returns a string containing the username.", "response": "def get_user(prompt=None):\n    \"\"\"\n    Prompts the user for his login name, defaulting to the USER environment\n    variable. Returns a string containing the username.\n    May throw an exception if EOF is given by the user.\n\n    :type  prompt: str|None\n    :param prompt: The user prompt or the default one if None.\n    :rtype:  string\n    :return: A username.\n    \"\"\"\n    # Read username and password.\n    try:\n        env_user = getpass.getuser()\n    except KeyError:\n        env_user = ''\n    if prompt is None:\n        prompt = \"Please enter your user name\"\n    if env_user is None or env_user == '':\n        user = input('%s: ' % prompt)\n    else:\n        user = input('%s [%s]: ' % (prompt, env_user))\n        if user == '':\n            user = env_user\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, key, default=None):\n        if not self.parser:\n            return default\n        try:\n            return self.parser.get(self.section, key)\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            return default", "response": "Returns the input with the given key from the config file or the default value if the key is not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, key, value):\n        if value is None:\n            return None\n\n        self.parser.set(self.section, key, value)\n\n        # Unfortunately ConfigParser attempts to write a string to the file\n        # object, and NamedTemporaryFile uses binary mode. So we nee to create\n        # the tempfile, and then re-open it.\n        with NamedTemporaryFile(delete=False) as tmpfile:\n            pass\n        with codecs.open(tmpfile.name, 'w', encoding='utf8') as fp:\n            self.parser.write(fp)\n\n        self.file.close()\n        shutil.move(tmpfile.name, self.file.name)\n        self.file = open(self.file.name)\n        return value", "response": "Sets the given key to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acquire(self, signal=True):\n        if not self.needs_lock:\n            return\n        with self.synclock:\n            while not self.lock.acquire(False):\n                self.synclock.wait()\n            if signal:\n                self.acquired_event(self)\n            self.synclock.notify_all()", "response": "Locks the account.\n        Method has no effect if the constructor argument `needs_lock`\n        wsa set to False.\n\n        :type signal: bool\n        :param signal: Whether to emit the acquired_event signal."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(self, signal=True):\n        if not self.needs_lock:\n            return\n        with self.synclock:\n            self.lock.release()\n            if signal:\n                self.released_event(self)\n            self.synclock.notify_all()", "response": "Unlocks the account.\n        Method has no effect if the constructor argument `needs_lock`\n        wsa set to False.\n\n        :type signal: bool\n        :param signal: Whether to emit the released_event signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_name(self, name):\n        self.name = name\n        self.changed_event.emit(self)", "response": "Changes the name of the account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the password of the account.", "response": "def set_password(self, password):\n        \"\"\"\n        Changes the password of the account.\n\n        :type  password: string\n        :param password: The account password.\n        \"\"\"\n        self.password = password\n        self.changed_event.emit(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the authorization password of the account.", "response": "def set_authorization_password(self, password):\n        \"\"\"\n        Changes the authorization password of the account.\n\n        :type  password: string\n        :param password: The new authorization password.\n        \"\"\"\n        self.authorization_password = password\n        self.changed_event.emit(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef for_host(parent, host):\n        account = AccountProxy(parent)\n        account.host = host\n        if account.acquire():\n            return account\n        return None", "response": "Returns a new AccountProxy that has an account acquired for the given host."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef for_account_hash(parent, account_hash):\n        account = AccountProxy(parent)\n        account.account_hash = account_hash\n        if account.acquire():\n            return account\n        return None", "response": "Returns a new AccountProxy that acquires the account with the given hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nacquire the account. Returns True on success False on failure.", "response": "def acquire(self):\n        \"\"\"\n        Locks the account. Returns True on success, False if the account\n        is thread-local and must not be locked.\n        \"\"\"\n        if self.host:\n            self.parent.send(('acquire-account-for-host', self.host))\n        elif self.account_hash:\n            self.parent.send(('acquire-account-from-hash', self.account_hash))\n        else:\n            self.parent.send(('acquire-account'))\n\n        response = self.parent.recv()\n        if isinstance(response, Exception):\n            raise response\n        if response is None:\n            return False\n\n        self.account_hash, \\\n            self.user, \\\n            self.password, \\\n            self.authorization_password, \\\n            self.key = response\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_account_from_hash(self, account_hash):\n        for account in self.accounts:\n            if account.__hash__() == account_hash:\n                return account\n        return None", "response": "Returns the account with the given hash or None if no such account is found."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_account(self, accounts):\n        with self.unlock_cond:\n            for account in to_list(accounts):\n                account.acquired_event.listen(self._on_account_acquired)\n                account.released_event.listen(self._on_account_released)\n                self.accounts.add(account)\n                self.unlocked_accounts.append(account)\n            self.unlock_cond.notify_all()", "response": "Adds one or more accounts to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_account(self, accounts):\n        for account in to_list(accounts):\n            if account not in self.accounts:\n                msg = 'attempt to remove unknown account %s' % account\n                raise Exception(msg)\n            if account not in self.unlocked_accounts:\n                raise Exception('account %s should be unlocked' % account)\n            account.acquired_event.disconnect(self._on_account_acquired)\n            account.released_event.disconnect(self._on_account_released)\n            self.accounts.remove(account)\n            self.unlocked_accounts.remove(account)", "response": "Removes the accounts from the list of accounts."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the internal state of the internal state.", "response": "def reset(self):\n        \"\"\"\n        Removes all accounts.\n        \"\"\"\n        with self.unlock_cond:\n            for owner in self.owner2account:\n                self.release_accounts(owner)\n            self._remove_account(self.accounts.copy())\n            self.unlock_cond.notify_all()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_account_from_name(self, name):\n        for account in self.accounts:\n            if account.get_name() == name:\n                return account\n        return None", "response": "Returns the account with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acquire_account(self, account=None, owner=None):\n        with self.unlock_cond:\n            if len(self.accounts) == 0:\n                raise ValueError('account pool is empty')\n\n            if account:\n                # Specific account requested.\n                while account not in self.unlocked_accounts:\n                    self.unlock_cond.wait()\n                self.unlocked_accounts.remove(account)\n            else:\n                # Else take the next available one.\n                while len(self.unlocked_accounts) == 0:\n                    self.unlock_cond.wait()\n                account = self.unlocked_accounts.popleft()\n\n            if owner is not None:\n                self.owner2account[owner].append(account)\n                self.account2owner[account] = owner\n            account.acquire(False)\n            self.unlock_cond.notify_all()\n            return account", "response": "Waits until an account becomes available then locks and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef release_accounts(self, owner):\n        with self.unlock_cond:\n            for account in self.owner2account[owner]:\n                self.account2owner.pop(account)\n                account.release(False)\n                self.unlocked_accounts.append(account)\n            self.owner2account.pop(owner)\n            self.unlock_cond.notify_all()", "response": "Releases all accounts that were acquired by the given owner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_pool(self, pool, match=None):\n        if match is None:\n            self.default_pool = pool\n        else:\n            self.pools.append((match, pool))", "response": "Adds a new account pool to the given account pool."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the account with the given hash if it is contained in any of the pools. Returns None otherwise.", "response": "def get_account_from_hash(self, account_hash):\n        \"\"\"\n        Returns the account with the given hash, if it is contained in any\n        of the pools. Returns None otherwise.\n\n        :type  account_hash: str\n        :param account_hash: The hash of an account object.\n        \"\"\"\n        for _, pool in self.pools:\n            account = pool.get_account_from_hash(account_hash)\n            if account is not None:\n                return account\n        return self.default_pool.get_account_from_hash(account_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nacquires the given account. If no account is given one is chosen from the default pool.", "response": "def acquire_account(self, account=None, owner=None):\n        \"\"\"\n        Acquires the given account. If no account is given, one is chosen\n        from the default pool.\n\n        :type  account: Account\n        :param account: The account that is added.\n        :type  owner: object\n        :param owner: An optional descriptor for the owner.\n        :rtype:  :class:`Account`\n        :return: The account that was acquired.\n        \"\"\"\n        if account is not None:\n            for _, pool in self.pools:\n                if pool.has_account(account):\n                    return pool.acquire_account(account, owner)\n\n            if not self.default_pool.has_account(account):\n                # The account is not in any pool.\n                account.acquire()\n                return account\n\n        return self.default_pool.acquire_account(account, owner)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquire_account_for(self, host, owner=None):\n        # Check whether a matching account pool exists.\n        for match, pool in self.pools:\n            if match(host) is True:\n                return pool.acquire_account(owner=owner)\n\n        # Else, choose an account from the default account pool.\n        return self.default_pool.acquire_account(owner=owner)", "response": "Acquires an account for the given host and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreleases all accounts that were acquired by the given owner.", "response": "def release_accounts(self, owner):\n        \"\"\"\n        Releases all accounts that were acquired by the given owner.\n\n        :type  owner: object\n        :param owner: The owner descriptor as passed to acquire_account().\n        \"\"\"\n        for _, pool in self.pools:\n            pool.release_accounts(owner)\n        self.default_pool.release_accounts(owner)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a command and response pair.", "response": "def add(self, command, response):\n        \"\"\"\n        Register a command/response pair.\n\n        The command may be either a string (which is then automatically\n        compiled into a regular expression), or a pre-compiled regular\n        expression object.\n\n        If the given response handler is a string, it is sent as the\n        response to any command that matches the given regular expression.\n        If the given response handler is a function, it is called\n        with the command passed as an argument.\n\n        :type  command: str|regex\n        :param command: A string or a compiled regular expression.\n        :type  response: function|str\n        :param response: A reponse, or a response handler.\n        \"\"\"\n        command = re.compile(command)\n        self.response_list.append((command, response))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_from_file(self, filename, handler_decorator=None):\n        args = {}\n        execfile(filename, args)\n        commands = args.get('commands')\n        if commands is None:\n            raise Exception(filename + ' has no variable named \"commands\"')\n        elif not hasattr(commands, '__iter__'):\n            raise Exception(filename + ': \"commands\" is not iterable')\n        for key, handler in commands:\n            if handler_decorator:\n                handler = handler_decorator(handler)\n            self.add(key, handler)", "response": "Adds the handlers from the given file to the internal list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the given command against all registered commands and return the defined response.", "response": "def eval(self, command):\n        \"\"\"\n        Evaluate the given string against all registered commands and\n        return the defined response.\n\n        :type  command: str\n        :param command: The command that is evaluated.\n        :rtype:  str or None\n        :return: The response, if one was defined.\n        \"\"\"\n        for cmd, response in self.response_list:\n            if not cmd.match(command):\n                continue\n            if response is None:\n                return None\n            elif hasattr(response, '__call__'):\n                return response(command)\n            else:\n                return response\n        if self.strict:\n            raise Exception('Undefined command: ' + repr(command))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(users, hosts, func, only_authenticate=False, **kwargs):\n    if only_authenticate:\n        run(users, hosts, autoauthenticate()(func), **kwargs)\n    else:\n        run(users, hosts, autologin()(func), **kwargs)", "response": "Starts a new object in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes quickrun(), but automatically logs into the host before passing the connection to the callback function. :type hosts: Host|list[Host] :param hosts: A list of Host objects. :type func: function :param func: The callback function. :type only_authenticate: bool :param only_authenticate: don't authorize, just authenticate? :type kwargs: dict :param kwargs: Passed to the Exscript.Queue constructor.", "response": "def quickstart(hosts, func, only_authenticate=False, **kwargs):\n    \"\"\"\n    Like quickrun(), but automatically logs into the host before passing\n    the connection to the callback function.\n\n    :type  hosts: Host|list[Host]\n    :param hosts: A list of Host objects.\n    :type  func: function\n    :param func: The callback function.\n    :type  only_authenticate: bool\n    :param only_authenticate: don't authorize, just authenticate?\n    :type  kwargs: dict\n    :param kwargs: Passed to the Exscript.Queue constructor.\n    \"\"\"\n    if only_authenticate:\n        quickrun(hosts, autoauthenticate()(func), **kwargs)\n    else:\n        quickrun(hosts, autologin()(func), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the given destination is in the given network.", "response": "def in_network(scope, prefixes, destination, default_pfxlen=[24]):\n    \"\"\"\n    Returns True if the given destination is in the network range that is\n    defined by the given prefix (e.g. 10.0.0.1/22). If the given prefix\n    does not have a prefix length specified, the given default prefix length\n    is applied. If no such prefix length is given, the default length is\n    /24.\n\n    If a list of prefixes is passed, this function returns True only if\n    the given destination is in ANY of the given prefixes.\n\n    :type  prefixes: string\n    :param prefixes: A prefix, or a list of IP prefixes.\n    :type  destination: string\n    :param destination: An IP address.\n    :type  default_pfxlen: int\n    :param default_pfxlen: The default prefix length.\n    :rtype:  True\n    :return: Whether the given destination is in the given network.\n    \"\"\"\n    needle = ipv4.ip2int(destination[0])\n    for prefix in prefixes:\n        network, pfxlen = ipv4.parse_prefix(prefix, default_pfxlen[0])\n        mask = ipv4.pfxlen2mask_int(pfxlen)\n        if needle & mask == ipv4.ip2int(network) & mask:\n            return [True]\n    return [False]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mask(scope, ips, mask):\n    mask = ipv4.ip2int(mask[0])\n    return [ipv4.int2ip(ipv4.ip2int(ip) & mask) for ip in ips]", "response": "Returns the network that result from applying the given IP mask."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of IP addresses that result from applying the given prefix length to the given ips.", "response": "def pfxmask(scope, ips, pfxlen):\n    \"\"\"\n    Applies the given prefix length to the given ips, resulting in a\n    (list of) IP network addresses.\n\n    :type  ips: string\n    :param ips: An IP address, or a list of IP addresses.\n    :type  pfxlen: int\n    :param pfxlen: An IP prefix length.\n    :rtype:  string\n    :return: The mask(s) that result(s) from converting the prefix length.\n    \"\"\"\n    mask = ipv4.pfxlen2mask_int(pfxlen[0])\n    return [ipv4.int2ip(ipv4.ip2int(ip) & mask) for ip in ips]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_host(host, default_protocol='telnet', default_domain=''):\n    if host is None:\n        raise TypeError('None can not be cast to Host')\n    if hasattr(host, 'get_address'):\n        return host\n    if default_domain and not '.' in host:\n        host += '.' + default_domain\n    return Exscript.Host(host, default_protocol=default_protocol)", "response": "Converts a string or Host object to a Host object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list of strings or Host objects or a list of Host objects this function returns a list of Host objects.", "response": "def to_hosts(hosts, default_protocol='telnet', default_domain=''):\n    \"\"\"\n    Given a string or a Host object, or a list of strings or Host objects,\n    this function returns a list of Host objects.\n\n    :type  hosts: string|Host|list(string)|list(Host)\n    :param hosts: One or more hosts or hostnames.\n    :type  default_protocol: str\n    :param default_protocol: Passed to the Host constructor.\n    :type  default_domain: str\n    :param default_domain: Appended to each hostname that has no domain.\n    :rtype:  list[Host]\n    :return: A list of Host objects.\n    \"\"\"\n    return [to_host(h, default_protocol, default_domain)\n            for h in to_list(hosts)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new re. RegexObject.", "response": "def to_regex(regex, flags=0):\n    \"\"\"\n    Given a string, this function returns a new re.RegexObject.\n    Given a re.RegexObject, this function just returns the same object.\n\n    :type  regex: string|re.RegexObject\n    :param regex: A regex or a re.RegexObject\n    :type  flags: int\n    :param flags: See Python's re.compile().\n    :rtype:  re.RegexObject\n    :return: The Python regex object.\n    \"\"\"\n    if regex is None:\n        raise TypeError('None can not be cast to re.RegexObject')\n    if hasattr(regex, 'match'):\n        return regex\n    return re.compile(regex, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the permissions of the given file or list of files.", "response": "def chmod(scope, filename, mode):\n    \"\"\"\n    Changes the permissions of the given file (or list of files)\n    to the given mode. You probably want to use an octal representation\n    for the integer, e.g. \"chmod(myfile, 0644)\".\n\n    :type  filename: string\n    :param filename: A filename.\n    :type  mode: int\n    :param mode: The access permissions.\n    \"\"\"\n    for file in filename:\n        os.chmod(file, mode[0])\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mkdir(scope, dirname, mode=None):\n    for dir in dirname:\n        if mode is None:\n            os.makedirs(dir)\n        else:\n            os.makedirs(dir, mode[0])\n    return True", "response": "Creates the given directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(scope, filename):\n    with open(filename[0], 'r') as fp:\n        lines = fp.readlines()\n    scope.define(__response__=lines)\n    return lines", "response": "Reads the given file and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the given string into the given file.", "response": "def write(scope, filename, lines, mode=['a']):\n    \"\"\"\n    Writes the given string into the given file.\n    The following modes are supported:\n\n      - 'a': Append to the file if it already exists.\n      - 'w': Replace the file if it already exists.\n\n    :type  filename: string\n    :param filename: A filename.\n    :type  lines: string\n    :param lines: The data that is written into the file.\n    :type  mode: string\n    :param mode: Any of the above listed modes.\n    \"\"\"\n    with open(filename[0], mode[0]) as fp:\n        fp.writelines(['%s\\n' % line.rstrip() for line in lines])\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the given string is an IPv4 address False otherwise.", "response": "def is_ip(string):\n    \"\"\"\n    Returns True if the given string is an IPv4 address, False otherwise.\n\n    :type  string: string\n    :param string: Any string.\n    :rtype:  bool\n    :return: True if the string is an IP address, False otherwise.\n    \"\"\"\n    mo = re.match(r'(\\d+)\\.(\\d+)\\.(\\d+)\\.(\\d+)', string)\n    if mo is None:\n        return False\n    for group in mo.groups():\n        if int(group) not in list(range(0, 256)):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_ip(ip):\n    theip = ip.split('.')\n    if len(theip) != 4:\n        raise ValueError('ip should be 4 tuples')\n    return '.'.join(str(int(l)).rjust(3, '0') for l in theip)", "response": "Transform the given IP address into a fixed - length form such as 192. 168. 0. 1. 1."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef network(prefix, default_length=24):\n    address, pfxlen = parse_prefix(prefix, default_length)\n    ip = ip2int(address)\n    return int2ip(ip & pfxlen2mask_int(pfxlen))", "response": "Given a prefix returns the corresponding network address."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an IP address, this function calculates the remaining available IP address under the assumption that it is a /30 network. In other words, given one link net address, this function returns the other link net address. :type local_ip: string :param local_ip: An IP address. :rtype: string :return: The other IP address of the link address pair.", "response": "def remote_ip(local_ip):\n    \"\"\"\n    Given an IP address, this function calculates the remaining available\n    IP address under the assumption that it is a /30 network.\n    In other words, given one link net address, this function returns the\n    other link net address.\n\n    :type  local_ip: string\n    :param local_ip: An IP address.\n    :rtype:  string\n    :return: The other IP address of the link address pair.\n    \"\"\"\n    local_ip = ip2int(local_ip)\n    network = local_ip & pfxlen2mask_int(30)\n    return int2ip(network + 3 - (local_ip - network))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef matches_prefix(ip, prefix):\n    ip_int = ip2int(ip)\n    network, pfxlen = parse_prefix(prefix)\n    network_int = ip2int(network)\n    mask_int = pfxlen2mask_int(pfxlen)\n    return ip_int&mask_int == network_int&mask_int", "response": "Returns True if the given IP address is part of the given network and returns False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the given IP address is private False otherwise.", "response": "def is_private(ip):\n    \"\"\"\n    Returns True if the given IP address is private,\n    returns False otherwise.\n\n    :type  ip: string\n    :param ip: An IP address.\n    :rtype:  bool\n    :return: True if the IP is private, False otherwise.\n    \"\"\"\n    if matches_prefix(ip, '10.0.0.0/8'):\n        return True\n    if matches_prefix(ip, '172.16.0.0/12'):\n        return True\n    if matches_prefix(ip, '192.168.0.0/16'):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an IP address list, this function sorts the list. :type iterable: Iterator :param iterable: An IP address list. :rtype: list :return: The sorted IP address list.", "response": "def sort(iterable):\n    \"\"\"\n    Given an IP address list, this function sorts the list.\n\n    :type  iterable: Iterator\n    :param iterable: An IP address list.\n    :rtype:  list\n    :return: The sorted IP address list.\n    \"\"\"\n    ips = sorted(normalize_ip(ip) for ip in iterable)\n    return [clean_ip(ip) for ip in ips]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_url(path):\n\n    # path changes from bytes to Unicode in going from Python 2 to\n    # Python 3.\n    if sys.version_info[0] < 3:\n        o = urlparse(urllib.parse.unquote_plus(path).decode('utf8'))\n    else:\n        o = urlparse(urllib.parse.unquote_plus(path))\n\n    path = o.path\n    args = {}\n\n    # Convert parse_qs' str --> [str] dictionary to a str --> str\n    # dictionary since we never use multi-value GET arguments\n    # anyway.\n    multiargs = parse_qs(o.query, keep_blank_values=True)\n    for arg, value in list(multiargs.items()):\n        args[arg] = value[0]\n\n    return path, args", "response": "Given a urlencoded path returns the path and the dictionary of\n    query arguments all in Unicode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_POSTGET(self, handler):\n        # at first, assume that the given path is the actual path and there are\n        # no arguments\n        self.server._dbg(self.path)\n\n        self.path, self.args = _parse_url(self.path)\n\n        # Extract POST data, if any. Clumsy syntax due to Python 2 and\n        # 2to3's lack of a byte literal.\n        self.data = u\"\".encode()\n        length = self.headers.get('Content-Length')\n        if length and length.isdigit():\n            self.data = self.rfile.read(int(length))\n\n        # POST data gets automatically decoded into Unicode. The bytestring\n        # will still be available in the bdata attribute.\n        self.bdata = self.data\n        try:\n            self.data = self.data.decode('utf8')\n        except UnicodeDecodeError:\n            self.data = None\n\n        # Run the handler.\n        try:\n            handler()\n        except:\n            self.send_response(500)\n            self.end_headers()\n            self.wfile.write(format_exc().encode('utf8'))", "response": "handle an HTTP request for GET requests"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_POST(self):\n        self.send_response(404)\n        self.end_headers()\n        self.wfile.write('not found'.encode('utf8'))", "response": "Override this method to handle a POST request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_GET(self):\n        self.send_response(404)\n        self.end_headers()\n        self.wfile.write('not found'.encode('utf8'))", "response": "Override this method to handle a GET request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the number of given bytes from the head of the buffer.", "response": "def head(self, bytes):\n        \"\"\"\n        Returns the number of given bytes from the head of the buffer.\n        The buffer remains unchanged.\n\n        :type  bytes: int\n        :param bytes: The number of bytes to return.\n        \"\"\"\n        oldpos = self.io.tell()\n        self.io.seek(0)\n        head = self.io.read(bytes)\n        self.io.seek(oldpos)\n        return head"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of given bytes from the tail of the buffer.", "response": "def tail(self, bytes):\n        \"\"\"\n        Returns the number of given bytes from the tail of the buffer.\n        The buffer remains unchanged.\n\n        :type  bytes: int\n        :param bytes: The number of bytes to return.\n        \"\"\"\n        self.io.seek(max(0, self.size() - bytes))\n        return self.io.read()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike head but also removes the tail from the buffer.", "response": "def pop(self, bytes):\n        \"\"\"\n        Like :class:`head()`, but also removes the head from the buffer.\n\n        :type  bytes: int\n        :param bytes: The number of bytes to return and remove.\n        \"\"\"\n        self.io.seek(0)\n        head = self.io.read(bytes)\n        tail = self.io.read()\n        self.io.seek(0)\n        self.io.write(tail)\n        self.io.truncate()\n        return head"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, data):\n        self.io.write(data)\n        if not self.monitors:\n            return\n\n        # Check whether any of the monitoring regular expressions matches.\n        # If it does, we need to disable that monitor until the matching\n        # data is no longer in the buffer. We accomplish this by keeping\n        # track of the position of the last matching byte.\n        buf = str(self)\n        for item in self.monitors:\n            regex_list, callback, bytepos, limit = item\n            bytepos = max(bytepos, len(buf) - limit)\n            for i, regex in enumerate(regex_list):\n                match = regex.search(buf, bytepos)\n                if match is not None:\n                    item[2] = match.end()\n                    callback(i, match)", "response": "Appends the given data to the buffer and triggers all connected\n            monitors if any of the regular expressions matches the buffer content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self):\n        self.io.seek(0)\n        self.io.truncate()\n        for item in self.monitors:\n            item[2] = 0", "response": "Clears the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a monitor to the list of monitors that match the given pattern.", "response": "def add_monitor(self, pattern, callback, limit=80):\n        \"\"\"\n        Calls the given function whenever the given pattern matches the\n        buffer.\n\n        Arguments passed to the callback are the index of the match, and\n        the match object of the regular expression.\n\n        :type  pattern: str|re.RegexObject|list(str|re.RegexObject)\n        :param pattern: One or more regular expressions.\n        :type  callback: callable\n        :param callback: The function that is called.\n        :type  limit: int\n        :param limit: The maximum size of the tail of the buffer\n                      that is searched, in number of bytes.\n        \"\"\"\n        self.monitors.append([to_regexs(pattern), callback, 0, limit])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_template_string(string, **kwargs):\n    tmpl = _render_template(string, **kwargs)\n    mail = Mail()\n    mail.set_from_template_string(tmpl)\n    return mail", "response": "Reads the given string and creates a new object containing the information from it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the template file with the given name instead of reading it.", "response": "def from_template(filename, **kwargs):\n    \"\"\"\n    Like from_template_string(), but reads the template from the file with\n    the given name instead.\n\n    :type  filename: string\n    :param filename: The name of the template file.\n    :type  kwargs: str\n    :param kwargs: Variables to replace in the template.\n    :rtype:  Mail\n    :return: The resulting mail.\n    \"\"\"\n    with open(filename) as fp:\n        return from_template_string(fp.read(), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(mail, server='localhost'):\n    sender = mail.get_sender()\n    rcpt = mail.get_receipients()\n    session = smtplib.SMTP(server)\n    message = MIMEMultipart()\n    message['Subject'] = mail.get_subject()\n    message['From'] = mail.get_sender()\n    message['To'] = ', '.join(mail.get_to())\n    message['Cc'] = ', '.join(mail.get_cc())\n    message.preamble = 'Your mail client is not MIME aware.'\n\n    body = MIMEText(mail.get_body().encode(\"utf-8\"), \"plain\", \"utf-8\")\n    body.add_header('Content-Disposition', 'inline')\n    message.attach(body)\n\n    for filename in mail.get_attachments():\n        message.attach(_get_mime_object(filename))\n\n    session.sendmail(sender, rcpt, message.as_string())", "response": "Sends the given mail."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the given string and sets all fields in the current object according to the template.", "response": "def set_from_template_string(self, string):\n        \"\"\"\n        Reads the given template (SMTP formatted) and sets all fields\n        accordingly.\n\n        :type  string: string\n        :param string: The template.\n        \"\"\"\n        in_header = True\n        body = ''\n        for line in string.split('\\n'):\n            if not in_header:\n                body += line + '\\n'\n                continue\n            if not _is_header_line(line):\n                body += line + '\\n'\n                in_header = False\n                continue\n            key, value = _get_var_from_header_line(line)\n            if key == 'from':\n                self.set_sender(value)\n            elif key == 'to':\n                self.add_to(value)\n            elif key == 'cc':\n                self.add_cc(value)\n            elif key == 'bcc':\n                self.add_bcc(value)\n            elif key == 'subject':\n                self.set_subject(value)\n            else:\n                raise Exception('Invalid header field \"%s\"' % key)\n        self.set_body(body.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_smtp_header(self):\n        header = \"From: %s\\r\\n\" % self.get_sender()\n        header += \"To: %s\\r\\n\" % ',\\r\\n '.join(self.get_to())\n        header += \"Cc: %s\\r\\n\" % ',\\r\\n '.join(self.get_cc())\n        header += \"Bcc: %s\\r\\n\" % ',\\r\\n '.join(self.get_bcc())\n        header += \"Subject: %s\\r\\n\" % self.get_subject()\n        return header", "response": "Returns the SMTP formatted header of the line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_smtp_mail(self):\n        header = self.get_smtp_header()\n        body = self.get_body().replace('\\n', '\\r\\n')\n        return header + '\\r\\n' + body + '\\r\\n'", "response": "Returns the SMTP formatted email as it may be passed to sendmail."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a one - line summary on the actions that were logged by the given Logger.", "response": "def status(logger):\n    \"\"\"\n    Creates a one-line summary on the actions that were logged by the given\n    Logger.\n\n    :type  logger: Logger\n    :param logger: The logger that recorded what happened in the queue.\n    :rtype:  string\n    :return: A string summarizing the status.\n    \"\"\"\n    aborted = logger.get_aborted_actions()\n    succeeded = logger.get_succeeded_actions()\n    total = aborted + succeeded\n    if total == 0:\n        return 'No actions done'\n    elif total == 1 and succeeded == 1:\n        return 'One action done (succeeded)'\n    elif total == 1 and succeeded == 0:\n        return 'One action done (failed)'\n    elif total == succeeded:\n        return '%d actions total (all succeeded)' % total\n    elif succeeded == 0:\n        return '%d actions total (all failed)' % total\n    else:\n        msg = '%d actions total (%d failed, %d succeeded)'\n        return msg % (total, aborted, succeeded)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef summarize(logger):\n    summary = []\n    for log in logger.get_logs():\n        thestatus = log.has_error() and log.get_error(False) or 'ok'\n        name = log.get_name()\n        summary.append(name + ': ' + thestatus)\n    return '\\n'.join(summary)", "response": "Returns a short summary of the actions that were logged by the given Logger."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(logger,\n           show_successful=True,\n           show_errors=True,\n           show_traceback=True):\n    \"\"\"\n    Prints a report of the actions that were logged by the given Logger.\n    The report contains a list of successful actions, as well as the full\n    error message on failed actions.\n\n    :type  logger: Logger\n    :param logger: The logger that recorded what happened in the queue.\n    :rtype:  string\n    :return: A string summarizing the status of every performed task.\n    \"\"\"\n    output = []\n\n    # Print failed actions.\n    errors = logger.get_aborted_actions()\n    if show_errors and errors:\n        output += _underline('Failed actions:')\n        for log in logger.get_aborted_logs():\n            if show_traceback:\n                output.append(log.get_name() + ':')\n                output.append(log.get_error())\n            else:\n                output.append(log.get_name() + ': ' + log.get_error(False))\n        output.append('')\n\n    # Print successful actions.\n    if show_successful:\n        output += _underline('Successful actions:')\n        for log in logger.get_succeeded_logs():\n            output.append(log.get_name())\n        output.append('')\n\n    return '\\n'.join(output).strip()", "response": "Formats the log messages for the given Logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_vars(self):\n        if self.parent is None:\n            vars = {}\n            vars.update(self.variables)\n            return vars\n        vars = self.parent.get_vars()\n        vars.update(self.variables)\n        return vars", "response": "Returns a complete dict of all variables that are defined in this object including the variables of the parent object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking get_vars but does not include any private variables and deep copies each variable.", "response": "def copy_public_vars(self):\n        \"\"\"\n        Like get_vars(), but does not include any private variables and\n        deep copies each variable.\n        \"\"\"\n        vars = self.get_vars()\n        vars = dict([k for k in list(vars.items()) if not k[0].startswith('_')])\n        return deepcopy(vars)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the item with the given name or None if no such item exists.", "response": "def get_from_name(self, name):\n        \"\"\"\n        Returns the item with the given name, or None if no such item\n        is known.\n        \"\"\"\n        with self.condition:\n            try:\n                item_id = self.name2id[name]\n            except KeyError:\n                return None\n            return self.id2item[item_id]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the given item to the end of the pipeline.", "response": "def append(self, item, name=None):\n        \"\"\"\n        Adds the given item to the end of the pipeline.\n        \"\"\"\n        with self.condition:\n            self.queue.append(item)\n            uuid = self._register_item(name, item)\n            self.condition.notify_all()\n            return uuid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prioritize(self, item, force=False):\n        with self.condition:\n            # If the job is already running (or about to be forced),\n            # there is nothing to be done.\n            if item in self.working or item in self.force:\n                return\n            self.queue.remove(item)\n            if force:\n                self.force.append(item)\n            else:\n                self.queue.appendleft(item)\n            self.condition.notify_all()", "response": "Moves the item to the very left of the queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping the current thread.", "response": "def stop(self):\n        \"\"\"\n        Force the next() method to return while in another thread.\n        The return value of next() will be None.\n        \"\"\"\n        with self.condition:\n            self.running = False\n            self.condition.notify_all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to get the next item in the queue.", "response": "def try_next(self):\n        \"\"\"\n        Like next(), but only returns the item that would be selected\n        right now, without locking and without changing the queue.\n        \"\"\"\n        with self.condition:\n            try:\n                return self.force[0]\n            except IndexError:\n                pass\n\n            return self._get_next(False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the size of the current terminal.", "response": "def get_terminal_size(default_rows=25, default_cols=80):\n    \"\"\"\n    Returns the number of lines and columns of the current terminal.\n    It attempts several strategies to determine the size and if all fail,\n    it returns (80, 25).\n\n    :rtype:  int, int\n    :return: The rows and columns of the terminal.\n    \"\"\"\n    # Collect a list of viable input channels that may tell us something\n    # about the terminal dimensions.\n    fileno_list = []\n    try:\n        fileno_list.append(sys.stdout.fileno())\n    except AttributeError:\n        # Channel was redirected to an object that has no fileno()\n        pass\n    except ValueError:\n        # Channel was closed while attemting to read it\n        pass\n    try:\n        fileno_list.append(sys.stdin.fileno())\n    except AttributeError:\n        pass\n    except ValueError:\n        # Channel was closed while attemting to read it\n        pass\n    try:\n        fileno_list.append(sys.stderr.fileno())\n    except AttributeError:\n        pass\n    except ValueError:\n        # Channel was closed while attemting to read it\n        pass\n\n    # Ask each channel for the terminal window size.\n    for fd in fileno_list:\n        try:\n            rows, cols = _get_terminal_size(fd)\n        except TypeError:\n            # _get_terminal_size() returned None.\n            pass\n        else:\n            return rows, cols\n\n    # Try os.ctermid()\n    try:\n        fd = os.open(os.ctermid(), os.O_RDONLY)\n    except AttributeError:\n        # os.ctermid does not exist on Windows.\n        pass\n    except OSError:\n        # The device pointed to by os.ctermid() does not exist.\n        pass\n    else:\n        try:\n            rows, cols = _get_terminal_size(fd)\n        except TypeError:\n            # _get_terminal_size() returned None.\n            pass\n        else:\n            return rows, cols\n        finally:\n            os.close(fd)\n\n    # Try `stty size`\n    with open(os.devnull, 'w') as devnull:\n        try:\n            process = Popen(['stty', 'size'], stderr=devnull, stdout=PIPE,\n                            close_fds=True)\n        except (OSError, ValueError):\n            pass\n        else:\n            errcode = process.wait()\n            output = process.stdout.read()\n            try:\n                rows, cols = output.split()\n                return int(rows), int(cols)\n            except (ValueError, TypeError):\n                pass\n\n    # Try environment variables.\n    try:\n        return tuple(int(os.getenv(var)) for var in ('LINES', 'COLUMNS'))\n    except (ValueError, TypeError):\n        pass\n\n    # Give up.\n    return default_rows, default_cols"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace all occurrences of source with dest in the given scope.", "response": "def replace(scope, strings, source, dest):\n    \"\"\"\n    Returns a copy of the given string (or list of strings) in which all\n    occurrences of the given source are replaced by the given dest.\n\n    :type  strings: string\n    :param strings: A string, or a list of strings.\n    :type  source: string\n    :param source: What to replace.\n    :type  dest: string\n    :param dest: What to replace it with.\n    :rtype:  string\n    :return: The resulting string, or list of strings.\n    \"\"\"\n    return [s.replace(source[0], dest[0]) for s in strings]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a list of user and password combinations from the given file and returns a list of Account instances.", "response": "def get_accounts_from_file(filename):\n    \"\"\"\n    Reads a list of user/password combinations from the given file\n    and returns a list of Account instances. The file content\n    has the following format::\n\n        [account-pool]\n        user1 = cGFzc3dvcmQ=\n        user2 = cGFzc3dvcmQ=\n\n    Note that \"cGFzc3dvcmQ=\" is a base64 encoded password.\n    If the input file contains extra config sections other than\n    \"account-pool\", they are ignored.\n    Each password needs to be base64 encrypted. To encrypt a password,\n    you may use the following command::\n\n        python -c 'import base64; print(base64.b64encode(\"thepassword\"))'\n\n    :type  filename: string\n    :param filename: The name of the file containing the list of accounts.\n    :rtype:  list[Account]\n    :return: The newly created account instances.\n    \"\"\"\n    accounts = []\n    cfgparser = __import__('configparser', {}, {}, [''])\n    parser = cfgparser.RawConfigParser()\n    parser.optionxform = str\n    parser.read(filename)\n    for user, password in parser.items('account-pool'):\n        password = base64.decodebytes(password.encode('latin1'))\n        accounts.append(Account(user, password.decode('latin1')))\n    return accounts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a list of hostnames from the file with the given name.", "response": "def get_hosts_from_file(filename,\n                        default_protocol='telnet',\n                        default_domain='',\n                        remove_duplicates=False,\n                        encoding='utf-8'):\n    \"\"\"\n    Reads a list of hostnames from the file with the given name.\n\n    :type  filename: string\n    :param filename: A full filename.\n    :type  default_protocol: str\n    :param default_protocol: Passed to the Host constructor.\n    :type  default_domain: str\n    :param default_domain: Appended to each hostname that has no domain.\n    :type  remove_duplicates: bool\n    :param remove_duplicates: Whether duplicates are removed.\n    :type  encoding: str\n    :param encoding: The encoding of the file.\n    :rtype:  list[Host]\n    :return: The newly created host instances.\n    \"\"\"\n    # Open the file.\n    if not os.path.exists(filename):\n        raise IOError('No such file: %s' % filename)\n\n    # Read the hostnames.\n    have = set()\n    hosts = []\n    with codecs.open(filename, 'r', encoding) as file_handle:\n        for line in file_handle:\n            hostname = line.split('#')[0].strip()\n            if hostname == '':\n                continue\n            if remove_duplicates and hostname in have:\n                continue\n            have.add(hostname)\n            hosts.append(to_host(hostname, default_protocol, default_domain))\n\n    return hosts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a list of hosts from a tab - separated. csv file.", "response": "def get_hosts_from_csv(filename,\n                       default_protocol='telnet',\n                       default_domain='',\n                       encoding='utf-8'):\n    \"\"\"\n    Reads a list of hostnames and variables from the tab-separated .csv file\n    with the given name. The first line of the file must contain the column\n    names, e.g.::\n\n        address\ttestvar1\ttestvar2\n        10.0.0.1\tvalue1\tothervalue\n        10.0.0.1\tvalue2\tothervalue2\n        10.0.0.2\tfoo\tbar\n\n    For the above example, the function returns *two* host objects, where\n    the 'testvar1' variable of the first host holds a list containing two\n    entries ('value1' and 'value2'), and the 'testvar1' variable of the\n    second host contains a list with a single entry ('foo').\n\n    Both, the address and the hostname of each host are set to the address\n    given in the first column. If you want the hostname set to another value,\n    you may add a second column containing the hostname::\n\n        address\thostname\ttestvar\n        10.0.0.1\tmyhost\tvalue\n        10.0.0.2\totherhost\tothervalue\n\n    :type  filename: string\n    :param filename: A full filename.\n    :type  default_protocol: str\n    :param default_protocol: Passed to the Host constructor.\n    :type  default_domain: str\n    :param default_domain: Appended to each hostname that has no domain.\n    :type  encoding: str\n    :param encoding: The encoding of the file.\n    :rtype:  list[Host]\n    :return: The newly created host instances.\n    \"\"\"\n    # Open the file.\n    if not os.path.exists(filename):\n        raise IOError('No such file: %s' % filename)\n\n    with codecs.open(filename, 'r', encoding) as file_handle:\n        # Read and check the header.\n        header = file_handle.readline().rstrip()\n        if re.search(r'^(?:hostname|address)\\b', header) is None:\n            msg = 'Syntax error in CSV file header:'\n            msg += ' File does not start with \"hostname\" or \"address\".'\n            raise Exception(msg)\n        if re.search(r'^(?:hostname|address)(?:\\t[^\\t]+)*$', header) is None:\n            msg = 'Syntax error in CSV file header:'\n            msg += ' Make sure to separate columns by tabs.'\n            raise Exception(msg)\n        varnames = [str(v) for v in header.split('\\t')]\n        varnames.pop(0)\n\n        # Walk through all lines and create a map that maps hostname to\n        # definitions.\n        last_uri = ''\n        line_re = re.compile(r'[\\r\\n]*$')\n        hosts = []\n        for line in file_handle:\n            if line.strip() == '':\n                continue\n\n            line = line_re.sub('', line)\n            values = line.split('\\t')\n            uri = values.pop(0).strip()\n\n            # Add the hostname to our list.\n            if uri != last_uri:\n                # print \"Reading hostname\", hostname_url, \"from csv.\"\n                host = to_host(uri, default_protocol, default_domain)\n                last_uri = uri\n                hosts.append(host)\n\n            # Define variables according to the definition.\n            for i, varname in enumerate(varnames):\n                try:\n                    value = values[i]\n                except IndexError:\n                    value = ''\n                if varname == 'hostname':\n                    host.set_name(value)\n                else:\n                    host.append(varname, value)\n\n    return hosts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a Python file containing functions and returns the content of the __lib__ variable.", "response": "def load_lib(filename):\n    \"\"\"\n    Loads a Python file containing functions, and returns the\n    content of the __lib__ variable. The __lib__ variable must contain\n    a dictionary mapping function names to callables.\n\n    Returns a dictionary mapping the namespaced function names to\n    callables. The namespace is the basename of the file, without file\n    extension.\n\n    The result of this function can later be passed to run_template::\n\n        functions = load_lib('my_library.py')\n        run_template(conn, 'foo.exscript', **functions)\n\n    :type  filename: string\n    :param filename: A full filename.\n    :rtype:  dict[string->object]\n    :return: The loaded functions.\n    \"\"\"\n    # Open the file.\n    if not os.path.exists(filename):\n        raise IOError('No such file: %s' % filename)\n\n    name = os.path.splitext(os.path.basename(filename))[0]\n    if sys.version_info[0] < 3:\n        module = imp.load_source(name, filename)\n    else:\n        module = importlib.machinery.SourceFileLoader(name, filename).load_module()\n\n    return dict((name + '.' + k, v) for (k, v) in list(module.__lib__.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a function that has a connection passed such that everything that happens on the connection is logged using the given logger. :type logger: Logger :param logger: The logger that handles the logging.", "response": "def log_to(logger):\n    \"\"\"\n    Wraps a function that has a connection passed such that everything that\n    happens on the connection is logged using the given logger.\n\n    :type  logger: Logger\n    :param logger: The logger that handles the logging.\n    \"\"\"\n    logger_id = id(logger)\n\n    def decorator(function):\n        func = add_label(function, 'log_to', logger_id=logger_id)\n        return func\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging a file to a logdir.", "response": "def log_to_file(logdir, mode='a', delete=False, clearmem=True):\n    \"\"\"\n    Like :class:`log_to()`, but automatically creates a new FileLogger\n    instead of having one passed.\n    Note that the logger stays alive (in memory) forever. If you need\n    to control the lifetime of a logger, use :class:`log_to()` instead.\n    \"\"\"\n    logger = FileLogger(logdir, mode, delete, clearmem)\n    _loggers.append(logger)\n    return log_to(logger)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a new instance of the Telnet class.", "response": "def open(self, host, port=0):\n        \"\"\"Connect to a host.\n\n        The optional second argument is the port number, which\n        defaults to the standard telnet port (23).\n\n        Don't try to reopen an already connected instance.\n\n        \"\"\"\n        self.eof = 0\n        if not port:\n            port = TELNET_PORT\n        self.host = host\n        self.port = port\n        msg = \"getaddrinfo returns an empty list\"\n        for res in socket.getaddrinfo(host, port, socket.AF_INET, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                self.sock = socket.socket(af, socktype, proto)\n                self.sock.settimeout(self.connect_timeout)\n                self.sock.connect(sa)\n            except socket.error as msg_:\n                msg = '{} => telnet://{}:{}'.format(msg_, host, port)\n                if self.sock:\n                    self.sock.close()\n                self.sock = None\n                continue\n            break\n        if not self.sock:\n            raise socket.error(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef msg(self, msg, *args):\n        if self.debuglevel > 0:\n            self.stderr.write('Telnet(%s,%d): ' % (self.host, self.port))\n            if args:\n                self.stderr.write(msg % args)\n            else:\n                self.stderr.write(msg)\n            self.stderr.write('\\n')", "response": "Print a debug message to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, buffer):\n        if type(buffer) == type(0):\n            buffer = chr(buffer)\n        elif not isinstance(buffer, bytes):\n            buffer = buffer.encode(self.encoding)\n        if IAC in buffer:\n            buffer = buffer.replace(IAC, IAC+IAC)\n        self.msg(\"send %s\", repr(buffer))\n        self.sock.send(buffer)", "response": "Write a string to the socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_all(self):\n        self.process_rawq()\n        while not self.eof:\n            self.fill_rawq()\n            self.process_rawq()\n        buf = self.cookedq.getvalue()\n        self.cookedq.seek(0)\n        self.cookedq.truncate()\n        return buf", "response": "Read all data until EOF ; block until connection closed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_some(self):\n        self.process_rawq()\n        while self.cookedq.tell() == 0 and not self.eof:\n            self.fill_rawq()\n            self.process_rawq()\n        buf = self.cookedq.getvalue()\n        self.cookedq.seek(0)\n        self.cookedq.truncate()\n        return buf", "response": "Read at least one byte of cooked data until EOF is hit. Return an empty string if EOF is hit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_very_eager(self):\n        self.process_rawq()\n        while not self.eof and self.sock_avail():\n            self.fill_rawq()\n            self.process_rawq()\n        return self.read_very_lazy()", "response": "Read everything that s possible without blocking in I/O."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_eager(self):\n        self.process_rawq()\n        while self.cookedq.tell() == 0 and not self.eof and self.sock_avail():\n            self.fill_rawq()\n            self.process_rawq()\n        return self.read_very_lazy()", "response": "Read readily available data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads any data from the cooked queue.", "response": "def read_very_lazy(self):\n        \"\"\"Return any data available in the cooked queue (very lazy).\n\n        Raise EOFError if connection closed and no data available.\n        Return '' if no cooked data available otherwise.  Don't block.\n\n        \"\"\"\n        buf = self.cookedq.getvalue()\n        self.cookedq.seek(0)\n        self.cookedq.truncate()\n        if not buf and self.eof and not self.rawq:\n            raise EOFError('telnet connection closed')\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_receive_callback(self, callback, *args, **kwargs):\n        self.data_callback = callback\n        self.data_callback_kwargs = kwargs", "response": "The callback function called after each receipt of any data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the size of the terminal window.", "response": "def set_window_size(self, rows, cols):\n        \"\"\"\n        Change the size of the terminal window, if the remote end supports\n        NAWS. If it doesn't, the method returns silently.\n        \"\"\"\n        if not self.can_naws:\n            return\n        self.window_size = rows, cols\n        size = struct.pack('!HH', cols, rows)\n        self.sock.send(IAC + SB + NAWS + size + IAC + SE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the raw queue.", "response": "def process_rawq(self):\n        \"\"\"Transfer from raw queue to cooked queue.\n\n        Set self.eof when connection is closed.  Don't block unless in\n        the midst of an IAC sequence.\n        \"\"\"\n        buf = b''\n        try:\n            while self.rawq:\n                # Handle non-IAC first (normal data).\n                char = self.rawq_getchar()\n                if char != IAC:\n                    buf = buf + char\n                    continue\n\n                # Interpret the command byte that follows after the IAC code.\n                command = self.rawq_getchar()\n                if command == theNULL:\n                    self.msg('IAC NOP')\n                    continue\n                elif command == IAC:\n                    self.msg('IAC DATA')\n                    buf = buf + command\n                    continue\n\n                # DO: Indicates the request that the other party perform,\n                # or confirmation that you are expecting the other party\n                # to perform, the indicated option.\n                elif command == DO:\n                    opt = self.rawq_getchar()\n                    self.msg('IAC DO %s', ord(opt))\n                    if opt == TTYPE:\n                        self.sock.send(IAC+WILL+opt)\n                    elif opt == NAWS:\n                        self.sock.send(IAC+WILL+opt)\n                        self.can_naws = True\n                        if self.window_size:\n                            self.set_window_size(*self.window_size)\n                    else:\n                        self.sock.send(IAC+WONT+opt)\n\n                # DON'T: Indicates the demand that the other party stop\n                # performing, or confirmation that you are no longer\n                # expecting the other party to perform, the indicated\n                # option.\n                elif command == DONT:\n                    opt = self.rawq_getchar()\n                    self.msg('IAC DONT %s', ord(opt))\n                    self.sock.send(IAC+WONT+opt)\n\n                # SB: Indicates that what follows is subnegotiation of the\n                # indicated option.\n                elif command == SB:\n                    opt = self.rawq_getchar()\n                    self.msg('IAC SUBCOMMAND %d', ord(opt))\n\n                    # We only handle the TTYPE command, so skip all other\n                    # commands.\n                    if opt != TTYPE:\n                        while self.rawq_getchar() != SE:\n                            pass\n                        continue\n\n                    # We also only handle the SEND_TTYPE option of TTYPE,\n                    # so skip everything else.\n                    subopt = self.rawq_getchar()\n                    if subopt != SEND_TTYPE:\n                        while self.rawq_getchar() != SE:\n                            pass\n                        continue\n\n                    # Mandatory end of the IAC subcommand.\n                    iac = self.rawq_getchar()\n                    end = self.rawq_getchar()\n                    if (iac, end) != (IAC, SE):\n                        # whoops, that's an unexpected response...\n                        self.msg(\n                            'expected IAC SE, but got %d %d', ord(iac), ord(end))\n                    self.msg('IAC SUBCOMMAND_END')\n\n                    # Send the next supported terminal.\n                    ttype = self.termtype.encode('latin1')\n                    self.msg('indicating support for terminal type %s', ttype)\n                    self.sock.send(IAC+SB+TTYPE+theNULL+ttype+IAC+SE)\n                elif command in (WILL, WONT):\n                    opt = self.rawq_getchar()\n                    self.msg('IAC %s %d',\n                             command == WILL and 'WILL' or 'WONT', ord(opt))\n                    if opt == ECHO:\n                        self.sock.send(IAC+DO+opt)\n                    else:\n                        self.sock.send(IAC+DONT+opt)\n                else:\n                    self.msg('IAC %d not recognized' % ord(command))\n        except EOFError:  # raised by self.rawq_getchar()\n            pass\n        buf = buf.decode(self.encoding)\n        self.cookedq.write(buf)\n        if self.data_callback is not None:\n            self.data_callback(buf, **self.data_callback_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling the raw queue with exactly one recv() system call.", "response": "def fill_rawq(self):\n        \"\"\"Fill raw queue from exactly one recv() system call.\n\n        Block if no data is immediately available.  Set self.eof when\n        connection is closed.\n\n        \"\"\"\n        if self.irawq >= len(self.rawq):\n            self.rawq = b''\n            self.irawq = 0\n        # The buffer size should be fairly small so as to avoid quadratic\n        # behavior in process_rawq() above.\n        buf = self.sock.recv(64)\n        self.msg(\"recv %s\", repr(buf))\n        self.eof = (not buf)\n        self.rawq = self.rawq + buf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interact(self):\n        if sys.platform == \"win32\":\n            self.mt_interact()\n            return\n        while True:\n            rfd, wfd, xfd = select.select([self, sys.stdin], [], [])\n            if self in rfd:\n                try:\n                    text = self.read_eager()\n                except EOFError:\n                    print('*** Connection closed by remote host ***')\n                    break\n                if text:\n                    self.stdout.write(text)\n                    self.stdout.flush()\n            if sys.stdin in rfd:\n                line = sys.stdin.readline()\n                if not line:\n                    break\n                self.write(line)", "response": "Interact function emulates a very dumb telnet client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef waitfor(self, relist, timeout=None, cleanup=None):\n        return self._waitfor(relist, timeout, False, cleanup)", "response": "Read until one of the regular expressions matches the input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expect(self, relist, timeout=None, cleanup=None):\n        return self._waitfor(relist, timeout, True, cleanup=cleanup)", "response": "Like waitfor but removes the matched data from the incoming\n        buffer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(path):\n    # Try to read the pid from the pidfile.\n    logging.info(\"Checking pidfile '%s'\", path)\n    try:\n        return int(open(path).read())\n    except IOError as xxx_todo_changeme:\n        (code, text) = xxx_todo_changeme.args\n        if code == errno.ENOENT:  # no such file or directory\n            return None\n        raise", "response": "Reads the process id from the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isalive(path):\n    # try to read the pid from the pidfile\n    pid = read(path)\n    if pid is None:\n        return False\n\n    # Check if a process with the given pid exists.\n    try:\n        os.kill(pid, 0)  # Signal 0 does not kill, but check.\n    except OSError as xxx_todo_changeme1:\n        (code, text) = xxx_todo_changeme1.args\n        if code == errno.ESRCH:  # No such process.\n            return False\n    return True", "response": "Checks if a process with the given name is still alive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkilling the process if it still exists.", "response": "def kill(path):\n    \"\"\"\n    Kills the process, if it still exists.\n\n    :type  path: str\n    :param path: The name of the pidfile.\n    \"\"\"\n    # try to read the pid from the pidfile\n    pid = read(path)\n    if pid is None:\n        return\n\n    # Try to kill the process.\n    logging.info(\"Killing PID %s\", pid)\n    try:\n        os.kill(pid, 9)\n    except OSError as xxx_todo_changeme2:\n        # re-raise if the error wasn't \"No such process\"\n        (code, text) = xxx_todo_changeme2.args\n        # re-raise if the error wasn't \"No such process\"\n        if code != errno.ESRCH:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(path):\n    pid = os.getpid()\n    logging.info(\"Writing PID %s to '%s'\", pid, path)\n    try:\n        pidfile = open(path, 'wb')\n        # get a non-blocking exclusive lock\n        fcntl.flock(pidfile.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n        # clear out the file\n        pidfile.seek(0)\n        pidfile.truncate(0)\n        # write the pid\n        pidfile.write(str(pid))\n    finally:\n        try:\n            pidfile.close()\n        except:\n            pass", "response": "Writes the current process id to the given pidfile."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the manual driver that is used to recognize prompts and implement behavior depending on the remote system.", "response": "def set_driver(self, driver=None):\n        \"\"\"\n        Defines the driver that is used to recognize prompts and implement\n        behavior depending on the remote system.\n        The driver argument may be an instance of a protocols.drivers.Driver\n        subclass, a known driver name (string), or None.\n        If the driver argument is None, the adapter automatically chooses\n        a driver using the guess_os() function.\n\n        :type  driver: Driver()|str\n        :param driver: The pattern that, when matched, causes an error.\n        \"\"\"\n        if driver is None:\n            self.manual_driver = None\n        elif isinstance(driver, str):\n            if driver not in driver_map:\n                raise TypeError('no such driver:' + repr(driver))\n            self.manual_driver = driver_map[driver]\n        elif isinstance(driver, Driver):\n            self.manual_driver = driver\n        else:\n            raise TypeError('unsupported argument type:' + type(driver))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_username_prompt(self, regex=None):\n        if regex is None:\n            self.manual_user_re = regex\n        else:\n            self.manual_user_re = to_regexs(regex)", "response": "Sets the manual user prompt pattern that is used to monitor the response of the user connected host for a username prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the manual password prompt pattern that is used to monitor the response of the available user s password.", "response": "def set_password_prompt(self, regex=None):\n        \"\"\"\n        Defines a pattern that is used to monitor the response of the\n        connected host for a password prompt.\n\n        :type  regex: RegEx\n        :param regex: The pattern that, when matched, causes an error.\n        \"\"\"\n        if regex is None:\n            self.manual_password_re = regex\n        else:\n            self.manual_password_re = to_regexs(regex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_prompt(self, prompt=None):\n        if prompt is None:\n            self.manual_prompt_re = prompt\n        else:\n            self.manual_prompt_re = to_regexs(prompt)", "response": "Sets the manual prompt pattern that is used when calling expect_prompt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_error_prompt(self, error=None):\n        if error is None:\n            self.manual_error_re = error\n        else:\n            self.manual_error_re = to_regexs(error)", "response": "Sets the manual error pattern that is used to monitor the response of the available orange items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_login_error_prompt(self, error=None):\n        if error is None:\n            self.manual_login_error_re = error\n        else:\n            self.manual_login_error_re = to_regexs(error)", "response": "Sets manual login error prompt for the log - in process."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the remote host or IP address.", "response": "def connect(self, hostname=None, port=None):\n        \"\"\"\n        Opens the connection to the remote host or IP address.\n\n        :type  hostname: string\n        :param hostname: The remote host or IP address.\n        :type  port: int\n        :param port: The remote TCP port number.\n        \"\"\"\n        if hostname is not None:\n            self.host = hostname\n        conn = self._connect_hook(self.host, port)\n        self.os_guesser.protocol_info(self.get_remote_version())\n        self.auto_driver = driver_map[self.guess_os()]\n        if self.get_banner():\n            self.os_guesser.data_received(self.get_banner(), False)\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging into the connected host using the best method available.", "response": "def login(self, account=None, app_account=None, flush=True):\n        \"\"\"\n        Log into the connected host using the best method available.\n        If an account is not given, default to the account that was\n        used during the last call to login(). If a previous call was not\n        made, use the account that was passed to the constructor. If that\n        also fails, raise a TypeError.\n\n        The app_account is passed to :class:`app_authenticate()` and\n        :class:`app_authorize()`.\n        If app_account is not given, default to the value of the account\n        argument.\n\n        :type  account: Account\n        :param account: The account for protocol level authentication.\n        :type  app_account: Account\n        :param app_account: The account for app level authentication.\n        :type  flush: bool\n        :param flush: Whether to flush the last prompt from the buffer.\n        \"\"\"\n        with self._get_account(account) as account:\n            if app_account is None:\n                app_account = account\n            self.authenticate(account, flush=False)\n            if self.get_driver().supports_auto_authorize():\n                self.expect_prompt()\n            self.auto_app_authorize(app_account, flush=flush)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticate(self, account=None, app_account=None, flush=True):\n        with self._get_account(account) as account:\n            if app_account is None:\n                app_account = account\n\n            if not self.proto_authenticated:\n                self.protocol_authenticate(account)\n            self.app_authenticate(app_account, flush=flush)", "response": "Authenticate the protocol and app level account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef protocol_authenticate(self, account=None):\n        with self._get_account(account) as account:\n            user = account.get_name()\n            password = account.get_password()\n            key = account.get_key()\n            if key is None:\n                self._dbg(1, \"Attempting to authenticate %s.\" % user)\n                self._protocol_authenticate(user, password)\n            else:\n                self._dbg(1, \"Authenticate %s with key.\" % user)\n                self._protocol_authenticate_by_key(user, key)\n        self.proto_authenticated = True", "response": "Performs protocol - level authentication on the specified account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattempt to perform application - level authentication on the remote device.", "response": "def app_authenticate(self, account=None, flush=True, bailout=False):\n        \"\"\"\n        Attempt to perform application-level authentication. Application\n        level authentication is needed on devices where the username and\n        password are requested from the user after the connection was\n        already accepted by the remote device.\n\n        The difference between app-level authentication and protocol-level\n        authentication is that in the latter case, the prompting is handled\n        by the client, whereas app-level authentication is handled by the\n        remote device.\n\n        App-level authentication comes in a large variety of forms, and\n        while this method tries hard to support them all, there is no\n        guarantee that it will always work.\n\n        We attempt to smartly recognize the user and password prompts;\n        for a list of supported operating systems please check the\n        Exscript.protocols.drivers module.\n\n        Returns upon finding the first command line prompt. Depending\n        on whether the flush argument is True, it also removes the\n        prompt from the incoming buffer.\n\n        :type  account: Account\n        :param account: An account object, like login().\n        :type  flush: bool\n        :param flush: Whether to flush the last prompt from the buffer.\n        :type  bailout: bool\n        :param bailout: Whether to wait for a prompt after sending the password.\n        \"\"\"\n        with self._get_account(account) as account:\n            user = account.get_name()\n            password = account.get_password()\n            self._dbg(1, \"Attempting to app-authenticate %s.\" % user)\n            self._app_authenticate(account, password, flush, bailout)\n        self.app_authenticated = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef app_authorize(self, account=None, flush=True, bailout=False):\n        with self._get_account(account) as account:\n            user = account.get_name()\n            password = account.get_authorization_password()\n            if password is None:\n                password = account.get_password()\n            self._dbg(1, \"Attempting to app-authorize %s.\" % user)\n            self._app_authenticate(account, password, flush, bailout)\n        self.app_authorized = True", "response": "This is a convenience method that uses the authorization password of the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auto_app_authorize(self, account=None, flush=True, bailout=False):\n        with self._get_account(account) as account:\n            self._dbg(1, 'Calling driver.auto_authorize().')\n            self.get_driver().auto_authorize(self, account, flush, bailout)", "response": "This method is used to automatically initiate an application authorization procedure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, command, consume=True):\n        self.send(command + '\\r')\n        return self.expect_prompt(consume)", "response": "Sends the given command to the remote host and waits for a prompt."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmonitoring the data received from the remote host and waits until the response matches the given prompt.", "response": "def waitfor(self, prompt):\n        \"\"\"\n        Monitors the data received from the remote host and waits until\n        the response matches the given prompt.\n        Once a match has been found, the buffer containing incoming data\n        is NOT changed. In other words, consecutive calls to this function\n        will always work, e.g.::\n\n            conn.waitfor('myprompt>')\n            conn.waitfor('myprompt>')\n            conn.waitfor('myprompt>')\n\n        will always work. Hence in most cases, you probably want to use\n        expect() instead.\n\n        This method also stores the received data in the response\n        attribute (self.response).\n\n        Returns the index of the regular expression that matched.\n\n        :type  prompt: str|re.RegexObject|list(str|re.RegexObject)\n        :param prompt: One or more regular expressions.\n        :rtype:  int, re.MatchObject\n        :return: The index of the regular expression that matched,\n          and the match object.\n\n        @raise TimeoutException: raised if the timeout was reached.\n\n        @raise ExpectCancelledException: raised when cancel_expect() was\n        called in a callback.\n\n        @raise ProtocolException: on other internal errors.\n\n        @raise Exception: May raise other exceptions that are caused\n        within the underlying protocol implementations.\n        \"\"\"\n        while True:\n            try:\n                result = self._waitfor(prompt)\n            except DriverReplacedException:\n                continue  # retry\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting for a given pattern in the buffer.", "response": "def expect(self, prompt):\n        \"\"\"\n        Like waitfor(), but also removes the matched string from the buffer\n        containing the incoming data. In other words, the following may not\n        alway complete::\n\n            conn.expect('myprompt>')\n            conn.expect('myprompt>') # timeout\n\n        Returns the index of the regular expression that matched.\n\n        .. HINT::\n            May raise the same exceptions as :class:`waitfor`.\n\n        :type  prompt: str|re.RegexObject|list(str|re.RegexObject)\n        :param prompt: One or more regular expressions.\n        :rtype:  int, re.MatchObject\n        :return: The index of the regular expression that matched,\n          and the match object.\n        \"\"\"\n        while True:\n            try:\n                result = self._expect(prompt)\n            except DriverReplacedException:\n                continue  # retry\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expect_prompt(self, consume=True):\n        if consume:\n            result = self.expect(self.get_prompt())\n        else:\n            self._dbg(1, \"DO NOT CONSUME PROMPT!\")\n            result = self.waitfor(self.get_prompt())\n\n        # We skip the first line because it contains the echo of the command\n        # sent.\n        self._dbg(5, \"Checking %s for errors\" % repr(self.response))\n        for line in self.response.split('\\n')[1:]:\n            for prompt in self.get_error_prompt():\n                if not prompt.search(line):\n                    continue\n                args = repr(prompt.pattern), repr(line)\n                self._dbg(5, \"error prompt (%s) matches %s\" % args)\n                raise InvalidCommandException('Device said:\\n' + self.response)\n\n        return result", "response": "Monitors the data received from the remote host and waits for a prompt to be received."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a monitor to the buffer that matches the given pattern.", "response": "def add_monitor(self, pattern, callback, limit=80):\n        \"\"\"\n        Calls the given function whenever the given pattern matches the\n        incoming data.\n\n        .. HINT::\n            If you want to catch all incoming data regardless of a\n            pattern, use the Protocol.data_received_event event instead.\n\n        Arguments passed to the callback are the protocol instance, the\n        index of the match, and the match object of the regular expression.\n\n        :type  pattern: str|re.RegexObject|list(str|re.RegexObject)\n        :param pattern: One or more regular expressions.\n        :type  callback: callable\n        :param callback: The function that is called.\n        :type  limit: int\n        :param limit: The maximum size of the tail of the buffer\n                      that is searched, in number of bytes.\n        \"\"\"\n        self.buffer.add_monitor(pattern, partial(callback, self), limit)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_connection(func):\n    def _wrapped(job, *args, **kwargs):\n        job_id = id(job)\n        to_parent = job.data['pipe']\n        host = job.data['host']\n\n        # Create a protocol adapter.\n        mkaccount = partial(_account_factory, to_parent, host)\n        pargs = {'account_factory': mkaccount,\n                 'stdout':          job.data['stdout']}\n        pargs.update(host.get_options())\n        conn = prepare(host, **pargs)\n\n        # Connect and run the function.\n        log_options = get_label(func, 'log_to')\n        if log_options is not None:\n            # Enable logging.\n            proxy = LoggerProxy(to_parent, log_options['logger_id'])\n            log_cb = partial(proxy.log, job_id)\n            proxy.add_log(job_id, job.name, job.failures + 1)\n            conn.data_received_event.listen(log_cb)\n            try:\n                conn.connect(host.get_address(), host.get_tcp_port())\n                result = func(job, host, conn, *args, **kwargs)\n                conn.close(force=True)\n            except:\n                proxy.log_aborted(job_id, serializeable_sys_exc_info())\n                raise\n            else:\n                proxy.log_succeeded(job_id)\n            finally:\n                conn.data_received_event.disconnect(log_cb)\n        else:\n            conn.connect(host.get_address(), host.get_tcp_port())\n            result = func(job, host, conn, *args, **kwargs)\n            conn.close(force=True)\n        return result\n\n    return _wrapped", "response": "A decorator that creates a protocol adapter and connects and runs the function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a pipe and returns the child end of the connection.", "response": "def _create_pipe(self):\n        \"\"\"\n        Creates a new pipe and returns the child end of the connection.\n        To request an account from the pipe, use::\n\n            pipe = queue._create_pipe()\n\n            # Let the account manager choose an account.\n            pipe.send(('acquire-account-for-host', host))\n            account = pipe.recv()\n            ...\n            pipe.send(('release-account', account.id()))\n\n            # Or acquire a specific account.\n            pipe.send(('acquire-account', account.id()))\n            account = pipe.recv()\n            ...\n            pipe.send(('release-account', account.id()))\n\n            pipe.close()\n        \"\"\"\n        child = _PipeHandler(self.account_manager)\n        self.pipe_handlers[id(child)] = child\n        child.start()\n        return child.to_parent"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting until all jobs are completed.", "response": "def join(self):\n        \"\"\"\n        Waits until all jobs are completed.\n        \"\"\"\n        self._dbg(2, 'Waiting for the queue to finish.')\n        self.workqueue.wait_until_done()\n        for child in list(self.pipe_handlers.values()):\n            child.join()\n        self._del_status_bar()\n        self._print_status_bar()\n        gc.collect()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshut down the queue.", "response": "def shutdown(self, force=False):\n        \"\"\"\n        Stop executing any further jobs. If the force argument is True,\n        the function does not wait until any queued jobs are completed but\n        stops immediately.\n\n        After emptying the queue it is restarted, so you may still call run()\n        after using this method.\n\n        :type  force: bool\n        :param force: Whether to wait until all jobs were processed.\n        \"\"\"\n        if not force:\n            self.join()\n\n        self._dbg(2, 'Shutting down queue...')\n        self.workqueue.shutdown(True)\n        self._dbg(2, 'Queue shut down.')\n        self._del_status_bar()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef destroy(self, force=False):\n        try:\n            if not force:\n                self.join()\n        finally:\n            self._dbg(2, 'Destroying queue...')\n            self.workqueue.destroy()\n            self.account_manager.reset()\n            self.completed = 0\n            self.total = 0\n            self.failed = 0\n            self.status_bar_length = 0\n            self._dbg(2, 'Queue destroyed.')\n            self._del_status_bar()", "response": "Like shutdown(), but also removes all accounts, hosts, etc., and\n        does not restart the queue. In other words, the queue can no longer\n        be used after calling this method.\n\n        :type  force: bool\n        :param force: Whether to wait until all jobs were processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self):\n        self._dbg(2, 'Resetting queue...')\n        self.account_manager.reset()\n        self.workqueue.shutdown(True)\n        self.completed = 0\n        self.total = 0\n        self.failed = 0\n        self.status_bar_length = 0\n        self._dbg(2, 'Queue reset.')\n        self._del_status_bar()", "response": "Reset the queue to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a function to a queue and call it once for each host.", "response": "def run(self, hosts, function, attempts=1):\n        \"\"\"\n        Add the given function to a queue, and call it once for each host\n        according to the threading options.\n        Use decorators.bind() if you also want to pass additional\n        arguments to the callback function.\n\n        Returns an object that represents the queued task, and that may be\n        passed to is_completed() to check the status.\n\n        :type  hosts: string|list(string)|Host|list(Host)\n        :param hosts: A hostname or Host object, or a list of them.\n        :type  function: function\n        :param function: The function to execute.\n        :type  attempts: int\n        :param attempts: The number of attempts on failure.\n        :rtype:  object\n        :return: An object representing the task.\n        \"\"\"\n        return self._run(hosts, function, self.workqueue.enqueue, attempts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike run but only appends hosts that are not already in the the queue.", "response": "def run_or_ignore(self, hosts, function, attempts=1):\n        \"\"\"\n        Like run(), but only appends hosts that are not already in the\n        queue.\n\n        :type  hosts: string|list(string)|Host|list(Host)\n        :param hosts: A hostname or Host object, or a list of them.\n        :type  function: function\n        :param function: The function to execute.\n        :type  attempts: int\n        :param attempts: The number of attempts on failure.\n        :rtype:  object\n        :return: A task object, or None if all hosts were duplicates.\n        \"\"\"\n        return self._run(hosts,\n                         function,\n                         self.workqueue.enqueue_or_ignore,\n                         attempts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike run but adds the task to the front of the queue.", "response": "def priority_run(self, hosts, function, attempts=1):\n        \"\"\"\n        Like run(), but adds the task to the front of the queue.\n\n        :type  hosts: string|list(string)|Host|list(Host)\n        :param hosts: A hostname or Host object, or a list of them.\n        :type  function: function\n        :param function: The function to execute.\n        :type  attempts: int\n        :param attempts: The number of attempts on failure.\n        :rtype:  object\n        :return: An object representing the task.\n        \"\"\"\n        return self._run(hosts,\n                         function,\n                         self.workqueue.priority_enqueue,\n                         False,\n                         attempts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef priority_run_or_raise(self, hosts, function, attempts=1):\n        return self._run(hosts,\n                         function,\n                         self.workqueue.priority_enqueue_or_raise,\n                         False,\n                         attempts)", "response": "A priority run method that enqueues a new entry in the queue and runs the function on the existing entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike priority_run but starts the task immediately even if max_threads is exceeded.", "response": "def force_run(self, hosts, function, attempts=1):\n        \"\"\"\n        Like priority_run(), but starts the task immediately even if that\n        max_threads is exceeded.\n\n        :type  hosts: string|list(string)|Host|list(Host)\n        :param hosts: A hostname or Host object, or a list of them.\n        :type  function: function\n        :param function: The function to execute.\n        :type  attempts: int\n        :param attempts: The number of attempts on failure.\n        :rtype:  object\n        :return: An object representing the task.\n        \"\"\"\n        return self._run(hosts,\n                         function,\n                         self.workqueue.priority_enqueue,\n                         True,\n                         attempts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enqueue(self, function, name=None, attempts=1):\n        self.total += 1\n        task = Task(self.workqueue)\n        job_id = self.workqueue.enqueue(function, name, attempts)\n        if job_id is not None:\n            task.add_job_id(job_id)\n        self._dbg(2, 'Function enqueued.')\n        return task", "response": "Enqueue a function in the queue and returns a Task object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of the list item with the given index.", "response": "def get(scope, source, index):\n    \"\"\"\n    Returns a copy of the list item with the given index.\n    It is an error if an item with teh given index does not exist.\n\n    :type  source: string\n    :param source: A list of strings.\n    :type  index: string\n    :param index: A list of strings.\n    :rtype:  string\n    :return: The cleaned up list of strings.\n    \"\"\"\n    try:\n        index = int(index[0])\n    except IndexError:\n        raise ValueError('index variable is required')\n    except ValueError:\n        raise ValueError('index is not an integer')\n    try:\n        return [source[index]]\n    except IndexError:\n        raise ValueError('no such item in the list')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(scope, command):\n    process = Popen(command[0],\n                    shell=True,\n                    stdin=PIPE,\n                    stdout=PIPE,\n                    stderr=STDOUT,\n                    close_fds=True)\n    scope.define(__response__=process.stdout.read())\n    return True", "response": "Executes the given command locally."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, key, value, confidence=100):\n        if value is None:\n            return\n        if key in self.info:\n            old_confidence, old_value = self.info.get(key)\n            if old_confidence >= confidence:\n                return\n        self.info[key] = (confidence, value)", "response": "Sets the value of the given key to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_from_match(self, key, regex_list, string):\n        for item in regex_list:\n            if hasattr(item, '__call__'):\n                self.set(key, *item(string))\n            else:\n                regex, value, confidence = item\n                if regex.search(string):\n                    self.set(key, value, confidence)", "response": "This function sets the value of the given key from the given regex_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, key, confidence=0):\n        if key not in self.info:\n            return None\n        conf, value = self.info.get(key)\n        if conf >= confidence:\n            return value\n        return None", "response": "Returns the info with the given key if it exists and has at least the given confidence. Returns None if the key does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps the given function such that when it is called, the given arguments are passed in addition to the connection argument. :type function: function :param function: The function that's ought to be wrapped. :type args: list :param args: Passed on to the called function. :type kwargs: dict :param kwargs: Passed on to the called function. :rtype: function :return: The wrapped function.", "response": "def bind(function, *args, **kwargs):\n    \"\"\"\n    Wraps the given function such that when it is called, the given arguments\n    are passed in addition to the connection argument.\n\n    :type  function: function\n    :param function: The function that's ought to be wrapped.\n    :type  args: list\n    :param args: Passed on to the called function.\n    :type  kwargs: dict\n    :param kwargs: Passed on to the called function.\n    :rtype:  function\n    :return: The wrapped function.\n    \"\"\"\n    def decorated(*inner_args, **inner_kwargs):\n        kwargs.update(inner_kwargs)\n        return function(*(inner_args + args), **kwargs)\n    copy_labels(function, decorated)\n    return decorated"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef os_function_mapper(map):\n    def decorated(job, host, conn, *args, **kwargs):\n        os = conn.guess_os()\n        func = map.get(os)\n        if func is None:\n            raise Exception('No handler for %s found.' % os)\n        return func(job, host, conn, *args, **kwargs)\n    return decorated", "response": "This is a simple function that returns a list of functions that are available on the operating system of the connected host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap the given function such that conn.login() or conn.authenticate() is executed. Doing the real work for autologin and autoauthenticate to minimize code duplication. :type flush: bool :param flush: Whether to flush the last prompt from the buffer. :type attempts: int :param attempts: The number of login attempts if login fails. :type only_authenticate: bool :param only_authenticate: login or only authenticate (don't authorize)? :rtype: function :return: The wrapped function.", "response": "def _decorate(flush=True, attempts=1, only_authenticate=False):\n    \"\"\"\n    Wraps the given function such that conn.login() or conn.authenticate() is\n    executed.\n    Doing the real work for autologin and autoauthenticate to minimize code\n    duplication.\n\n    :type  flush: bool\n    :param flush: Whether to flush the last prompt from the buffer.\n    :type  attempts: int\n    :param attempts: The number of login attempts if login fails.\n    :type only_authenticate: bool\n    :param only_authenticate: login or only authenticate (don't authorize)?\n    :rtype:  function\n    :return: The wrapped function.\n    \"\"\"\n    def decorator(function):\n        def decorated(job, host, conn, *args, **kwargs):\n            failed = 0\n            while True:\n                try:\n                    if only_authenticate:\n                        conn.authenticate(flush=flush)\n                    else:\n                        conn.login(flush=flush)\n                except LoginFailure as e:\n                    failed += 1\n                    if failed >= attempts:\n                        raise\n                    continue\n                break\n            return function(job, host, conn, *args, **kwargs)\n        copy_labels(function, decorated)\n        return decorated\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all valid problem files in the current language", "response": "def problem_glob(extension='.py'):\n    \"\"\"Returns ProblemFile objects for all valid problem files\"\"\"\n    filenames = glob.glob('*[0-9][0-9][0-9]*{}'.format(extension))\n    return [ProblemFile(file) for file in filenames]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the timespan in a human - readable format", "response": "def human_time(timespan, precision=3):\n    \"\"\"Formats the timespan in a human readable format\"\"\"\n\n    if timespan >= 60.0:\n        # Format time greater than one minute in a human-readable format\n        # Idea from http://snipplr.com/view/5713/\n        def _format_long_time(time):\n            suffixes = ('d', 'h', 'm', 's')\n            lengths = (24*60*60, 60*60, 60, 1)\n\n            for suffix, length in zip(suffixes, lengths):\n                value = int(time / length)\n\n                if value > 0:\n                    time %= length\n                    yield '%i%s' % (value, suffix)\n\n                if time < 1:\n                    break\n\n        return ' '.join(_format_long_time(timespan))\n\n    else:\n        units = ['s', 'ms', 'us', 'ns']\n\n        # Attempt to replace 'us' with '\u00b5s' if UTF-8 encoding has been set\n        if hasattr(sys.stdout, 'encoding') and sys.stdout.encoding == 'UTF-8':\n            try:\n                units[2] = b'\\xc2\\xb5s'.decode('utf-8')\n            except UnicodeEncodeError:\n                pass\n\n        scale = [1.0, 1e3, 1e6, 1e9]\n\n        if timespan > 0.0:\n            # Determine scale of timespan (s = 0, ms = 1, \u00b5s = 2, ns = 3)\n            order = min(-int(math.floor(math.log10(timespan)) // 3), 3)\n        else:\n            order = 3\n\n        return '%.*g %s' % (precision, timespan * scale[order], units[order])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_time(start, end):\n    try:\n        cpu_usr = end[0] - start[0]\n        cpu_sys = end[1] - start[1]\n\n    except TypeError:\n        # `clock()[1] == None` so subtraction results in a TypeError\n        return 'Time elapsed: {}'.format(human_time(cpu_usr))\n\n    else:\n        times = (human_time(x) for x in (cpu_usr, cpu_sys, cpu_usr + cpu_sys))\n        return 'Time elapsed: user: {}, sys: {}, total: {}'.format(*times)", "response": "Returns string with relevant time information formatted properly"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filename(self, prefix='', suffix='', extension='.py'):\n        return BASE_NAME.format(prefix, self.num, suffix, extension)", "response": "Returns filename padded with leading zeros"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef glob(self):\n        file_glob = glob.glob(BASE_NAME.format('*', self.num, '*', '.*'))\n\n        # Sort globbed files by tuple (filename, extension)\n        return sorted(file_glob, key=lambda f: os.path.splitext(f))", "response": "Returns a sorted glob of files belonging to a given problem"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of resources related to the problem or None if the problem is not part of a resource.", "response": "def resources(self):\n        \"\"\"Returns a list of resources related to the problem (or None)\"\"\"\n        with open(os.path.join(EULER_DATA, 'resources.json')) as data_file:\n            data = json.load(data_file)\n\n        problem_num = str(self.num)\n\n        if problem_num in data:\n            files = data[problem_num]\n\n            # Ensure a list of files is returned\n            return files if isinstance(files, list) else [files]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_resources(self):\n        if not os.path.isdir('resources'):\n            os.mkdir('resources')\n\n        resource_dir = os.path.join(os.getcwd(), 'resources', '')\n        copied_resources = []\n\n        for resource in self.resources:\n            src = os.path.join(EULER_DATA, 'resources', resource)\n            if os.path.isfile(src):\n                shutil.copy(src, resource_dir)\n                copied_resources.append(resource)\n\n        if copied_resources:\n            copied = ', '.join(copied_resources)\n            path = os.path.relpath(resource_dir, os.pardir)\n            msg = \"Copied {} to {}.\".format(copied, path)\n\n            click.secho(msg, fg='green')", "response": "Copies the relevant resources to a resources subdirectory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solution(self):\n        num = self.num\n\n        solution_file = os.path.join(EULER_DATA, 'solutions.txt')\n        solution_line = linecache.getline(solution_file, num)\n\n        try:\n            answer = solution_line.split('. ')[1].strip()\n        except IndexError:\n            answer = None\n\n        if answer:\n            return answer\n        else:\n            msg = 'Answer for problem %i not found in solutions.txt.' % num\n            click.secho(msg, fg='red')\n            click.echo('If you have an answer, consider submitting a pull '\n                       'request to EulerPy on GitHub.')\n            sys.exit(1)", "response": "Returns the answer to a given problem"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef text(self):\n        def _problem_iter(problem_num):\n            problem_file = os.path.join(EULER_DATA, 'problems.txt')\n\n            with open(problem_file) as f:\n                is_problem = False\n                last_line = ''\n\n                for line in f:\n                    if line.strip() == 'Problem %i' % problem_num:\n                        is_problem = True\n\n                    if is_problem:\n                        if line == last_line == '\\n':\n                            break\n                        else:\n                            yield line[:-1]\n                            last_line = line\n\n        problem_lines = [line for line in _problem_iter(self.num)]\n\n        if problem_lines:\n            # First three lines are the problem number, the divider line,\n            # and a newline, so don't include them in the returned string.\n            # Also, strip the final newline.\n            return '\\n'.join(problem_lines[3:-1])\n        else:\n            msg = 'Problem %i not found in problems.txt.' % self.num\n            click.secho(msg, fg='red')\n            click.echo('If this problem exists on Project Euler, consider '\n                       'submitting a pull request to EulerPy on GitHub.')\n            sys.exit(1)", "response": "Parses problems. txt and returns problem text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cheat(num):\n    # Define solution before echoing in case solution does not exist\n    solution = click.style(Problem(num).solution, bold=True)\n    click.confirm(\"View answer to problem %i?\" % num, abort=True)\n    click.echo(\"The answer to problem {} is {}.\".format(num, solution))", "response": "View the answer to a problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate(num, prompt_default=True):\n    p = Problem(num)\n\n    problem_text = p.text\n\n    msg = \"Generate file for problem %i?\" % num\n    click.confirm(msg, default=prompt_default, abort=True)\n\n    # Allow skipped problem files to be recreated\n    if p.glob:\n        filename = str(p.file)\n        msg = '\"{}\" already exists. Overwrite?'.format(filename)\n        click.confirm(click.style(msg, fg='red'), abort=True)\n    else:\n        # Try to keep prefix consistent with existing files\n        previous_file = Problem(num - 1).file\n        prefix = previous_file.prefix if previous_file else ''\n        filename = p.filename(prefix=prefix)\n\n    header = 'Project Euler Problem %i' % num\n    divider = '=' * len(header)\n    text = '\\n'.join([header, divider, '', problem_text])\n    content = '\\n'.join(['\"\"\"', text, '\"\"\"'])\n\n    with open(filename, 'w') as f:\n        f.write(content + '\\n\\n\\n')\n\n    click.secho('Successfully created \"{}\".'.format(filename), fg='green')\n\n    # Copy over problem resources if required\n    if p.resources:\n        p.copy_resources()", "response": "Generates a new Python file for a problem."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the text of a problem.", "response": "def preview(num):\n    \"\"\"Prints the text of a problem.\"\"\"\n    # Define problem_text before echoing in case problem does not exist\n    problem_text = Problem(num).text\n    click.secho(\"Project Euler Problem %i\" % num, bold=True)\n    click.echo(problem_text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef skip(num):\n    click.echo(\"Current problem is problem %i.\" % num)\n    generate(num + 1, prompt_default=False)\n    Problem(num).file.change_suffix('-skipped')", "response": "Generates Python file for the next problem."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify the solution to a problem.", "response": "def verify(num, filename=None, exit=True):\n    \"\"\"Verifies the solution to a problem.\"\"\"\n    p = Problem(num)\n\n    filename = filename or p.filename()\n\n    if not os.path.isfile(filename):\n        # Attempt to verify the first problem file matched by glob\n        if p.glob:\n            filename = str(p.file)\n        else:\n            click.secho('No file found for problem %i.' % p.num, fg='red')\n            sys.exit(1)\n\n    solution = p.solution\n    click.echo('Checking \"{}\" against solution: '.format(filename), nl=False)\n\n    cmd = (sys.executable or 'python', filename)\n    start = clock()\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    stdout = proc.communicate()[0]\n    end = clock()\n    time_info = format_time(start, end)\n\n    # Return value of anything other than 0 indicates an error\n    if proc.poll() != 0:\n        click.secho('Error calling \"{}\".'.format(filename), fg='red')\n        click.secho(time_info, fg='cyan')\n\n        # Return None if option is not --verify-all, otherwise exit\n        return sys.exit(1) if exit else None\n\n    # Decode output if returned as bytes (Python 3)\n    if isinstance(stdout, bytes):\n        output = stdout.decode('ascii')\n\n    # Split output lines into array; make empty output more readable\n    output_lines = output.splitlines() if output else ['[no output]']\n\n    # If output is multi-lined, print the first line of the output on a\n    # separate line from the \"checking against solution\" message, and\n    # skip the solution check (multi-line solution won't be correct)\n    if len(output_lines) > 1:\n        is_correct = False\n        click.echo()  # force output to start on next line\n        click.secho('\\n'.join(output_lines), bold=True, fg='red')\n    else:\n        is_correct = output_lines[0] == solution\n        fg_colour = 'green' if is_correct else 'red'\n        click.secho(output_lines[0], bold=True, fg=fg_colour)\n\n    click.secho(time_info, fg='cyan')\n\n    # Remove any suffix from the filename if its solution is correct\n    if is_correct:\n        p.file.change_suffix('')\n\n    # Exit here if answer was incorrect, otherwise return is_correct value\n    return sys.exit(1) if exit and not is_correct else is_correct"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_all(num):\n\n    # Define various problem statuses\n    keys = ('correct', 'incorrect', 'error', 'skipped', 'missing')\n    symbols = ('C', 'I', 'E', 'S', '.')\n    colours = ('green', 'red', 'yellow', 'cyan', 'white')\n\n    status = OrderedDict(\n        (key, click.style(symbol, fg=colour, bold=True))\n        for key, symbol, colour in zip(keys, symbols, colours)\n    )\n\n    overview = {}\n\n    # Search through problem files using glob module\n    files = problem_glob()\n\n    # No Project Euler files in the current directory\n    if not files:\n        click.echo(\"No Project Euler files found in the current directory.\")\n        sys.exit(1)\n\n    for file in files:\n        # Catch KeyboardInterrupt during verification to allow the user to\n        # skip the verification of a specific problem if it takes too long\n        try:\n            is_correct = verify(file.num, filename=str(file), exit=False)\n        except KeyboardInterrupt:\n            overview[file.num] = status['skipped']\n        else:\n            if is_correct is None:  # error was returned by problem file\n                overview[file.num] = status['error']\n            elif is_correct:\n                overview[file.num] = status['correct']\n            elif not is_correct:\n                overview[file.num] = status['incorrect']\n\n                # Attempt to add \"skipped\" suffix to the filename if the\n                # problem file is not the current problem. This is useful\n                # when the --verify-all is used in a directory containing\n                # files generated pre-v1.1 (before files with suffixes)\n                if file.num != num:\n                    file.change_suffix('-skipped')\n\n        # Separate each verification with a newline\n        click.echo()\n\n    # Print overview of the status of each problem\n    legend = ', '.join('{} = {}'.format(v, k) for k, v in status.items())\n\n    click.echo('-' * 63)\n    click.echo(legend + '\\n')\n\n    # Rows needed for overview is based on the current problem number\n    num_of_rows = (num + 19) // 20\n\n    for row in range(1, num_of_rows + 1):\n        low, high = (row * 20) - 19, (row * 20)\n        click.echo(\"Problems {:03d}-{:03d}: \".format(low, high), nl=False)\n\n        for problem in range(low, high + 1):\n            # Add missing status to problems with no corresponding file\n            status = overview[problem] if problem in overview else '.'\n\n            # Separate problem indicators into groups of 5\n            spacer = '   ' if (problem % 5 == 0) else ' '\n\n            # Start a new line at the end of each row\n            click.secho(status + spacer, nl=(problem % 20 == 0))\n\n    click.echo()", "response": "Verify all problem files in the current directory and print overview of each problem."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self, *args, **kwargs):\n        if args:\n            LOG.debug(\"args: %s\" % str(args))\n        if kwargs:\n            LOG.debug(\"kwargs: %s\" % str(kwargs))\n        try:\n            self._server.start()\n            self._server.proxyInit()\n            return True\n        except Exception as err:\n            LOG.critical(\"Failed to start server\")\n            LOG.debug(str(err))\n            self._server.stop()\n            return False", "response": "Start the server thread if it wasn t created with autostart = True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stop(self, *args, **kwargs):\n        if args:\n            LOG.debug(\"args: %s\" % str(args))\n        if kwargs:\n            LOG.debug(\"kwargs: %s\" % str(kwargs))\n        try:\n            self._server.stop()\n            self._server = None\n\n            # Device-storage clear\n            self.devices.clear()\n            self.devices_all.clear()\n            self.devices_raw.clear()\n            self.devices_raw_dict.clear()\n\n            return True\n        except Exception as err:\n            LOG.critical(\"Failed to stop server\")\n            LOG.debug(str(err))\n            return False", "response": "Stop the server thread."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getSystemVariable(self, remote, name):\n        if self._server is not None:\n            return self._server.getSystemVariable(remote, name)", "response": "Get single system variable from CCU or Homegear"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deleteSystemVariable(self, remote, name):\n        if self._server is not None:\n            return self._server.deleteSystemVariable(remote, name)", "response": "Delete a system variable from CCU / Homegear"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setSystemVariable(self, remote, name, value):\n        if self._server is not None:\n            return self._server.setSystemVariable(remote, name, value)", "response": "Set a system variable on CCU / Homegear"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nactivates or deactivate installmode on CCU / Homegear", "response": "def setInstallMode(self, remote, on=True, t=60, mode=1, address=None):\n        \"\"\"Activate or deactivate installmode on CCU / Homegear\"\"\"\n        if self._server is not None:\n            return self._server.setInstallMode(remote, on, t, mode, address)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all metadata of a device", "response": "def getAllMetadata(self, remote, address):\n        \"\"\"Get all metadata of device\"\"\"\n        if self._server is not None:\n            return self._server.getAllMetadata(remote, address)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget metadata of a device", "response": "def getMetadata(self, remote, address, key):\n        \"\"\"Get metadata of device\"\"\"\n        if self._server is not None:\n            # pylint: disable=E1121\n            return self._server.getAllMetadata(remote, address, key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setMetadata(self, remote, address, key, value):\n        if self._server is not None:\n            # pylint: disable=E1121\n            return self._server.getAllMetadata(remote, address, key, value)", "response": "Set metadata of a device"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deleteMetadata(self, remote, address, key):\n        if self._server is not None:\n            # pylint: disable=E1121\n            return self._server.deleteMetadata(remote, address, key)", "response": "Delete metadata of a device."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_hs_color(self):\n        # Get the color from homematic. In general this is just the hue parameter.\n        hm_color = self.getCachedOrUpdatedValue(\"COLOR\", channel=self._color_channel)\n\n        if hm_color >= 200:\n            # 200 is a special case (white), so we have a saturation of 0.\n            # Larger values are undefined. For the sake of robustness we return \"white\" anyway.\n            return 0, 0\n\n        # For all other colors we assume saturation of 1\n        return hm_color/200, 1", "response": "Get the hue and saturation color of the light."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_hs_color(self, hue: float, saturation: float):\n        self.turn_off_effect()\n\n        if saturation < 0.1:  # Special case (white)\n            hm_color = 200\n        else:\n            hm_color = int(round(max(min(hue, 1), 0) * 199))\n\n        self.setValue(key=\"COLOR\", channel=self._color_channel, value=hm_color)", "response": "Sets a fixed color and also turn off effects in order to see the color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_effect(self) -> str:\n        effect_value = self.getCachedOrUpdatedValue(\"PROGRAM\", channel=self._effect_channel)\n\n        try:\n            return self._light_effect_list[effect_value]\n        except IndexError:\n            LOG.error(\"Unexpected color effect returned by CCU\")\n            return \"Unknown\"", "response": "Return the current color change program of the light."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the color change program of the light.", "response": "def set_effect(self, effect_name: str):\n        \"\"\"Sets the color change program of the light.\"\"\"\n        try:\n            effect_index = self._light_effect_list.index(effect_name)\n        except ValueError:\n            LOG.error(\"Trying to set unknown light effect\")\n            return False\n\n        return self.setValue(key=\"PROGRAM\", channel=self._effect_channel, value=effect_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding auth part for api_url.", "response": "def make_http_credentials(username=None, password=None):\n    \"\"\"Build auth part for api_url.\"\"\"\n    credentials = ''\n    if username is None:\n        return credentials\n    if username is not None:\n        if ':' in username:\n            return credentials\n        credentials += username\n    if credentials and password is not None:\n        credentials += \":%s\" % password\n    return \"%s@\" % credentials"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_api_url(host=REMOTES['default']['ip'],\n                  port=REMOTES['default']['port'],\n                  path=REMOTES['default']['path'],\n                  username=None,\n                  password=None,\n                  ssl=False):\n    \"\"\"Build API URL from components.\"\"\"\n    credentials = make_http_credentials(username, password)\n    scheme = 'http'\n    if not path:\n        path = ''\n    if path and not path.startswith('/'):\n        path = \"/%s\" % path\n    if ssl:\n        scheme += 's'\n    return \"%s://%s%s:%i%s\" % (scheme, credentials, host, port, path)", "response": "Build API URL from components."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform the raw device descriptions into instances of devicetypes. generic. HMDevice or availabe subclass.", "response": "def createDeviceObjects(self, interface_id):\n        \"\"\"Transform the raw device descriptions into instances of devicetypes.generic.HMDevice or availabe subclass.\"\"\"\n        global WORKING\n        WORKING = True\n        remote = interface_id.split('-')[-1]\n        LOG.debug(\n            \"RPCFunctions.createDeviceObjects: iterating interface_id = %s\" % (remote, ))\n        # First create parent object\n        for dev in self._devices_raw[remote]:\n            if not dev['PARENT']:\n                if dev['ADDRESS'] not in self.devices_all[remote]:\n                    try:\n                        if dev['TYPE'] in devicetypes.SUPPORTED:\n                            deviceObject = devicetypes.SUPPORTED[dev['TYPE']](\n                                dev, self._proxies[interface_id], self.resolveparamsets)\n                            LOG.debug(\"RPCFunctions.createDeviceObjects: created %s  as SUPPORTED device for %s\" % (\n                                dev['ADDRESS'], dev['TYPE']))\n                        else:\n                            deviceObject = devicetypes.UNSUPPORTED(\n                                dev, self._proxies[interface_id], self.resolveparamsets)\n                            LOG.debug(\"RPCFunctions.createDeviceObjects: created %s  as UNSUPPORTED device for %s\" % (\n                                dev['ADDRESS'], dev['TYPE']))\n                        LOG.debug(\n                            \"RPCFunctions.createDeviceObjects: adding to self.devices_all\")\n                        self.devices_all[remote][dev['ADDRESS']] = deviceObject\n                        LOG.debug(\n                            \"RPCFunctions.createDeviceObjects: adding to self.devices\")\n                        self.devices[remote][dev['ADDRESS']] = deviceObject\n                    except Exception as err:\n                        LOG.critical(\n                            \"RPCFunctions.createDeviceObjects: Parent: %s\", str(err))\n        # Then create all children for parent\n        for dev in self._devices_raw[remote]:\n            if dev['PARENT']:\n                try:\n                    if dev['ADDRESS'] not in self.devices_all[remote]:\n                        deviceObject = HMChannel(\n                            dev, self._proxies[interface_id], self.resolveparamsets)\n                        self.devices_all[remote][dev['ADDRESS']] = deviceObject\n                        self.devices[remote][dev['PARENT']].CHANNELS[\n                            dev['INDEX']] = deviceObject\n                except Exception as err:\n                    LOG.critical(\n                        \"RPCFunctions.createDeviceObjects: Child: %s\", str(err))\n        if self.devices_all[remote] and self.remotes[remote].get('resolvenames', False):\n            self.addDeviceNames(remote)\n        WORKING = False\n        if self.systemcallback:\n            self.systemcallback('createDeviceObjects')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef error(self, interface_id, errorcode, msg):\n        LOG.debug(\"RPCFunctions.error: interface_id = %s, errorcode = %i, message = %s\" % (\n            interface_id, int(errorcode), str(msg)))\n        if self.systemcallback:\n            self.systemcallback('error', interface_id, errorcode, msg)\n        return True", "response": "This function is called when some error occurs the CCU is sending it s error message here"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves known devices into a json - file so we don t have to work through the whole list of devices the CCU is presents us", "response": "def saveDevices(self, remote):\n        \"\"\"We save known devices into a json-file so we don't have to work through the whole list of devices the CCU / Homegear presents us\"\"\"\n        LOG.debug(\"RPCFunctions.saveDevices: devicefile: %s, _devices_raw: %s\" % (\n            self.devicefile, str(self._devices_raw[remote])))\n        if self.devicefile:\n            try:\n                with open(self.devicefile, 'w') as df:\n                    df.write(json.dumps(self._devices_raw[remote]))\n                return True\n            except Exception as err:\n                LOG.warning(\n                    \"RPCFunctions.saveDevices: Exception saving _devices_raw: %s\", str(err))\n                return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef event(self, interface_id, address, value_key, value):\n        LOG.debug(\"RPCFunctions.event: interface_id = %s, address = %s, value_key = %s, value = %s\" % (\n            interface_id, address, value_key.upper(), str(value)))\n        self.devices_all[interface_id.split(\n            '-')[-1]][address].event(interface_id, value_key.upper(), value)\n        if self.eventcallback:\n            self.eventcallback(interface_id=interface_id, address=address,\n                               value_key=value_key.upper(), value=value)\n        return True", "response": "This function is called by the event callback function when a device emits some sort event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef listDevices(self, interface_id):\n        LOG.debug(\"RPCFunctions.listDevices: interface_id = %s, _devices_raw = %s\" % (\n            interface_id, str(self._devices_raw)))\n        remote = interface_id.split('-')[-1]\n        if remote not in self._devices_raw:\n            self._devices_raw[remote] = []\n        if self.systemcallback:\n            self.systemcallback('listDevices', interface_id)\n        return self._devices_raw[remote]", "response": "The CCU / Homegear asks for devices known to our XML - RPC server. We respond to that request using this method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addDeviceNames(self, remote):\n        LOG.debug(\"RPCFunctions.addDeviceNames\")\n\n        # First try to get names from metadata when nur credentials are set\n        if self.remotes[remote]['resolvenames'] == 'metadata':\n            for address in self.devices[remote]:\n                try:\n                    name = self.devices[remote][\n                        address]._proxy.getMetadata(address, 'NAME')\n                    self.devices[remote][address].NAME = name\n                    for address, device in self.devices[remote][address].CHANNELS.items():\n                        device.NAME = name\n                        self.devices_all[remote][device.ADDRESS].NAME = name\n                except Exception as err:\n                    LOG.debug(\n                        \"RPCFunctions.addDeviceNames: Unable to get name for %s from metadata.\" % str(address))\n\n        # Then try to get names via JSON-RPC\n        elif (self.remotes[remote]['resolvenames'] == 'json' and\n              self.remotes[remote]['username'] and\n              self.remotes[remote]['password']):\n            LOG.debug(\"RPCFunctions.addDeviceNames: Getting names via JSON-RPC\")\n            try:\n                session = False\n                params = {\"username\": self.remotes[remote][\n                    'username'], \"password\": self.remotes[remote]['password']}\n                response = self.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Session.login\", params)\n                if response['error'] is None and response['result']:\n                    session = response['result']\n\n                if not session:\n                    LOG.warning(\n                        \"RPCFunctions.addDeviceNames: Unable to open session.\")\n                    return\n\n                params = {\"_session_id_\": session}\n                response = self.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Interface.listInterfaces\", params)\n                interface = False\n                if response['error'] is None and response['result']:\n                    for i in response['result']:\n                        if i['port'] in [self.remotes[remote]['port'], self.remotes[remote]['port'] + 30000]:\n                            interface = i['name']\n                            break\n                LOG.debug(\n                    \"RPCFunctions.addDeviceNames: Got interface: %s\" % interface)\n                if not interface:\n                    params = {\"_session_id_\": session}\n                    response = self.jsonRpcPost(\n                        self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Session.logout\", params)\n                    return\n\n                params = {\"_session_id_\": session}\n                response = self.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Device.listAllDetail\", params)\n\n                if response['error'] is None and response['result']:\n                    LOG.debug(\n                        \"RPCFunctions.addDeviceNames: Resolving devicenames\")\n                    for i in response['result']:\n                        try:\n                            if i.get('address') in self.devices[remote]:\n                                self.devices[remote][\n                                    i['address']].NAME = i['name']\n                        except Exception as err:\n                            LOG.warning(\n                                \"RPCFunctions.addDeviceNames: Exception: %s\" % str(err))\n\n                params = {\"_session_id_\": session}\n                response = self.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Session.logout\", params)\n            except Exception as err:\n                params = {\"_session_id_\": session}\n                response = self.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Session.logout\", params)\n                LOG.warning(\n                    \"RPCFunctions.addDeviceNames: Exception: %s\" % str(err))\n\n        # Then try to get names from XML-API\n        elif self.remotes[remote]['resolvenames'] == 'xml':\n            LOG.warning(\"Resolving names with the XML-API addon will be disabled in a future release. Please switch to json.\")\n            try:\n                response = urllib.request.urlopen(\n                    \"http://%s%s\" % (self.remotes[remote]['ip'], XML_API_URL), timeout=5)\n                device_list = response.read().decode(\"ISO-8859-1\")\n            except Exception as err:\n                LOG.warning(\n                    \"RPCFunctions.addDeviceNames: Could not access XML-API: %s\" % (str(err), ))\n                return\n            device_list_tree = ET.ElementTree(ET.fromstring(device_list))\n            for device in device_list_tree.getroot():\n                address = device.attrib['address']\n                name = device.attrib['name']\n                if address in self.devices[remote]:\n                    self.devices[remote][address].NAME = name\n                    for address, device in self.devices[remote][address].CHANNELS.items():\n                        device.NAME = name\n                        self.devices_all[remote][device.ADDRESS].NAME = name", "response": "This function will add names to the CCU devices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __request(self, *args, **kwargs):\n\n        with self.lock:\n            parent = xmlrpc.client.ServerProxy\n            # pylint: disable=E1101\n            return parent._ServerProxy__request(self, *args, **kwargs)", "response": "Call method on server side"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes all the proxies and send events.", "response": "def proxyInit(self):\n        \"\"\"\n        To receive events the proxy has to tell the CCU / Homegear where to send the events. For that we call the init-method.\n        \"\"\"\n        # Call init() with local XML RPC config and interface_id (the name of\n        # the receiver) to receive events. XML RPC server has to be running.\n        for interface_id, proxy in self.proxies.items():\n            if proxy._skipinit:\n                continue\n            if proxy._callbackip and proxy._callbackport:\n                callbackip = proxy._callbackip\n                callbackport = proxy._callbackport\n            else:\n                callbackip = proxy._localip\n                callbackport = self._localport\n            LOG.debug(\"ServerThread.proxyInit: init('http://%s:%i', '%s')\" %\n                      (callbackip, callbackport, interface_id))\n            try:\n                proxy.init(\"http://%s:%i\" %\n                           (callbackip, callbackport), interface_id)\n                LOG.info(\"Proxy initialized\")\n            except Exception as err:\n                LOG.debug(\"proxyInit: Exception: %s\" % str(err))\n                LOG.warning(\"Failed to initialize proxy\")\n                self.failed_inits.append(interface_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop(self):\n        stopped = []\n        for interface_id, proxy in self.proxies.items():\n            if interface_id in self.failed_inits:\n                LOG.warning(\"ServerThread.stop: Not performing de-init for %s\" % interface_id)\n                continue\n            if proxy._callbackip and proxy._callbackport:\n                callbackip = proxy._callbackip\n                callbackport = proxy._callbackport\n            else:\n                callbackip = proxy._localip\n                callbackport = self._localport\n            remote = \"http://%s:%i\" % (callbackip, callbackport)\n            LOG.debug(\"ServerThread.stop: init('%s')\" % remote)\n            if not callbackip in stopped:\n                try:\n                    proxy.init(remote)\n                    stopped.append(callbackip)\n                    LOG.info(\"Proxy de-initialized: %s\" % remote)\n                except Exception as err:\n                    LOG.debug(\"proxyInit: Exception: %s\" % str(err))\n                    LOG.warning(\"Failed to de-initialize proxy\")\n        self.proxies.clear()\n        LOG.info(\"Shutting down server\")\n        self.server.shutdown()\n        LOG.debug(\"ServerThread.stop: Stopping ServerThread\")\n        self.server.server_close()\n        LOG.info(\"Server stopped\")", "response": "Stop the server thread."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlogin to CCU and return session", "response": "def jsonRpcLogin(self, remote):\n        \"\"\"Login to CCU and return session\"\"\"\n        session = False\n        try:\n            params = {\"username\": self.remotes[remote][\n                'username'], \"password\": self.remotes[remote]['password']}\n            response = self._rpcfunctions.jsonRpcPost(\n                self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"Session.login\", params)\n            if response['error'] is None and response['result']:\n                session = response['result']\n\n            if not session:\n                LOG.warning(\n                    \"ServerThread.jsonRpcLogin: Unable to open session.\")\n        except Exception as err:\n            LOG.debug(\n                \"ServerThread.jsonRpcLogin: Exception while logging in via JSON-RPC: %s\" % str(err))\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getAllSystemVariables(self, remote):\n        variables = {}\n        if self.remotes[remote]['username'] and self.remotes[remote]['password']:\n            LOG.debug(\n                \"ServerThread.getAllSystemVariables: Getting all System variables via JSON-RPC\")\n            session = self.jsonRpcLogin(remote)\n            if not session:\n                return\n            try:\n                params = {\"_session_id_\": session}\n                response = self._rpcfunctions.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"SysVar.getAll\", params)\n                if response['error'] is None and response['result']:\n                    for var in response['result']:\n                        key, value = self.parseCCUSysVar(var)\n                        variables[key] = value\n\n                self.jsonRpcLogout(remote, session)\n            except Exception as err:\n                self.jsonRpcLogout(remote, session)\n                LOG.warning(\n                    \"ServerThread.getAllSystemVariables: Exception: %s\" % str(err))\n        else:\n            try:\n                variables = self.proxies[\n                    \"%s-%s\" % (self._interface_id, remote)].getAllSystemVariables()\n            except Exception as err:\n                LOG.debug(\n                    \"ServerThread.getAllSystemVariables: Exception: %s\" % str(err))\n        return variables", "response": "Get all system variables from CCU or Homegear"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting single system variable from CCU or Homegear", "response": "def getSystemVariable(self, remote, name):\n        \"\"\"Get single system variable from CCU / Homegear\"\"\"\n        var = None\n        if self.remotes[remote]['username'] and self.remotes[remote]['password']:\n            LOG.debug(\n                \"ServerThread.getSystemVariable: Getting System variable via JSON-RPC\")\n            session = self.jsonRpcLogin(remote)\n            if not session:\n                return\n            try:\n                params = {\"_session_id_\": session, \"name\": name}\n                response = self._rpcfunctions.jsonRpcPost(\n                    self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"SysVar.getValueByName\", params)\n                if response['error'] is None and response['result']:\n                    try:\n                        var = float(response['result'])\n                    except Exception as err:\n                        var = response['result'] == 'true'\n\n                self.jsonRpcLogout(remote, session)\n            except Exception as err:\n                self.jsonRpcLogout(remote, session)\n                LOG.warning(\n                    \"ServerThread.getSystemVariable: Exception: %s\" % str(err))\n        else:\n            try:\n                var = self.proxies[\n                    \"%s-%s\" % (self._interface_id, remote)].getSystemVariable(name)\n            except Exception as err:\n                LOG.debug(\n                    \"ServerThread.getSystemVariable: Exception: %s\" % str(err))\n        return var"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setSystemVariable(self, remote, name, value):\n        if self.remotes[remote]['username'] and self.remotes[remote]['password']:\n            LOG.debug(\n                \"ServerThread.setSystemVariable: Setting System variable via JSON-RPC\")\n            session = self.jsonRpcLogin(remote)\n            if not session:\n                return\n            try:\n                params = {\"_session_id_\": session,\n                          \"name\": name, \"value\": value}\n                if value is True or value is False:\n                    params['value'] = int(value)\n                    response = self._rpcfunctions.jsonRpcPost(\n                        self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"SysVar.setBool\", params)\n                else:\n                    response = self._rpcfunctions.jsonRpcPost(\n                        self.remotes[remote]['ip'], self.remotes[remote].get('jsonport', DEFAULT_JSONPORT), \"SysVar.setFloat\", params)\n                if response['error'] is None and response['result']:\n                    res = response['result']\n                    LOG.debug(\n                        \"ServerThread.setSystemVariable: Result while setting variable: %s\" % str(res))\n                else:\n                    if response['error']:\n                        LOG.debug(\"ServerThread.setSystemVariable: Error while setting variable: %s\" % str(\n                            response['error']))\n\n                self.jsonRpcLogout(remote, session)\n            except Exception as err:\n                self.jsonRpcLogout(remote, session)\n                LOG.warning(\n                    \"ServerThread.setSystemVariable: Exception: %s\" % str(err))\n        else:\n            try:\n                return self.proxies[\"%s-%s\" % (self._interface_id, remote)].setSystemVariable(name, value)\n            except Exception as err:\n                LOG.debug(\n                    \"ServerThread.setSystemVariable: Exception: %s\" % str(err))", "response": "Set a system variable on CCU / Homegear"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getServiceMessages(self, remote):\n        try:\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].getServiceMessages()\n        except Exception as err:\n            LOG.debug(\"ServerThread.getServiceMessages: Exception: %s\" % str(err))", "response": "Get service messages from CCU / Homegear"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactivating or deactivate installmode on CCU / Homegear", "response": "def setInstallMode(self, remote, on=True, t=60, mode=1, address=None):\n        \"\"\"Activate or deactivate installmode on CCU / Homegear\"\"\"\n        try:\n            args = [on]\n            if on and t:\n                args.append(t)\n                if address:\n                    args.append(address)\n                else:\n                    args.append(mode)\n\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].setInstallMode(*args)\n        except Exception as err:\n            LOG.debug(\"ServerThread.setInstallMode: Exception: %s\" % str(err))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAllMetadata(self, remote, address):\n        try:\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].getAllMetadata(address)\n        except Exception as err:\n            LOG.debug(\"ServerThread.getAllMetadata: Exception: %s\" % str(err))", "response": "Get all metadata of a device"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setMetadata(self, remote, address, key, value):\n        try:\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].setMetadata(address, key, value)\n        except Exception as err:\n            LOG.debug(\"ServerThread.setMetadata: Exception: %s\" % str(err))", "response": "Set metadata of a device"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete metadata of a device", "response": "def deleteMetadata(self, remote, address, key):\n        \"\"\"Delete metadata of device\"\"\"\n        try:\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].deleteMetadata(address, key)\n        except Exception as err:\n            LOG.debug(\"ServerThread.deleteMetadata: Exception: %s\" % str(err))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends ping to CCU and Homegear to generate PONG event", "response": "def ping(self, remote):\n        \"\"\"Send ping to CCU/Homegear to generate PONG event\"\"\"\n        try:\n            self.proxies[\"%s-%s\" % (self._interface_id, remote)].ping(\"%s-%s\" % (self._interface_id, remote))\n        except Exception as err:\n            LOG.warning(\"ServerThread.ping: Exception: %s\" % str(err))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if proxy is still initialized", "response": "def homegearCheckInit(self, remote):\n        \"\"\"Check if proxy is still initialized\"\"\"\n        rdict = self.remotes.get(remote)\n        if not rdict:\n            return False\n        if rdict.get('type') != BACKEND_HOMEGEAR:\n            return False\n        try:\n            interface_id = \"%s-%s\" % (self._interface_id, remote)\n            return self.proxies[interface_id].clientServerInitialized(interface_id)\n        except Exception as err:\n            LOG.debug(\n                \"ServerThread.homegearCheckInit: Exception: %s\" % str(err))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_state(self, onoff, channel=None):\n        try:\n            onoff = bool(onoff)\n        except Exception as err:\n            LOG.debug(\"HelperActorState.set_state: Exception %s\" % (err,))\n            return False\n\n        self.writeNodeData(\"STATE\", onoff, channel)", "response": "Turn state on or off."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nseek a specific value by specifying a float from 0. 0 to 1. 0.", "response": "def set_level(self, position, channel=None):\n        \"\"\"Seek a specific value by specifying a float() from 0.0 to 1.0.\"\"\"\n        try:\n            position = float(position)\n        except Exception as err:\n            LOG.debug(\"HelperLevel.set_level: Exception %s\" % (err,))\n            return False\n\n        self.writeNodeData(\"LEVEL\", position, channel)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nseek a specific value by specifying a float from 0. 0 to 1. 0.", "response": "def set_cover_tilt_position(self, position, channel=None):\n        \"\"\"Seek a specific value by specifying a float() from 0.0 to 1.0.\"\"\"\n        try:\n            position = float(position)\n        except Exception as err:\n            LOG.debug(\"HelperActorBlindTilt.set_level_2: Exception %s\" % (err,))\n            return False\n\n        level = self.getWriteData(\"LEVEL\", channel)\n\n        self.writeNodeData(\"LEVEL_2\", position, channel)\n\n        # set level after level_2 to have level_2 updated\n        self.writeNodeData(\"LEVEL\", level, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ontime(self, ontime):\n        try:\n            ontime = float(ontime)\n        except Exception as err:\n            LOG.debug(\"SwitchPowermeter.set_ontime: Exception %s\" % (err,))\n            return False\n\n        self.actionNodeData(\"ON_TIME\", ontime)", "response": "Set duration th switch stays on when toggled."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles an event received by server.", "response": "def event(self, interface_id, key, value):\n        \"\"\"\n        Handle the event received by server.\n        \"\"\"\n        LOG.info(\n            \"HMGeneric.event: address=%s, interface_id=%s, key=%s, value=%s\"\n            % (self._ADDRESS, interface_id, key, value))\n\n        self._VALUES[key] = value   # Cache the value\n\n        for callback in self._eventcallbacks:\n            LOG.debug(\"HMGeneric.event: Using callback %s \" % str(callback))\n            callback(self._ADDRESS, interface_id, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getParamsetDescription(self, paramset):\n        try:\n            self._PARAMSET_DESCRIPTIONS[paramset] = self._proxy.getParamsetDescription(self._ADDRESS, paramset)\n        except Exception as err:\n            LOG.error(\"HMGeneric.getParamsetDescription: Exception: \" + str(err))\n            return False", "response": "Get the description of a given paramset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the internal list of paramsets for the specified resource.", "response": "def updateParamset(self, paramset):\n        \"\"\"\n        Devices should not update their own paramsets. They rely on the state of the server.\n        Hence we pull the specified paramset.\n        \"\"\"\n        try:\n            if paramset:\n                if self._proxy:\n                    returnset = self._proxy.getParamset(self._ADDRESS, paramset)\n                    if returnset:\n                        self._paramsets[paramset] = returnset\n                        if self.PARAMSETS:\n                            if self.PARAMSETS.get(PARAMSET_VALUES):\n                                self._VALUES[PARAM_UNREACH] = self.PARAMSETS.get(PARAMSET_VALUES).get(PARAM_UNREACH)\n                        return True\n            return False\n        except Exception as err:\n            LOG.debug(\"HMGeneric.updateParamset: Exception: %s, %s, %s\" % (str(err), str(self._ADDRESS), str(paramset)))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the paramsets of the device.", "response": "def updateParamsets(self):\n        \"\"\"\n        Devices should update their own paramsets. They rely on the state of the server. Hence we pull all paramsets.\n        \"\"\"\n        try:\n            for ps in self._PARAMSETS:\n                self.updateParamset(ps)\n            return True\n        except Exception as err:\n            LOG.error(\"HMGeneric.updateParamsets: Exception: \" + str(err))\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef putParamset(self, paramset, data={}):\n        try:\n            if paramset in self._PARAMSETS and data:\n                self._proxy.putParamset(self._ADDRESS, paramset, data)\n                # We update all paramsets to at least have a temporarily accurate state for the device.\n                # This might not be true for tasks that take long to complete (lifting a rollershutter completely etc.).\n                # For this the server-process has to call the updateParamsets-method when it receives events for the device.\n                self.updateParamsets()\n                return True\n            else:\n                return False\n        except Exception as err:\n            LOG.error(\"HMGeneric.putParamset: Exception: \" + str(err))\n            return False", "response": "Put a paramset into the specified paramset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getCachedOrUpdatedValue(self, key):\n        try:\n            return self._VALUES[key]\n        except KeyError:\n            return self.getValue(key)", "response": "Gets the value with the given key from the cache or the host s value if it is not found in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setValue(self, key, value):\n        LOG.debug(\"HMGeneric.setValue: address = '%s', key = '%s' value = '%s'\" % (self._ADDRESS, key, value))\n        try:\n            self._proxy.setValue(self._ADDRESS, key, value)\n            return True\n        except Exception as err:\n            LOG.error(\"HMGeneric.setValue: %s on %s Exception: %s\", key,\n                      self._ADDRESS, err)\n            return False", "response": "Sets the value of the key in the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getValue(self, key):\n        LOG.debug(\"HMGeneric.getValue: address = '%s', key = '%s'\" % (self._ADDRESS, key))\n        try:\n            returnvalue = self._proxy.getValue(self._ADDRESS, key)\n            self._VALUES[key] = returnvalue\n            return returnvalue\n        except Exception as err:\n            LOG.warning(\"HMGeneric.getValue: %s on %s Exception: %s\", key,\n                        self._ADDRESS, err)\n            return False", "response": "Get the value of the key from the HM."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getCachedOrUpdatedValue(self, key, channel=None):\n        if channel:\n            return self._hmchannels[channel].getCachedOrUpdatedValue(key)\n\n        try:\n            return self._VALUES[key]\n        except KeyError:\n            value = self._VALUES[key] = self.getValue(key)\n            return value", "response": "Gets the value with the given key from the cache or the respective channel s value if it is not found in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef UNREACH(self):\n        if self._VALUES.get(PARAM_UNREACH, False):\n            return True\n        else:\n            for device in self._hmchannels.values():\n                if device.UNREACH:\n                    return True\n        return False", "response": "Returns true if the device or any children is not reachable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a attribut with the given name and optional channel", "response": "def getAttributeData(self, name, channel=None):\n        \"\"\" Returns a attribut \"\"\"\n        return self._getNodeData(name, self._ATTRIBUTENODE, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getBinaryData(self, name, channel=None):\n        return self._getNodeData(name, self._BINARYNODE, channel)", "response": "Returns a binary node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sensor node", "response": "def getSensorData(self, name, channel=None):\n        \"\"\" Returns a sensor node \"\"\"\n        return self._getNodeData(name, self._SENSORNODE, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a sensor node", "response": "def getWriteData(self, name, channel=None):\n        \"\"\" Returns a sensor node \"\"\"\n        return self._getNodeData(name, self._WRITENODE, channel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getNodeData(self, name, metadata, channel=None):\n        nodeChannel = None\n        if name in metadata:\n            nodeChannelList = metadata[name]\n            if len(nodeChannelList) > 1:\n                nodeChannel = channel if channel is not None else nodeChannelList[0]\n            elif len(nodeChannelList) == 1:\n                nodeChannel = nodeChannelList[0]\n            else:\n                LOG.warning(\"HMDevice._getNodeData: %s not found in %s, empty nodeChannelList\" % (name, metadata))\n                return None\n            if nodeChannel is not None and nodeChannel in self.CHANNELS:\n                return self._hmchannels[nodeChannel].getValue(name)\n\n        LOG.error(\"HMDevice._getNodeData: %s not found in %s\" % (name, metadata))\n        return None", "response": "Returns a data point from data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setNodeData(self, name, metadata, data, channel=None):\n        nodeChannel = None\n        if name in metadata:\n            nodeChannelList = metadata[name]\n            if len(nodeChannelList) > 1:\n                nodeChannel = channel if channel is not None else nodeChannelList[0]\n            elif len(nodeChannelList) == 1:\n                nodeChannel = nodeChannelList[0]\n            if nodeChannel is not None and nodeChannel in self.CHANNELS:\n                return self._hmchannels[nodeChannel].setValue(name, data)\n\n        LOG.error(\"HMDevice.setNodeData: %s not found with value %s on %i\" %\n                  (name, data, nodeChannel))\n        return False", "response": "Sets a data point from data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setEventCallback(self, callback, bequeath=True, channel=0):\n        if hasattr(callback, '__call__'):\n            if channel == 0:\n                self._eventcallbacks.append(callback)\n            elif not bequeath and channel > 0 and channel in self._hmchannels:\n                self._hmchannels[channel]._eventcallbacks.append(callback)\n            if bequeath:\n                for channel, device in self._hmchannels.items():\n                    device._eventcallbacks.append(callback)", "response": "Set the callback for specific events."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the value of a key in a specific channel.", "response": "def setValue(self, key, value, channel=1):\n        \"\"\"\n        Some devices allow to directly set values to perform a specific task.\n        \"\"\"\n        if channel in self.CHANNELS:\n            return self.CHANNELS[channel].setValue(key, value)\n\n        LOG.error(\"HMDevice.setValue: channel not found %i!\" % channel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the value for a specific key in a specific channel.", "response": "def getValue(self, key, channel=1):\n        \"\"\"\n        Some devices allow to directly get values for specific parameters.\n        \"\"\"\n        if channel in self.CHANNELS:\n            return self.CHANNELS[channel].getValue(key)\n\n        LOG.error(\"HMDevice.getValue: channel not found %i!\" % channel)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_temperature(self, target_temperature):\n        try:\n            target_temperature = float(target_temperature)\n        except Exception as err:\n            LOG.debug(\"Thermostat.set_temperature: Exception %s\" % (err,))\n            return False\n        self.writeNodeData(\"SET_TEMPERATURE\", target_temperature)", "response": "Set the target temperature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the mode of the article.", "response": "def MODE(self, setmode):\n        \"\"\" Set mode. \"\"\"\n        if setmode == self.BOOST_MODE:\n            self.actionNodeData('BOOST_MODE', True)\n        elif setmode in [self.AUTO_MODE, self.MANU_MODE]:\n            if self.getAttributeData(\"BOOST_MODE\"):\n                self.actionNodeData('BOOST_MODE', False)\n            self.actionNodeData('CONTROL_MODE', setmode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self):\n        LOG.info(\"Shutting down server\")\n        self.server.shutdown()\n        LOG.debug(\"ServerThread.stop: Stopping ServerThread\")\n        self.server.server_close()\n        LOG.info(\"Server stopped\")", "response": "Shut down the server and close the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the normal shovel functionality", "response": "def run(*args):\n    '''Run the normal shovel functionality'''\n    import os\n    import sys\n    import argparse\n    import pkg_resources\n    # First off, read the arguments\n    parser = argparse.ArgumentParser(prog='shovel', description='Rake, for Python')\n\n    parser.add_argument('method', help='The task to run')\n    parser.add_argument('--verbose', dest='verbose', action='store_true',\n        help='Be extra talkative')\n    parser.add_argument('--dry-run', dest='dryRun', action='store_true',\n        help='Show the args that would be used')\n\n    ver = pkg_resources.require('shovel')[0].version\n    parser.add_argument('--version', action='version',\n        version='Shovel v %s' % ver, help='print the version of Shovel.')\n\n    # Parse our arguments\n    if args:\n        clargs, remaining = parser.parse_known_args(args=args)\n    else:  # pragma: no cover\n        clargs, remaining = parser.parse_known_args()\n\n    if clargs.verbose:\n        logger.setLevel(logging.DEBUG)\n\n    args, kwargs = parse(remaining)\n\n    # Import all of the files we want\n    shovel = Shovel()\n\n    # Read in any tasks that have already been defined\n    shovel.extend(Task.clear())\n\n    for path in [\n        os.path.expanduser('~/.shovel.py'),\n        os.path.expanduser('~/.shovel')]:\n        if os.path.exists(path):  # pragma: no cover\n            shovel.read(path, os.path.expanduser('~/'))\n\n    shovel_home = os.environ.get('SHOVEL_HOME')\n    if shovel_home and os.path.exists(shovel_home):\n        shovel.read(shovel_home, shovel_home)\n\n    for path in ['shovel.py', 'shovel']:\n        if os.path.exists(path):\n            shovel.read(path)\n\n    # If it's help we're looking for, look no further\n    if clargs.method == 'help':\n        print(help.shovel_help(shovel, *args, **kwargs))\n    elif clargs.method == 'tasks':\n        tasks = list(v for _, v in shovel.items())\n        if not tasks:\n            print('No tasks found!')\n        else:\n            names = list(t.fullname for t in tasks)\n            docs = list(t.doc for t in tasks)\n\n            # The width of the screen\n            width = 80\n            import shutil\n            try:\n                width, _ = shutil.get_terminal_size(fallback=(0, width))\n            except AttributeError:\n                pass\n\n            # Create the format with padding for the longest name, and to\n            # accomodate the screen width\n            format = '%%-%is # %%-%is' % (\n                max(len(name) for name in names), width)\n            for name, doc in zip(names, docs):\n                print(format % (name, doc))\n    elif clargs.method:\n        # Try to get the first command provided\n        try:\n            tasks = shovel.tasks(clargs.method)\n        except KeyError:\n            print('Could not find task \"%s\"' % clargs.method, file=sys.stderr)\n            exit(1)\n\n        if len(tasks) > 1:\n            print('Specifier \"%s\" matches multiple tasks:' % clargs.method, file=sys.stderr)\n            for task in tasks:\n                print('\\t%s' % task.fullname, file=sys.stderr)\n            exit(2)\n\n        task = tasks[0]\n        if clargs.dryRun:\n            print(task.dry(*args, **kwargs))\n        else:\n            task(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heirarchical_helper(shovel, prefix, level=0):\n    '''Return a list of tuples of (fullname, docstring, level) for all the\n    tasks in the provided shovel'''\n    result = []\n    for key, value in sorted(shovel.map.items()):\n        if prefix:\n            key = prefix + '.' + key\n        if isinstance(value, Shovel):\n            result.append((key, None, level))\n            result.extend(heirarchical_helper(value, key, level + 1))\n        else:\n            result.append((key, value.doc or '(No docstring)', level))\n    return result", "response": "Return a list of tuples of fullname docstring level for all the the\n    tasks in the provided shovel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a shovel of tasks display a heirarchical list of the tasks", "response": "def heirarchical_help(shovel, prefix):\n    '''Given a shovel of tasks, display a heirarchical list of the tasks'''\n    result = []\n    tuples = heirarchical_helper(shovel, prefix)\n    if not tuples:\n        return ''\n\n    # We need to figure out the longest fullname length\n    longest = max(len(name + '    ' * level) for name, _, level in tuples)\n    fmt = '%%%is => %%-50s' % longest\n    for name, docstring, level in tuples:\n        if docstring == None:\n            result.append('    ' * level + name + '/')\n        else:\n            docstring = re.sub(r'\\s+', ' ', docstring).strip()\n            if len(docstring) > 50:\n                docstring = docstring[:47] + '...'\n            result.append(fmt % (name, docstring))\n    return '\\n'.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shovel_help(shovel, *names):\n    '''Return a string about help with the tasks, or lists tasks available'''\n    # If names are provided, and the name refers to a group of tasks, print out\n    # the tasks and a brief docstring. Otherwise, just enumerate all the tasks\n    # available\n    if not len(names):\n        return heirarchical_help(shovel, '')\n    else:\n        for name in names:\n            task = shovel[name]\n            if isinstance(task, Shovel):\n                return heirarchical_help(task, name)\n            else:\n                return task.help()", "response": "Return a string about the tasks or lists tasks available"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extend(self, tasks):\n        '''Add tasks to this particular shovel'''\n        self._tasks.extend(tasks)\n        for task in tasks:\n            # We'll now go through all of our tasks and group them into\n            # sub-shovels\n            current = self.map\n            modules = task.fullname.split('.')\n            for module in modules[:-1]:\n                if not isinstance(current[module], Shovel):\n                    logger.warn('Overriding task %s with a module' %\n                        current[module].file)\n                    shovel = Shovel()\n                    shovel.overrides = current[module]\n                    current[module] = shovel\n                current = current[module].map\n\n            # Now we'll put the task in this particular sub-shovel\n            name = modules[-1]\n            if name in current:\n                logger.warn('Overriding %s with %s' % (\n                    '.'.join(modules), task.file))\n                task.overrides = current[name]\n            current[name] = task", "response": "Add tasks to this particular shovel"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, path, base=None):\n        '''Import some tasks'''\n        if base == None:\n            base = os.getcwd()\n        absolute = os.path.abspath(path)\n        if os.path.isfile(absolute):\n            # Load that particular file\n            logger.info('Loading %s' % absolute)\n            self.extend(Task.load(path, base))\n        elif os.path.isdir(absolute):\n            # Walk this directory looking for tasks\n            tasks = []\n            for root, _, files in os.walk(absolute):\n                files = [f for f in files if f.endswith('.py')]\n                for child in files:\n                    absolute = os.path.join(root, child)\n                    logger.info('Loading %s' % absolute)\n                    tasks.extend(Task.load(absolute, base))\n            self.extend(tasks)", "response": "Import some tasks from a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all valid keys", "response": "def keys(self):\n        '''Return all valid keys'''\n        keys = []\n        for key, value in self.map.items():\n            if isinstance(value, Shovel):\n                keys.extend([key + '.' + k for k in value.keys()])\n            else:\n                keys.append(key)\n        return sorted(keys)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef items(self):\n        '''Return a list of tuples of all the keys and tasks'''\n        pairs = []\n        for key, value in self.map.items():\n            if isinstance(value, Shovel):\n                pairs.extend([(key + '.' + k, v) for k, v in value.items()])\n            else:\n                pairs.append((key, value))\n        return sorted(pairs)", "response": "Return a list of tuples of all the keys and tasks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all the tasks that match a name", "response": "def tasks(self, name):\n        '''Get all the tasks that match a name'''\n        found = self[name]\n        if isinstance(found, Shovel):\n            return [v for _, v in found.items()]\n        return [found]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make(cls, obj):\n        '''Given a callable object, return a new callable object'''\n        try:\n            cls._cache.append(Task(obj))\n        except Exception:\n            logger.exception('Unable to make task for %s' % repr(obj))", "response": "Given a callable object return a new callable object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of the tasks stored in a file", "response": "def load(cls, path, base=None):\n        '''Return a list of the tasks stored in a file'''\n        base = base or os.getcwd()\n        absolute = os.path.abspath(path)\n        parent = os.path.dirname(absolute)\n        name, _, _ = os.path.basename(absolute).rpartition('.py')\n        fobj, path, description = imp.find_module(name, [parent])\n        try:\n            imp.load_module(name, fobj, path, description)\n        finally:\n            if fobj:\n                fobj.close()\n        # Manipulate the full names of the tasks to be relative to the provided\n        # base\n        relative, _, _ = os.path.relpath(path, base).rpartition('.py')\n        for task in cls._cache:\n            parts = relative.split(os.path.sep)\n            parts.append(task.name)\n            # If it's either in shovel.py, or folder/__init__.py, then we\n            # should consider it as being at one level above that file\n            parts = [part.strip('.') for part in parts if part not in\n                ('shovel', '.shovel', '__init__', '.', '..', '')]\n            task.fullname = '.'.join(parts)\n            logger.debug('Found task %s in %s' % (task.fullname, task.module))\n        return cls.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a task and return a dictionary with stderr stdout and the return value. Also the traceback from the exception if there was one", "response": "def capture(self, *args, **kwargs):\n        '''Run a task and return a dictionary with stderr, stdout and the\n        return value. Also, the traceback from the exception if there was\n        one'''\n        import traceback\n        try:\n            from StringIO import StringIO\n        except ImportError:\n            from io import StringIO\n        stdout, stderr = sys.stdout, sys.stderr\n        sys.stdout = out = StringIO()\n        sys.stderr = err = StringIO()\n        result = {\n            'exception': None,\n            'stderr': None,\n            'stdout': None,\n            'return': None\n        }\n        try:\n            result['return'] = self.__call__(*args, **kwargs)\n        except Exception:\n            result['exception'] = traceback.format_exc()\n        sys.stdout, sys.stderr = stdout, stderr\n        result['stderr'] = err.getvalue()\n        result['stdout'] = out.getvalue()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a dry - run of the task", "response": "def dry(self, *args, **kwargs):\n        '''Perform a dry-run of the task'''\n        return 'Would have executed:\\n%s%s' % (\n            self.name, Args(self.spec).explain(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the help string of the task", "response": "def help(self):\n        '''Return the help string of the task'''\n        # This returns a help string for a given task of the form:\n        #\n        # ==================================================\n        # <name>\n        # ============================== (If supplied)\n        # <docstring>\n        # ============================== (If overrides other tasks)\n        # Overrides <other task file>\n        # ==============================\n        # From <file> on <line>\n        # ==============================\n        # <name>(Argspec)\n        result = [\n            '=' * 50,\n            self.name\n        ]\n\n        # And the doc, if it exists\n        if self.doc:\n            result.extend([\n                '=' * 30,\n                self.doc\n            ])\n\n        override = self.overrides\n        while override:\n            if isinstance(override, Shovel):\n                result.append('Overrides module')\n            else:\n                result.append('Overrides %s' % override.file)\n            override = override.overrides\n\n        # Print where we read this function in from\n        result.extend([\n            '=' * 30,\n            'From %s on line %i' % (self.file, self.line),\n            '=' * 30,\n            '%s%s' % (self.name, str(Args(self.spec)))\n        ])\n        return os.linesep.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string that describes how these args are interpreted", "response": "def explain(self, *args, **kwargs):\n        '''Return a string that describes how these args are interpreted'''\n        args = self.get(*args, **kwargs)\n        results = ['%s = %s' % (name, value) for name, value in args.required]\n        results.extend(['%s = %s (overridden)' % (\n            name, value) for name, value in args.overridden])\n        results.extend(['%s = %s (default)' % (\n            name, value) for name, value in args.defaulted])\n        if self._varargs:\n            results.append('%s = %s' % (self._varargs, args.varargs))\n        if self._kwargs:\n            results.append('%s = %s' % (self._kwargs, args.kwargs))\n        return '\\n\\t'.join(results)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, *args, **kwargs):\n        '''Evaluate this argspec with the provided arguments'''\n        # We'll go through all of our required args and make sure they're\n        # present\n        required = [arg for arg in self._args if arg not in kwargs]\n        if len(args) < len(required):\n            raise TypeError('Missing arguments %s' % required[len(args):])\n        required = list(zip(required, args))\n        args = args[len(required):]\n\n        # Now we'll look through our defaults, if there are any\n        defaulted = [(name, default) for name, default in self._defaults\n            if name not in kwargs]\n        overridden = list(zip([d[0] for d in defaulted], args))\n        args = args[len(overridden):]\n        defaulted = defaulted[len(overridden):]\n\n        # And anything left over is in varargs\n        if args and not self._varargs:\n            raise TypeError('Too many arguments provided')\n\n        return ArgTuple(required, overridden, defaulted, args, kwargs)", "response": "Evaluate this argspec with the provided arguments"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(tokens):\n    '''Parse the provided string to produce *args and **kwargs'''\n    args = []\n    kwargs = {}\n    last = None\n    for token in tokens:\n        if token.startswith('--'):\n            # If this is a keyword flag, but we've already got one that we've\n            # parsed, then we're going to interpret it as a bool\n            if last:\n                kwargs[last] = True\n            # See if it is the --foo=5 style\n            last, _, value = token.strip('-').partition('=')\n            if value:\n                kwargs[last] = value\n                last = None\n        elif last != None:\n            kwargs[last] = token\n            last = None\n        else:\n            args.append(token)\n\n    # If there's a dangling last, set that bool\n    if last:\n        kwargs[last] = True\n\n    return args, kwargs", "response": "Parse the provided string to produce args and kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the sum of the provided numbers", "response": "def sumnum(*args):\n    '''Computes the sum of the provided numbers'''\n    print('%s = %f' % (' + '.join(args), sum(float(arg) for arg in args)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a name and all keyword attributes", "response": "def attributes(name, **kwargs):\n    '''Prints a name, and all keyword attributes'''\n    print('%s has attributes:' % name)\n    for key, value in kwargs.items():\n        print('\\t%s => %s' % (key, value))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, name, value, attrs=None, multi=False, renderer=None):\n        DJANGO_111_OR_UP = (VERSION[0] == 1 and VERSION[1] >= 11) or (\n            VERSION[0] >= 2\n        )\n        if DJANGO_111_OR_UP:\n            return super(DynamicRawIDWidget, self).render(\n                name, value, attrs, renderer=renderer\n            )\n\n        if attrs is None:\n            attrs = {}\n\n        related_url = reverse(\n            'admin:{0}_{1}_changelist'.format(\n                self.rel.to._meta.app_label,\n                self.rel.to._meta.object_name.lower(),\n            ),\n            current_app=self.admin_site.name,\n        )\n\n        params = self.url_parameters()\n        if params:\n            url = u'?' + u'&'.join(\n                [u'{0}={1}'.format(k, v) for k, v in params.items()]\n            )\n        else:\n            url = u''\n        if \"class\" not in attrs:\n            attrs[\n                'class'\n            ] = (\n                'vForeignKeyRawIdAdminField'\n            )  # The JavaScript looks for this hook.\n        app_name = self.rel.to._meta.app_label.strip()\n        model_name = self.rel.to._meta.object_name.lower().strip()\n        hidden_input = super(widgets.ForeignKeyRawIdWidget, self).render(\n            name, value, attrs\n        )\n\n        extra_context = {\n            'hidden_input': hidden_input,\n            'name': name,\n            'app_name': app_name,\n            'model_name': model_name,\n            'related_url': related_url,\n            'url': url,\n        }\n        return render_to_string(\n            'dynamic_raw_id/admin/widgets/dynamic_raw_id_field.html',\n            extra_context,\n        )", "response": "Render the dynamic raw id field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef queryset(self, request, queryset):\n        if self.form.is_valid():\n            # get no null params\n            filter_params = dict(\n                filter(lambda x: bool(x[1]), self.form.cleaned_data.items())\n            )\n            return queryset.filter(**filter_params)\n        return queryset", "response": "Filter queryset using params from the form."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecorates a control function in order to conduct an experiment when called. :param callable candidate: your candidate function :param iterable exp_args: positional arguments passed to :class:`Experiment` :param dict exp_kwargs: keyword arguments passed to :class:`Experiment` Usage:: candidate_func = lambda: True @Experiment.decorator(candidate_func) def control_func(): return True", "response": "def decorator(cls, candidate, *exp_args, **exp_kwargs):\n        '''\n        Decorate a control function in order to conduct an experiment when called.\n\n        :param callable candidate: your candidate function\n        :param iterable exp_args: positional arguments passed to :class:`Experiment`\n        :param dict exp_kwargs: keyword arguments passed to :class:`Experiment`\n\n        Usage::\n\n            candidate_func = lambda: True\n\n            @Experiment.decorator(candidate_func)\n            def control_func():\n                return True\n\n        '''\n        def wrapper(control):\n            @wraps(control)\n            def inner(*args, **kwargs):\n                experiment = cls(*exp_args, **exp_kwargs)\n                experiment.control(control, args=args, kwargs=kwargs)\n                experiment.candidate(candidate, args=args, kwargs=kwargs)\n                return experiment.conduct()\n            return inner\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the control function for the current experiment.", "response": "def control(self, control_func, args=None, kwargs=None, name='Control', context=None):\n        '''\n        Set the experiment's control function. Must be set before ``conduct()`` is called.\n\n        :param callable control_func: your control function\n        :param iterable args: positional arguments to pass to your function\n        :param dict kwargs: keyword arguments to pass to your function\n        :param string name: a name for your observation\n        :param dict context: observation-specific context\n\n        :raises LaboratoryException: If attempting to set a second control case\n        '''\n        if self._control is not None:\n            raise exceptions.LaboratoryException(\n                'You have already established a control case'\n            )\n\n        self._control = {\n            'func': control_func,\n            'args': args or [],\n            'kwargs': kwargs or {},\n            'name': name,\n            'context': context or {},\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef candidate(self, cand_func, args=None, kwargs=None, name='Candidate', context=None):\n        '''\n        Adds a candidate function to an experiment. Can be used multiple times for\n        multiple candidates.\n\n        :param callable cand_func: your control function\n        :param iterable args: positional arguments to pass to your function\n        :param dict kwargs: keyword arguments to pass to your function\n        :param string name: a name for your observation\n        :param dict context: observation-specific context\n        '''\n        self._candidates.append({\n            'func': cand_func,\n            'args': args or [],\n            'kwargs': kwargs or {},\n            'name': name,\n            'context': context or {},\n        })", "response": "Adds a candidate function to an experiment. Can be used multiple times for\n        multiple candidates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting all of the control & candidate functions and return the control s return value.", "response": "def conduct(self, randomize=True):\n        '''\n        Run control & candidate functions and return the control's return value.\n        ``control()`` must be called first.\n\n        :param bool randomize: controls whether we shuffle the order\n            of execution between control and candidate\n        :raise LaboratoryException: when no control case has been set\n        :return: Control function's return value\n        '''\n        if self._control is None:\n            raise exceptions.LaboratoryException(\n                'Your experiment must contain a control case'\n            )\n\n\n        # execute control and exit if experiment is not enabled\n        if not self.enabled():\n            control = self._run_tested_func(raise_on_exception=True, **self._control)\n            return control.value\n\n        # otherwise, let's wrap an executor around all of our functions and randomise the ordering\n\n        def get_func_executor(obs_def, is_control):\n            \"\"\"A lightweight wrapper around a tested function in order to retrieve state\"\"\"\n            return lambda *a, **kw: (self._run_tested_func(raise_on_exception=is_control, **obs_def), is_control)\n\n        funcs = [\n            get_func_executor(self._control, is_control=True),\n        ] + [get_func_executor(cand, is_control=False,) for cand in self._candidates]\n\n        if randomize:\n            random.shuffle(funcs)\n\n        control = None\n        candidates = []\n\n        # go through the randomised list and execute the functions\n        for func in funcs:\n            observation, is_control = func()\n            if is_control:\n                control = observation\n            else:\n                candidates.append(observation)\n\n        result = Result(self, control, candidates)\n\n        try:\n            self.publish(result)\n        except Exception:\n            msg = 'Exception occured when publishing %s experiment data'\n            logger.exception(msg % self.name)\n\n        return control.value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare(self, control, candidate):\n        '''\n        Compares two :class:`Observation` instances.\n\n        :param Observation control: The control block's :class:`Observation`\n        :param Observation candidate: A candidate block's :class:`Observation`\n\n        :raises MismatchException: If ``Experiment.raise_on_mismatch`` is True\n\n        :return bool: match?\n        '''\n        if candidate.failure or control.value != candidate.value:\n            return self._handle_comparison_mismatch(control, candidate)\n\n        return True", "response": "Compares two control block s elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes a byte string into a base64 - encoded version of the cashaddress.", "response": "def encode(data):\n    '''\n    bytes -> str\n    '''\n    if riemann.network.CASHADDR_PREFIX is None:\n        raise ValueError('Network {} does not support cashaddresses.'\n                         .format(riemann.get_current_network_name()))\n\n    data = convertbits(data, 8, 5)\n    checksum = calculate_checksum(riemann.network.CASHADDR_PREFIX, data)\n\n    payload = b32encode(data + checksum)\n\n    form = '{prefix}:{payload}'\n    return form.format(\n        prefix=riemann.network.CASHADDR_PREFIX,\n        payload=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(data):\n    '''\n    str -> bytes\n    '''\n    if riemann.network.CASHADDR_PREFIX is None:\n        raise ValueError('Network {} does not support cashaddresses.'\n                         .format(riemann.get_current_network_name()))\n    if data.find(riemann.network.CASHADDR_PREFIX) != 0:\n        raise ValueError('Malformed cashaddr. Cannot locate prefix: {}'\n                         .format(riemann.netowrk.CASHADDR_PREFIX))\n\n    # the data is everything after the colon\n    prefix, data = data.split(':')\n    decoded = b32decode(data)\n    if not verify_checksum(prefix, decoded):\n        raise ValueError('Bad cash address checksum')\n    converted = convertbits(decoded, 5, 8)\n\n    return bytes(converted[:-6])", "response": "Decode a cash address into a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a salt to the hash function.", "response": "def addsalt(self, salt):\n        \"\"\" adds a salt to the hash function (OPTIONAL)\n            should be called AFTER Init, and BEFORE update\n            salt:  a bytestring, length determined by hashbitlen.\n                   if not of sufficient length, the bytestring\n                   will be assumed to be a big endian number and\n                   prefixed with an appropriate number of null\n                   bytes, and if too large, only the low order\n                   bytes will be used.\n\n              if hashbitlen=224 or 256, then salt will be 16 bytes\n              if hashbitlen=384 or 512, then salt will be 32 bytes\n        \"\"\"\n        # fail if addsalt() was not called at the right time\n        if self.state != 1:\n            raise Exception('addsalt() not called after init() and before update()')\n        # salt size is to be 4x word size\n        saltsize = self.WORDBYTES * 4\n        # if too short, prefix with null bytes.  if too long,\n        # truncate high order bytes\n        if len(salt) < saltsize:\n            salt = (chr(0)*(saltsize-len(salt)) + salt)\n        else:\n            salt = salt[-saltsize:]\n        # prep the salt array\n        self.salt[0] = self.byte2int(salt[            : 4<<self.mul])\n        self.salt[1] = self.byte2int(salt[ 4<<self.mul: 8<<self.mul])\n        self.salt[2] = self.byte2int(salt[ 8<<self.mul:12<<self.mul])\n        self.salt[3] = self.byte2int(salt[12<<self.mul:            ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, data):\n        self.state = 2\n\n        BLKBYTES = self.BLKBYTES   # de-referenced for improved readability\n        BLKBITS  = self.BLKBITS\n\n        datalen = len(data)\n        if not datalen:  return\n\n        if type(data) == type(u''):\n\n            # use either of the next two lines for a proper\n            # response under both Python2 and Python3\n            data = data.encode('UTF-8')         # converts to byte string\n            #data = bytearray(data, 'utf-8')    # use if want mutable\n\n            # This next line works for Py3 but fails under\n            # Py2 because the Py2 version of bytes() will\n            # accept only *one* argument.   Arrrrgh!!!\n            #data = bytes(data, 'utf-8')        # converts to immutable byte\n                                                # string but... under p7\n                                                # bytes() wants only 1 arg\n            # ...a dummy, 2nd argument like encoding=None\n            # that does nothing would at least allow\n            # compatibility between Python2 and Python3.\n\n        left = len(self.cache)\n        fill = BLKBYTES - left\n\n        # if any cached data and any added new data will fill a\n        # full block, fill and compress\n        if left and datalen >= fill:\n            self.cache = self.cache + data[:fill]\n            self.t += BLKBITS           # update counter\n            self._compress(self.cache)\n            self.cache = b''\n            data = data[fill:]\n            datalen -= fill\n\n        # compress new data until not enough for a full block\n        while datalen >= BLKBYTES:\n            self.t += BLKBITS           # update counter\n            self._compress(data[:BLKBYTES])\n            data = data[BLKBYTES:]\n            datalen -= BLKBYTES\n\n        # cache all leftover bytes until next call to update()\n        if datalen > 0:\n            self.cache = self.cache + data[:datalen]", "response": "update the state with new data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef final(self, data=''):\n        if self.state == 3:\n            # we have already finalized so simply return the\n            # previously calculated/stored hash value\n            return self.hash\n\n        if data:\n            self.update(data)\n\n        ZZ = b'\\x00'\n        ZO = b'\\x01'\n        OZ = b'\\x80'\n        OO = b'\\x81'\n        PADDING = OZ + ZZ*128   # pre-formatted padding data\n\n        # copy nb. bits hash in total as a 64-bit BE word\n        # copy nb. bits hash in total as a 128-bit BE word\n        tt = self.t + (len(self.cache) << 3)\n        if self.BLKBYTES == 64:\n            msglen = self._int2eightByte(tt)\n        else:\n            low  = tt & self.MASK\n            high = tt >> self.WORDBITS\n            msglen = self._int2eightByte(high) + self._int2eightByte(low)\n\n        # size of block without the words at the end that count\n        # the number of bits, 55 or 111.\n        # Note: (((self.WORDBITS/8)*2)+1) equals ((self.WORDBITS>>2)+1)\n        sizewithout = self.BLKBYTES -  ((self.WORDBITS>>2)+1)\n\n        if len(self.cache) == sizewithout:\n            # special case of one padding byte\n            self.t -= 8\n            if self.hashbitlen in [224, 384]:\n                self.update(OZ)\n            else:\n                self.update(OO)\n        else:\n            if len(self.cache) < sizewithout:\n                # enough space to fill the block\n                # use t=0 if no remaining data\n                if len(self.cache) == 0:\n                    self.nullt=1\n                self.t -= (sizewithout - len(self.cache)) << 3\n                self.update(PADDING[:sizewithout - len(self.cache)])\n            else:\n                # NOT enough space, need 2 compressions\n                #   ...add marker, pad with nulls and compress\n                self.t -= (self.BLKBYTES - len(self.cache)) << 3\n                self.update(PADDING[:self.BLKBYTES - len(self.cache)])\n                #   ...now pad w/nulls leaving space for marker & bit count\n                self.t -= (sizewithout+1) << 3\n                self.update(PADDING[1:sizewithout+1]) # pad with zeroes\n\n                self.nullt = 1 # raise flag to set t=0 at the next _compress\n\n            # append a marker byte\n            if self.hashbitlen in [224, 384]:\n                self.update(ZZ)\n            else:\n                self.update(ZO)\n            self.t -= 8\n\n        # append the number of bits (long long)\n        self.t -= self.BLKBYTES\n        self.update(msglen)\n\n        hashval = []\n        if self.BLKBYTES == 64:\n            for h in self.h:\n                hashval.append(self._int2fourByte(h))\n        else:\n            for h in self.h:\n                hashval.append(self._int2eightByte(h))\n\n        self.hash  = b''.join(hashval)[:self.hashbitlen >> 3]\n        self.state = 3\n\n        return self.hash", "response": "Finalize the hash and return the hashval of the last set of data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, outpoint=None, stack_script=None,\n             redeem_script=None, sequence=None):\n        '''\n        TxIn -> TxIn\n        '''\n        return TxIn(\n            outpoint=outpoint if outpoint is not None else self.outpoint,\n            stack_script=(stack_script if stack_script is not None\n                          else self.stack_script),\n            redeem_script=(redeem_script if redeem_script is not None\n                           else self.redeem_script),\n            sequence=sequence if sequence is not None else self.sequence)", "response": "Returns a copy of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the script signature and return the stack script and redeem script.", "response": "def _parse_script_sig(TxIn, script_sig):\n        '''\n        byte_string -> (byte_string, byte_string)\n        '''\n        # Is there a better way to do this?\n        stack_script = script_sig\n        redeem_script = b''\n        try:\n            # If the last entry deserializes, it's a p2sh input\n            # There is a vanishingly small edge case where the pubkey\n            #   forms a deserializable script.\n            # Edge case: serialization errors on CODESEPARATOR\n            deserialized = serialization.deserialize(script_sig)\n            items = deserialized.split()\n            serialization.hex_deserialize(items[-1])\n            stack_script = serialization.serialize(' '.join(items[:-1]))\n            redeem_script = serialization.serialize(items[-1])\n        except (IndexError, ValueError, NotImplementedError):\n            pass\n\n        return stack_script, redeem_script"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a TxIn from a byte - like object.", "response": "def from_bytes(TxIn, byte_string):\n        '''\n        byte_string -> TxIn\n        parses a TxIn from a byte-like object\n        '''\n        outpoint = Outpoint.from_bytes(byte_string[:36])\n\n        script_sig_len = VarInt.from_bytes(byte_string[36:45])\n        script_start = 36 + len(script_sig_len)\n        script_end = script_start + script_sig_len.number\n        script_sig = byte_string[script_start:script_end]\n\n        sequence = byte_string[script_end:script_end + 4]\n        if script_sig == b'':\n            stack_script = b''\n            redeem_script = b''\n        else:\n            stack_script, redeem_script = TxIn._parse_script_sig(script_sig)\n        return TxIn(\n            outpoint=outpoint,\n            stack_script=stack_script,\n            redeem_script=redeem_script,\n            sequence=sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef no_witness(self):\n        '''\n        Tx -> bytes\n        '''\n        tx = bytes()\n        tx += self.version\n        tx += VarInt(len(self.tx_ins)).to_bytes()\n        for tx_in in self.tx_ins:\n            tx += tx_in.to_bytes()\n        tx += VarInt(len(self.tx_outs)).to_bytes()\n        for tx_out in self.tx_outs:\n            tx += tx_out.to_bytes()\n        tx += self.lock_time\n        return bytes(tx)", "response": "Return the bytes representation of the no witness."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_fee(self, input_values):\n        '''\n        Tx, list(int) -> int\n        Inputs don't know their value without the whole chain.\n        '''\n        return \\\n            sum(input_values) \\\n            - sum([utils.le2i(o.value) for o in self.tx_outs])", "response": "Calculate the fee of the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a copy of the current object.", "response": "def copy(self, version=None, flag=None, tx_ins=None,\n             tx_outs=None, tx_witnesses=None, lock_time=None):\n        '''\n        Tx, byte-like, byte-like, list(TxIn),\n        list(TxOut), list(InputWitness), byte-like -> Tx\n\n        Makes a copy. Allows over-writing specific pieces.\n        '''\n        return Tx(version=version if version is not None else self.version,\n                  flag=flag if flag is not None else self.flag,\n                  tx_ins=tx_ins if tx_ins is not None else self.tx_ins,\n                  tx_outs=tx_outs if tx_outs is not None else self.tx_outs,\n                  tx_witnesses=(tx_witnesses if tx_witnesses is not None\n                                else self.tx_witnesses),\n                  lock_time=(lock_time if lock_time is not None\n                             else self.lock_time))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sighash_single(self, index, script=None,\n                       prevout_value=None, anyone_can_pay=False):\n        '''\n        Tx, int, byte-like, byte-like, bool -> bytearray\n        Sighashes suck\n        Generates the hash to be signed with SIGHASH_SINGLE\n        https://en.bitcoin.it/wiki/OP_CHECKSIG#Procedure_for_Hashtype_SIGHASH_SINGLE\n        https://bitcoin.stackexchange.com/questions/3890/for-sighash-single-do-the-outputs-other-than-at-the-input-index-have-8-bytes-or\n        https://github.com/petertodd/python-bitcoinlib/blob/051ec4e28c1f6404fd46713c2810d4ebbed38de4/bitcoin/core/script.py#L913-L965\n        '''\n\n        if index >= len(self.tx_outs):\n            raise NotImplementedError(\n                'I refuse to implement the SIGHASH_SINGLE bug.')\n\n        if riemann.network.FORKID is not None:\n            return self._sighash_forkid(index=index,\n                                        script=script,\n                                        prevout_value=prevout_value,\n                                        sighash_type=shared.SIGHASH_SINGLE,\n                                        anyone_can_pay=anyone_can_pay)\n\n        if self.is_witness():\n            return self.segwit_sighash(\n                index=index,\n                script=script,\n                prevout_value=prevout_value,\n                sighash_type=shared.SIGHASH_SINGLE,\n                anyone_can_pay=anyone_can_pay)\n\n        copy_tx = self._sighash_prep(index=index, script=script)\n\n        # Remove outputs after the one we're signing\n        # Other tx_outs are set to -1 value and null scripts\n        copy_tx_outs = copy_tx.tx_outs[:index + 1]\n        copy_tx_outs = [TxOut(value=b'\\xff' * 8, output_script=b'')\n                        for _ in copy_tx.tx_ins]  # Null them all\n        copy_tx_outs[index] = copy_tx.tx_outs[index]  # Fix the current one\n\n        # Other tx_ins sequence numbers are set to 0\n        copy_tx_ins = [tx_in.copy(sequence=b'\\x00\\x00\\x00\\x00')\n                       for tx_in in copy_tx.tx_ins]  # Set all to 0\n        copy_tx_ins[index] = copy_tx.tx_ins[index]  # Fix the current one\n\n        copy_tx = copy_tx.copy(\n            tx_ins=copy_tx_ins,\n            tx_outs=copy_tx_outs)\n\n        if anyone_can_pay:  # Forward onwards\n            return self._sighash_anyone_can_pay(\n                index, copy_tx, shared.SIGHASH_SINGLE)\n\n        return self._sighash_final_hashing(copy_tx, shared.SIGHASH_SINGLE)", "response": "This function generates the SIGHASH_SINGLE signature for a single input."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the final hash of the object.", "response": "def _sighash_final_hashing(self, copy_tx, sighash_type):\n        '''\n        Tx, int -> bytes\n        Returns the hash that should be signed\n        https://en.bitcoin.it/wiki/OP_CHECKSIG#Procedure_for_Hashtype_SIGHASH_ANYONECANPAY\n        '''\n        sighash = ByteData()\n        sighash += copy_tx.to_bytes()\n        sighash += utils.i2le_padded(sighash_type, 4)\n\n        return utils.hash256(sighash.to_bytes())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _adjusted_script_code(self, script):\n        '''\n        Checks if the script code pased in to the sighash function is already\n        length-prepended\n        This will break if there's a redeem script that's just a pushdata\n        That won't happen in practice\n\n        Args:\n            script (bytes): the spend script\n        Returns:\n            (bytes): the length-prepended script (if necessary)\n        '''\n        script_code = ByteData()\n        if script[0] == len(script) - 1:\n            return script\n        script_code += VarInt(len(script))\n        script_code += script\n        return script_code", "response": "Adjusts the script code to be used by the sighash function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sighash_single(self, index, script=None,\n                       anyone_can_pay=False):\n        '''\n        https://github.com/decred/dcrd/blob/master/txscript/script.go\n        '''\n        copy_tx = self._sighash_prep(\n            index=index,\n            script=script)\n\n        try:\n            copy_tx_outs = copy_tx.tx_outs[:index + 1]\n            copy_tx_outs = [TxOut(value=b'\\xff' * 8, output_script=b'')\n                            for _ in copy_tx.tx_ins]\n            copy_tx_outs[index] = copy_tx.tx_outs[index]\n        except IndexError:\n            raise NotImplementedError(\n                'I refuse to implement the SIGHASH_SINGLE bug.')\n\n        copy_tx_ins = [tx_in.copy(sequence=b'\\x00\\x00\\x00\\x00')\n                       for tx_in in copy_tx.tx_ins]\n        copy_tx_ins[index] = copy_tx.tx_ins[index]\n        copy_tx = copy_tx.copy(tx_ins=copy_tx_ins, tx_outs=copy_tx_outs)\n\n        if anyone_can_pay:\n            return self._sighash_anyone_can_pay(\n                index=index,\n                copy_tx=copy_tx,\n                sighash_type=shared.SIGHASH_SINGLE)\n\n        return self._sighash_final_hashing(\n            index=index,\n            copy_tx=copy_tx,\n            sighash_type=shared.SIGHASH_SINGLE)", "response": "This function sighash a single entry in the DCRD."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sighash_all(self, index, script=None, anyone_can_pay=False):\n        '''\n        https://gist.github.com/davecgh/b00ec6e11f73620c3deddf160353961c\n        https://github.com/decred/dcrd/blob/master/txscript/script.go\n        '''\n        copy_tx = self._sighash_prep(index, script)\n\n        if anyone_can_pay:\n            return self._sighash_anyone_can_pay(\n                index=index,\n                copy_tx=copy_tx,\n                sighash_type=shared.SIGHASH_ALL)\n\n        return self._sighash_final_hashing(\n            index=index,\n            copy_tx=copy_tx,\n            sighash_type=shared.SIGHASH_ALL)", "response": "Sighash all the related objects in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a script string into a bytearray.", "response": "def serialize(script_string):\n    '''\n    str -> bytearray\n    '''\n    string_tokens = script_string.split()\n    serialized_script = bytearray()\n\n    for token in string_tokens:\n        if token == 'OP_CODESEPARATOR' or token == 'OP_PUSHDATA4':\n            raise NotImplementedError('{} is a bad idea.'.format(token))\n\n        if token in riemann.network.CODE_TO_INT_OVERWRITE:\n            serialized_script.extend(\n                [riemann.network.CODE_TO_INT_OVERWRITE[token]])\n\n        elif token in CODE_TO_INT:\n            serialized_script.extend([CODE_TO_INT[token]])\n\n        else:\n            token_bytes = bytes.fromhex(token)\n\n            if len(token_bytes) <= 75:\n                op = 'OP_PUSH_{}'.format(len(token_bytes))\n                serialized_script.extend([CODE_TO_INT[op]])\n                serialized_script.extend(token_bytes)\n\n            elif len(token_bytes) > 75 and len(token_bytes) <= 255:\n                op = 'OP_PUSHDATA1'\n                serialized_script.extend([CODE_TO_INT[op]])\n                serialized_script.extend(utils.i2le(len(token_bytes)))\n                serialized_script.extend(token_bytes)\n\n            elif len(token_bytes) > 255 and len(token_bytes) <= 1000:\n                op = 'OP_PUSHDATA2'\n                serialized_script.extend([CODE_TO_INT[op]])\n                serialized_script.extend(\n                    utils.i2le_padded(len(token_bytes), 2))\n                serialized_script.extend(token_bytes)\n\n            else:\n                raise NotImplementedError(\n                    'Hex string too long to serialize.')\n\n    return serialized_script"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deserialize(serialized_script):\n    '''\n    bytearray -> str\n    '''\n    deserialized = []\n    i = 0\n    while i < len(serialized_script):\n        current_byte = serialized_script[i]\n        if current_byte == 0xab:\n            raise NotImplementedError('OP_CODESEPARATOR is a bad idea.')\n        if current_byte <= 75 and current_byte != 0:\n\n            deserialized.append(\n                serialized_script[i + 1: i + 1 + current_byte].hex())\n\n            i += 1 + current_byte\n            if i > len(serialized_script):\n                raise IndexError(\n                    'Push {} caused out of bounds exception.'\n                    .format(current_byte))\n\n        elif current_byte == 76:\n            # next hex blob length\n            blob_len = serialized_script[i + 1]\n\n            deserialized.append(\n                serialized_script[i + 2: i + 2 + blob_len].hex())\n\n            i += 2 + blob_len\n\n        elif current_byte == 77:\n            # next hex blob length\n            blob_len = utils.le2i(serialized_script[i + 1: i + 3])\n\n            deserialized.append(\n                serialized_script[i + 3: i + 3 + blob_len].hex())\n\n            i += 3 + blob_len\n\n        elif current_byte == 78:\n            raise NotImplementedError('OP_PUSHDATA4 is a bad idea.')\n\n        else:\n            if current_byte in riemann.network.INT_TO_CODE_OVERWRITE:\n                deserialized.append(\n                    riemann.network.INT_TO_CODE_OVERWRITE[current_byte])\n            elif current_byte in INT_TO_CODE:\n                deserialized.append(INT_TO_CODE[current_byte])\n            else:\n                raise ValueError(\n                    'Unsupported opcode. '\n                    'Got 0x%x' % serialized_script[i])\n            i += 1\n\n    return ' '.join(deserialized)", "response": "Deserialize a bytearray into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _hsig_input(self, index):\n        '''\n        inputs for the hsig hash\n        '''\n        hsig_input = z.ZcashByteData()\n        hsig_input += self.tx_joinsplits[index].random_seed\n        hsig_input += self.tx_joinsplits[index].nullifiers\n        hsig_input += self.joinsplit_pubkey\n        return hsig_input.to_bytes()", "response": "Returns the hsig input for the hsig hash\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _primary_input(self, index):\n        '''\n        Primary input for the zkproof\n        '''\n        primary_input = z.ZcashByteData()\n        primary_input += self.tx_joinsplits[index].anchor\n        primary_input += self.tx_joinsplits[index].nullifiers\n        primary_input += self.tx_joinsplits[index].commitments\n        primary_input += self.tx_joinsplits[index].vpub_old\n        primary_input += self.tx_joinsplits[index].vpub_new\n        primary_input += self.hsigs[index]\n        primary_input += self.tx_joinsplits[index].vmacs\n        return primary_input.to_bytes()", "response": "Return the Zcash byte data for the zkproof\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_bytes(SproutTx, byte_string):\n        '''\n        byte-like -> SproutTx\n        '''\n        version = byte_string[0:4]\n        tx_ins = []\n        tx_ins_num = shared.VarInt.from_bytes(byte_string[4:])\n\n        current = 4 + len(tx_ins_num)\n        for _ in range(tx_ins_num.number):\n            tx_in = TxIn.from_bytes(byte_string[current:])\n            current += len(tx_in)\n            tx_ins.append(tx_in)\n\n        tx_outs = []\n        tx_outs_num = shared.VarInt.from_bytes(byte_string[current:])\n\n        current += len(tx_outs_num)\n        for _ in range(tx_outs_num.number):\n            tx_out = TxOut.from_bytes(byte_string[current:])\n            current += len(tx_out)\n            tx_outs.append(tx_out)\n\n        lock_time = byte_string[current:current + 4]\n        current += 4\n\n        tx_joinsplits = None\n        joinsplit_pubkey = None\n        joinsplit_sig = None\n        if utils.le2i(version) == 2:  # If we expect joinsplits\n            tx_joinsplits = []\n            tx_joinsplits_num = shared.VarInt.from_bytes(byte_string[current:])\n            current += len(tx_joinsplits_num)\n\n            for _ in range(tx_joinsplits_num.number):\n                joinsplit = z.SproutJoinsplit.from_bytes(byte_string[current:])\n                current += len(joinsplit)\n                tx_joinsplits.append(joinsplit)\n            joinsplit_pubkey = byte_string[current:current + 32]\n            current += 32\n            joinsplit_sig = byte_string[current:current + 64]\n\n        return SproutTx(\n            version=version,\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=lock_time,\n            tx_joinsplits=tx_joinsplits,\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig)", "response": "Convert byte - like object to SproutTx object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_fee(self, input_values):\n        '''\n        Tx, list(int) -> int\n        '''\n        total_in = sum(input_values)\n        total_out = sum([utils.le2i(tx_out.value) for tx_out in self.tx_outs])\n        for js in self.tx_joinsplits:\n            total_in += utils.le2i(js.vpub_new)\n            total_out += utils.le2i(js.vpub_old)\n        return total_in - total_out", "response": "Calculate the fee of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a copy of the current instance.", "response": "def copy(self, version=None, tx_ins=None, tx_outs=None, lock_time=None,\n             tx_joinsplits=None, joinsplit_pubkey=None, joinsplit_sig=None):\n        '''\n        SproutTx, ... -> Tx\n\n        Makes a copy. Allows over-writing specific pieces.\n        '''\n        return SproutTx(\n            version=version if version is not None else self.version,\n            tx_ins=tx_ins if tx_ins is not None else self.tx_ins,\n            tx_outs=tx_outs if tx_outs is not None else self.tx_outs,\n            lock_time=(lock_time if lock_time is not None\n                       else self.lock_time),\n            tx_joinsplits=(tx_joinsplits if tx_joinsplits is not None\n                           else self.tx_joinsplits),\n            joinsplit_pubkey=(joinsplit_pubkey if joinsplit_pubkey is not None\n                              else self.joinsplit_pubkey),\n            joinsplit_sig=(joinsplit_sig if joinsplit_sig is not None\n                           else self.joinsplit_sig))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the sighash for the current transaction.", "response": "def _sighash_prep(self, index, script):\n        '''\n        SproutTx, int, byte-like -> SproutTx\n        Sighashes suck\n        Performs the sighash setup described here:\n        https://en.bitcoin.it/wiki/OP_CHECKSIG#How_it_works\n        https://bitcoin.stackexchange.com/questions/3374/how-to-redeem-a-basic-tx\n        We save on complexity by refusing to support OP_CODESEPARATOR\n        '''\n\n        if len(self.tx_ins) == 0:\n            return self.copy(joinsplit_sig=b'')\n        # 0 out scripts in tx_ins\n        copy_tx_ins = [tx_in.copy(stack_script=b'', redeem_script=b'')\n                       for tx_in in self.tx_ins]\n\n        # NB: The script for the current transaction input in txCopy is set to\n        #     subScript (lead in by its length as a var-integer encoded!)\n        copy_tx_ins[index] = \\\n            copy_tx_ins[index].copy(stack_script=b'', redeem_script=script)\n\n        return self.copy(tx_ins=copy_tx_ins, joinsplit_sig=b'')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the final hash of the sighash_type.", "response": "def _sighash_final_hashing(self, copy_tx, sighash_type):\n        '''\n        SproutTx, int -> bytes\n        Returns the hash that should be signed\n        https://en.bitcoin.it/wiki/OP_CHECKSIG#Procedure_for_Hashtype_SIGHASH_ANYONECANPAY\n        '''\n        sighash = z.ZcashByteData()\n        sighash += copy_tx.to_bytes()\n        sighash += utils.i2le_padded(sighash_type, 4)\n        return utils.hash256(sighash.to_bytes())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode a segwit address.", "response": "def segwit_encode(hrp, witver, witprog):\n    \"\"\"Encode a segwit address.\"\"\"\n    ret = bech32_encode(hrp, [witver] + convertbits(witprog, 8, 5))\n    if segwit_decode(hrp, ret) == (None, None):\n        return None\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_immutable(self):\n        '''\n        Prevents any future changes to the object\n        '''\n        self._bytes = bytes(self._bytes)\n        self.__immutable = True", "response": "Prevents any future changes to the object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find(self, substring):\n        '''\n        byte-like -> int\n        Finds the index of substring\n        '''\n        if isinstance(substring, ByteData):\n            substring = substring.to_bytes()\n        return self._bytes.find(substring)", "response": "Find the index of substring in this array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that data is a byte - like object.", "response": "def validate_bytes(data, length=4):\n        '''\n        Raises ValueError if data is not bytes.\n        Raises ValueError if len(data) is not length.\n        Length may be None for unknown lengths (e.g. scripts).\n        length=None will allow 0 length data.\n        '''\n        if (not isinstance(data, ByteData)\n                and not isinstance(data, bytes)\n                and not isinstance(data, bytearray)):\n            raise ValueError('Expected byte-like object. '\n                             'Got: {}'.format(type(data)))\n\n        if length is None:\n            return\n\n        if len(data) != length:\n            raise ValueError('Expected byte-like object with length {}. '\n                             'Got {} with length {}.'\n                             .format(length, type(data), len(data)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a byte - like object and returns a VarInt object.", "response": "def from_bytes(VarInt, byte_string):\n        '''\n        byte-like -> VarInt\n        accepts arbitrary length input, gets a VarInt off the front\n        '''\n        num = byte_string\n        if num[0] <= 0xfc:\n            num = num[0:1]\n            non_compact = False\n        elif num[0] == 0xfd:\n            num = num[1:3]\n            non_compact = (num[-1:] == b'\\x00')\n        elif num[0] == 0xfe:\n            num = num[1:5]\n            non_compact = (num[-2:] == b'\\x00\\x00')\n        elif num[0] == 0xff:\n            num = num[1:9]\n            non_compact = (num[-4:] == b'\\x00\\x00\\x00\\x00')\n        if len(num) not in [1, 2, 4, 8]:\n            raise ValueError('Malformed VarInt. Got: {}'\n                             .format(byte_string.hex()))\n\n        if (non_compact\n            and ('overwinter' in riemann.get_current_network_name()\n                 or 'sapling' in riemann.get_current_network_name())):\n            raise ValueError('VarInt must be compact. Got: {}'\n                             .format(byte_string.hex()))\n\n        ret = VarInt(\n            utils.le2i(num),\n            length=len(num) + 1 if non_compact else 0)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the fee of the current state of the cluster.", "response": "def calculate_fee(self, input_values):\n        '''\n        SaplingTx, list(int) -> int\n        '''\n        total_in = sum(input_values)\n        total_out = sum([utils.le2i(tx_out.value) for tx_out in self.tx_outs])\n        shileded_net = utils.le2i(self.value_balance, signed=True)\n        for js in self.tx_joinsplits:\n            total_in += utils.le2i(js.vpub_new)\n            total_out += utils.le2i(js.vpub_old)\n        return total_in - total_out + shileded_net"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, tx_ins=None, tx_outs=None, lock_time=None,\n             expiry_height=None, value_balance=None, tx_shielded_spends=None,\n             tx_shielded_outputs=None, tx_joinsplits=None,\n             joinsplit_pubkey=None, joinsplit_sig=None, binding_sig=None):\n        '''\n        SaplingTx, ... -> SaplingTx\n\n        Makes a copy. Allows over-writing specific pieces.\n        '''\n        return SaplingTx(\n            tx_ins=tx_ins if tx_ins is not None else self.tx_ins,\n            tx_outs=tx_outs if tx_outs is not None else self.tx_outs,\n            lock_time=(lock_time if lock_time is not None\n                       else self.lock_time),\n            expiry_height=(expiry_height if expiry_height is not None\n                           else self.expiry_height),\n            value_balance=(value_balance if value_balance is not None\n                           else self.value_balance),\n            tx_shielded_spends=(\n                tx_shielded_spends if tx_shielded_spends is not None\n                else self.tx_shielded_spends),\n            tx_shielded_outputs=(\n                tx_shielded_outputs if tx_shielded_outputs is not None\n                else self.tx_shielded_outputs),\n            tx_joinsplits=(tx_joinsplits if tx_joinsplits is not None\n                           else self.tx_joinsplits),\n            joinsplit_pubkey=(joinsplit_pubkey if joinsplit_pubkey is not None\n                              else self.joinsplit_pubkey),\n            joinsplit_sig=(joinsplit_sig if joinsplit_sig is not None\n                           else self.joinsplit_sig),\n            binding_sig=(binding_sig if binding_sig is not None\n                         else self.binding_sig))", "response": "Makes a copy of the current SaplingTx."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a byte - like sequence of bytes into a SaplingTx object.", "response": "def from_bytes(SaplingTx, byte_string):\n        '''\n        byte-like -> SaplingTx\n        '''\n        header = byte_string[0:4]\n        group_id = byte_string[4:8]\n\n        if header != b'\\x04\\x00\\x00\\x80' or group_id != b'\\x85\\x20\\x2f\\x89':\n            raise ValueError(\n                'Bad header or group ID. Expected {} and {}. Got: {} and {}'\n                .format(b'\\x04\\x00\\x00\\x80'.hex(),\n                        b'\\x85\\x20\\x2f\\x89'.hex(),\n                        header.hex(),\n                        group_id.hex()))\n\n        tx_ins = []\n        tx_ins_num = shared.VarInt.from_bytes(byte_string[8:])\n\n        current = 8 + len(tx_ins_num)\n        for _ in range(tx_ins_num.number):\n            tx_in = TxIn.from_bytes(byte_string[current:])\n            current += len(tx_in)\n            tx_ins.append(tx_in)\n\n        tx_outs = []\n        tx_outs_num = shared.VarInt.from_bytes(byte_string[current:])\n\n        current += len(tx_outs_num)\n        for _ in range(tx_outs_num.number):\n            tx_out = TxOut.from_bytes(byte_string[current:])\n            current += len(tx_out)\n            tx_outs.append(tx_out)\n\n        lock_time = byte_string[current:current + 4]\n        current += 4\n        expiry_height = byte_string[current:current + 4]\n        current += 4\n        value_balance = byte_string[current:current + 8]\n        current += 8\n\n        tx_shielded_spends = []\n        shielded_spends_num = shared.VarInt.from_bytes(byte_string[current:])\n\n        current += len(shielded_spends_num)\n        for _ in range(shielded_spends_num.number):\n            ss = SaplingShieldedSpend.from_bytes(byte_string[current:])\n            current += len(ss)\n            tx_shielded_spends.append(ss)\n\n        tx_shielded_outputs = []\n        shielded_outputs_num = shared.VarInt.from_bytes(byte_string[current:])\n\n        current += len(shielded_outputs_num)\n        for _ in range(shielded_outputs_num.number):\n            so = SaplingShieldedOutput.from_bytes(byte_string[current:])\n            current += len(so)\n            tx_shielded_outputs.append(so)\n\n        tx_joinsplits = []\n        tx_joinsplits_num = shared.VarInt.from_bytes(byte_string[current:])\n        current += len(tx_outs_num)\n        for _ in range(tx_joinsplits_num.number):\n            tx_joinsplit = SaplingJoinsplit.from_bytes(\n                byte_string[current:])\n            current += len(tx_joinsplit)\n            tx_joinsplits.append(tx_joinsplit)\n\n        if len(tx_joinsplits) > 0:\n            joinsplit_pubkey = byte_string[current:current + 32]\n            current += 32\n            joinsplit_sig = byte_string[current:current + 64]\n            current += 64\n        else:\n            joinsplit_pubkey = None\n            joinsplit_sig = None\n\n        if len(tx_shielded_spends) + len(tx_shielded_outputs) > 0:\n            binding_sig = byte_string[current:current + 64]\n            current += 64\n        else:\n            binding_sig = None\n\n        return SaplingTx(\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=lock_time,\n            expiry_height=expiry_height,\n            value_balance=value_balance,\n            tx_shielded_spends=tx_shielded_spends,\n            tx_shielded_outputs=tx_shielded_outputs,\n            tx_joinsplits=tx_joinsplits,\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig,\n            binding_sig=binding_sig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhash a single entry in the zip file.", "response": "def sighash(self, sighash_type, index=0, joinsplit=False, script_code=None,\n                anyone_can_pay=False, prevout_value=None):\n        '''\n        ZIP243\n        https://github.com/zcash/zips/blob/master/zip-0243.rst\n        '''\n\n        if joinsplit and anyone_can_pay:\n            raise ValueError('ANYONECANPAY can\\'t be used with joinsplits')\n\n        data = z.ZcashByteData()\n\n        data += self.header\n        data += self.group_id\n\n        data += self._hash_prevouts(anyone_can_pay)\n        data += self._hash_sequence(sighash_type, anyone_can_pay)\n        data += self._hash_outputs(sighash_type, index)\n        data += self._hash_joinsplits()\n        data += self._hash_shielded_spends()\n        data += self._hash_shielded_outputs()\n\n        data += self.lock_time\n        data += self.expiry_height\n        data += self.value_balance\n\n        if anyone_can_pay:\n            sighash_type = sighash_type | shared.SIGHASH_ANYONECANPAY\n        data += utils.i2le_padded(sighash_type, 4)\n\n        if not joinsplit:\n            data += self.tx_ins[index].outpoint\n            data += script_code\n            data += prevout_value\n            data += self.tx_ins[index].sequence\n\n        return utils.blake2b(\n            data=data.to_bytes(),\n            digest_size=32,\n            person=b'ZcashSigHash' + bytes.fromhex('bb09b876'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert binary to base58 using BASE58_ALPHABET.", "response": "def encode(data, checksum=True):\n    \"\"\"Convert binary to base58 using BASE58_ALPHABET.\"\"\"\n\n    if checksum:\n        data = data + utils.hash256(data)[:4]\n    v, prefix = to_long(256, lambda x: x, iter(data))\n    data = from_long(v, prefix, BASE58_BASE, lambda v: BASE58_ALPHABET[v])\n    return data.decode(\"utf8\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode(s, checksum=True):\n    v, prefix = to_long(\n        BASE58_BASE, lambda c: BASE58_LOOKUP[c], s.encode(\"utf8\"))\n\n    data = from_long(v, prefix, 256, lambda x: x)\n\n    if checksum:\n        data, the_hash = data[:-4], data[-4:]\n        if utils.hash256(data)[:4] == the_hash:\n            return data\n        raise ValueError(\"hashed base58 has bad checksum %s\" % s)\n\n    return data", "response": "Convert base58 to binary using BASE58_ALPHABET."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_long(v, prefix, base, charset):\n    ba = bytearray()\n    while v > 0:\n        try:\n            v, mod = divmod(v, base)\n            ba.append(charset(mod))\n        except Exception:\n            raise ValueError(\n                \"can't convert to character corresponding to %d\" % mod)\n    ba.extend([charset(0)] * prefix)\n    ba.reverse()\n    return bytes(ba)", "response": "The inverse of to_long. Convert an integer to an arbitrary base."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an array to a ( possibly bignum ) integer along with a prefix value that is prefixed with 0.", "response": "def to_long(base, lookup_f, s):\n    \"\"\"\n    Convert an array to a (possibly bignum) integer, along with a prefix value\n    of how many prefixed zeros there are.\n    base:\n        the source base\n    lookup_f:\n        a function to convert an element of s to a value between 0 and base-1.\n    s:\n        the value to convert\n    \"\"\"\n    prefix = 0\n    v = 0\n    for c in s:\n        v *= base\n        try:\n            v += lookup_f(c)\n        except Exception:\n            raise ValueError(\"bad character %s in string %s\" % (c, s))\n        if v == 0:\n            prefix += 1\n    return v, prefix"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a script from a string.", "response": "def make_sh_output_script(script_string, witness=False):\n    '''\n    str -> bytearray\n    '''\n    if witness and not riemann.network.SEGWIT:\n        raise ValueError(\n            'Network {} does not support witness scripts.'\n            .format(riemann.get_current_network_name()))\n\n    script_bytes = serialization.serialize(script_string)\n    return make_sh_script_pubkey(script_bytes=script_bytes, witness=witness)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a P2PKH output script from a public key.", "response": "def make_pkh_output_script(pubkey, witness=False):\n    '''\n    bytearray -> bytearray\n    '''\n    if witness and not riemann.network.SEGWIT:\n        raise ValueError(\n            'Network {} does not support witness scripts.'\n            .format(riemann.get_current_network_name()))\n\n    output_script = bytearray()\n\n    if type(pubkey) is not bytearray and type(pubkey) is not bytes:\n        raise ValueError('Unknown pubkey format. '\n                         'Expected bytes. Got: {}'.format(type(pubkey)))\n\n    pubkey_hash = utils.hash160(pubkey)\n\n    if witness:\n        output_script.extend(riemann.network.P2WPKH_PREFIX)\n        output_script.extend(pubkey_hash)\n    else:\n        output_script.extend(b'\\x76\\xa9\\x14')  # OP_DUP OP_HASH160 PUSH14\n        output_script.extend(pubkey_hash)\n        output_script.extend(b'\\x88\\xac')  # OP_EQUALVERIFY OP_CHECKSIG\n    return output_script"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_output(value, output_script, version=None):\n    '''\n    byte-like, byte-like -> TxOut\n    '''\n    if 'decred' in riemann.get_current_network_name():\n        return tx.DecredTxOut(\n            value=value,\n            version=version,\n            output_script=output_script)\n    return tx.TxOut(value=value, output_script=output_script)", "response": "Make an output from a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_sh_output(value, output_script, witness=False):\n    '''\n    int, str -> TxOut\n    '''\n    return _make_output(\n        value=utils.i2le_padded(value, 8),\n        output_script=make_sh_output_script(output_script, witness))", "response": "Make a TxOut using the passed value and output script."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_pkh_output(value, pubkey, witness=False):\n    '''\n    int, bytearray -> TxOut\n    '''\n    return _make_output(\n        value=utils.i2le_padded(value, 8),\n        output_script=make_pkh_output_script(pubkey, witness))", "response": "Make a TxOut containing a PKH value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_op_return_output(data):\n    '''Generates OP_RETURN output for data less than 78 bytes.\n    If data is 76 or 77 bytes, OP_PUSHDATA1 is included:\n    <OP_RETURN><OP_PUSHDATA1><data len><data>\n    If data is less than 76 bytes, OP_PUSHDATA1 is not included:\n    <OP_RETURN><data len><data>\n    80 bytes is the default setting for an OP_RETURN output script.\n    https://github.com/bitpay/bitcore/issues/1389\n    Args:\n        data    (bytes):    data included in output\n    Returns:\n        (TxOut):            TxOut object with OP_RETURN output\n    '''\n    if len(data) > 77:  # 77 bytes is the limit\n        raise ValueError('Data is too long. Expected <= 77 bytes')\n\n    pk_script = bytearray()\n    pk_script.extend(b'\\x6a')       # OP_RETURN\n\n    # OP_PUSHDATA1 only used if data is greater than 75 bytes\n    if len(data) in [76, 77]:\n        pk_script.extend(b'\\x4c')  # OP_PUSHDATA1\n\n    pk_script.extend([len(data)])  # One byte for length of data\n    pk_script.extend(data)         # Data\n    return _make_output(utils.i2le_padded(0, 8), pk_script)", "response": "Generates an OP_RETURN output for the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_outpoint(tx_id_le, index, tree=None):\n    '''\n    byte-like, int, int -> Outpoint\n    '''\n    if 'decred' in riemann.get_current_network_name():\n        return tx.DecredOutpoint(tx_id=tx_id_le,\n                                 index=utils.i2le_padded(index, 4),\n                                 tree=utils.i2le_padded(tree, 1))\n    return tx.Outpoint(tx_id=tx_id_le,\n                       index=utils.i2le_padded(index, 4))", "response": "Make an outpoint from a byte - like index and tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_script_sig(stack_script, redeem_script):\n    '''\n    str, str -> bytearray\n    '''\n    stack_script += ' {}'.format(\n        serialization.hex_serialize(redeem_script))\n    return serialization.serialize(stack_script)", "response": "Make a script signature from a stack script and a redeem script."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a legacy TxIn object from the given outpoint stack script redeem script and sequence.", "response": "def make_legacy_input(outpoint, stack_script, redeem_script, sequence):\n    '''\n    Outpoint, byte-like, byte-like, int -> TxIn\n    '''\n    if 'decred' in riemann.get_current_network_name():\n        return tx.DecredTxIn(\n            outpoint=outpoint,\n            sequence=utils.i2le_padded(sequence, 4))\n    return tx.TxIn(outpoint=outpoint,\n                   stack_script=stack_script,\n                   redeem_script=redeem_script,\n                   sequence=utils.i2le_padded(sequence, 4))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a legacy input and empty witness.", "response": "def make_legacy_input_and_empty_witness(outpoint, stack_script,\n                                        redeem_script, sequence):\n    '''\n    Outpoint, byte-like, byte-like, int -> (TxIn, InputWitness)\n    '''\n    return (make_legacy_input(outpoint=outpoint,\n                              stack_script=stack_script,\n                              redeem_script=redeem_script,\n                              sequence=sequence),\n            make_empty_witness())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a witness input.", "response": "def make_witness_input(outpoint, sequence):\n    '''\n    Outpoint, int -> TxIn\n    '''\n    if 'decred' in riemann.get_current_network_name():\n        return tx.DecredTxIn(\n            outpoint=outpoint,\n            sequence=utils.i2le_padded(sequence, 4))\n    return tx.TxIn(outpoint=outpoint,\n                   stack_script=b'',\n                   redeem_script=b'',\n                   sequence=utils.i2le_padded(sequence, 4))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new Tx object from the given data.", "response": "def make_tx(version, tx_ins, tx_outs, lock_time,\n            expiry=None, value_balance=0, tx_shielded_spends=None,\n            tx_shielded_outputs=None, tx_witnesses=None, tx_joinsplits=None,\n            joinsplit_pubkey=None, joinsplit_sig=None, binding_sig=None):\n    '''\n    int, list(TxIn), list(TxOut), int, list(InputWitness) -> Tx\n    '''\n    n = riemann.get_current_network_name()\n    if 'decred' in n:\n        return tx.DecredTx(\n            version=utils.i2le_padded(version, 4),\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=utils.i2le_padded(lock_time, 4),\n            expiry=utils.i2le_padded(expiry, 4),\n            tx_witnesses=[tx_witnesses])\n    if 'sprout' in n and tx_joinsplits is not None:\n        return tx.SproutTx(\n            version=version,\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=utils.i2le_padded(lock_time, 4),\n            tx_joinsplits=tx_joinsplits if tx_joinsplits is not None else [],\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig)\n    if 'overwinter' in n:\n        return tx.OverwinterTx(\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=utils.i2le_padded(lock_time, 4),\n            expiry_height=utils.i2le_padded(expiry, 4),\n            tx_joinsplits=tx_joinsplits if tx_joinsplits is not None else [],\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig)\n    if 'sapling' in n:\n        return tx.SaplingTx(\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=utils.i2le_padded(lock_time, 4),\n            expiry_height=utils.i2le_padded(expiry, 4),\n            value_balance=utils.i2le_padded(value_balance, 8),\n            tx_shielded_spends=(tx_shielded_spends\n                                if tx_shielded_spends is not None else []),\n            tx_shielded_outputs=(tx_shielded_outputs\n                                 if tx_shielded_outputs is not None else []),\n            tx_joinsplits=tx_joinsplits if tx_joinsplits is not None else [],\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig,\n            binding_sig=binding_sig)\n    flag = riemann.network.SEGWIT_TX_FLAG \\\n        if tx_witnesses is not None else None\n    return tx.Tx(version=utils.i2le_padded(version, 4),\n                 flag=flag,\n                 tx_ins=tx_ins,\n                 tx_outs=tx_outs,\n                 tx_witnesses=tx_witnesses,\n                 lock_time=utils.i2le_padded(lock_time, 4))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the length of the given byte string before the current byte string.", "response": "def length_prepend(byte_string):\n    '''\n    bytes -> bytes\n    '''\n    length = tx.VarInt(len(byte_string))\n    return length.to_bytes() + byte_string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef i2le_script(number):\n    '''Convert int to signed little endian (l.e.) hex for scripts\n    Args:\n        number  (int): int value to convert to bytes in l.e. format\n    Returns:\n                (str): the hex-encoded signed LE number\n    '''\n    if number == 0:\n        return '00'\n    for i in range(80):\n        try:\n            return number.to_bytes(\n                length=i,  # minimal bytes lol\n                byteorder='little',\n                signed=True).hex()\n        except Exception:\n            continue", "response": "Convert int to signed little endian hex for scripts\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rmd160(msg_bytes):\n    '''\n    byte-like -> bytes\n    '''\n    h = hashlib.new('ripemd160')\n    h.update(msg_bytes)\n    return h.digest()", "response": "Returns the RIPEMD160 hash of the given bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the hash of the message bytes.", "response": "def hash160(msg_bytes):\n    '''\n    byte-like -> bytes\n    '''\n    h = hashlib.new('ripemd160')\n    if 'decred' in riemann.get_current_network_name():\n        h.update(blake256(msg_bytes))\n        return h.digest()\n    h.update(sha256(msg_bytes))\n    return h.digest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hash256(msg_bytes):\n    '''\n    byte-like -> bytes\n    '''\n    if 'decred' in riemann.get_current_network_name():\n        return blake256(blake256(msg_bytes))\n    return hashlib.sha256(hashlib.sha256(msg_bytes).digest()).digest()", "response": "Return the hash of the message bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a script hash to a SHA - 1 address.", "response": "def _hash_to_sh_address(script_hash, witness=False, cashaddr=True):\n    '''\n    bytes, bool, bool -> str\n    cashaddrs are preferred where possible\n    but cashaddr is ignored in most cases\n    is there a better way to structure this?\n    '''\n    addr_bytes = bytearray()\n    if riemann.network.CASHADDR_P2SH is not None and cashaddr:\n        addr_bytes.extend(riemann.network.CASHADDR_P2SH)\n        addr_bytes.extend(script_hash)\n        return riemann.network.CASHADDR_ENCODER.encode(addr_bytes)\n    if witness:\n        addr_bytes.extend(riemann.network.P2WSH_PREFIX)\n        addr_bytes.extend(script_hash)\n        return riemann.network.SEGWIT_ENCODER.encode(addr_bytes)\n    else:\n        addr_bytes.extend(riemann.network.P2SH_PREFIX)\n        addr_bytes.extend(script_hash)\n        return riemann.network.LEGACY_ENCODER.encode(addr_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking an p2sh address from a serialized script", "response": "def _ser_script_to_sh_address(script_bytes, witness=False, cashaddr=True):\n    '''\n    makes an p2sh address from a serialized script\n    '''\n    if witness:\n        script_hash = utils.sha256(script_bytes)\n    else:\n        script_hash = utils.hash160(script_bytes)\n    return _hash_to_sh_address(\n        script_hash=script_hash,\n        witness=witness,\n        cashaddr=cashaddr)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a script string to a SHA - 1 address.", "response": "def make_sh_address(script_string, witness=False, cashaddr=True):\n    '''\n    str, bool, bool -> str\n    '''\n    script_bytes = script_ser.serialize(script_string)\n\n    return _ser_script_to_sh_address(\n        script_bytes=script_bytes,\n        witness=witness,\n        cashaddr=cashaddr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_pkh_address(pubkey_hash, witness=False, cashaddr=True):\n    '''\n    bytes, bool -> str\n    '''\n    addr_bytes = bytearray()\n    if riemann.network.CASHADDR_P2PKH is not None and cashaddr:\n        addr_bytes.extend(riemann.network.CASHADDR_P2PKH)\n        addr_bytes.extend(pubkey_hash)\n        return riemann.network.CASHADDR_ENCODER.encode(addr_bytes)\n    if witness:\n        addr_bytes.extend(riemann.network.P2WPKH_PREFIX)\n        addr_bytes.extend(pubkey_hash)\n        return riemann.network.SEGWIT_ENCODER.encode(addr_bytes)\n    else:\n        addr_bytes.extend(riemann.network.P2PKH_PREFIX)\n        addr_bytes.extend(pubkey_hash)\n        return riemann.network.LEGACY_ENCODER.encode(addr_bytes)", "response": "Make the pkh address from a public key hash."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a PKH address from a public key.", "response": "def make_pkh_address(pubkey, witness=False, cashaddr=True):\n    '''\n    bytes, bool -> str\n    '''\n    pubkey_hash = utils.hash160(pubkey)\n    return _make_pkh_address(pubkey_hash=pubkey_hash,\n                             witness=witness,\n                             cashaddr=cashaddr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string to a bytes object.", "response": "def to_output_script(address):\n    '''\n    str -> bytes\n    There's probably a better way to do this\n    '''\n    parsed = parse(address)\n    parsed_hash = b''\n\n    try:\n        if (parsed.find(riemann.network.P2WPKH_PREFIX) == 0\n                and len(parsed) == 22):\n            return parsed\n    except TypeError:\n        pass\n\n    try:\n        if (parsed.find(riemann.network.P2WSH_PREFIX) == 0\n                and len(parsed) == 34):\n            return parsed\n    except TypeError:\n        pass\n\n    try:\n        if (parsed.find(riemann.network.CASHADDR_P2SH) == 0\n                and len(parsed) == len(riemann.network.CASHADDR_P2SH) + 20):\n            prefix = b'\\xa9\\x14'  # OP_HASH160 PUSH14\n            parsed_hash = parsed[len(riemann.network.P2SH_PREFIX):]\n            suffix = b'\\x87'  # OP_EQUAL\n    except TypeError:\n        pass\n\n    try:\n        if (parsed.find(riemann.network.CASHADDR_P2PKH) == 0\n                and len(parsed) == len(riemann.network.CASHADDR_P2PKH) + 20):\n            prefix = b'\\x76\\xa9\\x14'  # OP_DUP OP_HASH160 PUSH14\n            parsed_hash = parsed[len(riemann.network.P2PKH_PREFIX):]\n            suffix = b'\\x88\\xac'  # OP_EQUALVERIFY OP_CHECKSIG\n    except TypeError:\n        pass\n\n    if (parsed.find(riemann.network.P2PKH_PREFIX) == 0\n            and len(parsed) == len(riemann.network.P2PKH_PREFIX) + 20):\n        prefix = b'\\x76\\xa9\\x14'  # OP_DUP OP_HASH160 PUSH14\n        parsed_hash = parsed[len(riemann.network.P2PKH_PREFIX):]\n        suffix = b'\\x88\\xac'  # OP_EQUALVERIFY OP_CHECKSIG\n\n    if (parsed.find(riemann.network.P2SH_PREFIX) == 0\n            and len(parsed) == len(riemann.network.P2SH_PREFIX) + 20):\n        prefix = b'\\xa9\\x14'  # OP_HASH160 PUSH14\n        parsed_hash = parsed[len(riemann.network.P2SH_PREFIX):]\n        suffix = b'\\x87'  # OP_EQUAL\n\n    if parsed_hash == b'':\n        raise ValueError('Cannot parse output script from address.')\n\n    output_script = prefix + parsed_hash + suffix\n    return output_script"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_output_script(output_script, cashaddr=True):\n    '''\n    bytes -> str\n    Convert output script (the on-chain format) to an address\n    There's probably a better way to do this\n    '''\n    try:\n        if (len(output_script) == len(riemann.network.P2WSH_PREFIX) + 32\n                and output_script.find(riemann.network.P2WSH_PREFIX) == 0):\n            # Script hash is the last 32 bytes\n            return _hash_to_sh_address(\n                output_script[-32:], witness=True, cashaddr=cashaddr)\n    except TypeError:\n        pass\n    try:\n        if (len(output_script) == len(riemann.network.P2WPKH_PREFIX) + 20\n                and output_script.find(riemann.network.P2WPKH_PREFIX) == 0):\n            # PKH is the last 20 bytes\n            return _make_pkh_address(\n                output_script[-20:], witness=True, cashaddr=cashaddr)\n    except TypeError:\n        pass\n\n    if len(output_script) == 25 and output_script.find(b'\\x76\\xa9\\x14') == 0:\n        return _make_pkh_address(\n            output_script[3:23], witness=False, cashaddr=cashaddr)\n\n    elif len(output_script) == 23 and output_script.find(b'\\xa9\\x14') == 0:\n        return _hash_to_sh_address(\n            output_script[2:22], witness=False, cashaddr=cashaddr)\n\n    raise ValueError('Cannot parse address from script.')", "response": "Convert an output script to an address\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_hash(address):\n    '''\n    str -> bytes\n    There's probably a better way to do this.\n    '''\n\n    raw = parse(address)\n\n    # Cash addresses\n    try:\n        if address.find(riemann.network.CASHADDR_PREFIX) == 0:\n            if raw.find(riemann.network.CASHADDR_P2SH) == 0:\n                return raw[len(riemann.network.CASHADDR_P2SH):]\n            if raw.find(riemann.network.CASHADDR_P2PKH) == 0:\n                return raw[len(riemann.network.CASHADDR_P2PKH):]\n    except TypeError:\n        pass\n\n    # Segwit addresses\n    try:\n        if address.find(riemann.network.BECH32_HRP) == 0:\n            if raw.find(riemann.network.P2WSH_PREFIX) == 0:\n                return raw[len(riemann.network.P2WSH_PREFIX):]\n            if raw.find(riemann.network.P2WPKH_PREFIX) == 0:\n                return raw[len(riemann.network.P2WPKH_PREFIX):]\n    except TypeError:\n        pass\n\n    # Legacy Addresses\n    if raw.find(riemann.network.P2SH_PREFIX) == 0:\n        return raw[len(riemann.network.P2SH_PREFIX):]\n    if raw.find(riemann.network.P2PKH_PREFIX) == 0:\n        return raw[len(riemann.network.P2PKH_PREFIX):]", "response": "Parses the hash address and returns the object representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nguesses the version of the nSequence signaling script.", "response": "def guess_version(redeem_script):\n    '''\n    str -> int\n    Bitcoin uses tx version 2 for nSequence signaling.\n    Zcash uses tx version 2 for joinsplits.\n\n    We want to signal nSequence if we're using OP_CSV.\n    Unless we're in zcash.\n    '''\n    n = riemann.get_current_network_name()\n    if 'sprout' in n:\n        return 1\n    if 'overwinter' in n:\n        return 3\n    if 'sapling' in n:\n        return 4\n    try:\n        script_array = redeem_script.split()\n        script_array.index('OP_CHECKSEQUENCEVERIFY')\n        return 2\n    except ValueError:\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nguess the sequence number from OP_CSV if OP_CSV is used otherwise use an appropriate sequence number.", "response": "def guess_sequence(redeem_script):\n    '''\n    str -> int\n    If OP_CSV is used, guess an appropriate sequence\n    Otherwise, disable RBF, but leave lock_time on.\n    Fails if there's not a constant before OP_CSV\n    '''\n    try:\n        script_array = redeem_script.split()\n        loc = script_array.index('OP_CHECKSEQUENCEVERIFY')\n        return int(script_array[loc - 1], 16)\n    except ValueError:\n        return 0xFFFFFFFE"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nguess the lock time from OP_CLTV if OP_CLTV is used otherwise return 0", "response": "def guess_locktime(redeem_script):\n    '''\n    str -> int\n    If OP_CLTV is used, guess an appropriate lock_time\n    Otherwise return 0 (no lock time)\n    Fails if there's not a constant before OP_CLTV\n    '''\n    try:\n        script_array = redeem_script.split()\n        loc = script_array.index('OP_CHECKLOCKTIMEVERIFY')\n        return int(script_array[loc - 1], 16)\n    except ValueError:\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output(value, address):\n    '''\n    int, str -> TxOut\n    accepts base58 or bech32 addresses\n    '''\n    script = addr.to_output_script(address)\n    value = utils.i2le_padded(value, 8)\n    return tb._make_output(value, script)", "response": "This function is used to generate the TxOut tuple for the output of a block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unsigned_input(outpoint, redeem_script=None, sequence=None):\n    '''\n    Outpoint, byte-like, int -> TxIn\n    '''\n    if redeem_script is not None and sequence is None:\n        sequence = guess_sequence(redeem_script)\n    if sequence is None:\n        sequence = 0xFFFFFFFE\n    return tb.make_legacy_input(\n        outpoint=outpoint,\n        stack_script=b'',\n        redeem_script=b'',\n        sequence=sequence)", "response": "Make an unsigned input."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p2pkh_input(outpoint, sig, pubkey, sequence=0xFFFFFFFE):\n    '''\n    OutPoint, hex_string, hex_string, int -> TxIn\n    Create a signed legacy TxIn from a p2pkh prevout\n    '''\n    stack_script = '{sig} {pk}'.format(sig=sig, pk=pubkey)\n    stack_script = script_ser.serialize(stack_script)\n    return tb.make_legacy_input(outpoint, stack_script, b'', sequence)", "response": "Create a signed legacy TxIn from a p2pkh prevout\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a signed legacy TxIn from a p2pkh output and a witness", "response": "def p2pkh_input_and_witness(outpoint, sig, pubkey, sequence=0xFFFFFFFE):\n    '''\n    OutPoint, hex_string, hex_string, int -> (TxIn, InputWitness)\n    Create a signed legacy TxIn from a p2pkh prevout\n    Create an empty InputWitness for it\n    Useful for transactions spending some witness and some legacy prevouts\n    '''\n    stack_script = '{sig} {pk}'.format(sig=sig, pk=pubkey)\n    return tb.make_legacy_input_and_empty_witness(\n        outpoint=outpoint,\n        stack_script=script_ser.serialize(stack_script),\n        redeem_script=b'',\n        sequence=sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p2sh_input(outpoint, stack_script, redeem_script, sequence=None):\n    '''\n    OutPoint, str, str, int -> TxIn\n    Create a signed legacy TxIn from a p2pkh prevout\n    '''\n    if sequence is None:\n        sequence = guess_sequence(redeem_script)\n\n    stack_script = script_ser.serialize(stack_script)\n    redeem_script = script_ser.hex_serialize(redeem_script)\n    redeem_script = script_ser.serialize(redeem_script)\n\n    return tb.make_legacy_input(\n        outpoint=outpoint,\n        stack_script=stack_script,\n        redeem_script=redeem_script,\n        sequence=sequence)", "response": "Create a signed legacy TxIn from a p2pkh prevout."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a signed legacy TxIn from a p2pkh prevout and a witness", "response": "def p2sh_input_and_witness(outpoint, stack_script,\n                           redeem_script, sequence=None):\n    '''\n    OutPoint, str, str, int -> (TxIn, InputWitness)\n    Create a signed legacy TxIn from a p2pkh prevout\n    Create an empty InputWitness for it\n    Useful for transactions spending some witness and some legacy prevouts\n    '''\n    if sequence is None:\n        sequence = guess_sequence(redeem_script)\n\n    stack_script = script_ser.serialize(stack_script)\n    redeem_script = script_ser.hex_serialize(redeem_script)\n    redeem_script = script_ser.serialize(redeem_script)\n\n    return tb.make_legacy_input_and_empty_witness(\n        outpoint=outpoint,\n        stack_script=stack_script,\n        redeem_script=redeem_script,\n        sequence=sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a signed witness TxIn and InputWitness from a p2wpkh prevout", "response": "def p2wpkh_input_and_witness(outpoint, sig, pubkey, sequence=0xFFFFFFFE):\n    '''\n    Outpoint, hex_string, hex_string, int -> (TxIn, InputWitness)\n    Create a signed witness TxIn and InputWitness from a p2wpkh prevout\n    '''\n    return tb.make_witness_input_and_witness(\n        outpoint=outpoint,\n        sequence=sequence,\n        stack=[bytes.fromhex(sig), bytes.fromhex(pubkey)])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p2wsh_input_and_witness(outpoint, stack, witness_script, sequence=None):\n    '''\n    Outpoint, str, str, int -> (TxIn, InputWitness)\n    Create a signed witness TxIn and InputWitness from a p2wsh prevout\n    '''\n    if sequence is None:\n        sequence = guess_sequence(witness_script)\n    stack = list(map(\n        lambda x: b'' if x == 'NONE' else bytes.fromhex(x), stack.split()))\n    stack.append(script_ser.serialize(witness_script))\n    return tb.make_witness_input_and_witness(outpoint, sequence, stack)", "response": "Create a signed witness TxIn and InputWitness from a p2wsh prevout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unsigned_legacy_tx(tx_ins, tx_outs, **kwargs):\n    '''Create an unsigned transaction\n    Use this to generate sighashes for unsigned TxIns\n    Gotcha: it requires you to know the timelock and version\n            it will _not_ guess them\n            becuase it may not have acess to all scripts\n    Hint: set version to 2 if using sequence number relative time locks\n\n    Args:\n        tx_ins      list(TxIn instances): list of transaction inputs\n        tx_outs     list(TxOut instances): list of transaction outputs\n\n        **kwargs:\n        version     (int): transaction version number\n        locktime            (hex): transaction locktime\n        expiry              (int): overwinter expiry time\n        tx_joinsplits       (list): list of joinsplits transactions\n        joinsplit_pubkey    (bytes): joinsplit public key\n        joinsplit_sig       (bytes): joinsplit signature\n\n    Returns:\n        (Tx instance): unsigned transaction\n    '''\n    return tb.make_tx(\n        version=kwargs['version'] if 'version' in kwargs else 1,\n        tx_ins=tx_ins,\n        tx_outs=tx_outs,\n        lock_time=kwargs['lock_time'] if 'lock_time' in kwargs else 0,\n        expiry=kwargs['expiry'] if 'expiry' in kwargs else 0,\n        tx_joinsplits=(kwargs['tx_joinsplits']\n                       if 'tx_joinsplits' in kwargs else None),\n        joinsplit_pubkey=(kwargs['joinsplit_pubkey']\n                          if 'joinsplit_pubkey' in kwargs\n                          else None),\n        joinsplit_sig=(kwargs['joinsplit_sig']\n                       if 'joinsplit_sig' in kwargs else None))", "response": "Create an unsigned transaction with legacy parameters"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unsigned_witness_tx(tx_ins, tx_outs, **kwargs):\n    '''Create an unsigned segwit transaction\n    Create an unsigned segwit transaction\n    Use this to generate sighashes for unsigned TxIns\n    Gotcha: it requires you to know the timelock and version\n            it will _not_ guess them\n            becuase it may not have acess to all scripts\n    Hint: set version to 2 if using sequence number relative time locks\n\n    Args:\n        tx_ins      list(TxIn instances): list of transaction inputs\n        tx_outs     list(TxOut instances): list of transaction outputs\n\n        **kwargs:\n        version     (int): transaction version number\n        locktime    (hex): transaction locktime\n\n    Returns:\n        (Tx instance): unsigned transaction with empty witness\n    '''\n    return tb.make_tx(\n        version=kwargs['version'] if 'version' in kwargs else 1,\n        tx_ins=tx_ins,\n        tx_outs=tx_outs,\n        lock_time=kwargs['lock_time'] if 'lock_time' in kwargs else 0,\n        tx_witnesses=[tb.make_empty_witness() for _ in tx_ins])", "response": "Create an unsigned segwit transactionwith empty witnesses"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef legacy_tx(tx_ins, tx_outs, **kwargs):\n    '''\n    Construct a fully-signed legacy transaction\n    Args:\n        tx_ins      list(TxIn instances): list of transaction inputs\n        tx_outs     list(TxOut instances): list of transaction outputs\n\n        **kwargs:\n        version     (int): transaction version number\n        locktime    (hex): transaction locktime\n        expiry              (int): overwinter expiry time\n        tx_joinsplits       (list): list of joinsplits transactions\n        joinsplit_pubkey    (bytes): joinsplit public key\n        joinsplit_sig       (bytes): joinsplit signature\n\n    Returns:\n        (Tx instance): signed transaction with empty witness\n    '''\n\n    # Look at each input to guess lock_time and version\n    deser = [script_ser.deserialize(tx_in.redeem_script)\n             for tx_in in tx_ins if tx_in.redeem_script is not None]\n    version = max([guess_version(d) for d in deser])\n    lock_time = max([guess_locktime(d) for d in deser])\n\n    return tb.make_tx(\n        version=version,\n        tx_ins=tx_ins,\n        tx_outs=tx_outs,\n        lock_time=lock_time,\n        tx_witnesses=None,\n        expiry=kwargs['expiry'] if 'expiry' in kwargs else 0,\n        tx_joinsplits=(kwargs['tx_joinsplits']\n                       if 'tx_joinsplits' in kwargs else None),\n        joinsplit_pubkey=(kwargs['joinsplit_pubkey']\n                          if 'joinsplit_pubkey' in kwargs else None),\n        joinsplit_sig=(kwargs['joinsplit_sig']\n                       if 'joinsplit_sig' in kwargs else None))", "response": "Construct a fully - signed legacy transaction with empty witnesses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef witness_tx(tx_ins, tx_outs, tx_witnesses, **kwargs):\n    '''\n    Construct a fully-signed segwit transaction\n    Args:\n        tx_ins       list(TxIn instances): list of transaction inputs\n        tx_outs      list(TxOut instances): list of transaction outputs\n        tx_witnesses list(TxWitness instances): list of transaction witnsses\n        **kwargs:\n        version     (int): transaction version number\n        locktime    (hex): transaction locktime\n\n    Returns:\n        (Tx instance): signed transaction with witnesses\n    '''\n\n    # Parse legacy scripts AND witness scripts for OP_CLTV\n    deser = [script_ser.deserialize(tx_in.redeem_script) for tx_in in tx_ins\n             if tx_in is not None]\n    for w in tx_witnesses:\n        try:\n            deser.append(script_ser.deserialize(w.stack[-1].item))\n        except (NotImplementedError, ValueError):\n            pass\n    version = max([guess_version(d) for d in deser])\n    if 'lock_time' in kwargs:\n        lock_time = kwargs['lock_time']\n    else:\n        lock_time = max([guess_locktime(d) for d in deser])\n\n    return tb.make_tx(\n        version=version,\n        tx_ins=tx_ins,\n        tx_outs=tx_outs,\n        lock_time=lock_time,\n        tx_witnesses=tx_witnesses)", "response": "Construct a fully - signed segwit transaction with witnesses."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a copy of the OverwinterTx object.", "response": "def copy(self, tx_ins=None, tx_outs=None, lock_time=None,\n             expiry_height=None, tx_joinsplits=None, joinsplit_pubkey=None,\n             joinsplit_sig=None):\n        '''\n        OverwinterTx, ... -> OverwinterTx\n\n        Makes a copy. Allows over-writing specific pieces.\n        '''\n        return OverwinterTx(\n            tx_ins=tx_ins if tx_ins is not None else self.tx_ins,\n            tx_outs=tx_outs if tx_outs is not None else self.tx_outs,\n            lock_time=(lock_time if lock_time is not None\n                       else self.lock_time),\n            expiry_height=(expiry_height if expiry_height is not None\n                           else self.expiry_height),\n            tx_joinsplits=(tx_joinsplits if tx_joinsplits is not None\n                           else self.tx_joinsplits),\n            joinsplit_pubkey=(joinsplit_pubkey if joinsplit_pubkey is not None\n                              else self.joinsplit_pubkey),\n            joinsplit_sig=(joinsplit_sig if joinsplit_sig is not None\n                           else self.joinsplit_sig))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a byte - like OverwinterTx into a new OverwinterTx object.", "response": "def from_bytes(OverwinterTx, byte_string):\n        '''\n        byte-like -> OverwinterTx\n        '''\n        header = byte_string[0:4]\n        group_id = byte_string[4:8]\n\n        if header != b'\\x03\\x00\\x00\\x80' or group_id != b'\\x70\\x82\\xc4\\x03':\n            raise ValueError(\n                'Bad header or group ID. Expected {} and {}. Got: {} and {}'\n                .format(b'\\x03\\x00\\x00\\x80'.hex(),\n                        b'\\x70\\x82\\xc4\\x03'.hex(),\n                        header.hex(),\n                        group_id.hex()))\n\n        tx_ins = []\n        tx_ins_num = shared.VarInt.from_bytes(byte_string[8:])\n\n        current = 8 + len(tx_ins_num)\n        for _ in range(tx_ins_num.number):\n            tx_in = TxIn.from_bytes(byte_string[current:])\n            current += len(tx_in)\n            tx_ins.append(tx_in)\n\n        tx_outs = []\n        tx_outs_num = shared.VarInt.from_bytes(byte_string[current:])\n\n        current += len(tx_outs_num)\n        for _ in range(tx_outs_num.number):\n            tx_out = TxOut.from_bytes(byte_string[current:])\n            current += len(tx_out)\n            tx_outs.append(tx_out)\n\n        lock_time = byte_string[current:current + 4]\n        current += 4\n        expiry_height = byte_string[current:current + 4]\n        current += 4\n\n        if current == len(byte_string):\n            # No joinsplits\n            tx_joinsplits = tuple()\n            joinsplit_pubkey = None\n            joinsplit_sig = None\n        else:\n            tx_joinsplits = []\n            tx_joinsplits_num = shared.VarInt.from_bytes(byte_string[current:])\n            current += len(tx_outs_num)\n            for _ in range(tx_joinsplits_num.number):\n                tx_joinsplit = z.SproutJoinsplit.from_bytes(\n                    byte_string[current:])\n                current += len(tx_joinsplit)\n                tx_joinsplits.append(tx_joinsplit)\n\n            joinsplit_pubkey = byte_string[current:current + 32]\n            current += 32\n            joinsplit_sig = byte_string[current:current + 64]\n\n        return OverwinterTx(\n            tx_ins=tx_ins,\n            tx_outs=tx_outs,\n            lock_time=lock_time,\n            expiry_height=expiry_height,\n            tx_joinsplits=tx_joinsplits,\n            joinsplit_pubkey=joinsplit_pubkey,\n            joinsplit_sig=joinsplit_sig)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(ctx):\n    dir_path = os.path.join(os.path.expanduser('~'), '.keep')\n    if os.path.exists(dir_path):\n        if click.confirm('[CRITICAL] Remove everything inside ~/.keep ?', abort=True):\n            shutil.rmtree(dir_path)\n    utils.first_time_use(ctx)", "response": "Initializes the CLI environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(ctx, pattern, arguments, safe):\n\n    matches = utils.grep_commands(pattern)\n    if matches:\n        selected = utils.select_command(matches)\n        if selected >= 0:\n            cmd, desc = matches[selected]\n            pcmd = utils.create_pcmd(cmd)\n            raw_params, params, defaults = utils.get_params_in_pcmd(pcmd)\n\n            arguments = list(arguments)\n            kargs = {}\n            for r, p, d in zip(raw_params, params, defaults):\n                if arguments:\n                    val = arguments.pop(0)\n                    click.echo(\"{}: {}\".format(p, val))\n                    kargs[r] = val\n                elif safe:\n                    if d:\n                        kargs[r] = d\n                else:\n                    p_default = d if d else None\n                    val = click.prompt(\"Enter value for '{}'\".format(p), default=p_default)\n                    kargs[r] = val\n            click.echo(\"\\n\")\n\n            final_cmd = utils.substitute_pcmd(pcmd, kargs, safe)\n\n            command = \"$ {} :: {}\".format(final_cmd, desc)\n            if click.confirm(\"Execute\\n\\t{}\\n\\n?\".format(command), default=True):\n                os.system(final_cmd)\n    elif matches == []:\n        click.echo('No saved commands matches the pattern {}'.format(pattern))\n    else:\n        click.echo(\"No commands to run, Add one by 'keep new'. \")", "response": "Executes a saved command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving a new command", "response": "def cli(ctx):\n    \"\"\"Saves a new command\"\"\"\n    cmd = click.prompt('Command')\n    desc = click.prompt('Description ')\n    alias = click.prompt('Alias (optional)', default='')\n    utils.save_command(cmd, desc, alias)\n\n    utils.log(ctx, 'Saved the new command - {} - with the description - {}.'.format(cmd, desc))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log(self, msg, *args):\n        if args:\n            msg %= args\n        click.echo(msg, file=sys.stderr)", "response": "Logs a message to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vlog(self, msg, *args):\n        if self.verbose:\n            self.log(msg, *args)", "response": "Logs a message to stderr only if verbose is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(ctx):\n    utils.check_update(ctx, forced=True)\n    click.secho(\"Keep is at its latest version v{}\".format(about.__version__), fg='green')", "response": "Check for an update of Keep."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a saved command.", "response": "def cli(ctx, pattern):\n    \"\"\"Searches for a saved command.\"\"\"\n    matches = utils.grep_commands(pattern)\n    if matches:\n        for cmd, desc in matches:\n            click.secho(\"$ {} :: {}\".format(cmd, desc), fg='green')\n    elif matches == []:\n        click.echo('No saved commands matches the pattern {}'.format(pattern))\n    else:\n        click.echo('No commands to show. Add one by `keep new`.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the saved commands.", "response": "def cli(ctx):\n    \"\"\"Shows the saved commands.\"\"\"\n    json_path = os.path.join(os.path.expanduser('~'), '.keep', 'commands.json')\n    if not os.path.exists(json_path):\n        click.echo('No commands to show. Add one by `keep new`.')\n    else:\n        utils.list_commands(ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the local database with remote.", "response": "def cli(ctx, overwrite):\n    \"\"\"Updates the local database with remote.\"\"\"\n    credentials_path = os.path.join(os.path.expanduser('~'), '.keep', '.credentials')\n    if not os.path.exists(credentials_path):\n        click.echo('You are not registered.')\n        utils.register()\n    else:\n        utils.pull(ctx, overwrite)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister user over server.", "response": "def cli(ctx):\n    \"\"\"Register user over server.\"\"\"\n    dir_path = os.path.join(os.path.expanduser('~'), '.keep', '.credentials')\n    if os.path.exists(dir_path):\n        if click.confirm('[CRITICAL] Reset credentials saved in ~/.keep/.credentials ?', abort=True):\n            os.remove(dir_path)\n    utils.register()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for update on pypi.", "response": "def check_update(ctx, forced=False):\n    \"\"\"\n    Check for update on pypi. Limit to 1 check per day if not forced\n    \"\"\"\n    try:\n        if ctx.update_checked and not forced:\n            return\n    except AttributeError:\n        update_check_file = os.path.join(dir_path, 'update_check.txt')\n        today = datetime.date.today().strftime(\"%m/%d/%Y\")\n        if os.path.exists(update_check_file):\n            date = open(update_check_file, 'r').read()\n        else:\n            date = []\n        if forced or today != date:\n            ctx.update_checked = True\n            date = today\n            with open(update_check_file, 'w') as f:\n                f.write(date)\n            r = requests.get(\"https://pypi.org/pypi/keep/json\").json()\n            version = r['info']['version']\n            curr_version = about.__version__\n            if version > curr_version:\n                click.secho(\"Keep seems to be outdated. Current version = \"\n                            \"{}, Latest version = {}\".format(curr_version, version) +\n                            \"\\n\\nPlease update with \", bold=True, fg='red')\n                click.secho(\"\\tpip3 --no-cache-dir install -U keep==\" + str(version), fg='green')\n                click.secho(\"\\n\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(ctx, pattern):\n    matches = utils.grep_commands(pattern)\n    if matches:\n        selected = utils.select_command(matches)\n        if selected >= 0:\n            cmd, desc = matches[selected]\n            command = \"$ {} :: {}\".format(cmd, desc)\n            if click.confirm(\"Remove\\n\\t{}\\n\\n?\".format(command), default=True):\n                utils.remove_command(cmd)\n                click.echo('Command successfully removed!')\n    elif matches == []:\n        click.echo('No saved commands matches the pattern {}'.format(pattern))\n    else:\n        click.echo(\"No commands to remove, Add one by 'keep new'. \")", "response": "Deletes a saved command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_mesh(mesh):\n    lim_max = sp.amax(mesh.verts, axis=0)\n    lim_min = sp.amin(mesh.verts, axis=0)\n\n    # Display resulting triangular mesh using Matplotlib.\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n    mesh = Poly3DCollection(mesh.verts[mesh.faces])\n    mesh.set_edgecolor('k')\n\n    ax.add_collection3d(mesh)\n    ax.set_xlabel(\"x-axis\")\n    ax.set_ylabel(\"y-axis\")\n    ax.set_zlabel(\"z-axis\")\n    ax.set_xlim(lim_min[0], lim_max[0])\n    ax.set_ylim(lim_min[1], lim_max[1])\n    ax.set_zlim(lim_min[2], lim_max[2])\n\n    return fig", "response": "r Visualizes the mesh of a region as obtained by get_mesh function in\n    the metrics submodule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef representative_elementary_volume(im, npoints=1000):\n    im_temp = sp.zeros_like(im)\n    crds = sp.array(sp.rand(npoints, im.ndim)*im.shape, dtype=int)\n    pads = sp.array(sp.rand(npoints)*sp.amin(im.shape)/2+10, dtype=int)\n    im_temp[tuple(crds.T)] = True\n    labels, N = spim.label(input=im_temp)\n    slices = spim.find_objects(input=labels)\n    porosity = sp.zeros(shape=(N,), dtype=float)\n    volume = sp.zeros(shape=(N,), dtype=int)\n    for i in tqdm(sp.arange(0, N)):\n        s = slices[i]\n        p = pads[i]\n        new_s = extend_slice(s, shape=im.shape, pad=p)\n        temp = im[new_s]\n        Vp = sp.sum(temp)\n        Vt = sp.size(temp)\n        porosity[i] = Vp/Vt\n        volume[i] = Vt\n    profile = namedtuple('profile', ('volume', 'porosity'))\n    profile.volume = volume\n    profile.porosity = porosity\n    return profile", "response": "r This function calculates the porosity of the image and returns the volume and porosity of each subdomain of the porous material. This function returns a randomized version of the image as a function subdomain size."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef porosity(im):\n    im = sp.array(im, dtype=int)\n    Vp = sp.sum(im == 1)\n    Vs = sp.sum(im == 0)\n    e = Vp/(Vs + Vp)\n    return e", "response": "r Calculates the porosity of an image assuming 1 s are void space and 0 s are solid phase."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef two_point_correlation_bf(im, spacing=10):\n    if im.ndim != im.squeeze().ndim:\n        warnings.warn('Input image conains a singleton axis:' + str(im.shape) +\n                      ' Reduce dimensionality with np.squeeze(im) to avoid' +\n                      ' unexpected behavior.')\n    if im.ndim == 2:\n        pts = sp.meshgrid(range(0, im.shape[0], spacing),\n                          range(0, im.shape[1], spacing))\n        crds = sp.vstack([pts[0].flatten(),\n                          pts[1].flatten()]).T\n    elif im.ndim == 3:\n        pts = sp.meshgrid(range(0, im.shape[0], spacing),\n                          range(0, im.shape[1], spacing),\n                          range(0, im.shape[2], spacing))\n        crds = sp.vstack([pts[0].flatten(),\n                          pts[1].flatten(),\n                          pts[2].flatten()]).T\n    dmat = sptl.distance.cdist(XA=crds, XB=crds)\n    hits = im[tuple(pts)].flatten()\n    dmat = dmat[hits, :]\n    h1 = sp.histogram(dmat, bins=range(0, int(sp.amin(im.shape)/2), spacing))\n    dmat = dmat[:, hits]\n    h2 = sp.histogram(dmat, bins=h1[1])\n    tpcf = namedtuple('two_point_correlation_function',\n                      ('distance', 'probability'))\n    return tpcf(h2[1][:-1], h2[0]/h1[0])", "response": "r Calculates the two - point correlation function using brute - force."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _radial_profile(autocorr, r_max, nbins=100):\n    if len(autocorr.shape) == 2:\n        adj = sp.reshape(autocorr.shape, [2, 1, 1])\n        inds = sp.indices(autocorr.shape) - adj/2\n        dt = sp.sqrt(inds[0]**2 + inds[1]**2)\n    elif len(autocorr.shape) == 3:\n        adj = sp.reshape(autocorr.shape, [3, 1, 1, 1])\n        inds = sp.indices(autocorr.shape) - adj/2\n        dt = sp.sqrt(inds[0]**2 + inds[1]**2 + inds[2]**2)\n    else:\n        raise Exception('Image dimensions must be 2 or 3')\n    bin_size = np.int(np.ceil(r_max/nbins))\n    bins = np.arange(bin_size, r_max, step=bin_size)\n    radial_sum = np.zeros_like(bins)\n    for i, r in enumerate(bins):\n        # Generate Radial Mask from dt using bins\n        mask = (dt <= r) * (dt > (r-bin_size))\n        radial_sum[i] = np.sum(autocorr[mask])/np.sum(mask)\n    # Return normalized bin and radially summed autoc\n    norm_autoc_radial = radial_sum/np.max(autocorr)\n    tpcf = namedtuple('two_point_correlation_function',\n                      ('distance', 'probability'))\n    return tpcf(bins, norm_autoc_radial)", "response": "r Calculates the radial profile of the autocorrelation of the given image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pore_size_distribution(im, bins=10, log=True, voxel_size=1):\n    im = im.flatten()\n    vals = im[im > 0]*voxel_size\n    if log:\n        vals = sp.log10(vals)\n    h = _parse_histogram(sp.histogram(vals, bins=bins, density=True))\n    psd = namedtuple('pore_size_distribution',\n                     (log*'log' + 'R', 'pdf', 'cdf', 'satn',\n                      'bin_centers', 'bin_edges', 'bin_widths'))\n    return psd(h.bin_centers, h.pdf, h.cdf, h.relfreq,\n               h.bin_centers, h.bin_edges, h.bin_widths)", "response": "r Calculates a pore - size distribution based on the image produced by the pore_size_distribution_function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chord_counts(im):\n    labels, N = spim.label(im > 0)\n    props = regionprops(labels, coordinates='xy')\n    chord_lens = sp.array([i.filled_area for i in props])\n    return chord_lens", "response": "r Returns a list of chords that are in the same size as the image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chord_length_distribution(im, bins=None, log=False, voxel_size=1,\n                              normalization='count'):\n    r\"\"\"\n    Determines the distribution of chord lengths in an image containing chords.\n\n    Parameters\n    ----------\n    im : ND-image\n        An image with chords drawn in the pore space, as produced by\n        ``apply_chords`` or ``apply_chords_3d``.\n\n        ``im`` can be either boolean, in which case each chord will be\n        identified using ``scipy.ndimage.label``, or numerical values in which\n        case it is assumed that chords have already been identifed and labeled.\n        In both cases, the size of each chord will be computed as the number\n        of voxels belonging to each labelled region.\n\n    bins : scalar or array_like\n        If a scalar is given it is interpreted as the number of bins to use,\n        and if an array is given they are used as the bins directly.\n\n    log : Boolean\n        If true, the logarithm of the chord lengths will be used, which can\n        make the data more clear.\n\n    normalization : string\n        Indicates how to normalize the bin heights.  Options are:\n\n        *'count' or 'number'* - (default) This simply counts the number of\n        chords in each bin in the normal sense of a histogram.  This is the\n        rigorous definition according to Torquato [1].\n\n        *'length'* - This multiplies the number of chords in each bin by the\n        chord length (i.e. bin size).  The normalization scheme accounts for\n        the fact that long chords are less frequent than shorert chords,\n        thus giving a more balanced distribution.\n\n    voxel_size : scalar\n        The size of a voxel side in preferred units.  The default is 1, so the\n        user can apply the scaling to the returned results after the fact.\n\n    Returns\n    -------\n    result : named_tuple\n        A tuple containing the following elements, which can be retrieved by\n        attribute name:\n\n        *L* or *logL* - chord length, equivalent to ``bin_centers``\n\n        *pdf* - probability density function\n\n        *cdf* - cumulative density function\n\n        *relfreq* - relative frequency chords in each bin.  The sum of all bin\n        heights is 1.0.  For the cumulative relativce, use *cdf* which is\n        already normalized to 1.\n\n        *bin_centers* - the center point of each bin\n\n        *bin_edges* - locations of bin divisions, including 1 more value than\n        the number of bins\n\n        *bin_widths* - useful for passing to the ``width`` argument of\n        ``matplotlib.pyplot.bar``\n\n    References\n    ----------\n    [1] Torquato, S. Random Heterogeneous Materials: Mircostructure and\n    Macroscopic Properties. Springer, New York (2002) - See page 45 & 292\n    \"\"\"\n    x = chord_counts(im)\n    if bins is None:\n        bins = sp.array(range(0, x.max()+2))*voxel_size\n    x = x*voxel_size\n    if log:\n        x = sp.log10(x)\n    if normalization == 'length':\n        h = list(sp.histogram(x, bins=bins, density=False))\n        h[0] = h[0]*(h[1][1:]+h[1][:-1])/2  # Scale bin heigths by length\n        h[0] = h[0]/h[0].sum()/(h[1][1:]-h[1][:-1])  # Normalize h[0] manually\n    elif normalization in ['number', 'count']:\n        h = sp.histogram(x, bins=bins, density=True)\n    else:\n        raise Exception('Unsupported normalization:', normalization)\n    h = _parse_histogram(h)\n    cld = namedtuple('chord_length_distribution',\n                     (log*'log' + 'L', 'pdf', 'cdf', 'relfreq',\n                      'bin_centers', 'bin_edges', 'bin_widths'))\n    return cld(h.bin_centers, h.pdf, h.cdf, h.relfreq,\n               h.bin_centers, h.bin_edges, h.bin_widths)", "response": "r Returns a distribution of chord lengths in an image containing chords drawn in the pore space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef region_surface_areas(regions, voxel_size=1, strel=None):\n    print('_'*60)\n    print('Finding surface area of each region')\n    im = regions.copy()\n    # Get 'slices' into im for each pore region\n    slices = spim.find_objects(im)\n    # Initialize arrays\n    Ps = sp.arange(1, sp.amax(im)+1)\n    sa = sp.zeros_like(Ps, dtype=float)\n    # Start extracting marching cube area from im\n    for i in tqdm(Ps):\n        reg = i - 1\n        if slices[reg] is not None:\n            s = extend_slice(slices[reg], im.shape)\n            sub_im = im[s]\n            mask_im = sub_im == i\n            mesh = mesh_region(region=mask_im, strel=strel)\n            sa[reg] = mesh_surface_area(mesh)\n    result = sa * voxel_size**2\n    return result", "response": "r Returns the surface area of each region in a labeled image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef phase_fraction(im, normed=True):\n    if im.dtype == bool:\n        im = im.astype(int)\n    elif im.dtype != int:\n        raise Exception('Image must contain integer values for each phase')\n    labels = sp.arange(0, sp.amax(im)+1)\n    results = sp.zeros_like(labels)\n    for i in labels:\n        results[i] = sp.sum(im == i)\n    if normed:\n        results = results/im.size\n    return results", "response": "r Calculates the number or fraction of each phase in an image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xray(im, direction='X'):\n    im = sp.array(~im, dtype=int)\n    if direction in ['Y', 'y']:\n        im = sp.transpose(im, axes=[1, 0, 2])\n    if direction in ['Z', 'z']:\n        im = sp.transpose(im, axes=[2, 1, 0])\n    im = sp.sum(im, axis=0)\n    return im", "response": "r Simulates an X - ray radiograph in the specified direction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef props_to_DataFrame(regionprops):\n    # Parse the regionprops list and pull out all props with scalar values\n    metrics = []\n    reg = regionprops[0]\n    for item in reg.__dir__():\n        if not item.startswith('_'):\n            try:\n                if sp.shape(getattr(reg, item)) == ():\n                    metrics.append(item)\n            except (TypeError, NotImplementedError, AttributeError):\n                pass\n    # Create a dictionary of all metrics that are simple scalar propertie\n    d = {}\n    for k in metrics:\n        try:\n            d[k] = sp.array([r[k] for r in regionprops])\n        except ValueError:\n            print('Error encountered evaluating ' + k + ' so skipping it')\n    # Create pandas data frame an return\n    df = DataFrame(d)\n    return df", "response": "r Returns a Pandas DataFrame containing all the scalar metrics for each region and all the values for each key metric for each region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef regions_to_network(im, dt=None, voxel_size=1):\n    print('_'*60)\n    print('Extracting pore and throat information from image')\n    from skimage.morphology import disk, ball\n    struc_elem = disk if im.ndim == 2 else ball\n\n    # if ~sp.any(im == 0):\n    #     raise Exception('The received image has no solid phase (0\\'s)')\n\n    if dt is None:\n        dt = spim.distance_transform_edt(im > 0)\n        dt = spim.gaussian_filter(input=dt, sigma=0.5)\n\n    # Get 'slices' into im for each pore region\n    slices = spim.find_objects(im)\n\n    # Initialize arrays\n    Ps = sp.arange(1, sp.amax(im)+1)\n    Np = sp.size(Ps)\n    p_coords = sp.zeros((Np, im.ndim), dtype=float)\n    p_volume = sp.zeros((Np, ), dtype=float)\n    p_dia_local = sp.zeros((Np, ), dtype=float)\n    p_dia_global = sp.zeros((Np, ), dtype=float)\n    p_label = sp.zeros((Np, ), dtype=int)\n    p_area_surf = sp.zeros((Np, ), dtype=int)\n    t_conns = []\n    t_dia_inscribed = []\n    t_area = []\n    t_perimeter = []\n    t_coords = []\n    # dt_shape = sp.array(dt.shape)\n\n    # Start extracting size information for pores and throats\n    for i in tqdm(Ps):\n        pore = i - 1\n        if slices[pore] is None:\n            continue\n        s = extend_slice(slices[pore], im.shape)\n        sub_im = im[s]\n        sub_dt = dt[s]\n        pore_im = sub_im == i\n        padded_mask = sp.pad(pore_im, pad_width=1, mode='constant')\n        pore_dt = spim.distance_transform_edt(padded_mask)\n        s_offset = sp.array([i.start for i in s])\n        p_label[pore] = i\n        p_coords[pore, :] = spim.center_of_mass(pore_im) + s_offset\n        p_volume[pore] = sp.sum(pore_im)\n        p_dia_local[pore] = 2*sp.amax(pore_dt)\n        p_dia_global[pore] = 2*sp.amax(sub_dt)\n        p_area_surf[pore] = sp.sum(pore_dt == 1)\n        im_w_throats = spim.binary_dilation(input=pore_im, structure=struc_elem(1))\n        im_w_throats = im_w_throats*sub_im\n        Pn = sp.unique(im_w_throats)[1:] - 1\n        for j in Pn:\n            if j > pore:\n                t_conns.append([pore, j])\n                vx = sp.where(im_w_throats == (j + 1))\n                t_dia_inscribed.append(2*sp.amax(sub_dt[vx]))\n                t_perimeter.append(sp.sum(sub_dt[vx] < 2))\n                t_area.append(sp.size(vx[0]))\n                t_inds = tuple([i+j for i, j in zip(vx, s_offset)])\n                temp = sp.where(dt[t_inds] == sp.amax(dt[t_inds]))[0][0]\n                if im.ndim == 2:\n                    t_coords.append(tuple((t_inds[0][temp],\n                                           t_inds[1][temp])))\n                else:\n                    t_coords.append(tuple((t_inds[0][temp],\n                                           t_inds[1][temp],\n                                           t_inds[2][temp])))\n    # Clean up values\n    Nt = len(t_dia_inscribed)  # Get number of throats\n    if im.ndim == 2:  # If 2D, add 0's in 3rd dimension\n        p_coords = sp.vstack((p_coords.T, sp.zeros((Np, )))).T\n        t_coords = sp.vstack((sp.array(t_coords).T, sp.zeros((Nt, )))).T\n\n    net = {}\n    net['pore.all'] = sp.ones((Np, ), dtype=bool)\n    net['throat.all'] = sp.ones((Nt, ), dtype=bool)\n    net['pore.coords'] = sp.copy(p_coords)*voxel_size\n    net['pore.centroid'] = sp.copy(p_coords)*voxel_size\n    net['throat.centroid'] = sp.array(t_coords)*voxel_size\n    net['throat.conns'] = sp.array(t_conns)\n    net['pore.label'] = sp.array(p_label)\n    net['pore.volume'] = sp.copy(p_volume)*(voxel_size**3)\n    net['throat.volume'] = sp.zeros((Nt, ), dtype=float)\n    net['pore.diameter'] = sp.copy(p_dia_local)*voxel_size\n    net['pore.inscribed_diameter'] = sp.copy(p_dia_local)*voxel_size\n    net['pore.equivalent_diameter'] = 2*((3/4*net['pore.volume']/sp.pi)**(1/3))\n    net['pore.extended_diameter'] = sp.copy(p_dia_global)*voxel_size\n    net['pore.surface_area'] = sp.copy(p_area_surf)*(voxel_size)**2\n    net['throat.diameter'] = sp.array(t_dia_inscribed)*voxel_size\n    net['throat.inscribed_diameter'] = sp.array(t_dia_inscribed)*voxel_size\n    net['throat.area'] = sp.array(t_area)*(voxel_size**2)\n    net['throat.perimeter'] = sp.array(t_perimeter)*voxel_size\n    net['throat.equivalent_diameter'] = (sp.array(t_area) * (voxel_size**2))**0.5\n    P12 = net['throat.conns']\n    PT1 = sp.sqrt(sp.sum(((p_coords[P12[:, 0]]-t_coords) * voxel_size)**2, axis=1))\n    PT2 = sp.sqrt(sp.sum(((p_coords[P12[:, 1]]-t_coords) * voxel_size)**2, axis=1))\n    net['throat.total_length'] = PT1 + PT2\n    PT1 = PT1-p_dia_local[P12[:, 0]]/2*voxel_size\n    PT2 = PT2-p_dia_local[P12[:, 1]]/2*voxel_size\n    net['throat.length'] = PT1 + PT2\n    dist = (p_coords[P12[:, 0]]-p_coords[P12[:, 1]])*voxel_size\n    net['throat.direct_length'] = sp.sqrt(sp.sum(dist**2, axis=1))\n    # Make a dummy openpnm network to get the conduit lengths\n    pn = op.network.GenericNetwork()\n    pn.update(net)\n    pn.add_model(propname='throat.endpoints',\n                 model=op_gm.throat_endpoints.spherical_pores,\n                 pore_diameter='pore.inscribed_diameter',\n                 throat_diameter='throat.inscribed_diameter')\n    pn.add_model(propname='throat.conduit_lengths',\n                 model=op_gm.throat_length.conduit_lengths)\n    pn.add_model(propname='pore.area',\n                 model=op_gm.pore_area.sphere)\n    net['throat.endpoints.head'] = pn['throat.endpoints.head']\n    net['throat.endpoints.tail'] = pn['throat.endpoints.tail']\n    net['throat.conduit_lengths.pore1'] = pn['throat.conduit_lengths.pore1']\n    net['throat.conduit_lengths.pore2'] = pn['throat.conduit_lengths.pore2']\n    net['throat.conduit_lengths.throat'] = pn['throat.conduit_lengths.throat']\n    net['pore.area'] = pn['pore.area']\n    prj = pn.project\n    prj.clear()\n    wrk = op.Workspace()\n    wrk.close_project(prj)\n\n    return net", "response": "r This function takes an image that is partitioned into individual pore regions and returns a dictionary containing all the pore and throat geometry as well as network connectivity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict_to_vtk(data, path='./dictvtk', voxel_size=1, origin=(0, 0, 0)):\n    vs = voxel_size\n    for entry in data:\n        if data[entry].dtype == bool:\n            data[entry] = data[entry].astype(np.int8)\n        if data[entry].flags['C_CONTIGUOUS']:\n            data[entry] = np.ascontiguousarray(data[entry])\n    imageToVTK(path, cellData=data, spacing=(vs, vs, vs), origin=origin)", "response": "r Converts a dictionary of images into a vtk file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distance_transform_lin(im, axis=0, mode='both'):\n    if im.ndim != im.squeeze().ndim:\n        warnings.warn('Input image conains a singleton axis:' + str(im.shape) +\n                      ' Reduce dimensionality with np.squeeze(im) to avoid' +\n                      ' unexpected behavior.')\n    if mode in ['backward', 'reverse']:\n        im = sp.flip(im, axis)\n        im = distance_transform_lin(im=im, axis=axis, mode='forward')\n        im = sp.flip(im, axis)\n        return im\n    elif mode in ['both']:\n        im_f = distance_transform_lin(im=im, axis=axis, mode='forward')\n        im_b = distance_transform_lin(im=im, axis=axis, mode='backward')\n        return sp.minimum(im_f, im_b)\n    else:\n        b = sp.cumsum(im > 0, axis=axis)\n        c = sp.diff(b*(im == 0), axis=axis)\n        d = sp.minimum.accumulate(c, axis=axis)\n        if im.ndim == 1:\n            e = sp.pad(d, pad_width=[1, 0], mode='constant', constant_values=0)\n        elif im.ndim == 2:\n            ax = [[[1, 0], [0, 0]], [[0, 0], [1, 0]]]\n            e = sp.pad(d, pad_width=ax[axis], mode='constant', constant_values=0)\n        elif im.ndim == 3:\n            ax = [[[1, 0], [0, 0], [0, 0]],\n                  [[0, 0], [1, 0], [0, 0]],\n                  [[0, 0], [0, 0], [1, 0]]]\n            e = sp.pad(d, pad_width=ax[axis], mode='constant', constant_values=0)\n        f = im*(b + e)\n        return f", "response": "r This function transforms the image into a linear distance to the nearest solid image along the specified axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef snow_partitioning(im, dt=None, r_max=4, sigma=0.4, return_all=False,\n                      mask=True, randomize=True):\n    r\"\"\"\n    Partitions the void space into pore regions using a marker-based watershed\n    algorithm, with specially filtered peaks as markers.\n\n    The SNOW network extraction algorithm (Sub-Network of an Over-segmented\n    Watershed) was designed to handle to perculiarities of high porosity\n    materials, but it applies well to other materials as well.\n\n    Parameters\n    ----------\n    im : array_like\n        A boolean image of the domain, with ``True`` indicating the pore space\n        and ``False`` elsewhere.\n    dt : array_like, optional\n        The distance transform of the pore space.  This is done automatically\n        if not provided, but if the distance transform has already been\n        computed then supplying it can save some time.\n    r_max : int\n        The radius of the spherical structuring element to use in the Maximum\n        filter stage that is used to find peaks.  The default is 4\n    sigma : float\n        The standard deviation of the Gaussian filter used in step 1.  The\n        default is 0.4.  If 0 is given then the filter is not applied, which is\n        useful if a distance transform is supplied as the ``im`` argument that\n        has already been processed.\n    return_all : boolean\n        If set to ``True`` a named tuple is returned containing the original\n        image, the distance transform, the filtered peaks, and the final\n        pore regions.  The default is ``False``\n    mask : boolean\n        Apply a mask to the regions where the solid phase is.  Default is\n        ``True``\n    randomize : boolean\n        If ``True`` (default), then the region colors will be randomized before\n        returning.  This is helpful for visualizing otherwise neighboring\n        regions have simlar coloring are are hard to distinguish.\n\n    Returns\n    -------\n    image : ND-array\n        An image the same shape as ``im`` with the void space partitioned into\n        pores using a marker based watershed with the peaks found by the\n        SNOW algorithm [1].\n\n    Notes\n    -----\n    If ``return_all`` is ``True`` then a **named tuple** is returned containing\n    all of the images used during the process.  They can be access as\n    attriutes with the following names:\n\n        * ``im``: The binary image of the void space\n        * ``dt``: The distance transform of the image\n        * ``peaks``: The peaks of the distance transform after applying the\n        steps of the SNOW algorithm\n        * ``regions``: The void space partitioned into pores using a marker\n        based watershed with the peaks found by the SNOW algorithm\n\n    References\n    ----------\n    [1] Gostick, J. \"A versatile and efficient network extraction algorithm\n    using marker-based watershed segmenation\".  Physical Review E. (2017)\n\n    \"\"\"\n    tup = namedtuple('results', field_names=['im', 'dt', 'peaks', 'regions'])\n    print('_'*60)\n    print(\"Beginning SNOW Algorithm\")\n    im_shape = sp.array(im.shape)\n    if im.dtype is not bool:\n        print('Converting supplied image (im) to boolean')\n        im = im > 0\n    if dt is None:\n        print('Peforming Distance Transform')\n        if sp.any(im_shape == 1):\n            ax = sp.where(im_shape == 1)[0][0]\n            dt = spim.distance_transform_edt(input=im.squeeze())\n            dt = sp.expand_dims(dt, ax)\n        else:\n            dt = spim.distance_transform_edt(input=im)\n\n    tup.im = im\n    tup.dt = dt\n\n    if sigma > 0:\n        print('Applying Gaussian blur with sigma =', str(sigma))\n        dt = spim.gaussian_filter(input=dt, sigma=sigma)\n\n    peaks = find_peaks(dt=dt, r_max=r_max)\n    print('Initial number of peaks: ', spim.label(peaks)[1])\n    peaks = trim_saddle_points(peaks=peaks, dt=dt, max_iters=500)\n    print('Peaks after trimming saddle points: ', spim.label(peaks)[1])\n    peaks = trim_nearby_peaks(peaks=peaks, dt=dt)\n    peaks, N = spim.label(peaks)\n    print('Peaks after trimming nearby peaks: ', N)\n    tup.peaks = peaks\n    if mask:\n        mask_solid = im > 0\n    else:\n        mask_solid = None\n    regions = watershed(image=-dt, markers=peaks, mask=mask_solid)\n    if randomize:\n        regions = randomize_colors(regions)\n    if return_all:\n        tup.regions = regions\n        return tup\n    else:\n        return regions", "response": "r This function is used to partition the void space into pore regions using a marker - based watershed algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trim_floating_solid(im):\n    im = sp.copy(im)\n    holes = find_disconnected_voxels(~im)\n    im[holes] = True\n    return im", "response": "r Removes all solid that is not attached to edges of the image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_chords(im, spacing=1, axis=0, trim_edges=True, label=False):\n    if im.ndim != im.squeeze().ndim:\n        warnings.warn('Input image conains a singleton axis:' + str(im.shape) +\n                      ' Reduce dimensionality with np.squeeze(im) to avoid' +\n                      ' unexpected behavior.')\n    if spacing < 0:\n        raise Exception('Spacing cannot be less than 0')\n    if spacing == 0:\n        label = True\n    result = sp.zeros(im.shape, dtype=int)  # Will receive chords at end\n    slxyz = [slice(None, None, spacing*(axis != i) + 1) for i in [0, 1, 2]]\n    slices = tuple(slxyz[:im.ndim])\n    s = [[0, 1, 0], [0, 1, 0], [0, 1, 0]]  # Straight-line structuring element\n    if im.ndim == 3:  # Make structuring element 3D if necessary\n        s = sp.pad(sp.atleast_3d(s), pad_width=((0, 0), (0, 0), (1, 1)),\n                   mode='constant', constant_values=0)\n    im = im[slices]\n    s = sp.swapaxes(s, 0, axis)\n    chords = spim.label(im, structure=s)[0]\n    if trim_edges:  # Label on border chords will be set to 0\n        chords = clear_border(chords)\n    result[slices] = chords  # Place chords into empty image created at top\n    if label is False:  # Remove label if not requested\n        result = result > 0\n    return result", "response": "r This function applies chords to the void space in the specified direction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef local_thickness(im, sizes=25, mode='hybrid'):\n    im_new = porosimetry(im=im, sizes=sizes, access_limited=False, mode=mode)\n    return im_new", "response": "r This function calculates the radius of the largest sphere that is centered on the pore and fits entirely within the foreground."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef porosimetry(im, sizes=25, inlets=None, access_limited=True,\n                mode='hybrid'):\n    r\"\"\"\n    Performs a porosimetry simulution on the image\n\n    Parameters\n    ----------\n    im : ND-array\n        An ND image of the porous material containing True values in the\n        pore space.\n\n    sizes : array_like or scalar\n        The sizes to invade.  If a list of values of provided they are used\n        directly.  If a scalar is provided then that number of points spanning\n        the min and max of the distance transform are used.\n\n    inlets : ND-array, boolean\n        A boolean mask with True values indicating where the invasion\n        enters the image.  By default all faces are considered inlets,\n        akin to a mercury porosimetry experiment.  Users can also apply\n        solid boundaries to their image externally before passing it in,\n        allowing for complex inlets like circular openings, etc.  This argument\n        is only used if ``access_limited`` is ``True``.\n\n    access_limited : Boolean\n        This flag indicates if the intrusion should only occur from the\n        surfaces (``access_limited`` is True, which is the default), or\n        if the invading phase should be allowed to appear in the core of\n        the image.  The former simulates experimental tools like mercury\n        intrusion porosimetry, while the latter is useful for comparison\n        to gauge the extent of shielding effects in the sample.\n\n    mode : string\n        Controls with method is used to compute the result.  Options are:\n\n        'hybrid' - (default) Performs a distance tranform of the void space,\n        thresholds to find voxels larger than ``sizes[i]``, trims the resulting\n        mask if ``access_limitations`` is ``True``, then dilates it using the\n        efficient fft-method to obtain the non-wetting fluid configuration.\n\n        'dt' - Same as 'hybrid', except uses a second distance transform,\n        relative to the thresholded mask, to find the invading fluid\n        configuration.  The choice of 'dt' or 'hybrid' depends on speed, which\n        is system and installation specific.\n\n        'mio' - Using a single morphological image opening step to obtain the\n        invading fluid confirguration directly, *then* trims if\n        ``access_limitations`` is ``True``.  This method is not ideal and is\n        included mostly for comparison purposes.  The morphological operations\n        are done using fft-based method implementations.\n\n    Returns\n    -------\n    image : ND-array\n        A copy of ``im`` with voxel values indicating the sphere radius at\n        which it becomes accessible from the inlets.  This image can be used\n        to find invading fluid configurations as a function of applied\n        capillary pressure by applying a boolean comparison:\n        ``inv_phase = im > r`` where ``r`` is the radius (in voxels) of the\n        invading sphere.  Of course, ``r`` can be converted to capillary\n        pressure using your favorite model.\n\n    Notes\n    -----\n    There are many ways to perform this filter, and PoreSpy offer 3, which\n    users can choose between via the ``mode`` argument.  These methods all\n    work in a similar way by finding which foreground voxels can accomodate\n    a sphere of a given radius, then repeating for smaller radii.\n\n    See Also\n    --------\n    fftmorphology\n    local_thickness\n\n    \"\"\"\n    if im.ndim != im.squeeze().ndim:\n        warnings.warn('Input image conains a singleton axis:' + str(im.shape) +\n                      ' Reduce dimensionality with np.squeeze(im) to avoid' +\n                      ' unexpected behavior.')\n\n    dt = spim.distance_transform_edt(im > 0)\n\n    if inlets is None:\n        inlets = get_border(im.shape, mode='faces')\n\n    if isinstance(sizes, int):\n        sizes = sp.logspace(start=sp.log10(sp.amax(dt)), stop=0, num=sizes)\n    else:\n        sizes = sp.unique(sizes)[-1::-1]\n\n    if im.ndim == 2:\n        strel = ps_disk\n    else:\n        strel = ps_ball\n\n    if mode == 'mio':\n        pw = int(sp.floor(dt.max()))\n        impad = sp.pad(im, mode='symmetric', pad_width=pw)\n        inletspad = sp.pad(inlets, mode='symmetric', pad_width=pw)\n        inlets = sp.where(inletspad)\n#        sizes = sp.unique(sp.around(sizes, decimals=0).astype(int))[-1::-1]\n        imresults = sp.zeros(sp.shape(impad))\n        for r in tqdm(sizes):\n            imtemp = fftmorphology(impad, strel(r), mode='erosion')\n            if access_limited:\n                imtemp = trim_disconnected_blobs(imtemp, inlets)\n            imtemp = fftmorphology(imtemp, strel(r), mode='dilation')\n            if sp.any(imtemp):\n                imresults[(imresults == 0)*imtemp] = r\n        imresults = extract_subsection(imresults, shape=im.shape)\n    elif mode == 'dt':\n        inlets = sp.where(inlets)\n        imresults = sp.zeros(sp.shape(im))\n        for r in tqdm(sizes):\n            imtemp = dt >= r\n            if access_limited:\n                imtemp = trim_disconnected_blobs(imtemp, inlets)\n            if sp.any(imtemp):\n                imtemp = spim.distance_transform_edt(~imtemp) < r\n                imresults[(imresults == 0)*imtemp] = r\n    elif mode == 'hybrid':\n        inlets = sp.where(inlets)\n        imresults = sp.zeros(sp.shape(im))\n        for r in tqdm(sizes):\n            imtemp = dt >= r\n            if access_limited:\n                imtemp = trim_disconnected_blobs(imtemp, inlets)\n            if sp.any(imtemp):\n                imtemp = fftconvolve(imtemp, strel(r), mode='same') > 0.0001\n                imresults[(imresults == 0)*imtemp] = r\n    else:\n        raise Exception('Unreckognized mode ' + mode)\n    return imresults", "response": "r Performs a porosimetry simulution on the image and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim_disconnected_blobs(im, inlets):\n    temp = sp.zeros_like(im)\n    temp[inlets] = True\n    labels, N = spim.label(im + temp)\n    im = im ^ (clear_border(labels=labels) > 0)\n    return im", "response": "r Removes all voxels not connected to the specified inlets from the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nphase_border(im, include_diagonals=False):\n    r'''\n    Identifies the voxels in regions that border *N* other regions.\n\n    Useful for finding triple-phase boundaries.\n\n    Parameters\n    ----------\n    im : ND-array\n        An ND image of the porous material containing discrete values in the\n        pore space identifying different regions. e.g. the result of a\n        snow-partition\n\n    include_diagonals : boolean\n        When identifying bordering pixels (2D) and voxels (3D) include those\n        shifted along more than one axis\n\n    Returns\n    -------\n    image : ND-array\n        A copy of ``im`` with voxel values equal to the number of uniquely\n        different bordering values\n    '''\n    if im.ndim != im.squeeze().ndim:\n        warnings.warn('Input image conains a singleton axis:' + str(im.shape) +\n                      ' Reduce dimensionality with np.squeeze(im) to avoid' +\n                      ' unexpected behavior.')\n    # Get dimension of image\n    ndim = len(np.shape(im))\n    if ndim not in [2, 3]:\n        raise NotImplementedError(\"Function only works for 2d and 3d images\")\n    # Pad image to handle edges\n    im = np.pad(im, pad_width=1, mode='edge')\n    # Stack rolled images for each neighbor to be inspected\n    stack = _make_stack(im, include_diagonals)\n    # Sort the stack along the last axis\n    stack.sort()\n    out = np.ones_like(im)\n    # Run through stack recording when neighbor id changes\n    # Number of changes is number of unique bordering regions\n    for k in range(np.shape(stack)[ndim])[1:]:\n        if ndim == 2:\n            mask = stack[:, :, k] != stack[:, :, k-1]\n        elif ndim == 3:\n            mask = stack[:, :, :, k] != stack[:, :, :, k-1]\n        out += mask\n    # Un-pad\n    if ndim == 2:\n        return out[1:-1, 1:-1].copy()\n    else:\n        return out[1:-1, 1:-1, 1:-1].copy()", "response": "r Returns a ND array that contains the number of unique bordering voxels in the image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_boundary_regions(regions=None, faces=['front', 'back', 'left',\n                                              'right', 'top', 'bottom']):\n    r\"\"\"\n    Given an image partitioned into regions, pads specified faces with new\n    regions\n\n    Parameters\n    ----------\n    regions : ND-array\n        An image of the pore space partitioned into regions and labeled\n    faces : list of strings\n        The faces of ``regions`` which should have boundaries added.  Options\n        are:\n\n        *'right'* - Adds boundaries to the x=0 face (``im[0, :, :]``)\n\n        *'left'* - Adds boundaries to the x=X face (``im[-1, :, :]``)\n\n        *'front'* - Adds boundaries to the y=0 face (``im[:, ), :]``)\n\n        *'back'* - Adds boundaries to the x=0 face (``im[:, -1, :]``)\n\n        *'bottom'* - Adds boundaries to the x=0 face (``im[:, :, 0]``)\n\n        *'top'* - Adds boundaries to the x=0 face (``im[:, :, -1]``)\n\n        The default is all faces.\n\n    Returns\n    -------\n    image : ND-array\n        A copy of ``regions`` with the specified boundaries added, so will be\n        slightly larger in each direction where boundaries were added.\n\n    \"\"\"\n    # -------------------------------------------------------------------------\n    # Edge pad segmentation and distance transform\n    if faces is not None:\n        regions = sp.pad(regions, 1, 'edge')\n        # ---------------------------------------------------------------------\n        if regions.ndim == 3:\n            # Remove boundary nodes interconnection\n            regions[:, :, 0] = regions[:, :, 0] + regions.max()\n            regions[:, :, -1] = regions[:, :, -1] + regions.max()\n            regions[0, :, :] = regions[0, :, :] + regions.max()\n            regions[-1, :, :] = regions[-1, :, :] + regions.max()\n            regions[:, 0, :] = regions[:, 0, :] + regions.max()\n            regions[:, -1, :] = regions[:, -1, :] + regions.max()\n            regions[:, :, 0] = (~find_boundaries(regions[:, :, 0],\n                                                 mode='outer')) * regions[:, :, 0]\n            regions[:, :, -1] = (~find_boundaries(regions[:, :, -1],\n                                                  mode='outer')) * regions[:, :, -1]\n            regions[0, :, :] = (~find_boundaries(regions[0, :, :],\n                                                 mode='outer')) * regions[0, :, :]\n            regions[-1, :, :] = (~find_boundaries(regions[-1, :, :],\n                                                  mode='outer')) * regions[-1, :, :]\n            regions[:, 0, :] = (~find_boundaries(regions[:, 0, :],\n                                                 mode='outer')) * regions[:, 0, :]\n            regions[:, -1, :] = (~find_boundaries(regions[:, -1, :],\n                                                  mode='outer')) * regions[:, -1, :]\n            # -----------------------------------------------------------------\n            regions = sp.pad(regions, 2, 'edge')\n\n            # Remove unselected faces\n            if 'front' not in faces:\n                regions = regions[:, 3:, :]  # y\n            if 'back' not in faces:\n                regions = regions[:, :-3, :]\n            if 'left' not in faces:\n                regions = regions[3:, :, :]  # x\n            if 'right' not in faces:\n                regions = regions[:-3, :, :]\n            if 'bottom' not in faces:\n                regions = regions[:, :, 3:]  # z\n            if 'top' not in faces:\n                regions = regions[:, :, :-3]\n\n        elif regions.ndim == 2:\n            # Remove boundary nodes interconnection\n            regions[0, :] = regions[0, :] + regions.max()\n            regions[-1, :] = regions[-1, :] + regions.max()\n            regions[:, 0] = regions[:, 0] + regions.max()\n            regions[:, -1] = regions[:, -1] + regions.max()\n            regions[0, :] = (~find_boundaries(regions[0, :],\n                                              mode='outer')) * regions[0, :]\n            regions[-1, :] = (~find_boundaries(regions[-1, :],\n                                               mode='outer')) * regions[-1, :]\n            regions[:, 0] = (~find_boundaries(regions[:, 0],\n                                              mode='outer')) * regions[:, 0]\n            regions[:, -1] = (~find_boundaries(regions[:, -1],\n                                               mode='outer')) * regions[:, -1]\n            # -----------------------------------------------------------------\n            regions = sp.pad(regions, 2, 'edge')\n\n            # Remove unselected faces\n            if 'left' not in faces:\n                regions = regions[3:, :]  # x\n            if 'right' not in faces:\n                regions = regions[:-3, :]\n            if 'front' not in faces and 'bottom' not in faces:\n                regions = regions[:, 3:]  # y\n            if 'back' not in faces and 'top' not in faces:\n                regions = regions[:, :-3]\n        else:\n            print('add_boundary_regions works only on 2D and 3D images')\n        # ---------------------------------------------------------------------\n        # Make labels contiguous\n        regions = make_contiguous(regions)\n    else:\n        regions = regions\n\n    return regions", "response": "r Adds boundary nodes to the new ND - array containing the new ND - array of regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_voxel_image(network, pore_shape=\"sphere\", throat_shape=\"cylinder\",\n                         max_dim=None, verbose=1, rtol=0.1):\n    r\"\"\"\n    Generates voxel image from an OpenPNM network object.\n\n    Parameters\n    ----------\n    network : OpenPNM GenericNetwork\n        Network from which voxel image is to be generated\n\n    pore_shape : str\n        Shape of pores in the network, valid choices are \"sphere\", \"cube\"\n\n    throat_shape : str\n        Shape of throats in the network, valid choices are \"cylinder\", \"cuboid\"\n\n    max_dim : int\n        Number of voxels in the largest dimension of the network\n\n    rtol : float\n        Stopping criteria for finding the smallest voxel image such that\n        further increasing the number of voxels in each dimension by 25% would\n        improve the predicted porosity of the image by less that ``rtol``\n\n    Returns\n    -------\n    im : ND-array\n        Voxelated image corresponding to the given pore network model\n\n    Notes\n    -----\n    (1) The generated voxelated image is labeled with 0s, 1s and 2s signifying\n    solid phase, pores, and throats respectively.\n\n    (2) If max_dim is not provided, the method calculates it such that the\n    further increasing it doesn't change porosity by much.\n\n    \"\"\"\n    print(\"\\n\" + \"-\" * 44, flush=True)\n    print(\"| Generating voxel image from pore network |\", flush=True)\n    print(\"-\" * 44, flush=True)\n\n    # If max_dim is provided, generate voxel image using max_dim\n    if max_dim is not None:\n        return _generate_voxel_image(network, pore_shape, throat_shape,\n                                     max_dim=max_dim, verbose=verbose)\n    else:\n        max_dim = 200\n\n    # If max_dim is not provided, find best max_dim that predicts porosity\n    eps_old = 200\n    err = 100  # percent\n\n    while err > rtol:\n        im = _generate_voxel_image(network, pore_shape, throat_shape,\n                                   max_dim=max_dim, verbose=verbose)\n        eps = im.astype(bool).sum() / sp.prod(im.shape)\n\n        err = abs(1 - eps / eps_old)\n        eps_old = eps\n        max_dim = int(max_dim * 1.25)\n\n    if verbose:\n        print(\"\\nConverged at max_dim = {max_dim} voxels.\\n\")\n\n    return im", "response": "r Generate a voxel image from an OpenPNM network object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label_boundary_cells(network=None, boundary_faces=None):\n    f = boundary_faces\n    if f is not None:\n        coords = network['pore.coords']\n        condition = coords[~network['pore.boundary']]\n        dic = {'left': 0, 'right': 0, 'front': 1, 'back': 1,\n               'top': 2, 'bottom': 2}\n        if all(coords[:, 2] == 0):\n            dic['top'] = 1\n            dic['bottom'] = 1\n        for i in f:\n            if i in ['left', 'front', 'bottom']:\n                network['pore.{}'.format(i)] = (coords[:, dic[i]] <\n                                                min(condition[:, dic[i]]))\n            elif i in ['right', 'back', 'top']:\n                network['pore.{}'.format(i)] = (coords[:, dic[i]] >\n                                                max(condition[:, dic[i]]))\n\n    return network", "response": "r Assigns labels to boundary pores in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_shape(im, element, center=None, corner=None, value=1,\n                 mode='overwrite'):\n    r\"\"\"\n    Inserts sub-image into a larger image at the specified location.\n\n    If the inserted image extends beyond the boundaries of the image it will\n    be cropped accordingly.\n\n    Parameters\n    ----------\n    im : ND-array\n        The image into which the sub-image will be inserted\n    element : ND-array\n        The sub-image to insert\n    center : tuple\n        Coordinates indicating the position in the main image where the\n        inserted imaged will be centered.  If ``center`` is given then\n        ``corner`` cannot be specified.  Note that ``center`` can only be\n        used if all dimensions of ``element`` are odd, otherwise the meaning\n        of center is not defined.\n    corner : tuple\n        Coordinates indicating the position in the main image where the\n        lower corner (i.e. [0, 0, 0]) of the inserted image should be anchored.\n        If ``corner`` is given then ``corner`` cannot be specified.\n    value : scalar\n        A scalar value to apply to the sub-image.  The default is 1.\n    mode : string\n        If 'overwrite' (default) the inserted image replaces the values in the\n        main image.  If 'overlay' the inserted image is added to the main\n        image.  In both cases the inserted image is multiplied by ``value``\n        first.\n\n    Returns\n    -------\n    im : ND-array\n        A copy of ``im`` with the supplied element inserted.\n\n    \"\"\"\n    im = im.copy()\n    if im.ndim != element.ndim:\n        raise Exception('Image shape ' + str(im.shape)\n                        + ' and element shape ' + str(element.shape)\n                        + ' do not match')\n    s_im = []\n    s_el = []\n    if (center is not None) and (corner is None):\n        for dim in range(im.ndim):\n            r, d = sp.divmod(element.shape[dim], 2)\n            if d == 0:\n                raise Exception('Cannot specify center point when element ' +\n                                'has one or more even dimension')\n            lower_im = sp.amax((center[dim] - r, 0))\n            upper_im = sp.amin((center[dim] + r + 1, im.shape[dim]))\n            s_im.append(slice(lower_im, upper_im))\n            lower_el = sp.amax((lower_im - center[dim] + r, 0))\n            upper_el = sp.amin((upper_im - center[dim] + r,\n                                element.shape[dim]))\n            s_el.append(slice(lower_el, upper_el))\n    elif (corner is not None) and (center is None):\n        for dim in range(im.ndim):\n            L = int(element.shape[dim])\n            lower_im = sp.amax((corner[dim], 0))\n            upper_im = sp.amin((corner[dim] + L, im.shape[dim]))\n            s_im.append(slice(lower_im, upper_im))\n            lower_el = sp.amax((lower_im - corner[dim], 0))\n            upper_el = sp.amin((upper_im - corner[dim],\n                                element.shape[dim]))\n            s_el.append(slice(min(lower_el, upper_el), upper_el))\n    else:\n        raise Exception('Cannot specify both corner and center')\n\n    if mode == 'overlay':\n        im[tuple(s_im)] = im[tuple(s_im)] + element[tuple(s_el)]*value\n    elif mode == 'overwrite':\n        im[tuple(s_im)] = element[tuple(s_el)]*value\n    else:\n        raise Exception('Invalid mode ' + mode)\n    return im", "response": "r Inserts sub - image into a larger image at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef RSA(im: array, radius: int, volume_fraction: int = 1,\n        mode: str = 'extended'):\n    r\"\"\"\n    Generates a sphere or disk packing using Random Sequential Addition\n\n    This which ensures that spheres do not overlap but does not guarantee they\n    are tightly packed.\n\n    Parameters\n    ----------\n    im : ND-array\n        The image into which the spheres should be inserted.  By accepting an\n        image rather than a shape, it allows users to insert spheres into an\n        already existing image.  To begin the process, start with an array of\n        zero such as ``im = np.zeros([200, 200], dtype=bool)``.\n    radius : int\n        The radius of the disk or sphere to insert.\n    volume_fraction : scalar\n        The fraction of the image that should be filled with spheres.  The\n        spheres are addeds 1's, so each sphere addition increases the\n        ``volume_fraction`` until the specified limit is reach.\n    mode : string\n        Controls how the edges of the image are handled.  Options are:\n\n        'extended' - Spheres are allowed to extend beyond the edge of the image\n\n        'contained' - Spheres are all completely within the image\n\n        'periodic' - The portion of a sphere that extends beyond the image is\n        inserted into the opposite edge of the image (Not Implemented Yet!)\n\n    Returns\n    -------\n    image : ND-array\n        A copy of ``im`` with spheres of specified radius *added* to the\n        background.\n\n    Notes\n    -----\n    Each sphere is filled with 1's, but the center is marked with a 2.  This\n    allows easy boolean masking to extract only the centers, which can be\n    converted to coordinates using ``scipy.where`` and used for other purposes.\n    The obtain only the spheres, use``im = im == 1``.\n\n    This function adds spheres to the background of the received ``im``, which\n    allows iteratively adding spheres of different radii to the unfilled space.\n\n    References\n    ----------\n    [1] Random Heterogeneous Materials, S. Torquato (2001)\n\n    \"\"\"\n    # Note: The 2D vs 3D splitting of this just me being lazy...I can't be\n    # bothered to figure it out programmatically right now\n    # TODO: Ideally the spheres should be added periodically\n    print(78*'\u2015')\n    print('RSA: Adding spheres of size ' + str(radius))\n    d2 = len(im.shape) == 2\n    mrad = 2*radius\n    if d2:\n        im_strel = ps_disk(radius)\n        mask_strel = ps_disk(mrad)\n    else:\n        im_strel = ps_ball(radius)\n        mask_strel = ps_ball(mrad)\n    if sp.any(im > 0):\n        # Dilate existing objects by im_strel to remove pixels near them\n        # from consideration for sphere placement\n        mask = ps.tools.fftmorphology(im > 0, im_strel > 0, mode='dilate')\n        mask = mask.astype(int)\n    else:\n        mask = sp.zeros_like(im)\n    if mode == 'contained':\n        mask = _remove_edge(mask, radius)\n    elif mode == 'extended':\n        pass\n    elif mode == 'periodic':\n        raise Exception('Periodic edges are not implemented yet')\n    else:\n        raise Exception('Unrecognized mode: ' + mode)\n    vf = im.sum()/im.size\n    free_spots = sp.argwhere(mask == 0)\n    i = 0\n    while vf <= volume_fraction and len(free_spots) > 0:\n        choice = sp.random.randint(0, len(free_spots), size=1)\n        if d2:\n            [x, y] = free_spots[choice].flatten()\n            im = _fit_strel_to_im_2d(im, im_strel, radius, x, y)\n            mask = _fit_strel_to_im_2d(mask, mask_strel, mrad, x, y)\n            im[x, y] = 2\n        else:\n            [x, y, z] = free_spots[choice].flatten()\n            im = _fit_strel_to_im_3d(im, im_strel, radius, x, y, z)\n            mask = _fit_strel_to_im_3d(mask, mask_strel, mrad, x, y, z)\n            im[x, y, z] = 2\n        free_spots = sp.argwhere(mask == 0)\n        vf = im.sum()/im.size\n        i += 1\n    if vf > volume_fraction:\n        print('Volume Fraction', volume_fraction, 'reached')\n    if len(free_spots) == 0:\n        print('No more free spots', 'Volume Fraction', vf)\n    return im", "response": "r Generates a random set of spheres for the given image and radius and volume fraction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bundle_of_tubes(shape: List[int], spacing: int):\n    shape = sp.array(shape)\n    if sp.size(shape) == 1:\n        shape = sp.full((3, ), int(shape))\n    if sp.size(shape) == 2:\n        shape = sp.hstack((shape, [1]))\n    temp = sp.zeros(shape=shape[:2])\n    Xi = sp.ceil(sp.linspace(spacing/2,\n                             shape[0]-(spacing/2)-1,\n                             int(shape[0]/spacing)))\n    Xi = sp.array(Xi, dtype=int)\n    Yi = sp.ceil(sp.linspace(spacing/2,\n                             shape[1]-(spacing/2)-1,\n                             int(shape[1]/spacing)))\n    Yi = sp.array(Yi, dtype=int)\n    temp[tuple(sp.meshgrid(Xi, Yi))] = 1\n    inds = sp.where(temp)\n    for i in range(len(inds[0])):\n        r = sp.random.randint(1, (spacing/2))\n        try:\n            s1 = slice(inds[0][i]-r, inds[0][i]+r+1)\n            s2 = slice(inds[1][i]-r, inds[1][i]+r+1)\n            temp[s1, s2] = ps_disk(r)\n        except ValueError:\n            odd_shape = sp.shape(temp[s1, s2])\n            temp[s1, s2] = ps_disk(r)[:odd_shape[0], :odd_shape[1]]\n    im = sp.broadcast_to(array=sp.atleast_3d(temp), shape=shape)\n    return im", "response": "r Creates a 3D image of a bundle of tubes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_noise(shape: List[int], porosity=None, octaves: int = 3,\n                   frequency: int = 32, mode: str = 'simplex'):\n    r\"\"\"\n    Generate a field of spatially correlated random noise using the Perlin\n    noise algorithm, or the updated Simplex noise algorithm.\n\n    Parameters\n    ----------\n    shape : array_like\n        The size of the image to generate in [Nx, Ny, Nz] where N is the\n        number of voxels.\n\n    porosity : float\n        If specified, this will threshold the image to the specified value\n        prior to returning.  If no value is given (the default), then the\n        scalar noise field is returned.\n\n    octaves : int\n        Controls the *texture* of the noise, with higher octaves giving more\n        complex features over larger length scales.\n\n    frequency : array_like\n        Controls the relative sizes of the features, with higher frequencies\n        giving larger features.  A scalar value will apply the same frequency\n        in all directions, given an isotropic field; a vector value will\n        apply the specified values along each axis to create anisotropy.\n\n    mode : string\n        Which noise algorithm to use, either ``'simplex'`` (default) or\n        ``'perlin'``.\n\n    Returns\n    -------\n    image : ND-array\n        If porosity is given, then a boolean array with ``True`` values\n        denoting the pore space is returned.  If not, then normally\n        distributed and spatially correlated randomly noise is returned.\n\n    Notes\n    -----\n    This method depends the a package called 'noise' which must be\n    compiled. It is included in the Anaconda distribution, or a platform\n    specific binary can be downloaded.\n\n    See Also\n    --------\n    porespy.tools.norm_to_uniform\n\n    \"\"\"\n    try:\n        import noise\n    except ModuleNotFoundError:\n        raise Exception(\"The noise package must be installed\")\n    shape = sp.array(shape)\n    if sp.size(shape) == 1:\n        Lx, Ly, Lz = sp.full((3, ), int(shape))\n    elif len(shape) == 2:\n        Lx, Ly = shape\n        Lz = 1\n    elif len(shape) == 3:\n        Lx, Ly, Lz = shape\n    if mode == 'simplex':\n        f = noise.snoise3\n    else:\n        f = noise.pnoise3\n    frequency = sp.atleast_1d(frequency)\n    if frequency.size == 1:\n        freq = sp.full(shape=[3, ], fill_value=frequency[0])\n    elif frequency.size == 2:\n        freq = sp.concatenate((frequency, [1]))\n    else:\n        freq = sp.array(frequency)\n    im = sp.zeros(shape=[Lx, Ly, Lz], dtype=float)\n    for x in range(Lx):\n        for y in range(Ly):\n            for z in range(Lz):\n                im[x, y, z] = f(x=x/freq[0], y=y/freq[1], z=z/freq[2],\n                                octaves=octaves)\n    im = im.squeeze()\n    if porosity:\n        im = norm_to_uniform(im, scale=[0, 1])\n        im = im < porosity\n    return im", "response": "r Generate a random noise field for the specified size porosity octaves and frequency."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an image containing amorphous blobs.", "response": "def blobs(shape: List[int], porosity: float = 0.5, blobiness: int = 1):\n    \"\"\"\n    Generates an image containing amorphous blobs\n\n    Parameters\n    ----------\n    shape : list\n        The size of the image to generate in [Nx, Ny, Nz] where N is the\n        number of voxels\n\n    porosity : float\n        If specified, this will threshold the image to the specified value\n        prior to returning.  If ``None`` is specified, then the scalar noise\n        field is converted to a uniform distribution and returned without\n        thresholding.\n\n    blobiness : int or list of ints(default = 1)\n        Controls the morphology of the blobs.  A higher number results in\n        a larger number of small blobs.  If a list is supplied then the blobs\n        are anisotropic.\n\n    Returns\n    -------\n    image : ND-array\n        A boolean array with ``True`` values denoting the pore space\n\n    See Also\n    --------\n    norm_to_uniform\n\n    \"\"\"\n    blobiness = sp.array(blobiness)\n    shape = sp.array(shape)\n    if sp.size(shape) == 1:\n        shape = sp.full((3, ), int(shape))\n    sigma = sp.mean(shape)/(40*blobiness)\n    im = sp.random.random(shape)\n    im = spim.gaussian_filter(im, sigma=sigma)\n    im = norm_to_uniform(im, scale=[0, 1])\n    if porosity:\n        im = im < porosity\n    return im"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef line_segment(X0, X1):\n    X0 = sp.around(X0).astype(int)\n    X1 = sp.around(X1).astype(int)\n    if len(X0) == 3:\n        L = sp.amax(sp.absolute([[X1[0]-X0[0]], [X1[1]-X0[1]], [X1[2]-X0[2]]])) + 1\n        x = sp.rint(sp.linspace(X0[0], X1[0], L)).astype(int)\n        y = sp.rint(sp.linspace(X0[1], X1[1], L)).astype(int)\n        z = sp.rint(sp.linspace(X0[2], X1[2], L)).astype(int)\n        return [x, y, z]\n    else:\n        L = sp.amax(sp.absolute([[X1[0]-X0[0]], [X1[1]-X0[1]]])) + 1\n        x = sp.rint(sp.linspace(X0[0], X1[0], L)).astype(int)\n        y = sp.rint(sp.linspace(X0[1], X1[1], L)).astype(int)\n        return [x, y]", "response": "r Calculates the voxel coordinates of a straight line between the two given segments of the base segment X0 and X1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fit_strel_to_im_2d(im, strel, r, x, y):\n    elem = strel.copy()\n    x_dim, y_dim = im.shape\n    x_min = x-r\n    x_max = x+r+1\n    y_min = y-r\n    y_max = y+r+1\n    if x_min < 0:\n        x_adj = -x_min\n        elem = elem[x_adj:, :]\n        x_min = 0\n    elif x_max > x_dim:\n        x_adj = x_max - x_dim\n        elem = elem[:-x_adj, :]\n    if y_min < 0:\n        y_adj = -y_min\n        elem = elem[:, y_adj:]\n        y_min = 0\n    elif y_max > y_dim:\n        y_adj = y_max - y_dim\n        elem = elem[:, :-y_adj]\n    ex, ey = elem.shape\n    im[x_min:x_min+ex, y_min:y_min+ey] += elem\n    return im", "response": "r Helper function to add a structuring element to a 2D image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subdivide(im, divs=2):\n    # Expand scalar divs\n    if isinstance(divs, int):\n        divs = [divs for i in range(im.ndim)]\n    s = shape_split(im.shape, axis=divs)\n    return s", "response": "r This function returns slices into an image describing the specified number of sub - arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bbox_to_slices(bbox):\n    if len(bbox) == 4:\n        ret = (slice(bbox[0], bbox[2]),\n               slice(bbox[1], bbox[3]))\n    else:\n        ret = (slice(bbox[0], bbox[3]),\n               slice(bbox[1], bbox[4]),\n               slice(bbox[2], bbox[5]))\n    return ret", "response": "r Converts a bounding box coordinates into a tuple of slice objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_slice(im, center, size, pad=0):\n    p = sp.ones(shape=im.ndim, dtype=int) * sp.array(pad)\n    s = sp.ones(shape=im.ndim, dtype=int) * sp.array(size)\n    slc = []\n    for dim in range(im.ndim):\n        lower_im = sp.amax((center[dim] - s[dim] - p[dim], 0))\n        upper_im = sp.amin((center[dim] + s[dim] + 1 + p[dim], im.shape[dim]))\n        slc.append(slice(lower_im, upper_im))\n    return slc", "response": "r Returns a slice object into the image that bounds the feature and does not extend beyond the image boundaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_outer_region(im, r=0):\n    if r == 0:\n        dt = spim.distance_transform_edt(input=im)\n        r = int(sp.amax(dt)) * 2\n    im_padded = sp.pad(array=im, pad_width=r, mode='constant',\n                       constant_values=True)\n    dt = spim.distance_transform_edt(input=im_padded)\n    seeds = (dt >= r) + get_border(shape=im_padded.shape)\n    # Remove seeds not connected to edges\n    labels = spim.label(seeds)[0]\n    mask = labels == 1  # Assume label of 1 on edges, assured by adding border\n    dt = spim.distance_transform_edt(~mask)\n    outer_region = dt < r\n    outer_region = extract_subsection(im=outer_region, shape=im.shape)\n    return outer_region", "response": "r Finds regions of the image that are outside of the solid matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend_slice(s, shape, pad=1):\n    pad = int(pad)\n    a = []\n    for i, dim in zip(s, shape):\n        start = 0\n        stop = dim\n        if i.start - pad >= 0:\n            start = i.start - pad\n        if i.stop + pad < dim:\n            stop = i.stop + pad\n        a.append(slice(start, stop, None))\n    return tuple(a)", "response": "r Extends a list of slices to include additional voxels around the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef randomize_colors(im, keep_vals=[0]):\n    r'''\n    Takes a greyscale image and randomly shuffles the greyscale values, so that\n    all voxels labeled X will be labelled Y, and all voxels labeled Y will be\n    labeled Z, where X, Y, Z and so on are randomly selected from the values\n    in the input image.\n\n    This function is useful for improving the visibility of images with\n    neighboring regions that are only incrementally different from each other,\n    such as that returned by `scipy.ndimage.label`.\n\n    Parameters\n    ----------\n    im : array_like\n        An ND image of greyscale values.\n\n    keep_vals : array_like\n        Indicate which voxel values should NOT be altered.  The default is\n        `[0]` which is useful for leaving the background of the image\n        untouched.\n\n    Returns\n    -------\n    image : ND-array\n        An image the same size and type as ``im`` but with the greyscale values\n        reassigned.  The unique values in both the input and output images will\n        be identical.\n\n    Notes\n    -----\n    If the greyscale values in the input image are not contiguous then the\n    neither will they be in the output.\n\n    Examples\n    --------\n    >>> import porespy as ps\n    >>> import scipy as sp\n    >>> sp.random.seed(0)\n    >>> im = sp.random.randint(low=0, high=5, size=[4, 4])\n    >>> print(im)\n    [[4 0 3 3]\n     [3 1 3 2]\n     [4 0 0 4]\n     [2 1 0 1]]\n    >>> im_rand = ps.tools.randomize_colors(im)\n    >>> print(im_rand)\n    [[2 0 4 4]\n     [4 1 4 3]\n     [2 0 0 2]\n     [3 1 0 1]]\n\n    As can be seen, the 2's have become 3, 3's have become 4, and 4's have\n    become 2.  1's remained 1 by random accident.  0's remain zeros by default,\n    but this can be controlled using the `keep_vals` argument.\n\n    '''\n    im_flat = im.flatten()\n    keep_vals = sp.array(keep_vals)\n    swap_vals = ~sp.in1d(im_flat, keep_vals)\n    im_vals = sp.unique(im_flat[swap_vals])\n    new_vals = sp.random.permutation(im_vals)\n    im_map = sp.zeros(shape=[sp.amax(im_vals) + 1, ], dtype=int)\n    im_map[im_vals] = new_vals\n    im_new = im_map[im_flat]\n    im_new = sp.reshape(im_new, newshape=sp.shape(im))\n    return im_new", "response": "r Randomize the colors of the input image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_contiguous(im, keep_zeros=True):\n    im = sp.copy(im)\n    if keep_zeros:\n        mask = (im == 0)\n        im[mask] = im.min() - 1\n    im = im - im.min()\n    im_flat = im.flatten()\n    im_vals = sp.unique(im_flat)\n    im_map = sp.zeros(shape=sp.amax(im_flat) + 1)\n    im_map[im_vals] = sp.arange(0, sp.size(sp.unique(im_flat)))\n    im_new = im_map[im_flat]\n    im_new = sp.reshape(im_new, newshape=sp.shape(im))\n    im_new = sp.array(im_new, dtype=im_flat.dtype)\n    return im_new", "response": "r Creates a ND array that is contiguous with all values in the image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_border(shape, thickness=1, mode='edges', return_indices=False):\n    ndims = len(shape)\n    t = thickness\n    border = sp.ones(shape, dtype=bool)\n    if mode == 'faces':\n        if ndims == 2:\n            border[t:-t, t:-t] = False\n        if ndims == 3:\n            border[t:-t, t:-t, t:-t] = False\n    elif mode == 'edges':\n        if ndims == 2:\n            border[t:-t, t:-t] = False\n        if ndims == 3:\n            border[0::, t:-t, t:-t] = False\n            border[t:-t, 0::, t:-t] = False\n            border[t:-t, t:-t, 0::] = False\n    elif mode == 'corners':\n        if ndims == 2:\n            border[t:-t, 0::] = False\n            border[0::, t:-t] = False\n        if ndims == 3:\n            border[t:-t, 0::, 0::] = False\n            border[0::, t:-t, 0::] = False\n            border[0::, 0::, t:-t] = False\n    if return_indices:\n        border = sp.where(border)\n    return border", "response": "r Returns an array of specified shape with corners edges or faces labelled as\n    True."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest if a list of coordinates are inside a convex hull", "response": "def in_hull(points, hull):\n    \"\"\"\n    Test if a list of coordinates are inside a given convex hull\n\n    Parameters\n    ----------\n    points : array_like (N x ndims)\n        The spatial coordinates of the points to check\n\n    hull : scipy.spatial.ConvexHull object **OR** array_like\n        Can be either a convex hull object as returned by\n        ``scipy.spatial.ConvexHull`` or simply the coordinates of the points\n        that define the convex hull.\n\n    Returns\n    -------\n    result : 1D-array\n        A 1D-array Boolean array of length *N* indicating whether or not the\n        given points in ``points`` lies within the provided ``hull``.\n\n    \"\"\"\n    from scipy.spatial import Delaunay, ConvexHull\n    if isinstance(hull, ConvexHull):\n        hull = hull.points\n    hull = Delaunay(hull)\n    return hull.find_simplex(points) >= 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overlay(im1, im2, c):\n    shape = im2.shape\n    for ni in shape:\n        if ni % 2 == 0:\n            raise Exception(\"Structuring element must be odd-voxeled...\")\n\n    nx, ny, nz = [(ni - 1) // 2 for ni in shape]\n    cx, cy, cz = c\n\n    im1[cx-nx:cx+nx+1, cy-ny:cy+ny+1, cz-nz:cz+nz+1] += im2\n\n    return im1", "response": "r Overlays im2 onto im1 at the specified location in the specified voxelated image c."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_sphere(im, c, r):\n    c = sp.array(c, dtype=int)\n    if c.size != im.ndim:\n        raise Exception('Coordinates do not match dimensionality of image')\n\n    bbox = []\n    [bbox.append(sp.clip(c[i] - r, 0, im.shape[i])) for i in range(im.ndim)]\n    [bbox.append(sp.clip(c[i] + r, 0, im.shape[i])) for i in range(im.ndim)]\n    bbox = sp.ravel(bbox)\n    s = bbox_to_slices(bbox)\n    temp = im[s]\n    blank = sp.ones_like(temp)\n    blank[tuple(c - bbox[0:im.ndim])] = 0\n    blank = spim.distance_transform_edt(blank) < r\n    im[s] = blank\n    return im", "response": "r Inserts a sphere of a specified radius into an image and returns the image with a new sphere inerted at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_entry_type(hosts_entry=None):\n        if hosts_entry and isinstance(hosts_entry, str):\n            entry = hosts_entry.strip()\n            if not entry or not entry[0] or entry[0] == \"\\n\":\n                return 'blank'\n            if entry[0] == \"#\":\n                return 'comment'\n            entry_chunks = entry.split()\n            if is_ipv6(entry_chunks[0]):\n                return 'ipv6'\n            if is_ipv4(entry_chunks[0]):\n                return 'ipv4'", "response": "Return the type of entry for the line of hosts file passed\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform a line from a hosts file into an instance of HostsEntry", "response": "def str_to_hostentry(entry):\n        \"\"\"\n        Transform a line from a hosts file into an instance of HostsEntry\n        :param entry: A line from the hosts file\n        :return: An instance of HostsEntry\n        \"\"\"\n        line_parts = entry.strip().split()\n        if is_ipv4(line_parts[0]) and valid_hostnames(line_parts[1:]):\n            return HostsEntry(entry_type='ipv4',\n                              address=line_parts[0],\n                              names=line_parts[1:])\n        elif is_ipv6(line_parts[0]) and valid_hostnames(line_parts[1:]):\n            return HostsEntry(entry_type='ipv6',\n                              address=line_parts[0],\n                              names=line_parts[1:])\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the hosts file path based on the supplied platform.", "response": "def determine_hosts_path(platform=None):\n        \"\"\"\n        Return the hosts file path based on the supplied\n        or detected platform.\n        :param platform: a string used to identify the platform\n        :return: detected filesystem path of the hosts file\n        \"\"\"\n        if not platform:\n            platform = sys.platform\n        if platform.startswith('win'):\n            result = r\"c:\\windows\\system32\\drivers\\etc\\hosts\"\n            return result\n        else:\n            return '/etc/hosts'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite all of the hosts entries to the hosts file", "response": "def write(self, path=None):\n        \"\"\"\n        Write all of the HostsEntry instances back to the hosts file\n        :param path: override the write path\n        :return: Dictionary containing counts\n        \"\"\"\n        written_count = 0\n        comments_written = 0\n        blanks_written = 0\n        ipv4_entries_written = 0\n        ipv6_entries_written = 0\n        if path:\n            output_file_path = path\n        else:\n            output_file_path = self.hosts_path\n        try:\n            with open(output_file_path, 'w') as hosts_file:\n                for written_count, line in enumerate(self.entries):\n                    if line.entry_type == 'comment':\n                        hosts_file.write(line.comment + \"\\n\")\n                        comments_written += 1\n                    if line.entry_type == 'blank':\n                        hosts_file.write(\"\\n\")\n                        blanks_written += 1\n                    if line.entry_type == 'ipv4':\n                        hosts_file.write(\n                            \"{0}\\t{1}\\n\".format(\n                                line.address,\n                                ' '.join(line.names),\n                            )\n                        )\n                        ipv4_entries_written += 1\n                    if line.entry_type == 'ipv6':\n                        hosts_file.write(\n                            \"{0}\\t{1}\\n\".format(\n                                line.address,\n                                ' '.join(line.names), ))\n                        ipv6_entries_written += 1\n        except:\n            raise UnableToWriteHosts()\n        return {'total_written': written_count + 1,\n                'comments_written': comments_written,\n                'blanks_written': blanks_written,\n                'ipv4_entries_written': ipv4_entries_written,\n                'ipv6_entries_written': ipv6_entries_written}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the supplied address and names or comment exists in a HostsEntry within HostsEntry.", "response": "def exists(self, address=None, names=None, comment=None):\n        \"\"\"\n        Determine if the supplied address and/or names, or comment, exists in a HostsEntry within Hosts\n        :param address: An ipv4 or ipv6 address to search for\n        :param names: A list of names to search for\n        :param comment: A comment to search for\n        :return: True if a supplied address, name, or comment is found. Otherwise, False.\n        \"\"\"\n        for entry in self.entries:\n            if entry.entry_type in ('ipv4', 'ipv6'):\n                if address and address == entry.address:\n                    return True\n                if names:\n                    for name in names:\n                        if name in entry.names:\n                            return True\n            elif entry.entry_type == 'comment' and entry.comment == comment:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves all matching hosts from the Hosts object.", "response": "def remove_all_matching(self, address=None, name=None):\n        \"\"\"\n        Remove all HostsEntry instances from the Hosts object\n        where the supplied ip address or name matches\n        :param address: An ipv4 or ipv6 address\n        :param name: A host name\n        :return: None\n        \"\"\"\n        if self.entries:\n            if address and name:\n                func = lambda entry: not entry.is_real_entry() or (entry.address != address and name not in entry.names)\n            elif address:\n                func = lambda entry: not entry.is_real_entry() or entry.address != address\n            elif name:\n                func = lambda entry: not entry.is_real_entry() or name not in entry.names\n            else:\n                raise ValueError('No address or name was specified for removal.')\n            self.entries = list(filter(func, self.entries))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_url(self, url=None, force=None):\n        file_contents = self.get_hosts_by_url(url=url).decode('utf-8')\n        file_contents = file_contents.rstrip().replace('^M', '\\n')\n        file_contents = file_contents.rstrip().replace('\\r\\n', '\\n')\n        lines = file_contents.split('\\n')\n        skipped = 0\n        import_entries = []\n        for line in lines:\n            stripped_entry = line.strip()\n            if (not stripped_entry) or (stripped_entry.startswith('#')):\n                skipped += 1\n            else:\n                line = line.partition('#')[0]\n                line = line.rstrip()\n                import_entry = HostsEntry.str_to_hostentry(line)\n                if import_entry:\n                    import_entries.append(import_entry)\n        add_result = self.add(entries=import_entries, force=force)\n        write_result = self.write()\n        return {'result': 'success',\n                'skipped': skipped,\n                'add_result': add_result,\n                'write_result': write_result}", "response": "Read a list of host entries from a URL and convert them into instances of HostsEntry and append them to the list of entries in HostsEntry and return the result of the import"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_file(self, import_file_path=None):\n        skipped = 0\n        invalid_count = 0\n        if is_readable(import_file_path):\n            import_entries = []\n            with open(import_file_path, 'r') as infile:\n                for line in infile:\n                    stripped_entry = line.strip()\n                    if (not stripped_entry) or (stripped_entry.startswith('#')):\n                        skipped += 1\n                    else:\n                        line = line.partition('#')[0]\n                        line = line.rstrip()\n                        import_entry = HostsEntry.str_to_hostentry(line)\n                        if import_entry:\n                            import_entries.append(import_entry)\n                        else:\n                            invalid_count += 1\n            add_result = self.add(entries=import_entries)\n            write_result = self.write()\n            return {'result': 'success',\n                    'skipped': skipped,\n                    'invalid_count': invalid_count,\n                    'add_result': add_result,\n                    'write_result': write_result}\n        else:\n            return {'result': 'failed',\n                    'message': 'Cannot read: file {0}.'.format(import_file_path)}", "response": "Read a list of host entries from a file and convert them into instances\n        of HostsEntry and then append to the list of entries in HostsCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a list of HostsEntry instances to the instance of Hosts.", "response": "def add(self, entries=None, force=False, allow_address_duplication=False):\n        \"\"\"\n        Add instances of HostsEntry to the instance of Hosts.\n        :param entries: A list of instances of HostsEntry\n        :param force: Remove matching before adding\n        :param allow_address_duplication: Allow using multiple entries for same address\n        :return: The counts of successes and failures\n        \"\"\"\n        ipv4_count = 0\n        ipv6_count = 0\n        comment_count = 0\n        invalid_count = 0\n        duplicate_count = 0\n        replaced_count = 0\n        import_entries = []\n        existing_addresses = [x.address for x in self.entries if x.address]\n        existing_names = []\n        for item in self.entries:\n            if item.names:\n                existing_names.extend(item.names)\n        existing_names = dedupe_list(existing_names)\n        for entry in entries:\n            if entry.entry_type == 'comment':\n                entry.comment = entry.comment.strip()\n                if entry.comment[0] != \"#\":\n                    entry.comment = \"# \" + entry.comment\n                import_entries.append(entry)\n            elif entry.address in ('0.0.0.0', '127.0.0.1') or allow_address_duplication:\n                # Allow duplicates entries for addresses used for adblocking\n                if set(entry.names).intersection(existing_names):\n                    if force:\n                        for name in entry.names:\n                            self.remove_all_matching(name=name)\n                        import_entries.append(entry)\n                    else:\n                        duplicate_count += 1\n                else:\n                    import_entries.append(entry)\n            elif entry.address in existing_addresses:\n                if not force:\n                    duplicate_count += 1\n                elif force:\n                    self.remove_all_matching(address=entry.address)\n                    replaced_count += 1\n                    import_entries.append(entry)\n            elif set(entry.names).intersection(existing_names):\n                if not force:\n                    duplicate_count += 1\n                else:\n                    for name in entry.names:\n                        self.remove_all_matching(name=name)\n                    replaced_count += 1\n                    import_entries.append(entry)\n            else:\n                import_entries.append(entry)\n\n        for item in import_entries:\n            if item.entry_type == 'comment':\n                comment_count += 1\n                self.entries.append(item)\n            elif item.entry_type == 'ipv4':\n                ipv4_count += 1\n                self.entries.append(item)\n            elif item.entry_type == 'ipv6':\n                ipv6_count += 1\n                self.entries.append(item)\n        return {'comment_count': comment_count,\n                'ipv4_count': ipv4_count,\n                'ipv6_count': ipv6_count,\n                'invalid_count': invalid_count,\n                'duplicate_count': duplicate_count,\n                'replaced_count': replaced_count}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling by the initialiser of Hosts. This reads the entries from the local hosts file, converts them into instances of HostsEntry and adds them to the Hosts list of entries. :return: None", "response": "def populate_entries(self):\n        \"\"\"\n        Called by the initialiser of Hosts. This reads the entries from the local hosts file,\n        converts them into instances of HostsEntry and adds them to the Hosts list of entries.\n        :return: None\n        \"\"\"\n        try:\n            with open(self.hosts_path, 'r') as hosts_file:\n                hosts_entries = [line for line in hosts_file]\n                for hosts_entry in hosts_entries:\n                    entry_type = HostsEntry.get_entry_type(hosts_entry)\n                    if entry_type == \"comment\":\n                        hosts_entry = hosts_entry.replace(\"\\r\", \"\")\n                        hosts_entry = hosts_entry.replace(\"\\n\", \"\")\n                        self.entries.append(HostsEntry(entry_type=\"comment\",\n                                                       comment=hosts_entry))\n                    elif entry_type == \"blank\":\n                        self.entries.append(HostsEntry(entry_type=\"blank\"))\n                    elif entry_type in (\"ipv4\", \"ipv6\"):\n                        chunked_entry = hosts_entry.split()\n                        stripped_name_list = [name.strip() for name in chunked_entry[1:]]\n\n                        self.entries.append(\n                            HostsEntry(\n                                entry_type=entry_type,\n                                address=chunked_entry[0].strip(),\n                                names=stripped_name_list))\n        except IOError:\n            return {'result': 'failed',\n                    'message': 'Cannot read: {0}.'.format(self.hosts_path)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the string provided is a valid ipv6 address", "response": "def is_ipv6(entry):\n    \"\"\"\n    Check if the string provided is a valid ipv6 address\n    :param entry: A string representation of an IP address\n    :return: True if valid, False if invalid\n    \"\"\"\n    try:\n        if socket.inet_pton(socket.AF_INET6, entry):\n            return True\n    except socket.error:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef valid_hostnames(hostname_list):\n    for entry in hostname_list:\n        if len(entry) > 255:\n            return False\n        allowed = re.compile('(?!-)[A-Z\\d-]{1,63}(?<!-)$', re.IGNORECASE)\n        if not all(allowed.match(x) for x in entry.split(\".\")):\n            return False\n    return True", "response": "Checks if the supplied list of strings are valid hostnames."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_readable(path=None):\n    if os.path.isfile(path) and os.access(path, os.R_OK):\n        return True\n    return False", "response": "Test if the supplied filesystem path can be read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the hash object for the file filepath.", "response": "def _filehash(filepath, blocksize=4096):\n    \"\"\" Return the hash object for the file `filepath', processing the file\n    by chunk of `blocksize'.\n\n    :type filepath: str\n    :param filepath: Path to file\n\n    :type blocksize: int\n    :param blocksize: Size of the chunk when processing the file\n\n    \"\"\"\n    sha = hashlib.sha256()\n    with open(filepath, 'rb') as fp:\n        while 1:\n            data = fp.read(blocksize)\n            if data:\n                sha.update(data)\n            else:\n                break\n    return sha"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the diff between two directories.", "response": "def compute_diff(dir_base, dir_cmp):\n    \"\"\" Compare `dir_base' and `dir_cmp' and returns a list with\n    the following keys:\n     - deleted files `deleted'\n     - created files `created'\n     - updated files `updated'\n     - deleted directories `deleted_dirs'\n\n    \"\"\"\n    data = {}\n    data['deleted'] = list(set(dir_cmp['files']) - set(dir_base['files']))\n    data['created'] = list(set(dir_base['files']) - set(dir_cmp['files']))\n    data['updated'] = []\n    data['deleted_dirs'] = list(set(dir_cmp['subdirs']) - set(dir_base['subdirs']))\n\n    for f in set(dir_cmp['files']).intersection(set(dir_base['files'])):\n        if dir_base['index'][f] != dir_cmp['index'][f]:\n            data['updated'].append(f)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compress_to(self, archive_path=None):\n        if archive_path is None:\n            archive = tempfile.NamedTemporaryFile(delete=False)\n            tar_args = ()\n            tar_kwargs = {'fileobj': archive}\n            _return = archive.name\n        else:\n            tar_args = (archive_path)\n            tar_kwargs = {}\n            _return = archive_path\n        tar_kwargs.update({'mode': 'w:gz'})\n        with closing(tarfile.open(*tar_args, **tar_kwargs)) as tar:\n            tar.add(self.path, arcname=self.file)\n\n        return _return", "response": "Compress the directory with gzip using tarlib."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hash(self, index_func=os.path.getmtime):\n        # TODO alternative to filehash => mtime as a faster alternative\n        shadir = hashlib.sha256()\n        for f in self.files():\n            try:\n                shadir.update(str(index_func(os.path.join(self.path, f))))\n            except (IOError, OSError):\n                pass\n        return shadir.hexdigest()", "response": "Hash for the entire directory recursively."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sorted list containing relative path of all files in the current directory.", "response": "def files(self, pattern=None, sort_key=lambda k: k, sort_reverse=False, abspath=False):\n        \"\"\" Return a sorted list containing relative path of all files (recursively).\n\n        :type pattern: str\n        :param pattern: Unix style (glob like/gitignore like) pattern\n\n        :param sort_key: key argument for sorted\n\n        :param sort_reverse: reverse argument for sorted\n\n        :rtype: list\n        :return: List of all relative files paths.\n\n        \"\"\"\n        return sorted(self.iterfiles(pattern, abspath=abspath), key=sort_key, reverse=sort_reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subdirs(self, pattern=None, sort_key=lambda k: k, sort_reverse=False, abspath=False):\n        return sorted(self.itersubdirs(pattern, abspath=abspath), key=sort_key, reverse=sort_reverse)", "response": "Return a sorted list containing all relative path of all subdirs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the size of the current directory.", "response": "def size(self):\n        \"\"\" Return directory size in bytes.\n\n        :rtype: int\n        :return: Total directory size in bytes.\n        \"\"\"\n        dir_size = 0\n        for f in self.iterfiles(abspath=True):\n            dir_size += os.path.getsize(f)\n        return dir_size"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if path should be excluded given patterns in the exclude_file.", "response": "def is_excluded(self, path):\n        \"\"\" Return True if `path' should be excluded\n        given patterns in the `exclude_file'. \"\"\"\n        match = self.globster.match(self.relpath(path))\n        if match:\n            log.debug(\"{0} matched {1} for exclusion\".format(path, match))\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_projects(self, file_identifier=\".project\"):\n        projects = []\n        for d in self.subdirs():\n            project_file = os.path.join(self.directory, d, file_identifier)\n            if os.path.isfile(project_file):\n                projects.append(d)\n        return projects", "response": "Search all directories recursively for subdirs with a file_identifier in it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef relpath(self, path):\n        return os.path.relpath(path, start=self.path)", "response": "Return a relative filepath to path from Dir path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a single pipeline by pipeline - id.", "response": "def run(pipeline_id, verbose, use_cache, dirty, force, concurrency, slave):\n    \"\"\"Run a pipeline by pipeline-id.\n       pipeline-id supports '%' wildcard for any-suffix matching,\n       'all' for running all pipelines and\n       comma-delimited list of pipeline ids\"\"\"\n    exitcode = 0\n\n    running = []\n    progress = {}\n\n    def progress_cb(report):\n        pid, count, success, *_, stats = report\n\n        print('\\x1b[%sA' % (1+len(running)))\n        if pid not in progress:\n            running.append(pid)\n        progress[pid] = count, success\n\n        for pid in running:\n            count, success = progress[pid]\n            if success is None:\n                if count == 0:\n                    print('\\x1b[2K%s: \\x1b[31m%s\\x1b[0m' % (pid, 'WAITING FOR OUTPUT'))\n                else:\n                    print('\\x1b[2K%s: \\x1b[33mRUNNING, processed %s rows\\x1b[0m' % (pid, count))\n            else:\n                if success:\n                    print('\\x1b[2K%s: \\x1b[32mSUCCESS, processed %s rows\\x1b[0m' % (pid, count))\n                else:\n                    print('\\x1b[2K%s: \\x1b[31mFAILURE, processed %s rows\\x1b[0m' % (pid, count))\n\n    results = run_pipelines(pipeline_id, '.', use_cache,\n                            dirty, force, concurrency,\n                            verbose, progress_cb if not verbose else None,\n                            slave)\n    if not slave:\n        logging.info('RESULTS:')\n        errd = False\n        for result in results:\n            stats = user_facing_stats(result.stats)\n            errd = errd or result.errors or not result.success\n            logging.info('%s: %s %s%s',\n                         'SUCCESS' if result.success else 'FAILURE',\n                         result.pipeline_id,\n                         repr(stats) if stats is not None else '',\n                         (\n                            '\\nERROR log from processor %s:\\n+--------\\n| ' % result.errors[0] +\n                            '\\n| '.join(result.errors[1:]) +\n                            '\\n+--------'\n                         ) if result.errors else '')\n    else:\n        result_obj = []\n        errd = False\n        for result in results:\n            errd = errd or result.errors or not result.success\n            stats = user_facing_stats(result.stats)\n            result_obj.append(dict(\n                success=result.success,\n                pipeline_id=result.pipeline_id,\n                stats=result.stats,\n                errors=result.errors\n            ))\n            json.dump(result_obj, sys.stderr)\n\n    if errd:\n        exitcode = 1\n\n    exit(exitcode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef basic_auth_required(view_func):\n    @wraps(view_func)\n    def wrapper(*args, **kwargs):\n        if app.config.get('BASIC_AUTH_ACTIVE', False):\n            if basic_auth.authenticate():\n                return view_func(*args, **kwargs)\n            else:\n                return basic_auth.challenge()\n        else:\n            return view_func(*args, **kwargs)\n    return wrapper", "response": "Decorator that allows specific views with HTTP basic auth."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef badge_collection(pipeline_path):\n    '''Status badge for a collection of pipelines.'''\n    all_pipeline_ids = sorted(status.all_pipeline_ids())\n\n    if not pipeline_path.startswith('./'):\n        pipeline_path = './' + pipeline_path\n\n    # Filter pipeline ids to only include those that start with pipeline_path.\n    path_pipeline_ids = \\\n        [p for p in all_pipeline_ids if p.startswith(pipeline_path)]\n\n    statuses = []\n    for pipeline_id in path_pipeline_ids:\n        pipeline_status = status.get(pipeline_id)\n        if pipeline_status is None:\n            abort(404)\n        status_text = pipeline_status.state().lower()\n        statuses.append(status_text)\n\n    status_color = 'lightgray'\n    status_counter = Counter(statuses)\n    if status_counter:\n        if len(status_counter) == 1 and status_counter['succeeded'] > 0:\n            status_color = 'brightgreen'\n        elif status_counter['failed'] > 0:\n            status_color = 'red'\n        elif status_counter['failed'] == 0:\n            status_color = 'yellow'\n        status_text = \\\n            ', '.join(['{} {}'.format(v, k)\n                       for k, v in status_counter.items()])\n    else:\n        status_text = \"not found\"\n\n    return _make_badge_response('pipelines', status_text, status_color)", "response": "Status badge for a collection of pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_pipelines(pipeline_id_pattern,\n                  root_dir,\n                  use_cache=True,\n                  dirty=False,\n                  force=False,\n                  concurrency=1,\n                  verbose_logs=True,\n                  progress_cb=None,\n                  slave=False):\n    \"\"\"Run a pipeline by pipeline-id.\n       pipeline-id supports the '%' wildcard for any-suffix matching.\n       Use 'all' or '%' for running all pipelines\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency,\n                                               thread_name_prefix='T') as executor:\n        try:\n            results = []\n            pending_futures = set()\n            done_futures = set()\n            finished_futures = []\n            progress_thread = None\n            progress_queue = None\n            status_manager = status_mgr(root_dir)\n\n            if progress_cb is not None:\n                progress_queue = Queue()\n                progress_thread = threading.Thread(target=progress_report_handler, args=(progress_cb, progress_queue))\n                progress_thread.start()\n\n            all_specs = specs_to_execute(pipeline_id_pattern, root_dir, status_manager, force, dirty, results)\n\n            while True:\n\n                done = None\n                if len(done_futures) > 0:\n                    done = done_futures.pop()\n                    finished_futures.append(done)\n                    done = done.result()[0]\n\n                try:\n                    spec = all_specs.send(done)\n                except StopIteration:\n                    spec = None\n\n                if spec is None:\n                    # Wait for all runners to idle...\n                    if len(done_futures) == 0:\n                        if len(pending_futures) > 0:\n                            done_futures, pending_futures = \\\n                                concurrent.futures.wait(pending_futures,\n                                                        return_when=concurrent.futures.FIRST_COMPLETED)\n                            continue\n                        else:\n                            break\n                    else:\n                        continue\n\n                if len(spec.validation_errors) > 0:\n                    results.append(\n                        ExecutionResult(spec.pipeline_id,\n                                        False,\n                                        {},\n                                        ['init'] + list(map(str, spec.validation_errors)))\n                    )\n                    continue\n\n                if slave:\n                    ps = status_manager.get(spec.pipeline_id)\n                    ps.init(spec.pipeline_details,\n                            spec.source_details,\n                            spec.validation_errors,\n                            spec.cache_hash)\n                    eid = gen_execution_id()\n                    if ps.queue_execution(eid, 'manual'):\n                        success, stats, errors = \\\n                            execute_pipeline(spec, eid,\n                                             use_cache=use_cache)\n                        results.append(ExecutionResult(\n                            spec.pipeline_id,\n                            success,\n                            stats,\n                            errors\n                        ))\n                    else:\n                        results.append(\n                            ExecutionResult(spec.pipeline_id,\n                                            False,\n                                            None,\n                                            ['Already Running'])\n                        )\n\n                else:\n                    f = executor.submit(remote_execute_pipeline,\n                                        spec,\n                                        root_dir,\n                                        use_cache,\n                                        verbose_logs,\n                                        progress_queue)\n                    pending_futures.add(f)\n\n            for f in finished_futures:\n                ret = f.result()\n                results.append(ExecutionResult(*ret))\n\n        except KeyboardInterrupt:\n            pass\n        finally:\n            if slave:\n                finalize()\n\n            if progress_thread is not None:\n                progress_queue.put(None)\n                progress_thread.join()\n\n    return results", "response": "Runs a pipeline by pipeline - id pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an element at a dedicated position in an OrderedSet.", "response": "def insert(self, index, key):\n    \"\"\"Adds an element at a dedicated position in an OrderedSet.\n\n    This implementation is meant for the OrderedSet from the ordered_set\n    package only.\n    \"\"\"\n    if key in self.map:\n        return\n    # compute the right index\n    size = len(self.items)\n    if index < 0:\n        index = size + index if size + index > 0 else 0\n    else:\n        index = index if index < size else size\n    # insert the value\n    self.items.insert(index, key)\n    for k, v in self.map.items():\n        if v >= index:\n            self.map[k] = v + 1\n    self.map[key] = index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves an element from the tail of the OrderedSet or at a dedicated position.", "response": "def pop(self, index=None):\n    \"\"\"Removes an element at the tail of the OrderedSet or at a dedicated\n    position.\n\n    This implementation is meant for the OrderedSet from the ordered_set\n    package only.\n    \"\"\"\n    if not self.items:\n        raise KeyError('Set is empty')\n\n    def remove_index(i):\n        elem = self.items[i]\n        del self.items[i]\n        del self.map[elem]\n        return elem\n    if index is None:\n        elem = remove_index(-1)\n    else:\n        size = len(self.items)\n        if index < 0:\n            index = size + index\n            if index < 0:\n                raise IndexError('assignement index out of range')\n        elif index >= size:\n            raise IndexError('assignement index out of range')\n        elem = remove_index(index)\n        for k, v in self.map.items():\n            if v >= index and v > 0:\n                self.map[k] = v - 1\n    return elem"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_resource(self, uri):\n        if isinstance(uri, str):\n            uri = URI(uri)\n        try:\n            resource = self.resource_factory[uri.extension](uri)\n        except KeyError:\n            resource = self.resource_factory['*'](uri)\n        self.resources[uri.normalize()] = resource\n        resource.resource_set = self\n        resource.decoders.insert(0, self)\n        return resource", "response": "Creates a new Resource object based on the provided URI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclass decorator for creating PyEcore metaclass.", "response": "def EMetaclass(cls):\n    \"\"\"Class decorator for creating PyEcore metaclass.\"\"\"\n    superclass = cls.__bases__\n    if not issubclass(cls, EObject):\n        sclasslist = list(superclass)\n        if object in superclass:\n            index = sclasslist.index(object)\n            sclasslist.insert(index, EObject)\n            sclasslist.remove(object)\n        else:\n            sclasslist.insert(0, EObject)\n        superclass = tuple(sclasslist)\n    orig_vars = cls.__dict__.copy()\n    slots = orig_vars.get('__slots__')\n    if slots is not None:\n        if isinstance(slots, str):\n            slots = [slots]\n        for slots_var in slots:\n            orig_vars.pop(slots_var)\n    orig_vars.pop('__dict__', None)\n    orig_vars.pop('__weakref__', None)\n    return MetaEClass(cls.__name__, superclass, orig_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getEAnnotation(self, source):\n        for annotation in self.eAnnotations:\n            if annotation.source == source:\n                return annotation\n        return None", "response": "Return the annotation with a matching source attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_week_URL(date, day=0):\n    if day < 1:\n        day = 1\n    date = datetime(year=date.year, month=date.month, day=day, tzinfo=utc)\n    return reverse('calendar_week', kwargs={'year': date.isocalendar()[0],\n                                            'week': date.isocalendar()[1]})", "response": "Returns the week view URL for a given date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef monday_of_week(year, week):\n    str_time = time.strptime('{0} {1} 1'.format(year, week), '%Y %W %w')\n    date = timezone.datetime(year=str_time.tm_year, month=str_time.tm_mon,\n                             day=str_time.tm_mday, tzinfo=timezone.utc)\n    if timezone.datetime(year, 1, 4).isoweekday() > 4:\n        # ISO 8601 where week 1 is the first week that has at least 4 days in\n        # the current year\n        date -= timezone.timedelta(days=7)\n    return date", "response": "Returns a datetime for the monday of the given week of the given year."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a persisted occurrences matching the occ and remove it from the lookup since it has already been matched", "response": "def get_occurrence(self, occ):\n        \"\"\"\n        Return a persisted occurrences matching the occ and remove it from\n        lookup since it has already been matched\n        \"\"\"\n        return self.lookup.pop(\n            (occ.event, occ.original_start, occ.original_end),\n            occ)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_additional_occurrences(self, start, end):\n        return [occ for key, occ in self.lookup.items() if (\n            (end and occ.start < end) and\n            occ.end >= start and not occ.cancelled)]", "response": "Return the list of persisted occurrences which are now in the period"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _request(self, uri, method='GET', params=None, files=None, headers=None, auth=None):\n\n        # make up to three attempts to dance with the API, use a jittered\n        # exponential back-off delay\n        for i in range(3):\n            try:\n                full_url = '{b}{u}'.format(b=self.api_url, u=uri)\n\n                response = None\n                if method == 'POST':\n                    response = requests.post(full_url, data=params, files=files, headers=headers,\n                                             verify=self.verify_ssl, auth=auth, proxies=self.proxies)\n                else:\n                    response = requests.get(full_url, params=params, headers=headers,\n                                            verify=self.verify_ssl, auth=auth, proxies=self.proxies)\n\n                # if the status code is 503, is no longer available.\n                if response.status_code >= 500:\n                    # server error\n                    self.server_available = False\n                    raise SandboxError(\"server returned {c} status code on {u}, assuming unavailable...\".format(\n                        c=response.status_code, u=response.url))\n                else:\n                    return response\n\n            # 0.4, 1.6, 6.4, 25.6, ...\n            except requests.exceptions.RequestException:\n                time.sleep(random.uniform(0, 4 ** i * 100 / 1000.0))\n\n        # if we couldn't reach the API, we assume that the box is down and lower availability flag.\n        self.server_available = False\n\n        # raise an exception.\n        msg = \"exceeded 3 attempts with sandbox API: {u}, p:{p}, f:{f}\".format(u=full_url,\n                                                                               p=params, f=files)\n        try:\n            msg += \"\\n\" + response.content.decode('utf-8')\n        except AttributeError:\n            pass\n\n        raise SandboxError(msg)", "response": "Robustness wrapper. Tries up to 3 times to dance with the Sandbox API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef analyses(self):\n        response = self._request(\"tasks/list\")\n\n        return json.loads(response.content.decode('utf-8'))['tasks']", "response": "Retrieve a list of analyzed samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit a file for analysis.", "response": "def analyze(self, handle, filename):\n        \"\"\"Submit a file for analysis.\n\n        :type  handle:   File handle\n        :param handle:   Handle to file to upload for analysis.\n        :type  filename: str\n        :param filename: File name.\n\n        :rtype:  str\n        :return: Task ID as a string\n        \"\"\"\n        # multipart post files.\n        files = {\"file\": (filename, handle)}\n\n        # ensure the handle is at offset 0.\n        handle.seek(0)\n\n        response = self._request(\"tasks/create/file\", method='POST', files=files)\n\n        # return task id; try v1.3 and v2.0 API response formats\n        try:\n            return str(json.loads(response.content.decode('utf-8'))[\"task_id\"])\n        except KeyError:\n            return str(json.loads(response.content.decode('utf-8'))[\"task_ids\"][0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(self, item_id):\n        response = self._request(\"tasks/view/{id}\".format(id=item_id))\n\n        if response.status_code == 404:\n            # probably an unknown task id\n            return False\n\n        try:\n            content = json.loads(response.content.decode('utf-8'))\n            status = content['task'][\"status\"]\n            if status == 'completed' or status == \"reported\":\n                return True\n\n        except ValueError as e:\n            raise sandboxapi.SandboxError(e)\n\n        return False", "response": "Check if an analysis is complete."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, item_id):\n        try:\n            response = self._request(\"tasks/delete/{id}\".format(id=item_id))\n\n            if response.status_code == 200:\n                return True\n\n        except sandboxapi.SandboxError:\n            pass\n\n        return False", "response": "Delete the reports associated with the given item_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_available(self):\n        # if the availability flag is raised, return True immediately.\n        # NOTE: subsequent API failures will lower this flag. we do this here\n        # to ensure we don't keep hitting Cuckoo with requests while\n        # availability is there.\n        if self.server_available:\n            return True\n\n        # otherwise, we have to check with the cloud.\n        else:\n            try:\n                response = self._request(\"cuckoo/status\")\n\n                # we've got cuckoo.\n                if response.status_code == 200:\n                    self.server_available = True\n                    return True\n\n            except sandboxapi.SandboxError:\n                pass\n\n        self.server_available = False\n        return False", "response": "Determine if the Cuckoo Sandbox API servers are alive or in maintenance mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef queue_size(self):\n        response = self._request(\"tasks/list\")\n        tasks = json.loads(response.content.decode('utf-8'))[\"tasks\"]\n\n        return len([t for t in tasks if t['status'] == 'pending'])", "response": "Determine the number of submissions in the Cuckoo sandbox queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report(self, item_id, report_format=\"json\"):\n        report_format = report_format.lower()\n\n        response = self._request(\"tasks/report/{id}/{format}\".format(id=item_id, format=report_format))\n\n        # if response is JSON, return it as an object\n        if report_format == \"json\":\n            try:\n                return json.loads(response.content.decode('utf-8'))\n            except ValueError:\n                pass\n\n        # otherwise, return the raw content.\n        return response.content", "response": "Retrieves the specified report for the analyzed item referenced by item_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef score(self, report):\n        score = 0\n\n        try:\n            # cuckoo-modified format\n            score = report['malscore']\n        except KeyError:\n            # cuckoo-2.0 format\n            score = report.get('info', {}).get('score', 0)\n\n        return score", "response": "Pass in the report from self. report(), get back an int."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride the parent method to add extra parameters to the request.", "response": "def _request(self, uri, method='GET', params=None, files=None, headers=None, auth=None):\n        \"\"\"Override the parent _request method.\n\n        We have to do this here because FireEye requires some extra\n        authentication steps.\n        \"\"\"\n        if params:\n            params['environment_id'] = self.env_id\n        else:\n            params = {\n                'environment_id': self.env_id,\n            }\n\n        if headers:\n            headers['api-key'] = self.key\n            headers['User-Agent'] = 'Falcon Sandbox'\n            headers['Accept'] = 'application/json'\n        else:\n            headers = {\n                'api-key': self.key,\n                'User-Agent': 'Falcon Sandbox',\n                'Accept': 'application/json',\n            }\n\n        return sandboxapi.SandboxAPI._request(self, uri, method, params, files, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef analyze(self, handle, filename):\n        # multipart post files.\n        files = {\"file\" : (filename, handle)}\n\n        # ensure the handle is at offset 0.\n        handle.seek(0)\n\n        response = self._request(\"/submit/file\", method='POST', files=files)\n\n        try:\n            if response.status_code == 201:\n                # good response\n                return response.json()['job_id']\n            else:\n                raise sandboxapi.SandboxError(\"api error in analyze: {r}\".format(r=response.content.decode('utf-8')))\n        except (ValueError, KeyError) as e:\n            raise sandboxapi.SandboxError(\"error in analyze: {e}\".format(e=e))", "response": "Submit a file for analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the specified report for the analyzed item referenced by item_id.", "response": "def report(self, item_id, report_format=\"json\"):\n        \"\"\"Retrieves the specified report for the analyzed item, referenced by item_id.\n\n        Available formats include: json, html.\n\n        :type  item_id:     str\n        :param item_id:     File ID number\n        :type  report_format: str\n        :param report_format: Return format\n\n        :rtype:  dict\n        :return: Dictionary representing the JSON parsed data or raw, for other\n                 formats / JSON parsing failure.\n        \"\"\"\n        report_format = report_format.lower()\n\n        response = self._request(\"/report/{job_id}/summary\".format(job_id=item_id))\n\n        if response.status_code == 429:\n            raise sandboxapi.SandboxError('API rate limit exceeded while fetching report')\n\n        # if response is JSON, return it as an object\n        if report_format == \"json\":\n            try:\n                return json.loads(response.content.decode('utf-8'))\n            except ValueError:\n                pass\n\n        # otherwise, return the raw content.\n        return response.content.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npasses in the report from self. report(), get back an int 0 - 10.", "response": "def score(self, report):\n        \"\"\"Pass in the report from self.report(), get back an int 0-10.\"\"\"\n\n        try:\n            threatlevel = int(report['threat_level'])\n            threatscore = int(report['threat_score'])\n        except (KeyError, IndexError, ValueError, TypeError) as e:\n            raise sandboxapi.SandboxError(e)\n\n        # from falcon docs:\n        # threatlevel is the verdict field with values: 0 = no threat, 1 = suspicious, 2 = malicious\n        # threascore  is the \"heuristic\" confidence value of Falcon Sandbox in the verdict and is a value between 0\n        # and 100. A value above 75/100 is \"pretty sure\", a value above 90/100 is \"very sure\".\n\n        # the scoring below converts these values to a scalar. modify as needed.\n        score = 0\n        if threatlevel == 2 and threatscore >= 90:\n            score = 10\n        elif threatlevel == 2 and threatscore >= 75:\n            score = 9\n        elif threatlevel == 2:\n            score = 8\n        elif threatlevel == 1 and threatscore >= 90:\n            score = 7\n        elif threatlevel == 1 and threatscore >= 75:\n            score = 6\n        elif threatlevel == 1:\n            score = 5\n        elif threatlevel == 0 and threatscore < 75:\n            score = 1\n\n        return score"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef analyze(self, handle, filename):\n        # multipart post files.\n        files = {\"sample_file\": (filename, handle)}\n\n        # ensure the handle is at offset 0.\n        handle.seek(0)\n\n        response = self._request(\"/sample/submit\", method='POST', files=files, headers=self.headers)\n\n        try:\n            if response.status_code == 200 and not response.json()['data']['errors']:\n                # only support single-file submissions; just grab the first one.\n                return response.json()['data']['samples'][0]['sample_id']\n            else:\n                raise sandboxapi.SandboxError(\"api error in analyze ({u}): {r}\".format(u=response.url, r=response.content))\n        except (ValueError, KeyError, IndexError) as e:\n            raise sandboxapi.SandboxError(\"error in analyze: {e}\".format(e=e))", "response": "Submit a file for analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if an analysis is complete.", "response": "def check(self, item_id):\n        \"\"\"Check if an analysis is complete.\n\n        :type  item_id: str\n        :param item_id: File ID to check.\n\n        :rtype:  bool\n        :return: Boolean indicating if a report is done or not.\n        \"\"\"\n        response = self._request(\"/submission/sample/{sample_id}\".format(sample_id=item_id), headers=self.headers)\n\n        if response.status_code == 404:\n            # unknown id\n            return False\n\n        try:\n            finished = False\n            for submission in response.json()['data']:\n                finished = finished or submission['submission_finished']\n            if finished:\n                return True\n\n        except (ValueError, KeyError) as e:\n            raise sandboxapi.SandboxError(e)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the specified report for the analyzed item identified by item_id.", "response": "def report(self, item_id, report_format=\"json\"):\n        \"\"\"Retrieves the specified report for the analyzed item, referenced by item_id.\n\n        Available formats include: json.\n\n        :type  item_id:       str\n        :param item_id:       File ID number\n        :type  report_format: str\n        :param report_format: Return format\n\n        :rtype:  dict\n        :return: Dictionary representing the JSON parsed data or raw, for other\n                 formats / JSON parsing failure.\n        \"\"\"\n        if report_format == \"html\":\n            return \"Report Unavailable\"\n\n        # grab an analysis id from the submission id.\n        response = self._request(\"/analysis/sample/{sample_id}\".format(sample_id=item_id),\n                headers=self.headers)\n\n        try:\n            # the highest score is probably the most interesting.\n            # vmray uses this internally with sample_highest_vti_score so this seems like a safe assumption.\n            analysis_id = 0\n            top_score = -1\n            for analysis in response.json()['data']:\n                if analysis['analysis_vti_score'] > top_score:\n                    top_score = analysis['analysis_vti_score']\n                    analysis_id = analysis['analysis_id']\n\n        except (ValueError, KeyError) as e:\n            raise sandboxapi.SandboxError(e)\n\n        # assume report format json.\n        response = self._request(\"/analysis/{analysis_id}/archive/logs/summary.json\".format(analysis_id=analysis_id),\n                headers=self.headers)\n\n        # if response is JSON, return it as an object.\n        try:\n            return response.json()\n        except ValueError:\n            pass\n\n        # otherwise, return the raw content.\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the parent method to handle the request.", "response": "def _request(self, uri, method='GET', params=None, files=None, headers=None, auth=None):\n        \"\"\"Override the parent _request method.\n\n        We have to do this here because FireEye requires some extra\n        authentication steps. On each request we pass the auth headers, and\n        if the session has expired, we automatically reauthenticate.\n        \"\"\"\n        if headers:\n            headers['Accept'] = 'application/json'\n        else:\n            headers = {\n                'Accept': 'application/json',\n            }\n\n        if not self.api_token:\n            # need to log in\n            response = sandboxapi.SandboxAPI._request(self, '/auth/login', 'POST', headers=headers,\n                                                      auth=HTTPBasicAuth(self.username, self.password))\n            if response.status_code != 200:\n                raise sandboxapi.SandboxError(\"Can't log in, HTTP Error {e}\".format(e=response.status_code))\n            # we are now logged in, save the token\n            self.api_token = response.headers.get('X-FeApi-Token')\n\n        headers['X-FeApi-Token'] = self.api_token\n\n        response = sandboxapi.SandboxAPI._request(self, uri, method, params, files, headers)\n\n        # handle session timeout\n        unauthorized = False\n        try:\n            if json.loads(response.content.decode('utf-8'))['fireeyeapis']['httpStatus'] == 401:\n                unauthorized = True\n        except (ValueError, KeyError, TypeError):\n            # non-JSON response, or no such keys.\n            pass\n\n        if response.status_code == 401 or unauthorized:\n            self.api_token = None\n            try:\n                headers.pop('X-FeApi-Token')\n            except KeyError:\n                pass\n\n            # recurse\n            return self._request(uri, method, params, files, headers)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef analyze(self, handle, filename):\n        # multipart post files.\n        files = {\"file\": (filename, handle)}\n\n        # ensure the handle is at offset 0.\n        handle.seek(0)\n\n        # add submission options\n        data = {\n            #FIXME: These may need to change, see docs page 36\n            'options': '{\"application\":\"0\",\"timeout\":\"500\",\"priority\":\"0\",\"profiles\":[\"%s\"],\"analysistype\":\"0\",\"force\":\"true\",\"prefetch\":\"1\"}' % self.profile,\n        }\n\n        response = self._request(\"/submissions\", method='POST', params=data, files=files)\n\n        try:\n            if response.status_code == 200:\n                # good response\n                try:\n                    return response.json()['ID']\n                except TypeError:\n                    return response.json()[0]['ID']\n            else:\n                raise sandboxapi.SandboxError(\"api error in analyze ({u}): {r}\".format(u=response.url, r=response.content))\n        except (ValueError, KeyError) as e:\n            raise sandboxapi.SandboxError(\"error in analyze: {e}\".format(e=e))", "response": "Submit a file for analysis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if an analysis is complete.", "response": "def check(self, item_id):\n        \"\"\"Check if an analysis is complete.\n\n        :type  item_id: str\n        :param item_id: File ID to check.\n\n        :rtype:  bool\n        :return: Boolean indicating if a report is done or not.\n        \"\"\"\n        response = self._request(\"/submissions/status/{file_id}\".format(file_id=item_id))\n\n        if response.status_code == 404:\n            # unknown id\n            return False\n\n        try:\n            status = response.json()['submissionStatus']\n            if status == 'Done':\n                return True\n\n        except ValueError as e:\n            raise sandboxapi.SandboxError(e)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report(self, item_id, report_format=\"json\"):\n        if report_format == \"html\":\n            return \"Report Unavailable\"\n\n        # else we try JSON\n        response = self._request(\"/submissions/results/{file_id}?info_level=extended\".format(file_id=item_id))\n\n        # if response is JSON, return it as an object\n        try:\n            return response.json()\n        except ValueError:\n            pass\n\n        # otherwise, return the raw content.\n        return response.content", "response": "Retrieves the specified report for the analyzed item referenced by item_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits a file for analysis.", "response": "def analyze(self, handle, filename):\n        \"\"\"Submit a file for analysis.\n\n        :type  handle:   File handle\n        :param handle:   Handle to file to upload for analysis.\n        :type  filename: str\n        :param filename: File name.\n\n        :rtype:  str\n        :return: Task ID as a string\n        \"\"\"\n        # ensure the handle is at offset 0.\n        handle.seek(0)\n\n        try:\n            return self.jbx.submit_sample(handle)['webids'][0]\n        except (jbxapi.JoeException, KeyError, IndexError) as e:\n            raise sandboxapi.SandboxError(\"error in analyze: {e}\".format(e=e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(self, item_id):\n        try:\n            return self.jbx.info(item_id).get('status').lower() == 'finished'\n        except jbxapi.JoeException:\n            return False\n\n        return False", "response": "Check if an analysis is complete."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the Joe Sandbox API server is alive False otherwise.", "response": "def is_available(self):\n        \"\"\"Determine if the Joe Sandbox API server is alive.\n\n        :rtype:  bool\n        :return: True if service is available, False otherwise.\n        \"\"\"\n        # if the availability flag is raised, return True immediately.\n        # NOTE: subsequent API failures will lower this flag. we do this here\n        # to ensure we don't keep hitting Joe with requests while availability\n        # is there.\n        if self.server_available:\n            return True\n\n        # otherwise, we have to check with the cloud.\n        else:\n\n            try:\n                self.server_available = self.jbx.server_online()\n                return self.server_available\n            except jbxapi.JoeException:\n                pass\n\n        self.server_available = False\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the specified report for the analyzed item referenced by item_id.", "response": "def report(self, item_id, report_format=\"json\"):\n        \"\"\"Retrieves the specified report for the analyzed item, referenced by item_id.\n\n        For available report formats, see online Joe Sandbox documentation.\n\n        :type  item_id:       str\n        :param item_id:       File ID number\n        :type  report_format: str\n        :param report_format: Return format\n\n        :rtype:  dict\n        :return: Dictionary representing the JSON parsed data or raw, for other\n                 formats / JSON parsing failure.\n        \"\"\"\n        if report_format == \"json\":\n            report_format = \"jsonfixed\"\n\n        try:\n            return json.loads(self.jbx.download(item_id, report_format)[1].decode('utf-8'))\n        except (jbxapi.JoeException, ValueError, IndexError) as e:\n            raise sandboxapi.SandboxError(\"error in report fetch: {e}\".format(e=e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_isolated_cpus():\n    # The cpu/isolated sysfs was added in Linux 4.2\n    # (commit 59f30abe94bff50636c8cad45207a01fdcb2ee49)\n    path = sysfs_path('devices/system/cpu/isolated')\n    isolated = read_first_line(path)\n    if isolated:\n        return parse_cpu_list(isolated)\n\n    cmdline = read_first_line(proc_path('cmdline'))\n    if cmdline:\n        match = re.search(r'\\bisolcpus=([^ ]+)', cmdline)\n        if match:\n            isolated = match.group(1)\n            return parse_cpu_list(isolated)\n\n    return None", "response": "Get the list of isolated CPUs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_same_unique_benchmark(self):\n        \"True if all suites have one benchmark with the same name\"\n        if any(len(suite) > 1 for suite in self.suites):\n            return False\n        names = self.suites[0].get_benchmark_names()\n        return all(suite.get_benchmark_names() == names\n                   for suite in self.suites[1:])", "response": "True if all suites have one benchmark with the same name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_message(message, params, site, logger):\n    client.capture(\n        'Message',\n        message=message,\n        params=tuple(params),\n        data={\n            'site': site,\n            'logger': logger,\n        },\n    )", "response": "Send a message to the Sentry server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_arguments(args):\n    if args.sentryconfig:\n        print('Parsing DSN from %s' % args.sentryconfig)\n        os.environ['SENTRY_DSN'] = parse_sentry_configuration(args.sentryconfig)\n\n    if args.sentrydsn:\n        print('Using the DSN %s' % args.sentrydsn)\n        os.environ['SENTRY_DSN'] = args.sentrydsn\n\n    if args.nginxerrorpath:\n        print('Using the Nginx error log path %s' % args.nginxerrorpath)\n        os.environ['NGINX_ERROR_PATH'] = args.nginxerrorpath\n\n    from ..conf import settings  # noqa; pylint: disable=unused-variable\n\n    if args.daemonize:\n        print('Running process in background')\n        from ..daemonize import create_daemon\n        create_daemon()", "response": "Deal with arguments passed on the command line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing Sentry DSN out of an application or Sentry configuration file", "response": "def parse_sentry_configuration(filename):\n    \"\"\"Parse Sentry DSN out of an application or Sentry configuration file\"\"\"\n    filetype = os.path.splitext(filename)[-1][1:].lower()\n\n    if filetype == 'ini':  # Pyramid, Pylons\n        config = ConfigParser()\n        config.read(filename)\n        ini_key = 'dsn'\n        ini_sections = ['sentry', 'filter:raven']\n\n        for section in ini_sections:\n            if section in config:\n                print('- Using value from [{section}]:[{key}]'\n                      .format(section=section, key=ini_key))\n                try:\n                    return config[section][ini_key]\n                except KeyError:\n                    print('- Warning: Key \"{key}\" not found in section '\n                          '[{section}]'.format(section=section, key=ini_key))\n        raise SystemExit('No DSN found in {file}. Tried sections [{sec_list}]'\n                         .format(\n                             file=filename,\n                             sec_list='], ['.join(ini_sections),\n                         ))\n    elif filetype == 'py':  # Django, Flask, Bottle, ...\n        raise SystemExit('Parsing configuration from pure Python (Django,'\n                         'Flask, Bottle, etc.) not implemented yet.')\n    else:\n        raise SystemExit('Configuration file type not supported for parsing: '\n                         '%s' % filetype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the tail and follow the log file and parse the messages and send them to Sentry using Raven.", "response": "def follow_tail(self):\n        \"\"\"\n        Read (tail and follow) the log file, parse entries and send messages\n        to Sentry using Raven.\n        \"\"\"\n\n        try:\n            follower = tailhead.follow_path(self.filepath)\n        except (FileNotFoundError, PermissionError) as err:\n            raise SystemExit(\"Error: Can't read logfile %s (%s)\" %\n                             (self.filepath, err))\n\n        for line in follower:\n            self.message = None\n            self.params = None\n            self.site = None\n\n            if line is not None:\n                self.parse(line)\n                send_message(self.message,\n                             self.params,\n                             self.site,\n                             self.logger)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_daemon():\n\n    try:\n        # Fork a child process so the parent can exit.  This returns control to\n        # the command-line or shell.  It also guarantees that the child will not\n        # be a process group leader, since the child receives a new process ID\n        # and inherits the parent's process group ID.  This step is required\n        # to insure that the next call to os.setsid is successful.\n        pid = os.fork()\n    except OSError as err:\n        raise Exception(\"%s [%d]\" % (err.strerror, err.errno))\n\n    if pid == 0:  # The first child.\n        # To become the session leader of this new session and the process group\n        # leader of the new process group, we call os.setsid().  The process is\n        # also guaranteed not to have a controlling terminal.\n        os.setsid()\n\n        # Is ignoring SIGHUP necessary?\n        #\n        # It's often suggested that the SIGHUP signal should be ignored before\n        # the second fork to avoid premature termination of the process.  The\n        # reason is that when the first child terminates, all processes, e.g.\n        # the second child, in the orphaned group will be sent a SIGHUP.\n        #\n        # \"However, as part of the session management system, there are exactly\n        # two cases where SIGHUP is sent on the death of a process:\n        #\n        #   1) When the process that dies is the session leader of a session that\n        #      is attached to a terminal device, SIGHUP is sent to all processes\n        #      in the foreground process group of that terminal device.\n        #   2) When the death of a process causes a process group to become\n        #      orphaned, and one or more processes in the orphaned group are\n        #      stopped, then SIGHUP and SIGCONT are sent to all members of the\n        #      orphaned group.\" [2]\n        #\n        # The first case can be ignored since the child is guaranteed not to have\n        # a controlling terminal.  The second case isn't so easy to dismiss.\n        # The process group is orphaned when the first child terminates and\n        # POSIX.1 requires that every STOPPED process in an orphaned process\n        # group be sent a SIGHUP signal followed by a SIGCONT signal.  Since the\n        # second child is not STOPPED though, we can safely forego ignoring the\n        # SIGHUP signal.  In any case, there are no ill-effects if it is ignored.\n        #\n        # import signal           # Set handlers for asynchronous events.\n        # signal.signal(signal.SIGHUP, signal.SIG_IGN)\n\n        try:\n            # Fork a second child and exit immediately to prevent zombies.  This\n            # causes the second child process to be orphaned, making the init\n            # process responsible for its cleanup.  And, since the first child is\n            # a session leader without a controlling terminal, it's possible for\n            # it to acquire one by opening a terminal in the future (System V-\n            # based systems).  This second fork guarantees that the child is no\n            # longer a session leader, preventing the daemon from ever acquiring\n            # a controlling terminal.\n            pid = os.fork()  # Fork a second child.\n        except OSError as err:\n            raise Exception(\"%s [%d]\" % (err.strerror, err.errno))\n\n        if pid == 0:  # The second child.\n            # Since the current working directory may be a mounted filesystem, we\n            # avoid the issue of not being able to unmount the filesystem at\n            # shutdown time by changing it to the root directory.\n            os.chdir(WORKDIR)\n            # We probably don't want the file mode creation mask inherited from\n            # the parent, so we give the child complete control over permissions.\n            os.umask(UMASK)\n        else:\n            # exit() or _exit()?  See below.\n            _exit(0)  # Exit parent (the first child) of the second child.\n    else:\n        # exit() or _exit()?\n        # _exit is like exit(), but it doesn't call any functions registered\n        # with atexit (and on_exit) or any registered signal handlers.  It also\n        # closes any open file descriptors.  Using exit() may cause all stdio\n        # streams to be flushed twice and any temporary files may be unexpectedly\n        # removed.  It's therefore recommended that child branches of a fork()\n        # and the parent branch(es) of a daemon use _exit().\n        _exit(0)  # Exit parent of the first child.\n\n    # Close all open file descriptors.  This prevents the child from keeping\n    # open any file descriptors inherited from the parent.  There is a variety\n    # of methods to accomplish this task.  Three are listed below.\n    #\n    # Try the system configuration variable, SC_OPEN_MAX, to obtain the maximum\n    # number of open file descriptors to close.  If it doesn't exists, use\n    # the default value (configurable).\n    #\n    # try:\n    #    maxfd = os.sysconf(\"SC_OPEN_MAX\")\n    # except (AttributeError, ValueError):\n    #    maxfd = MAXFD\n    #\n    # OR\n    #\n    # if os.sysconf_names.has_key(\"SC_OPEN_MAX\"):\n    #    maxfd = os.sysconf(\"SC_OPEN_MAX\")\n    # else:\n    #    maxfd = MAXFD\n    #\n    # OR\n    #\n    # Use the getrlimit method to retrieve the maximum file descriptor number\n    # that can be opened by this process.  If there is not limit on the\n    # resource, use the default value.\n    #\n    import resource  # Resource usage information.\n    maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]\n    if maxfd == resource.RLIM_INFINITY:\n        maxfd = MAXFD\n\n    # Iterate through and close all file descriptors.\n    for file_desc in range(0, maxfd):\n        try:\n            os.close(file_desc)\n        except OSError:  # file descriptor wasn't open to begin with (ignored)\n            pass\n\n    # Redirect the standard I/O file descriptors to the specified file.  Since\n    # the daemon has no controlling terminal, most daemons redirect stdin,\n    # stdout, and stderr to /dev/null.  This is done to prevent side-effects\n    # from reads and writes to the standard I/O file descriptors.\n\n    # This call to open is guaranteed to return the lowest file descriptor,\n    # which will be 0 (stdin), since it was closed above.\n    os.open(REDIRECT_TO, os.O_RDWR)  # standard input (0)\n\n    # Duplicate standard input to standard output and standard error.\n    os.dup2(0, 1)  # standard output (1)\n    os.dup2(0, 2)  # standard error (2)\n\n    return 0", "response": "Create a daemon process from the controlling terminal and run it in the background."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the contents of a file located relative to setup. py", "response": "def read_file(filename):\n    \"\"\"Read the contents of a file located relative to setup.py\"\"\"\n    with open(join(abspath(dirname(__file__)), filename)) as file:\n        return file.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a line of the Nginx error log", "response": "def parse(self, line):\n        \"\"\"Parse a line of the Nginx error log\"\"\"\n        csv_list = line.split(\",\")\n        date_time_message = csv_list.pop(0).split(\" \", 2)\n        otherinfo = dict()\n\n        for item in csv_list:\n            key_value_pair = item.split(\":\", 1)\n            key = key_value_pair[0].strip()\n\n            if len(key_value_pair) > 1:\n                value = key_value_pair[1].strip()\n                if not value:\n                    value = \"-\"\n            else:\n                value = \"-\"\n\n            otherinfo[key] = value\n\n        self.message = '%s\\n' \\\n                       'Date: %s\\n' \\\n                       'Time: %s\\n' \\\n                       'Request: %s\\n' \\\n                       'Referrer: %s\\n' \\\n                       'Server: %s\\n' \\\n                       'Client: %s\\n' \\\n                       'Host: %s\\n' \\\n                       'Upstream: %s\\n'\n        self.params = [\n            date_time_message[2],\n            date_time_message[0],\n            date_time_message[1],\n            otherinfo.get(\"request\", \"-\"),\n            otherinfo.get(\"referrer\", \"-\"),\n            otherinfo.get(\"server\", \"-\"),\n            otherinfo.get(\"client\", \"-\"),\n            otherinfo.get(\"host\", \"-\"),\n            otherinfo.get(\"upstream\", \"-\"),\n        ]\n        self.site = otherinfo.get(\"referrer\", \"-\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare a set of setup test and teardown code to be run in the console.", "response": "def load(self, code, setup='', teardown=''):\n        \"\"\"Prepares a set of setup, test, and teardown code to be\n        run in the console.\n\n        PARAMETERS:\n        code     -- list; processed lines of code. Elements in the list are\n                    either strings (input) or CodeAnswer objects (output)\n        setup    -- str; raw setup code\n        teardown -- str; raw teardown code\n        \"\"\"\n        super().load(code, setup, teardown)\n        self._frame = self._original_frame.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprompting user if they want to resume an existing session or create a new session.", "response": "def prompt_for_existing_session(self, sessions):\n        \"\"\" Prompt user if they want to resume an old session\n        (or their partners session) or create a new session.\n        \"\"\"\n        if not sessions:\n            return None\n        print(\"Would you like to join a previous session or create a new session?\")\n        for index, session in enumerate(sessions):\n            print((\"{id} : {creator} started at {timestamp} ({hashid})\"\n                   .format(id=index+1, creator=session.get('creator'),\n                           timestamp=session.get('created'), hashid=session.get('id'))))\n        print(\"{new_id} : Create a new session with the current files?\"\n              .format(new_id=len(sessions)+1))\n        desired = input(\"Type the number of the session you'd like to join: \")\n        try:\n            outcome = int(desired.strip())\n        except:\n            outcome = len(sessions)+1\n            log.warning(\"Could not parse int for choice\")\n\n        if outcome >= len(sessions):\n            log.info(\"Chose to start new session\")\n            return None\n        else:\n            log.info(\"Resuming session {}\".format(outcome - 1))\n            desired = sessions[outcome - 1]\n            return session"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend messages to the server along with user authentication.", "response": "def send_messages(self, data, timeout=30, endpoint='/collab/start/'):\n        \"\"\"Send messages to server, along with user authentication.\"\"\"\n        address = 'https://{}{}'.format(self.COLLAB_SERVER, endpoint)\n        params = {\n            'client_name': 'ok-client',\n            'client_version': client.__version__,\n        }\n\n        log.info('Sending messages to %s', address)\n        try:\n            r = requests.post(address, params=params, json=data, timeout=timeout)\n            r.raise_for_status()\n            return r.json()\n        except (requests.exceptions.RequestException, requests.exceptions.BaseHTTPError, Exception) as ex:\n            message = '{}: {}'.format(ex.__class__.__name__, str(ex))\n            log.warning(message)\n            print(\"There was an error connecting to the server.\"\n                  \"Run with --debug for more details\")\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends messages to server along with user authentication.", "response": "def send_messages(self, messages, timeout):\n        \"\"\"Send messages to server, along with user authentication.\"\"\"\n        data = {\n            'assignment': self.assignment.endpoint,\n            'messages': messages,\n            'submit': self.args.submit\n        }\n        address = self.API_ENDPOINT\n        address_params = {\n            'client_name': 'ok-client',\n            'client_version': client.__version__,\n        }\n        log.info('Sending messages to %s', address)\n        try:\n            response = requests.post(address,\n                params=address_params, json=data, timeout=timeout)\n            response.raise_for_status()\n            return response.json()['url']\n        except (requests.exceptions.RequestException, requests.exceptions.BaseHTTPError, ValueError) as ex:\n            log.warning('%s: %s', ex.__class__.__name__, str(ex))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_json(self):\n        if not hasattr(self, 'guidance_json'):\n            return False\n\n        checksum = self.guidance_json.get('checksum')\n        contents = self.guidance_json.get('db')\n\n        hash_key = (\"{}{}\".format(json.dumps(contents, sort_keys=True),\n                                  self.assignment.endpoint).encode())\n\n        digest = hashlib.md5(hash_key).hexdigest()\n\n        if not checksum:\n            log.warning(\"Checksum on guidance not found. Invalidating file\")\n            return False\n        if digest != checksum:\n            log.warning(\"Checksum %s did not match actual digest %s\", checksum, digest)\n            return False\n        return True", "response": "Ensure that the checksum matches the actual checksum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_guidance_msg(self, unique_id, input_lines, hash_key,\n                          guidance_flag=False):\n        \"\"\"\n        Based on the student's answer (input_lines), we grab each associated\n        message if its corresponding misunderstanding's count is above the threshold\n        \"\"\"\n        if self.load_error:\n            print(GUIDANCE_DEFAULT_MSG)\n            return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n\n        response = repr(input_lines)\n        self.set_tg()\n        log.info(\"Guidance TG is %d\", self.tg_id)\n\n        if self.tg_id == TG_ERROR_VALUE:\n            # If self.tg_id == -1, there was an error when trying to access the server\n            log.warning(\"Error when trying to access server. TG == -1\")\n            print(GUIDANCE_DEFAULT_MSG)\n            return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n\n        lambda_string_key = self.guidance_json[\n            'dictTg2Func'].get(str(self.tg_id))\n\n        if not lambda_string_key:\n            log.info(\"Cannot find the correct lambda in the dictionary.\")\n            print(GUIDANCE_DEFAULT_MSG)\n            return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n        log.info(\"Lambda Group: %s\", lambda_string_key)\n\n        lambda_info_misu = lambda_string_key_to_func.get(lambda_string_key)\n        if not lambda_info_misu:\n            log.info(\"Cannot find info misU given the lambda string key.\")\n            print(GUIDANCE_DEFAULT_MSG)\n            return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n\n        shorten_unique_id = assess_id_util.canonicalize(unique_id)\n        # Try to get the info dictionary for this question. Maps wrong answer\n        # to dictionary\n        assess_dict_info = self.guidance_json[\n            'dictAssessId2Info'].get(shorten_unique_id)\n        if not assess_dict_info:\n            log.info(\"shorten_unique_id %s is not in dictAssessId2Info\", repr(shorten_unique_id))\n            print(GUIDANCE_DEFAULT_MSG)\n            return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n\n        wa_details = assess_dict_info['dictWA2DictInfo'].get(response)\n        if not wa_details:\n            log.info(\"Cannot find the wrong answer in the WA2Dict for this assesment.\")\n            lst_mis_u = []\n        else:\n            lst_mis_u = wa_details.get('lstMisU', [])\n\n        # No list of misunderstandings for this wrong answer, default message\n        if not lst_mis_u:\n            log.info(\"Cannot find the list of misunderstandings.\")\n\n        wa_count_threshold = self.guidance_json['wrongAnsThresh']\n        wa_lst_assess_num = assess_dict_info['dictWA2LstAssessNum_WA']\n        msg_id_set = set()\n        should_skip_propagation = self.tg_id == 3 or self.tg_id == 4\n\n        answerDict, countData = self.get_misUdata()\n        prev_responses = answerDict.get(shorten_unique_id, [])\n\n        # Confirm that this WA has not been given before\n        seen_before = response in prev_responses\n        if seen_before:\n            log.info(\"Answer has been seen before: {}\".format(response))\n        else:\n            answerDict[shorten_unique_id] = prev_responses + [response]\n            self.save_misUdata(answerDict, countData)\n\n            # Lookup the list of assessNum and WA related to this wrong answer\n            # in the question's dictWA2LstAssessNum_WA\n            lst_assess_num = wa_lst_assess_num.get(response, [])\n            if not lst_assess_num:\n                log.info(\"Cannot get the lst of assess nums given this reponse.\")\n            log.debug(\"Related LST_ASSESS_NUM: %s\", lst_assess_num)\n\n            # Check if the current wrong answer is in the question's dictWA2DictInfo\n            if wa_details:\n                log.info(\"The current wrong answer (%s) is in dictWA2DictInfo\", response)\n                # Check in answerDict to see if the student has ever given\n                # any of these wrong answers (sourced from dictWA2LstAssessNum_WA)\n                num_prev_responses = 1\n\n                for other_num, other_resp in lst_assess_num:\n                    # Get assess_id\n                    other_id = self.get_aid_from_anum(other_num)\n                    log.info(\"Checking if %s is in answerDict[%s]\", other_resp, repr(other_id))\n                    if other_resp in answerDict.get(other_id, []):\n                        log.debug(\"%s is in answerDict[%s]\", other_resp, repr(other_id))\n                        num_prev_responses += 1\n\n                log.info(\"Has given %d previous responses in lst_assess_num\", num_prev_responses)\n\n                if not should_skip_propagation:\n                    # Increment countDict by the number of wrong answers seen\n                    # for each tag assoicated with this wrong answerDict\n                    increment = num_prev_responses\n                    for misu in lst_mis_u:\n                        log.info(\"Updating the count of misu: %s by %s\", misu, increment)\n                        countData[misu] = countData.get(misu, 0) + increment\n\n                for misu in lst_mis_u:\n                    log.debug(\"Misu: %s has count %s\", misu, countData.get(misu, 0))\n                    if countData.get(misu, 0) >= wa_count_threshold:\n                        msg_info = lambda_info_misu(wa_details, misu)\n                        if msg_info:\n                            msg_id_set.add(msg_info)\n\n            elif not should_skip_propagation:\n                # Lookup the lst_mis_u of each wrong answer in the list of wrong\n                # answers related to the current wrong answer (lst_assess_num),\n                # using dictAssessNum2AssessId\n                assess_num_to_aid = self.guidance_json['dictAssessNum2AssessId']\n                log.debug(\"Looking up the lst_misu_u of all related WA\")\n\n                # misu -> list of wrong answers for that\n                related_misu_tags_dict = {}\n\n                for related_num, related_resp in lst_assess_num:\n                    related_aid = assess_num_to_aid.get(related_num)\n                    log.info(\"Getting related resp %s for AID %s\", repr(related_aid), related_resp)\n                    resp_seen_before = related_resp in answerDict.get(related_aid, [])\n\n                    if not resp_seen_before:\n                      continue\n\n                    # Get the lst_misu for this asssigmment\n                    related_info = self.guidance_json['dictAssessId2Info'].get(related_aid)\n                    if not related_info:\n                        log.info(\"Could not find related id: %s in info dict\",\n                                 related_aid)\n                        continue\n                    related_wa_info = related_info['dictWA2DictInfo'].get(related_resp)\n\n                    if not related_info:\n                        log.info(\"Could not find response %s in %s info dict\",\n                                 related_resp, related_aid)\n                        continue\n\n                    related_misu_list = related_wa_info.get('lstMisU', [])\n                    log.info(\"The related MISU list is %s\", related_misu_list)\n\n                    for misu in related_misu_list:\n                        existing_resps = related_misu_tags_dict.get(misu, [])\n                        # Add dictWA2DictInfo to list of responses for this misunderstanding.\n                        related_misu_tags_dict[misu] = existing_resps + [related_wa_info]\n                        # Increment countDict for each tag in the set of tags for each related resp\n                        countData[misu] = countData.get(misu, 0) + 1\n\n                    for misu, lst_wa_info in related_misu_tags_dict.items():\n                        if countData.get(misu, 0) >= wa_count_threshold:\n                            for wa_info in lst_wa_info:\n                                msg_id_set.add(lambda_info_misu(wa_info, misu))\n                        else:\n                            log.info(\"misu %s seen %s/%s times\",\n                                     misu, countData.get(misu, 0), wa_count_threshold)\n\n        self.save_misUdata(answerDict, countData)\n\n        wa_lst_explain_responses = assess_dict_info.get('lstWrongAnsWatch', [])\n        if response in wa_lst_explain_responses:\n            rationale = self.prompt_with_prob(orig_response=input_lines, prob=1.0)\n        else:\n            rationale = self.prompt_with_prob(orig_response=input_lines)\n\n        if len(msg_id_set) == 0:\n            log.info(\"No messages to display.\")\n            print(GUIDANCE_DEFAULT_MSG)\n            return (countData, self.tg_id, [], rationale)\n\n        print(\"\\n-- Helpful Hint --\")\n\n        printed_out_msgs = []\n        for message_id in msg_id_set:\n            msg = self.guidance_json['dictId2Msg'].get(str(message_id))\n            if msg:\n                printed_out_msgs.append(msg)\n                print(msg)\n                print(\"-\"*18)\n            else:\n                log.info(\"{} did not have a message\".format(message_id))\n        print()\n        print(GUIDANCE_DEFAULT_MSG)\n\n        return (countData, self.tg_id, printed_out_msgs, rationale)", "response": "This function is used to show the message of a student s answer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_tg(self):\n        # Checks to see the student currently has a treatment group number.\n        if not os.path.isfile(self.current_working_dir + LOCAL_TG_FILE):\n            cur_email = self.assignment.get_student_email()\n            log.info(\"Current email is %s\", cur_email)\n            if not cur_email:\n                self.tg_id = -1\n                return EMPTY_MISUCOUNT_TGID_PRNTEDMSG\n\n            tg_url = (\"{}{}/{}{}\"\n                      .format(TGSERVER, cur_email, self.assignment_name,\n                              TG_SERVER_ENDING))\n            try:\n                log.info(\"Accessing treatment server at %s\", tg_url)\n                data = requests.get(tg_url, timeout=1).json()\n            except IOError:\n                data = {\"tg\": -1}\n                log.warning(\"Failed to communicate to server\", exc_info=True)\n\n            if data.get(\"tg\") is None:\n                log.warning(\"Server returned back a bad treatment group ID.\")\n                data = {\"tg\": -1}\n\n            with open(self.current_working_dir + LOCAL_TG_FILE, \"w\") as fd:\n                fd.write(str(data[\"tg\"]))\n\n        tg_file = open(self.current_working_dir + LOCAL_TG_FILE, 'r')\n        self.tg_id = int(tg_file.read())", "response": "Try to get the treatment group number for the student. If there is no treatment group number available request it from the server and set the treatment ID to - 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nasks for rationale with a specific level of probability.", "response": "def prompt_with_prob(self, orig_response=None, prob=None):\n        \"\"\"Ask for rationale with a specific level of probability. \"\"\"\n        # Disable opt-out.\n        # if self.assignment.cmd_args.no_experiments:\n        #     log.info(\"Skipping prompt due to --no-experiments\")\n        #     return \"Skipped due to --no-experiments\"\n        if self.load_error:\n            return 'Failed to read guidance config file'\n        if hasattr(self.assignment, 'is_test'):\n            log.info(\"Skipping prompt due to test mode\")\n            return \"Test response\"\n\n        if prob is None:\n            prob = self.prompt_probability\n\n        if random.random() > prob:\n            log.info(\"Did not prompt for rationale: Insufficient Probability\")\n            return \"Did not prompt for rationale\"\n        with format.block(style=\"-\"):\n            rationale = prompt.explanation_msg(EXPLANTION_PROMPT,\n                                short_msg=CONFIRM_BLANK_EXPLANATION)\n\n        if prob is None:\n            # Reduce future prompt likelihood\n            self.prompt_probability = 0\n        if orig_response:\n            print('Thanks! Your original response was: {}'.format('\\n'.join(orig_response)))\n\n        return rationale"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef patch_requests():\n    config.create_config_directory()\n    ca_certs_file = config.CERT_FILE\n    ca_certs_contents = requests.__loader__.get_data('requests/cacert.pem')\n\n    should_write_certs = True\n\n    if os.path.isfile(ca_certs_file):\n        with open(ca_certs_file, 'rb') as f:\n            existing_certs = f.read()\n            if existing_certs != ca_certs_contents:\n                should_write_certs = True\n                print(\"Updating local SSL certificates\")\n            else:\n                should_write_certs = False\n\n    if should_write_certs:\n        with open(ca_certs_file, 'wb') as f:\n            f.write(ca_certs_contents)\n\n    os.environ['REQUESTS_CA_BUNDLE'] = ca_certs_file", "response": "Customize the cacerts. pem file that requests uses."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, messages):\n        files = {}\n        # TODO(albert): move this to AnalyticsProtocol\n        if self.args.submit:\n            files['submit'] = True\n        for file in self.assignment.src:\n            if not self.is_file(file):\n                # TODO(albert): add an error message\n                contents = ''\n                log.warning('File {} does not exist'.format(file))\n            else:\n                contents = self.read_file(file)\n                log.info('Loaded contents of {} to send to server'.format(file))\n            files[file] = contents\n\n        messages['file_contents'] = files", "response": "Find all source files and return their complete contents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to bind a port. Default = 0 selects a free port.", "response": "def pick_free_port(hostname=REDIRECT_HOST, port=0):\n    \"\"\" Try to bind a port. Default=0 selects a free port. \"\"\"\n    import socket\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.bind((hostname, port))  # port=0 finds an open port\n    except OSError as e:\n        log.warning(\"Could not bind to %s:%s %s\", hostname, port, e)\n        if port == 0:\n            print('Unable to find an open port for authentication.')\n            raise AuthenticationException(e)\n        else:\n            return pick_free_port(hostname, 0)\n    addr, port = s.getsockname()\n    s.close()\n    return port"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_token_post(server, data):\n    try:\n        response = requests.post(server + TOKEN_ENDPOINT, data=data, timeout=TIMEOUT)\n        body = response.json()\n    except Exception as e:\n        log.warning('Other error when exchanging code', exc_info=True)\n        raise OAuthException(\n            error='Authentication Failed',\n            error_description=str(e))\n    if 'error' in body:\n        log.error(body)\n        raise OAuthException(\n            error=body.get('error', 'Unknown Error'),\n            error_description = body.get('error_description', ''))\n    return body", "response": "Try getting an access token from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authenticate(cmd_args, endpoint='', force=False):\n    server = server_url(cmd_args)\n    network.check_ssl()\n    access_token = None\n\n    try:\n        assert not force\n        access_token = refresh_local_token(server)\n    except Exception:\n        print('Performing authentication')\n        access_token = perform_oauth(get_code, cmd_args, endpoint)\n        email = display_student_email(cmd_args, access_token)\n        if not email:\n            log.warning('Could not get login email. Try logging in again.')\n\n    log.debug('Authenticated with access token={}'.format(access_token))\n\n    return access_token", "response": "Returns an OAuth token that can be passed to the server for\n    identification."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef notebook_authenticate(cmd_args, force=False, silent=True):\n    server = server_url(cmd_args)\n    network.check_ssl()\n    access_token = None\n    if not force:\n        try:\n            access_token = refresh_local_token(server)\n        except OAuthException as e:\n            # Account for Invalid Grant Error During make_token_post\n            if not silent:\n                raise e\n            return notebook_authenticate(cmd_args, force=True, silent=False)\n\n    if not access_token:\n        access_token = perform_oauth(\n            get_code_via_terminal,\n            cmd_args,\n            copy_msg=NOTEBOOK_COPY_MESSAGE,\n            paste_msg=NOTEBOOK_PASTE_MESSAGE)\n\n    # Always display email\n    email = display_student_email(cmd_args, access_token)\n    if email is None and not force:\n        return notebook_authenticate(cmd_args, force=True)  # Token has expired\n    elif email is None:\n        # Did not get a valid token even after a fresh login\n        log.warning('Could not get login email. You may have been logged out. '\n                    ' Try logging in again.')\n    return access_token", "response": "Authenticate and display student emails after a fresh login."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to get the student s email. Returns the email or None.", "response": "def get_student_email(cmd_args, endpoint=''):\n    \"\"\"Attempts to get the student's email. Returns the email, or None.\"\"\"\n    log.info(\"Attempting to get student email\")\n    if cmd_args.local:\n        return None\n    access_token = authenticate(cmd_args, endpoint=endpoint, force=False)\n    if not access_token:\n        return None\n    try:\n        return get_info(cmd_args, access_token)['email']\n    except IOError as e:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain an anonmyzied identifier.", "response": "def get_identifier(cmd_args, endpoint=''):\n    \"\"\" Obtain anonmyzied identifier.\"\"\"\n    student_email = get_student_email(cmd_args, endpoint)\n    if not student_email:\n        return \"Unknown\"\n    return hashlib.md5(student_email.encode()).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_version(server, version, filename, timeout=SHORT_TIMEOUT):\n\n    address = VERSION_ENDPOINT.format(server=server)\n\n    print('Checking for software updates...')\n    log.info('Existing OK version: %s', version)\n    log.info('Checking latest version from %s', address)\n\n    try:\n        response = requests.get(address, timeout=timeout)\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.BaseHTTPError) as e:\n        print('Network error when checking for updates.')\n        log.warning('Network error when checking version from %s: %s', address,\n                    str(e), stack_info=True)\n        return False\n\n    response_json = response.json()\n    if not _validate_api_response(response_json):\n        print('Error while checking updates: malformed server response')\n        log.info('Malformed response from %s: %s', address, response.text)\n        return False\n\n    current_version = response_json['data']['results'][0]['current_version']\n    if current_version == version:\n        print('OK is up to date')\n        return True\n\n    download_link = response_json['data']['results'][0]['download_link']\n\n    log.info('Downloading version %s from %s', current_version, download_link)\n\n    try:\n        response = requests.get(download_link, timeout=timeout)\n        response.raise_for_status()\n    except (requests.exceptions.RequestException, requests.exceptions.BaseHTTPError) as e:\n        print('Error when downloading new version of OK')\n        log.warning('Error when downloading new version of OK: %s', str(e),\n                    stack_info=True)\n        return False\n\n    log.info('Writing new version to %s', filename)\n\n    zip_binary = response.content\n    try:\n        _write_zip(filename, zip_binary)\n    except IOError as e:\n        print('Error when downloading new version of OK')\n        log.warning('Error writing to %s: %s', filename, str(e))\n        return False\n    else:\n        print('Updated to version: {}'.format(current_version))\n        log.info('Successfully wrote to %s', filename)\n        return True", "response": "Check for the latest version of OK and update accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing the GradedTestCase interface.", "response": "def run(self):\n        \"\"\"Implements the GradedTestCase interface.\"\"\"\n        self.console.load(self.lines, setup=self.setup, teardown=self.teardown)\n        return self.console.interpret()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unlock(self, unique_id_prefix, case_id, interact):\n        print(self.setup.strip())\n        prompt_num = 0\n        current_prompt = []\n        try:\n            for line in self.lines:\n                if isinstance(line, str) and line:\n                    print(line)\n                    current_prompt.append(line)\n                elif isinstance(line, CodeAnswer):\n                    prompt_num += 1\n                    if not line.locked:\n                        print('\\n'.join(line.output))\n                        continue\n\n                    unique_id = self._construct_unique_id(unique_id_prefix, self.lines)\n                    line.output = interact(unique_id,\n                                           case_id + ' >  Prompt {}'.format(prompt_num),\n                                           '\\n'.join(current_prompt),\n                                           line.output, line.choices)\n                    line.locked = False\n                    current_prompt = []\n            self.locked = False\n        finally:\n            self._sync_code()", "response": "Unlocks the CodeCase.\n\n        PARAMETERS:\n        unique_id_prefix -- string; a prefix of a unique identifier for this\n                            Case, for purposes of analytics.\n        case_id          -- string; an identifier for this Case, for purposes of\n                            analytics.\n        interact         -- function; handles user interaction during the unlocking\n                            phase."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_code(cls, code, PS1, PS2):\n        processed_lines = []\n        for line in textwrap.dedent(code).splitlines():\n            if not line or line.startswith(PS1) or line.startswith(PS2):\n                processed_lines.append(line)\n                continue\n\n            assert len(processed_lines) > 0, 'code improperly formatted: {}'.format(code)\n            if not isinstance(processed_lines[-1], CodeAnswer):\n                processed_lines.append(CodeAnswer())\n            processed_lines[-1].update(line)\n        return processed_lines", "response": "Splits the given string of code based on the provided PS1 and PS2 symbols."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sync_code(self):\n        new_code = []\n        for line in self.lines:\n            if isinstance(line, CodeAnswer):\n                new_code.append(line.dump())\n            else:\n                new_code.append(line)\n        self.code = '\\n'.join(new_code)", "response": "Syncs the current state of self. lines with self. code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _construct_unique_id(self, id_prefix, lines):\n        text = []\n        for line in lines:\n            if isinstance(line, str):\n                text.append(line)\n            elif isinstance(line, CodeAnswer):\n                text.append(line.dump())\n        return id_prefix + '\\n' + '\\n'.join(text)", "response": "Constructs a unique ID for a particular prompt in this case."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, code, setup='', teardown=''):\n        self._setup = textwrap.dedent(setup).splitlines()\n        self._code = code\n        self._teardown = textwrap.dedent(teardown).splitlines()", "response": "Prepares a set of setup test and teardown code to be\n        run in the console."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninterprets the console on the loaded code. Returns True if the code passes False otherwise.", "response": "def interpret(self):\n        \"\"\"Interprets the console on the loaded code.\n\n        RETURNS:\n        bool; True if the code passes, False otherwise.\n        \"\"\"\n        if not self._interpret_lines(self._setup):\n            return False\n\n        success = self._interpret_lines(self._code, compare_all=True)\n        success &= self._interpret_lines(self._teardown)\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninterprets the set of lines of code AttributeNames and returns True if successful False otherwise.", "response": "def _interpret_lines(self, lines, compare_all=False):\n        \"\"\"Interprets the set of lines.\n\n        PARAMTERS:\n        lines       -- list of str; lines of code\n        compare_all -- bool; if True, check for no output for lines that are not\n                       followed by a CodeAnswer\n\n        RETURNS:\n        bool; True if successful, False otherwise.\n        \"\"\"\n        current = []\n        for line in lines + ['']:\n            if isinstance(line, str):\n                if current and (line.startswith(self.PS1) or not line):\n                    # Previous prompt ends when PS1 or a blank line occurs\n                    try:\n                        if compare_all:\n                            self._compare(CodeAnswer(), '\\n'.join(current))\n                        else:\n                            self.evaluate('\\n'.join(current))\n                    except ConsoleException:\n                        return False\n                    current = []\n                if line:\n                    print(line)\n                line = self._strip_prompt(line)\n                current.append(line)\n            elif isinstance(line, CodeAnswer):\n                assert len(current) > 0, 'Answer without a prompt'\n                try:\n                    self._compare(line, '\\n'.join(current))\n                except ConsoleException:\n                    return False\n                current = []\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self):\n        result = list(self.output_lines())\n        if self.locked:\n            result.append('# locked')\n            if self.choices:\n                for choice in self.choices:\n                    result.append('# choice: ' + choice)\n        if self.explanation:\n            result.append('# explanation: ' + self.explanation)\n        return '\\n'.join(result)", "response": "Serialize a test case to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sequence of lines suitable for printing or comparing answers.", "response": "def output_lines(self):\n        \"\"\"Return a sequence of lines, suitable for printing or comparing\n        answers.\n        \"\"\"\n        if self.exception:\n            return [self.EXCEPTION_HEADERS[0], '  ...'] + self.exception_detail\n        else:\n            return self.output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prettyjson(json, indentation='  '):\n    if isinstance(json, int) or isinstance(json, float):\n        return str(json)\n    elif isinstance(json, str):\n        if '\\n' in json:\n            return 'r\"\"\"\\n' + dedent(json) + '\\n\"\"\"'\n        return repr(json)\n    elif isinstance(json, list):\n        lst = [indent(prettyjson(el, indentation), indentation) for el in json]\n        return '[\\n' + ',\\n'.join(lst) + '\\n]'\n    elif isinstance(json, dict):\n        pairs = []\n        for k, v in sorted(json.items()):\n            k = prettyjson(k, indentation)\n            v = prettyjson(v, indentation)\n            pairs.append(indent(k + ': ' + v, indentation))\n        return '{\\n' + ',\\n'.join(pairs) + '\\n}'\n    else:\n        raise exceptions.SerializeException('Invalid json type: {}'.format(json))", "response": "Formats a Python object into a JSON - like string in a JSON like way."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure that all ipynb files in FILE_CONTENTS are valid JSON files.", "response": "def validate_contents(file_contents):\n    \"\"\"Ensures that all ipynb files in FILE_CONTENTS\n    are valid JSON files.\"\"\"\n    for name, contents in file_contents.items():\n        if os.path.splitext(name)[1] != '.ipynb':\n            continue\n        if not contents:\n            return False\n        try:\n            json_object = json.loads(contents)\n        except ValueError:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for_save(filename, timeout=5):\n    modification_time = os.path.getmtime(filename)\n    start_time = time.time()\n    while time.time() < start_time + timeout:\n        if (os.path.getmtime(filename) > modification_time and\n            os.path.getsize(filename) > 0):\n            return True\n        time.sleep(0.2)\n    return False", "response": "Waits up to TIMEOUT seconds for FILENAME to update. Returns True if a save was detected and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the scoring protocol.", "response": "def score(self, env=None, score_out=None):\n        \"\"\" Run the scoring protocol.\n\n        score_out -- str; a file name to write the point breakdown\n                     into.\n\n        Returns: dict; maps score tag (str) -> points (float)\n        \"\"\"\n        messages = {}\n        self.assignment.set_args(\n            score=True,\n            score_out=score_out,\n        )\n        if env is None:\n            import __main__\n            env = __main__.__dict__\n        self.run('scoring', messages, env=env)\n        return messages['scoring']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_notebook(self):\n        try:\n            from IPython.display import display, Javascript\n        except ImportError:\n            log.warning(\"Could not import IPython Display Function\")\n            print(\"Make sure to save your notebook before sending it to OK!\")\n            return\n\n        if self.mode == \"jupyter\":\n            display(Javascript('IPython.notebook.save_checkpoint();'))\n            display(Javascript('IPython.notebook.save_notebook();'))\n        elif self.mode == \"jupyterlab\":\n            display(Javascript('document.querySelector(\\'[data-command=\"docmanager:save\"]\\').click();'))   \n                       \n        print('Saving notebook...', end=' ')\n\n        ipynbs = [path for path in self.assignment.src\n                  if os.path.splitext(path)[1] == '.ipynb']\n        # Wait for first .ipynb to save\n        if ipynbs:\n            if wait_for_save(ipynbs[0]):\n                print(\"Saved '{}'.\".format(ipynbs[0]))\n            else:\n                log.warning(\"Timed out waiting for IPython save\")\n                print(\"Could not automatically save \\'{}\\'\".format(ipynbs[0]))\n                print(\"Make sure your notebook\"\n                      \" is correctly named and saved before submitting to OK!\".format(ipynbs[0]))\n                return False                \n        else:\n            print(\"No valid file sources found\")\n        return True", "response": "Saves the current notebook by injecting JavaScript to save to. ipynb file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing command line input.", "response": "def parse_input():\n    \"\"\"Parses command line input.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('-c', '--config', type=str,\n                        help=\"Specify a configuration file\")\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites all. py files in a source directory to a destination directory.", "response": "def write_tree(zipf, src_directory, dst_directory):\n    \"\"\"Write all .py files in a source directory to a destination directory\n    inside a zip archive.\n    \"\"\"\n    if not os.path.exists(src_directory):\n        abort('Tree ' + src_directory + ' does not exist.')\n    for root, _, files in os.walk(src_directory):\n        for filename in files:\n            if not filename.endswith(('.py', '.pem')):\n                continue\n            fullname = os.path.join(root, filename)\n            arcname = fullname.replace(src_directory, dst_directory)\n            zipf.write(fullname, arcname=arcname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a new log so that calls to write will append to the log. Returns the new log ID.", "response": "def new_log(self):\n        \"\"\"Registers a new log so that calls to write will append to the log.\n\n        RETURN:\n        int; a unique ID to reference the log.\n        \"\"\"\n        log_id = self._num_logs\n        self._logs[log_id] = []\n        self._num_logs += 1\n        return log_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, msg):\n        self._current_stream.write(msg)\n        for log in self._logs.values():\n            log.append(msg)", "response": "Writes a message to the current output stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake an assessment ID and canonicalizeicalizes it across iterations of a course.", "response": "def canonicalize(assess_id):\n    \"\"\"\n    Takes an assessment/question's ID and canonicalizeicalizes it across iterations of\n    a course.\n    \"\"\"\n    \n    hash_regex = re.compile(r'\\w{32}')\n    \n    lines = assess_id.split('\\n')\n    canon_lines = []\n  \n    parsing_code = False\n    for line in lines:\n        line = line.strip()\n\n        if not parsing_code and len(line) > 0:\n            for prompt in DICT_PROMPT_TO_CHARACTER:\n                if line.startswith(prompt):\n                    parsing_code = True\n                    comment_character = DICT_PROMPT_TO_CHARACTER[prompt]\n                    break\n\n        # If False still in preamble and do not include in canonicalized lines\n        if parsing_code:\n            # Remove any comments\n            comment_index = line.find(comment_character)\n            if comment_index >= 0:\n                line = line[0:comment_index].strip()\n\n            # If a hashed answer, replace with constant since these vary by semester\n            if hash_regex.match(line):\n                line = 'LOCKED_ANSWER'\n\n            # Remove any '# locked' text since these are here regardless of language\n            if line == '# locked':\n                line = ''\n\n            if len(line) > 0:\n                canon_lines.append(line)\n\n    return '\\n'.join(canon_lines) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, messages):\n        if self.args.local:\n            return\n\n        # Only run hinting protocol on supported assignments.\n        if self.assignment.endpoint not in self.SUPPORTED_ASSIGNMENTS:\n            message = \"{0} does not support hinting\".format(self.assignment.endpoint)\n            log.info(message)\n            if self.args.hint:\n                print(message)\n            return\n\n        if 'analytics' not in messages:\n            log.info('Analytics Protocol is required for hint generation')\n            return\n        if 'file_contents' not in messages:\n            log.info('File Contents needed to generate hints')\n            return\n\n        if self.args.no_experiments:\n            messages['hinting'] = {'disabled': 'user'}\n            return\n\n        messages['hinting'] = {}\n        history = messages['analytics'].get('history', {})\n        questions = history.get('questions', [])\n        current_q = history.get('question', {})\n        messages['hinting']['flagged'] = self.args.hint\n\n        for question in current_q:\n            if question not in questions:\n                continue\n            stats = questions[question]\n            is_solved = stats['solved'] == True\n            messages['hinting'][question] = {'prompts': {}, 'reflection': {}}\n            hint_info = messages['hinting'][question]\n\n            # Determine a users elgibility for a prompt\n\n            # If the user just solved this question, provide a reflection prompt\n            if is_solved:\n                hint_info['elgible'] = False\n                hint_info['disabled'] = 'solved'\n                if self.args.hint:\n                    print(\"This question has already been solved.\")\n                continue\n            elif stats['attempts'] < self.SMALL_EFFORT:\n                log.info(\"Question %s is not elgible: Attempts: %s, Solved: %s\",\n                         question, stats['attempts'], is_solved)\n                hint_info['elgible'] = False\n                if self.args.hint:\n                    hint_info['disabled'] = 'attempt-count'\n                    print(\"You need to make a few more attempts before the hint system is enabled\")\n                    continue\n            else:\n                # Only prompt every WAIT_ATTEMPTS attempts to avoid annoying user\n                if stats['attempts'] % self.WAIT_ATTEMPTS != 0:\n                    hint_info['disabled'] = 'timer'\n                    hint_info['elgible'] = False\n                    log.info('Waiting for %d more attempts before prompting',\n                             stats['attempts'] % self.WAIT_ATTEMPTS)\n                else:\n                    hint_info['elgible'] = not is_solved\n\n            if not self.args.hint:\n                if hint_info['elgible']:\n                    with format.block(\"-\"):\n                        print(\"To get hints, try using python3 ok --hint -q {}\".format(question))\n                    hint_info['suggested'] = True\n                continue\n\n            hint_info['accept'] = True\n\n            with format.block(\"-\"):\n                print((\"Thinking of a hint for {}\".format(question) +\n                       \"... (This could take up to 30 seconds)\"))\n                pre_hint = random.choice(PRE_HINT_MESSAGES)\n                print(\"In the meantime, consider: \\n{}\".format(pre_hint))\n                hint_info['pre-prompt'] = pre_hint\n\n                log.info('Prompting for hint on %s', question)\n                try:\n                    response = self.query_server(messages, question)\n                except (requests.exceptions.RequestException, requests.exceptions.BaseHTTPError):\n                    log.debug(\"Network error while fetching hint\", exc_info=True)\n                    hint_info['fetch_error'] = True\n                    print(\"\\r\\nNetwork Error while generating hint. Try again later\")\n                    response = None\n                    continue\n\n                if response:\n                    hint_info['response'] = response\n\n                    hint = response.get('message')\n                    pre_prompt = response.get('pre-prompt')\n                    post_prompt = response.get('post-prompt')\n                    system_error = response.get('system-error')\n                    log.info(\"Hint server response: {}\".format(response))\n                    if not hint:\n                        if system_error:\n                            print(\"{}\".format(system_error))\n                        else:\n                            print(\"Sorry. No hints found for the current code. Try again making after some changes\")\n                        continue\n\n                    # Provide padding for the the hint\n                    print(\"\\n{}\".format(hint.rstrip()))\n\n                    if post_prompt:\n                        results['prompts'][query] = prompt.explanation_msg(post_prompt)", "response": "Run the hinting protocol on the current student."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the point breakdown to outfile given a dictionary of scores.", "response": "def display_breakdown(scores, outfile=None):\n    \"\"\"Writes the point breakdown to `outfile` given a dictionary of scores.\n    `outfile` should be a string.  If `outfile` is None, write to stdout.\n\n    RETURNS:\n    dict; 'Total' -> finalized score (float)\n    \"\"\"\n    total = 0\n    outfile = open(outfile, 'w') if outfile else sys.stdout\n\n    format.print_line('-')\n    print('Point breakdown', file=outfile)\n    for name, (score, max_score) in scores.items():\n        print('    {}: {}/{}'.format(name, score, max_score), file=outfile)\n        total += score\n    print(file=outfile)\n\n    print('Score:', file=outfile)\n    print('    Total: {}'.format(total), file=outfile)\n    return {'Total': total}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, messages, env=None):\n        if not self.args.score or self.args.testing:\n            return\n\n        format.print_line('~')\n        print('Scoring tests')\n        print()\n\n        raw_scores = OrderedDict()\n        for test in self.assignment.specified_tests:\n            assert isinstance(test, sources_models.Test), 'ScoringProtocol received invalid test'\n\n            log.info('Scoring test {}'.format(test.name))\n\n            # A hack that allows programmatic API users to plumb a custom\n            # environment through to Python tests.\n            # Use type to ensure is an actual OkTest and not a subclass\n            if type(test) == ok_test_models.OkTest:\n                score = test.score(env=env)\n            else:\n                score = test.score()\n\n            raw_scores[test.name] = (score, test.points)\n\n        messages['scoring'] = display_breakdown(raw_scores, self.args.score_out)\n        print()", "response": "Score tests and print results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlock the given text using the given key and returns the result", "response": "def lock(key, text):\n    \"\"\"Locks the given text using the given key and returns the result\"\"\"\n    return hmac.new(key.encode('utf-8'), text.encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, messages):\n        if not self.args.unlock:\n            return\n\n        format.print_line('~')\n        print('Unlocking tests')\n        print()\n\n        print('At each \"{}\", type what you would expect the output to be.'.format(\n              self.PROMPT))\n        print('Type {} to quit'.format(self.EXIT_INPUTS[0]))\n        print()\n\n        for test in self.assignment.specified_tests:\n            log.info('Unlocking test {}'.format(test.name))\n            self.current_test = test.name\n\n            # Reset guidance explanation probability for every question\n            self.guidance_util.prompt_probability = guidance.DEFAULT_PROMPT_PROBABILITY\n\n            try:\n                test.unlock(self.interact)\n            except (KeyboardInterrupt, EOFError):\n                try:\n                    # TODO(albert): When you use Ctrl+C in Windows, it\n                    # throws two exceptions, so you need to catch both\n                    # of them. Find a cleaner fix for this.\n                    print()\n                    print('-- Exiting unlocker --')\n                except (KeyboardInterrupt, EOFError):\n                    pass\n                print()\n                break\n        messages['unlock'] = self.analytics", "response": "Unlocks each test and returns a dictionary of the status of the unlocking process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread student input for unlocking tests until the student answers correctly. PARAMETERS: unique_id -- str; the ID that is recorded with this unlocking attempt. case_id -- str; the ID that is recorded with this unlocking attempt. question_prompt -- str; the question prompt answer -- list; a list of locked lines in a test case answer. choices -- list or None; a list of choices. If None or an empty list, signifies the question is not multiple choice. randomize -- bool; if True, randomizes the choices on first invocation. DESCRIPTION: Continually prompt the student for an answer to an unlocking question until one of the folliwng happens: 1. The student supplies the correct answer, in which case the supplied answer is returned 2. The student aborts abnormally (either by typing 'exit()' or using Ctrl-C/D. In this case, return None Correctness is determined by the verify method. RETURNS: list; the correct solution (that the student supplied). Each element in the list is a line of the correct output.", "response": "def interact(self, unique_id, case_id, question_prompt, answer, choices=None, randomize=True):\n        \"\"\"Reads student input for unlocking tests until the student\n        answers correctly.\n\n        PARAMETERS:\n        unique_id       -- str; the ID that is recorded with this unlocking\n                           attempt.\n        case_id         -- str; the ID that is recorded with this unlocking\n                           attempt.\n        question_prompt -- str; the question prompt\n        answer          -- list; a list of locked lines in a test case answer.\n        choices         -- list or None; a list of choices. If None or an\n                           empty list, signifies the question is not multiple\n                           choice.\n        randomize       -- bool; if True, randomizes the choices on first\n                           invocation.\n\n        DESCRIPTION:\n        Continually prompt the student for an answer to an unlocking\n        question until one of the folliwng happens:\n\n            1. The student supplies the correct answer, in which case\n               the supplied answer is returned\n            2. The student aborts abnormally (either by typing 'exit()'\n               or using Ctrl-C/D. In this case, return None\n\n        Correctness is determined by the verify method.\n\n        RETURNS:\n        list; the correct solution (that the student supplied). Each element\n        in the list is a line of the correct output.\n        \"\"\"\n\n        if randomize and choices:\n            choices = random.sample(choices, len(choices))\n\n        correct = False\n        while not correct:\n            if choices:\n                assert len(answer) == 1, 'Choices must have 1 line of output'\n                choice_map = self._display_choices(choices)\n\n            question_timestamp = datetime.now()\n            input_lines = []\n\n            for line_number, line in enumerate(answer):\n                if len(answer) == 1:\n                    prompt = self.PROMPT\n                else:\n                    prompt = '(line {}){}'.format(line_number + 1, self.PROMPT)\n\n                student_input = format.normalize(self._input(prompt))\n                self._add_history(student_input)\n                if student_input in self.EXIT_INPUTS:\n                    raise EOFError\n\n                if choices and student_input in choice_map:\n                    student_input = choice_map[student_input]\n\n                correct_answer = self._verify_student_input(student_input, line)\n                if correct_answer:\n                    input_lines.append(correct_answer)\n                else:\n                    input_lines.append(student_input)\n                    break\n            else:\n                correct = True\n            tg_id = -1\n            misU_count_dict = {}\n            rationale = \"Unknown - Default Value\"\n\n            if not correct:\n                guidance_data = self.guidance_util.show_guidance_msg(unique_id, input_lines,\n                                                                     self.hash_key)\n                misU_count_dict, tg_id, printed_msg, rationale = guidance_data\n            else:\n                rationale = self.guidance_util.prompt_with_prob()\n                print(\"-- OK! --\")\n                printed_msg = [\"-- OK! --\"]\n\n            self.analytics.append({\n                'id': unique_id,\n                'case_id': case_id,\n                'question timestamp': self.unix_time(question_timestamp),\n                'answer timestamp': self.unix_time(datetime.now()),\n                'prompt': question_prompt,\n                'answer': input_lines,\n                'correct': correct,\n                'treatment group id': tg_id,\n                'rationale': rationale,\n                'misU count': misU_count_dict,\n                'printed msg': printed_msg\n            })\n            print()\n        return input_lines"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _verify_student_input(self, student_input, locked):\n        guesses = [student_input]\n        try:\n            guesses.append(repr(ast.literal_eval(student_input)))\n        except Exception:\n            pass\n        if student_input.title() in self.SPECIAL_INPUTS:\n            guesses.append(student_input.title())\n        for guess in guesses:\n            if self._verify(guess, locked):\n                return guess", "response": "Verify the student s answer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a mapping of numbers to choices and returns the mapping as a dictionary.", "response": "def _display_choices(self, choices):\n        \"\"\"Prints a mapping of numbers to choices and returns the\n        mapping as a dictionary.\n        \"\"\"\n        print(\"Choose the number of the correct choice:\")\n        choice_map = {}\n        for i, choice in enumerate(choices):\n            i = str(i)\n            print('{}) {}'.format(i, format.indent(choice,\n                                                   ' ' * (len(i) + 2)).strip()))\n            choice = format.normalize(choice)\n            choice_map[i] = choice\n        return choice_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unix_time(self, dt):\n        epoch = datetime.utcfromtimestamp(0)\n        delta = dt - epoch\n        return int(delta.total_seconds())", "response": "Returns the number of seconds since the UNIX epoch for the given datetime."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timed(timeout, fn, args=(), kargs={}):\n    if timeout == 0:\n        return fn(*args, **kargs)\n\n    submission = __ReturningThread(fn, args, kargs)\n    submission.start()\n    submission.join(timeout)\n    if submission.is_alive():\n        raise exceptions.Timeout(timeout)\n    if submission.error is not None:\n        raise submission.error\n    return submission.result", "response": "This function executes a function in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_messages(self, access_token, messages, timeout, current):\n        is_submit = current and self.args.submit and not self.args.revise\n        is_revision = current and self.args.revise\n\n        data = {\n            'assignment': self.assignment.endpoint,\n            'messages': messages,\n            'submit': is_submit\n        }\n\n        if is_revision:\n            address = self.REVISION_ENDPOINT.format(server=self.assignment.server_url)\n        else:\n            address = self.BACKUP_ENDPOINT.format(server=self.assignment.server_url)\n\n        address_params = {\n            'client_name': 'ok-client',\n            'client_version': client.__version__,\n        }\n\n        headers = {'Authorization': 'Bearer {}'.format(access_token)}\n\n        log.info('Sending messages to %s', address)\n        response = requests.post(address, headers=headers,\n            params=address_params, json=data, timeout=timeout)\n        response.raise_for_status()\n        return response.json()", "response": "Send messages to server along with user authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses command line input and returns a list of tuples.", "response": "def parse_input(command_input=None):\n    \"\"\"Parses command line input.\"\"\"\n    parser = argparse.ArgumentParser(\n        prog='python3 ok',\n        description=__doc__,\n        usage='%(prog)s [--help] [options]',\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    testing = parser.add_argument_group('running tests')\n    testing.add_argument('-q', '--question', type=str, action='append',\n                        help=\"run tests for a specific question\")\n    testing.add_argument('--suite', type=str, default=None,\n                        help=\"run cases from a specific suite\")\n    testing.add_argument('--case', type=str, action='append',\n                        help=\"run specific cases\")\n    testing.add_argument('-u', '--unlock', action='store_true',\n                        help=\"unlock tests interactively\")\n    testing.add_argument('-i', '--interactive', action='store_true',\n                        help=\"start the Python interpreter after a failed test\")\n    testing.add_argument('-v', '--verbose', action='store_true',\n                        help=\"show all tests, not just passing tests\")\n    testing.add_argument('-t', '--testing', nargs='?', type=str, const='mytests.rst', \n                        help='run tests from rst file (default: mytests.rst)')\n    testing.add_argument('--all', action='store_true',\n                        help=\"run tests for all questions in config file\")\n    testing.add_argument('--submit', action='store_true',\n                        help=\"submit the assignment\")\n    testing.add_argument('--backup', action='store_true',\n                        help=\"attempt to reliably backup your work\")\n    testing.add_argument('--revise', action='store_true',\n                        help=\"submit composition revision\")\n    testing.add_argument('--timeout', type=int, default=10,\n                        help=\"set the timeout duration (in seconds) for running tests\")\n    testing.add_argument('-cov', '--coverage', action='store_true',\n                        help=\"get suggestions on what lines to add tests for\")\n\n    # Experiments\n    experiment = parser.add_argument_group('experiment options')\n    experiment.add_argument('--no-experiments', action='store_true',\n                        help=\"do not run experimental features\")\n    experiment.add_argument('--hint', action='store_true',\n                        help=\"give a hint (if available)\")\n    experiment.add_argument('--style', action='store_true',\n                        help=\"run AutoStyle feedback system\")\n    experiment.add_argument('--collab', action='store_true',\n                        help=\"launch collaborative programming environment\")\n\n    # Debug information\n    debug = parser.add_argument_group('debugging options')\n    debug.add_argument('--version', action='store_true',\n                        help=\"print the version number and exit\")\n    debug.add_argument('--tests', action='store_true',\n                        help=\"display a list of all available tests\")\n    debug.add_argument('--debug', action='store_true',\n                        help=\"show debugging output\")\n\n    # Grading\n    grading = parser.add_argument_group('grading options')\n    grading.add_argument('--lock', action='store_true',\n                        help=\"lock the tests in a directory\")\n    grading.add_argument('--score', action='store_true',\n                        help=\"score the assignment\")\n    grading.add_argument('--score-out', type=str,\n                        nargs='?', const=None, default=None,\n                        help=\"write scores to a file\")\n    grading.add_argument('--config', type=str,\n                        help=\"use a specific configuration file\")\n\n    # Server parameters\n    server = parser.add_argument_group('server options')\n    server.add_argument('--local', action='store_true',\n                        help=\"disable any network activity\")\n    server.add_argument('--server', type=str,\n                        default='okpy.org',\n                        help=\"set the server address\")\n    server.add_argument('--authenticate', action='store_true',\n                        help=\"authenticate, ignoring previous authentication\")\n    server.add_argument('--no-browser', action='store_true',\n                        help=\"do not use a web browser for authentication\")\n    server.add_argument('--get-token', action='store_true',\n                        help=\"get ok access token\")\n    server.add_argument('--insecure', action='store_true',\n                        help=\"use http instead of https\")\n    server.add_argument('--no-update', action='store_true',\n                        help=\"do not check for ok updates\")\n    server.add_argument('--update', action='store_true',\n                        help=\"update ok and exit\")\n\n    return parser.parse_args(command_input)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning all relevant aspects of ok. py.", "response": "def main():\n    \"\"\"Run all relevant aspects of ok.py.\"\"\"\n    args = parse_input()\n    log.setLevel(logging.DEBUG if args.debug else logging.ERROR)\n    log.debug(args)\n\n    # Checking user's Python bit version\n    bit_v = (8 * struct.calcsize(\"P\"))\n    log.debug(\"Python {} ({}bit)\".format(sys.version, bit_v))\n\n    if args.version:\n        print(\"okpy=={}\".format(client.__version__))\n        exit(0)\n    elif args.update:\n        print(\"Current version: {}\".format(client.__version__))\n        did_update = software_update.check_version(\n                args.server, client.__version__, client.FILE_NAME, timeout=10)\n        exit(not did_update)  # exit with error if ok failed to update\n\n    assign = None\n    try:\n        if args.get_token:\n            access_token = auth.authenticate(args, force=True)\n            print(\"Token: {}\".format(access_token))\n            exit(not access_token)  # exit with error if no access_token\n\n        # Instantiating assignment\n        assign = assignment.load_assignment(args.config, args)\n\n        if args.tests:\n            print('Available tests:')\n            for name in assign.test_map:\n                print('    ' + name)\n            exit(0)\n\n        force_authenticate = args.authenticate\n        retry = True\n        while retry:\n            retry = False\n            if force_authenticate:\n                # Authenticate and check for success\n                if not assign.authenticate(force=True):\n                    exit(1)\n\n            try:\n                msgs = messages.Messages()\n                for name, proto in assign.protocol_map.items():\n                    log.info('Execute {}.run()'.format(name))\n                    proto.run(msgs)\n                msgs['timestamp'] = str(datetime.now())\n            except ex.AuthenticationException as e:\n                if not force_authenticate:\n                    force_authenticate = True\n                    retry = True\n                elif not args.no_browser:\n                    args.no_browser = True\n                    retry = True\n                if retry:\n                    msg = \"without a browser\" if args.no_browser else \"with a browser\"\n                    log.warning('Authentication exception occurred; will retry {0}'.format(msg), exc_info=True)\n                    print('Authentication error; will try to re-authenticate {0}...'.format(msg))\n                else:\n                    raise  # outer handler will be called\n\n    except ex.LoadingException as e:\n        log.warning('Assignment could not load', exc_info=True)\n        print('Error loading assignment: ' + str(e))\n    except ex.AuthenticationException as e:\n        log.warning('Authentication exception occurred', exc_info=True)\n        print('Authentication error: {0}'.format(e))\n    except ex.EarlyExit as e:\n        log.warning('OK exited early (non-error)')\n        print(str(e))\n    except ex.OkException as e:\n        log.warning('General OK exception occurred', exc_info=True)\n        print('Error: ' + str(e))\n    except KeyboardInterrupt:\n        log.info('KeyboardInterrupt received.')\n    finally:\n        if not args.no_update and not args.local:\n            try:\n                software_update.check_version(args.server, client.__version__,\n                                              client.FILE_NAME)\n            except KeyboardInterrupt:\n                pass\n\n        if assign:\n            assign.dump_tests()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grade(self, question, env=None, skip_locked_cases=False):\n        if env is None:\n            import __main__\n            env = __main__.__dict__\n        messages = {}\n        tests = self._resolve_specified_tests([question], all_tests=False)\n        for test in tests:\n            try:\n                for suite in test.suites:\n                    suite.skip_locked_cases = skip_locked_cases\n                    suite.console.skip_locked_cases = skip_locked_cases\n                    suite.console.hash_key = self.name\n            except AttributeError:\n                pass\n        test_name = tests[0].name\n        grade(tests, messages, env)\n        return messages['grading'][test_name]", "response": "Runs tests for a particular question."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn some analytics about this autograder run.", "response": "def run(self, messages):\n        \"\"\"Returns some analytics about this autograder run.\"\"\"\n        statistics = {}\n        statistics['time'] = str(datetime.now())\n        statistics['time-utc'] = str(datetime.utcnow())\n        statistics['unlock'] = self.args.unlock\n\n        if self.args.question:\n            statistics['question'] = [t.name for t in self.assignment.specified_tests]\n            statistics['requested-questions'] = self.args.question\n\n            if self.args.suite:\n                statistics['requested-suite'] = self.args.suite\n            if self.args.case:\n                statistics['requested-case'] = self.args.case\n\n        messages['analytics'] = statistics\n        self.log_run(messages)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replaced(self, contents):\n        line_num = len(contents.strip(' ').splitlines())\n        replace_marks = self.RE_REPLACE_MARK.findall(contents.strip())\n        if len(replace_marks) == line_num:\n            return False\n        return True", "response": "Return True if the contents contains some default code."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_run(self, messages):\n        # Load the contents of the local analytics file\n        history = self.read_history()\n        history['all_attempts'] += 1\n\n        # List of question names that the student asked to have graded\n        questions = messages['analytics'].get('question', [])\n        # The output of the grading protocol\n        grading = messages.get('grading')\n\n        # Attempt to figure out what the student is currently implementing\n        if not questions and grading:\n            # If questions are unspecified by the user, use the first failed test\n            failed = first_failed_test(self.assignment.specified_tests, grading)\n            logging.info('First failed test: {}'.format(failed))\n            if failed:\n                questions = [failed]\n\n            # Update question correctness status from previous attempts\n            for saved_q, details in history['questions'].items():\n                finished = details['solved']\n                if not finished and saved_q in grading:\n                    scoring = grading[saved_q]\n                    details['solved'] = is_correct(scoring)\n\n        # The question(s) that the student is testing right now.\n        history['question'] = questions\n\n        # Update attempt and correctness counts for the graded questions\n        for question in questions:\n            detail = history['questions']\n            if grading and question in grading:\n                scoring = is_correct(grading[question])\n            else:\n                scoring = False\n\n            # Update attempt counts or initialize counts\n            if question in history['questions']:\n                q_info = detail[question]\n                if grading and question in grading:\n                    if q_info['solved'] != True:\n                        q_info['solved'] = scoring\n                    else:\n                        continue  # Already solved. Do not change total\n                q_info['attempts'] += 1\n            else:\n                detail[question] = {\n                    'attempts': 1,\n                    'solved': scoring\n                }\n            logging.info('Attempt %d for Question %s : %r',\n                         history['questions'], question, scoring)\n\n        with open(self.ANALYTICS_FILE, 'wb') as f:\n            log.info('Saving history to %s', self.ANALYTICS_FILE)\n            pickle.dump(history, f)\n            os.fsync(f)\n\n        messages['analytics']['history'] = history", "response": "Log the results of the autograder run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_ssl():\n    try:\n        import ssl\n    except:\n        log.warning('Error importing SSL module', stack_info=True)\n        print(SSL_ERROR_MESSAGE)\n        sys.exit(1)\n    else:\n        log.info('SSL module is available')\n        return ssl", "response": "Attempts to import SSL or raises an exception."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun gradeable tests and print results and return analytics.", "response": "def run(self, messages, env=None):\n        \"\"\"Run gradeable tests and print results and return analytics.\n\n        RETURNS:\n        dict; a mapping of test name -> JSON-serializable object. It is up to\n        each test to determine what kind of data it wants to return as\n        significant for analytics. However, all tests must include the number\n        passed, the number of locked tests and the number of failed tests.\n        \"\"\"\n        if self.args.score or self.args.unlock or self.args.testing:\n            return\n        tests = self.assignment.specified_tests\n        for test in tests:\n            if self.args.suite and hasattr(test, 'suites'):\n                test.run_only = int(self.args.suite)\n                try:\n                    suite = test.suites[int(self.args.suite) - 1]\n                except IndexError as e:\n                    sys.exit(('python3 ok: error: '\n                        'Suite number must be valid.({})'.format(len(test.suites))))\n                if self.args.case:\n                    suite.run_only = [int(c) for c in self.args.case]\n        grade(tests, messages, env, verbose=self.args.verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride this method to coerce the argument to the correct type.", "response": "def coerce(self, value):\n        \"\"\"Subclasses should override this method for type coercion.\n\n        Default version will simply return the argument. If the argument\n        is not valid, a SerializeException is raised.\n\n        For primitives like booleans, ints, floats, and strings, use\n        this default version to avoid unintended type conversions.\"\"\"\n        if not self.is_valid(value):\n            raise ex.SerializeException('{} is not a valid value for '\n                                        'type {}'.format(value, self.__class__.__name__))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_json(self, value):\n        if not self.is_valid(value):\n            raise ex.SerializeException('Invalid value: {}'.format(value))\n        return value", "response": "Override this method for JSON encoding."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a response that contains Content - Encoding to retrieve", "response": "def parse_content_encoding(self, response_headers, response_data):\n        \"\"\"\n        Parses a response that contains Content-Encoding to retrieve\n        response_data\n        \"\"\"\n        if response_headers['content-encoding'] == 'gzip':\n            buf = StringIO.StringIO(response_data)\n            zipbuf = gzip.GzipFile(fileobj=buf)\n            response_data = zipbuf.read()\n        elif response_headers['content-encoding'] == 'deflate':\n            data = StringIO.StringIO(zlib.decompress(response_data))\n            response_data = data.read()\n        else:\n            raise errors.TestError(\n                'Received unknown Content-Encoding',\n                {\n                    'content-encoding':\n                        str(response_headers['content-encoding']),\n                    'function': 'http.HttpResponse.parse_content_encoding'\n                })\n        return response_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_response(self):\n        split_response = self.response.split(self.CRLF)\n        response_line = split_response[0]\n        response_headers = {}\n        response_data = None\n        data_line = None\n        for line_num in range(1, len(split_response[1:])):\n            # CRLF represents the start of data\n            if split_response[line_num] == '':\n                data_line = line_num + 1\n                break\n            else:\n                # Headers are all split by ':'\n                header = split_response[line_num].split(':', 1)\n                if len(header) != 2:\n                    raise errors.TestError(\n                        'Did not receive a response with valid headers',\n                        {\n                            'header_rcvd': str(header),\n                            'function': 'http.HttpResponse.process_response'\n                        })\n                response_headers[header[0].lower()] = header[1].lstrip()\n        if 'set-cookie' in response_headers.keys():\n            try:\n                cookie = Cookie.SimpleCookie()\n                cookie.load(response_headers['set-cookie'])\n            except Cookie.CookieError as err:\n                raise errors.TestError(\n                    'Error processing the cookie content into a SimpleCookie',\n                    {\n                        'msg': str(err),\n                        'set_cookie': str(response_headers['set-cookie']),\n                        'function': 'http.HttpResponse.process_response'\n                    })\n            # if the check_for_cookie is invalid then we don't save it\n            if self.check_for_cookie(cookie) is False:\n                raise errors.TestError(\n                    'An invalid cookie was specified',\n                    {\n                        'set_cookie': str(response_headers['set-cookie']),\n                        'function': 'http.HttpResponse.process_response'\n                    })\n            else:\n                self.cookiejar.append((cookie, self.dest_addr))\n        if data_line is not None and data_line < len(split_response):\n            response_data = self.CRLF.join(split_response[data_line:])\n\n        # if the output headers say there is encoding\n        if 'content-encoding' in response_headers.keys():\n            response_data = self.parse_content_encoding(\n                response_headers, response_data)\n        if len(response_line.split(' ', 2)) != 3:\n            raise errors.TestError(\n                'The HTTP response line returned the wrong args',\n                {\n                    'response_line': str(response_line),\n                    'function': 'http.HttpResponse.process_response'\n                })\n        try:\n            self.status = int(response_line.split(' ', 2)[1])\n        except ValueError:\n            raise errors.TestError(\n                'The status num of the response line isn\\'t convertable',\n                {\n                    'msg': 'This may be an HTTP 1.0 \\'Simple Req\\\\Res\\', it \\\n                    doesn\\'t have HTTP headers and FTW will not parse these',\n                    'response_line': str(response_line),\n                    'function': 'http.HttpResponse.process_response'\n                })\n        self.status_msg = response_line.split(' ', 2)[2]\n        self.version = response_line.split(' ', 2)[0]\n        self.response_line = response_line\n        self.headers = response_headers\n        self.data = response_data", "response": "Parses the response and stores the cookies into the cookiejar"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a request and get response", "response": "def send_request(self, http_request):\n        \"\"\"\n        Send a request and get response\n        \"\"\"\n        self.request_object = http_request\n        self.build_socket()\n        self.build_request()\n        try:\n            self.sock.send(self.request)\n        except socket.error as err:\n            raise errors.TestError(\n                'We were unable to send the request to the socket',\n                {\n                    'msg': err,\n                    'function': 'http.HttpUA.send_request'\n                })  \t\t\t\n        finally:\n            self.get_response()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates either an HTTPS or HTTP socket and connect to the server.", "response": "def build_socket(self):\n        \"\"\"\n        Generate either an HTTPS or HTTP socket\n        \"\"\"\n        try:\n            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.sock.settimeout(self.SOCKET_TIMEOUT)\n            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            # Check if TLS\n            if self.request_object.protocol == 'https':\n                self.sock = ssl.wrap_socket(self.sock, ciphers=self.CIPHERS)\n            self.sock.connect(\n                (self.request_object.dest_addr, self.request_object.port))\n        except socket.error as msg:\n            raise errors.TestError(\n                'Failed to connect to server',\n                {\n                    'host': self.request_object.dest_addr,\n                    'port': self.request_object.port,\n                    'proto': self.request_object.protocol,\n                    'message': msg,\n                    'function': 'http.HttpUA.build_socket'\n                })"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_cookie(self):\n        return_cookies = []\n        origin_domain = self.request_object.dest_addr\n        for cookie in self.cookiejar:\n            for cookie_morsals in cookie[0].values():\n                cover_domain = cookie_morsals['domain']\n                if cover_domain == '':\n                    if origin_domain == cookie[1]:\n                        return_cookies.append(cookie[0])\n                else:\n                    # Domain match algorithm\n                    bvalue = cover_domain.lower()\n                    hdn = origin_domain.lower()\n                    nend = hdn.find(bvalue)\n                    if nend is not False:\n                        return_cookies.append(cookie[0])\n        return return_cookies", "response": "Find a list of all cookies for a given domain"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_response(self):\n        self.sock.setblocking(0)\n        our_data = []\n        # Beginning time\n        begin = time.time()\n        while True:\n            # If we have data then if we're passed the timeout break\n            if our_data and time.time() - begin > self.HTTP_TIMEOUT:\n                break\n            # If we're dataless wait just a bit\n            elif time.time() - begin > self.HTTP_TIMEOUT * 2:\n                break\n            # Recv data\n            try:\n                data = self.sock.recv(self.RECEIVE_BYTES)\n                if data:\n                    our_data.append(data)\n                    begin = time.time()\n                else:\n                    # Sleep for sometime to indicate a gap\n                    time.sleep(self.HTTP_TIMEOUT)\n            except socket.error as err:\n                # Check if we got a timeout\n                if err.errno == errno.EAGAIN:\n                    pass\n                # SSL will return SSLWantRead instead of EAGAIN\n                elif sys.platform == 'win32' and \\\n                err.errno == errno.WSAEWOULDBLOCK:\n                        pass\n                elif (self.request_object.protocol == 'https' and\n                    err[0] == ssl.SSL_ERROR_WANT_READ):\n                        continue\n                # If we didn't it's an error\n                else:\n                    raise errors.TestError(\n                        'Failed to connect to server',\n                        {\n                            'host': self.request_object.dest_addr,\n                            'port': self.request_object.port,\n                            'proto': self.request_object.protocol,\n                            'message': err,\n                            'function': 'http.HttpUA.get_response'\n                        })\n        if ''.join(our_data) == '':\n            raise errors.TestError(\n                'No response from server. Request likely timed out.',\n                {\n                    'host': self.request_object.dest_addr,\n                    'port': self.request_object.port,\n                    'proto': self.request_object.protocol,\n                    'msg': 'Please send the request and check Wireshark',\n                    'function': 'http.HttpUA.get_response'\n                })                                    \n        self.response_object = HttpResponse(''.join(our_data), self)\n        try:\n            self.sock.shutdown(1)\n            self.sock.close()\n        except socket.error as err:\n            raise errors.TestError(\n                'We were unable to close the socket as expected.',\n                {\n                    'msg': err,\n                    'function': 'http.HttpUA.get_response'\n                })", "response": "Get the response from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_regex(self, key):\n        return re.compile(self.output_dict[key]) if \\\n            key in self.output_dict else None", "response": "Process the value of key from dictionary if available\n            and return a python regex"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef instantiate_database(sqlite_file='ftwj.sqlite'):\n    table_name = 'ftw'\n    col1 = 'rule_id'\n    col1_t = 'INTEGER'\n    col2 = 'test_id'\n    col2_t = 'STRING'\n    col3 = 'time_start'\n    col3_t = 'TEXT'\n    col4 = 'time_end'\n    col4_t = 'TEXT'\n    col5 = 'response_blob'\n    col5_t = 'TEXT'\n    col6 = 'status_code'\n    col6_t = 'INTEGER'\n    col7 = 'stage'\n    col7_t = 'INTEGER'\n    conn = sqlite3.connect(sqlite_file)\n    cur = conn.cursor()\n\n    q = 'CREATE TABLE {tn}({col1} {col1_t},{col2} {col2_t},{col3} {col3_t},{col4} {col4_t},{col5} {col5_t},{col6} {col6_t},{col7} {col7_t})'.format(\n        tn=table_name,\n        col1=col1, col1_t=col1_t,\n        col2=col2, col2_t=col2_t,\n        col3=col3, col3_t=col3_t,\n        col4=col4, col4_t=col4_t,\n        col5=col5, col5_t=col5_t,\n        col6=col6, col6_t=col6_t,\n        col7=col7, col7_t=col7_t)\n    cur.execute(q)\n    conn.commit()\n    conn.close()", "response": "Create the database for FTW runs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of ruleset objects extracted from the yaml directory ruledir", "response": "def get_rulesets(ruledir, recurse):\n    \"\"\"\n    List of ruleset objects extracted from the yaml directory\n    \"\"\"\n    if os.path.isdir(ruledir) and recurse:\n        yaml_files = [y for x in os.walk(ruledir) for y in glob(os.path.join(x[0], '*.yaml'))]\n    elif os.path.isdir(ruledir) and not recurse:\n        yaml_files = get_files(ruledir, 'yaml')\n    elif os.path.isfile(ruledir):\n        yaml_files = [ruledir]\n    extracted_files = extract_yaml(yaml_files)\n    rulesets = []\n    for extracted_yaml in extracted_files:\n        rulesets.append(ruleset.Ruleset(extracted_yaml))\n    return rulesets"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of yaml files and loads them to return back to the testing program", "response": "def extract_yaml(yaml_files):\n    \"\"\"\n    Take a list of yaml_files and load them to return back\n    to the testing program\n    \"\"\"\n    loaded_yaml = []\n    for yaml_file in yaml_files:\n        try:\n            with open(yaml_file, 'r') as fd:\n                loaded_yaml.append(yaml.safe_load(fd))\n        except IOError as e:\n            print('Error reading file', yaml_file)\n            raise e\n        except yaml.YAMLError as e:\n            print('Error parsing file', yaml_file)\n            raise e\n        except Exception as e:\n            print('General error')\n            raise e\n    return loaded_yaml"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deserialize(cls, raw_bytes):\n        instance, _ = cls.parse(raw_bytes, offset=0)\n\n        return instance", "response": "Deserialize the given raw bytes into an instance of the class cls."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, parts=None):\n        if not parts:\n            parts = self.parts\n\n        fmt = []\n        data = []\n\n        for name, part_class in parts:\n            if issubclass(part_class, Primitive):\n                part = part_class(getattr(self, name, None))\n            else:\n                part = getattr(self, name, None)\n\n            part_format, part_data = part.render()\n\n            fmt.extend(part_format)\n            data.extend(part_data)\n\n        return \"\".join(fmt), data", "response": "Returns a two - element tuple with the struct format and values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, buff, offset):\n        values = {}\n\n        for name, part in cls.parts:\n            value, new_offset = part.parse(buff, offset)\n\n            values[name] = value\n            offset = new_offset\n\n        return cls(**values), offset", "response": "Given a buffer and offset returns the parsed value and new offset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nabort a connection and puts all pending futures into an error state.", "response": "def abort(self, exception=exc.ConnectError):\n        \"\"\"\n        Aborts a connection and puts all pending futures into an error state.\n\n        If ``sys.exc_info()`` is set (i.e. this is being called in an exception\n        handler) then pending futures will have that exc info set.  Otherwise\n        the given ``exception`` parameter is used (defaults to\n        ``ConnectError``).\n        \"\"\"\n        log.warning(\"Aborting connection to %s:%s\", self.host, self.port)\n\n        def abort_pending(f):\n            exc_info = sys.exc_info()\n            # TODO\n            log.debug('Abort pending: {}'.format(f))\n            if False and any(exc_info):\n                f.set_exc_info(exc_info)\n            else:\n                f.set_exception(exception(self.host, self.port))\n\n        for pending in self.drain_all_pending():\n            if pending.done() or pending.cancelled():\n                continue\n            abort_pending(pending)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(cls, buff, offset):\n        primitive_struct = struct.Struct(\"!\" + cls.fmt)\n\n        value = primitive_struct.unpack_from(buff, offset)[0]\n        offset += primitive_struct.size\n\n        return value, offset", "response": "Parses the value from a buffer and offset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the struct format and list of the size and value.", "response": "def render(self):\n        \"\"\"\n        Returns the ``struct`` format and list of the size and value.\n\n        The format is derived from the size primitive and the length of the\n        resulting encoded value (e.g. the format for a string of 'foo' ends\n        up as 'h3s'.\n\n        .. note ::\n          The value is expected to be string-able (wrapped in ``str()``) and is\n          then encoded as UTF-8.\n        \"\"\"\n        size_format = self.size_primitive.fmt\n\n        if self.value is None:\n            return size_format, [-1]\n\n        value = self.render_value(self.value)\n\n        size = len(value)\n\n        fmt = \"%s%ds\" % (size_format, size)\n\n        return fmt, [size, value]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the value from a buffer and offset.", "response": "def parse(cls, buff, offset):\n        \"\"\"\n        Given a buffer and offset, returns the parsed value and new offset.\n\n        Parses the ``size_primitive`` first to determine how many more bytes to\n        consume to extract the value.\n        \"\"\"\n        size, offset = cls.size_primitive.parse(buff, offset)\n        if size == -1:\n            return None, offset\n\n        var_struct = struct.Struct(\"!%ds\" % size)\n\n        value = var_struct.unpack_from(buff, offset)[0]\n        value = cls.parse_value(value)\n        offset += var_struct.size\n\n        return value, offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef of(cls, part_class):\n        copy = type(\n            \"VectorOf%s\" % part_class.__name__,\n            cls.__bases__, dict(cls.__dict__)\n        )\n        copy.item_class = part_class\n\n        return copy", "response": "Returns a new class with the item_class attribute properly set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string representing the format and data for the array value.", "response": "def render(self):\n        \"\"\"\n        Creates a composite ``struct`` format and the data to render with it.\n\n        The format and data are prefixed with a 32-bit integer denoting the\n        number of elements, after which each of the items in the array value\n        are ``render()``-ed and added to the format and data as well.\n        \"\"\"\n        value = self.value\n        if value is None:\n            value = []\n\n        fmt = [Int.fmt]\n        data = [len(value)]\n\n        for item_value in value:\n            if issubclass(self.item_class, Primitive):\n                item = self.item_class(item_value)\n            else:\n                item = item_value\n\n            item_format, item_data = item.render()\n            fmt.extend(item_format)\n            data.extend(item_data)\n\n        return \"\".join(fmt), data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, buff, offset):\n        count, offset = Int.parse(buff, offset)\n\n        values = []\n        for _ in range(count):\n            value, new_offset = cls.item_class.parse(buff, offset)\n\n            values.append(value)\n            offset = new_offset\n\n        return values, offset", "response": "Parses a raw buffer at offset and returns the resulting array value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef round_robin(members, items):\n    allocation = collections.defaultdict(set)\n\n    for member, item in zip(itertools.cycle(members), items):\n        allocation[member].add(item)\n\n    return allocation", "response": "Default allocator with a round robin approach."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator that yields the frames from start to stop inclusive.", "response": "def xfrange(start, stop, step=1, maxSize=-1):\n    \"\"\"\n    Returns a generator that yields the frames from start to stop, inclusive.\n    In other words it adds or subtracts a frame, as necessary, to return the\n    stop value as well, if the stepped range would touch that value.\n\n    Args:\n        start (int):\n        stop (int):\n        step (int): Note that the sign will be ignored\n        maxSize (int):\n\n    Returns:\n        generator:\n\n    Raises:\n        :class:`fileseq.exceptions.MaxSizeException`: if size is exceeded\n    \"\"\"\n    if start <= stop:\n        stop, step = stop + 1, abs(step)\n    else:\n        stop, step = stop - 1, -abs(step)\n\n    if maxSize >= 0:\n        size = lenRange(start, stop, step)\n        if size > maxSize:\n            raise exceptions.MaxSizeException(\n                \"Size %d > %s (MAX_FRAME_SIZE)\" % (size, maxSize))\n\n    # because an xrange is an odd object all its own, we wrap it in a\n    # generator expression to get a proper Generator\n    return (f for f in xrange(start, stop, step))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a generator that returns unique items in iterables while preserving order.", "response": "def unique(seen, *iterables):\n    \"\"\"\n    Get the unique items in iterables while preserving order.  Note that this\n    mutates the seen set provided only when the returned generator is used.\n\n    Args:\n        seen (set): either an empty set, or the set of things already seen\n        *iterables: one or more iterable lists to chain together\n\n    Returns:\n        generator:\n    \"\"\"\n    _add = seen.add\n    # return a generator of the unique items and the set of the seen items\n    # the seen set will mutate when the generator is iterated over\n    return (i for i in chain(*iterables) if i not in seen and not _add(i))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a deep copy of this file sequence.", "response": "def copy(self):\n        \"\"\"\n        Create a deep copy of this sequence\n\n        Returns:\n            :obj:`.FileSequence`:\n        \"\"\"\n        fs = self.__class__.__new__(self.__class__)\n        fs.__dict__ = self.__dict__.copy()\n        fs._frameSet = None\n        if self._frameSet is not None:\n            fs._frameSet = self._frameSet.copy()\n        return fs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the file sequence as a string according to the given template.", "response": "def format(self, template=\"{basename}{range}{padding}{extension}\"):\n        \"\"\"Return the file sequence as a formatted string according to\n        the given template.\n\n        Utilizes the python string format syntax.  Available keys include:\n            * basename - the basename of the sequence.\n            * extension - the file extension of the sequence.\n            * start - the start frame.\n            * end - the end frame.\n            * length - the length of the frame range.\n            * padding - the detecting amount of padding.\n            * inverted - the inverted frame range. (returns \"\" if none)\n            * dirname - the directory name.\n\n        If asking for the inverted range value, and the new inverted range\n        exceeded :const:`fileseq.constants.MAX_FRAME_SIZE`, a ``MaxSizeException``\n        will be raised.\n\n        Args:\n            template (str):\n\n        Returns:\n            str:\n\n        Raises:\n            :class:`fileseq.exceptions.MaxSizeException`: If frame size exceeds\n            :const:`fileseq.constants.MAX_FRAME_SIZE`\n        \"\"\"\n        # Potentially expensive if inverted range is large\n        # and user never asked for it in template\n        inverted = (self.invertedFrameRange() or \"\") if \"{inverted}\" in template else \"\"\n\n        return template.format(\n            basename=self.basename(),\n            extension=self.extension(), start=self.start(),\n            end=self.end(), length=len(self),\n            padding=self.padding(),\n            range=self.frameRange() or \"\",\n            inverted=inverted,\n            dirname=self.dirname())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split(self):\n        result = []\n        for frange in self.frameRange().split(\",\"):\n            result.append(FileSequence(''.join(\n                (self._dir, self._base, frange, self._pad, self._ext))))\n        return result", "response": "Split the FileSequence into contiguous pieces and return them\n        as a list of FileSequence instances."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the directory name for the sequence.", "response": "def setDirname(self, dirname):\n        \"\"\"\n        Set a new directory name for the sequence.\n\n        Args:\n            dirname (str): the new directory name\n        \"\"\"\n        # Make sure the dirname always ends in\n        # a path separator character\n        sep = utils._getPathSep(dirname)\n        if not dirname.endswith(sep):\n            dirname += sep\n\n        self._dir = utils.asString(dirname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the new padding characters for the sequence.", "response": "def setPadding(self, padding):\n        \"\"\"\n        Set new padding characters for the sequence.\n        i.e. \"#\" or \"@@@\" or '%04d', or an empty string to disable range formatting.\n\n        Args:\n            padding (str): sequence padding to set\n        \"\"\"\n        self._pad = padding\n        self._zfill = self.__class__.getPaddingNum(self._pad)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setExtension(self, ext):\n        if ext[0] != \".\":\n            ext = \".\" + ext\n        self._ext = utils.asString(ext)", "response": "Sets the file extension for the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef frame(self, frame):\n        try:\n            zframe = str(int(frame)).zfill(self._zfill)\n        except ValueError:\n            zframe = frame\n\n        # There may have been no placeholder for frame IDs in\n        # the sequence, in which case we don't want to insert\n        # a frame ID\n\n        if self._zfill == 0:\n            zframe = \"\"\n\n        return \"\".join((self._dir, self._base, zframe, self._ext))", "response": "Return a path go the given frame in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef yield_sequences_in_list(paths):\n        seqs = {}\n        _check = DISK_RE.match\n\n        for match in ifilter(None, imap(_check, imap(utils.asString, paths))):\n            dirname, basename, frame, ext = match.groups()\n            if not basename and not ext:\n                continue\n            key = (dirname, basename, ext)\n            seqs.setdefault(key, set())\n            if frame:\n                seqs[key].add(frame)\n\n        for (dirname, basename, ext), frames in seqs.iteritems():\n            # build the FileSequence behind the scenes, rather than dupe work\n            seq = FileSequence.__new__(FileSequence)\n            seq._dir = dirname or ''\n            seq._base = basename or ''\n            seq._ext = ext or ''\n            if frames:\n                seq._frameSet = FrameSet(set(imap(int, frames))) if frames else None\n                seq._pad = FileSequence.getPaddingChars(min(imap(len, frames)))\n            else:\n                seq._frameSet = None\n                seq._pad = ''\n            seq.__init__(str(seq))\n            yield seq", "response": "Yield the discrete sequences within a list of paths."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the sequences in the given directory or pattern.", "response": "def findSequencesOnDisk(cls, pattern, include_hidden=False, strictPadding=False):\n        \"\"\"\n        Yield the sequences found in the given directory.\n\n        Examples:\n            >>> findSequencesOnDisk('/path/to/files')\n\n        The `pattern` can also specify glob-like shell wildcards including the following:\n            * ``?``         - 1 wildcard character\n            * ``*``         - 1 or more wildcard character\n            * ``{foo,bar}`` - either 'foo' or 'bar'\n\n        Exact frame ranges are not considered, and padding characters are converted to\n        wildcards (``#`` or ``@``)\n\n        Examples:\n            >>> findSequencesOnDisk('/path/to/files/image_stereo_{left,right}.#.jpg')\n            >>> findSequencesOnDisk('/path/to/files/imag?_*_{left,right}.@@@.jpg', strictPadding=True)\n\n        Args:\n            pattern (str): directory to scan, or pattern to filter in directory\n            include_hidden (bool): if true, show .hidden files as well\n            strictPadding (bool): if True, ignore files with padding length different from pattern\n\n        Returns:\n            list:\n        \"\"\"\n        # reserve some functions we're going to need quick access to\n        _not_hidden = lambda f: not f.startswith('.')\n        _match_pattern = None\n        _filter_padding = None\n        _join = os.path.join\n\n        seq = None\n        dirpath = pattern\n\n        # Support the pattern defining a filter for the files\n        # in the existing directory\n        if not os.path.isdir(pattern):\n            dirpath, filepat = os.path.split(pattern)\n\n            if not os.path.isdir(dirpath):\n                return []\n\n            # Start building a regex for filtering files\n            seq = cls(filepat)\n            patt = seq.basename().replace('.', r'\\.')\n            if seq.padding():\n                patt += '\\d+'\n            if seq.extension():\n                patt += seq.extension()\n\n            # Convert braces groups into regex capture groups\n            view = bytearray(patt)\n            matches = re.finditer(r'{(.*?)(?:,(.*?))*}', patt)\n            for match in reversed(list(matches)):\n                i, j = match.span()\n                view[i:j] = '(%s)' % '|'.join([m.strip() for m in match.groups()])\n            view = view.replace('*', '.*')\n            view = view.replace('?', '.')\n            view += '$'\n            try:\n                _match_pattern = re.compile(str(view)).match\n            except re.error:\n                msg = 'Invalid file pattern: {}'.format(filepat)\n                raise FileSeqException(msg)\n\n            if seq.padding() and strictPadding:\n                _filter_padding = functools.partial(cls._filterByPaddingNum, num=seq.zfill())\n\n        # Get just the immediate files under the dir.\n        # Avoids testing the os.listdir() for files as\n        # a second step.\n        ret = next(os.walk(dirpath), None)\n        files = ret[-1] if ret else []\n\n        # collapse some generators to get us the files that match our regex\n        if not include_hidden:\n            files = ifilter(_not_hidden, files)\n\n        # Filter by files that match the provided file pattern\n        if _match_pattern:\n            files = ifilter(_match_pattern, files)\n\n        # Filter by files that match the frame padding in the file pattern\n        if _filter_padding:\n            # returns a generator\n            files = _filter_padding(files)\n\n        # Ensure our dirpath ends with a path separator, so\n        # that we can control which sep is used during the\n        # os.path.join\n        sep = utils._getPathSep(dirpath)\n        if not dirpath.endswith(sep):\n            dirpath += sep\n\n        files = (_join(dirpath, f) for f in files)\n        files = list(files)\n\n        seqs = list(FileSequence.yield_sequences_in_list(files))\n\n        if _filter_padding and seq:\n            pad = cls.conformPadding(seq.padding())\n            # strict padding should preserve the original padding\n            # characters in the found sequences.\n            for s in seqs:\n                s.setPadding(pad)\n\n        return seqs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for a specific sequence on disk.", "response": "def findSequenceOnDisk(cls, pattern, strictPadding=False):\n        \"\"\"\n        Search for a specific sequence on disk.\n\n        The padding characters used in the `pattern` are used to filter the\n        frame values of the files on disk (if `strictPadding` is True).\n\n        Examples:\n            Find sequence matching basename and extension, and a wildcard for\n            any frame.\n            returns bar.1.exr bar.10.exr, bar.100.exr, bar.1000.exr, inclusive\n\n            >>> findSequenceOnDisk(\"seq/bar@@@@.exr\")\n\n            Find exactly 4-padded sequence, i.e. seq/bar1-100#.exr\n            returns only frames bar1000.exr through bar9999.exr\n\n            >>> findSequenceOnDisk(\"seq/bar#.exr\", strictPadding=True)\n\n        Args:\n            pattern (str): the sequence pattern being searched for\n            strictPadding (bool): if True, ignore files with padding length different from `pattern`\n\n        Returns:\n            str:\n\n        Raises:\n            :class:`.FileSeqException`: if no sequence is found on disk\n        \"\"\"\n        seq = cls(pattern)\n\n        if seq.frameRange() == '' and seq.padding() == '':\n            if os.path.isfile(pattern):\n                return seq\n\n        patt = seq.format('{dirname}{basename}*{extension}')\n\n        ext = seq.extension()\n        basename = seq.basename()\n        pad = seq.padding()\n\n        globbed = iglob(patt)\n        if pad and strictPadding:\n            globbed = cls._filterByPaddingNum(globbed, seq.zfill())\n            pad = cls.conformPadding(pad)\n\n        matches = cls.yield_sequences_in_list(globbed)\n        for match in matches:\n            if match.basename() == basename and match.extension() == ext:\n                if pad and strictPadding:\n                    match.setPadding(pad)\n                return match\n\n        msg = 'no sequence found on disk matching {0}'\n        raise FileSeqException(msg.format(pattern))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield only path elements from iterable which have a frame number that matches the given target padding number .", "response": "def _filterByPaddingNum(cls, iterable, num):\n        \"\"\"\n        Yield only path elements from iterable which have a frame\n        padding that matches the given target padding number\n\n        Args:\n            iterable (collections.Iterable):\n            num (int):\n\n        Yields:\n            str:\n        \"\"\"\n        _check = DISK_RE.match\n\n        for item in iterable:\n            # Add a filter for paths that don't match the frame\n            # padding of a given number\n            matches = _check(item)\n            if not matches:\n                if num <= 0:\n                    # Not a sequence pattern, but we were asked\n                    # to match on a zero padding\n                    yield item\n\n                continue\n\n            frame = matches.group(3) or ''\n\n            if not frame:\n                if num <= 0:\n                    # No frame value was parsed, but we were asked\n                    # to match on a zero padding\n                    yield item\n                continue\n\n            # We have a frame number\n\n            if frame[0] == '0' or frame[:2] == '-0':\n                if len(frame) == num:\n                    # A frame leading with '0' is explicitly\n                    # padded and can only be a match if its exactly\n                    # the target padding number\n                    yield item\n                continue\n\n            if len(frame) >= num:\n                # A frame that does not lead with '0' can match\n                # a padding width >= to the target padding number\n                yield item\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a supported group of padding characters return the amount of padding.", "response": "def getPaddingNum(chars):\n        \"\"\"\n        Given a supported group of padding characters, return the amount of padding.\n\n        Args:\n            chars (str): a supported group of padding characters\n\n        Returns:\n            int:\n\n        Raises:\n            ValueError: if unsupported padding character is detected\n        \"\"\"\n        match = PRINTF_SYNTAX_PADDING_RE.match(chars)\n        if match:\n            return int(match.group(1))\n\n        try:\n            return sum([PAD_MAP[char] for char in chars])\n        except KeyError:\n            msg = \"Detected an unsupported padding character: \\\"{}\\\".\"\n            msg += \" Supported padding characters: {} or printf syntax padding\"\n            msg += \" %<int>d\"\n            raise ValueError(msg.format(char, str(PAD_MAP.keys())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring alternate input padding formats are conformed to the format defined in PAD_MAP.", "response": "def conformPadding(cls, chars):\n        \"\"\"\n        Ensure alternate input padding formats are conformed\n        to formats defined in PAD_MAP\n\n        If chars is already a format defined in PAD_MAP, then\n        it is returned unmodified.\n\n        Example::\n            '#'    -> '#'\n            '@@@@' -> '@@@@'\n            '%04d' -> '#'\n\n        Args:\n            chars (str): input padding chars\n\n        Returns:\n            str: conformed padding chars\n\n        Raises:\n            ValueError: If chars contains invalid padding characters\n        \"\"\"\n        pad = chars\n        if pad and pad[0] not in PAD_MAP:\n            pad = cls.getPaddingChars(cls.getPaddingNum(pad))\n        return pad"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_iterable(cls, frames, sort=False):\n        return FrameSet(sorted(frames) if sort else frames)", "response": "Build a : class : FrameSet from an iterable containing frames as integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cast_to_frameset(cls, other):\n        if isinstance(other, FrameSet):\n            return other\n        try:\n            return FrameSet(other)\n        except Exception:\n            return NotImplemented", "response": "Private method to simplify comparison operations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invertedFrameRange(self, zfill=0):\n        result = []\n        frames = sorted(self.items)\n        for idx, frame in enumerate(frames[:-1]):\n            next_frame = frames[idx + 1]\n            if next_frame - frame != 1:\n                r = xrange(frame + 1, next_frame)\n                # Check if the next update to the result set\n                # will exceed out max frame size.\n                # Prevent memory overflows.\n                self._maxSizeCheck(len(r) + len(result))\n                result += r\n\n        if not result:\n            return ''\n\n        return FrameSet.framesToFrameRange(\n            result, zfill=zfill, sort=False, compress=False)", "response": "Returns the inverse of the FrameSet s frame range padded if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new normalized set with the items in the current set.", "response": "def normalize(self):\n        \"\"\"\n        Returns a new normalized (sorted and compacted) :class:`FrameSet`.\n\n        Returns:\n            :class:`FrameSet`:\n        \"\"\"\n        return FrameSet(FrameSet.framesToFrameRange(\n            self.items, sort=True, compress=False))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isdisjoint(self, other):\n        other = self._cast_to_frameset(other)\n        if other is NotImplemented:\n            return NotImplemented\n        return self.items.isdisjoint(other.items)", "response": "Check if the contents of self has no common intersection with the contents of other."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef issubset(self, other):\n        other = self._cast_to_frameset(other)\n        if other is NotImplemented:\n            return NotImplemented\n        return self.items <= other.items", "response": "Check if the contents of self is a subset of the contents of other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef issuperset(self, other):\n        other = self._cast_to_frameset(other)\n        if other is NotImplemented:\n            return NotImplemented\n        return self.items >= other.items", "response": "Check if the contents of self is a superset of the contents of other."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new set with elements in self but not in other.", "response": "def difference(self, *other):\n        \"\"\"\n        Returns a new :class:`FrameSet` with elements in `self` but not in\n        `other`.\n\n        Args:\n            other (:class:`FrameSet`): or objects that can cast to :class:`FrameSet`\n\n        Returns:\n            :class:`FrameSet`:\n        \"\"\"\n        from_frozenset = self.items.difference(*map(set, other))\n        return self.from_iterable(from_frozenset, sort=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef symmetric_difference(self, other):\n        other = self._cast_to_frameset(other)\n        if other is NotImplemented:\n            return NotImplemented\n        from_frozenset = self.items.symmetric_difference(other.items)\n        return self.from_iterable(from_frozenset, sort=True)", "response": "Returns a new set with all the elements in either set or other but not both."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _maxSizeCheck(cls, obj):\n        fail = False\n        size = 0\n\n        if isinstance(obj, numbers.Number):\n            if obj > constants.MAX_FRAME_SIZE:\n                fail = True\n                size = obj\n\n        elif hasattr(obj, '__len__'):\n            size = len(obj)\n            fail = size > constants.MAX_FRAME_SIZE\n\n        if fail:\n            raise MaxSizeException('Frame size %s > %s (MAX_FRAME_SIZE)' \\\n                    % (size, constants.MAX_FRAME_SIZE))", "response": "Check if the object is too large."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the given string is a frame range.", "response": "def isFrameRange(frange):\n        \"\"\"\n        Return True if the given string is a frame range. Any padding\n        characters, such as '#' and '@' are ignored.\n\n        Args:\n            frange (str): a frame range to test\n\n        Returns:\n            bool:\n        \"\"\"\n        # we're willing to trim padding characters from consideration\n        # this translation is orders of magnitude faster than prior method\n        frange = str(frange).translate(None, ''.join(PAD_MAP.keys()))\n        if not frange:\n            return True\n        for part in frange.split(','):\n            if not part:\n                continue\n            try:\n                FrameSet._parse_frange_part(part)\n            except ParseException:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a zero - padded version of the frame range string.", "response": "def padFrameRange(frange, zfill):\n        \"\"\"\n        Return the zero-padded version of the frame range string.\n\n        Args:\n            frange (str): a frame range to test\n            zfill (int):\n\n        Returns:\n            str:\n        \"\"\"\n        def _do_pad(match):\n            \"\"\"\n            Substitutes padded for unpadded frames.\n            \"\"\"\n            result = list(match.groups())\n            result[1] = pad(result[1], zfill)\n            if result[4]:\n                result[4] = pad(result[4], zfill)\n            return ''.join((i for i in result if i))\n        return PAD_RE.sub(_do_pad, frange)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef framesToFrameRanges(frames, zfill=0):\n        _build = FrameSet._build_frange_part\n        curr_start = None\n        curr_stride = None\n        curr_frame = None\n        last_frame = None\n        curr_count = 0\n        for curr_frame in frames:\n            if curr_start is None:\n                curr_start = curr_frame\n                last_frame = curr_frame\n                curr_count += 1\n                continue\n            if curr_stride is None:\n                curr_stride = abs(curr_frame-curr_start)\n            new_stride = abs(curr_frame-last_frame)\n            if curr_stride == new_stride:\n                last_frame = curr_frame\n                curr_count += 1\n            elif curr_count == 2 and curr_stride != 1:\n                yield _build(curr_start, curr_start, None, zfill)\n                curr_start = last_frame\n                curr_stride = new_stride\n                last_frame = curr_frame\n            else:\n                yield _build(curr_start, last_frame, curr_stride, zfill)\n                curr_stride = None\n                curr_start = curr_frame\n                last_frame = curr_frame\n                curr_count = 1\n        if curr_count == 2 and curr_stride != 1:\n            yield _build(curr_start, curr_start, None, zfill)\n            yield _build(curr_frame, curr_frame, None, zfill)\n        else:\n            yield _build(curr_start, curr_frame, curr_stride, zfill)", "response": "A generator function that takes a sequence of frames and returns a series of padded unicode strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef framesToFrameRange(frames, sort=True, zfill=0, compress=False):\n        if compress:\n            frames = unique(set(), frames)\n        frames = list(frames)\n        if not frames:\n            return ''\n        if len(frames) == 1:\n            return pad(frames[0], zfill)\n        if sort:\n            frames.sort()\n        return ','.join(FrameSet.framesToFrameRanges(frames, zfill))", "response": "Converts an iterator of frames into a sequence of frame ranges."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses and sets optional OAuth parameters on a request.", "response": "def _parse_optional_params(self, oauth_params, req_kwargs):\n        '''\n        Parses and sets optional OAuth parameters on a request.\n\n        :param oauth_param: The OAuth parameter to parse.\n        :type oauth_param: str\n        :param req_kwargs: The keyworded arguments passed to the request\n            method.\n        :type req_kwargs: dict\n        '''\n        params = req_kwargs.get('params', {})\n        data = req_kwargs.get('data') or {}\n\n        for oauth_param in OPTIONAL_OAUTH_PARAMS:\n            if oauth_param in params:\n                oauth_params[oauth_param] = params.pop(oauth_param)\n            if oauth_param in data:\n                oauth_params[oauth_param] = data.pop(oauth_param)\n\n            if params:\n                req_kwargs['params'] = params\n\n            if data:\n                req_kwargs['data'] = data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_oauth_params(self, req_kwargs):\n        '''Prepares OAuth params for signing.'''\n        oauth_params = {}\n\n        oauth_params['oauth_consumer_key'] = self.consumer_key\n        oauth_params['oauth_nonce'] = sha1(\n            str(random()).encode('ascii')).hexdigest()\n        oauth_params['oauth_signature_method'] = self.signature.NAME\n        oauth_params['oauth_timestamp'] = int(time())\n\n        if self.access_token is not None:\n            oauth_params['oauth_token'] = self.access_token\n\n        oauth_params['oauth_version'] = self.VERSION\n\n        self._parse_optional_params(oauth_params, req_kwargs)\n\n        return oauth_params", "response": "Prepares OAuth params for signing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, method, url, bearer_auth=True, **req_kwargs):\n        '''\n        A loose wrapper around Requests' :class:`~requests.sessions.Session`\n        which injects OAuth 2.0 parameters.\n\n        :param method: A string representation of the HTTP method to be used.\n        :type method: str\n        :param url: The resource to be requested.\n        :type url: str\n        :param bearer_auth: Whether to use Bearer Authentication or not,\n            defaults to `True`.\n        :type bearer_auth: bool\n        :param \\*\\*req_kwargs: Keyworded args to be passed down to Requests.\n        :type \\*\\*req_kwargs: dict\n        '''\n        req_kwargs.setdefault('params', {})\n\n        url = self._set_url(url)\n\n        if is_basestring(req_kwargs['params']):\n            req_kwargs['params'] = dict(parse_qsl(req_kwargs['params']))\n\n        if bearer_auth and self.access_token is not None:\n            req_kwargs['auth'] = OAuth2Auth(self.access_token)\n        else:\n            req_kwargs['params'].update({self.access_token_key:\n                                         self.access_token})\n\n        req_kwargs.setdefault('timeout', OAUTH2_DEFAULT_TIMEOUT)\n\n        return super(OAuth2Session, self).request(method, url, **req_kwargs)", "response": "A loose wrapper around Requests. Session. request which injects OAuth 2. 0 parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self,\n                method,\n                url,\n                user_id=None,\n                hash_meth='sha1',\n                **req_kwargs):\n        '''\n        A loose wrapper around Requests' :class:`~requests.sessions.Session`\n        which injects Ofly parameters.\n\n        :param method: A string representation of the HTTP method to be used.\n        :type method: str\n        :param url: The resource to be requested.\n        :type url: str\n        :param hash_meth: The hash method to use for signing, defaults to\n            \"sha1\".\n        :type hash_meth: str\n        :param user_id: The oflyUserid, defaults to `None`.\n        :type user_id: str\n        :param \\*\\*req_kwargs: Keyworded args to be passed down to Requests.\n        :type \\*\\*req_kwargs: dict\n        '''\n        req_kwargs.setdefault('params', {})\n        req_kwargs.setdefault('timeout', OFLY_DEFAULT_TIMEOUT)\n\n        url = self._set_url(url)\n\n        user_id = user_id or self.user_id\n        assert user_id is not None, \\\n            'An oflyUserid must be provided as `user_id`.'\n\n        if is_basestring(req_kwargs['params']):\n            req_kwargs['params'] = dict(parse_qsl(req_kwargs['params']))\n\n        req_kwargs['params'].update({'oflyUserid': user_id})\n\n        params = OflySession.sign(url,\n                                  self.app_id,\n                                  self.app_secret,\n                                  hash_meth=hash_meth,\n                                  **req_kwargs['params'])\n\n        # NOTE: Requests can't seem to handle unicode objects, instead we can\n        # encode a string here.\n        req_kwargs['params'] = params\n        if not isinstance(req_kwargs['params'], bytes):\n            req_kwargs['params'] = req_kwargs['params'].encode('utf-8')\n\n        return super(OflySession, self).request(method, url, **req_kwargs)", "response": "A loose wrapper around Requests which uses the ofly API to sign the request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the signature for the given url.", "response": "def sign(url, app_id, app_secret, hash_meth='sha1', **params):\n        '''\n        A signature method which generates the necessary Ofly parameters.\n\n        :param app_id: The oFlyAppId, i.e. \"application ID\".\n        :type app_id: str\n        :param app_secret: The oFlyAppSecret, i.e. \"shared secret\".\n        :type app_secret: str\n        :param hash_meth: The hash method to use for signing, defaults to\n            \"sha1\".\n        :type hash_meth: str\n        :param \\*\\*params: Additional parameters.\n        :type \\*\\*\\params: dict\n        '''\n        hash_meth_str = hash_meth\n        if hash_meth == 'sha1':\n            hash_meth = sha1\n        elif hash_meth == 'md5':\n            hash_meth = md5\n        else:\n            raise TypeError('hash_meth must be one of \"sha1\", \"md5\"')\n\n        now = datetime.utcnow()\n        milliseconds = now.microsecond // 1000\n\n        time_format = '%Y-%m-%dT%H:%M:%S.{0}Z'.format(milliseconds)\n        ofly_params = {'oflyAppId': app_id,\n                       'oflyHashMeth': hash_meth_str.upper(),\n                       'oflyTimestamp': now.strftime(time_format)}\n\n        url_path = urlsplit(url).path\n\n        signature_base_string = app_secret + url_path + '?'\n        if len(params):\n            signature_base_string += get_sorted_params(params) + '&'\n        signature_base_string += get_sorted_params(ofly_params)\n\n        if not isinstance(signature_base_string, bytes):\n            signature_base_string = signature_base_string.encode('utf-8')\n\n        ofly_params['oflyApiSig'] = \\\n            hash_meth(signature_base_string).hexdigest()\n\n        all_params = dict(tuple(ofly_params.items()) + tuple(params.items()))\n\n        return get_sorted_params(all_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs and returns an authentication header.", "response": "def _get_auth_header(self):\n        ''' Constructs and returns an authentication header. '''\n        realm = 'realm=\"{realm}\"'.format(realm=self.realm)\n        params = ['{k}=\"{v}\"'.format(k=k, v=quote(str(v), safe=''))\n                  for k, v in self.oauth_params.items()]\n        return 'OAuth ' + ','.join([realm] + params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _remove_qs(self, url):\n        '''\n        Removes a query string from a URL before signing.\n\n        :param url: The URL to strip.\n        :type url: str\n        '''\n        scheme, netloc, path, query, fragment = urlsplit(url)\n\n        return urlunsplit((scheme, netloc, path, '', fragment))", "response": "Removes a query string from a URL before signing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_request_parameters(self, oauth_params, req_kwargs):\n        '''\n        This process normalizes the request parameters as detailed in the OAuth\n        1.0 spec.\n\n        Additionally we apply a `Content-Type` header to the request of the\n        `FORM_URLENCODE` type if the `Content-Type` was previously set, i.e. if\n        this is a `POST` or `PUT` request. This ensures the correct header is\n        set as per spec.\n\n        Finally we sort the parameters in preparation for signing and return\n        a URL encoded string of all normalized parameters.\n\n        :param oauth_params: OAuth params to sign with.\n        :type oauth_params: dict\n        :param req_kwargs: Request kwargs to normalize.\n        :type req_kwargs: dict\n        '''\n        normalized = []\n\n        params = req_kwargs.get('params', {})\n        data = req_kwargs.get('data', {})\n        headers = req_kwargs.get('headers', {})\n\n        # process request parameters\n        for k, v in params.items():\n            if v is not None:\n                normalized += [(k, v)]\n\n        # process request data\n        if 'Content-Type' in headers and \\\n                headers['Content-Type'] == FORM_URLENCODED:\n            for k, v in data.items():\n                normalized += [(k, v)]\n\n        # extract values from our list of tuples\n        all_normalized = []\n        for t in normalized:\n            k, v = t\n            if is_basestring(v) and not isinstance(v, bytes):\n                v = v.encode('utf-8')\n            all_normalized += [(k, v)]\n\n        # add in the params from oauth_params for signing\n        for k, v in oauth_params.items():\n            if (k, v) in all_normalized:  # pragma: no cover\n                continue\n            all_normalized += [(k, v)]\n\n        # sort the params as per the OAuth 1.0/a spec\n        all_normalized.sort()\n\n        # finally encode the params as a string\n        return urlencode(all_normalized, True)\\\n            .replace('+', '%20')\\\n            .replace('%7E', '~')", "response": "This function normalizes the request parameters as detailed in OAuth 1. 0 spec and returns a URL encoded string of all parameters that are set in the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign(self,\n             consumer_secret,\n             access_token_secret,\n             method,\n             url,\n             oauth_params,\n             req_kwargs):\n        '''Sign request parameters.\n\n        :param consumer_secret: Consumer secret.\n        :type consumer_secret: str\n        :param access_token_secret: Access token secret.\n        :type access_token_secret: str\n        :param method: The method of this particular request.\n        :type method: str\n        :param url: The URL of this particular request.\n        :type url: str\n        :param oauth_params: OAuth parameters.\n        :type oauth_params: dict\n        :param req_kwargs: Keyworded args that will be sent to the request\n            method.\n        :type req_kwargs: dict\n        '''\n        url = self._remove_qs(url)\n\n        oauth_params = \\\n            self._normalize_request_parameters(oauth_params, req_kwargs)\n        parameters = map(self._escape, [method, url, oauth_params])\n\n        key = self._escape(consumer_secret) + b'&'\n        if access_token_secret is not None:\n            key += self._escape(access_token_secret)\n\n        # build a Signature Base String\n        signature_base_string = b'&'.join(parameters)\n\n        # hash the string with HMAC-SHA1\n        hashed = hmac.new(key, signature_base_string, sha1)\n\n        # return the signature\n        return base64.b64encode(hashed.digest()).decode()", "response": "Signs the request parameters and returns the signature base64 - encoded base64 - encoded signature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsign the request parameters and returns the signature base64 - encoded string.", "response": "def sign(self,\n             consumer_secret,\n             access_token_secret,\n             method,\n             url,\n             oauth_params,\n             req_kwargs):\n        '''Sign request parameters.\n\n        :param consumer_secret: RSA private key.\n        :type consumer_secret: str or RSA._RSAobj\n        :param access_token_secret: Unused.\n        :type access_token_secret: str\n        :param method: The method of this particular request.\n        :type method: str\n        :param url: The URL of this particular request.\n        :type url: str\n        :param oauth_params: OAuth parameters.\n        :type oauth_params: dict\n        :param req_kwargs: Keyworded args that will be sent to the request\n            method.\n        :type req_kwargs: dict\n        '''\n        url = self._remove_qs(url)\n\n        oauth_params = \\\n            self._normalize_request_parameters(oauth_params, req_kwargs)\n        parameters = map(self._escape, [method, url, oauth_params])\n\n        # build a Signature Base String\n        signature_base_string = b'&'.join(parameters)\n\n        # resolve the key\n        if is_basestring(consumer_secret):\n            consumer_secret = self.RSA.importKey(consumer_secret)\n        if not isinstance(consumer_secret, self.RSA._RSAobj):\n            raise ValueError('invalid consumer_secret')\n\n        # hash the string with RSA-SHA1\n        s = self.PKCS1_v1_5.new(consumer_secret)\n        # PyCrypto SHA.new requires an encoded byte string\n        h = self.SHA.new(signature_base_string)\n        hashed = s.sign(h)\n\n        # return the signature\n        return base64.b64encode(hashed).decode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsigns request using PLAINTEXT method.", "response": "def sign(self, consumer_secret, access_token_secret, method, url,\n             oauth_params, req_kwargs):\n        '''Sign request using PLAINTEXT method.\n\n        :param consumer_secret: Consumer secret.\n        :type consumer_secret: str\n        :param access_token_secret: Access token secret (optional).\n        :type access_token_secret: str\n        :param method: Unused\n        :type method: str\n        :param url: Unused\n        :type url: str\n        :param oauth_params: Unused\n        :type oauth_params: dict\n        :param req_kwargs: Unused\n        :type req_kwargs: dict\n        '''\n        key = self._escape(consumer_secret) + b'&'\n        if access_token_secret:\n            key += self._escape(access_token_secret)\n        return key.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_session(self, token=None, signature=None):\n        '''\n        If provided a `token` parameter, tries to retrieve a stored\n        `rauth.OAuth1Session` instance. Otherwise generates a new session\n        instance with the :class:`rauth.OAuth1Service.consumer_key` and\n        :class:`rauth.OAuth1Service.consumer_secret` stored on the\n        `rauth.OAuth1Service` instance.\n\n        :param token: A tuple of strings with which to memoize the session\n            object instance.\n        :type token: tuple\n        '''\n        if token is not None:\n            access_token, access_token_secret = token\n            session = self.session_obj(self.consumer_key,\n                                       self.consumer_secret,\n                                       access_token,\n                                       access_token_secret,\n                                       signature or self.signature_obj,\n                                       service=self)\n        else:  # pragma: no cover\n            signature = signature or self.signature_obj\n            session = self.session_obj(self.consumer_key,\n                                       self.consumer_secret,\n                                       signature=signature,\n                                       service=self)\n        return session", "response": "Gets a session object for the given token and signature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Requests response over the request_token_url and returns the response object.", "response": "def get_raw_request_token(self, method='GET', **kwargs):\n        '''\n        Returns a Requests' response over the\n        :attr:`rauth.OAuth1Service.request_token_url`.\n\n        Use this if your endpoint if you need the full `Response` object.\n\n        :param method: A string representation of the HTTP method to be used,\n            defaults to `GET`.\n        :type method: str\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        # ensure we've set the request_token_url\n        if self.request_token_url is None:\n            raise TypeError('request_token_url must not be None')\n\n        session = self.get_session()\n        self.request_token_response = session.request(method,\n                                                      self.request_token_url,\n                                                      **kwargs)\n        return self.request_token_response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a request token pair.", "response": "def get_request_token(self,\n                          method='GET',\n                          decoder=parse_utf8_qsl,\n                          key_token='oauth_token',\n                          key_token_secret='oauth_token_secret',\n                          **kwargs):\n        '''\n        Return a request token pair.\n\n        :param method: A string representation of the HTTP method to be used,\n            defaults to `GET`.\n        :type method: str\n        :param decoder: A function used to parse the Response content. Should\n            return a dictionary.\n        :type decoder: func\n        :param key_token: The key the access token will be decoded by, defaults\n            to 'oauth_token'.\n        :type string:\n        :param key_token_secret: The key the access token will be decoded by,\n            defaults to 'oauth_token_secret'.\n        :type string:\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        r = self.get_raw_request_token(method=method, **kwargs)\n        request_token, request_token_secret = \\\n            process_token_request(r, decoder, key_token, key_token_secret)\n        return request_token, request_token_secret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_authorize_url(self, request_token, **params):\n        '''\n        Returns a formatted authorize URL.\n\n        :param request_token: The request token as returned by\n            :class:`get_request_token`.\n        :type request_token: str\n        :param \\*\\*params: Additional keyworded arguments to be added to the\n            request querystring.\n        :type \\*\\*params: dict\n        '''\n        params.update({'oauth_token': request_token})\n        return self.authorize_url + '?' + urlencode(params)", "response": "Returns a formatted authorize URL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_raw_access_token(self,\n                             request_token,\n                             request_token_secret,\n                             method='GET',\n                             **kwargs):\n        '''\n        Returns a Requests' response over the\n        :attr:`rauth.OAuth1Service.access_token_url`.\n\n        Use this if your endpoint if you need the full `Response` object.\n\n        :param request_token: The request token as returned by\n            :meth:`get_request_token`.\n        :type request_token: str\n        :param request_token_secret: The request token secret as returned by\n            :meth:`get_request_token`.\n        :type request_token_secret: str\n        :param method: A string representation of the HTTP method to be\n            used, defaults to `GET`.\n        :type method: str\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        # ensure we've set the access_token_url\n        if self.access_token_url is None:\n            raise TypeError('access_token_url must not be None')\n\n        session = self.get_session((request_token, request_token_secret))\n        self.access_token_response = session.request(method,\n                                                     self.access_token_url,\n                                                     **kwargs)\n        return self.access_token_response", "response": "Returns a Requests response over the access_token_url and the request_token_secret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_access_token(self,\n                         request_token,\n                         request_token_secret,\n                         method='GET',\n                         decoder=parse_utf8_qsl,\n                         key_token='oauth_token',\n                         key_token_secret='oauth_token_secret',\n                         **kwargs):\n        '''\n        Returns an access token pair.\n\n        :param request_token: The request token as returned by\n            :meth:`get_request_token`.\n        :type request_token: str\n        :param request_token_secret: The request token secret as returned by\n            :meth:`get_request_token`.\n        :type request_token_secret: str\n        :param method: A string representation of the HTTP method to be\n            used, defaults to `GET`.\n        :type method: str\n        :param decoder: A function used to parse the Response content. Should\n            return a dictionary.\n        :type decoder: func\n        :param key_token: The key the access token will be decoded by, defaults\n            to 'oauth_token'.\n        :type string:\n        :param key_token_secret: The key the access token will be decoded by,\n            defaults to 'oauth_token_secret'.\n        :type string:\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        r = self.get_raw_access_token(request_token,\n                                      request_token_secret,\n                                      method=method,\n                                      **kwargs)\n\n        access_token, access_token_secret = \\\n            process_token_request(r, decoder, key_token, key_token_secret)\n        return access_token, access_token_secret", "response": "Get an access token pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an authenticated session with the specified request token and secret.", "response": "def get_auth_session(self,\n                         request_token,\n                         request_token_secret,\n                         method='GET',\n                         **kwargs):\n        '''\n        Gets an access token, intializes a new authenticated session with the\n        access token. Returns an instance of :attr:`session_obj`.\n\n        :param request_token: The request token as returned by\n            :meth:`get_request_token`.\n        :type request_token: str\n        :param request_token_secret: The request token secret as returned by\n            :meth:`get_request_token`.\n        :type request_token_secret: str\n        :param method: A string representation of the HTTP method to be\n            used, defaults to `GET`.\n        :type method: str\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        token = self.get_access_token(request_token,\n                                      request_token_secret,\n                                      method=method,\n                                      **kwargs)\n        session = self.get_session(token)\n\n        if self.request_token_response:\n            session.request_token_response = self.request_token_response\n        if self.access_token_response:\n            session.access_token_response = self.access_token_response\n\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_session(self, token=None):\n        '''\n        If provided, the `token` parameter is used to initialize an\n        authenticated session, otherwise an unauthenticated session object is\n        generated. Returns an instance of :attr:`session_obj`..\n\n        :param token: A token with which to initilize the session.\n        :type token: str\n        '''\n        if token is not None:\n            session = self.session_obj(self.client_id,\n                                       self.client_secret,\n                                       token,\n                                       service=self)\n        else:  # pragma: no cover\n            session = self.session_obj(self.client_id,\n                                       self.client_secret,\n                                       service=self)\n        return session", "response": "Returns an authenticated or unauthenticated session object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_authorize_url(self, **params):\n        '''\n        Returns a formatted authorize URL.\n\n        :param \\*\\*params: Additional keyworded arguments to be added to the\n            URL querystring.\n        :type \\*\\*params: dict\n        '''\n\n        params.update({'client_id': self.client_id})\n        return self.authorize_url + '?' + urlencode(params)", "response": "Returns a formatted authorize URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_raw_access_token(self, method='POST', **kwargs):\n        '''\n        Returns a Requests' response over the\n        :attr:`OAuth2Service.access_token_url`.\n\n        Use this if your endpoint if you need the full `Response` object.\n\n        :param method: A string representation of the HTTP method to be used,\n            defaults to `POST`.\n        :type method: str\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        key = 'params'\n        if method in ENTITY_METHODS:\n            key = 'data'\n\n        kwargs.setdefault(key, {})\n        kwargs[key].update({'client_id': self.client_id,\n                            'client_secret': self.client_secret})\n\n        session = self.get_session()\n        self.access_token_response = session.request(method,\n                                                     self.access_token_url,\n                                                     **kwargs)\n        return self.access_token_response", "response": "Returns a Requests response over the OAuth2Service. access_token_url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an access token.", "response": "def get_access_token(self,\n                         method='POST',\n                         decoder=parse_utf8_qsl,\n                         key='access_token',\n                         **kwargs):\n        '''\n        Returns an access token.\n\n        :param method: A string representation of the HTTP method to be used,\n            defaults to `POST`.\n        :type method: str\n        :param decoder: A function used to parse the Response content. Should\n            return a dictionary.\n        :type decoder: func\n        :param key: The key the access token will be decoded by, defaults to\n            'access_token'.\n        :type string:\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        r = self.get_raw_access_token(method, **kwargs)\n        access_token, = process_token_request(r, decoder, key)\n        return access_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an access token intializes a new authenticated session with the access token. Returns an instance of Session.", "response": "def get_auth_session(self, method='POST', **kwargs):\n        '''\n        Gets an access token, intializes a new authenticated session with the\n        access token. Returns an instance of :attr:`session_obj`.\n\n        :param method: A string representation of the HTTP method to be used,\n            defaults to `POST`.\n        :type method: str\n        :param \\*\\*kwargs: Optional arguments. Same as Requests.\n        :type \\*\\*kwargs: dict\n        '''\n        session = self.get_session(self.get_access_token(method, **kwargs))\n\n        if self.access_token_response:\n            session.access_token_response = self.access_token_response\n\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an authenticated session instance.", "response": "def get_session(self, token):\n        '''\n        The token parameter should be `oFlyUserid`. This is used to initialize\n        an authenticated session instance. Returns an instance of\n        :attr:`session_obj`.\n\n        :param token: A token with which to initialize the session with, e.g.\n            :attr:`OflyService.user_id`.\n        :type token: str\n        '''\n        return self.session_obj(self.app_id,\n                                self.app_secret,\n                                token,\n                                service=self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_authorize_url(self, **params):\n        '''\n        Returns a formatted authorize URL.\n\n        :param \\*\\*params: Additional keyworded arguments to be added to the\n            request querystring.\n        :type \\*\\*params: dict\n        '''\n        params = self.session_obj.sign(self.authorize_url,\n                                       self.app_id,\n                                       self.app_secret,\n                                       **params)\n        return self.authorize_url + '?' + params", "response": "Returns a formatted authorize URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbind a topic exchange to a queue", "response": "def bind_topic_exchange(self, exchange_name, routing_key, queue_name):\n        \"\"\"\n        \u7ed1\u5b9a\u4e3b\u9898\u4ea4\u6362\u673a\u548c\u961f\u5217\n        :param exchange_name: \u9700\u8981\u7ed1\u5b9a\u7684\u4ea4\u6362\u673a\u540d\n        :param routing_key:\n        :param queue_name: \u9700\u8981\u7ed1\u5b9a\u7684\u4ea4\u6362\u673a\u961f\u5217\u540d\n        :return:\n        \"\"\"\n        self._channel.queue_declare(\n            queue=queue_name,\n            auto_delete=True,\n            durable=True,\n        )\n        self._channel.exchange_declare(\n            exchange=exchange_name,\n            exchange_type='topic',\n            auto_delete=True,\n        )\n        self._channel.queue_bind(\n            exchange=exchange_name,\n            queue=queue_name,\n            routing_key=routing_key\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeclare exchange and bind queue to queue", "response": "def exchange_bind_to_queue(self, type, exchange_name, routing_key, queue):\n        \"\"\"\n        Declare exchange and bind queue to exchange\n        :param type: The type of exchange\n        :param exchange_name: The name of exchange\n        :param routing_key: The key of exchange bind to queue\n        :param queue: queue name\n        \"\"\"\n        self._channel.exchange_declare(exchange=exchange_name,\n                                       exchange_type=type)\n        self._channel.queue_bind(queue=queue,\n                                 exchange=exchange_name,\n                                 routing_key=routing_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_response(self, ch, method, props, body):\n        logger.info(\"on response => {}\".format(body))\n\n        corr_id = props.correlation_id  # \u4eceprops\u5f97\u5230\u670d\u52a1\u7aef\u8fd4\u56de\u7684\u5ba2\u6237\u7aef\u4f20\u5165\u7684corr_id\u503c\n        self.accept(corr_id, body)", "response": "Called when a response is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef declare_default_consuming(self, queue_name, callback, passive=False,\n                                  durable=False,exclusive=False, auto_delete=False,\n                                  arguments=None):\n        \"\"\"\n        \u58f0\u660e\u4e00\u4e2a\u9ed8\u8ba4\u7684\u4ea4\u6362\u673a\u7684\u961f\u5217\uff0c\u5e76\u4e14\u76d1\u542c\u8fd9\u4e2a\u961f\u5217\n        :param queue_name:\n        :param callback:\n        :return:\n        \"\"\"\n        result = self.declare_queue(\n            queue_name=queue_name,passive=passive,\n            durable=durable,exclusive=exclusive,\n            auto_delete=auto_delete,arguments=arguments\n        )\n        self.declare_basic_consuming(\n            queue_name=queue_name,\n            callback=callback\n        )\n        return result", "response": "declare a queue and basic consuming"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef declare_consuming(self, exchange_name, routing_key, queue_name, callback):\n        self.bind_topic_exchange(exchange_name, routing_key, queue_name)\n        self.declare_basic_consuming(\n            queue_name=queue_name,\n            callback=callback\n        )", "response": "declare_consuming - Declare consumers for a topic exchange"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_sync(self, body, exchange, key):\n        callback_queue = self.declare_queue(exclusive=True,\n                                            auto_delete=True)  # \u5f97\u5230\u968f\u673a\u56de\u8c03\u961f\u5217\u540d\n        self._channel.basic_consume(self.on_response,   # \u5ba2\u6237\u7aef\u6d88\u8d39\u56de\u8c03\u961f\u5217\n                                    no_ack=True,\n                                    queue=callback_queue)\n\n        corr_id = str(uuid.uuid4())  # \u751f\u6210\u5ba2\u6237\u7aef\u8bf7\u6c42id\n        self.data[corr_id] = {\n            'isAccept': False,\n            'result': None,\n            'callbackQueue': callback_queue\n        }\n        self._channel.basic_publish( # \u53d1\u9001\u6570\u636e\u7ed9\u670d\u52a1\u7aef\n            exchange=exchange,\n            routing_key=key,\n            body=body,\n            properties=pika.BasicProperties(\n                reply_to=callback_queue,\n                correlation_id=corr_id,\n            )\n        )\n\n        while not self.data[corr_id]['isAccept']:  # \u5224\u65ad\u662f\u5426\u63a5\u6536\u5230\u670d\u52a1\u7aef\u8fd4\u56de\u7684\u6d88\u606f\n            self._connection.process_data_events()\n            time.sleep(0.3)\n            continue\n\n        logger.info(\"Got the RPC server response => {}\".format(self.data[corr_id]['result']))\n        return self.data[corr_id]['result']", "response": "Send a message to the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_login_checker(user):\n    username = user.get('username')\n    password = user.get('password')\n    the_username = os.environ.get(\n        'SIMPLELOGIN_USERNAME',\n        current_app.config.get('SIMPLELOGIN_USERNAME', 'admin')\n    )\n    the_password = os.environ.get(\n        'SIMPLELOGIN_PASSWORD',\n        current_app.config.get('SIMPLELOGIN_PASSWORD', 'secret')\n    )\n    if username == the_username and password == the_password:\n        return True\n    return False", "response": "user must be a dictionary here default is\n    checking username/password\n    if login is ok returns True else False\n\n    :param user: dict {'username':'', 'password': ''}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if user is logged in if username is passed check if user is logged in if username is a list", "response": "def is_logged_in(username=None):\n    \"\"\"Checks if user is logged in if `username`\n    is passed check if specified user is logged in\n    username can be a list\"\"\"\n    if username:\n        if not isinstance(username, (list, tuple)):\n            username = [username]\n        return 'simple_logged_in' in session and get_username() in username\n    return 'simple_logged_in' in session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecorate views to require login @login_required @login_required() @login_required(username='admin') @login_required(username=['admin', 'jon']) @login_required(basic=True) @login_required(must=[function, another_function])", "response": "def login_required(function=None, username=None, basic=False, must=None):\n    \"\"\"Decorate views to require login\n    @login_required\n    @login_required()\n    @login_required(username='admin')\n    @login_required(username=['admin', 'jon'])\n    @login_required(basic=True)\n    @login_required(must=[function, another_function])\n    \"\"\"\n\n    if function and not callable(function):\n        raise ValueError(\n            'Decorator receives only named arguments, '\n            'try login_required(username=\"foo\")'\n        )\n\n    def check(validators):\n        \"\"\"Return in the first validation error, else return None\"\"\"\n        if validators is None:\n            return\n        if not isinstance(validators, (list, tuple)):\n            validators = [validators]\n        for validator in validators:\n            error = validator(get_username())\n            if error is not None:\n                return SimpleLogin.get_message('auth_error', error), 403\n\n    def dispatch(fun, *args, **kwargs):\n        if basic and request.is_json:\n            return dispatch_basic_auth(fun, *args, **kwargs)\n\n        if is_logged_in(username=username):\n            return check(must) or fun(*args, **kwargs)\n        elif is_logged_in():\n            return SimpleLogin.get_message('access_denied'), 403\n        else:\n            flash(SimpleLogin.get_message('login_required'), 'warning')\n            return redirect(url_for('simplelogin.login', next=request.path))\n\n    def dispatch_basic_auth(fun, *args, **kwargs):\n        simplelogin = current_app.extensions['simplelogin']\n        auth_response = simplelogin.basic_auth()\n        if auth_response is True:\n            return check(must) or fun(*args, **kwargs)\n        else:\n            return auth_response\n\n    if function:\n        @wraps(function)\n        def simple_decorator(*args, **kwargs):\n            \"\"\"This is for when decorator is @login_required\"\"\"\n            return dispatch(function, *args, **kwargs)\n        return simple_decorator\n\n    def decorator(f):\n        \"\"\"This is for when decorator is @login_required(...)\"\"\"\n        @wraps(f)\n        def wrap(*args, **kwargs):\n            return dispatch(f, *args, **kwargs)\n        return wrap\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_message(message, *args, **kwargs):\n        msg = current_app.extensions['simplelogin'].messages.get(message)\n        if msg and (args or kwargs):\n            return msg.format(*args, **kwargs)\n        return msg", "response": "Helper to get internal messages outside this instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsupport basic_auth via HTTP Basic Authentication", "response": "def basic_auth(self, response=None):\n        \"\"\"Support basic_auth via /login or login_required(basic=True)\"\"\"\n        auth = request.authorization\n        if auth and self._login_checker({'username': auth.username,\n                                         'password': auth.password}):\n            session['simple_logged_in'] = True\n            session['simple_basic_auth'] = True\n            session['simple_username'] = auth.username\n            return response or True\n        else:\n            headers = {'WWW-Authenticate': 'Basic realm=\"Login Required\"'}\n            return 'Invalid credentials', 401, headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if user exists and its credentials.", "response": "def check_my_users(user):\n    \"\"\"Check if user exists and its credentials.\n    Take a look at encrypt_app.py and encrypt_cli.py\n     to see how to encrypt passwords\n    \"\"\"\n    user_data = my_users.get(user['username'])\n    if not user_data:\n        return False  # <--- invalid credentials\n    elif user_data.get('password') == user['password']:\n        return True  # <--- user is logged in!\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_user(**data):\n    if 'username' not in data or 'password' not in data:\n        raise ValueError('username and password are required.')\n\n    # Hash the user password\n    data['password'] = generate_password_hash(\n        data.pop('password'),\n        method='pbkdf2:sha256'\n    )\n\n    # Here you insert the `data` in your users database\n    # for this simple example we are recording in a json file\n    db_users = json.load(open('users.json'))\n    # add the new created user to json\n    db_users[data['username']] = data\n    # commit changes to database\n    json.dump(db_users, open('users.json', 'w'))\n    return data", "response": "Creates a user with encrypted password"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls function passing app as first argument", "response": "def with_app(f):\n    \"\"\"Calls function passing app as first argument\"\"\"\n    @wraps(f)\n    def decorator(*args, **kwargs):\n        app = create_app()\n        configure_extensions(app)\n        configure_views(app)\n        return f(app=app, *args, **kwargs)\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adduser(app, username, password):\n    with app.app_context():\n        create_user(username=username, password=password)\n        click.echo('user created!')", "response": "Add new user with admin access"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef runserver(app=None, reloader=None, debug=None, host=None, port=None):\n    debug = debug or app.config.get('DEBUG', False)\n    reloader = reloader or app.config.get('RELOADER', False)\n    host = host or app.config.get('HOST', '127.0.0.1')\n    port = port or app.config.get('PORT', 5000)\n    app.run(\n        use_reloader=reloader,\n        debug=debug,\n        host=host,\n        port=port\n    )", "response": "Run the Flask development server i. e. app. run"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pulse_data(self, value):\n        if self._i2c_expander == 'PCF8574':\n            self.bus.write_byte(self._address, ((value & ~PCF8574_E) | self._backlight))\n            c.usleep(1)\n            self.bus.write_byte(self._address, value | PCF8574_E | self._backlight)\n            c.usleep(1)\n            self.bus.write_byte(self._address, ((value & ~PCF8574_E) | self._backlight))\n            c.usleep(100)\n        elif self._i2c_expander in ['MCP23008', 'MCP23017']:\n            self._mcp_data &= ~MCP230XX_DATAMASK\n            self._mcp_data |= value << MCP230XX_DATASHIFT\n            self._mcp_data &= ~MCP230XX_E\n            self.bus.write_byte_data(self._address, self._mcp_gpio, self._mcp_data)\n            c.usleep(1)\n            self._mcp_data |= MCP230XX_E\n            self.bus.write_byte_data(self._address, self._mcp_gpio, self._mcp_data)\n            c.usleep(1)\n            self._mcp_data &= ~MCP230XX_E\n            self.bus.write_byte_data(self._address, self._mcp_gpio, self._mcp_data)\n            c.usleep(100)", "response": "Pulse the enable flag to process value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the specified value to the display with automatic 4bit or 8bit mode.", "response": "def _send(self, value, mode):\n        \"\"\"Send the specified value to the display with automatic 4bit / 8bit\n        selection. The rs_mode is either ``RS_DATA`` or ``RS_INSTRUCTION``.\"\"\"\n        # Wait, if compatibility mode is enabled\n        if self.compat_mode:\n            self._wait()\n\n        # Choose instruction or data mode\n        GPIO.output(self.pins.rs, mode)\n\n        # If the RW pin is used, set it to low in order to write.\n        if self.pins.rw is not None:\n            GPIO.output(self.pins.rw, 0)\n\n        # Write data out in chunks of 4 or 8 bit\n        if self.data_bus_mode == c.LCD_8BITMODE:\n            self._write8bits(value)\n        else:\n            self._write4bits(value >> 4)\n            self._write4bits(value)\n\n        # Record the time for the tail-end of the last send event\n        if self.compat_mode:\n            self.last_send_event = now()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write4bits(self, value):\n        for i in range(4):\n            bit = (value >> i) & 0x01\n            GPIO.output(self.pins[i + 7], bit)\n        self._pulse_enable()", "response": "Write 4 bits of data into the data bus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pulse_enable(self):\n        GPIO.output(self.pins.e, 0)\n        c.usleep(1)\n        GPIO.output(self.pins.e, 1)\n        c.usleep(1)\n        GPIO.output(self.pins.e, 0)\n        c.usleep(100)", "response": "Pulse the enable flag to process data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a unicode string to the display.", "response": "def write_string(self, value):\n        \"\"\"\n        Write the specified unicode string to the display.\n\n        To control multiline behavior, use newline (``\\\\n``) and carriage\n        return (``\\\\r``) characters.\n\n        Lines that are too long automatically continue on next line, as long as\n        ``auto_linebreaks`` has not been disabled.\n\n        Make sure that you're only passing unicode objects to this function.\n        The unicode string is then converted to the correct LCD encoding by\n        using the charmap specified at instantiation time.\n\n        If you're dealing with bytestrings (the default string type in Python\n        2), convert it to a unicode object using the ``.decode(encoding)``\n        method and the appropriate encoding. Example for UTF-8 encoded strings:\n\n        .. code::\n\n            >>> bstring = 'Temperature: 30\u00b0C'\n            >>> bstring\n            'Temperature: 30\\xc2\\xb0C'\n            >>> bstring.decode('utf-8')\n            u'Temperature: 30\\xb0C'\n\n        \"\"\"\n        encoded = self.codec.encode(value)  # type: List[int]\n        ignored = False\n\n        for [char, lookahead] in c.sliding_window(encoded, lookahead=1):\n\n            # If the previous character has been ignored, skip this one too.\n            if ignored is True:\n                ignored = False\n                continue\n\n            # Write regular chars\n            if char not in [codecs.CR, codecs.LF]:\n                self.write(char)\n                continue\n\n            # We're now left with only CR and LF characters. If an auto\n            # linebreak happened recently, and the lookahead matches too,\n            # ignore this write.\n            if self.recent_auto_linebreak is True:\n                crlf = (char == codecs.CR and lookahead == codecs.LF)\n                lfcr = (char == codecs.LF and lookahead == codecs.CR)\n                if crlf or lfcr:\n                    ignored = True\n                    continue\n\n            # Handle newlines and carriage returns\n            row, col = self.cursor_pos\n            if char == codecs.LF:\n                if row < self.lcd.rows - 1:\n                    self.cursor_pos = (row + 1, col)\n                else:\n                    self.cursor_pos = (0, col)\n            elif char == codecs.CR:\n                if self.text_align_mode == 'left':\n                    self.cursor_pos = (row, 0)\n                else:\n                    self.cursor_pos = (row, self.lcd.cols - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear(self):\n        self.command(c.LCD_CLEARDISPLAY)\n        self._cursor_pos = (0, 0)\n        self._content = [[0x20] * self.lcd.cols for _ in range(self.lcd.rows)]\n        c.msleep(2)", "response": "Overwrite display with blank characters and reset cursor position."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets cursor to initial position and reset any shifting.", "response": "def home(self):\n        \"\"\"Set cursor to initial position and reset any shifting.\"\"\"\n        self.command(c.LCD_RETURNHOME)\n        self._cursor_pos = (0, 0)\n        c.msleep(2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shift_display(self, amount):\n        if amount == 0:\n            return\n        direction = c.LCD_MOVERIGHT if amount > 0 else c.LCD_MOVELEFT\n        for i in range(abs(amount)):\n            self.command(c.LCD_CURSORSHIFT | c.LCD_DISPLAYMOVE | direction)\n            c.usleep(50)", "response": "Shift the display. Use negative amounts to shift left and positive\n        amounts to shift right. Use positive amounts to shift right and positive\n        amounts to shift left and positive\n        amounts to shift right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_char(self, location, bitmap):\n        assert 0 <= location <= 7, 'Only locations 0-7 are valid.'\n        assert len(bitmap) == 8, 'Bitmap should have exactly 8 rows.'\n\n        # Store previous position\n        pos = self.cursor_pos\n\n        # Write character to CGRAM\n        self.command(c.LCD_SETCGRAMADDR | location << 3)\n        for row in bitmap:\n            self._send_data(row)\n\n        # Restore cursor pos\n        self.cursor_pos = pos", "response": "Create a new character in the HD44780."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a raw byte to the LCD.", "response": "def write(self, value):  # type: (int) -> None\n        \"\"\"Write a raw byte to the LCD.\"\"\"\n\n        # Get current position\n        row, col = self._cursor_pos\n\n        # Write byte if changed\n        try:\n            if self._content[row][col] != value:\n                self._send_data(value)\n                self._content[row][col] = value  # Update content cache\n                unchanged = False\n            else:\n                unchanged = True\n        except IndexError as e:\n            # Position out of range\n            if self.auto_linebreaks is True:\n                raise e\n            self._send_data(value)\n            unchanged = False\n\n        # Update cursor position.\n        if self.text_align_mode == 'left':\n            if self.auto_linebreaks is False or col < self.lcd.cols - 1:\n                # No newline, update internal pointer\n                newpos = (row, col + 1)\n                if unchanged:\n                    self.cursor_pos = newpos\n                else:\n                    self._cursor_pos = newpos\n                self.recent_auto_linebreak = False\n            else:\n                # Newline, reset pointer\n                if row < self.lcd.rows - 1:\n                    self.cursor_pos = (row + 1, 0)\n                else:\n                    self.cursor_pos = (0, 0)\n                self.recent_auto_linebreak = True\n        else:\n            if self.auto_linebreaks is False or col > 0:\n                # No newline, update internal pointer\n                newpos = (row, col - 1)\n                if unchanged:\n                    self.cursor_pos = newpos\n                else:\n                    self._cursor_pos = newpos\n                self.recent_auto_linebreak = False\n            else:\n                # Newline, reset pointer\n                if row < self.lcd.rows - 1:\n                    self.cursor_pos = (row + 1, self.lcd.cols - 1)\n                else:\n                    self.cursor_pos = (0, self.lcd.cols - 1)\n                self.recent_auto_linebreak = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cursor(lcd, row, col):\n    warnings.warn('The `cursor` context manager is deprecated', DeprecationWarning)\n    lcd.cursor_pos = (row, col)\n    yield", "response": "Context manager to control cursor position. DEPRECATED."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sliding_window(seq, lookahead):\n    it = itertools.chain(iter(seq), ' ' * lookahead)  # Padded iterator\n    window_size = lookahead + 1\n    result = tuple(itertools.islice(it, window_size))\n    if len(result) == window_size:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result", "response": "Create a sliding window of the specified number of lookahead characters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend the specified value to the display with automatic 4bit or 8bit selection. The rs_mode parameter can be either RS_DATA or RS_INSTRUCTION. The rs_mode parameter can be either RS_DATA or RS_INSTRUCTION.", "response": "def _send(self, value, mode):\n        \"\"\"Send the specified value to the display with automatic 4bit / 8bit\n        selection. The rs_mode is either ``RS_DATA`` or ``RS_INSTRUCTION``.\"\"\"\n\n        # Assemble the parameters sent to the pigpio script\n        params = [mode]\n        params.extend([(value >> i) & 0x01 for i in range(8)])\n        # Switch off pigpio's exceptions, so that we get the return codes\n        pigpio.exceptions = False\n        while True:\n            ret = self.pi.run_script(self._writescript, params)\n            if ret >= 0:\n                break\n            elif ret != pigpio.PI_SCRIPT_NOT_READY:\n                raise pigpio.error(pigpio.error_text(ret))\n            # If pigpio script is not ready, sleep and try again\n            c.usleep(1)\n        # Switch on pigpio's exceptions\n        pigpio.exceptions = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the cmdline params", "response": "def get_params():\n    \"\"\" get the cmdline params \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--connect-timeout\",\n                        type=float,\n                        default=10.0,\n                        help=\"ZK connect timeout\")\n    parser.add_argument(\"--run-once\",\n                        type=str,\n                        default=\"\",\n                        help=\"Run a command non-interactively and exit\")\n    parser.add_argument(\"--run-from-stdin\",\n                        action=\"store_true\",\n                        default=False,\n                        help=\"Read cmds from stdin, run them and exit\")\n    parser.add_argument(\"--sync-connect\",\n                        action=\"store_true\",\n                        default=False,\n                        help=\"Connect synchronously.\")\n    parser.add_argument(\"--readonly\",\n                        action=\"store_true\",\n                        default=False,\n                        help=\"Enable readonly.\")\n    parser.add_argument(\"--tunnel\",\n                        type=str,\n                        help=\"Create a ssh tunnel via this host\",\n                        default=None)\n    parser.add_argument(\"--version\",\n                        action=\"store_true\",\n                        default=False,\n                        help=\"Display version and exit.\")\n    parser.add_argument(\"hosts\",\n                        nargs=\"*\",\n                        help=\"ZK hosts to connect\")\n    params = parser.parse_args()\n    return CLIParams(\n        params.connect_timeout,\n        params.run_once,\n        params.run_from_stdin,\n        params.sync_connect,\n        params.hosts,\n        params.readonly,\n        params.tunnel,\n        params.version\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_acl(cls, acl):\n        try:\n            scheme, rest = acl.split(\":\", 1)\n            credential = \":\".join(rest.split(\":\")[0:-1])\n            cdrwa = rest.split(\":\")[-1]\n        except ValueError:\n            raise cls.BadACL(\"Bad ACL: %s. Format is scheme:id:perms\" % (acl))\n\n        if scheme not in cls.valid_schemes:\n            raise cls.BadACL(\"Invalid scheme: %s\" % (acl))\n\n        create = True if \"c\" in cdrwa else False\n        read = True if \"r\" in cdrwa else False\n        write = True if \"w\" in cdrwa else False\n        delete = True if \"d\" in cdrwa else False\n        admin = True if \"a\" in cdrwa else False\n\n        if scheme == \"username_password\":\n            try:\n                username, password = credential.split(\":\", 1)\n            except ValueError:\n                raise cls.BadACL(\"Bad ACL: %s. Format is scheme:id:perms\" % (acl))\n            return make_digest_acl(username,\n                                   password,\n                                   read,\n                                   write,\n                                   create,\n                                   delete,\n                                   admin)\n        else:\n            return make_acl(scheme,\n                            credential,\n                            read,\n                            write,\n                            create,\n                            delete,\n                            admin)", "response": "Parse an ACL string and return a dictionary of the relevant attributes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(cls, acl):\n        return {\n            \"perms\": acl.perms,\n            \"id\": {\n                \"scheme\": acl.id.scheme,\n                \"id\": acl.id.id\n            }\n        }", "response": "transform an ACL to a dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_dict(cls, acl_dict):\n        perms = acl_dict.get(\"perms\", Permissions.ALL)\n        id_dict = acl_dict.get(\"id\", {})\n        id_scheme = id_dict.get(\"scheme\", \"world\")\n        id_id = id_dict.get(\"id\", \"anyone\")\n        return ACL(perms, Id(id_scheme, id_id))", "response": "Create an ACL object from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck connected and fails otherwise", "response": "def connected(func):\n    \"\"\" check connected, fails otherwise \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        if not self.connected:\n            self.show_output(\"Not connected.\")\n        else:\n            try:\n                return func(*args, **kwargs)\n            except APIError:\n                self.show_output(\"ZooKeeper internal error.\")\n            except AuthFailedError:\n                self.show_output(\"Authentication failed.\")\n            except NoAuthError:\n                self.show_output(\"Not authenticated.\")\n            except BadVersionError:\n                self.show_output(\"Bad version.\")\n            except ConnectionLoss:\n                self.show_output(\"Connection loss.\")\n            except NotReadOnlyCallError:\n                self.show_output(\"Not a read-only operation.\")\n            except BadArgumentsError:\n                self.show_output(\"Bad arguments.\")\n            except SessionExpiredError:\n                self.show_output(\"Session expired.\")\n            except UnimplementedError as ex:\n                self.show_output(\"Not implemented by the server: %s.\" % str(ex))\n            except ZookeeperError as ex:\n                self.show_output(\"Unknown ZooKeeper error: %s\" % str(ex))\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_path_exists_foreach(path_params, func):\n    @wraps(func)\n    def wrapper(*args):\n        self = args[0]\n        params = args[1]\n\n        if not self.in_transaction:\n            for name in path_params:\n                value = getattr(params, name)\n                paths = value if type(value) == list else [value]\n                resolved = []\n                for path in paths:\n                    path = self.resolve_path(path)\n                    if not self.client.exists(path):\n                        self.show_output(\"Path %s doesn't exist\", path)\n                        return False\n                    resolved.append(path)\n\n                if type(value) == list:\n                    setattr(params, name, resolved)\n                else:\n                    setattr(params, name, resolved[0])\n\n        return func(self, params)\n\n    return wrapper", "response": "Decorator that checks that paths exist for the given list of path_params."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_path_absent(func):\n    @wraps(func)\n    def wrapper(*args):\n        self = args[0]\n        params = args[1]\n        orig_path = params.path\n        sequence = getattr(params, 'sequence', False)\n        params.path = self.resolve_path(params.path)\n        if self.in_transaction or sequence or not self.client.exists(params.path):\n            if sequence and orig_path.endswith(\"/\") and params.path != \"/\":\n                params.path += \"/\"\n            return func(self, params)\n        self.show_output(\"Path %s already exists\", params.path)\n    return wrapper", "response": "Decorator to check if path doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef complete_set_acls(self, cmd_param_text, full_cmd, *rest):\n        possible_acl = [\n            \"digest:\",\n            \"username_password:\",\n            \"world:anyone:c\",\n            \"world:anyone:cd\",\n            \"world:anyone:cdr\",\n            \"world:anyone:cdrw\",\n            \"world:anyone:cdrwa\",\n        ]\n        complete_acl = partial(complete_values, possible_acl)\n        completers = [self._complete_path, complete_acl, complete_labeled_boolean(\"recursive\")]\n        return complete(completers, cmd_param_text, full_cmd, *rest)", "response": "Complete the set ACLs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_ls(self, params):\n        watcher = lambda evt: self.show_output(str(evt))\n        kwargs = {\"watch\": watcher} if params.watch else {}\n        znodes = self._zk.get_children(params.path, **kwargs)\n        self.show_output(params.sep.join(sorted(znodes)))", "response": "\\ x1b [ 1mNAME \\ x1b [ 0mls - Lists the znodes for the given path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_watch(self, params):\n        wm = get_watch_manager(self._zk)\n        if params.command == \"start\":\n            debug = to_bool(params.debug)\n            children = to_int(params.sleep, -1)\n            wm.add(params.path, debug, children)\n        elif params.command == \"stop\":\n            wm.remove(params.path)\n        elif params.command == \"stats\":\n            repeat = to_int(params.debug, 1)\n            sleep = to_int(params.sleep, 1)\n            if repeat == 0:\n                while True:\n                    wm.stats(params.path)\n                    time.sleep(sleep)\n            else:\n                for _ in range(0, repeat):\n                    wm.stats(params.path)\n                    time.sleep(sleep)\n        else:\n            self.show_output(\"watch <start|stop|stats> <path> [verbose]\")", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m watch - Watches the tree under a given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_mirror(self, params):\n        question = \"Are you sure you want to replace %s with %s?\" % (params.dst, params.src)\n        if params.skip_prompt or self.prompt_yes_no(question):\n            self.copy(params, True, True, 0, True)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m mirror - Mirrors from local to remote"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_child_count(self, params):\n        for child, level in self._zk.tree(params.path, params.depth, full_path=True):\n            self.show_output(\"%s: %d\", child, self._zk.child_count(child))", "response": "\\ x1b [ 1mNAME \\ x1b [ 0mchild_count - Prints the child count for paths \\ x1b [ 1mSYNOPSIS \\ x1b [ 0mchild_count - Prints the child count for paths \\ x1b [ 1mOPTIONS \\ x1b [ 0mpath - Prints the path to the path to the path to the path to the path to the path to the path to the path to the zookeeper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_find(self, params):\n        for path in self._zk.find(params.path, params.match, 0):\n            self.show_output(path)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m find - Find znodes whose path matches a given text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_grep(self, params):\n        self.grep(params.path, params.content, 0, params.show_matches)", "response": "\\ x1b [ 1mGREP - Prints znodes with a value matching the given text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_get(self, params):\n        watcher = lambda evt: self.show_output(str(evt))\n        kwargs = {\"watch\": watcher} if params.watch else {}\n        value, _ = self._zk.get(params.path, **kwargs)\n\n        # maybe it's compressed?\n        if value is not None:\n            try:\n                value = zlib.decompress(value)\n            except:\n                pass\n\n        self.show_output(value)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m get - Gets the znode s value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_create(self, params):\n        try:\n            kwargs = {\"acl\": None, \"ephemeral\": params.ephemeral, \"sequence\": params.sequence}\n            if not self.in_transaction:\n                kwargs[\"makepath\"] = params.recursive\n\n            if params.asynchronous and not self.in_transaction:\n                self.client_context.create_async(params.path, decoded(params.value), **kwargs)\n            else:\n                self.client_context.create(params.path, decoded(params.value), **kwargs)\n        except NodeExistsError:\n            self.show_output(\"Path %s exists\", params.path)\n        except NoNodeError:\n            self.show_output(\"Missing path in %s (try recursive?)\", params.path)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m create - Creates a znode and returns the znode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncomplete the set of keys", "response": "def complete_set(self, cmd_param_text, full_cmd, *rest):\n        \"\"\" TODO: suggest the old value & the current version \"\"\"\n        complete_value = partial(complete_values, [\"updated-value\"])\n        complete_version = partial(complete_values, [str(i) for i in range(1, 11)])\n        completers = [self._complete_path, complete_value, complete_version]\n        return complete(completers, cmd_param_text, full_cmd, *rest)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset a znode s data", "response": "def set(self, path, value, version):\n        \"\"\" sets a znode's data \"\"\"\n        if self.in_transaction:\n            self.client_context.set_data(path, value, version=version)\n        else:\n            self.client_context.set(path, value, version=version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_session_info(self, params):\n        fmt_str = \"\"\"state=%s\nsessionid=%s\nauth_info=%s\nprotocol_version=%d\nxid=%d\nlast_zxid=0x%.16x\ntimeout=%d\nclient=%s\nserver=%s\ndata_watches=%s\nchild_watches=%s\"\"\"\n        content = fmt_str % (\n        self._zk.client_state,\n            self._zk.sessionid,\n            list(self._zk.auth_data),\n            self._zk.protocol_version,\n            self._zk.xid,\n            self._zk.last_zxid,\n            self._zk.session_timeout,\n            self._zk.client,\n            self._zk.server,\n            \",\".join(self._zk.data_watches),\n            \",\".join(self._zk.child_watches)\n        )\n\n        output = get_matching(content, params.match)\n        self.show_output(output)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m session_info - Shows information about the current session"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_mntr(self, params):\n        hosts = params.hosts if params.hosts != \"\" else None\n\n        if hosts is not None and invalid_hosts(hosts):\n            self.show_output(\"List of hosts has the wrong syntax.\")\n            return\n\n        if self._zk is None:\n            self._zk = XClient()\n\n        try:\n            content = get_matching(self._zk.mntr(hosts), params.match)\n            self.show_output(content)\n        except XClient.CmdFailed as ex:\n            self.show_output(str(ex))", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m mntr - Executes the mntr four - letter command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_cons(self, params):\n        hosts = params.hosts if params.hosts != \"\" else None\n\n        if hosts is not None and invalid_hosts(hosts):\n            self.show_output(\"List of hosts has the wrong syntax.\")\n            return\n\n        if self._zk is None:\n            self._zk = XClient()\n\n        try:\n            content = get_matching(self._zk.cons(hosts), params.match)\n            self.show_output(content)\n        except XClient.CmdFailed as ex:\n            self.show_output(str(ex))", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m cons - Executes the cons four - letter command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_rmr(self, params):\n        for path in params.paths:\n            self._zk.delete(path, recursive=True)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0mrmr - Delete a path and all its children"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_diff(self, params):\n        count = 0\n        for count, (diff, path) in enumerate(self._zk.diff(params.path_a, params.path_b), 1):\n            if diff == -1:\n                self.show_output(\"-- %s\", path)\n            elif diff == 0:\n                self.show_output(\"-+ %s\", path)\n            elif diff == 1:\n                self.show_output(\"++ %s\", path)\n\n        if count == 0:\n            self.show_output(\"Branches are equal.\")", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m diff - Display the differences between two paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_json_get(self, params):\n        try:\n            Keys.validate(params.keys)\n        except Keys.Bad as ex:\n            self.show_output(str(ex))\n            return\n\n        if params.recursive:\n            paths = self._zk.tree(params.path, 0, full_path=True)\n            print_path = True\n        else:\n            paths = [(params.path, 0)]\n            print_path = False\n\n        for cpath, _ in paths:\n            try:\n                jstr, _ = self._zk.get(cpath)\n                value = Keys.value(json_deserialize(jstr), params.keys)\n\n                if print_path:\n                    self.show_output(\"%s: %s\", os.path.basename(cpath), value)\n                else:\n                    self.show_output(value)\n            except BadJSON as ex:\n                self.show_output(\"Path %s has bad JSON.\", cpath)\n            except Keys.Missing as ex:\n                self.show_output(\"Path %s is missing key %s.\", cpath, ex)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m json_get - Get key or keys from a JSON object serialized in the given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompletes a JSON GET command.", "response": "def complete_json_get(self, cmd_param_text, full_cmd, *rest):\n        \"\"\" TODO: prefetch & parse znodes & suggest keys \"\"\"\n        complete_keys = partial(complete_values, [\"key1\", \"key2\", \"#{key1.key2}\"])\n        completers = [self._complete_path, complete_keys, complete_labeled_boolean(\"recursive\")]\n        return complete(completers, cmd_param_text, full_cmd, *rest)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_json_set(self, params):\n        try:\n            Keys.validate(params.keys)\n        except Keys.Bad as ex:\n            self.show_output(str(ex))\n            return\n\n        try:\n            jstr, stat = self._zk.get(params.path)\n            obj_src = json_deserialize(jstr)\n            obj_dst = copy.deepcopy(obj_src)\n\n            # Cast value to its given type.\n            value = to_type(params.value, params.value_type)\n            Keys.set(obj_dst, params.keys, value)\n\n            if params.confirm:\n                a = json.dumps(obj_src, sort_keys=True, indent=4)\n                b = json.dumps(obj_dst, sort_keys=True, indent=4)\n                diff = difflib.unified_diff(a.split(\"\\n\"), b.split(\"\\n\"))\n                self.show_output(\"\\n\".join(diff))\n                if not self.prompt_yes_no(\"Apply update?\"):\n                    return\n\n            # Pass along the read version, to ensure we are updating what we read.\n            self.set(params.path, json.dumps(obj_dst), version=stat.version)\n        except BadJSON:\n            self.show_output(\"Path %s has bad JSON.\", params.path)\n        except Keys.Missing as ex:\n            self.show_output(\"Path %s is missing key %s.\", params.path, ex)\n        except ValueError:\n            self.show_output(\"Bad value_type\")", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m json_set - Sets the value for the given key on a JSON object serialized in the given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_json_set_many(self, params):\n        # Ensure we have a balance set of (key, value, type) tuples.\n        if len(params.keys_values_types) % 3 != 0:\n            self.show_output('Bad list of parameters')\n            return\n\n        for key, _, _ in grouper(params.keys_values_types, 3):\n            try:\n                Keys.validate(key)\n            except Keys.Bad as ex:\n                self.show_output(str(ex))\n                return\n\n        # Fetch & deserialize znode.\n        jstr, stat = self._zk.get(params.path)\n        try:\n            obj_src = json_deserialize(jstr)\n        except BadJSON:\n            self.show_output(\"Path %s has bad JSON.\", params.path)\n        obj_dst = copy.deepcopy(obj_src)\n\n        # Cast values to their given type.\n        for key, value, ptype in grouper(params.keys_values_types, 3):\n            try:\n                Keys.set(obj_dst, key, to_type(value, ptype))\n            except Keys.Missing as ex:\n                self.show_output(\"Path %s is missing key %s.\", params.path, ex)\n                return\n            except ValueError:\n                self.show_output(\"Bad value_type\")\n                return\n\n        # Pass along the read version, to ensure we are updating what we read.\n        self.set(params.path, json.dumps(obj_dst), version=stat.version)", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m json_set_many - like json_set but for multiple key value pairs \\ x1b [ 0m DESCRIPTION \\ x1b [ 0m json_set_many - like json_set but for one key value pair \\ x1b [ 0m json_set_many - like json_set but for one key value pair \\ x1b [ 0m json_set_many - like"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_edit(self, params):\n        if os.getuid() == 0:\n            self.show_output(\"edit cannot be run as root.\")\n            return\n\n        editor = os.getenv(\"EDITOR\", os.getenv(\"VISUAL\", \"/usr/bin/vi\"))\n        if editor is None:\n            self.show_output(\"No editor found, please set $EDITOR\")\n            return\n\n        editor = which(editor)\n        if not editor:\n            self.show_output(\"Cannot find executable editor, please set $EDITOR\")\n            return\n\n        st = os.stat(editor)\n        if (st.st_mode & statlib.S_ISUID) or (st.st_mode & statlib.S_ISUID):\n            self.show_output(\"edit cannot use setuid/setgid binaries.\")\n            return\n\n        # copy content to tempfile\n        value, stat = self._zk.get(params.path)\n        _, tmppath = tempfile.mkstemp()\n        with open(tmppath, \"w\") as fh:\n            fh.write(value if value else \"\")\n\n        # launch editor\n        rv = os.system(\"%s %s\" % (editor, tmppath))\n        if rv != 0:\n            self.show_output(\"%s did not exit successfully\" % editor)\n            try:\n                os.unlink(tmppath)\n            except OSError: pass\n            return\n\n        # did it change? if so, save it\n        with open(tmppath, \"r\") as fh:\n            newvalue = fh.read()\n        if newvalue != value:\n            self.set(params.path, decoded(newvalue), stat.version)\n\n        try:\n            os.unlink(tmppath)\n        except OSError: pass", "response": "\\ x1b [ 1mNAME \\ x1b [ 0mMOD \\ x1b [ 0mDESCRIPTION \\ x1b [ 0mMOD \\ x1b [ 0mNAME \\ x1b [ 0mMOD \\ x1b [ 0mNAME \\ x1b [ 0mDESCRIPTION \\ x1b [ 0mMOD \\ x1b [ 0mNAME \\ x1b [ 0mNAME \\ x1b [ 0mDESCRIPTION \\ x1b \\ x1b ["}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_loop(self, params):\n        repeat = params.repeat\n        if repeat < 0:\n            self.show_output(\"<repeat> must be >= 0.\")\n            return\n\n        pause = params.pause\n        if pause < 0:\n            self.show_output(\"<pause> must be >= 0.\")\n            return\n\n        cmds = params.cmds\n        i = 0\n        with self.transitions_disabled():\n            while True:\n                for cmd in cmds:\n                    try:\n                        self.onecmd(cmd)\n                    except Exception as ex:\n                        self.show_output(\"Command failed: %s.\", ex)\n                if pause > 0.0:\n                    time.sleep(pause)\n                i += 1\n                if repeat > 0 and i >= repeat:\n                    break", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m loop - Runs commands in a loop \\ x1b [ 0mSYNOPSIS \\ x1b [ 0mDESCRIPTION \\ x1b [ 0mLOOP \\ x1b [ 0mCOMMAND \\ x1b [ 0mCOMMAND \\ x1b [ 0mCOMMAND \\ x1b [ 0mCOMMAND \\ x1b [ 0mCOMMAND \\ x1b [ 0mCOMMAND"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complete_session_endpoint(self, cmd_param_text, full_cmd, *rest):\n        complete_hosts = partial(complete_values, [\"127.0.0.1:2181\"])\n        completers = [self._complete_path, complete_hosts, complete_labeled_boolean(\"reverse\")]\n        return complete(completers, cmd_param_text, full_cmd, *rest)", "response": "Complete the session endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_fill(self, params):\n        self._zk.set(params.path, decoded(params.val * params.repeat))", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m fill - Fills a znode with the given value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_reconfig(self, params):\n        if params.cmd not in [\"add\", \"remove\"]:\n            raise ValueError(\"Bad command: %s\" % params.cmd)\n\n        joining, leaving, from_config = None, None, params.from_config\n\n        if params.cmd == \"add\":\n            joining = params.args\n        elif params.cmd == \"remove\":\n            leaving = params.args\n\n        try:\n            value, _ = self._zk.reconfig(\n                joining=joining, leaving=leaving, new_members=None, from_config=from_config)\n            self.show_output(value)\n        except NewConfigNoQuorumError:\n            self.show_output(\"No quorum available to perform reconfig.\")\n        except ReconfigInProcessError:\n            self.show_output(\"There's a reconfig in process.\")", "response": "\\ x1b [ 1mNAME \\ x1b [ 0m reconfig - Reconfigures a ZooKeeper cluster"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_echo(self, params):\n        values = []\n\n        with self.output_context() as context:\n            for cmd in params.cmds:\n                rv = self.onecmd(cmd)\n                val = \"\" if rv is False else context.value.rstrip(\"\\n\")\n                values.append(val)\n                context.reset()\n\n        try:\n            self.show_output(params.fmtstr, *values)\n        except TypeError:\n            self.show_output(\"Bad format string or missing arguments.\")", "response": "\\ x1b [ 1mECHO - displays formatted data for the current locale"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to a list of hosts and create a Zookeeper connection.", "response": "def _connect(self, hosts_list):\n        \"\"\"\n        In the basic case, hostsp is a list of hosts like:\n\n        ```\n        [10.0.0.2:2181, 10.0.0.3:2181]\n        ```\n\n        It might also contain auth info:\n\n        ```\n        [digest:foo:bar@10.0.0.2:2181, 10.0.0.3:2181]\n        ```\n        \"\"\"\n        self._disconnect()\n        auth_data = []\n        hosts = []\n\n        for auth_host in hosts_list:\n            nl = Netloc.from_string(auth_host)\n            rhost, rport = hosts_to_endpoints(nl.host)[0]\n            if self._tunnel is not None:\n                lhost, lport = TunnelHelper.create_tunnel(rhost, rport, self._tunnel)\n                hosts.append('{0}:{1}'.format(lhost, lport))\n            else:\n                hosts.append(nl.host)\n\n            if nl.scheme != \"\":\n                auth_data.append((nl.scheme, nl.credential))\n\n        self._zk = XClient(\",\".join(hosts),\n                           read_only=self._read_only,\n                           timeout=self._connect_timeout,\n                           auth_data=auth_data if len(auth_data) > 0 else None)\n        if self._asynchronous:\n            self._connect_async(hosts)\n        else:\n            self._connect_sync(hosts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connected_socket(address, timeout=3):\n    sock = socket.create_connection(address, timeout)\n    yield sock\n    sock.close()", "response": "yields a connected socket"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, path, value=b\"\", acl=None, ephemeral=False,\n               sequence=False):\n        \"\"\" wrapper that handles encoding (yay Py3k) \"\"\"\n        super(XTransactionRequest, self).create(path, to_bytes(value), acl, ephemeral, sequence)", "response": "wrapper that handles encoding"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap that handles encoding", "response": "def set_data(self, path, value, version=-1):\n        \"\"\" wrapper that handles encoding (yay Py3k) \"\"\"\n        super(XTransactionRequest, self).set_data(path, to_bytes(value), version)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping the default get method and deals with encoding", "response": "def get(self, *args, **kwargs):\n        \"\"\" wraps the default get() and deals with encoding \"\"\"\n        value, stat = super(XClient, self).get(*args, **kwargs)\n\n        try:\n            if value is not None:\n                value = value.decode(encoding=\"utf-8\")\n        except UnicodeDecodeError:\n            pass\n\n        return (value, stat)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bytes(self, *args, **kwargs):\n        return super(XClient, self).get(*args, **kwargs)", "response": "get the bytes representation of the cache entry"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the value of the key at the given path", "response": "def set(self, path, value, version=-1):\n        \"\"\" wraps the default set() and handles encoding (Py3k) \"\"\"\n        value = to_bytes(value)\n        super(XClient, self).set(path, value, version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_async(self, path, value=b\"\", acl=None, ephemeral=False, sequence=False, makepath=False):\n        value = to_bytes(value)\n        return super(XClient, self).create_async(path, value, acl, ephemeral, sequence, makepath)", "response": "wrap the default create method and handles encoding"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find(self, path, match, flags):\n        try:\n            match = re.compile(match, flags)\n        except sre_constants.error as ex:\n            print(\"Bad regexp: %s\" % (ex))\n            return\n\n        offset = len(path)\n        for cpath in Tree(self, path).get():\n            if match.search(cpath[offset:]):\n                yield cpath", "response": "find every matching child path under path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grep(self, path, content, flags):\n        try:\n            match = re.compile(content, flags)\n        except sre_constants.error as ex:\n            print(\"Bad regexp: %s\" % (ex))\n            return\n\n        for gpath, matches in self.do_grep(path, match):\n            yield (gpath, matches)", "response": "grep every child path under path for content"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef child_count(self, path):\n        stat = self.stat(path)\n        if not stat:\n            return 0\n\n        count = stat.numChildren\n        for _, _, stat in self.tree(path, 0, include_stat=True):\n            if stat:\n                count += stat.numChildren\n        return count", "response": "returns the number of children under the given path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diff(self, path_a, path_b):\n        path_a = path_a.rstrip(\"/\")\n        path_b = path_b.rstrip(\"/\")\n\n        if not self.exists(path_a) or not self.exists(path_b):\n            return\n\n        if not self.equal(path_a, path_b):\n            yield 0, \"/\"\n\n        seen = set()\n\n        len_a = len(path_a)\n        len_b = len(path_b)\n\n        # first, check what's missing & changed in dst\n        for child_a, level in self.tree(path_a, 0, True):\n            child_sub = child_a[len_a + 1:]\n            child_b = os.path.join(path_b, child_sub)\n\n            if not self.exists(child_b):\n                yield -1, child_sub\n            else:\n                if not self.equal(child_a, child_b):\n                    yield 0, child_sub\n\n            seen.add(child_sub)\n\n        # now, check what's new in dst\n        for child_b, level in self.tree(path_b, 0, True):\n            child_sub = child_b[len_b + 1:]\n            if child_sub not in seen:\n                yield 1, child_sub", "response": "A generator that returns the differences between two paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares if a and b have the same bytes", "response": "def equal(self, path_a, path_b):\n        \"\"\"\n        compare if a and b have the same bytes\n        \"\"\"\n        content_a, _ = self.get_bytes(path_a)\n        content_b, _ = self.get_bytes(path_b)\n\n        return content_a == content_b"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stat(self, path):\n        try:\n            stat = self.exists(str(path))\n        except (NoNodeError, NoAuthError):\n            stat = None\n        return stat", "response": "safely gets the Znode s Stat"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmd(self, endpoints, cmd):\n        replies = []\n        for ep in endpoints:\n            try:\n                replies.append(self._cmd(ep, cmd))\n            except self.CmdFailed as ex:\n                # if there's only 1 endpoint, give up.\n                # if there's more, keep trying.\n                if len(endpoints) == 1:\n                    raise ex\n\n        return \"\".join(replies)", "response": "command is a list of endpoints"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a command to the specified endpoint and returns the result as a string.", "response": "def _cmd(self, endpoint, cmd):\n        \"\"\" endpoint is (host, port) \"\"\"\n        cmdbuf = \"%s\\n\" % (cmd)\n        # some cmds have large outputs and ZK closes the connection as soon as it\n        # finishes writing. so read in huge chunks.\n        recvsize = 1 << 20\n        replies = []\n        host, port = endpoint\n\n        ips = get_ips(host, port)\n\n        if len(ips) == 0:\n            raise self.CmdFailed(\"Failed to resolve: %s\" % (host))\n\n        for ip in ips:\n            try:\n                with connected_socket((ip, port)) as sock:\n                    sock.send(cmdbuf.encode())\n                    while True:\n                        buf = sock.recv(recvsize).decode(\"utf-8\")\n                        if buf == \"\":\n                            break\n                        replies.append(buf)\n            except socket.error as ex:\n                # if there's only 1 record, give up.\n                # if there's more, keep trying.\n                if len(ips) == 1:\n                    raise self.CmdFailed(\"Error(%s): %s\" % (ip, ex))\n\n        return \"\".join(replies)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nforces a reconnect by shutting down the connected socket return True if the reconnect happened False otherwise", "response": "def reconnect(self):\n        \"\"\" forces a reconnect by shutting down the connected socket\n            return True if the reconnect happened, False otherwise\n        \"\"\"\n        state_change_event = self.handler.event_object()\n\n        def listener(state):\n            if state is KazooState.SUSPENDED:\n                state_change_event.set()\n\n        self.add_listener(listener)\n\n        self._connection._socket.shutdown(socket.SHUT_RDWR)\n\n        state_change_event.wait(1)\n        if not state_change_event.is_set():\n            return False\n\n        # wait until we are back\n        while not self.connected:\n            time.sleep(0.1)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_by_server(self, hosts):\n        dump_by_endpoint = {}\n\n        for endpoint in self._to_endpoints(hosts):\n            try:\n                out = self.cmd([endpoint], \"dump\")\n            except self.CmdFailed as ex:\n                out = \"\"\n            dump_by_endpoint[endpoint] = out\n\n        return dump_by_endpoint", "response": "Returns the output of dump for each server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns ClientInfo per path.", "response": "def ephemerals_info(self, hosts):\n        \"\"\"Returns ClientInfo per path.\n\n        :param hosts: comma separated lists of members of the ZK ensemble.\n        :returns: A dictionary of (path, ClientInfo).\n\n        \"\"\"\n        info_by_path, info_by_id = {}, {}\n\n        for server_endpoint, dump in self.dump_by_server(hosts).items():\n            server_ip, server_port = server_endpoint\n            sid = None\n            for line in dump.split(\"\\n\"):\n                mat = self.SESSION_REGEX.match(line)\n                if mat:\n                    sid = mat.group(1)\n                    continue\n\n                mat = self.PATH_REGEX.match(line)\n                if mat:\n                    info = info_by_id.get(sid, None)\n                    if info is None:\n                        info = info_by_id[sid] = ClientInfo(sid)\n                    info_by_path[mat.group(1)] = info\n                    continue\n\n                mat = self.IP_PORT_REGEX.match(line)\n                if mat:\n                    ip, port, sid = mat.groups()\n                    if sid not in info_by_id:\n                        continue\n                    info_by_id[sid](ip, int(port), server_ip, server_port)\n\n        return info_by_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn ClientInfo per session.", "response": "def sessions_info(self, hosts):\n        \"\"\"Returns ClientInfo per session.\n\n        :param hosts: comma separated lists of members of the ZK ensemble.\n        :returns: A dictionary of (session_id, ClientInfo).\n\n        \"\"\"\n        info_by_id = {}\n\n        for server_endpoint, dump in self.dump_by_server(hosts).items():\n            server_ip, server_port = server_endpoint\n            for line in dump.split(\"\\n\"):\n                mat = self.IP_PORT_REGEX.match(line)\n                if mat is None:\n                    continue\n                ip, port, sid = mat.groups()\n                info_by_id[sid] = ClientInfo(sid, ip, port, server_ip, server_port)\n\n        return info_by_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, path, verbose=False):\n        if path in self._by_path:\n            self.remove(path)\n        else:\n            self.add(path, verbose)", "response": "Update the internal cache with the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zk_client(host, scheme, credential):\n\n    if not re.match(r\".*:\\d+$\", host):\n        host = \"%s:%d\" % (host, DEFAULT_ZK_PORT)\n\n    client = KazooClient(hosts=host)\n    client.start()\n\n    if scheme != \"\":\n        client.add_auth(scheme, credential)\n\n    return client", "response": "returns a connected and possibly authenticated Zookeeper client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new instance of the class from a string.", "response": "def from_string(cls, string, exists=False, asynchronous=False, verbose=False):\n        \"\"\"\n        if exists is bool, then check it either exists or it doesn't.\n        if exists is None, we don't care.\n        \"\"\"\n        result = cls.parse(string)\n\n        if result.scheme not in cls.TYPES:\n            raise CopyError(\"Invalid scheme: %s\" % (result.scheme))\n\n        return cls.TYPES[result.scheme](result, exists, asynchronous, verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nskipping ephemeral znodes since there's no point in copying those", "response": "def zk_walk(self, root_path, branch_path):\n        \"\"\"\n        skip ephemeral znodes since there's no point in copying those\n        \"\"\"\n        full_path = os.path.join(root_path, branch_path) if branch_path else root_path\n\n        try:\n            children = self.client.get_children(full_path)\n        except NoNodeError:\n            children = set()\n        except NoAuthError:\n            raise AuthError(\"read children\", full_path)\n\n        for child in children:\n            child_path = os.path.join(branch_path, child) if branch_path else child\n\n            try:\n                stat = self.client.exists(os.path.join(root_path, child_path))\n            except NoAuthError:\n                raise AuthError(\"read\", child)\n\n            if stat is None or stat.ephemeralOwner != 0:\n                continue\n            yield child_path\n            for new_path in self.zk_walk(root_path, child_path):\n                yield new_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the path value to the file", "response": "def write_path(self, path_value):\n        \"\"\" this will overwrite dst path - be careful \"\"\"\n        parent_dir = os.path.dirname(self.path)\n        try:\n            os.makedirs(parent_dir)\n        except OSError:\n            pass\n        with open(self.path, \"w\") as fph:\n            fph.write(path_value.value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of all the keys that match the exclude_recurse parameter.", "response": "def get(self, exclude_recurse=None):\n        \"\"\"\n        Paths matching exclude_recurse will not be recursed.\n        \"\"\"\n        reqs = Queue()\n        pending = 1\n        path = self.path\n        zk = self.zk\n\n        def child_of(path):\n            return zk.get_children_async(path)\n\n        def dispatch(path):\n            return Request(path, child_of(path))\n\n        stat = zk.exists(path)\n        if stat is None or stat.numChildren == 0:\n            return\n\n        reqs.put(dispatch(path))\n\n        while pending:\n            req = reqs.get()\n\n            try:\n                children = req.value\n                for child in children:\n                    cpath = os.path.join(req.path, child)\n                    if exclude_recurse is None or exclude_recurse not in child:\n                        pending += 1\n                        reqs.put(dispatch(cpath))\n                    yield cpath\n            except (NoNodeError, NoAuthError): pass\n\n            pending -= 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a watch for a path and its children depending on the value of children.", "response": "def add(self, path, debug, children):\n        \"\"\"\n        Set a watch for path and (maybe) its children depending on the value\n        of children:\n\n         -1:  all children\n          0:  no children\n        > 0:  up to level depth children\n\n        If debug is true, print each received events.\n        \"\"\"\n        if path in self._stats_by_path:\n            print(\"%s is already being watched\" % (path))\n            return\n\n        # we can't watch child paths of what's already being watched,\n        # because that generates a race between firing and resetting\n        # watches for overlapping paths.\n        if \"/\" in self._stats_by_path:\n            print(\"/ is already being watched, so everything is watched\")\n            return\n\n        for epath in self._stats_by_path:\n            if epath.startswith(path):\n                print(self.PARENT_ERR % (path, epath))\n                return\n\n            if path.startswith(epath):\n                print(self.CHILD_ERR % (path, epath))\n                return\n\n        self._stats_by_path[path] = PathStats(debug)\n        self._watch(path, 0, children)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _watch(self, path, current_level, max_level):\n\n        # ephemeral znodes can't have children, so skip them\n        stat = self._client.exists(path)\n        if stat is None or stat.ephemeralOwner != 0:\n            return\n\n        try:\n            children = self._client.get_children(path, self._watcher)\n        except NoNodeError:\n            children = []\n\n        if max_level >= 0 and current_level + 1 > max_level:\n            return\n\n        for child in children:\n            self._watch(os.path.join(path, child), current_level + 1, max_level)", "response": "Recursively watch the znode at the given path and return the entry ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value of idx in the plist.", "response": "def safe_list_set(plist, idx, fill_with, value):\n    \"\"\"\n    Sets:\n\n    ```\n    plist[idx] = value\n    ```\n\n    If len(plist) is smaller than what idx is trying\n    to dereferece, we first grow plist to get the needed\n    capacity and fill the new elements with fill_with\n    (or fill_with(), if it's a callable).\n    \"\"\"\n\n    try:\n        plist[idx] = value\n        return\n    except IndexError:\n        pass\n\n    # Fill in the missing positions. Handle negative indexes.\n    end = idx + 1 if idx >= 0 else abs(idx)\n    for _ in range(len(plist), end):\n        if callable(fill_with):\n            plist.append(fill_with())\n        else:\n            plist.append(fill_with)\n\n    plist[idx] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_type(value, ptype):\n    if ptype == 'str':\n        return str(value)\n    elif ptype == 'int':\n        return int(value)\n    elif ptype == 'float':\n        return float(value)\n    elif ptype == 'bool':\n        if value.lower() == 'true':\n            return True\n        elif value.lower() == 'false':\n            return False\n        raise ValueError('Bad bool value: %s' % value)\n    elif ptype == 'json':\n        return json.loads(value)\n\n    return ValueError('Unknown type')", "response": "Convert value to ptype"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(cls, keystr):\n        regex = r'#{\\s*(%s)\\s*}' % cls.ALLOWED_KEY\n        return re.match(regex, keystr).group(1)", "response": "extracts the key from a string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_one(cls, keystr):\n        regex = r'%s$' % cls.ALLOWED_KEY\n        if re.match(regex, keystr) is None:\n            raise cls.Bad(\"Bad key syntax for: %s. Should be: key1.key2...\" % (keystr))\n\n        return True", "response": "validates one key string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_template(cls, template):\n        regex = r'#{\\s*%s\\s*}' % cls.ALLOWED_KEY\n        keys = re.findall(regex, template)\n        if len(keys) == 0:\n            raise cls.Bad(\"Bad keys template: %s. Should be: \\\"%s\\\"\" % (\n                template, \"a = #{key1}, b = #{key2.key3} ...\"))\n        return keys", "response": "Extract keys out of a template string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(cls, keystr):\n        if \"#{\" in keystr:\n            # it's a template with keys vars\n            keys = cls.from_template(keystr)\n            for k in keys:\n                cls.validate_one(cls.extract(k))\n        else:\n            # plain keys str\n            cls.validate_one(keystr)", "response": "Validates that the keystr is valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the value corresponding to keys from obj AttributeNames raises a KeyError if the key is not in keys", "response": "def fetch(cls, obj, keys):\n        \"\"\"\n        fetches the value corresponding to keys from obj\n        \"\"\"\n        current = obj\n        for key in keys.split(\".\"):\n            if type(current) == list:\n                try:\n                    key = int(key)\n                except TypeError:\n                    raise cls.Missing(key)\n\n            try:\n                current = current[key]\n            except (IndexError, KeyError, TypeError) as ex:\n                raise cls.Missing(key)\n\n        return current"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef value(cls, obj, keystr):\n        if \"#{\" in keystr:\n            # it's a template with keys vars\n            keys = cls.from_template(keystr)\n            for k in keys:\n                v = cls.fetch(obj, cls.extract(k))\n                keystr = keystr.replace(k, str(v))\n\n            value = keystr\n        else:\n            # plain keys str\n            value = cls.fetch(obj, keystr)\n\n        return value", "response": "Gets the value corresponding to the keys in obj."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value for the given keys on obj.", "response": "def set(cls, obj, keys, value, fill_list_value=None):\n        \"\"\"\n        sets the value for the given keys on obj. if any of the given\n        keys does not exist, create the intermediate containers.\n        \"\"\"\n        current = obj\n        keys_list = keys.split(\".\")\n\n        for idx, key in enumerate(keys_list, 1):\n            if type(current) == list:\n                # Validate this key works with a list.\n                try:\n                    key = int(key)\n                except ValueError:\n                    raise cls.Missing(key)\n\n            try:\n                # This is the last key, so set the value.\n                if idx == len(keys_list):\n                    if type(current) == list:\n                        safe_list_set(\n                            current,\n                            key,\n                            lambda: copy.copy(fill_list_value),\n                            value\n                        )\n                    else:\n                        current[key] = value\n\n                    # done.\n                    return\n\n                # More keys left, ensure we have a container for this key.\n                if type(key) == int:\n                    try:\n                        current[key]\n                    except IndexError:\n                        # Create a list for this key.\n                        cnext = container_for_key(keys_list[idx])\n                        if type(cnext) == list:\n                            def fill_with():\n                                return []\n                        else:\n                            def fill_with():\n                                return {}\n\n                        safe_list_set(\n                            current,\n                            key,\n                            fill_with,\n                            [] if type(cnext) == list else {}\n                        )\n                else:\n                    if key not in current:\n                        # Create a list for this key.\n                        current[key] = container_for_key(keys_list[idx])\n\n                # Move on to the next key.\n                current = current[key]\n            except (IndexError, KeyError, TypeError):\n                raise cls.Missing(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pretty_bytes(num):\n    for unit in ['', 'KB', 'MB', 'GB']:\n        if num < 1024.0:\n            if unit == '':\n                return \"%d\" % (num)\n            else:\n                return \"%3.1f%s\" % (num, unit)\n        num /= 1024.0\n    return \"%3.1f%s\" % (num, 'TB')", "response": "pretty print the given number of bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_bytes(value):\n    vtype = type(value)\n\n    if vtype == bytes or vtype == type(None):\n        return value\n\n    try:\n        return vtype.encode(value)\n    except UnicodeEncodeError:\n        pass\n    return value", "response": "str to bytes (py3k)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef valid_ipv4(ip):\n    match =  _valid_ipv4.match(ip)\n    if match is None:\n        return False\n\n    octets  = match.groups()\n    if len(octets) != 4:\n        return False\n\n    first = int(octets[0])\n    if first < 1 or first > 254:\n        return False\n\n    for i in range(1, 4):\n        octet = int(octets[i])\n        if octet < 0 or octet > 255:\n            return False\n\n    return True", "response": "check if ip is a valid ipv4"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a hostname is a valid one", "response": "def valid_host(host):\n    \"\"\" check valid hostname \"\"\"\n    for part in host.split(\".\"):\n        if not _valid_host_part.match(part):\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that a hostname or an IP or a port is valid.", "response": "def valid_host_with_port(hostport):\n    \"\"\"\n    matches hostname or an IP, optionally with a port\n    \"\"\"\n    host, port = hostport.rsplit(\":\", 1) if \":\" in hostport else (hostport, None)\n\n    # first, validate host or IP\n    if not valid_ipv4(host) and not valid_host(host):\n        return False\n\n    # now, validate port\n    if port is not None and not valid_port(port):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef valid_hosts(hosts):\n    if _empty.match(hosts):\n        return False\n\n    for host in hosts.split(\",\"):\n        if not valid_host_with_port(host):\n            return False\n\n    return True", "response": "Checks if a comma separated list of hosts is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a path into parent and child", "response": "def split(path):\n    \"\"\"\n    splits path into parent, child\n    \"\"\"\n    if path == '/':\n        return ('/', None)\n\n    parent, child = path.rsplit('/', 1)\n\n    if parent == '':\n        parent = '/'\n\n    return (parent, child)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ips(host, port):\n    ips = set()\n\n    for af_type in (socket.AF_INET, socket.AF_INET6):\n        try:\n            records = socket.getaddrinfo(host, port, af_type, socket.SOCK_STREAM)\n            ips.update(rec[4][0] for rec in records)\n        except socket.gaierror as ex:\n            pass\n\n    return ips", "response": "get all IP addresses from a given host and port"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hosts_to_endpoints(hosts, port=2181):\n    endpoints = []\n    for host in hosts.split(\",\"):\n        endpoints.append(tuple(host.rsplit(\":\", 1)) if \":\" in host else (host, port))\n    return endpoints", "response": "return a list of tuples from a given host and port"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding outliers in a list of items by delta", "response": "def find_outliers(group, delta):\n    \"\"\"\n    given a list of values, find those that are apart from the rest by\n    `delta`. the indexes for the outliers is returned, if any.\n\n    examples:\n\n    values = [100, 6, 7, 8, 9, 10, 150]\n    find_outliers(values, 5) -> [0, 6]\n\n    values = [5, 6, 5, 4, 5]\n    find_outliers(values, 3) -> []\n\n    \"\"\"\n    with_pos = sorted([pair for pair in enumerate(group)], key=lambda p: p[1])\n    outliers_start = outliers_end = -1\n\n    for i in range(0, len(with_pos) - 1):\n        cur = with_pos[i][1]\n        nex = with_pos[i + 1][1]\n\n        if nex - cur > delta:\n            # depending on where we are, outliers are the remaining\n            # items or the ones that we've already seen.\n            if i < (len(with_pos) - i):\n                # outliers are close to the start\n                outliers_start, outliers_end = 0, i + 1\n            else:\n                # outliers are close to the end\n                outliers_start, outliers_end = i + 1, len(with_pos)\n\n            break\n\n    if outliers_start != -1:\n        return [with_pos[i][0] for i in range(outliers_start, outliers_end)]\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering out lines that don t include match", "response": "def get_matching(content, match):\n    \"\"\" filters out lines that don't include match \"\"\"\n    if match != \"\":\n        lines = [line for line in content.split(\"\\n\") if match in line]\n        content = \"\\n\".join(lines)\n    return content"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(action, params=None, version=6):\n        if params is None:\n            params = dict()\n        to_send = {\n            'action': action,\n            'version': version,\n            'params': params\n        }\n\n        r = requests.post(AnkiConnect.URL, json=to_send)\n        return r.json()", "response": "This function is used to send a POST request to the Anki - Connect API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the representative json for a single object.", "response": "def get_representative_json(file_input=None,\n                            formatted=False, annotate_is_json=False,\n                            sampling_substitution_regex=('(.+)', '\\\\1_sample'),\n                            do_not_sample=('sqlite_stat1', ),\n                            sampling_limits=None):\n    \"\"\"\n    :param None|str file_input:\n    :param bool formatted:\n    :param bool annotate_is_json:\n    :param tuple sampling_substitution_regex: to shorten string by one, try ('(.+).{1}', '\\\\1') or ('(.+)s', '\\\\1')\n    :param list|tuple do_not_sample:\n    :param None|dict sampling_limits:\n    :return:\n    \"\"\"\n    if file_input is None:\n        file_input = get_collection_path()\n\n    source = file_input\n\n    if sampling_limits is None:\n        sampling_limits = {\n            'notes': 10,\n            'cards': 10\n        }\n\n    if os.path.splitext(file_input)[1] == '.apkg':\n        from AnkiTools.convert import anki_convert\n\n        tempdir = mkdtemp()\n        temp_anki2 = os.path.join(tempdir, 'temp.anki2')\n        anki_convert(file_input, out_file=temp_anki2)\n        file_input = temp_anki2\n\n    output_json = OrderedDict(\n        _meta=OrderedDict(\n            generated=datetime.fromtimestamp(datetime.now().timestamp()).isoformat(),\n            source=os.path.abspath(source),\n            data=OrderedDict()\n        )\n    )\n\n    with sqlite3.connect(file_input) as conn:\n        cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        for row in cursor:\n            table_name = row[0]\n            key = table_name\n\n            output = list(read_anki_table(conn, table_name))\n\n            if table_name not in output_json['_meta']['data'].keys():\n                output_json['_meta']['data'][table_name] = OrderedDict()\n\n            output_json['_meta']['data'][table_name]['number_of_entries'] = len(output)\n\n            if len(output) >= 1:\n                if len(output) > 1:\n                    if table_name in do_not_sample:\n                        output_json[key] = output\n                    else:\n                        re_match, re_replace = sampling_substitution_regex\n                        key = re.sub(re_match, re_replace, key)\n                        output_json[key] = random.sample(output, sampling_limits.get(table_name, 10))\n                else:\n                    output_json[key] = output[0]\n\n                if formatted:\n                    to_format = output_json[key]\n                    if isinstance(output_json[key], (dict, OrderedDict)):\n                        _format_representative_json(to_format, annotate_is_json)\n                    else:\n                        for item in to_format:\n                            _format_representative_json(item, annotate_is_json)\n            else:\n                output_json[key] = None\n\n    return output_json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new field with the given name and ordering.", "response": "def new_field(field_name: str, ordering: int, **kwargs):\n        \"\"\"\n        Fields have no unique ID.\n        :param field_name:\n        :param ordering:\n        :param kwargs:\n        :return:\n        \"\"\"\n        field = dict([\n            ('name', field_name),\n            ('rtl', False),\n            ('sticky', False),\n            ('media', []),\n            ('ord', ordering),\n            ('font', 'Arial'),\n            ('size', 12)\n        ])\n\n        for k, v in field.items():\n            if k in kwargs.keys():\n                field[k] = kwargs[k]\n\n        return field"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new template with the given name and ordering and kwargs.", "response": "def new_template(template_name: str, ordering: int, formatting: dict=None, **kwargs):\n        \"\"\"\n        Templates have no unique ID.\n        :param template_name:\n        :param ordering:\n        :param formatting:\n        :param kwargs:\n        :return:\n        \"\"\"\n        if formatting is not None:\n            kwargs.update(formatting)\n\n        template = dict([\n            ('name', template_name),\n            ('qfmt', DEFAULT_TEMPLATE['qfmt']),\n            ('did', None),\n            ('bafmt', DEFAULT_TEMPLATE['bafmt']),\n            ('afmt', DEFAULT_TEMPLATE['afmt']),\n            ('ord', ordering),\n            ('bqfmt', DEFAULT_TEMPLATE['bqfmt'])\n        ])\n\n        for k, v in template.items():\n            if k in kwargs.keys():\n                template[k] = kwargs[k]\n\n        return template"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_remote_db(self):\n\n        signature_version = self.settings_dict.get(\"SIGNATURE_VERSION\", \"s3v4\")\n        s3 = boto3.resource(\n            's3',\n            config=botocore.client.Config(signature_version=signature_version),\n        )\n\n        if '/tmp/' not in self.settings_dict['NAME']:\n            try:\n                etag = ''\n                if os.path.isfile('/tmp/' + self.settings_dict['NAME']):\n                    m = hashlib.md5()\n                    with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                        m.update(f.read())\n\n                    # In general the ETag is the md5 of the file, in some cases it's not,\n                    # and in that case we will just need to reload the file, I don't see any other way\n                    etag = m.hexdigest()\n\n                obj = s3.Object(self.settings_dict['BUCKET'], self.settings_dict['NAME'])\n                obj_bytes = obj.get(IfNoneMatch=etag)[\"Body\"]  # Will throw E on 304 or 404\n\n                with open('/tmp/' + self.settings_dict['NAME'], 'wb') as f:\n                    f.write(obj_bytes.read())\n\n                m = hashlib.md5()\n                with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                    m.update(f.read())\n\n                self.db_hash = m.hexdigest()\n\n            except botocore.exceptions.ClientError as e:\n                if e.response['Error']['Code'] == \"304\":\n                    logging.debug(\"ETag matches md5 of local copy, using local copy of DB!\")\n                    self.db_hash = etag\n                else:\n                    logging.debug(\"Couldn't load remote DB object.\")\n            except Exception as e:\n                # Weird one\n                logging.debug(e)\n\n        # SQLite DatabaseWrapper will treat our tmp as normal now\n        # Check because Django likes to call this function a lot more than it should\n        if '/tmp/' not in self.settings_dict['NAME']:\n            self.settings_dict['REMOTE_NAME'] = self.settings_dict['NAME']\n            self.settings_dict['NAME'] = '/tmp/' + self.settings_dict['NAME']\n\n        # Make sure it exists if it doesn't yet\n        if not os.path.isfile(self.settings_dict['NAME']):\n            open(self.settings_dict['NAME'], 'a').close()\n\n        logging.debug(\"Loaded remote DB!\")", "response": "Load remote S3 DB and set self. db_hash to the hash of the DB object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef id(self): # pylint: disable=invalid-name,too-many-branches,too-many-return-statements\n        # There are some times we want to trick the platform detection\n        # say if a raspberry pi doesn't have the right ID, or for testing\n        try:\n            return os.environ['BLINKA_FORCECHIP']\n        except KeyError: # no forced chip, continue with testing!\n            pass\n\n        # Special case, if we have an environment var set, we could use FT232H\n        try:\n            if os.environ['BLINKA_FT232H']:\n                # we can't have ftdi1 as a dependency cause its wierd\n                # to install, sigh.\n                import ftdi1 as ftdi # pylint: disable=import-error\n                try:\n                    ctx = None\n                    ctx = ftdi.new()  # Create a libftdi context.\n                    # Enumerate FTDI devices.\n                    count, _ = ftdi.usb_find_all(ctx, 0, 0)\n                    if count < 0:\n                        raise RuntimeError('ftdi_usb_find_all returned error %d : %s' %\n                                           count, ftdi.get_error_string(self._ctx))\n                    if count == 0:\n                        raise RuntimeError('BLINKA_FT232H environment variable' + \\\n                                           'set, but no FT232H device found')\n                finally:\n                    # Make sure to clean up list and context when done.\n                    if ctx is not None:\n                        ftdi.free(ctx)\n            return FT232H\n        except KeyError: # no FT232H environment var\n            pass\n\n        platform = sys.platform\n        if platform == \"linux\" or platform == \"linux2\":\n            return self._linux_id()\n        if platform == \"esp8266\":\n            return ESP8266\n        if platform == \"samd21\":\n            return SAMD21\n        if platform == \"pyboard\":\n            return STM32\n        # nothing found!\n        return None", "response": "Return a unique ID for the detected chip."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to detect the CPU on a computer running the Linux kernel.", "response": "def _linux_id(self):\n        \"\"\"Attempt to detect the CPU on a computer running the Linux kernel.\"\"\"\n        linux_id = None\n\n        hardware = self.detector.get_cpuinfo_field(\"Hardware\")\n\n        if hardware is None:\n            vendor_id = self.detector.get_cpuinfo_field(\"vendor_id\")\n            if vendor_id in (\"GenuineIntel\", \"AuthenticAMD\"):\n                linux_id = GENERIC_X86\n\n            compatible = self.detector.get_device_compatible()\n            if compatible and 'tegra' in compatible:\n                if 'cv' in compatible or 'nano' in compatible:\n                    linux_id = T210\n                elif 'quill' in compatible:\n                    linux_id = T186\n                elif 'xavier' in compatible:\n                    linux_id = T194\n        elif hardware in (\"BCM2708\", \"BCM2709\", \"BCM2835\"):\n            linux_id = BCM2XXX\n        elif \"AM33XX\" in hardware:\n            linux_id = AM33XX\n        elif \"sun8i\" in hardware:\n            linux_id = SUN8I\n        elif \"ODROIDC\" in hardware:\n            linux_id = S805\n        elif \"ODROID-C2\" in hardware:\n            linux_id = S905\n        elif \"SAMA5\" in hardware:\n            linux_id = SAMA5\n\n        return linux_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef id(self):\n        # There are some times we want to trick the platform detection\n        # say if a raspberry pi doesn't have the right ID, or for testing\n        try:\n            return os.environ['BLINKA_FORCEBOARD']\n        except KeyError: # no forced board, continue with testing!\n            pass\n\n        chip_id = self.detector.chip.id\n        board_id = None\n\n        if chip_id == ap_chip.BCM2XXX:\n            board_id = self._pi_id()\n        elif chip_id == ap_chip.AM33XX:\n            board_id = self._beaglebone_id()\n        elif chip_id == ap_chip.GENERIC_X86:\n            board_id = GENERIC_LINUX_PC\n        elif chip_id == ap_chip.SUN8I:\n            board_id = self._armbian_id()\n        elif chip_id == ap_chip.SAMA5:\n            board_id = self._sama5_id()\n        elif chip_id == ap_chip.ESP8266:\n            board_id = FEATHER_HUZZAH\n        elif chip_id == ap_chip.SAMD21:\n            board_id = FEATHER_M0_EXPRESS\n        elif chip_id == ap_chip.STM32:\n            board_id = PYBOARD\n        elif chip_id == ap_chip.S805:\n            board_id = ODROID_C1\n        elif chip_id == ap_chip.S905:\n            board_id = ODROID_C2\n        elif chip_id == ap_chip.FT232H:\n            board_id = FTDI_FT232H\n        elif chip_id in (ap_chip.T210, ap_chip.T186, ap_chip.T194):\n            board_id = self._tegra_id()\n        return board_id", "response": "Return a unique id for the detected board if any."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrying to detect id of a Raspberry Pi.", "response": "def _pi_id(self):\n        \"\"\"Try to detect id of a Raspberry Pi.\"\"\"\n        # Check for Pi boards:\n        pi_rev_code = self._pi_rev_code()\n        if pi_rev_code:\n            for model, codes in _PI_REV_CODES.items():\n                if pi_rev_code in codes:\n                    return model\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pi_rev_code(self):\n        # 2708 is Pi 1\n        # 2709 is Pi 2\n        # 2835 is Pi 3 (or greater) on 4.9.x kernel\n        # Anything else is not a Pi.\n        if self.detector.chip.id != ap_chip.BCM2XXX:\n            # Something else, not a Pi.\n            return None\n        return self.detector.get_cpuinfo_field('Revision')", "response": "Attempt to find a Raspberry Pi revision code for this board."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to detect id of a Beaglebone.", "response": "def _beaglebone_id(self):\n        \"\"\"Try to detect id of a Beaglebone.\"\"\"\n        try:\n            with open(\"/sys/bus/nvmem/devices/0-00500/nvmem\", \"rb\") as eeprom:\n                eeprom_bytes = eeprom.read(16)\n        except FileNotFoundError:\n            return None\n\n        if eeprom_bytes[:4] != b'\\xaaU3\\xee':\n            return None\n\n        id_string = eeprom_bytes[4:].decode(\"ascii\")\n        for model, bb_ids in _BEAGLEBONE_BOARD_IDS.items():\n            for bb_id in bb_ids:\n                if id_string == bb_id[1]:\n                    return model\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tegra_id(self):\n        board_value = self.detector.get_device_model()\n        if 'tx1' in board_value:\n            return JETSON_TX1\n        elif 'quill' in board_value:\n            return JETSON_TX2\n        elif 'xavier' in board_value:\n            return JETSON_XAVIER\n        elif 'nano' in board_value:\n            return JETSON_NANO\n        return None", "response": "Try to detect the id of a arch64 board."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether the current board is any embedded Linux device.", "response": "def any_embedded_linux(self):\n        \"\"\"Check whether the current board is any embedded Linux device.\"\"\"\n        return self.any_raspberry_pi or self.any_beaglebone or \\\n         self.any_orange_pi or self.any_giant_board or self.any_jetson_board"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches the cpuinfo file for a field and return its value if found otherwise None.", "response": "def get_cpuinfo_field(self, field):\n        \"\"\"\n        Search /proc/cpuinfo for a field and return its value, if found,\n        otherwise None.\n        \"\"\"\n        # Match a line like 'Hardware   : BCM2709':\n        pattern = r'^' + field + r'\\s+:\\s+(.*)$'\n\n        with open('/proc/cpuinfo', 'r') as infile:\n            cpuinfo = infile.read().split('\\n')\n            for line in cpuinfo:\n                match = re.search(pattern, line, flags=re.IGNORECASE)\n                if match:\n                    return match.group(1)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching the armbian - release file for a field and return its value. If it does not exist return None.", "response": "def get_armbian_release_field(self, field):\n        \"\"\"\n        Search /etc/armbian-release, if it exists, for a field and return its\n        value, if found, otherwise None.\n        \"\"\"\n        field_value = None\n        pattern = r'^' + field + r'=(.*)'\n        try:\n            with open(\"/etc/armbian-release\", 'r') as release_file:\n                armbian = release_file.read().split('\\n')\n                for line in armbian:\n                    match = re.search(pattern, line)\n                    if match:\n                        field_value = match.group(1)\n        except FileNotFoundError:\n            pass\n\n        return field_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def middleware(request, handler):\n    # Create X-Ray headers\n    xray_header = construct_xray_header(request.headers)\n    # Get name of service or generate a dynamic one from host\n    name = calculate_segment_name(request.headers['host'].split(':', 1)[0], xray_recorder)\n\n    sampling_req = {\n        'host': request.headers['host'],\n        'method': request.method,\n        'path': request.path,\n        'service': name,\n    }\n\n    sampling_decision = calculate_sampling_decision(\n        trace_header=xray_header,\n        recorder=xray_recorder,\n        sampling_req=sampling_req,\n    )\n\n    # Start a segment\n    segment = xray_recorder.begin_segment(\n        name=name,\n        traceid=xray_header.root,\n        parent_id=xray_header.parent,\n        sampling=sampling_decision,\n    )\n\n    segment.save_origin_trace_header(xray_header)\n    # Store request metadata in the current segment\n    segment.put_http_meta(http.URL, str(request.url))\n    segment.put_http_meta(http.METHOD, request.method)\n\n    if 'User-Agent' in request.headers:\n        segment.put_http_meta(http.USER_AGENT, request.headers['User-Agent'])\n\n    if 'X-Forwarded-For' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['X-Forwarded-For'])\n        segment.put_http_meta(http.X_FORWARDED_FOR, True)\n    elif 'remote_addr' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['remote_addr'])\n    else:\n        segment.put_http_meta(http.CLIENT_IP, request.remote)\n\n    try:\n        # Call next middleware or request handler\n        response = await handler(request)\n    except HTTPException as exc:\n        # Non 2XX responses are raised as HTTPExceptions\n        response = exc\n        raise\n    except Exception as err:\n        # Store exception information including the stacktrace to the segment\n        response = None\n        segment.put_http_meta(http.STATUS, 500)\n        stack = stacktrace.get_stacktrace(limit=xray_recorder.max_trace_back)\n        segment.add_exception(err, stack)\n        raise\n    finally:\n        if response is not None:\n            segment.put_http_meta(http.STATUS, response.status)\n            if 'Content-Length' in response.headers:\n                length = int(response.headers['Content-Length'])\n                segment.put_http_meta(http.CONTENT_LENGTH, length)\n\n            header_str = prepare_response_header(xray_header, segment)\n            response.headers[http.XRAY_HEADER] = header_str\n\n        xray_recorder.end_segment()\n\n    return response", "response": "Main middleware function deals with all the X - Ray segment logic\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_id(self):\n        return \"%s%s%s%s%s\" % (TraceId.VERSION, TraceId.DELIMITER,\n                               format(self.start_time, 'x'),\n                               TraceId.DELIMITER, self.__number)", "response": "Convert TraceId object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npatches the built - in httplib. client methods for tracing.", "response": "def patch():\n    \"\"\"\n    patch the built-in `urllib/httplib/httplib.client` methods for tracing.\n    \"\"\"\n    if getattr(httplib, PATCH_FLAG, False):\n        return\n    # we set an attribute to avoid multiple wrapping\n    setattr(httplib, PATCH_FLAG, True)\n\n    wrapt.wrap_function_wrapper(\n        httplib_client_module,\n        'HTTPConnection._send_request',\n        _send_request\n    )\n\n    wrapt.wrap_function_wrapper(\n        httplib_client_module,\n        'HTTPConnection.getresponse',\n        _xray_traced_http_getresponse\n    )\n\n    wrapt.wrap_function_wrapper(\n        httplib_client_module,\n        'HTTPResponse.read',\n        _xray_traced_http_client_read\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpatch():\n    _PATCHED_MODULES.discard('httplib')\n    setattr(httplib, PATCH_FLAG, False)\n    # _send_request encapsulates putrequest, putheader[s], and endheaders\n    unwrap(httplib.HTTPConnection, '_send_request')\n    unwrap(httplib.HTTPConnection, 'getresponse')\n    unwrap(httplib.HTTPResponse, 'read')", "response": "Unpatch any previously patched modules."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef task_factory(loop, coro):\n    task = asyncio.Task(coro, loop=loop)\n    if task._source_traceback:  # flake8: noqa\n        del task._source_traceback[-1]  # flake8: noqa\n\n    # Share context with new task if possible\n    current_task = asyncio.Task.current_task(loop=loop)\n    if current_task is not None and hasattr(current_task, 'context'):\n        setattr(task, 'context', current_task.context)\n\n    return task", "response": "Task factory function\n\n    Fuction closely mirrors the logic inside of\n    asyncio.BaseEventLoop.create_task. Then if there is a current\n    task and the current task has a context then share that context\n    with the new task"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match(self, sampling_req):\n        if sampling_req is None:\n            return False\n\n        host = sampling_req.get('host', None)\n        method = sampling_req.get('method', None)\n        path = sampling_req.get('path', None)\n        service = sampling_req.get('service', None)\n        service_type = sampling_req.get('service_type', None)\n\n        return (not host or wildcard_match(self._host, host)) \\\n            and (not method or wildcard_match(self._method, method)) \\\n            and (not path or wildcard_match(self._path, path)) \\\n            and (not service or wildcard_match(self._service, service)) \\\n            and (not service_type or wildcard_match(self._service_type, service_type))", "response": "Determines whether or not this sampling rule applies to the incoming request based on some of the request s parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef snapshot_statistics(self):\n        with self._lock:\n\n            stats = {\n                'request_count': self.request_count,\n                'borrow_count': self.borrow_count,\n                'sampled_count': self.sampled_count,\n            }\n\n            self._reset_statistics()\n            return stats", "response": "Take a snapshot of request count for reporting\n        and reset those counters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, rule):\n        with self._lock:\n            self._request_count = rule.request_count\n            self._borrow_count = rule.borrow_count\n            self._sampled_count = rule.sampled_count\n            self._reservoir = rule.reservoir\n            rule.reservoir = None", "response": "Merge all stateful attributes from the old rule into the new rule."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if we can borrow or take one quota from .", "response": "def borrow_or_take(self, now, can_borrow):\n        \"\"\"\n        Decide whether to borrow or take one quota from\n        the reservoir. Return ``False`` if it can neither\n        borrow nor take. This method is thread-safe.\n        \"\"\"\n        with self._lock:\n            return self._borrow_or_take(now, can_borrow)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_quota(self, quota, TTL, interval):\n        if quota is not None:\n            self._quota = quota\n        if TTL is not None:\n            self._TTL = TTL\n        if interval is not None:\n            self._report_interval = interval / 10", "response": "Load new quota with a TTL and set the interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self, end_time=None):\n        self._check_ended()\n\n        if end_time:\n            self.end_time = end_time\n        else:\n            self.end_time = time.time()\n        self.in_progress = False", "response": "Close the trace entity by setting end_time to False."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_subsegment(self, subsegment):\n        self._check_ended()\n        subsegment.parent_id = self.id\n        self.subsegments.append(subsegment)", "response": "Add input subsegment as a child subsegment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds http related metadata.", "response": "def put_http_meta(self, key, value):\n        \"\"\"\n        Add http related metadata.\n\n        :param str key: Currently supported keys are:\n            * url\n            * method\n            * user_agent\n            * client_ip\n            * status\n            * content_length\n        :param value: status and content_length are int and for other\n            supported keys string should be used.\n        \"\"\"\n        self._check_ended()\n\n        if value is None:\n            return\n\n        if key == http.STATUS:\n            if isinstance(value, string_types):\n                value = int(value)\n            self.apply_status_code(value)\n\n        if key in http.request_keys:\n            if 'request' not in self.http:\n                self.http['request'] = {}\n            self.http['request'][key] = value\n        elif key in http.response_keys:\n            if 'response' not in self.http:\n                self.http['response'] = {}\n            self.http['response'][key] = value\n        else:\n            log.warning(\"ignoring unsupported key %s in http meta.\", key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an annotation to the internal dictionary.", "response": "def put_annotation(self, key, value):\n        \"\"\"\n        Annotate segment or subsegment with a key-value pair.\n        Annotations will be indexed for later search query.\n\n        :param str key: annotation key\n        :param object value: annotation value. Any type other than\n            string/number/bool will be dropped\n        \"\"\"\n        self._check_ended()\n\n        if not isinstance(key, string_types):\n            log.warning(\"ignoring non string type annotation key with type %s.\", type(key))\n            return\n\n        if not isinstance(value, annotation_value_types):\n            log.warning(\"ignoring unsupported annotation value type %s.\", type(value))\n            return\n\n        if any(character not in _valid_annotation_key_characters for character in key):\n            log.warning(\"ignoring annnotation with unsupported characters in key: '%s'.\", key)\n            return\n\n        self.annotations[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd metadata to segment or subsegment.", "response": "def put_metadata(self, key, value, namespace='default'):\n        \"\"\"\n        Add metadata to segment or subsegment. Metadata is not indexed\n        but can be later retrieved by BatchGetTraces API.\n\n        :param str namespace: optional. Default namespace is `default`.\n            It must be a string and prefix `AWS.` is reserved.\n        :param str key: metadata key under specified namespace\n        :param object value: any object that can be serialized into JSON string\n        \"\"\"\n        self._check_ended()\n\n        if not isinstance(namespace, string_types):\n            log.warning(\"ignoring non string type metadata namespace\")\n            return\n\n        if namespace.startswith('AWS.'):\n            log.warning(\"Prefix 'AWS.' is reserved, drop metadata with namespace %s\", namespace)\n            return\n\n        if self.metadata.get(namespace, None):\n            self.metadata[namespace][key] = value\n        else:\n            self.metadata[namespace] = {key: value}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the status code to the internal flag list.", "response": "def apply_status_code(self, status_code):\n        \"\"\"\n        When a trace entity is generated under the http context,\n        the status code will affect this entity's fault/error/throttle flags.\n        Flip these flags based on status code.\n        \"\"\"\n        self._check_ended()\n        if not status_code:\n            return\n\n        if status_code >= 500:\n            self.add_fault_flag()\n        elif status_code == 429:\n            self.add_throttle_flag()\n            self.add_error_flag()\n        elif status_code >= 400:\n            self.add_error_flag()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_exception(self, exception, stack, remote=False):\n        self._check_ended()\n        self.add_fault_flag()\n\n        if hasattr(exception, '_recorded'):\n            setattr(self, 'cause', getattr(exception, '_cause_id'))\n            return\n\n        exceptions = []\n        exceptions.append(Throwable(exception, stack, remote))\n\n        self.cause['exceptions'] = exceptions\n        self.cause['working_directory'] = os.getcwd()", "response": "Add an exception to the trace entities."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize to JSON document that can be accepted by the X - Ray backend service service.", "response": "def serialize(self):\n        \"\"\"\n        Serialize to JSON document that can be accepted by the\n        X-Ray backend service. It uses jsonpickle to perform\n        serialization.\n        \"\"\"\n        try:\n            return jsonpickle.encode(self, unpicklable=False)\n        except Exception:\n            log.exception(\"got an exception during serialization\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete empty properties before serialization to avoid empty values in the output json.", "response": "def _delete_empty_properties(self, properties):\n        \"\"\"\n        Delete empty properties before serialization to avoid\n        extra keys with empty values in the output json.\n        \"\"\"\n        if not self.parent_id:\n            del properties['parent_id']\n        if not self.subsegments:\n            del properties['subsegments']\n        if not self.aws:\n            del properties['aws']\n        if not self.http:\n            del properties['http']\n        if not self.cause:\n            del properties['cause']\n        if not self.annotations:\n            del properties['annotations']\n        if not self.metadata:\n            del properties['metadata']\n        properties.pop(ORIGIN_TRACE_HEADER_ATTR_KEY, None)\n\n        del properties['sampled']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reload_settings(*args, **kwargs):\n    global settings\n    setting, value = kwargs['setting'], kwargs['value']\n    if setting == XRAY_NAMESPACE:\n        settings = XRaySettings(value)", "response": "Reload X - Ray user settings upon Django server hot restart"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_subsegment(self, subsegment):\n        super(Segment, self).add_subsegment(subsegment)\n        self.increment()", "response": "Add input subsegment as a child subsegment and increment the reference counter and total subsegments counter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the reference of input subsegment.", "response": "def remove_subsegment(self, subsegment):\n        \"\"\"\n        Remove the reference of input subsegment.\n        \"\"\"\n        super(Segment, self).remove_subsegment(subsegment)\n        self.decrement_subsegments_size()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets user of a segment", "response": "def set_user(self, user):\n        \"\"\"\n        set user of a segment. One segment can only have one user.\n        User is indexed and can be later queried.\n        \"\"\"\n        super(Segment, self)._check_ended()\n        self.user = user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_rule_name(self, rule_name):\n        if not self.aws.get('xray', None):\n            self.aws['xray'] = {}\n        self.aws['xray']['sampling_rule_name'] = rule_name", "response": "Set the name of the matched centralized sampling rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts trace id entity id and sampling decision from the input entity and inject these information into the HTTP headers.", "response": "def inject_trace_header(headers, entity):\n    \"\"\"\n    Extract trace id, entity id and sampling decision\n    from the input entity and inject these information\n    to headers.\n\n    :param dict headers: http headers to inject\n    :param Entity entity: trace entity that the trace header\n        value generated from.\n    \"\"\"\n    if not entity:\n        return\n\n    if hasattr(entity, 'type') and entity.type == 'subsegment':\n        header = entity.parent_segment.get_origin_trace_header()\n    else:\n        header = entity.get_origin_trace_header()\n    data = header.data if header else None\n\n    to_insert = TraceHeader(\n        root=entity.trace_id,\n        parent=entity.id,\n        sampled=entity.sampled,\n        data=data,\n    )\n\n    value = to_insert.to_header_str()\n\n    headers[http.XRAY_HEADER] = value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_sampling_decision(trace_header, recorder, sampling_req):\n    if trace_header.sampled is not None and trace_header.sampled != '?':\n        return trace_header.sampled\n    elif not recorder.sampling:\n        return 1\n    else:\n        decision = recorder.sampler.should_trace(sampling_req)\n    return decision if decision else 0", "response": "Calculates the sampling decision from the trace header and the recorder and the sampling request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a TraceHeader object from a dictionary of headers.", "response": "def construct_xray_header(headers):\n    \"\"\"\n    Construct a ``TraceHeader`` object from dictionary headers\n    of the incoming request. This method should always return\n    a ``TraceHeader`` object regardless of tracing header's presence\n    in the incoming request.\n    \"\"\"\n    header_str = headers.get(http.XRAY_HEADER) or headers.get(http.ALT_XRAY_HEADER)\n    if header_str:\n        return TraceHeader.from_header_str(header_str)\n    else:\n        return TraceHeader()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the segment name based on recorder configuration and input host name.", "response": "def calculate_segment_name(host_name, recorder):\n    \"\"\"\n    Returns the segment name based on recorder configuration and\n    input host name. This is a helper generally used in web framework\n    middleware where a host name is available from incoming request's headers.\n    \"\"\"\n    if recorder.dynamic_naming:\n        return recorder.dynamic_naming.get_name(host_name)\n    else:\n        return recorder.service"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare a trace header to be inserted into response based on origin header and the request segment.", "response": "def prepare_response_header(origin_header, segment):\n    \"\"\"\n    Prepare a trace header to be inserted into response\n    based on original header and the request segment.\n    \"\"\"\n    if origin_header and origin_header.sampled == '?':\n        new_header = TraceHeader(root=segment.trace_id,\n                                 sampled=segment.sampled)\n    else:\n        new_header = TraceHeader(root=segment.trace_id)\n\n    return new_header.to_header_str()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the input string to snake - cased string.", "response": "def to_snake_case(name):\n    \"\"\"\n    Convert the input string to snake-cased string.\n    \"\"\"\n    s1 = first_cap_re.sub(r'\\1_\\2', name)\n    # handle acronym words\n    return all_cap_re.sub(r'\\1_\\2', s1).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unwrap(obj, attr):\n    f = getattr(obj, attr, None)\n    if f and isinstance(f, wrapt.ObjectProxy) and hasattr(f, '__wrapped__'):\n        setattr(obj, attr, f.__wrapped__)", "response": "Unwraps a wrapt attribute on an object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef patch():\n    if hasattr(botocore.client, '_xray_enabled'):\n        return\n    setattr(botocore.client, '_xray_enabled', True)\n\n    wrapt.wrap_function_wrapper(\n        'botocore.client',\n        'BaseClient._make_api_call',\n        _xray_traced_botocore,\n    )\n\n    wrapt.wrap_function_wrapper(\n        'botocore.endpoint',\n        'Endpoint.prepare_request',\n        inject_header,\n    )", "response": "Patch botocore client so it generates subsegments\n    when calling AWS services."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure(self, sampling=None, plugins=None,\n                  context_missing=None, sampling_rules=None,\n                  daemon_address=None, service=None,\n                  context=None, emitter=None, streaming=None,\n                  dynamic_naming=None, streaming_threshold=None,\n                  max_trace_back=None, sampler=None,\n                  stream_sql=True):\n        \"\"\"Configure global X-Ray recorder.\n\n        Configure needs to run before patching thrid party libraries\n        to avoid creating dangling subsegment.\n        :param bool sampling: If sampling is enabled, every time the recorder\n            creates a segment it decides whether to send this segment to\n            the X-Ray daemon. This setting is not used if the recorder\n            is running in AWS Lambda. The recorder always respect the incoming\n            sampling decisions regardless of this setting.\n        :param sampling_rules: Pass a set of local custom sampling rules.\n            Can be an absolute path of the sampling rule config json file\n            or a dictionary that defines those rules. This will also be the\n            fallback rules in case of centralized sampling opted-in while\n            the cetralized sampling rules are not available.\n        :param sampler: The sampler used to make sampling decisions. The SDK\n            provides two built-in samplers. One is centralized rules based and\n            the other is local rules based. The former is the default.\n        :param tuple plugins: plugins that add extra metadata to each segment.\n            Currently available plugins are EC2Plugin, ECS plugin and\n            ElasticBeanstalkPlugin.\n            If you want to disable all previously enabled plugins,\n            pass an empty tuple ``()``.\n        :param str context_missing: recorder behavior when it tries to mutate\n            a segment or add a subsegment but there is no active segment.\n            RUNTIME_ERROR means the recorder will raise an exception.\n            LOG_ERROR means the recorder will only log the error and\n            do nothing.\n        :param str daemon_address: The X-Ray daemon address where the recorder\n            sends data to.\n        :param str service: default segment name if creating a segment without\n            providing a name.\n        :param context: You can pass your own implementation of context storage\n            for active segment/subsegment by overriding the default\n            ``Context`` class.\n        :param emitter: The emitter that sends a segment/subsegment to\n            the X-Ray daemon. You can override ``UDPEmitter`` class.\n        :param dynamic_naming: a string that defines a pattern that host names\n            should match. Alternatively you can pass a module which\n            overrides ``DefaultDynamicNaming`` module.\n        :param streaming: The streaming module to stream out trace documents\n            when they grow too large. You can override ``DefaultStreaming``\n            class to have your own implementation of the streaming process.\n        :param streaming_threshold: If breaks within a single segment it will\n            start streaming out children subsegments. By default it is the\n            maximum number of subsegments within a segment.\n        :param int max_trace_back: The maxinum number of stack traces recorded\n            by auto-capture. Lower this if a single document becomes too large.\n        :param bool stream_sql: Whether SQL query texts should be streamed.\n\n        Environment variables AWS_XRAY_DAEMON_ADDRESS, AWS_XRAY_CONTEXT_MISSING\n        and AWS_XRAY_TRACING_NAME respectively overrides arguments\n        daemon_address, context_missing and service.\n        \"\"\"\n\n        if sampling is not None:\n            self.sampling = sampling\n        if sampler:\n            self.sampler = sampler\n        if service:\n            self.service = os.getenv(TRACING_NAME_KEY, service)\n        if sampling_rules:\n            self._load_sampling_rules(sampling_rules)\n        if emitter:\n            self.emitter = emitter\n        if daemon_address:\n            self.emitter.set_daemon_address(os.getenv(DAEMON_ADDR_KEY, daemon_address))\n        if context:\n            self.context = context\n        if context_missing:\n            self.context.context_missing = os.getenv(CONTEXT_MISSING_KEY, context_missing)\n        if dynamic_naming:\n            self.dynamic_naming = dynamic_naming\n        if streaming:\n            self.streaming = streaming\n        if streaming_threshold:\n            self.streaming_threshold = streaming_threshold\n        if type(max_trace_back) == int and max_trace_back >= 0:\n            self.max_trace_back = max_trace_back\n        if stream_sql is not None:\n            self.stream_sql = stream_sql\n\n        if plugins:\n            plugin_modules = get_plugin_modules(plugins)\n            for plugin in plugin_modules:\n                plugin.initialize()\n                if plugin.runtime_context:\n                    self._aws_metadata[plugin.SERVICE_NAME] = plugin.runtime_context\n                    self._origin = plugin.ORIGIN\n        # handling explicitly using empty list to clean up plugins.\n        elif plugins is not None:\n            self._aws_metadata = copy.deepcopy(XRAY_META)\n            self._origin = None\n\n        if type(self.sampler).__name__ == 'DefaultSampler':\n            self.sampler.load_settings(DaemonConfig(daemon_address),\n                                       self.context, self._origin)", "response": "Configure the global X - Ray recorder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef begin_segment(self, name=None, traceid=None,\n                      parent_id=None, sampling=None):\n        \"\"\"\n        Begin a segment on the current thread and return it. The recorder\n        only keeps one segment at a time. Create the second one without\n        closing existing one will overwrite it.\n\n        :param str name: the name of the segment\n        :param str traceid: trace id of the segment\n        :param int sampling: 0 means not sampled, 1 means sampled\n        \"\"\"\n        seg_name = name or self.service\n        if not seg_name:\n            raise SegmentNameMissingException(\"Segment name is required.\")\n\n        # Sampling decision is None if not sampled.\n        # In a sampled case it could be either a string or 1\n        # depending on if centralized or local sampling rule takes effect.\n        decision = True\n\n        # To disable the recorder, we set the sampling decision to always be false.\n        # This way, when segments are generated, they become dummy segments and are ultimately never sent.\n        # The call to self._sampler.should_trace() is never called either so the poller threads are never started.\n        if not global_sdk_config.sdk_enabled():\n            sampling = 0\n\n        # we respect the input sampling decision\n        # regardless of recorder configuration.\n        if sampling == 0:\n            decision = False\n        elif sampling:\n            decision = sampling\n        elif self.sampling:\n            decision = self._sampler.should_trace()\n\n        if not decision:\n            segment = DummySegment(seg_name)\n        else:\n            segment = Segment(name=seg_name, traceid=traceid,\n                              parent_id=parent_id)\n            self._populate_runtime_context(segment, decision)\n\n        self.context.put_segment(segment)\n        return segment", "response": "Begin a segment on the current thread and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef end_segment(self, end_time=None):\n        self.context.end_segment(end_time)\n        segment = self.current_segment()\n        if segment and segment.ready_to_send():\n            self._send_segment()", "response": "End the current segment and send it to X - Ray daemon."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the currently active segment.", "response": "def current_segment(self):\n        \"\"\"\n        Return the currently active segment. In a multithreading environment,\n        this will make sure the segment returned is the one created by the\n        same thread.\n        \"\"\"\n        entity = self.get_trace_entity()\n        if self._is_subsegment(entity):\n            return entity.parent_segment\n        else:\n            return entity"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef begin_subsegment(self, name, namespace='local'):\n\n        segment = self.current_segment()\n        if not segment:\n            log.warning(\"No segment found, cannot begin subsegment %s.\" % name)\n            return None\n\n        if not segment.sampled:\n            subsegment = DummySubsegment(segment, name)\n        else:\n            subsegment = Subsegment(name, namespace, segment)\n\n        self.context.put_subsegment(subsegment)\n\n        return subsegment", "response": "Begin a new subsegment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nend the current active subsegment.", "response": "def end_subsegment(self, end_time=None):\n        \"\"\"\n        End the current active subsegment. If this is the last one open\n        under its parent segment, the entire segment will be sent.\n\n        :param float end_time: subsegment compeletion in unix epoch in seconds.\n        \"\"\"\n        if not self.context.end_subsegment(end_time):\n            return\n\n        # if segment is already close, we check if we can send entire segment\n        # otherwise we check if we need to stream some subsegments\n        if self.current_segment().ready_to_send():\n            self._send_segment()\n        else:\n            self.stream_subsegments()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_annotation(self, key, value):\n        entity = self.get_trace_entity()\n        if entity and entity.sampled:\n            entity.put_annotation(key, value)", "response": "Annotate current active trace entity with a key - value pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd metadata to the current active trace entity.", "response": "def put_metadata(self, key, value, namespace='default'):\n        \"\"\"\n        Add metadata to the current active trace entity.\n        Metadata is not indexed but can be later retrieved\n        by BatchGetTraces API.\n\n        :param str namespace: optional. Default namespace is `default`.\n            It must be a string and prefix `AWS.` is reserved.\n        :param str key: metadata key under specified namespace\n        :param object value: any object that can be serialized into JSON string\n        \"\"\"\n        entity = self.get_trace_entity()\n        if entity and entity.sampled:\n            entity.put_metadata(key, value, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstream all closed subsegments to the daemon and remove reference to the parent segment.", "response": "def stream_subsegments(self):\n        \"\"\"\n        Stream all closed subsegments to the daemon\n        and remove reference to the parent segment.\n        No-op for a not sampled segment.\n        \"\"\"\n        segment = self.current_segment()\n\n        if self.streaming.is_eligible(segment):\n            self.streaming.stream(segment, self._stream_subsegment_out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend the current segment to X - Ray daemon if it is present and if it is sampled then clear the trace entities.", "response": "def _send_segment(self):\n        \"\"\"\n        Send the current segment to X-Ray daemon if it is present and\n        sampled, then clean up context storage.\n        The emitter will handle failures.\n        \"\"\"\n        segment = self.current_segment()\n\n        if not segment:\n            return\n\n        if segment.sampled:\n            self.emitter.send_entity(segment)\n        self.clear_trace_entities()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef applies(self, host, method, path):\n        return (not host or wildcard_match(self.host, host)) \\\n            and (not method or wildcard_match(self.method, method)) \\\n            and (not path or wildcard_match(self.path, path))", "response": "Determines whether or not this sampling rule applies to the incoming request based on some of the request s parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ready(self):\n        if not settings.AWS_XRAY_TRACING_NAME:\n            raise SegmentNameMissingException('Segment name is required.')\n\n        xray_recorder.configure(\n            daemon_address=settings.AWS_XRAY_DAEMON_ADDRESS,\n            sampling=settings.SAMPLING,\n            sampling_rules=settings.SAMPLING_RULES,\n            context_missing=settings.AWS_XRAY_CONTEXT_MISSING,\n            plugins=settings.PLUGINS,\n            service=settings.AWS_XRAY_TRACING_NAME,\n            dynamic_naming=settings.DYNAMIC_NAMING,\n            streaming_threshold=settings.STREAMING_THRESHOLD,\n            max_trace_back=settings.MAX_TRACE_BACK,\n            stream_sql=settings.STREAM_SQL,\n        )\n\n        if settings.PATCH_MODULES:\n            if settings.AUTO_PATCH_PARENT_SEGMENT_NAME is not None:\n                with xray_recorder.in_segment(settings.AUTO_PATCH_PARENT_SEGMENT_NAME):\n                    patch(settings.PATCH_MODULES, ignore_module_patterns=settings.IGNORE_MODULE_PATTERNS)\n            else:\n                patch(settings.PATCH_MODULES, ignore_module_patterns=settings.IGNORE_MODULE_PATTERNS)\n\n        # if turned on subsegment will be generated on\n        # built-in database and template rendering\n        if settings.AUTO_INSTRUMENT:\n            try:\n                patch_db()\n            except Exception:\n                log.debug('failed to patch Django built-in database')\n            try:\n                patch_template()\n            except Exception:\n                log.debug('failed to patch Django built-in template engine')", "response": "Called by XRay recorder when XRay is ready to be used."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting rule and target poller once X - Ray daemon is in place.", "response": "def start(self):\n        \"\"\"\n        Start rule poller and target poller once X-Ray daemon address\n        and context manager is in place.\n        \"\"\"\n        if not global_sdk_config.sdk_enabled():\n            return\n\n        with self._lock:\n            if not self._started:\n                self._rule_poller.start()\n                self._target_poller.start()\n                self._started = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef should_trace(self, sampling_req=None):\n        if not global_sdk_config.sdk_enabled():\n            return False\n\n        if not self._started:\n            self.start() # only front-end that actually uses the sampler spawns poller threads\n\n        now = int(time.time())\n        if sampling_req and not sampling_req.get('service_type', None):\n            sampling_req['service_type'] = self._origin\n        elif sampling_req is None:\n            sampling_req = {'service_type': self._origin}\n        matched_rule = self._cache.get_matched_rule(sampling_req, now)\n        if matched_rule:\n            log.debug('Rule %s is selected to make a sampling decision.', matched_rule.name)\n            return self._process_matched_rule(matched_rule, now)\n        else:\n            log.info('No effective centralized sampling rule match. Fallback to local rules.')\n            return self._local_sampler.should_trace(sampling_req)", "response": "Returns True if the sampler should trace the given request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_settings(self, daemon_config, context, origin=None):\n        self._connector.setup_xray_client(ip=daemon_config.tcp_ip,\n                                          port=daemon_config.tcp_port,\n                                          client=self.xray_client)\n\n        self._connector.context = context\n        self._origin = origin", "response": "Load the settings for the X - Ray recorder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npatching PynamoDB so it generates subsegements when calling DynamoDB.", "response": "def patch():\n    \"\"\"Patch PynamoDB so it generates subsegements when calling DynamoDB.\"\"\"\n    import pynamodb\n\n    if hasattr(botocore.vendored.requests.sessions, '_xray_enabled'):\n        return\n    setattr(botocore.vendored.requests.sessions, '_xray_enabled', True)\n\n    wrapt.wrap_function_wrapper(\n        'botocore.vendored.requests.sessions',\n        'Session.send',\n        _xray_traced_pynamodb,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef end_segment(self, end_time=None):\n        entity = self.get_trace_entity()\n        if not entity:\n            log.warning(\"No segment to end\")\n            return\n        if self._is_subsegment(entity):\n            entity.parent_segment.close(end_time)\n        else:\n            entity.close(end_time)", "response": "End the current active segment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_subsegment(self, subsegment):\n        entity = self.get_trace_entity()\n        if not entity:\n            log.warning(\"Active segment or subsegment not found. Discarded %s.\" % subsegment.name)\n            return\n\n        entity.add_subsegment(subsegment)\n        self._local.entities.append(subsegment)", "response": "Store the subsegment created by xray_recorder to the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nends the current active segment. Return False if there is no subsegment to end.", "response": "def end_subsegment(self, end_time=None):\n        \"\"\"\n        End the current active segment. Return False if there is no\n        subsegment to end.\n\n        :param int end_time: epoch in seconds. If not specified the current\n            system time will be used.\n        \"\"\"\n        subsegment = self.get_trace_entity()\n        if self._is_subsegment(subsegment):\n            subsegment.close(end_time)\n            self._local.entities.pop()\n            return True\n        else:\n            log.warning(\"No subsegment to end.\")\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the current trace entity.", "response": "def get_trace_entity(self):\n        \"\"\"\n        Return the current trace entity(segment/subsegment). If there is none,\n        it behaves based on pre-defined ``context_missing`` strategy.\n        \"\"\"\n        if not getattr(self._local, 'entities', None):\n            return self.handle_context_missing()\n\n        return self._local.entities[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_context_missing(self):\n        if self.context_missing == 'RUNTIME_ERROR':\n            log.error(MISSING_SEGMENT_MSG)\n            raise SegmentNotFoundException(MISSING_SEGMENT_MSG)\n        else:\n            log.error(MISSING_SEGMENT_MSG)", "response": "Called whenever there is no trace entity to access or mutate."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_in_lambda():\n    if not os.getenv(LAMBDA_TASK_ROOT_KEY):\n        return None\n\n    try:\n        os.mkdir(TOUCH_FILE_DIR)\n    except OSError:\n        log.debug('directory %s already exists', TOUCH_FILE_DIR)\n\n    try:\n        f = open(TOUCH_FILE_PATH, 'w+')\n        f.close()\n        # utime force second parameter in python2.7\n        os.utime(TOUCH_FILE_PATH, None)\n    except (IOError, OSError):\n        log.warning(\"Unable to write to %s. Failed to signal SDK initialization.\" % TOUCH_FILE_PATH)\n\n    return LambdaContext()", "response": "Check if the SDK is loaded in AWS Lambda worker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a subsegment to the current entity.", "response": "def put_subsegment(self, subsegment):\n        \"\"\"\n        Refresh the facade segment every time this function is invoked to prevent\n        a new subsegment from being attached to a leaked segment/subsegment.\n        \"\"\"\n        current_entity = self.get_trace_entity()\n\n        if not self._is_subsegment(current_entity) and current_entity.initializing:\n            if global_sdk_config.sdk_enabled():\n                log.warning(\"Subsegment %s discarded due to Lambda worker still initializing\" % subsegment.name)\n            return\n\n        current_entity.add_subsegment(subsegment)\n        self._local.entities.append(subsegment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrefresh the current facade segment and subsegments stored in Lambda worker.", "response": "def _refresh_context(self):\n        \"\"\"\n        Get current facade segment. To prevent resource leaking in Lambda worker,\n        every time there is segment present, we compare its trace id to current\n        environment variables. If it is different we create a new facade segment\n        and clean up subsegments stored.\n        \"\"\"\n        header_str = os.getenv(LAMBDA_TRACE_HEADER_KEY)\n        trace_header = TraceHeader.from_header_str(header_str)\n        if not global_sdk_config.sdk_enabled():\n            trace_header._sampled = False\n\n        segment = getattr(self._local, 'segment', None)\n\n        if segment:\n            # Ensure customers don't have leaked subsegments across invocations\n            if not trace_header.root or trace_header.root == segment.trace_id:\n                return\n            else:\n                self._initialize_context(trace_header)\n        else:\n            self._initialize_context(trace_header)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize_context(self, trace_header):\n        sampled = None\n        if not global_sdk_config.sdk_enabled():\n            # Force subsequent subsegments to be disabled and turned into DummySegments.\n            sampled = False\n        elif trace_header.sampled == 0:\n            sampled = False\n        elif trace_header.sampled == 1:\n            sampled = True\n\n        segment = FacadeSegment(\n            name='facade',\n            traceid=trace_header.root,\n            entityid=trace_header.parent,\n            sampled=sampled,\n        )\n        setattr(self._local, 'segment', segment)\n        setattr(self._local, 'entities', [])", "response": "Initialize the facade segment based on environment variables and initialize storage for subsegments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_sdk_enabled(cls, value):\n        # Environment Variables take precedence over hardcoded configurations.\n        if cls.XRAY_ENABLED_KEY in os.environ:\n            cls.__SDK_ENABLED = str(os.getenv(cls.XRAY_ENABLED_KEY, 'true')).lower() != 'false'\n        else:\n            if type(value) == bool:\n                cls.__SDK_ENABLED = value\n            else:\n                cls.__SDK_ENABLED = True\n                log.warning(\"Invalid parameter type passed into set_sdk_enabled(). Defaulting to True...\")", "response": "Sets the enabled flag of the SDK."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _xray_register_type_fix(wrapped, instance, args, kwargs):\n    our_args = list(copy.copy(args))\n    if len(our_args) == 2 and isinstance(our_args[1], (XRayTracedConn, XRayTracedCursor)):\n        our_args[1] = our_args[1].__wrapped__\n\n    return wrapped(*our_args, **kwargs)", "response": "Send the actual connection or curser to register type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a TraceHeader object from a tracing header string extracted from a http request headers.", "response": "def from_header_str(cls, header):\n        \"\"\"\n        Create a TraceHeader object from a tracing header string\n        extracted from a http request headers.\n        \"\"\"\n        if not header:\n            return cls()\n\n        try:\n            params = header.strip().split(HEADER_DELIMITER)\n            header_dict = {}\n            data = {}\n\n            for param in params:\n                entry = param.split('=')\n                key = entry[0]\n                if key in (ROOT, PARENT, SAMPLE):\n                    header_dict[key] = entry[1]\n                # Ignore any \"Self=\" trace ids injected from ALB.\n                elif key != SELF:\n                    data[key] = entry[1]\n\n            return cls(\n                root=header_dict.get(ROOT, None),\n                parent=header_dict.get(PARENT, None),\n                sampled=header_dict.get(SAMPLE, None),\n                data=data,\n            )\n\n        except Exception:\n            log.warning(\"malformed tracing header %s, ignore.\", header)\n            return cls()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting to a tracing header string that can be injected to the outgoing http request headers.", "response": "def to_header_str(self):\n        \"\"\"\n        Convert to a tracing header string that can be injected to\n        outgoing http request headers.\n        \"\"\"\n        h_parts = []\n        if self.root:\n            h_parts.append(ROOT + '=' + self.root)\n        if self.parent:\n            h_parts.append(PARENT + '=' + self.parent)\n        if self.sampled is not None:\n            h_parts.append(SAMPLE + '=' + str(self.sampled))\n        if self.data:\n            for key in self.data:\n                h_parts.append(key + '=' + self.data[key])\n\n        return HEADER_DELIMITER.join(h_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_name(self, host_name):\n        if wildcard_match(self._pattern, host_name):\n            return host_name\n        else:\n            return self._fallback", "response": "Returns the segment name based on the input host name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize a segment or subsegment and sends it to the X - Ray daemon. By default it doesn t retry on failures.", "response": "def send_entity(self, entity):\n        \"\"\"\n        Serializes a segment/subsegment and sends it to the X-Ray daemon\n        over UDP. By default it doesn't retry on failures.\n\n        :param entity: a trace entity to send to the X-Ray daemon\n        \"\"\"\n        message = \"%s%s%s\" % (PROTOCOL_HEADER,\n                              PROTOCOL_DELIMITER,\n                              entity.serialize())\n\n        log.debug(\"sending: %s to %s:%s.\" % (message, self._ip, self._port))\n        self._send_data(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the IP and port of the daemon.", "response": "def set_daemon_address(self, address):\n        \"\"\"\n        Set up UDP ip and port from the raw daemon address\n        string using ``DaemonConfig`` class utlities.\n        \"\"\"\n        if address:\n            daemon_config = DaemonConfig(address)\n            self._ip, self._port = daemon_config.udp_ip, daemon_config.udp_port"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wildcard_match(pattern, text, case_insensitive=True):\n    if pattern is None or text is None:\n        return False\n\n    pattern_len = len(pattern)\n    text_len = len(text)\n    if pattern_len == 0:\n        return text_len == 0\n\n    # Check the special case of a single * pattern, as it's common\n    if pattern == '*':\n        return True\n\n    if case_insensitive:\n        pattern = pattern.lower()\n        text = text.lower()\n\n    # Infix globs are relatively rare, and the below search is expensive.\n    # Check for infix globs and, in their absence, do the simple thing.\n    if '*' not in pattern or pattern.index('*') == len(pattern) - 1:\n        return _simple_wildcard_match(pattern, text)\n\n    # The res[i] is used to record if there is a match between\n    # the first i chars in text and the first j chars in pattern.\n    # So will return res[textLength+1] in the end\n    # Loop from the beginning of the pattern\n    # case not '*': if text[i]==pattern[j] or pattern[j] is '?',\n    # and res[i] is true, set res[i+1] to true, otherwise false.\n    # case '*': since '*' can match any globing, as long as there is a true\n    # in res before i, all the res[i+1], res[i+2],...,res[textLength]\n    # could be true\n    res = [None] * (text_len + 1)\n    res[0] = True\n    for j in range(0, pattern_len):\n        p = pattern[j]\n        if p != '*':\n            for i in range(text_len - 1, -1, -1):\n                res[i + 1] = res[i] and (p == '?' or (p == text[i]))\n        else:\n            i = 0\n            while i <= text_len and not res[i]:\n                i += 1\n            for m in range(i, text_len + 1):\n                res[m] = True\n\n        res[0] = res[0] and (p == '*')\n\n    return res[text_len]", "response": "Performs a case - insensitive wildcard match against two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_candidates(self, all_rules):\n        candidates = []\n        for rule in all_rules:\n            if rule.ever_matched() and rule.time_to_report():\n                candidates.append(rule)\n        return candidates", "response": "Return a list of all rules that have a time_to_report set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npatch aiobotocore client so it generates subsegments when calling AWS services.", "response": "def patch():\n    \"\"\"\n    Patch aiobotocore client so it generates subsegments\n    when calling AWS services.\n    \"\"\"\n    if hasattr(aiobotocore.client, '_xray_enabled'):\n        return\n    setattr(aiobotocore.client, '_xray_enabled', True)\n\n    wrapt.wrap_function_wrapper(\n        'aiobotocore.client',\n        'AioBaseClient._make_api_call',\n        _xray_traced_aiobotocore,\n    )\n\n    wrapt.wrap_function_wrapper(\n        'aiobotocore.endpoint',\n        'AioEndpoint.prepare_request',\n        inject_header,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_plugin_modules(plugins):\n    if not plugins:\n        raise MissingPluginNames(\"input plugin names are required\")\n\n    modules = []\n\n    for plugin in plugins:\n        short_name = PLUGIN_MAPPING.get(plugin.lower(), plugin.lower())\n        full_path = '%s%s' % (module_prefix, short_name)\n        modules.append(importlib.import_module(full_path))\n\n    return tuple(modules)", "response": "Get plugin modules from input strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_eligible(self, segment):\n        if not segment or not segment.sampled:\n            return False\n\n        return segment.get_total_subsegments_size() > self.streaming_threshold", "response": "A segment is eligible to have its children subsegments streaminged if the streaming threshold is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstreams out all eligible children of the input entity.", "response": "def stream(self, entity, callback):\n        \"\"\"\n        Stream out all eligible children of the input entity.\n\n        :param entity: The target entity to be streamed.\n        :param callback: The function that takes the node and\n            actually send it out.\n        \"\"\"\n        with self._lock:\n            self._stream(entity, callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_bind(bind):\n    if isinstance(bind, Connection):\n        engine = bind.engine\n    else:\n        engine = bind\n    m = re.match(r\"Engine\\((.*?)\\)\", str(engine))\n    if m is not None:\n        u = urlparse(m.group(1))\n        # Add Scheme to uses_netloc or // will be missing from url.\n        uses_netloc.append(u.scheme)\n        safe_url = \"\"\n        if u.password is None:\n            safe_url = u.geturl()\n        else:\n            # Strip password from URL\n            host_info = u.netloc.rpartition('@')[-1]\n            parts = u._replace(netloc='{}@{}'.format(u.username, host_info))\n            safe_url = parts.geturl()\n        sql = {}\n        sql['database_type'] = u.scheme\n        sql['url'] = safe_url\n        if u.username is not None:\n            sql['user'] = \"{}\".format(u.username)\n    return sql", "response": "Parses a connection string and creates SQL trace metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding input subsegment as a child subsegment and increment the parent segment s reference counter and total subsegments counter of the parent segment.", "response": "def add_subsegment(self, subsegment):\n        \"\"\"\n        Add input subsegment as a child subsegment and increment\n        reference counter and total subsegments counter of the\n        parent segment.\n        \"\"\"\n        super(Subsegment, self).add_subsegment(subsegment)\n        self.parent_segment.increment()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove input subsegment from child subsegemnts and decrement parent segment total subsegments count.", "response": "def remove_subsegment(self, subsegment):\n        \"\"\"\n        Remove input subsegment from child subsegemnts and\n        decrement parent segment total subsegments count.\n\n        :param Subsegment: subsegment to remove.\n        \"\"\"\n        super(Subsegment, self).remove_subsegment(subsegment)\n        self.parent_segment.decrement_subsegments_size()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self, end_time=None):\n        super(Subsegment, self).close(end_time)\n        self.parent_segment.decrement_ref_counter()", "response": "Closes the trace entity by setting end_time and flip the in progress flag to False. Also decrement the parent segment s ref counter by 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps boto calls with dummy segment. This is because botocore has two dependencies (requests and httplib) that might be monkey-patched in user code to capture subsegments. The wrapper makes sure there is always a non-sampled segment present when the connector makes an AWS API call using botocore. This context wrapper doesn't work with asyncio based context as event loop is not thread-safe.", "response": "def _context_wrapped(func):\n        \"\"\"\n        Wrapping boto calls with dummy segment. This is because botocore\n        has two dependencies (requests and httplib) that might be\n        monkey-patched in user code to capture subsegments. The wrapper\n        makes sure there is always a non-sampled segment present when\n        the connector makes an  AWS API call using botocore.\n        This context wrapper doesn't work with asyncio based context\n        as event loop is not thread-safe.\n        \"\"\"\n        def wrapper(self, *args, **kargs):\n            if type(self.context).__name__ == 'AsyncContext':\n                return func(self, *args, **kargs)\n            segment = DummySegment()\n            self.context.set_trace_entity(segment)\n            result = func(self, *args, **kargs)\n            self.context.clear_trace_entities()\n            return result\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_sampling_rules(self):\n        new_rules = []\n\n        resp = self._xray_client.get_sampling_rules()\n        records = resp['SamplingRuleRecords']\n\n        for record in records:\n            rule_def = record['SamplingRule']\n            if self._is_rule_valid(rule_def):\n                rule = SamplingRule(name=rule_def['RuleName'],\n                                    priority=rule_def['Priority'],\n                                    rate=rule_def['FixedRate'],\n                                    reservoir_size=rule_def['ReservoirSize'],\n                                    host=rule_def['Host'],\n                                    service=rule_def['ServiceName'],\n                                    method=rule_def['HTTPMethod'],\n                                    path=rule_def['URLPath'],\n                                    service_type=rule_def['ServiceType'])\n                new_rules.append(rule)\n\n        return new_rules", "response": "Fetch the centralized sampling rules from the X - Ray service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the current statistics of sampling rules and the new assgiend quota and TTL for the given rules.", "response": "def fetch_sampling_target(self, rules):\n        \"\"\"\n        Report the current statistics of sampling rules and\n        get back the new assgiend quota/TTL froom the X-Ray service.\n        The call is proxied and signed via X-Ray Daemon.\n        \"\"\"\n        now = int(time.time())\n        report_docs = self._generate_reporting_docs(rules, now)\n        resp = self._xray_client.get_sampling_targets(\n            SamplingStatisticsDocuments=report_docs\n        )\n        new_docs = resp['SamplingTargetDocuments']\n\n        targets_mapping = {}\n        for doc in new_docs:\n            TTL = self._dt_to_epoch(doc['ReservoirQuotaTTL']) if doc.get('ReservoirQuotaTTL', None) else None\n            target = {\n                'rate': doc['FixedRate'],\n                'quota': doc.get('ReservoirQuota', None),\n                'TTL': TTL,\n                'interval': doc.get('Interval', None),\n            }\n            targets_mapping[doc['RuleName']] = target\n\n        return targets_mapping, self._dt_to_epoch(resp['LastRuleModification'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dt_to_epoch(self, dt):\n        if PY2:\n            # The input datetime is from botocore unmarshalling and it is\n            # offset-aware so the timedelta of subtracting this time\n            # to 01/01/1970 using the same tzinfo gives us\n            # Unix Time (also known as POSIX Time).\n            time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n            return int(time_delta.total_seconds())\n        else:\n            # Added in python 3.3+ and directly returns POSIX time.\n            return int(dt.timestamp())", "response": "Convert a datetime object to an epoch."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a TraceConfig object that can be used to identify the subsegment.", "response": "def aws_xray_trace_config(name=None):\n    \"\"\"\n    :param name: name used to identify the subsegment, with None internally the URL will\n                 be used as identifier.\n    :returns: TraceConfig.\n    \"\"\"\n\n    def _trace_config_ctx_factory(trace_request_ctx):\n        return SimpleNamespace(\n            name=name,\n            trace_request_ctx=trace_request_ctx\n        )\n\n    trace_config = aiohttp.TraceConfig(trace_config_ctx_factory=_trace_config_ctx_factory)\n    trace_config.on_request_start.append(begin_subsegment)\n    trace_config.on_request_end.append(end_subsegment)\n    trace_config.on_request_exception.append(end_subsegment_with_exception)\n    return trace_config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a full stacktrace for the current state of execution.", "response": "def get_stacktrace(limit=None):\n    \"\"\"\n    Get a full stacktrace for the current state of execution.\n\n    Include the current state of the stack, minus this function.\n    If there is an active exception, include the stacktrace information from\n    the exception as well.\n\n    :param int limit:\n        Optionally limit stack trace size results. This parmaeters has the same\n        meaning as the `limit` parameter in `traceback.print_stack`.\n    :returns:\n        List of stack trace objects, in the same form as\n        `traceback.extract_stack`.\n    \"\"\"\n    if limit is not None and limit == 0:\n        # Nothing to return. This is consistent with the behavior of the\n        # functions in the `traceback` module.\n        return []\n\n    stack = traceback.extract_stack()\n    # Remove this `get_stacktrace()` function call from the stack info.\n    # For what we want to report, this is superfluous information and arguably\n    # adds garbage to the report.\n    # Also drop the `traceback.extract_stack()` call above from the returned\n    # stack info, since this is also superfluous.\n    stack = stack[:-2]\n\n    _exc_type, _exc, exc_traceback = sys.exc_info()\n    if exc_traceback is not None:\n        # If and only if there is a currently triggered exception, combine the\n        # exception traceback information with the current stack state to get a\n        # complete trace.\n        exc_stack = traceback.extract_tb(exc_traceback)\n        stack += exc_stack\n\n    # Limit the stack trace size, if a limit was specified:\n    if limit is not None:\n        # Copy the behavior of `traceback` functions with a `limit` argument.\n        # See https://docs.python.org/3/library/traceback.html.\n        if limit > 0:\n            # limit > 0: include the last `limit` items\n            stack = stack[-limit:]\n        else:\n            # limit < 0: include the first `abs(limit)` items\n            stack = stack[:abs(limit)]\n    return stack"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the sampler decide to sample based on the input information and sampling rules.", "response": "def should_trace(self, sampling_req=None):\n        \"\"\"\n        Return True if the sampler decide to sample based on input\n        information and sampling rules. It will first check if any\n        custom rule should be applied, if not it falls back to the\n        default sampling rule.\n\n        All optional arugments are extracted from incoming requests by\n        X-Ray middleware to perform path based sampling.\n        \"\"\"\n        if sampling_req is None:\n            return self._should_trace(self._default_rule)\n\n        host = sampling_req.get('host', None)\n        method = sampling_req.get('method', None)\n        path = sampling_req.get('path', None)\n\n        for rule in self._rules:\n            if rule.applies(host, method, path):\n                return self._should_trace(rule)\n\n        return self._should_trace(self._default_rule)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd exception information and fault flag to the current segment.", "response": "def process_exception(self, request, exception):\n        \"\"\"\n        Add exception information and fault flag to the\n        current segment.\n        \"\"\"\n        if self.in_lambda_ctx:\n            segment = xray_recorder.current_subsegment()\n        else:\n            segment = xray_recorder.current_segment()\n        segment.put_http_meta(http.STATUS, 500)\n\n        stack = stacktrace.get_stacktrace(limit=xray_recorder._max_trace_back)\n        segment.add_exception(exception, stack)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if there are segments left within the current second otherwise return False.", "response": "def take(self):\n        \"\"\"\n        Returns True if there are segments left within the\n        current second, otherwise return False.\n        \"\"\"\n        with self._lock:\n            now = int(time.time())\n\n            if now != self.this_sec:\n                self.used_this_sec = 0\n                self.this_sec = now\n\n            if self.used_this_sec >= self.traces_per_sec:\n                return False\n\n            self.used_this_sec = self.used_this_sec + 1\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_connection():\n  tid = id(threading.current_thread())\n  conn = _conn_holder.get(tid)\n  if not conn:\n    with(_rlock):\n      # No other thread would insert a value in our slot, so no need\n      # to recheck existence inside the lock.\n      if 'project_endpoint' not in _options and 'project_id' not in _options:\n        _options['project_endpoint'] = helper.get_project_endpoint_from_env()\n      if 'credentials' not in _options:\n        _options['credentials'] = helper.get_credentials_from_env()\n      # We still need the lock when caching the thread local connection so we\n      # don't race with _conn_holder.clear() in set_options().\n      _conn_holder[tid] = conn = connection.Datastore(**_options)\n  return conn", "response": "Returns the default datastore connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying for all Todo items ordered by creation date.", "response": "def get_all(cls):\n    \"\"\"Query for all Todo items ordered by creation date.\n\n    This method is eventually consistent to avoid the need for an extra index.\n    \"\"\"\n\n    req = datastore.RunQueryRequest()\n    q = req.query\n    set_kind(q, kind='Todo')\n    add_property_orders(q, 'created')\n    resp = datastore.run_query(req)\n    todos = [Todo.from_proto(r.entity) for r in resp.batch.entity_results]\n    return todos"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes all Todo items that are done.", "response": "def archive(cls):\n    \"\"\"Delete all Todo items that are done.\"\"\"\n    req = datastore.BeginTransactionRequest()\n    resp = datastore.begin_transaction(req)\n    tx = resp.transaction\n    req = datastore.RunQueryRequest()\n    req.read_options.transaction = tx\n    q = req.query\n    set_kind(q, kind='Todo')\n    add_projection(q, '__key__')\n    set_composite_filter(q.filter,\n                         datastore.CompositeFilter.AND,\n                         set_property_filter(\n                             datastore.Filter(),\n                             'done', datastore.PropertyFilter.EQUAL, True),\n                         set_property_filter(\n                             datastore.Filter(),\n                             '__key__', datastore.PropertyFilter.HAS_ANCESTOR,\n                             default_todo_list.key))\n    resp = datastore.run_query(req)\n    req = datastore.CommitRequest()\n    req.transaction = tx\n    for result in resp.batch.entity_results:\n      req.mutations.add().delete.CopyFrom(result.entity.key)\n    resp = datastore.commit(req)\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate or insert a Todo item.", "response": "def save(self):\n    \"\"\"Update or insert a Todo item.\"\"\"\n    req = datastore.CommitRequest()\n    req.mode = datastore.CommitRequest.NON_TRANSACTIONAL\n    req.mutations.add().upsert.CopyFrom(self.to_proto())\n    resp = datastore.commit(req)\n    if not self.id:\n      self.id = resp.mutation_results[0].key.path[-1].id\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Get(self, project_id):\n    if project_id in self._emulators:\n      return self._emulators[project_id]\n\n    emulator = self.Create(project_id)\n    self._emulators[project_id] = emulator\n    return emulator", "response": "Returns an existing emulator instance for the provided project_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an emulator instance.", "response": "def Create(self, project_id, start_options=None, deadline=10):\n    \"\"\"Creates an emulator instance.\n\n    This method will wait for up to 'deadline' seconds for the emulator to\n    start.\n\n    Args:\n      project_id: project ID\n      start_options: a list of additional command-line options to pass to the\n          emulator 'start' command\n      deadline: number of seconds to wait for the datastore to respond\n\n    Returns:\n      a DatastoreEmulator\n\n    Raises:\n      IOError: if the emulator could not be started within the deadline\n    \"\"\"\n    return DatastoreEmulator(self._emulator_cmd, self._working_directory,\n                             project_id, deadline, start_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits for the emulator to start.", "response": "def _WaitForStartup(self, deadline):\n    \"\"\"Waits for the emulator to start.\n\n    Args:\n      deadline: deadline in seconds\n\n    Returns:\n      True if the emulator responds within the deadline, False otherwise.\n    \"\"\"\n    start = time.time()\n    sleep = 0.05\n\n    def Elapsed():\n      return time.time() - start\n\n    while True:\n      try:\n        response, _ = self._http.request(self._host)\n        if response.status == 200:\n          logging.info('emulator responded after %f seconds', Elapsed())\n          return True\n      except (socket.error, httplib.ResponseNotReady):\n        pass\n      if Elapsed() >= deadline:\n        # Out of time; give up.\n        return False\n      else:\n        time.sleep(sleep)\n        sleep *= 2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear all data from the emulator instance.", "response": "def Clear(self):\n    \"\"\"Clears all data from the emulator instance.\n\n    Returns:\n      True if the data was successfully cleared, False otherwise.\n    \"\"\"\n    headers = {'Content-length': '0'}\n    response, _ = self._http.request('%s/reset' % self._host, method='POST',\n                                     headers=headers)\n    if response.status == 200:\n      return True\n    else:\n      logging.warning('failed to clear emulator; response was: %s', response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Stop(self):\n    if not self.__running:\n      return\n    logging.info('shutting down the emulator running at %s', self._host)\n    headers = {'Content-length': '0'}\n    response, _ = self._http.request('%s/shutdown' % self._host,\n                                     method='POST', headers=headers)\n    if response.status != 200:\n      logging.warning('failed to shut down emulator; response: %s', response)\n\n    self.__running = False\n    # Delete temp files.\n    shutil.rmtree(self._tmp_dir)", "response": "Stops the emulator instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_credentials_from_env():\n  if os.getenv(_DATASTORE_USE_STUB_CREDENTIAL_FOR_TEST_ENV):\n    logging.info('connecting without credentials because %s is set.',\n                 _DATASTORE_USE_STUB_CREDENTIAL_FOR_TEST_ENV)\n    return None\n  if os.getenv(_DATASTORE_EMULATOR_HOST_ENV):\n    logging.info('connecting without credentials because %s is set.',\n                 _DATASTORE_EMULATOR_HOST_ENV)\n    return None\n  if (os.getenv(_DATASTORE_SERVICE_ACCOUNT_ENV)\n      and os.getenv(_DATASTORE_PRIVATE_KEY_FILE_ENV)):\n    with open(os.getenv(_DATASTORE_PRIVATE_KEY_FILE_ENV), 'rb') as f:\n      key = f.read()\n    credentials = client.SignedJwtAssertionCredentials(\n        os.getenv(_DATASTORE_SERVICE_ACCOUNT_ENV), key, SCOPE)\n    logging.info('connecting using private key file.')\n    return credentials\n  try:\n    credentials = client.GoogleCredentials.get_application_default()\n    credentials = credentials.create_scoped(SCOPE)\n    logging.info('connecting using Google Application Default Credentials.')\n    return credentials\n  except client.ApplicationDefaultCredentialsError, e:\n    logging.error('Unable to find any credentials to use. '\n                  'If you are running locally, make sure to set the '\n                  '%s environment variable.', _DATASTORE_EMULATOR_HOST_ENV)\n    raise e", "response": "Get credentials from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_project_endpoint_from_env(project_id=None, host=None):\n  project_id = project_id or os.getenv(_DATASTORE_PROJECT_ID_ENV)\n  if not project_id:\n    raise ValueError('project_id was not provided. Either pass it in '\n                     'directly or set DATASTORE_PROJECT_ID.')\n  # DATASTORE_HOST is deprecated.\n  if os.getenv(_DATASTORE_HOST_ENV):\n    logging.warning('Ignoring value of environment variable DATASTORE_HOST. '\n                    'To point datastore to a host running locally, use the '\n                    'environment variable DATASTORE_EMULATOR_HOST')\n\n  url_override = os.getenv(_DATASTORE_URL_OVERRIDE_ENV)\n  if url_override:\n    return '%s/projects/%s' % (url_override, project_id)\n\n  localhost = os.getenv(_DATASTORE_EMULATOR_HOST_ENV)\n  if localhost:\n    return ('http://%s/%s/projects/%s'\n            % (localhost, API_VERSION, project_id))\n\n  host = host or GOOGLEAPIS_HOST\n  return 'https://%s/%s/projects/%s' % (host, API_VERSION, project_id)", "response": "Get Datastore project endpoint from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds path elements to the given datastore. Key proto message.", "response": "def add_key_path(key_proto, *path_elements):\n  \"\"\"Add path elements to the given datastore.Key proto message.\n\n  Args:\n    key_proto: datastore.Key proto message.\n    *path_elements: list of ancestors to add to the key.\n        (kind1, id1/name1, ..., kindN, idN/nameN), the last 2 elements\n        represent the entity key, if no terminating id/name: they key\n        will be an incomplete key.\n\n  Raises:\n    TypeError: the given id or name has the wrong type.\n\n  Returns:\n    the same datastore.Key.\n\n  Usage:\n    >>> add_key_path(key_proto, 'Kind', 'name')  # no parent, with name\n    datastore.Key(...)\n    >>> add_key_path(key_proto, 'Kind2', 1)  # no parent, with id\n    datastore.Key(...)\n    >>> add_key_path(key_proto, 'Kind', 'name', 'Kind2', 1)  # parent, complete\n    datastore.Key(...)\n    >>> add_key_path(key_proto, 'Kind', 'name', 'Kind2')  # parent, incomplete\n    datastore.Key(...)\n  \"\"\"\n  for i in range(0, len(path_elements), 2):\n    pair = path_elements[i:i+2]\n    elem = key_proto.path.add()\n    elem.kind = pair[0]\n    if len(pair) == 1:\n      return  # incomplete key\n    id_or_name = pair[1]\n    if isinstance(id_or_name, (int, long)):\n      elem.id = id_or_name\n    elif isinstance(id_or_name, basestring):\n      elem.name = id_or_name\n    else:\n      raise TypeError(\n          'Expected an integer id or string name as argument %d; '\n          'received %r (a %s).' % (i + 2, id_or_name, type(id_or_name)))\n  return key_proto"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd values to the given datastore. Entity proto message.", "response": "def add_properties(entity_proto, property_dict, exclude_from_indexes=None):\n  \"\"\"Add values to the given datastore.Entity proto message.\n\n  Args:\n    entity_proto: datastore.Entity proto message.\n    property_dict: a dictionary from property name to either a python object or\n        datastore.Value.\n    exclude_from_indexes: if the value should be exclude from indexes. None\n        leaves indexing as is (defaults to False if value is not a Value\n        message).\n\n  Usage:\n    >>> add_properties(proto, {'foo': u'a', 'bar': [1, 2]})\n\n  Raises:\n    TypeError: if a given property value type is not supported.\n  \"\"\"\n  for name, value in property_dict.iteritems():\n    set_property(entity_proto.properties, name, value, exclude_from_indexes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets property value in the given datastore. Property proto message.", "response": "def set_property(property_map, name, value, exclude_from_indexes=None):\n  \"\"\"Set property value in the given datastore.Property proto message.\n\n  Args:\n    property_map: a string->datastore.Value protobuf map.\n    name: name of the property.\n    value: python object or datastore.Value.\n    exclude_from_indexes: if the value should be exclude from indexes. None\n        leaves indexing as is (defaults to False if value is not a Value message).\n\n  Usage:\n    >>> set_property(property_proto, 'foo', u'a')\n\n  Raises:\n    TypeError: if the given value type is not supported.\n  \"\"\"\n  set_value(property_map[name], value, exclude_from_indexes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the corresponding datastore. Value _value field for the given arg.", "response": "def set_value(value_proto, value, exclude_from_indexes=None):\n  \"\"\"Set the corresponding datastore.Value _value field for the given arg.\n\n  Args:\n    value_proto: datastore.Value proto message.\n    value: python object or datastore.Value. (unicode value will set a\n        datastore string value, str value will set a blob string value).\n        Undefined behavior if value is/contains value_proto.\n    exclude_from_indexes: if the value should be exclude from indexes. None\n        leaves indexing as is (defaults to False if value is not a Value\n        message).\n\n  Raises:\n    TypeError: if the given value type is not supported.\n  \"\"\"\n  value_proto.Clear()\n\n  if isinstance(value, (list, tuple)):\n    for sub_value in value:\n      set_value(value_proto.array_value.values.add(), sub_value,\n                exclude_from_indexes)\n    return  # do not set indexed for a list property.\n\n  if isinstance(value, entity_pb2.Value):\n    value_proto.MergeFrom(value)\n  elif isinstance(value, unicode):\n    value_proto.string_value = value\n  elif isinstance(value, str):\n    value_proto.blob_value = value\n  elif isinstance(value, bool):\n    value_proto.boolean_value = value\n  elif isinstance(value, (int, long)):\n    value_proto.integer_value = value\n  elif isinstance(value, float):\n    value_proto.double_value = value\n  elif isinstance(value, datetime.datetime):\n    to_timestamp(value, value_proto.timestamp_value)\n  elif isinstance(value, entity_pb2.Key):\n    value_proto.key_value.CopyFrom(value)\n  elif isinstance(value, entity_pb2.Entity):\n    value_proto.entity_value.CopyFrom(value)\n  else:\n    raise TypeError('value type: %r not supported' % (value,))\n\n  if exclude_from_indexes is not None:\n    value_proto.exclude_from_indexes = exclude_from_indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the python object equivalent for the given value proto message.", "response": "def get_value(value_proto):\n  \"\"\"Gets the python object equivalent for the given value proto.\n\n  Args:\n    value_proto: datastore.Value proto message.\n\n  Returns:\n    the corresponding python object value. timestamps are converted to\n    datetime, and datastore.Value is returned for blob_key_value.\n  \"\"\"\n  field = value_proto.WhichOneof('value_type')\n  if field in __native_value_types:\n      return getattr(value_proto, field)\n  if field == 'timestamp_value':\n    return from_timestamp(value_proto.timestamp_value)\n  if field == 'array_value':\n    return [get_value(sub_value)\n            for sub_value in value_proto.array_value.values]\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_property_dict(entity_proto):\n  return dict((p.key, p.value) for p in entity_proto.property)", "response": "Convert datastore. Entity to a dict of property name -> datastore. Value.\n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_kind(query_proto, kind):\n  del query_proto.kind[:]\n  query_proto.kind.add().name = kind", "response": "Set the kind constraint for the given datastore. Query proto message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_property_orders(query_proto, *orders):\n  for order in orders:\n    proto = query_proto.order.add()\n    if order[0] == '-':\n      order = order[1:]\n      proto.direction = query_pb2.PropertyOrder.DESCENDING\n    else:\n      proto.direction = query_pb2.PropertyOrder.ASCENDING\n    proto.property.name = order", "response": "Adds ordering constraint for the given datastore. Query proto message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_projection(query_proto, *projection):\n  for p in projection:\n    proto = query_proto.projection.add()\n    proto.property.name = p", "response": "Add projection properties to the given datatstore. Query proto message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_property_filter(filter_proto, name, op, value):\n  filter_proto.Clear()\n  pf = filter_proto.property_filter\n  pf.property.name = name\n  pf.op = op\n  set_value(pf.value, value)\n  return filter_proto", "response": "Set property filter contraint in the given datastore. Filter proto message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the composite filter contraint in the given datastore. Filter proto message.", "response": "def set_composite_filter(filter_proto, op, *filters):\n  \"\"\"Set composite filter contraint in the given datastore.Filter proto message.\n\n  Args:\n    filter_proto: datastore.Filter proto message\n    op: datastore.CompositeFilter.Operation\n    filters: vararg list of datastore.Filter\n\n  Returns:\n   the same datastore.Filter.\n\n  Usage:\n    >>> set_composite_filter(filter_proto, datastore.CompositeFilter.AND,\n    ...   set_property_filter(datastore.Filter(), ...),\n    ...   set_property_filter(datastore.Filter(), ...)) # WHERE ... AND ...\n  \"\"\"\n  filter_proto.Clear()\n  cf = filter_proto.composite_filter\n  cf.op = op\n  for f in filters:\n    cf.filters.add().CopyFrom(f)\n  return filter_proto"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef micros_to_timestamp(micros, timestamp):\n  seconds = long(micros / _MICROS_PER_SECOND)\n  micro_remainder = micros % _MICROS_PER_SECOND\n  timestamp.seconds = seconds\n  timestamp.nanos = micro_remainder * _NANOS_PER_MICRO", "response": "Convert microseconds from utc epoch to google. protobuf. timestamp. Timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a datetime to a google. protobuf. Timestamp.", "response": "def to_timestamp(dt, timestamp):\n  \"\"\"Convert datetime to google.protobuf.Timestamp.\n\n  Args:\n    dt: a timezone naive datetime.\n    timestamp: a google.protobuf.Timestamp to populate.\n\n  Raises:\n    TypeError: if a timezone aware datetime was provided.\n  \"\"\"\n  if dt.tzinfo:\n    # this is an \"aware\" datetime with an explicit timezone. Throw an error.\n    raise TypeError('Cannot store a timezone aware datetime. '\n                    'Convert to UTC and store the naive datetime.')\n  timestamp.seconds = calendar.timegm(dt.timetuple())\n  timestamp.nanos = dt.microsecond * _NANOS_PER_MICRO"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_params(self, kwargs, hyperparameters):\n        init_params = dict()\n        fit_params = dict()\n        produce_params = dict()\n\n        for name, param in hyperparameters.get('fixed', dict()).items():\n            if name in kwargs:\n                value = kwargs.pop(name)\n\n            elif 'default' in param:\n                value = param['default']\n\n            else:\n                raise TypeError(\"{} required argument '{}' not found\".format(self.name, name))\n\n            init_params[name] = value\n\n        for name, param in hyperparameters.get('tunable', dict()).items():\n            if name in kwargs:\n                init_params[name] = kwargs.pop(name)\n\n        fit_args = [arg['name'] for arg in self.fit_args]\n        produce_args = [arg['name'] for arg in self.produce_args]\n\n        for name in list(kwargs.keys()):\n            if name in fit_args:\n                fit_params[name] = kwargs.pop(name)\n\n            elif name in produce_args:\n                produce_params[name] = kwargs.pop(name)\n\n        if kwargs:\n            error = \"Unexpected hyperparameters '{}'\".format(', '.join(kwargs.keys()))\n            raise TypeError(error)\n\n        return init_params, fit_params, produce_params", "response": "Extract init fit and produce params from kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_hyperparameters(self, hyperparameters):\n        self._hyperparameters.update(hyperparameters)\n\n        if self._class:\n            LOGGER.debug('Creating a new primitive instance for %s', self.name)\n            self.instance = self.primitive(**self._hyperparameters)", "response": "Set new hyperparameters.\n\n        Only the specified hyperparameters are modified, so any other\n        hyperparameter keeps the value that had been previously given.\n\n        If necessary, a new instance of the primitive is created.\n\n        Args:\n            hyperparameters (dict): Dictionary containing as keys the name\n                                    of the hyperparameters and as values\n                                    the values to be used."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, **kwargs):\n        if self.fit_method is not None:\n            fit_args = self._fit_params.copy()\n            fit_args.update(kwargs)\n            getattr(self.instance, self.fit_method)(**fit_args)", "response": "Call the fit method of the primitive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the primitive function or the predict method of the primitive.", "response": "def produce(self, **kwargs):\n        \"\"\"Call the primitive function, or the predict method of the primitive.\n\n        The given keyword arguments will be passed directly to the primitive,\n        if it is a simple function, or to the `produce` method of the\n        primitive instance specified in the JSON annotation, if it is a class.\n\n        If any of the arguments expected by the fit method had been given\n        during the MLBlock initialization, they will be passed as well.\n\n        Returns:\n            The output of the call to the primitive function or primitive\n            produce method.\n        \"\"\"\n        produce_args = self._produce_params.copy()\n        produce_args.update(kwargs)\n        if self._class:\n            return getattr(self.instance, self.produce_method)(**produce_args)\n\n        produce_args.update(self._hyperparameters)\n        return self.primitive(**produce_args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the current hyperparamters of each block.", "response": "def get_hyperparameters(self):\n        \"\"\"Get the current hyperparamters of each block.\n\n        Returns:\n            dict:\n                A dictionary containing the block names as keys and\n                the current block hyperparameters dictionary as values.\n        \"\"\"\n        hyperparameters = {}\n        for block_name, block in self.blocks.items():\n            hyperparameters[block_name] = block.get_hyperparameters()\n\n        return hyperparameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_hyperparameters(self, hyperparameters):\n        for block_name, block_hyperparams in hyperparameters.items():\n            self.blocks[block_name].set_hyperparameters(block_hyperparams)", "response": "Set new hyperparameter values for some blocks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, X=None, y=None, **kwargs):\n        context = {\n            'X': X,\n            'y': y\n        }\n        context.update(kwargs)\n\n        last_block_name = list(self.blocks.keys())[-1]\n        for block_name, block in self.blocks.items():\n            LOGGER.debug(\"Fitting block %s\", block_name)\n            try:\n                fit_args = self._get_block_args(block_name, block.fit_args, context)\n                block.fit(**fit_args)\n            except Exception:\n                LOGGER.exception(\"Exception caught fitting MLBlock %s\", block_name)\n                raise\n\n            if block_name != last_block_name:\n                LOGGER.debug(\"Producing block %s\", block_name)\n                try:\n                    produce_args = self._get_block_args(block_name, block.produce_args, context)\n                    outputs = block.produce(**produce_args)\n\n                    output_dict = self._get_outputs(block_name, outputs, block.produce_output)\n                    context.update(output_dict)\n                except Exception:\n                    LOGGER.exception(\"Exception caught producing MLBlock %s\", block_name)\n                    raise", "response": "Fit the blocks of this pipeline and produce the outputs of each block."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict(self, X=None, **kwargs):\n        context = {\n            'X': X\n        }\n        context.update(kwargs)\n\n        last_block_name = list(self.blocks.keys())[-1]\n        for block_name, block in self.blocks.items():\n            LOGGER.debug(\"Producing block %s\", block_name)\n            try:\n                produce_args = self._get_block_args(block_name, block.produce_args, context)\n                outputs = block.produce(**produce_args)\n\n                if block_name != last_block_name:\n                    output_dict = self._get_outputs(block_name, outputs, block.produce_output)\n                    context.update(output_dict)\n\n            except Exception:\n                LOGGER.exception(\"Exception caught producing MLBlock %s\", block_name)\n                raise\n\n        return outputs", "response": "Produce predictions using the blocks of this pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(self):\n        return {\n            'primitives': self.primitives,\n            'init_params': self.init_params,\n            'input_names': self.input_names,\n            'output_names': self.output_names,\n            'hyperparameters': self.get_hyperparameters(),\n            'tunable_hyperparameters': self._tunable_hyperparameters\n        }", "response": "Return all the details of this MLPipeline in a dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the specification of this MLPipeline in a JSON file.", "response": "def save(self, path):\n        \"\"\"Save the specification of this MLPipeline in a JSON file.\n\n        The content of the JSON file is the dict returned by the `to_dict` method.\n\n        Args:\n            path (str): Path to the JSON file to write.\n        \"\"\"\n        with open(path, 'w') as out_file:\n            json.dump(self.to_dict(), out_file, indent=4)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(cls, metadata):\n        hyperparameters = metadata.get('hyperparameters')\n        tunable = metadata.get('tunable_hyperparameters')\n\n        pipeline = cls(\n            metadata['primitives'],\n            metadata.get('init_params'),\n            metadata.get('input_names'),\n            metadata.get('output_names'),\n        )\n\n        if hyperparameters:\n            pipeline.set_hyperparameters(hyperparameters)\n\n        if tunable is not None:\n            pipeline._tunable_hyperparameters = tunable\n\n        return pipeline", "response": "Create a new MLPipeline instance from a dictionary containing the pipeline specification."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new MLPipeline from a JSON specification.", "response": "def load(cls, path):\n        \"\"\"Create a new MLPipeline from a JSON specification.\n\n        The JSON file format is the same as the one created by the `to_dict` method.\n\n        Args:\n            path (str): Path of the JSON file to load.\n\n        Returns:\n            MLPipeline:\n                A new MLPipeline instance with the specification found\n                in the JSON file.\n        \"\"\"\n        with open(path, 'r') as in_file:\n            metadata = json.load(in_file)\n\n        return cls.from_dict(metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_primitives_path(path):\n    if path not in _PRIMITIVES_PATHS:\n        if not os.path.isdir(path):\n            raise ValueError('Invalid path: {}'.format(path))\n\n        LOGGER.debug('Adding new primitives path %s', path)\n        _PRIMITIVES_PATHS.insert(0, os.path.abspath(path))", "response": "Adds a new path to look for primitives."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_primitives_paths():\n\n    primitives_paths = list()\n    entry_points = pkg_resources.iter_entry_points('mlprimitives')\n    for entry_point in entry_points:\n        if entry_point.name == 'jsons_path':\n            path = entry_point.load()\n            primitives_paths.append(path)\n\n    return _PRIMITIVES_PATHS + primitives_paths", "response": "Get the list of folders where the primitives will be looked for."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlocating and load the JSON annotation of the given primitive.", "response": "def load_primitive(name):\n    \"\"\"Locate and load the JSON annotation of the given primitive.\n\n    All the paths found in PRIMTIVE_PATHS will be scanned to find a JSON file\n    with the given name, and as soon as a JSON with the given name is found it\n    is returned.\n\n    Args:\n        name (str): name of the primitive to look for. The name should\n                    correspond to the primitive, not to the filename, as the\n                    `.json` extension will be added dynamically.\n\n    Returns:\n        dict:\n            The content of the JSON annotation file loaded into a dict.\n\n    Raises:\n        ValueError: A `ValueError` will be raised if the primitive cannot be\n                    found.\n    \"\"\"\n\n    for base_path in get_primitives_paths():\n        parts = name.split('.')\n        number_of_parts = len(parts)\n\n        for folder_parts in range(number_of_parts):\n            folder = os.path.join(base_path, *parts[:folder_parts])\n            filename = '.'.join(parts[folder_parts:]) + '.json'\n            json_path = os.path.join(folder, filename)\n\n            if os.path.isfile(json_path):\n                with open(json_path, 'r') as json_file:\n                    LOGGER.debug('Loading primitive %s from %s', name, json_path)\n                    return json.load(json_file)\n\n    raise ValueError(\"Unknown primitive: {}\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhand Geometry Dataset. The data of this dataset is a 3d numpy array vector with shape (224, 224, 3) containing 112 224x224 RGB photos of hands, and the target is a 1d numpy float array containing the width of the wrist in centimeters.", "response": "def load_handgeometry():\n    \"\"\"Hand Geometry Dataset.\n\n    The data of this dataset is a 3d numpy array vector with shape (224, 224, 3)\n    containing 112 224x224 RGB photos of hands, and the target is a 1d numpy\n    float array containing the width of the wrist in centimeters.\n    \"\"\"\n    dataset_path = _load('handgeometry')\n\n    df = _load_csv(dataset_path, 'data')\n    X = _load_images(os.path.join(dataset_path, 'images'), df.image)\n    y = df.target.values\n\n    return Dataset(load_handgeometry.__doc__, X, y, r2_score)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_dic28():\n\n    dataset_path = _load('dic28')\n\n    X = _load_csv(dataset_path, 'data')\n    y = X.pop('label').values\n\n    graph1 = nx.Graph(nx.read_gml(os.path.join(dataset_path, 'graph1.gml')))\n    graph2 = nx.Graph(nx.read_gml(os.path.join(dataset_path, 'graph2.gml')))\n\n    graph = graph1.copy()\n    graph.add_nodes_from(graph2.nodes(data=True))\n    graph.add_edges_from(graph2.edges)\n    graph.add_edges_from(X[['graph1', 'graph2']].values)\n\n    graphs = {\n        'graph1': graph1,\n        'graph2': graph2,\n    }\n\n    return Dataset(load_dic28.__doc__, X, y, accuracy_score,\n                   stratify=True, graph=graph, graphs=graphs)", "response": "DIC28 Dataset from Pajek s dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_amazon():\n\n    dataset_path = _load('amazon')\n\n    X = _load_csv(dataset_path, 'data')\n    y = X.pop('label').values\n\n    graph = nx.Graph(nx.read_gml(os.path.join(dataset_path, 'graph.gml')))\n\n    return Dataset(load_amazon.__doc__, X, y, normalized_mutual_info_score, graph=graph)", "response": "Load Amazon product co - purchasing network and ground - truth communities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the Joke Recommender System dataset.", "response": "def load_jester():\n    \"\"\"Ratings from the Jester Online Joke Recommender System.\n\n    This dataset consists of over 1.7 million instances of (user_id, item_id, rating)\n    triples, which is split 50-50 into train and test data.\n\n    source: \"University of California Berkeley, CA\"\n    sourceURI: \"http://eigentaste.berkeley.edu/dataset/\"\n    \"\"\"\n\n    dataset_path = _load('jester')\n\n    X = _load_csv(dataset_path, 'data')\n    y = X.pop('rating').values\n\n    return Dataset(load_jester.__doc__, X, y, r2_score)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_wikiqa():\n\n    dataset_path = _load('wikiqa')\n\n    data = _load_csv(dataset_path, 'data', set_index=True)\n    questions = _load_csv(dataset_path, 'questions', set_index=True)\n    sentences = _load_csv(dataset_path, 'sentences', set_index=True)\n    vocabulary = _load_csv(dataset_path, 'vocabulary', set_index=True)\n\n    entities = {\n        'data': (data, 'd3mIndex', None),\n        'questions': (questions, 'qIndex', None),\n        'sentences': (sentences, 'sIndex', None),\n        'vocabulary': (vocabulary, 'index', None)\n    }\n    relationships = [\n        ('questions', 'qIndex', 'data', 'qIndex'),\n        ('sentences', 'sIndex', 'data', 'sIndex')\n    ]\n\n    target = data.pop('isAnswer').values\n\n    return Dataset(load_wikiqa.__doc__, data, target, accuracy_score, startify=True,\n                   entities=entities, relationships=relationships)", "response": "A Challenge Dataset for Open - Domain Question Answering."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_newsgroups():\n    dataset = datasets.fetch_20newsgroups()\n    return Dataset(load_newsgroups.__doc__, np.array(dataset.data), dataset.target,\n                   accuracy_score, stratify=True)", "response": "20 News Groups Dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of tuples containing the X train and test for each train and test for each test.", "response": "def get_splits(self, n_splits=1):\n        \"\"\"Return splits of this dataset ready for Cross Validation.\n\n        If n_splits is 1, a tuple containing the X for train and test\n        and the y for train and test is returned.\n        Otherwise, if n_splits is bigger than 1, a list of such tuples\n        is returned, one for each split.\n\n        Args:\n            n_splits (int): Number of times that the data needs to be splitted.\n\n        Returns:\n            tuple or list:\n                if n_splits is 1, a tuple containing the X for train and test\n                and the y for train and test is returned.\n                Otherwise, if n_splits is bigger than 1, a list of such tuples\n                is returned, one for each split.\n        \"\"\"\n        if n_splits == 1:\n            stratify = self.target if self._stratify else None\n\n            return train_test_split(\n                self.data,\n                self.target,\n                shuffle=self._shuffle,\n                stratify=stratify\n            )\n\n        else:\n            cv_class = StratifiedKFold if self._stratify else KFold\n            cv = cv_class(n_splits=n_splits, shuffle=self._shuffle)\n\n            splits = list()\n            for train, test in cv.split(self.data, self.target):\n                X_train = self._get_split(self.data, train)\n                y_train = self._get_split(self.target, train)\n                X_test = self._get_split(self.data, test)\n                y_test = self._get_split(self.target, test)\n                splits.append((X_train, X_test, y_train, y_test))\n\n            return splits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(key, value):\n        tr = TrackedRequest.instance()\n        tr.tag(key, value)", "response": "Adds a context to the currently executing request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef platform(cls):\n        system_name = platform.system()\n        if system_name == \"Linux\":\n            libc = cls.libc()\n            return \"unknown-linux-{libc}\".format(libc=libc)\n        elif system_name == \"Darwin\":\n            return \"apple-darwin\"\n        else:\n            return \"unknown\"", "response": "Return the name of the system that the user is running on."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef libc(cls):\n        try:\n            output = subprocess.check_output(\n                [\"ldd\", \"--version\"], stderr=subprocess.STDOUT\n            )\n        except (OSError, subprocess.CalledProcessError):\n            return \"gnu\"\n        else:\n            if b\"musl\" in output:\n                return \"musl\"\n            else:\n                return \"gnu\"", "response": "Return the core agent name for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling by the threading system", "response": "def run(self):\n        \"\"\"\n        Called by the threading system\n        \"\"\"\n\n        try:\n            self._connect()\n            self._register()\n            while True:\n                try:\n                    body = self.command_queue.get(block=True, timeout=1 * SECOND)\n                except queue.Empty:\n                    body = None\n\n                if body is not None:\n                    result = self._send(body)\n                    if result:\n                        self.command_queue.task_done()\n                    else:\n                        # Something was wrong with the socket.\n                        self._disconnect()\n                        self._connect()\n                        self._register()\n\n                # Check for stop event after a read from the queue. This is to\n                # allow you to open a socket, immediately send to it, and then\n                # stop it. We do this in the Metadata send at application start\n                # time\n                if self._stop_event.is_set():\n                    logger.debug(\"CoreAgentSocket thread stopping.\")\n                    break\n        except Exception:\n            logger.debug(\"CoreAgentSocket thread exception.\")\n        finally:\n            self._started_event.clear()\n            self._stop_event.clear()\n            self._stopped_event.set()\n            logger.debug(\"CoreAgentSocket thread stopped.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(cls, **kwargs):\n        global SCOUT_PYTHON_VALUES\n        for key, value in kwargs.items():\n            SCOUT_PYTHON_VALUES[key] = value", "response": "Sets a configuration value for the Scout agent."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef text(value, encoding=\"utf-8\", errors=\"strict\"):\n    if isinstance(value, text_type):\n        return value\n    elif isinstance(value, bytes):\n        return text_type(value, encoding, errors)\n    else:\n        return text_type(value)", "response": "Convert a value to str on Python 3 and unicode on Python 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall ScoutApm SQL Instrumentation by monkeypatching the cursor method of BaseDatabaseWrapper to return a wrapper that instruments any calls going through it.", "response": "def install():\n        \"\"\"\n        Installs ScoutApm SQL Instrumentation by monkeypatching the `cursor`\n        method of BaseDatabaseWrapper, to return a wrapper that instruments any\n        calls going through it.\n        \"\"\"\n\n        @monkeypatch_method(BaseDatabaseWrapper)\n        def cursor(original, self, *args, **kwargs):\n            result = original(*args, **kwargs)\n            return _DetailedTracingCursorWrapper(result, self)\n\n        logger.debug(\"Monkey patched SQL\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_from_headers(cls, headers):\n\n        # A single address, set by this server, returned as an Array\n        remote_addr = cls.ips_from(headers.get(\"REMOTE_ADDR\"))\n\n        # Could be a CSV list and/or repeated headers that were concatenated.\n        forwarded_ips = cls.ips_from(headers.get(\"HTTP_X_FORWARDED_FOR\"))\n        client_ips = cls.ips_from(headers.get(\"HTTP_CLIENT_IP\"))\n\n        # We assume these things about the IP headers:\n        #\n        #   - X-Forwarded-For will be a list of IPs, one per proxy, or blank.\n        #       in order: `client,proxy1,proxy2`\n        #   - Client-Ip is propagated from the outermost proxy, or is blank\n        #   - REMOTE_ADDR will be the IP that made the request to this server\n        #\n        # X-Forwarded-For and Client-Ip shouldn't be set at the same time, but\n        # if they are, use the one in Forwarded\n        ips = forwarded_ips + client_ips + remote_addr\n\n        try:\n            return ips[0]\n        except IndexError:\n            return None", "response": "Given a dictionary of HTTP headers return the most likely IP that is set by this server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninstalls the ScoutApm middleware.", "response": "def install_middleware(self):\n        \"\"\"\n        Attempts to insert the ScoutApm middleware as the first middleware\n        (first on incoming requests, last on outgoing responses).\n        \"\"\"\n        from django.conf import settings\n\n        # If MIDDLEWARE is set, update that, with handling of tuple vs array forms\n        if getattr(settings, \"MIDDLEWARE\", None) is not None:\n            if isinstance(settings.MIDDLEWARE, tuple):\n                settings.MIDDLEWARE = (\n                    (\"scout_apm.django.middleware.MiddlewareTimingMiddleware\",)\n                    + settings.MIDDLEWARE\n                    + (\"scout_apm.django.middleware.ViewTimingMiddleware\",)\n                )\n            else:\n                settings.MIDDLEWARE.insert(\n                    0, \"scout_apm.django.middleware.MiddlewareTimingMiddleware\"\n                )\n                settings.MIDDLEWARE.append(\n                    \"scout_apm.django.middleware.ViewTimingMiddleware\"\n                )\n\n        # Otherwise, we're doing old style middleware, do the same thing with\n        # the same handling of tuple vs array forms\n        else:\n            if isinstance(settings.MIDDLEWARE_CLASSES, tuple):\n                settings.MIDDLEWARE_CLASSES = (\n                    (\"scout_apm.django.middleware.OldStyleMiddlewareTimingMiddleware\",)\n                    + settings.MIDDLEWARE_CLASSES\n                    + (\"scout_apm.django.middleware.OldStyleViewMiddleware\",)\n                )\n            else:\n                settings.MIDDLEWARE_CLASSES.insert(\n                    0, \"scout_apm.django.middleware.OldStyleMiddlewareTimingMiddleware\"\n                )\n                settings.MIDDLEWARE_CLASSES.append(\n                    \"scout_apm.django.middleware.OldStyleViewMiddleware\"\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_flask_settings(self):\n        configs = {}\n        configs[\"application_root\"] = self.app.instance_path\n        for name in current_app.config:\n            if name.startswith(\"SCOUT_\"):\n                value = current_app.config[name]\n                clean_name = name.replace(\"SCOUT_\", \"\").lower()\n                configs[clean_name] = value\n        ScoutConfig.set(**configs)", "response": "Extracts the flask settings from the current app into Scout s config lookup\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dispatch_request(self):\n\n        req = _request_ctx_stack.top.request\n        app = current_app\n\n        # Return flask's default options response. See issue #40\n        if req.method == \"OPTIONS\":\n            return app.make_default_options_response()\n\n        if req.routing_exception is not None:\n            app.raise_routing_exception(req)\n\n        # The routing rule has some handy attributes to extract how Flask found\n        # this endpoint\n        rule = req.url_rule\n\n        # Wrap the real view_func\n        view_func = self.wrap_view_func(\n            app, rule, req, app.view_functions[rule.endpoint], req.view_args\n        )\n\n        return view_func(**req.view_args)", "response": "Modified version of Flask. dispatch_request to call process_view."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_view_func(self, app, rule, req, view_func, view_kwargs):\n        operation = view_func.__module__ + \".\" + view_func.__name__\n        return self.trace_view_function(\n            view_func, (\"Controller\", {\"path\": req.path, \"name\": operation})\n        )", "response": "Wrap the view function to trace it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_view(self, request, view_func, view_args, view_kwargs):\n        try:\n            if ignore_path(request.path):\n                TrackedRequest.instance().tag(\"ignore_transaction\", True)\n\n            view_name = request.resolver_match._func_path\n            span = TrackedRequest.instance().current_span()\n            if span is not None:\n                span.operation = \"Controller/\" + view_name\n                Context.add(\"path\", request.path)\n                Context.add(\"user_ip\", RemoteIp.lookup_from_headers(request.META))\n                if getattr(request, \"user\", None) is not None:\n                    Context.add(\"username\", request.user.get_username())\n        except Exception:\n            pass", "response": "Process the view_func that is about to execute"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a virtual network according to the given configuration.", "response": "def create(hypervisor, identifier, configuration):\n    \"\"\"Creates a virtual network according to the given configuration.\n\n    @param hypervisor: (libvirt.virConnect) connection to libvirt hypervisor.\n    @param identifier: (str) UUID for the virtual network.\n    @param configuration: (dict) network configuration.\n\n    @return: (libvirt.virNetwork) virtual network.\n\n    \"\"\"\n    counter = count()\n    xml_config = DEFAULT_NETWORK_XML\n\n    if not {'configuration', 'dynamic_address'} & set(configuration.keys()):\n        raise RuntimeError(\n            \"Either configuration or dynamic_address must be specified\")\n\n    if 'configuration' in configuration:\n        with open(configuration['configuration']) as xml_file:\n            xml_config = xml_file.read()\n\n    while True:\n        if 'dynamic_address' in configuration:\n            address = generate_address(hypervisor,\n                                       configuration['dynamic_address'])\n            xml_string = network_xml(identifier, xml_config, address=address)\n        else:\n            xml_string = network_xml(identifier, xml_config)\n\n        try:\n            return hypervisor.networkCreateXML(xml_string)\n        except libvirt.libvirtError as error:\n            if next(counter) > MAX_ATTEMPTS:\n                raise RuntimeError(\n                    \"Exceeded failed attempts ({}) to get IP address.\".format(\n                        MAX_ATTEMPTS),\n                    \"Last error: {}\".format(error))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookup(domain):\n    xml = domain.XMLDesc(0)\n    element = etree.fromstring(xml)\n    subelm = element.find('.//interface[@type=\"network\"]')\n\n    if subelm is not None:\n        network = subelm.find('.//source').get('network')\n        hypervisor = domain.connect()\n\n        return hypervisor.networkLookupByName(network)\n\n    return None", "response": "Find the virNetwork object associated to the domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndestroys the given network", "response": "def delete(network):\n    \"\"\"libvirt network cleanup.\n\n    @raise: libvirt.libvirtError.\n\n    \"\"\"\n    try:\n        network.destroy()\n    except libvirt.libvirtError as error:\n        raise RuntimeError(\"Unable to destroy network: {}\".format(error))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfill the XML file with the required fields.", "response": "def network_xml(identifier, xml, address=None):\n    \"\"\"Fills the XML file with the required fields.\n\n     * name\n     * uuid\n     * bridge\n     * ip\n     ** dhcp\n\n    \"\"\"\n    netname = identifier[:8]\n    network = etree.fromstring(xml)\n\n    subelement(network, './/name', 'name', identifier)\n    subelement(network, './/uuid', 'uuid', identifier)\n    subelement(network, './/bridge', 'bridge', None, name='virbr-%s' % netname)\n\n    if address is not None:\n        set_address(network, address)\n\n    return etree.tostring(network).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the given address to the Libvirt bridge XML element.", "response": "def set_address(network, address):\n    \"\"\"Sets the given address to the network XML element.\n\n    Libvirt bridge will have address and DHCP server configured\n    according to the subnet prefix length.\n\n    \"\"\"\n    if network.find('.//ip') is not None:\n        raise RuntimeError(\"Address already specified in XML configuration.\")\n\n    netmask = str(address.netmask)\n    ipv4 = str(address[1])\n    dhcp_start = str(address[2])\n    dhcp_end = str(address[-2])\n    ip = etree.SubElement(network, 'ip', address=ipv4, netmask=netmask)\n    dhcp = etree.SubElement(ip, 'dhcp')\n\n    etree.SubElement(dhcp, 'range', start=dhcp_start, end=dhcp_end)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_address(hypervisor, configuration):\n    ipv4 = configuration['ipv4']\n    prefix = configuration['prefix']\n    subnet_prefix = configuration['subnet_prefix']\n    subnet_address = ipaddress.IPv4Network(u'/'.join((str(ipv4), str(prefix))))\n    net_address_pool = subnet_address.subnets(new_prefix=subnet_prefix)\n\n    return address_lookup(hypervisor, net_address_pool)", "response": "Generate a valid IP address according to the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a valid and available network IP address.", "response": "def address_lookup(hypervisor, address_pool):\n    \"\"\"Retrieves a valid and available network IP address.\"\"\"\n    address_pool = set(address_pool)\n    active_addresses = set(active_network_addresses(hypervisor))\n\n    try:\n        return random.choice(tuple(address_pool - active_addresses))\n    except IndexError:\n        raise RuntimeError(\"All IP addresses are in use\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery libvirt for the already reserved addresses.", "response": "def active_network_addresses(hypervisor):\n    \"\"\"Query libvirt for the already reserved addresses.\"\"\"\n    active = []\n\n    for network in hypervisor.listNetworks():\n        try:\n            xml = hypervisor.networkLookupByName(network).XMLDesc(0)\n        except libvirt.libvirtError:  # network has been destroyed meanwhile\n            continue\n        else:\n            ip_element = etree.fromstring(xml).find('.//ip')\n            address = ip_element.get('address')\n            netmask = ip_element.get('netmask')\n\n            active.append(ipaddress.IPv4Network(u'/'.join((address, netmask)),\n                                                strict=False))\n\n    return active"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interface_lookup(interfaces, hwaddr, address_type):\n    for interface in interfaces.values():\n        if interface.get('hwaddr') == hwaddr:\n            for address in interface.get('addrs'):\n                if address.get('type') == address_type:\n                    return address.get('addr')", "response": "Search the address within the interface list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mac_address(self):\n        if self._mac_address is None:\n            self._mac_address = self._get_mac_address()\n\n        return self._mac_address", "response": "Returns the MAC address of the network interface."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ip4_address(self):\n        if self._ip4_address is None and self.network is not None:\n            self._ip4_address = self._get_ip_address(\n                libvirt.VIR_IP_ADDR_TYPE_IPV4)\n\n        return self._ip4_address", "response": "Returns the IPv4 address of the network interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the IPv6 address of the network interface.", "response": "def ip6_address(self):\n        \"\"\"Returns the IPv6 address of the network interface.\n\n        If multiple interfaces are provided,\n        the address of the first found is returned.\n\n        \"\"\"\n        if self._ip6_address is None and self.network is not None:\n            self._ip6_address = self._get_ip_address(\n                libvirt.VIR_IP_ADDR_TYPE_IPV6)\n\n        return self._ip6_address"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshut down the Guest Context.", "response": "def shutdown(self, timeout=None, **kwargs):\n        \"\"\"\n        Shuts down the Context. Sends an ACPI request to the OS for a clean\n        shutdown.\n\n        Triggered events::\n          * pre_poweroff\n          * post_poweroff\n\n        .. note::\n           The Guest OS needs to support ACPI requests sent from the host,\n           the completion of the operation is not ensured by the platform.\n           If the Guest OS is still running after the given timeout,\n           a RuntimeError will be raised.\n\n        @param timeout: (int) amout of seconds to wait for the machine shutdown.\n        @param kwargs: keyword arguments to pass altogether with the events.\n\n        \"\"\"\n        self._assert_transition('shutdown')\n        self.trigger('pre_shutdown', **kwargs)\n        self._execute_command(self.domain.shutdown)\n        self._wait_for_shutdown(timeout)\n        self.trigger('post_shutdown', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _command(self, event, command, *args, **kwargs):\n        self._assert_transition(event)\n        self.trigger('pre_%s' % event, **kwargs)\n        self._execute_command(command, *args)\n        self.trigger('post_%s' % event, **kwargs)", "response": "Internal method that is called by the controller when a command is generated by the controller."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _assert_transition(self, event):\n        state = self.domain.state()[0]\n        if event not in STATES_MAP[state]:\n            raise RuntimeError(\"State transition %s not allowed\" % event)", "response": "Asserts the state transition validity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the state transition command.", "response": "def _execute_command(self, command, *args):\n        \"\"\"Execute the state transition command.\"\"\"\n        try:\n            command(*args)\n        except libvirt.libvirtError as error:\n            raise RuntimeError(\"Unable to execute command. %s\" % error)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn a QEMU internal snapshot into a QCOW file.", "response": "def snapshot_to_checkpoint(volume, snapshot, folder_path):\n    \"\"\"Turns a QEMU internal snapshot into a QCOW file.\"\"\"\n    create_folder(folder_path)\n\n    name = snapshot.getName()\n    path = os.path.join(folder_path, '%s.qcow2' % name)\n\n    process = launch_process(QEMU_IMG, \"convert\", \"-f\", \"qcow2\", \"-o\",\n                             \"backing_file=%s\" % volume_backing_path(volume),\n                             \"-O\", \"qcow2\", \"-s\", name,\n                             volume_path(volume), path)\n    collect_process_output(process)\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two disks according to the given configuration.", "response": "def compare_disks(disk0, disk1, configuration):\n    \"\"\"Compares two disks according to the given configuration.\"\"\"\n    with DiskComparator(disk0, disk1) as comparator:\n        results = comparator.compare(\n            size=configuration.get('get_file_size', False),\n            identify=configuration.get('identify_files', False),\n            concurrent=configuration.get('use_concurrency', False))\n\n        if configuration.get('extract_files', False):\n            extract = results['created_files'] + results['modified_files']\n            files = comparator.extract(1, extract,\n                                       path=configuration['results_folder'])\n\n            results.update(files)\n\n        if configuration.get('compare_registries', False):\n            results['registry'] = comparator.compare_registry(\n                concurrent=configuration.get('use_concurrency', False))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_class(fully_qualified_name):\n    module_name, class_name = str(fully_qualified_name).rsplit(\".\", 1)\n    module = __import__(module_name, globals(), locals(), [class_name], 0)\n    Class = getattr(module, class_name)\n\n    if not inspect.isclass(Class):\n        raise TypeError(\n            \"%s is not of type class: %s\" % (class_name, type(Class)))\n\n    return Class", "response": "Given its fully qualified name finds the desired class and imports it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef screenshot(context):\n    handler = lambda _, buff, file_handler: file_handler.write(buff)\n\n    string = BytesIO()\n    stream = context.domain.connect().newStream(0)\n    context.domain.screenshot(stream, 0, 0)\n    stream.recvAll(handler, string)\n\n    return string.getvalue()", "response": "Takes a screenshot of the guest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prime_event(event, source, **kwargs):\n    if not isinstance(event, Event):\n        event = Event(event, source=source, **kwargs)\n\n    return event", "response": "Returns the Event instance ready to be triggered."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the function asynchronously taking care of exceptions.", "response": "def asynchronous(function, event):\n    \"\"\"\n    Runs the function asynchronously taking care of exceptions.\n    \"\"\"\n    thread = Thread(target=synchronous, args=(function, event))\n    thread.daemon = True\n    thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef synchronous(function, event):\n    try:\n        function(event)\n    except Exception as error:\n        logger = get_function_logger(function)\n        logger.exception(error)", "response": "Runs the function synchronously taking care of exceptions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes a Handler for the given Event.", "response": "def subscribe(self, event, handler):\n        \"\"\"\n        Subscribes a Handler for the given Event.\n\n        @param event: (str|see.Event) event to react to.\n        @param handler: (callable) function or method to subscribe.\n        \"\"\"\n        self._handlers.sync_handlers[event].append(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubscribes an asynchronous handler for the given Event.", "response": "def subscribe_async(self, event, handler):\n        \"\"\"\n        Subscribes an asynchronous Handler for the given Event.\n\n        An asynchronous handler is executed concurrently to the others\n        without blocking the Events flow.\n\n        @param event: (str|see.Event) event to react to.\n        @param handler: (callable) function or method to subscribe.\n        \"\"\"\n        self._handlers.async_handlers[event].append(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntriggering an event. All subscribed handlers will be executed, asynchronous ones won't block this call. @param event: (str|see.Event) event intended to be raised.", "response": "def trigger(self, event, **kwargs):\n        \"\"\"\n        Triggers an event.\n\n        All subscribed handlers will be executed, asynchronous ones\n        won't block this call.\n\n        @param event: (str|see.Event) event intended to be raised.\n        \"\"\"\n        with self._handlers.trigger_mutex:\n            event = prime_event(event, self.__class__.__name__, **kwargs)\n\n            for handler in self._handlers.async_handlers[event]:\n                asynchronous(handler, event)\n            for handler in self._handlers.sync_handlers[event]:\n                synchronous(handler, event)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef domain_xml(identifier, xml, disk_path):\n    domain = etree.fromstring(xml)\n\n    subelement(domain, './/name', 'name', identifier)\n    subelement(domain, './/uuid', 'uuid', identifier)\n    devices = subelement(domain, './/devices', 'devices', None)\n    disk = subelement(devices, './/disk', 'disk', None, type='file', device='disk')\n    subelement(disk, './/source', 'source', None, file=disk_path)\n\n    return etree.tostring(domain).decode('utf-8')", "response": "Fills the XML file with the required fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef domain_create(hypervisor, identifier, configuration, disk_path):\n    with open(configuration['configuration']) as config_file:\n        domain_config = config_file.read()\n\n    xml = domain_xml(identifier, domain_config, disk_path)\n\n    return hypervisor.defineXML(xml)", "response": "Create a libvirt domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an image path.", "response": "def provider_image(self):\n        \"\"\"Image path getter.\n\n        This method uses a pluggable image provider to retrieve an\n        image's path.\n\n        \"\"\"\n        if self._image is None:\n            if isinstance(self.configuration['disk']['image'], dict):\n                ProviderClass = lookup_provider_class(\n                    self.configuration['disk']['image']['provider'])\n                self._image = ProviderClass(\n                    self.configuration['disk']['image']).image\n            else:\n                # If image is not a dictionary, return it as is for backwards\n                # compatibility\n                self._image = self.configuration['disk']['image']\n        return self._image"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a command and returns its exit code and output.", "response": "def run_command(args, asynchronous=False):\n    \"\"\"Executes a command returning its exit code and output.\"\"\"\n    logging.info(\"Executing %s command %s.\",\n                 asynchronous and 'asynchronous' or 'synchronous', args)\n\n    process = subprocess.Popen(args,\n                               shell=True,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT)\n\n    try:\n        timeout = asynchronous and 1 or None\n        output = process.communicate(timeout=timeout)[0].decode('utf8')\n    except subprocess.TimeoutExpired:\n        pass\n\n    if asynchronous:\n        return PopenOutput(None, 'Asynchronous call.')\n    else:\n        return PopenOutput(process.returncode, output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_GET(self):\n        logging.debug(\"New GET request.\")\n\n        query = parse_qs(urlparse(self.path).query)\n        command = query['command'][0].split(' ')\n        async = bool(int(query.get('async', [False])[0]))\n        output = run_command(command, asynchronous=async)\n\n        self.respond(output)", "response": "Run simple command with parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_POST(self):\n        logging.debug(\"New POST request.\")\n\n        query = parse_qs(urlparse(self.path).query)\n        sample = query['sample'][0]\n        async = bool(int(query.get('async', [False])[0]))\n\n        path = self.store_file(mkdtemp(), sample)\n        command = query['command'][0].format(sample=path).split(' ')\n\n        output = run_command(command, asynchronous=async)\n\n        self.respond(output)", "response": "Upload a file and execute a command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring the uploaded file in the given folder.", "response": "def store_file(self, folder, name):\n        \"\"\"Stores the uploaded file in the given path.\"\"\"\n        path = os.path.join(folder, name)\n        length = self.headers['content-length']\n\n        with open(path, 'wb') as sample:\n            sample.write(self.rfile.read(int(length)))\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_processing_handler(self, event):\n        self.logger.debug(\"Event %s: starting Volatility process(es).\", event)\n\n        for snapshot in self.snapshots:\n            self.process_snapshot(snapshot)\n\n        self.processing_done.set()", "response": "Asynchronous handler starting the Volatility processes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the disk XML for the given identifier.", "response": "def disk_xml(identifier, pool_xml, base_volume_xml, cow):\n    \"\"\"Clones volume_xml updating the required fields.\n\n     * name\n     * target path\n     * backingStore\n\n    \"\"\"\n    pool = etree.fromstring(pool_xml)\n    base_volume = etree.fromstring(base_volume_xml)\n    pool_path = pool.find('.//path').text\n    base_path = base_volume.find('.//target/path').text\n    target_path = os.path.join(pool_path, '%s.qcow2' % identifier)\n    volume_xml = VOLUME_DEFAULT_CONFIG.format(identifier, target_path)\n    volume = etree.fromstring(volume_xml)\n    base_volume_capacity = base_volume.find(\".//capacity\")\n\n    volume.append(base_volume_capacity)\n\n    if cow:\n        backing_xml = BACKING_STORE_DEFAULT_CONFIG.format(base_path)\n        backing_store = etree.fromstring(backing_xml)\n        volume.append(backing_store)\n\n    return etree.tostring(volume).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pool_lookup(hypervisor, disk_path):\n    try:\n        volume = hypervisor.storageVolLookupByPath(disk_path)\n\n        return volume.storagePoolLookupByVolume()\n    except libvirt.libvirtError:\n        return None", "response": "Storage pool lookup.\n\n    Retrieves the the virStoragepool which contains the disk at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef volumes_delete(storage_pool, logger):\n    try:\n        for vol_name in storage_pool.listVolumes():\n            try:\n                vol = storage_pool.storageVolLookupByName(vol_name)\n                vol.delete(0)\n            except libvirt.libvirtError:\n                logger.exception(\n                    \"Unable to delete storage volume %s.\", vol_name)\n    except libvirt.libvirtError:\n        logger.exception(\"Unable to delete storage volumes.\")", "response": "Deletes all storage volume disks contained in the given storage pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef disk_clone(hypervisor, identifier, storage_pool, configuration, image, logger):\n    cow = configuration.get('copy_on_write', False)\n\n    try:\n        volume = hypervisor.storageVolLookupByPath(image)\n    except libvirt.libvirtError:\n        if os.path.exists(image):\n            pool_path = os.path.dirname(image)\n            logger.info(\"LibVirt pool does not exist, creating {} pool\".format(\n                pool_path.replace('/', '_')))\n            pool = hypervisor.storagePoolDefineXML(BASE_POOL_CONFIG.format(\n                pool_path.replace('/', '_'), pool_path))\n            pool.setAutostart(True)\n            pool.create()\n            pool.refresh()\n            volume = hypervisor.storageVolLookupByPath(image)\n        else:\n            raise RuntimeError(\n                \"%s disk does not exist.\" % image)\n\n    xml = disk_xml(identifier, storage_pool.XMLDesc(0), volume.XMLDesc(0), cow)\n\n    if cow:\n        storage_pool.createXML(xml, 0)\n    else:\n        storage_pool.createXMLFrom(xml, volume, 0)", "response": "This function clones a disk image into a new disk image."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clone_disk(self, configuration):\n        disk_clone(self._hypervisor, self.identifier, self._storage_pool,\n                   configuration, self.provider_image, self.logger)\n        disk_name = self._storage_pool.listVolumes()[0]\n\n        return self._storage_pool.storageVolLookupByName(disk_name).path()", "response": "Clones the disk and returns the path to the new disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary accepts a dictionary or a path to a JSON file.", "response": "def load_configuration(configuration):\n    \"\"\"Returns a dictionary, accepts a dictionary or a path to a JSON file.\"\"\"\n    if isinstance(configuration, dict):\n        return configuration\n    else:\n        with open(configuration) as configfile:\n            return json.load(configfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allocate(self):\n        self.logger.debug(\"Allocating environment.\")\n        self._allocate()\n        self.logger.debug(\"Environment successfully allocated.\")", "response": "Builds the context and the Hooks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning up the context and the Hooks.", "response": "def deallocate(self):\n        \"\"\"Cleans up the context and the Hooks.\"\"\"\n        self.logger.debug(\"Deallocating environment.\")\n        self._deallocate()\n        self.logger.debug(\"Environment successfully deallocated.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfills the XML file with the required fields.", "response": "def domain_xml(identifier, xml, mounts, network_name=None):\n    \"\"\"Fills the XML file with the required fields.\n\n    @param identifier: (str) UUID of the Environment.\n    @param xml: (str) XML configuration of the domain.\n    @param filesystem: (tuple) ((source, target), (source, target))\n\n     * name\n     * uuid\n     * devices\n     * network\n     * filesystem\n\n    \"\"\"\n    domain = etree.fromstring(xml)\n\n    subelement(domain, './/name', 'name', identifier)\n    subelement(domain, './/uuid', 'uuid', identifier)\n    devices = subelement(domain, './/devices', 'devices', None)\n\n    for mount in mounts:\n        filesystem = etree.SubElement(devices, 'filesystem', type='mount')\n        etree.SubElement(filesystem, 'source', dir=mount[0])\n        etree.SubElement(filesystem, 'target', dir=mount[1])\n\n    if network_name is not None:\n        network = subelement(devices, './/interface[@type=\"network\"]', 'interface', None, type='network')\n        subelement(network, './/source', 'source', None, network=network_name)\n\n    return etree.tostring(domain).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef domain_create(hypervisor, identifier, configuration, network_name=None):\n    mounts = []\n\n    with open(configuration['configuration']) as config_file:\n        domain_config = config_file.read()\n\n    if 'filesystem' in configuration:\n        if isinstance(configuration['filesystem'], (list, tuple)):\n            for mount in configuration['filesystem']:\n                mounts.append(mountpoint(mount, identifier))\n        else:\n            mounts.append(mountpoint(configuration['filesystem'], identifier))\n\n    xml_config = domain_xml(identifier, domain_config, tuple(mounts), network_name=network_name)\n\n    return hypervisor.defineXML(xml_config)", "response": "Create a libvirt domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef domain_delete(domain, logger, filesystem):\n    if domain is not None:\n        try:\n            if domain.isActive():\n                domain.destroy()\n        except libvirt.libvirtError:\n            logger.exception(\"Unable to destroy the domain.\")\n        try:\n            domain.undefine()\n        except libvirt.libvirtError:\n            logger.exception(\"Unable to undefine the domain.\")\n        try:\n            if filesystem is not None and os.path.exists(filesystem):\n                shutil.rmtree(filesystem)\n        except Exception:\n            logger.exception(\"Unable to remove the shared folder.\")", "response": "libvirt domain undefinition.\n\n    @raise: libvirt.libvirtError."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hooks_factory(identifier, configuration, context):\n    manager = HookManager(identifier, configuration)\n    manager.load_hooks(context)\n\n    return manager", "response": "Returns the initialized hooks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the Hooks within the Environment.", "response": "def load_hooks(self, context):\n        \"\"\"\n        Initializes the Hooks and loads them within the Environment.\n        \"\"\"\n        for hook in self.configuration.get('hooks', ()):\n            config = hook.get('configuration', {})\n            config.update(self.configuration.get('configuration', {}))\n\n            try:\n                self._load_hook(hook['name'], config, context)\n            except KeyError:\n                self.logger.exception('Provided hook has no name: %s.', hook)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subelement(element, xpath, tag, text, **kwargs):\n    subelm = element.find(xpath)\n\n    if subelm is None:\n        subelm = etree.SubElement(element, tag)\n    else:\n        subelm.tag = tag\n\n    subelm.text = text\n    for attr, value in kwargs.items():\n        subelm.set(attr, value)\n\n    return subelm", "response": "Searches element matching the xpath and replaces it s tag text and kwargs attributes. Returns the found element or created element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenumerate over the input tuple and generate a hash using the tuple values", "response": "def get_hash_as_int(*args, group: cmod.PairingGroup = None):\n    \"\"\"\n    Enumerate over the input tuple and generate a hash using the tuple values\n\n    :param args: sequence of either group or integer elements\n    :param group: pairing group if an element is a group element\n    :return:\n    \"\"\"\n\n    group = group if group else cmod.PairingGroup(PAIRING_GROUP)\n    h_challenge = sha256()\n\n    serialedArgs = [group.serialize(arg) if isGroupElement(arg)\n                    else cmod.Conversion.IP2OS(arg)\n                    for arg in args]\n\n    for arg in sorted(serialedArgs):\n        h_challenge.update(arg)\n    return bytes_to_int(h_challenge.digest())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a random string of the specified size.", "response": "def randomString(size: int = 20,\n                 chars: str = string.ascii_letters + string.digits) -> str:\n    \"\"\"\n    Generate a random string of the specified size.\n\n    Ensure that the size is less than the length of chars as this function uses random.choice\n    which uses random sampling without replacement.\n\n    :param size: size of the random string to generate\n    :param chars: the set of characters to use to generate the random string. Uses alphanumerics by default.\n    :return: the random string generated\n    \"\"\"\n\n    return ''.join(sample(chars, size))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating 2 large primes p_prime and q_prime and use them to generate another 2 large primes p and q of 1024 bits", "response": "def genPrime():\n    \"\"\"\n    Generate 2 large primes `p_prime` and `q_prime` and use them\n    to generate another 2 primes `p` and `q` of 1024 bits\n    \"\"\"\n    prime = cmod.randomPrime(LARGE_PRIME)\n    i = 0\n    while not cmod.isPrime(2 * prime + 1):\n        prime = cmod.randomPrime(LARGE_PRIME)\n        i += 1\n    return prime"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encoded(self):\n\n        encoded = {}\n        for i in range(len(self.credType.names)):\n            self.credType.names[i]\n            attr_types = self.credType.attrTypes[i]\n\n            for at in attr_types:\n                attrName = at.name\n                if attrName in self._vals:\n                    if at.encode:\n                        encoded[attrName] = encodeAttr(self._vals[attrName])\n                    else:\n                        encoded[attrName] = self._vals[at.name]\n        return encoded", "response": "This function will encode all the attributes to 256 bit integers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate and submits a new schema.", "response": "async def genSchema(self, name, version, attrNames) -> Schema:\n        \"\"\"\n        Generates and submits Schema.\n\n        :param name: schema name\n        :param version: schema version\n        :param attrNames: a list of attributes the schema contains\n        :return: submitted Schema\n        \"\"\"\n        schema = Schema(name, version, attrNames, self.issuerId)\n        return await self.wallet.submitSchema(schema)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate and submits public and secret keys.", "response": "async def genKeys(self, schemaId: ID, p_prime=None, q_prime=None) -> (\n            PublicKey, RevocationPublicKey):\n        \"\"\"\n        Generates and submits keys (both public and secret, primary and\n        non-revocation).\n\n        :param schemaId: The schema ID (reference to claim\n        definition schema)\n        :param p_prime: optional p_prime parameter\n        :param q_prime: optional q_prime parameter\n        :return: Submitted Public keys (both primary and non-revocation)\n        \"\"\"\n        pk, sk = await self._primaryIssuer.genKeys(schemaId, p_prime, q_prime)\n        pkR, skR = await self._nonRevocationIssuer.genRevocationKeys()\n        pk = await self.wallet.submitPublicKeys(schemaId=schemaId, pk=pk,\n                                                pkR=pkR)\n        pkR = await self.wallet.submitSecretKeys(schemaId=schemaId, sk=sk,\n                                                 skR=skR)\n        return pk, pkR"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def issueAccumulator(self, schemaId: ID, iA,\n                               L) -> AccumulatorPublicKey:\n        \"\"\"\n        Issues and submits an accumulator used for non-revocation proof.\n\n        :param schemaId: The schema ID (reference to claim\n        definition schema)\n        :param iA: accumulator ID\n        :param L: maximum number of claims within accumulator.\n        :return: Submitted accumulator public key\n        \"\"\"\n        accum, tails, accPK, accSK = await self._nonRevocationIssuer.issueAccumulator(\n            schemaId, iA, L)\n        accPK = await self.wallet.submitAccumPublic(schemaId=schemaId,\n                                                    accumPK=accPK,\n                                                    accum=accum, tails=tails)\n        await self.wallet.submitAccumSecret(schemaId=schemaId,\n                                            accumSK=accSK)\n        return accPK", "response": "Issues and submits an accumulator used for revocation proof proof."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def revoke(self, schemaId: ID, i):\n        acc, ts = await self._nonRevocationIssuer.revoke(schemaId, i)\n        await self.wallet.submitAccumUpdate(schemaId=schemaId, accum=acc,\n                                            timestampMs=ts)", "response": "Performs revocation of a Claim.\n\n        :param schemaId: The schema ID (reference to claim\n        definition schema)\n        :param i: claim's sequence number within accumulator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nissues a claim for the given user and schema.", "response": "async def issueClaim(self, schemaId: ID, claimRequest: ClaimRequest,\n                         iA=None,\n                         i=None) -> (Claims, Dict[str, ClaimAttributeValues]):\n        \"\"\"\n        Issue a claim for the given user and schema.\n\n        :param schemaId: The schema ID (reference to claim\n        definition schema)\n        :param claimRequest: A claim request containing prover ID and\n        prover-generated values\n        :param iA: accumulator ID\n        :param i: claim's sequence number within accumulator\n        :return: The claim (both primary and non-revocation)\n        \"\"\"\n\n        schemaKey = (await self.wallet.getSchema(schemaId)).getKey()\n        attributes = self._attrRepo.getAttributes(schemaKey,\n                                                  claimRequest.userId)\n\n        # TODO re-enable when revocation registry is implemented\n        # iA = iA if iA else (await self.wallet.getAccumulator(schemaId)).iA\n\n        # TODO this has un-obvious side-effects\n        await self._genContxt(schemaId, iA, claimRequest.userId)\n\n        (c1, claim) = await self._issuePrimaryClaim(schemaId, attributes,\n                                                    claimRequest.U)\n        # TODO re-enable when revocation registry is fully implemented\n        c2 = await self._issueNonRevocationClaim(schemaId, claimRequest.Ur,\n                                                 iA,\n                                                 i) if claimRequest.Ur else None\n\n        signature = Claims(primaryClaim=c1, nonRevocClaim=c2)\n\n        return (signature, claim)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nissues claims for the given users and schemas.", "response": "async def issueClaims(self, allClaimRequest: Dict[ID, ClaimRequest]) -> \\\n            Dict[ID, Claims]:\n        \"\"\"\n        Issue claims for the given users and schemas.\n\n        :param allClaimRequest: a map of schema ID to a claim\n        request containing prover ID and prover-generated values\n        :return: The claims (both primary and non-revocation)\n        \"\"\"\n        res = {}\n        for schemaId, claimReq in allClaimRequest.items():\n            res[schemaId] = await self.issueClaim(schemaId, claimReq)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying a proof from the prover.", "response": "async def verify(self, proofRequest: ProofRequest, proof: FullProof):\n        \"\"\"\n        Verifies a proof from the prover.\n\n        :param proofRequest: description of a proof to be presented (revealed\n        attributes, predicates, timestamps for non-revocation)\n        :param proof: a proof\n        :return: True if verified successfully and false otherwise.\n        \"\"\"\n\n        if proofRequest.verifiableAttributes.keys() != proof.requestedProof.revealed_attrs.keys():\n            raise ValueError('Received attributes ={} do not correspond to requested={}'.format(\n                proof.requestedProof.revealed_attrs.keys(), proofRequest.verifiableAttributes.keys()))\n\n        if proofRequest.predicates.keys() != proof.requestedProof.predicates.keys():\n            raise ValueError('Received predicates ={} do not correspond to requested={}'.format(\n                proof.requestedProof.predicates.keys(), proofRequest.predicates.keys()))\n\n        TauList = []\n        for (uuid, proofItem) in proof.proofs.items():\n            if proofItem.proof.nonRevocProof:\n                TauList += await self._nonRevocVerifier.verifyNonRevocation(\n                    proofRequest, proofItem.schema_seq_no, proof.aggregatedProof.cHash,\n                    proofItem.proof.nonRevocProof)\n            if proofItem.proof.primaryProof:\n                TauList += await self._primaryVerifier.verify(proofItem.schema_seq_no,\n                                                              proof.aggregatedProof.cHash,\n                                                              proofItem.proof.primaryProof)\n\n        CHver = self._get_hash(proof.aggregatedProof.CList, self._prepare_collection(TauList),\n                               cmod.integer(proofRequest.nonce))\n\n        return CHver == proof.aggregatedProof.cHash"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a ClaimRequest object for the specified schema ID.", "response": "async def createClaimRequest(self, schemaId: ID, proverId=None,\n                                 reqNonRevoc=True) -> ClaimRequest:\n        \"\"\"\n        Creates a claim request to the issuer.\n\n        :param schemaId: The schema ID (reference to claim\n        definition schema)\n        :param proverId: a prover ID request a claim for (if None then\n        the current prover default ID is used)\n        :param reqNonRevoc: whether to request non-revocation claim\n        :return: Claim Request\n        \"\"\"\n        await self._genMasterSecret(schemaId)\n        U = await self._genU(schemaId)\n        Ur = None if not reqNonRevoc else await self._genUr(schemaId)\n        proverId = proverId if proverId else self.proverId\n        return ClaimRequest(userId=proverId, U=U, Ur=Ur)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def createClaimRequests(self, schemaIds: Sequence[ID],\n                                  proverId=None,\n                                  reqNonRevoc=True) -> Dict[ID, ClaimRequest]:\n        \"\"\"\n        Creates a claim request to the issuer.\n\n        :param schemaIds: The schema IDs (references to claim\n        definition schema)\n        :param proverId: a prover ID request a claim for (if None then\n        the current prover default ID is used)\n        :param reqNonRevoc: whether to request non-revocation claim\n        :return: a dictionary of Claim Requests for each Schema.\n        \"\"\"\n        res = {}\n        for schemaId in schemaIds:\n            res[schemaId] = await self.createClaimRequest(schemaId,\n                                                          proverId,\n                                                          reqNonRevoc)\n        return res", "response": "Creates a Claim Request for each schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def processClaim(self, schemaId: ID, claimAttributes: Dict[str, ClaimAttributeValues], signature: Claims):\n        await self.wallet.submitContextAttr(schemaId, signature.primaryClaim.m2)\n        await self.wallet.submitClaimAttributes(schemaId, claimAttributes)\n\n        await self._initPrimaryClaim(schemaId, signature.primaryClaim)\n        if signature.nonRevocClaim:\n            await self._initNonRevocationClaim(schemaId, signature.nonRevocClaim)", "response": "Processes and saves a received Claim for the given Schema."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess and saves received Claims.", "response": "async def processClaims(self, allClaims: Dict[ID, Claims]):\n        \"\"\"\n        Processes and saves received Claims.\n\n        :param claims: claims to be processed and saved for each claim\n        definition.\n        \"\"\"\n        res = []\n        for schemaId, (claim_signature, claim_attributes) in allClaims.items():\n            res.append(await self.processClaim(schemaId, claim_attributes, claim_signature))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npresent a proof to the verifier.", "response": "async def presentProof(self, proofRequest: ProofRequest) -> FullProof:\n        \"\"\"\n        Presents a proof to the verifier.\n\n        :param proofRequest: description of a proof to be presented (revealed\n        attributes, predicates, timestamps for non-revocation)\n        :return: a proof (both primary and non-revocation) and revealed attributes (initial non-encoded values)\n        \"\"\"\n        claims, requestedProof = await self._findClaims(proofRequest)\n        proof = await self._prepareProof(claims, proofRequest.nonce, requestedProof)\n        return proof"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsigned_hex_to_signed_int(hex_string: str) -> int:\n    v = struct.unpack(\n        'q', struct.pack('Q', int(hex_string, 16)))[0]  # type: int\n    return v", "response": "Converts a 64 - bit hex string to a signed int value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef signed_int_to_unsigned_hex(signed_int: int) -> str:\n    hex_string = hex(struct.unpack('Q', struct.pack('q', signed_int))[0])[2:]\n    if hex_string.endswith('L'):\n        return hex_string[:-1]\n    return hex_string", "response": "Converts a signed int value to a 64 - bit hex string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(app: Application,\n          tracer: Tracer, *,\n          skip_routes: Optional[AbstractRoute] = None,\n          tracer_key: str = APP_AIOZIPKIN_KEY,\n          request_key: str = REQUEST_AIOZIPKIN_KEY) -> Application:\n    \"\"\"Sets required parameters in aiohttp applications for aiozipkin.\n\n    Tracer added into application context and cleaned after application\n    shutdown. You can provide custom tracer_key, if default name is not\n    suitable.\n    \"\"\"\n    app[tracer_key] = tracer\n    m = middleware_maker(skip_routes=skip_routes,\n                         tracer_key=tracer_key,\n                         request_key=request_key)\n    app.middlewares.append(m)\n\n    # register cleanup signal to close zipkin transport connections\n    async def close_aiozipkin(app: Application) -> None:\n        await app[tracer_key].close()\n\n    app.on_cleanup.append(close_aiozipkin)\n\n    return app", "response": "Setup the aiohttp application for aiozipkin."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns tracer object from application context.", "response": "def get_tracer(\n        app: Application, tracer_key: str = APP_AIOZIPKIN_KEY) -> Tracer:\n    \"\"\"Returns tracer object from application context.\n\n    By default tracer has APP_AIOZIPKIN_KEY in aiohttp application context,\n    you can provide own key, if for some reason default one is not suitable.\n    \"\"\"\n    return cast(Tracer, app[tracer_key])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn span created by middleware from request context.", "response": "def request_span(request: Request,\n                 request_key: str = REQUEST_AIOZIPKIN_KEY) -> SpanAbc:\n    \"\"\"Returns span created by middleware from request context, you can use it\n    as parent on next child span.\n    \"\"\"\n    return cast(SpanAbc, request[request_key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a aiohttp. TraceConfig with enabled aiozipking instrumentation for aiohttp client.", "response": "def make_trace_config(tracer: Tracer) -> aiohttp.TraceConfig:\n    \"\"\"Creates aiohttp.TraceConfig with enabled aiozipking instrumentation\n    for aiohttp client.\n    \"\"\"\n    trace_config = aiohttp.TraceConfig()\n    zipkin = ZipkinClientSignals(tracer)\n\n    trace_config.on_request_start.append(zipkin.on_request_start)\n    trace_config.on_request_end.append(zipkin.on_request_end)\n    trace_config.on_request_exception.append(zipkin.on_request_exception)\n    return trace_config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a zipkin timestamp in microseconds or convert available one from second.", "response": "def make_timestamp(ts: OptTs = None) -> int:\n    \"\"\"Create zipkin timestamp in microseconds, or convert available one\n    from second. Useful when user supplies ts from time.time() call.\n    \"\"\"\n    ts = ts if ts is not None else time.time()\n    return int(ts * 1000 * 1000)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate dict with zipkin headers from supplied trace context.", "response": "def make_headers(context: TraceContext) -> Headers:\n    \"\"\"Creates dict with zipkin headers from supplied trace context.\n    \"\"\"\n    headers = {\n        TRACE_ID_HEADER: context.trace_id,\n        SPAN_ID_HEADER: context.span_id,\n        FLAGS_HEADER: '0',\n        SAMPLED_ID_HEADER: '1' if context.sampled else '0',\n    }\n    if context.parent_id is not None:\n        headers[PARENT_ID_HEADER] = context.parent_id\n    return headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates dict with zipkin single header format.", "response": "def make_single_header(context: TraceContext) -> Headers:\n    \"\"\"Creates dict with zipkin single header format.\n    \"\"\"\n    # b3={TraceId}-{SpanId}-{SamplingState}-{ParentSpanId}\n    c = context\n\n    # encode sampled flag\n    if c.debug:\n        sampled = 'd'\n    elif c.sampled:\n        sampled = '1'\n    else:\n        sampled = '0'\n\n    params = [c.trace_id, c.span_id, sampled]  # type: List[str]\n    if c.parent_id is not None:\n        params.append(c.parent_id)\n\n    h = DELIMITER.join(params)\n    headers = {SINGLE_HEADER: h}\n    return headers"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert available headers to TraceContext if headers mapping does not contain zipkin headers function returns None.", "response": "def make_context(headers: Headers) -> Optional[TraceContext]:\n    \"\"\"Converts available headers to TraceContext, if headers mapping does\n    not contain zipkin headers, function returns None.\n    \"\"\"\n    # TODO: add validation for trace_id/span_id/parent_id\n\n    # normalize header names just in case someone passed regular dict\n    # instead dict with case insensitive keys\n    headers = {k.lower(): v for k, v in headers.items()}\n\n    required = (TRACE_ID_HEADER.lower(), SPAN_ID_HEADER.lower())\n    has_b3 = all(h in headers for h in required)\n    has_b3_single = SINGLE_HEADER in headers\n\n    if not(has_b3_single or has_b3):\n        return None\n\n    if has_b3:\n        debug = parse_debug_header(headers)\n        sampled = debug if debug else parse_sampled_header(headers)\n        context = TraceContext(\n            trace_id=headers[TRACE_ID_HEADER.lower()],\n            parent_id=headers.get(PARENT_ID_HEADER.lower()),\n            span_id=headers[SPAN_ID_HEADER.lower()],\n            sampled=sampled,\n            debug=debug,\n            shared=False,\n        )\n        return context\n    return _parse_single_header(headers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_none(data: Dict[str, Any],\n                keys: OptKeys = None) -> Dict[str, Any]:\n    \"\"\"Filter keys from dict with None values.\n\n    Check occurs only on root level. If list of keys specified, filter\n    works only for selected keys\n    \"\"\"\n\n    def limited_filter(k: str, v: Any) -> bool:\n        return k not in keys or v is not None  # type: ignore\n\n    def full_filter(k: str, v: Any) -> bool:\n        return v is not None\n\n    f = limited_filter if keys is not None else full_filter\n    return {k: v for k, v in data.items() if f(k, v)}", "response": "Filter keys from dict with None values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes term 3 in equation 2 page 462.", "response": "def _compute_term_3(self, C, rhypo):\n        \"\"\"\n        Compute term 3 in equation 2 page 462.\n        Distances are clipped at 15 km (as per Ezio Faccioli's personal\n        communication.)\n        \"\"\"\n        d = rhypo\n        d[d <= 15.0] = 15.0\n\n        return C['a3'] * np.log10(d)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the site term as a function of vs30 in the innermost level in the system.", "response": "def _compute_site_term(self, C, vs30):\n        \"\"\"\n        Compute site term as a function of vs30: 4th, 5th and 6th terms in\n        equation 2 page 462.\n        \"\"\"\n        # for rock values the site term is zero\n        site_term = np.zeros_like(vs30)\n\n        # hard soil\n        site_term[(vs30 >= 360) & (vs30 < 800)] = C['aB']\n\n        # medium soil\n        site_term[(vs30 >= 180) & (vs30 < 360)] = C['aC']\n\n        # soft soil\n        site_term[vs30 < 180] = C['aD']\n\n        return site_term"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute faulting style term as defined in equation 5 page 465.", "response": "def _compute_faulting_style_term(self, C, rake):\n        \"\"\"\n        Compute faulting style term as a function of rake angle value as given\n        in equation 5 page 465.\n        \"\"\"\n        if rake > -120.0 and rake <= -60.0:\n            return C['aN']\n        elif rake > 30.0 and rake <= 150.0:\n            return C['aR']\n        else:\n            return C['aS']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and return the mean value for PGV PGA and Displacement responce spectrum PGA and faulting related terms.", "response": "def _compute_mean(self, C, mag, dists, vs30, rake, imt):\n        \"\"\"\n        Compute mean value for PGV, PGA and Displacement responce spectrum,\n        as given in equation 2, page 462 with the addition of the faulting\n        style term as given in equation 5, page 465. Converts also\n        displacement responce spectrum values to SA.\n        \"\"\"\n        mean = (self._compute_term_1_2(C, mag) +\n                self._compute_term_3(C, dists.rhypo) +\n                self._compute_site_term(C, vs30) +\n                self._compute_faulting_style_term(C, rake))\n\n        # convert from cm/s**2 to g for SA and from m/s**2 to g for PGA (PGV\n        # is already in cm/s) and also convert from base 10 to base e.\n        if imt.name == \"PGA\":\n            mean = np.log((10 ** mean) / g)\n        elif imt.name == \"SA\":\n            mean = np.log((10 ** mean) * ((2 * np.pi / imt.period) ** 2) *\n                          1e-2 / g)\n        else:\n            mean = np.log(10 ** mean)\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_stddevs(self, C, stddev_types, num_sites):\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            stddevs.append(np.log(10 ** C['sigma']) + np.zeros(num_sites))\n\n        return stddevs", "response": "Return total standard deviation as defined in equation 2 page 970."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an earthquake rupture, the ground motion field calculator computes ground shaking over a set of sites, by randomly sampling a ground shaking intensity model. A ground motion field represents a possible 'realization' of the ground shaking due to an earthquake rupture. .. note:: This calculator is using random numbers. In order to reproduce the same results numpy random numbers generator needs to be seeded, see http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html :param openquake.hazardlib.source.rupture.Rupture rupture: Rupture to calculate ground motion fields radiated from. :param openquake.hazardlib.site.SiteCollection sites: Sites of interest to calculate GMFs. :param imts: List of intensity measure type objects (see :mod:`openquake.hazardlib.imt`). :param gsim: Ground-shaking intensity model, instance of subclass of either :class:`~openquake.hazardlib.gsim.base.GMPE` or :class:`~openquake.hazardlib.gsim.base.IPE`. :param truncation_level: Float, number of standard deviations for truncation of the intensity distribution, or ``None``. :param realizations: Integer number of GMF realizations to compute. :param correlation_model: Instance of correlation model object. See :mod:`openquake.hazardlib.correlation`. Can be ``None``, in which case non-correlated ground motion fields are calculated. Correlation model is not used if ``truncation_level`` is zero. :param int seed: The seed used in the numpy random number generator :returns: Dictionary mapping intensity measure type objects (same as in parameter ``imts``) to 2d numpy arrays of floats, representing different realizations of ground shaking intensity for all sites in the collection. First dimension represents sites and second one is for realizations.", "response": "def ground_motion_fields(rupture, sites, imts, gsim, truncation_level,\n                         realizations, correlation_model=None, seed=None):\n    \"\"\"\n    Given an earthquake rupture, the ground motion field calculator computes\n    ground shaking over a set of sites, by randomly sampling a ground shaking\n    intensity model. A ground motion field represents a possible 'realization'\n    of the ground shaking due to an earthquake rupture.\n\n    .. note::\n\n     This calculator is using random numbers. In order to reproduce the\n     same results numpy random numbers generator needs to be seeded, see\n     http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html\n\n    :param openquake.hazardlib.source.rupture.Rupture rupture:\n        Rupture to calculate ground motion fields radiated from.\n    :param openquake.hazardlib.site.SiteCollection sites:\n        Sites of interest to calculate GMFs.\n    :param imts:\n        List of intensity measure type objects (see\n        :mod:`openquake.hazardlib.imt`).\n    :param gsim:\n        Ground-shaking intensity model, instance of subclass of either\n        :class:`~openquake.hazardlib.gsim.base.GMPE` or\n        :class:`~openquake.hazardlib.gsim.base.IPE`.\n    :param truncation_level:\n        Float, number of standard deviations for truncation of the intensity\n        distribution, or ``None``.\n    :param realizations:\n        Integer number of GMF realizations to compute.\n    :param correlation_model:\n        Instance of correlation model object. See\n        :mod:`openquake.hazardlib.correlation`. Can be ``None``, in which case\n        non-correlated ground motion fields are calculated. Correlation model\n        is not used if ``truncation_level`` is zero.\n    :param int seed:\n        The seed used in the numpy random number generator\n    :returns:\n        Dictionary mapping intensity measure type objects (same\n        as in parameter ``imts``) to 2d numpy arrays of floats,\n        representing different realizations of ground shaking intensity\n        for all sites in the collection. First dimension represents\n        sites and second one is for realizations.\n    \"\"\"\n    cmaker = ContextMaker(rupture.tectonic_region_type, [gsim])\n    gc = GmfComputer(rupture, sites, [str(imt) for imt in imts],\n                     cmaker, truncation_level, correlation_model)\n    res, _sig, _eps = gc.compute(gsim, realizations, seed)\n    return {imt: res[imti] for imti, imt in enumerate(gc.imts)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute(self, gsim, num_events, seed=None):\n        try:  # read the seed from self.rupture.serial\n            seed = seed or self.rupture.serial\n        except AttributeError:\n            pass\n        if seed is not None:\n            numpy.random.seed(seed)\n        result = numpy.zeros((len(self.imts), len(self.sids), num_events), F32)\n        sig = numpy.zeros((len(self.imts), num_events), F32)\n        eps = numpy.zeros((len(self.imts), num_events), F32)\n        for imti, imt in enumerate(self.imts):\n            if isinstance(gsim, MultiGMPE):\n                gs = gsim[str(imt)]  # MultiGMPE\n            else:\n                gs = gsim  # regular GMPE\n            try:\n                result[imti], sig[imti], eps[imti] = self._compute(\n                    None, gs, num_events, imt)\n            except Exception as exc:\n                raise exc.__class__(\n                    '%s for %s, %s, srcidx=%s' % (exc, gs, imt, self.srcidx)\n                ).with_traceback(exc.__traceback__)\n        return result, sig, eps", "response": "Compute the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute(self, seed, gsim, num_events, imt):\n        rctx = getattr(self.rupture, 'rupture', self.rupture)\n        if seed is not None:\n            numpy.random.seed(seed)\n        dctx = self.dctx.roundup(gsim.minimum_distance)\n        if self.truncation_level == 0:\n            assert self.correlation_model is None\n            mean, _stddevs = gsim.get_mean_and_stddevs(\n                self.sctx, rctx, dctx, imt, stddev_types=[])\n            mean = gsim.to_imt_unit_values(mean)\n            mean.shape += (1, )\n            mean = mean.repeat(num_events, axis=1)\n            return (mean,\n                    numpy.zeros(num_events, F32),\n                    numpy.zeros(num_events, F32))\n        elif self.truncation_level is None:\n            distribution = scipy.stats.norm()\n        else:\n            assert self.truncation_level > 0\n            distribution = scipy.stats.truncnorm(\n                - self.truncation_level, self.truncation_level)\n\n        num_sids = len(self.sids)\n        if gsim.DEFINED_FOR_STANDARD_DEVIATION_TYPES == {StdDev.TOTAL}:\n            # If the GSIM provides only total standard deviation, we need\n            # to compute mean and total standard deviation at the sites\n            # of interest.\n            # In this case, we also assume no correlation model is used.\n            if self.correlation_model:\n                raise CorrelationButNoInterIntraStdDevs(\n                    self.correlation_model, gsim)\n\n            mean, [stddev_total] = gsim.get_mean_and_stddevs(\n                self.sctx, rctx, dctx, imt, [StdDev.TOTAL])\n            stddev_total = stddev_total.reshape(stddev_total.shape + (1, ))\n            mean = mean.reshape(mean.shape + (1, ))\n\n            total_residual = stddev_total * rvs(\n                distribution, num_sids, num_events)\n            gmf = gsim.to_imt_unit_values(mean + total_residual)\n            stddev_inter = numpy.empty(num_events, F32)\n            stddev_inter.fill(numpy.nan)\n            epsilons = numpy.empty(num_events, F32)\n            epsilons.fill(numpy.nan)\n        else:\n            mean, [stddev_inter, stddev_intra] = gsim.get_mean_and_stddevs(\n                self.sctx, rctx, dctx, imt,\n                [StdDev.INTER_EVENT, StdDev.INTRA_EVENT])\n            stddev_intra = stddev_intra.reshape(stddev_intra.shape + (1, ))\n            stddev_inter = stddev_inter.reshape(stddev_inter.shape + (1, ))\n            mean = mean.reshape(mean.shape + (1, ))\n            intra_residual = stddev_intra * rvs(\n                distribution, num_sids, num_events)\n\n            if self.correlation_model is not None:\n                ir = self.correlation_model.apply_correlation(\n                    self.sites, imt, intra_residual, stddev_intra)\n                # this fixes a mysterious bug: ir[row] is actually\n                # a matrix of shape (E, 1) and not a vector of size E\n                intra_residual = numpy.zeros(ir.shape)\n                for i, val in numpy.ndenumerate(ir):\n                    intra_residual[i] = val\n\n            epsilons = rvs(distribution, num_events)\n            inter_residual = stddev_inter * epsilons\n\n            gmf = gsim.to_imt_unit_values(\n                mean + intra_residual + inter_residual)\n        return gmf, stddev_inter.max(axis=0), epsilons", "response": "Compute the mean and total standard deviation at the seismic events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring a new oqdata directory from the data contained in the zip archive", "response": "def restore(archive, oqdata):\n    \"\"\"\n    Build a new oqdata directory from the data contained in the zip archive\n    \"\"\"\n    if os.path.exists(oqdata):\n        sys.exit('%s exists already' % oqdata)\n    if '://' in archive:\n        # get the zip archive from an URL\n        resp = requests.get(archive)\n        _, archive = archive.rsplit('/', 1)\n        with open(archive, 'wb') as f:\n            f.write(resp.content)\n    if not os.path.exists(archive):\n        sys.exit('%s does not exist' % archive)\n    t0 = time.time()\n    oqdata = os.path.abspath(oqdata)\n    assert archive.endswith('.zip'), archive\n    os.mkdir(oqdata)\n    zipfile.ZipFile(archive).extractall(oqdata)\n    dbpath = os.path.join(oqdata, 'db.sqlite3')\n    db = Db(sqlite3.connect, dbpath, isolation_level=None,\n            detect_types=sqlite3.PARSE_DECLTYPES)\n    n = 0\n    for fname in os.listdir(oqdata):\n        mo = re.match('calc_(\\d+)\\.hdf5', fname)\n        if mo:\n            job_id = int(mo.group(1))\n            fullname = os.path.join(oqdata, fname)[:-5]  # strip .hdf5\n            db(\"UPDATE job SET user_name=?x, ds_calc_dir=?x WHERE id=?x\",\n               getpass.getuser(), fullname, job_id)\n            safeprint('Restoring ' + fname)\n            n += 1\n    dt = time.time() - t0\n    safeprint('Extracted %d calculations into %s in %d seconds'\n              % (n, oqdata, dt))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of slice objects that can be used to float out the rupture locations of the rupture.", "response": "def _float_ruptures(rupture_area, rupture_length, cell_area, cell_length):\n    \"\"\"\n    Get all possible unique rupture placements on the fault surface.\n\n    :param rupture_area:\n        The area of the rupture to float on the fault surface, in squared km.\n    :param rupture_length:\n        The target length (spatial extension along fault trace) of the rupture,\n        in km.\n    :param cell_area:\n        2d numpy array representing area of mesh cells in squared km.\n    :param cell_length:\n        2d numpy array of the shape as ``cell_area`` representing cells'\n        length in km.\n    :returns:\n        A list of slice objects. Number of items in the list is equal to number\n        of possible locations of the requested rupture on the fault surface.\n        Each slice can be used to get a portion of the whole fault surface mesh\n        that would represent the location of the rupture.\n    \"\"\"\n    nrows, ncols = cell_length.shape\n\n    if rupture_area >= numpy.sum(cell_area):\n        # requested rupture area exceeds the total surface area.\n        # return the single slice that doesn't cut anything out.\n        return [slice(None)]\n\n    rupture_slices = []\n\n    dead_ends = set()\n    for row in range(nrows):\n        for col in range(ncols):\n            if col in dead_ends:\n                continue\n            # find the lengths of all possible subsurfaces containing\n            # only the current row and from the current column till\n            # the last one.\n            lengths_acc = numpy.add.accumulate(cell_length[row, col:])\n            # find the \"best match\" number of columns, the one that gives\n            # the least difference between actual and requested rupture\n            # length (note that we only consider top row here, mainly\n            # for simplicity: it's not yet clear how many rows will we\n            # end up with).\n            rup_cols = numpy.argmin(numpy.abs(lengths_acc - rupture_length))\n            last_col = rup_cols + col + 1\n            if last_col == ncols and lengths_acc[rup_cols] < rupture_length:\n                # rupture doesn't fit along length (the requested rupture\n                # length is greater than the length of the part of current\n                # row that starts from the current column).\n                if col != 0:\n                    # if we are not in the first column, it means that we\n                    # hit the right border, so we need to go to the next\n                    # row.\n                    break\n\n            # now try to find the optimum (the one providing the closest\n            # to requested area) number of rows.\n            areas_acc = numpy.sum(cell_area[row:, col:last_col], axis=1)\n            areas_acc = numpy.add.accumulate(areas_acc, axis=0)\n            rup_rows = numpy.argmin(numpy.abs(areas_acc - rupture_area))\n            last_row = rup_rows + row + 1\n            if last_row == nrows and areas_acc[rup_rows] < rupture_area:\n                # rupture doesn't fit along width.\n                # we can try to extend it along length but only if we are\n                # at the first row\n                if row == 0:\n                    if last_col == ncols:\n                        # there is no place to extend, exiting\n                        return rupture_slices\n                    else:\n                        # try to extend along length\n                        areas_acc = numpy.sum(cell_area[:, col:], axis=0)\n                        areas_acc = numpy.add.accumulate(areas_acc, axis=0)\n                        rup_cols = numpy.argmin(\n                            numpy.abs(areas_acc - rupture_area))\n                        last_col = rup_cols + col + 1\n                        if last_col == ncols \\\n                                and areas_acc[rup_cols] < rupture_area:\n                            # still doesn't fit, return\n                            return rupture_slices\n                else:\n                    # row is not the first and the required area exceeds\n                    # available area starting from target row and column.\n                    # mark the column as \"dead end\" so we don't create\n                    # one more rupture from the same column on all\n                    # subsequent rows.\n                    dead_ends.add(col)\n\n            # here we add 1 to last row and column numbers because we want\n            # to return slices for cutting the mesh of vertices, not the cell\n            # data (like cell_area or cell_length).\n            rupture_slices.append((slice(row, last_row + 1),\n                                   slice(col, last_col + 1)))\n    return rupture_slices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the distance scaling term in terms of the log10 term.", "response": "def _compute_distance_term(self, C, rhypo):\n        \"\"\"\n        Returns the distance scaling term\n        \"\"\"\n        return (C[\"c2\"] * rhypo) + (C[\"c3\"] * np.log10(rhypo))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rhypo_to_rjb(rhypo, mag):\n    epsilon = rhypo - (4.853 + 1.347E-6 * (mag ** 8.163))\n    rjb = np.zeros_like(rhypo)\n    idx = epsilon >= 3.\n    rjb[idx] = np.sqrt((epsilon[idx] ** 2.) - 9.0)\n    rjb[rjb < 0.0] = 0.0\n    return rjb", "response": "Converts hypocentral distance to equivalent Joyner - Boore distance"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the mean ground motion acceleration and velocity for the current object.", "response": "def _compute_mean(self, C, rup, dists, sites, imt):\n        \"\"\"\n        Returns the mean ground motion acceleration and velocity\n        \"\"\"\n        # Convert rhypo to rrup\n        rrup = rhypo_to_rrup(dists.rhypo, rup.mag)\n        mean = (self._get_magnitude_scaling_term(C, rup.mag) +\n                self._get_distance_scaling_term(C, rup.mag, rrup) +\n                self._get_style_of_faulting_term(C, rup.rake) +\n                self._get_site_amplification_term(C, sites.vs30))\n        # convert from cm/s**2 to g for SA and from cm/s**2 to g for PGA (PGV\n        # is already in cm/s) and also convert from base 10 to base e.\n        if isinstance(imt, PGA):\n            mean = np.log((10 ** mean) * ((2 * np.pi / 0.01) ** 2) *\n                          1e-2 / g)\n        elif isinstance(imt, SA):\n            mean = np.log((10 ** mean) * ((2 * np.pi / imt.period) ** 2) *\n                          1e-2 / g)\n        else:\n            mean = np.log(10 ** mean)\n\n        return mean + self.adjustment_factor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pn(self, rup, sites, dists, sof):\n        # List must be in following order\n        p_n = []\n        # Rjb\n        # Note that Rjb must be clipped at 0.1 km\n        rjb = rhypo_to_rjb(dists.rhypo, rup.mag)\n        rjb[rjb < 0.1] = 0.1\n        p_n.append(self._get_normalised_term(np.log10(rjb),\n                                             self.CONSTANTS[\"logMaxR\"],\n                                             self.CONSTANTS[\"logMinR\"]))\n        # Magnitude\n        p_n.append(self._get_normalised_term(rup.mag,\n                                             self.CONSTANTS[\"maxMw\"],\n                                             self.CONSTANTS[\"minMw\"]))\n        # Vs30\n        p_n.append(self._get_normalised_term(np.log10(sites.vs30),\n                                             self.CONSTANTS[\"logMaxVs30\"],\n                                             self.CONSTANTS[\"logMinVs30\"]))\n        # Depth\n        p_n.append(self._get_normalised_term(rup.hypo_depth,\n                                             self.CONSTANTS[\"maxD\"],\n                                             self.CONSTANTS[\"minD\"]))\n        # Style of Faulting\n        p_n.append(self._get_normalised_term(sof,\n                                             self.CONSTANTS[\"maxFM\"],\n                                             self.CONSTANTS[\"minFM\"]))\n        return p_n", "response": "Returns a list of normalised terms for the internal structure of the base system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a numpy dtype for a nested record with keys idx and a nested record with keys imt and intensity measure type strings.", "response": "def gsim_imt_dt(sorted_gsims, sorted_imts):\n    \"\"\"\n    Build a numpy dtype as a nested record with keys 'idx' and nested\n    (gsim, imt).\n\n    :param sorted_gsims: a list of GSIM instances, sorted lexicographically\n    :param sorted_imts: a list of intensity measure type strings\n    \"\"\"\n    dtlist = [(imt, numpy.float32) for imt in sorted_imts]\n    imt_dt = numpy.dtype(dtlist)\n    return numpy.dtype([(str(gsim), imt_dt) for gsim in sorted_gsims])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the survival function for the given values in the given distribution.", "response": "def _truncnorm_sf(truncation_level, values):\n    \"\"\"\n    Survival function for truncated normal distribution.\n\n    Assumes zero mean, standard deviation equal to one and symmetric\n    truncation.\n\n    :param truncation_level:\n        Positive float number representing the truncation on both sides\n        around the mean, in units of sigma.\n    :param values:\n        Numpy array of values as input to a survival function for the given\n        distribution.\n    :returns:\n        Numpy array of survival function results in a range between 0 and 1.\n\n    >>> from scipy.stats import truncnorm\n    >>> truncnorm(-3, 3).sf(0.12345) == _truncnorm_sf(3, 0.12345)\n    True\n    \"\"\"\n    # notation from http://en.wikipedia.org/wiki/Truncated_normal_distribution.\n    # given that mu = 0 and sigma = 1, we have alpha = a and beta = b.\n\n    # \"CDF\" in comments refers to cumulative distribution function\n    # of non-truncated distribution with that mu and sigma values.\n\n    # assume symmetric truncation, that is ``a = - truncation_level``\n    # and ``b = + truncation_level``.\n\n    # calculate CDF of b\n    phi_b = ndtr(truncation_level)\n\n    # calculate Z as ``Z = CDF(b) - CDF(a)``, here we assume that\n    # ``CDF(a) == CDF(- truncation_level) == 1 - CDF(b)``\n    z = phi_b * 2 - 1\n\n    # calculate the result of survival function of ``values``,\n    # and restrict it to the interval where probability is defined --\n    # 0..1. here we use some transformations of the original formula\n    # that is ``SF(x) = 1 - (CDF(x) - CDF(a)) / Z`` in order to minimize\n    # number of arithmetic operations and function calls:\n    # ``SF(x) = (Z - CDF(x) + CDF(a)) / Z``,\n    # ``SF(x) = (CDF(b) - CDF(a) - CDF(x) + CDF(a)) / Z``,\n    # ``SF(x) = (CDF(b) - CDF(x)) / Z``.\n    return ((phi_b - ndtr(values)) / z).clip(0.0, 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of values into a numpy array of natural logarithms of values.", "response": "def to_distribution_values(self, values):\n        \"\"\"\n        Returns numpy array of natural logarithms of ``values``.\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            # avoid RuntimeWarning: divide by zero encountered in log\n            return numpy.log(values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_parameters(self):\n        for key in (ADMITTED_STR_PARAMETERS + ADMITTED_FLOAT_PARAMETERS +\n                    ADMITTED_SET_PARAMETERS):\n            try:\n                val = getattr(self.gmpe, key)\n            except AttributeError:\n                pass\n            else:\n                setattr(self, key, val)", "response": "Sets the parameters of the GMPE object to the values of the GMPE attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_table_from_str(self, table, sa_damping):\n        table = table.strip().splitlines()\n        header = table.pop(0).split()\n        if not header[0].upper() == \"IMT\":\n            raise ValueError('first column in a table must be IMT')\n        coeff_names = header[1:]\n        for row in table:\n            row = row.split()\n            imt_name = row[0].upper()\n            if imt_name == 'SA':\n                raise ValueError('specify period as float value '\n                                 'to declare SA IMT')\n            imt_coeffs = dict(zip(coeff_names, map(float, row[1:])))\n            try:\n                sa_period = float(imt_name)\n            except Exception:\n                if imt_name not in imt_module.registry:\n                    raise ValueError('unknown IMT %r' % imt_name)\n                imt = imt_module.registry[imt_name]()\n                self.non_sa_coeffs[imt] = imt_coeffs\n            else:\n                if sa_damping is None:\n                    raise TypeError('attribute \"sa_damping\" is required '\n                                    'for tables defining SA')\n                imt = imt_module.SA(sa_period, sa_damping)\n                self.sa_coeffs[imt] = imt_coeffs", "response": "Builds the input tables from a string definition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the distance scaling term in equation 1 page 74.", "response": "def get_distance_term(self, C, rrup, mag):\n        \"\"\"\n        Returns distance scaling term\n        \"\"\"\n        return (C[\"r1\"] + C[\"r2\"] * mag) *\\\n            np.log(np.sqrt(rrup ** 2. + C[\"h1\"] ** 2.))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the standard deviations for the resource class as described in paragraph 2. 1 page 1070.", "response": "def get_stddevs(self, C, nsites, stddev_types):\n        \"\"\"\n        Returns the standard deviations\n        \"\"\"\n        stddevs = []\n        zeros_array = np.zeros(nsites)\n        for stddev in stddev_types:\n            assert stddev in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(C[\"tau\"] ** 2. + C[\"phi\"] ** 2.) +\n                               zeros_array)\n            elif stddev == const.StdDev.INTER_EVENT:\n                stddevs.append(C[\"tau\"] + zeros_array)\n            elif stddev == const.StdDev.INTRA_EVENT:\n                stddevs.append(C[\"phi\"] + zeros_array)\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sigma(imt):\n    if imt.period < 0.2:\n        return np.log(10**0.23)\n    elif imt.period > 1.0:\n        return np.log(10**0.27)\n    else:\n        return np.log(10**(0.23 + (imt.period - 0.2)/0.8 * 0.04))", "response": "Returns the value of the total sigma value of the total base base."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the config has each field in fields_spec.", "response": "def check_config(self, config, fields_spec):\n        \"\"\"\n        Check that `config` has each field in `fields_spec` if a default\n        has not been provided.\n        \"\"\"\n        for field, type_info in fields_spec.items():\n            has_default = not isinstance(type_info, type)\n            if field not in config and not has_default:\n                raise RuntimeError(\n                    \"Configuration not complete. %s missing\" % field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets default values got from fields_spec into the config dictionary.", "response": "def set_defaults(self, config, fields_spec):\n        \"\"\"\n        Set default values got from `fields_spec` into the `config`\n        dictionary\n        \"\"\"\n        defaults = dict([(f, d)\n                         for f, d in fields_spec.items()\n                         if not isinstance(d, type)])\n        for field, default_value in defaults.items():\n            if field not in config:\n                config[field] = default_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, method_name, completeness=False, **fields):\n        def class_decorator(class_obj):\n            original_method = getattr(class_obj, method_name)\n            if sys.version[0] == '2':  # Python 2\n                original_method = original_method.im_func\n\n            def caller(fn, obj, catalogue, config=None, *args, **kwargs):\n                config = config or {}\n                self.set_defaults(config, fields)\n                self.check_config(config, fields)\n                return fn(obj, catalogue, config, *args, **kwargs)\n            new_method = decorator(caller, original_method)\n            setattr(class_obj, method_name, new_method)\n            instance = class_obj()\n            func = functools.partial(new_method, instance)\n            func.fields = fields\n            func.model = instance\n            func.completeness = completeness\n            functools.update_wrapper(func, new_method)\n            self[class_obj.__name__] = func\n            return class_obj\n        return class_decorator", "response": "A class decorator that adds a method to the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_function(self, completeness=False, **fields):\n        def dec(fn):\n            if completeness:\n                def fn_with_config_and_c(\n                        catalogue, config, completeness_table=None):\n                    return fn(catalogue, completeness_table, **config)\n                fn_with_config = fn_with_config_and_c\n            else:\n                def fn_with_config_without_c(catalogue, config):\n                    return fn(catalogue, **config)\n                fn_with_config = fn_with_config_without_c\n            fn_with_config.fields = fields\n            fn_with_config.completeness = completeness\n            fn.fields = fields\n            self[fn.__name__] = fn_with_config\n            return fn\n        return dec", "response": "This is a decorator that adds a function to the registry by adding a call to set_defaults and check_config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        C = self.COEFFS[imt]\n        mean = (np.log(self.get_magnitude_term(C, rup) +\n                       self.get_distance_term(C, dists.rrup)) +\n                self.get_site_amplification(C, sites))\n        stddevs = self.get_stddevs(C, sites.vs30.shape, rup.mag, stddev_types)\n        return mean, stddevs", "response": "Returns the mean and standard deviation of the site in the site table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_magnitude_term(self, C, rup):\n        b0, stress_drop = self._get_sof_terms(C, rup.rake)\n        if rup.mag <= C[\"m1\"]:\n            return b0\n        else:\n            # Calculate moment (equation 5)\n            m_0 = 10.0 ** (1.5 * rup.mag + 16.05)\n            # Get stress-drop scaling (equation 6)\n            if rup.mag > C[\"m2\"]:\n                stress_drop += (C[\"b2\"] * (C[\"m2\"] - self.CONSTANTS[\"mstar\"]) +\n                                (C[\"b3\"] * (rup.mag - C[\"m2\"])))\n            else:\n                stress_drop += (C[\"b2\"] * (rup.mag - self.CONSTANTS[\"mstar\"]))\n            stress_drop = np.exp(stress_drop)\n            # Get corner frequency (equation 4)\n            f0 = 4.9 * 1.0E6 * 3.2 * ((stress_drop / m_0) ** (1. / 3.))\n            return 1. / f0", "response": "Returns the magnitude scaling term in equation 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_distance_term(self, C, rrup):\n        f_p = C[\"c1\"] * rrup\n        idx = np.logical_and(rrup > self.CONSTANTS[\"r1\"],\n                             rrup <= self.CONSTANTS[\"r2\"])\n        f_p[idx] = (C[\"c1\"] * self.CONSTANTS[\"r1\"]) +\\\n            C[\"c2\"] * (rrup[idx] - self.CONSTANTS[\"r1\"])\n        idx = rrup > self.CONSTANTS[\"r2\"]\n        f_p[idx] = C[\"c1\"] * self.CONSTANTS[\"r1\"] +\\\n            C[\"c2\"] * (self.CONSTANTS[\"r2\"] - self.CONSTANTS[\"r1\"]) +\\\n            C[\"c3\"] * (rrup[idx] - self.CONSTANTS[\"r2\"])\n        return f_p", "response": "Returns the distance scaling term in equation 7"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_sof_terms(self, C, rake):\n        if rake >= 45.0 and rake <= 135.0:\n            # Reverse faulting\n            return C[\"b0R\"], C[\"b1R\"]\n        elif rake <= -45. and rake >= -135.0:\n            # Normal faulting\n            return C[\"b0N\"], C[\"b1N\"]\n        else:\n            # Strike slip\n            return C[\"b0SS\"], C[\"b1SS\"]", "response": "Returns the style - of - faulting scaling parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the site amplification term for a given set of sites.", "response": "def get_site_amplification(self, C, sites):\n        \"\"\"\n        Returns the site amplification term\n        \"\"\"\n        # Gets delta normalised z1\n        dz1 = sites.z1pt0 - np.exp(self._get_lnmu_z1(sites.vs30))\n        f_s = C[\"c5\"] * dz1\n        # Calculates site amplification term\n        f_s[dz1 > self.CONSTANTS[\"dz1ref\"]] = (C[\"c5\"] *\n                                               self.CONSTANTS[\"dz1ref\"])\n        idx = sites.vs30 > self.CONSTANTS[\"v1\"]\n        f_s[idx] += (C[\"c4\"] * np.log(self.CONSTANTS[\"v1\"] / C[\"vref\"]))\n        idx = np.logical_not(idx)\n        f_s[idx] += (C[\"c4\"] * np.log(sites.vs30[idx] / C[\"vref\"]))\n        return f_s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the standard deviations for the resource class as described in paragraph 2. 1 page 970.", "response": "def get_stddevs(self, C, nsites, mag, stddev_types):\n        \"\"\"\n        Returns the standard deviations\n        \"\"\"\n        tau = self._get_tau(C, mag) + np.zeros(nsites)\n        phi = self._get_phi(C, mag) + np.zeros(nsites)\n        stddevs = []\n        for stddev in stddev_types:\n            assert stddev in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(tau ** 2. + phi ** 2.))\n            elif stddev == const.StdDev.INTER_EVENT:\n                stddevs.append(tau)\n            elif stddev == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi)\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning magnitude dependent inter - event standard deviation", "response": "def _get_tau(self, C, mag):\n        \"\"\"\n        Returns magnitude dependent inter-event standard deviation (tau)\n        (equation 14)\n        \"\"\"\n        if mag < 6.5:\n            return C[\"tau1\"]\n        elif mag < 7.:\n            return C[\"tau1\"] + (C[\"tau2\"] - C[\"tau1\"]) * ((mag - 6.5) / 0.5)\n        else:\n            return C[\"tau2\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_phi(self, C, mag):\n        if mag < 5.5:\n            return C[\"phi1\"]\n        elif mag < 5.75:\n            return C[\"phi1\"] + (C[\"phi2\"] - C[\"phi1\"]) * ((mag - 5.5) / 0.25)\n        else:\n            return C[\"phi2\"]", "response": "Returns the intra - event standard deviation in the specified magnitude dependent intra - event system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute mean according to equation on Table 2 page 2275.", "response": "def _compute_mean(self, C, g, mag, hypo_depth, dists, imt):\n        \"\"\"\n        Compute mean according to equation on Table 2, page 2275.\n        \"\"\"\n\n        delta = 0.00750 * 10 ** (0.507 * mag)\n\n        # computing R for different values of mag\n        if mag < 6.5:\n            R = np.sqrt(dists.rhypo ** 2 + delta ** 2)\n        else:\n            R = np.sqrt(dists.rrup ** 2 + delta ** 2)\n\n        mean = (\n            # 1st term\n            C['c1'] + C['c2'] * mag +\n            # 2nd term\n            C['c3'] * R -\n            # 3rd term\n            C['c4'] * np.log10(R) +\n            # 4th term\n            C['c5'] * hypo_depth\n        )\n        # convert from base 10 to base e\n        if imt == PGV():\n            mean = np.log(10 ** mean)\n        else:\n            # convert from cm/s**2 to g\n            mean = np.log((10 ** mean) * 1e-2 / g)\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pstats(pstatfile, n):\n    with tempfile.TemporaryFile(mode='w+') as stream:\n        ps = pstats.Stats(pstatfile, stream=stream)\n        ps.sort_stats('cumtime')\n        ps.print_stats(n)\n        stream.seek(0)\n        lines = list(stream)\n    for i, line in enumerate(lines):\n        if line.startswith('   ncalls'):\n            break\n    data = []\n    for line in lines[i + 2:]:\n        columns = line.split()\n        if len(columns) == 6:\n            data.append(PStatData(*columns))\n    rows = [(rec.ncalls, rec.cumtime, rec.path) for rec in data]\n    # here is an example of the expected output table:\n    # ====== ======= ========================================================\n    # ncalls cumtime path\n    # ====== ======= ========================================================\n    # 1      33.502  commands/run.py:77(_run)\n    # 1      33.483  calculators/base.py:110(run)\n    # 1      25.166  calculators/classical.py:115(execute)\n    # 1      25.104  baselib.parallel.py:249(apply_reduce)\n    # 1      25.099  calculators/classical.py:41(classical)\n    # 1      25.099  hazardlib/calc/hazard_curve.py:164(classical)\n    return views.rst_table(rows, header='ncalls cumtime path'.split())", "response": "Retrieve profiling information as an RST table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run2(job_haz, job_risk, calc_id, concurrent_tasks, pdb, loglevel,\n         exports, params):\n    \"\"\"\n    Run both hazard and risk, one after the other\n    \"\"\"\n    hcalc = base.calculators(readinput.get_oqparam(job_haz), calc_id)\n    hcalc.run(concurrent_tasks=concurrent_tasks, pdb=pdb,\n              exports=exports, **params)\n    hc_id = hcalc.datastore.calc_id\n    rcalc_id = logs.init(level=getattr(logging, loglevel.upper()))\n    oq = readinput.get_oqparam(job_risk, hc_id=hc_id)\n    rcalc = base.calculators(oq, rcalc_id)\n    rcalc.run(pdb=pdb, exports=exports, **params)\n    return rcalc", "response": "Run two base calculations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(job_ini, slowest=False, hc=None, param='', concurrent_tasks=None,\n        exports='', loglevel='info', pdb=None):\n    \"\"\"\n    Run a calculation bypassing the database layer\n    \"\"\"\n    dbserver.ensure_on()\n    if param:\n        params = oqvalidation.OqParam.check(\n            dict(p.split('=', 1) for p in param.split(',')))\n    else:\n        params = {}\n    if slowest:\n        prof = cProfile.Profile()\n        stmt = ('_run(job_ini, concurrent_tasks, pdb, loglevel, hc, '\n                'exports, params)')\n        prof.runctx(stmt, globals(), locals())\n        pstat = calc_path + '.pstat'\n        prof.dump_stats(pstat)\n        print('Saved profiling info in %s' % pstat)\n        print(get_pstats(pstat, slowest))\n    else:\n        _run(job_ini, concurrent_tasks, pdb, loglevel, hc, exports, params)", "response": "Run a calculation bypassing the database layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an ordered dictionary with the available classes in the scalerel submodule with classes that derives from base_class keyed by class name.", "response": "def _get_available_class(base_class):\n    '''\n    Return an ordered dictionary with the available classes in the\n    scalerel submodule with classes that derives from `base_class`,\n    keyed by class name.\n    '''\n    gsims = {}\n    for fname in os.listdir(os.path.dirname(__file__)):\n        if fname.endswith('.py'):\n            modname, _ext = os.path.splitext(fname)\n            mod = importlib.import_module(\n                'openquake.hazardlib.scalerel.' + modname)\n            for cls in mod.__dict__.values():\n                if inspect.isclass(cls) and issubclass(cls, base_class) \\\n                        and cls != base_class \\\n                        and not inspect.isabstract(cls):\n                    gsims[cls.__name__] = cls\n    return dict((k, gsims[k]) for k in sorted(gsims))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of pairs ( hostname running | not - running", "response": "def status(self, host=None):\n        \"\"\"\n        :returns: a list of pairs (hostname, 'running'|'not-running')\n        \"\"\"\n        if host is None:\n            host_cores = self.host_cores\n        else:\n            host_cores = [hc for hc in self.host_cores if hc[0] == host]\n        lst = []\n        for host, _ in host_cores:\n            ready = general.socket_ready((host, self.ctrl_port))\n            lst.append((host, 'running' if ready else 'not-running'))\n        return lst"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting multiple workerpools on remote servers via ssh and a streamer with multiprocessing. Process.", "response": "def start(self, streamer=False):\n        \"\"\"\n        Start multiple workerpools, possibly on remote servers via ssh,\n        and possibly a streamer, depending on the `streamercls`.\n\n        :param streamer:\n            if True, starts a streamer with multiprocessing.Process\n        \"\"\"\n        if streamer and not general.socket_ready(self.task_in_url):  # started\n            self.streamer = multiprocessing.Process(\n                target=_streamer,\n                args=(self.master_host, self.task_in_port, self.task_out_port))\n            self.streamer.start()\n        starting = []\n        for host, cores in self.host_cores:\n            if self.status(host)[0][1] == 'running':\n                print('%s:%s already running' % (host, self.ctrl_port))\n                continue\n            ctrl_url = 'tcp://%s:%s' % (host, self.ctrl_port)\n            if host == '127.0.0.1':  # localhost\n                args = [sys.executable]\n            else:\n                args = ['ssh', host, self.remote_python]\n            args += ['-m', 'openquake.baselib.workerpool',\n                     ctrl_url, self.task_out_url, cores]\n            starting.append(' '.join(args))\n            po = subprocess.Popen(args)\n            self.pids.append(po.pid)\n        return 'starting %s' % starting"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a stop command to all worker pools", "response": "def stop(self):\n        \"\"\"\n        Send a \"stop\" command to all worker pools\n        \"\"\"\n        stopped = []\n        for host, _ in self.host_cores:\n            if self.status(host)[0][1] == 'not-running':\n                print('%s not running' % host)\n                continue\n            ctrl_url = 'tcp://%s:%s' % (host, self.ctrl_port)\n            with z.Socket(ctrl_url, z.zmq.REQ, 'connect') as sock:\n                sock.send('stop')\n                stopped.append(host)\n        if hasattr(self, 'streamer'):\n            self.streamer.terminate()\n        return 'stopped %s' % stopped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef worker(self, sock):\n        setproctitle('oq-zworker')\n        with sock:\n            for cmd, args, mon in sock:\n                parallel.safely_call(cmd, args, mon)", "response": "A worker thread that handles the PULL receiving."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts worker processes and control loop for this instance.", "response": "def start(self):\n        \"\"\"\n        Start worker processes and a control loop\n        \"\"\"\n        setproctitle('oq-zworkerpool %s' % self.ctrl_url[6:])  # strip tcp://\n        # start workers\n        self.workers = []\n        for _ in range(self.num_workers):\n            sock = z.Socket(self.task_out_port, z.zmq.PULL, 'connect')\n            proc = multiprocessing.Process(target=self.worker, args=(sock,))\n            proc.start()\n            sock.pid = proc.pid\n            self.workers.append(sock)\n\n        # start control loop accepting the commands stop and kill\n        with z.Socket(self.ctrl_url, z.zmq.REP, 'bind') as ctrlsock:\n            for cmd in ctrlsock:\n                if cmd in ('stop', 'kill'):\n                    msg = getattr(self, cmd)()\n                    ctrlsock.send(msg)\n                    break\n                elif cmd == 'getpid':\n                    ctrlsock.send(self.pid)\n                elif cmd == 'get_num_workers':\n                    ctrlsock.send(self.num_workers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a SIGTERM to all workers", "response": "def stop(self):\n        \"\"\"\n        Send a SIGTERM to all worker processes\n        \"\"\"\n        for sock in self.workers:\n            os.kill(sock.pid, signal.SIGTERM)\n        return 'WorkerPool %s stopped' % self.ctrl_url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kill(self):\n        for sock in self.workers:\n            os.kill(sock.pid, signal.SIGKILL)\n        return 'WorkerPool %s killed' % self.ctrl_url", "response": "Send a SIGKILL to all worker processes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the DbServer is up.", "response": "def get_status(address=None):\n    \"\"\"\n    Check if the DbServer is up.\n\n    :param address: pair (hostname, port)\n    :returns: 'running' or 'not-running'\n    \"\"\"\n    address = address or (config.dbserver.host, DBSERVER_PORT)\n    return 'running' if socket_ready(address) else 'not-running'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if we are the right one", "response": "def check_foreign():\n    \"\"\"\n    Check if we the DbServer is the right one\n    \"\"\"\n    if not config.dbserver.multi_user:\n        remote_server_path = logs.dbcmd('get_path')\n        if different_paths(server_path, remote_server_path):\n            return('You are trying to contact a DbServer from another'\n                   ' instance (got %s, expected %s)\\n'\n                   'Check the configuration or stop the foreign'\n                   ' DbServer instance') % (remote_server_path, server_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the DbServer if it is off.", "response": "def ensure_on():\n    \"\"\"\n    Start the DbServer if it is off\n    \"\"\"\n    if get_status() == 'not-running':\n        if config.dbserver.multi_user:\n            sys.exit('Please start the DbServer: '\n                     'see the documentation for details')\n        # otherwise start the DbServer automatically; NB: I tried to use\n        # multiprocessing.Process(target=run_server).start() and apparently\n        # it works, but then run-demos.sh hangs after the end of the first\n        # calculation, but only if the DbServer is started by oq engine (!?)\n        subprocess.Popen([sys.executable, '-m', 'openquake.server.dbserver',\n                          '-l', 'INFO'])\n\n        # wait for the dbserver to start\n        waiting_seconds = 30\n        while get_status() == 'not-running':\n            if waiting_seconds == 0:\n                sys.exit('The DbServer cannot be started after 30 seconds. '\n                         'Please check the configuration')\n            time.sleep(1)\n            waiting_seconds -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the DbServer on the given database file and port.", "response": "def run_server(dbpath=os.path.expanduser(config.dbserver.file),\n               dbhostport=None, loglevel='WARN'):\n    \"\"\"\n    Run the DbServer on the given database file and port. If not given,\n    use the settings in openquake.cfg.\n    \"\"\"\n    if dbhostport:  # assume a string of the form \"dbhost:port\"\n        dbhost, port = dbhostport.split(':')\n        addr = (dbhost, int(port))\n    else:\n        addr = (config.dbserver.listen, DBSERVER_PORT)\n\n    # create the db directory if needed\n    dirname = os.path.dirname(dbpath)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    # create and upgrade the db if needed\n    db('PRAGMA foreign_keys = ON')  # honor ON DELETE CASCADE\n    actions.upgrade_db(db)\n    # the line below is needed to work around a very subtle bug of sqlite;\n    # we need new connections, see https://github.com/gem/oq-engine/pull/3002\n    db.close()\n\n    # reset any computation left in the 'executing' state\n    actions.reset_is_running(db)\n\n    # configure logging and start the server\n    logging.basicConfig(level=getattr(logging, loglevel))\n    DbServer(db, addr).start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(self):\n        # give a nice name to the process\n        w.setproctitle('oq-dbserver')\n\n        dworkers = []\n        for _ in range(self.num_workers):\n            sock = z.Socket(self.backend, z.zmq.REP, 'connect')\n            threading.Thread(target=self.dworker, args=(sock,)).start()\n            dworkers.append(sock)\n        logging.warning('DB server started with %s on %s, pid %d',\n                        sys.executable, self.frontend, self.pid)\n        if ZMQ:\n            # start task_in->task_out streamer thread\n            c = config.zworkers\n            threading.Thread(\n                target=w._streamer,\n                args=(self.master_host, c.task_in_port, c.task_out_port)\n            ).start()\n            logging.warning('Task streamer started from %s -> %s',\n                            c.task_in_port, c.task_out_port)\n\n            # start zworkers and wait a bit for them\n            msg = self.master.start()\n            logging.warning(msg)\n            time.sleep(1)\n\n        # start frontend->backend proxy for the database workers\n        try:\n            z.zmq.proxy(z.bind(self.frontend, z.zmq.ROUTER),\n                        z.bind(self.backend, z.zmq.DEALER))\n        except (KeyboardInterrupt, z.zmq.ZMQError):\n            for sock in dworkers:\n                sock.running = False\n                sock.zsocket.close()\n            logging.warning('DB server stopped')\n        finally:\n            self.stop()", "response": "Start the database server threads and start the database worker threads and the database streamer threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the DbServer and the zworkers if any", "response": "def stop(self):\n        \"\"\"Stop the DbServer and the zworkers if any\"\"\"\n        if ZMQ:\n            logging.warning(self.master.stop())\n            z.context.term()\n        self.db.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing mean value according to equation 30 page 1021.", "response": "def _compute_mean(self, C, rup, rjb):\n        \"\"\"\n        Compute mean value according to equation 30, page 1021.\n        \"\"\"\n        mean = (C['c1'] +\n                self._compute_magnitude_term(C, rup) +\n                self._compute_distance_term(C, rup, rjb))\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_magnitude_term(self, C, rup):\n        return C['c2'] * (rup.mag - 8.0) + C['c3'] * (rup.mag - 8.0) ** 2", "response": "Compute the magnitude term in equation 8 Drouet & Cotton"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_distance_term(self, C, rup, rjb):\n        return (C['c4'] + C['c5'] * rup.mag) * np.log(\n            np.sqrt(rjb ** 2. + C['c6'] ** 2.)) + C['c7'] * rjb", "response": "This computes the distance term in equation 8 Drouet & Cotton 2015"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_term_3_4(self, dists, C):\n        cutoff = 6.056877878\n        rhypo = dists.rhypo.copy()\n        rhypo[rhypo <= cutoff] = cutoff\n        return C['c3'] * np.log(rhypo) + C['c4'] * rhypo", "response": "Compute term 3 and 4 in equation 1 page 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_site_amplification(self, sites, imt, C):\n        S = self._get_site_type_dummy_variables(sites)\n        return (C['c5'] * S)", "response": "Compute the site amplification term in equation 1 p. 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget site type dummy variables S", "response": "def _get_site_type_dummy_variables(self, sites):\n        \"\"\"\n        Get site type dummy variables, ``S`` (for rock and soil sites)\n        \"\"\"\n        S = np.zeros_like(sites.vs30)\n        # S=0 for rock sites, S=1 otherwise pag 1.\n        idxS = (sites.vs30 < 760.0)\n        S[idxS] = 1\n        return S"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_mean(self, C, rup, dists, sites, imt):\n\n        mean = (self._compute_term_1_2(rup, C) +\n                self._compute_term_3_4(dists, C) +\n                self._get_site_amplification(sites, imt, C))\n\n        # convert from m/s**2 to g for PGA and from m/s to g for PSV\n        # and divided this value for the ratio(SA_larger/SA_geo_mean)\n        if imt.name == \"PGA\":\n            mean = (np.exp(mean) / g) / C['r_SA']\n        else:\n            W = (2. * np.pi)/imt.period\n            mean = ((np.exp(mean) * W) / g) / C['r_SA']\n        return np.log(mean)", "response": "Compute mean value for PGA and pseudo - velocity response spectrum values using the standardized method of equation 1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the distance scaling term accounting for geometric and anelastic attenuation", "response": "def _compute_distance_scaling(self, C, rhypo):\n        \"\"\"\n        Returns the distance scaling term accounting for geometric and\n        anelastic attenuation\n        \"\"\"\n        return C[\"c\"] * np.log10(np.sqrt((rhypo ** 2.) + (C[\"h\"] ** 2.))) +\\\n            (C[\"d\"] * rhypo)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the site scaling term as a simple coefficient", "response": "def _compute_site_scaling(self, C, vs30):\n        \"\"\"\n        Returns the site scaling term as a simple coefficient\n        \"\"\"\n        site_term = np.zeros(len(vs30), dtype=float)\n        # For soil sites add on the site coefficient\n        site_term[vs30 < 760.0] = C[\"e\"]\n        return site_term"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_distances(rupture, mesh, param):\n    if param == 'rrup':\n        dist = rupture.surface.get_min_distance(mesh)\n    elif param == 'rx':\n        dist = rupture.surface.get_rx_distance(mesh)\n    elif param == 'ry0':\n        dist = rupture.surface.get_ry0_distance(mesh)\n    elif param == 'rjb':\n        dist = rupture.surface.get_joyner_boore_distance(mesh)\n    elif param == 'rhypo':\n        dist = rupture.hypocenter.distance_to_mesh(mesh)\n    elif param == 'repi':\n        dist = rupture.hypocenter.distance_to_mesh(mesh, with_depths=False)\n    elif param == 'rcdpp':\n        dist = rupture.get_cdppvalue(mesh)\n    elif param == 'azimuth':\n        dist = rupture.surface.get_azimuth(mesh)\n    elif param == \"rvolc\":\n        # Volcanic distance not yet supported, defaulting to zero\n        dist = numpy.zeros_like(mesh.lons)\n    else:\n        raise ValueError('Unknown distance measure %r' % param)\n    return dist", "response": "returns an array of distances from a given rupture and mesh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_num_distances(gsims):\n    dists = set()\n    for gsim in gsims:\n        dists.update(gsim.REQUIRES_DISTANCES)\n    return len(dists)", "response": "Returns the number of distances required for the given GSIMs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(self, sites, rupture):\n        distances = get_distances(rupture, sites, self.filter_distance)\n        if self.maximum_distance:\n            mask = distances <= self.maximum_distance(\n                rupture.tectonic_region_type, rupture.mag)\n            if mask.any():\n                sites, distances = sites.filter(mask), distances[mask]\n            else:\n                raise FarAwayRupture(\n                    '%d: %d km' % (rupture.serial, distances.min()))\n        return sites, DistancesContext([(self.filter_distance, distances)])", "response": "Filter the site collection with respect to the rupture."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the required rupture parameters to the rupture object.", "response": "def add_rup_params(self, rupture):\n        \"\"\"\n        Add .REQUIRES_RUPTURE_PARAMETERS to the rupture\n        \"\"\"\n        for param in self.REQUIRES_RUPTURE_PARAMETERS:\n            if param == 'mag':\n                value = rupture.mag\n            elif param == 'strike':\n                value = rupture.surface.get_strike()\n            elif param == 'dip':\n                value = rupture.surface.get_dip()\n            elif param == 'rake':\n                value = rupture.rake\n            elif param == 'ztor':\n                value = rupture.surface.get_top_edge_depth()\n            elif param == 'hypo_lon':\n                value = rupture.hypocenter.longitude\n            elif param == 'hypo_lat':\n                value = rupture.hypocenter.latitude\n            elif param == 'hypo_depth':\n                value = rupture.hypocenter.depth\n            elif param == 'width':\n                value = rupture.surface.get_width()\n            else:\n                raise ValueError('%s requires unknown rupture parameter %r' %\n                                 (type(self).__name__, param))\n            setattr(rupture, param, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter the site collection with respect to the rupture and create context objects.", "response": "def make_contexts(self, sites, rupture):\n        \"\"\"\n        Filter the site collection with respect to the rupture and\n        create context objects.\n\n        :param sites:\n            Instance of :class:`openquake.hazardlib.site.SiteCollection`.\n\n        :param rupture:\n            Instance of\n            :class:`openquake.hazardlib.source.rupture.BaseRupture`\n\n        :returns:\n            Tuple of two items: sites and distances context.\n\n        :raises ValueError:\n            If any of declared required parameters (site, rupture and\n            distance parameters) is unknown.\n        \"\"\"\n        sites, dctx = self.filter(sites, rupture)\n        for param in self.REQUIRES_DISTANCES - set([self.filter_distance]):\n            distances = get_distances(rupture, sites, param)\n            setattr(dctx, param, distances)\n        reqv_obj = (self.reqv.get(rupture.tectonic_region_type)\n                    if self.reqv else None)\n        if reqv_obj and isinstance(rupture.surface, PlanarSurface):\n            reqv = reqv_obj.get(dctx.repi, rupture.mag)\n            if 'rjb' in self.REQUIRES_DISTANCES:\n                dctx.rjb = reqv\n            if 'rrup' in self.REQUIRES_DISTANCES:\n                reqv_rup = numpy.sqrt(reqv**2 + rupture.hypocenter.depth**2)\n                dctx.rrup = reqv_rup\n        self.add_rup_params(rupture)\n        # NB: returning a SitesContext make sures that the GSIM cannot\n        # access site parameters different from the ones declared\n        sctx = SitesContext(self.REQUIRES_SITES_PARAMETERS, sites)\n        return sctx, dctx"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef poe_map(self, src, s_sites, imtls, trunclevel, rup_indep=True):\n        pmap = ProbabilityMap.build(\n            len(imtls.array), len(self.gsims), s_sites.sids,\n            initvalue=rup_indep)\n        eff_ruptures = 0\n        for rup, sctx, dctx in self.gen_rup_contexts(src, s_sites):\n            eff_ruptures += 1\n            with self.poe_mon:\n                pnes = self._make_pnes(rup, sctx, dctx, imtls, trunclevel)\n                for sid, pne in zip(sctx.sids, pnes):\n                    if rup_indep:\n                        pmap[sid].array *= pne\n                    else:\n                        pmap[sid].array += (1.-pne) * rup.weight\n        if rup_indep:\n            pmap = ~pmap\n        pmap.eff_ruptures = eff_ruptures\n        return pmap", "response": "Creates a ProbabilityMap instance for the source object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disaggregate(self, sitecol, ruptures, iml4, truncnorm, epsilons,\n                     monitor=Monitor()):\n        \"\"\"\n        Disaggregate (separate) PoE in different contributions.\n\n        :param sitecol: a SiteCollection with N sites\n        :param ruptures: an iterator over ruptures with the same TRT\n        :param iml4: a 4d array of IMLs of shape (N, R, M, P)\n        :param truncnorm: an instance of scipy.stats.truncnorm\n        :param epsilons: the epsilon bins\n        :param monitor: a Monitor instance\n        :returns:\n            an AccumDict with keys (poe, imt, rlzi) and mags, dists, lons, lats\n        \"\"\"\n        acc = AccumDict(accum=[])\n        ctx_mon = monitor('disagg_contexts', measuremem=False)\n        pne_mon = monitor('disaggregate_pne', measuremem=False)\n        clo_mon = monitor('get_closest', measuremem=False)\n        for rupture in ruptures:\n            with ctx_mon:\n                orig_dctx = DistancesContext(\n                    (param, get_distances(rupture, sitecol, param))\n                    for param in self.REQUIRES_DISTANCES)\n                self.add_rup_params(rupture)\n            with clo_mon:  # this is faster than computing orig_dctx\n                closest_points = rupture.surface.get_closest_points(sitecol)\n            cache = {}\n            for rlz, gsim in self.gsim_by_rlzi.items():\n                dctx = orig_dctx.roundup(gsim.minimum_distance)\n                for m, imt in enumerate(iml4.imts):\n                    for p, poe in enumerate(iml4.poes_disagg):\n                        iml = tuple(iml4.array[:, rlz, m, p])\n                        try:\n                            pne = cache[gsim, imt, iml]\n                        except KeyError:\n                            with pne_mon:\n                                pne = gsim.disaggregate_pne(\n                                    rupture, sitecol, dctx, imt, iml,\n                                    truncnorm, epsilons)\n                                cache[gsim, imt, iml] = pne\n                        acc[poe, str(imt), rlz].append(pne)\n            acc['mags'].append(rupture.mag)\n            acc['dists'].append(getattr(dctx, self.filter_distance))\n            acc['lons'].append(closest_points.lons)\n            acc['lats'].append(closest_points.lats)\n        return acc", "response": "Disaggregate the PoE in different contributions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef roundup(self, minimum_distance):\n        if not minimum_distance:\n            return self\n        ctx = DistancesContext()\n        for dist, array in vars(self).items():\n            small_distances = array < minimum_distance\n            if small_distances.any():\n                array = array[:]  # make a copy first\n                array[small_distances] = minimum_distance\n            setattr(ctx, dist, array)\n        return ctx", "response": "Returns a new DistancesContext with updated distances rounded up to the specified minimum_distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_probability_no_exceedance(self, poes):\n        if numpy.isnan(self.occurrence_rate):  # nonparametric rupture\n            # Uses the formula\n            #\n            #    \u2211 p(k|T) * p(X<x|rup)^k\n            #\n            # where `p(k|T)` is the probability that the rupture occurs k times\n            # in the time span `T`, `p(X<x|rup)` is the probability that a\n            # rupture occurrence does not cause a ground motion exceedance, and\n            # thesummation `\u2211` is done over the number of occurrences `k`.\n            #\n            # `p(k|T)` is given by the attribute probs_occur and\n            # `p(X<x|rup)` is computed as ``1 - poes``.\n            # Converting from 1d to 2d\n            if len(poes.shape) == 1:\n                poes = numpy.reshape(poes, (-1, len(poes)))\n            p_kT = self.probs_occur\n            prob_no_exceed = numpy.array(\n                [v * ((1 - poes) ** i) for i, v in enumerate(p_kT)])\n            prob_no_exceed = numpy.sum(prob_no_exceed, axis=0)\n            prob_no_exceed[prob_no_exceed > 1.] = 1.  # sanity check\n            prob_no_exceed[poes == 0.] = 1.  # avoid numeric issues\n            return prob_no_exceed\n        # parametric rupture\n        tom = self.temporal_occurrence_model\n        return tom.get_probability_no_exceedance(self.occurrence_rate, poes)", "response": "Compute and return the probability that the rupture does not generate a ground motion value higher than a given level at a given site."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the source is splittable False otherwise.", "response": "def splittable(src):\n    \"\"\"\n    :returns: True if the source is splittable, False otherwise\n    \"\"\"\n    return (src.__class__.__iter__ is not BaseSeismicSource.__iter__\n            and getattr(src, 'mutex_weight', 1) == 1 and src.splittable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _float_check(self, attribute_array, value, irow, key):\n        '''Checks if value is valid float, appends to array if valid, appends\n        nan if not'''\n        value = value.strip(' ')\n        try:\n            if value:\n                attribute_array = np.hstack([attribute_array, float(value)])\n            else:\n                attribute_array = np.hstack([attribute_array, np.nan])\n        except:\n            print(irow, key)\n            msg = 'Input file format error at line: %d' % (irow + 2)\n            msg += ' key: %s' % (key)\n            raise ValueError(msg)\n        return attribute_array", "response": "Checks if value is valid float appends to array if not"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _int_check(self, attribute_array, value, irow, key):\n        '''Checks if value is valid integer, appends to array if valid, appends\n        nan if not'''\n        value = value.strip(' ')\n        try:\n            if value:\n                attribute_array = np.hstack([attribute_array, int(value)])\n            else:\n                attribute_array = np.hstack([attribute_array, np.nan])\n        except:\n            msg = 'Input file format error at line: %d' % (irow + 2)\n            msg += ' key: %s' % (key)\n            raise ValueError(msg)\n        return attribute_array", "response": "Checks if value is valid integer appends to array if not"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the catalogue to file purging events if necessary.", "response": "def write_file(self, catalogue, flag_vector=None, magnitude_table=None):\n        '''\n        Writes the catalogue to file, purging events if necessary.\n\n        :param catalogue:\n            Earthquake catalogue as instance of :class:\n            openquake.hmtk.seismicity.catalogue.Catalogue\n        :param numpy.array flag_vector:\n            Boolean vector specifying whether each event is valid (therefore\n            written) or otherwise\n        :param numpy.ndarray magnitude_table:\n            Magnitude-time table specifying the year and magnitudes of\n            completeness\n        '''\n        # First apply purging conditions\n        output_catalogue = self.apply_purging(catalogue,\n                                              flag_vector,\n                                              magnitude_table)\n        outfile = open(self.output_file, 'wt')\n        writer = csv.DictWriter(outfile, fieldnames=self.OUTPUT_LIST)\n\n        writer.writeheader()\n        # Quick check to remove nan arrays\n        for key in self.OUTPUT_LIST:\n            cond = (isinstance(output_catalogue.data[key], np.ndarray)\n                    and np.all(np.isnan(output_catalogue.data[key])))\n            if cond:\n                output_catalogue.data[key] = []\n        # Write the catalogue\n        for iloc in range(0, output_catalogue.get_number_events()):\n            row_dict = {}\n            for key in self.OUTPUT_LIST:\n                if len(output_catalogue.data[key]) > 0:\n                    row_dict[key] = output_catalogue.data[key][iloc]\n                else:\n                    row_dict[key] = ''\n            writer.writerow(row_dict)\n        outfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_purging(self, catalogue, flag_vector, magnitude_table):\n        '''\n        Apply all the various purging conditions, if specified.\n\n        :param catalogue:\n            Earthquake catalogue as instance of :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n        :param numpy.array flag_vector:\n            Boolean vector specifying whether each event is valid (therefore\n            written) or otherwise\n        :param numpy.ndarray magnitude_table:\n            Magnitude-time table specifying the year and magnitudes of\n            completeness\n        '''\n        output_catalogue = deepcopy(catalogue)\n        if magnitude_table is not None:\n            if flag_vector is not None:\n                output_catalogue.catalogue_mt_filter(\n                    magnitude_table, flag_vector)\n                return output_catalogue\n            else:\n                output_catalogue.catalogue_mt_filter(\n                    magnitude_table)\n                return output_catalogue\n\n        if flag_vector is not None:\n            output_catalogue.purge_catalogue(flag_vector)\n        return output_catalogue", "response": "Applies all the various purging conditions to the given catalogue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xyz_from_ll(projected, reference):\n\n    azims = geod.azimuth(reference.longitude, reference.latitude,\n                         projected.longitude, projected.latitude)\n    depths = np.subtract(reference.depth, projected.depth)\n    dists = geod.geodetic_distance(reference.longitude,\n                                   reference.latitude,\n                                   projected.longitude,\n                                   projected.latitude)\n    return (dists * math.sin(math.radians(azims)),\n            dists * math.cos(math.radians(azims)),\n            depths)", "response": "This method computes the x y z coordinates of a set of points that are in the target point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_plane_equation(p0, p1, p2, reference):\n    '''\n    Define the equation of target fault plane passing through 3 given points\n    which includes two points on the fault trace and one point on the\n    fault plane but away from the fault trace. Note: in order to remain the\n    consistency of the fault normal vector direction definition, the order\n    of the three given points is strickly defined.\n\n    :param p0:\n        The fault trace and is the closer points from the starting point of\n        fault trace.\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the one vertex of the fault patch.\n    :param p1:\n        The fault trace and is the further points from the starting point of\n        fault trace.\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the one vertex of the fault patch.\n    :param p2:\n        The point on the fault plane but away from the fault trace.\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the one vertex of the fault patch.\n    :param reference:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the origin of the cartesian system used the represent\n        objects in a projected reference\n    :returns:\n        normal: normal vector of the plane (a,b,c)\n        dist_to_plane: d in the plane equation, ax + by + cz = d\n    '''\n\n    p0_xyz = get_xyz_from_ll(p0, reference)\n    p1_xyz = get_xyz_from_ll(p1, reference)\n    p2_xyz = get_xyz_from_ll(p2, reference)\n\n    p0 = np.array(p0_xyz)\n    p1 = np.array(p1_xyz)\n    p2 = np.array(p2_xyz)\n\n    u = p1 - p0\n    v = p2 - p0\n\n    # vector normal to plane, ax+by+cy = d, normal=(a,b,c)\n    normal = np.cross(u, v)\n\n    # Define the d for the plane equation\n    dist_to_plane = np.dot(p0, normal)\n\n    return normal, dist_to_plane", "response": "This function returns the equation of target fault plane passing through 3 given points p0 p1 and p2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vectors2angle(v1, v2):\n    cosang = np.dot(v1, v2)\n    sinang = np.linalg.norm(np.cross(v1, v2))\n    return np.arctan2(sinang, cosang)", "response": "Returns the angle in radians between vectors v1 and v2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the average S - wave radiation pattern for a given site hypocenter reference point pp and distance to the hypocentral point p0 p1 and the delta_slip.", "response": "def average_s_rad(site, hypocenter, reference, pp,\n                  normal, dist_to_plane, e, p0, p1, delta_slip):\n    \"\"\"\n    Gets the average S-wave radiation pattern given an e-path as described in:\n    Spudich et al. (2013) \"Final report of the NGA-West2 directivity working\n    group\", PEER report, page 90- 92 and computes: the site to the direct point\n    distance, rd, and the hypocentral distance, r_hyp.\n\n    :param site:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the target site\n    :param hypocenter:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of hypocenter\n    :param reference:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location\n        of the reference point for coordinate projection within the\n        calculation. The suggested reference point is Epicentre.\n    :param pp:\n        the projection point pp on the patch plane,\n        a numpy array\n    :param normal:\n        normal of the plane, describe by a normal vector[a, b, c]\n    :param dist_to_plane:\n        d is the constant term in the plane equation, e.g., ax + by + cz = d\n    :param e:\n        a float defining the E-path length, which is the distance from\n        Pd(direction) point to hypocentre. In km.\n    :param p0:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the starting point on fault segment\n    :param p1:\n        :class:`~openquake.hazardlib.geo.point.Point` object\n        representing the location of the ending point on fault segment.\n    :param delta_slip:\n        slip direction away from the strike direction, in decimal degrees.\n        A positive angle is generated by a counter-clockwise rotation.\n    :returns:\n        fs, float value of the average S-wave radiation pattern.\n        rd, float value of the distance from site to the direct point.\n        r_hyp, float value of the hypocetre distance.\n    \"\"\"\n    # Obtain the distance of Ps and Pp. If Ps is above the fault plane\n    # zs is positive, and negative when Ps is below the fault plane\n    site_xyz = get_xyz_from_ll(site, reference)\n    zs = dst.pdist([pp, site_xyz])\n\n    if site_xyz[0] * normal[0] + site_xyz[1] * normal[1] + site_xyz[2] * \\\n       normal[2] - dist_to_plane > 0:\n        zs = -zs\n\n    # Obtain the distance of Pp and hypocentre\n    hyp_xyz = get_xyz_from_ll(hypocenter, reference)\n    hyp_xyz = np.array(hyp_xyz).reshape(1, 3).flatten()\n    l2 = dst.pdist([pp, hyp_xyz])\n\n    rd = ((l2 - e) ** 2 + zs ** 2) ** 0.5\n    r_hyp = (l2 ** 2 + zs ** 2) ** 0.5\n    p0_xyz = get_xyz_from_ll(p0, reference)\n    p1_xyz = get_xyz_from_ll(p1, reference)\n    u = (np.array(p1_xyz) - np.array(p0_xyz))\n    v = pp - hyp_xyz\n    phi = vectors2angle(u, v) - np.deg2rad(delta_slip)\n    ix = np.cos(phi) * (2 * zs * (l2 / r_hyp - (l2 - e) / rd) -\n                        zs * np.log((l2 + r_hyp) / (l2 - e + rd)))\n    inn = np.cos(phi) * (-2 * zs ** 2 * (1 / r_hyp - 1 / rd)\n                         - (r_hyp - rd))\n    iphi = np.sin(phi) * (zs * np.log((l2 + r_hyp) / (l2 - e + rd)))\n\n    # Obtain the final average radiation pattern value\n    fs = (ix ** 2 + inn ** 2 + iphi ** 2) ** 0.5 / e\n\n    return fs, rd, r_hyp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the isochone ratio of a given E - path length.", "response": "def isochone_ratio(e, rd, r_hyp):\n    \"\"\"\n    Get the isochone ratio as described in Spudich et al. (2013) PEER\n    report, page 88.\n\n    :param e:\n        a float defining the E-path length, which is the distance from\n        Pd(direction) point to hypocentre. In km.\n    :param rd:\n        float, distance from the site to the direct point.\n    :param r_hyp:\n        float, the hypocentre distance.\n    :returns:\n        c_prime, a float defining the isochone ratio\n    \"\"\"\n\n    if e == 0.:\n        c_prime = 0.8\n    elif e > 0.:\n        c_prime = 1. / ((1. / 0.8) - ((r_hyp - rd) / e))\n\n    return c_prime"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the intersection point between two segments. The calculation is in Catestian coordinate system. :param seg1_start: A numpy array, representing one end point of a segment(e.g. segment1) segment. :param seg1_end: A numpy array, representing the other end point of the first segment(e.g. segment1) :param seg2_start: A numpy array, representing one end point of the other segment(e.g. segment2) segment. :param seg2_end: A numpy array, representing the other end point of the second segment(e.g. segment2) :returns: p_intersect, :a numpy ndarray. representing the location of intersection point of the two given segments vector1, a numpy array, vector defined by intersection point and seg2_end vector2, a numpy array, vector defined by seg2_start and seg2_end vector3, a numpy array, vector defined by seg1_start and seg1_end vector4, a numpy array, vector defined by intersection point and seg1_start", "response": "def _intersection(seg1_start, seg1_end, seg2_start, seg2_end):\n    \"\"\"\n    Get the intersection point between two segments. The calculation is in\n    Catestian coordinate system.\n\n    :param seg1_start:\n        A numpy array,\n        representing one end point of a segment(e.g. segment1)\n        segment.\n    :param seg1_end:\n        A numpy array,\n        representing the other end point of the first segment(e.g. segment1)\n    :param seg2_start:\n        A numpy array,\n        representing one end point of the other segment(e.g. segment2)\n        segment.\n    :param seg2_end:\n        A numpy array,\n        representing the other end point of the second segment(e.g. segment2)\n    :returns:\n        p_intersect, :a numpy ndarray.\n        representing the location of intersection point of the two\n        given segments\n        vector1, a numpy array, vector defined by intersection point and\n        seg2_end\n        vector2, a numpy array, vector defined by seg2_start and seg2_end\n        vector3, a numpy array, vector defined by seg1_start and seg1_end\n        vector4, a numpy array, vector defined by intersection point\n        and seg1_start\n    \"\"\"\n\n    pa = np.array([seg1_start, seg2_start])\n    pb = np.array([seg1_end, seg2_end])\n\n    si = pb - pa\n\n    ni = si / np.power(\n        np.dot(np.sum(si ** 2, axis=1).reshape(2, 1),\n               np.ones((1, 3))), 0.5)\n\n    nx = ni[:, 0].reshape(2, 1)\n    ny = ni[:, 1].reshape(2, 1)\n    nz = ni[:, 2].reshape(2, 1)\n    sxx = np.sum(nx ** 2 - 1)\n    syy = np.sum(ny ** 2 - 1)\n    szz = np.sum(nz ** 2 - 1)\n    sxy = np.sum(nx * ny)\n    sxz = np.sum(nx * nz)\n    syz = np.sum(ny * nz)\n    s = np.array([sxx, sxy, sxz, sxy, syy, syz, sxz, syz,\n                 szz]).reshape(3, 3)\n\n    cx = np.sum(pa[:, 0].reshape(2, 1) * (nx ** 2 - 1) +\n                pa[:, 1].reshape(2, 1) * [nx * ny] +\n                pa[:, 2].reshape(2, 1) * (nx * nz))\n    cy = np.sum(pa[:, 0].reshape(2, 1) * [nx * ny] +\n                pa[:, 1].reshape(2, 1) * [ny ** 2 - 1] +\n                pa[:, 2].reshape(2, 1) * [ny * nz])\n    cz = np.sum(pa[:, 0].reshape(2, 1) * [nx * nz] +\n                pa[:, 1].reshape(2, 1) * [ny * nz] +\n                pa[:, 2].reshape(2, 1) * [nz ** 2 - 1])\n    c = np.array([cx, cy, cz]).reshape(3, 1)\n    p_intersect = np.linalg.solve(s, c)\n\n    vector1 = (p_intersect.flatten() - seg2_end) / \\\n        sum((p_intersect.flatten() - seg2_end) ** 2) ** 0.5\n    vector2 = (seg2_start - seg2_end) / \\\n        sum((seg2_start - seg2_end) ** 2) ** 0.5\n    vector3 = (seg1_end - seg1_start) / \\\n        sum((seg1_end - seg1_start) ** 2) ** 0.5\n    vector4 = (p_intersect.flatten() - seg1_start) / \\\n        sum((p_intersect.flatten() - seg1_start) ** 2) ** 0.5\n\n    return p_intersect, vector1, vector2, vector3, vector4"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef directp(node0, node1, node2, node3, hypocenter, reference, pp):\n\n    # Find the intersection point Pd, by checking if the PdPh share the\n    # same vector with PpPh,  and PpPh >= PdPh\n    # Transform to xyz coordinate\n\n    node0_xyz = get_xyz_from_ll(node0, reference)\n    node1_xyz = get_xyz_from_ll(node1, reference)\n    node2_xyz = get_xyz_from_ll(node2, reference)\n    node3_xyz = get_xyz_from_ll(node3, reference)\n    hypocenter_xyz = get_xyz_from_ll(hypocenter, reference)\n    hypocenter_xyz = np.array(hypocenter_xyz).flatten()\n\n    pp_xyz = pp\n\n    e = []\n\n    # Loop each segments on the patch to find Pd\n    segment_s = [node0_xyz, node1_xyz, node2_xyz, node3_xyz]\n    segment_e = [node1_xyz, node2_xyz, node3_xyz, node0_xyz]\n\n    # set buffering bu\n    buf = 0.0001\n    atol = 0.0001\n\n    loop = True\n    exit_flag = False\n    looptime = 0.\n    while loop:\n        x_min = np.min(np.array([node0_xyz[0], node1_xyz[0], node2_xyz[0],\n                                node3_xyz[0]])) - buf\n        x_max = np.max(np.array([node0_xyz[0], node1_xyz[0], node2_xyz[0],\n                                node3_xyz[0]])) + buf\n        y_min = np.min(np.array([node0_xyz[1], node1_xyz[1], node2_xyz[1],\n                                node3_xyz[1]])) - buf\n        y_max = np.max(np.array([node0_xyz[1], node1_xyz[1], node2_xyz[1],\n                                node3_xyz[1]])) + buf\n        n_seg = 0\n        exit_flag = False\n        for (seg_s, seg_e) in zip(segment_s, segment_e):\n            seg_s = np.array(seg_s).flatten()\n            seg_e = np.array(seg_e).flatten()\n            p_intersect, vector1, vector2, vector3, vector4 = _intersection(\n                seg_s, seg_e, pp_xyz, hypocenter_xyz)\n\n            ppph = dst.pdist([pp, hypocenter_xyz])\n            pdph = dst.pdist([p_intersect.flatten(), hypocenter_xyz])\n            n_seg = n_seg + 1\n\n            # Check that the direction of the hyp-pp and hyp-pd vectors\n            # have are the same.\n            if (np.allclose(vector1.flatten(), vector2,\n                            atol=atol, rtol=0.)):\n                if ((np.allclose(vector3.flatten(), vector4, atol=atol,\n                                 rtol=0.))):\n\n                    # Check if ppph >= pdph.\n                    if (ppph >= pdph):\n                        if (p_intersect[0] >= x_min) & (p_intersect[0] <=\n                                                        x_max):\n                            if (p_intersect[1] >= y_min) & (p_intersect[1]\n                                                            <= y_max):\n                                e = pdph\n                                pd = p_intersect\n                                exit_flag = True\n                                break\n            # when the pp located within the fault rupture plane, e = ppph\n            if not e:\n                if (pp_xyz[0] >= x_min) & (pp_xyz[0] <= x_max):\n                    if (pp_xyz[1] >= y_min) & (pp_xyz[1] <= y_max):\n                        pd = pp_xyz\n                        e = ppph\n                        exit_flag = True\n        if exit_flag:\n            break\n\n        if not e:\n            looptime += 1\n            atol = 0.0001 * looptime\n            buf = 0.0001 * looptime\n    # if pd is located at 2nd fault segment, then the DPP calculation will\n    # keep going on the next fault patch\n    if n_seg == 2:\n        go_next_patch = True\n    else:\n        go_next_patch = False\n\n    return pd, e, go_next_patch", "response": "This method returns the Direct Point and E - path of the neighbouring patch."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the mean and standard deviation of the site and site class for the given intensity measure type.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n        C_SITE = self.SITE_COEFFS[imt]\n        s_c, idx = self._get_site_classification(sites.vs30)\n        sa_rock = (self.get_magnitude_scaling_term(C, rup) +\n                   self.get_sof_term(C, rup) +\n                   self.get_depth_term(C, rup) +\n                   self.get_distance_term(C, dists, rup))\n\n        sa_soil = self.add_site_amplification(C, C_SITE, sites,\n                                              sa_rock, idx, rup)\n\n        stddevs = self.get_stddevs(C, sites.vs30.shape, idx, stddev_types)\n        return sa_soil, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_magnitude_scaling_term(self, C, rup):\n        if rup.mag <= self.CONSTANTS[\"m_c\"]:\n            return C[\"ccr\"] * rup.mag\n        else:\n            return (C[\"ccr\"] * self.CONSTANTS[\"m_c\"]) +\\\n                (C[\"dcr\"] * (rup.mag - self.CONSTANTS[\"m_c\"]))", "response": "Returns the magnitude scaling term in equations 1 and 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_site_amplification(self, C, C_SITE, sites, sa_rock, idx, rup):\n        n_sites = sites.vs30.shape\n        # Convert from reference rock to hard rock\n        hard_rock_sa = sa_rock - C[\"lnSC1AM\"]\n        # Gets the elastic site amplification ratio\n        ln_a_n_max = self._get_ln_a_n_max(C, n_sites, idx, rup)\n\n        # Retrieves coefficients needed to determine smr\n        sreff, sreffc, f_sr = self._get_smr_coeffs(C, C_SITE, idx, n_sites,\n                                                   hard_rock_sa)\n        snc = np.zeros(n_sites)\n        alpha = self.CONSTANTS[\"alpha\"]\n        beta = self.CONSTANTS[\"beta\"]\n        smr = np.zeros(n_sites)\n        sa_soil = hard_rock_sa + ln_a_n_max\n        # Get lnSF\n        ln_sf = self._get_ln_sf(C, C_SITE, idx, n_sites, rup)\n        lnamax_idx = np.exp(ln_a_n_max) < 1.25\n        not_lnamax_idx = np.logical_not(lnamax_idx)\n        for i in range(1, 5):\n            idx_i = idx[i]\n            if not np.any(idx_i):\n                # No sites of the given site class\n                continue\n            idx2 = np.logical_and(lnamax_idx, idx_i)\n            if np.any(idx2):\n                # Use the approximate method for SRC and SNC\n                c_a = C_SITE[\"LnAmax1D{:g}\".format(i)] /\\\n                    (np.log(beta) - np.log(sreffc[idx2] ** alpha + beta))\n                c_b = -c_a * np.log(sreffc[idx2] ** alpha + beta)\n\n                snc[idx2] = np.exp((c_a * (alpha - 1.) *\n                                   np.log(beta) * np.log(10.0 * beta) -\n                                   np.log(10.0) * (c_b + ln_sf[idx2])) /\n                                   (c_a * (alpha * np.log(10.0 * beta) -\n                                    np.log(beta))))\n            # For the cases when ln_a_n_max >= 1.25\n            idx2 = np.logical_and(not_lnamax_idx, idx_i)\n            if np.any(idx2):\n                snc[idx2] = (np.exp((ln_a_n_max[idx2] *\n                             np.log(sreffc[idx2] ** alpha + beta) -\n                             ln_sf[idx2] * np.log(beta)) /\n                             C_SITE[\"LnAmax1D{:g}\".format(i)]) - beta) **\\\n                             (1.0 / alpha)\n\n            smr[idx_i] = sreff[idx_i] * (snc[idx_i] / sreffc[idx_i]) *\\\n                f_sr[idx_i]\n            # For the cases when site class = i and SMR != 0\n            idx2 = np.logical_and(idx_i, np.fabs(smr) > 0.0)\n            if np.any(idx2):\n                sa_soil[idx2] += (-C_SITE[\"LnAmax1D{:g}\".format(i)] *\n                                  (np.log(smr[idx2] ** alpha + beta) -\n                                  np.log(beta)) /\n                                  (np.log(sreffc[idx2] ** alpha + beta) -\n                                   np.log(beta)))\n        return sa_soil", "response": "Adds site amplification to the elastic site class based on the equation from 10 to 15"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_smr_coeffs(self, C, C_SITE, idx, n_sites, sa_rock):\n        # Get SR\n        sreff = np.zeros(n_sites)\n        sreffc = np.zeros(n_sites)\n        f_sr = np.zeros(n_sites)\n        for i in range(1, 5):\n            sreff[idx[i]] += (np.exp(sa_rock[idx[i]]) * self.IMF[i])\n            sreffc[idx[i]] += (C_SITE[\"Src1D{:g}\".format(i)] * self.IMF[i])\n            # Get f_SR\n            f_sr[idx[i]] += C_SITE[\"fsr{:g}\".format(i)]\n        return sreff, sreffc, f_sr", "response": "Returns the SReff and SReffC terms needed for equation 14 and 15"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the maximum ln_a_n_max defined in equations 10a and 10b", "response": "def _get_ln_a_n_max(self, C, n_sites, idx, rup):\n        \"\"\"\n        Defines the rock site amplification defined in equations 10a and 10b\n        \"\"\"\n        ln_a_n_max = C[\"lnSC1AM\"] * np.ones(n_sites)\n        for i in [2, 3, 4]:\n            if np.any(idx[i]):\n                ln_a_n_max[idx[i]] += C[\"S{:g}\".format(i)]\n        return ln_a_n_max"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the log SF term required for equation 12", "response": "def _get_ln_sf(self, C, C_SITE, idx, n_sites, rup):\n        \"\"\"\n        Returns the log SF term required for equation 12\n        \"\"\"\n        ln_sf = np.zeros(n_sites)\n        for i in range(1, 5):\n            ln_sf_i = (C[\"lnSC1AM\"] - C_SITE[\"LnAmax1D{:g}\".format(i)])\n            if i > 1:\n                ln_sf_i += C[\"S{:g}\".format(i)]\n            ln_sf[idx[i]] += ln_sf_i\n        return ln_sf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefining the site class categories based on Vs30. Returns a vector of site class values and a dictionary containing the logical version of the site class vectors for each of the site classes. Returns a vector of site class values and a dictionary containing the logical version of the site class vectors for each of the site classes.", "response": "def _get_site_classification(self, vs30):\n        \"\"\"\n        Define the site class categories based on Vs30. Returns a\n        vector of site class values and a dictionary containing logical\n        vectors for each of the site classes\n        \"\"\"\n        site_class = np.ones(vs30.shape, dtype=int)\n        idx = {}\n        idx[1] = vs30 > 600.\n        idx[2] = np.logical_and(vs30 > 300., vs30 <= 600.)\n        idx[3] = np.logical_and(vs30 > 200., vs30 <= 300.)\n        idx[4] = vs30 <= 200.\n        for i in [2, 3, 4]:\n            site_class[idx[i]] = i\n        return site_class, idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sof_term(self, C, rup):\n        if rup.rake <= -45.0 and rup.rake >= -135.0:\n            # Normal faulting\n            return C[\"FN_UM\"]\n        elif rup.rake > 45.0 and rup.rake < 135.0:\n            # Reverse faulting\n            return C[\"FRV_UM\"]\n        else:\n            # No adjustment for strike-slip faulting\n            return 0.0", "response": "Returns the sof term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the distance attenuation term in equation 1 page 74.", "response": "def get_distance_term(self, C, dists, rup):\n        \"\"\"\n        Returns the distance attenuation term\n        \"\"\"\n        x_ij = dists.rrup\n        gn_exp = np.exp(C[\"c1\"] + 6.5 * C[\"c2\"])\n        g_n = C[\"gcrN\"] * np.log(self.CONSTANTS[\"xcro\"] + 30. + gn_exp) *\\\n            np.ones_like(x_ij)\n        idx = x_ij <= 30.0\n        if np.any(idx):\n            g_n[idx] = C[\"gcrN\"] * np.log(self.CONSTANTS[\"xcro\"] +\n                                          x_ij[idx] + gn_exp)\n        c_m = min(rup.mag, self.CONSTANTS[\"m_c\"])\n        r_ij = self.CONSTANTS[\"xcro\"] + x_ij + np.exp(C[\"c1\"] + C[\"c2\"] * c_m)\n        return C[\"gUM\"] * np.log(r_ij) +\\\n            C[\"gcrL\"] * np.log(x_ij + 200.0) +\\\n            g_n + C[\"eum\"] * x_ij + C[\"ecrV\"] * dists.rvolc + C[\"gamma_S\"]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_magnitude_scaling_term(self, C, rup):\n        if rup.ztor > 25.0:\n            # Deep interface events\n            c_int = C[\"cint\"]\n        else:\n            c_int = C[\"cintS\"]\n\n        if rup.mag <= self.CONSTANTS[\"m_c\"]:\n            return c_int * rup.mag\n        else:\n            return (c_int * self.CONSTANTS[\"m_c\"]) +\\\n                (C[\"dint\"] * (rup.mag - self.CONSTANTS[\"m_c\"]))", "response": "Returns the magnitude scaling term which is dependent on top of rupture s depth and the magnitude scaling term which is dependent on top of rupture s depth."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_magnitude_scaling_term(self, C, rup):\n        m_c = self.CONSTANTS[\"m_c\"]\n        if rup.mag <= m_c:\n            return C[\"cSL\"] * rup.mag +\\\n                C[\"cSL2\"] * ((rup.mag - self.CONSTANTS[\"m_sc\"]) ** 2.)\n        else:\n            return C[\"cSL\"] * m_c +\\\n               C[\"cSL2\"] * ((m_c - self.CONSTANTS[\"m_sc\"]) ** 2.) +\\\n               C[\"dSL\"] * (rup.mag - m_c)", "response": "Returns the magnitude scaling term defined in equation 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_depth_term(self, C, rup):\n        if rup.ztor > 100.0:\n            return C[\"bSLH\"] * 100.0\n        else:\n            return C[\"bSLH\"] * rup.ztor", "response": "Returns the depth term in equation 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the distance scaling term in equation 2a", "response": "def get_distance_term(self, C, dists, rup):\n        \"\"\"\n        Returns the distance scaling term in equation 2a\n\n        Note that the paper describes a lower and upper cap on Rvolc that\n        is not found in the Fortran code, and is thus neglected here.\n        \"\"\"\n        x_ij = dists.rrup\n        # Get anelastic scaling term in quation 5\n        if rup.ztor >= 50.:\n            qslh = C[\"eSLH\"] * (0.02 * rup.ztor - 1.0)\n        else:\n            qslh = 0.0\n        # r_volc = np.copy(dists.rvolc)\n        # r_volc[np.logical_and(r_volc > 0.0, r_volc <= 12.0)] = 12.0\n        # r_volc[r_volc >= 80.0] = 80.0\n        # Get r_ij - distance for geometric spreading (equations 3 and 4)\n        c_m = min(rup.mag, self.CONSTANTS[\"m_c\"])\n        r_ij = x_ij + np.exp(C[\"alpha\"] + C[\"beta\"] * c_m)\n        return C[\"gSL\"] * np.log(r_ij) + \\\n            C[\"gLL\"] * np.log(x_ij + 200.) +\\\n            C[\"eSL\"] * x_ij + qslh * x_ij +\\\n            C[\"eSLV\"] * dists.rvolc + C[\"gamma\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n\n        COEFFS = self.COEFFS[imt]\n        R = self._compute_term_r(COEFFS, rup.mag, dists.rrup)\n\n        mean = 10 ** (self._compute_mean(COEFFS, rup.mag, R))\n\n        # Convert units to g,\n        # but only for PGA and SA (not PGV):\n        if imt.name in \"SA PGA\":\n            mean = np.log(mean / (g*100.))\n        else:\n            # PGV:\n            mean = np.log(mean)\n\n        c1_rrup = _compute_C1_term(COEFFS, dists.rrup)\n        log_phi_ss = 1.00\n\n        stddevs = self._get_stddevs(\n            COEFFS, stddev_types, sites.vs30.shape[0], rup.mag, c1_rrup,\n            log_phi_ss, COEFFS['mean_phi_ss']\n        )\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the logarithmic site term."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_stddevs(self, C, stddev_types, num_sites, mag, c1_rrup,\n                     log_phi_ss, mean_phi_ss):\n        \"\"\"\n        Return standard deviations\n        \"\"\"\n        phi_ss = _compute_phi_ss(C, mag, c1_rrup, log_phi_ss, mean_phi_ss)\n\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(\n                               C['tau'] * C['tau'] +\n                               phi_ss * phi_ss) +\n                               np.zeros(num_sites))\n\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi_ss + np.zeros(num_sites))\n\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(C['tau'] + np.zeros(num_sites))\n        return stddevs", "response": "Return standard deviations for the resource class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_term_r(self, C, mag, rrup):\n        if mag > self.M1:\n            rrup_min = 0.55\n\n        elif mag > self.M2:\n            rrup_min = -2.80 * mag + 14.55\n\n        else:\n            rrup_min = -0.295 * mag + 2.65\n\n        R = np.maximum(rrup, rrup_min)\n\n        return np.log10(R)", "response": "Compute distance term in the hierarchy from the magnetic magnitude and rrup."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_term_1(self, C, mag):\n        return (\n            C['a1'] + C['a2'] * mag + C['a3'] *\n            np.power(mag, 2) + C['a4'] * np.power(mag, 3)\n            + C['a5'] * np.power(mag, 4) + C['a6'] *\n            np.power(mag, 5) + C['a7'] * np.power(mag, 6)\n        )", "response": "Compute term 1 in equation 2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the term 2 in equation 1 page 74.", "response": "def _compute_term_2(self, C, mag, R):\n        \"\"\"\n        (a8 + a9.*M + a10.*M.*M + a11.*M.*M.*M).*d(r)\n        \"\"\"\n        return (\n            (C['a8'] + C['a9'] * mag + C['a10'] * np.power(mag, 2) +\n             C['a11'] * np.power(mag, 3)) * R\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_term_3(self, C, mag, R):\n        return (\n            (C['a12'] + C['a13'] * mag + C['a14'] * np.power(mag, 2) +\n             C['a15'] * np.power(mag, 3)) * np.power(R, 2)\n        )", "response": "Compute the 3rd order term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_term_4(self, C, mag, R):\n        return (\n            (C['a16'] + C['a17'] * mag + C['a18'] * np.power(mag, 2) +\n             C['a19'] * np.power(mag, 3)) * np.power(R, 3)\n        )", "response": "Compute the term 4 in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the 5th term in equation 1 page 74.", "response": "def _compute_term_5(self, C, mag, R):\n        \"\"\"\n        (a20 + a21.*M + a22.*M.*M + a23.*M.*M.*M).*(d(r).^4)\n        \"\"\"\n        return (\n            (C['a20'] + C['a21'] * mag + C['a22'] * np.power(mag, 2) +\n             C['a23'] * np.power(mag, 3)) * np.power(R, 4)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the mean term in the logarithmic page.", "response": "def _compute_mean(self, C, mag, term_dist_r):\n        \"\"\"\n        compute mean\n        \"\"\"\n        return (self._compute_term_1(C, mag) +\n                self._compute_term_2(C, mag, term_dist_r) +\n                self._compute_term_3(C, mag, term_dist_r) +\n                self._compute_term_4(C, mag, term_dist_r) +\n                self._compute_term_5(C, mag, term_dist_r))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the last calculation in the datastore", "response": "def read(calc_id, username=None):\n    \"\"\"\n    :param calc_id: a calculation ID\n    :param username: if given, restrict the search to the user's calculations\n    :returns: the associated DataStore instance\n    \"\"\"\n    if isinstance(calc_id, str) or calc_id < 0 and not username:\n        # get the last calculation in the datastore of the current user\n        return datastore.read(calc_id)\n    job = logs.dbcmd('get_job', calc_id, username)\n    if job:\n        return datastore.read(job.ds_calc_dir + '.hdf5')\n    else:\n        # calc_id can be present in the datastore and not in the database:\n        # this happens if the calculation was run with `oq run`\n        return datastore.read(calc_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the maximum relative difference between two curves.", "response": "def max_rel_diff(curve_ref, curve, min_value=0.01):\n    \"\"\"\n    Compute the maximum relative difference between two curves. Only values\n    greather or equal than the min_value are considered.\n\n    >>> curve_ref = [0.01, 0.02, 0.03, 0.05, 1.0]\n    >>> curve = [0.011, 0.021, 0.031, 0.051, 1.0]\n    >>> round(max_rel_diff(curve_ref, curve), 2)\n    0.1\n    \"\"\"\n    assert len(curve_ref) == len(curve), (len(curve_ref), len(curve))\n    assert len(curve), 'The curves are empty!'\n    max_diff = 0\n    for c1, c2 in zip(curve_ref, curve):\n        if c1 >= min_value:\n            max_diff = max(max_diff, abs(c1 - c2) / c1)\n    return max_diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the maximum relative difference between two sets of curves.", "response": "def max_rel_diff_index(curve_ref, curve, min_value=0.01):\n    \"\"\"\n    Compute the maximum relative difference between two sets of curves.\n    Only values greather or equal than the min_value are considered.\n    Return both the maximum difference and its location (array index).\n\n    >>> curve_refs = [[0.01, 0.02, 0.03, 0.05], [0.01, 0.02, 0.04, 0.06]]\n    >>> curves = [[0.011, 0.021, 0.031, 0.051], [0.012, 0.022, 0.032, 0.051]]\n    >>> max_rel_diff_index(curve_refs, curves)\n    (0.2, 1)\n    \"\"\"\n    assert len(curve_ref) == len(curve), (len(curve_ref), len(curve))\n    assert len(curve), 'The curves are empty!'\n    diffs = [max_rel_diff(c1, c2, min_value)\n             for c1, c2 in zip(curve_ref, curve)]\n    maxdiff = max(diffs)\n    maxindex = diffs.index(maxdiff)\n    return maxdiff, maxindex"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the relative distance between two arrays.", "response": "def rmsep(array_ref, array, min_value=0):\n    \"\"\"\n    Root Mean Square Error Percentage for two arrays.\n\n    :param array_ref: reference array\n    :param array: another array\n    :param min_value: compare only the elements larger than min_value\n    :returns: the relative distance between the arrays\n\n    >>> curve_ref = numpy.array([[0.01, 0.02, 0.03, 0.05],\n    ... [0.01, 0.02, 0.04, 0.06]])\n    >>> curve = numpy.array([[0.011, 0.021, 0.031, 0.051],\n    ... [0.012, 0.022, 0.032, 0.051]])\n    >>> str(round(rmsep(curve_ref, curve, .01), 5))\n    '0.11292'\n    \"\"\"\n    bigvalues = array_ref > min_value\n    reldiffsquare = (1. - array[bigvalues] / array_ref[bigvalues]) ** 2\n    return numpy.sqrt(reldiffsquare.mean())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(array, cutoff):\n    arr = numpy.copy(array)\n    arr[arr < cutoff] = cutoff\n    return numpy.log(arr)", "response": "Compute the logarithm of an array with a cutoff on the small values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the closest R array to the reference array", "response": "def closest_to_ref(arrays, ref, cutoff=1E-12):\n    \"\"\"\n    :param arrays: a sequence of R arrays\n    :param ref: the reference array\n    :returns: a dictionary with keys rlz, value, and dist\n    \"\"\"\n    dist = numpy.zeros(len(arrays))\n    logref = log(ref, cutoff)\n    for rlz, array in enumerate(arrays):\n        diff = log(array, cutoff) - logref\n        dist[rlz] = numpy.sqrt((diff * diff).sum())\n    rlz = dist.argmin()\n    closest = dict(rlz=rlz, value=arrays[rlz], dist=dist[rlz])\n    return closest"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomposes two arrays by generating an extended datatype containing all the fields.", "response": "def compose_arrays(a1, a2, firstfield='etag'):\n    \"\"\"\n    Compose composite arrays by generating an extended datatype containing\n    all the fields. The two arrays must have the same length.\n    \"\"\"\n    assert len(a1) == len(a2),  (len(a1), len(a2))\n    if a1.dtype.names is None and len(a1.shape) == 1:\n        # the first array is not composite, but it is one-dimensional\n        a1 = numpy.array(a1, numpy.dtype([(firstfield, a1.dtype)]))\n\n    fields1 = [(f, a1.dtype.fields[f][0]) for f in a1.dtype.names]\n    if a2.dtype.names is None:  # the second array is not composite\n        assert len(a2.shape) == 2, a2.shape\n        width = a2.shape[1]\n        fields2 = [('value%d' % i, a2.dtype) for i in range(width)]\n        composite = numpy.zeros(a1.shape, numpy.dtype(fields1 + fields2))\n        for f1 in dict(fields1):\n            composite[f1] = a1[f1]\n        for i in range(width):\n            composite['value%d' % i] = a2[:, i]\n        return composite\n\n    fields2 = [(f, a2.dtype.fields[f][0]) for f in a2.dtype.names]\n    composite = numpy.zeros(a1.shape, numpy.dtype(fields1 + fields2))\n    for f1 in dict(fields1):\n        composite[f1] = a1[f1]\n    for f2 in dict(fields2):\n        composite[f2] = a2[f2]\n    return composite"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_assets(dstore):\n    assetcol = dstore['assetcol']\n    tagnames = sorted(assetcol.tagnames)\n    tag = {t: getattr(assetcol.tagcol, t) for t in tagnames}\n    dtlist = [('asset_ref', (bytes, 100))]\n    for tagname in tagnames:\n        dtlist.append((tagname, (bytes, 100)))\n    dtlist.extend([('lon', F32), ('lat', F32)])\n    asset_data = []\n    for aref, a in zip(assetcol.asset_refs, assetcol.array):\n        tup = tuple(b'\"%s\"' % tag[t][a[t]].encode('utf-8') for t in tagnames)\n        asset_data.append((aref,) + tup + (a['lon'], a['lat']))\n    return numpy.array(asset_data, dtlist)", "response": "Get the assets for the next page of the next page of the next page of the next page."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the mean and standard deviation of the current site class and return the result.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # Compute SA with primed coeffs and PGA with both unprimed and\n        # primed coeffs\n        C = self.COEFFS_PRIMED[imt]\n        C_PGA = self.COEFFS_PRIMED[PGA()]\n        C_PGA_unprimed = self.COEFFS_UNPRIMED[PGA()]\n        SC = self.STRESS_COEFFS[imt]\n\n        # Get S term to determine if consider site term is applied\n        S = self._get_site_class(sites)\n\n        # Abrahamson and Silva (1997) hanging wall term. This is not used\n        # in the latest version of GMPE but is defined in functional form in\n        # the paper so we keep it here as a placeholder\n        f4HW = self._compute_f4(C, rup.mag, dists.rrup)\n\n        # Flags for rake angles\n        CN, CR = self._get_fault_mechanism_flags(rup.rake)\n\n        # Get volcanic path distance which Rvol=0 for current implementation\n        # of McVerry2006Asc, but kept here as placeholder for future use\n        rvol = self._get_volcanic_path_distance(dists.rrup)\n\n        # Get delta_C and delta_D terms for site class\n        delta_C, delta_D = self._get_deltas(sites)\n\n        # Get Atkinson and Boore (2006) stress drop factors or additional\n        # standard deviation adjustment. Only apply these factors to sources\n        # located within the boundaries of the CSHM.\n        in_cshm = self._check_in_cshm_polygon(rup)\n        if in_cshm is True:\n            stress_drop_factor = self._compute_stress_drop_adjustment(SC,\n                                                                      rup.mag)\n            additional_sigma = self._compute_additional_sigma()\n        else:\n            stress_drop_factor = 0\n            additional_sigma = 0\n\n        # Compute lnPGA_ABCD primed\n        lnPGAp_ABCD = self._compute_mean(C_PGA, S, rup.mag, dists.rrup, rvol,\n                                         rup.hypo_depth, CN, CR, f4HW,\n                                         delta_C, delta_D)\n\n        # Compute lnPGA_ABCD unprimed\n        lnPGA_ABCD = self._compute_mean(C_PGA_unprimed, S, rup.mag, dists.rrup,\n                                        rvol, rup.hypo_depth, CN, CR, f4HW,\n                                        delta_C, delta_D)\n\n        # Compute lnSA_ABCD\n        lnSAp_ABCD = self._compute_mean(C, S, rup.mag, dists.rrup, rvol,\n                                        rup.hypo_depth, CN, CR, f4HW,\n                                        delta_C, delta_D)\n\n        # Stage 3: Equation 6 SA_ABCD(T). This is lnSA_ABCD\n        # need to calculate final lnSA_ABCD from non-log values but return log\n        mean = np.log(np.exp(lnSAp_ABCD) *\n                      (np.exp(lnPGA_ABCD) /\n                       np.exp(lnPGAp_ABCD))) + stress_drop_factor\n\n        # Compute standard deviations\n        C_STD = self.COEFFS_STD[imt]\n        stddevs = self._get_stddevs_chch(\n            C_STD, rup.mag, stddev_types, sites, additional_sigma\n        )\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_in_cshm_polygon(self, rup):\n        lats = np.ravel(rup.surface.mesh.array[1])\n        lons = np.ravel(rup.surface.mesh.array[0])\n        # These coordinates are provided by M Gerstenberger (personal\n        # communication, 10 August 2018)\n        polygon = shapely.geometry.Polygon([(171.6, -43.3), (173.2, -43.3),\n                                            (173.2, -43.9), (171.6, -43.9)])\n        points_in_polygon = [\n            shapely.geometry.Point(lons[i], lats[i]).within(polygon)\n            for i in np.arange(len(lons))]\n        in_cshm = any(points_in_polygon)\n\n        return in_cshm", "response": "Checks if any part of the rupture surface mesh is located within the Seismic Hazard Model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the given SQL script to the database", "response": "def apply_sql_script(conn, fname):\n    \"\"\"\n    Apply the given SQL script to the database\n\n    :param conn: a DB API 2 connection\n    :param fname: full path to the creation script\n    \"\"\"\n    sql = open(fname).read()\n    try:\n        # we cannot use conn.executescript which is non transactional\n        for query in sql.split('\\n\\n'):\n            conn.execute(query)\n    except Exception:\n        logging.error('Error executing %s' % fname)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upgrade_db(conn, pkg_name='openquake.server.db.schema.upgrades',\n               skip_versions=()):\n    \"\"\"\n    Upgrade a database by running several scripts in a single transaction.\n\n    :param conn: a DB API 2 connection\n    :param str pkg_name: the name of the package with the upgrade scripts\n    :param list skip_versions: the versions to skip\n    :returns: the version numbers of the new scripts applied the database\n    \"\"\"\n    upgrader = UpgradeManager.instance(conn, pkg_name)\n    t0 = time.time()\n    # run the upgrade scripts\n    try:\n        versions_applied = upgrader.upgrade(conn, skip_versions)\n    except:\n        conn.rollback()\n        raise\n    else:\n        conn.commit()\n    dt = time.time() - t0\n    logging.info('Upgrade completed in %s seconds', dt)\n    return versions_applied", "response": "Upgrade a database by running several scripts in a single transaction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef db_version(conn, pkg_name='openquake.server.db.schema.upgrades'):\n    upgrader = UpgradeManager.instance(conn, pkg_name)\n    return max(upgrader.get_db_versions(conn))", "response": "Returns the current version of the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string that can be used to determine what the database is up to date", "response": "def what_if_I_upgrade(conn, pkg_name='openquake.server.db.schema.upgrades',\n                      extract_scripts='extract_upgrade_scripts'):\n    \"\"\"\n    :param conn:\n        a DB API 2 connection\n    :param str pkg_name:\n        the name of the package with the upgrade scripts\n    :param extract_scripts:\n        name of the method to extract the scripts\n    \"\"\"\n    msg_safe_ = '\\nThe following scripts can be applied safely:\\n%s'\n    msg_slow_ = '\\nPlease note that the following scripts could be slow:\\n%s'\n    msg_danger_ = ('\\nPlease note that the following scripts are potentially '\n                   'dangerous and could destroy your data:\\n%s')\n    upgrader = UpgradeManager.instance(conn, pkg_name)\n    applied_versions = upgrader.get_db_versions(conn)\n    current_version = max(applied_versions)\n    slow = []\n    danger = []\n    safe = []\n    for script in getattr(upgrader, extract_scripts)():\n        url = script['url']\n        if script['version'] in applied_versions:\n            continue\n        elif script['version'] <= current_version:\n            # you cannot apply a script with a version number lower than the\n            # current db version: ensure that upgrades are strictly incremental\n            raise VersionTooSmall(\n                'Your database is at version %s but you want to apply %s??'\n                % (current_version, script['fname']))\n        elif script['flag'] == '-slow':\n            slow.append(url)\n        elif script['flag'] == '-danger':\n            danger.append(url)\n        else:  # safe script\n            safe.append(url)\n\n    if not safe and not slow and not danger:\n        return 'Your database is already updated at version %s.' % \\\n            current_version\n\n    header = 'Your database is at version %s.' % current_version\n    msg_safe = msg_safe_ % '\\n'.join(safe)\n    msg_slow = msg_slow_ % '\\n'.join(slow)\n    msg_danger = msg_danger_ % '\\n'.join(danger)\n    msg = header + (msg_safe if safe else '') + (msg_slow if slow else '') \\\n        + (msg_danger if danger else '')\n    msg += ('\\nClick on the links if you want to know what exactly the '\n            'scripts are doing.')\n    if slow:\n        msg += ('\\nEven slow script can be fast if your database is small or'\n                ' the upgrade affects tables that are empty.')\n    if danger:\n        msg += ('\\nEven dangerous scripts are fine if they '\n                'affect empty tables or data you are not interested in.')\n    return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, templ, *args):\n        curs = self._conn.cursor()\n        query = curs.mogrify(templ, args)\n        if self.debug:\n            print(query)\n        curs.execute(query)\n        return curs", "response": "A simple utility to run SQL queries."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the version table into an already populated database and insert the base script.", "response": "def install_versioning(self, conn):\n        \"\"\"\n        Create the version table into an already populated database\n        and insert the base script.\n\n        :param conn: a DB API 2 connection\n        \"\"\"\n        logging.info('Creating the versioning table %s', self.version_table)\n        conn.executescript(CREATE_VERSIONING % self.version_table)\n        self._insert_script(self.read_scripts()[0], conn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the version table and run the base script on an empty database.", "response": "def init(self, conn):\n        \"\"\"\n        Create the version table and run the base script on an empty database.\n\n        :param conn: a DB API 2 connection\n        \"\"\"\n        base = self.read_scripts()[0]['fname']\n        logging.info('Creating the initial schema from %s',  base)\n        apply_sql_script(conn, os.path.join(self.upgrade_dir, base))\n        self.install_versioning(conn)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupgrade the database from the current version to the maximum version in the upgrade scripts.", "response": "def upgrade(self, conn, skip_versions=()):\n        '''\n        Upgrade the database from the current version to the maximum\n        version in the upgrade scripts.\n\n        :param conn: a DBAPI 2 connection\n        :param skip_versions: the versions to skip\n        '''\n        db_versions = self.get_db_versions(conn)\n        self.starting_version = max(db_versions)\n        to_skip = sorted(db_versions | set(skip_versions))\n        scripts = self.read_scripts(None, None, to_skip)\n        if not scripts:  # no new scripts to apply\n            return []\n        self.ending_version = max(s['version'] for s in scripts)\n        return self._upgrade(conn, scripts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the database has any new versions", "response": "def check_versions(self, conn):\n        \"\"\"\n        :param conn: a DB API 2 connection\n        :returns: a message with the versions that will be applied or None\n        \"\"\"\n        scripts = self.read_scripts(skip_versions=self.get_db_versions(conn))\n        versions = [s['version'] for s in scripts]\n        if versions:\n            return ('Your database is not updated. You can update it by '\n                    'running oq engine --upgrade-db which will process the '\n                    'following new versions: %s' % versions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all the versions stored in the database as a set.", "response": "def get_db_versions(self, conn):\n        \"\"\"\n        Get all the versions stored in the database as a set.\n\n        :param conn: a DB API 2 connection\n        \"\"\"\n        curs = conn.cursor()\n        query = 'select version from {}'.format(self.version_table)\n        try:\n            curs.execute(query)\n            return set(version for version, in curs.fetchall())\n        except:\n            raise VersioningNotInstalled('Run oq engine --upgrade-db')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_script_name(self, script_name):\n        '''\n        Parse a script name and return a dictionary with fields\n        fname, name, version and ext (or None if the name does not match).\n\n        :param name: name of the script\n        '''\n        match = re.match(self.pattern, script_name)\n        if not match:\n            return\n        version, flag, name, ext = match.groups()\n        return dict(fname=script_name, version=version, name=name,\n                    flag=flag, ext=ext, url=self.upgrades_url + script_name)", "response": "Parse a script name and return a dictionary with fields\n        fname name version and ext."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_scripts(self, minversion=None, maxversion=None, skip_versions=()):\n        scripts = []\n        versions = {}  # a script is unique per version\n        for scriptname in sorted(os.listdir(self.upgrade_dir)):\n            match = self.parse_script_name(scriptname)\n            if match:\n                version = match['version']\n                if version in skip_versions:\n                    continue  # do not collect scripts already applied\n                elif minversion and version <= minversion:\n                    continue  # do not collect versions too old\n                elif maxversion and version > maxversion:\n                    continue  # do not collect versions too new\n                try:\n                    previousname = versions[version]\n                except KeyError:  # no previous script with the same version\n                    scripts.append(match)\n                    versions[version] = scriptname\n                else:\n                    raise DuplicatedVersion(\n                        'Duplicated versions {%s,%s}' %\n                        (scriptname, previousname))\n        return scripts", "response": "Extract the upgrade scripts from a directory as a list of dictionaries ordered by version."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the OpenQuake upgrade scripts from the GitHub page.", "response": "def extract_upgrade_scripts(self):\n        \"\"\"\n        Extract the OpenQuake upgrade scripts from the links in the GitHub page\n        \"\"\"\n        link_pattern = '>\\s*{0}\\s*<'.format(self.pattern[1:-1])\n        page = urllib.request.urlopen(self.upgrades_url).read()\n        for mo in re.finditer(link_pattern, page):\n            scriptname = mo.group(0)[1:-1].strip()\n            yield self.parse_script_name(scriptname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef instance(cls, conn, pkg_name='openquake.server.db.schema.upgrades'):\n        try:\n            # upgrader is an UpgradeManager instance defined in the __init__.py\n            upgrader = importlib.import_module(pkg_name).upgrader\n        except ImportError:\n            raise SystemExit(\n                'Could not import %s (not in the PYTHONPATH?)' % pkg_name)\n        if not upgrader.read_scripts():\n            raise SystemExit(\n                'The upgrade_dir does not contain scripts matching '\n                'the pattern %s' % upgrader.pattern)\n        curs = conn.cursor()\n        # check if there is already a versioning table\n        curs.execute(\"SELECT name FROM sqlite_master \"\n                     \"WHERE name=%r\" % upgrader.version_table)\n        versioning_table = curs.fetchall()\n        # if not, run the base script and create the versioning table\n        if not versioning_table:\n            upgrader.init(conn)\n            conn.commit()\n        return upgrader", "response": "Return an instance of the class cls defined in the upgrade scripts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef average_azimuth(self):\n        if len(self.points) == 2:\n            return self.points[0].azimuth(self.points[1])\n        lons = numpy.array([point.longitude for point in self.points])\n        lats = numpy.array([point.latitude for point in self.points])\n        azimuths = geodetic.azimuth(lons[:-1], lats[:-1], lons[1:], lats[1:])\n        distances = geodetic.geodetic_distance(lons[:-1], lats[:-1],\n                                               lons[1:], lats[1:])\n        azimuths = numpy.radians(azimuths)\n        # convert polar coordinates to Cartesian ones and calculate\n        # the average coordinate of each component\n        avg_x = numpy.mean(distances * numpy.sin(azimuths))\n        avg_y = numpy.mean(distances * numpy.cos(azimuths))\n        # find the mean azimuth from that mean vector\n        azimuth = numpy.degrees(numpy.arctan2(avg_x, avg_y))\n        if azimuth < 0:\n            azimuth += 360\n        return azimuth", "response": "Calculate and return weighted average azimuth of all line s segments\n        in decimal degrees."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resample(self, section_length):\n\n        if len(self.points) < 2:\n            return Line(self.points)\n\n        resampled_points = []\n\n        # 1. Resample the first section. 2. Loop over the remaining points\n        # in the line and resample the remaining sections.\n        # 3. Extend the list with the resampled points, except the first one\n        # (because it's already contained in the previous set of\n        # resampled points).\n\n        resampled_points.extend(\n            self.points[0].equally_spaced_points(self.points[1],\n                                                 section_length)\n        )\n\n        # Skip the first point, it's already resampled\n        for i in range(2, len(self.points)):\n            points = resampled_points[-1].equally_spaced_points(\n                self.points[i], section_length\n            )\n\n            resampled_points.extend(points[1:])\n\n        return Line(resampled_points)", "response": "Resample this line into sections."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_length(self):\n        length = 0\n        for i, point in enumerate(self.points):\n            if i != 0:\n                length += point.distance(self.points[i - 1])\n        return length", "response": "Calculates and returns the length of the line as a sum of lengths\n            of all its segments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resample_to_num_points(self, num_points):\n        assert len(self.points) > 1, \"can not resample the line of one point\"\n\n        section_length = self.get_length() / (num_points - 1)\n        resampled_points = [self.points[0]]\n\n        segment = 0\n        acc_length = 0\n        last_segment_length = 0\n\n        for i in range(num_points - 1):\n            tot_length = (i + 1) * section_length\n            while tot_length > acc_length and segment < len(self.points) - 1:\n                last_segment_length = self.points[segment].distance(\n                    self.points[segment + 1]\n                )\n                acc_length += last_segment_length\n                segment += 1\n            p1, p2 = self.points[segment - 1:segment + 1]\n            offset = tot_length - (acc_length - last_segment_length)\n            if offset < 1e-5:\n                # forward geodetic transformations for very small distances\n                # are very inefficient (and also unneeded). if target point\n                # is just 1 cm away from original (non-resampled) line vertex,\n                # don't even bother doing geodetic calculations.\n                resampled = p1\n            else:\n                resampled = p1.equally_spaced_points(p2, offset)[1]\n            resampled_points.append(resampled)\n\n        return Line(resampled_points)", "response": "Resample the line to a specified number of points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _at_percentile(tau, var_tau, percentile):\n    assert (percentile >= 0.0) and (percentile <= 1.0)\n    c_val = _scaling(tau, var_tau)\n    k_val = _dof(tau, var_tau)\n    return np.sqrt(c_val * chi2.ppf(percentile, df=k_val))", "response": "Returns the inverse chi - 2 distribution at the given percentile from the mean and variance of the uncertainty model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the global tau for the given magnitude.", "response": "def global_tau(imt, mag, params):\n    \"\"\"\n    'Global' model of inter-event variability, as presented in equation 5.6\n    (p103)\n    \"\"\"\n    if imt.name == \"PGV\":\n        C = params[\"PGV\"]\n    else:\n        C = params[\"SA\"]\n    if mag > 6.5:\n        return C[\"tau4\"]\n    elif (mag > 5.5) and (mag <= 6.5):\n        return ITPL(mag, C[\"tau4\"], C[\"tau3\"], 5.5, 1.0)\n    elif (mag > 5.0) and (mag <= 5.5):\n        return ITPL(mag, C[\"tau3\"], C[\"tau2\"], 5.0, 0.5)\n    elif (mag > 4.5) and (mag <= 5.0):\n        return ITPL(mag, C[\"tau2\"], C[\"tau1\"], 4.5, 0.5)\n    else:\n        return C[\"tau1\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the inter - event standard deviation tau for the CENA case.", "response": "def cena_tau(imt, mag, params):\n    \"\"\"\n    Returns the inter-event standard deviation, tau, for the CENA case\n    \"\"\"\n    if imt.name == \"PGV\":\n        C = params[\"PGV\"]\n    else:\n        C = params[\"SA\"]\n    if mag > 6.5:\n        return C[\"tau3\"]\n    elif (mag > 5.5) and (mag <= 6.5):\n        return ITPL(mag, C[\"tau3\"], C[\"tau2\"], 5.5, 1.0)\n    elif (mag > 5.0) and (mag <= 5.5):\n        return ITPL(mag, C[\"tau2\"], C[\"tau1\"], 5.0, 0.5)\n    else:\n        return C[\"tau1\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the value of tau at a given quantile in the form of a dictionary", "response": "def get_tau_at_quantile(mean, stddev, quantile):\n    \"\"\"\n    Returns the value of tau at a given quantile in the form of a dictionary\n    organised by intensity measure\n    \"\"\"\n    tau_model = {}\n    for imt in mean:\n        tau_model[imt] = {}\n        for key in mean[imt]:\n            if quantile is None:\n                tau_model[imt][key] = mean[imt][key]\n            else:\n                tau_model[imt][key] = _at_percentile(mean[imt][key],\n                                                     stddev[imt][key],\n                                                     quantile)\n    return tau_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_phi_ss_at_quantile(phi_model, quantile):\n    # Setup SA coeffs - the backward compatible Python 2.7 way\n    coeffs = deepcopy(phi_model.sa_coeffs)\n    coeffs.update(phi_model.non_sa_coeffs)\n    for imt in coeffs:\n        if quantile is None:\n            coeffs[imt] = {\"a\": phi_model[imt][\"mean_a\"],\n                           \"b\": phi_model[imt][\"mean_b\"]}\n        else:\n            coeffs[imt] = {\n                \"a\": _at_percentile(phi_model[imt][\"mean_a\"],\n                                    phi_model[imt][\"var_a\"],\n                                    quantile),\n                \"b\": _at_percentile(phi_model[imt][\"mean_b\"],\n                                    phi_model[imt][\"var_b\"],\n                                    quantile)\n                }\n    return CoeffsTable(sa_damping=5., table=coeffs)", "response": "Returns the phi_ss values at the specified quantile as an instance of\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_phi_ss(imt, mag, params):\n    C = params[imt]\n    if mag <= 5.0:\n        phi = C[\"a\"]\n    elif mag > 6.5:\n        phi = C[\"b\"]\n    else:\n        phi = C[\"a\"] + (mag - 5.0) * ((C[\"b\"] - C[\"a\"]) / 1.5)\n    return phi", "response": "Returns the single station phi for a given intensity measure type and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the standard deviations for the ergodic or non - ergodic models.", "response": "def get_stddevs(self, mag, imt, stddev_types, num_sites):\n        \"\"\"\n        Returns the standard deviations for either the ergodic or\n        non-ergodic models\n        \"\"\"\n        tau = self._get_tau(imt, mag)\n        phi = self._get_phi(imt, mag)\n        sigma = np.sqrt(tau ** 2. + phi ** 2.)\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(sigma + np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(phi + np.zeros(num_sites))\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(tau + np.zeros(num_sites))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the inter - event standard deviation of the specified intensity measure and magnitude.", "response": "def _get_tau(self, imt, mag):\n        \"\"\"\n        Returns the inter-event standard deviation (tau)\n        \"\"\"\n        return TAU_EXECUTION[self.tau_model](imt, mag, self.TAU)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the within - event standard deviation of the related object.", "response": "def _get_phi(self, imt, mag):\n        \"\"\"\n        Returns the within-event standard deviation (phi)\n        \"\"\"\n        phi = get_phi_ss(imt, mag, self.PHI_SS)\n        if self.ergodic:\n            C = self.PHI_S2SS[imt]\n            phi = np.sqrt(phi ** 2. + C[\"phi_s2ss\"] ** 2.)\n        return phi"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_stddevs(self, mag, imt, stddev_types, num_sites):\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                sigma = self._get_total_sigma(imt, mag)\n                stddevs.append(sigma + np.zeros(num_sites))\n        return stddevs", "response": "Returns the total standard deviation for a specific resource table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the total standard deviation at the specified quantile.", "response": "def _get_sigma_at_quantile(self, sigma_quantile):\n        \"\"\"\n        Calculates the total standard deviation at the specified quantile\n        \"\"\"\n        # Mean mean is found in self.TAU. Get the variance in tau\n        tau_std = TAU_SETUP[self.tau_model][\"STD\"]\n        # Mean phiss is found in self.PHI_SS. Get the variance in phi\n        phi_std = deepcopy(self.PHI_SS.sa_coeffs)\n        phi_std.update(self.PHI_SS.non_sa_coeffs)\n        for key in phi_std:\n            phi_std[key] = {\"a\": PHI_SETUP[self.phi_model][key][\"var_a\"],\n                            \"b\": PHI_SETUP[self.phi_model][key][\"var_b\"]}\n        if self.ergodic:\n            # IMT list should be taken from the PHI_S2SS_MODEL\n            imt_list = list(\n                PHI_S2SS_MODEL[self.phi_s2ss_model].non_sa_coeffs.keys())\n            imt_list += \\\n                list(PHI_S2SS_MODEL[self.phi_s2ss_model].sa_coeffs.keys())\n        else:\n            imt_list = phi_std.keys()\n        phi_std = CoeffsTable(sa_damping=5, table=phi_std)\n        tau_bar, tau_std = self._get_tau_vector(self.TAU, tau_std, imt_list)\n        phi_bar, phi_std = self._get_phi_vector(self.PHI_SS, phi_std, imt_list)\n        sigma = {}\n        # Calculate the total standard deviation\n        for imt in imt_list:\n            sigma[imt] = {}\n            for i, key in enumerate(self.tau_keys):\n                # Calculates the expected standard deviation\n                sigma_bar = np.sqrt(tau_bar[imt][i] ** 2. +\n                                    phi_bar[imt][i] ** 2.)\n                # Calculated the variance in the standard deviation\n                sigma_std = np.sqrt(tau_std[imt][i] ** 2. +\n                                    phi_std[imt][i] ** 2.)\n                # The keys swap from tau to sigma\n                new_key = key.replace(\"tau\", \"sigma\")\n                if sigma_quantile is not None:\n                    sigma[imt][new_key] =\\\n                        _at_percentile(sigma_bar, sigma_std, sigma_quantile)\n                else:\n                    sigma[imt][new_key] = sigma_bar\n                self.tau_keys[i] = new_key\n        self.SIGMA = CoeffsTable(sa_damping=5, table=sigma)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_tau_vector(self, tau_mean, tau_std, imt_list):\n        self.magnitude_limits = MAG_LIMS_KEYS[self.tau_model][\"mag\"]\n        self.tau_keys = MAG_LIMS_KEYS[self.tau_model][\"keys\"]\n        t_bar = {}\n        t_std = {}\n        for imt in imt_list:\n            t_bar[imt] = []\n            t_std[imt] = []\n            for mag, key in zip(self.magnitude_limits, self.tau_keys):\n                t_bar[imt].append(\n                    TAU_EXECUTION[self.tau_model](imt, mag, tau_mean))\n                t_std[imt].append(\n                    TAU_EXECUTION[self.tau_model](imt, mag, tau_std))\n        return t_bar, t_std", "response": "Gets the vector of mean and variance of tau values corresponding to\n        the specific model and returns them as dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_phi_vector(self, phi_mean, phi_std, imt_list):\n        p_bar = {}\n        p_std = {}\n\n        for imt in imt_list:\n            p_bar[imt] = []\n            p_std[imt] = []\n            for mag in self.magnitude_limits:\n                phi_ss_mean = get_phi_ss(imt, mag, phi_mean)\n                phi_ss_std = get_phi_ss(imt, mag, phi_std)\n                if self.ergodic:\n                    # Add on the phi_s2ss term according to Eqs. 5.15 and 5.16\n                    # of Al Atik (2015)\n                    phi_ss_mean = np.sqrt(\n                        phi_ss_mean ** 2. +\n                        PHI_S2SS_MODEL[self.phi_s2ss_model][imt][\"mean\"] ** 2.\n                        )\n                    phi_ss_std = np.sqrt(\n                        phi_ss_std ** 2. +\n                        PHI_S2SS_MODEL[self.phi_s2ss_model][imt][\"var\"] ** 2.\n                        )\n                p_bar[imt].append(phi_ss_mean)\n                p_std[imt].append(phi_ss_std)\n        return p_bar, p_std", "response": "Gets the vector of mean and variance of phi values corresponding to the specific model and returns them as dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the estimated total standard deviation for a given intensity measure type and magnitude.", "response": "def _get_total_sigma(self, imt, mag):\n        \"\"\"\n        Returns the estimated total standard deviation for a given intensity\n        measure type and magnitude\n        \"\"\"\n        C = self.SIGMA[imt]\n        if mag <= self.magnitude_limits[0]:\n            # The CENA constant model is always returned here\n            return C[self.tau_keys[0]]\n        elif mag > self.magnitude_limits[-1]:\n            return C[self.tau_keys[-1]]\n        else:\n            # Needs interpolation\n            for i in range(len(self.tau_keys) - 1):\n                l_m = self.magnitude_limits[i]\n                u_m = self.magnitude_limits[i + 1]\n                if mag > l_m and mag <= u_m:\n                    return ITPL(mag,\n                                C[self.tau_keys[i + 1]],\n                                C[self.tau_keys[i]],\n                                l_m,\n                                u_m - l_m)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        F, HW = self._get_fault_type_hanging_wall(rup.rake)\n        S = self._get_site_class(sites.vs30)\n\n        # compute pga on rock (used then to compute site amplification factor)\n        C = self.COEFFS[PGA()]\n        pga_rock = np.exp(\n            self._compute_mean_on_rock(C, rup.mag, dists.rrup, F, HW)\n        )\n\n        # compute mean for the given imt (do not repeat the calculation if\n        # imt is PGA, just add the site amplification term)\n        if imt == PGA():\n            mean = np.log(pga_rock) + S * self._compute_f5(C, pga_rock)\n        else:\n            C = self.COEFFS[imt]\n            mean = (\n                self._compute_mean_on_rock(C, rup.mag, dists.rrup, F, HW) +\n                S * self._compute_f5(C, pga_rock)\n            )\n\n        C_STD = self.COEFFS_STD[imt]\n        stddevs = self._get_stddevs(\n            C_STD, rup.mag, stddev_types, sites.vs30.size\n        )\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation for the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute mean value on rock page 105", "response": "def _compute_mean_on_rock(self, C, mag, rrup, F, HW):\n        \"\"\"\n        Compute mean value on rock (that is eq.1, page 105 with S = 0)\n        \"\"\"\n        f1 = self._compute_f1(C, mag, rrup)\n        f3 = self._compute_f3(C, mag)\n        f4 = self._compute_f4(C, mag, rrup)\n\n        return f1 + F * f3 + HW * f4"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_fault_type_hanging_wall(self, rake):\n        F, HW = 0, 0\n\n        if 45 <= rake <= 135:\n            F, HW = 1, 1\n\n        return F, HW", "response": "Returns fault type and hanging - wall flags depending on rake angle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing f1 term in equation 1 page 105", "response": "def _compute_f1(self, C, mag, rrup):\n        \"\"\"\n        Compute f1 term (eq.4, page 105)\n        \"\"\"\n        r = np.sqrt(rrup ** 2 + C['c4'] ** 2)\n\n        f1 = (\n            C['a1'] +\n            C['a12'] * (8.5 - mag) ** C['n'] +\n            (C['a3'] + C['a13'] * (mag - C['c1'])) * np.log(r)\n        )\n\n        if mag <= C['c1']:\n            f1 += C['a2'] * (mag - C['c1'])\n        else:\n            f1 += C['a4'] * (mag - C['c1'])\n\n        return f1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes f3 term in equation 1 page 106", "response": "def _compute_f3(self, C, mag):\n        \"\"\"\n        Compute f3 term (eq.6, page 106)\n\n        NOTE: In the original manuscript, for the case 5.8 < mag < c1,\n        the term in the numerator '(mag - 5.8)' is missing, while is\n        present in the software used for creating the verification tables\n        \"\"\"\n        if mag <= 5.8:\n            return C['a5']\n        elif 5.8 < mag < C['c1']:\n            return (\n                C['a5'] +\n                (C['a6'] - C['a5']) * (mag - 5.8) / (C['c1'] - 5.8)\n            )\n        else:\n            return C['a6']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_f4(self, C, mag, rrup):\n        fhw_m = 0\n        fhw_r = np.zeros_like(rrup)\n\n        if mag <= 5.5:\n            fhw_m = 0\n        elif 5.5 < mag < 6.5:\n            fhw_m = mag - 5.5\n        else:\n            fhw_m = 1\n\n        idx = (rrup > 4) & (rrup <= 8)\n        fhw_r[idx] = C['a9'] * (rrup[idx] - 4.) / 4.\n\n        idx = (rrup > 8) & (rrup <=18)\n        fhw_r[idx] = C['a9']\n\n        idx = (rrup > 18) & (rrup <= 24)\n        fhw_r[idx] = C['a9'] * (1 - (rrup[idx] - 18.) / 7.)\n\n        return fhw_m * fhw_r", "response": "Compute f4 term in equation 1 page 106"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_f5(self, C, pga_rock):\n        return C['a10'] + C['a11'] * np.log(pga_rock + C['c5'])", "response": "Compute f5 term in equation 1 page 74"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hcurves_and_means(dstore):\n    rlzs_assoc = dstore['csm_info'].get_rlzs_assoc()\n    getter = getters.PmapGetter(dstore, rlzs_assoc)\n    pmaps = getter.get_pmaps()\n    return dict(zip(getter.rlzs, pmaps)), dstore['hcurves/mean']", "response": "Extract hcurves from the datastore and compute their means."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show(what='contents', calc_id=-1, extra=()):\n    datadir = datastore.get_datadir()\n    if what == 'all':  # show all\n        if not os.path.exists(datadir):\n            return\n        rows = []\n        for calc_id in datastore.get_calc_ids(datadir):\n            try:\n                ds = util.read(calc_id)\n                oq = ds['oqparam']\n                cmode, descr = oq.calculation_mode, oq.description\n            except Exception:\n                # invalid datastore file, or missing calculation_mode\n                # and description attributes, perhaps due to a manual kill\n                f = os.path.join(datadir, 'calc_%s.hdf5' % calc_id)\n                logging.warning('Unreadable datastore %s', f)\n                continue\n            else:\n                rows.append((calc_id, cmode, descr.encode('utf-8')))\n        for row in sorted(rows, key=lambda row: row[0]):  # by calc_id\n            print('#%d %s: %s' % row)\n        return\n\n    ds = util.read(calc_id)\n\n    # this part is experimental\n    if what == 'rlzs' and 'poes' in ds:\n        min_value = 0.01  # used in rmsep\n        getter = getters.PmapGetter(ds)\n        pmaps = getter.get_pmaps()\n        weights = [rlz.weight for rlz in getter.rlzs]\n        mean = stats.compute_pmap_stats(\n            pmaps, [numpy.mean], weights, getter.imtls)\n        dists = []\n        for rlz, pmap in zip(getter.rlzs, pmaps):\n            dist = util.rmsep(mean.array, pmap.array, min_value)\n            dists.append((dist, rlz))\n        print('Realizations in order of distance from the mean curves')\n        for dist, rlz in sorted(dists):\n            print('%s: rmsep=%s' % (rlz, dist))\n    elif view.keyfunc(what) in view:\n        print(view(what, ds))\n    elif what.split('/', 1)[0] in extract:\n        print(extract(ds, what, *extra))\n    elif what in ds:\n        obj = ds[what]\n        if hasattr(obj, 'value'):  # an array\n            print(write_csv(io.BytesIO(), obj.value).decode('utf8'))\n        else:\n            print(obj)\n    else:\n        print('%s not found' % what)\n\n    ds.close()", "response": "Show the content of a single or all of the items in the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        mean = np.zeros_like(sites.vs30)\n        stddevs = [np.zeros_like(sites.vs30) for _ in stddev_types]\n\n        idx_rock = sites.vs30 >= self.ROCK_VS30\n        idx_soil = sites.vs30 < self.ROCK_VS30\n\n        if idx_rock.any():\n            C = self.COEFFS_ROCK[imt]\n            self._compute_mean(C, rup.mag, dists.rhypo, rup.hypo_depth, mean,\n                               idx_rock)\n            self._compute_std(C, stddevs, idx_rock)\n\n        if idx_soil.any():\n            C = self.COEFFS_SOIL[imt]\n            self._compute_mean(C, rup.mag, dists.rhypo, rup.hypo_depth, mean,\n                               idx_soil)\n            self._compute_std(C, stddevs, idx_soil)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute mean value according to equations 10 and 11 page 226.", "response": "def _compute_mean(self, C, mag, rhypo, hypo_depth, mean, idx):\n        \"\"\"\n        Compute mean value according to equations 10 and 11 page 226.\n        \"\"\"\n        mean[idx] = (C['C1'] + C['C2'] * mag + C['C3'] * np.log(rhypo[idx] +\n                     C['C4'] * np.exp(C['C5'] * mag)) + C['C6'] * hypo_depth)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_std(self, C, stddevs, idx):\n        for stddev in stddevs:\n            stddev[idx] += C['sigma']", "response": "Compute total standard deviation see tables 3 and 4 pages 227 and 228."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        idx_rock = sites.vs30 >= self.ROCK_VS30\n        idx_soil = sites.vs30 < self.ROCK_VS30\n\n        mean[idx_rock] += 0.275\n        mean[idx_soil] += 0.31\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_config(config):\n    '''Check config file inputs and overwrite bad values with the defaults'''\n    essential_keys = ['number_earthquakes']\n\n    for key in essential_keys:\n        if key not in config:\n            raise ValueError('For Kijko Nonparametric Gaussian the key %s '\n                             'needs to be set in the configuation' % key)\n\n    if config.get('tolerance', 0.0) <= 0.0:\n        config['tolerance'] = 0.05\n\n    if config.get('maximum_iterations', 0) < 1:\n        config['maximum_iterations'] = 100\n\n    if config.get('number_samples', 0) < 2:\n        config['number_samples'] = 51\n\n    return config", "response": "Check config file inputs and overwrite bad values with the defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to return a set of exponentially spaced values between mmin and mmax", "response": "def _get_exponential_spaced_values(mmin, mmax, number_samples):\n    '''\n    Function to return a set of exponentially spaced values between mmin and\n    mmax\n\n    :param float mmin:\n        Minimum value\n    :param float mmax:\n        Maximum value\n    :param float number_samples:\n        Number of exponentially spaced samples\n    :return np.ndarray:\n        Set of 'number_samples' exponentially spaced values\n    '''\n    lhs = np.exp(mmin) + np.arange(0., number_samples - 1., 1.) *\\\n        ((np.exp(mmax) - np.exp(mmin)) / (number_samples - 1.))\n    magval = np.hstack([lhs, np.exp(mmax)])\n\n    return np.log(magval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef importcalc(calc_id):\n    dbserver.ensure_on()\n    try:\n        calc_id = int(calc_id)\n    except ValueError:  # assume calc_id is a pathname\n        calc_id, datadir = datastore.extract_calc_id_datadir(calc_id)\n        status = 'complete'\n        remote = False\n    else:\n        remote = True\n    job = logs.dbcmd('get_job', calc_id)\n    if job is not None:\n        sys.exit('There is already a job #%d in the local db' % calc_id)\n    if remote:\n        datadir = datastore.get_datadir()\n        webex = WebExtractor(calc_id)\n        status = webex.status['status']\n        hc_id = webex.oqparam.hazard_calculation_id\n        if hc_id:\n            sys.exit('The job has a parent (#%d) and cannot be '\n                     'downloaded' % hc_id)\n        webex.dump('%s/calc_%d.hdf5' % (datadir, calc_id))\n        webex.close()\n    with datastore.read(calc_id) as dstore:\n        engine.expose_outputs(dstore, status=status)\n    logging.info('Imported calculation %d successfully', calc_id)", "response": "Import a remote calculation into the local database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type and for PGA\n        C = self.COEFFS[imt]\n        # For inslab GMPEs the correction term is fixed at -0.3\n        dc1 = -0.3\n        C_PGA = self.COEFFS[PGA()]\n        # compute median pga on rock (vs30=1000), needed for site response\n        # term calculation\n        pga1000 = np.exp(\n            self._compute_pga_rock(C_PGA, dc1, sites, rup, dists))\n        mean = (self._compute_magnitude_term(C, dc1, rup.mag) +\n                self._compute_distance_term(C, rup.mag, dists) +\n                self._compute_focal_depth_term(C, rup) +\n                self._compute_forearc_backarc_term(C, sites, dists) +\n                self._compute_site_response_term(C, sites, pga1000))\n        stddevs = self._get_stddevs(C, stddev_types, len(sites.vs30))\n        return mean, stddevs", "response": "This method calculates the mean and standard deviation of the site response for the given intensity measure type and site response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_distance_term(self, C, mag, dists):\n        return ((C['theta2'] + C['theta14'] + C['theta3'] *\n                (mag - 7.8)) * np.log(dists.rhypo + self.CONSTS['c4'] *\n                np.exp((mag - 6.) * self.CONSTS['theta9'])) +\n                (C['theta6'] * dists.rhypo)) + C[\"theta10\"]", "response": "Compute the distance scaling term in equation 1a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_log_record(self, record):\n    if not hasattr(record, 'hostname'):\n        record.hostname = '-'\n    if not hasattr(record, 'job_id'):\n        record.job_id = self.job_id", "response": "Update the log record with the current values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_last_calc_id(username=None):\n    if config.dbserver.multi_user:\n        job = dbcmd('get_job', -1, username)  # can be None\n        return getattr(job, 'id', 0)\n    else:  # single user\n        return datastore.get_last_calc_id()", "response": "get the last calculation id for a user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the root logger and the root handlers.", "response": "def init(calc_id='nojob', level=logging.INFO):\n    \"\"\"\n    1. initialize the root logger (if not already initialized)\n    2. set the format of the root handlers (if any)\n    3. return a new calculation ID candidate if calc_id is 'job' or 'nojob'\n       (with 'nojob' the calculation ID is not stored in the database)\n    \"\"\"\n    if not logging.root.handlers:  # first time\n        logging.basicConfig(level=level)\n    if calc_id == 'job':  # produce a calc_id by creating a job in the db\n        calc_id = dbcmd('create_job', datastore.get_datadir())\n    elif calc_id == 'nojob':  # produce a calc_id without creating a job\n        calc_id = datastore.get_last_calc_id() + 1\n    else:\n        assert isinstance(calc_id, int), calc_id\n    fmt = '[%(asctime)s #{} %(levelname)s] %(message)s'.format(calc_id)\n    for handler in logging.root.handlers:\n        handler.setFormatter(logging.Formatter(fmt))\n    return calc_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_median_area(self, mag, rake):\n        # strike slip\n        length = 10.0 ** (-2.57 + 0.62 * mag)\n        seis_wid = 20.0\n\n        # estimate area based on length\n        if length < seis_wid:\n            return length ** 2.\n        else:\n            return length * seis_wid", "response": "Calculates the median area of a set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _construct_surface(lons, lats, upper_depth, lower_depth):\n    depths = np.array([\n        np.zeros_like(lons) + upper_depth,\n        np.zeros_like(lats) + lower_depth\n    ])\n\n    mesh = RectangularMesh(\n        np.tile(lons, (2, 1)), np.tile(lats, (2, 1)), depths\n    )\n    return SimpleFaultSurface(mesh)", "response": "Utility method that constructs and returns a simple fault surface with topometric edges specified by lons and lats."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes and return minimum distance between subduction trench and points specified by lon and lat.", "response": "def _get_min_distance_to_sub_trench(lons, lats):\n    \"\"\"\n    Compute and return minimum distance between subduction trench\n    and points specified by 'lon' and 'lat'\n\n    The method creates an instance of\n    :class:`openquake.hazardlib.geo.SimpleFaultSurface` to model the subduction\n    trench. The surface is assumed vertical and extending from 0 to 10 km\n    depth.\n    The 10 km depth value is arbitrary given that distance calculation depend\n    only on top edge depth. The method calls then\n    :meth:`openquake.hazardlib.geo.base.BaseSurface.get_rx_distance`\n    and return its absolute value.\n    \"\"\"\n    trench = _construct_surface(SUB_TRENCH_LONS, SUB_TRENCH_LATS, 0., 10.)\n    sites = Mesh(lons, lats, None)\n    return np.abs(trench.get_rx_distance(sites))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes and return minimum distance between volcanic front and points specified by lon and lat.", "response": "def _get_min_distance_to_volcanic_front(lons, lats):\n    \"\"\"\n    Compute and return minimum distance between volcanic front and points\n    specified by 'lon' and 'lat'.\n\n    Distance is negative if point is located east of the volcanic front,\n    positive otherwise.\n\n    The method uses the same approach as :meth:`_get_min_distance_to_sub_trench`\n    but final distance is returned without taking the absolute value.\n    \"\"\"\n    vf = _construct_surface(VOLCANIC_FRONT_LONS, VOLCANIC_FRONT_LATS, 0., 10.)\n    sites = Mesh(lons, lats, None)\n    return vf.get_rx_distance(sites)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies subduction trench correction to the internal system.", "response": "def _apply_subduction_trench_correction(mean, x_tr, H, rrup, imt):\n    \"\"\"\n    Implement equation for subduction trench correction as described in\n    equation 3.5.2-1, page 3-148 of \"Technical Reports on National Seismic\n    Hazard Maps for Japan\"\n    \"\"\"\n    if imt.name == 'PGV':\n        V1 = 10 ** ((-4.021e-5 * x_tr + 9.905e-3) * (H - 30))\n        V2 = np.maximum(1., (10 ** (-0.012)) * ((rrup / 300.) ** 2.064))\n        corr = V2\n        if H > 30:\n            corr *= V1\n    else:\n        V2 = np.maximum(1., (10 ** (+0.13)) * ((rrup / 300.) ** 3.2))\n        corr = V2\n        if H > 30:\n            V1 = 10 ** ((-8.1e-5 * x_tr + 2.0e-2) * (H - 30))\n            corr *= V1\n    return np.log(np.exp(mean) * corr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the volcanic front correction term for the current log - likelihood of the term at the given mean and value of the term at the given temperature H.", "response": "def _apply_volcanic_front_correction(mean, x_vf, H, imt):\n    \"\"\"\n    Implement equation for volcanic front correction as described in equation\n    3.5.2.-2, page 3-149 of \"Technical Reports on National Seismic\n    Hazard Maps for Japan\"\n    \"\"\"\n    V1 = np.zeros_like(x_vf)\n    if imt.name == 'PGV':\n        idx = x_vf <= 75\n        V1[idx] = 4.28e-5 * x_vf[idx] * (H - 30)\n        idx = x_vf > 75\n        V1[idx] = 3.21e-3 * (H - 30)\n        V1 = 10 ** V1\n    else:\n        idx = x_vf <= 75\n        V1[idx] = 7.06e-5 * x_vf[idx] * (H - 30)\n        idx = x_vf > 75\n        V1[idx] = 5.30e-3 * (H - 30)\n        V1 = 10 ** V1\n    return np.log(np.exp(mean) * V1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        mean = self._get_mean(imt, rup.mag, rup.hypo_depth, dists.rrup, d=0)\n        stddevs = self._get_stddevs(stddev_types, dists.rrup)\n        mean = self._apply_amplification_factor(mean, sites.vs30)\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mean value for the base class of the object.", "response": "def _get_mean(self, imt, mag, hypo_depth, rrup, d):\n        \"\"\"\n        Return mean value as defined in equation 3.5.1-1 page 148\n        \"\"\"\n        # clip magnitude at 8.3 as per note at page 3-36 in table Table 3.3.2-6\n        # in \"Technical Reports on National Seismic Hazard Maps for Japan\"\n        mag = min(mag, 8.3)\n        if imt.name == 'PGV':\n            mean = (\n                0.58 * mag +\n                0.0038 * hypo_depth +\n                d -\n                1.29 -\n                np.log10(rrup + 0.0028 * 10 ** (0.5 * mag)) -\n                0.002 * rrup\n            )\n        else:\n            mean = (\n                0.50 * mag +\n                0.0043 * hypo_depth +\n                d +\n                0.61 -\n                np.log10(rrup + 0.0055 * 10 ** (0.5 * mag)) -\n                0.003 * rrup\n            )\n            mean = np.log10(10**(mean)/(g*100))\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_stddevs(self, stddev_types, rrup):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n        std = np.zeros_like(rrup)\n        std[rrup <= 20] = 0.23\n        idx = (rrup > 20) & (rrup <= 30)\n        std[idx] = 0.23 - 0.03 * np.log10(rrup[idx] / 20) / np.log10(30. / 20.)\n        std[rrup > 30] = 0.20\n        # convert from log10 to ln\n        std = np.log(10 ** std)\n        return [std for stddev_type in stddev_types]", "response": "Return standard deviations as defined in equation 3. 5. 5 - 2 page 151."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_amplification_factor(self, mean, vs30):\n        assert np.all(vs30 == vs30[0])\n        if abs(vs30[0]-600.) < 1e-10:\n            return mean * np.log(10)\n        elif abs(vs30[0]-400.) < 1e-10:\n            return mean * np.log(10) + np.log(self.AMP_F)\n        elif abs(vs30[0]-800.) < 1e-10:\n            return mean * np.log(10) - np.log(1.25)\n        else:\n            raise ValueError('Si and Midorikawa 1999 do not support this Vs30 value')", "response": "Apply amplification factor to scale PGV value from 600 m/s vs30 to 800 m/s and convert mean from base 10 to base 10."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_stddevs(self, stddev_types, pgv):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n        std = np.zeros_like(pgv)\n        std[pgv <= 25] = 0.20\n        idx = (pgv > 25) & (pgv <= 50)\n        std[idx] = 0.20 - 0.05 * (pgv[idx] - 25) / 25\n        std[pgv > 50] = 0.15\n        # convert from log10 to ln\n        std = np.log(10 ** std)\n        return [std for stddev_type in stddev_types]", "response": "Return standard deviations as defined in equation 3. 5. 5 - 1 page 151."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the mean value and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        Implements equation 3.5.1-1 page 148 for mean value and equation\n        3.5.5-1 page 151 for total standard deviation.\n\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n        x_tr = _get_min_distance_to_sub_trench(sites.lon, sites.lat)\n        mean = _apply_subduction_trench_correction(\n            mean, x_tr, rup.hypo_depth, dists.rrup, imt)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the mean value and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        Implements equation 3.5.1-1 page 148 for mean value and equation\n        3.5.5-1 page 151 for total standard deviation.\n\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n        x_vf = _get_min_distance_to_volcanic_front(sites.lon, sites.lat)\n        mean = _apply_volcanic_front_correction(mean, x_vf, rup.hypo_depth, imt)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a figure for the series of tasks and memory arrays.", "response": "def make_figure(plots):\n    \"\"\"\n    :param plots: list of pairs (task_name, memory array)\n    \"\"\"\n    # NB: matplotlib is imported inside since it is a costly import\n    import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots()\n    ax.grid(True)\n    ax.set_xlabel('tasks')\n    ax.set_ylabel('GB')\n    start = 0\n    for task_name, mem in plots:\n        ax.plot(range(start, start + len(mem)), mem, label=task_name)\n        start += len(mem)\n    ax.legend()\n    return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_memory(calc_id=-1):\n    dstore = util.read(calc_id)\n    plots = []\n    for task_name in dstore['task_info']:\n        mem = dstore['task_info/' + task_name]['mem_gb']\n        plots.append((task_name, mem))\n    plt = make_figure(plots)\n    plt.show()", "response": "Plot the memory occupation of the given calculation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_hazard_maps(curves, imls, poes):\n    poes = numpy.array(poes)\n\n    if len(poes.shape) == 0:\n        # `poes` was passed in as a scalar;\n        # convert it to 1D array of 1 element\n        poes = poes.reshape(1)\n\n    if len(curves.shape) == 1:\n        # `curves` was passed as 1 dimensional array, there is a single site\n        curves = curves.reshape((1,) + curves.shape)  # 1 x L\n\n    L = curves.shape[1]  # number of levels\n    if L != len(imls):\n        raise ValueError('The curves have %d levels, %d were passed' %\n                         (L, len(imls)))\n    result = []\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # avoid RuntimeWarning: divide by zero encountered in log\n        # happening in the classical_tiling tests\n        imls = numpy.log(numpy.array(imls[::-1]))\n    for curve in curves:\n        # the hazard curve, having replaced the too small poes with EPSILON\n        curve_cutoff = [max(poe, EPSILON) for poe in curve[::-1]]\n        hmap_val = []\n        for poe in poes:\n            # special case when the interpolation poe is bigger than the\n            # maximum, i.e the iml must be smaller than the minumum\n            if poe > curve_cutoff[-1]:  # the greatest poes in the curve\n                # extrapolate the iml to zero as per\n                # https://bugs.launchpad.net/oq-engine/+bug/1292093\n                # a consequence is that if all poes are zero any poe > 0\n                # is big and the hmap goes automatically to zero\n                hmap_val.append(0)\n            else:\n                # exp-log interpolation, to reduce numerical errors\n                # see https://bugs.launchpad.net/oq-engine/+bug/1252770\n                val = numpy.exp(\n                    numpy.interp(\n                        numpy.log(poe), numpy.log(curve_cutoff), imls))\n                hmap_val.append(val)\n\n        result.append(hmap_val)\n    return numpy.array(result)", "response": "Given a set of hazard curves and a set of hazard levels and a geographical location values compute a set of hazard maps at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a set of ground motion values (``gmvs``) and intensity measure levels (``imls``), compute hazard curve probabilities of exceedance. :param gmvs: A list of ground motion values, as floats. :param imls: A list of intensity measure levels, as floats. :param float invest_time: Investigation time, in years. It is with this time span that we compute probabilities of exceedance. Another way to put it is the following. When computing a hazard curve, we want to answer the question: What is the probability of ground motion meeting or exceeding the specified levels (``imls``) in a given time span (``invest_time``). :param float duration: Time window during which GMFs occur. Another was to say it is, the period of time over which we simulate ground motion occurrences. NOTE: Duration is computed as the calculation investigation time multiplied by the number of stochastic event sets. :returns: Numpy array of PoEs (probabilities of exceedance).", "response": "def _gmvs_to_haz_curve(gmvs, imls, invest_time, duration):\n    \"\"\"\n    Given a set of ground motion values (``gmvs``) and intensity measure levels\n    (``imls``), compute hazard curve probabilities of exceedance.\n\n    :param gmvs:\n        A list of ground motion values, as floats.\n    :param imls:\n        A list of intensity measure levels, as floats.\n    :param float invest_time:\n        Investigation time, in years. It is with this time span that we compute\n        probabilities of exceedance.\n\n        Another way to put it is the following. When computing a hazard curve,\n        we want to answer the question: What is the probability of ground\n        motion meeting or exceeding the specified levels (``imls``) in a given\n        time span (``invest_time``).\n    :param float duration:\n        Time window during which GMFs occur. Another was to say it is, the\n        period of time over which we simulate ground motion occurrences.\n\n        NOTE: Duration is computed as the calculation investigation time\n        multiplied by the number of stochastic event sets.\n\n    :returns:\n        Numpy array of PoEs (probabilities of exceedance).\n    \"\"\"\n    # convert to numpy array and redimension so that it can be broadcast with\n    # the gmvs for computing PoE values; there is a gmv for each rupture\n    # here is an example: imls = [0.03, 0.04, 0.05], gmvs=[0.04750576]\n    # => num_exceeding = [1, 1, 0] coming from 0.04750576 > [0.03, 0.04, 0.05]\n    imls = numpy.array(imls).reshape((len(imls), 1))\n    num_exceeding = numpy.sum(numpy.array(gmvs) >= imls, axis=1)\n    poes = 1 - numpy.exp(- (invest_time / duration) * num_exceeding)\n    return poes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the hazard maps associated to the passed probability map.", "response": "def make_hmap(pmap, imtls, poes):\n    \"\"\"\n    Compute the hazard maps associated to the passed probability map.\n\n    :param pmap: hazard curves in the form of a ProbabilityMap\n    :param imtls: DictArray with M intensity measure types\n    :param poes: P PoEs where to compute the maps\n    :returns: a ProbabilityMap with size (N, M, P)\n    \"\"\"\n    M, P = len(imtls), len(poes)\n    hmap = probability_map.ProbabilityMap.build(M, P, pmap, dtype=F32)\n    if len(pmap) == 0:\n        return hmap  # empty hazard map\n    for i, imt in enumerate(imtls):\n        curves = numpy.array([pmap[sid].array[imtls(imt), 0]\n                              for sid in pmap.sids])\n        data = compute_hazard_maps(curves, imtls[imt], poes)  # array (N, P)\n        for sid, value in zip(pmap.sids, data):\n            array = hmap[sid].array\n            for j, val in enumerate(value):\n                array[i, j] = val\n    return hmap"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an array of hazard maps for the given probability map imtls and poes.", "response": "def make_hmap_array(pmap, imtls, poes, nsites):\n    \"\"\"\n    :returns: a compound array of hazard maps of shape nsites\n    \"\"\"\n    if isinstance(pmap, probability_map.ProbabilityMap):\n        # this is here for compatibility with the\n        # past, it could be removed in the future\n        hmap = make_hmap(pmap, imtls, poes)\n        pdic = general.DictArray({imt: poes for imt in imtls})\n        return convert_to_array(hmap, nsites, pdic)\n    try:\n        hcurves = pmap.value\n    except AttributeError:\n        hcurves = pmap\n    dtlist = [('%s-%s' % (imt, poe), F32) for imt in imtls for poe in poes]\n    array = numpy.zeros(len(pmap), dtlist)\n    for imt, imls in imtls.items():\n        curves = hcurves[:, imtls(imt)]\n        for poe in poes:\n            array['%s-%s' % (imt, poe)] = compute_hazard_maps(\n                curves, imls, poe).flat\n    return array"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_uhs(hmap, info):\n    uhs = numpy.zeros(len(hmap), info['uhs_dt'])\n    for p, poe in enumerate(info['poes']):\n        for m, imt in enumerate(info['imtls']):\n            if imt.startswith(('PGA', 'SA')):\n                uhs[str(poe)][imt] = hmap[:, m, p]\n    return uhs", "response": "Make Uniform Hazard Spectra curves for each location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_array(self, ebruptures):\n        data = []\n        for ebr in ebruptures:\n            rup = ebr.rupture\n            self.cmaker.add_rup_params(rup)\n            ruptparams = tuple(getattr(rup, param) for param in self.params)\n            point = rup.surface.get_middle_point()\n            multi_lons, multi_lats = rup.surface.get_surface_boundaries()\n            bounds = ','.join('((%s))' % ','.join(\n                '%.5f %.5f' % (lon, lat) for lon, lat in zip(lons, lats))\n                              for lons, lats in zip(multi_lons, multi_lats))\n            try:\n                rate = ebr.rupture.occurrence_rate\n            except AttributeError:  # for nonparametric sources\n                rate = numpy.nan\n            data.append(\n                (ebr.serial, ebr.srcidx, ebr.n_occ, rate,\n                 rup.mag, point.x, point.y, point.z, rup.surface.get_strike(),\n                 rup.surface.get_dip(), rup.rake,\n                 'MULTIPOLYGON(%s)' % decode(bounds)) + ruptparams)\n        return numpy.array(data, self.dt)", "response": "Convert a list of ebruptures into a numpy array of dtype RuptureRata. dt\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, rup_array):\n        self.nruptures += len(rup_array)\n        offset = len(self.datastore['rupgeoms'])\n        rup_array.array['gidx1'] += offset\n        rup_array.array['gidx2'] += offset\n        previous = self.datastore.get_attr('ruptures', 'nbytes', 0)\n        self.datastore.extend(\n            'ruptures', rup_array, nbytes=previous + rup_array.nbytes)\n        self.datastore.extend('rupgeoms', rup_array.geom)\n        # TODO: PMFs for nonparametric ruptures are not stored\n        self.datastore.flush()", "response": "Save the rupture array in array format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving information about the rupture codes as attributes of the UCERF dataset.", "response": "def close(self):\n        \"\"\"\n        Save information about the rupture codes as attributes of the\n        'ruptures' dataset.\n        \"\"\"\n        if 'ruptures' not in self.datastore:  # for UCERF\n            return\n        codes = numpy.unique(self.datastore['ruptures']['code'])\n        attr = {'code_%d' % code: ' '.join(\n            cls.__name__ for cls in BaseRupture.types[code])\n                for code in codes}\n        self.datastore.set_attrs('ruptures', **attr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a LossCurvesMapsBuilder instance for the given event based risk calculation.", "response": "def get_loss_builder(dstore, return_periods=None, loss_dt=None):\n    \"\"\"\n    :param dstore: datastore for an event based risk calculation\n    :returns: a LossCurvesMapsBuilder instance\n    \"\"\"\n    oq = dstore['oqparam']\n    weights = dstore['weights'][:, 0]\n    eff_time = oq.investigation_time * oq.ses_per_logic_tree_path\n    num_events = countby(dstore['events'].value, 'rlz')\n    periods = return_periods or oq.return_periods or scientific.return_periods(\n        eff_time, max(num_events.values()))\n    return scientific.LossCurvesMapsBuilder(\n        oq.conditional_loss_poes, numpy.array(periods),\n        loss_dt or oq.loss_dt(), weights, num_events,\n        eff_time, oq.risk_investigation_time)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, what):\n        if '/' not in what:\n            key, spec = what, ''\n        else:\n            key, spec = what.split('/')\n        if spec and not spec.startswith(('ref-', 'sid-')):\n            raise ValueError('Wrong specification in %s' % what)\n        elif spec == '':  # export losses for all assets\n            aids = []\n            arefs = []\n            for aid, rec in enumerate(self.assetcol.array):\n                aids.append(aid)\n                arefs.append(self.asset_refs[aid])\n        elif spec.startswith('sid-'):  # passed the site ID\n            sid = int(spec[4:])\n            aids = []\n            arefs = []\n            for aid, rec in enumerate(self.assetcol.array):\n                if rec['site_id'] == sid:\n                    aids.append(aid)\n                    arefs.append(self.asset_refs[aid])\n        elif spec.startswith('ref-'):  # passed the asset name\n            arefs = [spec[4:]]\n            aids = [self.str2asset[arefs[0]]['ordinal']]\n        else:\n            raise ValueError('Wrong specification in %s' % what)\n        return aids, arefs, spec, key", "response": "parse what into a list of aids arefs and spec"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_csv(self, spec, asset_refs, curves_dict):\n        writer = writers.CsvWriter(fmt=writers.FIVEDIGITS)\n        ebr = hasattr(self, 'builder')\n        for key in sorted(curves_dict):\n            recs = curves_dict[key]\n            data = [['asset', 'loss_type', 'loss', 'period' if ebr else 'poe']]\n            for li, loss_type in enumerate(self.loss_types):\n                if ebr:  # event_based_risk\n                    array = recs[:, :, li]  # shape (A, P, LI)\n                    periods = self.builder.return_periods\n                    for aref, losses in zip(asset_refs, array):\n                        for period, loss in zip(periods, losses):\n                            data.append((aref, loss_type, loss, period))\n                else:  # classical_risk\n                    array = recs[loss_type]  # shape (A,) loss_curve_dt\n                    for aref, losses, poes in zip(\n                            asset_refs, array['losses'], array['poes']):\n                        for loss, poe in zip(losses, poes):\n                            data.append((aref, loss_type, loss, poe))\n            dest = self.dstore.build_fname(\n                'loss_curves', '%s-%s' % (spec, key) if spec else key, 'csv')\n            writer.save(data, dest)\n        return writer.getsaved()", "response": "Export the loss curves for the asset_refs and asset_refs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the data for the current node.", "response": "def export(self, export_type, what):\n        \"\"\"\n        :param export_type: 'csv', 'json', ...\n        :param what: string describing what to export\n        :returns: list of exported file names\n        \"\"\"\n        aids, arefs, spec, key = self.parse(what)\n        if key.startswith('rlz'):\n            curves = self.export_curves_rlzs(aids, key)\n        else:  # statistical exports\n            curves = self.export_curves_stats(aids, key)\n        return getattr(self, 'export_' + export_type)(spec, arefs, curves)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary key - > record of dtype loss_curve_dt", "response": "def export_curves_rlzs(self, aids, key):\n        \"\"\"\n        :returns: a dictionary key -> record of dtype loss_curve_dt\n        \"\"\"\n        if 'loss_curves-stats' in self.dstore:  # classical_risk\n            if self.R == 1:\n                data = self.dstore['loss_curves-stats'][aids]  # shape (A, 1)\n            else:\n                data = self.dstore['loss_curves-rlzs'][aids]  # shape (A, R)\n            if key.startswith('rlz-'):\n                rlzi = int(key[4:])\n                return {key: data[:, rlzi]}\n            # else key == 'rlzs', returns all data\n            return {'rlz-%03d' % rlzi: data[:, rlzi] for rlzi in range(self.R)}\n\n        # otherwise event_based\n        curves = self.dstore['curves-rlzs'][aids]  # shape (A, R, P)\n        if key.startswith('rlz-'):\n            rlzi = int(key[4:])\n            return {'rlz-%03d' % rlzi: curves[:, rlzi]}\n        else:  # key is 'rlzs', return a dictionary will all realizations\n            # this may be disabled in the future unless an asset is specified\n            dic = {}\n            for rlzi in range(self.R):\n                dic['rlz-%03d' % rlzi] = curves[:, rlzi]\n            return dic"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary rlzi - > record of dtype loss_curve_dt", "response": "def export_curves_stats(self, aids, key):\n        \"\"\"\n        :returns: a dictionary rlzi -> record of dtype loss_curve_dt\n        \"\"\"\n        oq = self.dstore['oqparam']\n        stats = oq.hazard_stats().items()  # pair (name, func)\n        stat2idx = {stat[0]: s for s, stat in enumerate(stats)}\n        if 'loss_curves-stats' in self.dstore:  # classical_risk\n            dset = self.dstore['loss_curves-stats']\n            data = dset[aids]  # shape (A, S)\n            if key == 'stats':\n                return {stat[0]: data[:, s] for s, stat in enumerate(stats)}\n            else:  # a specific statistics\n                return {key: data[:, stat2idx[key]]}\n        elif 'curves-stats' in self.dstore:  # event_based_risk\n            dset = self.dstore['curves-stats']\n            data = dset[aids]\n            if key == 'stats':\n                return {stat[0]: data[:, s] for s, stat in enumerate(stats)}\n            else:  # a specific statistics\n                return {key: data[:, stat2idx[key]]}\n        else:\n            raise KeyError('no loss curves in %s' % self.dstore)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and return the mean and standard deviation of the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n\n        Implements equation 14 of Hong & Goda (2007)\n        \"\"\"\n\n        C = self.COEFFS[imt]\n        C_PGA = self.COEFFS[PGA()]\n        C_AMP = self.AMP_COEFFS[imt]\n\n        # Gets the PGA on rock - need to convert from g to cm/s/s\n        pga_rock = self._compute_pga_rock(C_PGA, rup.mag, dists.rjb) * 980.665\n        # Get the mean ground motion value\n        mean = (self._compute_nonlinear_magnitude_term(C, rup.mag) +\n                self._compute_magnitude_distance_term(C, dists.rjb, rup.mag) +\n                self._get_site_amplification(C_AMP, sites.vs30, pga_rock))\n\n        # Get standard deviations\n        stddevs = self._get_stddevs(C, stddev_types, dists.rjb.shape)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the PGA on rock based on the linear magnitude and simple distance term.", "response": "def _compute_pga_rock(self, C_PGA, mag, rjb):\n        \"\"\"\n        Returns the PGA (g) on rock, as defined in equation 15\n        \"\"\"\n        return np.exp(self._compute_linear_magnitude_term(C_PGA, mag) +\n                      self._compute_simple_distance_term(C_PGA, rjb))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the non - linear magnitude term in equation 2 page 1070.", "response": "def _compute_nonlinear_magnitude_term(self, C, mag):\n        \"\"\"\n        Computes the non-linear magnitude term\n        \"\"\"\n        return self._compute_linear_magnitude_term(C, mag) +\\\n            C[\"b3\"] * ((mag - 7.0) ** 2.)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_simple_distance_term(self, C, rjb):\n        return C[\"b4\"] * np.log(np.sqrt(rjb ** 2. + C[\"h\"] ** 2.))", "response": "Compute the simple distance term for PGA case."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_magnitude_distance_term(self, C, rjb, mag):\n        rval = np.sqrt(rjb ** 2. + C[\"h\"] ** 2.)\n        return (C[\"b4\"] + C[\"b5\"] * (mag - 4.5)) * np.log(rval)", "response": "Compute the magnitude distance term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the site amplification term based on equations 7 and 8 of the Atkinson and Boore 2006.", "response": "def _get_site_amplification(self, C_AMP, vs30, pga_rock):\n        \"\"\"\n        Gets the site amplification term based on equations 7 and 8 of\n        Atkinson & Boore (2006)\n        \"\"\"\n        # Get nonlinear term\n        bnl = self._get_bnl(C_AMP, vs30)\n        #\n        f_nl_coeff = np.log(60.0 / 100.0) * np.ones_like(vs30)\n        idx = pga_rock > 60.0\n        f_nl_coeff[idx] = np.log(pga_rock[idx] / 100.0)\n        return np.log(np.exp(\n            C_AMP[\"blin\"] * np.log(vs30 / self.CONSTS[\"Vref\"]) +\n            bnl * f_nl_coeff))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the nonlinear term given by equation 8 of Atkinson & Boore 2006", "response": "def _get_bnl(self, C_AMP, vs30):\n        \"\"\"\n        Gets the nonlinear term, given by equation 8 of Atkinson & Boore 2006\n        \"\"\"\n        # Default case 8d\n        bnl = np.zeros_like(vs30)\n        if np.all(vs30 >= self.CONSTS[\"Vref\"]):\n            return bnl\n        # Case 8a\n        bnl[vs30 < self.CONSTS[\"v1\"]] = C_AMP[\"b1sa\"]\n        # Cade 8b\n        idx = np.logical_and(vs30 > self.CONSTS[\"v1\"],\n                             vs30 <= self.CONSTS[\"v2\"])\n\n        if np.any(idx):\n            bnl[idx] = (C_AMP[\"b1sa\"] - C_AMP[\"b2sa\"]) *\\\n                (np.log(vs30[idx] / self.CONSTS[\"v2\"]) /\n                 np.log(self.CONSTS[\"v1\"] / self.CONSTS[\"v2\"])) + C_AMP[\"b2sa\"]\n        # Case 8c\n        idx = np.logical_and(vs30 > self.CONSTS[\"v2\"],\n                             vs30 < self.CONSTS[\"Vref\"])\n        if np.any(idx):\n            bnl[idx] = C_AMP[\"b2sa\"] *\\\n                np.log(vs30[idx] / self.CONSTS[\"Vref\"]) /\\\n                np.log(self.CONSTS[\"v2\"] / self.CONSTS[\"Vref\"])\n        return bnl"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the standard deviations given in Table 2 .", "response": "def _get_stddevs(self, C, stddev_types, stddev_shape):\n        \"\"\"\n        Returns the standard deviations given in Table 2\n        \"\"\"\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(C[\"sigtot\"] + np.zeros(stddev_shape))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(C['sig2'] + np.zeros(stddev_shape))\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(C['sig1'] + np.zeros(stddev_shape))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tidy(fnames):\n    for fname in fnames:\n        try:\n            node = nrml.read(fname)\n        except ValueError as err:\n            print(err)\n            return\n        with open(fname + '.bak', 'wb') as f:\n            f.write(open(fname, 'rb').read())\n        with open(fname, 'wb') as f:\n            # make sure the xmlns i.e. the NRML version is unchanged\n            nrml.write(node.nodes, f, writers.FIVEDIGITS, xmlns=node['xmlns'])\n        print('Reformatted %s, original left in %s.bak' % (fname, fname))", "response": "Reformat a NRML file in a canonical form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the rupture s planar - surface geometry for a given surface.", "response": "def get_geom(surface, is_from_fault_source, is_multi_surface,\n             is_gridded_surface):\n    \"\"\"\n    The following fields can be interpreted different ways,\n    depending on the value of `is_from_fault_source`. If\n    `is_from_fault_source` is True, each of these fields should\n    contain a 2D numpy array (all of the same shape). Each triple\n    of (lon, lat, depth) for a given index represents the node of\n    a rectangular mesh. If `is_from_fault_source` is False, each\n    of these fields should contain a sequence (tuple, list, or\n    numpy array, for example) of 4 values. In order, the triples\n    of (lon, lat, depth) represent top left, top right, bottom\n    left, and bottom right corners of the the rupture's planar\n    surface. Update: There is now a third case. If the rupture\n    originated from a characteristic fault source with a\n    multi-planar-surface geometry, `lons`, `lats`, and `depths`\n    will contain one or more sets of 4 points, similar to how\n    planar surface geometry is stored (see above).\n\n    :param surface: a Surface instance\n    :param is_from_fault_source: a boolean\n    :param is_multi_surface: a boolean\n    \"\"\"\n    if is_from_fault_source:\n        # for simple and complex fault sources,\n        # rupture surface geometry is represented by a mesh\n        surf_mesh = surface.mesh\n        lons = surf_mesh.lons\n        lats = surf_mesh.lats\n        depths = surf_mesh.depths\n    else:\n        if is_multi_surface:\n            # `list` of\n            # openquake.hazardlib.geo.surface.planar.PlanarSurface\n            # objects:\n            surfaces = surface.surfaces\n\n            # lons, lats, and depths are arrays with len == 4*N,\n            # where N is the number of surfaces in the\n            # multisurface for each `corner_*`, the ordering is:\n            #   - top left\n            #   - top right\n            #   - bottom left\n            #   - bottom right\n            lons = numpy.concatenate([x.corner_lons for x in surfaces])\n            lats = numpy.concatenate([x.corner_lats for x in surfaces])\n            depths = numpy.concatenate([x.corner_depths for x in surfaces])\n        elif is_gridded_surface:\n            # the surface mesh has shape (1, N)\n            lons = surface.mesh.lons[0]\n            lats = surface.mesh.lats[0]\n            depths = surface.mesh.depths[0]\n        else:\n            # For area or point source,\n            # rupture geometry is represented by a planar surface,\n            # defined by 3D corner points\n            lons = numpy.zeros((4))\n            lats = numpy.zeros((4))\n            depths = numpy.zeros((4))\n\n            # NOTE: It is important to maintain the order of these\n            # corner points. TODO: check the ordering\n            for i, corner in enumerate((surface.top_left,\n                                        surface.top_right,\n                                        surface.bottom_left,\n                                        surface.bottom_right)):\n                lons[i] = corner.longitude\n                lats[i] = corner.latitude\n                depths[i] = corner.depth\n    return lons, lats, depths"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_eids(rup_array, samples_by_grp, num_rlzs_by_grp):\n    all_eids = []\n    for rup in rup_array:\n        grp_id = rup['grp_id']\n        samples = samples_by_grp[grp_id]\n        num_rlzs = num_rlzs_by_grp[grp_id]\n        num_events = rup['n_occ'] if samples > 1 else rup['n_occ'] * num_rlzs\n        eids = TWO32 * U64(rup['serial']) + numpy.arange(num_events, dtype=U64)\n        all_eids.append(eids)\n    return numpy.concatenate(all_eids)", "response": "Returns a composite array with fields serial n_occ and grp_id\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples the number of occurrences of the class by using the inverse transform Sampling method.", "response": "def sample_number_of_occurrences(self, n=1):\n        \"\"\"\n        See :meth:`superclass method\n        <.rupture.BaseRupture.sample_number_of_occurrences>`\n        for spec of input and result values.\n\n        Uses 'Inverse Transform Sampling' method.\n        \"\"\"\n        # compute cdf from pmf\n        cdf = numpy.cumsum(self.probs_occur)\n        n_occ = numpy.digitize(numpy.random.random(n), cdf)\n        return n_occ"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_events(self, rlzs_by_gsim):\n        all_eids, rlzs = [], []\n        for rlz, eids in self.get_eids_by_rlz(rlzs_by_gsim).items():\n            all_eids.extend(eids)\n            rlzs.extend([rlz] * len(eids))\n        return numpy.fromiter(zip(all_eids, rlzs), events_dt)", "response": "returns an array of events with fields eid rlz"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an array of event IDs for the given group", "response": "def get_eids(self, num_rlzs):\n        \"\"\"\n        :param num_rlzs: the number of realizations for the given group\n        :returns: an array of event IDs\n        \"\"\"\n        num_events = self.n_occ if self.samples > 1 else self.n_occ * num_rlzs\n        return TWO32 * U64(self.serial) + numpy.arange(num_events, dtype=U64)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_events_by_ses(self, events, num_ses):\n        numpy.random.seed(self.serial)\n        sess = numpy.random.choice(num_ses, len(events)) + 1\n        events_by_ses = collections.defaultdict(list)\n        for ses, event in zip(sess, events):\n            events_by_ses[ses].append(event)\n        for ses in events_by_ses:\n            events_by_ses[ses] = numpy.array(events_by_ses[ses])\n        return events_by_ses", "response": "returns a dictionary ses index - > events array"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, mesh, rlzs_by_gsim, num_ses):\n        rupture = self.rupture\n        events = self.get_events(rlzs_by_gsim)\n        events_by_ses = self.get_events_by_ses(events, num_ses)\n        new = ExportedRupture(self.serial, events_by_ses)\n        new.mesh = mesh[()]\n        if isinstance(rupture.surface, geo.ComplexFaultSurface):\n            new.typology = 'complexFaultsurface'\n        elif isinstance(rupture.surface, geo.SimpleFaultSurface):\n            new.typology = 'simpleFaultsurface'\n        elif isinstance(rupture.surface, geo.GriddedSurface):\n            new.typology = 'griddedRupture'\n        elif isinstance(rupture.surface, geo.MultiSurface):\n            new.typology = 'multiPlanesRupture'\n        else:\n            new.typology = 'singlePlaneRupture'\n        new.is_from_fault_source = iffs = isinstance(\n            rupture.surface, (geo.ComplexFaultSurface,\n                              geo.SimpleFaultSurface))\n        new.is_gridded_surface = igs = isinstance(\n            rupture.surface, geo.GriddedSurface)\n        new.is_multi_surface = ims = isinstance(\n            rupture.surface, geo.MultiSurface)\n        new.lons, new.lats, new.depths = get_geom(\n            rupture.surface, iffs, ims, igs)\n        new.surface = rupture.surface\n        new.strike = rupture.surface.get_strike()\n        new.dip = rupture.surface.get_dip()\n        new.rake = rupture.rake\n        new.hypocenter = rupture.hypocenter\n        new.tectonic_region_type = rupture.tectonic_region_type\n        new.magnitude = new.mag = rupture.mag\n        new.top_left_corner = None if iffs or ims or igs else (\n            new.lons[0], new.lats[0], new.depths[0])\n        new.top_right_corner = None if iffs or ims or igs else (\n            new.lons[1], new.lats[1], new.depths[1])\n        new.bottom_left_corner = None if iffs or ims or igs else (\n            new.lons[2], new.lats[2], new.depths[2])\n        new.bottom_right_corner = None if iffs or ims or igs else (\n            new.lons[3], new.lats[3], new.depths[3])\n        return new", "response": "Yields a new instance of ExportedRupture for the given mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_hdf5(input):\n    with performance.Monitor('to_hdf5') as mon:\n        for input_file in input:\n            if input_file.endswith('.npz'):\n                output = convert_npz_hdf5(input_file, input_file[:-3] + 'hdf5')\n            elif input_file.endswith('.xml'):  # for source model files\n                output = convert_xml_hdf5(input_file, input_file[:-3] + 'hdf5')\n            else:\n                continue\n            print('Generated %s' % output)\n    print(mon)", "response": "Convert. xml and. npz files to. hdf5 files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a generator that yields a set of seismic ruptures that are contained in a set of seismic sources.", "response": "def stochastic_event_set(sources, source_site_filter=nofilter):\n    \"\"\"\n    Generates a 'Stochastic Event Set' (that is a collection of earthquake\n    ruptures) representing a possible *realization* of the seismicity as\n    described by a source model.\n\n    The calculator loops over sources. For each source, it loops over ruptures.\n    For each rupture, the number of occurrence is randomly sampled by\n    calling\n    :meth:`openquake.hazardlib.source.rupture.BaseProbabilisticRupture.sample_number_of_occurrences`\n\n    .. note::\n        This calculator is using random numbers. In order to reproduce the\n        same results numpy random numbers generator needs to be seeded, see\n        http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html\n\n    :param sources:\n        An iterator of seismic sources objects (instances of subclasses\n        of :class:`~openquake.hazardlib.source.base.BaseSeismicSource`).\n    :param source_site_filter:\n        The source filter to use (default noop filter)\n    :returns:\n        Generator of :class:`~openquake.hazardlib.source.rupture.Rupture`\n        objects that are contained in an event set. Some ruptures can be\n        missing from it, others can appear one or more times in a row.\n    \"\"\"\n    for source, s_sites in source_site_filter(sources):\n        try:\n            for rupture in source.iter_ruptures():\n                [n_occ] = rupture.sample_number_of_occurrences()\n                for _ in range(n_occ):\n                    yield rupture\n        except Exception as err:\n            etype, err, tb = sys.exc_info()\n            msg = 'An error occurred with source id=%s. Error: %s'\n            msg %= (source.source_id, str(err))\n            raise_(etype, msg, tb)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_rup_array(ebruptures, srcfilter=nofilter):\n    if not BaseRupture._code:\n        BaseRupture.init()  # initialize rupture codes\n\n    rups = []\n    geoms = []\n    nbytes = 0\n    offset = 0\n    for ebrupture in ebruptures:\n        rup = ebrupture.rupture\n        mesh = surface_to_array(rup.surface)\n        sy, sz = mesh.shape[1:]  # sanity checks\n        assert sy < TWO16, 'Too many multisurfaces: %d' % sy\n        assert sz < TWO16, 'The rupture mesh spacing is too small'\n        points = mesh.reshape(3, -1).T   # shape (n, 3)\n        minlon = points[:, 0].min()\n        minlat = points[:, 1].min()\n        maxlon = points[:, 0].max()\n        maxlat = points[:, 1].max()\n        if srcfilter.integration_distance and len(srcfilter.close_sids(\n                (minlon, minlat, maxlon, maxlat),\n                rup.tectonic_region_type, rup.mag)) == 0:\n            continue\n        hypo = rup.hypocenter.x, rup.hypocenter.y, rup.hypocenter.z\n        rate = getattr(rup, 'occurrence_rate', numpy.nan)\n        tup = (ebrupture.serial, ebrupture.srcidx, ebrupture.grp_id,\n               rup.code, ebrupture.n_occ, rup.mag, rup.rake, rate,\n               minlon, minlat, maxlon, maxlat,\n               hypo, offset, offset + len(points), sy, sz)\n        offset += len(points)\n        rups.append(tup)\n        geoms.append(numpy.array([tuple(p) for p in points], point3d))\n        nbytes += rupture_dt.itemsize + mesh.nbytes\n    if not rups:\n        return ()\n    dic = dict(geom=numpy.concatenate(geoms), nbytes=nbytes)\n    # TODO: PMFs for nonparametric ruptures are not converted\n    return hdf5.ArrayWrapper(numpy.array(rups, rupture_dt), dic)", "response": "Convert a list of EBRuptures into a numpy composite array by filtering out the ruptures far away from every site"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a set of ruptures generated by a cluster of sources.", "response": "def sample_cluster(sources, srcfilter, num_ses, param):\n    \"\"\"\n    Yields ruptures generated by a cluster of sources.\n\n    :param sources:\n        A sequence of sources of the same group\n    :param num_ses:\n        Number of stochastic event sets\n    :param param:\n        a dictionary of additional parameters including\n        ses_per_logic_tree_path\n    :yields:\n        dictionaries with keys rup_array, calc_times, eff_ruptures\n    \"\"\"\n    eb_ruptures = []\n    numpy.random.seed(sources[0].serial)\n    [grp_id] = set(src.src_group_id for src in sources)\n    # AccumDict of arrays with 3 elements weight, nsites, calc_time\n    calc_times = AccumDict(accum=numpy.zeros(3, numpy.float32))\n    # Set the parameters required to compute the number of occurrences\n    # of the group of sources\n    #  assert param['oqparam'].number_of_logic_tree_samples > 0\n    samples = getattr(sources[0], 'samples', 1)\n    tom = getattr(sources, 'temporal_occurrence_model')\n    rate = tom.occurrence_rate\n    time_span = tom.time_span\n    # Note that using a single time interval corresponding to the product\n    # of the investigation time and the number of realisations as we do\n    # here is admitted only in the case of a time-independent model\n    grp_num_occ = numpy.random.poisson(rate * time_span * samples *\n                                       num_ses)\n    # Now we process the sources included in the group. Possible cases:\n    # * The group is a cluster. In this case we choose one rupture per each\n    #   source; uncertainty in the ruptures can be handled in this case\n    #   using mutually exclusive ruptures (note that this is admitted\n    #   only for nons-parametric sources).\n    # * The group contains mutually exclusive sources. In this case we\n    #   choose one source and then one rupture from this source.\n    rup_counter = {}\n    rup_data = {}\n    eff_ruptures = 0\n    for rlz_num in range(grp_num_occ):\n        if sources.cluster:\n            for src, _sites in srcfilter(sources):\n                # Sum Ruptures\n                if rlz_num == 0:\n                    eff_ruptures += src.num_ruptures\n                # Track calculation time\n                t0 = time.time()\n                rup = src.get_one_rupture()\n                # The problem here is that we do not know a-priori the\n                # number of occurrences of a given rupture.\n                if src.id not in rup_counter:\n                    rup_counter[src.id] = {}\n                    rup_data[src.id] = {}\n                if rup.idx not in rup_counter[src.id]:\n                    rup_counter[src.id][rup.idx] = 1\n                    rup_data[src.id][rup.idx] = [rup, src.id, grp_id]\n                else:\n                    rup_counter[src.id][rup.idx] += 1\n                # Store info\n                dt = time.time() - t0\n                calc_times[src.id] += numpy.array([len(rup_data[src.id]),\n                                                   src.nsites, dt])\n        elif param['src_interdep'] == 'mutex':\n            print('Not yet implemented')\n            exit(0)\n    # Create event based ruptures\n    for src_key in rup_data:\n        for rup_key in rup_data[src_key]:\n            dat = rup_data[src_key][rup_key]\n            cnt = rup_counter[src_key][rup_key]\n            ebr = EBRupture(dat[0], dat[1], dat[2], cnt, samples)\n            eb_ruptures.append(ebr)\n\n    return eb_ruptures, calc_times, eff_ruptures, grp_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_ruptures(sources, srcfilter, param, monitor=Monitor()):\n    # AccumDict of arrays with 3 elements weight, nsites, calc_time\n    calc_times = AccumDict(accum=numpy.zeros(3, numpy.float32))\n    # Compute and save stochastic event sets\n    num_ses = param['ses_per_logic_tree_path']\n    eff_ruptures = 0\n    ir_mon = monitor('iter_ruptures', measuremem=False)\n    # Compute the number of occurrences of the source group. This is used\n    # for cluster groups or groups with mutually exclusive sources.\n    if (getattr(sources, 'atomic', False) and\n            getattr(sources, 'cluster', False)):\n            eb_ruptures, calc_times, eff_ruptures, grp_id = sample_cluster(\n                sources, srcfilter, num_ses, param)\n\n            # Yield ruptures\n            yield AccumDict(rup_array=get_rup_array(eb_ruptures),\n                            calc_times=calc_times,\n                            eff_ruptures={grp_id: eff_ruptures})\n    else:\n        eb_ruptures = []\n        # AccumDict of arrays with 3 elements weight, nsites, calc_time\n        calc_times = AccumDict(accum=numpy.zeros(3, numpy.float32))\n        [grp_id] = set(src.src_group_id for src in sources)\n        for src, _sites in srcfilter(sources):\n            t0 = time.time()\n            if len(eb_ruptures) > MAX_RUPTURES:\n                # yield partial result to avoid running out of memory\n                yield AccumDict(rup_array=get_rup_array(eb_ruptures,\n                                                        srcfilter),\n                                calc_times={},\n                                eff_ruptures={grp_id: eff_ruptures})\n                eb_ruptures.clear()\n            samples = getattr(src, 'samples', 1)\n            n_occ = 0\n            for rup, n_occ in src.sample_ruptures(samples * num_ses, ir_mon):\n                ebr = EBRupture(rup, src.id, grp_id, n_occ, samples)\n                eb_ruptures.append(ebr)\n                n_occ += ebr.n_occ\n            eff_ruptures += src.num_ruptures\n            dt = time.time() - t0\n            calc_times[src.id] += numpy.array([n_occ, src.nsites, dt])\n        rup_array = get_rup_array(eb_ruptures, srcfilter)\n        yield AccumDict(rup_array=rup_array, calc_times=calc_times,\n                        eff_ruptures={grp_id: eff_ruptures})", "response": "Generate a set of rupture arrays for each source group in the source group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_shakemap_array(grid_file, uncertainty_file=None):\n    data = _get_shakemap_array(grid_file)\n    if uncertainty_file:\n        data2 = _get_shakemap_array(uncertainty_file)\n        # sanity check: lons and lats must be the same\n        for coord in ('lon', 'lat'):\n            numpy.testing.assert_equal(data[coord], data2[coord])\n        # copy the stddevs from the uncertainty array\n        for imt in data2['std'].dtype.names:\n            data['std'][imt] = data2['std'][imt]\n    return data", "response": "returns a numpy array with fields lon lat vs30 val std\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the sites in the next iteration of the base class.", "response": "def plot_sites(calc_id=-1):\n    \"\"\"\n    Plot the sites\n    \"\"\"\n    # NB: matplotlib is imported inside since it is a costly import\n    import matplotlib.pyplot as p\n    dstore = util.read(calc_id)\n    sitecol = dstore['sitecol']\n    lons, lats = sitecol.lons, sitecol.lats\n    if len(lons) > 1 and cross_idl(*lons):\n        lons %= 360\n\n    fig, ax = p.subplots()\n    ax.grid(True)\n    if 'site_model' in dstore:\n        sm = dstore['site_model']\n        sm_lons, sm_lats = sm['lon'], sm['lat']\n        if len(sm_lons) > 1 and cross_idl(*sm_lons):\n            sm_lons %= 360\n        p.scatter(sm_lons, sm_lats, marker='.', color='orange')\n    p.scatter(lons, lats, marker='+')\n    p.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mean and standard deviation of the logarithm of the site and the site site.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n\n        imean = (self._get_magnitude_term(C, rup.mag) +\n                 self._get_distance_term(C, dists.rrup, sites.backarc) +\n                 self._get_site_term(C, sites.vs30) +\n                 self._get_scaling_term(C, dists.rrup))\n        # Convert mean from cm/s and cm/s/s and from common logarithm to\n        # natural logarithm\n        if imt.name in \"SA PGA\":\n            mean = np.log((10.0 ** (imean - 2.0)) / g)\n        else:\n            mean = np.log((10.0 ** (imean)))\n        stddevs = self._get_stddevs(C, len(dists.rrup), stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the distance scaling term which varies depending on whether the site is in the forearc or the backarc.", "response": "def _get_distance_term(self, C, rrup, backarc):\n        \"\"\"\n        Returns the distance scaling term, which varies depending on whether\n        the site is in the forearc or the backarc\n        \"\"\"\n        # Geometric attenuation function\n        distance_scale = -np.log10(np.sqrt(rrup ** 2 + 3600.0))\n        # Anelastic attenuation in the backarc\n        distance_scale[backarc] += (C[\"c2\"] * rrup[backarc])\n        # Anelastic Attenuation in the forearc\n        idx = np.logical_not(backarc)\n        distance_scale[idx] += (C[\"c1\"] * rrup[idx])\n        return distance_scale"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the scaling term for the logarithmic logarithm", "response": "def _get_scaling_term(self, C, rrup):\n        \"\"\"\n        Applies the Cascadia correction factor from Table 2 and the positive\n        correction factor given on Page 567\n        \"\"\"\n        a_f = 0.15 + 0.0007 * rrup\n        a_f[a_f > 0.35] = 0.35\n        return C[\"af\"] + a_f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the disaggregation matrix for a given set of sources.", "response": "def compute_disagg(sitecol, sources, cmaker, iml4, trti, bin_edges,\n                   oqparam, monitor):\n    # see https://bugs.launchpad.net/oq-engine/+bug/1279247 for an explanation\n    # of the algorithm used\n    \"\"\"\n    :param sitecol:\n        a :class:`openquake.hazardlib.site.SiteCollection` instance\n    :param sources:\n        list of hazardlib source objects\n    :param cmaker:\n        a :class:`openquake.hazardlib.gsim.base.ContextMaker` instance\n    :param iml4:\n        an array of intensities of shape (N, R, M, P)\n    :param dict trti:\n        tectonic region type index\n    :param bin_egdes:\n        a dictionary site_id -> edges\n    :param oqparam:\n        the parameters in the job.ini file\n    :param monitor:\n        monitor of the currently running job\n    :returns:\n        a dictionary of probability arrays, with composite key\n        (sid, rlzi, poe, imt, iml, trti).\n    \"\"\"\n    result = {'trti': trti, 'num_ruptures': 0}\n    # all the time is spent in collect_bin_data\n    ruptures = []\n    for src in sources:\n        ruptures.extend(src.iter_ruptures())\n    bin_data = disagg.collect_bin_data(\n        ruptures, sitecol, cmaker, iml4,\n        oqparam.truncation_level, oqparam.num_epsilon_bins, monitor)\n    if bin_data:  # dictionary poe, imt, rlzi -> pne\n        for sid in sitecol.sids:\n            for (poe, imt, rlzi), matrix in disagg.build_disagg_matrix(\n                    bin_data, bin_edges, sid, monitor).items():\n                result[sid, rlzi, poe, imt] = matrix\n        result['cache_info'] = monitor.cache_info\n        result['num_ruptures'] = len(bin_data.mags)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_mean(self, C, mag, rjb):\n        m1 = 6.4\n        r1 = 50.\n        h = 6.\n        R = np.sqrt(rjb ** 2 + h ** 2)\n        R1 = np.sqrt(r1 ** 2 + h ** 2)\n        less_r1 = rjb < r1\n        ge_r1 = rjb >= r1\n\n        mean = (C['c1'] + C['c4'] * (mag - m1) * np.log(R) + C['c5'] * rjb +\n                C['c8'] * (8.5 - mag) ** 2)\n\n        mean[less_r1] += C['c3'] * np.log(R[less_r1])\n        mean[ge_r1] += (C['c3'] * np.log(R1) +\n                        C['c6'] * (np.log(R[ge_r1]) - np.log(R1)))\n\n        if mag < m1:\n            mean += C['c2'] * (mag - m1)\n        else:\n            mean += C['c7'] * (mag - m1)\n\n        return mean", "response": "Compute mean value in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hermann_adjustment_factors(bval, min_mag, mag_inc):\n    '''\n    Returns the adjustment factors (fval, fival) proposed by Hermann (1978)\n\n    :param float bval:\n        Gutenberg & Richter (1944) b-value\n\n    :param np.ndarray min_mag:\n        Minimum magnitude of completeness table\n\n    :param non-negative float mag_inc:\n        Magnitude increment of the completeness table\n    '''\n\n    fval = 10. ** (bval * min_mag)\n    fival = 10. ** (bval * (mag_inc / 2.)) - 10. ** (-bval * (mag_inc / 2.))\n    return fval, fival", "response": "Returns the adjustment factors fval fival"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the incremental a - value from cumulative - using the version of the Hermann ( 2003 ) formula described in Wesson et al. 2003.", "response": "def incremental_a_value(bval, min_mag, mag_inc):\n    '''\n    Incremental a-value from cumulative - using the version of the\n    Hermann (1979) formula described in Wesson et al. (2003)\n\n    :param float bval:\n        Gutenberg & Richter (1944) b-value\n\n    :param np.ndarray min_mag:\n        Minimum magnitude of completeness table\n\n    :param float mag_inc:\n        Magnitude increment of the completeness table\n    '''\n    a_cum = 10. ** (bval * min_mag)\n    a_inc = a_cum + np.log10((10. ** (bval * mag_inc)) -\n                             (10. ** (-bval * mag_inc)))\n\n    return a_inc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_weichert_factor(beta, cmag, cyear, end_year):\n    '''\n    Gets the Weichert adjustment factor for each the magnitude bins\n\n    :param float beta:\n        Beta value of Gutenberg & Richter parameter (b * log(10.))\n\n    :param np.ndarray cmag:\n        Magnitude values of the completeness table\n\n    :param np.ndarray cyear:\n        Year values of the completeness table\n\n    :param float end_year:\n        Last year for consideration in the catalogue\n\n    :returns:\n        Weichert adjustment factor (float)\n    '''\n    if len(cmag) > 1:\n        # cval corresponds to the mid-point of the completeness bins\n        # In the original code it requires that the magnitude bins be\n        # equal sizedclass IsotropicGaussian(BaseSmoothingKernel):\n        dmag = (cmag[1:] + cmag[:-1]) / 2.\n        cval = np.hstack([dmag, cmag[-1] + (dmag[-1] - cmag[-2])])\n    else:\n        # Single completeness value so Weichert factor is unity\n        return 1.0 / (end_year - cyear[0] + 1), None\n\n    t_f = sum(np.exp(-beta * cval)) / sum((end_year - cyear + 1) *\n                                          np.exp(-beta * cval))\n    return t_f, cval", "response": "Gets the Weichert adjustment factor for each magnitude bin in the catalogue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that the given completeness table is in the correct format.", "response": "def check_completeness_table(completeness_table, catalogue):\n    '''\n    Check to ensure completeness table is in the correct format\n    `completeness_table = np.array([[year_, mag_i]]) for i in number of bins`\n\n    :param np.ndarray completeness_table:\n        Completeness table in format [[year, mag]]\n\n    :param catalogue:\n        Instance of openquake.hmtk.seismicity.catalogue.Catalogue class\n\n    :returns:\n        Correct completeness table\n\n    '''\n    if isinstance(completeness_table, np.ndarray):\n        assert np.shape(completeness_table)[1] == 2\n        return completeness_table\n    elif isinstance(completeness_table, list):\n        # Assuming list has only two elements\n        assert len(completeness_table) == 2\n        return np.array([[completeness_table[0], completeness_table[1]]])\n    else:\n        # Accepts the minimum magnitude and earliest year of the catalogue\n        return np.array([[np.min(catalogue.data['year']),\n                          np.min(catalogue.data['magnitude'])]])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the evenly spaced completeness table for the given magnitude.", "response": "def get_even_magnitude_completeness(completeness_table, catalogue=None):\n    '''\n    To make the magnitudes evenly spaced, render to a constant 0.1\n    magnitude unit\n\n    :param np.ndarray completeness_table:\n        Completeness table in format [[year, mag]]\n\n    :param catalogue:\n        Instance of openquake.hmtk.seismicity.catalogue.Catalogue class\n\n    :returns:\n        Correct completeness table\n\n    '''\n    mmax = np.floor(10. * np.max(catalogue.data['magnitude'])) / 10.\n    check_completeness_table(completeness_table, catalogue)\n    cmag = np.hstack([completeness_table[:, 1], mmax + 0.1])\n    cyear = np.hstack([completeness_table[:, 0], completeness_table[-1, 0]])\n    if np.shape(completeness_table)[0] == 1:\n        # Simple single-valued table\n        return completeness_table, 0.1\n\n    for iloc in range(0, len(cmag) - 1):\n        mrange = np.arange(np.floor(10. * cmag[iloc]) / 10.,\n                           (np.ceil(10. * cmag[iloc + 1]) / 10.),\n                           0.1)\n        temp_table = np.column_stack([\n            cyear[iloc] * np.ones(len(mrange), dtype=float),\n            mrange])\n        if iloc == 0:\n            completeness_table = np.copy(temp_table)\n        else:\n            completeness_table = np.vstack([completeness_table,\n                                            temp_table])\n    # completeness_table = np.vstack([completeness_table,\n    #    np.array([[cyear[-1], cmag[-1]]])])\n    return completeness_table, 0.1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of unique objects.", "response": "def unique(objects, key=None):\n    \"\"\"\n    Raise a ValueError if there is a duplicated object, otherwise\n    returns the objects as they are.\n    \"\"\"\n    dupl = []\n    for obj, group in itertools.groupby(sorted(objects), key):\n        if sum(1 for _ in group) > 1:\n            dupl.append(obj)\n    if dupl:\n        raise ValueError('Found duplicates %s' % dupl)\n    return objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_effective_rlzs(rlzs):\n    effective = []\n    for uid, group in groupby(rlzs, operator.attrgetter('uid')).items():\n        rlz = group[0]\n        if all(path == '@' for path in rlz.lt_uid):  # empty realization\n            continue\n        effective.append(\n            Realization(rlz.value, sum(r.weight for r in group),\n                        rlz.lt_path, rlz.ordinal, rlz.lt_uid))\n    return effective", "response": "Return a list of all effective realizations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake random samples of a sequence of weighted objects and return a subsequence of the original sequence with num_samples elements.", "response": "def sample(weighted_objects, num_samples, seed):\n    \"\"\"\n    Take random samples of a sequence of weighted objects\n\n    :param weighted_objects:\n        A finite sequence of objects with a `.weight` attribute.\n        The weights must sum up to 1.\n    :param num_samples:\n        The number of samples to return\n    :param seed:\n        A random seed\n    :return:\n        A subsequence of the original sequence with `num_samples` elements\n    \"\"\"\n    weights = []\n    for obj in weighted_objects:\n        w = obj.weight\n        if isinstance(obj.weight, float):\n            weights.append(w)\n        else:\n            weights.append(w['weight'])\n    numpy.random.seed(seed)\n    idxs = numpy.random.choice(len(weights), num_samples, p=weights)\n    # NB: returning an array would break things\n    return [weighted_objects[idx] for idx in idxs]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a source model XML file and returns a list of SourceGroup objects containing source nodes", "response": "def read_source_groups(fname):\n    \"\"\"\n    :param fname: a path to a source model XML file\n    :return: a list of SourceGroup objects containing source nodes\n    \"\"\"\n    smodel = nrml.read(fname).sourceModel\n    src_groups = []\n    if smodel[0].tag.endswith('sourceGroup'):  # NRML 0.5 format\n        for sg_node in smodel:\n            sg = SourceGroup(sg_node['tectonicRegion'])\n            sg.sources = sg_node.nodes\n            src_groups.append(sg)\n    else:  # NRML 0.4 format: smodel is a list of source nodes\n        src_groups.extend(SourceGroup.collect(smodel))\n    return src_groups"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toml(uncertainty):\n    text = uncertainty.text.strip()\n    if not text.startswith('['):  # a bare GSIM name was passed\n        text = '[%s]' % text\n    for k, v in uncertainty.attrib.items():\n        try:\n            v = ast.literal_eval(v)\n        except ValueError:\n            v = repr(v)\n        text += '\\n%s = %s' % (k, v)\n    return text", "response": "Converts an uncertainty node into a TOML string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef name(self):\n        names = self.names.split()\n        if len(names) == 1:\n            return names[0]\n        elif len(names) == 2:\n            return ' '.join(names)\n        else:\n            return ' '.join([names[0], '...', names[-1]])", "response": "Returns a compact representation of the names"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_skeleton(self):\n        src_groups = []\n        for grp in self.src_groups:\n            sg = copy.copy(grp)\n            sg.sources = []\n            src_groups.append(sg)\n        return self.__class__(self.names, self.weight, self.path, src_groups,\n                              self.num_gsim_paths, self.ordinal, self.samples)", "response": "Returns an empty copy of the source model with the proper attributes for each SourceGroup contained within."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates all possible paths starting from this branch set.", "response": "def enumerate_paths(self):\n        \"\"\"\n        Generate all possible paths starting from this branch set.\n\n        :returns:\n            Generator of two-item tuples. Each tuple contains weight\n            of the path (calculated as a product of the weights of all path's\n            branches) and list of path's :class:`Branch` objects. Total sum\n            of all paths' weights is 1.0\n        \"\"\"\n        for path in self._enumerate_paths([]):\n            flat_path = []\n            weight = 1.0\n            while path:\n                path, branch = path\n                weight *= branch.weight\n                flat_path.append(branch)\n            yield weight, flat_path[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator over all paths in the tree that are located under prefix_path.", "response": "def _enumerate_paths(self, prefix_path):\n        \"\"\"\n        Recursive (private) part of :func:`enumerate_paths`. Returns generator\n        of recursive lists of two items, where second item is the branch object\n        and first one is itself list of two items.\n        \"\"\"\n        for branch in self.branches:\n            path = [prefix_path, branch]\n            if branch.child_branchset is not None:\n                for subpath in branch.child_branchset._enumerate_paths(path):\n                    yield subpath\n            else:\n                yield path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a branch object corresponding to the branch_id.", "response": "def get_branch_by_id(self, branch_id):\n        \"\"\"\n        Return :class:`Branch` object belonging to this branch set with id\n        equal to ``branch_id``.\n        \"\"\"\n        for branch in self.branches:\n            if branch.branch_id == branch_id:\n                return branch\n        raise AssertionError(\"couldn't find branch '%s'\" % branch_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if uncertainty should be applied to the source.", "response": "def filter_source(self, source):\n        # pylint: disable=R0911,R0912\n        \"\"\"\n        Apply filters to ``source`` and return ``True`` if uncertainty should\n        be applied to it.\n        \"\"\"\n        for key, value in self.filters.items():\n            if key == 'applyToTectonicRegionType':\n                if value != source.tectonic_region_type:\n                    return False\n            elif key == 'applyToSourceType':\n                if value == 'area':\n                    if not isinstance(source, ohs.AreaSource):\n                        return False\n                elif value == 'point':\n                    # area source extends point source\n                    if (not isinstance(source, ohs.PointSource)\n                            or isinstance(source, ohs.AreaSource)):\n                        return False\n                elif value == 'simpleFault':\n                    if not isinstance(source, ohs.SimpleFaultSource):\n                        return False\n                elif value == 'complexFault':\n                    if not isinstance(source, ohs.ComplexFaultSource):\n                        return False\n                elif value == 'characteristicFault':\n                    if not isinstance(source, ohs.CharacteristicFaultSource):\n                        return False\n                else:\n                    raise AssertionError(\"unknown source type '%s'\" % value)\n            elif key == 'applyToSources':\n                if source and source.source_id not in value:\n                    return False\n            else:\n                raise AssertionError(\"unknown filter '%s'\" % key)\n        # All filters pass, return True.\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply this branchset s uncertainty with value to source .", "response": "def apply_uncertainty(self, value, source):\n        \"\"\"\n        Apply this branchset's uncertainty with value ``value`` to source\n        ``source``, if it passes :meth:`filters <filter_source>`.\n\n        This method is not called for uncertainties of types \"gmpeModel\"\n        and \"sourceModel\".\n\n        :param value:\n            The actual uncertainty value of :meth:`sampled <sample>` branch.\n            Type depends on uncertainty type.\n        :param source:\n            The opensha source data object.\n        :return:\n            0 if the source was not changed, 1 otherwise\n        \"\"\"\n        if not self.filter_source(source):\n            # source didn't pass the filter\n            return 0\n        if self.uncertainty_type in MFD_UNCERTAINTY_TYPES:\n            self._apply_uncertainty_to_mfd(source.mfd, value)\n        elif self.uncertainty_type in GEOMETRY_UNCERTAINTY_TYPES:\n            self._apply_uncertainty_to_geometry(source, value)\n        else:\n            raise AssertionError(\"unknown uncertainty type '%s'\"\n                                 % self.uncertainty_type)\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _apply_uncertainty_to_geometry(self, source, value):\n        if self.uncertainty_type == 'simpleFaultDipRelative':\n            source.modify('adjust_dip', dict(increment=value))\n        elif self.uncertainty_type == 'simpleFaultDipAbsolute':\n            source.modify('set_dip', dict(dip=value))\n        elif self.uncertainty_type == 'simpleFaultGeometryAbsolute':\n            trace, usd, lsd, dip, spacing = value\n            source.modify(\n                'set_geometry',\n                dict(fault_trace=trace, upper_seismogenic_depth=usd,\n                     lower_seismogenic_depth=lsd, dip=dip, spacing=spacing))\n        elif self.uncertainty_type == 'complexFaultGeometryAbsolute':\n            edges, spacing = value\n            source.modify('set_geometry', dict(edges=edges, spacing=spacing))\n        elif self.uncertainty_type == 'characteristicFaultGeometryAbsolute':\n            source.modify('set_geometry', dict(surface=value))", "response": "Modify source geometry with the uncertainty value value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_uncertainty_to_mfd(self, mfd, value):\n        if self.uncertainty_type == 'abGRAbsolute':\n            a, b = value\n            mfd.modify('set_ab', dict(a_val=a, b_val=b))\n\n        elif self.uncertainty_type == 'bGRRelative':\n            mfd.modify('increment_b', dict(value=value))\n\n        elif self.uncertainty_type == 'maxMagGRRelative':\n            mfd.modify('increment_max_mag', dict(value=value))\n\n        elif self.uncertainty_type == 'maxMagGRAbsolute':\n            mfd.modify('set_max_mag', dict(value=value))\n\n        elif self.uncertainty_type == 'incrementalMFDAbsolute':\n            min_mag, bin_width, occur_rates = value\n            mfd.modify('set_mfd', dict(min_mag=min_mag, bin_width=bin_width,\n                                       occurrence_rates=occur_rates))", "response": "Applies uncertainty value to the specified mfd object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_source_models(self, gsim_lt):\n        num_gsim_paths = 1 if self.num_samples else gsim_lt.get_num_paths()\n        for i, rlz in enumerate(self):\n            yield LtSourceModel(\n                rlz.value, rlz.weight, ('b1',), [], num_gsim_paths, i, 1)", "response": "Yields the underlying LtSourceModel instances multiple times if there is sampling\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_each_source(self):\n        return (self.info.applytosources and\n                self.info.applytosources == self.source_ids)", "response": "True if there is an applyToSources for each source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the whole tree and point root_branchset attribute to the tree s root.", "response": "def parse_tree(self, tree_node, validate):\n        \"\"\"\n        Parse the whole tree and point ``root_branchset`` attribute\n        to the tree's root.\n        \"\"\"\n        self.info = collect_info(self.filename)\n        self.source_ids = collections.defaultdict(list)\n        t0 = time.time()\n        for depth, branchinglevel_node in enumerate(tree_node.nodes):\n            self.parse_branchinglevel(branchinglevel_node, depth, validate)\n        dt = time.time() - t0\n        if validate:\n            bname = os.path.basename(self.filename)\n            logging.info('Validated %s in %.2f seconds', bname, dt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_branchinglevel(self, branchinglevel_node, depth, validate):\n        new_open_ends = set()\n        branchsets = branchinglevel_node.nodes\n        for number, branchset_node in enumerate(branchsets):\n            branchset = self.parse_branchset(branchset_node, depth, number,\n                                             validate)\n            self.parse_branches(branchset_node, branchset, validate)\n            if self.root_branchset is None:  # not set yet\n                self.num_paths = 1\n                self.root_branchset = branchset\n            else:\n                self.apply_branchset(branchset_node, branchset)\n            for branch in branchset.branches:\n                new_open_ends.add(branch)\n            self.num_paths *= len(branchset.branches)\n        if number > 0:\n            logging.warning('There is a branching level with multiple '\n                            'branchsets in %s', self.filename)\n        self.open_ends.clear()\n        self.open_ends.update(new_open_ends)", "response": "Parse a branching level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a branchset node.", "response": "def parse_branchset(self, branchset_node, depth, number, validate):\n        \"\"\"\n        Create :class:`BranchSet` object using data in ``branchset_node``.\n\n        :param branchset_node:\n            ``etree.Element`` object with tag \"logicTreeBranchSet\".\n        :param depth:\n            The sequential number of branchset's branching level, based on 0.\n        :param number:\n            Index number of this branchset inside branching level, based on 0.\n        :param validate:\n            Whether or not filters defined in branchset and the branchset\n            itself should be validated.\n        :returns:\n            An instance of :class:`BranchSet` with filters applied but with\n            no branches (they're attached in :meth:`parse_branches`).\n        \"\"\"\n        uncertainty_type = branchset_node.attrib.get('uncertaintyType')\n        filters = dict((filtername, branchset_node.attrib.get(filtername))\n                       for filtername in self.FILTERS\n                       if filtername in branchset_node.attrib)\n        if validate:\n            self.validate_filters(branchset_node, uncertainty_type, filters)\n        filters = self.parse_filters(branchset_node, uncertainty_type, filters)\n        branchset = BranchSet(uncertainty_type, filters)\n        if validate:\n            self.validate_branchset(branchset_node, depth, number, branchset)\n        return branchset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_branches(self, branchset_node, branchset, validate):\n        weight_sum = 0\n        branches = branchset_node.nodes\n        values = []\n        for branchnode in branches:\n            weight = ~branchnode.uncertaintyWeight\n            weight_sum += weight\n            value_node = node_from_elem(branchnode.uncertaintyModel)\n            if value_node.text is not None:\n                values.append(value_node.text.strip())\n            if validate:\n                self.validate_uncertainty_value(\n                    value_node, branchnode, branchset)\n            value = self.parse_uncertainty_value(value_node, branchset)\n            branch_id = branchnode.attrib.get('branchID')\n            branch = Branch(branch_id, weight, value)\n            if branch_id in self.branches:\n                raise LogicTreeError(\n                    branchnode, self.filename,\n                    \"branchID '%s' is not unique\" % branch_id)\n            self.branches[branch_id] = branch\n            branchset.branches.append(branch)\n        if abs(weight_sum - 1.0) > pmf.PRECISION:\n            raise LogicTreeError(\n                branchset_node, self.filename,\n                \"branchset weights don't sum up to 1.0\")\n        if len(set(values)) < len(values):\n            # TODO: add a test for this case\n            # <logicTreeBranch branchID=\"b71\">\n            #     <uncertaintyModel> 7.7 </uncertaintyModel>\n            #     <uncertaintyWeight>0.333</uncertaintyWeight>\n            # </logicTreeBranch>\n            # <logicTreeBranch branchID=\"b72\">\n            #     <uncertaintyModel> 7.695 </uncertaintyModel>\n            #     <uncertaintyWeight>0.333</uncertaintyWeight>\n            # </logicTreeBranch>\n            # <logicTreeBranch branchID=\"b73\">\n            #     <uncertaintyModel> 7.7 </uncertaintyModel>\n            #     <uncertaintyWeight>0.334</uncertaintyWeight>\n            # </logicTreeBranch>\n            raise LogicTreeError(\n                branchset_node, self.filename,\n                \"there are duplicate values in uncertaintyModel: \" +\n                ' '.join(values))", "response": "Parse the branchset node and attach branches at branchset_node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield all source models for the given gsim_lt.", "response": "def gen_source_models(self, gsim_lt):\n        \"\"\"\n        Yield empty LtSourceModel instances (one per effective realization)\n        \"\"\"\n        samples_by_lt_path = self.samples_by_lt_path()\n        for i, rlz in enumerate(get_effective_rlzs(self)):\n            smpath = rlz.lt_path\n            num_samples = samples_by_lt_path[smpath]\n            num_gsim_paths = (num_samples if self.num_samples\n                              else gsim_lt.get_num_paths())\n            yield LtSourceModel(\n                rlz.value, rlz.weight / num_samples, smpath, [],\n                num_gsim_paths, i, num_samples)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the model name and a list of branch ids.", "response": "def sample_path(self, seed):\n        \"\"\"\n        Return the model name and a list of branch ids.\n\n        :param seed: the seed used for the sampling\n        \"\"\"\n        branchset = self.root_branchset\n        branch_ids = []\n        while branchset is not None:\n            [branch] = sample(branchset.branches, 1, seed)\n            branch_ids.append(branch.branch_id)\n            branchset = branch.child_branchset\n        modelname = self.root_branchset.get_branch_by_id(branch_ids[0]).value\n        return modelname, branch_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the uncertainty value from the node and returns it as a pair of floats.", "response": "def parse_uncertainty_value(self, node, branchset):\n        \"\"\"\n        See superclass' method for description and signature specification.\n\n        Doesn't change source model file name, converts other values to either\n        pair of floats or a single float depending on uncertainty type.\n        \"\"\"\n        if branchset.uncertainty_type == 'sourceModel':\n            return node.text.strip()\n        elif branchset.uncertainty_type == 'abGRAbsolute':\n            [a, b] = node.text.strip().split()\n            return float(a), float(b)\n        elif branchset.uncertainty_type == 'incrementalMFDAbsolute':\n            min_mag, bin_width = (node.incrementalMFD[\"minMag\"],\n                                  node.incrementalMFD[\"binWidth\"])\n            return min_mag,  bin_width, ~node.incrementalMFD.occurRates\n        elif branchset.uncertainty_type == 'simpleFaultGeometryAbsolute':\n            return self._parse_simple_fault_geometry_surface(\n                node.simpleFaultGeometry)\n        elif branchset.uncertainty_type == 'complexFaultGeometryAbsolute':\n            return self._parse_complex_fault_geometry_surface(\n                node.complexFaultGeometry)\n        elif branchset.uncertainty_type ==\\\n                'characteristicFaultGeometryAbsolute':\n            surfaces = []\n            for geom_node in node.surface:\n                if \"simpleFaultGeometry\" in geom_node.tag:\n                    trace, usd, lsd, dip, spacing =\\\n                        self._parse_simple_fault_geometry_surface(geom_node)\n                    surfaces.append(geo.SimpleFaultSurface.from_fault_data(\n                        trace, usd, lsd, dip, spacing))\n                elif \"complexFaultGeometry\" in geom_node.tag:\n                    edges, spacing =\\\n                        self._parse_complex_fault_geometry_surface(geom_node)\n                    surfaces.append(geo.ComplexFaultSurface.from_fault_data(\n                        edges, spacing))\n                elif \"planarSurface\" in geom_node.tag:\n                    surfaces.append(\n                        self._parse_planar_geometry_surface(geom_node))\n                else:\n                    pass\n            if len(surfaces) > 1:\n                return geo.MultiSurface(surfaces)\n            else:\n                return surfaces[0]\n        else:\n            return float(node.text.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a simple fault geometry surface node.", "response": "def _parse_simple_fault_geometry_surface(self, node):\n        \"\"\"\n        Parses a simple fault geometry surface\n        \"\"\"\n        spacing = node[\"spacing\"]\n        usd, lsd, dip = (~node.upperSeismoDepth, ~node.lowerSeismoDepth,\n                         ~node.dip)\n        # Parse the geometry\n        coords = split_coords_2d(~node.LineString.posList)\n        trace = geo.Line([geo.Point(*p) for p in coords])\n        return trace, usd, lsd, dip, spacing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a complex fault geometry surface node.", "response": "def _parse_complex_fault_geometry_surface(self, node):\n        \"\"\"\n        Parses a complex fault geometry surface\n        \"\"\"\n        spacing = node[\"spacing\"]\n        edges = []\n        for edge_node in node.nodes:\n            coords = split_coords_3d(~edge_node.LineString.posList)\n            edges.append(geo.Line([geo.Point(*p) for p in coords]))\n        return edges, spacing"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_planar_geometry_surface(self, node):\n        nodes = []\n        for key in [\"topLeft\", \"topRight\", \"bottomRight\", \"bottomLeft\"]:\n            nodes.append(geo.Point(getattr(node, key)[\"lon\"],\n                                   getattr(node, key)[\"lat\"],\n                                   getattr(node, key)[\"depth\"]))\n        top_left, top_right, bottom_right, bottom_left = tuple(nodes)\n        return geo.PlanarSurface.from_corner_points(\n            top_left, top_right, bottom_right, bottom_left)", "response": "Parses a planar geometry surface from a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_uncertainty_value(self, node, branchnode, branchset):\n        _float_re = re.compile(r'^(\\+|\\-)?(\\d+|\\d*\\.\\d+)$')\n\n        if branchset.uncertainty_type == 'sourceModel':\n            try:\n                for fname in node.text.strip().split():\n                    self.collect_source_model_data(\n                        branchnode['branchID'], fname)\n            except Exception as exc:\n                raise LogicTreeError(node, self.filename, str(exc)) from exc\n\n        elif branchset.uncertainty_type == 'abGRAbsolute':\n            ab = (node.text.strip()).split()\n            if len(ab) == 2:\n                a, b = ab\n                if _float_re.match(a) and _float_re.match(b):\n                    return\n            raise LogicTreeError(\n                node, self.filename,\n                'expected a pair of floats separated by space')\n        elif branchset.uncertainty_type == 'incrementalMFDAbsolute':\n            pass\n        elif branchset.uncertainty_type == 'simpleFaultGeometryAbsolute':\n            self._validate_simple_fault_geometry(node.simpleFaultGeometry,\n                                                 _float_re)\n        elif branchset.uncertainty_type == 'complexFaultGeometryAbsolute':\n            self._validate_complex_fault_geometry(node.complexFaultGeometry,\n                                                  _float_re)\n        elif branchset.uncertainty_type ==\\\n                'characteristicFaultGeometryAbsolute':\n            for geom_node in node.surface:\n                if \"simpleFaultGeometry\" in geom_node.tag:\n                    self._validate_simple_fault_geometry(geom_node, _float_re)\n                elif \"complexFaultGeometry\" in geom_node.tag:\n                    self._validate_complex_fault_geometry(geom_node, _float_re)\n                elif \"planarSurface\" in geom_node.tag:\n                    self._validate_planar_fault_geometry(geom_node, _float_re)\n                else:\n                    raise LogicTreeError(\n                        geom_node, self.filename,\n                        \"Surface geometry type not recognised\")\n        else:\n            try:\n                float(node.text)\n            except (TypeError, ValueError):\n                raise LogicTreeError(\n                    node, self.filename, 'expected single float value')", "response": "Validate the uncertainty value of a node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_simple_fault_geometry(self, node, _float_re):\n        try:\n            # Parse the geometry\n            coords = split_coords_2d(~node.LineString.posList)\n            trace = geo.Line([geo.Point(*p) for p in coords])\n        except ValueError:\n            # If the geometry cannot be created then use the LogicTreeError\n            # to point the user to the incorrect node. Hence, if trace is\n            # compiled successfully then len(trace) is True, otherwise it is\n            # False\n            trace = []\n        if len(trace):\n            return\n        raise LogicTreeError(\n            node, self.filename,\n            \"'simpleFaultGeometry' node is not valid\")", "response": "Validate a simple fault geometry node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates a complex fault geometry node", "response": "def _validate_complex_fault_geometry(self, node, _float_re):\n        \"\"\"\n        Validates a node representation of a complex fault geometry - this\n        check merely verifies that the format is correct. If the geometry\n        does not conform to the Aki & Richards convention this will not be\n        verified here, but will raise an error when the surface is created.\n        \"\"\"\n        valid_edges = []\n        for edge_node in node.nodes:\n            try:\n                coords = split_coords_3d(edge_node.LineString.posList.text)\n                edge = geo.Line([geo.Point(*p) for p in coords])\n            except ValueError:\n                # See use of validation error in simple geometry case\n                # The node is valid if all of the edges compile correctly\n                edge = []\n            if len(edge):\n                valid_edges.append(True)\n            else:\n                valid_edges.append(False)\n        if node[\"spacing\"] and all(valid_edges):\n            return\n        raise LogicTreeError(\n            node, self.filename,\n            \"'complexFaultGeometry' node is not valid\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_planar_fault_geometry(self, node, _float_re):\n        valid_spacing = node[\"spacing\"]\n        for key in [\"topLeft\", \"topRight\", \"bottomLeft\", \"bottomRight\"]:\n            lon = getattr(node, key)[\"lon\"]\n            lat = getattr(node, key)[\"lat\"]\n            depth = getattr(node, key)[\"depth\"]\n            valid_lon = (lon >= -180.0) and (lon <= 180.0)\n            valid_lat = (lat >= -90.0) and (lat <= 90.0)\n            valid_depth = (depth >= 0.0)\n            is_valid = valid_lon and valid_lat and valid_depth\n            if not is_valid or not valid_spacing:\n                raise LogicTreeError(\n                    node, self.filename,\n                    \"'planarFaultGeometry' node is not valid\")", "response": "Validates a planar fault geometry node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_filters(self, branchset_node, uncertainty_type, filters):\n        if 'applyToSources' in filters:\n            filters['applyToSources'] = filters['applyToSources'].split()\n        return filters", "response": "Parse the filters for the branchset node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_filters(self, branchset_node, uncertainty_type, filters):\n        if uncertainty_type == 'sourceModel' and filters:\n            raise LogicTreeError(\n                branchset_node, self.filename,\n                'filters are not allowed on source model uncertainty')\n\n        if len(filters) > 1:\n            raise LogicTreeError(\n                branchset_node, self.filename,\n                \"only one filter is allowed per branchset\")\n\n        if 'applyToTectonicRegionType' in filters:\n            if not filters['applyToTectonicRegionType'] \\\n                    in self.tectonic_region_types:\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    \"source models don't define sources of tectonic region \"\n                    \"type '%s'\" % filters['applyToTectonicRegionType'])\n\n        if uncertainty_type in ('abGRAbsolute', 'maxMagGRAbsolute',\n                                'simpleFaultGeometryAbsolute',\n                                'complexFaultGeometryAbsolute'):\n            if not filters or not list(filters) == ['applyToSources'] \\\n                    or not len(filters['applyToSources'].split()) == 1:\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    \"uncertainty of type '%s' must define 'applyToSources' \"\n                    \"with only one source id\" % uncertainty_type)\n        if uncertainty_type in ('simpleFaultDipRelative',\n                                'simpleFaultDipAbsolute'):\n            if not filters or (not ('applyToSources' in filters.keys()) and not\n                               ('applyToSourceType' in filters.keys())):\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    \"uncertainty of type '%s' must define either\"\n                    \"'applyToSources' or 'applyToSourceType'\"\n                    % uncertainty_type)\n\n        if 'applyToSourceType' in filters:\n            if not filters['applyToSourceType'] in self.source_types:\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    \"source models don't define sources of type '%s'\" %\n                    filters['applyToSourceType'])\n\n        if 'applyToSources' in filters:\n            for source_id in filters['applyToSources'].split():\n                for source_ids in self.source_ids.values():\n                    if source_id not in source_ids:\n                        raise LogicTreeError(\n                            branchset_node, self.filename,\n                            \"source with id '%s' is not defined in source \"\n                            \"models\" % source_id)", "response": "Validate the filters for the given uncertainty type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the branchset_node is valid and that the branchset_node is not already in the hierarchy of branchsets.", "response": "def validate_branchset(self, branchset_node, depth, number, branchset):\n        \"\"\"\n        See superclass' method for description and signature specification.\n\n        Checks that the following conditions are met:\n\n        * First branching level must contain exactly one branchset, which\n          must be of type \"sourceModel\".\n        * All other branchsets must not be of type \"sourceModel\"\n          or \"gmpeModel\".\n        \"\"\"\n        if depth == 0:\n            if number > 0:\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    'there must be only one branch set '\n                    'on first branching level')\n            elif branchset.uncertainty_type != 'sourceModel':\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    'first branchset must define an uncertainty '\n                    'of type \"sourceModel\"')\n        else:\n            if branchset.uncertainty_type == 'sourceModel':\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    'uncertainty of type \"sourceModel\" can be defined '\n                    'on first branchset only')\n            elif branchset.uncertainty_type == 'gmpeModel':\n                raise LogicTreeError(\n                    branchset_node, self.filename,\n                    'uncertainty of type \"gmpeModel\" is not allowed '\n                    'in source model logic tree')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_branchset(self, branchset_node, branchset):\n        apply_to_branches = branchset_node.attrib.get('applyToBranches')\n        if apply_to_branches:\n            apply_to_branches = apply_to_branches.split()\n            for branch_id in apply_to_branches:\n                if branch_id not in self.branches:\n                    raise LogicTreeError(\n                        branchset_node, self.filename,\n                        \"branch '%s' is not yet defined\" % branch_id)\n                branch = self.branches[branch_id]\n                if branch.child_branchset is not None:\n                    raise LogicTreeError(\n                        branchset_node, self.filename,\n                        \"branch '%s' already has child branchset\" % branch_id)\n                if branch not in self.open_ends:\n                    raise LogicTreeError(\n                        branchset_node, self.filename,\n                        'applyToBranches must reference only branches '\n                        'from previous branching level')\n                branch.child_branchset = branchset\n        else:\n            for branch in self.open_ends:\n                branch.child_branchset = branchset", "response": "Applies branchset to branchset_node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the source model file and collect information about source types tectonic region types and source ids.", "response": "def collect_source_model_data(self, branch_id, source_model):\n        \"\"\"\n        Parse source model file and collect information about source ids,\n        source types and tectonic region types available in it. That\n        information is used then for :meth:`validate_filters` and\n        :meth:`validate_uncertainty_value`.\n        \"\"\"\n        # using regular expressions is a lot faster than using the\n        with self._get_source_model(source_model) as sm:\n            xml = sm.read()\n        self.tectonic_region_types.update(TRT_REGEX.findall(xml))\n        self.source_ids[branch_id].extend(ID_REGEX.findall(xml))\n        self.source_types.update(SOURCE_TYPE_REGEX.findall(xml))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the path through the source model logic tree and return the original source_group with modified sources.", "response": "def apply_uncertainties(self, branch_ids, source_group):\n        \"\"\"\n        Parse the path through the source model logic tree and return\n        \"apply uncertainties\" function.\n\n        :param branch_ids:\n            List of string identifiers of branches, representing the path\n            through source model logic tree.\n        :param source_group:\n            A group of sources\n        :return:\n            A copy of the original group with modified sources\n        \"\"\"\n        branchset = self.root_branchset\n        branchsets_and_uncertainties = []\n        branch_ids = list(branch_ids[::-1])\n\n        while branchset is not None:\n            branch = branchset.get_branch_by_id(branch_ids.pop(-1))\n            if not branchset.uncertainty_type == 'sourceModel':\n                branchsets_and_uncertainties.append((branchset, branch.value))\n            branchset = branch.child_branchset\n\n        if not branchsets_and_uncertainties:\n            return source_group  # nothing changed\n\n        sg = copy.deepcopy(source_group)\n        sg.applied_uncertainties = []\n        sg.changed = numpy.zeros(len(sg.sources), int)\n        for branchset, value in branchsets_and_uncertainties:\n            for s, source in enumerate(sg.sources):\n                changed = branchset.apply_uncertainty(value, source)\n                if changed:\n                    sg.changed[s] += changed\n                    sg.applied_uncertainties.append(\n                        (branchset.uncertainty_type, value))\n        return sg"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_one(self):\n        return all(abs(v - 1.) < pmf.PRECISION for v in self.dic.values())", "response": "Check that all the inner weights are 1 up to the precision\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a trivial GsimLogicTree from a single GSIM instance.", "response": "def from_(cls, gsim):\n        \"\"\"\n        Generate a trivial GsimLogicTree from a single GSIM instance.\n        \"\"\"\n        ltbranch = N('logicTreeBranch', {'branchID': 'b1'},\n                     nodes=[N('uncertaintyModel', text=str(gsim)),\n                            N('uncertaintyWeight', text='1.0')])\n        lt = N('logicTree', {'logicTreeID': 'lt1'},\n               nodes=[N('logicTreeBranchingLevel', {'branchingLevelID': 'bl1'},\n                        nodes=[N('logicTreeBranchSet',\n                                 {'applyToTectonicRegionType': '*',\n                                  'branchSetID': 'bs1',\n                                  'uncertaintyType': 'gmpeModel'},\n                                 nodes=[ltbranch])])])\n        return cls(repr(gsim), ['*'], ltnode=lt)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_imts(self, imts):\n        for trt in self.values:\n            for gsim in self.values[trt]:\n                for attr in dir(gsim):\n                    coeffs = getattr(gsim, attr)\n                    if not isinstance(coeffs, CoeffsTable):\n                        continue\n                    for imt in imts:\n                        if imt.startswith('SA'):\n                            try:\n                                coeffs[from_string(imt)]\n                            except KeyError:\n                                raise ValueError(\n                                    '%s is out of the period range defined '\n                                    'for %s' % (imt, gsim))", "response": "Check that the IMTs are recognized by all GSIMs in the logic tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing the GsimLogicTree instance by taking a subset of tectonic region types.", "response": "def reduce(self, trts):\n        \"\"\"\n        Reduce the GsimLogicTree.\n\n        :param trts: a subset of tectonic region types\n        :returns: a reduced GsimLogicTree instance\n        \"\"\"\n        new = object.__new__(self.__class__)\n        vars(new).update(vars(self))\n        if trts != {'*'}:\n            new.branches = []\n            for br in self.branches:\n                branch = BranchTuple(br.trt, br.id, br.gsim, br.weight,\n                                     br.trt in trts)\n                new.branches.append(branch)\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of effective branches for tectonic region type and tectonic region class as a dictionary.", "response": "def get_num_branches(self):\n        \"\"\"\n        Return the number of effective branches for tectonic region type,\n        as a dictionary.\n        \"\"\"\n        num = {}\n        for trt, branches in itertools.groupby(\n                self.branches, operator.attrgetter('trt')):\n            num[trt] = sum(1 for br in branches if br.effective)\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_num_paths(self):\n        # NB: the algorithm assume a symmetric logic tree for the GSIMs;\n        # in the future we may relax such assumption\n        num_branches = self.get_num_branches()\n        if not sum(num_branches.values()):\n            return 0\n        num = 1\n        for val in num_branches.values():\n            if val:  # the branch is effective\n                num *= val\n        return num", "response": "Return the effective number of paths in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_gsims(self, trt):\n        if trt == '*' or trt == b'*':  # fake logictree\n            [trt] = self.values\n        return sorted(self.values[trt])", "response": "returns a list of available GSIMs for a given tectonic region"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes faulting style adjustment term.", "response": "def _compute_faulting_style_term(Frss, pR, Fnss, pN, rake):\n    \"\"\"\n    Compute SHARE faulting style adjustment term.\n    \"\"\"\n    if rake > 30.0 and rake <= 150.0:\n        return np.power(Frss, 1 - pR) * np.power(Fnss, -pN)\n    elif rake > -120.0 and rake <= -60.0:\n        return np.power(Frss, - pR) * np.power(Fnss, 1 - pN)\n    else:\n        return np.power(Frss, - pR) * np.power(Fnss, - pN)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_mean(self, C, mag, rrup):\n        mean = (C['c1'] +\n                self._compute_term1(C, mag) +\n                self._compute_term2(C, mag, rrup) +\n                self._compute_term3(C, rrup))\n        return mean", "response": "Compute mean value according to equation 30 page 1021."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stddevs(self, C, stddev_types, mag, num_sites):\n        stddevs = []\n        for _ in stddev_types:\n            if mag < 7.16:\n                sigma = C['c11'] + C['c12'] * mag\n            elif mag >= 7.16:\n                sigma = C['c13']\n            stddevs.append(np.zeros(num_sites) + sigma)\n\n        return stddevs", "response": "Returns the standard deviation as for equation 35 page 1021."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_term2(self, C, mag, rrup):\n        c78_factor = (C['c7'] * np.exp(C['c8'] * mag)) ** 2\n        R = np.sqrt(rrup ** 2 + c78_factor)\n\n        return C['c4'] * np.log(R) + (C['c5'] + C['c6'] * mag) * rrup", "response": "This computes the term f2 in equation 32 page 1021"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_term3(self, C, rrup):\n        f3 = np.zeros_like(rrup)\n\n        idx_between_70_130 = (rrup > 70) & (rrup <= 130)\n        idx_greater_130 = rrup > 130\n\n        f3[idx_between_70_130] = (\n            C['c9'] * (np.log(rrup[idx_between_70_130]) - np.log(70))\n        )\n\n        f3[idx_greater_130] = (\n            C['c9'] * (np.log(rrup[idx_greater_130]) - np.log(70)) +\n            C['c10'] * (np.log(rrup[idx_greater_130]) - np.log(130))\n        )\n\n        return f3", "response": "This computes the term f3 in equation 34 page 1021 but corrected\n        according to the erratum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extract faulting style  and rock adjustment coefficients for the\n        # given imt\n        C_ADJ = self.COEFFS_FS_ROCK[imt]\n\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        # apply faulting style and rock adjustment factor for mean and std\n        mean = np.log(np.exp(mean) *\n                      _compute_faulting_style_term(C_ADJ['Frss'],\n                                                   self.CONSTS_FS['pR'],\n                                                   self.CONSTS_FS['Fnss'],\n                                                   self.CONSTS_FS['pN'],\n                                                   rup.rake) * C_ADJ['AFrock'])\n        stddevs = np.array(stddevs)\n\n        return mean, stddevs", "response": "This method is used to compute the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_mean(self, C, mag, rrup):\n        mean = np.zeros_like(rrup)\n\n        mean += C['c1'] + C['c2'] * mag + C['c3'] * (8.5 - mag) ** 2\n\n        idx = rrup > 70.\n        mean[idx] += C['c7'] * (np.log(rrup[idx]) - np.log(70.))\n\n        idx = rrup > 130.\n        mean[idx] += C['c8'] * (np.log(rrup[idx]) - np.log(130.))\n\n        R = np.sqrt(\n            rrup ** 2 + (C['c5'] * np.exp(C['c6'] * mag)) ** 2\n        )\n        mean += C['c4'] * np.log(R) + (C['c9'] + C['c10'] * mag) * rrup\n\n        return mean", "response": "Compute mean value in equation 30 in USGS report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef time_window_cutoff(sw_time, time_cutoff):\n    sw_time = np.array(\n        [(time_cutoff / DAYS) if x > (time_cutoff / DAYS)\n            else x for x in sw_time])\n    return(sw_time)", "response": "Allows for cutting the declustering time window at a specific time"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_geometry(self, input_geometry, upper_depth, lower_depth):\n        '''\n        If geometry is defined as a numpy array then create instance of\n        nhlib.geo.point.Point class, otherwise if already instance of class\n        accept class\n\n        :param input_geometry:\n            Input geometry (point) as either\n            i) instance of nhlib.geo.point.Point class\n            ii) numpy.ndarray [Longitude, Latitude]\n\n        :param float upper_depth:\n            Upper seismogenic depth (km)\n\n        :param float lower_depth:\n            Lower seismogenic depth (km)\n        '''\n        self._check_seismogenic_depths(upper_depth, lower_depth)\n\n        # Check/create the geometry class\n        if not isinstance(input_geometry, Point):\n            if not isinstance(input_geometry, np.ndarray):\n                raise ValueError('Unrecognised or unsupported geometry '\n                                 'definition')\n            self.geometry = Point(input_geometry[0], input_geometry[1])\n        else:\n            self.geometry = input_geometry", "response": "Create the geometry of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the seismic depths for physical consistency.", "response": "def _check_seismogenic_depths(self, upper_depth, lower_depth):\n        '''\n        Checks the seismic depths for physical consistency\n        :param float upper_depth:\n            Upper seismogenic depth (km)\n        :param float lower_depth:\n            Lower seismogenis depth (km)\n        '''\n        # Simple check on depths\n        if upper_depth:\n            if upper_depth < 0.:\n                raise ValueError('Upper seismogenic depth must be greater than'\n                                 ' or equal to 0.0!')\n            else:\n                self.upper_depth = upper_depth\n        else:\n            self.upper_depth = 0.0\n\n        if lower_depth:\n            if lower_depth < self.upper_depth:\n                raise ValueError('Lower seismogenic depth must take a greater'\n                                 ' value than upper seismogenic depth')\n            else:\n                self.lower_depth = lower_depth\n        else:\n            self.lower_depth = np.inf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting the catalogue associated to a point source.", "response": "def select_catalogue(self, selector, distance, selector_type='circle',\n                         distance_metric='epicentral', point_depth=None,\n                         upper_eq_depth=None, lower_eq_depth=None):\n        '''\n        Selects the catalogue associated to the point source.\n        Effectively a wrapper to the two functions select catalogue within\n        a distance of the point and select catalogue within cell centred on\n        point\n\n        :param selector:\n            Populated instance of :class:\n            `openquake.hmtk.seismicity.selector.CatalogueSelector`\n        :param float distance:\n            Distance from point (km) for selection\n        :param str selector_type:\n            Chooses whether to select within {'circle'} or within a {'square'}.\n        :param str distance_metric:\n            'epicentral' or 'hypocentral' (only for 'circle' selector type)\n        :param float point_depth:\n            Assumed hypocentral depth of the point (only applied to 'circle'\n            distance type)\n        :param float upper_depth:\n            Upper seismogenic depth (km) (only for 'square')\n        :param float lower_depth:\n            Lower seismogenic depth (km) (only for 'square')\n        '''\n\n        if selector.catalogue.get_number_events() < 1:\n            raise ValueError('No events found in catalogue!')\n\n        if 'square' in selector_type:\n            # Calls select catalogue within cell function\n            self.select_catalogue_within_cell(selector,\n                                              distance,\n                                              upper_depth=upper_eq_depth,\n                                              lower_depth=lower_eq_depth)\n\n        elif 'circle' in selector_type:\n            # Calls select catalogue within distance function\n            self.select_catalogue_within_distance(selector, distance,\n                                                  distance_metric, point_depth)\n\n        else:\n            raise ValueError('Unrecognised selection type for point source!')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_catalogue_within_distance(\n            self, selector, distance,\n            distance_metric='epicentral', point_depth=None):\n        '''\n        Selects catalogue of earthquakes within distance from point\n\n        :param selector:\n            Populated instance of :class:\n            `openquake.hmtk.seismicity.selector.CatalogueSelector`\n        :param distance:\n            Distance from point (km) for selection\n        :param str distance_metric:\n            Choice of point source distance metric 'epicentral' or\n            'hypocentral'\n        '''\n        if ('hypocentral' in distance_metric) and point_depth:\n            # If a hypocentral distance metric is chosen and a\n            # hypocentral depth specified then update geometry\n            self.geometry = Point(self.geometry.longitude,\n                                  self.geometry.latitude,\n                                  point_depth)\n\n        self.catalogue = selector.circular_distance_from_point(\n            self.geometry,\n            distance,\n            distance_type=distance_metric)\n\n        if self.catalogue.get_number_events() < 5:\n            # Throw a warning regarding the small number of earthquakes in\n            # the source!\n            warnings.warn('Source %s (%s) has fewer than 5 events'\n                          % (self.id, self.name))", "response": "Selects the catalogue of earthquakes within a given distance from a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_catalogue_within_cell(self, selector, distance,\n                                     upper_depth=None, lower_depth=None):\n        '''\n        Selects catalogue of earthquakes within distance from point\n\n        :param selector:\n            Populated instance of :class:\n            `openquake.hmtk.seismicity.selector.CatalogueSelector`\n        :param distance:\n            Distance from point (km) for selection\n        '''\n\n        self.catalogue = selector.cartesian_square_centred_on_point(\n            self.geometry, distance)\n\n        if self.catalogue.get_number_events() < 5:\n            # Throw a warning regarding the small number of earthquakes in\n            # the source!\n            warnings.warn('Source %s (%s) has fewer than 5 events'\n                          % (self.id, self.name))", "response": "Selects catalogue of earthquakes within a given distance from point."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsees :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for specification of input and result values. Implements the following equations: Equation (5) on p. 881 predicts ground motion for shallow events (depth <= 30 km): ``log(pre) = a*M + b*X - log(X + d*10^(e*M)) + c + epsilon`` \"where pre is the predicted PGA (cm/sec^2), PGV (cm/sec), or 5% damped response spectral acceleration (cm/sec^2)\" (p. 883) and a, b, c and d are tabulated regression coefficients. Note that subscripts on the regression coeffients have been dropped - subscript `1` denoted \"shallow\" while subscript `2` denoted \"deep\" - so that the \"deep\" model of equation (6) can be implemented trivally by changing coefficients and setting d = 0. Equation (8) on p. 883 gives the model used for site amplitfication: ``G = p*log(VS30) + q`` Where p and q are tabulated regression coefficients. Equation (9) on p. 884 for the ground motion at a given site: ``log(pre_G) = log(pre) + G`` No adjustment of epsilon is made as a function of VS30. Note finally that \"log represents log_10 in the present study\" (p. 880).", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for specification of input and result values.\n\n        Implements the following equations:\n\n        Equation (5) on p. 881 predicts ground motion for shallow events\n        (depth <= 30 km):\n\n        ``log(pre) = a*M + b*X - log(X + d*10^(e*M)) + c + epsilon``\n\n        \"where pre is the predicted PGA (cm/sec^2), PGV (cm/sec), or 5%\n        damped response spectral acceleration (cm/sec^2)\" (p. 883) and a,\n        b, c and d are tabulated regression coefficients. Note that\n        subscripts on the regression coeffients have been dropped -\n        subscript `1` denoted \"shallow\" while subscript `2` denoted \"deep\"\n        - so that the \"deep\" model of equation (6) can be implemented trivally\n        by changing coefficients and setting d = 0.\n\n        Equation (8) on p. 883 gives the model used for site amplitfication:\n\n        ``G = p*log(VS30) + q``\n\n        Where p and q are tabulated regression coefficients.\n\n        Equation (9) on p. 884 for the ground motion at a given site:\n\n        ``log(pre_G) = log(pre) + G``\n\n        No adjustment of epsilon is made as a function of VS30.\n\n        Note finally that \"log represents log_10 in the present study\"\n        (p. 880).\n\n        \"\"\"\n\n        # obtain coefficients for required intensity measure type (IMT)\n        coeffs = self.COEFFS_BASE[imt].copy()\n        coeffs.update(self.COEFFS_SITE[imt])\n\n        # obtain IMT-independent coefficients\n        coeffs.update(self.CONSTS)\n\n        # compute bedrock motion, equation (5)\n        log_mean = self._compute_mag_dist_terms(rup, dists, coeffs)\n\n        # make site corrections, equation (9)\n        log_mean += self._compute_site_amplification(sites, coeffs)\n\n        # retrieve standard deviations\n        log_stddevs = self._get_stddevs(coeffs, sites.vs30.size, stddev_types)\n\n        # convert from common to natural logarithm\n        ln_mean = log_mean*LOG10\n        ln_stddevs = np.array(log_stddevs)*LOG10\n\n        # convert accelerations from cm/s^2 to g\n        if not imt.name == \"PGV\":\n            ln_mean -= np.log(100*g)\n\n        return ln_mean, ln_stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the magnitude and distance terms of the log - pre - magnitude of the record.", "response": "def _compute_mag_dist_terms(cls, rup, dists, coeffs):\n        \"\"\"\n        Compute equation (5) and implcitly equation (6):\n\n        ``log(pre) = c + a*M + b*X - log(X + d*10^(e*M)) + epsilon``\n        \"\"\"\n\n        log_pre = coeffs['c'] + coeffs['a']*rup.mag + coeffs['b']*dists.rrup \\\n            - np.log10(dists.rrup + coeffs['d']*10**(coeffs['e']*rup.mag))\n\n        return log_pre"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the site amplification of a set of sites.", "response": "def _compute_site_amplification(cls, sites, coeffs):\n        \"\"\"\n        Compute equation (8):\n\n        ``G = p*log(VS30) + q``\n        \"\"\"\n\n        return coeffs['p']*np.log10(sites.vs30) + coeffs['q']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the magnitude term of the resource table entry in equation 1", "response": "def _compute_magnitude(self, rup, C):\n        \"\"\"\n        Compute the third term of the equation 1:\n\n        e1 + b1 * (M-Mh) + b2 * (M-Mh)**2 for M<=Mh\n        e1 + b3 * (M-Mh) otherwise\n        \"\"\"\n        m_h = 6.75\n        b_3 = 0.0\n        if rup.mag <= m_h:\n            return C[\"e1\"] + (C['b1'] * (rup.mag - m_h)) +\\\n                (C['b2'] * (rup.mag - m_h) ** 2)\n        else:\n            return C[\"e1\"] + (b_3 * (rup.mag - m_h))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the site amplification term for the given set of sites.", "response": "def _get_site_amplification(self, sites, C):\n        \"\"\"\n        Compute the fourth term of the equation 1 described on paragraph :\n        The functional form Fs in Eq. (1) represents the site amplification and\n        it is given by FS = sj Cj , for j = 1,...,5, where sj are the\n        coefficients to be determined through the regression analysis,\n        while Cj are dummy variables used to denote the five different EC8\n        site classes\n        \"\"\"\n        ssa, ssb, ssc, ssd, sse = self._get_site_type_dummy_variables(sites)\n\n        return (C['sA'] * ssa) + (C['sB'] * ssb) + (C['sC'] * ssc) + \\\n            (C['sD'] * ssd) + (C['sE'] * sse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_site_type_dummy_variables(self, sites):\n        ssa = np.zeros(len(sites.vs30))\n        ssb = np.zeros(len(sites.vs30))\n        ssc = np.zeros(len(sites.vs30))\n        ssd = np.zeros(len(sites.vs30))\n        sse = np.zeros(len(sites.vs30))\n\n        # Class E Vs30 = 0 m/s. We fixed this value to define class E\n        idx = (np.fabs(sites.vs30) < 1E-10)\n        sse[idx] = 1.0\n        # Class D;  Vs30 < 180 m/s.\n        idx = (sites.vs30 >= 1E-10) & (sites.vs30 < 180.0)\n        ssd[idx] = 1.0\n        # SClass C; 180 m/s <= Vs30 <= 360 m/s.\n        idx = (sites.vs30 >= 180.0) & (sites.vs30 < 360.0)\n        ssc[idx] = 1.0\n        # Class B; 360 m/s <= Vs30 <= 800 m/s.\n        idx = (sites.vs30 >= 360.0) & (sites.vs30 < 800)\n        ssb[idx] = 1.0\n        # Class A; Vs30 > 800 m/s.\n        idx = (sites.vs30 >= 800.0)\n        ssa[idx] = 1.0\n        return ssa, ssb, ssc, ssd, sse", "response": "Get site type dummy variables for the given site set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the mechanism term in equation 1", "response": "def _get_mechanism(self, rup, C):\n        \"\"\"\n        Compute the fifth term of the equation 1 described on paragraph :\n        Get fault type dummy variables, see Table 1\n        \"\"\"\n        U, SS, NS, RS = self._get_fault_type_dummy_variables(rup)\n\n        return C['f1'] * NS + C['f2'] * RS + C['f3'] * SS"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_fault_type_dummy_variables(self, rup):\n        U, SS, NS, RS = 0, 0, 0, 0\n        if np.abs(rup.rake) <= 30.0 or (180.0 - np.abs(rup.rake)) <= 30.0:\n            # strike-slip\n            SS = 1\n        elif rup.rake > 30.0 and rup.rake < 150.0:\n            # reverse\n            RS = 1\n        else:\n            # normal\n            NS = 1\n        return U, SS, NS, RS", "response": "Returns dummy variables for fault types which are not defined in the spec."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        mean, stddevs = super().get_mean_and_stddevs(sites, rup, dists, imt,\n                                                     stddev_types)\n        delta = self._get_delta(imt, rup.mag)\n        return mean-delta, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the figure for the H curves.", "response": "def make_figure_hcurves(extractors, what):\n    \"\"\"\n    $ oq plot 'hcurves?kind=mean&imt=PGA&site_id=0'\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    got = {}  # (calc_id, kind) -> curves\n    for i, ex in enumerate(extractors):\n        hcurves = ex.get(what)\n        for kind in hcurves.kind:\n            got[ex.calc_id, kind] = hcurves[kind]\n    oq = ex.oqparam\n    n_imts = len(hcurves.imt)\n    [site] = hcurves.site_id\n    for j, imt in enumerate(hcurves.imt):\n        imls = oq.imtls[imt]\n        imt_slice = oq.imtls(imt)\n        ax = fig.add_subplot(n_imts, 1, j + 1)\n        ax.set_xlabel('%s, site %s, inv_time=%dy' %\n                      (imt, site, oq.investigation_time))\n        ax.set_ylabel('PoE')\n        for ck, arr in got.items():\n            if (arr == 0).all():\n                logging.warning('There is a zero curve %s_%s', *ck)\n            ax.loglog(imls, arr[0, imt_slice], '-', label='%s_%s' % ck)\n            ax.loglog(imls, arr[0, imt_slice], '.')\n        ax.grid(True)\n        ax.legend()\n    return plt"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the figure for the HMaps of the given extractors.", "response": "def make_figure_hmaps(extractors, what):\n    \"\"\"\n    $ oq plot 'hmaps?kind=mean&imt=PGA'\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ncalcs = len(extractors)\n    for i, ex in enumerate(extractors):\n        oq = ex.oqparam\n        n_poes = len(oq.poes)\n        sitecol = ex.get('sitecol')\n        hmaps = ex.get(what)\n        [imt] = hmaps.imt\n        [kind] = hmaps.kind\n        for j, poe in enumerate(oq.poes):\n            ax = fig.add_subplot(n_poes, ncalcs, j * ncalcs + i + 1)\n            ax.grid(True)\n            ax.set_xlabel('hmap for IMT=%s, kind=%s, poe=%s\\ncalculation %d, '\n                          'inv_time=%dy' %\n                          (imt, kind, poe, ex.calc_id, oq.investigation_time))\n            bmap = basemap('cyl', sitecol)\n            bmap.scatter(sitecol['lon'], sitecol['lat'],\n                         c=hmaps[kind][:, 0, j], cmap='jet')\n    return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_figure_uhs(extractors, what):\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    got = {}  # (calc_id, kind) -> curves\n    for i, ex in enumerate(extractors):\n        uhs = ex.get(what)\n        for kind in uhs.kind:\n            got[ex.calc_id, kind] = uhs[kind]\n    oq = ex.oqparam\n    n_poes = len(oq.poes)\n    periods = [imt.period for imt in oq.imt_periods()]\n    [site] = uhs.site_id\n    for j, poe in enumerate(oq.poes):\n        ax = fig.add_subplot(n_poes, 1, j + 1)\n        ax.set_xlabel('UHS on site %s, poe=%s, inv_time=%dy' %\n                      (site, poe, oq.investigation_time))\n        ax.set_ylabel('SA')\n        for ck, arr in got.items():\n            ax.plot(periods, arr[0, :, j], '-', label='%s_%s' % ck)\n            ax.plot(periods, arr[0, :, j], '.')\n        ax.grid(True)\n        ax.legend()\n    return plt", "response": "Make a figure with the UHS of the given extractors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a figure with the geometry of a given sources", "response": "def make_figure_source_geom(extractors, what):\n    \"\"\"\n    Extract the geometry of a given sources\n    Example:\n    http://127.0.0.1:8800/v1/calc/30/extract/source_geom/1,2,3\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    [ex] = extractors\n    sitecol = ex.get('sitecol')\n    geom_by_src = vars(ex.get(what))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.grid(True)\n    ax.set_xlabel('Source')\n    bmap = basemap('cyl', sitecol)\n    for src, geom in geom_by_src.items():\n        if src != 'array':\n            bmap.plot(geom['lon'], geom['lat'], label=src)\n    bmap.plot(sitecol['lon'], sitecol['lat'], 'x')\n    ax.legend()\n    return plt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(what, calc_id=-1, other_id=None, webapi=False):\n    if '?' not in what:\n        raise SystemExit('Missing ? in %r' % what)\n    prefix, rest = what.split('?', 1)\n    assert prefix in 'source_geom hcurves hmaps uhs', prefix\n    if prefix in 'hcurves hmaps' and 'imt=' not in rest:\n        raise SystemExit('Missing imt= in %r' % what)\n    elif prefix == 'uhs' and 'imt=' in rest:\n        raise SystemExit('Invalid IMT in %r' % what)\n    elif prefix in 'hcurves uhs' and 'site_id=' not in rest:\n        what += '&site_id=0'\n    if webapi:\n        xs = [WebExtractor(calc_id)]\n        if other_id:\n            xs.append(WebExtractor(other_id))\n    else:\n        xs = [Extractor(calc_id)]\n        if other_id:\n            xs.append(Extractor(other_id))\n    make_figure = globals()['make_figure_' + prefix]\n    plt = make_figure(xs, what)\n    plt.show()", "response": "Generic plotter for local and remote calculations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute distance scaling term in equation 1 page 74.", "response": "def _compute_distance_scaling(self, C, rrup, mag):\n        \"\"\"\n        Returns the distance scaling term\n        \"\"\"\n        rscale1 = rrup + C[\"c2\"] * (10.0 ** (C[\"c3\"] * mag))\n        return -np.log10(rscale1) - (C[\"c4\"] * rrup)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute and return the mean and standard deviation of the cluster table entry.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n\n        imean = (self._compute_magnitude_scaling(C, rup.mag) +\n                 self._compute_distance_scaling(C, dists.rrup, rup.mag))\n        # Original GMPE returns log10 acceleration in cm/s/s\n        # Converts to natural logarithm of g\n        mean = np.log((10.0 ** (imean - 2.0)) / g)\n        mean = self._compute_site_scaling(sites.vs30, mean)\n        istddevs = self._compute_stddevs(\n            C, dists.rrup.shape, stddev_types\n        )\n        # Convert from common logarithm to natural logarithm\n        stddevs = np.log(10 ** np.array(istddevs))\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_site_scaling(self, vs30, mean):\n        site_factor = np.ones(len(vs30), dtype=float)\n        idx = vs30 <= 360.\n        site_factor[idx] = 1.4\n        idx = vs30 > 760.0\n        site_factor[idx] = 0.6\n        return np.log(np.exp(mean) * site_factor)", "response": "Compute the site scaling factor for NEHRP class D and E sites."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for specification of input and result values. Implements equation (11) on p. 484: ``ln(P) = c1 + c2*M + c3*(10 - M)^3 + c4*ln(R + c5*exp(c6*M)``", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for specification of input and result values.\n\n        Implements equation (11) on p. 484:\n\n        ``ln(P) = c1 + c2*M + c3*(10 - M)^3 + c4*ln(R + c5*exp(c6*M)``\n        \"\"\"\n\n        # obtain coefficients for required intensity measure type (IMT)\n        coeffs = self.COEFFS_BEDROCK[imt].copy()\n\n        # obtain IMT-independent coefficients\n        coeffs.update(self.CONSTS)\n\n        # compute bedrock motion, equation (11)\n        ln_mean = self._compute_mean(rup, dists, coeffs)\n\n        # obtain standard deviation\n        ln_stddev = self._get_stddevs(coeffs, stddev_types)\n\n        return ln_mean, [ln_stddev]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_mean(self, rup, dists, coeffs):\n\n        ln_p = coeffs['c1'] + coeffs['c2']*rup.mag + \\\n            coeffs['c3']*(self.CONSTS['ref_mag'] - rup.mag)**3 +\\\n            coeffs['c4']*np.log(dists.rrup +\n                                coeffs['c5']*np.exp(coeffs['c6']*rup.mag))\n\n        return ln_p", "response": "Compute the mean value of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for specification of input and result values. Implements the correction factor for the upper crust, equation (12) on p. 484: ``P' = P x Correction_factor``", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for specification of input and result values.\n\n        Implements the correction factor for the upper crust, equation (12) on\n        p. 484:\n\n        ``P' = P x Correction_factor``\n        \"\"\"\n        ln_mean, [ln_stddev] = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        # compute site corrections, equation (9)\n        coeffs = self.COEFFS_UPPER[imt]\n        ln_mean += np.log(coeffs['correction'])\n\n        return ln_mean, [ln_stddev]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mean and standard deviation for the given resource class and resource class.", "response": "def get_mean_and_stddevs(self, sctx, rctx, dctx, imt, stddev_types):\n        \"\"\"\n        Call the get mean and stddevs of the GMPE for the respective IMT\n        \"\"\"\n        return self.kwargs[str(imt)].get_mean_and_stddevs(\n            sctx, rctx, dctx, imt, stddev_types)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the aggregate loss curves for the given calculation id", "response": "def plot_ac(calc_id):\n    \"\"\"\n    Aggregate loss curves plotter.\n    \"\"\"\n    # read the hazard data\n    dstore = util.read(calc_id)\n    agg_curve = dstore['agg_curve-rlzs']\n    plt = make_figure(agg_curve)\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes distance scaling term in equation 3", "response": "def _compute_distance_term(self, C, rrup, mag):\n        \"\"\"\n        Returns the distance scaling term\n        \"\"\"\n        exponent_term = (1.0 + C[\"c3\"] * np.exp(mag - 5.)) ** 2.\n        return C[\"c2\"] * np.log(np.sqrt(rrup ** 2. + exponent_term))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stddevs(self, C, distance, stddev_types):\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                sigma = C[\"s1\"] + (C[\"s2\"] / (1.0 +\n                                   ((distance / C[\"s3\"]) ** 2.)))\n                stddevs.append(sigma + np.zeros_like(distance))\n        return stddevs", "response": "Returns the total standard deviation as defined in equation 2 page 970."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute and return mean and standard deviation for a specific resource type.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        C = self.COEFFS[imt]\n\n        mean = (self._compute_magnitude_term(C, rup.mag) +\n                self._compute_distance_term(C, dists.rhypo, rup.mag))\n        stddevs = self._get_stddevs(C, dists.rhypo, stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_distance_term(self, C, rhypo, mag):\n        r_m = C[\"m1\"] + C[\"m2\"] * np.exp(mag - 5.)\n        f_r = C[\"c2\"] * np.log(np.sqrt(rhypo ** 2. + r_m ** 2.))\n        # For distances greater than 50 km an anelastic term is added\n        idx = rhypo > 50.0\n        f_r[idx] += C[\"c4\"] * np.log(rhypo[idx] / 50.)\n        return f_r", "response": "Compute the distance scaling term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing mean value according to equation 18 page 32.", "response": "def _compute_mean(self, C, mag, rrup):\n        \"\"\"\n        Compute mean value according to equation 18, page 32.\n        \"\"\"\n        # see table 3, page 14\n        R1 = 90.\n        R2 = 150.\n        # see equation 19, page 32\n        m_ref = mag - 4\n        r1 = R1 + C['c8'] * m_ref\n        r2 = R2 + C['c11'] * m_ref\n        assert r1 > 0\n        assert r2 > 0\n        g0 = np.log10(\n            np.sqrt(np.minimum(rrup, r1) ** 2 + (1 + C['c5'] * m_ref) ** 2)\n        )\n        g1 = np.maximum(np.log10(rrup / r1), 0)\n        g2 = np.maximum(np.log10(rrup / r2), 0)\n\n        mean = (C['c0'] + C['c1'] * m_ref + C['c2'] * m_ref ** 2 +\n                (C['c3'] + C['c4'] * m_ref) * g0 +\n                (C['c6'] + C['c7'] * m_ref) * g1 +\n                (C['c9'] + C['c10'] * m_ref) * g2)\n\n        # convert from log10 to ln and units from cm/s2 to g\n        mean = np.log((10 ** mean) * 1e-2 / g)\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_stddevs(self, C, stddev_types, num_sites):\n        # standard deviation is converted from log10 to ln\n        std_total = np.log(10 ** C['sigma'])\n        stddevs = []\n        for _ in stddev_types:\n            stddevs.append(np.zeros(num_sites) + std_total)\n\n        return stddevs", "response": "Return total standard deviation as described in paragraph 2. 1 page 1070."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract an output from the datastore and save it into an. hdf5 file.", "response": "def extract(what, calc_id, webapi=True):\n    \"\"\"\n    Extract an output from the datastore and save it into an .hdf5 file.\n    By default uses the WebAPI, otherwise the extraction is done locally.\n    \"\"\"\n    with performance.Monitor('extract', measuremem=True) as mon:\n        if webapi:\n            obj = WebExtractor(calc_id).get(what)\n        else:\n            obj = Extractor(calc_id).get(what)\n        fname = '%s_%d.hdf5' % (what.replace('/', '-').replace('?', '-'),\n                                calc_id)\n        obj.save(fname)\n        print('Saved', fname)\n    if mon.duration > 1:\n        print(mon)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_kreemer_cell(data, loc):\n    '''\n    Constructs the \"Kreemer Cell\" from the input file. The Kreemer cell is\n    simply a set of five lines describing the four nodes of the square (closed)\n    :param list data:\n        Strain data as list of text lines (input from linecache.getlines)\n    :param int loc:\n        Pointer to location in data\n    :returns:\n        temp_poly - 5 by 2 numpy array of cell longitudes and latitudes\n    '''\n\n    temp_poly = np.empty([5, 2], dtype=float)\n    for ival in range(1, 6):\n        value = data[loc + ival].rstrip('\\n')\n        value = value.lstrip(' ')\n        value = np.array((value.split(' ', 1))).astype(float)\n        temp_poly[ival - 1, :] = value.flatten()\n    return temp_poly", "response": "Builds the Kreemer Cell from the input file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the tectonic region type for every element inside the strain model and creates a new one", "response": "def get_regionalisation(self, strain_model):\n        '''\n        Gets the tectonic region type for every element inside the strain model\n\n        :paramm strain_model:\n            Input strain model as instance of\n            openquake.hmtk.strain.geodetic_strain.GeodeticStrain\n\n\n        :returns:\n            Strain model with complete regionalisation\n        '''\n        self.strain = strain_model\n        self.strain.data['region'] = np.array(\n            ['IPL'\n             for _ in range(self.strain.get_number_observations())],\n            dtype='|S13')\n        self.strain.data['area'] = np.array(\n            [np.nan\n             for _ in range(self.strain.get_number_observations())])\n\n        regional_model = self.define_kreemer_regionalisation()\n        for polygon in regional_model:\n            self._point_in_tectonic_region(polygon)\n        return self.strain"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _point_in_tectonic_region(self, polygon):\n        '''\n        Returns the region type and area according to the tectonic\n        region\n        :param polygon: Dictionary containing the following attributes -\n            'long_lims' - Longitude limits (West, East)\n            'lat_lims' - Latitude limits (South, North)\n            'region_type' - Tectonic region type (str)\n            'area' - Area of cell in m ^ 2\n        '''\n\n        marker = np.zeros(self.strain.get_number_observations(), dtype=bool)\n        idlong = np.logical_and(\n            self.strain.data['longitude'] >= polygon['long_lims'][0],\n            self.strain.data['longitude'] < polygon['long_lims'][1])\n        id0 = np.where(np.logical_and(idlong, np.logical_and(\n            self.strain.data['latitude'] >= polygon['lat_lims'][0],\n            self.strain.data['latitude'] < polygon['lat_lims'][1])))[0]\n        if len(id0) > 0:\n            marker[id0] = True\n            for iloc in id0:\n                self.strain.data['region'][iloc] = \\\n                    polygon['region_type']\n                self.strain.data['area'][iloc] = polygon['area']\n        marker = np.logical_not(marker)\n        return marker", "response": "Returns the region type and area according to the tectonic region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef define_kreemer_regionalisation(self, north=90., south=-90., east=180.,\n                                       west=-180.):\n        '''\n        Applies the regionalisation defined according to the regionalisation\n        typology of Corne Kreemer\n        '''\n        '''Applies the regionalisation of Kreemer (2003)\n        :param input_file:\n            Filename (str) of input file contraining Kreemer regionalisation\n        :param north:\n            Northern limit (decimal degrees)for consideration (float)\n        :param south:\n            Southern limit (decimal degrees)for consideration (float)\n        :param east:\n            Eastern limit (decimal degrees)for consideration (float)\n        :param west:\n            Western limit (decimal degrees)for consideration (float)\n        :returns: List of polygons corresonding to the Kreemer cells.\n        '''\n        input_data = getlines(self.filename)\n        kreemer_polygons = []\n\n        for line_loc, line in enumerate(input_data):\n            if '>' in line[0]:\n                polygon_dict = {}\n                # Get region type (char) and area (m ^ 2) from header\n                primary_data = line[2:].rstrip('\\n')\n                primary_data = primary_data.split(' ', 1)\n                polygon_dict['region_type'] = primary_data[0].strip(' ')\n                polygon_dict['area'] = float(primary_data[1].strip(' '))\n                polygon_dict['cell'] = _build_kreemer_cell(input_data,\n                                                           line_loc)\n                polygon_dict['long_lims'] = np.array([\n                    np.min(polygon_dict['cell'][:, 0]),\n                    np.max(polygon_dict['cell'][:, 0])])\n                polygon_dict['lat_lims'] = np.array([\n                    np.min(polygon_dict['cell'][:, 1]),\n                    np.max(polygon_dict['cell'][:, 1])])\n                polygon_dict['cell'] = None\n\n                if polygon_dict['long_lims'][0] >= 180.0:\n                    polygon_dict['long_lims'] = \\\n                        polygon_dict['long_lims'] - 360.0\n                valid_check = [\n                    polygon_dict['long_lims'][0] >= west,\n                    polygon_dict['long_lims'][1] <= east,\n                    polygon_dict['lat_lims'][0] >= south,\n                    polygon_dict['lat_lims'][1] <= north]\n                if all(valid_check):\n                    kreemer_polygons.append(polygon_dict)\n\n        return kreemer_polygons", "response": "This function defines the regionalisation of Kreemer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading and unzip an archive and extract the underlying fname", "response": "def urlextract(url, fname):\n    \"\"\"\n    Download and unzip an archive and extract the underlying fname\n    \"\"\"\n    with urlopen(url) as f:\n        data = io.BytesIO(f.read())\n    with zipfile.ZipFile(data) as z:\n        try:\n            return z.open(fname)\n        except KeyError:\n            # for instance the ShakeMap ci3031111 has inside a file\n            # data/verified_atlas2.0/reviewed/19920628115739/output/\n            # uncertainty.xml\n            # instead of just uncertainty.xml\n            zinfo = z.filelist[0]\n            if zinfo.filename.endswith(fname):\n                return z.open(zinfo)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading a single USGS Shakemap ID and return an array with the corresponding USGS Shakemap ID.", "response": "def download_array(shakemap_id, shakemap_url=SHAKEMAP_URL):\n    \"\"\"\n    :param shakemap_id: USGS Shakemap ID\n    :returns: an array with the shakemap\n    \"\"\"\n    url = shakemap_url.format(shakemap_id)\n    logging.info('Downloading %s', url)\n    contents = json.loads(urlopen(url).read())[\n        'properties']['products']['shakemap'][-1]['contents']\n    grid = contents.get('download/grid.xml')\n    if grid is None:\n        raise MissingLink('Could not find grid.xml link in %s' % url)\n    uncertainty = contents.get('download/uncertainty.xml.zip')\n    if uncertainty is None:\n        with urlopen(grid['url']) as f:\n            return get_shakemap_array(f)\n    else:\n        with urlopen(grid['url']) as f1, urlextract(\n                uncertainty['url'], 'uncertainty.xml') as f2:\n            return get_shakemap_array(f1, f2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sitecol_shakemap(array_or_id, imts, sitecol=None,\n                         assoc_dist=None, discard_assets=False):\n    \"\"\"\n    :param array_or_id: shakemap array or shakemap ID\n    :param imts: required IMTs as a list of strings\n    :param sitecol: SiteCollection used to reduce the shakemap\n    :param assoc_dist: association distance\n    :param discard_assets: set to zero the risk on assets with missing IMTs\n    :returns: a pair (filtered site collection, filtered shakemap)\n    \"\"\"\n    if isinstance(array_or_id, str):  # shakemap ID\n        array = download_array(array_or_id)\n    else:  # shakemap array\n        array = array_or_id\n    available_imts = set(array['val'].dtype.names)\n    missing = set(imts) - available_imts\n    if missing:\n        msg = ('The IMT %s is required but not in the available set %s, '\n               'please change the riskmodel otherwise you will have '\n               'incorrect zero losses for the associated taxonomies' %\n               (missing.pop(), ', '.join(available_imts)))\n        if discard_assets:\n            logging.error(msg)\n        else:\n            raise RuntimeError(msg)\n\n    # build a copy of the ShakeMap with only the relevant IMTs\n    dt = [(imt, F32) for imt in sorted(available_imts)]\n    dtlist = [('lon', F32), ('lat', F32), ('vs30', F32),\n              ('val', dt), ('std', dt)]\n    data = numpy.zeros(len(array), dtlist)\n    for name in ('lon',  'lat', 'vs30'):\n        data[name] = array[name]\n    for name in ('val', 'std'):\n        for im in available_imts:\n            data[name][im] = array[name][im]\n\n    if sitecol is None:  # extract the sites from the shakemap\n        return site.SiteCollection.from_shakemap(data), data\n\n    # associate the shakemap to the (filtered) site collection\n    bbox = (data['lon'].min(), data['lat'].min(),\n            data['lon'].max(), data['lat'].max())\n    indices = sitecol.within_bbox(bbox)\n    if len(indices) == 0:\n        raise RuntimeError('There are no sites within the boundind box %s'\n                           % str(bbox))\n    sites = sitecol.filtered(indices)\n    logging.info('Associating %d GMVs to %d sites', len(data), len(sites))\n    return geo.utils.assoc(data, sites, assoc_dist, 'warn')", "response": "This function takes an array of IMTs and returns a site collection and a SiteCollection object for the given site collection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spatial_correlation_array(dmatrix, imts, correl='yes',\n                              vs30clustered=True):\n    \"\"\"\n    :param dmatrix: distance matrix of shape (N, N)\n    :param imts: M intensity measure types\n    :param correl: 'yes', 'no' or 'full'\n    :param vs30clustered: flag, True by default\n    :returns: array of shape (M, N, N)\n    \"\"\"\n    assert correl in 'yes no full', correl\n    n = len(dmatrix)\n    corr = numpy.zeros((len(imts), n, n))\n    for imti, im in enumerate(imts):\n        if correl == 'no':\n            corr[imti] = numpy.eye(n)\n        if correl == 'full':\n            corr[imti] = numpy.ones((n, n))\n        elif correl == 'yes':\n            corr[imti] = correlation.jbcorrelation(dmatrix, im, vs30clustered)\n    return corr", "response": "Compute spatial correlation matrix for the given intensity measure types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a numpy array of spatial covariance matrices", "response": "def spatial_covariance_array(stddev, corrmatrices):\n    \"\"\"\n    :param stddev: array of shape (M, N)\n    :param corrmatrices: array of shape (M, N, N)\n    :returns: an array of shape (M, N, N)\n    \"\"\"\n    # this depends on sPGA, sSa03, sSa10, sSa30\n    M, N = corrmatrices.shape[:2]\n    matrices = []\n    for i, std in enumerate(stddev):\n        covmatrix = numpy.zeros((N, N))\n        for j in range(N):\n            for k in range(N):\n                covmatrix[j, k] = corrmatrices[i, j, k] * std[j] * std[k]\n        matrices.append(covmatrix)\n    return numpy.array(matrices)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the cross correlation matrix for the given intensity measure types.", "response": "def cross_correlation_matrix(imts, corr='yes'):\n    \"\"\"\n    :param imts: M intensity measure types\n    :param corr: 'yes', 'no' or 'full'\n    :returns: an array of shape (M, M)\n    \"\"\"\n    assert corr in 'yes no full', corr\n    # if there is only PGA this is a 1x1 identity matrix\n    M = len(imts)\n    cross_matrix = numpy.zeros((M, M))\n    for i, im in enumerate(imts):\n        T1 = im.period or 0.05\n\n        for j in range(M):\n            T2 = imts[j].period or 0.05\n            if i == j:\n                cross_matrix[i, j] = 1\n            else:\n                Tmax = max([T1, T2])\n                Tmin = min([T1, T2])\n                II = 1 if Tmin < 0.189 else 0\n                if corr == 'full':\n                    cross_matrix[i, j] = 0.99999\n                elif corr == 'yes':\n                    cross_matrix[i, j] = 1 - math.cos(math.pi / 2 - (\n                        0.359 + 0.163 * II * math.log(Tmin / 0.189)\n                    ) * math.log(Tmax / Tmin))\n\n    return cross_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef amplify_gmfs(imts, vs30s, gmfs):\n    n = len(vs30s)\n    out = [amplify_ground_shaking(im.period, vs30s[i], gmfs[m * n + i])\n           for m, im in enumerate(imts) for i in range(n)]\n    return numpy.array(out)", "response": "Amplify the ground shaking depending on the vs30s\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\namplifying the current site in the given period and velocity.", "response": "def amplify_ground_shaking(T, vs30, gmvs):\n    \"\"\"\n    :param T: period\n    :param vs30: velocity\n    :param gmvs: ground motion values for the current site in units of g\n    \"\"\"\n    gmvs[gmvs > MAX_GMV] = MAX_GMV  # accelerations > 5g are absurd\n    interpolator = interpolate.interp1d(\n        [0, 0.1, 0.2, 0.3, 0.4, 5],\n        [(760 / vs30)**0.35,\n         (760 / vs30)**0.35,\n         (760 / vs30)**0.25,\n         (760 / vs30)**0.10,\n         (760 / vs30)**-0.05,\n         (760 / vs30)**-0.05],\n    ) if T <= 0.3 else interpolate.interp1d(\n        [0, 0.1, 0.2, 0.3, 0.4, 5],\n        [(760 / vs30)**0.65,\n         (760 / vs30)**0.65,\n         (760 / vs30)**0.60,\n         (760 / vs30)**0.53,\n         (760 / vs30)**0.45,\n         (760 / vs30)**0.45],\n    )\n    return interpolator(gmvs) * gmvs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cholesky(spatial_cov, cross_corr):\n    M, N = spatial_cov.shape[:2]\n    L = numpy.array([numpy.linalg.cholesky(spatial_cov[i]) for i in range(M)])\n    LLT = []\n    for i in range(M):\n        row = [numpy.dot(L[i], L[j].T) * cross_corr[i, j] for j in range(M)]\n        for j in range(N):\n            singlerow = numpy.zeros(M * N)\n            for i in range(M):\n                singlerow[i * N:(i + 1) * N] = row[i][j]\n            LLT.append(singlerow)\n    return numpy.linalg.cholesky(numpy.array(LLT))", "response": "Decomposes the spatial covariance and cross correlation matrices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_gmfs(shakemap, spatialcorr, crosscorr, site_effects, trunclevel,\n            num_gmfs, seed, imts=None):\n    \"\"\"\n    :returns: (IMT-strings, array of GMFs of shape (R, N, E, M)\n    \"\"\"\n    N = len(shakemap)  # number of sites\n    std = shakemap['std']\n    if imts is None or len(imts) == 0:\n        imts = std.dtype.names\n    else:\n        imts = [imt for imt in imts if imt in std.dtype.names]\n    val = {imt: numpy.log(shakemap['val'][imt]) - std[imt] ** 2 / 2.\n           for imt in imts}\n    imts_ = [imt.from_string(name) for name in imts]\n    M = len(imts_)\n    cross_corr = cross_correlation_matrix(imts_, crosscorr)\n    mu = numpy.array([numpy.ones(num_gmfs) * val[str(imt)][j]\n                      for imt in imts_ for j in range(N)])\n    dmatrix = geo.geodetic.distance_matrix(\n        shakemap['lon'], shakemap['lat'])\n    spatial_corr = spatial_correlation_array(dmatrix, imts_, spatialcorr)\n    stddev = [std[str(imt)] for imt in imts_]\n    for im, std in zip(imts_, stddev):\n        if std.sum() == 0:\n            raise ValueError('Cannot decompose the spatial covariance '\n                             'because stddev==0 for IMT=%s' % im)\n    spatial_cov = spatial_covariance_array(stddev, spatial_corr)\n    L = cholesky(spatial_cov, cross_corr)  # shape (M * N, M * N)\n    if trunclevel:\n        Z = truncnorm.rvs(-trunclevel, trunclevel, loc=0, scale=1,\n                          size=(M * N, num_gmfs), random_state=seed)\n    else:\n        Z = norm.rvs(loc=0, scale=1, size=(M * N, num_gmfs), random_state=seed)\n    # Z has shape (M * N, E)\n    gmfs = numpy.exp(numpy.dot(L, Z) + mu) / PCTG\n    if site_effects:\n        gmfs = amplify_gmfs(imts_, shakemap['vs30'], gmfs)\n    if gmfs.max() > MAX_GMV:\n        logging.warning('There suspiciously large GMVs of %.2fg', gmfs.max())\n    return imts, gmfs.reshape((M, N, num_gmfs)).transpose(1, 2, 0)", "response": "Converts a dictionary of shakemap values into a list of GMFs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_header(dtype):\n    header = _build_header(dtype, ())\n    h = []\n    for col in header:\n        name = '~'.join(col[:-2])\n        numpytype = col[-2]\n        shape = col[-1]\n        coldescr = name\n        if numpytype != 'float32' and not numpytype.startswith('|S'):\n            coldescr += ':' + numpytype\n        if shape:\n            coldescr += ':' + ':'.join(map(str, shape))\n        h.append(coldescr)\n    return h", "response": "Convert a numpy nested dtype into a list of strings suitable as header\n    of csv file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a CSV file to dest.", "response": "def write_csv(dest, data, sep=',', fmt='%.6E', header=None, comment=None):\n    \"\"\"\n    :param dest: None, file, filename or io.BytesIO instance\n    :param data: array to save\n    :param sep: separator to use (default comma)\n    :param fmt: formatting string (default '%12.8E')\n    :param header:\n       optional list with the names of the columns to display\n    :param comment:\n       optional first line starting with a # character\n    \"\"\"\n    close = True\n    if dest is None:  # write on a temporary file\n        fd, dest = tempfile.mkstemp(suffix='.csv')\n        os.close(fd)\n    if hasattr(dest, 'write'):\n        # file-like object in append mode\n        # it must be closed by client code\n        close = False\n    elif not hasattr(dest, 'getvalue'):\n        # not a BytesIO, assume dest is a filename\n        dest = open(dest, 'wb')\n    try:\n        # see if data is a composite numpy array\n        data.dtype.fields\n    except AttributeError:\n        # not a composite array\n        autoheader = []\n    else:\n        autoheader = build_header(data.dtype)\n\n    if comment:\n        dest.write(encode('# %s\\n' % comment))\n\n    someheader = header or autoheader\n    if header != 'no-header' and someheader:\n        dest.write(encode(sep.join(htranslator.write(someheader)) + u'\\n'))\n\n    if autoheader:\n        all_fields = [col.split(':', 1)[0].split('~')\n                      for col in autoheader]\n        for record in data:\n            row = []\n            for fields in all_fields:\n                val = extract_from(record, fields)\n                if fields[0] in ('lon', 'lat', 'depth'):\n                    row.append('%.5f' % val)\n                else:\n                    row.append(scientificformat(val, fmt))\n            dest.write(encode(sep.join(row) + u'\\n'))\n    else:\n        for row in data:\n            dest.write(encode(sep.join(scientificformat(col, fmt)\n                                       for col in row) + u'\\n'))\n    if hasattr(dest, 'getvalue'):\n        return dest.getvalue()[:-1]  # a newline is strangely added\n    elif close:\n        dest.close()\n    return dest.name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_header(header):\n    triples = []\n    fields = []\n    for col_str in header:\n        col = col_str.strip().split(':')\n        n = len(col)\n        if n == 1:  # default dtype and no shape\n            col = [col[0], 'float32', '']\n        elif n == 2:\n            if castable_to_int(col[1]):  # default dtype and shape\n                col = [col[0], 'float32', col[1]]\n            else:  # dtype and no shape\n                col = [col[0], col[1], '']\n        elif n > 3:\n            raise ValueError('Invalid column description: %s' % col_str)\n        field = col[0]\n        numpytype = col[1]\n        shape = () if not col[2].strip() else (int(col[2]),)\n        triples.append((field, numpytype, shape))\n        fields.append(field)\n    return fields, numpy.dtype(triples)", "response": "Parses a list of header strings into a numpy composite dtype."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_comment(comment):\n    names, vals = [], []\n    pieces = comment.split('=')\n    for i, piece in enumerate(pieces):\n        if i == 0:  # first line\n            names.append(piece.strip())\n        elif i == len(pieces) - 1:  # last line\n            vals.append(ast.literal_eval(piece))\n        else:\n            val, name = piece.rsplit(',', 1)\n            vals.append(ast.literal_eval(val))\n            names.append(name.strip())\n    return list(zip(names, vals))", "response": "Parse a comment of the form\n    = b1 = b2 = b3 = b4 = b5 = b6 = b7 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9 = b9"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_array(fname, sep=','):\n    with open(fname) as f:\n        records = []\n        for line in f:\n            row = line.split(sep)\n            record = [list(map(float, col.split())) for col in row]\n            records.append(record)\n        return numpy.array(records)", "response": "r Convert a CSV file with header into a numpy array of floats."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, names):\n        descrs = []\n        for name in names:\n            mo = re.match(self.short_regex, name)\n            if mo:\n                idx = mo.lastindex  # matching group index, starting from 1\n                suffix = self.suffix[idx - 1].replace(r':\\|', ':|')\n                descrs.append(mo.group(mo.lastindex) + suffix +\n                              name[mo.end():])\n            else:\n                descrs.append(name)\n        return descrs", "response": "Convert names into descriptions\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a list of names into a list of names.", "response": "def write(self, descrs):\n        \"\"\"\n        Convert descriptions into names\n        \"\"\"\n        # example: '(poe-[\\d\\.]+):float32' -> 'poe-[\\d\\.]+'\n        names = []\n        for descr in descrs:\n            mo = re.match(self.long_regex, descr)\n            if mo:\n                names.append(mo.group(mo.lastindex) + descr[mo.end():])\n            else:\n                names.append(descr)\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, data, fname, header=None):\n        write_csv(fname, data, self.sep, self.fmt, header)\n        self.fnames.add(getattr(fname, 'name', fname))", "response": "Save data on fname."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave data to dest.", "response": "def save_block(self, data, dest):\n        \"\"\"\n        Save data on dest, which is file open in 'a' mode\n        \"\"\"\n        write_csv(dest, data, self.sep, self.fmt, 'no-header')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for specification of input and result values. Implements the following equations: Equation (8) on p. 203 for the bedrock ground motion: ``ln(y_br) = c1 + c2*(M - 6) + c3*(M - 6)**2 - lnR - c4*R + ln(\u03b5_br)`` Equation (9) on p. 207 gives the site amplification factor: ``ln(F_s) = a1*y_br + a2 + ln(\u03b4_site)`` Equation (10) on p. 207 for the ground motion at a given site: ``y_site = y_br*F_s`` Equation (11) on p. 207 for total standard error at a given site: ``\u03c3{ln(\u03b5_site)} = sqrt(\u03c3{ln(\u03b5_br)}**2 + \u03c3{ln(\u03b4_site)}**2)``", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # pylint: disable=too-many-arguments\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for specification of input and result values.\n\n        Implements the following equations:\n\n        Equation (8) on p. 203 for the bedrock ground motion:\n\n        ``ln(y_br) = c1 + c2*(M - 6) + c3*(M - 6)**2 - lnR - c4*R + ln(\u03b5_br)``\n\n        Equation (9) on p. 207 gives the site amplification factor:\n\n        ``ln(F_s) = a1*y_br + a2 + ln(\u03b4_site)``\n\n        Equation (10) on p. 207 for the ground motion at a given site:\n\n        ``y_site = y_br*F_s``\n\n        Equation (11) on p. 207 for total standard error at a given site:\n\n        ``\u03c3{ln(\u03b5_site)} = sqrt(\u03c3{ln(\u03b5_br)}**2 + \u03c3{ln(\u03b4_site)}**2)``\n\n        \"\"\"\n\n        # obtain coefficients for required intensity measure type\n        coeffs = self.COEFFS_BEDROCK[imt].copy()\n\n        # obtain site-class specific coefficients\n        a_1, a_2, sigma_site = self._get_site_coeffs(sites, imt)\n        coeffs.update({'a1': a_1, 'a2': a_2, 'sigma_site': sigma_site})\n\n        # compute bedrock motion, equation (8)\n        ln_mean = (self._compute_magnitude_terms(rup, coeffs) +\n                   self._compute_distance_terms(dists, coeffs))\n\n        # adjust for site class, equation (10)\n        ln_mean += self._compute_site_amplification(ln_mean, coeffs)\n        # No need to convert to g since \"In [equation (8)], y_br = (SA/g)\"\n\n        ln_stddevs = self._get_stddevs(coeffs, stddev_types)\n\n        return ln_mean, [ln_stddevs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_magnitude_terms(self, rup, coeffs):\n\n        adj_mag = rup.mag - self.CONSTS['ref_mag']\n        return coeffs['c1'] + coeffs['c2']*adj_mag + coeffs['c3']*adj_mag**2", "response": "Compute the magnitude terms of the logarithmic logarithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_distance_terms(cls, dists, coeffs):\n        return - np.log(dists.rhypo) - coeffs['c4']*dists.rhypo", "response": "Compute the distance terms of the class based on the coefficients of the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_stddevs(self, coeffs, stddev_types):\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n\n        return np.sqrt(coeffs['sigma_bedrock']**2 + coeffs['sigma_site']**2)", "response": "Returns the total standard error at a given site."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the correct coefficients for each site.", "response": "def _get_site_coeffs(self, sites, imt):\n        \"\"\"\n        Extracts correct coefficients for each site from Table 5 on p. 208\n        for each site.\n\n        :raises UserWarning:\n            If vs30 is below limit for site class D, since \"E- and F-type\n            sites [...] are susceptible for liquefaction and failure.\" p. 205.\n        \"\"\"\n\n        site_classes = self.get_nehrp_classes(sites)\n        is_bedrock = self.is_bedrock(sites)\n\n        if 'E' in site_classes:\n            msg = ('Site class E and F not supported by %s'\n                   % type(self).__name__)\n            warnings.warn(msg, UserWarning)\n\n        a_1 = np.nan*np.ones_like(sites.vs30)\n        a_2 = np.nan*np.ones_like(sites.vs30)\n        sigma = np.nan*np.ones_like(sites.vs30)\n        for key in self.COEFFS_NEHRP.keys():\n            indices = (site_classes == key) & ~is_bedrock\n            a_1[indices] = self.COEFFS_NEHRP[key][imt]['a1']\n            a_2[indices] = self.COEFFS_NEHRP[key][imt]['a2']\n            sigma[indices] = self.COEFFS_NEHRP[key][imt]['sigma']\n\n        a_1[is_bedrock] = 0.\n        a_2[is_bedrock] = 0.\n        sigma[is_bedrock] = 0.\n\n        return (a_1, a_2, sigma)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_nehrp_classes(self, sites):\n\n        classes = sorted(self.NEHRP_VS30_UPPER_BOUNDS.keys())\n        bounds = [self.NEHRP_VS30_UPPER_BOUNDS[item] for item in classes]\n        bounds = np.reshape(np.array(bounds), (-1, 1))\n        vs30s = np.reshape(sites.vs30, (1, -1))\n        site_classes = np.choose((vs30s < bounds).sum(axis=0) - 1, classes)\n\n        return site_classes.astype('object')", "response": "Get the site classes from the Nep - Hat site classification coefficients p. 205."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the user name from the request.", "response": "def get_user(request):\n    \"\"\"\n    Returns the users from `request` if authentication is enabled, otherwise\n    returns the default user (from settings, or as reported by the OS).\n    \"\"\"\n    if settings.LOCKDOWN and hasattr(request, 'user'):\n        if request.user.is_authenticated:\n            user = request.user.username\n        else:\n            # This may happen with crafted requests\n            user = ''\n    else:\n        user = getattr(settings, 'DEFAULT_USER', getpass.getuser())\n\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_valid_users(request):\n    users = [get_user(request)]\n    if settings.LOCKDOWN and hasattr(request, 'user'):\n        if request.user.is_authenticated:\n            groups = request.user.groups.all()\n            if groups:\n                users = list(User.objects.filter(groups__in=groups)\n                             .values_list('username', flat=True))\n        else:\n            # This may happen with crafted requests\n            users = []\n    return users", "response": "Returns a list of users based on groups membership."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if ACL should be honorated returns otherwise False.", "response": "def get_acl_on(request):\n    \"\"\"\n    Returns `True` if ACL should be honorated, returns otherwise `False`.\n    \"\"\"\n    acl_on = settings.ACL_ON\n    if settings.LOCKDOWN and hasattr(request, 'user'):\n        # ACL is always disabled for superusers\n        if request.user.is_superuser:\n            acl_on = False\n    return acl_on"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef oq_server_context_processor(request):\n\n    context = {}\n\n    context['oq_engine_server_url'] = ('//' +\n                                       request.META.get('HTTP_HOST',\n                                                        'localhost:8800'))\n    # this context var is also evaluated by the STANDALONE_APPS to identify\n    # the running environment. Keep it as it is\n    context['oq_engine_version'] = oqversion\n    context['server_name'] = settings.SERVER_NAME\n    return context", "response": "A custom context processor which allows injection of additional oq_server_context variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_webserver_running(url=\"http://localhost:8800\", max_retries=30):\n\n    retry = 0\n    response = ''\n    success = False\n\n    while response != requests.codes.ok and retry < max_retries:\n        try:\n            response = requests.head(url, allow_redirects=True).status_code\n            success = True\n        except:\n            sleep(1)\n\n        retry += 1\n\n    if not success:\n        logging.warning('Unable to connect to %s within %s retries'\n                     % (url, max_retries))\n    return success", "response": "Checks if a given URL is responding within a given timeout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_csv(ekey, dstore):\n    name = ekey[0] + '.csv'\n    try:\n        array = dstore[ekey[0]].value\n    except AttributeError:\n        # this happens if the key correspond to a HDF5 group\n        return []  # write a custom exporter in this case\n    if len(array.shape) == 1:  # vector\n        array = array.reshape((len(array), 1))\n    return [write_csv(dstore.export_path(name), array)]", "response": "Default csv exporter for arrays stored in the output. hdf5 file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport the data in the input_zip dataset as a. zip file.", "response": "def export_input_zip(ekey, dstore):\n    \"\"\"\n    Export the data in the `input_zip` dataset as a .zip file\n    \"\"\"\n    dest = dstore.export_path('input.zip')\n    nbytes = dstore.get_attr('input/zip', 'nbytes')\n    zbytes = dstore['input/zip'].value\n    # when reading input_zip some terminating null bytes are truncated (for\n    # unknown reasons) therefore they must be restored\n    zbytes += b'\\x00' * (nbytes - len(zbytes))\n    open(dest, 'wb').write(zbytes)\n    return [dest]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef classical_risk(riskinputs, riskmodel, param, monitor):\n    result = dict(loss_curves=[], stat_curves=[])\n    weights = [w['default'] for w in param['weights']]\n    statnames, stats = zip(*param['stats'])\n    for ri in riskinputs:\n        A = len(ri.assets)\n        L = len(riskmodel.lti)\n        R = ri.hazard_getter.num_rlzs\n        loss_curves = numpy.zeros((R, L, A), object)\n        avg_losses = numpy.zeros((R, L, A))\n        for out in riskmodel.gen_outputs(ri, monitor):\n            r = out.rlzi\n            for l, loss_type in enumerate(riskmodel.loss_types):\n                # loss_curves has shape (A, C)\n                for i, asset in enumerate(ri.assets):\n                    loss_curves[out.rlzi, l, i] = lc = out[loss_type][i]\n                    aid = asset['ordinal']\n                    avg = scientific.average_loss(lc)\n                    avg_losses[r, l, i] = avg\n                    lcurve = (lc['loss'], lc['poe'], avg)\n                    result['loss_curves'].append((l, r, aid, lcurve))\n\n        # compute statistics\n        for l, loss_type in enumerate(riskmodel.loss_types):\n            for i, asset in enumerate(ri.assets):\n                avg_stats = compute_stats(avg_losses[:, l, i], stats, weights)\n                losses = loss_curves[0, l, i]['loss']\n                all_poes = numpy.array(\n                    [loss_curves[r, l, i]['poe'] for r in range(R)])\n                poes_stats = compute_stats(all_poes, stats, weights)\n                result['stat_curves'].append(\n                    (l, asset['ordinal'], losses, poes_stats, avg_stats))\n    if R == 1:  # the realization is the same as the mean\n        del result['loss_curves']\n    return result", "response": "Compute and return the average losses for each asset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_taglist(node):\n    return [re.sub(r'\\{[^}]*\\}', \"\", copy(subnode.tag))\n            for subnode in node.nodes]", "response": "Returns a list of tags representing the order of the nodes within a node"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef linestring_node_to_line(node, with_depth=False):\n    assert \"LineString\" in node.tag\n    crds = [float(x) for x in node.nodes[0].text.split()]\n    if with_depth:\n        return Line([Point(crds[iloc], crds[iloc + 1], crds[iloc + 2])\n                     for iloc in range(0, len(crds), 3)])\n    else:\n        return Line([Point(crds[iloc], crds[iloc + 1])\n                     for iloc in range(0, len(crds), 2)])", "response": "Converts a Line node to a Line object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef node_to_point_geometry(node):\n    assert \"pointGeometry\" in node.tag\n    for subnode in node.nodes:\n        if \"Point\" in subnode.tag:\n            # Position\n            lon, lat = map(float, subnode.nodes[0].text.split())\n            point = Point(lon, lat)\n        elif \"upperSeismoDepth\" in subnode.tag:\n            upper_depth = float_(subnode.text)\n        elif \"lowerSeismoDepth\" in subnode.tag:\n            lower_depth = float_(subnode.text)\n        else:\n            # Redundent\n            pass\n    assert lower_depth > upper_depth\n    return point, upper_depth, lower_depth", "response": "Reads the node and returns the point geometry upper depth and lower depth."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef node_to_area_geometry(node):\n    assert \"areaGeometry\" in node.tag\n    for subnode in node.nodes:\n        if \"Polygon\" in subnode.tag:\n            crds = [float(x)\n                    for x in subnode.nodes[0].nodes[0].nodes[0].text.split()]\n            polygon = Polygon([Point(crds[iloc], crds[iloc + 1])\n                               for iloc in range(0, len(crds), 2)])\n        elif \"upperSeismoDepth\" in subnode.tag:\n            upper_depth = float_(subnode.text)\n        elif \"lowerSeismoDepth\" in subnode.tag:\n            lower_depth = float_(subnode.text)\n        else:\n            # Redundent\n            pass\n    assert lower_depth > upper_depth\n    return polygon, upper_depth, lower_depth", "response": "Reads an area geometry node and returns the polygon upper depth and lower depth."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a simple fault geometry node and returns an OpenQuake representation of the fault as instance Formula", "response": "def node_to_simple_fault_geometry(node):\n    \"\"\"\n    Reads a simple fault geometry node and returns an OpenQuake representation\n\n    :returns:\n        trace - Trace of fault as instance\n    \"\"\"\n    assert \"simpleFaultGeometry\" in node.tag\n    for subnode in node.nodes:\n        if \"LineString\" in subnode.tag:\n            trace = linestring_node_to_line(subnode, with_depth=False)\n        elif \"dip\" in subnode.tag:\n            dip = float(subnode.text)\n        elif \"upperSeismoDepth\" in subnode.tag:\n            upper_depth = float(subnode.text)\n        elif \"lowerSeismoDepth\" in subnode.tag:\n            lower_depth = float(subnode.text)\n        else:\n            # Redundent\n            pass\n    assert lower_depth > upper_depth\n    return trace, dip, upper_depth, lower_depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a complex fault geometry node and returns an anatomical fault geometry list.", "response": "def node_to_complex_fault_geometry(node):\n    \"\"\"\n    Reads a complex fault geometry node and returns an\n    \"\"\"\n    assert \"complexFaultGeometry\" in node.tag\n    intermediate_edges = []\n    for subnode in node.nodes:\n        if \"faultTopEdge\" in subnode.tag:\n            top_edge = linestring_node_to_line(subnode.nodes[0],\n                                               with_depth=True)\n        elif \"intermediateEdge\" in subnode.tag:\n            int_edge = linestring_node_to_line(subnode.nodes[0],\n                                               with_depth=True)\n            intermediate_edges.append(int_edge)\n        elif \"faultBottomEdge\" in subnode.tag:\n            bottom_edge = linestring_node_to_line(subnode.nodes[0],\n                                                  with_depth=True)\n        else:\n            # Redundent\n            pass\n    return [top_edge] + intermediate_edges + [bottom_edge]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef node_to_truncated_gr(node, bin_width=0.1):\n    # Parse to float dictionary\n    if not all([node.attrib[key]\n                for key in [\"minMag\", \"maxMag\", \"aValue\", \"bValue\"]]):\n        return None\n    tgr = dict((key, float_(node.attrib[key])) for key in node.attrib)\n    return mfd.truncated_gr.TruncatedGRMFD(min_mag=tgr[\"minMag\"],\n                                           max_mag=tgr[\"maxMag\"],\n                                           bin_width=bin_width,\n                                           a_val=tgr[\"aValue\"],\n                                           b_val=tgr[\"bValue\"])", "response": "Parses truncated GR node to an openquake. hazardlib. mfd. truncated_gr. TruncatedGRMFD object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the evenly discretized mfd node to an instance of the evenly discretized MFD class.", "response": "def node_to_evenly_discretized(node):\n    \"\"\"\n    Parses the evenly discretized mfd node to an instance of the\n    :class: openquake.hazardlib.mfd.evenly_discretized.EvenlyDiscretizedMFD,\n    or to None if not all parameters are available\n    \"\"\"\n    if not all([node.attrib[\"minMag\"], node.attrib[\"binWidth\"],\n                node.nodes[0].text]):\n        return None\n    # Text to float\n    rates = [float(x) for x in node.nodes[0].text.split()]\n    return mfd.evenly_discretized.EvenlyDiscretizedMFD(\n        float(node.attrib[\"minMag\"]),\n        float(node.attrib[\"binWidth\"]),\n        rates)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef node_to_mfd(node, taglist):\n    if \"incrementalMFD\" in taglist:\n        mfd = node_to_evenly_discretized(\n            node.nodes[taglist.index(\"incrementalMFD\")])\n    elif \"truncGutenbergRichterMFD\" in taglist:\n        mfd = node_to_truncated_gr(\n            node.nodes[taglist.index(\"truncGutenbergRichterMFD\")])\n    else:\n        mfd = None\n    return mfd", "response": "Reads the node to return a magnitude frequency distribution"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the nodal plane distribution to a PMF.", "response": "def node_to_nodal_planes(node):\n    \"\"\"\n    Parses the nodal plane distribution to a PMF\n    \"\"\"\n    if not len(node):\n        return None\n    npd_pmf = []\n    for plane in node.nodes:\n        if not all(plane.attrib[key] for key in plane.attrib):\n            # One plane fails - return None\n            return None\n        npd = NodalPlane(float(plane.attrib[\"strike\"]),\n                         float(plane.attrib[\"dip\"]),\n                         float(plane.attrib[\"rake\"]))\n        npd_pmf.append((float(plane.attrib[\"probability\"]), npd))\n    return PMF(npd_pmf)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the node to a hpyocentral depth distribution PMF", "response": "def node_to_hdd(node):\n    \"\"\"\n    Parses the node to a hpyocentral depth distribution PMF\n    \"\"\"\n    if not len(node):\n        return None\n    hdds = []\n    for subnode in node.nodes:\n        if not all([subnode.attrib[key] for key in [\"depth\", \"probability\"]]):\n            return None\n        hdds.append((float(subnode.attrib[\"probability\"]),\n                     float(subnode.attrib[\"depth\"])))\n    return PMF(hdds)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_point_source_node(node, mfd_spacing=0.1):\n    assert \"pointSource\" in node.tag\n    pnt_taglist = get_taglist(node)\n    # Get metadata\n    point_id, name, trt = (node.attrib[\"id\"],\n                           node.attrib[\"name\"],\n                           node.attrib[\"tectonicRegion\"])\n    assert point_id  # Defensive validation!\n    # Process geometry\n    location, upper_depth, lower_depth = node_to_point_geometry(\n        node.nodes[pnt_taglist.index(\"pointGeometry\")])\n    # Process scaling relation\n    msr = node_to_scalerel(node.nodes[pnt_taglist.index(\"magScaleRel\")])\n    # Process aspect ratio\n    aspect = float_(node.nodes[pnt_taglist.index(\"ruptAspectRatio\")].text)\n    # Process MFD\n    mfd = node_to_mfd(node, pnt_taglist)\n    # Process nodal planes\n    npds = node_to_nodal_planes(\n        node.nodes[pnt_taglist.index(\"nodalPlaneDist\")])\n    # Process hypocentral depths\n    hdds = node_to_hdd(node.nodes[pnt_taglist.index(\"hypoDepthDist\")])\n    return mtkPointSource(point_id, name, trt,\n                          geometry=location,\n                          upper_depth=upper_depth,\n                          lower_depth=lower_depth,\n                          mag_scale_rel=msr,\n                          rupt_aspect_ratio=aspect,\n                          mfd=mfd,\n                          nodal_plane_dist=npds,\n                          hypo_depth_dist=hdds)", "response": "Parses a point source node into an areaSource node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse an area source node into an openquake. hmtk. sources. area. mtkAreaSource object.", "response": "def parse_area_source_node(node, mfd_spacing=0.1):\n    \"\"\"\n    Returns an \"areaSource\" node into an instance of the :class:\n    openquake.hmtk.sources.area.mtkAreaSource\n    \"\"\"\n    assert \"areaSource\" in node.tag\n    area_taglist = get_taglist(node)\n    # Get metadata\n    area_id, name, trt = (node.attrib[\"id\"],\n                          node.attrib[\"name\"],\n                          node.attrib[\"tectonicRegion\"])\n    assert area_id  # Defensive validation!\n    # Process geometry\n    polygon, upper_depth, lower_depth = node_to_area_geometry(\n        node.nodes[area_taglist.index(\"areaGeometry\")])\n    # Process scaling relation\n    msr = node_to_scalerel(node.nodes[area_taglist.index(\"magScaleRel\")])\n    # Process aspect ratio\n    aspect = float_(node.nodes[area_taglist.index(\"ruptAspectRatio\")].text)\n    # Process MFD\n    mfd = node_to_mfd(node, area_taglist)\n    # Process nodal planes\n    npds = node_to_nodal_planes(\n        node.nodes[area_taglist.index(\"nodalPlaneDist\")])\n    # Process hypocentral depths\n    hdds = node_to_hdd(node.nodes[area_taglist.index(\"hypoDepthDist\")])\n    return mtkAreaSource(area_id, name, trt,\n                         geometry=polygon,\n                         upper_depth=upper_depth,\n                         lower_depth=lower_depth,\n                         mag_scale_rel=msr,\n                         rupt_aspect_ratio=aspect,\n                         mfd=mfd,\n                         nodal_plane_dist=npds,\n                         hypo_depth_dist=hdds)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_simple_fault_node(node, mfd_spacing=0.1, mesh_spacing=1.0):\n    assert \"simpleFaultSource\" in node.tag\n    sf_taglist = get_taglist(node)\n    # Get metadata\n    sf_id, name, trt = (node.attrib[\"id\"],\n                        node.attrib[\"name\"],\n                        node.attrib[\"tectonicRegion\"])\n    # Process geometry\n    trace, dip, upper_depth, lower_depth = node_to_simple_fault_geometry(\n        node.nodes[sf_taglist.index(\"simpleFaultGeometry\")])\n    # Process scaling relation\n    msr = node_to_scalerel(node.nodes[sf_taglist.index(\"magScaleRel\")])\n    # Process aspect ratio\n    aspect = float_(node.nodes[sf_taglist.index(\"ruptAspectRatio\")].text)\n    # Process MFD\n    mfd = node_to_mfd(node, sf_taglist)\n    # Process rake\n    rake = float_(node.nodes[sf_taglist.index(\"rake\")].text)\n    simple_fault = mtkSimpleFaultSource(sf_id, name, trt,\n                                        geometry=None,\n                                        dip=dip,\n                                        upper_depth=upper_depth,\n                                        lower_depth=lower_depth,\n                                        mag_scale_rel=msr,\n                                        rupt_aspect_ratio=aspect,\n                                        mfd=mfd,\n                                        rake=rake)\n    simple_fault.create_geometry(trace, dip, upper_depth, lower_depth,\n                                 mesh_spacing)\n    return simple_fault", "response": "Parses a simple fault source node and returns an instance of the openquake. hmtk. sources. simple_fault. mtkSimpleFaultSource class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_complex_fault_node(node, mfd_spacing=0.1, mesh_spacing=4.0):\n    assert \"complexFaultSource\" in node.tag\n    sf_taglist = get_taglist(node)\n    # Get metadata\n    sf_id, name, trt = (node.attrib[\"id\"],\n                        node.attrib[\"name\"],\n                        node.attrib[\"tectonicRegion\"])\n    # Process geometry\n    edges = node_to_complex_fault_geometry(\n        node.nodes[sf_taglist.index(\"complexFaultGeometry\")])\n    # Process scaling relation\n    msr = node_to_scalerel(node.nodes[sf_taglist.index(\"magScaleRel\")])\n    # Process aspect ratio\n    aspect = float_(node.nodes[sf_taglist.index(\"ruptAspectRatio\")].text)\n    # Process MFD\n    mfd = node_to_mfd(node, sf_taglist)\n    # Process rake\n    rake = float_(node.nodes[sf_taglist.index(\"rake\")].text)\n    complex_fault = mtkComplexFaultSource(sf_id, name, trt,\n                                          geometry=None,\n                                          mag_scale_rel=msr,\n                                          rupt_aspect_ratio=aspect,\n                                          mfd=mfd,\n                                          rake=rake)\n    complex_fault.create_geometry(edges, mesh_spacing)\n    return complex_fault", "response": "Parses a complex fault source node and returns an instance of the : class : mtkComplexFaultSource object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread in the source model from the file and returns an instance of the class openquake. hmtk. sources. mtkSourceModel. mtkSourceModel.", "response": "def read_file(self, identifier, mfd_spacing=0.1, simple_mesh_spacing=1.0,\n                  complex_mesh_spacing=4.0, area_discretization=10.):\n        \"\"\"\n        Reads in the source model in returns an instance of the :class:\n        openquake.hmtk.sourcs.source_model.mtkSourceModel\n        \"\"\"\n        sm_node = node_from_xml(self.input_file)[0]\n        if sm_node[0].tag.startswith('{http://openquake.org/xmlns/nrml/0.4}'):\n            node_sets = [sm_node]\n            sm_name = sm_node.get(\"name\", \"\")\n        else:  # format NRML 0.5+\n            node_sets = sm_node\n            sm_name = sm_node[\"name\"]\n        source_model = mtkSourceModel(identifier, name=sm_name)\n        for node_set in node_sets:\n            for node in node_set:\n                if \"pointSource\" in node.tag:\n                    source_model.sources.append(\n                        parse_point_source_node(node, mfd_spacing))\n                elif \"areaSource\" in node.tag:\n                    source_model.sources.append(\n                        parse_area_source_node(node, mfd_spacing))\n                elif \"simpleFaultSource\" in node.tag:\n                    source_model.sources.append(\n                        parse_simple_fault_node(node, mfd_spacing,\n                                                simple_mesh_spacing))\n                elif \"complexFaultSource\" in node.tag:\n                    source_model.sources.append(\n                        parse_complex_fault_node(node, mfd_spacing,\n                                                 complex_mesh_spacing))\n                # TODO: multiPointSource are not supported\n                else:\n                    print(\"Source typology %s not recognised - skipping!\"\n                          % node.tag)\n        return source_model"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        C = self.COEFFS[imt]\n\n        mag = rup.mag - 6\n        d = np.sqrt(dists.rjb ** 2 + C['c7'] ** 2)\n        mean = np.zeros_like(d)\n\n        mean += C['c1'] + C['c2'] * mag + C['c3'] * mag ** 2 + C['c6']\n\n        idx = d <= 100.\n        mean[idx] = mean[idx] + C['c5'] * np.log10(d[idx])\n\n        idx = d > 100.\n        mean[idx] = (mean[idx] + C['c5'] * np.log10(100.) -\n                     np.log10(d[idx] / 100.) + C['c4'] * (d[idx] - 100.))\n\n        # convert from log10 to ln and from cm/s**2 to g\n        mean = np.log((10.0 ** (mean - 2.0)) / g)\n\n        stddevs = self._get_stddevs(C, stddev_types,  dists.rjb.shape[0])\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS_SINTER[imt]\n\n        # cap magnitude values at 8.5, see page 1709\n        mag = rup.mag\n        if mag > 8.5:\n            mag = 8.5\n\n        # compute PGA on rock (needed for site amplification calculation)\n        G = 10 ** (1.2 - 0.18 * mag)\n        pga_rock = self._compute_mean(self.COEFFS_SINTER[PGA()], G, mag,\n                                      rup.hypo_depth, dists.rrup, sites.vs30,\n                                      # by passing pga_rock > 500 the soil\n                                      # amplification is 0\n                                      np.zeros_like(sites.vs30) + 600,\n                                      PGA())\n        pga_rock = 10 ** (pga_rock)\n\n        # periods 0.4 s (2.5 Hz) and 0.2 s (5 Hz) need a special case because\n        # of the erratum. SA for 0.4s and 0.2s is computed and a weighted sum\n        # is returned\n        if imt.period in (0.2, 0.4):\n            C04 = self.COEFFS_SINTER[SA(period=0.4, damping=5.0)]\n            C02 = self.COEFFS_SINTER[SA(period=0.2, damping=5.0)]\n            mean04 = self._compute_mean(C04, G, mag, rup.hypo_depth,\n                                        dists.rrup, sites.vs30, pga_rock, imt)\n            mean02 = self._compute_mean(C02, G, mag, rup.hypo_depth,\n                                        dists.rrup, sites.vs30, pga_rock, imt)\n\n            if imt.period == 0.2:\n                mean = 0.333 * mean02 + 0.667 * mean04\n            else:\n                mean = 0.333 * mean04 + 0.667 * mean02\n        else:\n            mean = self._compute_mean(C, G, mag, rup.hypo_depth, dists.rrup,\n                                      sites.vs30, pga_rock, imt)\n\n        # convert from log10 to ln and units from cm/s**2 to g\n        mean = np.log((10 ** mean) * 1e-2 / g)\n\n        if imt.period == 4.0:\n            mean /= 0.550\n\n        stddevs = self._get_stddevs(C, stddev_types, sites.vs30.shape[0])\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation of the site and site amplification for the related object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_mean(self, C, g, mag, hypo_depth, rrup, vs30, pga_rock, imt):\n        if hypo_depth > 100:\n            hypo_depth = 100\n        delta = 0.00724 * 10 ** (0.507 * mag)\n        R = np.sqrt(rrup ** 2 + delta ** 2)\n\n        s_amp = self._compute_soil_amplification(C, vs30, pga_rock, imt)\n\n        mean = (\n            # 1st term\n            C['c1'] + C['c2'] * mag +\n            # 2nd term\n            C['c3'] * hypo_depth +\n            # 3rd term\n            C['c4'] * R -\n            # 4th term\n            g * np.log10(R) +\n            # 5th, 6th and 7th terms\n            s_amp\n        )\n\n        return mean", "response": "Compute mean according to equation 1 page 1706."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and return soil linear factor as explained in paragraph Functional ontology Form page 1706.", "response": "def _compute_soil_linear_factor(cls, pga_rock, imt):\n        \"\"\"\n        Compute soil linear factor as explained in paragraph 'Functional\n        Form', page 1706.\n        \"\"\"\n        if imt.period >= 1:\n            return np.ones_like(pga_rock)\n        else:\n            sl = np.zeros_like(pga_rock)\n\n            pga_between_100_500 = (pga_rock > 100) & (pga_rock < 500)\n            pga_greater_equal_500 = pga_rock >= 500\n\n            is_SA_between_05_1 = 0.5 < imt.period < 1\n\n            is_SA_less_equal_05 = imt.period <= 0.5\n\n            if is_SA_between_05_1:\n                sl[pga_between_100_500] = (1 - (1. / imt.period - 1) *\n                                           (pga_rock[pga_between_100_500] -\n                                           100) / 400)\n                sl[pga_greater_equal_500] = 1 - (1. / imt.period - 1)\n\n            if is_SA_less_equal_05 or imt.period == 0:\n                sl[pga_between_100_500] = (1 - (pga_rock[pga_between_100_500] -\n                                           100) / 400)\n\n            sl[pga_rock <= 100] = 1\n\n            return sl"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS_SSLAB[imt]\n\n        # cap magnitude values at 8.0, see page 1709\n        mag = rup.mag\n        if mag >= 8.0:\n            mag = 8.0\n\n        # compute PGA on rock (needed for site amplification calculation)\n        G = 10 ** (0.301 - 0.01 * mag)\n        pga_rock = self._compute_mean(self.COEFFS_SSLAB[PGA()], G, mag,\n                                      rup.hypo_depth, dists.rrup, sites.vs30,\n                                      # by passing pga_rock > 500 the soil\n                                      # amplification is 0\n                                      np.zeros_like(sites.vs30) + 600,\n                                      PGA())\n        pga_rock = 10 ** (pga_rock)\n\n        # compute actual mean and convert from log10 to ln and units from\n        # cm/s**2 to g\n        mean = self._compute_mean(C, G, mag, rup.hypo_depth, dists.rrup,\n                                  sites.vs30, pga_rock, imt)\n        mean = np.log((10 ** mean) * 1e-2 / g)\n\n        if imt.period == 4.0:\n            mean /= 0.550\n\n        stddevs = self._get_stddevs(C, stddev_types, sites.vs30.shape[0])\n\n        return mean, stddevs", "response": "This method computes the mean and standard deviation of the site site"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nseeing :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values. Call super class method with hypocentral depth fixed at 20 km", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n\n        Call super class method with hypocentral depth fixed at 20 km\n        \"\"\"\n        # fix hypocentral depth to 20 km. Create new rupture context to avoid\n        # changing the original one\n        new_rup = copy.deepcopy(rup)\n        new_rup.hypo_depth = 20.\n\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, new_rup, dists, imt, stddev_types)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the soil amplification term in equation 1 page 1706 and add the B - C site condition as implemented by NSHMP.", "response": "def _compute_soil_amplification(cls, C, vs30, pga_rock, imt):\n        \"\"\"\n        Compute soil amplification (5th, 6th, and 7th terms in equation 1,\n        page 1706) and add the B/C site condition as implemented by NSHMP.\n        \"\"\"\n        Sbc, Sc, Sd, Se = cls._compute_site_class_dummy_variables(vs30)\n        sl = cls._compute_soil_linear_factor(pga_rock, imt)\n\n        return (\n            C['c5'] * sl * Sbc * 0.5 +\n            C['c5'] * sl * Sc +\n            C['c6'] * sl * Sd +\n            C['c7'] * sl * Se\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute dummy variables for the site class.", "response": "def _compute_site_class_dummy_variables(cls, vs30):\n        \"\"\"\n        Extend\n        :meth:`AtkinsonBoore2003SInter._compute_site_class_dummy_variables`\n        and includes dummy variable for B/C site conditions (vs30 > 760.)\n        \"\"\"\n        Sbc = np.zeros_like(vs30)\n        Sc = np.zeros_like(vs30)\n        Sd = np.zeros_like(vs30)\n        Se = np.zeros_like(vs30)\n\n        Sbc[vs30 > 760.] = 1\n        Sc[(vs30 > 360) & (vs30 <= 760)] = 1\n        Sd[(vs30 >= 180) & (vs30 <= 360)] = 1\n        Se[vs30 < 180] = 1\n\n        return Sbc, Sc, Sd, Se"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_soil_amplification(cls, C, vs30, pga_rock, imt):\n        return AtkinsonBoore2003SInterNSHMP2008._compute_soil_amplification(\n            C, vs30, pga_rock, imt)", "response": "Compute the soil amplification for the given site and intensity measure type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the mean and standard deviation of the current site class and return the corresponding value.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        # Compute SA with primed coeffs and PGA with both unprimed and\n        # primed coeffs\n        C = self.COEFFS_PRIMED[imt]\n        C_PGA = self.COEFFS_PRIMED[PGA()]\n        C_PGA_unprimed = self.COEFFS_UNPRIMED[PGA()]\n\n        # Get S term to determine if consider site term is applied\n        S = self._get_site_class(sites)\n        # Abrahamson and Silva (1997) hanging wall term. This is not used\n        # in the latest version of GMPE but is defined in functional form in\n        # the paper so we keep it here as a placeholder\n        f4HW = self._compute_f4(C, rup.mag, dists.rrup)\n\n        # Flags for rake angles\n        CN, CR = self._get_fault_mechanism_flags(rup.rake)\n\n        # Get volcanic path distance which Rvol=0 for current implementation\n        # of McVerry2006Asc, but kept here as placeholder for future use\n        rvol = self._get_volcanic_path_distance(dists.rrup)\n\n        # Get delta_C and delta_D terms for site class\n        delta_C, delta_D = self._get_deltas(sites)\n        # Compute lnPGA_ABCD primed\n        lnPGAp_ABCD = self._compute_mean(C_PGA, S, rup.mag, dists.rrup, rvol,\n                                         rup.hypo_depth, CN, CR, f4HW,\n                                         delta_C, delta_D)\n\n        # Compute lnPGA_ABCD unprimed\n        lnPGA_ABCD = self._compute_mean(C_PGA_unprimed, S, rup.mag, dists.rrup,\n                                        rvol, rup.hypo_depth, CN, CR, f4HW,\n                                        delta_C, delta_D)\n\n        # Compute lnSA_ABCD\n        lnSAp_ABCD = self._compute_mean(C, S, rup.mag, dists.rrup, rvol,\n                                        rup.hypo_depth, CN, CR, f4HW,\n                                        delta_C, delta_D)\n\n        # Stage 3: Equation 6 SA_ABCD(T). This is lnSA_ABCD\n        # need to calculate final lnSA_ABCD from non-log values but return log\n        mean = np.log(np.exp(lnSAp_ABCD) *\n                      (np.exp(lnPGA_ABCD) / np.exp(lnPGAp_ABCD)))\n\n        # Compute standard deviations\n        C_STD = self.COEFFS_STD[imt]\n        stddevs = self._get_stddevs(\n            C_STD, rup.mag, stddev_types, sites\n        )\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_mean(self, C, S, mag, rrup, rvol, hypo_depth, CN, CR, f4HW,\n                      delta_C, delta_D):\n        \"\"\"\n        Compute mean value on site class A,B,C,D (equation 4)\n        returns lnSA_ABCD\n        \"\"\"\n\n        # Stage 1: compute PGA_ABCD and PGA'_ABCD which are then used in\n        # equation 6\n        # Equation 1 PGA unprimed version\n        lnSA_AB = self._compute_mean_on_rock(C, mag, rrup, rvol, hypo_depth,\n                                             CN, CR, f4HW)\n\n        # Equation 4 PGA unprimed version\n        lnSA_ABCD = lnSA_AB + S *\\\n            self._compute_nonlinear_soil_term(C, lnSA_AB, delta_C, delta_D)\n\n        return lnSA_ABCD", "response": "Compute the mean value on site class A B C D and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_mean_on_rock(self, C, mag, rrup, rvol, hypo_depth, CN, CR,\n                              f4HW):\n        \"\"\"\n        Compute mean value on site class A/B (equation 1 on page 22)\n        \"\"\"\n\n        lnSA_AB = (\n            # line 1 of equation 1\n            C['c1'] + C['c4as'] * (mag - 6) +\n            # line 2\n            C['c3as'] * (8.5 - mag) ** 2 +\n            # line 3\n            C['c5'] * rrup +\n            # line 3 and 4\n            (C['c8'] + C['c6as'] * (mag - 6)) *\n            np.log((rrup ** 2 + C['c10as'] ** 2) ** 0.5) +\n            # line 5\n            C['c46'] * rvol +\n            # line 6\n            C['c32'] * CN + C['c33as'] * CR + f4HW\n        )\n\n        return lnSA_AB", "response": "Compute mean value on site class A and B based on equation 1 on page 22."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_nonlinear_soil_term(self, C, lnSA_AB, delta_C, delta_D):\n\n        lnSA_CD = (\n            # line 1 equation 4 without first term (lnSA_AB)\n            C['c29'] * delta_C +\n            # line 2 and 3\n            (C['c30as'] * np.log(np.exp(lnSA_AB) + 0.03) + C['c43']) * delta_D\n            )\n\n        return lnSA_CD", "response": "Compute the nonlinear soil term in equation 4 on page 22 without first term."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn standard deviation as defined on page 29 infrastructure and interevent.", "response": "def _get_stddevs(self, C, mag, stddev_types, sites):\n        \"\"\"\n        Return standard deviation as defined on page 29 in\n        equation 8a,b,c and 9.\n        \"\"\"\n        num_sites = sites.vs30.size\n        sigma_intra = np.zeros(num_sites)\n\n        # interevent stddev\n        tau = sigma_intra + C['tau']\n\n        # intraevent std (equations 8a-8c page 29)\n        if mag < 5.0:\n            sigma_intra += C['sigmaM6'] - C['sigSlope']\n        elif 5.0 <= mag < 7.0:\n            sigma_intra += C['sigmaM6'] + C['sigSlope'] * (mag - 6)\n        else:\n            sigma_intra += C['sigmaM6'] + C['sigSlope']\n\n        std = []\n\n        for stddev_type in stddev_types:\n            if stddev_type == const.StdDev.TOTAL:\n                # equation 9 page 29\n                std += [np.sqrt(sigma_intra**2 + tau**2)]\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                std.append(sigma_intra)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                std.append(tau)\n\n        return std"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_site_class(self, sites):\n        vs30 = sites.vs30\n        S = np.zeros_like(vs30)\n        S[vs30 <= 760] = 1\n\n        return S", "response": "Return site class flag"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the fault mechanism flag CN and CR page 23.", "response": "def _get_fault_mechanism_flags(self, rake):\n        \"\"\"\n        Return the fault mechanism flag CN and CR, page 23\n        CN = -1 for normal (-146<rake<-33), 0 otherwise\n        CR = 0.5 for reverse-oblique (33<rake<66), 1 for reverse (67<rake<123)\n        and 0 otherwise\n        \"\"\"\n\n        CN, CR = 0, 0\n\n        # Pure Normal: rake = -90\n        if rake > -147 and rake < -33:\n            CN = -1\n\n        # Pure Reverse: rake = 90\n        if rake > 67 and rake < 123:\n            CR = 1\n\n        # Pure Oblique Reverse: rake = 45\n        if rake > 33 and rake < 66:\n            CR = 0.5\n\n        return CN, CR"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns delta s for equation 4", "response": "def _get_deltas(self, sites):\n        \"\"\"\n        Return delta's for equation 4\n        delta_C = 1 for site class C (360<=Vs30<760), 0 otherwise\n        delta_D = 1 for site class D (Vs30<=360), 0 otherwise\n        \"\"\"\n        vs30 = sites.vs30\n        delta_C = np.zeros(len(vs30))\n        delta_C[(vs30 >= 360) & (vs30 < 760)] = 1\n\n        delta_D = np.zeros(len(vs30))\n        delta_D[vs30 < 360] = 1\n\n        return delta_C, delta_D"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_mean_on_rock(self, C, mag, rrup, rvol, hypo_depth, CN, CR,\n                              f4HW):\n        \"\"\"\n        Compute mean value on site class A/B (equation 2 on page 22)\n        \"\"\"\n\n        # Define subduction flag (page 23)\n        # SI=1 for subduction interface, 0 otherwise\n        # DS=1 for subduction intraslab, 0 otherwise\n        SI = 0\n        DS = 1\n\n        lnSA_AB = (\n            # line 1 and 2 of equation 2\n            C['c11'] + (C['c12y'] + (C['c15'] - C['c17']) * C['c19y']) *\n            (mag - 6) +\n            # line 3\n            C['c13y'] * (10 - mag) ** 3 +\n            # line 4\n            C['c17'] * np.log(rrup + C['c18y'] * np.exp(C['c19y'] * mag)) +\n            # line 5\n            C['c20'] * hypo_depth + C['c24'] * SI +\n            # line 6\n            C['c46'] * rvol * (1 - DS)\n        )\n\n        return lnSA_AB", "response": "Compute mean value on site class A and B based on equation 2 and page 22."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns delta s for equation 4", "response": "def _get_deltas(self, sites):\n        \"\"\"\n        Return delta's for equation 4\n        delta_C = 1 for site class C, 0 otherwise\n        delta_D = 1 for site class D, 0 otherwise\n        \"\"\"\n        siteclass = sites.siteclass\n        delta_C = np.zeros_like(siteclass, dtype=np.float)\n        delta_C[siteclass == b'C'] = 1\n\n        delta_D = np.zeros_like(siteclass, dtype=np.float)\n        delta_D[siteclass == b'D'] = 1\n\n        return delta_C, delta_D"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the site class flag.", "response": "def _get_site_class(self, sites):\n        \"\"\"\n        Return site class flag (0 if class A or B, that is rock, or 1 if\n        class C or D).\n        \"\"\"\n        siteclass = sites.siteclass\n        S = np.zeros_like(siteclass, dtype=np.float)\n        S[(siteclass == b'C') | (siteclass == b'D')] = 1\n\n        return S"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and return mean value in table 8 page 8", "response": "def _compute_mean(self, C, mag, rjb):\n        \"\"\"\n        Compute and return mean value (table 8, page 8)\n        \"\"\"\n        d1 = np.sqrt(50. ** 2 + 6. ** 2)\n        d = np.sqrt(rjb ** 2 + 6 ** 2)\n\n        mean = np.zeros_like(rjb)\n\n        mean += (\n            C['a1'] + C['a2'] * (mag - 6.4) +\n            C['a7'] * (8.5 - mag) ** 2\n        )\n\n        idx = rjb < 50.\n        mean[idx] += (\n            C['a3'] * np.log(d[idx]) +\n            C['a4'] * (mag - 6.4) * np.log(d[idx]) +\n            C['a5'] * rjb[idx]\n        )\n\n        idx = rjb >=50.\n        mean[idx] += (\n            C['a3'] * np.log(d1) +\n            C['a4'] * (mag - 6.4) * np.log(d[idx]) +\n            C['a5'] * rjb[idx] + C['a6'] * (np.log(d[idx]) - np.log(d1))\n        )\n\n        return mean"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_assets_by_taxo(assets, epspath=None):\n    assets_by_taxo = AccumDict(group_array(assets, 'taxonomy'))\n    assets_by_taxo.idxs = numpy.argsort(numpy.concatenate([\n        a['ordinal'] for a in assets_by_taxo.values()]))\n    assets_by_taxo.eps = {}\n    if epspath is None:  # no epsilons\n        return assets_by_taxo\n    # otherwise read the epsilons and group them by taxonomy\n    with hdf5.File(epspath, 'r') as h5:\n        dset = h5['epsilon_matrix']\n        for taxo, assets in assets_by_taxo.items():\n            lst = [dset[aid] for aid in assets['ordinal']]\n            assets_by_taxo.eps[taxo] = numpy.array(lst)\n    return assets_by_taxo", "response": "returns an array of assets by taxonomy and idxs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding an array of epsilons for each asset in the asset_array.", "response": "def make_eps(asset_array, num_samples, seed, correlation):\n    \"\"\"\n    :param asset_array: an array of assets\n    :param int num_samples: the number of ruptures\n    :param int seed: a random seed\n    :param float correlation: the correlation coefficient\n    :returns: epsilons matrix of shape (num_assets, num_samples)\n    \"\"\"\n    assets_by_taxo = group_array(asset_array, 'taxonomy')\n    eps = numpy.zeros((len(asset_array), num_samples), numpy.float32)\n    for taxonomy, assets in assets_by_taxo.items():\n        shape = (len(assets), num_samples)\n        logging.info('Building %s epsilons for taxonomy %s', shape, taxonomy)\n        zeros = numpy.zeros(shape)\n        epsilons = scientific.make_epsilons(zeros, seed, correlation)\n        for asset, epsrow in zip(assets, epsilons):\n            eps[asset['ordinal']] = epsrow\n    return eps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating an epsilon matrix of shape A E and save it in the cache file.", "response": "def cache_epsilons(dstore, oq, assetcol, riskmodel, E):\n    \"\"\"\n    Do nothing if there are no coefficients of variation of ignore_covs is\n    set. Otherwise, generate an epsilon matrix of shape (A, E) and save it\n    in the cache file, by returning the path to it.\n    \"\"\"\n    if oq.ignore_covs or not riskmodel.covs:\n        return\n    A = len(assetcol)\n    hdf5path = dstore.hdf5cache()\n    logging.info('Storing the epsilon matrix in %s', hdf5path)\n    if oq.calculation_mode == 'scenario_risk':\n        eps = make_eps(assetcol.array, E, oq.master_seed, oq.asset_correlation)\n    else:  # event based\n        if oq.asset_correlation:\n            numpy.random.seed(oq.master_seed)\n            eps = numpy.array([numpy.random.normal(size=E)] * A)\n        else:\n            seeds = oq.master_seed + numpy.arange(E)\n            eps = numpy.zeros((A, E), F32)\n            for i, seed in enumerate(seeds):\n                numpy.random.seed(seed)\n                eps[:, i] = numpy.random.normal(size=A)\n    with hdf5.File(hdf5path) as cache:\n        cache['epsilon_matrix'] = eps\n    return hdf5path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef str2rsi(key):\n    rlzi, sid, imt = key.split('/')\n    return int(rlzi[4:]), int(sid[4:]), imt", "response": "Convert a string of the form rlz -XXXX sid - YYYY ZZZ into a triple (XXXX YYYY ZZZ"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(cls, dstore):\n        oqparam = dstore['oqparam']\n        tmap = (dstore['taxonomy_mapping'] if 'taxonomy_mapping' in dstore\n                else {})\n        crm = dstore.getitem('risk_model')\n        # building dictionaries riskid -> loss_type -> risk_func\n        fragdict, vulndict, consdict, retrodict = (\n            AccumDict(), AccumDict(), AccumDict(), AccumDict())\n        fragdict.limit_states = crm.attrs['limit_states']\n        for quoted_id, rm in crm.items():\n            riskid = unquote_plus(quoted_id)\n            fragdict[riskid] = {}\n            vulndict[riskid] = {}\n            consdict[riskid] = {}\n            retrodict[riskid] = {}\n            for lt_kind in rm:\n                lt, kind = lt_kind.rsplit('-', 1)\n                rf = dstore['risk_model/%s/%s' % (quoted_id, lt_kind)]\n                if kind == 'consequence':\n                    consdict[riskid][lt, kind] = rf\n                elif kind == 'fragility':  # rf is a FragilityFunctionList\n                    try:\n                        rf = rf.build(\n                            fragdict.limit_states,\n                            oqparam.continuous_fragility_discretization,\n                            oqparam.steps_per_interval)\n                    except ValueError as err:\n                        raise ValueError('%s: %s' % (riskid, err))\n                    fragdict[riskid][lt, kind] = rf\n                else:  # rf is a vulnerability function\n                    rf.init()\n                    if lt.endswith('_retrofitted'):\n                        # strip _retrofitted, since len('_retrofitted') = 12\n                        retrodict[riskid][lt[:-12], kind] = rf\n                    else:\n                        vulndict[riskid][lt, kind] = rf\n        return CompositeRiskModel(\n            oqparam, tmap, fragdict, vulndict, consdict, retrodict)", "response": "Read the composite risk model for the current language and taxonomy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict taxonomy string - > taxonomy index", "response": "def taxonomy_dict(self):\n        \"\"\"\n        :returns: a dict taxonomy string -> taxonomy index\n        \"\"\"\n        # .taxonomy must be set by the engine\n        tdict = {taxo: idx for idx, taxo in enumerate(self.taxonomy)}\n        return tdict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the extra IMTs in the risk functions i. e. the ones not in the imts set.", "response": "def get_extra_imts(self, imts):\n        \"\"\"\n        Returns the extra IMTs in the risk functions, i.e. the ones not in\n        the `imts` set (the set of IMTs for which there is hazard).\n        \"\"\"\n        extra_imts = set()\n        for taxonomy in self.taxonomies:\n            for (lt, kind), rf in self[taxonomy].risk_functions.items():\n                if rf.imt not in imts:\n                    extra_imts.add(rf.imt)\n        return extra_imts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_loss_ratios(self):\n        lst = [('user_provided', numpy.bool)]\n        for cp in self.curve_params:\n            lst.append((cp.loss_type, F32, len(cp.ratios)))\n        loss_ratios = numpy.zeros(1, numpy.dtype(lst))\n        for cp in self.curve_params:\n            loss_ratios['user_provided'] = cp.user_provided\n            loss_ratios[cp.loss_type] = tuple(cp.ratios)\n        return loss_ratios", "response": "returns a 1 - dimensional composite array with loss ratios by loss type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_dmg_csq(self, assets_by_site, gmf):\n        A = sum(len(assets) for assets in assets_by_site)\n        L = len(self.loss_types)\n        D = len(self.damage_states)\n        out = numpy.zeros((A, L, 1, D + 1), F32)\n        for assets, gmv in zip(assets_by_site, gmf):\n            group = group_array(assets, 'taxonomy')\n            for taxonomy, assets in group.items():\n                for l, loss_type in enumerate(self.loss_types):\n                    fracs = self[taxonomy](loss_type, assets, [gmv])\n                    for asset, frac in zip(assets, fracs):\n                        dmg = asset['number'] * frac[0, :D]\n                        csq = asset['value-' + loss_type] * frac[0, D]\n                        out[asset['ordinal'], l, 0, :D] = dmg\n                        out[asset['ordinal'], l, 0, D] = csq\n        return out", "response": "returns the dmg and csq for each asset and loss type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_outputs(self, riskinput, monitor, epspath=None, hazard=None):\n        self.monitor = monitor\n        hazard_getter = riskinput.hazard_getter\n        if hazard is None:\n            with monitor('getting hazard'):\n                hazard_getter.init()\n                hazard = hazard_getter.get_hazard()\n        sids = hazard_getter.sids\n        assert len(sids) == 1\n        with monitor('computing risk', measuremem=False):\n            # this approach is slow for event_based_risk since a lot of\n            # small arrays are passed (one per realization) instead of\n            # a long array with all realizations; ebrisk does the right\n            # thing since it calls get_output directly\n            assets_by_taxo = get_assets_by_taxo(riskinput.assets, epspath)\n            for rlzi, haz in sorted(hazard[sids[0]].items()):\n                out = self.get_output(assets_by_taxo, haz, rlzi)\n                yield out", "response": "Generator that yields one output per realization per asset per taxonomy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_output(self, assets_by_taxo, haz, rlzi=None):\n        if isinstance(haz, numpy.ndarray):\n            # NB: in GMF-based calculations the order in which\n            # the gmfs are stored is random since it depends on\n            # which hazard task ends first; here we reorder\n            # the gmfs by event ID; this is convenient in\n            # general and mandatory for the case of\n            # VulnerabilityFunctionWithPMF, otherwise the\n            # sample method would receive the means in random\n            # order and produce random results even if the\n            # seed is set correctly; very tricky indeed! (MS)\n            haz.sort(order='eid')\n            eids = haz['eid']\n            data = haz['gmv']  # shape (E, M)\n        elif not haz:  # no hazard for this site\n            eids = numpy.arange(1)\n            data = []\n        else:  # classical\n            eids = []\n            data = haz  # shape M\n        dic = dict(eids=eids)\n        if rlzi is not None:\n            dic['rlzi'] = rlzi\n        for l, lt in enumerate(self.loss_types):\n            ls = []\n            for taxonomy, assets_ in assets_by_taxo.items():\n                if len(assets_by_taxo.eps):\n                    epsilons = assets_by_taxo.eps[taxonomy][:, eids]\n                else:  # no CoVs\n                    epsilons = ()\n                rm = self[taxonomy]\n                if len(data) == 0:\n                    dat = [0]\n                elif len(eids):  # gmfs\n                    dat = data[:, rm.imti[lt]]\n                else:  # hcurves\n                    dat = data[rm.imti[lt]]\n                ls.append(rm(lt, assets_, dat, eids, epsilons))\n            arr = numpy.concatenate(ls)\n            dic[lt] = arr[assets_by_taxo.idxs] if len(arr) else arr\n        return hdf5.ArrayWrapper((), dic)", "response": "This method returns the output of the KMeans algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce(self, taxonomies):\n        new = copy.copy(self)\n        new.taxonomies = sorted(taxonomies)\n        new._riskmodels = {}\n        for riskid, rm in self._riskmodels.items():\n            if riskid in taxonomies:\n                new._riskmodels[riskid] = rm\n                rm.compositemodel = new\n        return new", "response": "Returns a new CompositeRiskModel with the given set of taxonomies reduced to the given set of taxonomies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an ordered dictionary with the available GSIM classes keyed by class name", "response": "def get_available_mfds():\n    '''\n    Returns an ordered dictionary with the available GSIM classes\n    keyed by class name\n    '''\n    mfds = {}\n    for fname in os.listdir(os.path.dirname(__file__)):\n        if fname.endswith('.py'):\n            modname, _ext = os.path.splitext(fname)\n            mod = importlib.import_module(\n                'openquake.hmtk.faults.mfd.' + modname)\n            for cls in mod.__dict__.values():\n                if inspect.isclass(cls) and issubclass(cls, BaseMFDfromSlip):\n                    mfds[cls.__name__] = cls\n    return dict((k, mfds[k]) for k in sorted(mfds))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the mean and standard deviation for the base class.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n\n        sites.vs30 = 700 * np.ones(len(sites.vs30))\n\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        tau_ss = 'tauC'\n        log_phi_ss = 1.00\n        C = ZhaoEtAl2006AscSWISS05.COEFFS_ASC\n        mean, stddevs = _apply_adjustments(\n            C, self.COEFFS_FS_ROCK[imt], tau_ss,\n            mean, stddevs, sites, rup, dists.rrup, imt, stddev_types,\n            log_phi_ss)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_stepp_plot(model, figure_size=(8, 6),\n                      filename=None, filetype='png', dpi=300, ax=None):\n    '''\n    Creates the classic Stepp (1972) plots for a completed Stepp analysis,\n    and exports the figure to a file.\n\n    :param model:\n        Completed Stepp (1972) analysis as instance of :class:\n        `openquake.hmtk.seismicity.completeness.comp_stepp_1971.Stepp1971`\n    :param string filename:\n        Name of output file\n    :param string filetype:\n        Type of file (from list supported by matplotlib)\n    :param int dpi:\n        Resolution (dots per inch) of output file\n    '''\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figure_size)\n    else:\n        fig = ax.get_figure()\n\n    if filename and os.path.exists(filename):\n        raise IOError('File already exists!')\n\n    # get colours from current axes: thus user can set up before calling\n    prop_cycler = ax._get_lines.prop_cycler\n    prop_cyclers = itertools.tee(itertools.cycle(prop_cycler), 3)\n    marker_cyclers = itertools.tee(itertools.cycle(VALID_MARKERS), 3)\n\n    # plot observed Sigma lambda\n    for i, (min_mag, max_mag) in enumerate(zip(model.magnitude_bin[:-1],\n                                               model.magnitude_bin[1:])):\n        label = '(%g, %g]: %d' % (min_mag, max_mag,\n                                  model.completeness_table[i, 0])\n        colour = next(prop_cyclers[0])['color']\n        ax.loglog(model.time_values, model.sigma[:, i],\n                  linestyle='none',\n                  marker=next(marker_cyclers[0]),\n                  markersize=3,\n                  markerfacecolor=colour,\n                  markeredgecolor=colour,\n                  label=label)\n\n    # plot expected Poisson rate\n    for i in range(0, len(model.magnitude_bin) - 1):\n        ax.loglog(model.time_values, model.model_line[:, i],\n                  color=next(prop_cyclers[1])['color'],\n                  linewidth=0.5)\n\n    # mark breaks from expected rate\n    for i in range(0, len(model.magnitude_bin) - 1):\n        colour = next(prop_cyclers[2])['color']\n        if np.any(np.isnan(model.model_line[:, i])):\n            continue\n        xmarker = model.end_year - model.completeness_table[i, 0]\n        knee = model.model_line[:, i] > 0.\n        ymarker = 10.0 ** np.interp(np.log10(xmarker),\n                                    np.log10(model.time_values[knee]),\n                                    np.log10(model.model_line[knee, i]))\n        ax.loglog(xmarker, ymarker,\n                  marker=next(marker_cyclers[2]),\n                  markerfacecolor='white',\n                  markeredgecolor=colour)\n\n    ax.legend(loc='center left',\n              bbox_to_anchor=(1, 0.5), frameon=False, fontsize='small')\n    ax.set_xlabel('Time (years)')\n    ax.set_ylabel(\"$\\\\sigma_{\\\\lambda} = \\\\sqrt{\\\\lambda} / \\\\sqrt{T}$\")\n    ax.autoscale(enable=True, axis='both', tight=True)\n\n    # save figure to file\n    if filename is not None:\n        fig.savefig(filename, dpi=dpi, format=filetype)", "response": "Creates the classic Stepp plot for a completed Stepp analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the config file contains all required parameters", "response": "def check_config(config, data):\n    '''Checks that the config file contains all required parameters\n\n    :param dict config:\n        Configuration file\n\n    :returns:\n        Configuration file with all correct parameters\n    '''\n    if 'tolerance' not in config.keys() or not config['tolerance']:\n        config['tolerance'] = 1E-5\n\n    if not config.get('maximum_iterations', None):\n        config['maximum_iterations'] = 1000\n\n    mmin_obs = np.min(data['magnitude'])\n    if config.get('input_mmin', 0) < mmin_obs:\n        config['input_mmin'] = mmin_obs\n\n    if fabs(config['b-value']) < 1E-7:\n        config['b-value'] = 1E-7\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating disaggregation outputs. For instance is TRT Mag_Dist and Mag_Dist.", "response": "def disagg_outputs(value):\n    \"\"\"\n    Validate disaggregation outputs. For instance\n\n    >>> disagg_outputs('TRT Mag_Dist')\n    ['TRT', 'Mag_Dist']\n    >>> disagg_outputs('TRT, Mag_Dist')\n    ['TRT', 'Mag_Dist']\n    \"\"\"\n    values = value.replace(',', ' ').split()\n    for val in values:\n        if val not in disagg.pmf_map:\n            raise ValueError('Invalid disagg output: %s' % val)\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string in TOML format into a GSIM instance", "response": "def gsim(value):\n    \"\"\"\n    Convert a string in TOML format into a GSIM instance\n\n    >>> gsim('[BooreAtkinson2011]')\n    [BooreAtkinson2011]\n    \"\"\"\n    if not value.startswith('['):  # assume the GSIM name\n        value = '[%s]' % value\n    [(gsim_name, kwargs)] = toml.loads(value).items()\n    minimum_distance = float(kwargs.pop('minimum_distance', 0))\n    if gsim_name == 'FromFile':\n        return FromFile()\n    try:\n        gsim_class = registry[gsim_name]\n    except KeyError:\n        raise ValueError('Unknown GSIM: %s' % gsim_name)\n    gs = gsim_class(**kwargs)\n    gs._toml = '\\n'.join(line.strip() for line in value.splitlines())\n    gs.minimum_distance = minimum_distance\n    return gs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of hazard_id values.", "response": "def hazard_id(value):\n    \"\"\"\n    >>> hazard_id('')\n    ()\n    >>> hazard_id('-1')\n    (-1,)\n    >>> hazard_id('42')\n    (42,)\n    >>> hazard_id('42,3')\n    (42, 3)\n    >>> hazard_id('42,3,4')\n    (42, 3, 4)\n    >>> hazard_id('42:3')\n    Traceback (most recent call last):\n       ...\n    ValueError: Invalid hazard_id '42:3'\n    \"\"\"\n    if not value:\n        return ()\n    try:\n        return tuple(map(int, value.split(',')))\n    except Exception:\n        raise ValueError('Invalid hazard_id %r' % value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef utf8(value):\n    try:\n        if isinstance(value, bytes):\n            return value.decode('utf-8')\n        else:\n            return value\n    except Exception:\n        raise ValueError('Not UTF-8: %r' % value)", "response": "r Check that the string is UTF - 8. Returns an encode bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of identifiers separated by whitespace or commas.", "response": "def namelist(value):\n    \"\"\"\n    :param value: input string\n    :returns: list of identifiers separated by whitespace or commas\n\n    >>> namelist('a,b')\n    ['a', 'b']\n    >>> namelist('a1  b_2\\t_c')\n    ['a1', 'b_2', '_c']\n\n    >>> namelist('a1 b_2 1c')\n    Traceback (most recent call last):\n        ...\n    ValueError: List of names containing an invalid name: 1c\n    \"\"\"\n    names = value.replace(',', ' ').split()\n    for n in names:\n        try:\n            name(n)\n        except ValueError:\n            raise ValueError('List of names containing an invalid name:'\n                             ' %s' % n)\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef longitude(value):\n    lon = round(float_(value), 5)\n    if lon > 180.:\n        raise ValueError('longitude %s > 180' % lon)\n    elif lon < -180.:\n        raise ValueError('longitude %s < -180' % lon)\n    return lon", "response": ":param value: input string\n    :returns: longitude float, rounded to 5 digits, i.e. 1 meter maximum\n\n    >>> longitude('0.123456')\n    0.12346"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef latitude(value):\n    lat = round(float_(value), 5)\n    if lat > 90.:\n        raise ValueError('latitude %s > 90' % lat)\n    elif lat < -90.:\n        raise ValueError('latitude %s < -90' % lat)\n    return lat", "response": "Returns the latitude of the base on the base on the base."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lon_lat(value):\n    lon, lat = value.split()\n    return longitude(lon), latitude(lat)", "response": "Returns a tuple of longitude and latitude for a given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string to a point in the national domain.", "response": "def point(value):\n    \"\"\"\n    :param value: a tuple of coordinates as a string (2D or 3D)\n    :returns: a tuple of coordinates as a string (2D or 3D)\n    \"\"\"\n    lst = value.split()\n    dim = len(lst)\n    if dim == 2:\n        return longitude(lst[0]), latitude(lst[1]), 0.\n    elif dim == 3:\n        return longitude(lst[0]), latitude(lst[1]), depth(lst[2])\n    else:\n        raise ValueError('Invalid point format: %s' % value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a non - empty string into a list of lon - lat coordinates.", "response": "def coordinates(value):\n    \"\"\"\n    Convert a non-empty string into a list of lon-lat coordinates.\n\n    >>> coordinates('')\n    Traceback (most recent call last):\n    ...\n    ValueError: Empty list of coordinates: ''\n    >>> coordinates('1.1 1.2')\n    [(1.1, 1.2, 0.0)]\n    >>> coordinates('1.1 1.2, 2.2 2.3')\n    [(1.1, 1.2, 0.0), (2.2, 2.3, 0.0)]\n    >>> coordinates('1.1 1.2 -0.4, 2.2 2.3 -0.5')\n    [(1.1, 1.2, -0.4), (2.2, 2.3, -0.5)]\n    >>> coordinates('0 0 0, 0 0 -1')\n    Traceback (most recent call last):\n    ...\n    ValueError: Found overlapping site #2,  0 0 -1\n    \"\"\"\n    if not value.strip():\n        raise ValueError('Empty list of coordinates: %r' % value)\n    points = []\n    pointset = set()\n    for i, line in enumerate(value.split(','), 1):\n        pnt = point(line)\n        if pnt[:2] in pointset:\n            raise ValueError(\"Found overlapping site #%d, %s\" % (i, line))\n        pointset.add(pnt[:2])\n        points.append(pnt)\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a string with a comma separated list of coordinates into a WKT polygon", "response": "def wkt_polygon(value):\n    \"\"\"\n    Convert a string with a comma separated list of coordinates into\n    a WKT polygon, by closing the ring.\n    \"\"\"\n    points = ['%s %s' % (lon, lat) for lon, lat, dep in coordinates(value)]\n    # close the linear polygon ring by appending the first coord to the end\n    points.append(points[0])\n    return 'POLYGON((%s))' % ', '.join(points)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef positiveint(value):\n    i = int(not_empty(value))\n    if i < 0:\n        raise ValueError('integer %d < 0' % i)\n    return i", "response": ":param value: input string\n    :returns: positive integer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef boolean(value):\n    value = value.strip().lower()\n    try:\n        return _BOOL_DICT[value]\n    except KeyError:\n        raise ValueError('Not a boolean: %s' % value)", "response": "Convert a string such as 0 1 true false t into a boolean."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef probabilities(value, rows=0, cols=0):\n    probs = list(map(probability, value.replace(',', ' ').split()))\n    if rows and cols:\n        probs = numpy.array(probs).reshape((len(rows), len(cols)))\n    return probs", "response": "returns a list of probabilities for a single string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of decreasing probabilities", "response": "def decreasing_probabilities(value):\n    \"\"\"\n    :param value: input string, comma separated or space separated\n    :returns: a list of decreasing probabilities\n\n    >>> decreasing_probabilities('1')\n    Traceback (most recent call last):\n    ...\n    ValueError: Not enough probabilities, found '1'\n    >>> decreasing_probabilities('0.2 0.1')\n    [0.2, 0.1]\n    >>> decreasing_probabilities('0.1 0.2')\n    Traceback (most recent call last):\n    ...\n    ValueError: The probabilities 0.1 0.2 are not in decreasing order\n    \"\"\"\n    probs = probabilities(value)\n    if len(probs) < 2:\n        raise ValueError('Not enough probabilities, found %r' % value)\n    elif sorted(probs, reverse=True) != probs:\n        raise ValueError('The probabilities %s are not in decreasing order'\n                         % value)\n    return probs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef intensity_measure_types(value):\n    imts = []\n    for chunk in value.split(','):\n        imts.append(imt.from_string(chunk.strip()))\n    sorted_imts = sorted(imts, key=lambda im: getattr(im, 'period', 1))\n    if len(distinct(imts)) < len(imts):\n        raise ValueError('Duplicated IMTs in %s' % value)\n    if sorted_imts != imts:\n        raise ValueError('The IMTs are not sorted by period: %s' % value)\n    return [str(imt) for imt in imts]", "response": "Parses the input string into a list of Intensity Measure Type objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_levels(imls, imt, min_iml=1E-10):\n    if len(imls) < 1:\n        raise ValueError('No imls for %s: %s' % (imt, imls))\n    elif imls != sorted(imls):\n        raise ValueError('The imls for %s are not sorted: %s' % (imt, imls))\n    elif len(distinct(imls)) < len(imls):\n        raise ValueError(\"Found duplicated levels for %s: %s\" % (imt, imls))\n    elif imls[0] == 0 and imls[1] <= min_iml:  # apply the cutoff\n        raise ValueError(\"The min_iml %s=%s is larger than the second level \"\n                         \"for %s\" % (imt, min_iml, imls))\n    elif imls[0] == 0 and imls[1] > min_iml:  # apply the cutoff\n        imls[0] = min_iml", "response": "Checks that the given intensity measure and levels are valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the string value into a dictionary of Intensity Measure Type and Levels.", "response": "def intensity_measure_types_and_levels(value):\n    \"\"\"\n    :param value: input string\n    :returns: Intensity Measure Type and Levels dictionary\n\n    >>> intensity_measure_types_and_levels('{\"SA(0.10)\": [0.1, 0.2]}')\n    {'SA(0.1)': [0.1, 0.2]}\n    \"\"\"\n    dic = dictionary(value)\n    for imt_str, imls in dic.items():\n        norm_imt = str(imt.from_string(imt_str))\n        if norm_imt != imt_str:\n            dic[norm_imt] = imls\n            del dic[imt_str]\n        check_levels(imls, imt_str)  # ValueError if the levels are invalid\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loss_ratios(value):\n    dic = dictionary(value)\n    for lt, ratios in dic.items():\n        for ratio in ratios:\n            if not 0 <= ratio <= 1:\n                raise ValueError('Loss ratio %f for loss_type %s is not in '\n                                 'the range [0, 1]' % (ratio, lt))\n        check_levels(ratios, lt)  # ValueError if the levels are invalid\n    return dic", "response": "Returns a dictionary loss_type - > loss ratios"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog scale the log of a single resource set.", "response": "def logscale(x_min, x_max, n):\n    \"\"\"\n    :param x_min: minumum value\n    :param x_max: maximum value\n    :param n: number of steps\n    :returns: an array of n values from x_min to x_max\n    \"\"\"\n    if not (isinstance(n, int) and n > 0):\n        raise ValueError('n must be a positive integer, got %s' % n)\n    if x_min <= 0:\n        raise ValueError('x_min must be positive, got %s' % x_min)\n    if x_max <= x_min:\n        raise ValueError('x_max (%s) must be bigger than x_min (%s)' %\n                         (x_max, x_min))\n    delta = numpy.log(x_max / x_min)\n    return numpy.exp(delta * numpy.arange(n) / (n - 1)) * x_min"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Python dictionary representation of the value.", "response": "def dictionary(value):\n    \"\"\"\n    :param value:\n        input string corresponding to a literal Python object\n    :returns:\n        the Python object\n\n    >>> dictionary('')\n    {}\n    >>> dictionary('{}')\n    {}\n    >>> dictionary('{\"a\": 1}')\n    {'a': 1}\n    >>> dictionary('\"vs30_clustering: true\"')  # an error really done by a user\n    Traceback (most recent call last):\n       ...\n    ValueError: '\"vs30_clustering: true\"' is not a valid Python dictionary\n    >>> dictionary('{\"ls\": logscale(0.01, 2, 5)}')\n    {'ls': [0.01, 0.03760603093086393, 0.14142135623730948, 0.5318295896944986, 1.9999999999999991]}\n    \"\"\"\n    if not value:\n        return {}\n    value = value.replace('logscale(', '(\"logscale\", ')  # dirty but quick\n    try:\n        dic = dict(ast.literal_eval(value))\n    except Exception:\n        raise ValueError('%r is not a valid Python dictionary' % value)\n    for key, val in dic.items():\n        try:\n            has_logscale = (val[0] == 'logscale')\n        except Exception:  # no val[0]\n            continue\n        if has_logscale:\n            dic[key] = list(logscale(*val[1:]))\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Python dictionary key - > number", "response": "def floatdict(value):\n    \"\"\"\n    :param value:\n        input string corresponding to a literal Python number or dictionary\n    :returns:\n        a Python dictionary key -> number\n\n    >>> floatdict(\"200\")\n    {'default': 200}\n\n    >>> text = \"{'active shallow crust': 250., 'default': 200}\"\n    >>> sorted(floatdict(text).items())\n    [('active shallow crust', 250.0), ('default', 200)]\n    \"\"\"\n    value = ast.literal_eval(value)\n    if isinstance(value, (int, float, list)):\n        return {'default': value}\n    dic = {'default': value[next(iter(value))]}\n    dic.update(value)\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a mapping from the maximum distance to the base base", "response": "def maximum_distance(value):\n    \"\"\"\n    :param value:\n        input string corresponding to a valid maximum distance\n    :returns:\n        a IntegrationDistance mapping\n    \"\"\"\n    dic = floatdict(value)\n    for trt, magdists in dic.items():\n        if isinstance(magdists, list):  # could be a scalar otherwise\n            magdists.sort()  # make sure the list is sorted by magnitude\n            for mag, dist in magdists:  # validate the magnitudes\n                magnitude(mag)\n    return IntegrationDistance(dic)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mag_scale_rel(value):\n    value = value.strip()\n    if value not in SCALEREL:\n        raise ValueError(\n            \"'%s' is not a recognized magnitude-scale relationship\" % value)\n    return value", "response": "Returns the value of a Magnitude - Scale relationship in the hazardlib object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a string into a Probability Mass Function.", "response": "def pmf(value):\n    \"\"\"\n    Comvert a string into a Probability Mass Function.\n\n    :param value:\n        a sequence of probabilities summing up to 1 (no commas)\n    :returns:\n        a list of pairs [(probability, index), ...] with index starting from 0\n\n    >>> pmf(\"0.157 0.843\")\n    [(0.157, 0), (0.843, 1)]\n    \"\"\"\n    probs = probabilities(value)\n    if abs(1.-sum(map(float, value.split()))) > 1e-12:\n        raise ValueError('The probabilities %s do not sum up to 1!' % value)\n    return [(p, i) for i, p in enumerate(probs)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure that the sum of the values of the weights is 1.", "response": "def check_weights(nodes_with_a_weight):\n    \"\"\"\n    Ensure that the sum of the values is 1\n\n    :param nodes_with_a_weight: a list of Node objects with a weight attribute\n    \"\"\"\n    weights = [n['weight'] for n in nodes_with_a_weight]\n    if abs(sum(weights) - 1.) > PRECISION:\n        raise ValueError('The weights do not sum up to 1: %s' % weights)\n    return nodes_with_a_weight"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef weights(value):\n    probs = probabilities(value)\n    if abs(sum(probs) - 1.) > PRECISION:\n        raise ValueError('The weights do not sum up to 1: %s' % probs)\n    return probs", "response": "Return a list of weights for a given value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hypo_list(nodes):\n    check_weights(nodes)\n    data = []\n    for node in nodes:\n        data.append([node['alongStrike'], node['downDip'], node['weight']])\n    return numpy.array(data, float)", "response": "returns a numpy array of shape N with strike dip and weight"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slip_list(nodes):\n    check_weights(nodes)\n    data = []\n    for node in nodes:\n        data.append([slip_range(~node), node['weight']])\n    return numpy.array(data, float)", "response": "returns a numpy array with shape N with slip angle and weight"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef posList(value):\n    values = value.split()\n    num_values = len(values)\n    if num_values % 3 and num_values % 2:\n        raise ValueError('Wrong number: nor pairs not triplets: %s' % values)\n    try:\n        return list(map(float_, values))\n    except Exception as exc:\n        raise ValueError('Found a non-float in %s: %s' % (value, exc))", "response": "Convert a string with the form lon1 latN... \u00b0 lonN... \u00b0 depth1... \u00b0 depthN... \u00b0 depthN... \u00b0 depthN... \u00b0 depthN... \u00b0 depthN... \u00b0 depthN..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ab_values(value):\n    a, b = value.split()\n    return positivefloat(a), float_(b)", "response": "Returns a tuple of positive float and float values of the GR magniture - scaling relation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string to a list of integers.", "response": "def integers(value):\n    \"\"\"\n    :param value: input string\n    :returns: non-empty list of integers\n\n    >>> integers('1, 2')\n    [1, 2]\n    >>> integers(' ')\n    Traceback (most recent call last):\n       ...\n    ValueError: Not a list of integers: ' '\n    \"\"\"\n    if '.' in value:\n        raise ValueError('There are decimal points in %s' % value)\n    values = value.replace(',', ' ').split()\n    if not values:\n        raise ValueError('Not a list of integers: %r' % value)\n    try:\n        ints = [int(float(v)) for v in values]\n    except Exception:\n        raise ValueError('Not a list of integers: %r' % value)\n    return ints"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef positiveints(value):\n    ints = integers(value)\n    for val in ints:\n        if val < 0:\n            raise ValueError('%d is negative in %r' % (val, value))\n    return ints", "response": ">>> positiveints('1, -1')\n    Traceback (most recent call last):\n       ...\n    ValueError: -1 is negative in '1, -1'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simple_slice(value):\n    try:\n        start, stop = value.split(':')\n        start = ast.literal_eval(start)\n        stop = ast.literal_eval(stop)\n        if start is not None and stop is not None:\n            assert start < stop\n    except Exception:\n        raise ValueError('invalid slice: %s' % value)\n    return (start, stop)", "response": "Simple slice for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef site_param(dic):\n    new = {}\n    for name, val in dic.items():\n        if name == 'vs30Type':\n            # avoid \"Unrecognized parameter vs30Type\"\n            new['vs30measured'] = val == 'measured'\n        elif name not in site.site_param_dt:\n            raise ValueError('Unrecognized parameter %s' % name)\n        else:\n            new[name] = val\n    return new", "response": "Convert a dictionary site_model_param - > string into a dictionary\n    of valid casted site parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(cls, dic):\n        res = {}\n        for name, text in dic.items():\n            try:\n                p = getattr(cls, name)\n            except AttributeError:\n                logging.warning('Ignored unknown parameter %s', name)\n            else:\n                res[name] = p.validator(text)\n        return res", "response": "Convert a dictionary name - > string into a dictionary name - > value - > a\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_(cls, dic):\n        self = cls.__new__(cls)\n        for k, v in dic.items():\n            setattr(self, k, ast.literal_eval(v))\n        return self", "response": "Build a new ParamSet from a dictionary of string - valued parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the instance dictionary into a sorted list of pairs name valrepr is the string representation of the underlying value of .", "response": "def to_params(self):\n        \"\"\"\n        Convert the instance dictionary into a sorted list of pairs\n        (name, valrepr) where valrepr is the string representation of\n        the underlying value.\n        \"\"\"\n        dic = self.__dict__\n        return [(k, repr(dic[k])) for k in sorted(dic)\n                if not k.startswith('_')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the current object.", "response": "def validate(self):\n        \"\"\"\n        Apply the `is_valid` methods to self and possibly raise a ValueError.\n        \"\"\"\n        # it is important to have the validator applied in a fixed order\n        valids = [getattr(self, valid)\n                  for valid in sorted(dir(self.__class__))\n                  if valid.startswith('is_valid_')]\n        for is_valid in valids:\n            if not is_valid():\n                docstring = '\\n'.join(\n                    line.strip() for line in is_valid.__doc__.splitlines())\n                doc = docstring.format(**vars(self))\n                raise ValueError(doc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an array of equivalent distances in the range self. repi and self. mags", "response": "def get(self, repi, mag):\n        \"\"\"\n        :param repi: an array of epicentral distances in the range self.repi\n        :param mag: a magnitude in the range self.mags\n        :returns: an array of equivalent distances\n        \"\"\"\n        mag_idx = numpy.abs(mag - self.mags).argmin()\n        dists = []\n        for dist in repi:\n            repi_idx = numpy.abs(dist - self.repi).argmin()\n            dists.append(self.reqv[repi_idx, mag_idx])\n        return numpy.array(dists)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setUp(self, mfd_conf):\n        '''\n        Input core configuration parameters as specified in the\n        configuration file\n\n        :param dict mfd_conf:\n            Configuration file containing the following attributes:\n            * 'Model_Weight' - Logic tree weight of model type (float)\n            * 'MFD_spacing' - Width of MFD bin (float)\n            * 'Minimum_Magnitude' - Minimum magnitude of activity rates (float)\n            * 'Maximum_Magnitude' - Characteristic magnituded (float)\n            (if not defined will use scaling relation)\n            * 'Maximum_Magnitude_Uncertainty' - Uncertainty on\n            maximum magnitude\n            (If not defined and the MSR has a sigma term then this will be\n            taken from sigma)\n            * 'Lower_Bound' - Lower bound in terms of number of sigma (float)\n            * 'Upper_Bound' - Upper bound in terms of number of sigma (float)\n            * 'Sigma' - Standard deviation (in magnitude units) of distribution\n        '''\n        self.mfd_model = 'Characteristic'\n        self.mfd_weight = mfd_conf['Model_Weight']\n        self.bin_width = mfd_conf['MFD_spacing']\n        self.mmin = None\n        self.mmax = None\n        self.mmax_sigma = None\n        self.lower_bound = mfd_conf['Lower_Bound']\n        self.upper_bound = mfd_conf['Upper_Bound']\n        self.sigma = mfd_conf['Sigma']\n        self.occurrence_rate = None", "response": "This method sets up the internal properties of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the activity rate of the fault and returns the MFD.", "response": "def get_mfd(self, slip, area, shear_modulus=30.0):\n        '''\n        Calculates activity rate on the fault\n\n        :param float slip:\n            Slip rate in mm/yr\n\n        :param fault_width:\n            Width of the fault (km)\n\n        :param float disp_length_ratio:\n            Displacement to length ratio (dimensionless)\n\n        :param float shear_modulus:\n            Shear modulus of the fault (GPa)\n\n        :returns:\n            * Minimum Magnitude (float)\n            * Bin width (float)\n            * Occurrence Rates (numpy.ndarray)\n        '''\n        # Working in Nm so convert:  shear_modulus - GPa -> Nm\n        # area - km ** 2. -> m ** 2.\n        # slip - mm/yr -> m/yr\n        moment_rate = (shear_modulus * 1.E9) * (area * 1.E6) * (slip / 1000.)\n        moment_mag = _scale_moment(self.mmax, in_nm=True)\n        characteristic_rate = moment_rate / moment_mag\n        if self.sigma and (fabs(self.sigma) > 1E-5):\n            self.mmin = self.mmax + (self.lower_bound * self.sigma)\n            mag_upper = self.mmax + (self.upper_bound * self.sigma)\n            mag_range = np.arange(self.mmin,\n                                  mag_upper + self.bin_width,\n                                  self.bin_width)\n            self.occurrence_rate = characteristic_rate * (\n                truncnorm.cdf(mag_range + (self.bin_width / 2.),\n                              self.lower_bound, self.upper_bound,\n                              loc=self.mmax, scale=self.sigma) -\n                truncnorm.cdf(mag_range - (self.bin_width / 2.),\n                              self.lower_bound, self.upper_bound,\n                              loc=self.mmax, scale=self.sigma))\n        else:\n            # Returns only a single rate\n            self.mmin = self.mmax\n            self.occurrence_rate = np.array([characteristic_rate], dtype=float)\n\n        return self.mmin, self.bin_width, self.occurrence_rate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the mean and standard deviation for a specific intensity measure type.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n        C = self.COEFFS[imt]\n\n        mean = (self._get_magnitude_scaling_term(C, rup.mag) +\n                self._get_distance_scaling_term(C, rup.mag, dists.rrup) +\n                self._get_style_of_faulting_term(C, rup.rake) +\n                self._get_site_scaling_term(C, sites.vs30))\n\n        stddevs = self._get_stddevs(imt,\n                                    rup.mag,\n                                    len(dists.rrup),\n                                    stddev_types)\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the magnitude scaling term defined in equation 3. 1. 2.", "response": "def _get_magnitude_scaling_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term defined in equation 3\n        \"\"\"\n        if mag < 6.75:\n            return C[\"a1_lo\"] + C[\"a2_lo\"] * mag + C[\"a3\"] *\\\n                ((8.5 - mag) ** 2.0)\n        else:\n            return C[\"a1_hi\"] + C[\"a2_hi\"] * mag + C[\"a3\"] *\\\n                ((8.5 - mag) ** 2.0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_distance_scaling_term(self, C, mag, rrup):\n        if mag < 6.75:\n            mag_factor = -(C[\"b1_lo\"] + C[\"b2_lo\"] * mag)\n        else:\n            mag_factor = -(C[\"b1_hi\"] + C[\"b2_hi\"] * mag)\n        return mag_factor * np.log(rrup + 10.0) + (C[\"gamma\"] * rrup)", "response": "Returns the magnitude dependent distance scaling term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_site_scaling_term(self, C, vs30):\n        site_amp = C[\"xi\"] * np.log(1200.0) * np.ones(len(vs30))\n        idx = vs30 < 1200.0\n        site_amp[idx] = C[\"xi\"] * np.log(vs30[idx])\n        return site_amp", "response": "Returns the site scaling term in equation 1 page 970."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the standard deviation for the entry class entry.", "response": "def _get_stddevs(self, imt, mag, n_sites, stddev_types):\n        \"\"\"\n        The standard error (assumed equivalent to total standard deviation)\n        is defined as a function of magnitude and period (equation 4,\n        page 1168). For magnitudes lower than 5.0 the standard deviation is\n        equal to that for the case in which magnitude is 5.0. For short\n        periods (T < 0.05), including PGA, the standard deviation is\n        assumed to be equal to the case in which T = 0.05, whilst for long\n        periods (T > 3.0) it is assumed to be equal to the case in which\n        T = 3.0\n        \"\"\"\n        if mag < 5.0:\n            stddev_mag = 5.0\n        else:\n            stddev_mag = mag\n\n        if imt.name == \"PGA\" or imt.period < 0.05:\n            total_sigma = 1.18 + 0.035 * np.log(0.05) - 0.06 * stddev_mag\n        elif imt.period > 3.0:\n            total_sigma = 1.18 + 0.035 * np.log(3.0) - 0.06 * stddev_mag\n        else:\n            total_sigma = 1.18 + 0.035 * np.log(imt.period) - 0.06 * stddev_mag\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(total_sigma + np.zeros(n_sites, dtype=float))\n        return stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extract dictionaries of coefficients specific to required\n        # intensity measure type and for PGA\n        C = self.COEFFS[imt]\n        C_PGA = self.COEFFS[PGA()]\n\n        # compute median pga on rock (vs30=1100), needed for site response\n        # term calculation\n        pga1100 = np.exp(self._compute_imt1100(PGA(), sites, rup, dists))\n\n        mean = (self._compute_base_term(C, rup, dists) +\n                self._compute_faulting_style_term(C, rup) +\n                self._compute_site_response_term(C, imt, sites, pga1100) +\n                self._compute_hanging_wall_term(C, dists, rup) +\n                self._compute_top_of_rupture_depth_term(C, rup) +\n                self._compute_large_distance_term(C, dists, rup) +\n                self._compute_soil_depth_term(C, imt, sites.z1pt0, sites.vs30))\n\n        stddevs = self._get_stddevs(C, C_PGA, pga1100, rup, sites,\n                                    stddev_types)\n\n        return mean, stddevs", "response": "This method calculates the mean and standard deviation of the site response page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_base_term(self, C, rup, dists):\n        c1 = self.CONSTS['c1']\n        R = np.sqrt(dists.rrup ** 2 + self.CONSTS['c4'] ** 2)\n\n        base_term = (C['a1'] +\n                     C['a8'] * ((8.5 - rup.mag) ** 2) +\n                     (C['a2'] + self.CONSTS['a3'] * (rup.mag - c1)) *\n                     np.log(R))\n\n        if rup.mag <= c1:\n            return base_term + self.CONSTS['a4'] * (rup.mag - c1)\n        else:\n            return base_term + self.CONSTS['a5'] * (rup.mag - c1)", "response": "Compute and return base model term in equation 1 page 74."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes and return faulting style term in equation 1 page 74.", "response": "def _compute_faulting_style_term(self, C, rup):\n        \"\"\"\n        Compute and return faulting style term, that is the sum of the second\n        and third terms in equation 1, page 74.\n        \"\"\"\n        # ranges of rake values for each faulting mechanism are specified in\n        # table 2, page 75\n        return (C['a12'] * float(rup.rake > 30 and rup.rake < 150) +\n                C['a13'] * float(rup.rake > -120 and rup.rake < -60))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and return site response model term that is the fifth term in equation 1 page 74.", "response": "def _compute_site_response_term(self, C, imt, sites, pga1100):\n        \"\"\"\n        Compute and return site response model term, that is the fifth term\n        in equation 1, page 74.\n        \"\"\"\n        site_resp_term = np.zeros_like(sites.vs30)\n\n        vs30_star, _ = self._compute_vs30_star_factor(imt, sites.vs30)\n        vlin, c, n = C['VLIN'], self.CONSTS['c'], self.CONSTS['n']\n        a10, b = C['a10'], C['b']\n\n        idx = sites.vs30 < vlin\n        arg = vs30_star[idx] / vlin\n        site_resp_term[idx] = (a10 * np.log(arg) -\n                               b * np.log(pga1100[idx] + c) +\n                               b * np.log(pga1100[idx] + c * (arg ** n)))\n\n        idx = sites.vs30 >= vlin\n        site_resp_term[idx] = (a10 + b * n) * np.log(vs30_star[idx] / vlin)\n\n        return site_resp_term"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_hanging_wall_term(self, C, dists, rup):\n        if rup.dip == 90.0:\n            return np.zeros_like(dists.rx)\n        else:\n            idx = dists.rx > 0\n            Fhw = np.zeros_like(dists.rx)\n            Fhw[idx] = 1\n\n            # equation 8, page 77\n            T1 = np.zeros_like(dists.rx)\n            idx1 = (dists.rjb < 30.0) & (idx)\n            T1[idx1] = 1.0 - dists.rjb[idx1] / 30.0\n\n            # equation 9, page 77\n            T2 = np.ones_like(dists.rx)\n            idx2 = ((dists.rx <= rup.width * np.cos(np.radians(rup.dip))) &\n                    (idx))\n            T2[idx2] = (0.5 + dists.rx[idx2] /\n                        (2 * rup.width * np.cos(np.radians(rup.dip))))\n\n            # equation 10, page 78\n            T3 = np.ones_like(dists.rx)\n            idx3 = (dists.rx < rup.ztor) & (idx)\n            T3[idx3] = dists.rx[idx3] / rup.ztor\n\n            # equation 11, page 78\n            if rup.mag <= 6.0:\n                T4 = 0.0\n            elif rup.mag > 6 and rup.mag < 7:\n                T4 = rup.mag - 6\n            else:\n                T4 = 1.0\n\n            # equation 5, in AS08_NGA_errata.pdf\n            if rup.dip >= 30:\n                T5 = 1.0 - (rup.dip - 30.0) / 60.0\n            else:\n                T5 = 1.0\n\n            return Fhw * C['a14'] * T1 * T2 * T3 * T4 * T5", "response": "Compute and return the hanging wall model term in the resource table page 74."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and return the top of rupture depth term in equation 1 page 74.", "response": "def _compute_top_of_rupture_depth_term(self, C, rup):\n        \"\"\"\n        Compute and return top of rupture depth term, that is the seventh term\n        in equation 1, page 74. The calculation of this term is explained in\n        paragraph 'Depth-to-Top of Rupture Model', page 78.\n        \"\"\"\n        if rup.ztor >= 10.0:\n            return C['a16']\n        else:\n            return C['a16'] * rup.ztor / 10.0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes and return soil depth model term in the page 74.", "response": "def _compute_soil_depth_term(self, C, imt, z1pt0, vs30):\n        \"\"\"\n        Compute and return soil depth model term, that is the 9-th term in\n        equation 1, page 74. The calculation of this term is explained in\n        paragraph 'Soil Depth Model', page 79.\n        \"\"\"\n        a21 = self._compute_a21_factor(C, imt, z1pt0, vs30)\n        a22 = self._compute_a22_factor(imt)\n        median_z1pt0 = self._compute_median_z1pt0(vs30)\n\n        soil_depth_term = a21 * np.log((z1pt0 + self.CONSTS['c2']) /\n                                       (median_z1pt0 + self.CONSTS['c2']))\n\n        idx = z1pt0 >= 200\n        soil_depth_term[idx] += a22 * np.log(z1pt0[idx] / 200)\n\n        return soil_depth_term"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_imt1100(self, imt, sites, rup, dists):\n        vs30_1100 = np.zeros_like(sites.vs30) + 1100\n        vs30_star, _ = self._compute_vs30_star_factor(imt, vs30_1100)\n        C = self.COEFFS[imt]\n        mean = (self._compute_base_term(C, rup, dists) +\n                self._compute_faulting_style_term(C, rup) +\n                self._compute_hanging_wall_term(C, dists, rup) +\n                self._compute_top_of_rupture_depth_term(C, rup) +\n                self._compute_large_distance_term(C, dists, rup) +\n                self._compute_soil_depth_term(C, imt, sites.z1pt0, vs30_1100) +\n                # this is the site response term in case of vs30=1100\n                ((C['a10'] + C['b'] * self.CONSTS['n']) *\n                np.log(vs30_star / C['VLIN'])))\n\n        return mean", "response": "Compute and return mean value for rock conditions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_stddevs(self, C, C_PGA, pga1100, rup, sites, stddev_types):\n        std_intra = self._compute_intra_event_std(C, C_PGA, pga1100, rup.mag,\n                                                  sites.vs30,\n                                                  sites.vs30measured)\n        std_inter = self._compute_inter_event_std(C, C_PGA, pga1100, rup.mag,\n                                                  sites.vs30)\n        stddevs = []\n        for stddev_type in stddev_types:\n            assert stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n            if stddev_type == const.StdDev.TOTAL:\n                stddevs.append(np.sqrt(std_intra ** 2 + std_inter ** 2))\n            elif stddev_type == const.StdDev.INTRA_EVENT:\n                stddevs.append(std_intra)\n            elif stddev_type == const.StdDev.INTER_EVENT:\n                stddevs.append(std_inter)\n        return stddevs", "response": "Returns the standard deviations as described in paragraph Equations for the standard deviations page 81."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the intra event standard deviation in equation 24 page 74.", "response": "def _compute_intra_event_std(self, C, C_PGA, pga1100, mag, vs30,\n                                 vs30measured):\n        \"\"\"\n        Compute intra event standard deviation (equation 24) as described\n        in the errata and not in the original paper.\n        \"\"\"\n        sigma_b = self._compute_sigma_b(C, mag, vs30measured)\n        sigma_b_pga = self._compute_sigma_b(C_PGA, mag, vs30measured)\n        delta_amp = self._compute_partial_derivative_site_amp(C, pga1100, vs30)\n\n        std_intra = np.sqrt(sigma_b ** 2 + self.CONSTS['sigma_amp'] ** 2 +\n                            (delta_amp ** 2) * (sigma_b_pga ** 2) +\n                            2 * delta_amp * sigma_b * sigma_b_pga * C['rho'])\n\n        return std_intra"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_inter_event_std(self, C, C_PGA, pga1100, mag, vs30):\n        tau_0 = self._compute_std_0(C['s3'], C['s4'], mag)\n        tau_b_pga = self._compute_std_0(C_PGA['s3'], C_PGA['s4'], mag)\n        delta_amp = self._compute_partial_derivative_site_amp(C, pga1100, vs30)\n\n        std_inter = np.sqrt(tau_0 ** 2 + (delta_amp ** 2) * (tau_b_pga ** 2) +\n                            2 * delta_amp * tau_0 * tau_b_pga * C['rho'])\n\n        return std_inter", "response": "Compute inter event standard deviation in equation 25 page 82."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_sigma_b(self, C, mag, vs30measured):\n        sigma_0 = self._compute_sigma_0(C, mag, vs30measured)\n        sigma_amp = self.CONSTS['sigma_amp']\n\n        return np.sqrt(sigma_0 ** 2 - sigma_amp ** 2)", "response": "Compute the sigma b term in equation 23 page 81."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the standard deviation in terms of the standard deviation in terms of the site - level vs30measured attribute.", "response": "def _compute_sigma_0(self, C, mag, vs30measured):\n        \"\"\"\n        Equation 27, page 82.\n        \"\"\"\n        s1 = np.zeros_like(vs30measured, dtype=float)\n        s2 = np.zeros_like(vs30measured, dtype=float)\n\n        idx = vs30measured == 1\n        s1[idx] = C['s1mea']\n        s2[idx] = C['s2mea']\n\n        idx = vs30measured == 0\n        s1[idx] = C['s1est']\n        s2[idx] = C['s2est']\n\n        return self._compute_std_0(s1, s2, mag)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_std_0(self, c1, c2, mag):\n        if mag < 5:\n            return c1\n        elif mag >= 5 and mag <= 7:\n            return c1 + (c2 - c1) * (mag - 5) / 2\n        else:\n            return c2", "response": "Compute standard deviation for the current log entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the partial derivative of site amplification term with respect to the notange site amplification term with respect to pga1100 and vs30.", "response": "def _compute_partial_derivative_site_amp(self, C, pga1100, vs30):\n        \"\"\"\n        Partial derivative of site amplification term with respect to\n        PGA on rock (equation 26), as described in the errata and not\n        in the original paper.\n        \"\"\"\n        delta_amp = np.zeros_like(vs30)\n        vlin = C['VLIN']\n        c = self.CONSTS['c']\n        b = C['b']\n        n = self.CONSTS['n']\n\n        idx = vs30 < vlin\n        delta_amp[idx] = (- b * pga1100[idx] / (pga1100[idx] + c) +\n                          b * pga1100[idx] / (pga1100[idx] + c *\n                          ((vs30[idx] / vlin) ** n)))\n\n        return delta_amp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_a21_factor(self, C, imt, z1pt0, vs30):\n        e2 = self._compute_e2_factor(imt, vs30)\n        a21 = e2.copy()\n\n        vs30_star, v1 = self._compute_vs30_star_factor(imt, vs30)\n        median_z1pt0 = self._compute_median_z1pt0(vs30)\n\n        numerator = ((C['a10'] + C['b'] * self.CONSTS['n']) *\n                     np.log(vs30_star / np.min([v1, 1000])))\n        denominator = np.log((z1pt0 + self.CONSTS['c2']) /\n                             (median_z1pt0 + self.CONSTS['c2']))\n\n        idx = numerator + e2 * denominator < 0\n        a21[idx] = - numerator[idx] / denominator[idx]\n\n        idx = vs30 >= 1000\n        a21[idx] = 0.0\n\n        return a21", "response": "Compute and return a21 factor equation 18 page 80."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_vs30_star_factor(self, imt, vs30):\n        v1 = self._compute_v1_factor(imt)\n        vs30_star = vs30.copy()\n        vs30_star[vs30_star >= v1] = v1\n\n        return vs30_star, v1", "response": "Compute and return vs30 star factor equation 5 page 77."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and return v1 factor equation 6 page 77.", "response": "def _compute_v1_factor(self, imt):\n        \"\"\"\n        Compute and return v1 factor, equation 6, page 77.\n        \"\"\"\n        if imt.name == \"SA\":\n            t = imt.period\n            if t <= 0.50:\n                v1 = 1500.0\n            elif t > 0.50 and t <= 1.0:\n                v1 = np.exp(8.0 - 0.795 * np.log(t / 0.21))\n            elif t > 1.0 and t < 2.0:\n                v1 = np.exp(6.76 - 0.297 * np.log(t))\n            else:\n                v1 = 700.0\n        elif imt.name == \"PGA\":\n            v1 = 1500.0\n        else:\n            # this is for PGV\n            v1 = 862.0\n\n        return v1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_e2_factor(self, imt, vs30):\n        e2 = np.zeros_like(vs30)\n\n        if imt.name == \"PGV\":\n            period = 1\n        elif imt.name == \"PGA\":\n            period = 0\n        else:\n            period = imt.period\n\n        if period < 0.35:\n            return e2\n        else:\n            idx = vs30 <= 1000\n            if period >= 0.35 and period <= 2.0:\n                e2[idx] = (-0.25 * np.log(vs30[idx] / 1000) *\n                           np.log(period / 0.35))\n            elif period > 2.0:\n                e2[idx] = (-0.25 * np.log(vs30[idx] / 1000) *\n                           np.log(2.0 / 0.35))\n            return e2", "response": "Compute and return e2 factor for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute and return median z1pt0", "response": "def _compute_median_z1pt0(self, vs30):\n        \"\"\"\n        Compute and return median z1pt0 (in m), equation 17, pqge 79.\n        \"\"\"\n        z1pt0_median = np.zeros_like(vs30) + 6.745\n\n        idx = np.where((vs30 >= 180.0) & (vs30 <= 500.0))\n        z1pt0_median[idx] = 6.745 - 1.35 * np.log(vs30[idx] / 180.0)\n\n        idx = vs30 > 500.0\n        z1pt0_median[idx] = 5.394 - 4.48 * np.log(vs30[idx] / 500.0)\n\n        return np.exp(z1pt0_median)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_a22_factor(self, imt):\n        if imt.name == 'PGV':\n            return 0.0\n        period = imt.period\n        if period < 2.0:\n            return 0.0\n        else:\n            return 0.0625 * (period - 2.0)", "response": "Compute and return the a22 factor equation 20 page 80."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        mean = np.zeros_like(sites.vs30)\n        stddevs = [np.zeros_like(sites.vs30) for _ in stddev_types]\n\n        idx_rock = sites.vs30 >= self.ROCK_VS30\n        idx_soil = sites.vs30 < self.ROCK_VS30\n\n        if idx_rock.any():\n            C = self.COEFFS_ROCK[imt]\n            self._compute_mean(C, self.CONSTS['A1_rock'],\n                               self.CONSTS['A2_rock'], self.CONSTS['A3_rock'],\n                               self.CONSTS['A4_rock'], self.CONSTS['A5_rock'],\n                               self.CONSTS['A6_rock'], rup.mag, rup.hypo_depth,\n                               dists.rrup, mean, idx_rock)\n            self._compute_std(C, rup.mag, stddevs, idx_rock)\n\n            if imt == SA(period=4.0, damping=5.0):\n                mean = mean / 0.399\n\n        if idx_soil.any():\n            C = self.COEFFS_SOIL[imt]\n            self._compute_mean(C, self.CONSTS['A1_soil'],\n                               self.CONSTS['A2_soil'], self.CONSTS['A3_soil'],\n                               self.CONSTS['A4_soil'], self.CONSTS['A5_soil'],\n                               self.CONSTS['A6_soil'], rup.mag, rup.hypo_depth,\n                               dists.rrup, mean, idx_soil)\n            self._compute_std(C, rup.mag, stddevs, idx_soil)\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_mean(self, C, A1, A2, A3, A4, A5, A6, mag, hypo_depth,\n                      rrup, mean, idx):\n        \"\"\"\n        Compute mean for subduction interface events, as explained in table 2,\n        page 67.\n        \"\"\"\n        mean[idx] = (A1 + A2 * mag + C['C1'] + C['C2'] * (A3 - mag) ** 3 +\n                     C['C3'] * np.log(rrup[idx] + A4 * np.exp(A5 * mag)) +\n                     A6 * hypo_depth)", "response": "Compute mean for subduction interface events as explained in table 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_std(self, C, mag, stddevs, idx):\n        if mag > 8.0:\n            mag = 8.0\n\n        for stddev in stddevs:\n            stddev[idx] += C['C4'] + C['C5'] * mag", "response": "Compute total standard deviation as explained in table 2 page 67."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsee :meth:`superclass method <.base.GroundShakingIntensityModel.get_mean_and_stddevs>` for spec of input and result values.", "response": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"\n        mean, stddevs = super().get_mean_and_stddevs(\n            sites, rup, dists, imt, stddev_types)\n\n        # this is the firm ground adjustment\n        mean += np.log(1.162)\n\n        return mean, stddevs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_basemap(self):\n        '''\n        Creates the map according to the input configuration\n        '''\n        if self.config['min_lon'] >= self.config['max_lon']:\n            raise ValueError('Upper limit of long is smaller than lower limit')\n\n        if self.config['min_lon'] >= self.config['max_lon']:\n            raise ValueError('Upper limit of long is smaller than lower limit')\n        # Corners of the map\n        lowcrnrlat = self.config['min_lat']\n        lowcrnrlon = self.config['min_lon']\n        uppcrnrlat = self.config['max_lat']\n        uppcrnrlon = self.config['max_lon']\n        if 'resolution' not in self.config.keys():\n            self.config['resolution'] = 'l'\n\n        lat0 = lowcrnrlat + ((uppcrnrlat - lowcrnrlat) / 2)\n        lon0 = lowcrnrlon + ((uppcrnrlon - lowcrnrlon) / 2)\n        if (uppcrnrlat - lowcrnrlat) >= (uppcrnrlon - lowcrnrlon):\n            fig_aspect = PORTRAIT_ASPECT\n        else:\n            fig_aspect = LANDSCAPE_ASPECT\n\n        if self.ax is None:\n            self.fig, self.ax = plt.subplots(figsize=fig_aspect,\n                                             facecolor='w',\n                                             edgecolor='k')\n        else:\n            self.fig = self.ax.get_figure()\n\n        if self.title:\n            self.ax.set_title(self.title, fontsize=16)\n        parallels = np.arange(-90., 90., self.lat_lon_spacing)\n        meridians = np.arange(0., 360., self.lat_lon_spacing)\n\n        # Build Map\n        # Do not import Basemap at top level since it's an optional feature\n        # and it would break doctests\n        from mpl_toolkits.basemap import Basemap\n        self.m = Basemap(\n            llcrnrlon=lowcrnrlon, llcrnrlat=lowcrnrlat,\n            urcrnrlon=uppcrnrlon, urcrnrlat=uppcrnrlat,\n            projection='stere', resolution=self.config['resolution'],\n            area_thresh=1000.0, lat_0=lat0, lon_0=lon0, ax=self.ax)\n        self.m.drawcountries()\n        self.m.drawmapboundary()\n        self.m.drawcoastlines()\n        self.m.drawstates()\n        self.m.drawparallels(parallels, labels=[1, 0, 0, 0], fontsize=12)\n        self.m.drawmeridians(meridians, labels=[0, 0, 0, 1], fontsize=12)\n        self.m.fillcontinents(color='wheat')", "response": "Builds the basemap for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the figure to a file.", "response": "def savemap(self, filename, filetype='png', papertype=\"a4\"):\n        \"\"\"\n        Save the figure\n        \"\"\"\n        self.fig.savefig(filename,\n                         dpi=self.dpi,\n                         format=filetype,\n                         papertype=papertype)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_catalogue(self, catalogue, overlay=False):\n        '''\n        :param catalogue:\n            Earthquake catalogue as instance of\n            :class:`openquake.hmtk.seismicity.catalogue.Catalogue`\n\n        :param dict config:\n            Configuration parameters of the algorithm, containing the\n            following information:\n            'min_lat' Minimum value of latitude (in degrees, float)\n            'max_lat' Minimum value of longitude (in degrees, float)\n            (min_lat, min_lon) Defines the inferior corner of the map\n            'min_lon' Maximum value of latitude (in degrees, float)\n            'max_lon' Maximum value of longitude (in degrees, float)\n            (min_lon, max_lon) Defines the upper corner of the map\n\n        :returns:\n            Figure with the spatial distribution of the events.\n        '''\n        # Magnitudes bins and minimum marrker size\n        # min_mag = np.min(catalogue.data['magnitude'])\n        # max_mag = np.max(catalogue.data['magnitude'])\n        con_min = np.where(np.array([symb[0] for symb in DEFAULT_SYMBOLOGY]) <\n                           np.min(catalogue.data['magnitude']))[0]\n        con_max = np.where(np.array([symb[1] for symb in DEFAULT_SYMBOLOGY]) >\n                           np.max(catalogue.data['magnitude']))[0]\n        if len(con_min) == 1:\n            min_loc = con_min[0]\n        else:\n            min_loc = con_min[-1]\n        if len(con_max) == 1:\n            max_loc = con_max[0]\n        else:\n            max_loc = con_max[1]\n        # min_loc = np.where(np.array([symb[0] for symb in DEFAULT_SYMBOLOGY])\n        #                   < np.min(catalogue.data['magnitude']))[0][-1]\n        # max_loc = np.where(np.array([symb[1] for symb in DEFAULT_SYMBOLOGY])\n        #                   > np.max(catalogue.data['magnitude']))[0][1]\n        symbology = DEFAULT_SYMBOLOGY[min_loc:max_loc]\n        for sym in symbology:\n            # Create legend string\n            if np.isinf(sym[0]):\n                leg_str = 'M < %5.2f' % sym[1]\n            elif np.isinf(sym[1]):\n                leg_str = 'M >= %5.2f' % sym[0]\n            else:\n                leg_str = '%5.2f <= M < %5.2f' % (sym[0], sym[1])\n            idx = np.logical_and(catalogue.data['magnitude'] >= sym[0],\n                                 catalogue.data['magnitude'] < sym[1])\n            mag_size = 1.2 * np.min([sym[0] + 0.5, sym[1] - 0.5])\n            x, y = self.m(catalogue.data['longitude'][idx],\n                          catalogue.data['latitude'][idx])\n            self.m.plot(x, y, sym[2], markersize=mag_size, label=leg_str)\n\n        self.ax.legend(bbox_to_anchor=LEGEND_OFFSET)\n        if self.title:\n            self.ax.set_title(self.title, fontsize=16)\n        if not overlay:\n            plt.show()", "response": "Adds the given Earthquake catalogue to the map."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the area source as a matplotlib plot.", "response": "def _plot_area_source(self, source, border='k-', border_width=1.0):\n        \"\"\"\n        Plots the area source\n        :param source:\n            Area source as instance of :class: mtkAreaSource\n        :param str border:\n            Line properties of border (see matplotlib documentation for detail)\n        :param float border_width:\n            Line width of border (see matplotlib documentation for detail)\n        \"\"\"\n        lons = np.hstack([source.geometry.lons, source.geometry.lons[0]])\n        lats = np.hstack([source.geometry.lats, source.geometry.lats[0]])\n        x, y = self.m(lons, lats)\n        self.m.plot(x, y, border, linewidth=border_width)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the area source as a point.", "response": "def _plot_point_source(self, source, point_marker='ks', point_size=2.0):\n        \"\"\"\n        Plots the area source\n        :param source:\n            Area source as instance of :class: mtkPointSource\n        :param str point_marker:\n            Marker style for point (see matplotlib documentation for detail)\n        :param float marker size for point:\n            Line width of border (see matplotlib documentation for detail)\n        \"\"\"\n        x, y = self.m(source.geometry.longitude, source.geometry.latitude)\n        self.m.plot(x, y, point_marker, markersize=point_size)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the simple fault source as a composite of the fault trace and the surface projection of the fault.", "response": "def _plot_simple_fault(self, source, border='k-', border_width=1.0):\n        \"\"\"\n        Plots the simple fault source as a composite of the fault trace\n        and the surface projection of the fault.\n        :param source:\n            Fault source as instance of :class: mtkSimpleFaultSource\n        :param str border:\n            Line properties of border (see matplotlib documentation for detail)\n        :param float border_width:\n            Line width of border (see matplotlib documentation for detail)\n        \"\"\"\n        # Get the trace\n        trace_lons = np.array([pnt.longitude\n                               for pnt in source.fault_trace.points])\n        trace_lats = np.array([pnt.latitude\n                               for pnt in source.fault_trace.points])\n        surface_projection = _fault_polygon_from_mesh(source)\n        # Plot surface projection first\n        x, y = self.m(surface_projection[:, 0], surface_projection[:, 1])\n        self.m.plot(x, y, border, linewidth=border_width)\n        # Plot fault trace\n        x, y = self.m(trace_lons, trace_lats)\n        self.m.plot(x, y, border, linewidth=1.3 * border_width)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _plot_complex_fault(self, source, border='k-', border_width=1.0,\n                            min_depth=0., max_depth=None, alpha=1.0):\n        \"\"\"\n        Plots the simple fault source as a composite of the fault trace\n        and the surface projection of the fault.\n        :param source:\n            Fault source as instance of :class: mtkSimpleFaultSource\n        :param str border:\n            Line properties of border (see matplotlib documentation for detail)\n        :param float border_width:\n            Line width of border (see matplotlib documentation for detail)\n        \"\"\"\n        if not max_depth:\n            max_depth = 70.\n        # Get outline\n        top_edge = np.column_stack([source.geometry.mesh.lons[0],\n                                    source.geometry.mesh.lats[0]])\n\n        bottom_edge = np.column_stack([source.geometry.mesh.lons[-1][::-1],\n                                       source.geometry.mesh.lats[-1][::-1]])\n        outline = np.vstack([top_edge, bottom_edge, top_edge[0, :]])\n        lons = source.geometry.mesh.lons.flatten()\n        lats = source.geometry.mesh.lats.flatten()\n        depths = source.geometry.mesh.depths.flatten()\n        norm = Normalize(vmin=min_depth, vmax=max_depth)\n        x1, y1 = self.m(lons, lats)\n        self.m.scatter(x1, y1,\n                       marker=\".\",\n                       s=20,\n                       c=depths,\n                       norm=norm,\n                       cmap=\"jet_r\",\n                       alpha=alpha,\n                       linewidths=0.0,\n                       zorder=4)\n        # Plot border\n        x2, y2 = self.m(outline[:, 0], outline[:, 1])\n        self.m.plot(x2, y2, border, linewidth=border_width)", "response": "Plots a complex fault source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a source model to the map.", "response": "def add_source_model(\n            self, model, area_border='k-', border_width=1.0,\n            point_marker='ks', point_size=2.0, overlay=False, min_depth=0.,\n            max_depth=None, alpha=1.0):\n        \"\"\"\n        Adds a source model to the map\n\n        :param model:\n            Source model of mixed typologies as instance of :class:\n            openquake.hmtk.sources.source_model.mtkSourceModel\n        \"\"\"\n        for source in model.sources:\n            if isinstance(source, mtkAreaSource):\n                self._plot_area_source(source, area_border, border_width)\n            elif isinstance(source, mtkPointSource):\n                self._plot_point_source(source, point_marker, point_size)\n            elif isinstance(source, mtkComplexFaultSource):\n                self._plot_complex_fault(source, area_border, border_width,\n                                         min_depth, max_depth, alpha)\n            elif isinstance(source, mtkSimpleFaultSource):\n                self._plot_simple_fault(source, area_border, border_width)\n            else:\n                pass\n        if not overlay:\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a set of points to a map with a fixed size but colour scaled by the data.", "response": "def add_colour_scaled_points(self, longitude, latitude, data, shape='s',\n                                 alpha=1.0, size=20, norm=None, overlay=False):\n        \"\"\"\n        Overlays a set of points on a map with a fixed size but colour scaled\n        according to the data\n\n        :param np.ndarray longitude:\n            Longitude\n        :param np.ndarray latitude:\n            Latitude\n        :param np.ndarray data:\n            Data for plotting\n        :param str shape:\n            Marker style\n        :param float alpha:\n            Sets the transparency of the marker (0 for transparent, 1 opaque)\n        :param int size:\n            Marker size\n        :param norm:\n            Normalisation as instance of :class: matplotlib.colors.Normalize\n        \"\"\"\n        if not norm:\n            norm = Normalize(vmin=np.min(data), vmax=np.max(data))\n        x, y, = self.m(longitude, latitude)\n        mappable = self.m.scatter(x, y,\n                                  marker=shape,\n                                  s=size,\n                                  c=data,\n                                  norm=norm,\n                                  alpha=alpha,\n                                  linewidths=0.0,\n                                  zorder=4)\n        self.m.colorbar(mappable=mappable, fig=self.fig, ax=self.ax)\n        if not overlay:\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding points with size scaled according to the data.", "response": "def add_size_scaled_points(\n            self, longitude, latitude, data, shape='o',\n            logplot=False, alpha=1.0, colour='b', smin=2.0, sscale=2.0,\n            overlay=False):\n        \"\"\"\n        Plots a set of points with size scaled according to the data\n\n        :param bool logplot:\n            Choose to scale according to the logarithm (base 10) of the data\n        :param float smin:\n            Minimum scale size\n        :param float sscale:\n            Scaling factor\n        \"\"\"\n        if logplot:\n            data = np.log10(data.copy())\n\n        x, y, = self.m(longitude, latitude)\n        self.m.scatter(x, y,\n                       marker=shape,\n                       s=(smin + data ** sscale),\n                       c=colour,\n                       alpha=alpha,\n                       zorder=2)\n        if not overlay:\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_focal_mechanism(self, catalogue, magnitude=None, overlay=True):\n        longitude = catalogue.data['longitude']\n        latitude = catalogue.data['latitude']\n        strike = catalogue.data['strike1']\n        dip = catalogue.data['dip1']\n        rake = catalogue.data['rake1']\n\n        if not magnitude or (magnitude < 0):\n            magnitude = catalogue.data['magnitude']\n            for i, mag in enumerate(magnitude):\n                color = self._select_color_mag(mag)\n                focal_mechanism = [strike[i], dip[i], rake[i]]\n                x, y = self.m(longitude[i], latitude[i])\n                self.m.plot(x, y)\n                size = mag * 10000\n                beach = Beach(focal_mechanism, linewidth=1, xy=(x, y),\n                              width=size, zorder=size, facecolor=color)\n                self.ax.add_collection(beach)\n                if not overlay:\n                    plt.show()\n        else:\n            for i in range(0, catalogue.get_number_tensors()):\n                x, y = self.m(longitude[i], latitude[i])\n                self.m.plot(x, y)\n                focal_mechanism = [strike[i], dip[i], rake[i]]\n                size = magnitude * 10000.\n                beach = Beach(focal_mechanism, linewidth=1, xy=(x, y),\n                              width=size, zorder=size, facecolor='r')\n                self.ax.add_collection(beach)\n                if not overlay:\n                    plt.show()", "response": "Adds a focal mechanism to the beachball representation of the catalogue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_catalogue_cluster(self, catalogue, vcl, flagvector,\n                              cluster_id=None, overlay=True):\n        \"\"\"\n        Creates a plot of a catalogue showing where particular clusters exist\n        \"\"\"\n        # Create simple magnitude scaled point basemap\n        self.add_size_scaled_points(catalogue.data['longitude'],\n                                    catalogue.data['latitude'],\n                                    catalogue.data['magnitude'],\n                                    shape=\"o\",\n                                    alpha=0.8,\n                                    colour=(0.5, 0.5, 0.5),\n                                    smin=1.0,\n                                    sscale=1.5,\n                                    overlay=True)\n        # If cluster ID is not specified just show mainshocks\n        if cluster_id is None:\n            idx = flagvector == 0\n            self.add_size_scaled_points(catalogue.data['longitude'][idx],\n                                        catalogue.data['latitude'][idx],\n                                        catalogue.data['magnitude'][idx],\n                                        shape=\"o\",\n                                        colour=\"r\",\n                                        smin=1.0,\n                                        sscale=1.5,\n                                        overlay=overlay)\n            return\n        if not isinstance(cluster_id, collections.Iterable):\n            cluster_id = [cluster_id]\n        for iloc, clid in enumerate(cluster_id):\n            if iloc == (len(cluster_id) - 1):\n                # On last iteration set overlay to function overlay\n                temp_overlay = overlay\n            else:\n                temp_overlay = True\n            idx = vcl == clid\n            self.add_size_scaled_points(\n                catalogue.data[\"longitude\"][idx],\n                catalogue.data[\"latitude\"][idx],\n                catalogue.data[\"magnitude\"][idx],\n                shape=\"o\",\n                colour=DISSIMILAR_COLOURLIST[(iloc + 1) % NCOLS],\n                smin=1.0,\n                sscale=1.5,\n                overlay=temp_overlay)", "response": "Adds a single cluster to the catalogue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        assert all(stddev_type in self.DEFINED_FOR_STANDARD_DEVIATION_TYPES\n                   for stddev_type in stddev_types)\n\n        C = self.COEFFS[imt]\n        mean = self._get_mean(\n            C, rup.mag, rup.rake, rup.dip, dists.rrup, dists.rjb\n        )\n        stddevs = self._get_stddevs(C, rup.mag, stddev_types, dists.rrup.size)\n\n        return mean, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_mean(self, C, mag, rake, dip, rrup, rjb):\n        f1 = self._compute_magnitude_scaling(C, mag)\n        f2 = self._compute_distance_scaling(C, mag, rrup)\n        f3 = self._compute_faulting_mechanism(C, rake, dip)\n        f4 = self._compute_far_source_soil_effect(C)\n        f5 = self._compute_hanging_wall_effect(C, rjb, rrup, dip, mag)\n\n        mean = (\n            C['c1'] + f1 + C['c4'] * np.log(np.sqrt(f2)) + f3 + f4 + f5\n        )\n\n        return mean", "response": "Compute and return mean value in equation 1 page 319."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_stddevs(self, C, mag, stddev_types, num_sites):\n        std = C['c16'] + np.zeros(num_sites)\n\n        if mag < 7.4:\n            std -= 0.07 * mag\n        else:\n            std -= 0.518\n\n        # only the 'total' standard deviation is supported, therefore the\n        # std is always the same for all types\n        stddevs = [std for _ in stddev_types]\n\n        return stddevs", "response": "Returns the standard deviation as defined in eq. 11 page 319."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing distance scaling term in equation 3 page 319.", "response": "def _compute_distance_scaling(self, C, mag, rrup):\n        \"\"\"\n        Compute distance scaling term (eq.3, page 319).\n\n        The distance scaling assumes the near-source effect of local site\n        conditions due to 50% very firm soil and soft rock and 50% firm rock.\n        \"\"\"\n        g = C['c5'] + C['c6'] * 0.5 + C['c7'] * 0.5\n\n        return (\n            rrup ** 2 +\n            (np.exp(C['c8'] * mag + C['c9'] * (8.5 - mag) ** 2) * g) ** 2\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_faulting_mechanism(self, C, rake, dip):\n        # flag for reverse faulting\n        frv = float((dip > 45) and (22.5 <= rake <= 157.5))\n        # flag for thrust faulting\n        fth = float((dip <= 45) and (22.5 <= rake <= 157.5))\n\n        return C['c10'] * frv + C['c11'] * fth", "response": "Compute faulting mechanism term in equation 3. 1 page 319."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_hanging_wall_effect(self, C, rjb, rrup, dip, mag):\n        # eq. 8 (to be noticed that the USGS-NSHMP implementation defines\n        # the hanging-wall term for all rjb distances, while in the original\n        # manuscript, hw is computed only for rjb < 5). Again the 'firm rock'\n        # is considered\n        hw = np.zeros_like(rjb)\n        if dip <= 70.:\n            hw = (5. - rjb) / 5.\n\n        # eq. 9\n        f_m = 1 if mag > 6.5 else mag - 5.5\n\n#        # eq. 10\n        f_rrup = C['c15'] + np.zeros_like(rrup)\n        idx = rrup < 8\n        f_rrup[idx] *= rrup[idx] / 8\n\n        # eq. 7 (to be noticed that the f3 factor is not included\n        # while this is defined in the original manuscript)\n        f_hw = hw * f_m * f_rrup\n\n        return f_hw", "response": "Compute hanging - wall effect for USGS - NSHMP entries in the internal table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cached_property(method):\n    name = method.__name__\n\n    def newmethod(self):\n        try:\n            val = self.__dict__[name]\n        except KeyError:\n            val = method(self)\n            self.__dict__[name] = val\n        return val\n    newmethod.__name__ = method.__name__\n    newmethod.__doc__ = method.__doc__\n    return property(newmethod)", "response": "A property decorator that caches the result of a method."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the distinct keys in order.", "response": "def distinct(keys):\n    \"\"\"\n    Return the distinct keys in order.\n    \"\"\"\n    known = set()\n    outlist = []\n    for key in keys:\n        if key not in known:\n            outlist.append(key)\n        known.add(key)\n    return outlist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndivide a and b and return the biggest integer close to the quotient.", "response": "def ceil(a, b):\n    \"\"\"\n    Divide a / b and return the biggest integer close to the quotient.\n\n    :param a:\n        a number\n    :param b:\n        a positive number\n    :returns:\n        the biggest integer close to the quotient\n    \"\"\"\n    assert b > 0, b\n    return int(math.ceil(float(a) / b))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef block_splitter(items, max_weight, weight=lambda item: 1, key=nokey):\n    if max_weight <= 0:\n        raise ValueError('max_weight=%s' % max_weight)\n    ws = WeightedSequence([])\n    prev_key = 'Unspecified'\n    for item in items:\n        w = weight(item)\n        k = key(item)\n        if w < 0:  # error\n            raise ValueError('The item %r got a negative weight %s!' %\n                             (item, w))\n        elif ws.weight + w > max_weight or k != prev_key:\n            new_ws = WeightedSequence([(item, w)])\n            if ws:\n                yield ws\n            ws = new_ws\n        elif w > 0:  # ignore items with 0 weight\n            ws.append((item, w))\n        prev_key = k\n    if ws:\n        yield ws", "response": "A generator that yields the items of the same kind in order to split them into two lists of items of the same kind."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a number into num_slices", "response": "def split_in_slices(number, num_slices):\n    \"\"\"\n    :param number: a positive number to split in slices\n    :param num_slices: the number of slices to return (at most)\n    :returns: a list of slices\n\n    >>> split_in_slices(4, 2)\n    [slice(0, 2, None), slice(2, 4, None)]\n    >>> split_in_slices(5, 1)\n    [slice(0, 5, None)]\n    >>> split_in_slices(5, 2)\n    [slice(0, 3, None), slice(3, 5, None)]\n    >>> split_in_slices(2, 4)\n    [slice(0, 1, None), slice(1, 2, None)]\n    \"\"\"\n    assert number > 0, number\n    assert num_slices > 0, num_slices\n    blocksize = int(math.ceil(number / num_slices))\n    slices = []\n    start = 0\n    while True:\n        stop = min(start + blocksize, number)\n        slices.append(slice(start, stop))\n        if stop == number:\n            break\n        start += blocksize\n    return slices"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits the sequence in a number of WeightedSequences close to hint.", "response": "def split_in_blocks(sequence, hint, weight=lambda item: 1, key=nokey):\n    \"\"\"\n    Split the `sequence` in a number of WeightedSequences close to `hint`.\n\n    :param sequence: a finite sequence of items\n    :param hint: an integer suggesting the number of subsequences to generate\n    :param weight: a function returning the weigth of a given item\n    :param key: a function returning the key of a given item\n\n    The WeightedSequences are of homogeneous key and they try to be\n    balanced in weight. For instance\n\n     >>> items = 'ABCDE'\n     >>> list(split_in_blocks(items, 3))\n     [<WeightedSequence ['A', 'B'], weight=2>, <WeightedSequence ['C', 'D'], weight=2>, <WeightedSequence ['E'], weight=1>]\n\n    \"\"\"\n    if isinstance(sequence, int):\n        return split_in_slices(sequence, hint)\n    elif hint in (0, 1) and key is nokey:  # do not split\n        return [sequence]\n    elif hint in (0, 1):  # split by key\n        blocks = []\n        for k, group in groupby(sequence, key).items():\n            blocks.append(group)\n        return blocks\n    items = sorted(sequence, key=lambda item: (key(item), weight(item)))\n    assert hint > 0, hint\n    assert len(items) > 0, len(items)\n    total_weight = float(sum(weight(item) for item in items))\n    return block_splitter(items, math.ceil(total_weight / hint), weight, key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_close(a, b, rtol=1e-07, atol=0, context=None):\n    if isinstance(a, float) or isinstance(a, numpy.ndarray) and a.shape:\n        # shortcut\n        numpy.testing.assert_allclose(a, b, rtol, atol)\n        return\n    if isinstance(a, (str, bytes, int)):\n        # another shortcut\n        assert a == b, (a, b)\n        return\n    if hasattr(a, '_slots_'):  # record-like objects\n        assert a._slots_ == b._slots_\n        for x in a._slots_:\n            assert_close(getattr(a, x), getattr(b, x), rtol, atol, x)\n        return\n    if hasattr(a, 'keys'):  # dict-like objects\n        assert a.keys() == b.keys()\n        for x in a:\n            if x != '__geom__':\n                assert_close(a[x], b[x], rtol, atol, x)\n        return\n    if hasattr(a, '__dict__'):  # objects with an attribute dictionary\n        assert_close(vars(a), vars(b), context=a)\n        return\n    if hasattr(a, '__iter__'):  # iterable objects\n        xs, ys = list(a), list(b)\n        assert len(xs) == len(ys), ('Lists of different lenghts: %d != %d'\n                                    % (len(xs), len(ys)))\n        for x, y in zip(xs, ys):\n            assert_close(x, y, rtol, atol, x)\n        return\n    if a == b:  # last attempt to avoid raising the exception\n        return\n    ctx = '' if context is None else 'in context ' + repr(context)\n    raise AssertionError('%r != %r %s' % (a, b, ctx))", "response": "Compare for equality up to a given precision two composite objects\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a temporary file with the given content.", "response": "def gettemp(content=None, dir=None, prefix=\"tmp\", suffix=\"tmp\"):\n    \"\"\"Create temporary file with the given content.\n\n    Please note: the temporary file must be deleted by the caller.\n\n    :param string content: the content to write to the temporary file.\n    :param string dir: directory where the file should be created\n    :param string prefix: file name prefix\n    :param string suffix: file name suffix\n    :returns: a string with the path to the temporary file\n    \"\"\"\n    if dir is not None:\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n    fh, path = tempfile.mkstemp(dir=dir, prefix=prefix, suffix=suffix)\n    _tmp_paths.append(path)\n    if content:\n        fh = os.fdopen(fh, \"wb\")\n        if hasattr(content, 'encode'):\n            content = content.encode('utf8')\n        fh.write(content)\n        fh.close()\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removetmp():\n    for path in _tmp_paths:\n        if os.path.exists(path):  # not removed yet\n            try:\n                os.remove(path)\n            except PermissionError:\n                pass", "response": "Remove the temporary files created by gettemp\nridge"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef git_suffix(fname):\n    # we assume that the .git folder is two levels above any package\n    # i.e. openquake/engine/../../.git\n    git_path = os.path.join(os.path.dirname(fname), '..', '..', '.git')\n\n    # macOS complains if we try to execute git and it's not available.\n    # Code will run, but a pop-up offering to install bloatware (Xcode)\n    # is raised. This is annoying in end-users installations, so we check\n    # if .git exists before trying to execute the git executable\n    if os.path.isdir(git_path):\n        try:\n            gh = subprocess.check_output(\n                ['git', 'rev-parse', '--short', 'HEAD'],\n                stderr=open(os.devnull, 'w'),\n                cwd=os.path.dirname(git_path)).strip()\n            gh = \"-git\" + decode(gh) if gh else ''\n            return gh\n        except Exception:\n            # trapping everything on purpose; git may not be installed or it\n            # may not work properly\n            pass\n\n    return ''", "response": "Returns the short git hash if git repository found otherwise returns an empty string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a given Python code and returns the output of the external process as a Python object.", "response": "def run_in_process(code, *args):\n    \"\"\"\n    Run in an external process the given Python code and return the\n    output as a Python object. If there are arguments, then code is\n    taken as a template and traditional string interpolation is performed.\n\n    :param code: string or template describing Python code\n    :param args: arguments to be used for interpolation\n    :returns: the output of the process, as a Python object\n    \"\"\"\n    if args:\n        code %= args\n    try:\n        out = subprocess.check_output([sys.executable, '-c', code])\n    except subprocess.CalledProcessError as exc:\n        print(exc.cmd[-1], file=sys.stderr)\n        raise\n    if out:\n        return eval(out, {}, {})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport all modules contained in a package and returns the names of the modules that were imported.", "response": "def import_all(module_or_package):\n    \"\"\"\n    If `module_or_package` is a module, just import it; if it is a package,\n    recursively imports all the modules it contains. Returns the names of\n    the modules that were imported as a set. The set can be empty if\n    the modules were already in sys.modules.\n    \"\"\"\n    already_imported = set(sys.modules)\n    mod_or_pkg = importlib.import_module(module_or_package)\n    if not hasattr(mod_or_pkg, '__path__'):  # is a simple module\n        return set(sys.modules) - already_imported\n    # else import all modules contained in the package\n    [pkg_path] = mod_or_pkg.__path__\n    n = len(pkg_path)\n    for cwd, dirs, files in os.walk(pkg_path):\n        if all(os.path.basename(f) != '__init__.py' for f in files):\n            # the current working directory is not a subpackage\n            continue\n        for f in files:\n            if f.endswith('.py'):\n                # convert PKGPATH/subpackage/module.py -> subpackage.module\n                # works at any level of nesting\n                modname = (module_or_package + cwd[n:].replace(os.sep, '.') +\n                           '.' + os.path.basename(f[:-3]))\n                importlib.import_module(modname)\n    return set(sys.modules) - already_imported"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nasserting that the specified package is not depend on the specified packages.", "response": "def assert_independent(package, *packages):\n    \"\"\"\n    :param package: Python name of a module/package\n    :param packages: Python names of modules/packages\n\n    Make sure the `package` does not depend from the `packages`.\n    \"\"\"\n    assert packages, 'At least one package must be specified'\n    import_package = 'from openquake.baselib.general import import_all\\n' \\\n                     'print(import_all(\"%s\"))' % package\n    imported_modules = run_in_process(import_package)\n    for mod in imported_modules:\n        for pkg in packages:\n            if mod.startswith(pkg):\n                raise CodeDependencyError('%s depends on %s' % (package, pkg))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a module in syspath and return the filepath of the module.", "response": "def search_module(module, syspath=sys.path):\n    \"\"\"\n    Given a module name (possibly with dots) returns the corresponding\n    filepath, or None, if the module cannot be found.\n\n    :param module: (dotted) name of the Python module to look for\n    :param syspath: a list of directories to search (default sys.path)\n    \"\"\"\n    lst = module.split(\".\")\n    pkg, submodule = lst[0], \".\".join(lst[1:])\n    try:\n        fileobj, filepath, descr = imp.find_module(pkg, syspath)\n    except ImportError:\n        return\n    if submodule:  # recursive search\n        return search_module(submodule, [filepath])\n    return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef groupby(objects, key, reducegroup=list):\n    kgroups = itertools.groupby(sorted(objects, key=key), key)\n    return {k: reducegroup(group) for k, group in kgroups}", "response": "Group by a list of objects by a key function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract a subarray by filtering on the keyword arguments", "response": "def get_array(array, **kw):\n    \"\"\"\n    Extract a subarray by filtering on the given keyword arguments\n    \"\"\"\n    for name, value in kw.items():\n        array = array[array[name] == value]\n    return array"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef not_equal(array_or_none1, array_or_none2):\n    if array_or_none1 is None and array_or_none2 is None:\n        return False\n    elif array_or_none1 is None and array_or_none2 is not None:\n        return True\n    elif array_or_none1 is not None and array_or_none2 is None:\n        return True\n    if array_or_none1.shape != array_or_none2.shape:\n        return True\n    return (array_or_none1 != array_or_none2).any()", "response": "Compare two arrays that can also be None or have diffent shapes\n    and returns a boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a human - friendly file size in a human - friendly format.", "response": "def humansize(nbytes, suffixes=('B', 'KB', 'MB', 'GB', 'TB', 'PB')):\n    \"\"\"\n    Return file size in a human-friendly format\n    \"\"\"\n    if nbytes == 0:\n        return '0 B'\n    i = 0\n    while nbytes >= 1024 and i < len(suffixes) - 1:\n        nbytes /= 1024.\n        i += 1\n    f = ('%.2f' % nbytes).rstrip('0').rstrip('.')\n    return '%s %s' % (f, suffixes[i])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deprecated(func, msg='', *args, **kw):\n    msg = '%s.%s has been deprecated. %s' % (\n        func.__module__, func.__name__, msg)\n    if not hasattr(func, 'called'):\n        warnings.warn(msg, DeprecationWarning, stacklevel=2)\n        func.called = 0\n    func.called += 1\n    return func(*args, **kw)", "response": "A decorator that marks a function as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a list of objects returns a sublist by extracting randomly some elements.", "response": "def random_filter(objects, reduction_factor, seed=42):\n    \"\"\"\n    Given a list of objects, returns a sublist by extracting randomly\n    some elements. The reduction factor (< 1) tells how small is the extracted\n    list compared to the original list.\n    \"\"\"\n    assert 0 < reduction_factor <= 1, reduction_factor\n    rnd = random.Random(seed)\n    out = []\n    for obj in objects:\n        if rnd.random() <= reduction_factor:\n            out.append(obj)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_histogram(counts, nbins, seed):\n    numpy.random.seed(seed)\n    return numpy.histogram(numpy.random.random(counts), nbins, (0, 1))[0]", "response": "Distribute a total number of counts on a set of bins homogenously."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_indices(integers):\n    indices = AccumDict(accum=[])  # idx -> [(start, stop), ...]\n    start = 0\n    for i, vals in itertools.groupby(integers):\n        n = sum(1 for val in vals)\n        indices[i].append((start, start + n))\n        start += n\n    return indices", "response": "Returns a dictionary of indices for the given sequence of integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts and print characters using the proper encoding", "response": "def safeprint(*args, **kwargs):\n    \"\"\"\n    Convert and print characters using the proper encoding\n    \"\"\"\n    new_args = []\n    # when stdout is redirected to a file, python 2 uses ascii for the writer;\n    # python 3 uses what is configured in the system (i.e. 'utf-8')\n    # if sys.stdout is replaced by a StringIO instance, Python 2 does not\n    # have an attribute 'encoding', and we assume ascii in that case\n    str_encoding = getattr(sys.stdout, 'encoding', None) or 'ascii'\n    for s in args:\n        new_args.append(s.encode('utf-8').decode(str_encoding, 'ignore'))\n\n    return print(*new_args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a socket is ready and returns True if the socket is ready and False otherwise.", "response": "def socket_ready(hostport):\n    \"\"\"\n    :param hostport: a pair (host, port) or a string (tcp://)host:port\n    :returns: True if the socket is ready and False otherwise\n    \"\"\"\n    if hasattr(hostport, 'startswith'):\n        # string representation of the hostport combination\n        if hostport.startswith('tcp://'):\n            hostport = hostport[6:]  # strip tcp://\n        host, port = hostport.split(':')\n        hostport = (host, int(port))\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        exc = sock.connect_ex(hostport)\n    finally:\n        sock.close()\n    return False if exc else True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zipfiles(fnames, archive, mode='w', log=lambda msg: None, cleanup=False):\n    prefix = len(os.path.commonprefix([os.path.dirname(f) for f in fnames]))\n    with zipfile.ZipFile(\n            archive, mode, zipfile.ZIP_DEFLATED, allowZip64=True) as z:\n        for f in fnames:\n            log('Archiving %s' % f)\n            z.write(f, f[prefix:])\n            if cleanup:  # remove the zipped file\n                os.remove(f)\n    log('Generated %s' % archive)\n    return archive", "response": "Builds a zip archive from the given file names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detach_process():\n    # see https://pagure.io/python-daemon/blob/master/f/daemon/daemon.py and\n    # https://stackoverflow.com/questions/45911705/why-use-os-setsid-in-python\n    def fork_then_exit_parent():\n        pid = os.fork()\n        if pid:  # in parent\n            os._exit(0)\n    fork_then_exit_parent()\n    os.setsid()\n    fork_then_exit_parent()", "response": "Detach the current process from the controlling terminal by using double fork."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef println(msg):\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n    sys.stdout.write('\\x08' * len(msg))\n    sys.stdout.flush()", "response": "Print a message on the terminal"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a debug line to the file", "response": "def debug(templ, *args):\n    \"\"\"\n    Append a debug line to the file /tmp/debug.txt\n    \"\"\"\n    msg = templ % args if args else templ\n    tmp = tempfile.gettempdir()\n    with open(os.path.join(tmp, 'debug.txt'), 'a', encoding='utf8') as f:\n        f.write(msg + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a warning on stderr", "response": "def warn(msg, *args):\n    \"\"\"\n    Print a warning on stderr\n    \"\"\"\n    if not args:\n        sys.stderr.write('WARNING: ' + msg)\n    else:\n        sys.stderr.write('WARNING: ' + msg % args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts an item into the sequence with the given weight.", "response": "def insert(self, i, item_weight):\n        \"\"\"\n        Insert an item with the given weight in the sequence\n        \"\"\"\n        item, weight = item_weight\n        self._seq.insert(i, item)\n        self.weight += weight"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, *keys):\n        def decorator(func):\n            for key in keys:\n                self[key] = func\n            return func\n        return decorator", "response": "Returns a decorator registering a new implementation for the given keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply(self, func, *extras):\n        return self.__class__({key: func(value, *extras)\n                               for key, value in self.items()})", "response": "Apply a function to all the keys and values in the current dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self, array):\n        assert len(self.array) == len(array)\n        arr = object.__new__(self.__class__)\n        arr.dt = self.dt\n        arr.slicedic = self.slicedic\n        arr.array = array\n        return arr", "response": "Convert an array of compatible length into a new DictArray"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        # extracting dictionary of coefficients specific to required\n        # intensity measure type.\n\n        C = self.COEFFS[imt]\n        imean = self._get_mean(C, rup, dists, sites)\n        if imt.name in \"SA PGA\":\n            # Convert units to g,\n            # but only for PGA and SA (not PGV):\n            mean = np.log((10.0 ** (imean - 2.0)) / g)\n        else:\n            # PGV:\n            mean = np.log(10.0 ** imean)\n\n        istddevs = self._get_stddevs(C, stddev_types, len(sites.vs30))\n        stddevs = np.log(10.0 ** np.array(istddevs))\n        return mean + self.adjustment_factor, stddevs", "response": "Returns the mean and standard deviation for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_magnitude_scaling_term(self, C, mag):\n        dmag = mag - self.CONSTS[\"Mh\"]\n        if mag < self.CONSTS[\"Mh\"]:\n            return C[\"e1\"] + (C[\"b1\"] * dmag) + (C[\"b2\"] * (dmag ** 2.0))\n        else:\n            return C[\"e1\"] + (C[\"b3\"] * dmag)", "response": "Returns the magnitude scaling term of the GMPE described in the equation 3\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the distance scaling term in equation 2", "response": "def _get_distance_scaling_term(self, C, rval, mag):\n        \"\"\"\n        Returns the distance scaling term of the GMPE described in equation 2\n        \"\"\"\n        r_adj = np.sqrt(rval ** 2.0 + C[\"h\"] ** 2.0)\n        return (\n            (C[\"c1\"] + C[\"c2\"] * (mag - self.CONSTS[\"Mref\"])) *\n            np.log10(r_adj / self.CONSTS[\"Rref\"]) -\n            (C[\"c3\"] * (r_adj - self.CONSTS[\"Rref\"])))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_style_of_faulting_term(self, C, rup):\n        SS, NS, RS = 0.0, 0.0, 0.0\n        if np.abs(rup.rake) <= 30.0 or (180.0 - np.abs(rup.rake)) <= 30.0:\n            # strike-slip\n            SS = 1.0\n        elif rup.rake > 30.0 and rup.rake < 150.0:\n            # reverse\n            RS = 1.0\n        else:\n            # normal\n            NS = 1.0\n        return (C[\"sofN\"] * NS) + (C[\"sofR\"] * RS) + (C[\"sofS\"] * SS)", "response": "Returns the style - of - faulting term in equation 1 page 970."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_site_amplification_term(self, C, vs30):\n        return C[\"gamma\"] * np.log10(vs30 / self.CONSTS[\"Vref\"])", "response": "Returns the site amplification term for the case in which Vs30\n        is used directly"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the site amplification term given Eurocode 8 site classification", "response": "def _get_site_amplification_term(self, C, vs30):\n        \"\"\"\n        Returns the site amplification given Eurocode 8 site classification\n        \"\"\"\n        f_s = np.zeros_like(vs30)\n        # Site class B\n        idx = np.logical_and(vs30 < 800.0, vs30 >= 360.0)\n        f_s[idx] = C[\"eB\"]\n        # Site Class C\n        idx = np.logical_and(vs30 < 360.0, vs30 >= 180.0)\n        f_s[idx] = C[\"eC\"]\n        # Site Class D\n        idx = vs30 < 180.0\n        f_s[idx] = C[\"eD\"]\n        return f_s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_mean(self, C, rup, dists, sites):\n        return (self._get_magnitude_scaling_term(C, rup.mag) +\n                self._get_distance_scaling_term(C, dists.rjb, rup.mag) +\n                self._get_site_amplification_term(C, sites.vs30))", "response": "Returns the mean value of the ground motion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_loss_tables(dstore):\n    oq = dstore['oqparam']\n    L = len(oq.loss_dt().names)\n    R = dstore['csm_info'].get_num_rlzs()\n    serials = dstore['ruptures']['serial']\n    idx_by_ser = dict(zip(serials, range(len(serials))))\n    tbl = numpy.zeros((len(serials), L), F32)\n    lbr = numpy.zeros((R, L), F32)  # losses by rlz\n    for rec in dstore['losses_by_event'].value:  # call .value for speed\n        idx = idx_by_ser[rec['eid'] // TWO32]\n        tbl[idx] += rec['loss']\n        lbr[rec['rlzi']] += rec['loss']\n    return tbl, lbr", "response": "Compute the total losses by rupture and losses by rlzi."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef event_based_risk(riskinputs, riskmodel, param, monitor):\n    L = len(riskmodel.lti)\n    epspath = param['epspath']\n    for ri in riskinputs:\n        with monitor('getting hazard'):\n            ri.hazard_getter.init()\n            hazard = ri.hazard_getter.get_hazard()\n        mon = monitor('build risk curves', measuremem=False)\n        A = len(ri.aids)\n        R = ri.hazard_getter.num_rlzs\n        try:\n            avg = numpy.zeros((A, R, L), F32)\n        except MemoryError:\n            raise MemoryError(\n                'Building array avg of shape (%d, %d, %d)' % (A, R, L))\n        result = dict(aids=ri.aids, avglosses=avg)\n        acc = AccumDict()  # accumulator eidx -> agglosses\n        aid2idx = {aid: idx for idx, aid in enumerate(ri.aids)}\n        if 'builder' in param:\n            builder = param['builder']\n            P = len(builder.return_periods)\n            all_curves = numpy.zeros((A, R, P), builder.loss_dt)\n        # update the result dictionary and the agg array with each output\n        for out in riskmodel.gen_outputs(ri, monitor, epspath, hazard):\n            if len(out.eids) == 0:  # this happens for sites with no events\n                continue\n            r = out.rlzi\n            agglosses = numpy.zeros((len(out.eids), L), F32)\n            for l, loss_type in enumerate(riskmodel.loss_types):\n                loss_ratios = out[loss_type]\n                if loss_ratios is None:  # for GMFs below the minimum_intensity\n                    continue\n                avalues = riskmodels.get_values(loss_type, ri.assets)\n                for a, asset in enumerate(ri.assets):\n                    aval = avalues[a]\n                    aid = asset['ordinal']\n                    idx = aid2idx[aid]\n                    ratios = loss_ratios[a]  # length E\n\n                    # average losses\n                    avg[idx, r, l] = (\n                        ratios.sum(axis=0) * param['ses_ratio'] * aval)\n\n                    # agglosses\n                    agglosses[:, l] += ratios * aval\n                    if 'builder' in param:\n                        with mon:  # this is the heaviest part\n                            all_curves[idx, r][loss_type] = (\n                                builder.build_curve(aval, ratios, r))\n\n            # NB: I could yield the agglosses per output, but then I would\n            # have millions of small outputs with big data transfer and slow\n            # saving time\n            acc += dict(zip(out.eids, agglosses))\n\n        if 'builder' in param:\n            clp = param['conditional_loss_poes']\n            result['curves-rlzs'], result['curves-stats'] = builder.pair(\n                all_curves, param['stats'])\n            if R > 1 and param['individual_curves'] is False:\n                del result['curves-rlzs']\n            if clp:\n                result['loss_maps-rlzs'], result['loss_maps-stats'] = (\n                    builder.build_maps(all_curves, clp, param['stats']))\n                if R > 1 and param['individual_curves'] is False:\n                    del result['loss_maps-rlzs']\n\n        # store info about the GMFs, must be done at the end\n        result['agglosses'] = (numpy.array(list(acc)),\n                               numpy.array(list(acc.values())))\n        yield result", "response": "Generate an event - based risk for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a list of tuples describing a table into a HTML string", "response": "def html(header_rows):\n    \"\"\"\n    Convert a list of tuples describing a table into a HTML string\n    \"\"\"\n    name = 'table%d' % next(tablecounter)\n    return HtmlTable([map(str, row) for row in header_rows], name).render()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a HTML string containing all the tabs we want to display.", "response": "def make_tabs(tag_ids, tag_status, tag_contents):\n    \"\"\"\n    Return a HTML string containing all the tabs we want to display\n    \"\"\"\n    templ = '''\n<div id=\"tabs\">\n<ul>\n%s\n</ul>\n%s\n</div>'''\n    lis = []\n    contents = []\n    for i, (tag_id, status, tag_content) in enumerate(\n            zip(tag_ids, tag_status, tag_contents), 1):\n        mark = '.' if status == 'complete' else '!'\n        lis.append('<li><a href=\"#tabs-%d\">%s%s</a></li>' % (i, tag_id, mark))\n        contents.append('<div id=\"tabs-%d\">%s</div>' % (\n            i, tag_content))\n    return templ % ('\\n'.join(lis), '\\n'.join(contents))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_report(isodate='today'):\n    if isodate == 'today':\n        isodate = date.today()\n    else:\n        isodate = date(*time.strptime(isodate, '%Y-%m-%d')[:3])\n    isodate1 = isodate + timedelta(1)  # +1 day\n\n    tag_ids = []\n    tag_status = []\n    tag_contents = []\n\n    # the fetcher returns an header which is stripped with [1:]\n    jobs = dbcmd(\n        'fetch', ALL_JOBS, isodate.isoformat(), isodate1.isoformat())\n    page = '<h2>%d job(s) finished before midnight of %s</h2>' % (\n        len(jobs), isodate)\n    for job_id, user, status, ds_calc in jobs:\n        tag_ids.append(job_id)\n        tag_status.append(status)\n        [stats] = dbcmd('fetch', JOB_STATS, job_id)\n        (job_id, user, start_time, stop_time, status, duration) = stats\n        try:\n            ds = read(job_id, datadir=os.path.dirname(ds_calc))\n            txt = view_fullreport('fullreport', ds)\n            report = html_parts(txt)\n        except Exception as exc:\n            report = dict(\n                html_title='Could not generate report: %s' % cgi.escape(\n                    str(exc), quote=True),\n                fragment='')\n        page = report['html_title']\n        page += html([stats._fields, stats])\n        page += report['fragment']\n        tag_contents.append(page)\n\n    page = make_tabs(tag_ids, tag_status, tag_contents) + (\n        'Report last updated: %s' % datetime.now())\n    fname = 'jobs-%s.html' % isodate\n    with open(fname, 'w') as f:\n        f.write(PAGE_TEMPLATE % page)\n    return fname", "response": "Builds a HTML report with the computations performed at the given isodate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scenario_risk(riskinputs, riskmodel, param, monitor):\n    E = param['E']\n    L = len(riskmodel.loss_types)\n    result = dict(agg=numpy.zeros((E, L), F32), avg=[],\n                  all_losses=AccumDict(accum={}))\n    for ri in riskinputs:\n        for out in riskmodel.gen_outputs(ri, monitor, param['epspath']):\n            r = out.rlzi\n            weight = param['weights'][r]\n            slc = param['event_slice'](r)\n            for l, loss_type in enumerate(riskmodel.loss_types):\n                losses = out[loss_type]\n                if numpy.product(losses.shape) == 0:  # happens for all NaNs\n                    continue\n                stats = numpy.zeros(len(ri.assets), stat_dt)  # mean, stddev\n                for a, asset in enumerate(ri.assets):\n                    stats['mean'][a] = losses[a].mean()\n                    stats['stddev'][a] = losses[a].std(ddof=1)\n                    result['avg'].append((l, r, asset['ordinal'], stats[a]))\n                agglosses = losses.sum(axis=0)  # shape num_gmfs\n                result['agg'][slc, l] += agglosses * weight\n                if param['asset_loss_table']:\n                    aids = ri.assets['ordinal']\n                    result['all_losses'][l, r] += AccumDict(zip(aids, losses))\n    return result", "response": "This function is used to compute the risk model for a single object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef workers(cmd):\n    if config.dbserver.multi_user and getpass.getuser() != 'openquake':\n        sys.exit('oq workers only works in single user mode')\n\n    master = workerpool.WorkerMaster(config.dbserver.host,\n                                     **config.zworkers)\n    print(getattr(master, cmd)())", "response": "start stop or return their status"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the activity rate as an instance of the : class : EvenlyDiscretizedMFD class.", "response": "def to_evenly_discretized_mfd(self):\n        \"\"\"\n        Returns the activity rate as an instance of the :class:\n        openquake.hazardlib.mfd.evenly_discretized.EvenlyDiscretizedMFD\n        \"\"\"\n        return EvenlyDiscretizedMFD(self.mmin + self.bin_width / 2.,\n                                    self.bin_width,\n                                    self.occurrence_rate.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the upper and lower depth limits of the seismogenic tree element.", "response": "def _check_depth_limits(input_dict):\n    '''Returns the default upper and lower depth values if not in dictionary\n\n    :param input_dict:\n        Dictionary corresponding to the kwargs dictionary of calling function\n\n    :returns:\n        'upper_depth': Upper seismogenic depth (float)\n        'lower_depth': Lower seismogenic depth (float)\n    '''\n    if ('upper_depth' in input_dict.keys()) and input_dict['upper_depth']:\n        if input_dict['upper_depth'] < 0.:\n            raise ValueError('Upper seismogenic depth must be positive')\n        else:\n            upper_depth = input_dict['upper_depth']\n    else:\n        upper_depth = 0.0\n\n    if ('lower_depth' in input_dict.keys()) and input_dict['lower_depth']:\n        if input_dict['lower_depth'] < upper_depth:\n            raise ValueError('Lower depth must take a greater value than'\n                             ' upper depth!')\n        else:\n            lower_depth = input_dict['lower_depth']\n    else:\n        lower_depth = np.inf\n    return upper_depth, lower_depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_decimal_from_datetime(time):\n    '''\n    As the decimal time function requires inputs in the form of numpy\n    arrays need to convert each value in the datetime object  to a single\n    numpy array\n    '''\n\n    # Get decimal seconds from seconds + microseconds\n    temp_seconds = np.float(time.second) + (np.float(time.microsecond) / 1.0E6)\n    return decimal_time(np.array([time.year], dtype=int),\n                        np.array([time.month], dtype=int),\n                        np.array([time.day], dtype=int),\n                        np.array([time.hour], dtype=int),\n                        np.array([time.minute], dtype=int),\n                        np.array([temp_seconds], dtype=int))", "response": "Returns a decimal time object from a datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_catalogue(self, valid_id):\n        '''\n        Method to post-process the catalogue based on the selection options\n\n        :param numpy.ndarray valid_id:\n            Boolean vector indicating whether each event is selected (True)\n            or not (False)\n\n        :returns:\n            Catalogue of selected events as instance of\n            openquake.hmtk.seismicity.catalogue.Catalogue class\n        '''\n        if not np.any(valid_id):\n            # No events selected - create clean instance of class\n            output = Catalogue()\n            output.processes = self.catalogue.processes\n\n        elif np.all(valid_id):\n            if self.copycat:\n                output = deepcopy(self.catalogue)\n            else:\n                output = self.catalogue\n        else:\n            if self.copycat:\n                output = deepcopy(self.catalogue)\n            else:\n                output = self.catalogue\n            output.purge_catalogue(valid_id)\n        return output", "response": "Method to post - process the catalogue based on the selection options."}
